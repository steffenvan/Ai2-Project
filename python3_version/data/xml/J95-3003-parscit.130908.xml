<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996163">
Collaborating on Referring Expressions
</title>
<author confidence="0.994948">
Peter A. Heeman* Graeme Hirstt
</author>
<affiliation confidence="0.999302">
University of Rochester University of Toronto
</affiliation>
<bodyText confidence="0.998006083333333">
This paper presents a computational model of how conversational participants collaborate in
order to make a referring action successful. The model is based on the view of language as
goal-directed behavior. We propose that the content of a referring expression can be accounted
for by the planning paradigm. Not only does this approach allow the processes of building re-
ferring expressions and identifying their referents to be captured by plan construction and plan
inference, it also allows us to account for how participants clarify a referring expression by us-
ing meta-actions that reason about and manipulate the plan derivation that corresponds to the
referring expression. To account for how clarification goals arise and how inferred clarification
plans affect the agent, we propose that the agents are in a certain state of mind, and that this state
includes an intention to achieve the goal of referring and a plan that the agents are currently
considering. It is this mental state that sanctions the adoption of goals and the acceptance of
inferred plans, and so acts as a link between understanding and generation.
</bodyText>
<sectionHeader confidence="0.990065" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.998646347826087">
People are goal-oriented and can plan courses of actions to achieve their goals. But
sometimes they might lack the knowledge needed to formulate a plan of action, or
some of the actions that they plan might depend on coordinating their activity with
other agents. How do they cope? One way is to work together, or collaborate, in for-
mulating a plan of action with other people who are involved in the actions or who
know the relevant information.
Even the apparently simple linguistic task of referring, in an utterance, to some
object or idea can involve exactly this kind of activity: a collaboration between the
speaker and the hearer. The speaker has the goal of the hearer identifying the object
that the speaker has in mind. The speaker attempts to achieve this goal by constructing
a description of the object that she thinks will enable the hearer to identify it. But since
the speaker and the hearer will inevitably have different beliefs about the world, the
hearer might not be able to identify the object. Often, when the hearer cannot do so, the
speaker and hearer collaborate in making a new referring expression that accomplishes
the goal.
This paper presents a computational model of how a conversational participant
collaborates in making a referring action successful. We use as our basis the model
proposed by Clark and Wilkes-Gibbs (1986), which gives a descriptive account of
the conversational moves that participants make when collaborating upon a referring
expression. We cast their work into a model based on the planning paradigm.
We propose that referring expressions can be represented by plan derivations, and
that plan construction and plan inference can be used to generate and understand
them. Not only does this approach allow the processes of building referring expres-
</bodyText>
<affiliation confidence="0.9321955">
* Department of Computer Science, Rochester, New York 14627. E-mail: heeman@cs.rochester.edu.
t Department of Computer Science, Toronto, Canada, M5S 1A4. E-mail: gh@cs.toronto.edu.
</affiliation>
<note confidence="0.879945">
© 1995 Association for Computational Linguistics
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.99993022">
sions and identifying their referents to be captured in the planning paradigm, but it also
allows us to use the planning paradigm to account for how participants clarify a refer-
ring expression. In this case, we use meta-actions that encode how a plan derivation
corresponding to a referring expression can be reasoned about and manipulated.
To complete the picture, we also need to account for the fact that the conversants
are collaborating. We propose that the agents are in a mental state that includes not
only an intention to achieve the goal of the collaborative activity but also a plan that
the participants are currently considering. In the case of referring, this will be the plan
derivation that corresponds to the referring expression. This plan is in the common
ground of the participants, and we propose rules that are sanctioned by the mental
state both for accepting plans that clarify the current plan, and for adopting goals to do
likewise. The acceptance of a clarification results in the current plan being updated.
So, it is these rules that specify how plan inference and plan construction affect and
are affected by the mental state of the agent. Thus, the mental state, together with the
rules, provides the link between these two processes. An important consequence of
our proposal is that the current plan need not allow the successful achievement of the
goal. Likewise, the clarifications that agents propose need not result in a successful
plan in order for them to be accepted.
As can be seen, our approach consists of two tiers. The first tier is the planning
component, which accounts for how utterances are both understood and generated.
Using the planning paradigm has several advantages: it allows both tasks to be cap-
tured in a single paradigm that is used for modeling general intelligent behavior; it
allows more of the content of an utterance to be accounted for by a uniform process;
and only a single knowledge source for referring expressions is needed instead of
having this knowledge embedded in special algorithms for each task. The second tier
accounts for the collaborative behavior of the agents: how they adopt goals and co-
ordinate their activity It provides the link between the mental state of the agent and
the planning processes.
In accounting for how agents collaborate in making a referring action, our work
aims to make the following contributions to the field. First, although much work has
been done on how agents request clarifications, or respond to such requests, little
attention has been paid to the collaborative aspects of clarification discourse. Our
work attempts a plan-based formalization of what linguistic collaboration is, both in
terms of the goals and intentions that underlie it and the surface speech acts that
result from it. Second, we address the act of referring and show how it can be better
accounted for by the planning paradigm. Third, previous plan-based linguistic research
has concentrated on either construction or understanding of utterances, but not both.
By doing both, we will give our work generality in the direction of a complete model
of the collaborative process. Finally, by using Clark and Wilkes-Gibbs&apos;s model as a
basis for our work, we aim not only to add support to their model, but also to gain a
much richer understanding of the subject.
In order to address the problem that we have set out, we have limited the scope
of our work. First, we look at referring expressions in isolation, rather than as part
of a larger speech act. Second, we assume that agents have mutual knowledge of the
mechanisms of referring expressions and collaboration. Third, we deal with objects
that both the speaker and hearer know of, though they might have different beliefs
about what propositions hold for these objects. Fourth, as the input and the output
to our system, we use representations of surface speech actions, not natural language
strings. Finally, although belief revision is an important part of how agents collaborate,
we do not explicitly address this.
</bodyText>
<page confidence="0.987921">
352
</page>
<note confidence="0.65228">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
2. Referring as a Collaborative Process
</note>
<bodyText confidence="0.999416466666667">
Clark and Wilkes-Gibbs (1986) investigated how participants in a conversation collab-
orate in making a referring action successful. They conducted experiments in which
participants had to refer to objects—tangram patterns—that are difficult to describe.
They found that typically the participant trying to describe a tangram pattern would
present an initial referring expression. The other participant would then pass judg-
ment on it, either accepting it, rejecting it, or postponing his decision. If it was rejected or
the decision postponed, then one participant or the other would refashion the referring
expression. This would take the form of either repairing the expression by correcting
speech errors, expanding it by adding further qualifications, or replacing the original
expression with a new expression. The referring expression that results from this is
then judged, and the process continues until the referring expression is acceptable
enough to the participants for current purposes. This final expression is contributed
to the participants&apos; common ground.
Below are two excerpts from Clark and Wilkes-Gibbs&apos;s experiments that illustrate
the acceptance process.
</bodyText>
<listItem confidence="0.47412625">
(2.1) A: 1 Um, third one is the guy reading with, holding his book to the left.
B: 2 Okay, kind of standing up?
A:3 Yeah.
B: Okay.
</listItem>
<bodyText confidence="0.9839021">
In this dialog, person A makes an initial presentation in line 1. Person B postpones
his decision in line 2 by voicing a tentative &amp;quot;okay,&amp;quot; and then proceeds to refashion
the referring expression, the result being &amp;quot;the guy reading, holding his book to the
left, kind of standing up.&amp;quot; A accepts the new expression in line 3, and B signals his
acceptance in line 4.
(2.2) A:1 Okay, and the next one is the person that looks like they&apos;re carrying
something and it&apos;s sticking out to the left. It looks like a hat that&apos;s
upside down.
B: 2 The guy that&apos;s pointing to the left again?
A: 3 Yeah, pointing to the left, that&apos;s it! (laughs)
B: 4 Okay.
In the second dialog, B implicitly rejects A&apos;s initial presentation by replacing it with a
new referring expression in line 2, &amp;quot;the guy that&apos;s pointing to the left again.&amp;quot; A then
accepts the refashioned referring expression in line 3.
Below, we give an algorithmic interpretation of Clark and Wilkes-Gibbs&apos;s collabo-
rative model, where present, judge, and refashion are the conversational moves that
the participants make, and ref, re, and judgment are variables that represent the referent,
the current referring expression, and its judgment, respectively. (Since the conversa-
tional moves update the referring expression and its judgment, they are presented as
functions.)
</bodyText>
<page confidence="0.997202">
353
</page>
<note confidence="0.847308">
Computational Linguistics Volume 21, Number 3
</note>
<equation confidence="0.987037166666667">
re = present(ref)
judgment = judge(ref,re)
while (judgment accept)
re = refashion(refre)
judgment = judge(ref,re)
end-while
</equation>
<bodyText confidence="0.999967619047619">
The algorithm illustrates how the collaborative activity progresses by the participants
judging and refashioning the previously proposed referring expression.&apos; In fact, we
can see that the state of the process is characterized by the current referring expression,
re, and the judgment of it, judgment, and that this state must be part of the common
ground of the participants. The algorithm also illustrates how the model of Clark and
Wilkes-Gibbs minimizes the distinction between the roles of the person who initiated
the referring expression and the person who is trying to identify it. Both have the same
moves available to them, for either can judge the description and either can refashion
it. Neither is controlling the dialog, they are simply collaborating.
In later work, Clark and Schaefer (1989) propose that &amp;quot;each part of the acceptance
phase is itself a contribution&amp;quot; (p. 269), and the acceptance of these contributions de-
pends on whether the hearer &amp;quot;believes he is understanding well enough for current
purposes&amp;quot; (p. 267). Although Clark and Schaefer use the term contribution with respect
to the discourse, rather than the collaborative effort of referring, their proposal is still
relevant here: judgments and refashionings are contributions to the collaborative effort
and are subjected to an acceptance process, with the result being that once they are ac-
cepted, the state of the collaborative activity is updated. So, what constitutes grounds
for accepting a judgment or clarification? In order to be consistent with Clark and
Wilkes-Gibbs&apos;s model, we can see that if one agent finds the current referring expres-
sion problematic, the other must accept that judgment. Likewise, if one agent proposes
a referring expression, through a refashioning, the other must accept the refashioning.
</bodyText>
<sectionHeader confidence="0.996017" genericHeader="method">
3. Referring Expressions
</sectionHeader>
<subsectionHeader confidence="0.999331">
3.1 Planning and Referring
</subsectionHeader>
<bodyText confidence="0.999919117647059">
By viewing language as action, the planning paradigm can be applied to natural lan-
guage processing. The actions in this case are speech acts (Austin 1962; Searle 1969),
and include such things as promising, informing, and requesting. Cohen and Perrault
(1979) developed a system that uses plan construction to map an agent&apos;s goals to
speech acts, and Allen and Perrault (1980) use plan inference to understand an agent&apos;s
plan from its speech acts. By viewing it as action (Searle 1969), referring can be incor-
porated into a planning model. Cohen&apos;s model (1981) planned requests that the hearer
identify a referent, whereas Appelt (1985) planned concept activations, a generalization
of referring actions.
Although acts of reference have been incorporated into plan-based models, de-
termining the content of referring expressions hasn&apos;t been. For instance, in Appelt&apos;s
model, concept activations can be achieved by the action describe, which is a primitive,
not further decomposed. Rather, this action has an associated procedure that deter-
mines a description that satisfies the preconditions of describe. Such special procedures
have been the mainstay for accounting for the content of referring expressions, both in
constructing and in understanding them, as exemplified by Dale (1989), who chose de-
scriptors on the basis of their discriminatory power; Ehud Reiter (1990), who focused
</bodyText>
<footnote confidence="0.54321">
1 For simplicity, we have not shown the change in speakers between refashionings and judgments.
</footnote>
<page confidence="0.994082">
354
</page>
<note confidence="0.752607">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
</note>
<bodyText confidence="0.999933071428571">
on avoiding misleading conversational implicatures when generating descriptions; and
Mellish (1985), who used a constraint satisfaction algorithm to identify referents.
Our work follows the plan-based approach to language generation and under-
standing. We extend the earlier approaches of Cohen and Appelt by accounting for
the content of the description at the planning level. This is done by having surface
speech actions for each component of a description, plus a surface speech action that
expresses a speaker&apos;s intention to refer. A referring action is composed of these prim-
itive actions, and the speaker utters them in her attempt to refer to an object.
These speech actions are the building blocks that referring expressions are made
from. Acting as the mortar are intermediate actions, which have constraints that the
plan construction and plan inference processes can reason about. These constraints
encode the knowledge of how a description can allow a hearer to identify an object.
First, the constraints express the conditions under which an attribute can be used
to refer to an object; for instance, that it be mutually believed that the object has a
certain property (Clark and Marshall 1981; Perrault and Cohen 1981; Nadathur and
Joshi 1983). Second, the constraints keep track of which objects could be believed to be
the referent of the referring expression. Third, the constraints ensure that a sufficient
number of surface speech actions are added so that the set of candidates associated
with the entire referring expression consists of only a single object, the referent. These
constraints enable the speaker to construct a referring expression that she believes will
allow the hearer to identify the referent. As for the hearer, the explicit encoding of the
adequacy of referring expressions allows referent identification to fall out of the plan
inference process.
Our approach to treating referring as a plan in which surface speech actions cor-
respond to the components of the description allows us to capture how participants
collaborate in building a referring expression. Plan repair techniques can be used to
refashion an expression if it is not adequate, and clarifications can refer to the part
of the plan derivation that is in question or is being repaired. Thus we can model a
collaborative dialog in terms of the changes that are being made to the plan derivation.
The referring expression plans that we propose are not simply data structures, but
are mental objects that agents have beliefs about (Pollack 1990). The plan derivation
expresses beliefs of the speaker: how actions contribute to the achievement of the goal,
and what constraints hold that will allow successful identification.&apos; So plan construc-
tion reasons about the beliefs of the agent in constructing a referring plan; likewise,
plan inference, after hypothesizing a plan that is consistent with the observed actions,
reasons about the other participant&apos;s (believed) beliefs in satisfying the constraints of
the plan. If the hearer is able to satisfy the constraints, then he will have understood
the plan and be able to identify the referent, since a term corresponding to it would
have been instantiated in the inferred plan. Otherwise, there will be an action that
includes a constraint that is unsatisfiable, and the hearer construes the action as being
in error. (We do not reason about how the error affects the satisfiability of the goal of
the plan nor use the error to revise the beliefs of the hearer.)
</bodyText>
<subsectionHeader confidence="0.99993">
3.2 Vocabulary and Notation
</subsectionHeader>
<bodyText confidence="0.9895395">
Before we present the action schemas for referring expressions, we need to introduce
the notation that we use. Our terminology for planning follows the general literature.3
</bodyText>
<footnote confidence="0.91086675">
2 Since we assume that the agents have mutual knowledge of the action schemas and that agents can
execute surface speech actions, we do not consider beliefs about generation or about the executability
of primitive actions.
3 See the introductory chapter of Allen, Hendler, and Tate (1990) for an overview of planning.
</footnote>
<page confidence="0.99202">
355
</page>
<note confidence="0.464196">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.99990580952381">
We use the terms action schema, plan derivation, plan construction, and plan inference.
An action schema consists of a header, constraints, a decomposition, and an effect; and
it encodes the constraints under which an effect can be achieved by performing the
steps in the decomposition. A plan derivation is an instance of an action that has
been recursively expanded into primitive actions—its yield. Each component in the
plan—the action headers, constraints, steps, and effects—are referred to as nodes of
the plan, and are given names so as to distinguish two nodes that have the same
content. Finally, plan construction is the process of finding a plan derivation whose
yield will achieve a given effect, and plan inference is the process of finding a plan
derivation whose yield is a set of observed primitive actions.
The action schemas make use of a number of predicates, and these are defined
in Table 1. We adopt the Prolog convention that variables begin with an uppercase
letter, and all predicates and constants begin with a lowercase letter. Two constants
that need to be mentioned are system and user. The first denotes the agent that we are
modeling, and the latter, her conversational partner. Since the action schemas are used
for both constructing the plans of the system, and inferring the plans of the user, it is
sometimes necessary to refer to the speaker or hearer in a general way. For this we use
the propositions speaker(Speaker) and hearer(Hearer). These instantiate the variables
Speaker and Hearer to system or user; which is which depends on whether the rule
is being used for plan construction or plan inference. These propositions are included
as constraints in the action schemas as needed.
</bodyText>
<subsectionHeader confidence="0.992469">
3.3 Action Schemas
</subsectionHeader>
<bodyText confidence="0.9998380625">
This section presents action schemas for referring expressions. (We omit discussion of
actions that account for superlative adjectives, such as &amp;quot;largest,&amp;quot; that describe an object
relative to the set of objects that match the rest of the description. A full presentation
is given by Heeman [19911.)
As we mentioned, the action for referring, called refer, is mapped to the surface
speech actions through the use of intermediate actions and plan decomposition. All of
the reasoning is done in the refer action and the intermediate actions, so no constraints
or effects are included in the surface speech actions.
We use three surface speech actions. The first is s-refer(Entity), which is used to
express the speaker&apos;s intention to refer. The second is s-attrib(Entity,Predicate), and
is used for describing an object in terms of an attribute; Entity is the discourse entity
of the object, and Predicate is a lambda expression, such as AX. category(X,bird), that
encodes the attribute. The third is s-attrib-rel(Entity,OtherEntity,Predicate), and is
used for describing an object in terms of some other object. In this case Predicate is
a lambda expression of two variables, one corresponding to Entity, and the other to
OtherEntity; for instance, AX. AY. in(X,Y).
</bodyText>
<sectionHeader confidence="0.322573" genericHeader="method">
Refer Action. The schema for refer is shown in Figure 1. The refer action decomposes
</sectionHeader>
<figureCaption confidence="0.971818">
Figure 1
refer schema.
</figureCaption>
<figure confidence="0.817188444444444">
Header:
Constraint:
Decomposition:
Effect:
refer(Entity,Object)
knowref(Hearer,Speaker,Entity,Object)
s-refer(Entity)
describe(Entity,Object)
bel(Hearer,goal(Speaker,knowref(Hearer,Speaker,Entity,Object)))
</figure>
<page confidence="0.78016">
356
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
</page>
<tableCaption confidence="0.996739">
Table 1
</tableCaption>
<subsectionHeader confidence="0.2398375">
Predicates and actions.
Belief
</subsectionHeader>
<bodyText confidence="0.406967833333333">
bel(Agt,Prop): Agt believes that Prop is true.
bmb(Agt1,Agt2,Prop): Agt1 believes that it is mutually believed between himself and Agt2 that
Prop is true.
knowref(Agt1,Agt2,Ent,Obj): Agt1 knows the referent that Agt2 associates with the discourse
entity Ent (Webber 1983), which Agt1 believes to be Obj. (Proving this proposition with
Ent unbound will cause a unique identifier to be created for Ent.)
</bodyText>
<subsectionHeader confidence="0.953622">
Goals and Plans
</subsectionHeader>
<bodyText confidence="0.807849583333333">
goal(Agt,Goal): Agt has the goal Goal. Agents act to make their goals true.
plan(Agt,Plan,Goal): Agt has the goal of Goal and has adopted the plan derivation Plan as a
means to achieve it. The agent believes that each action of Plan contributes to the goal, but
not necessarily that all of the constraints hold; in other words, the plan must be coherent
but not necessarily valid (Pollack 1990, p. 94).
content(Plan,Node,Content): The node named by Node in Plan has content Content.
yield(Plan,Node,Actions): The subplan rooted at Node in Plan has a yield of the primitive
actions Actions.
achieve(Plan,Goal): Executing Plan will cause Goal to be true.
error(Plan,Node): Plan has an error at the action labeled Node. Errors are attributed to the action
that contains the failed constraint. This predicate is used to encode an agent&apos;s belief about
an invalidity in a plan.
</bodyText>
<subsectionHeader confidence="0.982905">
Plan Repair
</subsectionHeader>
<bodyText confidence="0.915463333333333">
substitute(Plan,Node,NewAction,NewPlan): Undo all variable bindings in Plan (except those
in primitive actions, and then substitute the action header NewAction into Plan at Node,
resulting in the partial plan NewPlan.
replan(Plan,Actions): Complete the partial plan Plan. Actions are the primitive actions that are
added to the plan.
replace(Plan,NewPlan): The plan NewPlan replaces Plan.
</bodyText>
<subsectionHeader confidence="0.831139">
Miscellaneous
</subsectionHeader>
<bodyText confidence="0.9385398">
subset(Set,Lambda,Subset): Compute the subset, Subset, of Set that satisfies the lambda expres-
sion Lambda.
modifier-absolute-pred(Pred): Pred is a predicate that an object can be described in terms of.
Used by the modifier-absolute schema given in Figure 6.
modifier-relative-pred(Pred): Pred is a predicate that describes the relationship between two
objects. Used by the modifier-relative schema given in Figure 7.
pick-one(Object,Set): Pick one object, Object, of the members of Set.
speaker(Agt): Agt is the current speaker.
hearer(Agt): Agt is the current hearer.
into two steps: s-refer, which expresses the speaker&apos;s intention to refer, and describe,
which accounts for the content of the referring expression (given next). The effect
of refer is that the hearer should believe that the speaker has a goal of the hearer
knowing the referent of the referring expression. The effect has been formulated in
this way because we are assuming that when a speaker has a communicative goal
she plans to achieve the goal by making the hearer recognize it; the effect will be
</bodyText>
<page confidence="0.989269">
357
</page>
<figure confidence="0.98355725">
Computational Linguistics Volume 21, Number 3
Header: describe(Entity,Object)
Decomposition: headnoun(Entity,Object,Cand)
modifiers(Entity,Object,Cand)
</figure>
<figureCaption confidence="0.69501">
Figure 2
describe schema.
Figure 3
</figureCaption>
<bodyText confidence="0.987332034482758">
headnoun schema.
achieved by the hearer inferring the speaker&apos;s plan, regardless of whether or not the
hearer is able to determine the actual referent. To simplify our implementation, this
is the only effect that is stated for the action schemas for referring expressions. It
corresponds to the literal goal that Appelt and Kronfeld (1987) propose (whereas the
actual identification is their condition of satisfaction).
Intermediate Actions. The describe action, shown in Figure 2, is used to construct a
description of the object through its decomposition into headnoun and modifiers. The
variable Cand is the candidate set, the set of potential referents associated with the
head noun that is chosen, and it is passed to the modifiers action so that it can ensure
that the rest of the description rules out all of the alternatives.
The action headnoun, shown in Figure 3, has a single step, s-attrib, which is the
surface speech action used to describe an object in terms of some predicate, which
for the headnoun schema, is restricted to the category of the object.&apos; The schema also
has two constraints. The first ensures that the referent is of the chosen category and
the second determines the candidate set, Cand, associated with the head noun that
is chosen. The candidate set is computed by finding the subset of the objects in the
world that the speaker believes could be referred to by the head noun—the objects
that the speaker and hearer have an appropriate mutual belief about.
The modifiers action attempts to ensure that the referring expression that is being
constructed is believed by the speaker to allow the hearer to uniquely identify the
referent. We have defined modifiers as a recursive action, with two schemas.5 The first
schema, shown in Figure 4, is used to terminate the recursion, and its constraint spec-
ifies that only one object can be in the candidate set.6 The second schema, shown in
Figure 5, embodies the recursion. It uses the modifier plan, which adds a component
to the description and updates the candidate set by computing the subset of it that sat-
isfies the new component. The modifier plan thus accounts for individual components
of the description.
There are two different action schemas for modifier; one is for absolute modifiers,
</bodyText>
<footnote confidence="0.9267464">
4 Note that several category predications might be true of an object, and we do not explore which would
be best to use, but see Edmonds (1994) for how preferences can be encoded.
5 We use specialization axioms (Kautz and Allen 1986) to map the modifiers action to the two schemas:
modifiers-terminate and modifiers-recurse.
6 In order to distinguish this action from the primitive actions, it has a step that is marked null.
</footnote>
<figure confidence="0.935605230769231">
Header:
Constraint:
Decomposition:
headnoun(Entity,Object,Cand)
world(World)
bmb(Speaker,Hearer,category(Object,Category))
subset(World,AX. bmb(Speaker,Hearer,category(X,Category)),Cand)
s-attrib(Entity,AX. categonf(X,Category))
358
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
Header: modifiers-terminate(Entity,Object,Cand)
Constraint: Cand = (Object]
Decomposition: null
</figure>
<figureCaption confidence="0.600979">
Figure 4
</figureCaption>
<bodyText confidence="0.278454">
modifiers schema for terminating the recursion.
</bodyText>
<equation confidence="0.674131833333333">
Header: modifiers-recurse(Entity,Object,Cand)
Decomposition: modifier(Entity,Object,Cand,NewCand)
modifiers(Entity,Object,NewCand)
Figure 5
modifiers schema for recursing.
Header: modifier-absolute(Entity,Object,Cand,NewCand)
Constraint: modifier-pred(Pred)
bmb(Speaker,Hearer,Pred(X))
subset(Cand,AX. bmb(Speaker,Hearer,Pred(X)),NewCand)
Decomposition: s-attrib(Entity,Pred)
Figure 6
modifier schema for absolute modifiers.
Header: modifier-relative(Entity,Object,Cand,NewCand)
Constraint: modifier-rel-pred(Pred)
bmb(Speaker,Hearer,Pred(Object,OtherObject))
subset(Cand,).X.bmb(Speaker,Hearer,Pred(X)(0ther)),NewCand)
Decomposition: s-attrib-rel(Entity,OtherEntity,Pred)
refer(OtherEntity,Other)
</equation>
<figureCaption confidence="0.994303">
Figure 7
</figureCaption>
<bodyText confidence="0.996455875">
modifier schema for relative modifiers.
such as &amp;quot;black,&amp;quot; and the other is for relative modifiers, such as &amp;quot;larger.&amp;quot; The former
is shown in Figure 6; it decomposes into the surface speech action s-attrib and has
a constraint that determines the new candidate set, NewCand, by including only the
objects from the old candidate set, Cand, for which the predicate could be believed
to be true. The other schema is shown in Figure 7 and is used for describing objects
in terms of some other object. It uses the surface speech action s-attrib-rel and also
includes a step to refer to the object of comparison.
</bodyText>
<subsectionHeader confidence="0.999801">
3.4 Plan Construction and Plan Inference
</subsectionHeader>
<bodyText confidence="0.99940075">
The goals that we are interested in achieving are communicative goals. Since these
goals cannot be directly achieved by a plan of action, the speaker must instead plan
actions that will achieve them indirectly, for instance by planning an utterance that
results in the hearer recognizing her goal. So, if the speaker wants to achieve Goal,
she will attempt to construct a plan whose effect is bel(Hearer,goal(Speaker,Goal)).
Plan Construction. Given an effect, the plan constructor finds a plan derivation that
has a minimal number of primitive actions, that is valid (with respect to the planning
agent&apos;s beliefs), and whose root action achieves the effect. The plan constructor uses a
</bodyText>
<page confidence="0.988988">
359
</page>
<note confidence="0.449167">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.999856571428571">
best-first search strategy, expanding the derivation with the fewest number of surface
speech actions. The yield of this plan derivation can then be given as input to a module
that generates the surface form of the utterance.
Plan Inference. Following Pollack (1990), our plan inference process can infer plans in
which, in the hearer&apos;s view, a constraint does not hold. In inferring a plan derivation,
we first find the set of plan derivations that account for the primitive actions that
were observed, without regard to whether the constraints hold. This is done by using
a chart parser that parses actions rather than words (Sidner 1994; Vilain 1990). For
referring plans that contain more than one modifier, there will be multiple derivations
corresponding to the order of the modifiers. We avoid this ambiguity by choosing an
arbitrary ordering of the modifiers for each such plan.
In the second part of the plan inference process, we evaluate each derivation by
attempting to find an instantiation for the variables such that all of the constraints
hold with respect to the hearer&apos;s beliefs about the speaker&apos;s beliefs. It could, however,
be the case that there is no instantiation, either because this is not the right derivation
or because the plan is based on beliefs not shared by the speaker and the hearer. In
the latter case, we need to determine which action in the plan is to blame, so that this
knowledge can be shared with the other participant.
After each derivation has been evaluated, if there is just one valid derivation, an
instantiated derivation whose constraints all hold, then the hearer will believe that he
has understood. If there is just one derivation and it is invalid, the action containing
the constraint that is the source of the invalidity is noted. (We have not explored
ambiguous situations, those in which more than one valid derivation remains, or, in
the absence of validity, more than one invalid derivation.)
We now need to address how we evaluate a derivation. In the case where the plan
is invalid, we need to partially evaluate the plan in order to determine which action
contains a constraint that cannot be satisfied. However, any instantiation will lead to
some constraint being found not to hold. Care must therefore be taken in finding the
right instantiation so that blame is attributed to the action at fault. So, we evaluate the
constraints in order of mention in the derivation, but postpone any constraints that
have multiple solutions until the end. We have found that this simple approach can
find the instantiation for valid plans and can find the action that is in error for the
others.
To illustrate this, consider the headnoun action, which has the following con-
straints.
</bodyText>
<equation confidence="0.9958612">
speaker(Speaker)
hearer(Hearer)
world(World)
bmb(Speaker,Hearer,category(Object,Category))
subset(VVorld,AX.bmb(Speaker,Hearer,category(X,Category)),Cand)
</equation>
<bodyText confidence="0.999788">
During the first step, finding the derivation, all co-referential variables will be unified.
In particular, the variable Category will be instantiated from the co-referential variable
in the surface speech action. The first three constraints have only a single solution, so
they are instantiated. The fourth constraint contains Object. If there is exactly one
object that the system believes to be mutually believed to be of Category, then Object
is instantiated to it. If there is none, then this constraint is unsatisfiable, and so the
evaluation of this plan stops with this action marked as being in error, since no object
matches this part of the description. If there is more than one, then this constraint is
</bodyText>
<page confidence="0.972946">
360
</page>
<note confidence="0.371913">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
</note>
<bodyText confidence="0.999985583333333">
postponed and the evaluator moves on to the subset constraint. This constraint has
one uninstantiated variable, Cand, which has a unique (non-null) solution, namely the
candidate set associated with the head noun. So, this constraint is evaluated.
The evaluation then proceeds through the actions in the rest of the plan. Assum-
ing that no intervening errors are encountered, the evaluator will eventually reach the
constraint on the terminating instance of modifiers, Cand = [Object], with Cand instan-
tiated to a non-null set. If Cand contains more than one object, then this constraint will
fail, pinning the blame on the terminating instance of modifiers for there not being
enough descriptors to allow the referent to be identified. Otherwise, the terminating
constraint will be satisfiable, and so Object will be instantiated to the single object in
the candidate set. This will then allow all of the mutual belief constraints that were
postponed to be evaluated, since they will now have only a single solution.
</bodyText>
<sectionHeader confidence="0.987583" genericHeader="method">
4. Clarifications
</sectionHeader>
<subsectionHeader confidence="0.99994">
4.1 Planning and Clarifying
</subsectionHeader>
<bodyText confidence="0.999896033333333">
Clark and Wilkes-Gibbs (1986) have presented a model of how conversational partic-
ipants collaborate in making a referring action successful (see Section 2 above). Their
model consists of conversational moves that express a judgment of a referring expres-
sion and conversational moves that refashion an expression. However, their model
is not computational. They do not account for how the judgment is made, how the
judgment affects the refashioning, or the content of the moves.
Following the work of Litman and Allen (1987) in understanding clarification sub-
dialogs, we formalize the conversational moves of Clark and Wilkes-Gibbs as discourse
actions. These discourse actions are meta-actions that take as a parameter a referring
expression plan. The constraints and decompositions of the discourse actions encode
the conditions under which they can be applied, how the referring expression deriva-
tions can be refashioned, and how the speaker&apos;s beliefs can be communicated to the
hearer. So, the conversational moves, or clarifications, can be generated and under-
stood within the planning paradigm/
Surface Speech Actions. An important part of our model is the surface speech ac-
tions. These actions serve as the basis for communication between the two agents,
and so they must convey the information that is dictated by Clark and Wilkes-Gibbs&apos;s
model. For the judgment plans, we have the surface speech actions s-accept, s-reject,
and s-postpone, corresponding to the three possibilities in their model. These take as
a parameter the plan that is being judged, and for s-reject, also a subset of the speech
actions of the referring expression plan. The purpose of this subset is to inform the
hearer of the surface speech actions that the speaker found problematic. So, if the re-
ferring expression was &amp;quot;the weird creature,&amp;quot; and the hearer couldn&apos;t identify anything
that he thought &amp;quot;weird,&amp;quot; he might say &amp;quot;what weird thing,&amp;quot; thus indicating he had
problems with the surface speech action corresponding to &amp;quot;weird.&amp;quot;
For the refashioning plans, we propose that there is a single surface speech action,
s-actions, that is used for both replacing a part of a plan, and expanding it. This
action takes as a parameter the plan that is being refashioned and a set of surface
speech actions that the speaker wants to incorporate into the referring expression
plan. Since there is only one action, if it is uttered in isolation, it will be ambiguous
</bodyText>
<footnote confidence="0.724642">
7 We use the term clarification, since the conversational moves of judging and refashioning a referring
expression can be viewed as clarifying it.
</footnote>
<page confidence="0.989493">
361
</page>
<note confidence="0.450807">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.999943023255814">
between a replacement and an expansion; however, the speech action resulting from
the judgment will provide the proper context to disambiguate its meaning. In fact,
during linguistic realization, if the two actions are being uttered by the same person,
they could be combined into a single utterance. For instance, the utterance &amp;quot;no, the
red one&amp;quot; could be interpreted as an s-reject of the color that was previously used to
describe something and an s-actions for the color &amp;quot;red.&amp;quot;
So, as we can see, the surface speech actions for clarifications operate on compo-
nents of the plan that is being built, namely the surface speech actions of referring
expression plans. This is consistent with our use of plan derivations to represent ut-
terances. Although we could have viewed the clarification speech actions as acts of
informing (Litman and Allen 1987), this would have shifted the complexity into the
parameter of the inform, and it is unclear whether anything would have been gained.
Instead, we feel that a parser with a model of the discourse and the context can de-
termine the surface speech actions.8 Additionally, it should be easier for the generator
to determine an appropriate surface form.
Judgment Plans. The evaluation of the referring expression plan indicates whether
the referring action was successful or not. If it was successful, then the referent has
been identified, and so a goal to communicate this is input to the plan constructor.
This goal would be achieved by an instance of accept-plan, which decomposes into
the surface speech action s-accept.
If the evaluation wasn&apos;t successful, then the goal of communicating the error is
given to the plan constructor, where the error is simply represented by the node in
the derivation that the evaluation failed at. There are two reasons why the evaluation
could have failed: either no objects match or more than one matches. In the first case,
the referring expression is overconstrained, and the evaluation would have failed on
an action that decomposes into surface speech actions. In the second case, the referring
expression is underconstrained, and so the evaluation would have failed on the con-
straint that specifies the termination of the addition of modifiers. In our formalization
of the conversational moves, we have equated the first case to reject-plan and the
second case to postpone-plan, and their constraints test for the abovementioned con-
ditions. The actions reject-plan and postpone-plan decompose into the surface speech
actions s-reject and s-postpone, respectively.
By observing the surface speech action corresponding to the judgment, the hearer,
using plan inference, should be able to derive the speaker&apos;s judgment plan. If the
judgment was reject-plan or postpone-plan, then the evaluation of the judgment plan
should enable the hearer to determine the action in the referring plan that the speaker
found problematic due to the constraints specified in the action schemas. The identity
of the action in error will provide context for the subsequent refashioning of the
referring expression.&apos;
Refashioning Plans. If a conversant rejects a referring expression or postpones judg-
ment on it, then either the speaker or the hearer will refashion the expression in the
context of the rejection or postponement. In keeping with Clark and Wilkes-Gibbs, we
use two discourse plans for refashioning: replace-plan and expand-plan. The first is
</bodyText>
<footnote confidence="0.9425564">
8 See Levelt (1989, Chapter 12) for how prosody and clue words can be used in determining the type of
clarification.
9 Another approach would be to use the identity of the action in error to revise the beliefs that the agent
has attributed to the other conversant and to use the revised beliefs in refashioning the plan. However,
such reasoning is beyond the scope of this work.
</footnote>
<page confidence="0.993027">
362
</page>
<note confidence="0.383507">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
</note>
<bodyText confidence="0.999840742857143">
used to replace some of the actions in the referring expression plan with new ones, and
the second is to add new actions. Replacements can be used if the referring expression
either overconstrains or underconstrains the choice of referent, while the expansion
can be used only if it underconstrains the choice. So, these plans can check for these
conditions.
The decomposition of the refashioning plans encodes how a new referring ex-
pression can be constructed from the old one. This involves three tasks: first, a sin-
gle candidate referent is chosen; second, the referring expression is refashioned; and
third, this is communicated to the hearer by way of the action s-actions, which was
already discussed.&apos; The first step involves choosing a candidate. If the speaker of the
refashioning is the agent who initiated the referring expression, then this choice is
obviously pre-determined. Otherwise, the speaker must choose the candidate. Good-
man (1985) has addressed this problem for the case of when the referring expression
overconstrains the choice of referent. He uses heuristics to relax the constraints of the
description and to pick one that nearly fits it. This problem is beyond the scope of this
paper, and so we choose one of the referents arbitrarily (but see Heeman [19911 for
how a simplified version of Goodman&apos;s algorithm that relaxes only a single constraint
can be incorporated into the planning paradigm).
The second step is to refashion the referring expression so that it identifies the
candidate chosen in the first step. This is done by using plan repair techniques (Hayes
1975; Wilensky 1981; Wilkens 1985). Our technique is to remove the subplan rooted at
the action in error and replan with another action schema inserted in its place. This
technique has been encoded into our refashioning plans, and so can be used for both
constructing repairs and inferring how another agent has repaired a plan.
Now we consider the effect of these refashioning plans. As we mentioned in Sec-
tion 2, once the refashioning plan is accepted, the common ground of the participants
is updated with the new referring expression. So, the effect of the refashioning plans
is that the hearer will believe that the speaker wants the new referring expression
plan to replace the current one. Note that this effect does not make any claims about
whether the new expression will in fact enable the successful identification of the ref-
erent. For if it did, and if the new referring expression were invalid, this would imply
that the refashioning plan was also invalid, which is contrary to Clark and Wilkes-
Gibbs&apos;s model of the acceptance process. So, the understanding of a refashioning does
not depend on the understanding of the new proposed referring expression, but only
on its derivation.
</bodyText>
<subsectionHeader confidence="0.999848">
4.2 Action Schemas
</subsectionHeader>
<bodyText confidence="0.999948125">
This section presents action schemas for clarifications. Each clarification action includes
a surface speech action in its decomposition. However, all reasoning is done at the level
of the clarification actions, and so the surface actions do not include any constraints
or effects. The notation used in the action schemas was given in Table 1 above.
accept-plan. The discourse action accept-plan, shown in Figure 8, is used by the
speaker to establish the mutual belief that a plan will achieve its goal. The constraints
of the schema specify that the plan being accepted achieves its goal and the decompo-
sition is the surface speech action s-accept. The effect of the schema is that the hearer
</bodyText>
<footnote confidence="0.877506">
10 Another approach would have been to separate the communicative task from the first two (Lambert
and Carberry 1991).
</footnote>
<page confidence="0.994804">
363
</page>
<table confidence="0.947126583333333">
Computational Linguistics Volume 21, Number 3
Header: accept-plan(Plan)
Constraint: bel(Speaker,achieve(Plan,Goal))
Decomposition: s-accept(Plan)
Effect: bel(Hearer,goal(Speaker,bel(Hearer,bel(Speaker,
achieve(Plan,Goal)))))
Figure 8
accept-plan schema.
Header: reject-plan(Plan)
Constraint: bel(Speaker,error(Plan,ErrorNode))
Decomposition: yield(Plan,ErrorNode,Acts)
Effect: not(Acts = fl)
s-reject(Plan,Acts)
- bel(Hearer,goal(Speaker,bel(Hearer,bel(System,
error(Plan,ErrorNode)))))
Figure 9
reject-plan schema.
Header: postpone-plan(Plan)
Constraint: bel(Speaker,error(Plan,ErrorNode))
Decomposition: yield(Plan,ErrorNode,Acts)
Effect: Acts = 11
s-postpone(Plan,Acts)
bel(Hearer,goal(Speaker,bel(Hearer,bel(Speaker,
error(Plan,ErrorNode)))))
</table>
<figureCaption confidence="0.851605">
Figure 10
postpone-plan schema.
</figureCaption>
<bodyText confidence="0.993655235294117">
will believe that the speaker has the goal that it be mutually believed that the plan
achieves its goal.
reject-plan. The discourse action reject-plan, shown in Figure 9, is used by the speaker
if the referring expression plan overconstrains the choice of referent. The speaker uses
this schema in order to tell the hearer that the plan is invalid and which action instance
the evaluation failed in. The constraints require that the error occurred in an action
instance whose yield includes at least one primitive action. The decomposition consists
of s-reject, which takes as its parameter the surface speech actions that are in the yield
of the problematic action.
postpone-plan. The schema for postpone-plan, shown in Figure 10, is similar to reject-
plan. However, it requires that the error in the evaluation occurred in an action that
does not decompose into any primitive actions, which for referring expressions will
be the instance of modifiers that terminates the addition of modifiers.
replace-plan. The replace-plan schema is used by the speaker to replace some of the
primitive actions in a plan with new actions. Because we need knowledge of the type
of action where the error occurred in order to refashion the invalid plan, the constraints
of this schema are more specific than those of the judgment plans. The schema that
</bodyText>
<page confidence="0.987397">
364
</page>
<note confidence="0.622512">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
</note>
<equation confidence="0.903241923076923">
Header: replace-plan(Plan)
Constraint: bel(Speaker,error(Plan,ErrorNode))
content(Plan,ErrorNode,ErrorContent)
ErrorContent = modifier(Entity,Objectl,Cand,Candl)
Decomposition: pick-one(Object,Cand)
Replacement = modifier(Entity,Object,Cand,Cand2)
substitute(Plan,Node,Replacement,NewPlan)
replan(NewPlan,Acts)
s-actions(Plan,Acts)
Effect: bel(Hearer,goal(Speaker,bel(Hearer,bel(Speaker,
replace(Plan,NewPlan)))))
Figure 11
replace-plan schema.
</equation>
<bodyText confidence="0.999464380952381">
we give in Figure 11, for instance, is used to refashion a referring expression plan in
which the error occurred in an instance of a modifier action.&apos;
The decomposition of the schema specifies how a new referring expression plan
can be built.&apos; The first step, pick-one(Object,Cand), chooses one of the objects that
matched the part of the description that preceded the error; if the speaker is not the
initiator of the referring expression, then this is an arbitrary choice. The second step
specifies the header of the action schema that will be used to replace the subplan
that contained the error. The third step substitutes the replacement into the referring
expression plan, undoing all variable instantiations in the old plan. This results in
the partial plan NewPlan. The fourth step calls the plan constructor to complete the
partial plan. Finally, the fifth step is the surface speech action s-actions, which is used
to inform the hearer of the surface speech actions that are being added to the referring
expression plan.
expand-plan. The expand-plan schema, shown in Figure 12, is similar to the replace-
plan schema shown in Figure 11. The difference is that instead of replacing one of
the instances of modifier, it replaces the terminal instance of modifiers by a modifiers
subplan that distinguishes one of the objects from the others that match, thus effect-
ing an expansion of the surface speech actions. Even if the speaker thought that the
referring expression as it stands were adequate (since the candidate set Cand contains
only one member), she will construct a non-null expansion since the replacement is
the recursive version of modifiers.
</bodyText>
<subsectionHeader confidence="0.9991">
4.3 Plan Construction and Plan Inference
</subsectionHeader>
<bodyText confidence="0.9999042">
The general plan construction and plan inference processes are essentially the same
as those for referring expressions. However, the plan inference process has been aug-
mented so as to embody the criteria for understanding that were outlined in Section 4.1.
The inference of judgment plans must be sensitive to the fact that such a plan includes
the constraint that the speaker found the judged plan to be in error even though the
</bodyText>
<footnote confidence="0.9616122">
11 If the error occurred in an instance of headnoun, a different replace-plan schema would need to be
used, one that for instance relaxed the category that was used in describing the object (Goodman 1985;
Heeman 1991).
12 We refer to the steps in the decomposition that are not action headers as mental actions. They need to be
proved, just like constraints.
</footnote>
<page confidence="0.989032">
365
</page>
<figure confidence="0.588004666666667">
Computational Linguistics Volume 21, Number 3
Header: expand-plan(Plan)
Constraint: bel(Speaker,error(Plan,ErrorNode))
content(Plan,ErrorNode,ErrorContent)
ErrorContent = modifiers-terminate(Entity,Objectl,Cand)
Decomposition: pick-one(Object,Cand)
Replacement = modifiers-recurse(Entity,Object,Cand)
substitute(Plan,ErrorNode,Replacement,NewPlan)
repla- n(NewPlan,Acts)
s-actions(Plan,Acts)
Effect: bel(Hearer,goal(Speaker,bel(Hearer,bel(Speaker,
replace(Plan,NewPlan)))))
</figure>
<figureCaption confidence="0.680926">
Figure 12
</figureCaption>
<bodyText confidence="0.947993">
expand-plan schema.
hearer might not believe it to be. So, the inference process is allowed to assume that
the speaker believes any constraint that the goal of the plan implies.
In the case of a refashioning, the hearer might not view the proposed referring
expression plan as being sufficient for identifying the referent, but would nonetheless
understand the refashioning. So, the inference process requires only that the proposed
referring expression be derived—so that it can serve to replace the current plan—but
not that it be acceptable. So, when a replan action is part of a plan that is being
evaluated, the success of this action depends only on whether the plan that is its
parameter can be derived, but not whether the derived plan is valid.&apos;
</bodyText>
<sectionHeader confidence="0.971826" genericHeader="method">
5. Modeling Collaboration
</sectionHeader>
<bodyText confidence="0.936301956521739">
In the last two sections, we discussed how initial referring expressions, judgments,
and refashionings can be generated and understood in our plan-based model. In this
section, we show how plan construction and plan inference fit into a complete model
of how an agent collaborates in making a referring action successful. Previous natural
language systems that use plans to account for the surface speech acts underlying an
utterance (such as Cohen and Perrault 1979; Allen and Perrault 1980; Appelt 1985;
Litman and Allen 1987) model only the recognition or only the construction of an
agent&apos;s plans, and so do not address this issue.
In order to model an agent&apos;s participation in a dialog, we need to model how the
mental state of the agent changes as a result of the contributions that are made to the
dialog. The change in mental state can be modeled by the beliefs and goals that a par-
ticipant adopts. When a speaker produces an utterance, as long as the hearer finds it co-
herent, he can add a belief that the speaker has made the utterance to accomplish some
communicative goal. The hearer might then adopt some goal of his own in response to
this, and make an utterance that he believes will achieve this goal. Participants expect
each other to act in this way. These social norms allow participants to add to their
common ground by adopting the inferences about an utterance as mutual beliefs.
To account for how conversants collaborate in dialog, however, this cooperation
is not strong enough. Not only must participants form mutual beliefs about what was
said, they must also form mutual beliefs about the adequacy of the plan for the task
13 Another approach would be to have the plan inference process reason about the intended effects of the
plan that it is inferring in order to decide whether it should evaluate embedded plans and whether this
evaluation should affect the evaluation of the parent plan.
</bodyText>
<page confidence="0.9916">
366
</page>
<note confidence="0.391194">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
</note>
<bodyText confidence="0.999332833333333">
they are collaborating upon. If the plan is not adequate, then they must work together
to refashion it. This level of cooperation is due to what Clark and Wilkes-Gibbs refer
to as a mutual responsibility, or what Searle (1990) refers to as a we-intention. This allows
the agents to interact so that neither assumes control of the dialog, thus allowing both
to contribute to the best of their ability without being controlled or impeded by the
other. This is different from what Grosz and Sidner (1990) have called master—servant
dialogs, which occur in teacher—apprentice or information-seeking dialogs, in which
one of the participants is controlling the conversation (cf. Walker and Whittaker 1990).
Note that the noncontrolling agent may be helpful by anticipating obstacles in the
plan (Allen and Perrault 1980), but this is not the same as collaborating.
The mutual responsibility that the agents share not only concerns the goal they
are trying to achieve, but also the plan that they are currently considering. This plan
serves to coordinate their activity and so agents will have intentions to keep this
plan in their common ground. The plan might not be valid (unlike the shared plan of
Grosz and Sidner [19901), so the agents might not mutually believe that each action
contributes to the goal of the plan. Because of this, agents will have a belief regarding
the validity of the plan, and an intention that this belief be mutually believed.
The discourse plans that we described in the previous section can now be seen
as plans that can be used to further the collaborative activity. Judgment plans express
beliefs about the success of the current plan, and refashioning plans update it. So, the
mental state of an agent sanctions the adoption both of goals to express judgment and
of goals to refashion. It also sanctions the adoption of beliefs about the current plan.&apos;
If it is mutually believed that one of the conversants believes there is an error with
the current plan, the other also adopts this belief. Likewise, if one of the conversants
proposes a replacement, the other accepts it. Since both conversants expect the other to
behave in this way, each judgment and refashioning, so long as they are understood,
results in the judgment or refashioning being mutually believed. Thus the current plan,
through all of its refashionings, remains in the common ground of the participants.
Below, we discuss the rules for updating the mental state after a contribution is
made. We then give rules that account for the collaborative process.&apos;
</bodyText>
<subsectionHeader confidence="0.730025">
5.1 Rules for Updating the Mental State
</subsectionHeader>
<bodyText confidence="0.952749272727273">
After a plan has been contributed to the conversation, by way of its surface speech
actions, the speaker and hearer update their beliefs to reflect the contribution that has
been made. Both assume that the hearer is observant, can derive a coherent plan (not
necessarily valid), and can infer the communicative goal, which is expressed by the
effect of the top-level action in the plan. We capture this by having the agent that we
are modeling, the system, adopt the belief that it is mutually believed that the speaker
intends to achieve the goal by means of the plan.16
bmb(system,user,plan(Speaker,Plan,Goal))
The system will also add a belief about whether she believes the plan will achieve the
goal, and if not, the action that she believes to be in error. So, one of the following
propositions will be adopted.
</bodyText>
<footnote confidence="0.890732">
14 The collaborative activity also sanctions discourse expectations that the other participant&apos;s utterances
will pertain to the collaborative activity. We do not explicitly address this, however.
15 For simplicity, we represent the rules for entering into a collaborative activity, adopting beliefs, and
adopting goals with the same operator, For a more formal account, three different operators
should be used.
16 See Perrault (1990) for how these inferences can be drawn by using default rules.
</footnote>
<page confidence="0.951186">
367
</page>
<figure confidence="0.867758">
Computational Linguistics Volume 21, Number 3
bel(system,achieve(Plan,Goal))
bel(system,error(Plan,Node))
</figure>
<bodyText confidence="0.9992648">
After the above beliefs have been added, there are a number of inferences that
the agents can make and, in fact, can believe will be made by the other participant
as well, and so these inferences can be mutually believed. The first rule is that if it is
mutually believed that the speaker intends to achieve Goal by means of Plan, then it
will be mutually believed that the speaker has Goal as one of her goals.&apos;
</bodyText>
<equation confidence="0.92854525">
Rule 1
bmb(system,user,goal(Agt1,Goal)) &lt;
bmb(system,user,plan(Agt1,Plan,Goal)) &amp;
Agt1 E {system,user}
</equation>
<bodyText confidence="0.999913833333333">
The next rule concerns the adoption by the hearer of the intended goals of com-
municative acts. The communicative goal that we are concerned with is where the
speaker wants the hearer to believe that the speaker believes some proposition. This
only requires that the hearer believe the speaker to be sincere. We assume that both
conversants are sincere, and so when such a communicative goal arises, both partici-
pants will assume that the hearer has adopted the goal. This is captured by Rule 2.
</bodyText>
<equation confidence="0.9747988">
Rule 2
bmb(system,user,bel(Agt1,Prop)) &lt;
bmb(system,user,goal(Agt1,bel(Agt2,bel(Agt1,Prop)))) &amp;
Agt1,Agt2 E {system,user} &amp;
not(Agt1 = Agt2)
</equation>
<bodyText confidence="0.999841666666667">
The last rule involves an inference that is not shared. When the user makes a
contribution to a conversation, the system assumes that the user believes that the plan
will achieve its intended goal.
</bodyText>
<equation confidence="0.785502">
Rule 3
bel(system,bel(user,achieve(Plan,Goal))) &lt;
bmb(system,user,plan(user,Plan,Goal))
</equation>
<subsectionHeader confidence="0.995046">
5.2 Rules for Updating the Collaborative State
</subsectionHeader>
<bodyText confidence="0.999490272727273">
The second set of rules that we give concern how the agents update the collaborative
state. These rules have been revised from an earlier version (Heeman 1991) so as to
better model the acceptance process.
5.2.1 Entering into a Collaborative Activity. We need a rule that permits an agent
to enter into a collaborative activity. We use the predicate cstate to represent that an
agent is in such a state, and this predicate takes as its parameters the agents involved,
the goal they are trying to achieve, and their current plan. Our view of when such a
collaborative activity can be entered is very simple: the system believes it is mutually
believed that one of them has a goal to refer and has a plan for doing so, but one of
them believes this plan to be in error. The last part of the condition states that if the
speaker&apos;s referring expression was successful from the beginning, no collaboration is
</bodyText>
<page confidence="0.796694">
17 All variables mentioned in the rules are existentially quantified.
368
</page>
<note confidence="0.457234">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
</note>
<bodyText confidence="0.994482">
necessary. It is not required that both participants mutually believe there is an error.
Rather, if either detects an error, then that conversant can presuppose that they are
collaborating, and make a judgment. Once the other recognizes the judgment that the
plan is in error, the criteria for him entering will be fulfilled for him as well.
</bodyText>
<equation confidence="0.967295">
Rule 4
cstate(system,user,Plan,Goal)
bmb(system,user,goal(Agt1,Goal)) &amp;
bmb(system,user,plan(Agt1,Plan/Goal)) &amp;
Goal = knowref(Agt2,Agt1,Entity,Object) &amp;
bel(system,bel(Agt3,error(Plan,Node))) &amp;
Agt1,Agt2,Agt3 E {system,user} &amp;
not(Agt1 = Agt2)
</equation>
<bodyText confidence="0.980674333333333">
5.2.2 Adoption of Mutual Beliefs. In order to model how the state of the collaborative
activity progresses, we need to account for the mutual beliefs that the agents adopt as
a result of the utterances that are made.
The first rule is for judgment moves in which the speaker finds the current plan in
error. Given that the move is understood, both conversants, by way of the rules given
in Section 5.1, will believe that it is mutually believed that the speaker believes the
current plan to be in error. In this case, the hearer, in the spirit of collaboration, must
accept the judgment and so also adopt the belief that the plan is in error, even if he
initially found the plan adequate. Since both conversants expect the hearer to behave
in this way, the belief that there is an error can be mutually believed. Rule 5, below,
captures this. (The adoption of this belief will cause the retraction of any beliefs that
the plan is adequate.)
</bodyText>
<equation confidence="0.9081664">
Rule 5
bmb(system,user,error(Plan,Node)) &lt;
cstate(system,user,Plan,Goal) &amp;
bmb(system,user,bel(Agt1,error(Plan,Node))) &amp;
Agt1 E {system/user}
</equation>
<bodyText confidence="0.999803666666667">
The second rule is for refashioning moves. After such a move, the conversants
will believe it mutually believed that the speaker has a replacement, NewPlan, for
the current plan, Plan. Again, in the spirit of collaboration, the hearer must accept
this replacement, and since both expect each other to behave this way, both adopt the
belief that it is mutually believed that the new referring expression plan replaces the
old one.
</bodyText>
<equation confidence="0.929006333333333">
Rule 6
bmb(system,user,replace(Plan,NewPlan)) &lt;
Agt1 e {system,user} &amp;
cstate(system,user,Plan,Goal) &amp;
bmb(system,user,error(Plan,Node)) &amp;
bmb(system,user,bel(Agt1,replace(Plan,NewPlan)))
</equation>
<bodyText confidence="0.999881">
In adopting this belief, the system updates the cst ate by replacing the current plan with
the new plan, and adding beliefs that capture the utterance of NewPlan as outlined
in Section 5.1 above.
</bodyText>
<page confidence="0.996817">
369
</page>
<note confidence="0.665946">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.9996789">
The third rule is for judgment moves in which the speaker finds the current plan
acceptable. Given that the move has been understood, each conversant will believe it is
mutually believed that the speaker believes that the current plan will achieve the goal
(second condition of the rule). However, in order to accept this move, each participant
also needs to believe that the hearer also finds the plan acceptable (third condition).
This belief would have been inferred if it were the hearer who had proposed the
current plan, or the last refashioning. In this case, the speaker (of the acceptance)
would have inferred by way of Rule 3 that the hearer believes the plan to be valid; as
for the hearer, given that he contributed the current plan, he undoubtedly also believes
it to be acceptable.
</bodyText>
<equation confidence="0.998169142857143">
Rule 7
bmb(system,user,achieve(Plan,Goal))
cstate(system,user,Plan,Goal) &amp;
bmb(system,user,bel(Agttachieve(Plan,Goal))) &amp;
bel(system,bel(Agt2,achieve(Plan,Goal))) &amp;
Agt1,Agt2 E {system,user} &amp;
not(Agt1 = Agt2)
</equation>
<subsubsectionHeader confidence="0.597705">
5.2.3 Adopting Goals. The last set of rules completes the circle. They account for how
</subsubsectionHeader>
<bodyText confidence="0.999158">
agents adopt goals to further the collaborative activity. These goals lead to judgment
and refashioning moves, and so correspond to the rules that we just gave for adopting
mutual beliefs.
The first goal adoption rule is for informing the hearer that there is an error in the
current plan. The conditions specify that Plan is the current plan of a collaborative
activity and that the speaker believes that there is an error in it.
</bodyText>
<equation confidence="0.93189225">
Rule 8
goal(system,bel(user,bel(system,error(Plan,Node))))
cstate(system,user,Plan,Goal) &amp;
bel(system,error(Plan,Node))
</equation>
<bodyText confidence="0.999952833333333">
The second rule is used to adopt the goal of replacing the current plan, Plan, if it
has an error. The rule requires that the agent believe that it is mutually believed that
there is an error in the current plan. So, this goal cannot be adopted before the goal
of expressing judgment has been planned. Note that the consequent has an unbound
variable, NewPlan. This variable will become bound when the system develops a plan
to achieve this goal, by using the action schema replace-plan (see Figure 11 above).
</bodyText>
<equation confidence="0.87590225">
Rule 9
goal(system,bel(user,bel(system,replace(Plan,NewPlan))))
cstate(system,user,Plan,Goal) &amp;
bmb(system,user,error(Plan,Node))
</equation>
<bodyText confidence="0.999885666666667">
The third rule is used to adopt the goal of communicating the system&apos;s acceptance
of the current plan. Not only must the system believe that the plan achieves the goal,
but it must also believe that the user also believes this. As mentioned above for Rule 7,
this last condition prevents the system from trying to accept a plan that it has itself
just proposed. Rather, it can only try to accept a plan that the other agent contributed,
for it is just such plans for which it will have the belief, by way of Rule 3, that the
</bodyText>
<page confidence="0.993105">
370
</page>
<note confidence="0.431583">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
</note>
<bodyText confidence="0.499387">
user believes the plan achieves the goal.
</bodyText>
<equation confidence="0.616345">
Rule 10
goal(system,bel(user,bel(system,achieve(Plan,Goal)))) &lt;
cstate(system,user,Plan,Goal) &amp;
bel(system,achieve(Plan,Goal)) &amp;
bel(system,bel(user,achieve(Plan,Goal)))
</equation>
<subsectionHeader confidence="0.999885">
5.3 Applying the Rules
</subsectionHeader>
<bodyText confidence="0.9999883">
The rules that we have given are used to update the mental state of the agent and
to guide its activity Acting as the hearer, the system performs plan inference on each
set of actions that it observes, and then applies any rules that it can. When all of the
observed actions are processed, the system switches from the role of hearer to speaker.
As the speaker, the system checks whether there is a goal that it can try to achieve,
and if so, constructs a plan to achieve it. Next, presupposing its partner&apos;s acceptance
of the plan, it applies any rules that it can. It repeats this until there are no more goals.
The actions of the constructed plans form the response of the system; in a complete
natural language system, they would be converted to a surface utterance. The system
then switches to the role of hearer.
</bodyText>
<sectionHeader confidence="0.592823" genericHeader="method">
6. An Example
</sectionHeader>
<bodyText confidence="0.987713">
We are now ready to illustrate our system in action.18 For this example, we use a
simplified version of a subdialog from the London-Lund corpus (Svartvik and Quirk
1980, S.2.4a:1-8):
</bodyText>
<equation confidence="0.8961435">
(6.1) A: I See the weird creature.
B: 2 In the corner?
A:3 No, on the television.
B: 4 Okay.
</equation>
<bodyText confidence="0.999911666666667">
The system will take the role of person B and we will give it the belief that there are
two objects that are &amp;quot;weird&amp;quot;—a television antenna, which is on the television, and a
fern plant, which is in the corner.
</bodyText>
<subsectionHeader confidence="0.999919">
6.1 Understanding &amp;quot;The Weird Creature&amp;quot;
</subsectionHeader>
<bodyText confidence="0.9999545">
For the first sentence, the system is given as input the surface speech actions under-
lying &amp;quot;the weird creature,&amp;quot; as shown below:
</bodyText>
<equation confidence="0.725943333333333">
s-refer(entityl)
s-attrib(entityl,AX• assessment(X,weird))
s-attrib(entityl, AX. category(X,creature))
</equation>
<bodyText confidence="0.9998036">
The system invokes the plan inference process, which finds the plan derivations whose
yield is the above set of surface speech actions. In this case, there is only one, and the
system labels it pl. Figure 13 shows the derivation; arrows represent decomposition,
and for brevity constraints and mental actions have been omitted and the parameters
only of the surface speech actions are shown.
</bodyText>
<page confidence="0.899026">
18 The system is implemented in C-Prolog under UNIX.
371
</page>
<figure confidence="0.993722857142857">
Computational Linguistics Volume 21, Number 3
refer
s-refer(entityl) describe
headnoun
modifiers-recurse
modifier-absolute
s-attrib(entityl,AX. assessment(X,weird)) null
</figure>
<figureCaption confidence="0.989503">
Figure 13
</figureCaption>
<bodyText confidence="0.975275230769231">
Plan derivation (p/) for &amp;quot;The weird creature.&amp;quot;
Next, the plan derivation is evaluated. The subset constraint in the headnoun action
is evaluated, which narrows the candidate set to the antenna and the fern plant. The
subset constraint in the modifier action is then evaluated, which does not eliminate
either of the candidates, since the system finds both of them &amp;quot;weird.&amp;quot; The constraint
on the modifiers action that terminates the addition of modifiers is then evaluated.
However, this constraint fails, since there are two objects that match the description
rather than one, as required.
The system then updates its beliefs. As described in Section 5.1, the system adds
the following beliefs to capture the results of the plan inference process: that it is
mutually believed that the user has the goal of knowref and has adopted p1 as a
means to achieve it, and that p1 has an error on the terminating instance of modifiers,
node p22.
</bodyText>
<equation confidence="0.998846">
bmb(system,user,plan(user,p1,knowref(system,user,entity1,Object))) (11)
bel(system,error(pl,p22)) (12)
</equation>
<bodyText confidence="0.999414">
The system next tries to apply the belief and goal adoption rules. From Rule 1 and
belief (11), the system adds the belief that it is mutually believed that the user has the
goal that the system knowref , and from Rule 3 and belief (11), it adds the belief that
the user believes that the plan achieves its goal.
</bodyText>
<equation confidence="0.999799">
bmb(system,user,goal(user,knowref(system,user,entityl,Object))) (13)
bel(system,bel(user,achieve(pl,knowref(system,user,entity1,Object)))) (14)
</equation>
<bodyText confidence="0.980729666666667">
Belief (13), along with (11) and (12), allows the system to apply Rule 4, and so the
system enters into a collaborative activity, in which the goal is for it to know the
referent and in which the current plan is pl.
</bodyText>
<equation confidence="0.973586">
cstate(system,user,p1,knowref(system,user,entityl,Object)) (15)
</equation>
<bodyText confidence="0.998663">
Since the system believes there is an error in the current plan, it applies Rule 8, and
so gives itself the communicative goal of informing the user of the error in the current
plan.
</bodyText>
<figure confidence="0.4474695">
s-attrib(entityl,AX. categmyp(,creature))
modifiers-terminate
</figure>
<page confidence="0.831346">
372
</page>
<bodyText confidence="0.349446">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
</bodyText>
<equation confidence="0.842141">
goal(system,bel(user,bel(system,error(pl,p22)))) (16)
</equation>
<subsectionHeader confidence="0.997888">
6.2 Constructing &amp;quot;In the Corner?&amp;quot;
</subsectionHeader>
<bodyText confidence="0.999710888888889">
Since there are no further belief or goal adoption rules that can be applied, the system
next checks for any goals that it can try to achieve. The only goal is (16), which is to
inform the user of the error in the plan. Since the error in the referring plan is in the
terminating instance of modifiers, the plan constructor builds an instance of postpone-
plan, which it names p26. (The schema was given in Figure 10.) Rather than realizing
the surface speech action immediately, the system plans ahead. (This would allow an
opportunistic process to combine surface speech actions into a single utterance [Appelt
19851.) So, the system, presupposing that the user understands the system&apos;s plan, adds
the following belief.
</bodyText>
<equation confidence="0.958826333333333">
bmb(user,system,plan(system,p26,bel(user,bel(system,error(pl,p22))))) (17)
It also adds the belief that this plan will achieve its goal.
bel(system,achieve(p26,bel(user,bel(system,error(pl,p22))))) (18)
Then by Rule 1, the system adds the belief that it is mutually believed that it has the
goal.
bmb(system,user,goal(system,bel(user,bel(system,error(pl,p22))))) (19)
</equation>
<bodyText confidence="0.988242666666667">
Then by Rule 2, which captures the cooperativity of the agents in communicative
goals, it adds the belief that it is mutually believed that the system believes there is
an error.
</bodyText>
<equation confidence="0.962014">
bmb(system,user,bel(system,error(pl,p22))) (20)
</equation>
<bodyText confidence="0.999591">
Then, on the basis of (15) and (20) the system applies Rule 5, thus adopting the belief
that it is mutually believed that there is an error in the plan. This presupposes the
user&apos;s acceptance of the judgment plan.
</bodyText>
<equation confidence="0.983933">
bmb(system,user,error(pl,p22)) (21)
</equation>
<bodyText confidence="0.995827333333333">
The system is now able to apply Rule 9, on the basis of (15) and (21), and so adopts
the goal of refashioning the invalid referring expression plan and of informing the
user of the new plan.
</bodyText>
<equation confidence="0.963306">
goal(system,bel(user,bel(system,replace(pl,RPlan)))) (22)
</equation>
<bodyText confidence="0.999881818181818">
Since no further rules can be applied, the system checks for goals that it can try
to fulfill, which will result in choosing (22). To achieve this goal, the plan constructor
builds an instance of expand-plan (previously shown in Figure 12). In doing this, the
system chooses one of the objects that matched the original description as the likely
referent; in this case it happens to choose the object in the corner, the fern plant, which
the system represents as fernl. It then substitutes the modifiers subplan that terminates
the addition of modifiers with the header of the modifiers-recurse action (with the
chosen object instantiated in). The plan constructor is then called to fill in the details,
thereby creating the expansion. The expansion it chooses includes a relative modifier
(see Figure 7) that describes the object as being in the corner. The new referring plan
(labeled p34) is shown in Figure 14, with the expansion circled (we have abbreviated
</bodyText>
<page confidence="0.996712">
373
</page>
<figure confidence="0.998809909090909">
Volume 21, Number 3
Computational Linguistics
refer
s-refer(entityl) describe
s-attrib(entityl,AX. category(X,creature))
modifiers-recurse
modifier-absolute
s-attrib(entityl,AX. assessment(X,weird)) modifier-relative modifiers-terminate
refer null
s-attrib-regentityl,entity2,AX. A Y. in(X,Y))
&amp;quot;the corner&amp;quot;
</figure>
<figureCaption confidence="0.98919">
Figure 14
</figureCaption>
<bodyText confidence="0.903147">
Plan derivation (p34) for &amp;quot;The weird creature in the corner.&amp;quot;
the derivation of &amp;quot;the corner&amp;quot;). The surface speech action of expand-plan is s-actions,
which takes the surface speech actions of the expansion, listed below, as its parameter.
</bodyText>
<equation confidence="0.958376666666667">
s-attrib-rel(entityl,entity2,AX. )Y. in(X,Y))
s-refer(entity2)
s-attrib(entity2,AX. category(X,corner))
</equation>
<bodyText confidence="0.934379">
Next, the system assumes the user will understand the refashioning, and, by way
of Rules 1 and 2, will be cooperative and adopt the communicative goal that the system
believes that the new expanded plan replaces the old referring expression plan. The
end result is given below as (23).
bmb(system,user,bel(system,replace(pl,p34))) (23)
The system, on the basis of (15) and (23), applies Rule 6, and so assumes that the user
will accept the refashioning. So, the system adds the belief that it is mutually believed
that the new expanded plan replaces the old referring expression.
</bodyText>
<equation confidence="0.904608">
bmb(system,user,replace(pl,p34)) (24)
</equation>
<bodyText confidence="0.6375615">
This causes the belief module to update the current plan of the collaborative activity
(25). Also, it adds the beliefs that capture the utterance of the refashioned plan: that
the system intends it as a means to achieve the referring action and that it does achieve
this goal.19
</bodyText>
<page confidence="0.86577">
19 Even though the system has the referent incorrectly identified in the goal of knowref , the goal itself is
374
</page>
<figure confidence="0.4214845">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
cstate(system,user,p34,knowref(system,user,entityl,Object)))
bmb(system,user,plan(system,p34,knowref(system,user,entityl,fern1)))
bel(system,achieve(p34,knowref(system,user,entity1,fern1)))
</figure>
<bodyText confidence="0.997384">
The two plans that were constructed, postpone-plan and expand-plan, give rise
to the output of the surface speech actions s-postpone and s-expand, which would be
realized as &amp;quot;in the corner?&amp;quot;.20
</bodyText>
<subsectionHeader confidence="0.999678">
6.3 Understanding &amp;quot;No, on the Television&amp;quot;
</subsectionHeader>
<bodyText confidence="0.974702294117647">
The user next utters &amp;quot;No, on the television.&amp;quot; This would get parsed into two separate
surface speech actions, an s-reject corresponding to &amp;quot;no,&amp;quot; and an s-actions corre-
sponding to &amp;quot;on the television.&amp;quot; For simplicity, the plan inference process is invoked
separately on each.
The system starts with the s-reject action. We assume that the parser can determine
from context that the &amp;quot;no&amp;quot; is rejecting the surface speech actions that were previously
added and so the parameter of s-reject is a list of these actions. From this, it derives
a plan whose yield is the s-reject action, and this plan is an instance of reject-plan
(previously shown in Figure 9). The system then evaluates the constraints of the plan,
which results in it determining which action in the plan the user found to be in error.
This is done by evaluating the constraints of reject-plan, and so finding the action
whose yield is the surface speech actions that were rejected. This will be p56, the
modifiers-relative action that described the object as being in the corner. The resulting
belief, after applying Rules 1 and 2, is the following.
bmb(system,user,bel(user,error(p34,p56))) (28)
The system then applies the appropriate acceptance rule, Rule 5, and so adopts the
belief that the error is mutually believed.
</bodyText>
<equation confidence="0.963627">
bmb(system,user,error(p34,p56)) (29)
</equation>
<bodyText confidence="0.9998414">
With this belief, the system will have the context that it needs to understand the user&apos;s
refashioning plan.
The system next performs plan recognition starting with the second surface speech
action, s-actions, which corresponds to the refashioning &amp;quot;on the television.&amp;quot; It takes
as a parameter the following list of actions:21
</bodyText>
<equation confidence="0.994831666666667">
s-attrib-rel(entity1,entity3,AX- )Y. on(X,Y))
s-refer(entity3)
s-attrib(entity3,AX. category(X,television))
</equation>
<bodyText confidence="0.995014666666667">
The system finds two plan derivations that account for the primitive action, one an
instance of replace-plan (see Figure 11) and the other an instance of expand-plan (see
Figure 12). Next it evaluates the constraints of each derivation. The constraints of
</bodyText>
<footnote confidence="0.8955954">
still valid: for it to identify the referent corresponding to entityl.
20 Although our model does not account for the questioning intonation, it could be a manifestation of the
s-postpone.
21 We assume that the parser determines the appropriate discourse entities in these actions: entityl is the
discourse entity for the object being referred to, and entity3 is another discourse entity.
</footnote>
<page confidence="0.992918">
375
</page>
<note confidence="0.749199">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.998305875">
expand-plan do not hold since the action in error, p56, is not an instance of modifiers-
terminate, so this plan is eliminated. The constraints (and mental actions) of replace-
plan do hold, and so the system is able to derive the refashioned referring plan, which
it labels p104.
Since this instance of replace-plan is the only valid derivation corresponding to
the surface speech actions observed, the system takes it as the plan behind the user&apos;s
utterance. As a result, the system adds the following belief (after applying Rules 1 and
2).
</bodyText>
<equation confidence="0.515622">
bmb(system,user,bel(user,replace(p34,p104))) (30)
</equation>
<bodyText confidence="0.9879765">
The system then applies the acceptance rule for refashioning plans, Rule 6, and so
adopts the refashioning as mutually believed.
</bodyText>
<equation confidence="0.992366">
bmb(system,user,replace(p34,p104)) (31)
</equation>
<bodyText confidence="0.999294">
This causes the belief module to update the current plan of the collaborative activity
and to add the belief that the user contributed the new referring expression plan.
</bodyText>
<equation confidence="0.999898">
cstate(system,user,p104,knowref(system,user,entity1,Object)) (32)
bmb(system,user,plan(user,p104,knowref(system,user,entitytantenna1)) (33)
</equation>
<bodyText confidence="0.999944166666667">
The new referring plan will already have been evaluated. The subplan corresponding
to &amp;quot;the television&amp;quot; would have been understood without problem,&apos; and the modifier
corresponding to &amp;quot;on the television&amp;quot; would have narrowed down the candidates that
matched &amp;quot;weird creature&amp;quot; to a single object, antenna1. So, the belief module adds the
belief that the system finds the new referring plan to be valid. Also, by way of Rule 3,
the system adds the belief that the user also does, since the user had proposed it.
</bodyText>
<equation confidence="0.9991115">
bel(system,achieve(p104,knowref(system,user,entitytantenna1))) (34)
bel(system,bel(user,achieve(p104,knowref(system,user,entitytantenna1)))) (35)
</equation>
<subsectionHeader confidence="0.999851">
6.4 Constructing &amp;quot;Okay&amp;quot;
</subsectionHeader>
<bodyText confidence="0.999777">
On the basis of (32), (34), and (35), the system is able to apply Rule 10, and so adopts
the goal of accepting the plan.
</bodyText>
<equation confidence="0.959183">
goal(system,bel(user,bel(system,achieve(p104,
knowref(system,user,entity1,antenna1)))))) (36)
</equation>
<bodyText confidence="0.999951">
The plan constructor achieves this by planning an instance of accept-plan, which
results in the surface speech action s-accept, which would be realized as &amp;quot;Okay.&amp;quot;
Then, after the application of Rules 1, 2, and most importantly 7, the system adopts
the belief that it is mutually believed that the plan achieves the goal of referring.
</bodyText>
<equation confidence="0.9723">
bmb(system,user,achieve(p104,knowref(system,user,entitytantenna1))) (37)
</equation>
<bodyText confidence="0.9867315">
22 If &amp;quot;the television&amp;quot; is not understood, then since it is a referring expression in its own right, the
conversants could collaborate on identifying its referent independently of the referent of &amp;quot;the weird
creature&amp;quot;; that is, the participants could enter into an embedded collaborative activity by focusing on
one part of the current plan.
</bodyText>
<page confidence="0.997533">
376
</page>
<note confidence="0.52192">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
7. Comparisons to Related Work
</note>
<bodyText confidence="0.999953166666667">
In providing a computational model of how agents collaborate upon referring expres-
sions, we have touched on several different areas of research. First, our work has built
on previous work in referring expressions, especially their incorporation into a model
based on the planning paradigm. Second, our work has built on the research done in
modeling clarifications in the planning paradigm and on plan repair. Third, our work
is related to the research being done on modeling collaborative and joint activity.
</bodyText>
<subsectionHeader confidence="0.998758">
7.1 Referring Expressions
</subsectionHeader>
<bodyText confidence="0.999961333333333">
Cohen (1981) and Appelt (1985) have also addressed the generation of referring expres-
sions in the planning paradigm. They have integrated this into a model of generating
utterances, a step that we haven&apos;t taken. However, we have extended their model
by incorporating even the generation of the components of the description into our
planning model. One result of this is that our surface speech actions are much more
fine-grained.
</bodyText>
<subsectionHeader confidence="0.997472">
7.2 Clarifications and Plan Repair
</subsectionHeader>
<bodyText confidence="0.981525096774194">
An important part of our work involves accounting for clarifications of referring ex-
pressions by using meta-actions that incorporate plan repair techniques. This approach
is based on Litman and Allen&apos;s work (1987) on understanding clarification subdialogs,
in which meta-actions were used to model discourse relations, such as clarifications.
There are several major differences between our work and theirs. First, our work ad-
dresses not only understanding, but also generation, and how these two tasks fit into
a model of how agents collaborate in discourse. Second, Litman and Allen use a stack
of unchanging plans to represent the state of the discourse. We, however, use a single
current plan, modifying it as clarifications are made. This difference has an impor-
tant ramification, for it results in different interpretations of the discourse structure.
Consider dialog (7.1), which was collected at an information booth in a Toronto train
station (Horrigan 1977). (Although the participants are not collaborating in making a
referring expression, the dialog will serve to illustrate our point.)
(7.1) P: 1 The 8:50 to Montreal?
C: 2 8:50 to Montreal. Gate 7.
P: 3 Where is it?
C: 4 Down this way to your left. Second one on the left.
P: 5 OK. Thank you.
Litman and Allen represent the state of the discourse after the second utterance as
a clarification of the passenger&apos;s take-train-trip plan. The information that the train
boards at gate 7 is represented only in the clarification plan. So, when the passenger
asks &amp;quot;Where is it?,&amp;quot; their system, acting as the clerk, cannot interpret this as a clarifica-
tion of the take-train-trip plan, since the utterance &amp;quot;cannot be seen as a step of [that]
plan&amp;quot; (p. 188). So, it is interpreted instead as a request for a clarification of the clerk&apos;s
&amp;quot;Gate 7&amp;quot; response, implicitly assuming that &amp;quot;Gate 7&amp;quot; was not accepted. In our model,
the acceptance of &amp;quot;Gate 7&amp;quot; would be presupposed, and so it would be incorporated
into the take-train-trip plan. So, the passenger&apos;s question of &amp;quot;Where is it?&amp;quot; would be
viewed as a request for the clerk to clarify that plan.
The work of Moore and Swartout (1991), Cawsey (1991), and Carletta (1991) on
interactive explanations also addresses clarifications using plan repair techniques. This
body of work uses plan construction techniques to generate explanations, and uses the
</bodyText>
<page confidence="0.992849">
377
</page>
<note confidence="0.750046">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.999925153846154">
constructed plan as a basis for recovery strategies if the user doesn&apos;t understand the
explanation. In the cases of Cawsey and Carletta, both use meta-actions to encode the
plan repair techniques. However, none of these approaches is within a collaborative
framework, in which either agent can contribute to the development of the plan.
Other relevant work is that of Lambert and Carberry (1991). In their model of un-
derstanding information-seeking dialogs, they propose a distinction between problem-
solving activities and discourse activities. In contrast, our clarifications embody both
functions in the same actions, thus allowing for a simpler approach to inferring the
refashioned referring expressions, since we need not chain to a meta-operator. In later
work, Chu-Carroll and Carberry (1994) extended this model to generate responses to
proposals that are viewed as sub-optimal or invalid. Like Litman and Allen (1987),
they adopt the view that subsequent modifications apply to the preceding modifica-
tion, rather than the underlying plan.
</bodyText>
<subsectionHeader confidence="0.961581">
7.3 Collaboration
</subsectionHeader>
<bodyText confidence="0.999928888888889">
Grosz, Sidner, and Lochbaum (Grosz and Sidner, 1990; Lochbaum, Grosz, and Sidner,
1990) are interested in the type of plans that underlie discourse in which the agents
are collaborating in order to achieve some goal. They propose that agents are building
a shared plan in which participants have a collection of beliefs and intentions about the
actions in the plan. Our model differs from theirs in two important aspects. First, not
only do agents have a collection of beliefs and intentions regarding the actions of a
shared plan, but we feel that they also have an intention about the goal (Searle 1990;
Cohen and Levesque 1991). It is this intention, in conjunction with the current plan,
that sanctions the adoption of beliefs and intentions about potential actions that will
contribute to the goal, rather than just the shared plan.
Second, we feel that their definition of a partial shared plan is too restrictive.
Although they address partial plans, they require, in order for an action to be part of
a partial shared plan, that both agents believe that the action contributes to the goal.
However, this is too strong. In collaborating to achieve a mutual goal, participants
sometimes propose an action that is not believed by the other participant or even by
the participant that is proposing it. In failing to represent such states, their model is
unable to represent the intermediate states in which a hearer might have understood
how the speaker&apos;s utterance contributes to a plan, but doesn&apos;t agree with it. This is
important, since if the refashioned plan is invalid, only the referring expression should
be refashioned, not the refashioning itself.
Traum (1991; Traum and Hinkelman, 1992) is concerned with providing a compu-
tational model of grounding, the process in which conversational participants add to the
common ground of a conversation (Clark and Schaefer 1989; Clark and Brennan 1990).
Traum models the grounding process by proposing that utterances move through a
number of states, &apos;pushed&apos; by grounding acts, which include initiate, continue, repair,
request repair, acknowledge, and request acknowledge. Once an utterance has been
acknowledged, it will reside in mutual belief as a proposal of the person who initiated
it. The proposal state is a subspace of the mutual belief space of the conversants. Only
once it has been accepted will it be moved into the shared space (also in mutual belief).
Unlike Traum&apos;s, our work does not differentiate the proposal state from the shared
state. If a proposal is understood, it is incorporated into the current plan. Judgments
of acceptability are not on proposals but on the current plan, or a part of it.
Sidner (1994) addressed the issue of how conversational participants collaborate
in building a shared plan. In this work, Sidner presents a number of speech actions
for use in collaborative tasks. These actions are those that an artificial agent could use
in negotiating which actions or beliefs to accept into the shared plan of the agents. As
</bodyText>
<page confidence="0.995705">
378
</page>
<note confidence="0.623407">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
</note>
<bodyText confidence="0.999678444444445">
with Traum, it is the proposals that are refashioned, before they are integrated into the
shared plan, rather than the shared plan.
Cohen and Levesque (1991) focus on formalizing joint intention in a logic. They use
this formalism to explain how such elements of communication as confirmations arise
when agents are engaging in a joint action. However, they have not addressed how
agents collaborate in building a plan, only how agents collaborate while executing
a plan. Once this limitation is overcome, their approach could offer us a route for
formalizing the mental states of the collaborating agents in our model and for proving
that our acceptance and goal adoption rules follow from such states.
</bodyText>
<sectionHeader confidence="0.884235" genericHeader="conclusions">
8. Conclusion
</sectionHeader>
<bodyText confidence="0.999994179487179">
We have presented a computational model of how a conversational participant col-
laborates in making and understanding a referring expression, based on the view that
language is goal-oriented behavior. This has allowed us to do the following. First,
we have accounted for the tasks of building a referring expression and identifying its
referent by using plan construction and plan inference. Second, we have accounted
for the conversational moves that participants make during the acceptance process by
using meta-actions. Third, we have accounted for collaborative activity by proposing
that agents are in a certain mental state that includes a goal, a plan that they are
currently considering, and intentions. This mental state sanctions the acceptance of
clarification plans, and sanctions the adoption of goals to clarify Although our work
has focused on referring expressions, we feel that it is relevant to collaboration in
general and to how agents contribute to discourse.
This paper is based on the model of collaboration proposed by Clark and Wilkes-
Gibbs (1986). Their model makes two strong claims about how agents collaborate.
First, it minimizes the distinction between the roles of the person who initiates the
referring expression and the person who is trying to identify it. Both have the same
moves available to them, for either can judge the description and either can refashion
it. This allows both participants to contribute without being controlled or impeded
by the other. Second, their model gives special status to the role of the current refer-
ring expression (current plan): participants judge and refashion the current referring
expression directly, rather than recursively modifying modifications (e.g. Litman and
Allen 1987; Chu-Carroll and Carberry 1994) or incrementally adding to the current
plan with each accepted proposal (e.g. Traum and Hinkelman 1992; Sidner 1992). In
our work, we have taken Clark and Wilkes-Gibbs&apos;s descriptive model and recast it
into a computational one, thus demonstrating the computational feasibility of their
work and its compatibility with current practices in artificial intelligence.
There are many ways that this research could be extended. Perhaps the most ob-
vious would be to extend the planning component of our model. First, our coverage
of referring expressions could be extended to handle references to objects in focus
and to descriptions that include a plan of physical actions for identifying the referent.
Second, the treatment of clarifications could be improved; specifically, how plan fail-
ures are reasoned about, how plan failures affect the agent&apos;s beliefs, and how these
failures are repaired. Third, this research needs to be integrated into a more complete
plan-based approach to language, and needs to be extended so as to handle more
general discourse plan failures (McRoy and Hirst 1993; McRoy and Hirst 1995; Hor-
ton and Hirst 1991; Heeman 1993; Edmonds 1994; Hirst et al. 1994). A benchmark for
such future work could be dialog (8.1) below, from the London-Lund corpus (Svartvik
and Quirk 1980, S.2.4a:1-8, which is the basis of the example used in Section 6. This
dialog shows how collaboration on a referring expression can be embedded in other
</bodyText>
<page confidence="0.995188">
379
</page>
<note confidence="0.695297">
Computational Linguistics Volume 21, Number 3
</note>
<bodyText confidence="0.923453476190476">
activities, how agents can return back to a collaborative activity, and even how agents
can take advantage of a mistaken referent.
(8.1) A: 1 What&apos;s that weird creature over there?
B: 2 In the corner?
A: 3 affirmative noise
B: 4 Ws just a fern plant.
A: 5 No, the one to the left of it.
B: 6 That&apos;s the television aerial. It pulls out.
A second avenue for future work is to further investigate collaborative behavior
and protocols for interaction. We need to formalize what it means for agents to be
collaborating, in a theory that takes account of rational interaction and the beliefs
and knowledge of the participants. Such a theory would do the following. First, it
would give a more complete motivation for the processing rules that we used for how
agents interact in a collaborative activity. Second, it would account for why agents
would enter into such a mode of interaction, how it is initiated, how it is carried
forward (especially how agents&apos; beliefs and knowledge influence their actions), and
how it ends. Third, it would be extendable to other forms of interaction, such as
information-seeking dialogs. Fourth, it would specify how collaborative activity could
be embedded in, or embed, other types of interactions. By answering these questions,
we will not only have a better model to base natural language interfaces on, but we
will also have a better understanding of how people interact.
</bodyText>
<sectionHeader confidence="0.9854" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999507428571429">
This research was begun at the Department
of Computer Science, University of Toronto,
as part of the first author&apos;s M.S. thesis
under the supervision of the second author.
We would like to thank James Allen,
Hector Levesque, and the referees at
Computational Linguistics for their comments
on an earlier version of this paper. We
would also like to especially thank Janyce
Wiebe for her invaluable contribution to the
development of this work. As well, we are
grateful for comments from, and discussions
with, Diane Horton, Susan McRoy, Massimo
Poesio, and David Traum. Funding at the
University of Toronto and the University of
Rochester was provided by the Natural
Sciences and Engineering Research Council
of Canada, with additional funding at
Rochester provided by NSF under Grant
IRI-90-13160 and ONR/DARPA under
Grant N00014-92+1512.
</bodyText>
<sectionHeader confidence="0.993786" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998664810810811">
Allen, James; Hendler, James; and Tate,
Austin (editors) (1990). Readings in
Planning. Morgan Kaufmann Publishers.
Allen, James F., and Perrault, C. Raymond
(1980). &amp;quot;Analyzing intention in
utterances.&amp;quot; Artificial Intelligence, 15,
143-178. Reprinted in Readings in Natural
Language Processing, edited by Barbara J.
Grosz, Karen Sparck Jones, and Bonnie
Lynn Weber, 441-458. Morgan Kaufmann
Publishers.
Appelt, Douglas E. (1985). &amp;quot;Planning
English referring expressions.&amp;quot; Artificial
Intelligence, 26(1), 1-33. Reprinted in
Readings in Natural Language Processing,
edited by Barbara J. Grosz, Karen Sparck
Jones, and Bonnie Lynn Webber, 501-517.
Morgan Kaufmann Publishers.
Appelt, Douglas, and Kronfeld, Amichai
(1987). &amp;quot;A computational model of
referring.&amp;quot; In Proceedings, International
Joint Conference on Artificial Intelligence
(IJCAI &apos;87), 640-647.
Austin, J. L. (1962). How to Do Things with
Words. Oxford University Press.
Carletta, Jean (1991). &amp;quot;Recovering from plan
failure using a layered architecture.&amp;quot;
Research Paper 524, Department of
Artificial Intelligence, University of
Edinburgh.
Cawsey, Alison (1991). &amp;quot;Generating
interactive explanations.&amp;quot; In Proceedings,
National Conference on Artificial Intelligence
(AAAI &apos;91), 86-91.
Chu-Carroll, Jennifer, and Carberry, Sandra
(1994). &amp;quot;A plan-based model for response
generation in collaborative task-oriented
</reference>
<page confidence="0.995137">
380
</page>
<note confidence="0.650088">
Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions
</note>
<reference confidence="0.998837893442623">
dialogues.&amp;quot; In Proceedings, National
Conference on Artificial Intelligence
(AAA&apos; &apos;94), 799-805.
Clark, Herbert H. (editor) (1992). Arenas of
Language Use. University of Chicago Press
and CSLI.
Clark, Herbert H., and Brennan, S. E. (1990).
&amp;quot;Grounding in communication.&amp;quot; In
Perspectives on Socially Shared Cognition,
edited by L.B. Resnick, J. Levine, and
S.D. Behreno, 127-149. APA.
Clark, Herbert H., and Marshall,
Catherine R. (1981). &amp;quot;Definite reference
and mutual knowledge.&amp;quot; In Elements of
Discourse Understanding, edited by
Aravind K. Joshi, Bonnie Lynn Webber,
and Ivan Sag, Cambridge University
Press, 10-62.
Clark, Herbert H., and Schaefer, Edward F.
(1989). &amp;quot;Contributing to discourse.&amp;quot;
Cognitive Science, 13,259-294. Reprinted in
Arenas of Language Use, edited by Herbert
H. Clark, 144-175. University of Chicago
Press and CSLI.
Clark, Herbert H., and Wilkes-Gibbs,
Deanna (1986). &amp;quot;Referring as a
collaborative process.&amp;quot; Cognition, 22,1-39.
Reprinted in Arenas of Language Use,
edited by Herbert H. Clark, 107-143.
University of Chicago Press and CSLI.
Cohen, Philip R. (1981). &amp;quot;The need for
referent identification as a planned
action.&amp;quot; In Proceedings, International Joint
Conference on Artificial Intelligence
(IJCAI &apos;81), 31-36.
Cohen, Philip R., and Levesque, Hector J.
(1991). &amp;quot;Confirmation and joint action.&amp;quot; In
Proceedings, International Joint Conference on
Artificial Intelligence (IJCAI &apos;91), 951-958.
Cohen, Philip R., and Perrault, C. Raymond
(1979). &amp;quot;Elements of a plan-based theory
of speech acts.&amp;quot; Cognitive Science, 3(3),
177-212. Reprinted in Readings in Natural
Language Processing, edited by Barbara J.
Grosz, Karen Sparck Jones, and Bonnie
Lynn Webber, 423-440. Morgan
Kaufmann Publishers.
Dale, R. (1989). &amp;quot;Cooking up referring
expressions.&amp;quot; In Proceedings, 27th Annual
Meeting of the Association for Computational
Linguistics, 68-75.
Edmonds, Philip G. (1994). &amp;quot;Collaboration
on reference to objects that are not
mutually known.&amp;quot; In Proceedings, 15th
International Conference on Computational
Linguistics (COLING &apos;94), Kyoto,
1118-1122.
Goodman, Bradley A. (1985). &amp;quot;Repairing
reference identification failures by
relaxation.&amp;quot; In Proceedings, 23rd Annual
Meeting of the Association for Computational
Linguistics, 204-217.
Grosz, Barbara J., and Sidner, Candace L.
(1990). &amp;quot;Plans for discourse.&amp;quot; In Intentions
in Communication, SDF Benchmark Series,
edited by Philip R. Cohen, Jerry Morgan,
and Martha E. Pollack, 417-444. MIT
Press.
Hayes, Philip J. (1975). &amp;quot;A representation
for robot plans.&amp;quot; In Proceedings,
International Joint Conference on Artificial
Intelligence (IJCAI &apos;75), 181-188. Reprinted
in Readings in Planning, edited by James
Allen, James Hendler, and Austin Tate,
154-161. Morgan Kaufmann Publishers.
Heeman, Peter A. (1993). &amp;quot;Speech actions
and mental states in task-oriented
dialogs.&amp;quot; In Working Notes AAAI Spring
Symposium on Reasoning about Mental
States: Formal Theories and Applications,
Stanford, 68-73.
Heeman, Peter Anthony (1991). &amp;quot;A
computational model of collaboration on
referring expressions.&amp;quot; Master&apos;s thesis,
Technical Report CSRI 251, Department of
Computer Science, University of Toronto.
Hirst, Graeme; McRoy, Susan; Heeman,
Peter; Edmonds, Philip; and Horton,
Diane (1994). &amp;quot;Repairing conversational
misunderstandings and
non-understandings.&amp;quot; Speech
Communications, 15(3/4), 213-229.
Horrigan, Mary Katherine (1977).
&amp;quot;Modelling simple dialogs.&amp;quot; Master&apos;s
thesis, Technical Report 108, Department
of Computer Science, University of
Toronto.
Horton, Diane, and Hirst, Graeme (1991).
&amp;quot;Discrepancies in discourse models and
miscommunication in conversation.&amp;quot; In
Working Notes of the AAA&apos; Symposium:
Discourse Structure in Natural Language
Understanding and Generation, 31-32.
Kautz, Henry A., and Allen, James F. (1986).
&amp;quot;Generalized plan recognition.&amp;quot; In
Proceedings, National Conference on Artificial
Intelligence (AAAI &apos;86), 32-37.
Lambert, Lynn, and Carberry, Sandra (1991).
&amp;quot;A tripartite plan-based model for
dialogue.&amp;quot; In Proceedings, 29th Annual
Meeting of the Association for Computational
Linguistics, 47-54.
Levelt, Willem J. M. (1989). Speaking: From
Intention to Articulation. Cambridge
University Press.
Litman, Diane J., and Allen, James F. (1987).
&amp;quot;A plan recognition model for subdialogs
in conversations.&amp;quot; Cognitive Science, 11(2),
163-200.
Lochbaum, Karen E.; Grosz, Barbara J.; and
Sidner, Candace L. (1990). &amp;quot;Models of
plans to support communication: An
</reference>
<page confidence="0.961661">
381
</page>
<note confidence="0.374469">
Computational Linguistics Volume 21, Number 3
</note>
<reference confidence="0.999604341880342">
initial report.&amp;quot; In Proceedings, National
Conference on Artificial Intelligence
(AAAI &apos;90), 485-490.
McRoy, Susan, and Hirst, Graeme (1993).
&amp;quot;Abductive explanations of dialogue
misunderstanding.&amp;quot; In Proceedings, 6th
Conference of the European Chapter of the
Association for Computational Linguistics.
Utrecht, Netherlands, April 1993,277-286.
McRoy, Susan, and Hirst, Graeme (1995).
&amp;quot;The repair of speech act
misunderstandings by abductive
inference.&amp;quot; Computational Linguistics, 21(4),
to appear.
Mellish, C. S. (1985). Computer Interpretation
of Natural Language Descriptions, Ellis
Horwood Series in Artificial Intelligence.
Ellis Horwood.
Moore, Jahanna D., and Swartout,
William R. (1991). &amp;quot;A reactive approach to
explanation: Taking the user&apos;s feedback
into account.&amp;quot; In Natural Language
Generation in Artificial Intelligence and
Computational Linguistics, edited by
Cecile L. Paris, William R. Swartout, and
William C. Mann, 3-48. Kluwer Academic
Publishers.
Nadathur, Gopalan, and Johi, Aravind K.
(1983). &amp;quot;Mutual beliefs in conversational
systems: Their role in referring
expressions.&amp;quot; In Proceedings, International
Joint Conference on Artificial Intelligence
(IJCAI &apos;83), 603-605.
Perrault, C. R. (1990). &amp;quot;An application of
default logic to speech act theory.&amp;quot; In
Intentions in Communication, SDF
Benchmark Series, edited by Philip R.
Cohen, Jerry Morgan, and Martha E.
Pollack, 161-185. MIT Press.
Perrault, C. Raymond, and Cohen, Philip R.
(1981). &amp;quot;It&apos;s for your own good: A note on
inaccurate reference.&amp;quot; In Elements of
Discourse Understanding, edited by
Aravind K. joshi, Bonnie Lynn Webber,
and Ivan Sag, 217-230. Cambridge
University Press.
Pollack, Martha E. (1990). &amp;quot;Plans as complex
mental attitudes.&amp;quot; In Intentions in
Communication, SDF Benchmark Series,
edited by Philip R. Cohen, Jerry Morgan,
and Martha E. Pollack, 77-103. MIT Press.
Reiter, Ehud (1990). &amp;quot;The computational
complexity of avoiding conversational
implicature.&amp;quot; In Proceedings, 28th Annual
Meeting of the Association for Computational
Linguistics, 97-104.
Searle, J. R. (1969). Speech Acts: An Essay in
the Philosophy of Language. Cambridge
University Press.
Searle, John R. (1990). &amp;quot;Collective intentions
and actions.&amp;quot; In Intentions in
Communication, SDF Benchmark Series,
edited by Philip R. Cohen, Jerry Morgan,
and Martha E. Pollack, 401-415. MIT
Press.
Sidner, Candace L. (1985). &amp;quot;Plan parsing for
intended response recognition in
discourse.&amp;quot; Computational Intelligence, 1(1),
1-10.
Sidner, Candace L. (1994). &amp;quot;An Artificial
Discourse Language for Collaborative
Negotiation.&amp;quot; In Proceedings of the National
Conference on Artificial Intelligence
(AAAI&apos;94). 814-819.
Svartvik, J., and Quirk, R. (1980). A Corpus of
English Conversation. Lund Studies in
English, 56. C. W. K. Gleerup.
Traum, David R. (1991). &amp;quot;Towards a
computational theory of grounding in
natural language conversation.&amp;quot; Technical
Report 401, Department of Computer
Science, University of Rochester,
Rochester, New York.
Traum, David R., and Hinkelman,
Elizabeth A. (1992). &amp;quot;Conversation acts in
task-oriented spoken dialogue.&amp;quot; Special
issue on non-literal language,
Computational Intelligence, 8(3), 575-599.
Vilain, Marc (1990). &amp;quot;Getting serious about
parsing plans: A grammatical analysis of
plan recognition.&amp;quot; In Proceedings, National
Conference on Artificial Intelligence
(AAAI &apos;90), 190-197.
Walker, Marilyn, and Whittaker, Steve
(1990). &amp;quot;Mixed initiative in dialogue: An
investigation into discourse
segmentation.&amp;quot; In Proceedings, 28th Annual
Meeting of the Association for Computational
Linguistics, 70-78.
Webber, Bonnie Lynn (1983). &amp;quot;So what can
we talk about now?&amp;quot; In Computational
Models of Discourse, edited by Michael
Brady and Robert C. Berwick, 331-371.
MIT Press. Reprinted in Readings in
Natural Language Processing, edited by
Barbara J. Grosz, Karen Sparck Jones, and
Bonnie Lynn Webber, 395-414. Morgan
Kaufmann Publishers.
Wilensky, Robert (1981). &amp;quot;A model for
planning in complex situations.&amp;quot; Cognition
and Brain Theory, 4. Reprinted in Readings
in Planning, edited by James Allen, James
Hendler, and Austin Tate, 263-374.
Morgan Kaufmann Publishers.
Wilkens, David E. (1985). &amp;quot;Recovering from
execution errors in SIPE.&amp;quot; Computational
Intelligence, 1,33-45.
</reference>
<page confidence="0.998299">
382
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.863144">
<title confidence="0.999985">Collaborating on Referring Expressions</title>
<author confidence="0.999354">Peter A Heeman Graeme Hirstt</author>
<affiliation confidence="0.977773">University of Rochester University of Toronto</affiliation>
<abstract confidence="0.986008666666667">This paper presents a computational model of how conversational participants collaborate in order to make a referring action successful. The model is based on the view of language as goal-directed behavior. We propose that the content of a referring expression can be accounted for by the planning paradigm. Not only does this approach allow the processes of building referring expressions and identifying their referents to be captured by plan construction and plan inference, it also allows us to account for how participants clarify a referring expression by using meta-actions that reason about and manipulate the plan derivation that corresponds to the referring expression. To account for how clarification goals arise and how inferred clarification plans affect the agent, we propose that the agents are in a certain state of mind, and that this state includes an intention to achieve the goal of referring and a plan that the agents are currently considering. It is this mental state that sanctions the adoption of goals and the acceptance of inferred plans, and so acts as a link between understanding and generation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>1990</date>
<booktitle>Readings in Planning.</booktitle>
<editor>Allen, James; Hendler, James; and Tate, Austin (editors)</editor>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="13530" citStr="(1990)" startWordPosition="2153" endWordPosition="2153">based models, determining the content of referring expressions hasn&apos;t been. For instance, in Appelt&apos;s model, concept activations can be achieved by the action describe, which is a primitive, not further decomposed. Rather, this action has an associated procedure that determines a description that satisfies the preconditions of describe. Such special procedures have been the mainstay for accounting for the content of referring expressions, both in constructing and in understanding them, as exemplified by Dale (1989), who chose descriptors on the basis of their discriminatory power; Ehud Reiter (1990), who focused 1 For simplicity, we have not shown the change in speakers between refashionings and judgments. 354 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions on avoiding misleading conversational implicatures when generating descriptions; and Mellish (1985), who used a constraint satisfaction algorithm to identify referents. Our work follows the plan-based approach to language generation and understanding. We extend the earlier approaches of Cohen and Appelt by accounting for the content of the description at the planning level. This is done by having surface speech</context>
<context position="17721" citStr="(1990)" startWordPosition="2821" endWordPosition="2821">the error affects the satisfiability of the goal of the plan nor use the error to revise the beliefs of the hearer.) 3.2 Vocabulary and Notation Before we present the action schemas for referring expressions, we need to introduce the notation that we use. Our terminology for planning follows the general literature.3 2 Since we assume that the agents have mutual knowledge of the action schemas and that agents can execute surface speech actions, we do not consider beliefs about generation or about the executability of primitive actions. 3 See the introductory chapter of Allen, Hendler, and Tate (1990) for an overview of planning. 355 Computational Linguistics Volume 21, Number 3 We use the terms action schema, plan derivation, plan construction, and plan inference. An action schema consists of a header, constraints, a decomposition, and an effect; and it encodes the constraints under which an effect can be achieved by performing the steps in the decomposition. A plan derivation is an instance of an action that has been recursively expanded into primitive actions—its yield. Each component in the plan—the action headers, constraints, steps, and effects—are referred to as nodes of the plan, a</context>
<context position="29721" citStr="(1990)" startWordPosition="4599" endWordPosition="4599">Hearer,goal(Speaker,Goal)). Plan Construction. Given an effect, the plan constructor finds a plan derivation that has a minimal number of primitive actions, that is valid (with respect to the planning agent&apos;s beliefs), and whose root action achieves the effect. The plan constructor uses a 359 Computational Linguistics Volume 21, Number 3 best-first search strategy, expanding the derivation with the fewest number of surface speech actions. The yield of this plan derivation can then be given as input to a module that generates the surface form of the utterance. Plan Inference. Following Pollack (1990), our plan inference process can infer plans in which, in the hearer&apos;s view, a constraint does not hold. In inferring a plan derivation, we first find the set of plan derivations that account for the primitive actions that were observed, without regard to whether the constraints hold. This is done by using a chart parser that parses actions rather than words (Sidner 1994; Vilain 1990). For referring plans that contain more than one modifier, there will be multiple derivations corresponding to the order of the modifiers. We avoid this ambiguity by choosing an arbitrary ordering of the modifiers</context>
<context position="52966" citStr="(1990)" startWordPosition="8206" endWordPosition="8206"> adequacy of the plan for the task 13 Another approach would be to have the plan inference process reason about the intended effects of the plan that it is inferring in order to decide whether it should evaluate embedded plans and whether this evaluation should affect the evaluation of the parent plan. 366 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions they are collaborating upon. If the plan is not adequate, then they must work together to refashion it. This level of cooperation is due to what Clark and Wilkes-Gibbs refer to as a mutual responsibility, or what Searle (1990) refers to as a we-intention. This allows the agents to interact so that neither assumes control of the dialog, thus allowing both to contribute to the best of their ability without being controlled or impeded by the other. This is different from what Grosz and Sidner (1990) have called master—servant dialogs, which occur in teacher—apprentice or information-seeking dialogs, in which one of the participants is controlling the conversation (cf. Walker and Whittaker 1990). Note that the noncontrolling agent may be helpful by anticipating obstacles in the plan (Allen and Perrault 1980), but this </context>
<context position="56588" citStr="(1990)" startWordPosition="8798" endWordPosition="8798"> about whether she believes the plan will achieve the goal, and if not, the action that she believes to be in error. So, one of the following propositions will be adopted. 14 The collaborative activity also sanctions discourse expectations that the other participant&apos;s utterances will pertain to the collaborative activity. We do not explicitly address this, however. 15 For simplicity, we represent the rules for entering into a collaborative activity, adopting beliefs, and adopting goals with the same operator, For a more formal account, three different operators should be used. 16 See Perrault (1990) for how these inferences can be drawn by using default rules. 367 Computational Linguistics Volume 21, Number 3 bel(system,achieve(Plan,Goal)) bel(system,error(Plan,Node)) After the above beliefs have been added, there are a number of inferences that the agents can make and, in fact, can believe will be made by the other participant as well, and so these inferences can be mutually believed. The first rule is that if it is mutually believed that the speaker intends to achieve Goal by means of Plan, then it will be mutually believed that the speaker has Goal as one of her goals.&apos; Rule 1 bmb(sys</context>
</contexts>
<marker>1990</marker>
<rawString>Allen, James; Hendler, James; and Tate, Austin (editors) (1990). Readings in Planning. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
<author>C Raymond Perrault</author>
</authors>
<title>Analyzing intention in utterances.&amp;quot;</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>15</volume>
<pages>143--178</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<note>Reprinted in Readings in Natural Language Processing, edited by</note>
<contexts>
<context position="12535" citStr="Allen and Perrault (1980)" startWordPosition="2001" endWordPosition="2004">rrent referring expression problematic, the other must accept that judgment. Likewise, if one agent proposes a referring expression, through a refashioning, the other must accept the refashioning. 3. Referring Expressions 3.1 Planning and Referring By viewing language as action, the planning paradigm can be applied to natural language processing. The actions in this case are speech acts (Austin 1962; Searle 1969), and include such things as promising, informing, and requesting. Cohen and Perrault (1979) developed a system that uses plan construction to map an agent&apos;s goals to speech acts, and Allen and Perrault (1980) use plan inference to understand an agent&apos;s plan from its speech acts. By viewing it as action (Searle 1969), referring can be incorporated into a planning model. Cohen&apos;s model (1981) planned requests that the hearer identify a referent, whereas Appelt (1985) planned concept activations, a generalization of referring actions. Although acts of reference have been incorporated into plan-based models, determining the content of referring expressions hasn&apos;t been. For instance, in Appelt&apos;s model, concept activations can be achieved by the action describe, which is a primitive, not further decompos</context>
<context position="51235" citStr="Allen and Perrault 1980" startWordPosition="7903" endWordPosition="7906">nly on whether the plan that is its parameter can be derived, but not whether the derived plan is valid.&apos; 5. Modeling Collaboration In the last two sections, we discussed how initial referring expressions, judgments, and refashionings can be generated and understood in our plan-based model. In this section, we show how plan construction and plan inference fit into a complete model of how an agent collaborates in making a referring action successful. Previous natural language systems that use plans to account for the surface speech acts underlying an utterance (such as Cohen and Perrault 1979; Allen and Perrault 1980; Appelt 1985; Litman and Allen 1987) model only the recognition or only the construction of an agent&apos;s plans, and so do not address this issue. In order to model an agent&apos;s participation in a dialog, we need to model how the mental state of the agent changes as a result of the contributions that are made to the dialog. The change in mental state can be modeled by the beliefs and goals that a participant adopts. When a speaker produces an utterance, as long as the hearer finds it coherent, he can add a belief that the speaker has made the utterance to accomplish some communicative goal. The he</context>
<context position="53555" citStr="Allen and Perrault 1980" startWordPosition="8294" endWordPosition="8297">sponsibility, or what Searle (1990) refers to as a we-intention. This allows the agents to interact so that neither assumes control of the dialog, thus allowing both to contribute to the best of their ability without being controlled or impeded by the other. This is different from what Grosz and Sidner (1990) have called master—servant dialogs, which occur in teacher—apprentice or information-seeking dialogs, in which one of the participants is controlling the conversation (cf. Walker and Whittaker 1990). Note that the noncontrolling agent may be helpful by anticipating obstacles in the plan (Allen and Perrault 1980), but this is not the same as collaborating. The mutual responsibility that the agents share not only concerns the goal they are trying to achieve, but also the plan that they are currently considering. This plan serves to coordinate their activity and so agents will have intentions to keep this plan in their common ground. The plan might not be valid (unlike the shared plan of Grosz and Sidner [19901), so the agents might not mutually believe that each action contributes to the goal of the plan. Because of this, agents will have a belief regarding the validity of the plan, and an intention th</context>
</contexts>
<marker>Allen, Perrault, 1980</marker>
<rawString>Allen, James F., and Perrault, C. Raymond (1980). &amp;quot;Analyzing intention in utterances.&amp;quot; Artificial Intelligence, 15, 143-178. Reprinted in Readings in Natural Language Processing, edited by Barbara J. Grosz, Karen Sparck Jones, and Bonnie Lynn Weber, 441-458. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
</authors>
<title>Planning English referring expressions.&amp;quot;</title>
<date>1985</date>
<journal>Artificial Intelligence,</journal>
<volume>26</volume>
<issue>1</issue>
<pages>1--33</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<note>Reprinted in Readings in Natural Language Processing, edited by</note>
<contexts>
<context position="12795" citStr="Appelt (1985)" startWordPosition="2045" endWordPosition="2046">action, the planning paradigm can be applied to natural language processing. The actions in this case are speech acts (Austin 1962; Searle 1969), and include such things as promising, informing, and requesting. Cohen and Perrault (1979) developed a system that uses plan construction to map an agent&apos;s goals to speech acts, and Allen and Perrault (1980) use plan inference to understand an agent&apos;s plan from its speech acts. By viewing it as action (Searle 1969), referring can be incorporated into a planning model. Cohen&apos;s model (1981) planned requests that the hearer identify a referent, whereas Appelt (1985) planned concept activations, a generalization of referring actions. Although acts of reference have been incorporated into plan-based models, determining the content of referring expressions hasn&apos;t been. For instance, in Appelt&apos;s model, concept activations can be achieved by the action describe, which is a primitive, not further decomposed. Rather, this action has an associated procedure that determines a description that satisfies the preconditions of describe. Such special procedures have been the mainstay for accounting for the content of referring expressions, both in constructing and in </context>
<context position="51248" citStr="Appelt 1985" startWordPosition="7907" endWordPosition="7908">hat is its parameter can be derived, but not whether the derived plan is valid.&apos; 5. Modeling Collaboration In the last two sections, we discussed how initial referring expressions, judgments, and refashionings can be generated and understood in our plan-based model. In this section, we show how plan construction and plan inference fit into a complete model of how an agent collaborates in making a referring action successful. Previous natural language systems that use plans to account for the surface speech acts underlying an utterance (such as Cohen and Perrault 1979; Allen and Perrault 1980; Appelt 1985; Litman and Allen 1987) model only the recognition or only the construction of an agent&apos;s plans, and so do not address this issue. In order to model an agent&apos;s participation in a dialog, we need to model how the mental state of the agent changes as a result of the contributions that are made to the dialog. The change in mental state can be modeled by the beliefs and goals that a participant adopts. When a speaker produces an utterance, as long as the hearer finds it coherent, he can add a belief that the speaker has made the utterance to accomplish some communicative goal. The hearer might th</context>
<context position="69969" citStr="Appelt 1985" startWordPosition="10878" endWordPosition="10879">e there are no further belief or goal adoption rules that can be applied, the system next checks for any goals that it can try to achieve. The only goal is (16), which is to inform the user of the error in the plan. Since the error in the referring plan is in the terminating instance of modifiers, the plan constructor builds an instance of postponeplan, which it names p26. (The schema was given in Figure 10.) Rather than realizing the surface speech action immediately, the system plans ahead. (This would allow an opportunistic process to combine surface speech actions into a single utterance [Appelt 19851.) So, the system, presupposing that the user understands the system&apos;s plan, adds the following belief. bmb(user,system,plan(system,p26,bel(user,bel(system,error(pl,p22))))) (17) It also adds the belief that this plan will achieve its goal. bel(system,achieve(p26,bel(user,bel(system,error(pl,p22))))) (18) Then by Rule 1, the system adds the belief that it is mutually believed that it has the goal. bmb(system,user,goal(system,bel(user,bel(system,error(pl,p22))))) (19) Then by Rule 2, which captures the cooperativity of the agents in communicative goals, it adds the belief that it is mutually b</context>
<context position="80135" citStr="Appelt (1985)" startWordPosition="12345" endWordPosition="12346">ing Expressions 7. Comparisons to Related Work In providing a computational model of how agents collaborate upon referring expressions, we have touched on several different areas of research. First, our work has built on previous work in referring expressions, especially their incorporation into a model based on the planning paradigm. Second, our work has built on the research done in modeling clarifications in the planning paradigm and on plan repair. Third, our work is related to the research being done on modeling collaborative and joint activity. 7.1 Referring Expressions Cohen (1981) and Appelt (1985) have also addressed the generation of referring expressions in the planning paradigm. They have integrated this into a model of generating utterances, a step that we haven&apos;t taken. However, we have extended their model by incorporating even the generation of the components of the description into our planning model. One result of this is that our surface speech actions are much more fine-grained. 7.2 Clarifications and Plan Repair An important part of our work involves accounting for clarifications of referring expressions by using meta-actions that incorporate plan repair techniques. This ap</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Appelt, Douglas E. (1985). &amp;quot;Planning English referring expressions.&amp;quot; Artificial Intelligence, 26(1), 1-33. Reprinted in Readings in Natural Language Processing, edited by Barbara J. Grosz, Karen Sparck Jones, and Bonnie Lynn Webber, 501-517. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Appelt</author>
<author>Amichai Kronfeld</author>
</authors>
<title>A computational model of referring.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<volume>87</volume>
<pages>640--647</pages>
<contexts>
<context position="24545" citStr="Appelt and Kronfeld (1987)" startWordPosition="3857" endWordPosition="3860">cative goal she plans to achieve the goal by making the hearer recognize it; the effect will be 357 Computational Linguistics Volume 21, Number 3 Header: describe(Entity,Object) Decomposition: headnoun(Entity,Object,Cand) modifiers(Entity,Object,Cand) Figure 2 describe schema. Figure 3 headnoun schema. achieved by the hearer inferring the speaker&apos;s plan, regardless of whether or not the hearer is able to determine the actual referent. To simplify our implementation, this is the only effect that is stated for the action schemas for referring expressions. It corresponds to the literal goal that Appelt and Kronfeld (1987) propose (whereas the actual identification is their condition of satisfaction). Intermediate Actions. The describe action, shown in Figure 2, is used to construct a description of the object through its decomposition into headnoun and modifiers. The variable Cand is the candidate set, the set of potential referents associated with the head noun that is chosen, and it is passed to the modifiers action so that it can ensure that the rest of the description rules out all of the alternatives. The action headnoun, shown in Figure 3, has a single step, s-attrib, which is the surface speech action u</context>
</contexts>
<marker>Appelt, Kronfeld, 1987</marker>
<rawString>Appelt, Douglas, and Kronfeld, Amichai (1987). &amp;quot;A computational model of referring.&amp;quot; In Proceedings, International Joint Conference on Artificial Intelligence (IJCAI &apos;87), 640-647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Austin</author>
</authors>
<title>How to Do Things with Words.</title>
<date>1962</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="12312" citStr="Austin 1962" startWordPosition="1968" endWordPosition="1969">llaborative activity is updated. So, what constitutes grounds for accepting a judgment or clarification? In order to be consistent with Clark and Wilkes-Gibbs&apos;s model, we can see that if one agent finds the current referring expression problematic, the other must accept that judgment. Likewise, if one agent proposes a referring expression, through a refashioning, the other must accept the refashioning. 3. Referring Expressions 3.1 Planning and Referring By viewing language as action, the planning paradigm can be applied to natural language processing. The actions in this case are speech acts (Austin 1962; Searle 1969), and include such things as promising, informing, and requesting. Cohen and Perrault (1979) developed a system that uses plan construction to map an agent&apos;s goals to speech acts, and Allen and Perrault (1980) use plan inference to understand an agent&apos;s plan from its speech acts. By viewing it as action (Searle 1969), referring can be incorporated into a planning model. Cohen&apos;s model (1981) planned requests that the hearer identify a referent, whereas Appelt (1985) planned concept activations, a generalization of referring actions. Although acts of reference have been incorporate</context>
</contexts>
<marker>Austin, 1962</marker>
<rawString>Austin, J. L. (1962). How to Do Things with Words. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Recovering from plan failure using a layered architecture.&amp;quot;</title>
<date>1991</date>
<tech>Research Paper 524,</tech>
<institution>Department of Artificial Intelligence, University of Edinburgh.</institution>
<contexts>
<context position="82747" citStr="Carletta (1991)" startWordPosition="12773" endWordPosition="12774">s the clerk, cannot interpret this as a clarification of the take-train-trip plan, since the utterance &amp;quot;cannot be seen as a step of [that] plan&amp;quot; (p. 188). So, it is interpreted instead as a request for a clarification of the clerk&apos;s &amp;quot;Gate 7&amp;quot; response, implicitly assuming that &amp;quot;Gate 7&amp;quot; was not accepted. In our model, the acceptance of &amp;quot;Gate 7&amp;quot; would be presupposed, and so it would be incorporated into the take-train-trip plan. So, the passenger&apos;s question of &amp;quot;Where is it?&amp;quot; would be viewed as a request for the clerk to clarify that plan. The work of Moore and Swartout (1991), Cawsey (1991), and Carletta (1991) on interactive explanations also addresses clarifications using plan repair techniques. This body of work uses plan construction techniques to generate explanations, and uses the 377 Computational Linguistics Volume 21, Number 3 constructed plan as a basis for recovery strategies if the user doesn&apos;t understand the explanation. In the cases of Cawsey and Carletta, both use meta-actions to encode the plan repair techniques. However, none of these approaches is within a collaborative framework, in which either agent can contribute to the development of the plan. Other relevant work is that of La</context>
</contexts>
<marker>Carletta, 1991</marker>
<rawString>Carletta, Jean (1991). &amp;quot;Recovering from plan failure using a layered architecture.&amp;quot; Research Paper 524, Department of Artificial Intelligence, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alison Cawsey</author>
</authors>
<title>Generating interactive explanations.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, National Conference on Artificial Intelligence (AAAI &apos;91),</booktitle>
<pages>86--91</pages>
<contexts>
<context position="82726" citStr="Cawsey (1991)" startWordPosition="12770" endWordPosition="12771">ir system, acting as the clerk, cannot interpret this as a clarification of the take-train-trip plan, since the utterance &amp;quot;cannot be seen as a step of [that] plan&amp;quot; (p. 188). So, it is interpreted instead as a request for a clarification of the clerk&apos;s &amp;quot;Gate 7&amp;quot; response, implicitly assuming that &amp;quot;Gate 7&amp;quot; was not accepted. In our model, the acceptance of &amp;quot;Gate 7&amp;quot; would be presupposed, and so it would be incorporated into the take-train-trip plan. So, the passenger&apos;s question of &amp;quot;Where is it?&amp;quot; would be viewed as a request for the clerk to clarify that plan. The work of Moore and Swartout (1991), Cawsey (1991), and Carletta (1991) on interactive explanations also addresses clarifications using plan repair techniques. This body of work uses plan construction techniques to generate explanations, and uses the 377 Computational Linguistics Volume 21, Number 3 constructed plan as a basis for recovery strategies if the user doesn&apos;t understand the explanation. In the cases of Cawsey and Carletta, both use meta-actions to encode the plan repair techniques. However, none of these approaches is within a collaborative framework, in which either agent can contribute to the development of the plan. Other releva</context>
</contexts>
<marker>Cawsey, 1991</marker>
<rawString>Cawsey, Alison (1991). &amp;quot;Generating interactive explanations.&amp;quot; In Proceedings, National Conference on Artificial Intelligence (AAAI &apos;91), 86-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Sandra Carberry</author>
</authors>
<title>A plan-based model for response generation in collaborative task-oriented dialogues.&amp;quot;</title>
<date>1994</date>
<booktitle>In Proceedings, National Conference on Artificial Intelligence (AAA&apos; &apos;94),</booktitle>
<pages>799--805</pages>
<contexts>
<context position="83776" citStr="Chu-Carroll and Carberry (1994)" startWordPosition="12922" endWordPosition="12925"> the plan repair techniques. However, none of these approaches is within a collaborative framework, in which either agent can contribute to the development of the plan. Other relevant work is that of Lambert and Carberry (1991). In their model of understanding information-seeking dialogs, they propose a distinction between problemsolving activities and discourse activities. In contrast, our clarifications embody both functions in the same actions, thus allowing for a simpler approach to inferring the refashioned referring expressions, since we need not chain to a meta-operator. In later work, Chu-Carroll and Carberry (1994) extended this model to generate responses to proposals that are viewed as sub-optimal or invalid. Like Litman and Allen (1987), they adopt the view that subsequent modifications apply to the preceding modification, rather than the underlying plan. 7.3 Collaboration Grosz, Sidner, and Lochbaum (Grosz and Sidner, 1990; Lochbaum, Grosz, and Sidner, 1990) are interested in the type of plans that underlie discourse in which the agents are collaborating in order to achieve some goal. They propose that agents are building a shared plan in which participants have a collection of beliefs and intention</context>
<context position="89640" citStr="Chu-Carroll and Carberry 1994" startWordPosition="13857" endWordPosition="13860">mizes the distinction between the roles of the person who initiates the referring expression and the person who is trying to identify it. Both have the same moves available to them, for either can judge the description and either can refashion it. This allows both participants to contribute without being controlled or impeded by the other. Second, their model gives special status to the role of the current referring expression (current plan): participants judge and refashion the current referring expression directly, rather than recursively modifying modifications (e.g. Litman and Allen 1987; Chu-Carroll and Carberry 1994) or incrementally adding to the current plan with each accepted proposal (e.g. Traum and Hinkelman 1992; Sidner 1992). In our work, we have taken Clark and Wilkes-Gibbs&apos;s descriptive model and recast it into a computational one, thus demonstrating the computational feasibility of their work and its compatibility with current practices in artificial intelligence. There are many ways that this research could be extended. Perhaps the most obvious would be to extend the planning component of our model. First, our coverage of referring expressions could be extended to handle references to objects i</context>
</contexts>
<marker>Chu-Carroll, Carberry, 1994</marker>
<rawString>Chu-Carroll, Jennifer, and Carberry, Sandra (1994). &amp;quot;A plan-based model for response generation in collaborative task-oriented dialogues.&amp;quot; In Proceedings, National Conference on Artificial Intelligence (AAA&apos; &apos;94), 799-805.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Arenas of Language Use.</title>
<date>1992</date>
<publisher>University of Chicago Press and CSLI.</publisher>
<marker>Clark, 1992</marker>
<rawString>Clark, Herbert H. (editor) (1992). Arenas of Language Use. University of Chicago Press and CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>S E Brennan</author>
</authors>
<title>Grounding in communication.&amp;quot;</title>
<date>1990</date>
<booktitle>In Perspectives on Socially Shared Cognition, edited</booktitle>
<pages>127--149</pages>
<publisher>APA.</publisher>
<contexts>
<context position="85951" citStr="Clark and Brennan 1990" startWordPosition="13274" endWordPosition="13277">hat is proposing it. In failing to represent such states, their model is unable to represent the intermediate states in which a hearer might have understood how the speaker&apos;s utterance contributes to a plan, but doesn&apos;t agree with it. This is important, since if the refashioned plan is invalid, only the referring expression should be refashioned, not the refashioning itself. Traum (1991; Traum and Hinkelman, 1992) is concerned with providing a computational model of grounding, the process in which conversational participants add to the common ground of a conversation (Clark and Schaefer 1989; Clark and Brennan 1990). Traum models the grounding process by proposing that utterances move through a number of states, &apos;pushed&apos; by grounding acts, which include initiate, continue, repair, request repair, acknowledge, and request acknowledge. Once an utterance has been acknowledged, it will reside in mutual belief as a proposal of the person who initiated it. The proposal state is a subspace of the mutual belief space of the conversants. Only once it has been accepted will it be moved into the shared space (also in mutual belief). Unlike Traum&apos;s, our work does not differentiate the proposal state from the shared </context>
</contexts>
<marker>Clark, Brennan, 1990</marker>
<rawString>Clark, Herbert H., and Brennan, S. E. (1990). &amp;quot;Grounding in communication.&amp;quot; In Perspectives on Socially Shared Cognition, edited by L.B. Resnick, J. Levine, and S.D. Behreno, 127-149. APA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Catherine R Marshall</author>
</authors>
<title>Definite reference and mutual knowledge.&amp;quot;</title>
<date>1981</date>
<booktitle>In Elements of Discourse Understanding, edited by Aravind</booktitle>
<pages>10--62</pages>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="14922" citStr="Clark and Marshall 1981" startWordPosition="2367" endWordPosition="2370">ve actions, and the speaker utters them in her attempt to refer to an object. These speech actions are the building blocks that referring expressions are made from. Acting as the mortar are intermediate actions, which have constraints that the plan construction and plan inference processes can reason about. These constraints encode the knowledge of how a description can allow a hearer to identify an object. First, the constraints express the conditions under which an attribute can be used to refer to an object; for instance, that it be mutually believed that the object has a certain property (Clark and Marshall 1981; Perrault and Cohen 1981; Nadathur and Joshi 1983). Second, the constraints keep track of which objects could be believed to be the referent of the referring expression. Third, the constraints ensure that a sufficient number of surface speech actions are added so that the set of candidates associated with the entire referring expression consists of only a single object, the referent. These constraints enable the speaker to construct a referring expression that she believes will allow the hearer to identify the referent. As for the hearer, the explicit encoding of the adequacy of referring exp</context>
</contexts>
<marker>Clark, Marshall, 1981</marker>
<rawString>Clark, Herbert H., and Marshall, Catherine R. (1981). &amp;quot;Definite reference and mutual knowledge.&amp;quot; In Elements of Discourse Understanding, edited by Aravind K. Joshi, Bonnie Lynn Webber, and Ivan Sag, Cambridge University Press, 10-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Edward F Schaefer</author>
</authors>
<title>Contributing to discourse.&amp;quot;</title>
<date>1989</date>
<journal>Cognitive Science,</journal>
<pages>13--259</pages>
<institution>University of Chicago Press and CSLI.</institution>
<note>Reprinted in Arenas of Language Use, edited by</note>
<contexts>
<context position="11107" citStr="Clark and Schaefer (1989)" startWordPosition="1779" endWordPosition="1782">hat the state of the process is characterized by the current referring expression, re, and the judgment of it, judgment, and that this state must be part of the common ground of the participants. The algorithm also illustrates how the model of Clark and Wilkes-Gibbs minimizes the distinction between the roles of the person who initiated the referring expression and the person who is trying to identify it. Both have the same moves available to them, for either can judge the description and either can refashion it. Neither is controlling the dialog, they are simply collaborating. In later work, Clark and Schaefer (1989) propose that &amp;quot;each part of the acceptance phase is itself a contribution&amp;quot; (p. 269), and the acceptance of these contributions depends on whether the hearer &amp;quot;believes he is understanding well enough for current purposes&amp;quot; (p. 267). Although Clark and Schaefer use the term contribution with respect to the discourse, rather than the collaborative effort of referring, their proposal is still relevant here: judgments and refashionings are contributions to the collaborative effort and are subjected to an acceptance process, with the result being that once they are accepted, the state of the collabor</context>
<context position="85926" citStr="Clark and Schaefer 1989" startWordPosition="13270" endWordPosition="13273">even by the participant that is proposing it. In failing to represent such states, their model is unable to represent the intermediate states in which a hearer might have understood how the speaker&apos;s utterance contributes to a plan, but doesn&apos;t agree with it. This is important, since if the refashioned plan is invalid, only the referring expression should be refashioned, not the refashioning itself. Traum (1991; Traum and Hinkelman, 1992) is concerned with providing a computational model of grounding, the process in which conversational participants add to the common ground of a conversation (Clark and Schaefer 1989; Clark and Brennan 1990). Traum models the grounding process by proposing that utterances move through a number of states, &apos;pushed&apos; by grounding acts, which include initiate, continue, repair, request repair, acknowledge, and request acknowledge. Once an utterance has been acknowledged, it will reside in mutual belief as a proposal of the person who initiated it. The proposal state is a subspace of the mutual belief space of the conversants. Only once it has been accepted will it be moved into the shared space (also in mutual belief). Unlike Traum&apos;s, our work does not differentiate the propos</context>
</contexts>
<marker>Clark, Schaefer, 1989</marker>
<rawString>Clark, Herbert H., and Schaefer, Edward F. (1989). &amp;quot;Contributing to discourse.&amp;quot; Cognitive Science, 13,259-294. Reprinted in Arenas of Language Use, edited by Herbert H. Clark, 144-175. University of Chicago Press and CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Deanna Wilkes-Gibbs</author>
</authors>
<title>Referring as a collaborative process.&amp;quot;</title>
<date>1986</date>
<journal>Cognition,</journal>
<pages>22--1</pages>
<institution>University of Chicago Press and CSLI.</institution>
<note>Reprinted in Arenas of Language Use, edited by</note>
<contexts>
<context position="2617" citStr="Clark and Wilkes-Gibbs (1986)" startWordPosition="422" endWordPosition="425">. The speaker attempts to achieve this goal by constructing a description of the object that she thinks will enable the hearer to identify it. But since the speaker and the hearer will inevitably have different beliefs about the world, the hearer might not be able to identify the object. Often, when the hearer cannot do so, the speaker and hearer collaborate in making a new referring expression that accomplishes the goal. This paper presents a computational model of how a conversational participant collaborates in making a referring action successful. We use as our basis the model proposed by Clark and Wilkes-Gibbs (1986), which gives a descriptive account of the conversational moves that participants make when collaborating upon a referring expression. We cast their work into a model based on the planning paradigm. We propose that referring expressions can be represented by plan derivations, and that plan construction and plan inference can be used to generate and understand them. Not only does this approach allow the processes of building referring expres* Department of Computer Science, Rochester, New York 14627. E-mail: heeman@cs.rochester.edu. t Department of Computer Science, Toronto, Canada, M5S 1A4. E-</context>
<context position="7536" citStr="Clark and Wilkes-Gibbs (1986)" startWordPosition="1215" endWordPosition="1218">tual knowledge of the mechanisms of referring expressions and collaboration. Third, we deal with objects that both the speaker and hearer know of, though they might have different beliefs about what propositions hold for these objects. Fourth, as the input and the output to our system, we use representations of surface speech actions, not natural language strings. Finally, although belief revision is an important part of how agents collaborate, we do not explicitly address this. 352 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions 2. Referring as a Collaborative Process Clark and Wilkes-Gibbs (1986) investigated how participants in a conversation collaborate in making a referring action successful. They conducted experiments in which participants had to refer to objects—tangram patterns—that are difficult to describe. They found that typically the participant trying to describe a tangram pattern would present an initial referring expression. The other participant would then pass judgment on it, either accepting it, rejecting it, or postponing his decision. If it was rejected or the decision postponed, then one participant or the other would refashion the referring expression. This would </context>
<context position="34229" citStr="Clark and Wilkes-Gibbs (1986)" startWordPosition="5315" endWordPosition="5318">ers, Cand = [Object], with Cand instantiated to a non-null set. If Cand contains more than one object, then this constraint will fail, pinning the blame on the terminating instance of modifiers for there not being enough descriptors to allow the referent to be identified. Otherwise, the terminating constraint will be satisfiable, and so Object will be instantiated to the single object in the candidate set. This will then allow all of the mutual belief constraints that were postponed to be evaluated, since they will now have only a single solution. 4. Clarifications 4.1 Planning and Clarifying Clark and Wilkes-Gibbs (1986) have presented a model of how conversational participants collaborate in making a referring action successful (see Section 2 above). Their model consists of conversational moves that express a judgment of a referring expression and conversational moves that refashion an expression. However, their model is not computational. They do not account for how the judgment is made, how the judgment affects the refashioning, or the content of the moves. Following the work of Litman and Allen (1987) in understanding clarification subdialogs, we formalize the conversational moves of Clark and Wilkes-Gibb</context>
</contexts>
<marker>Clark, Wilkes-Gibbs, 1986</marker>
<rawString>Clark, Herbert H., and Wilkes-Gibbs, Deanna (1986). &amp;quot;Referring as a collaborative process.&amp;quot; Cognition, 22,1-39. Reprinted in Arenas of Language Use, edited by Herbert H. Clark, 107-143. University of Chicago Press and CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
</authors>
<title>The need for referent identification as a planned action.&amp;quot;</title>
<date>1981</date>
<booktitle>In Proceedings, International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<volume>81</volume>
<pages>31--36</pages>
<contexts>
<context position="14947" citStr="Cohen 1981" startWordPosition="2373" endWordPosition="2374">m in her attempt to refer to an object. These speech actions are the building blocks that referring expressions are made from. Acting as the mortar are intermediate actions, which have constraints that the plan construction and plan inference processes can reason about. These constraints encode the knowledge of how a description can allow a hearer to identify an object. First, the constraints express the conditions under which an attribute can be used to refer to an object; for instance, that it be mutually believed that the object has a certain property (Clark and Marshall 1981; Perrault and Cohen 1981; Nadathur and Joshi 1983). Second, the constraints keep track of which objects could be believed to be the referent of the referring expression. Third, the constraints ensure that a sufficient number of surface speech actions are added so that the set of candidates associated with the entire referring expression consists of only a single object, the referent. These constraints enable the speaker to construct a referring expression that she believes will allow the hearer to identify the referent. As for the hearer, the explicit encoding of the adequacy of referring expressions allows referent </context>
<context position="80117" citStr="Cohen (1981)" startWordPosition="12342" endWordPosition="12343">orating on Referring Expressions 7. Comparisons to Related Work In providing a computational model of how agents collaborate upon referring expressions, we have touched on several different areas of research. First, our work has built on previous work in referring expressions, especially their incorporation into a model based on the planning paradigm. Second, our work has built on the research done in modeling clarifications in the planning paradigm and on plan repair. Third, our work is related to the research being done on modeling collaborative and joint activity. 7.1 Referring Expressions Cohen (1981) and Appelt (1985) have also addressed the generation of referring expressions in the planning paradigm. They have integrated this into a model of generating utterances, a step that we haven&apos;t taken. However, we have extended their model by incorporating even the generation of the components of the description into our planning model. One result of this is that our surface speech actions are much more fine-grained. 7.2 Clarifications and Plan Repair An important part of our work involves accounting for clarifications of referring expressions by using meta-actions that incorporate plan repair t</context>
</contexts>
<marker>Cohen, 1981</marker>
<rawString>Cohen, Philip R. (1981). &amp;quot;The need for referent identification as a planned action.&amp;quot; In Proceedings, International Joint Conference on Artificial Intelligence (IJCAI &apos;81), 31-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>Hector J Levesque</author>
</authors>
<title>Confirmation and joint action.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, International Joint Conference on Artificial Intelligence (IJCAI &apos;91),</booktitle>
<pages>951--958</pages>
<contexts>
<context position="84673" citStr="Cohen and Levesque 1991" startWordPosition="13068" endWordPosition="13071"> Sidner, and Lochbaum (Grosz and Sidner, 1990; Lochbaum, Grosz, and Sidner, 1990) are interested in the type of plans that underlie discourse in which the agents are collaborating in order to achieve some goal. They propose that agents are building a shared plan in which participants have a collection of beliefs and intentions about the actions in the plan. Our model differs from theirs in two important aspects. First, not only do agents have a collection of beliefs and intentions regarding the actions of a shared plan, but we feel that they also have an intention about the goal (Searle 1990; Cohen and Levesque 1991). It is this intention, in conjunction with the current plan, that sanctions the adoption of beliefs and intentions about potential actions that will contribute to the goal, rather than just the shared plan. Second, we feel that their definition of a partial shared plan is too restrictive. Although they address partial plans, they require, in order for an action to be part of a partial shared plan, that both agents believe that the action contributes to the goal. However, this is too strong. In collaborating to achieve a mutual goal, participants sometimes propose an action that is not believe</context>
<context position="87297" citStr="Cohen and Levesque (1991)" startWordPosition="13493" endWordPosition="13496">sals but on the current plan, or a part of it. Sidner (1994) addressed the issue of how conversational participants collaborate in building a shared plan. In this work, Sidner presents a number of speech actions for use in collaborative tasks. These actions are those that an artificial agent could use in negotiating which actions or beliefs to accept into the shared plan of the agents. As 378 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions with Traum, it is the proposals that are refashioned, before they are integrated into the shared plan, rather than the shared plan. Cohen and Levesque (1991) focus on formalizing joint intention in a logic. They use this formalism to explain how such elements of communication as confirmations arise when agents are engaging in a joint action. However, they have not addressed how agents collaborate in building a plan, only how agents collaborate while executing a plan. Once this limitation is overcome, their approach could offer us a route for formalizing the mental states of the collaborating agents in our model and for proving that our acceptance and goal adoption rules follow from such states. 8. Conclusion We have presented a computational model</context>
</contexts>
<marker>Cohen, Levesque, 1991</marker>
<rawString>Cohen, Philip R., and Levesque, Hector J. (1991). &amp;quot;Confirmation and joint action.&amp;quot; In Proceedings, International Joint Conference on Artificial Intelligence (IJCAI &apos;91), 951-958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>C Raymond Perrault</author>
</authors>
<title>Elements of a plan-based theory of speech acts.&amp;quot;</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<volume>3</volume>
<issue>3</issue>
<pages>177--212</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<note>Reprinted in Readings in Natural Language Processing, edited by</note>
<contexts>
<context position="12418" citStr="Cohen and Perrault (1979)" startWordPosition="1981" endWordPosition="1984">arification? In order to be consistent with Clark and Wilkes-Gibbs&apos;s model, we can see that if one agent finds the current referring expression problematic, the other must accept that judgment. Likewise, if one agent proposes a referring expression, through a refashioning, the other must accept the refashioning. 3. Referring Expressions 3.1 Planning and Referring By viewing language as action, the planning paradigm can be applied to natural language processing. The actions in this case are speech acts (Austin 1962; Searle 1969), and include such things as promising, informing, and requesting. Cohen and Perrault (1979) developed a system that uses plan construction to map an agent&apos;s goals to speech acts, and Allen and Perrault (1980) use plan inference to understand an agent&apos;s plan from its speech acts. By viewing it as action (Searle 1969), referring can be incorporated into a planning model. Cohen&apos;s model (1981) planned requests that the hearer identify a referent, whereas Appelt (1985) planned concept activations, a generalization of referring actions. Although acts of reference have been incorporated into plan-based models, determining the content of referring expressions hasn&apos;t been. For instance, in A</context>
<context position="51210" citStr="Cohen and Perrault 1979" startWordPosition="7899" endWordPosition="7902"> of this action depends only on whether the plan that is its parameter can be derived, but not whether the derived plan is valid.&apos; 5. Modeling Collaboration In the last two sections, we discussed how initial referring expressions, judgments, and refashionings can be generated and understood in our plan-based model. In this section, we show how plan construction and plan inference fit into a complete model of how an agent collaborates in making a referring action successful. Previous natural language systems that use plans to account for the surface speech acts underlying an utterance (such as Cohen and Perrault 1979; Allen and Perrault 1980; Appelt 1985; Litman and Allen 1987) model only the recognition or only the construction of an agent&apos;s plans, and so do not address this issue. In order to model an agent&apos;s participation in a dialog, we need to model how the mental state of the agent changes as a result of the contributions that are made to the dialog. The change in mental state can be modeled by the beliefs and goals that a participant adopts. When a speaker produces an utterance, as long as the hearer finds it coherent, he can add a belief that the speaker has made the utterance to accomplish some c</context>
</contexts>
<marker>Cohen, Perrault, 1979</marker>
<rawString>Cohen, Philip R., and Perrault, C. Raymond (1979). &amp;quot;Elements of a plan-based theory of speech acts.&amp;quot; Cognitive Science, 3(3), 177-212. Reprinted in Readings in Natural Language Processing, edited by Barbara J. Grosz, Karen Sparck Jones, and Bonnie Lynn Webber, 423-440. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
</authors>
<title>Cooking up referring expressions.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>68--75</pages>
<contexts>
<context position="13444" citStr="Dale (1989)" startWordPosition="2138" endWordPosition="2139">lization of referring actions. Although acts of reference have been incorporated into plan-based models, determining the content of referring expressions hasn&apos;t been. For instance, in Appelt&apos;s model, concept activations can be achieved by the action describe, which is a primitive, not further decomposed. Rather, this action has an associated procedure that determines a description that satisfies the preconditions of describe. Such special procedures have been the mainstay for accounting for the content of referring expressions, both in constructing and in understanding them, as exemplified by Dale (1989), who chose descriptors on the basis of their discriminatory power; Ehud Reiter (1990), who focused 1 For simplicity, we have not shown the change in speakers between refashionings and judgments. 354 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions on avoiding misleading conversational implicatures when generating descriptions; and Mellish (1985), who used a constraint satisfaction algorithm to identify referents. Our work follows the plan-based approach to language generation and understanding. We extend the earlier approaches of Cohen and Appelt by accounting for the c</context>
</contexts>
<marker>Dale, 1989</marker>
<rawString>Dale, R. (1989). &amp;quot;Cooking up referring expressions.&amp;quot; In Proceedings, 27th Annual Meeting of the Association for Computational Linguistics, 68-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip G Edmonds</author>
</authors>
<title>Collaboration on reference to objects that are not mutually known.&amp;quot;</title>
<date>1994</date>
<booktitle>In Proceedings, 15th International Conference on Computational Linguistics (COLING &apos;94), Kyoto,</booktitle>
<pages>1118--1122</pages>
<contexts>
<context position="26625" citStr="Edmonds (1994)" startWordPosition="4209" endWordPosition="4210">ion, and its constraint specifies that only one object can be in the candidate set.6 The second schema, shown in Figure 5, embodies the recursion. It uses the modifier plan, which adds a component to the description and updates the candidate set by computing the subset of it that satisfies the new component. The modifier plan thus accounts for individual components of the description. There are two different action schemas for modifier; one is for absolute modifiers, 4 Note that several category predications might be true of an object, and we do not explore which would be best to use, but see Edmonds (1994) for how preferences can be encoded. 5 We use specialization axioms (Kautz and Allen 1986) to map the modifiers action to the two schemas: modifiers-terminate and modifiers-recurse. 6 In order to distinguish this action from the primitive actions, it has a step that is marked null. Header: Constraint: Decomposition: headnoun(Entity,Object,Cand) world(World) bmb(Speaker,Hearer,category(Object,Category)) subset(World,AX. bmb(Speaker,Hearer,category(X,Category)),Cand) s-attrib(Entity,AX. categonf(X,Category)) 358 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions Header: modi</context>
<context position="90801" citStr="Edmonds 1994" startWordPosition="14042" endWordPosition="14043">could be extended to handle references to objects in focus and to descriptions that include a plan of physical actions for identifying the referent. Second, the treatment of clarifications could be improved; specifically, how plan failures are reasoned about, how plan failures affect the agent&apos;s beliefs, and how these failures are repaired. Third, this research needs to be integrated into a more complete plan-based approach to language, and needs to be extended so as to handle more general discourse plan failures (McRoy and Hirst 1993; McRoy and Hirst 1995; Horton and Hirst 1991; Heeman 1993; Edmonds 1994; Hirst et al. 1994). A benchmark for such future work could be dialog (8.1) below, from the London-Lund corpus (Svartvik and Quirk 1980, S.2.4a:1-8, which is the basis of the example used in Section 6. This dialog shows how collaboration on a referring expression can be embedded in other 379 Computational Linguistics Volume 21, Number 3 activities, how agents can return back to a collaborative activity, and even how agents can take advantage of a mistaken referent. (8.1) A: 1 What&apos;s that weird creature over there? B: 2 In the corner? A: 3 affirmative noise B: 4 Ws just a fern plant. A: 5 No, </context>
</contexts>
<marker>Edmonds, 1994</marker>
<rawString>Edmonds, Philip G. (1994). &amp;quot;Collaboration on reference to objects that are not mutually known.&amp;quot; In Proceedings, 15th International Conference on Computational Linguistics (COLING &apos;94), Kyoto, 1118-1122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley A Goodman</author>
</authors>
<title>Repairing reference identification failures by relaxation.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>204--217</pages>
<contexts>
<context position="41670" citStr="Goodman (1985)" startWordPosition="6506" endWordPosition="6508">r these conditions. The decomposition of the refashioning plans encodes how a new referring expression can be constructed from the old one. This involves three tasks: first, a single candidate referent is chosen; second, the referring expression is refashioned; and third, this is communicated to the hearer by way of the action s-actions, which was already discussed.&apos; The first step involves choosing a candidate. If the speaker of the refashioning is the agent who initiated the referring expression, then this choice is obviously pre-determined. Otherwise, the speaker must choose the candidate. Goodman (1985) has addressed this problem for the case of when the referring expression overconstrains the choice of referent. He uses heuristics to relax the constraints of the description and to pick one that nearly fits it. This problem is beyond the scope of this paper, and so we choose one of the referents arbitrarily (but see Heeman [19911 for how a simplified version of Goodman&apos;s algorithm that relaxes only a single constraint can be incorporated into the planning paradigm). The second step is to refashion the referring expression so that it identifies the candidate chosen in the first step. This is </context>
<context position="49319" citStr="Goodman 1985" startWordPosition="7643" endWordPosition="7644">truction and plan inference processes are essentially the same as those for referring expressions. However, the plan inference process has been augmented so as to embody the criteria for understanding that were outlined in Section 4.1. The inference of judgment plans must be sensitive to the fact that such a plan includes the constraint that the speaker found the judged plan to be in error even though the 11 If the error occurred in an instance of headnoun, a different replace-plan schema would need to be used, one that for instance relaxed the category that was used in describing the object (Goodman 1985; Heeman 1991). 12 We refer to the steps in the decomposition that are not action headers as mental actions. They need to be proved, just like constraints. 365 Computational Linguistics Volume 21, Number 3 Header: expand-plan(Plan) Constraint: bel(Speaker,error(Plan,ErrorNode)) content(Plan,ErrorNode,ErrorContent) ErrorContent = modifiers-terminate(Entity,Objectl,Cand) Decomposition: pick-one(Object,Cand) Replacement = modifiers-recurse(Entity,Object,Cand) substitute(Plan,ErrorNode,Replacement,NewPlan) repla- n(NewPlan,Acts) s-actions(Plan,Acts) Effect: bel(Hearer,goal(Speaker,bel(Hearer,bel(S</context>
</contexts>
<marker>Goodman, 1985</marker>
<rawString>Goodman, Bradley A. (1985). &amp;quot;Repairing reference identification failures by relaxation.&amp;quot; In Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics, 204-217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Plans for discourse.&amp;quot;</title>
<date>1990</date>
<booktitle>In Intentions in Communication, SDF Benchmark Series, edited by</booktitle>
<pages>417--444</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="53241" citStr="Grosz and Sidner (1990)" startWordPosition="8250" endWordPosition="8253">uld affect the evaluation of the parent plan. 366 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions they are collaborating upon. If the plan is not adequate, then they must work together to refashion it. This level of cooperation is due to what Clark and Wilkes-Gibbs refer to as a mutual responsibility, or what Searle (1990) refers to as a we-intention. This allows the agents to interact so that neither assumes control of the dialog, thus allowing both to contribute to the best of their ability without being controlled or impeded by the other. This is different from what Grosz and Sidner (1990) have called master—servant dialogs, which occur in teacher—apprentice or information-seeking dialogs, in which one of the participants is controlling the conversation (cf. Walker and Whittaker 1990). Note that the noncontrolling agent may be helpful by anticipating obstacles in the plan (Allen and Perrault 1980), but this is not the same as collaborating. The mutual responsibility that the agents share not only concerns the goal they are trying to achieve, but also the plan that they are currently considering. This plan serves to coordinate their activity and so agents will have intentions to</context>
<context position="84094" citStr="Grosz and Sidner, 1990" startWordPosition="12970" endWordPosition="12973">oblemsolving activities and discourse activities. In contrast, our clarifications embody both functions in the same actions, thus allowing for a simpler approach to inferring the refashioned referring expressions, since we need not chain to a meta-operator. In later work, Chu-Carroll and Carberry (1994) extended this model to generate responses to proposals that are viewed as sub-optimal or invalid. Like Litman and Allen (1987), they adopt the view that subsequent modifications apply to the preceding modification, rather than the underlying plan. 7.3 Collaboration Grosz, Sidner, and Lochbaum (Grosz and Sidner, 1990; Lochbaum, Grosz, and Sidner, 1990) are interested in the type of plans that underlie discourse in which the agents are collaborating in order to achieve some goal. They propose that agents are building a shared plan in which participants have a collection of beliefs and intentions about the actions in the plan. Our model differs from theirs in two important aspects. First, not only do agents have a collection of beliefs and intentions regarding the actions of a shared plan, but we feel that they also have an intention about the goal (Searle 1990; Cohen and Levesque 1991). It is this intentio</context>
</contexts>
<marker>Grosz, Sidner, 1990</marker>
<rawString>Grosz, Barbara J., and Sidner, Candace L. (1990). &amp;quot;Plans for discourse.&amp;quot; In Intentions in Communication, SDF Benchmark Series, edited by Philip R. Cohen, Jerry Morgan, and Martha E. Pollack, 417-444. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Hayes</author>
</authors>
<title>A representation for robot plans.&amp;quot;</title>
<date>1975</date>
<booktitle>In Proceedings, International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<volume>75</volume>
<pages>181--188</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<note>Reprinted in Readings in Planning, edited by</note>
<contexts>
<context position="42318" citStr="Hayes 1975" startWordPosition="6615" endWordPosition="6616">se of when the referring expression overconstrains the choice of referent. He uses heuristics to relax the constraints of the description and to pick one that nearly fits it. This problem is beyond the scope of this paper, and so we choose one of the referents arbitrarily (but see Heeman [19911 for how a simplified version of Goodman&apos;s algorithm that relaxes only a single constraint can be incorporated into the planning paradigm). The second step is to refashion the referring expression so that it identifies the candidate chosen in the first step. This is done by using plan repair techniques (Hayes 1975; Wilensky 1981; Wilkens 1985). Our technique is to remove the subplan rooted at the action in error and replan with another action schema inserted in its place. This technique has been encoded into our refashioning plans, and so can be used for both constructing repairs and inferring how another agent has repaired a plan. Now we consider the effect of these refashioning plans. As we mentioned in Section 2, once the refashioning plan is accepted, the common ground of the participants is updated with the new referring expression. So, the effect of the refashioning plans is that the hearer will </context>
</contexts>
<marker>Hayes, 1975</marker>
<rawString>Hayes, Philip J. (1975). &amp;quot;A representation for robot plans.&amp;quot; In Proceedings, International Joint Conference on Artificial Intelligence (IJCAI &apos;75), 181-188. Reprinted in Readings in Planning, edited by James Allen, James Hendler, and Austin Tate, 154-161. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
</authors>
<title>Speech actions and mental states in task-oriented dialogs.&amp;quot;</title>
<date>1993</date>
<booktitle>In Working Notes AAAI Spring Symposium on Reasoning about Mental States: Formal Theories and Applications,</booktitle>
<pages>68--73</pages>
<location>Stanford,</location>
<contexts>
<context position="90787" citStr="Heeman 1993" startWordPosition="14040" endWordPosition="14041"> expressions could be extended to handle references to objects in focus and to descriptions that include a plan of physical actions for identifying the referent. Second, the treatment of clarifications could be improved; specifically, how plan failures are reasoned about, how plan failures affect the agent&apos;s beliefs, and how these failures are repaired. Third, this research needs to be integrated into a more complete plan-based approach to language, and needs to be extended so as to handle more general discourse plan failures (McRoy and Hirst 1993; McRoy and Hirst 1995; Horton and Hirst 1991; Heeman 1993; Edmonds 1994; Hirst et al. 1994). A benchmark for such future work could be dialog (8.1) below, from the London-Lund corpus (Svartvik and Quirk 1980, S.2.4a:1-8, which is the basis of the example used in Section 6. This dialog shows how collaboration on a referring expression can be embedded in other 379 Computational Linguistics Volume 21, Number 3 activities, how agents can return back to a collaborative activity, and even how agents can take advantage of a mistaken referent. (8.1) A: 1 What&apos;s that weird creature over there? B: 2 In the corner? A: 3 affirmative noise B: 4 Ws just a fern pl</context>
</contexts>
<marker>Heeman, 1993</marker>
<rawString>Heeman, Peter A. (1993). &amp;quot;Speech actions and mental states in task-oriented dialogs.&amp;quot; In Working Notes AAAI Spring Symposium on Reasoning about Mental States: Formal Theories and Applications, Stanford, 68-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Anthony Heeman</author>
</authors>
<title>A computational model of collaboration on referring expressions.&amp;quot;</title>
<date>1991</date>
<tech>Master&apos;s thesis, Technical Report CSRI 251,</tech>
<institution>Department of Computer Science, University of Toronto.</institution>
<contexts>
<context position="49333" citStr="Heeman 1991" startWordPosition="7645" endWordPosition="7646">lan inference processes are essentially the same as those for referring expressions. However, the plan inference process has been augmented so as to embody the criteria for understanding that were outlined in Section 4.1. The inference of judgment plans must be sensitive to the fact that such a plan includes the constraint that the speaker found the judged plan to be in error even though the 11 If the error occurred in an instance of headnoun, a different replace-plan schema would need to be used, one that for instance relaxed the category that was used in describing the object (Goodman 1985; Heeman 1991). 12 We refer to the steps in the decomposition that are not action headers as mental actions. They need to be proved, just like constraints. 365 Computational Linguistics Volume 21, Number 3 Header: expand-plan(Plan) Constraint: bel(Speaker,error(Plan,ErrorNode)) content(Plan,ErrorNode,ErrorContent) ErrorContent = modifiers-terminate(Entity,Objectl,Cand) Decomposition: pick-one(Object,Cand) Replacement = modifiers-recurse(Entity,Object,Cand) substitute(Plan,ErrorNode,Replacement,NewPlan) repla- n(NewPlan,Acts) s-actions(Plan,Acts) Effect: bel(Hearer,goal(Speaker,bel(Hearer,bel(Speaker, replac</context>
<context position="58405" citStr="Heeman 1991" startWordPosition="9074" endWordPosition="9075">r,bel(Agt1,Prop)) &lt; bmb(system,user,goal(Agt1,bel(Agt2,bel(Agt1,Prop)))) &amp; Agt1,Agt2 E {system,user} &amp; not(Agt1 = Agt2) The last rule involves an inference that is not shared. When the user makes a contribution to a conversation, the system assumes that the user believes that the plan will achieve its intended goal. Rule 3 bel(system,bel(user,achieve(Plan,Goal))) &lt; bmb(system,user,plan(user,Plan,Goal)) 5.2 Rules for Updating the Collaborative State The second set of rules that we give concern how the agents update the collaborative state. These rules have been revised from an earlier version (Heeman 1991) so as to better model the acceptance process. 5.2.1 Entering into a Collaborative Activity. We need a rule that permits an agent to enter into a collaborative activity. We use the predicate cstate to represent that an agent is in such a state, and this predicate takes as its parameters the agents involved, the goal they are trying to achieve, and their current plan. Our view of when such a collaborative activity can be entered is very simple: the system believes it is mutually believed that one of them has a goal to refer and has a plan for doing so, but one of them believes this plan to be i</context>
</contexts>
<marker>Heeman, 1991</marker>
<rawString>Heeman, Peter Anthony (1991). &amp;quot;A computational model of collaboration on referring expressions.&amp;quot; Master&apos;s thesis, Technical Report CSRI 251, Department of Computer Science, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>Susan McRoy</author>
<author>Peter Heeman</author>
<author>Philip Edmonds</author>
<author>Diane Horton</author>
</authors>
<title>Repairing conversational misunderstandings and non-understandings.&amp;quot;</title>
<date>1994</date>
<journal>Speech Communications,</journal>
<volume>15</volume>
<issue>3</issue>
<pages>213--229</pages>
<contexts>
<context position="90821" citStr="Hirst et al. 1994" startWordPosition="14044" endWordPosition="14047">ded to handle references to objects in focus and to descriptions that include a plan of physical actions for identifying the referent. Second, the treatment of clarifications could be improved; specifically, how plan failures are reasoned about, how plan failures affect the agent&apos;s beliefs, and how these failures are repaired. Third, this research needs to be integrated into a more complete plan-based approach to language, and needs to be extended so as to handle more general discourse plan failures (McRoy and Hirst 1993; McRoy and Hirst 1995; Horton and Hirst 1991; Heeman 1993; Edmonds 1994; Hirst et al. 1994). A benchmark for such future work could be dialog (8.1) below, from the London-Lund corpus (Svartvik and Quirk 1980, S.2.4a:1-8, which is the basis of the example used in Section 6. This dialog shows how collaboration on a referring expression can be embedded in other 379 Computational Linguistics Volume 21, Number 3 activities, how agents can return back to a collaborative activity, and even how agents can take advantage of a mistaken referent. (8.1) A: 1 What&apos;s that weird creature over there? B: 2 In the corner? A: 3 affirmative noise B: 4 Ws just a fern plant. A: 5 No, the one to the left </context>
</contexts>
<marker>Hirst, McRoy, Heeman, Edmonds, Horton, 1994</marker>
<rawString>Hirst, Graeme; McRoy, Susan; Heeman, Peter; Edmonds, Philip; and Horton, Diane (1994). &amp;quot;Repairing conversational misunderstandings and non-understandings.&amp;quot; Speech Communications, 15(3/4), 213-229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Katherine Horrigan</author>
</authors>
<title>Modelling simple dialogs.&amp;quot;</title>
<date>1977</date>
<tech>Master&apos;s thesis, Technical Report 108,</tech>
<institution>Department of Computer Science, University of Toronto.</institution>
<contexts>
<context position="81538" citStr="Horrigan 1977" startWordPosition="12562" endWordPosition="12563">several major differences between our work and theirs. First, our work addresses not only understanding, but also generation, and how these two tasks fit into a model of how agents collaborate in discourse. Second, Litman and Allen use a stack of unchanging plans to represent the state of the discourse. We, however, use a single current plan, modifying it as clarifications are made. This difference has an important ramification, for it results in different interpretations of the discourse structure. Consider dialog (7.1), which was collected at an information booth in a Toronto train station (Horrigan 1977). (Although the participants are not collaborating in making a referring expression, the dialog will serve to illustrate our point.) (7.1) P: 1 The 8:50 to Montreal? C: 2 8:50 to Montreal. Gate 7. P: 3 Where is it? C: 4 Down this way to your left. Second one on the left. P: 5 OK. Thank you. Litman and Allen represent the state of the discourse after the second utterance as a clarification of the passenger&apos;s take-train-trip plan. The information that the train boards at gate 7 is represented only in the clarification plan. So, when the passenger asks &amp;quot;Where is it?,&amp;quot; their system, acting as the </context>
</contexts>
<marker>Horrigan, 1977</marker>
<rawString>Horrigan, Mary Katherine (1977). &amp;quot;Modelling simple dialogs.&amp;quot; Master&apos;s thesis, Technical Report 108, Department of Computer Science, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Horton</author>
<author>Graeme Hirst</author>
</authors>
<title>Discrepancies in discourse models and miscommunication in conversation.&amp;quot;</title>
<date>1991</date>
<booktitle>In Working Notes of the AAA&apos; Symposium: Discourse Structure in Natural Language Understanding and Generation,</booktitle>
<pages>31--32</pages>
<contexts>
<context position="90774" citStr="Horton and Hirst 1991" startWordPosition="14035" endWordPosition="14039">r coverage of referring expressions could be extended to handle references to objects in focus and to descriptions that include a plan of physical actions for identifying the referent. Second, the treatment of clarifications could be improved; specifically, how plan failures are reasoned about, how plan failures affect the agent&apos;s beliefs, and how these failures are repaired. Third, this research needs to be integrated into a more complete plan-based approach to language, and needs to be extended so as to handle more general discourse plan failures (McRoy and Hirst 1993; McRoy and Hirst 1995; Horton and Hirst 1991; Heeman 1993; Edmonds 1994; Hirst et al. 1994). A benchmark for such future work could be dialog (8.1) below, from the London-Lund corpus (Svartvik and Quirk 1980, S.2.4a:1-8, which is the basis of the example used in Section 6. This dialog shows how collaboration on a referring expression can be embedded in other 379 Computational Linguistics Volume 21, Number 3 activities, how agents can return back to a collaborative activity, and even how agents can take advantage of a mistaken referent. (8.1) A: 1 What&apos;s that weird creature over there? B: 2 In the corner? A: 3 affirmative noise B: 4 Ws j</context>
</contexts>
<marker>Horton, Hirst, 1991</marker>
<rawString>Horton, Diane, and Hirst, Graeme (1991). &amp;quot;Discrepancies in discourse models and miscommunication in conversation.&amp;quot; In Working Notes of the AAA&apos; Symposium: Discourse Structure in Natural Language Understanding and Generation, 31-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry A Kautz</author>
<author>James F Allen</author>
</authors>
<title>Generalized plan recognition.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, National Conference on Artificial Intelligence (AAAI &apos;86),</booktitle>
<pages>32--37</pages>
<contexts>
<context position="26715" citStr="Kautz and Allen 1986" startWordPosition="4222" endWordPosition="4225">6 The second schema, shown in Figure 5, embodies the recursion. It uses the modifier plan, which adds a component to the description and updates the candidate set by computing the subset of it that satisfies the new component. The modifier plan thus accounts for individual components of the description. There are two different action schemas for modifier; one is for absolute modifiers, 4 Note that several category predications might be true of an object, and we do not explore which would be best to use, but see Edmonds (1994) for how preferences can be encoded. 5 We use specialization axioms (Kautz and Allen 1986) to map the modifiers action to the two schemas: modifiers-terminate and modifiers-recurse. 6 In order to distinguish this action from the primitive actions, it has a step that is marked null. Header: Constraint: Decomposition: headnoun(Entity,Object,Cand) world(World) bmb(Speaker,Hearer,category(Object,Category)) subset(World,AX. bmb(Speaker,Hearer,category(X,Category)),Cand) s-attrib(Entity,AX. categonf(X,Category)) 358 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions Header: modifiers-terminate(Entity,Object,Cand) Constraint: Cand = (Object] Decomposition: null Figure</context>
</contexts>
<marker>Kautz, Allen, 1986</marker>
<rawString>Kautz, Henry A., and Allen, James F. (1986). &amp;quot;Generalized plan recognition.&amp;quot; In Proceedings, National Conference on Artificial Intelligence (AAAI &apos;86), 32-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Lambert</author>
<author>Sandra Carberry</author>
</authors>
<title>A tripartite plan-based model for dialogue.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="44341" citStr="Lambert and Carberry 1991" startWordPosition="6948" endWordPosition="6951">he clarification actions, and so the surface actions do not include any constraints or effects. The notation used in the action schemas was given in Table 1 above. accept-plan. The discourse action accept-plan, shown in Figure 8, is used by the speaker to establish the mutual belief that a plan will achieve its goal. The constraints of the schema specify that the plan being accepted achieves its goal and the decomposition is the surface speech action s-accept. The effect of the schema is that the hearer 10 Another approach would have been to separate the communicative task from the first two (Lambert and Carberry 1991). 363 Computational Linguistics Volume 21, Number 3 Header: accept-plan(Plan) Constraint: bel(Speaker,achieve(Plan,Goal)) Decomposition: s-accept(Plan) Effect: bel(Hearer,goal(Speaker,bel(Hearer,bel(Speaker, achieve(Plan,Goal))))) Figure 8 accept-plan schema. Header: reject-plan(Plan) Constraint: bel(Speaker,error(Plan,ErrorNode)) Decomposition: yield(Plan,ErrorNode,Acts) Effect: not(Acts = fl) s-reject(Plan,Acts) - bel(Hearer,goal(Speaker,bel(Hearer,bel(System, error(Plan,ErrorNode))))) Figure 9 reject-plan schema. Header: postpone-plan(Plan) Constraint: bel(Speaker,error(Plan,ErrorNode)) Dec</context>
<context position="83372" citStr="Lambert and Carberry (1991)" startWordPosition="12865" endWordPosition="12868">1) on interactive explanations also addresses clarifications using plan repair techniques. This body of work uses plan construction techniques to generate explanations, and uses the 377 Computational Linguistics Volume 21, Number 3 constructed plan as a basis for recovery strategies if the user doesn&apos;t understand the explanation. In the cases of Cawsey and Carletta, both use meta-actions to encode the plan repair techniques. However, none of these approaches is within a collaborative framework, in which either agent can contribute to the development of the plan. Other relevant work is that of Lambert and Carberry (1991). In their model of understanding information-seeking dialogs, they propose a distinction between problemsolving activities and discourse activities. In contrast, our clarifications embody both functions in the same actions, thus allowing for a simpler approach to inferring the refashioned referring expressions, since we need not chain to a meta-operator. In later work, Chu-Carroll and Carberry (1994) extended this model to generate responses to proposals that are viewed as sub-optimal or invalid. Like Litman and Allen (1987), they adopt the view that subsequent modifications apply to the prec</context>
</contexts>
<marker>Lambert, Carberry, 1991</marker>
<rawString>Lambert, Lynn, and Carberry, Sandra (1991). &amp;quot;A tripartite plan-based model for dialogue.&amp;quot; In Proceedings, 29th Annual Meeting of the Association for Computational Linguistics, 47-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem J M Levelt</author>
</authors>
<title>Speaking: From Intention to Articulation.</title>
<date>1989</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="40280" citStr="Levelt (1989" startWordPosition="6280" endWordPosition="6281">determine the action in the referring plan that the speaker found problematic due to the constraints specified in the action schemas. The identity of the action in error will provide context for the subsequent refashioning of the referring expression.&apos; Refashioning Plans. If a conversant rejects a referring expression or postpones judgment on it, then either the speaker or the hearer will refashion the expression in the context of the rejection or postponement. In keeping with Clark and Wilkes-Gibbs, we use two discourse plans for refashioning: replace-plan and expand-plan. The first is 8 See Levelt (1989, Chapter 12) for how prosody and clue words can be used in determining the type of clarification. 9 Another approach would be to use the identity of the action in error to revise the beliefs that the agent has attributed to the other conversant and to use the revised beliefs in refashioning the plan. However, such reasoning is beyond the scope of this work. 362 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions used to replace some of the actions in the referring expression plan with new ones, and the second is to add new actions. Replacements can be used if the referring</context>
</contexts>
<marker>Levelt, 1989</marker>
<rawString>Levelt, Willem J. M. (1989). Speaking: From Intention to Articulation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>James F Allen</author>
</authors>
<title>A plan recognition model for subdialogs in conversations.&amp;quot;</title>
<date>1987</date>
<journal>Cognitive Science,</journal>
<volume>11</volume>
<issue>2</issue>
<pages>163--200</pages>
<contexts>
<context position="34723" citStr="Litman and Allen (1987)" startWordPosition="5393" endWordPosition="5396">ted, since they will now have only a single solution. 4. Clarifications 4.1 Planning and Clarifying Clark and Wilkes-Gibbs (1986) have presented a model of how conversational participants collaborate in making a referring action successful (see Section 2 above). Their model consists of conversational moves that express a judgment of a referring expression and conversational moves that refashion an expression. However, their model is not computational. They do not account for how the judgment is made, how the judgment affects the refashioning, or the content of the moves. Following the work of Litman and Allen (1987) in understanding clarification subdialogs, we formalize the conversational moves of Clark and Wilkes-Gibbs as discourse actions. These discourse actions are meta-actions that take as a parameter a referring expression plan. The constraints and decompositions of the discourse actions encode the conditions under which they can be applied, how the referring expression derivations can be refashioned, and how the speaker&apos;s beliefs can be communicated to the hearer. So, the conversational moves, or clarifications, can be generated and understood within the planning paradigm/ Surface Speech Actions.</context>
<context position="37687" citStr="Litman and Allen 1987" startWordPosition="5870" endWordPosition="5873">ered by the same person, they could be combined into a single utterance. For instance, the utterance &amp;quot;no, the red one&amp;quot; could be interpreted as an s-reject of the color that was previously used to describe something and an s-actions for the color &amp;quot;red.&amp;quot; So, as we can see, the surface speech actions for clarifications operate on components of the plan that is being built, namely the surface speech actions of referring expression plans. This is consistent with our use of plan derivations to represent utterances. Although we could have viewed the clarification speech actions as acts of informing (Litman and Allen 1987), this would have shifted the complexity into the parameter of the inform, and it is unclear whether anything would have been gained. Instead, we feel that a parser with a model of the discourse and the context can determine the surface speech actions.8 Additionally, it should be easier for the generator to determine an appropriate surface form. Judgment Plans. The evaluation of the referring expression plan indicates whether the referring action was successful or not. If it was successful, then the referent has been identified, and so a goal to communicate this is input to the plan constructo</context>
<context position="51272" citStr="Litman and Allen 1987" startWordPosition="7909" endWordPosition="7912">rameter can be derived, but not whether the derived plan is valid.&apos; 5. Modeling Collaboration In the last two sections, we discussed how initial referring expressions, judgments, and refashionings can be generated and understood in our plan-based model. In this section, we show how plan construction and plan inference fit into a complete model of how an agent collaborates in making a referring action successful. Previous natural language systems that use plans to account for the surface speech acts underlying an utterance (such as Cohen and Perrault 1979; Allen and Perrault 1980; Appelt 1985; Litman and Allen 1987) model only the recognition or only the construction of an agent&apos;s plans, and so do not address this issue. In order to model an agent&apos;s participation in a dialog, we need to model how the mental state of the agent changes as a result of the contributions that are made to the dialog. The change in mental state can be modeled by the beliefs and goals that a participant adopts. When a speaker produces an utterance, as long as the hearer finds it coherent, he can add a belief that the speaker has made the utterance to accomplish some communicative goal. The hearer might then adopt some goal of hi</context>
<context position="83903" citStr="Litman and Allen (1987)" startWordPosition="12942" endWordPosition="12945">e to the development of the plan. Other relevant work is that of Lambert and Carberry (1991). In their model of understanding information-seeking dialogs, they propose a distinction between problemsolving activities and discourse activities. In contrast, our clarifications embody both functions in the same actions, thus allowing for a simpler approach to inferring the refashioned referring expressions, since we need not chain to a meta-operator. In later work, Chu-Carroll and Carberry (1994) extended this model to generate responses to proposals that are viewed as sub-optimal or invalid. Like Litman and Allen (1987), they adopt the view that subsequent modifications apply to the preceding modification, rather than the underlying plan. 7.3 Collaboration Grosz, Sidner, and Lochbaum (Grosz and Sidner, 1990; Lochbaum, Grosz, and Sidner, 1990) are interested in the type of plans that underlie discourse in which the agents are collaborating in order to achieve some goal. They propose that agents are building a shared plan in which participants have a collection of beliefs and intentions about the actions in the plan. Our model differs from theirs in two important aspects. First, not only do agents have a colle</context>
<context position="89608" citStr="Litman and Allen 1987" startWordPosition="13853" endWordPosition="13856">aborate. First, it minimizes the distinction between the roles of the person who initiates the referring expression and the person who is trying to identify it. Both have the same moves available to them, for either can judge the description and either can refashion it. This allows both participants to contribute without being controlled or impeded by the other. Second, their model gives special status to the role of the current referring expression (current plan): participants judge and refashion the current referring expression directly, rather than recursively modifying modifications (e.g. Litman and Allen 1987; Chu-Carroll and Carberry 1994) or incrementally adding to the current plan with each accepted proposal (e.g. Traum and Hinkelman 1992; Sidner 1992). In our work, we have taken Clark and Wilkes-Gibbs&apos;s descriptive model and recast it into a computational one, thus demonstrating the computational feasibility of their work and its compatibility with current practices in artificial intelligence. There are many ways that this research could be extended. Perhaps the most obvious would be to extend the planning component of our model. First, our coverage of referring expressions could be extended t</context>
</contexts>
<marker>Litman, Allen, 1987</marker>
<rawString>Litman, Diane J., and Allen, James F. (1987). &amp;quot;A plan recognition model for subdialogs in conversations.&amp;quot; Cognitive Science, 11(2), 163-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen E Lochbaum</author>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Models of plans to support communication: An initial report.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, National Conference on Artificial Intelligence (AAAI &apos;90),</booktitle>
<pages>485--490</pages>
<contexts>
<context position="84129" citStr="Lochbaum, Grosz, and Sidner, 1990" startWordPosition="12974" endWordPosition="12978">and discourse activities. In contrast, our clarifications embody both functions in the same actions, thus allowing for a simpler approach to inferring the refashioned referring expressions, since we need not chain to a meta-operator. In later work, Chu-Carroll and Carberry (1994) extended this model to generate responses to proposals that are viewed as sub-optimal or invalid. Like Litman and Allen (1987), they adopt the view that subsequent modifications apply to the preceding modification, rather than the underlying plan. 7.3 Collaboration Grosz, Sidner, and Lochbaum (Grosz and Sidner, 1990; Lochbaum, Grosz, and Sidner, 1990) are interested in the type of plans that underlie discourse in which the agents are collaborating in order to achieve some goal. They propose that agents are building a shared plan in which participants have a collection of beliefs and intentions about the actions in the plan. Our model differs from theirs in two important aspects. First, not only do agents have a collection of beliefs and intentions regarding the actions of a shared plan, but we feel that they also have an intention about the goal (Searle 1990; Cohen and Levesque 1991). It is this intention, in conjunction with the current </context>
</contexts>
<marker>Lochbaum, Grosz, Sidner, 1990</marker>
<rawString>Lochbaum, Karen E.; Grosz, Barbara J.; and Sidner, Candace L. (1990). &amp;quot;Models of plans to support communication: An initial report.&amp;quot; In Proceedings, National Conference on Artificial Intelligence (AAAI &apos;90), 485-490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan McRoy</author>
<author>Graeme Hirst</author>
</authors>
<title>Abductive explanations of dialogue misunderstanding.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings, 6th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<location>Utrecht, Netherlands,</location>
<contexts>
<context position="90729" citStr="McRoy and Hirst 1993" startWordPosition="14027" endWordPosition="14030">e planning component of our model. First, our coverage of referring expressions could be extended to handle references to objects in focus and to descriptions that include a plan of physical actions for identifying the referent. Second, the treatment of clarifications could be improved; specifically, how plan failures are reasoned about, how plan failures affect the agent&apos;s beliefs, and how these failures are repaired. Third, this research needs to be integrated into a more complete plan-based approach to language, and needs to be extended so as to handle more general discourse plan failures (McRoy and Hirst 1993; McRoy and Hirst 1995; Horton and Hirst 1991; Heeman 1993; Edmonds 1994; Hirst et al. 1994). A benchmark for such future work could be dialog (8.1) below, from the London-Lund corpus (Svartvik and Quirk 1980, S.2.4a:1-8, which is the basis of the example used in Section 6. This dialog shows how collaboration on a referring expression can be embedded in other 379 Computational Linguistics Volume 21, Number 3 activities, how agents can return back to a collaborative activity, and even how agents can take advantage of a mistaken referent. (8.1) A: 1 What&apos;s that weird creature over there? B: 2 In</context>
</contexts>
<marker>McRoy, Hirst, 1993</marker>
<rawString>McRoy, Susan, and Hirst, Graeme (1993). &amp;quot;Abductive explanations of dialogue misunderstanding.&amp;quot; In Proceedings, 6th Conference of the European Chapter of the Association for Computational Linguistics. Utrecht, Netherlands, April 1993,277-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan McRoy</author>
<author>Graeme Hirst</author>
</authors>
<title>The repair of speech act misunderstandings by abductive inference.&amp;quot;</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<note>to appear.</note>
<contexts>
<context position="90751" citStr="McRoy and Hirst 1995" startWordPosition="14031" endWordPosition="14034">f our model. First, our coverage of referring expressions could be extended to handle references to objects in focus and to descriptions that include a plan of physical actions for identifying the referent. Second, the treatment of clarifications could be improved; specifically, how plan failures are reasoned about, how plan failures affect the agent&apos;s beliefs, and how these failures are repaired. Third, this research needs to be integrated into a more complete plan-based approach to language, and needs to be extended so as to handle more general discourse plan failures (McRoy and Hirst 1993; McRoy and Hirst 1995; Horton and Hirst 1991; Heeman 1993; Edmonds 1994; Hirst et al. 1994). A benchmark for such future work could be dialog (8.1) below, from the London-Lund corpus (Svartvik and Quirk 1980, S.2.4a:1-8, which is the basis of the example used in Section 6. This dialog shows how collaboration on a referring expression can be embedded in other 379 Computational Linguistics Volume 21, Number 3 activities, how agents can return back to a collaborative activity, and even how agents can take advantage of a mistaken referent. (8.1) A: 1 What&apos;s that weird creature over there? B: 2 In the corner? A: 3 affi</context>
</contexts>
<marker>McRoy, Hirst, 1995</marker>
<rawString>McRoy, Susan, and Hirst, Graeme (1995). &amp;quot;The repair of speech act misunderstandings by abductive inference.&amp;quot; Computational Linguistics, 21(4), to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Mellish</author>
</authors>
<title>Computer Interpretation of Natural Language Descriptions, Ellis Horwood Series in Artificial Intelligence.</title>
<date>1985</date>
<publisher>Ellis Horwood.</publisher>
<contexts>
<context position="13815" citStr="Mellish (1985)" startWordPosition="2191" endWordPosition="2192">s a description that satisfies the preconditions of describe. Such special procedures have been the mainstay for accounting for the content of referring expressions, both in constructing and in understanding them, as exemplified by Dale (1989), who chose descriptors on the basis of their discriminatory power; Ehud Reiter (1990), who focused 1 For simplicity, we have not shown the change in speakers between refashionings and judgments. 354 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions on avoiding misleading conversational implicatures when generating descriptions; and Mellish (1985), who used a constraint satisfaction algorithm to identify referents. Our work follows the plan-based approach to language generation and understanding. We extend the earlier approaches of Cohen and Appelt by accounting for the content of the description at the planning level. This is done by having surface speech actions for each component of a description, plus a surface speech action that expresses a speaker&apos;s intention to refer. A referring action is composed of these primitive actions, and the speaker utters them in her attempt to refer to an object. These speech actions are the building </context>
</contexts>
<marker>Mellish, 1985</marker>
<rawString>Mellish, C. S. (1985). Computer Interpretation of Natural Language Descriptions, Ellis Horwood Series in Artificial Intelligence. Ellis Horwood.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jahanna D Moore</author>
<author>Swartout</author>
</authors>
<marker>Moore, Swartout, </marker>
<rawString>Moore, Jahanna D., and Swartout,</rawString>
</citation>
<citation valid="true">
<authors>
<author>R William</author>
</authors>
<title>A reactive approach to explanation: Taking the user&apos;s feedback into account.&amp;quot;</title>
<date>1991</date>
<booktitle>In Natural Language Generation in Artificial Intelligence and Computational Linguistics, edited by Cecile</booktitle>
<pages>3--48</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>William, 1991</marker>
<rawString>William R. (1991). &amp;quot;A reactive approach to explanation: Taking the user&apos;s feedback into account.&amp;quot; In Natural Language Generation in Artificial Intelligence and Computational Linguistics, edited by Cecile L. Paris, William R. Swartout, and William C. Mann, 3-48. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gopalan Nadathur</author>
<author>Aravind K Johi</author>
</authors>
<title>Mutual beliefs in conversational systems: Their role in referring expressions.&amp;quot;</title>
<date>1983</date>
<booktitle>In Proceedings, International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<volume>83</volume>
<pages>603--605</pages>
<marker>Nadathur, Johi, 1983</marker>
<rawString>Nadathur, Gopalan, and Johi, Aravind K. (1983). &amp;quot;Mutual beliefs in conversational systems: Their role in referring expressions.&amp;quot; In Proceedings, International Joint Conference on Artificial Intelligence (IJCAI &apos;83), 603-605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Perrault</author>
</authors>
<title>An application of default logic to speech act theory.&amp;quot;</title>
<date>1990</date>
<booktitle>In Intentions in Communication, SDF Benchmark Series, edited by</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="56588" citStr="Perrault (1990)" startWordPosition="8797" endWordPosition="8798"> a belief about whether she believes the plan will achieve the goal, and if not, the action that she believes to be in error. So, one of the following propositions will be adopted. 14 The collaborative activity also sanctions discourse expectations that the other participant&apos;s utterances will pertain to the collaborative activity. We do not explicitly address this, however. 15 For simplicity, we represent the rules for entering into a collaborative activity, adopting beliefs, and adopting goals with the same operator, For a more formal account, three different operators should be used. 16 See Perrault (1990) for how these inferences can be drawn by using default rules. 367 Computational Linguistics Volume 21, Number 3 bel(system,achieve(Plan,Goal)) bel(system,error(Plan,Node)) After the above beliefs have been added, there are a number of inferences that the agents can make and, in fact, can believe will be made by the other participant as well, and so these inferences can be mutually believed. The first rule is that if it is mutually believed that the speaker intends to achieve Goal by means of Plan, then it will be mutually believed that the speaker has Goal as one of her goals.&apos; Rule 1 bmb(sys</context>
</contexts>
<marker>Perrault, 1990</marker>
<rawString>Perrault, C. R. (1990). &amp;quot;An application of default logic to speech act theory.&amp;quot; In Intentions in Communication, SDF Benchmark Series, edited by Philip R. Cohen, Jerry Morgan, and Martha E. Pollack, 161-185. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Raymond Perrault</author>
<author>Philip R Cohen</author>
</authors>
<title>It&apos;s for your own good: A note on inaccurate reference.&amp;quot;</title>
<date>1981</date>
<booktitle>In Elements of Discourse Understanding,</booktitle>
<pages>217--230</pages>
<publisher>Cambridge University Press.</publisher>
<note>edited by Aravind</note>
<contexts>
<context position="14947" citStr="Perrault and Cohen 1981" startWordPosition="2371" endWordPosition="2374">er utters them in her attempt to refer to an object. These speech actions are the building blocks that referring expressions are made from. Acting as the mortar are intermediate actions, which have constraints that the plan construction and plan inference processes can reason about. These constraints encode the knowledge of how a description can allow a hearer to identify an object. First, the constraints express the conditions under which an attribute can be used to refer to an object; for instance, that it be mutually believed that the object has a certain property (Clark and Marshall 1981; Perrault and Cohen 1981; Nadathur and Joshi 1983). Second, the constraints keep track of which objects could be believed to be the referent of the referring expression. Third, the constraints ensure that a sufficient number of surface speech actions are added so that the set of candidates associated with the entire referring expression consists of only a single object, the referent. These constraints enable the speaker to construct a referring expression that she believes will allow the hearer to identify the referent. As for the hearer, the explicit encoding of the adequacy of referring expressions allows referent </context>
</contexts>
<marker>Perrault, Cohen, 1981</marker>
<rawString>Perrault, C. Raymond, and Cohen, Philip R. (1981). &amp;quot;It&apos;s for your own good: A note on inaccurate reference.&amp;quot; In Elements of Discourse Understanding, edited by Aravind K. joshi, Bonnie Lynn Webber, and Ivan Sag, 217-230. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha E Pollack</author>
</authors>
<title>Plans as complex mental attitudes.&amp;quot;</title>
<date>1990</date>
<booktitle>In Intentions in Communication, SDF Benchmark Series, edited by</booktitle>
<pages>77--103</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16263" citStr="Pollack 1990" startWordPosition="2585" endWordPosition="2586">in which surface speech actions correspond to the components of the description allows us to capture how participants collaborate in building a referring expression. Plan repair techniques can be used to refashion an expression if it is not adequate, and clarifications can refer to the part of the plan derivation that is in question or is being repaired. Thus we can model a collaborative dialog in terms of the changes that are being made to the plan derivation. The referring expression plans that we propose are not simply data structures, but are mental objects that agents have beliefs about (Pollack 1990). The plan derivation expresses beliefs of the speaker: how actions contribute to the achievement of the goal, and what constraints hold that will allow successful identification.&apos; So plan construction reasons about the beliefs of the agent in constructing a referring plan; likewise, plan inference, after hypothesizing a plan that is consistent with the observed actions, reasons about the other participant&apos;s (believed) beliefs in satisfying the constraints of the plan. If the hearer is able to satisfy the constraints, then he will have understood the plan and be able to identify the referent, </context>
<context position="22048" citStr="Pollack 1990" startWordPosition="3492" endWordPosition="3493">eferent that Agt2 associates with the discourse entity Ent (Webber 1983), which Agt1 believes to be Obj. (Proving this proposition with Ent unbound will cause a unique identifier to be created for Ent.) Goals and Plans goal(Agt,Goal): Agt has the goal Goal. Agents act to make their goals true. plan(Agt,Plan,Goal): Agt has the goal of Goal and has adopted the plan derivation Plan as a means to achieve it. The agent believes that each action of Plan contributes to the goal, but not necessarily that all of the constraints hold; in other words, the plan must be coherent but not necessarily valid (Pollack 1990, p. 94). content(Plan,Node,Content): The node named by Node in Plan has content Content. yield(Plan,Node,Actions): The subplan rooted at Node in Plan has a yield of the primitive actions Actions. achieve(Plan,Goal): Executing Plan will cause Goal to be true. error(Plan,Node): Plan has an error at the action labeled Node. Errors are attributed to the action that contains the failed constraint. This predicate is used to encode an agent&apos;s belief about an invalidity in a plan. Plan Repair substitute(Plan,Node,NewAction,NewPlan): Undo all variable bindings in Plan (except those in primitive action</context>
<context position="29721" citStr="Pollack (1990)" startWordPosition="4598" endWordPosition="4599"> is bel(Hearer,goal(Speaker,Goal)). Plan Construction. Given an effect, the plan constructor finds a plan derivation that has a minimal number of primitive actions, that is valid (with respect to the planning agent&apos;s beliefs), and whose root action achieves the effect. The plan constructor uses a 359 Computational Linguistics Volume 21, Number 3 best-first search strategy, expanding the derivation with the fewest number of surface speech actions. The yield of this plan derivation can then be given as input to a module that generates the surface form of the utterance. Plan Inference. Following Pollack (1990), our plan inference process can infer plans in which, in the hearer&apos;s view, a constraint does not hold. In inferring a plan derivation, we first find the set of plan derivations that account for the primitive actions that were observed, without regard to whether the constraints hold. This is done by using a chart parser that parses actions rather than words (Sidner 1994; Vilain 1990). For referring plans that contain more than one modifier, there will be multiple derivations corresponding to the order of the modifiers. We avoid this ambiguity by choosing an arbitrary ordering of the modifiers</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>Pollack, Martha E. (1990). &amp;quot;Plans as complex mental attitudes.&amp;quot; In Intentions in Communication, SDF Benchmark Series, edited by Philip R. Cohen, Jerry Morgan, and Martha E. Pollack, 77-103. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>The computational complexity of avoiding conversational implicature.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>97--104</pages>
<contexts>
<context position="13530" citStr="Reiter (1990)" startWordPosition="2152" endWordPosition="2153">o plan-based models, determining the content of referring expressions hasn&apos;t been. For instance, in Appelt&apos;s model, concept activations can be achieved by the action describe, which is a primitive, not further decomposed. Rather, this action has an associated procedure that determines a description that satisfies the preconditions of describe. Such special procedures have been the mainstay for accounting for the content of referring expressions, both in constructing and in understanding them, as exemplified by Dale (1989), who chose descriptors on the basis of their discriminatory power; Ehud Reiter (1990), who focused 1 For simplicity, we have not shown the change in speakers between refashionings and judgments. 354 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions on avoiding misleading conversational implicatures when generating descriptions; and Mellish (1985), who used a constraint satisfaction algorithm to identify referents. Our work follows the plan-based approach to language generation and understanding. We extend the earlier approaches of Cohen and Appelt by accounting for the content of the description at the planning level. This is done by having surface speech</context>
</contexts>
<marker>Reiter, 1990</marker>
<rawString>Reiter, Ehud (1990). &amp;quot;The computational complexity of avoiding conversational implicature.&amp;quot; In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics, 97-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Searle</author>
</authors>
<title>Speech Acts: An Essay in the Philosophy of Language.</title>
<date>1969</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="12326" citStr="Searle 1969" startWordPosition="1970" endWordPosition="1971">ctivity is updated. So, what constitutes grounds for accepting a judgment or clarification? In order to be consistent with Clark and Wilkes-Gibbs&apos;s model, we can see that if one agent finds the current referring expression problematic, the other must accept that judgment. Likewise, if one agent proposes a referring expression, through a refashioning, the other must accept the refashioning. 3. Referring Expressions 3.1 Planning and Referring By viewing language as action, the planning paradigm can be applied to natural language processing. The actions in this case are speech acts (Austin 1962; Searle 1969), and include such things as promising, informing, and requesting. Cohen and Perrault (1979) developed a system that uses plan construction to map an agent&apos;s goals to speech acts, and Allen and Perrault (1980) use plan inference to understand an agent&apos;s plan from its speech acts. By viewing it as action (Searle 1969), referring can be incorporated into a planning model. Cohen&apos;s model (1981) planned requests that the hearer identify a referent, whereas Appelt (1985) planned concept activations, a generalization of referring actions. Although acts of reference have been incorporated into plan-ba</context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>Searle, J. R. (1969). Speech Acts: An Essay in the Philosophy of Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Searle</author>
</authors>
<title>Collective intentions and actions.&amp;quot;</title>
<date>1990</date>
<booktitle>In Intentions in Communication, SDF Benchmark Series, edited by</booktitle>
<pages>401--415</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="52966" citStr="Searle (1990)" startWordPosition="8205" endWordPosition="8206">out the adequacy of the plan for the task 13 Another approach would be to have the plan inference process reason about the intended effects of the plan that it is inferring in order to decide whether it should evaluate embedded plans and whether this evaluation should affect the evaluation of the parent plan. 366 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions they are collaborating upon. If the plan is not adequate, then they must work together to refashion it. This level of cooperation is due to what Clark and Wilkes-Gibbs refer to as a mutual responsibility, or what Searle (1990) refers to as a we-intention. This allows the agents to interact so that neither assumes control of the dialog, thus allowing both to contribute to the best of their ability without being controlled or impeded by the other. This is different from what Grosz and Sidner (1990) have called master—servant dialogs, which occur in teacher—apprentice or information-seeking dialogs, in which one of the participants is controlling the conversation (cf. Walker and Whittaker 1990). Note that the noncontrolling agent may be helpful by anticipating obstacles in the plan (Allen and Perrault 1980), but this </context>
<context position="84647" citStr="Searle 1990" startWordPosition="13066" endWordPosition="13067">ration Grosz, Sidner, and Lochbaum (Grosz and Sidner, 1990; Lochbaum, Grosz, and Sidner, 1990) are interested in the type of plans that underlie discourse in which the agents are collaborating in order to achieve some goal. They propose that agents are building a shared plan in which participants have a collection of beliefs and intentions about the actions in the plan. Our model differs from theirs in two important aspects. First, not only do agents have a collection of beliefs and intentions regarding the actions of a shared plan, but we feel that they also have an intention about the goal (Searle 1990; Cohen and Levesque 1991). It is this intention, in conjunction with the current plan, that sanctions the adoption of beliefs and intentions about potential actions that will contribute to the goal, rather than just the shared plan. Second, we feel that their definition of a partial shared plan is too restrictive. Although they address partial plans, they require, in order for an action to be part of a partial shared plan, that both agents believe that the action contributes to the goal. However, this is too strong. In collaborating to achieve a mutual goal, participants sometimes propose an </context>
</contexts>
<marker>Searle, 1990</marker>
<rawString>Searle, John R. (1990). &amp;quot;Collective intentions and actions.&amp;quot; In Intentions in Communication, SDF Benchmark Series, edited by Philip R. Cohen, Jerry Morgan, and Martha E. Pollack, 401-415. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace L Sidner</author>
</authors>
<title>Plan parsing for intended response recognition in discourse.&amp;quot;</title>
<date>1985</date>
<journal>Computational Intelligence,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>1--10</pages>
<marker>Sidner, 1985</marker>
<rawString>Sidner, Candace L. (1985). &amp;quot;Plan parsing for intended response recognition in discourse.&amp;quot; Computational Intelligence, 1(1), 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace L Sidner</author>
</authors>
<title>An Artificial Discourse Language for Collaborative Negotiation.&amp;quot;</title>
<date>1994</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI&apos;94).</booktitle>
<pages>814--819</pages>
<contexts>
<context position="30094" citStr="Sidner 1994" startWordPosition="4661" endWordPosition="4662">expanding the derivation with the fewest number of surface speech actions. The yield of this plan derivation can then be given as input to a module that generates the surface form of the utterance. Plan Inference. Following Pollack (1990), our plan inference process can infer plans in which, in the hearer&apos;s view, a constraint does not hold. In inferring a plan derivation, we first find the set of plan derivations that account for the primitive actions that were observed, without regard to whether the constraints hold. This is done by using a chart parser that parses actions rather than words (Sidner 1994; Vilain 1990). For referring plans that contain more than one modifier, there will be multiple derivations corresponding to the order of the modifiers. We avoid this ambiguity by choosing an arbitrary ordering of the modifiers for each such plan. In the second part of the plan inference process, we evaluate each derivation by attempting to find an instantiation for the variables such that all of the constraints hold with respect to the hearer&apos;s beliefs about the speaker&apos;s beliefs. It could, however, be the case that there is no instantiation, either because this is not the right derivation or</context>
<context position="86732" citStr="Sidner (1994)" startWordPosition="13404" endWordPosition="13405"> request repair, acknowledge, and request acknowledge. Once an utterance has been acknowledged, it will reside in mutual belief as a proposal of the person who initiated it. The proposal state is a subspace of the mutual belief space of the conversants. Only once it has been accepted will it be moved into the shared space (also in mutual belief). Unlike Traum&apos;s, our work does not differentiate the proposal state from the shared state. If a proposal is understood, it is incorporated into the current plan. Judgments of acceptability are not on proposals but on the current plan, or a part of it. Sidner (1994) addressed the issue of how conversational participants collaborate in building a shared plan. In this work, Sidner presents a number of speech actions for use in collaborative tasks. These actions are those that an artificial agent could use in negotiating which actions or beliefs to accept into the shared plan of the agents. As 378 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions with Traum, it is the proposals that are refashioned, before they are integrated into the shared plan, rather than the shared plan. Cohen and Levesque (1991) focus on formalizing joint intenti</context>
</contexts>
<marker>Sidner, 1994</marker>
<rawString>Sidner, Candace L. (1994). &amp;quot;An Artificial Discourse Language for Collaborative Negotiation.&amp;quot; In Proceedings of the National Conference on Artificial Intelligence (AAAI&apos;94). 814-819.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Svartvik</author>
<author>R Quirk</author>
</authors>
<title>A Corpus of English Conversation.</title>
<date>1980</date>
<journal>Lund Studies in English, 56. C. W. K. Gleerup.</journal>
<contexts>
<context position="65951" citStr="Svartvik and Quirk 1980" startWordPosition="10261" endWordPosition="10264">s whether there is a goal that it can try to achieve, and if so, constructs a plan to achieve it. Next, presupposing its partner&apos;s acceptance of the plan, it applies any rules that it can. It repeats this until there are no more goals. The actions of the constructed plans form the response of the system; in a complete natural language system, they would be converted to a surface utterance. The system then switches to the role of hearer. 6. An Example We are now ready to illustrate our system in action.18 For this example, we use a simplified version of a subdialog from the London-Lund corpus (Svartvik and Quirk 1980, S.2.4a:1-8): (6.1) A: I See the weird creature. B: 2 In the corner? A:3 No, on the television. B: 4 Okay. The system will take the role of person B and we will give it the belief that there are two objects that are &amp;quot;weird&amp;quot;—a television antenna, which is on the television, and a fern plant, which is in the corner. 6.1 Understanding &amp;quot;The Weird Creature&amp;quot; For the first sentence, the system is given as input the surface speech actions underlying &amp;quot;the weird creature,&amp;quot; as shown below: s-refer(entityl) s-attrib(entityl,AX• assessment(X,weird)) s-attrib(entityl, AX. category(X,creature)) The system i</context>
<context position="90937" citStr="Svartvik and Quirk 1980" startWordPosition="14063" endWordPosition="14066">entifying the referent. Second, the treatment of clarifications could be improved; specifically, how plan failures are reasoned about, how plan failures affect the agent&apos;s beliefs, and how these failures are repaired. Third, this research needs to be integrated into a more complete plan-based approach to language, and needs to be extended so as to handle more general discourse plan failures (McRoy and Hirst 1993; McRoy and Hirst 1995; Horton and Hirst 1991; Heeman 1993; Edmonds 1994; Hirst et al. 1994). A benchmark for such future work could be dialog (8.1) below, from the London-Lund corpus (Svartvik and Quirk 1980, S.2.4a:1-8, which is the basis of the example used in Section 6. This dialog shows how collaboration on a referring expression can be embedded in other 379 Computational Linguistics Volume 21, Number 3 activities, how agents can return back to a collaborative activity, and even how agents can take advantage of a mistaken referent. (8.1) A: 1 What&apos;s that weird creature over there? B: 2 In the corner? A: 3 affirmative noise B: 4 Ws just a fern plant. A: 5 No, the one to the left of it. B: 6 That&apos;s the television aerial. It pulls out. A second avenue for future work is to further investigate co</context>
</contexts>
<marker>Svartvik, Quirk, 1980</marker>
<rawString>Svartvik, J., and Quirk, R. (1980). A Corpus of English Conversation. Lund Studies in English, 56. C. W. K. Gleerup.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
</authors>
<title>Towards a computational theory of grounding in natural language conversation.&amp;quot;</title>
<date>1991</date>
<tech>Technical Report 401,</tech>
<institution>Department of Computer Science, University of Rochester,</institution>
<location>Rochester, New York.</location>
<contexts>
<context position="85717" citStr="Traum (1991" startWordPosition="13240" endWordPosition="13241">he action contributes to the goal. However, this is too strong. In collaborating to achieve a mutual goal, participants sometimes propose an action that is not believed by the other participant or even by the participant that is proposing it. In failing to represent such states, their model is unable to represent the intermediate states in which a hearer might have understood how the speaker&apos;s utterance contributes to a plan, but doesn&apos;t agree with it. This is important, since if the refashioned plan is invalid, only the referring expression should be refashioned, not the refashioning itself. Traum (1991; Traum and Hinkelman, 1992) is concerned with providing a computational model of grounding, the process in which conversational participants add to the common ground of a conversation (Clark and Schaefer 1989; Clark and Brennan 1990). Traum models the grounding process by proposing that utterances move through a number of states, &apos;pushed&apos; by grounding acts, which include initiate, continue, repair, request repair, acknowledge, and request acknowledge. Once an utterance has been acknowledged, it will reside in mutual belief as a proposal of the person who initiated it. The proposal state is a </context>
</contexts>
<marker>Traum, 1991</marker>
<rawString>Traum, David R. (1991). &amp;quot;Towards a computational theory of grounding in natural language conversation.&amp;quot; Technical Report 401, Department of Computer Science, University of Rochester, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Elizabeth A Hinkelman</author>
</authors>
<title>Conversation acts in task-oriented spoken dialogue.&amp;quot; Special issue on non-literal language,</title>
<date>1992</date>
<journal>Computational Intelligence,</journal>
<volume>8</volume>
<issue>3</issue>
<pages>575--599</pages>
<contexts>
<context position="85745" citStr="Traum and Hinkelman, 1992" startWordPosition="13242" endWordPosition="13245">tributes to the goal. However, this is too strong. In collaborating to achieve a mutual goal, participants sometimes propose an action that is not believed by the other participant or even by the participant that is proposing it. In failing to represent such states, their model is unable to represent the intermediate states in which a hearer might have understood how the speaker&apos;s utterance contributes to a plan, but doesn&apos;t agree with it. This is important, since if the refashioned plan is invalid, only the referring expression should be refashioned, not the refashioning itself. Traum (1991; Traum and Hinkelman, 1992) is concerned with providing a computational model of grounding, the process in which conversational participants add to the common ground of a conversation (Clark and Schaefer 1989; Clark and Brennan 1990). Traum models the grounding process by proposing that utterances move through a number of states, &apos;pushed&apos; by grounding acts, which include initiate, continue, repair, request repair, acknowledge, and request acknowledge. Once an utterance has been acknowledged, it will reside in mutual belief as a proposal of the person who initiated it. The proposal state is a subspace of the mutual belie</context>
<context position="89743" citStr="Traum and Hinkelman 1992" startWordPosition="13873" endWordPosition="13876">ho is trying to identify it. Both have the same moves available to them, for either can judge the description and either can refashion it. This allows both participants to contribute without being controlled or impeded by the other. Second, their model gives special status to the role of the current referring expression (current plan): participants judge and refashion the current referring expression directly, rather than recursively modifying modifications (e.g. Litman and Allen 1987; Chu-Carroll and Carberry 1994) or incrementally adding to the current plan with each accepted proposal (e.g. Traum and Hinkelman 1992; Sidner 1992). In our work, we have taken Clark and Wilkes-Gibbs&apos;s descriptive model and recast it into a computational one, thus demonstrating the computational feasibility of their work and its compatibility with current practices in artificial intelligence. There are many ways that this research could be extended. Perhaps the most obvious would be to extend the planning component of our model. First, our coverage of referring expressions could be extended to handle references to objects in focus and to descriptions that include a plan of physical actions for identifying the referent. Secon</context>
</contexts>
<marker>Traum, Hinkelman, 1992</marker>
<rawString>Traum, David R., and Hinkelman, Elizabeth A. (1992). &amp;quot;Conversation acts in task-oriented spoken dialogue.&amp;quot; Special issue on non-literal language, Computational Intelligence, 8(3), 575-599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
</authors>
<title>Getting serious about parsing plans: A grammatical analysis of plan recognition.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, National Conference on Artificial Intelligence (AAAI &apos;90),</booktitle>
<pages>190--197</pages>
<contexts>
<context position="30108" citStr="Vilain 1990" startWordPosition="4663" endWordPosition="4664"> derivation with the fewest number of surface speech actions. The yield of this plan derivation can then be given as input to a module that generates the surface form of the utterance. Plan Inference. Following Pollack (1990), our plan inference process can infer plans in which, in the hearer&apos;s view, a constraint does not hold. In inferring a plan derivation, we first find the set of plan derivations that account for the primitive actions that were observed, without regard to whether the constraints hold. This is done by using a chart parser that parses actions rather than words (Sidner 1994; Vilain 1990). For referring plans that contain more than one modifier, there will be multiple derivations corresponding to the order of the modifiers. We avoid this ambiguity by choosing an arbitrary ordering of the modifiers for each such plan. In the second part of the plan inference process, we evaluate each derivation by attempting to find an instantiation for the variables such that all of the constraints hold with respect to the hearer&apos;s beliefs about the speaker&apos;s beliefs. It could, however, be the case that there is no instantiation, either because this is not the right derivation or because the p</context>
</contexts>
<marker>Vilain, 1990</marker>
<rawString>Vilain, Marc (1990). &amp;quot;Getting serious about parsing plans: A grammatical analysis of plan recognition.&amp;quot; In Proceedings, National Conference on Artificial Intelligence (AAAI &apos;90), 190-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Steve Whittaker</author>
</authors>
<title>Mixed initiative in dialogue: An investigation into discourse segmentation.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>70--78</pages>
<contexts>
<context position="53440" citStr="Walker and Whittaker 1990" startWordPosition="8276" endWordPosition="8279">ork together to refashion it. This level of cooperation is due to what Clark and Wilkes-Gibbs refer to as a mutual responsibility, or what Searle (1990) refers to as a we-intention. This allows the agents to interact so that neither assumes control of the dialog, thus allowing both to contribute to the best of their ability without being controlled or impeded by the other. This is different from what Grosz and Sidner (1990) have called master—servant dialogs, which occur in teacher—apprentice or information-seeking dialogs, in which one of the participants is controlling the conversation (cf. Walker and Whittaker 1990). Note that the noncontrolling agent may be helpful by anticipating obstacles in the plan (Allen and Perrault 1980), but this is not the same as collaborating. The mutual responsibility that the agents share not only concerns the goal they are trying to achieve, but also the plan that they are currently considering. This plan serves to coordinate their activity and so agents will have intentions to keep this plan in their common ground. The plan might not be valid (unlike the shared plan of Grosz and Sidner [19901), so the agents might not mutually believe that each action contributes to the g</context>
</contexts>
<marker>Walker, Whittaker, 1990</marker>
<rawString>Walker, Marilyn, and Whittaker, Steve (1990). &amp;quot;Mixed initiative in dialogue: An investigation into discourse segmentation.&amp;quot; In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics, 70-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Lynn Webber</author>
</authors>
<title>So what can we talk about now?&amp;quot; In Computational Models of Discourse, edited by Michael Brady and</title>
<date>1983</date>
<pages>331--371</pages>
<publisher>MIT Press.</publisher>
<note>Reprinted in Readings in Natural Language Processing, edited by</note>
<contexts>
<context position="21508" citStr="Webber 1983" startWordPosition="3399" endWordPosition="3400">ure 1 refer schema. Header: Constraint: Decomposition: Effect: refer(Entity,Object) knowref(Hearer,Speaker,Entity,Object) s-refer(Entity) describe(Entity,Object) bel(Hearer,goal(Speaker,knowref(Hearer,Speaker,Entity,Object))) 356 Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions Table 1 Predicates and actions. Belief bel(Agt,Prop): Agt believes that Prop is true. bmb(Agt1,Agt2,Prop): Agt1 believes that it is mutually believed between himself and Agt2 that Prop is true. knowref(Agt1,Agt2,Ent,Obj): Agt1 knows the referent that Agt2 associates with the discourse entity Ent (Webber 1983), which Agt1 believes to be Obj. (Proving this proposition with Ent unbound will cause a unique identifier to be created for Ent.) Goals and Plans goal(Agt,Goal): Agt has the goal Goal. Agents act to make their goals true. plan(Agt,Plan,Goal): Agt has the goal of Goal and has adopted the plan derivation Plan as a means to achieve it. The agent believes that each action of Plan contributes to the goal, but not necessarily that all of the constraints hold; in other words, the plan must be coherent but not necessarily valid (Pollack 1990, p. 94). content(Plan,Node,Content): The node named by Node</context>
</contexts>
<marker>Webber, 1983</marker>
<rawString>Webber, Bonnie Lynn (1983). &amp;quot;So what can we talk about now?&amp;quot; In Computational Models of Discourse, edited by Michael Brady and Robert C. Berwick, 331-371. MIT Press. Reprinted in Readings in Natural Language Processing, edited by Barbara J. Grosz, Karen Sparck Jones, and Bonnie Lynn Webber, 395-414. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Wilensky</author>
</authors>
<title>A model for planning in complex situations.&amp;quot;</title>
<date>1981</date>
<journal>Cognition and Brain Theory,</journal>
<volume>4</volume>
<pages>263--374</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<note>Reprinted in Readings in Planning, edited by</note>
<contexts>
<context position="42333" citStr="Wilensky 1981" startWordPosition="6617" endWordPosition="6618">he referring expression overconstrains the choice of referent. He uses heuristics to relax the constraints of the description and to pick one that nearly fits it. This problem is beyond the scope of this paper, and so we choose one of the referents arbitrarily (but see Heeman [19911 for how a simplified version of Goodman&apos;s algorithm that relaxes only a single constraint can be incorporated into the planning paradigm). The second step is to refashion the referring expression so that it identifies the candidate chosen in the first step. This is done by using plan repair techniques (Hayes 1975; Wilensky 1981; Wilkens 1985). Our technique is to remove the subplan rooted at the action in error and replan with another action schema inserted in its place. This technique has been encoded into our refashioning plans, and so can be used for both constructing repairs and inferring how another agent has repaired a plan. Now we consider the effect of these refashioning plans. As we mentioned in Section 2, once the refashioning plan is accepted, the common ground of the participants is updated with the new referring expression. So, the effect of the refashioning plans is that the hearer will believe that th</context>
</contexts>
<marker>Wilensky, 1981</marker>
<rawString>Wilensky, Robert (1981). &amp;quot;A model for planning in complex situations.&amp;quot; Cognition and Brain Theory, 4. Reprinted in Readings in Planning, edited by James Allen, James Hendler, and Austin Tate, 263-374. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Wilkens</author>
</authors>
<title>Recovering from execution errors in SIPE.&amp;quot;</title>
<date>1985</date>
<journal>Computational Intelligence,</journal>
<pages>1--33</pages>
<contexts>
<context position="42348" citStr="Wilkens 1985" startWordPosition="6619" endWordPosition="6620">pression overconstrains the choice of referent. He uses heuristics to relax the constraints of the description and to pick one that nearly fits it. This problem is beyond the scope of this paper, and so we choose one of the referents arbitrarily (but see Heeman [19911 for how a simplified version of Goodman&apos;s algorithm that relaxes only a single constraint can be incorporated into the planning paradigm). The second step is to refashion the referring expression so that it identifies the candidate chosen in the first step. This is done by using plan repair techniques (Hayes 1975; Wilensky 1981; Wilkens 1985). Our technique is to remove the subplan rooted at the action in error and replan with another action schema inserted in its place. This technique has been encoded into our refashioning plans, and so can be used for both constructing repairs and inferring how another agent has repaired a plan. Now we consider the effect of these refashioning plans. As we mentioned in Section 2, once the refashioning plan is accepted, the common ground of the participants is updated with the new referring expression. So, the effect of the refashioning plans is that the hearer will believe that the speaker wants</context>
</contexts>
<marker>Wilkens, 1985</marker>
<rawString>Wilkens, David E. (1985). &amp;quot;Recovering from execution errors in SIPE.&amp;quot; Computational Intelligence, 1,33-45.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>