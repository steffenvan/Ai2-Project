<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000876">
<title confidence="0.947055">
Summarizing Spoken and Written Conversations
</title>
<author confidence="0.969501">
Gabriel Murray and Giuseppe Carenini
</author>
<affiliation confidence="0.9992105">
Department of Computer Science
University of British Columbia
</affiliation>
<address confidence="0.496649">
Vancouver, BC V6T 1Z4 Canada
</address>
<sectionHeader confidence="0.990514" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994259692307692">
In this paper we describe research on sum-
marizing conversations in the meetings and
emails domains. We introduce a conver-
sation summarization system that works in
multiple domains utilizing general conversa-
tional features, and compare our results with
domain-dependent systems for meeting and
email data. We find that by treating meet-
ings and emails as conversations with general
conversational features in common, we can
achieve competitive results with state-of-the-
art systems that rely on more domain-specific
features.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985954545455">
Our lives are increasingly comprised of multimodal
conversations with others. We email for business
and personal purposes, attend meetings in person
and remotely, chat online, and participate in blog or
forum discussions. It is clear that automatic summa-
rization can be of benefit in dealing with this over-
whelming amount of interactional information. Au-
tomatic meeting abstracts would allow us to prepare
for an upcoming meeting or review the decisions of a
previous group. Email summaries would aid corpo-
rate memory and provide efficient indices into large
mail folders.
When summarizing in each of these domains,
there will be potentially useful domain-specific fea-
tures – e.g. prosodic features for meeting speech,
subject headers for emails – but there are also un-
derlying similarites between these domains. They
are all multiparty conversations, and we hypothe-
size that effective summarization techniques can be
designed that would lead to robust summarization
performance on a wide array of such conversation
types. Such a general conversation summarization
system would make it possible to summarize a wide
variety of conversational data without needing to
develop unique summarizers in each domain and
across modalities. While progress has been made in
summarizing conversations in individual domains,
as described below, little or no work has been done
on summarizing unrestricted, multimodal conversa-
tions.
In this research we take an extractive approach
to summarization, presenting a novel set of conver-
sational features for locating the most salient sen-
tences in meeting speech and emails. We demon-
strate that using these conversational features in a
machine-learning sentence classification framework
yields performance that is competitive or superior
to more restricted domain-specific systems, while
having the advantage of being portable across con-
versational modalities. The robust performance of
the conversation-based system is attested via several
summarization evaluation techniques, and we give
an in-depth analysis of the effectiveness of the indi-
vidual features and feature subclasses used.
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="related work">
2 Related Work on Meetings and Emails
</sectionHeader>
<bodyText confidence="0.984766666666667">
In this section we give a brief overview of previous
research on meeting summarization and email sum-
marization, respectively.
</bodyText>
<page confidence="0.982146">
773
</page>
<note confidence="0.9726495">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 773–782,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.972526">
2.1 Meeting Summarization
</subsectionHeader>
<bodyText confidence="0.999970878787879">
Among early work on meeting summarization,
Waibel et al. (1998) implemented a modified version
of the Maximal Marginal Relevance algorithm (Car-
bonell and Goldstein, 1998) applied to speech tran-
scripts, presenting the user with the n best sentences
in a meeting browser interface. Zechner (2002) in-
vestigated summarizing several genres of speech, in-
cluding spontaneous meeting speech. Though rele-
vance detection in his work relied largely on tf.idf
scores, Zechner also explored cross-speaker infor-
mation linking and question/answer detection.
More recently, researchers have investigated
the utility of employing speech-specific features
for summarization, including prosodic information.
Murray et al. (2005a; 2005b) compared purely
textual summarization approaches with feature-
based approaches incorporating prosodic features,
with human judges favoring the feature-based ap-
proaches. In subsequent work (2006; 2007), they
began to look at additional speech-specific char-
acteristics such as speaker status, discourse mark-
ers and high-level meta comments in meetings, i.e.
comments that refer to the meeting itself. Galley
(2006) used skip-chain Conditional Random Fields
to model pragmatic dependencies between paired
meeting utterances (e.g. QUESTION-ANSWER re-
lations), and used a combination of lexical, prosodic,
structural and discourse features to rank utterances
by importance. Galley found that while the most
useful single feature class was lexical features, a
combination of acoustic, durational and structural
features exhibited comparable performance accord-
ing to Pyramid evaluation.
</bodyText>
<subsectionHeader confidence="0.989841">
2.2 Email Summarization
</subsectionHeader>
<bodyText confidence="0.999966064516129">
Work on email summarization can be divided into
summarization of individual email messages and
summarization of entire email threads. Muresan et
al. (2001) took the approach of summarizing indi-
vidual email messages, first using linguistic tech-
niques to extract noun phrases and then employ-
ing machine learning methods to label the extracted
noun phrases as salient or not. Corston-Oliver et al.
(2004) focused on identifying speech acts within a
given email, with a particular interest in task-related
sentences.
Rambow et al. (2004) addressed the challenge of
summarizing entire threads by treating it as a binary
sentence classification task. They considered three
types of features: basic features that simply treat the
email as text (e.g. tf.idf, which scores words highly if
they are frequent in the document but rare across all
documents), features that consider the thread to be a
sequence of turns (e.g. the position of the turn in the
thread), and email-specific features such as number
of recipients and subject line similarity.
Carenini et al. (2007) took an approach to thread
summarization using the Enron corpus (described
below) wherein the thread is represented as a
fragment quotation graph. A single node in the
graph represents an email fragment, a portion of
the email that behaves as a unit in a fine-grain
representation of the conversation structure. A
fragment sometimes consists of an entire email and
sometimes a portion of an email. For example, if a
given email has the structure
</bodyText>
<equation confidence="0.751038666666667">
A
&gt; B
C
</equation>
<bodyText confidence="0.999683652173913">
where B is a quoted section in the middle of
the email, then there are three email fragments in
total: two new fragments A and C separated by
one quoted fragment B. Sentences in a fragment
are weighted according to the Clue Word Score
(CWS) measure, a lexical cohesion metric based
on the recurrence of words in parent and child
nodes. In subsequent work, Carenini et al. (2008)
determined that subjectivity detection (i.e., whether
the sentence contains sentiments or opinions from
the author) gave additional improvement for email
thread summaries.
Also on the Enron corpus, Zajic et al. (2008) com-
pared Collective Message Summarization (CMS)
to Individual Message Summarization (IMS) and
found the former to be a more effective technique
for summarizing email data. CMS essentially treats
thread summarization as a multi-document summa-
rization problem, while IMS summarizes individual
emails in the thread and then concatenates them to
form a thread summary.
In our work described below we also address the
task of thread summarization as opposed to sum-
</bodyText>
<page confidence="0.996764">
774
</page>
<bodyText confidence="0.976158">
marization of individual email messages, following
Carenini et al. and the CMS approach of Zajic et al.
</bodyText>
<sectionHeader confidence="0.996789" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99993725">
In this section we describe the classifier employed
for our machine learning experiments, the corpora
used, the relevant summarization annotations for
each corpus, and the evaluation methods employed.
</bodyText>
<subsectionHeader confidence="0.999834">
3.1 Statistical Classifier
</subsectionHeader>
<bodyText confidence="0.999675071428571">
Our approach to extractive summarization views
sentence extraction as a classification problem. For
all machine learning experiments, we utilize logistic
regression classifiers. This choice was partly moti-
vated by our earlier summarization research, where
logistic regression classifiers were compared along-
side support vector machines (SVMs) (Cortes and
Vapnik, 1995). The two classifier types yielded very
similar results, with logistic regression classifiers
being much faster to train and thus expediting fur-
ther development.
The liblinear toolkit 1 implements simple feature
subset selection based on the F statistic (Chen and
Lin, 2006) .
</bodyText>
<subsectionHeader confidence="0.999269">
3.2 Corpora Description
</subsectionHeader>
<bodyText confidence="0.999339333333333">
For these experiments we utilize two corpora, the
Enron corpus for email summarization and the AMI
corpus for meeting summarization.
</bodyText>
<subsectionHeader confidence="0.663186">
3.2.1 The Enron Email Corpus
</subsectionHeader>
<bodyText confidence="0.971013">
The Enron email corpus2 is a collection of emails
released as part of the investigation into the Enron
corporation (Klimt and Yang, 2004). It has become
a popular corpus for NLP research (e.g. (Bekkerman
et al., 2004; Yeh and Harnly, 2006; Chapanond et al.,
2005; Diesner et al., 2005)) due to being realistic,
naturally-occurring data from a corporate environ-
ment, and moreover because privacy concerns mean
that there is very low availability for other publicly
available email data.
39 threads have been annotated for extractive
summarization, with five annotators assigned to
each thread. The annotators were asked to select
30% of the sentences in a thread, subsequently la-
beling each selected sentence as either ’essential’ or
</bodyText>
<footnote confidence="0.9998845">
1http://www.csie.ntu.edu.tw/˜cjlin/liblinear/
2http://www.cs.cmu.edu/˜enron/
</footnote>
<bodyText confidence="0.999866736842105">
’optional.’ Essential sentences are weighted three
times as highly as optional sentences. A sentence
score, or GSValue, can therefore range between 0
and 15, with the maximum GSValue achieved when
all five annotators consider the sentence essential,
and a score of 0 achieved when no annotator selects
the given sentence. For the purpose of training a bi-
nary classifier, we rank the sentences in each email
thread according to their GSValues, then extract sen-
tences until our summary reaches 30% of the to-
tal thread word count. We label these sentences as
positive instances and the remainder as the negative
class. Approximately 19% of sentences are labeled
as positive, extractive examples.
Because the amount of labeled data available for
the Enron email corpus is fairly small, for our classi-
fication experiments we employ a leave-one-out pro-
ceedure for the 39 email threads. The labeled data as
a whole total just under 1400 sentences.
</bodyText>
<subsectionHeader confidence="0.597529">
3.2.2 The AMI Meetings Corpus
</subsectionHeader>
<bodyText confidence="0.999976925925926">
For our meeting summarization experiments, we
use the scenario portion of the AMI corpus (Carletta
et al., 2005). The corpus consists of about 100 hours
of recorded and annotated meetings. In the scenario
meetings, groups of four participants take part in a
series of four meetings and play roles within a ficti-
tious company. While the scenario given to them is
artificial, the speech and the actions are completely
spontaneous and natural. There are 96 meetings in
the training set, 24 in the development set, and 20
meetings for the test set.
For this corpus, annotators wrote abstract sum-
maries of each meeting and extracted transcript dia-
logue act segments (DAs) that best conveyed or sup-
ported the information in the abstracts. A many-
to-many mapping between transcript DAs and sen-
tences from the human abstract was obtained for
each annotator, with three annotators assigned to
each meeting. It is possible for a DA to be extracted
by an annotator but not linked to the abstract, but for
training our binary classifiers, we simply consider a
dialogue act to be a positive example if it is linked
to a given human summary, and a negative example
otherwise. This is done to maximize the likelihood
that a data point labeled as “extractive” is truly an
informative example for training purposes. Approx-
imately 13% of the total DAs are ultimately labeled
</bodyText>
<page confidence="0.990646">
775
</page>
<bodyText confidence="0.999884714285714">
as positive, extractive examples.
The AMI corpus contains automatic speech
recognition (ASR) output in addition to manual
meeting transcripts, and we report results on both
transcript types. The ASR output was provided by
the AMI-ASR team (Hain et al., 2007), and the word
error rate for the AMI corpus is 38.9%.
</bodyText>
<subsectionHeader confidence="0.999663">
3.3 Summarization Evaluation
</subsectionHeader>
<bodyText confidence="0.999989045454545">
For evaluating our extractive summaries, we imple-
ment existing evaluation schemes from previous re-
search, with somewhat similar methods for meet-
ings versus emails. These are described and com-
pared below. We also evaluate our extractive classi-
fiers more generally by plotting the receiver operator
characteristic (ROC) curve and calculating the area
under the curve (AUROC). This allows us to gauge
the true-positive/false-positive ratio as the posterior
threshold is varied.
We use the differing evaluation metrics for emails
versus meetings for two primary reasons. First,
the differing summarization annotations in the AMI
and Enron corpora naturally lend themselves to
slightly divergent metrics, one based on extract-
abstract links and the other based on the essen-
tial/option/uninformative distinction. Second, and
more importantly, using these two metrics allow us
to compare our results with state-of-the-art results
in the two fields of speech summarization and email
summarization. In future work we plan to use a sin-
gle evaluation metric.
</bodyText>
<subsectionHeader confidence="0.969258">
3.3.1 Evaluating Meeting Summaries
</subsectionHeader>
<bodyText confidence="0.999797666666667">
To evaluate meeting summaries we use the
weighted f-measure metric (Murray et al., 2006).
This evaluation scheme relies on the multiple human
annotated summary links described in Section 3.2.2.
Both weighted precision and recall share the same
numerator
</bodyText>
<equation confidence="0.98542">
L(si,aj) (1)
</equation>
<bodyText confidence="0.946451714285714">
where L(si, aj) is the number of links for a DA
si in the machine extractive summary according to
annotator ai, M is the number of DAs in the ma-
chine summary, and N is the number of annotators.
Weighted precision is defined as:
and weighted recall is given by
num
</bodyText>
<equation confidence="0.986085">
recall = (3)
E� 1 1 L(si, aj )
</equation>
<bodyText confidence="0.999940888888889">
where O is the total number of DAs in the meeting,
N is the number of annotators, and the denominator
represents the total number of links made between
DAs and abstract sentences by all annotators. The
weighted f-measure is calculated as the harmonic
mean of weighted precision and recall. The intuition
behind weighted f-score is that DAs that are linked
multiple times by multiple annotators are the most
informative.
</bodyText>
<subsectionHeader confidence="0.968992">
3.3.2 Evaluating Email Summaries
</subsectionHeader>
<bodyText confidence="0.999995086956522">
For evaluating email thread summaries, we follow
Carenini et al. (2008) by implementing their pyra-
mid precision scheme, inspired by Nenkova’s pyra-
mid scheme (2004). In Section 3.2.1 we introduced
the idea of a GSValue for each sentence in an email
thread, based on multiple human annotations. We
can evaluate a summary of a given length by com-
paring its total GSValues to the maximum possible
total for that summary length. For instance, if in a
thread the three top scoring sentences had GSValues
of 15, 12 and 12, and the sentences selected by a
given automatic summarization method had GSVal-
ues of 15, 10 and 8, the pyramid precision would be
0.85.
Pyramid precision and weighted f-score are simi-
lar evaluation schemes in that they are both sentence
based (as opposed to, for example, n-gram based)
and that they score sentences based on multiple hu-
man annotations. Pyramid precision is very simi-
lar to equation 3 normalized by the maximum score
for the summary length. For now we use these two
slightly different schemes in order to maintain con-
sistency with prior art in each domain.
</bodyText>
<sectionHeader confidence="0.99823" genericHeader="method">
4 A Conversation Summarization System
</sectionHeader>
<bodyText confidence="0.99674825">
In our conversation summarization approach, we
treat emails and meetings as conversations com-
prised of turns between multiple participants. We
follow Carenini et al. (2007) in working at the finer
</bodyText>
<equation confidence="0.990176888888889">
M
i=1
num =
N
j=1
num
precision =
(2)
N · M
</equation>
<page confidence="0.981813">
776
</page>
<bodyText confidence="0.999974160714286">
granularity of email fragments, so that for an email
thread, a turn consists of a single email fragment in
the exchange. For meetings, a turn is a sequence of
dialogue acts by one speaker, with the turn bound-
aries delimited by dialogue acts from other meet-
ing participants. The features we derive for summa-
rization are based on this view of the conversational
structure.
We calculate two length features. For each sen-
tence, we derive a word-count feature normalized
by the longest sentence in the conversation (SLEN)
and a word-count feature normalized by the longest
sentence in the turn (SLEN2). Sentence length has
previously been found to be an effective feature in
speech and text summarization (e.g. (Maskey and
Hirschberg, 2005; Murray et al., 2005a; Galley,
2006)).
There are several structural features used, in-
cluding position of the sentence in the turn (TLOC)
and position of the sentence in the conversation
(CLOC). We also include the time from the begin-
ning of the conversation to the current turn (TPOS])
and from the current turn to the end of the conversa-
tion (TPOS2). Conversations in both modalities can
be well-structured, with introductory turns, general
discussion, and ultimate resolution or closure, and
sentence informativeness might significantly corre-
late with this structure. We calculate two pause-style
features: the time between the following turn and the
current turn (SPAU), and the time between the cur-
rent turn and previous turn (PPAU), both normalized
by the overall length of the conversation. These fea-
tures are based on the email and meeting transcript
timestamps. We hypothesize that pause features may
be useful if informative turns tend to elicit a large
number of responses in a short period of time, or if
they tend to quickly follow a preceding turn, to give
two examples.
There are two features related to the conversation
participants directly. One measures how dominant
the current participant is in terms of words in the
conversation (DOM), and the other is a binary fea-
ture indicating whether the current participant ini-
tiated the conversation (BEGAUTH), based simply
on whether they were the first contributor. It is hy-
pothesized that informative sentences may more of-
ten belong to participants who lead the conversation
or have a good deal of dominance in the discussion.
There are several lexical features used in these
experiments. For each unique word, we calculate
two conditional probabilities. For each conversation
participant, we calculate the probability of the par-
ticipant given the word, estimating the probability
from the actual term counts, and take the maximum
of these conditional probabilities as our first term
score, which we will call Sprob.
</bodyText>
<equation confidence="0.8165525">
Sprob(t) = maxp(S|t)
S
</equation>
<bodyText confidence="0.999952916666667">
where t is the word and S is a participant. For ex-
ample, if the word budget is used ten times in total,
with seven uses by participant A, three uses by par-
ticipant B and no uses by the other participants, then
the Sprob score for this term is 0.70. The intuition
is that certain words will tend to be associated with
one conversation participant more than the others,
owing to varying interests and expertise between the
people involved.
Using the same procedure, we calculate a score
called Tprob based on the probability of each turn
given the word.
</bodyText>
<equation confidence="0.9809705">
Tprob(t) = max
T
</equation>
<bodyText confidence="0.999992045454545">
The motivating factor for this metric is that certain
words will tend to cluster into a small number of
turns, owing to shifting topics within a conversation.
Having derived Sprob and Tprob, we then calcu-
late several sentence-level features based on these
term scores. Each sentence has features related to
max, mean and sum of the term scores for the
words in that sentence (MYS, MNS and SMS for
Sprob, and MYT, MNT and SMT for Tprob). Us-
ing a vector representation, we calculate the cosine
between the conversation preceding the given sen-
tence and the conversation subsequent to the sen-
tence, first using Sprob as the vector weights (COS])
and then using Tprob as the vector weights (COS2).
This is motivated by the hypothesis that informative
sentences might change the conversation in some
fashion, leading to a low cosine between the preced-
ing and subsequent portions. We similarly calculate
two scores measuring the cosine between the cur-
rent sentence and the rest of the converation, using
each term-weight metric as vector weights (CENT]
for Sprob and CENT2 for Tprob). This measures
</bodyText>
<equation confidence="0.93644">
p(T |t)
</equation>
<page confidence="0.992292">
777
</page>
<table confidence="0.881939785714286">
Feature ID Description
MXS max Sprob score
MNS mean Sprob score
SMS sum of Sprob scores
MXT max Tprob score
MNT mean Tprob score
SMT sum of Tprob scores
TLOC position in turn
CLOC position in conv.
SLEN word count, globally normalized
SLEN2 word count, locally normalized
TPOS1 time from beg. of conv. to turn
TPOS2 time from turn to end of conv.
DOM participant dominance in words
</table>
<tableCaption confidence="0.96767825">
COS1 cos. of conv. splits, w/ Sprob
COS2 cos. of conv. splits, w/ Tprob
PENT entro. of conv. up to sentence
SENT entro. of conv. after the sentence
THISENT entropy of current sentence
PPAU time btwn. current and prior turn
SPAU time btwn. current and next turn
BEGAUTH is first participant (0/1)
CWS rough ClueWordScore
CENT1 cos. of sentence &amp; conv., w/ Sprob
CENT2 cos. of sentence &amp; conv., w/ Tprob
Table 1: Features Key
</tableCaption>
<bodyText confidence="0.99632575">
whether the candidate sentence is generally similar
to the conversation overall.
There are three word entropy features, calculated
using the formula
</bodyText>
<equation confidence="0.999871">
went(s) —_ ENi p(xi)
· −log(p(xi))
(N·−log( N1 )) · M
</equation>
<bodyText confidence="0.999976461538462">
where s is a string of words, xi is a word type
in that string, p(xi) is the probability of the word
based on its normalized frequency in the string, N
is the number of word types in the string, and M is
the number of word tokens in the string.
Note that word entropy essentially captures infor-
mation about type-token ratios. For example, if each
word token in the string was a unique type then the
word entropy score would be 1. We calculate the
word entropy of the current sentence (THISENT),
as well as the word entropy for the conversation up
until the current sentence (PENT) and the word en-
tropy for the conversation subsequent to the current
sentence (SENT). We hypothesize that informative
sentences themselves may have a diversity of word
types, and that if they represent turning points in the
conversation they may affect the entropy of the sub-
sequent conversation.
Finally, we include a feature that is a rough ap-
proximation of the ClueWordScore (CWS) used by
Carenini et al. (2007). For each sentence we remove
stopwords and count the number of words that occur
in other turns besides the current turn. The CWS is
therefore a measure of conversation cohesion.
For ease of reference, we hereafter refer to this
conversation features system as ConverSumm.
</bodyText>
<sectionHeader confidence="0.988682" genericHeader="method">
5 Comparison Summarization Systems
</sectionHeader>
<bodyText confidence="0.999995666666667">
In order to compare the ConverSumm system with
state-of-the-art systems for meeting and email sum-
marization, respectively, we also present results us-
ing the features described by Murray and Renals
(2008) for meetings and the features described by
Rambow (2004) for email. Because the work by
Murray and Renals used the same dataset, we can
compare our scores directly. However, Rambow car-
ried out summarization work on a different, unavail-
able email corpus, and so we re-implemented their
summarization system for our current email data.
In their work on meeting summarization, Murray
and Renals creating 700-word summaries of each
meeting using several classes of features: prosodic,
lexical, structural and speaker-related. While there
are two features overlapping between our systems
(word-count and speaker/participant dominance),
their system is primarily domain-dependent in its
use of prosodic features while our features represent
a more general conversational view.
Rambow presented 14 features for the summa-
rization task, including email-specific information
such as the number of recipients, number of re-
sponses, and subject line overlap. There is again a
slight overlap in features between our two systems,
as we both include length and position of the sen-
tence in the thread/conversation.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9999745">
Here we present, in turn, the summarization results
for meeting and email data.
</bodyText>
<subsectionHeader confidence="0.998142">
6.1 Meeting Summarization Results
</subsectionHeader>
<bodyText confidence="0.99966725">
Figure 1 shows the F statistics for each Conver-
summ feature in the meeting data, providing a mea-
sure of the usefulness of each feature in discriminat-
ing between the positive and negative classes. Some
</bodyText>
<page confidence="0.996285">
778
</page>
<figureCaption confidence="0.993777">
Figure 1: Feature F statistics for AMI meeting corpus
</figureCaption>
<table confidence="0.9483932">
System Weighted F-Score AUROC
Speech - Man 0.23 0.855
Speech - ASR 0.24 0.850
Conv. - Man 0.23 0.852
Conv. - ASR 0.22 0.853
</table>
<tableCaption confidence="0.81099">
Table 2: Weighted F-Scores and AUROCs for Meeting
Summaries
</tableCaption>
<bodyText confidence="0.99988364">
features such as participant dominance have very
low F statistics because each sentence by a given
participant will receive the same score; so while the
feature itself may have a low score because it does
not discriminate informative versus non-informative
sentences on its own, it may well be useful in con-
junction with the other features. The best individual
ConverSumm features for meeting summarization
are sentence length (SLEN), sum of 5prob scores,
sum of Tprob scores, the simplified CWS score
(CWS), and the two centroid measures (CENT1 and
CENT2). The word entropy of the candidate sen-
tence is very effective for manual transcripts but
much less effective on ASR output. This is due to
the fact that ASR errors can incorrectly lead to high
entropy scores.
Table 2 provides the weighted f-scores for all
summaries of the meeting data, as well as AUROC
scores for the classifiers themselves. For our 700-
word summaries, the Conversumm approach scores
comparably to the speech-specific approach on both
manual and ASR transcripts according to weighted
f-score. There are no significant differences accord-
ing to paired t-test. For the AUROC measures, there
are again no significant differences between the con-
</bodyText>
<figure confidence="0.995717142857143">
0 0.2 0.4 0.6 0.8 1
FP
Fea. Subset AUROC
Structural 0.652
Participant 0.535
Length 0.837
Lexical 0.852
</figure>
<figureCaption confidence="0.981957">
Figure 2: AUROC Values for Feature Subclasses, AMI
Corpus
</figureCaption>
<bodyText confidence="0.999931678571429">
versation summarizers and speech-specific summa-
rizers. The AUROC for the conversation system
is slightly lower on manual transcripts and slightly
higher when applied to ASR output.
For all systems the weighted f-scores are some-
what low. This is partly owing to the fact that out-
put summaries are very short, leading to high pre-
cision and low recall. The low f-scores are also in-
dicative of the difficulty of the task. Human perfor-
mance, gauged by comparing each annotator’s sum-
maries to the remaining annotators’ summaries, ex-
hibits an average weighted f-score of 0.47 on the
same test set. The average kappa value on the test set
is 0.48, showing the relatively low inter-annotator
agreement that is typical of summarization annota-
tion. There is no additional benefit to combining the
conversational and speech-specific features. In that
case, the weighted f-scores are 0.23 for both manual
and ASR transcripts. The overall AUROC is 0.85
for manual transcripts and 0.86 for ASR.
We can expand the features analysis by consid-
ering the effectiveness of certain subclasses of fea-
tures. Specifically, we group the summarization fea-
tures into lexical, structural, participant and length
features. Figure 2 shows the AUROCs for the fea-
ture subset classifiers, illustrating that the lexical
subclass is very effective while the length features
also constitute a challenging baseline. A weakness
</bodyText>
<figure confidence="0.995404857142857">
f statistic
0.25
0.15
0.05
0.2
0.1
0
feature ID (see key)
manual
ASR
1
0.8
0.6
0.4
0.2
0
lexical features
structural features
participant features
length features
TP
</figure>
<page confidence="0.985239">
779
</page>
<table confidence="0.919667666666667">
System Pyramid Precision AUROC
Rambow 0.50 0.64
Conv. 0.46 0.75
</table>
<tableCaption confidence="0.9852795">
Table 3: Pyramid Precision and AUROCs for Email Sum-
maries
</tableCaption>
<bodyText confidence="0.999828142857143">
of systems that depend heavily on length features,
however, is that recall scores tend to decrease be-
cause the extracted units are much longer - weighted
recall scores for the 700 word summaries are sig-
nificantly worse according to paired t-test (p&lt;0.05)
when using just length features compared to the full
feature set.
</bodyText>
<subsectionHeader confidence="0.981967">
6.2 Email Summarization Results
</subsectionHeader>
<bodyText confidence="0.999966857142857">
Figure 3 shows the F statistic for each ConverSumm
feature in the email data.The two most useful fea-
tures are sentence length and CWS. The Sprob and
Tprob features rate very well according to the F
statistic. The two centroid features incorporating
Sprob and Tprob are comparable to one another and
are very effective features as well.
</bodyText>
<figureCaption confidence="0.989563">
Figure 3: Feature F statistics for Enron email corpus
</figureCaption>
<bodyText confidence="0.9997207">
After creating 30% word compression summaries
using both the ConverSumm and Rambow ap-
proaches, we score the 39 thread summaries using
Pyramid Precision. The results are given in Table 3.
On average, the Rambow system is slightly higher
with a score of 0.50 compared with 0.46 for the con-
versational system, but there is no statistical differ-
ence according to paired t-test.
The average AUROC for the Rambow system is
0.64 compared with 0.75 for the ConverSumm sys-
</bodyText>
<figure confidence="0.8356852">
Fea. Subset AUROC
Structural 0.63
Participant 0.51
Length 0.71
Lexical 0.71
</figure>
<figureCaption confidence="0.983525">
Figure 4: AUROC Values for Feature Subclasses, Enron
Corpus
</figureCaption>
<bodyText confidence="0.952946181818182">
tem, with ConverSumm system significantly better
according to paired t-test (p&lt;0.05). Random classi-
fication performance would yield an AUROC of 0.5.
Combining the Rambow and ConverSumm fea-
tures does not yield any overall improvement. The
Pyramid Precision score in that case is 0.47 while
the AUROC is 0.74.
Figure 4 illustrates that the lexical and length
features are the most effective feature subclasses,
though the best results overall are derived from a
combination of all feature classes.
</bodyText>
<sectionHeader confidence="0.999743" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999964866666667">
According to multiple evaluations, the ConverSumm
features yield competitive summarization perfor-
mance with the comparison systems. There is a clear
set of features that are similarly effective in both do-
mains, especially CWS, the centroid features, the
Sprob features, the Tprob features, and sentence
length. There are other features that are more ef-
fective in one domain than the other. For exam-
ple, the BEGAUTH feature, indicating whether the
current participant began the conversation, is more
useful for emails. It seems that being the first per-
son to speak in a meeting is not as significant as
being the first person to email in a given thread.
SLEN2, which normalizes sentence length by the
longest sentence in the turn, also is much more ef-
</bodyText>
<figure confidence="0.99443356">
f statistic
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
feature ID (see key)
TP
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
FP
lexical features
structural features
participant features
length features
</figure>
<page confidence="0.983579">
780
</page>
<bodyText confidence="0.999762666666667">
fective for emails. The reason is that many meet-
ing turns consist of a single, brief utterance such as
“Okay, yeah.”
The finding that the summary evaluations are
not significantly worse on noisy ASR compared
with manual transcripts has been previously attested
(Valenza et al., 1999; Murray et al., 2005a), and it is
encouraging that our ConverSumm features are sim-
ilarly robust to this noisy data.
</bodyText>
<sectionHeader confidence="0.998507" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999833">
We have shown that a general conversation summa-
rization approach can achieve results on par with
state-of-the-art systems that rely on features specific
to more focused domains. We have introduced a
conversation feature set that is similarly effective in
both the meetings and emails domains. The use of
multiple summarization evaluation techniques con-
firms that the system is robust, even when applied
to the noisy ASR output in the meetings domain.
Such a general conversation summarization system
is valuable in that it may save time and effort re-
quired to implement unique systems in a variety of
conversational domains.
We are currently working on extending our sys-
tem to other conversation domains such as chats,
blogs and telephone speech. We are also investigat-
ing domain adaptation techniques; for example, we
hypothesize that the relatively well-resourced do-
main of meetings can be leveraged to improve email
results, and preliminary findings are encouraging.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998955">
R. Bekkerman, A. McCallum, and G. Huang. 2004. Au-
tomatic categorization of email into folders: Bench-
mark experiments on Enron and SRI corpora. Tech-
nical Report IR-418, Center of Intelligent Information
Retrieval, UMass Amherst.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. of ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval 1998, Melbourne, Australia, pages 335–
336.
G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing
email conversations with clue words. In Proc. ofACM
WWW 07, Banff, Canada.
G. Carenini, X. Zhou, and R. Ng. 2008. Summarizing
emails with conversational cohesion and subjectivity.
In Proc. ofACL 2008, Columbus, Ohio, USA.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Well-
ner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28–39.
A. Chapanond, M. Krishnamoorthy, and B. Yener. 2005.
Graph theoretic and spectral analysis of enron email
data. Comput. Math. Organ. Theory, 11(3):265–281.
Y-W. Chen and C-J. Lin. 2006. Combining SVMs
with various feature selection strategies. In I. Guyon,
S. Gunn, M. Nikravesh, and L. Zadeh, editors, Feature
extraction, foundations and applications. Springer.
S. Corston-Oliver, E. Ringger, M. Gamon, and R. Camp-
bell. 2004. Integration of email and task lists. In Proc.
of CEAS 2004, Mountain View, CA, USA.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine Learning, 20(3):273–297.
J. Diesner, T. Frantz, and K. Carley. 2005. Communi-
cation networks from the enron email corpus ”it’s al-
ways about the people. enron is no different”. Comput.
Math. Organ. Theory, 11(3):201–228.
M. Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proc. of EMNLP 2006, Sydney, Australia, pages 364–
372.
T. Hain, L. Burget, J. Dines, G. Garau, V. Wan,
M. Karafiat, J. Vepa, and M. Lincoln. 2007. The
AMI system for transcription of speech in meetings.
In Proc. ofICASSP 2007,, pages 357–360.
B. Klimt and Y. Yang. 2004. Introducing the enron cor-
pus. In Proc. of CEAS 2004, Mountain View, CA, USA.
S. Maskey and J. Hirschberg. 2005. Comparing lexial,
acoustic/prosodic, discourse and structural features for
speech summarization. In Proc. of Interspeech 2005,
Lisbon, Portugal, pages 621–624.
S. Muresan, E. Tzoukermann, and J. Klavans. 2001.
Combining linguistic and machine learning techniques
for email summarization. In Proc. of ConLL 2001,
Toulouse, France.
G. Murray and S. Renals. 2008. Meta comments for
summarizing meeting speech. In Proc. ofMLMI2008,
Utrecht, Netherlands.
G. Murray, S. Renals, and J. Carletta. 2005a. Extrac-
tive summarization of meeting recordings. In Proc. of
Interspeech 2005, Lisbon, Portugal, pages 593–596.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005b.
Evaluating automatic summaries of meeting record-
ings. In Proc. of the ACL 2005 MTSE Workshop, Ann
Arbor, MI, USA, pages 33–40.
</reference>
<page confidence="0.970854">
781
</page>
<reference confidence="0.999803636363636">
G. Murray, S. Renals, J. Moore, and J. Carletta. 2006. In-
corporating speaker and discourse features into speech
summarization. In Proc. of the HLT-NAACL 2006,
New York City, USA, pages 367–374.
G. Murray. 2007. Using Speech-Specific Features for
Automatic Speech Summarization. Ph.D. thesis, Uni-
versity of Edinburgh.
A. Nenkova and B. Passonneau. 2004. Evaluating con-
tent selection in summarization: The Pyramid method.
In Proc. of HLT-NAACL 2004, Boston, MA, USA,
pages 145–152.
O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004. Summarizing email threads. In Proc. of HLT-
NAACL 2004, Boston, USA.
R. Valenza, T. Robinson, M. Hickey, and R. Tucker.
1999. Summarization of spoken audio through infor-
mation extraction. In Proc. of the ESCA Workshop on
Accessing Information in Spoken Audio, Cambridge
UK, pages 111–116.
A. Waibel, M. Bett, M. Finke, and R. Stiefelhagen. 1998.
Meeting browser: Tracking and summarizing meet-
ings. In D. E. M. Penrose, editor, Proc. of the Broad-
cast News Transcription and Understanding Work-
shop, Lansdowne, VA, USA, pages 281–286.
J. Yeh and A. Harnly. 2006. Email thread reassembly
using similarity matching. In Proc of CEAS 2006.
D. Zajic, B. Dorr, and J. Lin. 2008. Single-document and
multi-document summarization techniques for email
threads using sentence compression. Information Pro-
cessing and Management, to appear.
K. Zechner. 2002. Automatic summarization of open-
domain multiparty dialogues in diverse genres. Com-
putational Linguistics, 28(4):447–485.
</reference>
<page confidence="0.997342">
782
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.771147">
<title confidence="0.999234">Summarizing Spoken and Written Conversations</title>
<author confidence="0.925572">Murray</author>
<affiliation confidence="0.999957">Department of Computer University of British</affiliation>
<address confidence="0.984435">Vancouver, BC V6T 1Z4 Canada</address>
<abstract confidence="0.988865071428571">In this paper we describe research on summarizing conversations in the meetings and emails domains. We introduce a conversation summarization system that works in multiple domains utilizing general conversational features, and compare our results with domain-dependent systems for meeting and email data. We find that by treating meetings and emails as conversations with general conversational features in common, we can achieve competitive results with state-of-theart systems that rely on more domain-specific features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bekkerman</author>
<author>A McCallum</author>
<author>G Huang</author>
</authors>
<title>Automatic categorization of email into folders: Benchmark experiments on Enron and SRI corpora.</title>
<date>2004</date>
<tech>Technical Report IR-418,</tech>
<institution>Center of Intelligent Information Retrieval, UMass Amherst.</institution>
<contexts>
<context position="8802" citStr="Bekkerman et al., 2004" startWordPosition="1307" endWordPosition="1310">with logistic regression classifiers being much faster to train and thus expediting further development. The liblinear toolkit 1 implements simple feature subset selection based on the F statistic (Chen and Lin, 2006) . 3.2 Corpora Description For these experiments we utilize two corpora, the Enron corpus for email summarization and the AMI corpus for meeting summarization. 3.2.1 The Enron Email Corpus The Enron email corpus2 is a collection of emails released as part of the investigation into the Enron corporation (Klimt and Yang, 2004). It has become a popular corpus for NLP research (e.g. (Bekkerman et al., 2004; Yeh and Harnly, 2006; Chapanond et al., 2005; Diesner et al., 2005)) due to being realistic, naturally-occurring data from a corporate environment, and moreover because privacy concerns mean that there is very low availability for other publicly available email data. 39 threads have been annotated for extractive summarization, with five annotators assigned to each thread. The annotators were asked to select 30% of the sentences in a thread, subsequently labeling each selected sentence as either ’essential’ or 1http://www.csie.ntu.edu.tw/˜cjlin/liblinear/ 2http://www.cs.cmu.edu/˜enron/ ’optio</context>
</contexts>
<marker>Bekkerman, McCallum, Huang, 2004</marker>
<rawString>R. Bekkerman, A. McCallum, and G. Huang. 2004. Automatic categorization of email into folders: Benchmark experiments on Enron and SRI corpora. Technical Report IR-418, Center of Intelligent Information Retrieval, UMass Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proc. of ACM SIGIR Conference on Research and Development in Information Retrieval</booktitle>
<pages>335--336</pages>
<location>Melbourne, Australia,</location>
<contexts>
<context position="3376" citStr="Carbonell and Goldstein, 1998" startWordPosition="487" endWordPosition="491">epth analysis of the effectiveness of the individual features and feature subclasses used. 2 Related Work on Meetings and Emails In this section we give a brief overview of previous research on meeting summarization and email summarization, respectively. 773 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 773–782, Honolulu, October 2008.c�2008 Association for Computational Linguistics 2.1 Meeting Summarization Among early work on meeting summarization, Waibel et al. (1998) implemented a modified version of the Maximal Marginal Relevance algorithm (Carbonell and Goldstein, 1998) applied to speech transcripts, presenting the user with the n best sentences in a meeting browser interface. Zechner (2002) investigated summarizing several genres of speech, including spontaneous meeting speech. Though relevance detection in his work relied largely on tf.idf scores, Zechner also explored cross-speaker information linking and question/answer detection. More recently, researchers have investigated the utility of employing speech-specific features for summarization, including prosodic information. Murray et al. (2005a; 2005b) compared purely textual summarization approaches wit</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. of ACM SIGIR Conference on Research and Development in Information Retrieval 1998, Melbourne, Australia, pages 335– 336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carenini</author>
<author>R Ng</author>
<author>X Zhou</author>
</authors>
<title>Summarizing email conversations with clue words.</title>
<date>2007</date>
<booktitle>In Proc. ofACM WWW 07,</booktitle>
<location>Banff, Canada.</location>
<contexts>
<context position="5887" citStr="Carenini et al. (2007)" startWordPosition="852" endWordPosition="855">thin a given email, with a particular interest in task-related sentences. Rambow et al. (2004) addressed the challenge of summarizing entire threads by treating it as a binary sentence classification task. They considered three types of features: basic features that simply treat the email as text (e.g. tf.idf, which scores words highly if they are frequent in the document but rare across all documents), features that consider the thread to be a sequence of turns (e.g. the position of the turn in the thread), and email-specific features such as number of recipients and subject line similarity. Carenini et al. (2007) took an approach to thread summarization using the Enron corpus (described below) wherein the thread is represented as a fragment quotation graph. A single node in the graph represents an email fragment, a portion of the email that behaves as a unit in a fine-grain representation of the conversation structure. A fragment sometimes consists of an entire email and sometimes a portion of an email. For example, if a given email has the structure A &gt; B C where B is a quoted section in the middle of the email, then there are three email fragments in total: two new fragments A and C separated by one</context>
<context position="15465" citStr="Carenini et al. (2007)" startWordPosition="2380" endWordPosition="2383">ghted f-score are similar evaluation schemes in that they are both sentence based (as opposed to, for example, n-gram based) and that they score sentences based on multiple human annotations. Pyramid precision is very similar to equation 3 normalized by the maximum score for the summary length. For now we use these two slightly different schemes in order to maintain consistency with prior art in each domain. 4 A Conversation Summarization System In our conversation summarization approach, we treat emails and meetings as conversations comprised of turns between multiple participants. We follow Carenini et al. (2007) in working at the finer M i=1 num = N j=1 num precision = (2) N · M 776 granularity of email fragments, so that for an email thread, a turn consists of a single email fragment in the exchange. For meetings, a turn is a sequence of dialogue acts by one speaker, with the turn boundaries delimited by dialogue acts from other meeting participants. The features we derive for summarization are based on this view of the conversational structure. We calculate two length features. For each sentence, we derive a word-count feature normalized by the longest sentence in the conversation (SLEN) and a word</context>
<context position="21952" citStr="Carenini et al. (2007)" startWordPosition="3480" endWordPosition="3483">was a unique type then the word entropy score would be 1. We calculate the word entropy of the current sentence (THISENT), as well as the word entropy for the conversation up until the current sentence (PENT) and the word entropy for the conversation subsequent to the current sentence (SENT). We hypothesize that informative sentences themselves may have a diversity of word types, and that if they represent turning points in the conversation they may affect the entropy of the subsequent conversation. Finally, we include a feature that is a rough approximation of the ClueWordScore (CWS) used by Carenini et al. (2007). For each sentence we remove stopwords and count the number of words that occur in other turns besides the current turn. The CWS is therefore a measure of conversation cohesion. For ease of reference, we hereafter refer to this conversation features system as ConverSumm. 5 Comparison Summarization Systems In order to compare the ConverSumm system with state-of-the-art systems for meeting and email summarization, respectively, we also present results using the features described by Murray and Renals (2008) for meetings and the features described by Rambow (2004) for email. Because the work by </context>
</contexts>
<marker>Carenini, Ng, Zhou, 2007</marker>
<rawString>G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing email conversations with clue words. In Proc. ofACM WWW 07, Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carenini</author>
<author>X Zhou</author>
<author>R Ng</author>
</authors>
<title>Summarizing emails with conversational cohesion and subjectivity.</title>
<date>2008</date>
<booktitle>In Proc. ofACL 2008,</booktitle>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="6720" citStr="Carenini et al. (2008)" startWordPosition="998" endWordPosition="1001">ion of the email that behaves as a unit in a fine-grain representation of the conversation structure. A fragment sometimes consists of an entire email and sometimes a portion of an email. For example, if a given email has the structure A &gt; B C where B is a quoted section in the middle of the email, then there are three email fragments in total: two new fragments A and C separated by one quoted fragment B. Sentences in a fragment are weighted according to the Clue Word Score (CWS) measure, a lexical cohesion metric based on the recurrence of words in parent and child nodes. In subsequent work, Carenini et al. (2008) determined that subjectivity detection (i.e., whether the sentence contains sentiments or opinions from the author) gave additional improvement for email thread summaries. Also on the Enron corpus, Zajic et al. (2008) compared Collective Message Summarization (CMS) to Individual Message Summarization (IMS) and found the former to be a more effective technique for summarizing email data. CMS essentially treats thread summarization as a multi-document summarization problem, while IMS summarizes individual emails in the thread and then concatenates them to form a thread summary. In our work desc</context>
<context position="14237" citStr="Carenini et al. (2008)" startWordPosition="2174" endWordPosition="2177">ghted precision is defined as: and weighted recall is given by num recall = (3) E� 1 1 L(si, aj ) where O is the total number of DAs in the meeting, N is the number of annotators, and the denominator represents the total number of links made between DAs and abstract sentences by all annotators. The weighted f-measure is calculated as the harmonic mean of weighted precision and recall. The intuition behind weighted f-score is that DAs that are linked multiple times by multiple annotators are the most informative. 3.3.2 Evaluating Email Summaries For evaluating email thread summaries, we follow Carenini et al. (2008) by implementing their pyramid precision scheme, inspired by Nenkova’s pyramid scheme (2004). In Section 3.2.1 we introduced the idea of a GSValue for each sentence in an email thread, based on multiple human annotations. We can evaluate a summary of a given length by comparing its total GSValues to the maximum possible total for that summary length. For instance, if in a thread the three top scoring sentences had GSValues of 15, 12 and 12, and the sentences selected by a given automatic summarization method had GSValues of 15, 10 and 8, the pyramid precision would be 0.85. Pyramid precision a</context>
</contexts>
<marker>Carenini, Zhou, Ng, 2008</marker>
<rawString>G. Carenini, X. Zhou, and R. Ng. 2008. Summarizing emails with conversational cohesion and subjectivity. In Proc. ofACL 2008, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>S Ashby</author>
<author>S Bourban</author>
<author>M Flynn</author>
<author>M Guillemot</author>
<author>T Hain</author>
<author>J Kadlec</author>
<author>V Karaiskos</author>
<author>W Kraaij</author>
<author>M Kronenthal</author>
<author>G Lathoud</author>
<author>M Lincoln</author>
<author>A Lisowska</author>
<author>I McCowan</author>
<author>W Post</author>
<author>D Reidsma</author>
<author>P Wellner</author>
</authors>
<title>The AMI meeting corpus: A preannouncement.</title>
<date>2005</date>
<booktitle>In Proc. of MLMI 2005,</booktitle>
<pages>28--39</pages>
<location>Edinburgh, UK,</location>
<contexts>
<context position="10479" citStr="Carletta et al., 2005" startWordPosition="1564" endWordPosition="1567">nces until our summary reaches 30% of the total thread word count. We label these sentences as positive instances and the remainder as the negative class. Approximately 19% of sentences are labeled as positive, extractive examples. Because the amount of labeled data available for the Enron email corpus is fairly small, for our classification experiments we employ a leave-one-out proceedure for the 39 email threads. The labeled data as a whole total just under 1400 sentences. 3.2.2 The AMI Meetings Corpus For our meeting summarization experiments, we use the scenario portion of the AMI corpus (Carletta et al., 2005). The corpus consists of about 100 hours of recorded and annotated meetings. In the scenario meetings, groups of four participants take part in a series of four meetings and play roles within a fictitious company. While the scenario given to them is artificial, the speech and the actions are completely spontaneous and natural. There are 96 meetings in the training set, 24 in the development set, and 20 meetings for the test set. For this corpus, annotators wrote abstract summaries of each meeting and extracted transcript dialogue act segments (DAs) that best conveyed or supported the informati</context>
</contexts>
<marker>Carletta, Ashby, Bourban, Flynn, Guillemot, Hain, Kadlec, Karaiskos, Kraaij, Kronenthal, Lathoud, Lincoln, Lisowska, McCowan, Post, Reidsma, Wellner, 2005</marker>
<rawString>J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner. 2005. The AMI meeting corpus: A preannouncement. In Proc. of MLMI 2005, Edinburgh, UK, pages 28–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Chapanond</author>
<author>M Krishnamoorthy</author>
<author>B Yener</author>
</authors>
<title>Graph theoretic and spectral analysis of enron email data.</title>
<date>2005</date>
<journal>Comput. Math. Organ. Theory,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="8848" citStr="Chapanond et al., 2005" startWordPosition="1315" endWordPosition="1318">h faster to train and thus expediting further development. The liblinear toolkit 1 implements simple feature subset selection based on the F statistic (Chen and Lin, 2006) . 3.2 Corpora Description For these experiments we utilize two corpora, the Enron corpus for email summarization and the AMI corpus for meeting summarization. 3.2.1 The Enron Email Corpus The Enron email corpus2 is a collection of emails released as part of the investigation into the Enron corporation (Klimt and Yang, 2004). It has become a popular corpus for NLP research (e.g. (Bekkerman et al., 2004; Yeh and Harnly, 2006; Chapanond et al., 2005; Diesner et al., 2005)) due to being realistic, naturally-occurring data from a corporate environment, and moreover because privacy concerns mean that there is very low availability for other publicly available email data. 39 threads have been annotated for extractive summarization, with five annotators assigned to each thread. The annotators were asked to select 30% of the sentences in a thread, subsequently labeling each selected sentence as either ’essential’ or 1http://www.csie.ntu.edu.tw/˜cjlin/liblinear/ 2http://www.cs.cmu.edu/˜enron/ ’optional.’ Essential sentences are weighted three t</context>
</contexts>
<marker>Chapanond, Krishnamoorthy, Yener, 2005</marker>
<rawString>A. Chapanond, M. Krishnamoorthy, and B. Yener. 2005. Graph theoretic and spectral analysis of enron email data. Comput. Math. Organ. Theory, 11(3):265–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-W Chen</author>
<author>C-J Lin</author>
</authors>
<title>Combining SVMs with various feature selection strategies. In</title>
<date>2006</date>
<editor>I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="8397" citStr="Chen and Lin, 2006" startWordPosition="1241" endWordPosition="1244">ive summarization views sentence extraction as a classification problem. For all machine learning experiments, we utilize logistic regression classifiers. This choice was partly motivated by our earlier summarization research, where logistic regression classifiers were compared alongside support vector machines (SVMs) (Cortes and Vapnik, 1995). The two classifier types yielded very similar results, with logistic regression classifiers being much faster to train and thus expediting further development. The liblinear toolkit 1 implements simple feature subset selection based on the F statistic (Chen and Lin, 2006) . 3.2 Corpora Description For these experiments we utilize two corpora, the Enron corpus for email summarization and the AMI corpus for meeting summarization. 3.2.1 The Enron Email Corpus The Enron email corpus2 is a collection of emails released as part of the investigation into the Enron corporation (Klimt and Yang, 2004). It has become a popular corpus for NLP research (e.g. (Bekkerman et al., 2004; Yeh and Harnly, 2006; Chapanond et al., 2005; Diesner et al., 2005)) due to being realistic, naturally-occurring data from a corporate environment, and moreover because privacy concerns mean th</context>
</contexts>
<marker>Chen, Lin, 2006</marker>
<rawString>Y-W. Chen and C-J. Lin. 2006. Combining SVMs with various feature selection strategies. In I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh, editors, Feature extraction, foundations and applications. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>E Ringger</author>
<author>M Gamon</author>
<author>R Campbell</author>
</authors>
<title>Integration of email and task lists.</title>
<date>2004</date>
<booktitle>In Proc. of CEAS</booktitle>
<location>Mountain View, CA, USA.</location>
<contexts>
<context position="5227" citStr="Corston-Oliver et al. (2004)" startWordPosition="747" endWordPosition="750">ound that while the most useful single feature class was lexical features, a combination of acoustic, durational and structural features exhibited comparable performance according to Pyramid evaluation. 2.2 Email Summarization Work on email summarization can be divided into summarization of individual email messages and summarization of entire email threads. Muresan et al. (2001) took the approach of summarizing individual email messages, first using linguistic techniques to extract noun phrases and then employing machine learning methods to label the extracted noun phrases as salient or not. Corston-Oliver et al. (2004) focused on identifying speech acts within a given email, with a particular interest in task-related sentences. Rambow et al. (2004) addressed the challenge of summarizing entire threads by treating it as a binary sentence classification task. They considered three types of features: basic features that simply treat the email as text (e.g. tf.idf, which scores words highly if they are frequent in the document but rare across all documents), features that consider the thread to be a sequence of turns (e.g. the position of the turn in the thread), and email-specific features such as number of re</context>
</contexts>
<marker>Corston-Oliver, Ringger, Gamon, Campbell, 2004</marker>
<rawString>S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell. 2004. Integration of email and task lists. In Proc. of CEAS 2004, Mountain View, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="8123" citStr="Cortes and Vapnik, 1995" startWordPosition="1200" endWordPosition="1203">et al. 3 Experimental Setup In this section we describe the classifier employed for our machine learning experiments, the corpora used, the relevant summarization annotations for each corpus, and the evaluation methods employed. 3.1 Statistical Classifier Our approach to extractive summarization views sentence extraction as a classification problem. For all machine learning experiments, we utilize logistic regression classifiers. This choice was partly motivated by our earlier summarization research, where logistic regression classifiers were compared alongside support vector machines (SVMs) (Cortes and Vapnik, 1995). The two classifier types yielded very similar results, with logistic regression classifiers being much faster to train and thus expediting further development. The liblinear toolkit 1 implements simple feature subset selection based on the F statistic (Chen and Lin, 2006) . 3.2 Corpora Description For these experiments we utilize two corpora, the Enron corpus for email summarization and the AMI corpus for meeting summarization. 3.2.1 The Enron Email Corpus The Enron email corpus2 is a collection of emails released as part of the investigation into the Enron corporation (Klimt and Yang, 2004)</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support-vector networks. Machine Learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Diesner</author>
<author>T Frantz</author>
<author>K Carley</author>
</authors>
<title>Communication networks from the enron email corpus ”it’s always about the people. enron is no different”.</title>
<date>2005</date>
<journal>Comput. Math. Organ. Theory,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="8871" citStr="Diesner et al., 2005" startWordPosition="1319" endWordPosition="1322">us expediting further development. The liblinear toolkit 1 implements simple feature subset selection based on the F statistic (Chen and Lin, 2006) . 3.2 Corpora Description For these experiments we utilize two corpora, the Enron corpus for email summarization and the AMI corpus for meeting summarization. 3.2.1 The Enron Email Corpus The Enron email corpus2 is a collection of emails released as part of the investigation into the Enron corporation (Klimt and Yang, 2004). It has become a popular corpus for NLP research (e.g. (Bekkerman et al., 2004; Yeh and Harnly, 2006; Chapanond et al., 2005; Diesner et al., 2005)) due to being realistic, naturally-occurring data from a corporate environment, and moreover because privacy concerns mean that there is very low availability for other publicly available email data. 39 threads have been annotated for extractive summarization, with five annotators assigned to each thread. The annotators were asked to select 30% of the sentences in a thread, subsequently labeling each selected sentence as either ’essential’ or 1http://www.csie.ntu.edu.tw/˜cjlin/liblinear/ 2http://www.cs.cmu.edu/˜enron/ ’optional.’ Essential sentences are weighted three times as highly as optio</context>
</contexts>
<marker>Diesner, Frantz, Carley, 2005</marker>
<rawString>J. Diesner, T. Frantz, and K. Carley. 2005. Communication networks from the enron email corpus ”it’s always about the people. enron is no different”. Comput. Math. Organ. Theory, 11(3):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
</authors>
<title>A skip-chain conditional random field for ranking meeting utterances by importance.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP 2006,</booktitle>
<pages>364--372</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="4335" citStr="Galley (2006)" startWordPosition="622" endWordPosition="623">question/answer detection. More recently, researchers have investigated the utility of employing speech-specific features for summarization, including prosodic information. Murray et al. (2005a; 2005b) compared purely textual summarization approaches with featurebased approaches incorporating prosodic features, with human judges favoring the feature-based approaches. In subsequent work (2006; 2007), they began to look at additional speech-specific characteristics such as speaker status, discourse markers and high-level meta comments in meetings, i.e. comments that refer to the meeting itself. Galley (2006) used skip-chain Conditional Random Fields to model pragmatic dependencies between paired meeting utterances (e.g. QUESTION-ANSWER relations), and used a combination of lexical, prosodic, structural and discourse features to rank utterances by importance. Galley found that while the most useful single feature class was lexical features, a combination of acoustic, durational and structural features exhibited comparable performance according to Pyramid evaluation. 2.2 Email Summarization Work on email summarization can be divided into summarization of individual email messages and summarization </context>
<context position="16309" citStr="Galley, 2006" startWordPosition="2528" endWordPosition="2529"> dialogue acts by one speaker, with the turn boundaries delimited by dialogue acts from other meeting participants. The features we derive for summarization are based on this view of the conversational structure. We calculate two length features. For each sentence, we derive a word-count feature normalized by the longest sentence in the conversation (SLEN) and a word-count feature normalized by the longest sentence in the turn (SLEN2). Sentence length has previously been found to be an effective feature in speech and text summarization (e.g. (Maskey and Hirschberg, 2005; Murray et al., 2005a; Galley, 2006)). There are several structural features used, including position of the sentence in the turn (TLOC) and position of the sentence in the conversation (CLOC). We also include the time from the beginning of the conversation to the current turn (TPOS]) and from the current turn to the end of the conversation (TPOS2). Conversations in both modalities can be well-structured, with introductory turns, general discussion, and ultimate resolution or closure, and sentence informativeness might significantly correlate with this structure. We calculate two pause-style features: the time between the follow</context>
</contexts>
<marker>Galley, 2006</marker>
<rawString>M. Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In Proc. of EMNLP 2006, Sydney, Australia, pages 364– 372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hain</author>
<author>L Burget</author>
<author>J Dines</author>
<author>G Garau</author>
<author>V Wan</author>
<author>M Karafiat</author>
<author>J Vepa</author>
<author>M Lincoln</author>
</authors>
<title>The AMI system for transcription of speech in meetings.</title>
<date>2007</date>
<booktitle>In Proc. ofICASSP</booktitle>
<pages>357--360</pages>
<contexts>
<context position="11985" citStr="Hain et al., 2007" startWordPosition="1816" endWordPosition="1819">g our binary classifiers, we simply consider a dialogue act to be a positive example if it is linked to a given human summary, and a negative example otherwise. This is done to maximize the likelihood that a data point labeled as “extractive” is truly an informative example for training purposes. Approximately 13% of the total DAs are ultimately labeled 775 as positive, extractive examples. The AMI corpus contains automatic speech recognition (ASR) output in addition to manual meeting transcripts, and we report results on both transcript types. The ASR output was provided by the AMI-ASR team (Hain et al., 2007), and the word error rate for the AMI corpus is 38.9%. 3.3 Summarization Evaluation For evaluating our extractive summaries, we implement existing evaluation schemes from previous research, with somewhat similar methods for meetings versus emails. These are described and compared below. We also evaluate our extractive classifiers more generally by plotting the receiver operator characteristic (ROC) curve and calculating the area under the curve (AUROC). This allows us to gauge the true-positive/false-positive ratio as the posterior threshold is varied. We use the differing evaluation metrics f</context>
</contexts>
<marker>Hain, Burget, Dines, Garau, Wan, Karafiat, Vepa, Lincoln, 2007</marker>
<rawString>T. Hain, L. Burget, J. Dines, G. Garau, V. Wan, M. Karafiat, J. Vepa, and M. Lincoln. 2007. The AMI system for transcription of speech in meetings. In Proc. ofICASSP 2007,, pages 357–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Klimt</author>
<author>Y Yang</author>
</authors>
<title>Introducing the enron corpus.</title>
<date>2004</date>
<booktitle>In Proc. of CEAS 2004,</booktitle>
<location>Mountain View, CA, USA.</location>
<contexts>
<context position="8723" citStr="Klimt and Yang, 2004" startWordPosition="1293" endWordPosition="1296">tes and Vapnik, 1995). The two classifier types yielded very similar results, with logistic regression classifiers being much faster to train and thus expediting further development. The liblinear toolkit 1 implements simple feature subset selection based on the F statistic (Chen and Lin, 2006) . 3.2 Corpora Description For these experiments we utilize two corpora, the Enron corpus for email summarization and the AMI corpus for meeting summarization. 3.2.1 The Enron Email Corpus The Enron email corpus2 is a collection of emails released as part of the investigation into the Enron corporation (Klimt and Yang, 2004). It has become a popular corpus for NLP research (e.g. (Bekkerman et al., 2004; Yeh and Harnly, 2006; Chapanond et al., 2005; Diesner et al., 2005)) due to being realistic, naturally-occurring data from a corporate environment, and moreover because privacy concerns mean that there is very low availability for other publicly available email data. 39 threads have been annotated for extractive summarization, with five annotators assigned to each thread. The annotators were asked to select 30% of the sentences in a thread, subsequently labeling each selected sentence as either ’essential’ or 1htt</context>
</contexts>
<marker>Klimt, Yang, 2004</marker>
<rawString>B. Klimt and Y. Yang. 2004. Introducing the enron corpus. In Proc. of CEAS 2004, Mountain View, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Maskey</author>
<author>J Hirschberg</author>
</authors>
<title>Comparing lexial, acoustic/prosodic, discourse and structural features for speech summarization.</title>
<date>2005</date>
<booktitle>In Proc. of Interspeech</booktitle>
<pages>621--624</pages>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="16272" citStr="Maskey and Hirschberg, 2005" startWordPosition="2520" endWordPosition="2523">the exchange. For meetings, a turn is a sequence of dialogue acts by one speaker, with the turn boundaries delimited by dialogue acts from other meeting participants. The features we derive for summarization are based on this view of the conversational structure. We calculate two length features. For each sentence, we derive a word-count feature normalized by the longest sentence in the conversation (SLEN) and a word-count feature normalized by the longest sentence in the turn (SLEN2). Sentence length has previously been found to be an effective feature in speech and text summarization (e.g. (Maskey and Hirschberg, 2005; Murray et al., 2005a; Galley, 2006)). There are several structural features used, including position of the sentence in the turn (TLOC) and position of the sentence in the conversation (CLOC). We also include the time from the beginning of the conversation to the current turn (TPOS]) and from the current turn to the end of the conversation (TPOS2). Conversations in both modalities can be well-structured, with introductory turns, general discussion, and ultimate resolution or closure, and sentence informativeness might significantly correlate with this structure. We calculate two pause-style </context>
</contexts>
<marker>Maskey, Hirschberg, 2005</marker>
<rawString>S. Maskey and J. Hirschberg. 2005. Comparing lexial, acoustic/prosodic, discourse and structural features for speech summarization. In Proc. of Interspeech 2005, Lisbon, Portugal, pages 621–624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Muresan</author>
<author>E Tzoukermann</author>
<author>J Klavans</author>
</authors>
<title>Combining linguistic and machine learning techniques for email summarization.</title>
<date>2001</date>
<booktitle>In Proc. of ConLL</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="4981" citStr="Muresan et al. (2001)" startWordPosition="708" endWordPosition="711">nal Random Fields to model pragmatic dependencies between paired meeting utterances (e.g. QUESTION-ANSWER relations), and used a combination of lexical, prosodic, structural and discourse features to rank utterances by importance. Galley found that while the most useful single feature class was lexical features, a combination of acoustic, durational and structural features exhibited comparable performance according to Pyramid evaluation. 2.2 Email Summarization Work on email summarization can be divided into summarization of individual email messages and summarization of entire email threads. Muresan et al. (2001) took the approach of summarizing individual email messages, first using linguistic techniques to extract noun phrases and then employing machine learning methods to label the extracted noun phrases as salient or not. Corston-Oliver et al. (2004) focused on identifying speech acts within a given email, with a particular interest in task-related sentences. Rambow et al. (2004) addressed the challenge of summarizing entire threads by treating it as a binary sentence classification task. They considered three types of features: basic features that simply treat the email as text (e.g. tf.idf, whic</context>
</contexts>
<marker>Muresan, Tzoukermann, Klavans, 2001</marker>
<rawString>S. Muresan, E. Tzoukermann, and J. Klavans. 2001. Combining linguistic and machine learning techniques for email summarization. In Proc. of ConLL 2001, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
</authors>
<title>Meta comments for summarizing meeting speech.</title>
<date>2008</date>
<booktitle>In Proc. ofMLMI2008,</booktitle>
<location>Utrecht, Netherlands.</location>
<contexts>
<context position="22463" citStr="Murray and Renals (2008)" startWordPosition="3559" endWordPosition="3562">lly, we include a feature that is a rough approximation of the ClueWordScore (CWS) used by Carenini et al. (2007). For each sentence we remove stopwords and count the number of words that occur in other turns besides the current turn. The CWS is therefore a measure of conversation cohesion. For ease of reference, we hereafter refer to this conversation features system as ConverSumm. 5 Comparison Summarization Systems In order to compare the ConverSumm system with state-of-the-art systems for meeting and email summarization, respectively, we also present results using the features described by Murray and Renals (2008) for meetings and the features described by Rambow (2004) for email. Because the work by Murray and Renals used the same dataset, we can compare our scores directly. However, Rambow carried out summarization work on a different, unavailable email corpus, and so we re-implemented their summarization system for our current email data. In their work on meeting summarization, Murray and Renals creating 700-word summaries of each meeting using several classes of features: prosodic, lexical, structural and speaker-related. While there are two features overlapping between our systems (word-count and </context>
</contexts>
<marker>Murray, Renals, 2008</marker>
<rawString>G. Murray and S. Renals. 2008. Meta comments for summarizing meeting speech. In Proc. ofMLMI2008, Utrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Carletta</author>
</authors>
<title>Extractive summarization of meeting recordings.</title>
<date>2005</date>
<booktitle>In Proc. of Interspeech</booktitle>
<pages>593--596</pages>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="3914" citStr="Murray et al. (2005" startWordPosition="562" endWordPosition="565">rsion of the Maximal Marginal Relevance algorithm (Carbonell and Goldstein, 1998) applied to speech transcripts, presenting the user with the n best sentences in a meeting browser interface. Zechner (2002) investigated summarizing several genres of speech, including spontaneous meeting speech. Though relevance detection in his work relied largely on tf.idf scores, Zechner also explored cross-speaker information linking and question/answer detection. More recently, researchers have investigated the utility of employing speech-specific features for summarization, including prosodic information. Murray et al. (2005a; 2005b) compared purely textual summarization approaches with featurebased approaches incorporating prosodic features, with human judges favoring the feature-based approaches. In subsequent work (2006; 2007), they began to look at additional speech-specific characteristics such as speaker status, discourse markers and high-level meta comments in meetings, i.e. comments that refer to the meeting itself. Galley (2006) used skip-chain Conditional Random Fields to model pragmatic dependencies between paired meeting utterances (e.g. QUESTION-ANSWER relations), and used a combination of lexical, p</context>
<context position="16293" citStr="Murray et al., 2005" startWordPosition="2524" endWordPosition="2527"> turn is a sequence of dialogue acts by one speaker, with the turn boundaries delimited by dialogue acts from other meeting participants. The features we derive for summarization are based on this view of the conversational structure. We calculate two length features. For each sentence, we derive a word-count feature normalized by the longest sentence in the conversation (SLEN) and a word-count feature normalized by the longest sentence in the turn (SLEN2). Sentence length has previously been found to be an effective feature in speech and text summarization (e.g. (Maskey and Hirschberg, 2005; Murray et al., 2005a; Galley, 2006)). There are several structural features used, including position of the sentence in the turn (TLOC) and position of the sentence in the conversation (CLOC). We also include the time from the beginning of the conversation to the current turn (TPOS]) and from the current turn to the end of the conversation (TPOS2). Conversations in both modalities can be well-structured, with introductory turns, general discussion, and ultimate resolution or closure, and sentence informativeness might significantly correlate with this structure. We calculate two pause-style features: the time be</context>
<context position="30295" citStr="Murray et al., 2005" startWordPosition="4826" endWordPosition="4829"> given thread. SLEN2, which normalizes sentence length by the longest sentence in the turn, also is much more eff statistic 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 feature ID (see key) TP 1 0.8 0.6 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1 FP lexical features structural features participant features length features 780 fective for emails. The reason is that many meeting turns consist of a single, brief utterance such as “Okay, yeah.” The finding that the summary evaluations are not significantly worse on noisy ASR compared with manual transcripts has been previously attested (Valenza et al., 1999; Murray et al., 2005a), and it is encouraging that our ConverSumm features are similarly robust to this noisy data. 8 Conclusion We have shown that a general conversation summarization approach can achieve results on par with state-of-the-art systems that rely on features specific to more focused domains. We have introduced a conversation feature set that is similarly effective in both the meetings and emails domains. The use of multiple summarization evaluation techniques confirms that the system is robust, even when applied to the noisy ASR output in the meetings domain. Such a general conversation summarizatio</context>
</contexts>
<marker>Murray, Renals, Carletta, 2005</marker>
<rawString>G. Murray, S. Renals, and J. Carletta. 2005a. Extractive summarization of meeting recordings. In Proc. of Interspeech 2005, Lisbon, Portugal, pages 593–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Carletta</author>
<author>J Moore</author>
</authors>
<title>Evaluating automatic summaries of meeting recordings.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL 2005 MTSE Workshop,</booktitle>
<pages>33--40</pages>
<location>Ann Arbor, MI, USA,</location>
<contexts>
<context position="3914" citStr="Murray et al. (2005" startWordPosition="562" endWordPosition="565">rsion of the Maximal Marginal Relevance algorithm (Carbonell and Goldstein, 1998) applied to speech transcripts, presenting the user with the n best sentences in a meeting browser interface. Zechner (2002) investigated summarizing several genres of speech, including spontaneous meeting speech. Though relevance detection in his work relied largely on tf.idf scores, Zechner also explored cross-speaker information linking and question/answer detection. More recently, researchers have investigated the utility of employing speech-specific features for summarization, including prosodic information. Murray et al. (2005a; 2005b) compared purely textual summarization approaches with featurebased approaches incorporating prosodic features, with human judges favoring the feature-based approaches. In subsequent work (2006; 2007), they began to look at additional speech-specific characteristics such as speaker status, discourse markers and high-level meta comments in meetings, i.e. comments that refer to the meeting itself. Galley (2006) used skip-chain Conditional Random Fields to model pragmatic dependencies between paired meeting utterances (e.g. QUESTION-ANSWER relations), and used a combination of lexical, p</context>
<context position="16293" citStr="Murray et al., 2005" startWordPosition="2524" endWordPosition="2527"> turn is a sequence of dialogue acts by one speaker, with the turn boundaries delimited by dialogue acts from other meeting participants. The features we derive for summarization are based on this view of the conversational structure. We calculate two length features. For each sentence, we derive a word-count feature normalized by the longest sentence in the conversation (SLEN) and a word-count feature normalized by the longest sentence in the turn (SLEN2). Sentence length has previously been found to be an effective feature in speech and text summarization (e.g. (Maskey and Hirschberg, 2005; Murray et al., 2005a; Galley, 2006)). There are several structural features used, including position of the sentence in the turn (TLOC) and position of the sentence in the conversation (CLOC). We also include the time from the beginning of the conversation to the current turn (TPOS]) and from the current turn to the end of the conversation (TPOS2). Conversations in both modalities can be well-structured, with introductory turns, general discussion, and ultimate resolution or closure, and sentence informativeness might significantly correlate with this structure. We calculate two pause-style features: the time be</context>
<context position="30295" citStr="Murray et al., 2005" startWordPosition="4826" endWordPosition="4829"> given thread. SLEN2, which normalizes sentence length by the longest sentence in the turn, also is much more eff statistic 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 feature ID (see key) TP 1 0.8 0.6 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1 FP lexical features structural features participant features length features 780 fective for emails. The reason is that many meeting turns consist of a single, brief utterance such as “Okay, yeah.” The finding that the summary evaluations are not significantly worse on noisy ASR compared with manual transcripts has been previously attested (Valenza et al., 1999; Murray et al., 2005a), and it is encouraging that our ConverSumm features are similarly robust to this noisy data. 8 Conclusion We have shown that a general conversation summarization approach can achieve results on par with state-of-the-art systems that rely on features specific to more focused domains. We have introduced a conversation feature set that is similarly effective in both the meetings and emails domains. The use of multiple summarization evaluation techniques confirms that the system is robust, even when applied to the noisy ASR output in the meetings domain. Such a general conversation summarizatio</context>
</contexts>
<marker>Murray, Renals, Carletta, Moore, 2005</marker>
<rawString>G. Murray, S. Renals, J. Carletta, and J. Moore. 2005b. Evaluating automatic summaries of meeting recordings. In Proc. of the ACL 2005 MTSE Workshop, Ann Arbor, MI, USA, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Moore</author>
<author>J Carletta</author>
</authors>
<title>Incorporating speaker and discourse features into speech summarization.</title>
<date>2006</date>
<booktitle>In Proc. of the HLT-NAACL 2006,</booktitle>
<pages>367--374</pages>
<location>New York City, USA,</location>
<contexts>
<context position="13239" citStr="Murray et al., 2006" startWordPosition="2004" endWordPosition="2007">primary reasons. First, the differing summarization annotations in the AMI and Enron corpora naturally lend themselves to slightly divergent metrics, one based on extractabstract links and the other based on the essential/option/uninformative distinction. Second, and more importantly, using these two metrics allow us to compare our results with state-of-the-art results in the two fields of speech summarization and email summarization. In future work we plan to use a single evaluation metric. 3.3.1 Evaluating Meeting Summaries To evaluate meeting summaries we use the weighted f-measure metric (Murray et al., 2006). This evaluation scheme relies on the multiple human annotated summary links described in Section 3.2.2. Both weighted precision and recall share the same numerator L(si,aj) (1) where L(si, aj) is the number of links for a DA si in the machine extractive summary according to annotator ai, M is the number of DAs in the machine summary, and N is the number of annotators. Weighted precision is defined as: and weighted recall is given by num recall = (3) E� 1 1 L(si, aj ) where O is the total number of DAs in the meeting, N is the number of annotators, and the denominator represents the total num</context>
</contexts>
<marker>Murray, Renals, Moore, Carletta, 2006</marker>
<rawString>G. Murray, S. Renals, J. Moore, and J. Carletta. 2006. Incorporating speaker and discourse features into speech summarization. In Proc. of the HLT-NAACL 2006, New York City, USA, pages 367–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
</authors>
<title>Using Speech-Specific Features for Automatic Speech Summarization.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<marker>Murray, 2007</marker>
<rawString>G. Murray. 2007. Using Speech-Specific Features for Automatic Speech Summarization. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>B Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The Pyramid method.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL 2004,</booktitle>
<pages>145--152</pages>
<location>Boston, MA, USA,</location>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>A. Nenkova and B. Passonneau. 2004. Evaluating content selection in summarization: The Pyramid method. In Proc. of HLT-NAACL 2004, Boston, MA, USA, pages 145–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Rambow</author>
<author>L Shrestha</author>
<author>J Chen</author>
<author>C Lauridsen</author>
</authors>
<title>Summarizing email threads.</title>
<date>2004</date>
<booktitle>In Proc. of HLTNAACL 2004,</booktitle>
<location>Boston, USA.</location>
<contexts>
<context position="5359" citStr="Rambow et al. (2004)" startWordPosition="767" endWordPosition="770">ibited comparable performance according to Pyramid evaluation. 2.2 Email Summarization Work on email summarization can be divided into summarization of individual email messages and summarization of entire email threads. Muresan et al. (2001) took the approach of summarizing individual email messages, first using linguistic techniques to extract noun phrases and then employing machine learning methods to label the extracted noun phrases as salient or not. Corston-Oliver et al. (2004) focused on identifying speech acts within a given email, with a particular interest in task-related sentences. Rambow et al. (2004) addressed the challenge of summarizing entire threads by treating it as a binary sentence classification task. They considered three types of features: basic features that simply treat the email as text (e.g. tf.idf, which scores words highly if they are frequent in the document but rare across all documents), features that consider the thread to be a sequence of turns (e.g. the position of the turn in the thread), and email-specific features such as number of recipients and subject line similarity. Carenini et al. (2007) took an approach to thread summarization using the Enron corpus (descri</context>
</contexts>
<marker>Rambow, Shrestha, Chen, Lauridsen, 2004</marker>
<rawString>O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen. 2004. Summarizing email threads. In Proc. of HLTNAACL 2004, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Valenza</author>
<author>T Robinson</author>
<author>M Hickey</author>
<author>R Tucker</author>
</authors>
<title>Summarization of spoken audio through information extraction.</title>
<date>1999</date>
<booktitle>In Proc. of the ESCA Workshop on Accessing Information in Spoken Audio,</booktitle>
<pages>111--116</pages>
<location>Cambridge UK,</location>
<contexts>
<context position="30274" citStr="Valenza et al., 1999" startWordPosition="4822" endWordPosition="4825">t person to email in a given thread. SLEN2, which normalizes sentence length by the longest sentence in the turn, also is much more eff statistic 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 feature ID (see key) TP 1 0.8 0.6 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1 FP lexical features structural features participant features length features 780 fective for emails. The reason is that many meeting turns consist of a single, brief utterance such as “Okay, yeah.” The finding that the summary evaluations are not significantly worse on noisy ASR compared with manual transcripts has been previously attested (Valenza et al., 1999; Murray et al., 2005a), and it is encouraging that our ConverSumm features are similarly robust to this noisy data. 8 Conclusion We have shown that a general conversation summarization approach can achieve results on par with state-of-the-art systems that rely on features specific to more focused domains. We have introduced a conversation feature set that is similarly effective in both the meetings and emails domains. The use of multiple summarization evaluation techniques confirms that the system is robust, even when applied to the noisy ASR output in the meetings domain. Such a general conv</context>
</contexts>
<marker>Valenza, Robinson, Hickey, Tucker, 1999</marker>
<rawString>R. Valenza, T. Robinson, M. Hickey, and R. Tucker. 1999. Summarization of spoken audio through information extraction. In Proc. of the ESCA Workshop on Accessing Information in Spoken Audio, Cambridge UK, pages 111–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Waibel</author>
<author>M Bett</author>
<author>M Finke</author>
<author>R Stiefelhagen</author>
</authors>
<title>Meeting browser: Tracking and summarizing meetings. In</title>
<date>1998</date>
<booktitle>Proc. of the Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>281--286</pages>
<editor>D. E. M. Penrose, editor,</editor>
<location>Lansdowne, VA, USA,</location>
<contexts>
<context position="3269" citStr="Waibel et al. (1998)" startWordPosition="473" endWordPosition="476">ion-based system is attested via several summarization evaluation techniques, and we give an in-depth analysis of the effectiveness of the individual features and feature subclasses used. 2 Related Work on Meetings and Emails In this section we give a brief overview of previous research on meeting summarization and email summarization, respectively. 773 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 773–782, Honolulu, October 2008.c�2008 Association for Computational Linguistics 2.1 Meeting Summarization Among early work on meeting summarization, Waibel et al. (1998) implemented a modified version of the Maximal Marginal Relevance algorithm (Carbonell and Goldstein, 1998) applied to speech transcripts, presenting the user with the n best sentences in a meeting browser interface. Zechner (2002) investigated summarizing several genres of speech, including spontaneous meeting speech. Though relevance detection in his work relied largely on tf.idf scores, Zechner also explored cross-speaker information linking and question/answer detection. More recently, researchers have investigated the utility of employing speech-specific features for summarization, includ</context>
</contexts>
<marker>Waibel, Bett, Finke, Stiefelhagen, 1998</marker>
<rawString>A. Waibel, M. Bett, M. Finke, and R. Stiefelhagen. 1998. Meeting browser: Tracking and summarizing meetings. In D. E. M. Penrose, editor, Proc. of the Broadcast News Transcription and Understanding Workshop, Lansdowne, VA, USA, pages 281–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yeh</author>
<author>A Harnly</author>
</authors>
<title>Email thread reassembly using similarity matching.</title>
<date>2006</date>
<booktitle>In Proc of CEAS</booktitle>
<contexts>
<context position="8824" citStr="Yeh and Harnly, 2006" startWordPosition="1311" endWordPosition="1314"> classifiers being much faster to train and thus expediting further development. The liblinear toolkit 1 implements simple feature subset selection based on the F statistic (Chen and Lin, 2006) . 3.2 Corpora Description For these experiments we utilize two corpora, the Enron corpus for email summarization and the AMI corpus for meeting summarization. 3.2.1 The Enron Email Corpus The Enron email corpus2 is a collection of emails released as part of the investigation into the Enron corporation (Klimt and Yang, 2004). It has become a popular corpus for NLP research (e.g. (Bekkerman et al., 2004; Yeh and Harnly, 2006; Chapanond et al., 2005; Diesner et al., 2005)) due to being realistic, naturally-occurring data from a corporate environment, and moreover because privacy concerns mean that there is very low availability for other publicly available email data. 39 threads have been annotated for extractive summarization, with five annotators assigned to each thread. The annotators were asked to select 30% of the sentences in a thread, subsequently labeling each selected sentence as either ’essential’ or 1http://www.csie.ntu.edu.tw/˜cjlin/liblinear/ 2http://www.cs.cmu.edu/˜enron/ ’optional.’ Essential senten</context>
</contexts>
<marker>Yeh, Harnly, 2006</marker>
<rawString>J. Yeh and A. Harnly. 2006. Email thread reassembly using similarity matching. In Proc of CEAS 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zajic</author>
<author>B Dorr</author>
<author>J Lin</author>
</authors>
<title>Single-document and multi-document summarization techniques for email threads using sentence compression. Information Processing and Management,</title>
<date>2008</date>
<note>to appear.</note>
<contexts>
<context position="6938" citStr="Zajic et al. (2008)" startWordPosition="1029" endWordPosition="1032"> the structure A &gt; B C where B is a quoted section in the middle of the email, then there are three email fragments in total: two new fragments A and C separated by one quoted fragment B. Sentences in a fragment are weighted according to the Clue Word Score (CWS) measure, a lexical cohesion metric based on the recurrence of words in parent and child nodes. In subsequent work, Carenini et al. (2008) determined that subjectivity detection (i.e., whether the sentence contains sentiments or opinions from the author) gave additional improvement for email thread summaries. Also on the Enron corpus, Zajic et al. (2008) compared Collective Message Summarization (CMS) to Individual Message Summarization (IMS) and found the former to be a more effective technique for summarizing email data. CMS essentially treats thread summarization as a multi-document summarization problem, while IMS summarizes individual emails in the thread and then concatenates them to form a thread summary. In our work described below we also address the task of thread summarization as opposed to sum774 marization of individual email messages, following Carenini et al. and the CMS approach of Zajic et al. 3 Experimental Setup In this sec</context>
</contexts>
<marker>Zajic, Dorr, Lin, 2008</marker>
<rawString>D. Zajic, B. Dorr, and J. Lin. 2008. Single-document and multi-document summarization techniques for email threads using sentence compression. Information Processing and Management, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
</authors>
<title>Automatic summarization of opendomain multiparty dialogues in diverse genres.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="3500" citStr="Zechner (2002)" startWordPosition="510" endWordPosition="511">tion we give a brief overview of previous research on meeting summarization and email summarization, respectively. 773 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 773–782, Honolulu, October 2008.c�2008 Association for Computational Linguistics 2.1 Meeting Summarization Among early work on meeting summarization, Waibel et al. (1998) implemented a modified version of the Maximal Marginal Relevance algorithm (Carbonell and Goldstein, 1998) applied to speech transcripts, presenting the user with the n best sentences in a meeting browser interface. Zechner (2002) investigated summarizing several genres of speech, including spontaneous meeting speech. Though relevance detection in his work relied largely on tf.idf scores, Zechner also explored cross-speaker information linking and question/answer detection. More recently, researchers have investigated the utility of employing speech-specific features for summarization, including prosodic information. Murray et al. (2005a; 2005b) compared purely textual summarization approaches with featurebased approaches incorporating prosodic features, with human judges favoring the feature-based approaches. In subse</context>
</contexts>
<marker>Zechner, 2002</marker>
<rawString>K. Zechner. 2002. Automatic summarization of opendomain multiparty dialogues in diverse genres. Computational Linguistics, 28(4):447–485.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>