<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.974277">
Syntactic Decision Tree LMs: Random Selection or Intelligent Design?
</title>
<author confidence="0.972694">
Denis Filimonov†$
</author>
<affiliation confidence="0.886756666666667">
$Human Language Technology
Center of Excellence
Johns Hopkins University
</affiliation>
<email confidence="0.831593">
den@cs.umd.edu
</email>
<affiliation confidence="0.9614285">
Mary Harper††Department of Computer Science
University of Maryland, College Park
</affiliation>
<email confidence="0.998712">
mharper@umd.edu
</email>
<sectionHeader confidence="0.995618" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998836882352941">
Decision trees have been applied to a vari-
ety of NLP tasks, including language mod-
eling, for their ability to handle a variety of
attributes and sparse context space. More-
over, forests (collections of decision trees)
have been shown to substantially outperform
individual decision trees. In this work, we in-
vestigate methods for combining trees in a for-
est, as well as methods for diversifying trees
for the task of syntactic language modeling.
We show that our tree interpolation technique
outperforms the standard method used in the
literature, and that, on this particular task, re-
stricting tree contexts in a principled way pro-
duces smaller and better forests, with the best
achieving an 8% relative reduction in Word
Error Rate over an n-gram baseline.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995525">
Language Models (LMs) are an essential part of
NLP applications that require selection of the most
fluent word sequence among multiple hypotheses.
The most prominent applications include Automatic
Speech Recognition (ASR) and Machine Transla-
tion (MT).
Statistical LMs formulate the problem as the
computation of the model’s probability to gener-
ate the word sequence w1, w2, ... , wm (denoted as
wm1 ), assuming that higher probability corresponds
to more fluent hypotheses. LMs are often repre-
sented in the following generative form:
</bodyText>
<page confidence="0.870443">
p(wi|wi−1
1 )
691
</page>
<bodyText confidence="0.8312346">
Note the context space for this function, wi−1
1 is ar-
bitrarily long, necessitating some independence as-
sumption, which usually consists of reducing the rel-
evant context to n−1 immediately preceding tokens:
</bodyText>
<equation confidence="0.990805666666667">
p(wi|wi−1
1 ) ^ p(wi|wi−1
i−n+1) (1)
</equation>
<bodyText confidence="0.999785714285714">
These distributions are typically estimated from ob-
served counts of n-grams wii−n+1 in the training
data. The context space is still far too large1; there-
fore, the models are recursively smoothed using
lower order distributions. For instance, in a widely
used n-gram LM, the probabilities are estimated as
follows:
</bodyText>
<equation confidence="0.993652833333333">
˜p(wi|wi−1
i−n+1) = ρ(wi|wi−1
i−n+1) + (2)
i−1 /
γ( wi−n+1) - Awi|wi−1
i−n+2)
</equation>
<bodyText confidence="0.999089818181818">
where ρ is a discounted probability2.
Note that this type of model is a simple Markov
chain lacking any notion of syntax. It is widely
accepted that languages do have some structure.
Moreover, it has been shown that incorporating syn-
tax into a language model can improve its perfor-
mance (Bangalore, 1996; Heeman, 1998; Chelba
and Jelinek, 2000; Filimonov and Harper, 2009). A
straightforward way of incorporating syntax into a
language model is by assigning a tag to each word
and modeling them jointly; then to obtain the proba-
</bodyText>
<footnote confidence="0.746358">
1O(|V |&amp;quot;−1) in n-gram model with typical order n =
3 ... 5, and a vocabulary size of IV I = 104 ... 106.
2We refer the reader to (Chen and Goodman, 1996) for a
survey of the discounting methods for n-gram models.
p(wm1 ) = �m
i=1
</footnote>
<note confidence="0.957176">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 691–699,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.94510525">
bility of a word sequence, the tags must be marginal-
ized out:
An independence assumption similar to Eq. 1 can be
made:
</bodyText>
<equation confidence="0.971094">
p( Z i1 i—i i−1) N p( Z it i−1 ) ()
w•t• w1 t1 w•t• wi−n+1ti−1
i−n+1 3
</equation>
<bodyText confidence="0.999820333333333">
A primary goal of our research is to build strong
syntactic language models and provide effective
methods for constructing them to the research com-
munity. Note that the tags in the context of the joint
model in Eq. 3 exacerbate the already sparse prob-
lem in Eq. 1, which makes the probability estima-
tion particularly challenging. We utilize decision
trees for joint syntactic language models to clus-
ter context because of their strengths (reliance on
information theoretic metrics to cluster context in
the face of extreme sparsity and the ability to in-
corporate attributes of different types3), and at the
same time, unlike log-linear models (Rosenfeld et
al., 1994), computationally expensive probability
normalization does not have to be postponed until
runtime.
In Section 2, we describe the details of the syntac-
tic decision tree LM. Construction of a single-tree
model is difficult due to the inevitable greediness
of the tree construction process and its tendency to
overfit the data. This problem is often addressed by
interpolating with lower order decision trees. In Sec-
tion 3, we point out the inappropriateness of backoff
methods borrowed from n-gram models for decision
tree LMs and briefly describe a generalized interpo-
lation for such models. The generalized interpola-
tion method allows the addition of any number of
trees to the model, and thus raises the question: what
is the best way to create diverse decision trees so that
their combination results in a stronger model, while
at the same time keeping the total number of trees in
the model relatively low for computational practical-
ity. In Section 4, we explore and evaluate a variety
</bodyText>
<footnote confidence="0.887799">
3For example, morphological features can be very helpful
for modeling highly inflectional languages (Bilmes and Kirch-
hoff, 2003).
</footnote>
<bodyText confidence="0.99982625">
of methods for creating different trees. To support
our findings, we evaluate several of the models on
an ASR rescoring task in Section 5. Finally, we dis-
cuss our findings in Section 6.
</bodyText>
<sectionHeader confidence="0.837312" genericHeader="method">
2 Joint Syntactic Decision Tree LM
</sectionHeader>
<bodyText confidence="0.8335785">
A decision tree provides us with a clustering func-
tion Φ(wi−1
</bodyText>
<equation confidence="0.9815805">
i−n+1ti−1
i−n+1) → {Φ1, ... , ΦN}, where N
</equation>
<bodyText confidence="0.94993625">
is the number of clusters, and clusters Φk are disjoint
subsets of the context space. The probability estima-
tion for a joint decision tree model is approximated
as follows:
</bodyText>
<equation confidence="0.999300666666667">
i−1 i−1 i−1 i−1
p(witi|wi−n+1ti−n+1) ≈ p(witi|Φ(wi−n+1ti−n+1))
(4)
</equation>
<bodyText confidence="0.99993675">
In the remainder of this section, we briefly describe
the techniques that we use to construct such a deci-
sion tree Φ and to estimate the probability distribu-
tion for the joint model in Eq. 4.
</bodyText>
<subsectionHeader confidence="0.993201">
2.1 Decision Tree Construction
</subsectionHeader>
<bodyText confidence="0.9999305">
We use recursive partitioning to grow decision trees.
In this approach, a number of alternative binary
splits of the training data associated with a node are
evaluated using some metric, the best split is chosen,
checked against a stopping rule (which aims at pre-
venting overfitting to the training data and usually
involves a heldout set), and then the two partitions
become the child nodes if the stopping rule does not
apply. Then the algorithm proceeds recursively into
the newly constructed leaves.
Binary splits are often referred to as questions
about the context because a binary partition can
be represented by a binary function that decides
whether an element of context space belongs to one
partition or the other. We utilize univariate questions
where each question partitions the context on one
attribute, e.g., wi−2 or ti−1. The questions about
words and tags are constructed differently:
</bodyText>
<listItem confidence="0.602489">
• The questions q about the words are in the form
q(x) ≡ wi+x E S, where x is an integer be-
tween −n + 1 and −1, and S C V is a subset
of the word vocabulary V . To construct the set
S, we take the set of words So observed at the
offset x in the training data associated with the
</listItem>
<equation confidence="0.995405">
� �p(wm1 tm1 ) = �m p(witi|wi−1
p(wm 1 ) = t1...t— i=1 1 ti−1
t1 ... t— 1 )
</equation>
<page confidence="0.979861">
692
</page>
<bodyText confidence="0.9981673">
current node and split it into two complemen-
tary subsets S U S¯ = So using the Exchange
algorithm (Martin et al., 1998). Because the
algorithm is greedy and depends on the initial-
ization, we construct 4 questions per word po-
sition using different random initializations of
the Exchange algorithm.
Since we need to account for words that were
not observed in the training data, we utilize
the structure depicted in Figure 1. To estimate
the probability at the backoff node (B in Fig-
ure 1), we can either use the probability from its
grandparent node A or estimate it using a lower
order tree (see Section 3), or combine the two.
We have observed no noticeable difference be-
tween these methods, which suggests that only
a small fraction of probability is estimated from
these nodes; therefore, for simplicity, we use
the probability estimated at the backoff node’s
grandparent.
</bodyText>
<listItem confidence="0.938382333333333">
• To create questions about tags we create a hi-
erarchical clustering of all tags in the form of
a binary tree. This is done beforehand, using
the Minimum Discriminating Information al-
gorithm (Zitouni, 2007) with the entire train-
ing data set. In this tree, each leaf is an in-
</listItem>
<bodyText confidence="0.760144285714286">
dividual tag and each internal node is associ-
ated with the subset of tags that the node dom-
inates. Questions about tags are constructed in
the form q(x, k) ~ ti+x E Tk, where k is a
node in the tag tree and Tk is the subset of tags
associated with that node. The rationale behind
constructing tag questions in this form is that
it enables a more efficient decoding algorithm
than standard HMM decoding (Filimonov and
Harper, 2009).
Questions are evaluated in two steps. First the
context attribute x is selected using a metric simi-
lar to information gain ratio proposed by (Quinlan,
1986):
</bodyText>
<equation confidence="0.997729">
H(wi) − H(wijx)
M = 1 −H(x)
</equation>
<bodyText confidence="0.949245">
where x is one of the context attributes, e.g., wi−2
or ti−1. Then, among the questions about attribute
</bodyText>
<figureCaption confidence="0.998716">
Figure 1: A fragment of the decision tree with a backoff
node. S U S¯ is the set of words observed in the training
data at the node A. To account for unseen words, we add
the backoff node B.
</figureCaption>
<bodyText confidence="0.999276222222222">
x, we select the question that maximizes the entropy
reduction.
Instead of dedicating an explicit heldout set for
the stopping criterion, we utilize a technique simi-
lar to cross validation: the training data set is par-
titioned into four folds, and the best question is re-
quired to reduce entropy on each of the folds.
Note that the tree induction algorithm can also be
used to construct trees without tags:
</bodyText>
<equation confidence="0.991173">
p(wijwi−1
i−n+1) ~ p(wijΦ(wi−1
i−n+1))
</equation>
<bodyText confidence="0.9999464">
We refer to this model as the word-tree model. By
comparing syntactic and word-tree models, we are
able to separate the effects of decision tree modeling
and syntactic information on language modeling by
comparing both models to an n-gram baseline.
</bodyText>
<subsectionHeader confidence="0.989297">
2.2 In-tree Smoothing
</subsectionHeader>
<bodyText confidence="0.999536">
A decision tree offers a hierarchy of clusterings that
can be exploited for smoothing. We can interpo-
late the observed distributions at leaves recursively
with their parents, as in (Bahl et al., 1990; Heeman,
1998):
</bodyText>
<equation confidence="0.979679">
˜pk(witi) = λkpML(witi) + (1 − λk)˜pk (witi) (5)
</equation>
<bodyText confidence="0.88818675">
where pML is the observed distribution at node k
and k0 is the parent of k. The coefficients λk are
estimated using an EM algorithm.
We can also combine p(witijΦ(wi−1
</bodyText>
<equation confidence="0.4467735">
i−n+1ti−1
i−n+1))
</equation>
<bodyText confidence="0.565002">
with lower order decision trees, i.e.,
</bodyText>
<figure confidence="0.989552666666667">
wi−2ES
A
wi−2E
no
S
yes
no
yes
B
Backoff leaf
j(x; wi)
= 1 −H(x)
</figure>
<page confidence="0.785245">
693
</page>
<equation confidence="0.5751045">
p(witi|4(wi−1
i−n+2ti−1
</equation>
<bodyText confidence="0.970454285714286">
i−n+2)), and so on up until
p(witi) which is a one-node tree (essentially a
unigram model). Although superficially similar to
backoff in n-gram models, lower order decision
trees differ substantially from lower order n-gram
models and require different interpolation methods.
In the next section, we discuss this difference and
present a generalized interpolation that is more
suitable for combining decision tree models.
3 Interpolation with Backoff Tree Models
In this section, for simplicity of presentation, we fo-
cus on the equations for word models, but the same
equations apply equally to joint models (Eq. 3) with
trivial transformations.
</bodyText>
<subsectionHeader confidence="0.998585">
3.1 Backoff Property
</subsectionHeader>
<bodyText confidence="0.994741">
Let us rewrite the interpolation Eq. 2 in a more
generic way:
</bodyText>
<equation confidence="0.9984555">
˜p(wi|wi−1
1 ) = Pn(wi|4n(wi−1
1 )) + (6)
-y(4n(wi−1
1 )) · ˜p(wi|BOn−1(wi−1
1 ))
</equation>
<bodyText confidence="0.935981">
where, Pn is a discounted distribution, 4n is a clus-
tering function of order n, and -y(4n(wi−1
</bodyText>
<equation confidence="0.667676">
1 )) is the
</equation>
<bodyText confidence="0.99904225">
backoff weight chosen to normalize the distribution.
BOn−1 is the backoff clustering function of order
n − 1, representing a reduction of context size. In
the case of an n-gram model, 4n(wi−1
</bodyText>
<equation confidence="0.632058">
1 ) is the set
</equation>
<bodyText confidence="0.940264">
of word sequences where the last n − 1 words are
</bodyText>
<equation confidence="0.992316333333333">
wi−1
i−n+1. Similarly, BOn−1(wi−1
1 ) is the set of se-
</equation>
<bodyText confidence="0.851416142857143">
quences ending with wi−1
i−n+2. In the case of a de-
cision tree model, the same backoff function is typ-
ically used, but the clustering function can be arbi-
trary.
The intuition behind Eq. 6 is that the backoff con-
text BOn−1(wi−1
</bodyText>
<figure confidence="0.840322">
1 ) allows for a more robust (but
less informed) probability estimation than the con-
text cluster 4n(wi−1
1 ). More precisely:
(a) Backoff Property satisfied (b) Backoff Property violated
</figure>
<figureCaption confidence="0.951109">
Figure 2: Backoff Property
</figureCaption>
<figure confidence="0.754355833333333">
trivially holds since BOn−1(wi−1
1 ) and 4n(wi−1
1 )
are defined as sets of sequences ending with wi−1
i−n+2
and wi−1
</figure>
<bodyText confidence="0.99868624">
i−n+1, with the former clearly being a superset
of the latter. However, when 4 can be arbitrary, e.g.,
a decision tree, the property is not necessarily satis-
fied. Figure 2 illustrates cases when the Property 7
is satisfied (a) and violated (b).
Let us consider what happens when we have
two context sequences W and W&apos; that belong to
the same cluster 4n(W) = 4n(W&apos;) but differ-
ent backoff clusters BOn−1(W) =� BOn−1(W&apos;).
For example: suppose we have 4(wi−2wi−1) =
({on}, {may,june}) and two corresponding backoff
clusters: BO&apos; = ({may}) and BO&apos;&apos; = ({june}).
Following on, the word may is likely to be a month
rather than a modal verb, although the latter is
more frequent and will dominate in BO&apos;. There-
fore we have much less faith in ˜p(wi|BO&apos;) than in
˜p(wi|BO&apos;&apos;) and would like a much smaller weight
-y assigned to BO&apos;. However this would not be pos-
sible in the backoff scheme in Eq. 6, thus we will
have to settle on a compromise value of -y, resulting
in suboptimal performance.
Hence arbitrary clustering (an advantage of deci-
sion trees) leads to a violation of Property 7, which
is likely to produce a degradation in performance if
backoff interpolation Eq. 6 is used.
</bodyText>
<figure confidence="0.7293652">
context space context space
Contexts from the same Φ
n
belong to different BOn-1
BOn−1
Φn
∀wi−1 1,W : W ∈ 4n(wi1)⇒W ∈ BOn−1(wi 1)
(7)
that is, every word sequence W that belongs to a
context cluster 4n(wi−1
</figure>
<footnote confidence="0.665037">
1 ), belongs to the same back-
off cluster BOn−1(wi−1
1 ) (hence has the same back-
off distribution). For n-gram models, Property 7
</footnote>
<subsectionHeader confidence="0.956608">
3.2 Generalized Interpolation
</subsectionHeader>
<bodyText confidence="0.94418375">
Recursive linear interpolation similar to Jelinek-
Mercer smoothing for n-gram models (Jelinek and
Mercer, 1980) has been applied to decision tree
models:
</bodyText>
<page confidence="0.90687">
694
</page>
<equation confidence="0.997725">
˜pn(wi|wi−1
i−n+1) = λn(φn) · pn(wi|φn) + (8)
(1 − λn(φn)) · ˜pn−1(wi|wi−1
i−n+2)
�M
p(wi|wi−1
i−n+1) = M 1
m=1
pm(wi|wi−1
i−n+1) (10)
</equation>
<bodyText confidence="0.997592066666667">
where φn ≡ Φn(wi−1
i−n+1), and λn(φn) ∈ [0, 1] are
assigned to each cluster and are optimized on a held-
out set using EM. pn(wi|φn) is the probability dis-
tribution at the cluster φn in the tree of order n. This
interpolation method is particularly useful as, un-
like count-based discounting methods (e.g., Kneser-
Ney), it can be applied to already smoothed distribu-
tions pn.
In (Filimonov and Harper, 2011), we observed
that because of the violation of Property 7 in deci-
sion tree models, the interpolation method of Eq. 8
is not appropriate for such models. Instead we pro-
posed the following generalized form of linear inter-
polation:
</bodyText>
<equation confidence="0.984981">
pn(wi |wi−n+1) Enm=1Em1 (φm pm (wi |φm) (9)
</equation>
<bodyText confidence="0.9582499">
Note that the recursive interpolation of Eq. 8 can
be represented in this form with the additional con-
straint En
m=1 λm(φm) = 1, which is not required in
the generalized interpolation of Eq. 9; thus, the gen-
eralized interpolation, albeit having the same num-
ber of parameters, has more degrees of freedom. We
also showed that the recursive interpolation Eq. 8 is
a special case of Eq. 9 that occurs when the Prop-
erty 7 holds.
</bodyText>
<sectionHeader confidence="0.972123" genericHeader="method">
4 From Backoff Trees to Forest
</sectionHeader>
<bodyText confidence="0.99996585">
Note that, in Eq. 9, individual trees do not have ex-
plicit higher-lower order relations, they are treated
as a collection of trees, i.e., as a forest. Naturally,
to benefit from the forest model, its trees must differ
in some way. Different trees can be created based
on differences in the training data, differences in the
tree growing algorithm, or some non-determinism in
the way the trees are constructed.
(Xu, 2005) used randomization techniques to pro-
duce a large forest of decision trees that were com-
bined as follows:
where M is the number of decision trees in the forest
(he proposed M = 100) and pm is the m-th tree
model4. Note that this type of interpolation assumes
that each tree model is “equal” a priori and therefore
is only appropriate when the tree models are grown
in the same way (particularly, using the same order
of context). Note that Eq. 10 is a special case of
Eq. 9 when all parameters λ are equal.
(Xu, 2005) showed that, although each individual
tree is a fairly weak model, their combination out-
performs the n-gram baseline substantially. How-
ever, we find this approach impractical for online
application of any sizable model: In our experi-
ments, fourgram trees have approximately 1.8 mil-
lion leaves and the tree structure itself (without prob-
abilities) occupies nearly 200MB of disk space af-
ter compression. It would be infeasible to apply a
model consisting of more than a handful of such
trees without distributed computing of some sort.
Therefore, we pose the following question: If we
can afford to have only a handful of trees in the
model, what would be best approach to construct
those trees?
In the remainder of this section, we will describe
the experimental setup, discuss and evaluate differ-
ent ways of building decision tree forests for lan-
guage modeling, and compare combination methods
based on Eq. 9 and Eq. 10 (when Eq. 10 is applica-
ble).
</bodyText>
<subsectionHeader confidence="0.97887">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9998103">
To train our models we use 35M words of WSJ
94-96 from LDC2008T13. The text was converted
into speech-like form, namely numbers and abbrevi-
ations were verbalized, text was downcased, punctu-
ation was removed, and contractions and possessives
were joined with the previous word (i.e., they ’ll be-
comes they’ll). For the syntactic modeling, we used
tags comprised of the POS tags of the word and it’s
head. Parsing of the text for tag extraction occurred
after verbalization of numbers and abbreviations but
</bodyText>
<footnote confidence="0.9737495">
4Note that (Xu, 2005) used lower order models to estimate
Pm.
</footnote>
<page confidence="0.996935">
695
</page>
<bodyText confidence="0.992937295454546">
before any further processing; we used a latent vari- domization methods. Since we do not have an a
able PCFG parser as in (Huang and Harper, 2009). priori preference for choosing initializations for the
For reference, we include an n-gram model with Exchange algorithm, by using random initializations
modified interpolated KN discounting. All mod- it is hoped that due to the greedy nature of the al-
els use the same vocabulary of approximately 50k gorithm, the constructed trees, while being “unde-
words. graded,”8 will be sufficiently different so that their
Perplexity numbers reported in Tables 1, 2, 3, combination improves over an individual tree. By
and 4 are computed on WSJ section 23 (tokenized introducing Bernoulli trials, on the other hand, there
in the same way)5. is a choice to purposely degrade the quality of in-
In Table 1, we show results reported in (Filimonov dividual trees in the hope that additional diversity
and Harper, 2011), which we use as the baseline for would enable their combination to compensate for
further experiments. We constructed two sets of de- the loss of quality in individual trees.
cision trees (a joint syntactic model and a word-tree Another way of introducing randomness to the
model) as described in Section 2. Each set was com- tree construction without apparent degradation of in-
prised of a fourgram tree with backoff trigram, bi- dividual tree quality is through varying the data, e.g.,
gram, and unigram trees. We combined these trees using different folds of the training data (see Sec-
using either Eq. 8 or Eq. 9. The A parameters in tion 2.1).
Eq. 8 were estimated using EM by maximizing like- Let us take a closer look at the effect of differ-
lihood of a heldout set (we utilized 4-way cross- ent types of randomization on individual trees and
validation); whereas, the parameters in Eq. 9 were their combinations. In the first set of experiments,
estimated using L-BFGS because the denominator we compare the performance of a single undegraded
in Eq. 9 makes the maximization step problematic. fourgram tree9 with forests of fourgram trees grown
4.2 Random Forest randomly with Bernoulli trials. Having only same-
(Xu, 2005) evaluated a variety of randomization order trees in a forest allows us to apply interpola-
techniques that can be used to build trees. He used tion of Eq. 10 (used in (Xu, 2005)) and compare
a word-only model, with questions constructed us- with the interpolation method presented in Eq. 9. By
ing the Exchange algorithm, similar to our model. comparing forests of different sizes with the baseline
He tried two methods of randomization: selecting from Table 1, we are able to evaluate the effect of
the positions in the history for question construction randomization in decision tree growing and assess
by a Bernoulli trials6, and random initialization of the importance of the lower order trees.
the Exchange algorithm. He found that when the The results are shown in Table 2. Note that, while
Exchange algorithm was initialized randomly, the an undegraded syntactic tree is better than the word
Bernoulli trial parameter did not matter; however, tree, the situation is reversed when the trees are
when the Exchange algorithm was initialized deter- grown randomly. This can be explained by the fact
ministically; lower values for the Bernoulli trial pa- that the joint model has a much higher dimensional-
rameter r yielded better overall forest performance. ity of the context space, and therefore is much more
We implemented a similar method, namely, initial- sensitive to the clustering method.
izing the Exchange algorithm randomly and using As we increase the number of random trees in the
r = 0.1 for Bernoulli trials7. forest, the perplexity decreases as expected, with the
There is a key difference between the two ran- interpolation method of Eq. 9 showing improvement
of a few percentile points over Eq. 10. Note that
in the case of the word-tree model, it takes 4 ran-
dom decision trees to reach the performance of a sin-
gle undegraded tree, while in the joint model, even
</bodyText>
<table confidence="0.994032764705882">
5This section was not used for training the parser or for the
LM training.
6In this method, positions in the history are ignored with
probability 1 − r, where r is the Bernoulli trials parameter.
7Note that because in the joint model, the question about
tags are deterministic, we use a lower value of r than (Xu, 2005)
to increase randomness.
8Here and henceforth, by “undegraded” we mean “accord-
ing to the algorithm described in Section 2.”
9Since each tree has a smooth distribution based on Eq. 5,
lower order trees are not strictly required.
696
Eq. 8 Eq. 9 (generalized)
order n-gram word-tree syntactic word-tree syntactic
2-gram 261.0 257.8 214.3 258.1 214.6
3-gram 174.3 (33.2%) 168.7 (34.6%) 156.8 (26.8%) 168.4 (34.8%) 155.3 (27.6%)
4-gram 161.7 (7.2%) 164.0 (2.8%) 156.5 (0.2%) 155.7 (7.5%) 147.1 (5.3%)
</table>
<tableCaption confidence="0.994271">
Table 1: Perplexity results on PTB WSJ section 23. Percentage numbers in parentheses denote the reduction of
perplexity relative to the lower order model of the same type.
</tableCaption>
<table confidence="0.999151222222222">
word-tree syntactic
Eq. 10 Eq. 9 Eq. 10 Eq. 9
1 x undgr 204.9 189.1
1 x rnd 250.2 289.9
2 x rnd 229.5 221.5 244.6 240.9
3 x rnd 227.5 214.5 226.2 220.0
4 x rnd 219.5 205.0 219.5 212.2
5 x rnd 200.9 184.1 216.5 209.0
baseline N/A 155.7 N/A 147.1
</table>
<tableCaption confidence="0.8099195">
Table 2: Perplexity numbers obtained using fourgram
trees only. Note that “undgr” and “rnd” denote unde-
</tableCaption>
<bodyText confidence="0.93150872">
graded and randomly grown trees with Bernoulli trials,
respectively, and the number indicates the number of
trees in the forest. Also “baseline” refers to the fourgram
models with lower order trees (from Table 1, Eq. 9).
5 trees are much worse than a single decision tree
constructed without randomization. Finally, com-
pare the performance of single undegraded fourgram
trees in Table 2 with fourgram models in Table 1,
which are constructed with lower order trees: both
word-tree and joint models in Table 1 have over
20% lower perplexity compared to the correspond-
ing models consisting of a single fourgram tree.
In Table 3, we evaluate forests of fourgram trees
produced using randomizations without degrading
the tree construction algorithm. That is, we use ran-
dom initializations of the Exchange algorithm and,
additionally, variations in the training data fold. All
forests in this table use the interpolation method
of Eq. 9. Note that, while these perplexity num-
bers are substantially better than trees produced with
Bernoulli trials in Table 2, they are still significantly
worse than the baseline model from Table 1.
These results suggest that, while it is beneficial
to combine different decision trees, we should in-
troduce differences to the tree construction process
</bodyText>
<table confidence="0.998848125">
word-tree syntactic
# trees Exchng. +data Exchng. +data
1 204.9 189.1
2 185.9 186.5 174.5 173.7
3 179.5 179.9 168.8 167.2
4 176.2 176.4 165.1 164.0
5 173.7 172.0 163.0 162.0
baseline 155.7 147.1
</table>
<tableCaption confidence="0.933027571428571">
Table 3: Perplexity numbers obtained using fourgram
trees produced using random initialization of the Ex-
change algorithm (Exchng. columns) and, additionally,
variations in training data folds (+data columns). Note
that “baseline” refers to the fourgram models with lower
order trees (from Table 1). All models use the interpola-
tion method of Eq. 9.
</tableCaption>
<bodyText confidence="0.99961775">
without degrading the trees when introducing ran-
domness, especially for joint models. In addition,
lower order trees seem to play an important role for
high quality model combination.
</bodyText>
<subsectionHeader confidence="0.992783">
4.3 Context-Restricted Forest
</subsectionHeader>
<bodyText confidence="0.99999625">
As we have mentioned above, combining higher and
lower order decision trees produces much better re-
sults. A lower order decision tree is grown from
a lower order context space, i.e., the context space
where we purposely ignore some attributes. Note
that in this case, rather than randomly ignoring con-
texts via Bernoulli trials at every node in the decision
tree, we discard some context attributes upfront in
a principled manner (i.e., most distant context) and
then grow the decision tree without degradation.
Since the joint model, having more context at-
tributes, affords a larger variety of different contexts,
we use this model in the remaining experiments.
In Table 4, we present the perplexity numbers for
our standard model with additional trees. We de-
note context-restricted trees by their Markovian or-
</bodyText>
<page confidence="0.993137">
697
</page>
<table confidence="0.990953272727273">
Model size PPL
1w1t + 2w2t + 3w3t + 4w4t (*) 294MB 147.1
(*) + 4w3t + 3w2t 579MB 143.5
(*) + 4w3t + 3w4t 587MB 144.9
(*) + 4w3t + 3w4t + 3w2t + 2w3t 699MB 140.7
(*) + 1 x bernoulli-rnd 464MB 149.7
(*) + 2 x bernoulli-rnd 632MB 150.4
(*) + 3 x bernoulli-rnd 804MB 151.1
(*) + 1 x data-rnd 484MB 147.0
(*) + 2 x data-rnd 673MB 145.0
(*) + 3 x data-rnd 864MB 145.2
</table>
<tableCaption confidence="0.928799666666667">
Table 4: Perplexity results using the standard syntactic
model with additional trees. “bernoulli-rnd” and “data-
rnd” indicate fourgram trees randomized using Bernoulli
trials and varying training data, respectively. The second
column shows the combined size of decision trees in the
forest.
</tableCaption>
<bodyText confidence="0.99982065">
ders (words w and tags t independently), so 3w2t
indicates a decision tree implementing the probabil-
ity function: p(witi|wi−1wi−2ti−1). The fourgram
joint model presented in Table 1 has four trees and
is labeled with the formula “1w1t + 2w2t + 3w3t +
4w4t” in Table 4. The randomly grown trees (de-
noted “bernoulli-rnd”) are grown utilizing the full
context 4w4t using the methods described in Sec-
tion 4.2. All models utilize the generalized interpo-
lation method described in Section 3.2.
As can be seen in Table 4, adding undegraded
trees consistently improves the performance of an
already strong baseline, while adding random trees
only increases the perplexity because their quality
is worse than undegraded trees’. Trees produced
by data randomization (denoted “data-rnd”) also im-
prove the performance of the model; however, the
improvement is not greater than that of additional
lower order trees, which are considerably smaller in
size.
</bodyText>
<sectionHeader confidence="0.997236" genericHeader="method">
5 ASR Rescoring Results
</sectionHeader>
<bodyText confidence="0.999766">
In order to verify that the improvements in perplex-
ity that we observe in Tables 1 and 4 are sufficient
for an impact on a task, we measure Word Error
Rate (WER) of our models on an Automatic Speech
Recognition (ASR) rescoring task using the Wall
Street Journal corpus (WSJ) for evaluation. The test
set consists of 4,088 utterances of WSJ0. We opti-
</bodyText>
<table confidence="0.9739166">
Model PPL WER
n-gram 161.7 7.81%
1w1t + 2w2t + 3w3t + 4w4t (Eq.8) 156.5 7.57%
1w1t + 2w2t + 3w3t + 4w4t (*) 147.1 7.32%
(*) + 4w3t + 3w4t + 3w2t + 2w3t 140.7 7.20%
</table>
<tableCaption confidence="0.993927666666667">
Table 5: Perplexity and WER results. Note that the last
two rows are syntactic models using the interpolation
method of Eq. 9.
</tableCaption>
<bodyText confidence="0.99974225">
mized the weights for the combination of acoustic
and language model scores on a separate develop-
ment set comprised of 1,243 utterances from Hub2
5k closed vocabulary and the WSJ1 5k open vocab-
ulary sets.
The ASR system used to produce lattices is based
on the 2007 IBM Speech transcription system for the
GALE Distillation Go/No-go Evaluation (Chen et
al., 2006). The acoustic models are state-of-the-art
discriminatively trained models which are trained on
Broadcast News (BN) Hub4 acoustic training data.
Lattices were produced using a trigram LM trained
on the same data as the models we evaluate, then
1,000 best unique hypotheses were extracted from
the lattices. WER of the 1-best hypothesis on the
test set is 8.07% and the oracle WER is 3.54%.
In Table 5, we present WER results along with
the corresponding perplexity numbers from Ta-
bles 1 and 4 for our lowest perplexity syntactic
model, as well as the baselines (modified KN n-gram
model and standard decision tree models using in-
terpolation methods of Eq. 8 and Eq. 9). The in-
terpolation method of Eq. 9 substantially improves
performance over the interpolation method of Eq. 8,
reducing WER by 0.25% absolute (p &lt; 10−5).
Adding four trees utilizing context restricted in dif-
ferent ways further reduces WER by 0.12%, which
is also a statistically significant (p &lt; 0.025) im-
provement over the baseline models labeled (*). Al-
together, the improvements over the n-gram baseline
add up to 0.61% absolute (8% relative) WER reduc-
tion.
</bodyText>
<sectionHeader confidence="0.99968" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998801">
In this paper, we investigate various aspects of com-
bining multiple decision trees in a single language
model. We observe that the generalized interpola-
</bodyText>
<page confidence="0.99566">
698
</page>
<bodyText confidence="0.99995588">
tion (Eq. 9) for decision tree models proposed in
(Filimonov and Harper, 2011) is in fact a forest in-
terpolation method rather than a backoff interpola-
tion because, in Eq. 9, models do not have explicit
higher-lower order relation as they do in backoff in-
terpolation (Eq. 6). Thus, in this paper we investi-
gate the question of how to construct decision trees
so that their combination results in improved per-
formance (under the assumption that computational
tractability allows only a handful of decision trees
in a forest). We compare various techniques for
producing forests of trees and observe that methods
that diversify trees by introducing random degrada-
tion of the tree construction algorithm perform more
poorly (especially with joint models) than methods
in which the trees are constructed without degrada-
tion and with variability being introduced via param-
eters that are inherently arbitrary (e.g., training data
fold differences or initializations of greedy search
algorithms). Additionally, we observe that simply
restricting the context used to construct trees in dif-
ferent ways, not only produces smaller trees (be-
cause of the context reduction), but the resulting
variations in trees also produce forests that are at
least as good as forests of larger trees.
</bodyText>
<sectionHeader confidence="0.999308" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999535">
We would like to thank Ariya Rastrow for providing
word lattices for the ASR rescoring experiments.
</bodyText>
<sectionHeader confidence="0.999248" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9990236">
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and
Robert L. Mercer. 1990. A tree-based statistical lan-
guage model for natural language speech recognition.
Readings in speech recognition, pages 507–514.
Srinivas Bangalore. 1996. ‘Almost parsing’ technique
for language modeling. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 1173–1176.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. In
Proceedings of HLT/NAACL, pages 4–6.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling for speech recognition. CoRR.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proceedings of the 34th annual meeting on As-
sociation for Computational Linguistics, pages 310–
318.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596–1608.
Denis Filimonov and Mary Harper. 2009. A joint lan-
guage model with fine-grain syntactic tags. In Pro-
ceedings of the EMNLP 2009.
Denis Filimonov and Mary Harper. 2011. Generalized
interpolation in decision tree LM. In Proceedings of
the 49st Annual Meeting of the Association for Com-
putational Linguistics.
Peter Heeman. 1998. POS tagging versus classes in lan-
guage modeling. In Sixth Workshop on Very Large
Corpora.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In Proceedings of the EMNLP 2009.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice, pages 381–397.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering. In
Speech Communication, pages 1253–1256.
J. R. Quinlan. 1986. Induction of decision trees. Ma-
chine Learning, 1(1):81–106.
Ronald Rosenfeld, Jaime Carbonell, and Alexander Rud-
nicky. 1994. Adaptive statistical language modeling:
A maximum entropy approach. Technical report.
Peng Xu. 2005. Random Forests and Data Sparseness
Problem in Language Modeling. Ph.D. thesis, Balti-
more, Maryland, April.
Imed Zitouni. 2007. Backoff hierarchical class n-
gram language models: effectiveness to model unseen
events in speech recognition. Computer Speech &amp;
Language, 21(1):88–104.
</reference>
<page confidence="0.998837">
699
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.206451">
<title confidence="0.8729805">Syntactic Decision Tree LMs: Random Selection or Intelligent Design? Language</title>
<affiliation confidence="0.851902">Center of</affiliation>
<author confidence="0.490812">Johns Hopkins</author>
<email confidence="0.996935">den@cs.umd.edu</email>
<affiliation confidence="0.997622">of Computer University of Maryland, College</affiliation>
<email confidence="0.99981">mharper@umd.edu</email>
<abstract confidence="0.987775666666667">Decision trees have been applied to a variety of NLP tasks, including language modeling, for their ability to handle a variety of attributes and sparse context space. Moreover, forests (collections of decision trees) have been shown to substantially outperform individual decision trees. In this work, we investigate methods for combining trees in a forest, as well as methods for diversifying trees for the task of syntactic language modeling. We show that our tree interpolation technique outperforms the standard method used in the literature, and that, on this particular task, restricting tree contexts in a principled way produces smaller and better forests, with the best achieving an 8% relative reduction in Word Error Rate over an n-gram baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Peter F Brown</author>
<author>Peter V de Souza</author>
<author>Robert L Mercer</author>
</authors>
<title>A tree-based statistical language model for natural language speech recognition. Readings in speech recognition,</title>
<date>1990</date>
<pages>507--514</pages>
<marker>Bahl, Brown, de Souza, Mercer, 1990</marker>
<rawString>Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and Robert L. Mercer. 1990. A tree-based statistical language model for natural language speech recognition. Readings in speech recognition, pages 507–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
</authors>
<title>Almost parsing’ technique for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>1173--1176</pages>
<contexts>
<context position="2546" citStr="Bangalore, 1996" startWordPosition="397" endWordPosition="398">+1 in the training data. The context space is still far too large1; therefore, the models are recursively smoothed using lower order distributions. For instance, in a widely used n-gram LM, the probabilities are estimated as follows: ˜p(wi|wi−1 i−n+1) = ρ(wi|wi−1 i−n+1) + (2) i−1 / γ( wi−n+1) - Awi|wi−1 i−n+2) where ρ is a discounted probability2. Note that this type of model is a simple Markov chain lacking any notion of syntax. It is widely accepted that languages do have some structure. Moreover, it has been shown that incorporating syntax into a language model can improve its performance (Bangalore, 1996; Heeman, 1998; Chelba and Jelinek, 2000; Filimonov and Harper, 2009). A straightforward way of incorporating syntax into a language model is by assigning a tag to each word and modeling them jointly; then to obtain the proba1O(|V |&amp;quot;−1) in n-gram model with typical order n = 3 ... 5, and a vocabulary size of IV I = 104 ... 106. 2We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models. p(wm1 ) = �m i=1 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 691–699, Edinburgh, Scotland, UK, July 27–31, 2011.</context>
</contexts>
<marker>Bangalore, 1996</marker>
<rawString>Srinivas Bangalore. 1996. ‘Almost parsing’ technique for language modeling. In Proceedings of the International Conference on Spoken Language Processing, volume 2, pages 1173–1176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>4--6</pages>
<contexts>
<context position="5170" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="833" endWordPosition="837">-gram models for decision tree LMs and briefly describe a generalized interpolation for such models. The generalized interpolation method allows the addition of any number of trees to the model, and thus raises the question: what is the best way to create diverse decision trees so that their combination results in a stronger model, while at the same time keeping the total number of trees in the model relatively low for computational practicality. In Section 4, we explore and evaluate a variety 3For example, morphological features can be very helpful for modeling highly inflectional languages (Bilmes and Kirchhoff, 2003). of methods for creating different trees. To support our findings, we evaluate several of the models on an ASR rescoring task in Section 5. Finally, we discuss our findings in Section 6. 2 Joint Syntactic Decision Tree LM A decision tree provides us with a clustering function Φ(wi−1 i−n+1ti−1 i−n+1) → {Φ1, ... , ΦN}, where N is the number of clusters, and clusters Φk are disjoint subsets of the context space. The probability estimation for a joint decision tree model is approximated as follows: i−1 i−1 i−1 i−1 p(witi|wi−n+1ti−n+1) ≈ p(witi|Φ(wi−n+1ti−n+1)) (4) In the remainder of this section</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff Bilmes and Katrin Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Proceedings of HLT/NAACL, pages 4–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling for speech recognition.</title>
<date>2000</date>
<journal>CoRR.</journal>
<contexts>
<context position="2586" citStr="Chelba and Jelinek, 2000" startWordPosition="401" endWordPosition="404">ntext space is still far too large1; therefore, the models are recursively smoothed using lower order distributions. For instance, in a widely used n-gram LM, the probabilities are estimated as follows: ˜p(wi|wi−1 i−n+1) = ρ(wi|wi−1 i−n+1) + (2) i−1 / γ( wi−n+1) - Awi|wi−1 i−n+2) where ρ is a discounted probability2. Note that this type of model is a simple Markov chain lacking any notion of syntax. It is widely accepted that languages do have some structure. Moreover, it has been shown that incorporating syntax into a language model can improve its performance (Bangalore, 1996; Heeman, 1998; Chelba and Jelinek, 2000; Filimonov and Harper, 2009). A straightforward way of incorporating syntax into a language model is by assigning a tag to each word and modeling them jointly; then to obtain the proba1O(|V |&amp;quot;−1) in n-gram model with typical order n = 3 ... 5, and a vocabulary size of IV I = 104 ... 106. 2We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models. p(wm1 ) = �m i=1 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 691–699, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Li</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling for speech recognition. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="2924" citStr="Chen and Goodman, 1996" startWordPosition="465" endWordPosition="468">of model is a simple Markov chain lacking any notion of syntax. It is widely accepted that languages do have some structure. Moreover, it has been shown that incorporating syntax into a language model can improve its performance (Bangalore, 1996; Heeman, 1998; Chelba and Jelinek, 2000; Filimonov and Harper, 2009). A straightforward way of incorporating syntax into a language model is by assigning a tag to each word and modeling them jointly; then to obtain the proba1O(|V |&amp;quot;−1) in n-gram model with typical order n = 3 ... 5, and a vocabulary size of IV I = 104 ... 106. 2We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models. p(wm1 ) = �m i=1 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 691–699, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics bility of a word sequence, the tags must be marginalized out: An independence assumption similar to Eq. 1 can be made: p( Z i1 i—i i−1) N p( Z it i−1 ) () w•t• w1 t1 w•t• wi−n+1ti−1 i−n+1 3 A primary goal of our research is to build strong syntactic language models and provide effective methods for constructing them to the res</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 310– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>B Kingsbury</author>
<author>L Mangu</author>
<author>D Povey</author>
<author>G Saon</author>
<author>H Soltau</author>
<author>G Zweig</author>
</authors>
<date>2006</date>
<booktitle>Advances in speech transcription at IBM under the DARPA EARS program. IEEE Transactions on Audio, Speech and Language Processing,</booktitle>
<pages>1596--1608</pages>
<contexts>
<context position="28688" citStr="Chen et al., 2006" startWordPosition="4856" endWordPosition="4859">t + 2w2t + 3w3t + 4w4t (Eq.8) 156.5 7.57% 1w1t + 2w2t + 3w3t + 4w4t (*) 147.1 7.32% (*) + 4w3t + 3w4t + 3w2t + 2w3t 140.7 7.20% Table 5: Perplexity and WER results. Note that the last two rows are syntactic models using the interpolation method of Eq. 9. mized the weights for the combination of acoustic and language model scores on a separate development set comprised of 1,243 utterances from Hub2 5k closed vocabulary and the WSJ1 5k open vocabulary sets. The ASR system used to produce lattices is based on the 2007 IBM Speech transcription system for the GALE Distillation Go/No-go Evaluation (Chen et al., 2006). The acoustic models are state-of-the-art discriminatively trained models which are trained on Broadcast News (BN) Hub4 acoustic training data. Lattices were produced using a trigram LM trained on the same data as the models we evaluate, then 1,000 best unique hypotheses were extracted from the lattices. WER of the 1-best hypothesis on the test set is 8.07% and the oracle WER is 3.54%. In Table 5, we present WER results along with the corresponding perplexity numbers from Tables 1 and 4 for our lowest perplexity syntactic model, as well as the baselines (modified KN n-gram model and standard </context>
</contexts>
<marker>Chen, Kingsbury, Mangu, Povey, Saon, Soltau, Zweig, 2006</marker>
<rawString>S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon, H. Soltau, and G. Zweig. 2006. Advances in speech transcription at IBM under the DARPA EARS program. IEEE Transactions on Audio, Speech and Language Processing, pages 1596–1608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Filimonov</author>
<author>Mary Harper</author>
</authors>
<title>A joint language model with fine-grain syntactic tags.</title>
<date>2009</date>
<booktitle>In Proceedings of the EMNLP</booktitle>
<contexts>
<context position="2615" citStr="Filimonov and Harper, 2009" startWordPosition="405" endWordPosition="408">oo large1; therefore, the models are recursively smoothed using lower order distributions. For instance, in a widely used n-gram LM, the probabilities are estimated as follows: ˜p(wi|wi−1 i−n+1) = ρ(wi|wi−1 i−n+1) + (2) i−1 / γ( wi−n+1) - Awi|wi−1 i−n+2) where ρ is a discounted probability2. Note that this type of model is a simple Markov chain lacking any notion of syntax. It is widely accepted that languages do have some structure. Moreover, it has been shown that incorporating syntax into a language model can improve its performance (Bangalore, 1996; Heeman, 1998; Chelba and Jelinek, 2000; Filimonov and Harper, 2009). A straightforward way of incorporating syntax into a language model is by assigning a tag to each word and modeling them jointly; then to obtain the proba1O(|V |&amp;quot;−1) in n-gram model with typical order n = 3 ... 5, and a vocabulary size of IV I = 104 ... 106. 2We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models. p(wm1 ) = �m i=1 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 691–699, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics bility of a word se</context>
<context position="8803" citStr="Filimonov and Harper, 2009" startWordPosition="1479" endWordPosition="1482">ags in the form of a binary tree. This is done beforehand, using the Minimum Discriminating Information algorithm (Zitouni, 2007) with the entire training data set. In this tree, each leaf is an individual tag and each internal node is associated with the subset of tags that the node dominates. Questions about tags are constructed in the form q(x, k) ~ ti+x E Tk, where k is a node in the tag tree and Tk is the subset of tags associated with that node. The rationale behind constructing tag questions in this form is that it enables a more efficient decoding algorithm than standard HMM decoding (Filimonov and Harper, 2009). Questions are evaluated in two steps. First the context attribute x is selected using a metric similar to information gain ratio proposed by (Quinlan, 1986): H(wi) − H(wijx) M = 1 −H(x) where x is one of the context attributes, e.g., wi−2 or ti−1. Then, among the questions about attribute Figure 1: A fragment of the decision tree with a backoff node. S U S¯ is the set of words observed in the training data at the node A. To account for unseen words, we add the backoff node B. x, we select the question that maximizes the entropy reduction. Instead of dedicating an explicit heldout set for the</context>
</contexts>
<marker>Filimonov, Harper, 2009</marker>
<rawString>Denis Filimonov and Mary Harper. 2009. A joint language model with fine-grain syntactic tags. In Proceedings of the EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Filimonov</author>
<author>Mary Harper</author>
</authors>
<title>Generalized interpolation in decision tree LM.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14607" citStr="Filimonov and Harper, 2011" startWordPosition="2486" endWordPosition="2489">r n-gram models (Jelinek and Mercer, 1980) has been applied to decision tree models: 694 ˜pn(wi|wi−1 i−n+1) = λn(φn) · pn(wi|φn) + (8) (1 − λn(φn)) · ˜pn−1(wi|wi−1 i−n+2) �M p(wi|wi−1 i−n+1) = M 1 m=1 pm(wi|wi−1 i−n+1) (10) where φn ≡ Φn(wi−1 i−n+1), and λn(φn) ∈ [0, 1] are assigned to each cluster and are optimized on a heldout set using EM. pn(wi|φn) is the probability distribution at the cluster φn in the tree of order n. This interpolation method is particularly useful as, unlike count-based discounting methods (e.g., KneserNey), it can be applied to already smoothed distributions pn. In (Filimonov and Harper, 2011), we observed that because of the violation of Property 7 in decision tree models, the interpolation method of Eq. 8 is not appropriate for such models. Instead we proposed the following generalized form of linear interpolation: pn(wi |wi−n+1) Enm=1Em1 (φm pm (wi |φm) (9) Note that the recursive interpolation of Eq. 8 can be represented in this form with the additional constraint En m=1 λm(φm) = 1, which is not required in the generalized interpolation of Eq. 9; thus, the generalized interpolation, albeit having the same number of parameters, has more degrees of freedom. We also showed that th</context>
<context position="30064" citStr="Filimonov and Harper, 2011" startWordPosition="5083" endWordPosition="5086">nterpolation method of Eq. 8, reducing WER by 0.25% absolute (p &lt; 10−5). Adding four trees utilizing context restricted in different ways further reduces WER by 0.12%, which is also a statistically significant (p &lt; 0.025) improvement over the baseline models labeled (*). Altogether, the improvements over the n-gram baseline add up to 0.61% absolute (8% relative) WER reduction. 6 Conclusion In this paper, we investigate various aspects of combining multiple decision trees in a single language model. We observe that the generalized interpola698 tion (Eq. 9) for decision tree models proposed in (Filimonov and Harper, 2011) is in fact a forest interpolation method rather than a backoff interpolation because, in Eq. 9, models do not have explicit higher-lower order relation as they do in backoff interpolation (Eq. 6). Thus, in this paper we investigate the question of how to construct decision trees so that their combination results in improved performance (under the assumption that computational tractability allows only a handful of decision trees in a forest). We compare various techniques for producing forests of trees and observe that methods that diversify trees by introducing random degradation of the tree </context>
</contexts>
<marker>Filimonov, Harper, 2011</marker>
<rawString>Denis Filimonov and Mary Harper. 2011. Generalized interpolation in decision tree LM. In Proceedings of the 49st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Heeman</author>
</authors>
<title>POS tagging versus classes in language modeling.</title>
<date>1998</date>
<booktitle>In Sixth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="2560" citStr="Heeman, 1998" startWordPosition="399" endWordPosition="400">g data. The context space is still far too large1; therefore, the models are recursively smoothed using lower order distributions. For instance, in a widely used n-gram LM, the probabilities are estimated as follows: ˜p(wi|wi−1 i−n+1) = ρ(wi|wi−1 i−n+1) + (2) i−1 / γ( wi−n+1) - Awi|wi−1 i−n+2) where ρ is a discounted probability2. Note that this type of model is a simple Markov chain lacking any notion of syntax. It is widely accepted that languages do have some structure. Moreover, it has been shown that incorporating syntax into a language model can improve its performance (Bangalore, 1996; Heeman, 1998; Chelba and Jelinek, 2000; Filimonov and Harper, 2009). A straightforward way of incorporating syntax into a language model is by assigning a tag to each word and modeling them jointly; then to obtain the proba1O(|V |&amp;quot;−1) in n-gram model with typical order n = 3 ... 5, and a vocabulary size of IV I = 104 ... 106. 2We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models. p(wm1 ) = �m i=1 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 691–699, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Associ</context>
<context position="10217" citStr="Heeman, 1998" startWordPosition="1725" endWordPosition="1726">Note that the tree induction algorithm can also be used to construct trees without tags: p(wijwi−1 i−n+1) ~ p(wijΦ(wi−1 i−n+1)) We refer to this model as the word-tree model. By comparing syntactic and word-tree models, we are able to separate the effects of decision tree modeling and syntactic information on language modeling by comparing both models to an n-gram baseline. 2.2 In-tree Smoothing A decision tree offers a hierarchy of clusterings that can be exploited for smoothing. We can interpolate the observed distributions at leaves recursively with their parents, as in (Bahl et al., 1990; Heeman, 1998): ˜pk(witi) = λkpML(witi) + (1 − λk)˜pk (witi) (5) where pML is the observed distribution at node k and k0 is the parent of k. The coefficients λk are estimated using an EM algorithm. We can also combine p(witijΦ(wi−1 i−n+1ti−1 i−n+1)) with lower order decision trees, i.e., wi−2ES A wi−2E no S yes no yes B Backoff leaf j(x; wi) = 1 −H(x) 693 p(witi|4(wi−1 i−n+2ti−1 i−n+2)), and so on up until p(witi) which is a one-node tree (essentially a unigram model). Although superficially similar to backoff in n-gram models, lower order decision trees differ substantially from lower order n-gram models a</context>
</contexts>
<marker>Heeman, 1998</marker>
<rawString>Peter Heeman. 1998. POS tagging versus classes in language modeling. In Sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>SelfTraining PCFG grammars with latent annotations across languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the EMNLP</booktitle>
<contexts>
<context position="17964" citStr="Huang and Harper, 2009" startWordPosition="3067" endWordPosition="3070">speech-like form, namely numbers and abbreviations were verbalized, text was downcased, punctuation was removed, and contractions and possessives were joined with the previous word (i.e., they ’ll becomes they’ll). For the syntactic modeling, we used tags comprised of the POS tags of the word and it’s head. Parsing of the text for tag extraction occurred after verbalization of numbers and abbreviations but 4Note that (Xu, 2005) used lower order models to estimate Pm. 695 before any further processing; we used a latent vari- domization methods. Since we do not have an a able PCFG parser as in (Huang and Harper, 2009). priori preference for choosing initializations for the For reference, we include an n-gram model with Exchange algorithm, by using random initializations modified interpolated KN discounting. All mod- it is hoped that due to the greedy nature of the alels use the same vocabulary of approximately 50k gorithm, the constructed trees, while being “undewords. graded,”8 will be sufficiently different so that their Perplexity numbers reported in Tables 1, 2, 3, combination improves over an individual tree. By and 4 are computed on WSJ section 23 (tokenized introducing Bernoulli trials, on the other</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2009. SelfTraining PCFG grammars with latent annotations across languages. In Proceedings of the EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>Interpolated estimation of markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Proceedings of the Workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<contexts>
<context position="14022" citStr="Jelinek and Mercer, 1980" startWordPosition="2383" endWordPosition="2386"> decision trees) leads to a violation of Property 7, which is likely to produce a degradation in performance if backoff interpolation Eq. 6 is used. context space context space Contexts from the same Φ n belong to different BOn-1 BOn−1 Φn ∀wi−1 1,W : W ∈ 4n(wi1)⇒W ∈ BOn−1(wi 1) (7) that is, every word sequence W that belongs to a context cluster 4n(wi−1 1 ), belongs to the same backoff cluster BOn−1(wi−1 1 ) (hence has the same backoff distribution). For n-gram models, Property 7 3.2 Generalized Interpolation Recursive linear interpolation similar to JelinekMercer smoothing for n-gram models (Jelinek and Mercer, 1980) has been applied to decision tree models: 694 ˜pn(wi|wi−1 i−n+1) = λn(φn) · pn(wi|φn) + (8) (1 − λn(φn)) · ˜pn−1(wi|wi−1 i−n+2) �M p(wi|wi−1 i−n+1) = M 1 m=1 pm(wi|wi−1 i−n+1) (10) where φn ≡ Φn(wi−1 i−n+1), and λn(φn) ∈ [0, 1] are assigned to each cluster and are optimized on a heldout set using EM. pn(wi|φn) is the probability distribution at the cluster φn in the tree of order n. This interpolation method is particularly useful as, unlike count-based discounting methods (e.g., KneserNey), it can be applied to already smoothed distributions pn. In (Filimonov and Harper, 2011), we observed t</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Frederick Jelinek and Robert L. Mercer. 1980. Interpolated estimation of markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, pages 381–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Martin</author>
<author>Jorg Liermann</author>
<author>Hermann Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering. In Speech Communication,</title>
<date>1998</date>
<pages>1253--1256</pages>
<contexts>
<context position="7342" citStr="Martin et al., 1998" startWordPosition="1224" endWordPosition="1227">re each question partitions the context on one attribute, e.g., wi−2 or ti−1. The questions about words and tags are constructed differently: • The questions q about the words are in the form q(x) ≡ wi+x E S, where x is an integer between −n + 1 and −1, and S C V is a subset of the word vocabulary V . To construct the set S, we take the set of words So observed at the offset x in the training data associated with the � �p(wm1 tm1 ) = �m p(witi|wi−1 p(wm 1 ) = t1...t— i=1 1 ti−1 t1 ... t— 1 ) 692 current node and split it into two complementary subsets S U S¯ = So using the Exchange algorithm (Martin et al., 1998). Because the algorithm is greedy and depends on the initialization, we construct 4 questions per word position using different random initializations of the Exchange algorithm. Since we need to account for words that were not observed in the training data, we utilize the structure depicted in Figure 1. To estimate the probability at the backoff node (B in Figure 1), we can either use the probability from its grandparent node A or estimate it using a lower order tree (see Section 3), or combine the two. We have observed no noticeable difference between these methods, which suggests that only a</context>
</contexts>
<marker>Martin, Liermann, Ney, 1998</marker>
<rawString>Sven Martin, Jorg Liermann, and Hermann Ney. 1998. Algorithms for bigram and trigram word clustering. In Speech Communication, pages 1253–1256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Induction of decision trees.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="8961" citStr="Quinlan, 1986" startWordPosition="1507" endWordPosition="1508">s tree, each leaf is an individual tag and each internal node is associated with the subset of tags that the node dominates. Questions about tags are constructed in the form q(x, k) ~ ti+x E Tk, where k is a node in the tag tree and Tk is the subset of tags associated with that node. The rationale behind constructing tag questions in this form is that it enables a more efficient decoding algorithm than standard HMM decoding (Filimonov and Harper, 2009). Questions are evaluated in two steps. First the context attribute x is selected using a metric similar to information gain ratio proposed by (Quinlan, 1986): H(wi) − H(wijx) M = 1 −H(x) where x is one of the context attributes, e.g., wi−2 or ti−1. Then, among the questions about attribute Figure 1: A fragment of the decision tree with a backoff node. S U S¯ is the set of words observed in the training data at the node A. To account for unseen words, we add the backoff node B. x, we select the question that maximizes the entropy reduction. Instead of dedicating an explicit heldout set for the stopping criterion, we utilize a technique similar to cross validation: the training data set is partitioned into four folds, and the best question is requir</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>J. R. Quinlan. 1986. Induction of decision trees. Machine Learning, 1(1):81–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
<author>Jaime Carbonell</author>
<author>Alexander Rudnicky</author>
</authors>
<title>Adaptive statistical language modeling: A maximum entropy approach.</title>
<date>1994</date>
<tech>Technical report.</tech>
<contexts>
<context position="4053" citStr="Rosenfeld et al., 1994" startWordPosition="655" endWordPosition="658">trong syntactic language models and provide effective methods for constructing them to the research community. Note that the tags in the context of the joint model in Eq. 3 exacerbate the already sparse problem in Eq. 1, which makes the probability estimation particularly challenging. We utilize decision trees for joint syntactic language models to cluster context because of their strengths (reliance on information theoretic metrics to cluster context in the face of extreme sparsity and the ability to incorporate attributes of different types3), and at the same time, unlike log-linear models (Rosenfeld et al., 1994), computationally expensive probability normalization does not have to be postponed until runtime. In Section 2, we describe the details of the syntactic decision tree LM. Construction of a single-tree model is difficult due to the inevitable greediness of the tree construction process and its tendency to overfit the data. This problem is often addressed by interpolating with lower order decision trees. In Section 3, we point out the inappropriateness of backoff methods borrowed from n-gram models for decision tree LMs and briefly describe a generalized interpolation for such models. The gener</context>
</contexts>
<marker>Rosenfeld, Carbonell, Rudnicky, 1994</marker>
<rawString>Ronald Rosenfeld, Jaime Carbonell, and Alexander Rudnicky. 1994. Adaptive statistical language modeling: A maximum entropy approach. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
</authors>
<date>2005</date>
<booktitle>Random Forests and Data Sparseness Problem in Language Modeling. Ph.D. thesis,</booktitle>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="15756" citStr="Xu, 2005" startWordPosition="2689" endWordPosition="2690">ameters, has more degrees of freedom. We also showed that the recursive interpolation Eq. 8 is a special case of Eq. 9 that occurs when the Property 7 holds. 4 From Backoff Trees to Forest Note that, in Eq. 9, individual trees do not have explicit higher-lower order relations, they are treated as a collection of trees, i.e., as a forest. Naturally, to benefit from the forest model, its trees must differ in some way. Different trees can be created based on differences in the training data, differences in the tree growing algorithm, or some non-determinism in the way the trees are constructed. (Xu, 2005) used randomization techniques to produce a large forest of decision trees that were combined as follows: where M is the number of decision trees in the forest (he proposed M = 100) and pm is the m-th tree model4. Note that this type of interpolation assumes that each tree model is “equal” a priori and therefore is only appropriate when the tree models are grown in the same way (particularly, using the same order of context). Note that Eq. 10 is a special case of Eq. 9 when all parameters λ are equal. (Xu, 2005) showed that, although each individual tree is a fairly weak model, their combinati</context>
<context position="17772" citStr="Xu, 2005" startWordPosition="3034" endWordPosition="3035">hods based on Eq. 9 and Eq. 10 (when Eq. 10 is applicable). 4.1 Experimental Setup To train our models we use 35M words of WSJ 94-96 from LDC2008T13. The text was converted into speech-like form, namely numbers and abbreviations were verbalized, text was downcased, punctuation was removed, and contractions and possessives were joined with the previous word (i.e., they ’ll becomes they’ll). For the syntactic modeling, we used tags comprised of the POS tags of the word and it’s head. Parsing of the text for tag extraction occurred after verbalization of numbers and abbreviations but 4Note that (Xu, 2005) used lower order models to estimate Pm. 695 before any further processing; we used a latent vari- domization methods. Since we do not have an a able PCFG parser as in (Huang and Harper, 2009). priori preference for choosing initializations for the For reference, we include an n-gram model with Exchange algorithm, by using random initializations modified interpolated KN discounting. All mod- it is hoped that due to the greedy nature of the alels use the same vocabulary of approximately 50k gorithm, the constructed trees, while being “undewords. graded,”8 will be sufficiently different so that </context>
<context position="19995" citStr="Xu, 2005" startWordPosition="3402" endWordPosition="3404">he A parameters in tion 2.1). Eq. 8 were estimated using EM by maximizing like- Let us take a closer look at the effect of differlihood of a heldout set (we utilized 4-way cross- ent types of randomization on individual trees and validation); whereas, the parameters in Eq. 9 were their combinations. In the first set of experiments, estimated using L-BFGS because the denominator we compare the performance of a single undegraded in Eq. 9 makes the maximization step problematic. fourgram tree9 with forests of fourgram trees grown 4.2 Random Forest randomly with Bernoulli trials. Having only same(Xu, 2005) evaluated a variety of randomization order trees in a forest allows us to apply interpolatechniques that can be used to build trees. He used tion of Eq. 10 (used in (Xu, 2005)) and compare a word-only model, with questions constructed us- with the interpolation method presented in Eq. 9. By ing the Exchange algorithm, similar to our model. comparing forests of different sizes with the baseline He tried two methods of randomization: selecting from Table 1, we are able to evaluate the effect of the positions in the history for question construction randomization in decision tree growing and ass</context>
<context position="22196" citStr="Xu, 2005" startWordPosition="3774" endWordPosition="3775">key difference between the two ran- interpolation method of Eq. 9 showing improvement of a few percentile points over Eq. 10. Note that in the case of the word-tree model, it takes 4 random decision trees to reach the performance of a single undegraded tree, while in the joint model, even 5This section was not used for training the parser or for the LM training. 6In this method, positions in the history are ignored with probability 1 − r, where r is the Bernoulli trials parameter. 7Note that because in the joint model, the question about tags are deterministic, we use a lower value of r than (Xu, 2005) to increase randomness. 8Here and henceforth, by “undegraded” we mean “according to the algorithm described in Section 2.” 9Since each tree has a smooth distribution based on Eq. 5, lower order trees are not strictly required. 696 Eq. 8 Eq. 9 (generalized) order n-gram word-tree syntactic word-tree syntactic 2-gram 261.0 257.8 214.3 258.1 214.6 3-gram 174.3 (33.2%) 168.7 (34.6%) 156.8 (26.8%) 168.4 (34.8%) 155.3 (27.6%) 4-gram 161.7 (7.2%) 164.0 (2.8%) 156.5 (0.2%) 155.7 (7.5%) 147.1 (5.3%) Table 1: Perplexity results on PTB WSJ section 23. Percentage numbers in parentheses denote the reducti</context>
</contexts>
<marker>Xu, 2005</marker>
<rawString>Peng Xu. 2005. Random Forests and Data Sparseness Problem in Language Modeling. Ph.D. thesis, Baltimore, Maryland, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imed Zitouni</author>
</authors>
<title>Backoff hierarchical class ngram language models: effectiveness to model unseen events in speech recognition.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>1</issue>
<contexts>
<context position="8305" citStr="Zitouni, 2007" startWordPosition="1387" endWordPosition="1388">ode (B in Figure 1), we can either use the probability from its grandparent node A or estimate it using a lower order tree (see Section 3), or combine the two. We have observed no noticeable difference between these methods, which suggests that only a small fraction of probability is estimated from these nodes; therefore, for simplicity, we use the probability estimated at the backoff node’s grandparent. • To create questions about tags we create a hierarchical clustering of all tags in the form of a binary tree. This is done beforehand, using the Minimum Discriminating Information algorithm (Zitouni, 2007) with the entire training data set. In this tree, each leaf is an individual tag and each internal node is associated with the subset of tags that the node dominates. Questions about tags are constructed in the form q(x, k) ~ ti+x E Tk, where k is a node in the tag tree and Tk is the subset of tags associated with that node. The rationale behind constructing tag questions in this form is that it enables a more efficient decoding algorithm than standard HMM decoding (Filimonov and Harper, 2009). Questions are evaluated in two steps. First the context attribute x is selected using a metric simil</context>
</contexts>
<marker>Zitouni, 2007</marker>
<rawString>Imed Zitouni. 2007. Backoff hierarchical class ngram language models: effectiveness to model unseen events in speech recognition. Computer Speech &amp; Language, 21(1):88–104.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>