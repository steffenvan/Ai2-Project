<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000078">
<title confidence="0.998283">
Phonological Constraints and Morphological Preprocessing for
Grapheme-to-Phoneme Conversion
</title>
<author confidence="0.996274">
Vera Demberg
</author>
<affiliation confidence="0.998312">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.922239">
Edinburgh, EH8 9LW, GB
</address>
<email confidence="0.996056">
v.demberg@sms.ed.ac.uk
</email>
<author confidence="0.650691">
Helmut Schmid
</author>
<affiliation confidence="0.742996">
IMS
University of Stuttgart
</affiliation>
<address confidence="0.689049">
D-70174 Stuttgart
</address>
<email confidence="0.987847">
schmid@ims.uni-stuttgart.de
</email>
<author confidence="0.847842">
Gregor M¨ohler
</author>
<affiliation confidence="0.704576">
Speech Technologies
IBM Deutschland Entwicklung
</affiliation>
<address confidence="0.746311">
D-71072 B¨oblingen
</address>
<email confidence="0.996483">
moehler@de.ibm.com
</email>
<sectionHeader confidence="0.998567" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965277777778">
Grapheme-to-phoneme conversion (g2p) is a
core component of any text-to-speech sys-
tem. We show that adding simple syllab-
ification and stress assignment constraints,
namely ‘one nucleus per syllable’ and ‘one
main stress per word’, to a joint n-gram
model for g2p conversion leads to a dramatic
improvement in conversion accuracy.
Secondly, we assessed morphological pre-
processing for g2p conversion. While mor-
phological information has been incorpo-
rated in some past systems, its contribution
has never been quantitatively assessed for
German. We compare the relevance of mor-
phological preprocessing with respect to the
morphological segmentation method, train-
ing set size, the g2p conversion algorithm,
and two languages, English and German.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929285714286">
Grapheme-to-Phoneme conversion (g2p) is the task
of converting a word from its spelling (e.g. “Stern-
anis¨ol”, Engl: star-anise oil) to its pronunciation
(/&amp;quot;ftern?ani:s?ø:l/). Speech synthesis modules with
a g2p component are used in text-to-speech (TTS)
systems and can be be applied in spoken dialogue
systems or speech-to-speech translation systems.
</bodyText>
<subsectionHeader confidence="0.980612">
1.1 Syllabification and Stress in g2p conversion
</subsectionHeader>
<bodyText confidence="0.997384043478261">
In order to correctly synthesize a word, it is not only
necessary to convert the letters into phonemes, but
also to syllabify the word and to assign word stress.
96
The problems of word phonemization, syllabifica-
tion and word stress assignment are inter-dependent.
Information about the position of a syllable bound-
ary helps grapheme-to-phoneme conversion. (Marc-
hand and Damper, 2005) report a word error rate
(WER) reduction of approx. 5 percentage points for
English when the letter string is augmented with syl-
labification information. The same holds vice-versa:
we found that WER was reduced by 50% when run-
ning our syllabifier on phonemes instead of letters
(see Table 4). Finally, word stress is usually defined
on syllables; in languages where word stress is as-
sumed1 to partly depend on syllable weight (such as
German or Dutch), it is important to know where ex-
actly the syllable boundaries are in order to correctly
calculate syllable weight. For German, (M¨uller,
2001) show that information about stress assignment
and the position of a syllable within a word improve
g2p conversion.
</bodyText>
<subsectionHeader confidence="0.97988">
1.2 Morphological Preprocessing
</subsectionHeader>
<bodyText confidence="0.999343727272727">
It has been argued that using morphological in-
formation is important for languages where mor-
phology has an important influence on pronuncia-
tion, syllabification and word stress such as Ger-
man, Dutch, Swedish or, to a smaller extent, also
English (Sproat, 1996; M¨obius, 2001; Pounder and
Kommenda, 1986; Black et al., 1998; Taylor, 2005).
Unfortunately, these papers do not quantify the con-
tribution of morphological preprocessing in the task.
Important questions when considering the inte-
gration of a morphological component into a speech
</bodyText>
<footnote confidence="0.995632">
1This issue is controversial among linguists; for an overview
see (Jessen, 1998).
</footnote>
<note confidence="0.945272">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 96–103,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999812851851852">
synthesis system are 1) How large are the im-
provements to be gained from morphological pre-
processing? 2) Must the morphological system be
perfect or can performance improvements also be
reached with relatively simple morphological com-
ponents? and 3) How much does the benefit to
be expected from explicit morphological informa-
tion depend on the g2p algorithm? To determine
these factors, we compared morphological segmen-
tations based on manual morphological annotation
from CELEX to two rule-based systems and several
unsupervised data-based approaches. We also anal-
ysed the role of explicit morphological preprocess-
ing on data sets of different sizes and compared its
relevance with respect to a decision tree and a joint
n-gram model for g2p conversion.
The paper is structured as follows: We introduce
the g2p conversion model we used in section 2 and
explain how we implemented the phonological con-
straints in section 3. Section 4 is concerned with
the relation between morphology, word pronuncia-
tion, syllabification and word stress in German, and
presents different sources for morphological seg-
mentation. In section 5, we evaluate the contribution
of each of the components and compare our meth-
ods to state-of-the-art systems. Section 6 summa-
rizes our results.
</bodyText>
<sectionHeader confidence="0.996156" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.999925823529412">
We used a joint n-gram model for the grapheme-
to-phoneme conversion task. Models of this type
have previously been shown to yield very good g2p
conversion results (Bisani and Ney, 2002; Galescu
and Allen, 2001; Chen, 2003). Models that do not
use joint letter-phoneme states, and therefore are not
conditional on the preceding letters, but only on the
actual letter and the preceding phonemes, achieved
inferior results. Examples of such approaches using
Hidden Markov Models are (Rentzepopoulos and
Kokkinakis, 1991) (who applied the HMM to the
related task of phoneme-to-grapheme conversion),
(Taylor, 2005) and (Minker, 1996).
The g2p task is formulated as searching for the
most probable sequence of phonemes given the or-
thographic form of a word. One can think of it as a
tagging problem where each letter is tagged with a
(possibly empty) phoneme-sequence p. In our par-
ticular implementation, the model is defined as a
higher-order Hidden Markov Model, where the hid-
den states are a letter–phoneme-sequence pair hl; pi,
and the observed symbols are the letters l. The out-
put probability of a hidden state is then equal to one,
since all hidden states that do not contain the ob-
served letter are pruned directly.
The model for grapheme-to-phoneme conver-
sion uses the Viterbi algorithm to efficiently com-
pute the most probable sequence pn1 of phonemes
p1, p2, ..., Pn for a given letter sequence ln1 . The
probability of a letter–phon-seq pair depends on the
k preceding letter–phon-seq pairs. Dummy states ‘#’
are appended at both ends of each word to indicate
the word boundary and to ensure that all conditional
probabilities are well-defined.
</bodyText>
<equation confidence="0.9984044">
n+1
i−k)
P(hl;pii  |hl;pii−1
pn1 = arg max
pi i=1
</equation>
<bodyText confidence="0.998727545454545">
In an integrated model where g2p conversion, syl-
labification and word stress assignment are all per-
formed at the same time, a state additionally con-
tains a syllable boundary flag b and a stress flag a,
yielding hl; p; b; aii.
As an alternative architecture, we also designed a
modular system that comprises one component for
syllabification and one for word stress assignment.
The model for syllabification computes the most
probable sequence �bn1 of syllable boundary-tags 61,
b2, ..., bn for a given letter sequence ln1 .
</bodyText>
<equation confidence="0.995541">
n+1
i−k)
P(hl; bii  |hl; bii−1
�bn1 = arg max
bi i=1
</equation>
<bodyText confidence="0.94709225">
The stress assignment model works on syllables.
It computes the most probable sequence an1 of word
accent-tags a1, a2, ..., an for a given syllable se-
quence syln1 .
</bodyText>
<equation confidence="0.9847495">
P(hsyl; aii  |hsyl; aii−1
i−k)
</equation>
<subsectionHeader confidence="0.991614">
2.1 Smoothing
</subsectionHeader>
<bodyText confidence="0.992174428571429">
Because of major data sparseness problems, smooth-
ing is an important issue, in particular for the stress
model which is based on syllable–stress-tag pairs.
Performance varied by up to 20% in function of the
smoothing algorithm chosen. Best results were ob-
tained when using a variant of Modified Kneser-Ney
Smoothing2 (Chen and Goodman, 1996).
</bodyText>
<footnote confidence="0.923209">
2For a formal definition, see(Demberg, 2006).
</footnote>
<equation confidence="0.988086">
tn 1 = arg max
ai
n+1
i=1
</equation>
<page confidence="0.998281">
97
</page>
<subsectionHeader confidence="0.998617">
2.2 Pruning
</subsectionHeader>
<bodyText confidence="0.999979793103448">
In the g2p-model, each letter can on average map
onto one of 12 alternative phoneme-sequences.
When working with 5-grams3, there are about 125 =
250,000 state sequences. To improve time and space
efficiency, we implemented a simple pruning strat-
egy that only considers the t best states at any mo-
ment in time. With a threshold of t = 15, about 120
words are processed per minute on a 1.5GHz ma-
chine. Conversion quality is only marginally worse
than when the whole search space is calculated.
Running time for English is faster, because the av-
erage number of candidate phonemes for each let-
ter is lower. We measured running time (including
training and the actual g2p conversion in 10-fold
cross validation) for a Perl implementation of our
algorithm on the English NetTalk corpus (20,008
words) on an Intel Pentium 4, 3.0 GHz machine.
Running time was less than 1h for each of the fol-
lowing three test conditions: c1) g2p conversion
only, c2) syllabification first, then g2p conversion,
c3) simultaneous g2p conversion and syllabification,
given perfect syllable boundary input, c4) simulta-
neous g2p conversion and syllabification when cor-
rect syllabification is not available beforehand. This
is much faster than the times for Pronunciation by
Analogy (PbA) (Marchand and Damper, 2005) on
the same corpus. Marchand and Damper reported a
processing time of several hours for c4), two days
for c2) and several days for c3).
</bodyText>
<subsectionHeader confidence="0.998921">
2.3 Alignment
</subsectionHeader>
<bodyText confidence="0.999961285714286">
Our current implementation of the joint n-gram
model is not integrated with an automatic alignment
procedure. We therefore first aligned letters and
phonemes in a separate, semi-automatic step. Each
letter was aligned with zero to two phonemes and,
in the integrated model, zero or one syllable bound-
aries and stress markers.
</bodyText>
<sectionHeader confidence="0.964892" genericHeader="method">
3 Integration of Phonological Constraints
</sectionHeader>
<bodyText confidence="0.981860840909091">
When analysing the results from the model that does
g2p conversion, syllabification and stress assign-
3There is a trade-off between long context windows which
capture the context accurately and data sparseness issues. The
optimal value k for the context window size depends on the
source language (existence of multiletter graphemes, complex-
ity of syllables etc.).
ment in a single step, we found that a large propor-
tion of the errors was due to the violation of basic
phonological constraints.
Some syllables had no syllable nucleus, while
others contained several vowels. The reason for the
errors is that German syllables can be very long and
therefore sparse, often causing the model to back-
off to smaller contexts. If the context is too small to
cover the syllable, the model cannot decide whether
the current syllable contains a nucleus.
In stress assignment, this problem is even worse:
the context window rarely covers the whole word.
The algorithm does not know whether it already as-
signed a word stress outside the context window.
This leads to a high error rate with 15-20% of in-
correctly stressed words. Thereof, 37% have more
than one main stress, about 27% are not assigned any
stress and 36% are stressed in the wrong position.
This means that we can hope to reduce the errors by
almost 2/3 by using phonological constraints.
Word stress assignment is a difficult problem in
German because the underlying processes involve
some deeper morphological knowledge which is not
available to the simple model. In complex words,
stress mainly depends on morphological structure
(i.e. on the compositionality of compounds and
on the stressing status of affixes). Word stress in
simplex words is assumed to depend on the sylla-
ble position within the word stem and on syllable
weight. The current language-independent approach
does not model these processes, but only captures
some of its statistics.
Simple constraints can help to overcome the prob-
lem of lacking context by explicitly requiring that
every syllable must have exactly one syllable nu-
cleus and that every word must have exactly one syl-
lable receiving primary stress.
</bodyText>
<subsectionHeader confidence="0.997831">
3.1 Implementation
</subsectionHeader>
<bodyText confidence="0.999902777777778">
Our goal is to find the most probable syllabified
and stressed phonemization of a word that does not
violate the constraints. We tried two different ap-
proaches to enforce the constraints.
In the first variant (v1), we modified the proba-
bility model to enforce the constraints. Each state
now corresponds to a sequence of 4-tuples consist-
ing of a letter l, a phoneme sequence p, a syllable
boundary tag b, an accent tag a (as before) plus two
</bodyText>
<page confidence="0.994991">
98
</page>
<bodyText confidence="0.9990225">
new flags A and N which indicate whether an ac-
cent/nucleus precedes or not. The A and N flags of
the new state are a function of its accent and syllable
boundary tag and the A and N flag of the preceding
state. They split each state into four new states. The
new transition probabilities are defined as:
</bodyText>
<equation confidence="0.830994">
P((l; p; b; a)i (l; p; b; a)i−�
i−k , A, N)
</equation>
<bodyText confidence="0.999491">
The probability is 0 if the transition violates a con-
straint, e.g., when the A flag is set and ai indicates
another accent.
A positive side effect of the syllable flag is that it
stores separate phonemization probabilities for con-
sonants in the syllable onset vs. consonants in the
coda. The flag in the onset is 0 since the nucleus has
not yet been encountered, whereas it is set to 1 in the
coda. In German, this can e.g. help in for syllable-
final devoicing of voiced stops and fricatives.
The increase in the number of states aggravates
sparse-data problems. Therefore, we implemented
another variant (v2) which uses the same set of states
(with A and N flags), but with the transition proba-
bilities of the original model, which did not enforce
the constraints. Instead, we modified the Viterbi al-
gorithm to eliminate the invalid transitions: For ex-
ample, a transition from a state with the A flag set
to a state where ai introduces a second stress, is al-
ways ignored. On small data sets, better results were
achieved with v2 (see Table 5).
</bodyText>
<sectionHeader confidence="0.995564" genericHeader="method">
4 Morphological Preprocessing
</sectionHeader>
<bodyText confidence="0.999923580645161">
In German, information about morphological
boundaries is needed to correctly insert glottal stops
[?] in complex words, to determine irregular pro-
nunciation of affixes (v is pronounced [v] in ver-
tikal but [f] in ver+ticker+n, and the suffix syllable
heit is not stressed although superheavy and word
final) and to disambiguate letters (e.g. e is always
pronounced /@/ when occurring in inflectional suf-
fixes). Vowel length and quality has been argued
to also depend on morphological structure (Pounder
and Kommenda, 1986). Furthermore, morphologi-
cal boundaries overrun default syllabification rules,
such as the maximum onset principle.
Applying default syllabification to the word
“Sternanis¨ol” would result in a syllabification into
Ster-na-ni-s¨ol (and subsequent phonemiza-
tion to something like /SteR&amp;quot;na:nizø:l/) instead of
Stern-a-nis-¨ol (/&amp;quot;SteRn?ani:s?ø:l/). Syllabifi-
cation in turn affects phonemization since voiced
fricatives and stops are devoiced in syllable-final po-
sition. Morphological information also helps for
graphemic parsing of words such as “R¨oschen”
(Engl: little rose) where the morphological bound-
ary between R¨os and chen causes the string sch to
be transcribed to /sr/ instead of /S/. Similar ambigui-
ties can arise for all other sounds that are represented
by several letters in orthography (e.g. doubled con-
sonants, diphtongs, ie, ph, th), and is also valid for
English. Finally, morphological information is also
crucial to determine word stress in morphologically
complex words.
</bodyText>
<subsectionHeader confidence="0.578476">
4.1 Methods for Morphological Segmentation
</subsectionHeader>
<bodyText confidence="0.999966285714286">
Good segmentation performance on arbitrary words
is hard to achieve. We compared several approaches
with different amounts of built-in knowledge. The
morphological information is encoded in the let-
ter string, where different digits represent different
kinds of morphological boundaries (prefixes, stems,
derivational and inflectional suffixes).
</bodyText>
<subsectionHeader confidence="0.931525">
Manual Annotation from CELEX
</subsectionHeader>
<bodyText confidence="0.9999875">
To determine the upper bound of what can be
achieved when exploiting perfect morphological in-
formation, we extracted morphological boundaries
and boundary types from the CELEX database.
The manual annotation is not perfect as it con-
tains some errors and many cases where words are
not decomposed entirely. The words tagged [F] for
“lexicalized inflection”, e.g. gedr¨angt (past partici-
ple of dr¨angen, Engl: push) were decomposed semi-
automatically for the purpose of this evaluation. As
expected, annotating words with CELEX morpho-
logical segmentation yielded the best g2p conver-
sion results. Manual annotation is only available for
a small number of words. Therefore, only automati-
cally annotated morphological information can scale
up to real applications.
</bodyText>
<subsectionHeader confidence="0.544432">
Rule-based Systems
</subsectionHeader>
<bodyText confidence="0.9998595">
The traditional approach is to use large morpheme
lexica and a set of rules that segment words into af-
fixes and stems. Drawbacks of using such a system
are the high development costs, limited coverage
</bodyText>
<page confidence="0.993915">
99
</page>
<bodyText confidence="0.9998805">
and problems with ambiguity resolution between al-
ternative analyses of a word.
The two rule-based systems we evaluated, the
ETI4 morphological system and SMOR5 (Schmid et
al., 2004), are both high-quality systems with large
lexica that have been developed over several years.
Their performance results can help to estimate what
can realistically be expected from an automatic seg-
mentation system. Both of the rule-based systems
achieved an F-score of approx. 80% morphological
boundaries correct with respect to CELEX manual
annotation.
</bodyText>
<subsectionHeader confidence="0.766632">
Unsupervised Morphological Systems
</subsectionHeader>
<bodyText confidence="0.9999801875">
Most attractive among automatic systems are
methods that use unsupervised learning, because
these require neither an expert linguist to build large
rule-sets and lexica nor large manually annotated
word lists, but only large amounts of tokenized
text, which can be acquired e.g. from the internet.
Unsupervised methods are in principle6 language-
independent, and can therefore easily be applied to
other languages.
We compared four different state-of-the-art unsu-
pervised systems for morphological decomposition
(cf. (Demberg, 2006; Demberg, 2007)). The algo-
rithms were trained on a German newspaper cor-
pus (taz), containing about 240 million words. The
same algorithms have previously been shown to help
a speech recognition task (Kurimo et al., 2006).
</bodyText>
<sectionHeader confidence="0.999821" genericHeader="method">
5 Experimental Evaluations
</sectionHeader>
<subsectionHeader confidence="0.999926">
5.1 Training Set and Test Set Design
</subsectionHeader>
<bodyText confidence="0.997686666666667">
The German corpus used in these experiments is
CELEX (German Linguistic User Guide, 1995).
CELEX contains a phonemic representation of each
</bodyText>
<subsectionHeader confidence="0.309799">
4Eloquent Technology, Inc. (ETI) TTS system.
</subsectionHeader>
<bodyText confidence="0.8067462">
http://www.mindspring.com/˜ssshp/ssshp_cd/
ss_eloq.htm
5The lexicon used by SMOR, IMSLEX, contains morpho-
logically complex entries, which leads to high precision and low
recall. The results reported here refer to a version of SMOR,
where the lexicon entries were decomposed using a rather naive
high-recall segmentation method. SMOR itself does not disam-
biguate morphological analyses of a word. Our version used
transition weights learnt from CELEX morphological annota-
tion. For more details refer to (Demberg, 2006).
</bodyText>
<footnote confidence="0.888359">
6Most systems make some assumptions about the underly-
ing morphological system, for instance that morphology is a
concatenative process, that stems have a certain minimal length
or that prefixing and suffixing are the most relevant phenomena.
</footnote>
<bodyText confidence="0.9992195625">
word, syllable boundaries and word stress infor-
mation. Furthermore, it contains manually verified
morphological boundaries.
Our training set contains approx. 240,000 words
and the test set consists of 12,326 words. The test
set is designed such that word stems in training and
test sets are disjoint, i.e. the inflections of a certain
stem are either all in the training set or all in the test
set. Stem overlap between training and test set only
occurs in compounds and derivations. If a simple
random splitting (90% for training set, 10% for test
set) is used on inflected corpora, results are much
better: Word error rates (WER) are about 60% lower
when the set of stems in training and test set are not
disjoint. The same effect can also be observed for
the syllabification task (see Table 4).
</bodyText>
<sectionHeader confidence="0.684379" genericHeader="evaluation">
5.2 Results for the Joint n-gram Model
</sectionHeader>
<bodyText confidence="0.999960366666667">
The joint n-gram model is language-independent.
An aligned corpus with words and their pronuncia-
tions is needed, but no further adaptation is required.
Table 1 shows the performance of our model in
comparison to alternative approaches on the German
and English versions of the CELEX corpus, the En-
glish NetTalk corpus, the English Teacher’s Word
Book (TWB) corpus, the English beep corpus and
the French Brulex corpus. The joint n-gram model
performs significantly better than the decision tree
(essentially based on (Lucassen and Mercer, 1984)),
and achieves scores comparable to the Pronuncia-
tion by Analogy (PbA) algorithm (Marchand and
Damper, 2005). For the Nettalk data, we also com-
pared the influence of syllable boundary annotation
from a) automatically learnt and b) manually anno-
tated syllabification information on phoneme accu-
racy. Automatic syllabification for our model in-
tegrated phonological constraints (as described in
section 3.1), and therefore led to an improvement
in phoneme accuracy, while the word error rate in-
creased for the PbA approach, which does not incor-
porate such constraints.
(Chen, 2003) also used a joint n-gram model.
The two approaches differ in that Chen uses small
chunks (((l : |0..1|) : (p : |0..1|)) pairs only) and it-
eratively optimizes letter-phoneme alignment during
training. Chen smoothes higher-order Markov Mod-
els with Gaussian Priors and implements additional
language modelling such as consonant doubling.
</bodyText>
<page confidence="0.813468">
100
</page>
<table confidence="0.999734555555556">
corpus size jnt n-gr PbA Chen dec.tree
G - CELEX 230k 7.5% 15.0%
E - Nettalk 20k 35.4% 34.65% 34.6%
a) auto.syll 35.3% 35.2%
b) man.syll 29.4% 28.3%
E - TWB 18k 28.5% 28.2%
E - beep 200k 14.3% 13.3%
E - CELEX 100k 23.7% 31.7%
F - Brulex 27k 10.9%
</table>
<tableCaption confidence="0.802027">
Table 1: Word error rates for different g2p conver-
sion algorithms. Constraints were only used in the
E-Nettalk auto. syll condition.
</tableCaption>
<subsectionHeader confidence="0.999194">
5.3 Benefit of Integrating Constraints
</subsectionHeader>
<bodyText confidence="0.999793869565218">
The accuracy improvements achieved by integrat-
ing the constraints (see Table 2) are highly statis-
tically significant. The numbers for conditions “G-
syllab.+stress+g2p” and “E-syllab.+g2p” in Table 2
differ from the numbers for “G-CELEX” and “E-
Nettalk” in Table 1 because phoneme conversion
errors, syllabification errors and stress assignment
errors are all counted towards word error rates re-
ported in Table 2.
Word error rate in the combined g2p-syllable-
stress model was reduced from 21.5% to 13.7%. For
the separate tasks, we observed similar effects: The
word error rate for inserting syllable boundaries was
reduced from 3.48% to 3.1% on letters and from
1.84% to 1.53% on phonemes. Most significantly,
word error rate was decreased from 30.9% to 9.9%
for word stress assignment on graphemes.
We also found similarly important improvements
when applying the syllabification constraint to En-
glish grapheme-to-phoneme conversion and syllabi-
fication. This suggests that our findings are not spe-
cific to German but that this kind of general con-
straints can be beneficial for a range of languages.
</bodyText>
<table confidence="0.995164285714286">
no constr. constraint(s)
G - syllab.+stress+g2p 21.5% 13.7%
G - syllab. on letters 3.5% 3.1%
G - syllab. on phonemes 1.84% 1.53%
G - stress assignm. on letters 30.9% 9.9%
E - syllab.+g2p 40.5% 37.5%
E - syllab. on phonemes 12.7% 8.8%
</table>
<tableCaption confidence="0.997577">
Table 2: Improving performance on g2p conver-
</tableCaption>
<bodyText confidence="0.9183065">
sion, syllabification and stress assignment through
the introduction of constraints. The table shows
word error rates for German CELEX (G) and En-
glish NetTalk (E).
</bodyText>
<subsectionHeader confidence="0.992052">
5.4 Modularity
</subsectionHeader>
<bodyText confidence="0.999994807692308">
Modularity is an advantage if the individual compo-
nents are more specialized to their task (e.g. by ap-
plying a particular level of description of the prob-
lem, or by incorporating some additional source of
knowledge).In a modular system, one component
can easily be substituted by another – for example,
if a better way of doing stress assignment in German
was found. On the other hand, keeping everything in
one module for strongly inter-dependent tasks (such
as determining word stress and phonemization) al-
lows us to simultaneously optimize for the best com-
bination of phonemes and stress.
Best results were obtained from the joint n-gram
model that does syllabification, stress assignment
and g2p conversion all in a single step and inte-
grates phonological constraints for syllabification
and word stress (WER = 14.4% using method v1,
WER = 13.7% using method v2). If the modular ar-
chitecture is chosen, best results are obtained when
g2p conversion is done before syllabification and
stress assignment (15.2% WER), whereas doing syl-
labification and stress assignment first and then g2p
conversion leads to a WER of 16.6%. We can con-
clude from this finding that an integrated approach is
superior to a pipeline architecture for strongly inter-
dependent tasks such as these.
</bodyText>
<subsectionHeader confidence="0.9648425">
5.5 The Contribution of Morphological
Preprocessing
</subsectionHeader>
<bodyText confidence="0.999951375">
A statistically significant (according to a two-tailed
t-test) improvement in g2p conversion accuracy
(from 13.7% WER to 13.2% WER) was obtained
with the manually annotated morphological bound-
aries from CELEX. The segmentation from both of
the rule-based systems (ETI and SMOR) also re-
sulted in an accuracy increase with respect to the
baseline (13.6% WER), which is not annotated with
morphological boundaries.
Among the unsupervised systems, best results7 on
the g2p task with morphological annotation were ob-
tained with the RePortS system (Keshava and Pitler,
2006). But none of the segmentations led to an er-
ror reduction when compared to a baseline that used
no morphological information (see Table 3). Word
error rate even increased when the quality of the
</bodyText>
<footnote confidence="0.987796">
7For all results refer to (Demberg, 2006).
</footnote>
<page confidence="0.985649">
101
</page>
<table confidence="0.9997475">
Precis. Recall F-Meas. WER
RePortS (unsuperv.) 71.1% 50.7% 59.2% 15.1%
no morphology 13.7%
SMOR (rule-based) 87.1% 80.4% 83.6%
ETI (rule-based) 75.4% 84.1% 79.5% 13.6%
CELEX (manual) 100% 100% 100% 13.2%
</table>
<tableCaption confidence="0.990972">
Table 3: Systems evaluation on German CELEX
</tableCaption>
<bodyText confidence="0.985336086956522">
manual annotation and on the g2p task using a joint
n-gram model. WERs refer to implementation v2.
morphological segmentation was too low (the unsu-
pervised algorithms achieved 52%-62% F-measure
with respect to CELEX manual annotation).
Table 4 shows that high-quality morphological
information can also significantly improve perfor-
mance on a syllabification task for German. We used
the syllabifier described in (Schmid et al., 2005),
which works similar to the joint n-gram model used
for g2p conversion. Just as for g2p conversion, we
found a significant accuracy improvement when us-
ing the manually annotated data, a smaller improve-
ment for using data from the rule-based morpholog-
ical system, and no improvement when using seg-
mentations from an unsupervised algorithm. Syllab-
ification works best when performed on phonemes,
because syllables are phonological units and there-
fore can be determined most easily in terms of
phonological entities such as phonemes.
Whether morphological segmentation is worth the
effort depends on many factors such as training set
size, the g2p algorithm and the language considered.
</bodyText>
<table confidence="0.989041666666667">
disj. stems random
RePortS (unsupervised morph.) 4.95%
no morphology 3.10% 0.72%
ETI (rule-based morph.) 2.63%
CELEX (manual annot.) 1.91% 0.53%
on phonemes 1.53% 0.18%
</table>
<tableCaption confidence="0.970211">
Table 4: Word error rates (WER) for syllabification
</tableCaption>
<bodyText confidence="0.812329">
with a joint n-gram model for two different training
and test set designs (see Section 5.1).
</bodyText>
<subsectionHeader confidence="0.804516">
Morphology for Data Sparseness Reduction
</subsectionHeader>
<bodyText confidence="0.975047709677419">
Probably the most important aspect of morpho-
logical segmentation information is that it can help
to resolve data sparseness issues. Because of the ad-
ditional knowledge given to the system through the
morphological information, similarly-behaving let-
ter sequences can be grouped more effectively.
Therefore, we hypothesized that morphological
information is most beneficial in situations where
the training corpus is rather small. Our findings con-
firm this expectation, as the relative error reduction
through morphological annotation for a training cor-
pus of 9,600 words is 6.67%, while it is only 3.65%
for a 240,000-word training corpus.
In our implementation, the stress flags and sylla-
ble flags we use to enforce the phonological con-
straints increase data sparseness. We found v2 (the
implementation that uses the states without stress
and syllable flags and enforces the constraints by
eliminating invalid transitions, cf. section 3.1) to
outperform the integrated version, v1, and more sig-
nificantly in the case of more severe data sparseness.
The only condition when we found v1 to perform
better than v2 was with a large data set and addi-
tional data sparseness reduction through morpholog-
ical annotation, as in section 4 (see Table 5).
WER: designs v1 v2
data set size
no morph.
CELEX
Table 5: The interactions of constraints in training
and different levels of data sparseness.
</bodyText>
<subsectionHeader confidence="0.654668">
g2p Conversion Algorithms
</subsectionHeader>
<bodyText confidence="0.999979217391304">
The benefit of using morphological preprocessing
is also affected by the algorithm that is used for g2p
conversion. Therefore, we also evaluated the relative
improvement of morphological annotation when us-
ing a decision tree for g2p conversion.
Decision trees were one of the first data-based ap-
proaches to g2p and are still widely used (Kienappel
and Kneser, 2001; Black et al., 1998). The tree’s
efficiency and ability for generalization largely de-
pends on pruning and the choice of possible ques-
tions. In our implementation, the decision tree can
ask about letters within a context window of five
back and five ahead, about five phonemes back and
groups of letters (e.g. consonants vs. vowels).
Both the decision tree and the joint n-gram model
convert graphemes to phonemes, insert syllable
boundaries and assign word stress in a single step
(marked as “WER-ss” in Table 6. The imple-
mentation of the joint n-gram model incorporates
the phonological constraints described in section 3
(“WER-ss+”). Our main finding is that the joint
n-gram model profits less from morphological an-
notation. Without the constraints, the performance
</bodyText>
<table confidence="0.963388333333333">
240k 9.6k 240k 9.6k
14.4% 32.3% 13.7% 25.5%
12.5% 29% 13.2% 23.8%
</table>
<page confidence="0.996309">
102
</page>
<bodyText confidence="0.999911285714286">
difference is smaller: the joint n-gram model then
achieves a word error rate of 21.5% on the no-
morphology-condition.
In very recent work, (Demberg, 2007) developed
an unsupervised algorithm (f-meas: 68%; an exten-
sion of RePortS) whose segmentations improve g2p
when using a the decision tree (PER: 3.45%).
</bodyText>
<table confidence="0.999814">
decision tree joint n-gram
PER WER-ss PER WER-ss+
RePortS 3.83% 28.3% 15.1%
no morph. 3.63% 26.59% 2.52% 13.7%
ETI 2.8% 21.13% 2.53% 13.6%
CELEX 2.64% 21.64% 2.36% 13.2%
</table>
<tableCaption confidence="0.993879">
Table 6: The effect of morphological preprocessing
</tableCaption>
<bodyText confidence="0.8581985">
on phoneme error rates (PER) and word error rates
(WER) in grapheme-to-phoneme conversion.
</bodyText>
<subsectionHeader confidence="0.702999">
Morphology for other Languages
</subsectionHeader>
<bodyText confidence="0.9999752">
We also investigated the effect of morphological
information on g2p conversion and syllabification
in English, using manually annotated morphological
boundaries from CELEX and the automatic unsuper-
vised RePortS system which achieves an F-score of
about 77% for English. The cases where morpho-
logical information affects word pronunciation are
relatively few in comparison to German, therefore
the overall effect is rather weak and we did not even
find improvements with perfect boundaries.
</bodyText>
<sectionHeader confidence="0.999823" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999985111111111">
Our results confirm that the integration of phonolog-
ical constraints ‘one nucleus per syllable’ and ‘one
main stress per word’ can significantly boost ac-
curacy for g2p conversion in German and English.
We implemented the constraints using a joint n-
gram model for g2p conversion, which is language-
independent and well-suited to the g2p task.
We systematically evaluated the benefit to be
gained from morphological preprocessing on g2p
conversion and syllabification. We found that mor-
phological segmentations from rule-based systems
led to some improvement. But the magnitude of
the accuracy improvement strongly depends on the
g2p algorithm and on training set size. State-of-
the-art unsupervised morphological systems do not
yet yield sufficiently good segmentations to help the
task, if a good conversion algorithm is used: Low
quality segmentation even led to higher error rates.
</bodyText>
<sectionHeader confidence="0.998643" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99883625">
We would like to thank Hinrich Sch¨utze, Frank Keller and the
ACL reviewers for valuable comments and discussion.
The first author was supported by Evangelisches Studienwerk
e.V. Villigst.
</bodyText>
<sectionHeader confidence="0.998319" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999532140350877">
M. Bisani and H. Ney. 2002. Investigations on joint multigram
models for grapheme-to-phoneme conversion. In ICSLP.
A. Black, K. Lenzo, and V. Pagel. 1998. Issues in building gen-
eral letter to sound rules. In 3. ESCA on Speech Synthesis.
SF Chen and J Goodman. 1996. An empirical study of smooth-
ing techniques for language modeling. In Proc. of ACL.
S. F. Chen. 2003. Conditional and joint models for grapheme-
to-phoneme conversion. In Eurospeech.
V. Demberg. 2006. Letter-to-phoneme conversion for a Ger-
man TTS-System. Master’s thesis. IMS, Univ. of Stuttgart.
V. Demberg. 2007. A language-independent unsupervised
model for morphological segmentation. In Proc. of ACL-07.
L. Galescu and J. Allen. 2001. Bi-directional conversion be-
tween graphemes and phonemes using a joint n-gram model.
In Proc. of the 4th ISCA Workshop on Speech Synthesis.
CELEX German Linguistic User Guide, 1995. Center for Lex-
ical Information. Max-Planck-Institut for Psycholinguistics,
Nijmegen.
M. Jessen, 1998. Word Prosodic Systems in the Languages of
Europe. Mouton de Gruyter: Berlin.
S. Keshava and E. Pitler. 2006. A simpler, intuitive approach
to morpheme induction. In Proceedings of 2nd Pascal Chal-
lenges Workshop, pages 31–35, Venice, Italy.
A. K. Kienappel and R. Kneser. 2001. Designing very com-
pact decision trees for grapheme-to-phoneme transcription.
In Eurospeech, Scandinavia.
M. Kurimo, M. Creutz, M. Varjokallio, E. Arisoy, and M. Sar-
aclar. 2006. Unsupervsied segmentation of words into mor-
phemes – Challenge 2005: An introduction and evaluation
report. In Proc. of 2nd Pascal Challenges Workshop, Italy.
J. Lucassen and R. Mercer. 1984. An information theoretic
approach to the automatic determination of phonemic base-
forms. In ICASSP 9.
Y. Marchand and R. I. Damper. 2005. Can syllabification im-
prove pronunciation by analogy of English? Natural Lan-
guage Engineering.
W. Minker. 1996. Grapheme-to-phoneme conversion - an ap-
proach based on hidden markov models.
B. M¨obius. 2001. German and Multilingual Speech Synthesis.
phonetic AIMS, Arbeitspapiere des Instituts f¨ur Maschinelle
Spachverarbeitung.
K. M¨uller. 2001. Automatic detection of syllable boundaries
combining the advantages of treebank and bracketed corpora
training. In Proceedings of ACL, pages 402–409.
A. Pounder and M. Kommenda. 1986. Morphological analysis
for a German text-to-speech system. In COLING 1986.
P.A. Rentzepopoulos and G.K. Kokkinakis. 1991. Phoneme to
grapheme conversion using HMM. In Eurospeech.
H. Schmid, A. Fitschen, and U. Heid. 2004. SMOR: A German
computational morphology covering derivation, composition
and inflection. In Proc. of LREC.
H. Schmid, B. M¨obius, and J. Weidenkaff. 2005. Tagging syl-
lable boundaries with hidden Markov models. IMS, unpub.
R. Sproat. 1996. Multilingual text analysis for text-to-speech
synthesis. In Proc. ICSLP ’96, Philadelphia, PA.
P. Taylor. 2005. Hidden Markov models for grapheme to
phoneme conversion. In INTERSPEECH.
</reference>
<page confidence="0.999299">
103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.545359">
<title confidence="0.9992255">Phonological Constraints and Morphological Preprocessing for Grapheme-to-Phoneme Conversion</title>
<author confidence="0.999329">Vera Demberg</author>
<affiliation confidence="0.9999605">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.997511">Edinburgh, EH8 9LW, GB</address>
<email confidence="0.984115">v.demberg@sms.ed.ac.uk</email>
<author confidence="0.940031">Helmut Schmid</author>
<affiliation confidence="0.998247">IMS University of Stuttgart</affiliation>
<address confidence="0.996743">D-70174 Stuttgart</address>
<email confidence="0.989699">schmid@ims.uni-stuttgart.de</email>
<author confidence="0.99588">Gregor M¨ohler</author>
<affiliation confidence="0.8697085">Speech Technologies IBM Deutschland Entwicklung</affiliation>
<address confidence="0.815458">D-71072 B¨oblingen</address>
<email confidence="0.999353">moehler@de.ibm.com</email>
<abstract confidence="0.997000368421053">Grapheme-to-phoneme conversion (g2p) is a core component of any text-to-speech system. We show that adding simple syllabification and stress assignment constraints, namely ‘one nucleus per syllable’ and ‘one main stress per word’, to a joint n-gram model for g2p conversion leads to a dramatic improvement in conversion accuracy. Secondly, we assessed morphological preprocessing for g2p conversion. While morphological information has been incorporated in some past systems, its contribution has never been quantitatively assessed for German. We compare the relevance of morphological preprocessing with respect to the morphological segmentation method, training set size, the g2p conversion algorithm, and two languages, English and German.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Bisani</author>
<author>H Ney</author>
</authors>
<title>Investigations on joint multigram models for grapheme-to-phoneme conversion.</title>
<date>2002</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="4951" citStr="Bisani and Ney, 2002" startWordPosition="734" endWordPosition="737">section 2 and explain how we implemented the phonological constraints in section 3. Section 4 is concerned with the relation between morphology, word pronunciation, syllabification and word stress in German, and presents different sources for morphological segmentation. In section 5, we evaluate the contribution of each of the components and compare our methods to state-of-the-art systems. Section 6 summarizes our results. 2 Methods We used a joint n-gram model for the graphemeto-phoneme conversion task. Models of this type have previously been shown to yield very good g2p conversion results (Bisani and Ney, 2002; Galescu and Allen, 2001; Chen, 2003). Models that do not use joint letter-phoneme states, and therefore are not conditional on the preceding letters, but only on the actual letter and the preceding phonemes, achieved inferior results. Examples of such approaches using Hidden Markov Models are (Rentzepopoulos and Kokkinakis, 1991) (who applied the HMM to the related task of phoneme-to-grapheme conversion), (Taylor, 2005) and (Minker, 1996). The g2p task is formulated as searching for the most probable sequence of phonemes given the orthographic form of a word. One can think of it as a tagging</context>
</contexts>
<marker>Bisani, Ney, 2002</marker>
<rawString>M. Bisani and H. Ney. 2002. Investigations on joint multigram models for grapheme-to-phoneme conversion. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Black</author>
<author>K Lenzo</author>
<author>V Pagel</author>
</authors>
<title>Issues in building general letter to sound rules.</title>
<date>1998</date>
<booktitle>In 3. ESCA on Speech Synthesis. SF Chen and</booktitle>
<contexts>
<context position="3004" citStr="Black et al., 1998" startWordPosition="436" endWordPosition="439">tch), it is important to know where exactly the syllable boundaries are in order to correctly calculate syllable weight. For German, (M¨uller, 2001) show that information about stress assignment and the position of a syllable within a word improve g2p conversion. 1.2 Morphological Preprocessing It has been argued that using morphological information is important for languages where morphology has an important influence on pronunciation, syllabification and word stress such as German, Dutch, Swedish or, to a smaller extent, also English (Sproat, 1996; M¨obius, 2001; Pounder and Kommenda, 1986; Black et al., 1998; Taylor, 2005). Unfortunately, these papers do not quantify the contribution of morphological preprocessing in the task. Important questions when considering the integration of a morphological component into a speech 1This issue is controversial among linguists; for an overview see (Jessen, 1998). Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 96–103, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics synthesis system are 1) How large are the improvements to be gained from morphological preprocessing? 2) Must the </context>
<context position="28683" citStr="Black et al., 1998" startWordPosition="4510" endWordPosition="4513">through morphological annotation, as in section 4 (see Table 5). WER: designs v1 v2 data set size no morph. CELEX Table 5: The interactions of constraints in training and different levels of data sparseness. g2p Conversion Algorithms The benefit of using morphological preprocessing is also affected by the algorithm that is used for g2p conversion. Therefore, we also evaluated the relative improvement of morphological annotation when using a decision tree for g2p conversion. Decision trees were one of the first data-based approaches to g2p and are still widely used (Kienappel and Kneser, 2001; Black et al., 1998). The tree’s efficiency and ability for generalization largely depends on pruning and the choice of possible questions. In our implementation, the decision tree can ask about letters within a context window of five back and five ahead, about five phonemes back and groups of letters (e.g. consonants vs. vowels). Both the decision tree and the joint n-gram model convert graphemes to phonemes, insert syllable boundaries and assign word stress in a single step (marked as “WER-ss” in Table 6. The implementation of the joint n-gram model incorporates the phonological constraints described in section</context>
</contexts>
<marker>Black, Lenzo, Pagel, 1998</marker>
<rawString>A. Black, K. Lenzo, and V. Pagel. 1998. Issues in building general letter to sound rules. In 3. ESCA on Speech Synthesis. SF Chen and J Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
</authors>
<title>Conditional and joint models for graphemeto-phoneme conversion.</title>
<date>2003</date>
<booktitle>In Eurospeech.</booktitle>
<contexts>
<context position="4989" citStr="Chen, 2003" startWordPosition="742" endWordPosition="743">onological constraints in section 3. Section 4 is concerned with the relation between morphology, word pronunciation, syllabification and word stress in German, and presents different sources for morphological segmentation. In section 5, we evaluate the contribution of each of the components and compare our methods to state-of-the-art systems. Section 6 summarizes our results. 2 Methods We used a joint n-gram model for the graphemeto-phoneme conversion task. Models of this type have previously been shown to yield very good g2p conversion results (Bisani and Ney, 2002; Galescu and Allen, 2001; Chen, 2003). Models that do not use joint letter-phoneme states, and therefore are not conditional on the preceding letters, but only on the actual letter and the preceding phonemes, achieved inferior results. Examples of such approaches using Hidden Markov Models are (Rentzepopoulos and Kokkinakis, 1991) (who applied the HMM to the related task of phoneme-to-grapheme conversion), (Taylor, 2005) and (Minker, 1996). The g2p task is formulated as searching for the most probable sequence of phonemes given the orthographic form of a word. One can think of it as a tagging problem where each letter is tagged w</context>
<context position="20713" citStr="Chen, 2003" startWordPosition="3252" endWordPosition="3253">d on (Lucassen and Mercer, 1984)), and achieves scores comparable to the Pronunciation by Analogy (PbA) algorithm (Marchand and Damper, 2005). For the Nettalk data, we also compared the influence of syllable boundary annotation from a) automatically learnt and b) manually annotated syllabification information on phoneme accuracy. Automatic syllabification for our model integrated phonological constraints (as described in section 3.1), and therefore led to an improvement in phoneme accuracy, while the word error rate increased for the PbA approach, which does not incorporate such constraints. (Chen, 2003) also used a joint n-gram model. The two approaches differ in that Chen uses small chunks (((l : |0..1|) : (p : |0..1|)) pairs only) and iteratively optimizes letter-phoneme alignment during training. Chen smoothes higher-order Markov Models with Gaussian Priors and implements additional language modelling such as consonant doubling. 100 corpus size jnt n-gr PbA Chen dec.tree G - CELEX 230k 7.5% 15.0% E - Nettalk 20k 35.4% 34.65% 34.6% a) auto.syll 35.3% 35.2% b) man.syll 29.4% 28.3% E - TWB 18k 28.5% 28.2% E - beep 200k 14.3% 13.3% E - CELEX 100k 23.7% 31.7% F - Brulex 27k 10.9% Table 1: Word</context>
</contexts>
<marker>Chen, 2003</marker>
<rawString>S. F. Chen. 2003. Conditional and joint models for graphemeto-phoneme conversion. In Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
</authors>
<title>Letter-to-phoneme conversion for a German TTS-System. Master’s thesis.</title>
<date>2006</date>
<institution>IMS, Univ. of Stuttgart.</institution>
<contexts>
<context position="7639" citStr="Demberg, 2006" startWordPosition="1180" endWordPosition="1181"> = arg max bi i=1 The stress assignment model works on syllables. It computes the most probable sequence an1 of word accent-tags a1, a2, ..., an for a given syllable sequence syln1 . P(hsyl; aii |hsyl; aii−1 i−k) 2.1 Smoothing Because of major data sparseness problems, smoothing is an important issue, in particular for the stress model which is based on syllable–stress-tag pairs. Performance varied by up to 20% in function of the smoothing algorithm chosen. Best results were obtained when using a variant of Modified Kneser-Ney Smoothing2 (Chen and Goodman, 1996). 2For a formal definition, see(Demberg, 2006). tn 1 = arg max ai n+1 i=1 97 2.2 Pruning In the g2p-model, each letter can on average map onto one of 12 alternative phoneme-sequences. When working with 5-grams3, there are about 125 = 250,000 state sequences. To improve time and space efficiency, we implemented a simple pruning strategy that only considers the t best states at any moment in time. With a threshold of t = 15, about 120 words are processed per minute on a 1.5GHz machine. Conversion quality is only marginally worse than when the whole search space is calculated. Running time for English is faster, because the average number of</context>
<context position="17520" citStr="Demberg, 2006" startWordPosition="2754" endWordPosition="2755">orrect with respect to CELEX manual annotation. Unsupervised Morphological Systems Most attractive among automatic systems are methods that use unsupervised learning, because these require neither an expert linguist to build large rule-sets and lexica nor large manually annotated word lists, but only large amounts of tokenized text, which can be acquired e.g. from the internet. Unsupervised methods are in principle6 languageindependent, and can therefore easily be applied to other languages. We compared four different state-of-the-art unsupervised systems for morphological decomposition (cf. (Demberg, 2006; Demberg, 2007)). The algorithms were trained on a German newspaper corpus (taz), containing about 240 million words. The same algorithms have previously been shown to help a speech recognition task (Kurimo et al., 2006). 5 Experimental Evaluations 5.1 Training Set and Test Set Design The German corpus used in these experiments is CELEX (German Linguistic User Guide, 1995). CELEX contains a phonemic representation of each 4Eloquent Technology, Inc. (ETI) TTS system. http://www.mindspring.com/˜ssshp/ssshp_cd/ ss_eloq.htm 5The lexicon used by SMOR, IMSLEX, contains morphologically complex entri</context>
<context position="25158" citStr="Demberg, 2006" startWordPosition="3968" endWordPosition="3969">ndaries from CELEX. The segmentation from both of the rule-based systems (ETI and SMOR) also resulted in an accuracy increase with respect to the baseline (13.6% WER), which is not annotated with morphological boundaries. Among the unsupervised systems, best results7 on the g2p task with morphological annotation were obtained with the RePortS system (Keshava and Pitler, 2006). But none of the segmentations led to an error reduction when compared to a baseline that used no morphological information (see Table 3). Word error rate even increased when the quality of the 7For all results refer to (Demberg, 2006). 101 Precis. Recall F-Meas. WER RePortS (unsuperv.) 71.1% 50.7% 59.2% 15.1% no morphology 13.7% SMOR (rule-based) 87.1% 80.4% 83.6% ETI (rule-based) 75.4% 84.1% 79.5% 13.6% CELEX (manual) 100% 100% 100% 13.2% Table 3: Systems evaluation on German CELEX manual annotation and on the g2p task using a joint n-gram model. WERs refer to implementation v2. morphological segmentation was too low (the unsupervised algorithms achieved 52%-62% F-measure with respect to CELEX manual annotation). Table 4 shows that high-quality morphological information can also significantly improve performance on a syll</context>
</contexts>
<marker>Demberg, 2006</marker>
<rawString>V. Demberg. 2006. Letter-to-phoneme conversion for a German TTS-System. Master’s thesis. IMS, Univ. of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
</authors>
<title>A language-independent unsupervised model for morphological segmentation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL-07.</booktitle>
<contexts>
<context position="17536" citStr="Demberg, 2007" startWordPosition="2756" endWordPosition="2757">pect to CELEX manual annotation. Unsupervised Morphological Systems Most attractive among automatic systems are methods that use unsupervised learning, because these require neither an expert linguist to build large rule-sets and lexica nor large manually annotated word lists, but only large amounts of tokenized text, which can be acquired e.g. from the internet. Unsupervised methods are in principle6 languageindependent, and can therefore easily be applied to other languages. We compared four different state-of-the-art unsupervised systems for morphological decomposition (cf. (Demberg, 2006; Demberg, 2007)). The algorithms were trained on a German newspaper corpus (taz), containing about 240 million words. The same algorithms have previously been shown to help a speech recognition task (Kurimo et al., 2006). 5 Experimental Evaluations 5.1 Training Set and Test Set Design The German corpus used in these experiments is CELEX (German Linguistic User Guide, 1995). CELEX contains a phonemic representation of each 4Eloquent Technology, Inc. (ETI) TTS system. http://www.mindspring.com/˜ssshp/ssshp_cd/ ss_eloq.htm 5The lexicon used by SMOR, IMSLEX, contains morphologically complex entries, which leads </context>
<context position="29656" citStr="Demberg, 2007" startWordPosition="4668" endWordPosition="4669">del convert graphemes to phonemes, insert syllable boundaries and assign word stress in a single step (marked as “WER-ss” in Table 6. The implementation of the joint n-gram model incorporates the phonological constraints described in section 3 (“WER-ss+”). Our main finding is that the joint n-gram model profits less from morphological annotation. Without the constraints, the performance 240k 9.6k 240k 9.6k 14.4% 32.3% 13.7% 25.5% 12.5% 29% 13.2% 23.8% 102 difference is smaller: the joint n-gram model then achieves a word error rate of 21.5% on the nomorphology-condition. In very recent work, (Demberg, 2007) developed an unsupervised algorithm (f-meas: 68%; an extension of RePortS) whose segmentations improve g2p when using a the decision tree (PER: 3.45%). decision tree joint n-gram PER WER-ss PER WER-ss+ RePortS 3.83% 28.3% 15.1% no morph. 3.63% 26.59% 2.52% 13.7% ETI 2.8% 21.13% 2.53% 13.6% CELEX 2.64% 21.64% 2.36% 13.2% Table 6: The effect of morphological preprocessing on phoneme error rates (PER) and word error rates (WER) in grapheme-to-phoneme conversion. Morphology for other Languages We also investigated the effect of morphological information on g2p conversion and syllabification in En</context>
</contexts>
<marker>Demberg, 2007</marker>
<rawString>V. Demberg. 2007. A language-independent unsupervised model for morphological segmentation. In Proc. of ACL-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Galescu</author>
<author>J Allen</author>
</authors>
<title>Bi-directional conversion between graphemes and phonemes using a joint n-gram model.</title>
<date>2001</date>
<booktitle>In Proc. of the 4th ISCA Workshop on Speech Synthesis.</booktitle>
<contexts>
<context position="4976" citStr="Galescu and Allen, 2001" startWordPosition="738" endWordPosition="741">how we implemented the phonological constraints in section 3. Section 4 is concerned with the relation between morphology, word pronunciation, syllabification and word stress in German, and presents different sources for morphological segmentation. In section 5, we evaluate the contribution of each of the components and compare our methods to state-of-the-art systems. Section 6 summarizes our results. 2 Methods We used a joint n-gram model for the graphemeto-phoneme conversion task. Models of this type have previously been shown to yield very good g2p conversion results (Bisani and Ney, 2002; Galescu and Allen, 2001; Chen, 2003). Models that do not use joint letter-phoneme states, and therefore are not conditional on the preceding letters, but only on the actual letter and the preceding phonemes, achieved inferior results. Examples of such approaches using Hidden Markov Models are (Rentzepopoulos and Kokkinakis, 1991) (who applied the HMM to the related task of phoneme-to-grapheme conversion), (Taylor, 2005) and (Minker, 1996). The g2p task is formulated as searching for the most probable sequence of phonemes given the orthographic form of a word. One can think of it as a tagging problem where each lette</context>
</contexts>
<marker>Galescu, Allen, 2001</marker>
<rawString>L. Galescu and J. Allen. 2001. Bi-directional conversion between graphemes and phonemes using a joint n-gram model. In Proc. of the 4th ISCA Workshop on Speech Synthesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CELEX German</author>
</authors>
<title>Linguistic User Guide,</title>
<date>1995</date>
<institution>Center for Lexical Information. Max-Planck-Institut for Psycholinguistics,</institution>
<location>Nijmegen.</location>
<marker>German, 1995</marker>
<rawString>CELEX German Linguistic User Guide, 1995. Center for Lexical Information. Max-Planck-Institut for Psycholinguistics, Nijmegen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jessen</author>
</authors>
<date>1998</date>
<booktitle>Word Prosodic Systems in the Languages of Europe. Mouton de Gruyter:</booktitle>
<location>Berlin.</location>
<contexts>
<context position="3302" citStr="Jessen, 1998" startWordPosition="481" endWordPosition="482">been argued that using morphological information is important for languages where morphology has an important influence on pronunciation, syllabification and word stress such as German, Dutch, Swedish or, to a smaller extent, also English (Sproat, 1996; M¨obius, 2001; Pounder and Kommenda, 1986; Black et al., 1998; Taylor, 2005). Unfortunately, these papers do not quantify the contribution of morphological preprocessing in the task. Important questions when considering the integration of a morphological component into a speech 1This issue is controversial among linguists; for an overview see (Jessen, 1998). Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 96–103, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics synthesis system are 1) How large are the improvements to be gained from morphological preprocessing? 2) Must the morphological system be perfect or can performance improvements also be reached with relatively simple morphological components? and 3) How much does the benefit to be expected from explicit morphological information depend on the g2p algorithm? To determine these factors, we compared morphologica</context>
</contexts>
<marker>Jessen, 1998</marker>
<rawString>M. Jessen, 1998. Word Prosodic Systems in the Languages of Europe. Mouton de Gruyter: Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Keshava</author>
<author>E Pitler</author>
</authors>
<title>A simpler, intuitive approach to morpheme induction.</title>
<date>2006</date>
<booktitle>In Proceedings of 2nd Pascal Challenges Workshop,</booktitle>
<pages>31--35</pages>
<location>Venice, Italy.</location>
<contexts>
<context position="24922" citStr="Keshava and Pitler, 2006" startWordPosition="3925" endWordPosition="3928">ese. 5.5 The Contribution of Morphological Preprocessing A statistically significant (according to a two-tailed t-test) improvement in g2p conversion accuracy (from 13.7% WER to 13.2% WER) was obtained with the manually annotated morphological boundaries from CELEX. The segmentation from both of the rule-based systems (ETI and SMOR) also resulted in an accuracy increase with respect to the baseline (13.6% WER), which is not annotated with morphological boundaries. Among the unsupervised systems, best results7 on the g2p task with morphological annotation were obtained with the RePortS system (Keshava and Pitler, 2006). But none of the segmentations led to an error reduction when compared to a baseline that used no morphological information (see Table 3). Word error rate even increased when the quality of the 7For all results refer to (Demberg, 2006). 101 Precis. Recall F-Meas. WER RePortS (unsuperv.) 71.1% 50.7% 59.2% 15.1% no morphology 13.7% SMOR (rule-based) 87.1% 80.4% 83.6% ETI (rule-based) 75.4% 84.1% 79.5% 13.6% CELEX (manual) 100% 100% 100% 13.2% Table 3: Systems evaluation on German CELEX manual annotation and on the g2p task using a joint n-gram model. WERs refer to implementation v2. morphologic</context>
</contexts>
<marker>Keshava, Pitler, 2006</marker>
<rawString>S. Keshava and E. Pitler. 2006. A simpler, intuitive approach to morpheme induction. In Proceedings of 2nd Pascal Challenges Workshop, pages 31–35, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Kienappel</author>
<author>R Kneser</author>
</authors>
<title>Designing very compact decision trees for grapheme-to-phoneme transcription.</title>
<date>2001</date>
<booktitle>In Eurospeech, Scandinavia.</booktitle>
<contexts>
<context position="28662" citStr="Kienappel and Kneser, 2001" startWordPosition="4506" endWordPosition="4509">l data sparseness reduction through morphological annotation, as in section 4 (see Table 5). WER: designs v1 v2 data set size no morph. CELEX Table 5: The interactions of constraints in training and different levels of data sparseness. g2p Conversion Algorithms The benefit of using morphological preprocessing is also affected by the algorithm that is used for g2p conversion. Therefore, we also evaluated the relative improvement of morphological annotation when using a decision tree for g2p conversion. Decision trees were one of the first data-based approaches to g2p and are still widely used (Kienappel and Kneser, 2001; Black et al., 1998). The tree’s efficiency and ability for generalization largely depends on pruning and the choice of possible questions. In our implementation, the decision tree can ask about letters within a context window of five back and five ahead, about five phonemes back and groups of letters (e.g. consonants vs. vowels). Both the decision tree and the joint n-gram model convert graphemes to phonemes, insert syllable boundaries and assign word stress in a single step (marked as “WER-ss” in Table 6. The implementation of the joint n-gram model incorporates the phonological constraints</context>
</contexts>
<marker>Kienappel, Kneser, 2001</marker>
<rawString>A. K. Kienappel and R. Kneser. 2001. Designing very compact decision trees for grapheme-to-phoneme transcription. In Eurospeech, Scandinavia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kurimo</author>
<author>M Creutz</author>
<author>M Varjokallio</author>
<author>E Arisoy</author>
<author>M Saraclar</author>
</authors>
<title>Unsupervsied segmentation of words into morphemes – Challenge 2005: An introduction and evaluation report.</title>
<date>2006</date>
<booktitle>In Proc. of 2nd Pascal Challenges Workshop,</booktitle>
<location>Italy.</location>
<contexts>
<context position="17741" citStr="Kurimo et al., 2006" startWordPosition="2788" endWordPosition="2791">t to build large rule-sets and lexica nor large manually annotated word lists, but only large amounts of tokenized text, which can be acquired e.g. from the internet. Unsupervised methods are in principle6 languageindependent, and can therefore easily be applied to other languages. We compared four different state-of-the-art unsupervised systems for morphological decomposition (cf. (Demberg, 2006; Demberg, 2007)). The algorithms were trained on a German newspaper corpus (taz), containing about 240 million words. The same algorithms have previously been shown to help a speech recognition task (Kurimo et al., 2006). 5 Experimental Evaluations 5.1 Training Set and Test Set Design The German corpus used in these experiments is CELEX (German Linguistic User Guide, 1995). CELEX contains a phonemic representation of each 4Eloquent Technology, Inc. (ETI) TTS system. http://www.mindspring.com/˜ssshp/ssshp_cd/ ss_eloq.htm 5The lexicon used by SMOR, IMSLEX, contains morphologically complex entries, which leads to high precision and low recall. The results reported here refer to a version of SMOR, where the lexicon entries were decomposed using a rather naive high-recall segmentation method. SMOR itself does not </context>
</contexts>
<marker>Kurimo, Creutz, Varjokallio, Arisoy, Saraclar, 2006</marker>
<rawString>M. Kurimo, M. Creutz, M. Varjokallio, E. Arisoy, and M. Saraclar. 2006. Unsupervsied segmentation of words into morphemes – Challenge 2005: An introduction and evaluation report. In Proc. of 2nd Pascal Challenges Workshop, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lucassen</author>
<author>R Mercer</author>
</authors>
<title>An information theoretic approach to the automatic determination of phonemic baseforms.</title>
<date>1984</date>
<booktitle>In ICASSP 9.</booktitle>
<contexts>
<context position="20134" citStr="Lucassen and Mercer, 1984" startWordPosition="3162" endWordPosition="3165">or the syllabification task (see Table 4). 5.2 Results for the Joint n-gram Model The joint n-gram model is language-independent. An aligned corpus with words and their pronunciations is needed, but no further adaptation is required. Table 1 shows the performance of our model in comparison to alternative approaches on the German and English versions of the CELEX corpus, the English NetTalk corpus, the English Teacher’s Word Book (TWB) corpus, the English beep corpus and the French Brulex corpus. The joint n-gram model performs significantly better than the decision tree (essentially based on (Lucassen and Mercer, 1984)), and achieves scores comparable to the Pronunciation by Analogy (PbA) algorithm (Marchand and Damper, 2005). For the Nettalk data, we also compared the influence of syllable boundary annotation from a) automatically learnt and b) manually annotated syllabification information on phoneme accuracy. Automatic syllabification for our model integrated phonological constraints (as described in section 3.1), and therefore led to an improvement in phoneme accuracy, while the word error rate increased for the PbA approach, which does not incorporate such constraints. (Chen, 2003) also used a joint n-</context>
</contexts>
<marker>Lucassen, Mercer, 1984</marker>
<rawString>J. Lucassen and R. Mercer. 1984. An information theoretic approach to the automatic determination of phonemic baseforms. In ICASSP 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Marchand</author>
<author>R I Damper</author>
</authors>
<title>Can syllabification improve pronunciation by analogy of English? Natural Language Engineering.</title>
<date>2005</date>
<contexts>
<context position="1938" citStr="Marchand and Damper, 2005" startWordPosition="263" endWordPosition="267">rn?ani:s?ø:l/). Speech synthesis modules with a g2p component are used in text-to-speech (TTS) systems and can be be applied in spoken dialogue systems or speech-to-speech translation systems. 1.1 Syllabification and Stress in g2p conversion In order to correctly synthesize a word, it is not only necessary to convert the letters into phonemes, but also to syllabify the word and to assign word stress. 96 The problems of word phonemization, syllabification and word stress assignment are inter-dependent. Information about the position of a syllable boundary helps grapheme-to-phoneme conversion. (Marchand and Damper, 2005) report a word error rate (WER) reduction of approx. 5 percentage points for English when the letter string is augmented with syllabification information. The same holds vice-versa: we found that WER was reduced by 50% when running our syllabifier on phonemes instead of letters (see Table 4). Finally, word stress is usually defined on syllables; in languages where word stress is assumed1 to partly depend on syllable weight (such as German or Dutch), it is important to know where exactly the syllable boundaries are in order to correctly calculate syllable weight. For German, (M¨uller, 2001) sho</context>
<context position="8968" citStr="Marchand and Damper, 2005" startWordPosition="1399" endWordPosition="1402">al g2p conversion in 10-fold cross validation) for a Perl implementation of our algorithm on the English NetTalk corpus (20,008 words) on an Intel Pentium 4, 3.0 GHz machine. Running time was less than 1h for each of the following three test conditions: c1) g2p conversion only, c2) syllabification first, then g2p conversion, c3) simultaneous g2p conversion and syllabification, given perfect syllable boundary input, c4) simultaneous g2p conversion and syllabification when correct syllabification is not available beforehand. This is much faster than the times for Pronunciation by Analogy (PbA) (Marchand and Damper, 2005) on the same corpus. Marchand and Damper reported a processing time of several hours for c4), two days for c2) and several days for c3). 2.3 Alignment Our current implementation of the joint n-gram model is not integrated with an automatic alignment procedure. We therefore first aligned letters and phonemes in a separate, semi-automatic step. Each letter was aligned with zero to two phonemes and, in the integrated model, zero or one syllable boundaries and stress markers. 3 Integration of Phonological Constraints When analysing the results from the model that does g2p conversion, syllabificati</context>
<context position="20243" citStr="Marchand and Damper, 2005" startWordPosition="3178" endWordPosition="3181">anguage-independent. An aligned corpus with words and their pronunciations is needed, but no further adaptation is required. Table 1 shows the performance of our model in comparison to alternative approaches on the German and English versions of the CELEX corpus, the English NetTalk corpus, the English Teacher’s Word Book (TWB) corpus, the English beep corpus and the French Brulex corpus. The joint n-gram model performs significantly better than the decision tree (essentially based on (Lucassen and Mercer, 1984)), and achieves scores comparable to the Pronunciation by Analogy (PbA) algorithm (Marchand and Damper, 2005). For the Nettalk data, we also compared the influence of syllable boundary annotation from a) automatically learnt and b) manually annotated syllabification information on phoneme accuracy. Automatic syllabification for our model integrated phonological constraints (as described in section 3.1), and therefore led to an improvement in phoneme accuracy, while the word error rate increased for the PbA approach, which does not incorporate such constraints. (Chen, 2003) also used a joint n-gram model. The two approaches differ in that Chen uses small chunks (((l : |0..1|) : (p : |0..1|)) pairs onl</context>
</contexts>
<marker>Marchand, Damper, 2005</marker>
<rawString>Y. Marchand and R. I. Damper. 2005. Can syllabification improve pronunciation by analogy of English? Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Minker</author>
</authors>
<title>Grapheme-to-phoneme conversion - an approach based on hidden markov models.</title>
<date>1996</date>
<contexts>
<context position="5395" citStr="Minker, 1996" startWordPosition="801" endWordPosition="802">t n-gram model for the graphemeto-phoneme conversion task. Models of this type have previously been shown to yield very good g2p conversion results (Bisani and Ney, 2002; Galescu and Allen, 2001; Chen, 2003). Models that do not use joint letter-phoneme states, and therefore are not conditional on the preceding letters, but only on the actual letter and the preceding phonemes, achieved inferior results. Examples of such approaches using Hidden Markov Models are (Rentzepopoulos and Kokkinakis, 1991) (who applied the HMM to the related task of phoneme-to-grapheme conversion), (Taylor, 2005) and (Minker, 1996). The g2p task is formulated as searching for the most probable sequence of phonemes given the orthographic form of a word. One can think of it as a tagging problem where each letter is tagged with a (possibly empty) phoneme-sequence p. In our particular implementation, the model is defined as a higher-order Hidden Markov Model, where the hidden states are a letter–phoneme-sequence pair hl; pi, and the observed symbols are the letters l. The output probability of a hidden state is then equal to one, since all hidden states that do not contain the observed letter are pruned directly. The model </context>
</contexts>
<marker>Minker, 1996</marker>
<rawString>W. Minker. 1996. Grapheme-to-phoneme conversion - an approach based on hidden markov models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B M¨obius</author>
</authors>
<title>German and Multilingual Speech Synthesis. phonetic AIMS, Arbeitspapiere des Instituts f¨ur Maschinelle Spachverarbeitung.</title>
<date>2001</date>
<marker>M¨obius, 2001</marker>
<rawString>B. M¨obius. 2001. German and Multilingual Speech Synthesis. phonetic AIMS, Arbeitspapiere des Instituts f¨ur Maschinelle Spachverarbeitung.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K M¨uller</author>
</authors>
<title>Automatic detection of syllable boundaries combining the advantages of treebank and bracketed corpora training.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>402--409</pages>
<marker>M¨uller, 2001</marker>
<rawString>K. M¨uller. 2001. Automatic detection of syllable boundaries combining the advantages of treebank and bracketed corpora training. In Proceedings of ACL, pages 402–409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pounder</author>
<author>M Kommenda</author>
</authors>
<title>Morphological analysis for a German text-to-speech system.</title>
<date>1986</date>
<booktitle>In COLING</booktitle>
<contexts>
<context position="2984" citStr="Pounder and Kommenda, 1986" startWordPosition="432" endWordPosition="435">weight (such as German or Dutch), it is important to know where exactly the syllable boundaries are in order to correctly calculate syllable weight. For German, (M¨uller, 2001) show that information about stress assignment and the position of a syllable within a word improve g2p conversion. 1.2 Morphological Preprocessing It has been argued that using morphological information is important for languages where morphology has an important influence on pronunciation, syllabification and word stress such as German, Dutch, Swedish or, to a smaller extent, also English (Sproat, 1996; M¨obius, 2001; Pounder and Kommenda, 1986; Black et al., 1998; Taylor, 2005). Unfortunately, these papers do not quantify the contribution of morphological preprocessing in the task. Important questions when considering the integration of a morphological component into a speech 1This issue is controversial among linguists; for an overview see (Jessen, 1998). Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 96–103, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics synthesis system are 1) How large are the improvements to be gained from morphological preproc</context>
<context position="14029" citStr="Pounder and Kommenda, 1986" startWordPosition="2250" endWordPosition="2253">ed. On small data sets, better results were achieved with v2 (see Table 5). 4 Morphological Preprocessing In German, information about morphological boundaries is needed to correctly insert glottal stops [?] in complex words, to determine irregular pronunciation of affixes (v is pronounced [v] in vertikal but [f] in ver+ticker+n, and the suffix syllable heit is not stressed although superheavy and word final) and to disambiguate letters (e.g. e is always pronounced /@/ when occurring in inflectional suffixes). Vowel length and quality has been argued to also depend on morphological structure (Pounder and Kommenda, 1986). Furthermore, morphological boundaries overrun default syllabification rules, such as the maximum onset principle. Applying default syllabification to the word “Sternanis¨ol” would result in a syllabification into Ster-na-ni-s¨ol (and subsequent phonemization to something like /SteR&amp;quot;na:nizø:l/) instead of Stern-a-nis-¨ol (/&amp;quot;SteRn?ani:s?ø:l/). Syllabification in turn affects phonemization since voiced fricatives and stops are devoiced in syllable-final position. Morphological information also helps for graphemic parsing of words such as “R¨oschen” (Engl: little rose) where the morphological bo</context>
</contexts>
<marker>Pounder, Kommenda, 1986</marker>
<rawString>A. Pounder and M. Kommenda. 1986. Morphological analysis for a German text-to-speech system. In COLING 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Rentzepopoulos</author>
<author>G K Kokkinakis</author>
</authors>
<title>Phoneme to grapheme conversion using HMM.</title>
<date>1991</date>
<booktitle>In Eurospeech.</booktitle>
<contexts>
<context position="5284" citStr="Rentzepopoulos and Kokkinakis, 1991" startWordPosition="783" endWordPosition="786">each of the components and compare our methods to state-of-the-art systems. Section 6 summarizes our results. 2 Methods We used a joint n-gram model for the graphemeto-phoneme conversion task. Models of this type have previously been shown to yield very good g2p conversion results (Bisani and Ney, 2002; Galescu and Allen, 2001; Chen, 2003). Models that do not use joint letter-phoneme states, and therefore are not conditional on the preceding letters, but only on the actual letter and the preceding phonemes, achieved inferior results. Examples of such approaches using Hidden Markov Models are (Rentzepopoulos and Kokkinakis, 1991) (who applied the HMM to the related task of phoneme-to-grapheme conversion), (Taylor, 2005) and (Minker, 1996). The g2p task is formulated as searching for the most probable sequence of phonemes given the orthographic form of a word. One can think of it as a tagging problem where each letter is tagged with a (possibly empty) phoneme-sequence p. In our particular implementation, the model is defined as a higher-order Hidden Markov Model, where the hidden states are a letter–phoneme-sequence pair hl; pi, and the observed symbols are the letters l. The output probability of a hidden state is the</context>
</contexts>
<marker>Rentzepopoulos, Kokkinakis, 1991</marker>
<rawString>P.A. Rentzepopoulos and G.K. Kokkinakis. 1991. Phoneme to grapheme conversion using HMM. In Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
<author>A Fitschen</author>
<author>U Heid</author>
</authors>
<title>SMOR: A German computational morphology covering derivation, composition and inflection.</title>
<date>2004</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="16599" citStr="Schmid et al., 2004" startWordPosition="2622" endWordPosition="2625">entation yielded the best g2p conversion results. Manual annotation is only available for a small number of words. Therefore, only automatically annotated morphological information can scale up to real applications. Rule-based Systems The traditional approach is to use large morpheme lexica and a set of rules that segment words into affixes and stems. Drawbacks of using such a system are the high development costs, limited coverage 99 and problems with ambiguity resolution between alternative analyses of a word. The two rule-based systems we evaluated, the ETI4 morphological system and SMOR5 (Schmid et al., 2004), are both high-quality systems with large lexica that have been developed over several years. Their performance results can help to estimate what can realistically be expected from an automatic segmentation system. Both of the rule-based systems achieved an F-score of approx. 80% morphological boundaries correct with respect to CELEX manual annotation. Unsupervised Morphological Systems Most attractive among automatic systems are methods that use unsupervised learning, because these require neither an expert linguist to build large rule-sets and lexica nor large manually annotated word lists,</context>
</contexts>
<marker>Schmid, Fitschen, Heid, 2004</marker>
<rawString>H. Schmid, A. Fitschen, and U. Heid. 2004. SMOR: A German computational morphology covering derivation, composition and inflection. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
<author>B M¨obius</author>
<author>J Weidenkaff</author>
</authors>
<title>Tagging syllable boundaries with hidden Markov</title>
<date>2005</date>
<booktitle>In Proc. ICSLP ’96,</booktitle>
<location>Philadelphia, PA.</location>
<marker>Schmid, M¨obius, Weidenkaff, 2005</marker>
<rawString>H. Schmid, B. M¨obius, and J. Weidenkaff. 2005. Tagging syllable boundaries with hidden Markov models. IMS, unpub. R. Sproat. 1996. Multilingual text analysis for text-to-speech synthesis. In Proc. ICSLP ’96, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Taylor</author>
</authors>
<title>Hidden Markov models for grapheme to phoneme conversion.</title>
<date>2005</date>
<booktitle>In INTERSPEECH.</booktitle>
<contexts>
<context position="3019" citStr="Taylor, 2005" startWordPosition="440" endWordPosition="441">t to know where exactly the syllable boundaries are in order to correctly calculate syllable weight. For German, (M¨uller, 2001) show that information about stress assignment and the position of a syllable within a word improve g2p conversion. 1.2 Morphological Preprocessing It has been argued that using morphological information is important for languages where morphology has an important influence on pronunciation, syllabification and word stress such as German, Dutch, Swedish or, to a smaller extent, also English (Sproat, 1996; M¨obius, 2001; Pounder and Kommenda, 1986; Black et al., 1998; Taylor, 2005). Unfortunately, these papers do not quantify the contribution of morphological preprocessing in the task. Important questions when considering the integration of a morphological component into a speech 1This issue is controversial among linguists; for an overview see (Jessen, 1998). Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 96–103, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics synthesis system are 1) How large are the improvements to be gained from morphological preprocessing? 2) Must the morphological s</context>
<context position="5376" citStr="Taylor, 2005" startWordPosition="798" endWordPosition="799">hods We used a joint n-gram model for the graphemeto-phoneme conversion task. Models of this type have previously been shown to yield very good g2p conversion results (Bisani and Ney, 2002; Galescu and Allen, 2001; Chen, 2003). Models that do not use joint letter-phoneme states, and therefore are not conditional on the preceding letters, but only on the actual letter and the preceding phonemes, achieved inferior results. Examples of such approaches using Hidden Markov Models are (Rentzepopoulos and Kokkinakis, 1991) (who applied the HMM to the related task of phoneme-to-grapheme conversion), (Taylor, 2005) and (Minker, 1996). The g2p task is formulated as searching for the most probable sequence of phonemes given the orthographic form of a word. One can think of it as a tagging problem where each letter is tagged with a (possibly empty) phoneme-sequence p. In our particular implementation, the model is defined as a higher-order Hidden Markov Model, where the hidden states are a letter–phoneme-sequence pair hl; pi, and the observed symbols are the letters l. The output probability of a hidden state is then equal to one, since all hidden states that do not contain the observed letter are pruned d</context>
</contexts>
<marker>Taylor, 2005</marker>
<rawString>P. Taylor. 2005. Hidden Markov models for grapheme to phoneme conversion. In INTERSPEECH.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>