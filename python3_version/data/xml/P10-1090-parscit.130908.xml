<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.838408">
Convolution Kernel over Packed Parse Forest
</title>
<author confidence="0.913197">
Min Zhang Hui Zhang Haizhou Li
</author>
<affiliation confidence="0.654467">
Institute for Infocomm Research
A-STAR, Singapore
</affiliation>
<email confidence="0.929814">
{mzhang,vishz,hli}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.981711" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999883684210527">
This paper proposes a convolution forest ker-
nel to effectively explore rich structured fea-
tures embedded in a packed parse forest. As
opposed to the convolution tree kernel, the
proposed forest kernel does not have to com-
mit to a single best parse tree, is thus able to
explore very large object spaces and much
more structured features embedded in a forest.
This makes the proposed kernel more robust
against parsing errors and data sparseness is-
sues than the convolution tree kernel. The pa-
per presents the formal definition of convolu-
tion forest kernel and also illustrates the com-
puting algorithm to fast compute the proposed
convolution forest kernel. Experimental results
on two NLP applications, relation extraction
and semantic role labeling, show that the pro-
posed forest kernel significantly outperforms
the baseline of the convolution tree kernel.
</bodyText>
<sectionHeader confidence="0.992477" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99964268852459">
Parse tree and packed forest of parse trees are
two widely used data structures to represent the
syntactic structure information of sentences in
natural language processing (NLP). The struc-
tured features embedded in a parse tree have
been well explored together with different ma-
chine learning algorithms and proven very useful
in many NLP applications (Collins and Duffy,
2002; Moschitti, 2004; Zhang et al., 2007). A
forest (Tomita, 1987) compactly encodes an ex-
ponential number of parse trees. In this paper, we
study how to effectively explore structured fea-
tures embedded in a forest using convolution
kernel (Haussler, 1999).
As we know, feature-based machine learning
methods are less effective in modeling highly
structured objects (Vapnik, 1998), such as parse
tree or semantic graph in NLP. This is due to the
fact that it is usually very hard to represent struc-
tured objects using vectors of reasonable dimen-
sions without losing too much information. For
example, it is computationally infeasible to enu-
merate all subtree features (using subtree a fea-
ture) for a parse tree into a linear feature vector.
Kernel-based machine learning method is a good
way to overcome this problem. Kernel methods
employ a kernel function, that must satisfy the
properties of being symmetric and positive, to
measure the similarity between two objects by
computing implicitly the dot product of certain
features of the input objects in high (or even in-
finite) dimensional feature spaces without enu-
merating all the features (Vapnik, 1998).
Many learning algorithms, such as SVM
(Vapnik, 1998), the Perceptron learning algo-
rithm (Rosenblatt, 1962) and Voted Perceptron
(Freund and Schapire, 1999), can work directly
with kernels by replacing the dot product with a
particular kernel function. This nice property of
kernel methods, that implicitly calculates the dot
product in a high-dimensional space over the
original representations of objects, has made
kernel methods an effective solution to modeling
structured objects in NLP.
In the context of parse tree, convolution tree
kernel (Collins and Duffy, 2002) defines a fea-
ture space consisting of all subtree types of parse
trees and counts the number of common subtrees
as the syntactic similarity between two parse
trees. The tree kernel has shown much success in
many NLP applications like parsing (Collins and
Duffy, 2002), semantic role labeling (Moschitti,
2004; Zhang et al., 2007), relation extraction
(Zhang et al., 2006), pronoun resolution (Yang et
al., 2006), question classification (Zhang and
Lee, 2003) and machine translation (Zhang and
Li, 2009), where the tree kernel is used to com-
pute the similarity between two NLP application
instances that are usually represented by parse
trees. However, in those studies, the tree kernel
only covers the features derived from single 1-
</bodyText>
<page confidence="0.469698">
875
</page>
<note confidence="0.9980095">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 875–885,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999176">
Figure 1. A parse tree and its 11 subtree features covered by convolution tree kernel
</figureCaption>
<figure confidence="0.998510473684211">
PP
PP
PP
PP
PP
DT NN
in bank
IN
in the
in the bank
IN
DT NN
IN
the bank
DT NN
DT NN
IN
in
IN
IN
PP
DT NN
PP
NN IN
the
PP
DT NN
bank
PP
IN DT NN
DT NN
the bank
in the bank
IN
in
IN
DT
DT NN
</figure>
<bodyText confidence="0.999146533333333">
best parse tree. This may largely compromise the
performance of tree kernel due to parsing errors
and data sparseness.
To address the above issues, this paper con-
structs a forest-based convolution kernel to mine
structured features directly from packed forest. A
packet forest compactly encodes exponential
number of n-best parse trees, and thus containing
much more rich structured features than a single
parse tree. This advantage enables the forest ker-
nel not only to be more robust against parsing
errors, but also to be able to learn more reliable
feature values and help to solve the data sparse-
ness issue that exists in the traditional tree kernel.
We evaluate the proposed kernel in two real NLP
applications, relation extraction and semantic
role labeling. Experimental results on the
benchmark data show that the forest kernel sig-
nificantly outperforms the tree kernel.
The rest of the paper is organized as follows.
Section 2 reviews the convolution tree kernel
while section 3 discusses the proposed forest
kernel in details. Experimental results are re-
ported in section 4. Finally, we conclude the pa-
per in section 5.
of its 11 subtree features covered by the convolu-
tion tree kernel. In the tree kernel, a parse treeT
is represented by a vector of integer counts of
each subtree type (i.e., subtree regardless of its
ancestors, descendants and span covered):
</bodyText>
<equation confidence="0.925028">
(# subtreetype1(T), ..., # subtreetypen(T))
</equation>
<bodyText confidence="0.997596454545455">
where # subtreetypei(T) is the occurrence number
of the ith subtree type in T. The tree kernel counts
the number of common subtrees as the syntactic
similarity between two parse trees. Since the
number of subtrees is exponential with the tree
size, it is computationally infeasible to directly
use the feature vector . To solve this com-
putational issue, Collins and Duffy (2002) pro-
posed the following tree kernel to calculate the
dot product between the above high dimensional
vectors implicitly.
</bodyText>
<equation confidence="0.999637428571429">
E # subtreetypei (T)  # subtreetypei (T2 )
i
K(T,T2) = O(T ),O(T2) 
   
= 
E E Isubtreei (n1 ) E Isubtreei (n2 )
i n1eN1 n2eN2 
</equation>
<bodyText confidence="0.96123125">
2 Convolution Kernel over Parse Tree
Convolution kernel was proposed as a concept of
kernels for discrete structures by Haussler (1999)
and related but independently conceived ideas on
string kernels first presented in (Watkins, 1999).
The framework defines the kernel function be-
tween input objects as the convolution of “sub-
kernels”, i.e. the kernels for the decompositions
(parts) of the input objects.
The parse tree kernel (Collins and Duffy, 2002)
is an instantiation of convolution kernel over
syntactic parse trees. Given a parse tree, its fea-
tures defined by a tree kernel are all of its subtree
types and the value of a given feature is the
number of the occurrences of the subtree in the
parse tree. Fig. 1 illustrates a parse tree with all
</bodyText>
<equation confidence="0.9985122">
n1eN1 n2eN2
= E EA
( , )
n n
1 2
</equation>
<bodyText confidence="0.9957825">
where N1 and N2 are the sets of nodes in trees T1
and T2, respectively, and is a function
that is 1 iff the subtreetypei occurs with root at
node n and zero otherwise, and is the
number of the common subtrees rooted at n1 and
n2, i.e.,
</bodyText>
<equation confidence="0.762941">
A(n1, n2 )  i Isubtreei (n1 ) Ibtreei (n2 )
(n1, n2) can be computed by the following recur-
sive rules:
</equation>
<page confidence="0.513528">
876
</page>
<figure confidence="0.998958322580645">
c) A Parse Tree T1
IP
PP
VP
NP
VP
NNP VV DT NN IN DT NN
John saw a man in the bank
d) A Parse Tree T2
IP
VP
NP
PP
NP
NNP VV DT NN IN DT NN
John saw a man in the bank
a) A Forest f
IP[1,7]
NP[3,4]
PP[5,7]
VP[2,7]
VP[2,4] NP[3,7]
John saw a man
( 1 )
A (n„n2)=A,• I-I (1+A(ch(n1,j),ch(n2,j))),
b) A Hyper-edge e
in the bank
IP[1,7]
NNP[1,1]
NNP[1,1] VV[2,2] DT[3,3] NN[4,4] IN[5,5] DT[6,6] NN[7,7]
VP[2,7]
</figure>
<figureCaption confidence="0.998551">
Figure 2. An example of a packed forest, a hyper-edge and two parse trees covered by the packed forest
</figureCaption>
<equation confidence="0.95564375">
Rule 1: if the productions (CFG rules) at and
n2 are different, ;
Rule 2: else if both n, and are pre-terminals
(POS tags), ;
Rule 3: else,
n
j=
1
</equation>
<bodyText confidence="0.986185352941177">
where nc(n,) is the child number of n, , ch(n ,j) is
the jth child of node and A, (0&lt;A, &lt;_1) is the de-
cay factor in order to make the kernel value less
variable with respect to the subtree sizes (Collins
and Duffy, 2002). The recursive Rule 3 holds
because given two nodes with the same children,
one can construct common subtrees using these
children and common subtrees of further
offspring. The time complexity for computing
this kernel is O( |N1  |  |N2 |) .
As discussed in previous section, when convo-
lution tree kernel is applied to NLP applications,
its performance is vulnerable to the errors from
the single parse tree and data sparseness. In this
paper, we present a convolution kernel over
packed forest to address the above issues by ex-
ploring structured features embedded in a forest.
</bodyText>
<sectionHeader confidence="0.922016" genericHeader="method">
3 Convolution Kernel over Forest
</sectionHeader>
<bodyText confidence="0.9999825">
In this section, we first illustrate the concept of
packed forest and then give a detailed discussion
on the covered feature space, fractional count,
feature value and the forest kernel function itself.
</bodyText>
<subsectionHeader confidence="0.983101">
3.1 Packed forest of parse trees
</subsectionHeader>
<bodyText confidence="0.999989733333333">
Informally, a packed parse forest, or (packed)
forest in short, is a compact representation of all
the derivations (i.e. parse trees) for a given sen-
tence under context-free grammar (Tomita, 1987;
Billot and Lang, 1989; Klein and Manning,
2001). It is the core data structure used in natural
language parsing and other downstream NLP
applications, such as syntax-based machine
translation (Zhang et al., 2008; Zhang et al.,
2009a). In parsing, a sentence corresponds to
exponential number of parse trees with different
tree probabilities, where a forest can compact all
the parse trees by sharing their common subtrees
in a bottom-up manner. Formally, a packed for-
est F can be described as a triple:
</bodyText>
<equation confidence="0.503007">
F= &lt; V,E,S&gt;
</equation>
<bodyText confidence="0.908964">
where Vis the set of non-terminal nodes, E is the
set of hyper-edges and S is a sentence
nc
</bodyText>
<page confidence="0.439293">
877
</page>
<bodyText confidence="0.999979388888889">
represented as an ordered word sequence. A hy-
per-edge 𝑒 is a group of edges in a parse tree
which connects a father node and its all child
nodes, representing a CFG rule. A non-terminal
node in a forest is represented as a “label [start,
end]”, where the “label” is its syntax category
and “[start, end]” is the span of words it covers.
As shown in Fig. 2, these two parse trees (𝑇1
and 𝑇2) can be represented as a single forest by
sharing their common subtrees (such as NP[3,4]
and PP[5,7]) and merging common non-terminal
nodes covering the same span (such as VP[2,7],
where there are two hyper-edges attach to it).
Given the definition of forest, we introduce
the concepts of inside probability β . and out-
side probability α(.) that are widely-used in
parsing (Baker, 1979; Lari and Young, 1990) and
are also to be used in our kernel calculation.
</bodyText>
<equation confidence="0.990736625">
β 𝑣 𝑝,𝑝 = 𝑃(𝑣 → 𝑆[𝑝])
β 𝑣 𝑝, 𝑞 = I 𝑃 𝑒
𝑒 𝑖𝑠 𝑎 𝑕𝑦𝑝𝑒𝑟 −𝑒𝑑𝑔𝑒
𝑎𝑡𝑡𝑎𝑐 𝑕𝑒𝑑 𝑡𝑜 𝑣
∙ 𝛽 (𝑐𝑖 [𝑝𝑖, 𝑞𝑖])
𝑐𝑖 𝑝𝑖,𝑞𝑖 𝑖𝑠 𝑎 𝑙𝑒𝑎𝑓
𝑛𝑜𝑑𝑒 𝑜𝑓 𝑒
α 𝑟𝑜𝑜𝑡(𝑓) = 1
</equation>
<bodyText confidence="0.99737576">
α 𝑣 𝑝, 𝑞 = α 𝑟𝑜𝑜𝑡 𝑒 ∙ 𝑃 𝑒
𝑒 𝑖𝑠 𝑎 𝑕𝑦𝑝𝑒𝑟 −
𝑒𝑑𝑔𝑒 𝑎𝑛𝑑 𝑣
𝑖𝑠 𝑖𝑡𝑠 𝑜𝑛𝑒
𝑙𝑒𝑎𝑓 𝑛𝑜𝑑𝑒
∙ 𝛽 (𝑐𝑖 [𝑝𝑖, 𝑞𝑖]))
𝑐𝑖 𝑝𝑖,𝑞𝑖 𝑖𝑠 𝑎
𝑐𝑕𝑖𝑙𝑑𝑟𝑒𝑛 𝑛𝑜𝑑𝑒
𝑜𝑓 𝑒 𝑒𝑥𝑐𝑒𝑝𝑡 𝑣
where 𝑣 is a forest node, 𝑆[𝑝] is the 𝑝𝑡𝑕 word of
input sentence 𝑆, 𝑃 (𝑣 → 𝑆 [𝑝]) is the probability
of the CFG rule 𝑣 → 𝑆[𝑝], 𝑟𝑜𝑜𝑡(.) returns the
root node of input structure, [𝑝𝑖, 𝑞𝑖] is a sub-span
of 𝑝, 𝑞 , being covered by 𝑐𝑖, and 𝑃 𝑒 is the
PCFG probability of 𝑒. From these definitions,
we can see that the inside probability is total
probability of generating words 𝑆 𝑝, 𝑞 from
non-terminal node 𝑣 𝑝, 𝑞 while the outside
probability is the total probability of generating
node 𝑣 𝑝, 𝑞 and words outside 𝑆[𝑝, 𝑞] from the
root of forest. The inside probability can be cal-
culated using dynamic programming in a bottom-
up fashion while the outside probability can be
calculated using dynamic programming in a top-
to-down way.
</bodyText>
<subsectionHeader confidence="0.998465">
3.2 Convolution forest kernel
</subsectionHeader>
<bodyText confidence="0.990439428571429">
In this subsection, we first define the feature
space covered by forest kernel, and then define
the forest kernel function.
3.2.1 Feature space, object space and fea-
ture value
The forest kernel counts the number of common
subtrees as the syntactic similarity between two
forests. Therefore, in the same way as tree kernel,
its feature space is also defined as all the possible
subtree types that a CFG grammar allows. In a
forest kernel, forest 𝐹 is represented by a vector
of fractional counts of each subtree type (subtree
regardless of its ancestors, descendants and span
covered):
</bodyText>
<equation confidence="0.970165">
O(F) = (# subtreetype1(F), ...,
# subtreetypen(F))
= (#subtreetype1(n-best parse trees), ..., (1)
# subtreetypen(n-best parse trees))
</equation>
<bodyText confidence="0.99998252">
where # subtreetypei(F) is the occurrence number
of the ith subtree type (subtreetypei) in forest F,
i.e., a n-best parse tree lists with a huge n.
Although the feature spaces of the two kernels
are the same, their object spaces (tree vs. forest)
and feature values (integer counts vs. fractional
counts) differ very much. A forest encodes expo-
nential number of parse trees, and thus contain-
ing exponential times more subtrees than a single
parse tree. This ensures forest kernel to learn
more reliable feature values and is also able to
help to address the data sparseness issues in a
better way than tree kernel does. Forest kernel is
also expected to yield more non-zero feature val-
ues than tree kernel. Furthermore, different parse
tree in a forest represents different derivation and
interpretation for a given sentence. Therefore,
forest kernel should be more robust to parsing
errors than tree kernel.
In tree kernel, one occurrence of a subtree
contributes 1 to the value of its corresponding
feature (subtree type), so the feature value is an
integer count. However, the case turns out very
complicated in forest kernel. In a forest, each of
its parse trees, when enumerated, has its own
</bodyText>
<page confidence="0.656149">
878
</page>
<bodyText confidence="0.999718428571429">
probability. So one subtree extracted from differ-
ent parse trees should have different fractional
count with regard to the probabilities of different
parse trees. Following the previous work (Char-
niak and Johnson, 2005; Huang, 2008), we de-
fine the fractional count of the occurrence of a
subtree in a parse tree 𝑡𝑖 as
</bodyText>
<equation confidence="0.996252">
𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑡𝑖 = 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒,𝑡𝑖|𝑓,𝑠 𝑜𝑡𝑕𝑒𝑟𝑤𝑖𝑠𝑒
0 𝑖𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∉ 𝑡𝑖
0 𝑖𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∉ 𝑡𝑖
</equation>
<bodyText confidence="0.97232825">
= 𝑃 𝑡𝑖 |𝑓, 𝑠 𝑜𝑡𝑕𝑒𝑟𝑤𝑖𝑠𝑒
where we have 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑡𝑖 |𝑓, 𝑠 = 𝑃 𝑡𝑖 |𝑓, 𝑠 if
𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∈ 𝑡𝑖. Then we define the fractional count
of the occurrence of a subtree in a forest f as
</bodyText>
<equation confidence="0.852195333333333">
𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑓 = 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒|𝑓, 𝑠
= 𝑡𝑖 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑡𝑖 |𝑓, 𝑠 (2)
= 𝑡𝑖 𝐼𝑠𝑢𝑏𝑡𝑟𝑒𝑒 𝑡𝑖 ∙ 𝑃 𝑡𝑖 |𝑓, 𝑠
</equation>
<bodyText confidence="0.999749076923077">
where 𝐼𝑠𝑢𝑏𝑡𝑟𝑒𝑒 𝑡𝑖 is a binary function that is 1
iif the 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∈ 𝑡𝑖 and zero otherwise. Ob-
viously, it needs exponential time to compute the
above fractional counts. However, due to the
property of forest that compactly represents all
the parse trees, the posterior probability of a
subtree in a forest, 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 |𝑓, 𝑠 , can be easi-
ly computed in an Inside-Outside fashion as the
product of three parts: the outside probability of
its root node, the probabilities of parse hyper-
edges involved in the subtree, and the inside
probabilities of its leaf nodes (Lari and Young,
1990; Mi and Huang, 2008).
</bodyText>
<equation confidence="0.99787775">
𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑓 = 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒|𝑓, 𝑠 (3)
𝛼𝛽(𝑠𝑢𝑏𝑡𝑟𝑒𝑒)
=
𝛼𝛽(𝑟𝑜𝑜𝑡 𝑓 )
</equation>
<bodyText confidence="0.703485">
where
</bodyText>
<equation confidence="0.906436285714286">
𝛼𝛽 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 = 𝛼 𝑟𝑜𝑜𝑡 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 (4)
∙ 𝑃 𝑒
∙ 𝑒∈𝑠𝑢𝑏𝑡𝑟𝑒𝑒 𝛽 𝑣
𝑣∈𝑙𝑒𝑎𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒
and
𝛼𝛽 𝑟𝑜𝑜𝑡 𝑓 = 𝛼 𝑟𝑜𝑜𝑡 𝑓 ∙ 𝛽 𝑟𝑜𝑜𝑡 𝑓
= 𝛽 𝑟𝑜𝑜𝑡 𝑓
</equation>
<bodyText confidence="0.998266857142857">
where 𝛼 . and 𝛽 (. ) denote the outside and in-
side probabilities. They can be easily obtained
using the equations introduced at section 3.1.
Given a subtree, we can easily compute its
fractional count (i.e. its feature value) directly
using eq. (3) and (4) without the need of enume-
rating each parse trees as shown at eq. (2)1.
Nonetheless, it is still computationally infeasible
to directly use the feature vector 𝜙 (𝐹) (see eq.
(1)) by explicitly enumerating all subtrees al-
though its fractional count is easily calculated. In
the next subsection, we present the forest kernel
that implicitly calculates the dot-product between
two 𝜙(𝐹)s in a polynomial time.
</bodyText>
<subsubsectionHeader confidence="0.598534">
3.2.2 Convolution forest kernel
</subsubsectionHeader>
<bodyText confidence="0.978004333333333">
The forest kernel counts the fractional numbers
of common subtrees as the syntactic similarity
between two forests. We define the forest kernel
</bodyText>
<table confidence="0.996746363636364">
function 𝐾𝑓 𝑓1, 𝑓2 in the following way.
𝐾𝑓 𝑓1, 𝑓2 =&lt; 𝜙 𝑓1 , 𝜙 𝑓2 &gt; (5)
= #𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑡𝑦𝑝𝑒𝑖(𝑓1). #𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑡𝑦𝑝𝑒𝑖(𝑓2)
𝑖
= 𝐼𝑒𝑞 𝑠𝑢𝑏𝑡𝑟𝑒𝑒1, 𝑠𝑢𝑏𝑡𝑟𝑒𝑒2
𝑠𝑢𝑏𝑡𝑟𝑒𝑒 1∈𝑓1
𝑠𝑢𝑏𝑡𝑟𝑒𝑒 2∈𝑓2
∙ 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒1, 𝑓1
∙ 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒2,𝑓2
=𝑣1 𝑣2∈𝑁2 Δ′ 𝑣1, 𝑣2
∈𝑁1
</table>
<bodyText confidence="0.892436">
where
</bodyText>
<listItem confidence="0.958229833333334">
• 𝐼𝑒𝑞 ∙,∙ is a binary function that is 1 iif
the input two subtrees are identical (i.e.
they have the same typology and node
labels) and zero otherwise;
• 𝑐 ∙,∙ is the fractional count defined at
eq. (3);
• 𝑁1 and 𝑁2 are the sets of nodes in fo-
rests 𝑓1 and 𝑓2;
• Δ′ 𝑣1, 𝑣2 returns the accumulated value
of products between each two fractional
counts of the common subtrees rooted at
𝑣1 and 𝑣2, i.e.,
</listItem>
<equation confidence="0.973474">
Δ′ 𝑣1, 𝑣2
= 𝐼𝑒𝑞 𝑠𝑢𝑏𝑡𝑟𝑒𝑒1, 𝑠𝑢𝑏𝑡𝑟𝑒𝑒2
E
𝑟𝑜𝑜𝑡 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 1 =𝑣1
𝑟𝑜𝑜𝑡 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 2 =𝑣2
∙ 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒1, 𝑓1
∙ 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒2,𝑓2
</equation>
<bodyText confidence="0.9173774">
1 It has been proven in parsing literatures (Baker,
1979; Lari and Young, 1990) that eq. (3) defined by
Inside-Outside probabilities is exactly to compute the
sum of those parse tree probabilities that cover the
subtree of being considered as defined at eq. (2).
</bodyText>
<page confidence="0.715889">
879
</page>
<bodyText confidence="0.999984115384615">
We next show that Δ′ 𝑣1, 𝑣2 can be computed
recursively in a polynomial time as illustrated at
Algorithm 1. To facilitate discussion, we tempo-
rarily ignore all fractional counts in Algorithm 1.
Indeed, Algorithm 1 can be viewed as a natural
extension of convolution kernel from over tree to
over forest. In forest2, a node can root multiple
hyper-edges and each hyper-edge is independent
to each other. Therefore, Algorithm 1 iterates
each hyper-edge pairs with roots at 𝑣1 and 𝑣2
(line 3-4), and sums over (eq. (7) at line 9) each
recursively-accumulated sub-kernel scores of
subtree pairs extended from the hyper-edge pair
𝑒1, 𝑒2 (eq. (6) at line 8). Eq. (7) holds because
the hyper-edges attached to the same node are
independent to each other. Eq. (6) is very similar
to the Rule 3 of tree kernel (see section 2) except
its inputs are hyper-edges and its further expan-
sion is based on forest nodes. Similar to tree ker-
nel (Collins and Duffy, 2002), eq. (6) holds be-
cause a common subtree by extending from
(𝑒1, 𝑒2) can be formed by taking the hyper-edge
(𝑒1, 𝑒2), together with a choice at each of their
leaf nodes of simply taking the non-terminal at
the leaf node, or any one of the common subtrees
with root at the leaf node. Thus there are
</bodyText>
<equation confidence="0.925657">
1 + Δ′ 𝑙𝑒𝑎𝑓 𝑒1, 𝑗 , 𝑙𝑒𝑎𝑓 𝑒2, 𝑗 possible
</equation>
<bodyText confidence="0.972177904761905">
choices at the jth leaf node. In total, there are
Δ′′ 𝑒1, 𝑒2 (eq. (6)) common subtrees by extend-
ing from (𝑒1, 𝑒2) and Δ′ 𝑣1, 𝑣2 (eq. (7)) com-
mon subtrees with root at 𝑣1, 𝑣2 .
Obviously Δ′ 𝑣1, 𝑣2 calculated by Algorithm
1 is a proper convolution kernel since it simply
counts the number of common subtrees under the
root 𝑣1, 𝑣2 . Therefore, 𝐾𝑓 𝑓1, 𝑓2 defined at eq.
(5) and calculated through Δ′ 𝑣1, 𝑣2 is also a
proper convolution kernel. From eq. (5) and Al-
gorithm 1, we can see that each hyper-edge pair
(𝑒1, 𝑒2) is only visited at most one time in com-
puting the forest kernel. Thus the time complexi-
ty for computing 𝐾𝑓 𝑓1, 𝑓2 is 𝑂(|𝐸1 |∙ |𝐸2|) ,
where 𝐸1 and 𝐸2 are the set of hyper-edges in
forests 𝑓1 and 𝑓2, respectively. Given a forest
and the best parse trees, the number of hyper-
edges is only several times (normally &lt;=3 after
pruning) than that of tree nodes in the parse tree3.
2 Tree can be viewed as a special case of forest with
only one hyper-edge attached to each tree node.
</bodyText>
<figure confidence="0.458923375">
3 Suppose there are K forest nodes in a forest, each
node has M associated hyper-edges fan out and each
hyper-edge has N children. Then the forest is capable
𝐾−1
of encoding 𝑀𝑁−1 parse trees at most (Zhang et al.,
2009b).
Algorithm 1.
Input:
</figure>
<equation confidence="0.9121045">
𝑓1, 𝑓2: two packed forests
𝑣1, 𝑣2: any two nodes of 𝑓1 and 𝑓2
Notation:
𝐼𝑒𝑞 ∙,∙ : defined at eq. (5)
𝑛𝑙 𝑒1 : number of leaf node of 𝑒1
𝑙𝑒𝑎𝑓 𝑒1, 𝑗 : the jth leaf node of 𝑒1
Output: Δ′ 𝑣1, 𝑣2
1. Δ′ 𝑣1, 𝑣2 = 0
</equation>
<listItem confidence="0.9306305">
2. if 𝑣1.𝑙𝑎𝑏𝑒𝑙 ≠ 𝑣2. 𝑙𝑎𝑏𝑒𝑙 exit
3. for each hyper-edge 𝑒1 attached to 𝑣1 do
4. for each hyper-edge 𝑒2 attached to 𝑣2 do
5. if 𝐼𝑒𝑞 𝑒1, 𝑒2 == 0 do
6. goto line 3
7. else do
</listItem>
<equation confidence="0.9990115">
𝑛𝑙 𝑒1
𝑒1, 𝑒2 = 𝑗=1 1 +
Δ′ 𝑙𝑒𝑎𝑓 𝑒1, 𝑗 , 𝑙𝑒𝑎𝑓 𝑒2, 𝑗 (6)
𝑣1, 𝑣2 += Δ′′ 𝑒1, 𝑒2 (7)
</equation>
<bodyText confidence="0.94011225">
Same as tree kernel, forest kernel is running
more efficiently in practice since only two nodes
with the same label needs to be further processed
(line 2 of Algorithm 1).
Now let us see how to integrate fractional
counts into forest kernel. According to Algo-
rithm 1 (eq. (7)), we have (𝑒1/𝑒2 are attached to
𝑣1/𝑣2, respectively)
</bodyText>
<equation confidence="0.9996025">
Δ′ 𝑣1, 𝑣2 = 𝑒1 Δ′′
=𝑒2 𝑒1, 𝑒2
</equation>
<bodyText confidence="0.98074525">
Recall eq. (4), a fractional count consists of
outside, inside and subtree probabilities. It is
more straightforward to incorporate the outside
and subtree probabilities since all the subtrees
with roots at 𝑣1, 𝑣2 share the same outside
probability and each hyper-edge pair is only vi-
sited one time. Thus we can integrate the two
probabilities into Δ′ 𝑣1, 𝑣2 as follows.
</bodyText>
<equation confidence="0.999691">
Δ′ 𝑣1, 𝑣2 = 𝜆 ∙ 𝛼 𝑣1 ∙ 𝛼 𝑣2
∙𝑒1 𝑃 =𝑒2 𝑒1 ∙ 𝑃 𝑒2 ∙ Δ′′ 𝑒1, 𝑒2 (8)
</equation>
<bodyText confidence="0.999771428571429">
where, following tree kernel, a decay factor
𝜆(0 &lt; 𝜆 ≤ 1) is also introduced in order to make
the kernel value less variable with respect to the
subtree sizes (Collins and Duffy, 2002). It func-
tions like multiplying each feature value by
𝜆𝑠𝑖𝑧𝑒𝑖, where 𝑠𝑖𝑧𝑒𝑖 is the number of hyper-edges
in 𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑖.
</bodyText>
<figure confidence="0.951517333333333">
8. Δ′′
9. Δ′
10. end if
11. end for
12. end for
880
</figure>
<bodyText confidence="0.9989848">
The inside probability is only involved when a
node does not need to be further expanded. The
integer 1 at eq. (6) represents such case. So the
inside probability is integrated into eq. (6) by
replacing the integer 1 as follows.
</bodyText>
<equation confidence="0.985670333333333">
nl(e1
a(leaf (e1, j)) • a(leaf (e2, j))) ( )
D� (leaf (e1,j), leaf (e2,j))9
</equation>
<bodyText confidence="0.999312714285714">
where in the last expression the two outside
probabilities a(leaf (e1, j)) and a(leaf (e2, j))
are removed. This is because leaf (e1, j)and
leaf (e2, j) are not roots of the subtrees of being
explored (only outside probabilities of the root of
a subtree should be counted in its fractional
count), and D� (leaf (e1, j), leaf (e2, j)) already
contains the two outside probabilities of
leaf (e1, j) and leaf (e2, j).
Referring to eq. (3), each fractional count
needs to be normalized by afi(root(f)). Since
afi (root(f )) is independent to each individual
fractional count, we do the normalization outside
the recursive function D�� e1, e2 ) . Then we can
</bodyText>
<equation confidence="0.999303">
(10)
afi(root(f1)) • afi(root(f2))
</equation>
<bodyText confidence="0.999335333333333">
Finally, since the size of input forests is not
constant, the forest kernel value is normalized
using the following equation.
</bodyText>
<equation confidence="0.998306">
Kf (f1, f2 ) = Kf(f1,f2) (11)
Kf (f1, f1 ) • Kf (f2, f2)
</equation>
<bodyText confidence="0.9999785">
From the above discussion, we can see that the
proposed forest kernel is defined together by eqs.
(11), (10), (9) and (8). Thanks to the compact
representation of trees in forest and the recursive
nature of the kernel function, the introduction of
fractional counts and normalization do not
change the convolution property and the time
complexity of the forest kernel. Therefore, the
forest kernel Kf (f1, f2 ) is still a proper convolu-
tion kernel with quadratic time complexity.
</bodyText>
<subsectionHeader confidence="0.999748">
3.3 Comparison with previous work
</subsectionHeader>
<bodyText confidence="0.999982957446809">
To the best of our knowledge, this is the first
work to address convolution kernel over packed
parse forest.
Convolution tree kernel is a special case of the
proposed forest kernel. From feature exploration
viewpoint, although theoretically they explore
the same subtree feature spaces (defined recur-
sively by CFG parsing rules), their feature values
are different. Forest encodes exponential number
of trees. So the number of subtree instances ex-
tracted from a forest is exponential number of
times greater than that from its corresponding
parse tree. The significant difference of the
amount of subtree instances makes the parame-
ters learned from forests more reliable and also
can help to address the data sparseness issue. To
some degree, forest kernel can be viewed as a
tree kernel with very powerful back-off mechan-
ism. In addition, forest kernel is much more ro-
bust against parsing errors than tree kernel.
Aiolli et al. (2006; 2007) propose using Direct
Acyclic Graphs (DAG) as a compact representa-
tion of tree kernel-based models. This can largely
reduce the computational burden and storage re-
quirements by sharing the common structures
and feature vectors in the kernel-based model.
There are a few other previous works done by
generalizing convolution tree kernels (Kashima
and Koyanagi, 2003; Moschitti, 2006; Zhang et
al., 2007). However, all of these works limit
themselves to single tree structure from modeling
viewpoint in nature.
From a broad viewpoint, as suggested by one
reviewer of the paper, we can consider the forest
kernel as an alternative solution proposed for the
general problem of noisy inference pipelines (eg.
speech translation by composition of FSTs, ma-
chine translation by translating over &apos;lattices&apos; of
segmentations (Dyer et al., 2008) or using parse
tree info for downstream applications in our cas-
es) . Following this line, Bunescu (2008) and
Finkel et al. (2006) are two typical related works
done in reducing cascading noisy. However, our
works are not overlapped with each other as
there are two totally different solutions for the
same general problem. In addition, the main mo-
tivation of this paper is also different from theirs.
</bodyText>
<sectionHeader confidence="0.997647" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99995575">
Forest kernel has a broad application potential in
NLP. In this section, we verify the effectiveness
of the forest kernel on two NLP applications,
semantic role labeling (SRL) (Gildea, 2002) and
relation extraction (RE) (ACE, 2002-2006).
In our experiments, SVM (Vapnik, 1998) is
selected as our classifier and the one vs. others
strategy is adopted to select the one with the
</bodyText>
<equation confidence="0.852662571428571">
D�� (e1, e2 ) = (fi(leaf(e1,j)) • fi (leaf (e2,j
j=1
+
re-formulize eq. (5) as
Kf(f1,f2 ) =&lt; 95(f1 ), 95 (f2 ) &gt;
V1EN1 L V2EN2 D V1, V2
881
</equation>
<bodyText confidence="0.999838636363636">
largest margin as the final answer. In our imple-
mentation, we use the binary SVMLight (Joa-
chims, 1998) and borrow the framework of the
Tree Kernel Tools (Moschitti, 2004) to integrate
our forest kernel into the SVMLight. We modify
Charniak parser (Charniak, 2001) to output a
packed forest. Following previous forest-based
studies (Charniak and Johnson, 2005), we use the
marginal probabilities of hyper-edges (i.e., the
Viterbi-style inside-outside probabilities and set
the pruning threshold as 8) for forest pruning.
</bodyText>
<subsectionHeader confidence="0.998439">
4.1 Semantic role labeling
</subsectionHeader>
<bodyText confidence="0.992180963636364">
Given a sentence and each predicate (either a
target verb or a noun), SRL recognizes and maps
all the constituents in the sentence into their cor-
responding semantic arguments (roles, e.g., A0
for Agent, A1 for Patient ...) of the predicate or
non-argument. We use the CoNLL-2005 shared
task on Semantic Role Labeling (Carreras and
Ma rquez, 2005) for the evaluation of our forest
kernel method. To speed up the evaluation
process, the same as Che et al. (2008), we use a
subset of the entire training corpus (WSJ sections
02-05 of the entire sections 02-21) for training,
section 24 for development and section 23 for
test, where there are 35 roles including 7 Core
(A0–A5, AA), 14 Adjunct (AM-) and 14 Refer-
ence (R-) arguments.
The state-of-the-art SRL methods (Carreras
and Ma rquez, 2005) use constituents as the labe-
ling units to form the labeled arguments. Due to
the errors from automatic parsing, it is impossi-
ble for all arguments to find their matching con-
stituents in the single 1-best parse trees. Statistics
on the training data shows that 9.78% of argu-
ments have no matching constituents using the
Charniak parser (Charniak, 2001), and the num-
ber increases to 11.76% when using the Collins
parser (Collins, 1999). In our method, we break
the limitation of 1-best parse tree and regard each
span rooted by a single forest node (i.e., a sub-
forest with one or more roots) as a candidate ar-
gument. This largely reduces the unmatched ar-
guments from 9.78% to 1.31% after forest prun-
ing. However, it also results in a very large
amount of argument candidates that is 5.6 times
as many as that from 1-best tree. Fortunately,
after the pre-processing stage of argument prun-
ing (Xue and Palmer, 2004) 4 , although the
4 We extend (Xue and Palmer, 2004)’s argument
pruning algorithm from tree-based to forest-based.
The algorithm is very effective. It can prune out
around 90% argument candidates in parse tree-based
amount of unmatched argument increases a little
bit to 3.1%, its generated total candidate amount
decreases substantially to only 1.31 times of that
from 1-best parse tree. This clearly shows the
advantages of the forest-based method over tree-
based in SRL.
The best-reported tree kernel method for SRL
𝐾𝑕𝑦𝑏𝑟𝑖𝑑 = 𝜃∙ 𝐾𝑝𝑎𝑡𝑕+ (1 − 𝜃) ∙ 𝐾𝑐𝑠 (0 ≤ 𝜃≤
1), proposed by Che et al. (2006)5, is adopted as
our baseline kernel. We implemented the 𝐾𝑕𝑦𝑏𝑟𝑖𝑑
in tree case (𝐾𝑇−𝑕𝑦𝑏𝑟𝑖𝑑 , using tree kernel to
compute 𝐾𝑝𝑎𝑡 𝑕 and 𝐾𝑐𝑠) and in forest case
(𝐾𝐹−𝑕𝑦𝑏𝑟𝑖𝑑 , using tree kernel to compute 𝐾𝑝𝑎𝑡 𝑕
and 𝐾𝑐𝑠).
</bodyText>
<table confidence="0.999886333333333">
Precision Recall F-Score
𝐾𝑇−𝑕𝑦𝑏𝑟𝑖𝑑 (Tree) 76.02 67.38 71.44
𝐾𝐹−𝑕𝑦𝑏𝑟𝑖𝑑 (Forest) 79.06 69.12 73.76
</table>
<tableCaption confidence="0.999874">
Table 1: Performance comparison of SRL (%)
</tableCaption>
<bodyText confidence="0.978026029411765">
Table 1 shows that the forest kernel significant-
ly outperforms (𝜒2 test with p=0.01) the tree ker-
nel with an absolute improvement of 2.32 (73.76-
71.42) percentage in F-Score, representing a rela-
tive error rate reduction of 8.19% (2.32/(100-
71.64)). This convincingly demonstrates the ad-
vantage of the forest kernel over the tree kernel. It
suggests that the structured features represented
by subtree are very useful to SRL. The perfor-
mance improvement is mainly due to the fact that
forest encodes much more such structured features
and the forest kernel is able to more effectively
capture such structured features than the tree ker-
nel. Besides F-Score, both precision and recall
also show significantly improvement (𝜒2 test with
p=0.01). The reason for recall improvement is
mainly due to the lower rate of unmatched argu-
ment (3.1% only) with only a little bit overhead
(1.31 times) (see the previous discussion in this
section). The precision improvement is mainly
attributed to fact that we use sub-forest to
represent argument instances, rather than sub-
tree used in tree kernel, where the sub-tree is on-
ly one tree encoded in the sub-forest.
SRL and thus makes the amounts of positive and neg-
ative training instances (arguments) more balanced.
We apply the same pruning strategies to forest plus
our heuristic rules to prune out some of the arguments
with span overlapped with each other and those ar-
guments with very small inside probabilities, depend-
ing on the numbers of candidates in the span.
5 Kpath and Kcs are two standard convolution tree ker-
nels to describe predicate-argument path substructures
and argument syntactic substructures, respectively.
</bodyText>
<page confidence="0.556239">
882
</page>
<subsectionHeader confidence="0.9219">
4.2 Relation extraction
</subsectionHeader>
<bodyText confidence="0.999984666666667">
As a subtask of information extraction, relation
extraction is to extract various semantic relations
between entity pairs from text. For example, the
sentence “Bill Gates is chairman and chief soft-
ware architect of Microsoft Corporation” con-
veys the semantic relation “EMPLOY-
MENT.executive” between the entities “Bill
Gates” (person) and “Microsoft Corporation”
(company). We adopt the method reported in
Zhang et al. (2006) as our baseline method as it
reports the state-of-the-art performance using
tree kernel-based composite kernel method for
RE. We replace their tree kernels with our forest
kernels and use the same experimental settings as
theirs. We carry out the same five-fold cross va-
lidation experiment on the same subset of ACE
2004 data (LDC2005T09, ACE 2002-2004) as
that in Zhang et al. (2006). The data contain 348
documents and 4400 relation instances.
In SRL, constituents are used as the labeling
units to form the labeled arguments. However,
previous work (Zhang et al., 2006) shows that if
we use complete constituent (MCT) as done in
SRL to represent relation instance, there is a
large performance drop compared with using the
path-enclosed tree (PT)6. By simulating PT, we
use the minimal fragment of a forest covering the
two entities and their internal words to represent
a relation instance by only parsing the span cov-
ering the two entities and their internal words.
</bodyText>
<table confidence="0.997815666666667">
Precision Recall F-Score
Zhang et al. (2006):Tree 68.6 59.3 63.6
Ours: Forest 70.3 60.0 64.7
</table>
<tableCaption confidence="0.9648045">
Table 2: Performance Comparison of RE (%)
over 23 subtypes on the ACE 2004 data
</tableCaption>
<bodyText confidence="0.996304862068966">
Table 2 compares the performance of the for-
est kernel and the tree kernel on relation extrac-
tion. We can see that the forest kernel significant-
ly outperforms (𝜒2 test with p=0.05) the tree ker-
nel by 1.1 point of F-score. This further verifies
the effectiveness of the forest kernel method for
6 MCT is the minimal constituent rooted by the near-
est common ancestor of the two entities under consid-
eration while PT is the minimal portion of the parse
tree (may not be a complete subtree) containing the
two entities and their internal lexical words. Since in
many cases, the two entities and their internal words
cannot form a grammatical constituent, MCT may
introduce too many noisy context features and thus
lead to the performance drop.
modeling NLP structured data. In summary, we
further observe the high precision improvement
that is consistent with the SRL experiments. How-
ever, the recall improvement is not as significant
as observed in SRL. This is because unlike SRL,
RE has no un-matching issues in generating rela-
tion instances. Moreover, we find that the perfor-
mance improvement in RE is not as good as that
in SRL. Although we know that performance is
task-dependent, one of the possible reasons is
that SRL tends to be long-distance grammatical
structure-related while RE is local and semantic-
related as observed from the two experimental
benchmark data.
</bodyText>
<sectionHeader confidence="0.993536" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999835">
Many NLP applications have benefited from the
success of convolution kernel over parse tree.
Since a packed parse forest contains much richer
structured features than a parse tree, we are mo-
tivated to develop a technology to measure the
syntactic similarity between two forests.
To achieve this goal, in this paper, we design a
convolution kernel over packed forest by genera-
lizing the tree kernel. We analyze the object
space of the forest kernel, the fractional count for
feature value computing and design a dynamic
programming algorithm to realize the forest ker-
nel with quadratic time complexity. Compared
with the tree kernel, the forest kernel is more ro-
bust against parsing errors and data sparseness
issues. Among the broad potential NLP applica-
tions, the problems in SRL and RE provide two
pointed scenarios to verify our forest kernel. Ex-
perimental results demonstrate the effectiveness
of the proposed kernel in structured NLP data
modeling and the advantages over tree kernel.
In the future, we would like to verify the forest
kernel in more NLP applications. In addition, as
suggested by one reviewer, we may consider res-
caling the probabilities (exponentiating them by
a constant value) that are used to compute the
fractional counts. We can sharpen or flatten the
distributions. This basically says &amp;quot;how seriously
do we want to take the very best derivation&amp;quot;
compared to the rest. However, the challenge is
that we compute the fractional counts together
with the forest kernel recursively by using the
Inside-Outside probabilities. We cannot differen-
tiate the individual parse tree’s contribution to a
fractional count on the fly. One possible solution
is to do the probability rescaling off-line before
kernel calculation. This would be a very interest-
ing research topic of our future work.
</bodyText>
<page confidence="0.907527">
883
</page>
<sectionHeader confidence="0.992599" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999585613861386">
ACE (2002-2006). The Automatic Content Extraction
Projects. http://www.ldc.upenn.edu/Projects/ACE/
Fabio Aiolli, Giovanni Da San Martino, Alessandro
Sperduti and Alessandro Moschitti. 2006. Fast On-
line Kernel Learning for Trees. ICDM-2006
Fabio Aiolli, Giovanni Da San Martino, Alessandro
Sperduti and Alessandro Moschitti. 2007. Efficient
Kernel-based Learning for Trees. IEEE Sympo-
sium on Computational Intelligence and Data Min-
ing (CIDM-2007)
J. Baker. 1979. Trainable grammars for speech rec-
ognition. The 97th meeting of the Acoustical So-
ciety of America
S. Billot and S. Lang. 1989. The structure of shared
forest in ambiguous parsing. ACL-1989
Razvan Bunescu. 2008. Learning with Probabilistic
Features for Improved Pipeline Models. EMNLP-
2008
X. Carreras and Lluıs Marquez. 2005. Introduction to
the CoNLL-2005 shared task: SRL. CoNLL-2005
E. Charniak. 2001. Immediate-head Parsing for Lan-
guage Models. ACL-2001
E. Charniak and Mark Johnson. 2005. Corse-to-fine-
grained n-best parsing and discriminative re-
ranking. ACL-2005
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li.
2006. A hybrid convolution tree kernel for seman-
tic role labeling. COLING-ACL-2006 (poster)
WanXiang Che, Min Zhang, Aiti Aw, Chew Lim Tan,
Ting Liu and Sheng Li. 2008. Using a Hybrid
Convolution Tree Kernel for Semantic Role Labe-
ling. ACM Transaction on Asian Language Infor-
mation Processing
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. dissertation,
Pennsylvania University
M. Collins and N. Duffy. 2002. Convolution Kernels
for Natural Language. NIPS-2002
Christopher Dyer, Smaranda Muresan and Philip Res-
nik. 2008. Generalizing Word Lattice Translation.
ACL-HLT-2008
Jenny Rose Finkel, Christopher D. Manning and And-
rew Y. Ng. 2006. Solving the Problem of Cascad-
ing Errors: Approximate Bayesian Inference for
Linguistic Annotation Pipelines. EMNLP-2006
Y. Freund and R. E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277-296
D. Guldea. 2002. Probabilistic models of verb-
argument structure. COLING-2002
D. Haussler. 1999. Convolution Kernels on Discrete
Structures. Technical Report UCS-CRL-99-10,
University of California, Santa Cruz
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. ACL-2008
Karim Lari and Steve J. Young. 1990. The estimation
of stochastic context-free grammars using the in-
side-outside algorithm. Computer Speech and Lan-
guage. 4(35–56)
H. Kashima and T. Koyanagi. 2003. Kernels for Semi-
Structured Data. ICML-2003
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. IWPT-2001
T. Joachims. 1998. Text Categorization with Support
Vecor Machine: learning with many relevant fea-
tures. ECML-1998
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. EMNLP-2008
Alessandro Moschitti. 2004. A Study on Convolution
Kernels for Shallow Semantic Parsing. ACL-2004
Alessandro Moschitti. 2006. Syntactic kernels for
natural language learning: the semantic role labe-
ling case. HLT-NAACL-2006 (short paper)
Martha Palmer, Dan Gildea and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Computational Linguistics. 31(1)
F. Rosenblatt. 1962. Principles of Neurodynamics:
Perceptrons and the theory of brain mechanisms.
Spartan Books, Washington D.C.
Masaru Tomita. 1987. An Efficient Augmented-
Context-Free Parsing Algorithm. Computational
Linguistics 13(1-2): 31-46
Vladimir N. Vapnik. 1998. Statistical Learning
Theory. Wiley
C. Watkins. 1999. Dynamic alignment kernels. In A. J.
Smola, B. Sch¨olkopf, P. Bartlett, and D. Schuur-
mans (Eds.), Advances in kernel methods. MIT
Press
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. EMNLP-2004
Xiaofeng Yang, Jian Su and Chew Lim Tan. 2006.
Kernel-Based Pronoun Resolution with Structured
Syntactic Knowledge. COLING-ACL-2006
Dell Zhang and W. Lee. 2003. Question classification
using support vector machines. SIGIR-2003
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and
Chew Lim Tan. 2009a. Forest-based Tree Se-
quence to String Translation Model. ACL-
IJCNLP-2009
Hui Zhang, Min Zhang, Haizhou Li and Chew Lim
Tan. 2009b. Fast Translation Rule Matching for
</reference>
<page confidence="0.661257">
884
</page>
<reference confidence="0.995522294117647">
Syntax-based Statistical Machine Translation.
EMNLP-2009
Min Zhang, Jie Zhang, Jian Su and GuoDong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Fea-
tures. COLING-ACL-2006
Min Zhang, W. Che, A. Aw, C. Tan, G. Zhou, T. Liu
and S. Li. 2007. A Grammar-driven Convolution
Tree Kernel for Semantic Role Classification.
ACL-2007
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation
Model. ACL-2008
Min Zhang and Haizhou Li. 2009. Tree Kernel-based
SVM with Structured Syntactic Knowledge for
BTG-based Phrase Reordering. EMNLP-2009
</reference>
<page confidence="0.936479">
885
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.548877">
<title confidence="0.997889">Convolution Kernel over Packed Parse Forest</title>
<author confidence="0.999674">Min Zhang Hui Zhang Haizhou Li</author>
<affiliation confidence="0.999804">Institute for Infocomm Research</affiliation>
<address confidence="0.572125">A-STAR, Singapore</address>
<email confidence="0.972398">mzhang@i2r.a-star.edu.sg</email>
<email confidence="0.972398">vishz@i2r.a-star.edu.sg</email>
<email confidence="0.972398">hli@i2r.a-star.edu.sg</email>
<abstract confidence="0.99865095">This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>ACE</author>
</authors>
<title>The Automatic Content Extraction Projects.</title>
<note>http://www.ldc.upenn.edu/Projects/ACE/</note>
<marker>ACE, </marker>
<rawString>ACE (2002-2006). The Automatic Content Extraction Projects. http://www.ldc.upenn.edu/Projects/ACE/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Aiolli</author>
<author>Giovanni Da San Martino</author>
</authors>
<title>Alessandro Sperduti and Alessandro Moschitti.</title>
<date>2006</date>
<pages>2006</pages>
<marker>Aiolli, Martino, 2006</marker>
<rawString>Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti and Alessandro Moschitti. 2006. Fast Online Kernel Learning for Trees. ICDM-2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Aiolli</author>
<author>Giovanni Da San Martino</author>
</authors>
<title>Alessandro Sperduti and Alessandro Moschitti.</title>
<date>2007</date>
<booktitle>IEEE Symposium on Computational Intelligence and Data Mining (CIDM-2007)</booktitle>
<marker>Aiolli, Martino, 2007</marker>
<rawString>Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti and Alessandro Moschitti. 2007. Efficient Kernel-based Learning for Trees. IEEE Symposium on Computational Intelligence and Data Mining (CIDM-2007)</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>The 97th meeting of the Acoustical</booktitle>
<publisher>Society of America</publisher>
<contexts>
<context position="10840" citStr="Baker, 1979" startWordPosition="1847" endWordPosition="1848"> rule. A non-terminal node in a forest is represented as a “label [start, end]”, where the “label” is its syntax category and “[start, end]” is the span of words it covers. As shown in Fig. 2, these two parse trees (𝑇1 and 𝑇2) can be represented as a single forest by sharing their common subtrees (such as NP[3,4] and PP[5,7]) and merging common non-terminal nodes covering the same span (such as VP[2,7], where there are two hyper-edges attach to it). Given the definition of forest, we introduce the concepts of inside probability β . and outside probability α(.) that are widely-used in parsing (Baker, 1979; Lari and Young, 1990) and are also to be used in our kernel calculation. β 𝑣 𝑝,𝑝 = 𝑃(𝑣 → 𝑆[𝑝]) β 𝑣 𝑝, 𝑞 = I 𝑃 𝑒 𝑒 𝑖𝑠 𝑎 𝑦𝑝𝑒𝑟 −𝑒𝑑𝑔𝑒 𝑎𝑡𝑡𝑎𝑐 𝑒𝑑 𝑡𝑜 𝑣 ∙ 𝛽 (𝑐𝑖 [𝑝𝑖, 𝑞𝑖]) 𝑐𝑖 𝑝𝑖,𝑞𝑖 𝑖𝑠 𝑎 𝑙𝑒𝑎𝑓 𝑛𝑜𝑑𝑒 𝑜𝑓 𝑒 α 𝑟𝑜𝑜𝑡(𝑓) = 1 α 𝑣 𝑝, 𝑞 = α 𝑟𝑜𝑜𝑡 𝑒 ∙ 𝑃 𝑒 𝑒 𝑖𝑠 𝑎 𝑦𝑝𝑒𝑟 − 𝑒𝑑𝑔𝑒 𝑎𝑛𝑑 𝑣 𝑖𝑠 𝑖𝑡𝑠 𝑜𝑛𝑒 𝑙𝑒𝑎𝑓 𝑛𝑜𝑑𝑒 ∙ 𝛽 (𝑐𝑖 [𝑝𝑖, 𝑞𝑖])) 𝑐𝑖 𝑝𝑖,𝑞𝑖 𝑖𝑠 𝑎 𝑐𝑖𝑙𝑑𝑟𝑒𝑛 𝑛𝑜𝑑𝑒 𝑜𝑓 𝑒 𝑒𝑥𝑐𝑒𝑝𝑡 𝑣 where 𝑣 is a forest node, 𝑆[𝑝] is the 𝑝𝑡 word of input sentence 𝑆, 𝑃 (𝑣 → 𝑆 [𝑝]) is the probability of the CFG rule 𝑣 → 𝑆[𝑝], 𝑟𝑜𝑜𝑡(.) returns the root node of input structure, [𝑝𝑖, 𝑞𝑖] is a sub-span of 𝑝, 𝑞 , being covered by 𝑐𝑖, and 𝑃 𝑒 is the PCFG probab</context>
<context position="16929" citStr="Baker, 1979" startWordPosition="2980" endWordPosition="2981"> ∙ 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒2,𝑓2 =𝑣1 𝑣2∈𝑁2 Δ′ 𝑣1, 𝑣2 ∈𝑁1 where • 𝐼𝑒𝑞 ∙,∙ is a binary function that is 1 iif the input two subtrees are identical (i.e. they have the same typology and node labels) and zero otherwise; • 𝑐 ∙,∙ is the fractional count defined at eq. (3); • 𝑁1 and 𝑁2 are the sets of nodes in forests 𝑓1 and 𝑓2; • Δ′ 𝑣1, 𝑣2 returns the accumulated value of products between each two fractional counts of the common subtrees rooted at 𝑣1 and 𝑣2, i.e., Δ′ 𝑣1, 𝑣2 = 𝐼𝑒𝑞 𝑠𝑢𝑏𝑡𝑟𝑒𝑒1, 𝑠𝑢𝑏𝑡𝑟𝑒𝑒2 E 𝑟𝑜𝑜𝑡 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 1 =𝑣1 𝑟𝑜𝑜𝑡 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 2 =𝑣2 ∙ 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒1, 𝑓1 ∙ 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒2,𝑓2 1 It has been proven in parsing literatures (Baker, 1979; Lari and Young, 1990) that eq. (3) defined by Inside-Outside probabilities is exactly to compute the sum of those parse tree probabilities that cover the subtree of being considered as defined at eq. (2). 879 We next show that Δ′ 𝑣1, 𝑣2 can be computed recursively in a polynomial time as illustrated at Algorithm 1. To facilitate discussion, we temporarily ignore all fractional counts in Algorithm 1. Indeed, Algorithm 1 can be viewed as a natural extension of convolution kernel from over tree to over forest. In forest2, a node can root multiple hyper-edges and each hyper-edge is independent t</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J. Baker. 1979. Trainable grammars for speech recognition. The 97th meeting of the Acoustical Society of America</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Billot</author>
<author>S Lang</author>
</authors>
<title>The structure of shared forest in ambiguous parsing.</title>
<date>1989</date>
<pages>1989</pages>
<contexts>
<context position="9475" citStr="Billot and Lang, 1989" startWordPosition="1606" endWordPosition="1609">we present a convolution kernel over packed forest to address the above issues by exploring structured features embedded in a forest. 3 Convolution Kernel over Forest In this section, we first illustrate the concept of packed forest and then give a detailed discussion on the covered feature space, fractional count, feature value and the forest kernel function itself. 3.1 Packed forest of parse trees Informally, a packed parse forest, or (packed) forest in short, is a compact representation of all the derivations (i.e. parse trees) for a given sentence under context-free grammar (Tomita, 1987; Billot and Lang, 1989; Klein and Manning, 2001). It is the core data structure used in natural language parsing and other downstream NLP applications, such as syntax-based machine translation (Zhang et al., 2008; Zhang et al., 2009a). In parsing, a sentence corresponds to exponential number of parse trees with different tree probabilities, where a forest can compact all the parse trees by sharing their common subtrees in a bottom-up manner. Formally, a packed forest F can be described as a triple: F= &lt; V,E,S&gt; where Vis the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence nc 877 represente</context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>S. Billot and S. Lang. 1989. The structure of shared forest in ambiguous parsing. ACL-1989</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
</authors>
<title>Learning with Probabilistic Features for Improved Pipeline Models.</title>
<date>2008</date>
<pages>2008</pages>
<contexts>
<context position="24837" citStr="Bunescu (2008)" startWordPosition="4410" endWordPosition="4411">ree kernels (Kashima and Koyanagi, 2003; Moschitti, 2006; Zhang et al., 2007). However, all of these works limit themselves to single tree structure from modeling viewpoint in nature. From a broad viewpoint, as suggested by one reviewer of the paper, we can consider the forest kernel as an alternative solution proposed for the general problem of noisy inference pipelines (eg. speech translation by composition of FSTs, machine translation by translating over &apos;lattices&apos; of segmentations (Dyer et al., 2008) or using parse tree info for downstream applications in our cases) . Following this line, Bunescu (2008) and Finkel et al. (2006) are two typical related works done in reducing cascading noisy. However, our works are not overlapped with each other as there are two totally different solutions for the same general problem. In addition, the main motivation of this paper is also different from theirs. 4 Experiments Forest kernel has a broad application potential in NLP. In this section, we verify the effectiveness of the forest kernel on two NLP applications, semantic role labeling (SRL) (Gildea, 2002) and relation extraction (RE) (ACE, 2002-2006). In our experiments, SVM (Vapnik, 1998) is selected </context>
</contexts>
<marker>Bunescu, 2008</marker>
<rawString>Razvan Bunescu. 2008. Learning with Probabilistic Features for Improved Pipeline Models. EMNLP2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>Lluıs Marquez</author>
</authors>
<date>2005</date>
<booktitle>Introduction to the CoNLL-2005 shared task: SRL. CoNLL-2005</booktitle>
<marker>Carreras, Marquez, 2005</marker>
<rawString>X. Carreras and Lluıs Marquez. 2005. Introduction to the CoNLL-2005 shared task: SRL. CoNLL-2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head Parsing for Language Models.</title>
<date>2001</date>
<contexts>
<context position="25928" citStr="Charniak, 2001" startWordPosition="4598" endWordPosition="4599">ling (SRL) (Gildea, 2002) and relation extraction (RE) (ACE, 2002-2006). In our experiments, SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted to select the one with the D�� (e1, e2 ) = (fi(leaf(e1,j)) • fi (leaf (e2,j j=1 + re-formulize eq. (5) as Kf(f1,f2 ) =&lt; 95(f1 ), 95 (f2 ) &gt; V1EN1 L V2EN2 D V1, V2 881 largest margin as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and borrow the framework of the Tree Kernel Tools (Moschitti, 2004) to integrate our forest kernel into the SVMLight. We modify Charniak parser (Charniak, 2001) to output a packed forest. Following previous forest-based studies (Charniak and Johnson, 2005), we use the marginal probabilities of hyper-edges (i.e., the Viterbi-style inside-outside probabilities and set the pruning threshold as 8) for forest pruning. 4.1 Semantic role labeling Given a sentence and each predicate (either a target verb or a noun), SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles, e.g., A0 for Agent, A1 for Patient ...) of the predicate or non-argument. We use the CoNLL-2005 shared task on Semantic Role Labeling</context>
<context position="27355" citStr="Charniak, 2001" startWordPosition="4831" endWordPosition="4832">of the entire sections 02-21) for training, section 24 for development and section 23 for test, where there are 35 roles including 7 Core (A0–A5, AA), 14 Adjunct (AM-) and 14 Reference (R-) arguments. The state-of-the-art SRL methods (Carreras and Ma rquez, 2005) use constituents as the labeling units to form the labeled arguments. Due to the errors from automatic parsing, it is impossible for all arguments to find their matching constituents in the single 1-best parse trees. Statistics on the training data shows that 9.78% of arguments have no matching constituents using the Charniak parser (Charniak, 2001), and the number increases to 11.76% when using the Collins parser (Collins, 1999). In our method, we break the limitation of 1-best parse tree and regard each span rooted by a single forest node (i.e., a subforest with one or more roots) as a candidate argument. This largely reduces the unmatched arguments from 9.78% to 1.31% after forest pruning. However, it also results in a very large amount of argument candidates that is 5.6 times as many as that from 1-best tree. Fortunately, after the pre-processing stage of argument pruning (Xue and Palmer, 2004) 4 , although the 4 We extend (Xue and P</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. Immediate-head Parsing for Language Models. ACL-2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Corse-to-finegrained n-best parsing and discriminative reranking.</title>
<date>2005</date>
<pages>2005</pages>
<contexts>
<context position="14077" citStr="Charniak and Johnson, 2005" startWordPosition="2429" endWordPosition="2433">rpretation for a given sentence. Therefore, forest kernel should be more robust to parsing errors than tree kernel. In tree kernel, one occurrence of a subtree contributes 1 to the value of its corresponding feature (subtree type), so the feature value is an integer count. However, the case turns out very complicated in forest kernel. In a forest, each of its parse trees, when enumerated, has its own 878 probability. So one subtree extracted from different parse trees should have different fractional count with regard to the probabilities of different parse trees. Following the previous work (Charniak and Johnson, 2005; Huang, 2008), we define the fractional count of the occurrence of a subtree in a parse tree 𝑡𝑖 as 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑡𝑖 = 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒,𝑡𝑖|𝑓,𝑠 𝑜𝑡𝑒𝑟𝑤𝑖𝑠𝑒 0 𝑖𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∉ 𝑡𝑖 0 𝑖𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∉ 𝑡𝑖 = 𝑃 𝑡𝑖 |𝑓, 𝑠 𝑜𝑡𝑒𝑟𝑤𝑖𝑠𝑒 where we have 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑡𝑖 |𝑓, 𝑠 = 𝑃 𝑡𝑖 |𝑓, 𝑠 if 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∈ 𝑡𝑖. Then we define the fractional count of the occurrence of a subtree in a forest f as 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑓 = 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒|𝑓, 𝑠 = 𝑡𝑖 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑡𝑖 |𝑓, 𝑠 (2) = 𝑡𝑖 𝐼𝑠𝑢𝑏𝑡𝑟𝑒𝑒 𝑡𝑖 ∙ 𝑃 𝑡𝑖 |𝑓, 𝑠 where 𝐼𝑠𝑢𝑏𝑡𝑟𝑒𝑒 𝑡𝑖 is a binary function that is 1 iif the 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∈ 𝑡𝑖 and zero otherwise. Obviously, it needs exponential time to compute the above fractional co</context>
<context position="26024" citStr="Charniak and Johnson, 2005" startWordPosition="4609" endWordPosition="4612">riments, SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted to select the one with the D�� (e1, e2 ) = (fi(leaf(e1,j)) • fi (leaf (e2,j j=1 + re-formulize eq. (5) as Kf(f1,f2 ) =&lt; 95(f1 ), 95 (f2 ) &gt; V1EN1 L V2EN2 D V1, V2 881 largest margin as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and borrow the framework of the Tree Kernel Tools (Moschitti, 2004) to integrate our forest kernel into the SVMLight. We modify Charniak parser (Charniak, 2001) to output a packed forest. Following previous forest-based studies (Charniak and Johnson, 2005), we use the marginal probabilities of hyper-edges (i.e., the Viterbi-style inside-outside probabilities and set the pruning threshold as 8) for forest pruning. 4.1 Semantic role labeling Given a sentence and each predicate (either a target verb or a noun), SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles, e.g., A0 for Agent, A1 for Patient ...) of the predicate or non-argument. We use the CoNLL-2005 shared task on Semantic Role Labeling (Carreras and Ma rquez, 2005) for the evaluation of our forest kernel method. To speed up the e</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and Mark Johnson. 2005. Corse-to-finegrained n-best parsing and discriminative reranking. ACL-2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Min Zhang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>A hybrid convolution tree kernel for semantic role labeling.</title>
<date>2006</date>
<note>COLING-ACL-2006 (poster)</note>
<contexts>
<context position="28508" citStr="Che et al. (2006)" startWordPosition="5030" endWordPosition="5033">g (Xue and Palmer, 2004) 4 , although the 4 We extend (Xue and Palmer, 2004)’s argument pruning algorithm from tree-based to forest-based. The algorithm is very effective. It can prune out around 90% argument candidates in parse tree-based amount of unmatched argument increases a little bit to 3.1%, its generated total candidate amount decreases substantially to only 1.31 times of that from 1-best parse tree. This clearly shows the advantages of the forest-based method over treebased in SRL. The best-reported tree kernel method for SRL 𝐾𝑦𝑏𝑟𝑖𝑑 = 𝜃∙ 𝐾𝑝𝑎𝑡+ (1 − 𝜃) ∙ 𝐾𝑐𝑠 (0 ≤ 𝜃≤ 1), proposed by Che et al. (2006)5, is adopted as our baseline kernel. We implemented the 𝐾𝑦𝑏𝑟𝑖𝑑 in tree case (𝐾𝑇−𝑦𝑏𝑟𝑖𝑑 , using tree kernel to compute 𝐾𝑝𝑎𝑡  and 𝐾𝑐𝑠) and in forest case (𝐾𝐹−𝑦𝑏𝑟𝑖𝑑 , using tree kernel to compute 𝐾𝑝𝑎𝑡  and 𝐾𝑐𝑠). Precision Recall F-Score 𝐾𝑇−𝑦𝑏𝑟𝑖𝑑 (Tree) 76.02 67.38 71.44 𝐾𝐹−𝑦𝑏𝑟𝑖𝑑 (Forest) 79.06 69.12 73.76 Table 1: Performance comparison of SRL (%) Table 1 shows that the forest kernel significantly outperforms (𝜒2 test with p=0.01) the tree kernel with an absolute improvement of 2.32 (73.76- 71.42) percentage in F-Score, representing a relative error rate reduction of 8.19% (2.32/(100- 71.6</context>
</contexts>
<marker>Che, Zhang, Liu, Li, 2006</marker>
<rawString>Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 2006. A hybrid convolution tree kernel for semantic role labeling. COLING-ACL-2006 (poster)</rawString>
</citation>
<citation valid="true">
<authors>
<author>WanXiang Che</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Chew Lim Tan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Using a Hybrid Convolution Tree Kernel for Semantic Role Labeling.</title>
<date>2008</date>
<journal>ACM Transaction on Asian Language Information Processing</journal>
<contexts>
<context position="26672" citStr="Che et al. (2008)" startWordPosition="4714" endWordPosition="4717">ties of hyper-edges (i.e., the Viterbi-style inside-outside probabilities and set the pruning threshold as 8) for forest pruning. 4.1 Semantic role labeling Given a sentence and each predicate (either a target verb or a noun), SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles, e.g., A0 for Agent, A1 for Patient ...) of the predicate or non-argument. We use the CoNLL-2005 shared task on Semantic Role Labeling (Carreras and Ma rquez, 2005) for the evaluation of our forest kernel method. To speed up the evaluation process, the same as Che et al. (2008), we use a subset of the entire training corpus (WSJ sections 02-05 of the entire sections 02-21) for training, section 24 for development and section 23 for test, where there are 35 roles including 7 Core (A0–A5, AA), 14 Adjunct (AM-) and 14 Reference (R-) arguments. The state-of-the-art SRL methods (Carreras and Ma rquez, 2005) use constituents as the labeling units to form the labeled arguments. Due to the errors from automatic parsing, it is impossible for all arguments to find their matching constituents in the single 1-best parse trees. Statistics on the training data shows that 9.78% of</context>
</contexts>
<marker>Che, Zhang, Aw, Tan, Liu, Li, 2008</marker>
<rawString>WanXiang Che, Min Zhang, Aiti Aw, Chew Lim Tan, Ting Liu and Sheng Li. 2008. Using a Hybrid Convolution Tree Kernel for Semantic Role Labeling. ACM Transaction on Asian Language Information Processing</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<institution>Pennsylvania University</institution>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="27437" citStr="Collins, 1999" startWordPosition="4845" endWordPosition="4846">23 for test, where there are 35 roles including 7 Core (A0–A5, AA), 14 Adjunct (AM-) and 14 Reference (R-) arguments. The state-of-the-art SRL methods (Carreras and Ma rquez, 2005) use constituents as the labeling units to form the labeled arguments. Due to the errors from automatic parsing, it is impossible for all arguments to find their matching constituents in the single 1-best parse trees. Statistics on the training data shows that 9.78% of arguments have no matching constituents using the Charniak parser (Charniak, 2001), and the number increases to 11.76% when using the Collins parser (Collins, 1999). In our method, we break the limitation of 1-best parse tree and regard each span rooted by a single forest node (i.e., a subforest with one or more roots) as a candidate argument. This largely reduces the unmatched arguments from 9.78% to 1.31% after forest pruning. However, it also results in a very large amount of argument candidates that is 5.6 times as many as that from 1-best tree. Fortunately, after the pre-processing stage of argument pruning (Xue and Palmer, 2004) 4 , although the 4 We extend (Xue and Palmer, 2004)’s argument pruning algorithm from tree-based to forest-based. The alg</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. dissertation, Pennsylvania University</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2002</date>
<pages>2002</pages>
<contexts>
<context position="1422" citStr="Collins and Duffy, 2002" startWordPosition="214" endWordPosition="217">d convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1 Introduction Parse tree and packed forest of parse trees are two widely used data structures to represent the syntactic structure information of sentences in natural language processing (NLP). The structured features embedded in a parse tree have been well explored together with different machine learning algorithms and proven very useful in many NLP applications (Collins and Duffy, 2002; Moschitti, 2004; Zhang et al., 2007). A forest (Tomita, 1987) compactly encodes an exponential number of parse trees. In this paper, we study how to effectively explore structured features embedded in a forest using convolution kernel (Haussler, 1999). As we know, feature-based machine learning methods are less effective in modeling highly structured objects (Vapnik, 1998), such as parse tree or semantic graph in NLP. This is due to the fact that it is usually very hard to represent structured objects using vectors of reasonable dimensions without losing too much information. For example, it</context>
<context position="3149" citStr="Collins and Duffy, 2002" startWordPosition="484" endWordPosition="487">without enumerating all the features (Vapnik, 1998). Many learning algorithms, such as SVM (Vapnik, 1998), the Perceptron learning algorithm (Rosenblatt, 1962) and Voted Perceptron (Freund and Schapire, 1999), can work directly with kernels by replacing the dot product with a particular kernel function. This nice property of kernel methods, that implicitly calculates the dot product in a high-dimensional space over the original representations of objects, has made kernel methods an effective solution to modeling structured objects in NLP. In the context of parse tree, convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. The tree kernel has shown much success in many NLP applications like parsing (Collins and Duffy, 2002), semantic role labeling (Moschitti, 2004; Zhang et al., 2007), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006), question classification (Zhang and Lee, 2003) and machine translation (Zhang and Li, 2009), where the tree kernel is used to compute the similarity between two NLP application instances </context>
<context position="6146" citStr="Collins and Duffy (2002)" startWordPosition="990" endWordPosition="993">e convolution tree kernel. In the tree kernel, a parse treeT is represented by a vector of integer counts of each subtree type (i.e., subtree regardless of its ancestors, descendants and span covered): (# subtreetype1(T), ..., # subtreetypen(T)) where # subtreetypei(T) is the occurrence number of the ith subtree type in T. The tree kernel counts the number of common subtrees as the syntactic similarity between two parse trees. Since the number of subtrees is exponential with the tree size, it is computationally infeasible to directly use the feature vector . To solve this computational issue, Collins and Duffy (2002) proposed the following tree kernel to calculate the dot product between the above high dimensional vectors implicitly. E # subtreetypei (T)  # subtreetypei (T2 ) i K(T,T2) = O(T ),O(T2)      =  E E Isubtreei (n1 ) E Isubtreei (n2 ) i n1eN1 n2eN2  2 Convolution Kernel over Parse Tree Convolution kernel was proposed as a concept of kernels for discrete structures by Haussler (1999) and related but independently conceived ideas on string kernels first presented in (Watkins, 1999). The framework defines the kernel function between input objects as the convolution of “subkernels”, i.e. t</context>
<context position="8410" citStr="Collins and Duffy, 2002" startWordPosition="1432" endWordPosition="1435"> (n„n2)=A,• I-I (1+A(ch(n1,j),ch(n2,j))), b) A Hyper-edge e in the bank IP[1,7] NNP[1,1] NNP[1,1] VV[2,2] DT[3,3] NN[4,4] IN[5,5] DT[6,6] NN[7,7] VP[2,7] Figure 2. An example of a packed forest, a hyper-edge and two parse trees covered by the packed forest Rule 1: if the productions (CFG rules) at and n2 are different, ; Rule 2: else if both n, and are pre-terminals (POS tags), ; Rule 3: else, n j= 1 where nc(n,) is the child number of n, , ch(n ,j) is the jth child of node and A, (0&lt;A, &lt;_1) is the decay factor in order to make the kernel value less variable with respect to the subtree sizes (Collins and Duffy, 2002). The recursive Rule 3 holds because given two nodes with the same children, one can construct common subtrees using these children and common subtrees of further offspring. The time complexity for computing this kernel is O( |N1 | |N2 |) . As discussed in previous section, when convolution tree kernel is applied to NLP applications, its performance is vulnerable to the errors from the single parse tree and data sparseness. In this paper, we present a convolution kernel over packed forest to address the above issues by exploring structured features embedded in a forest. 3 Convolution Kernel o</context>
<context position="18091" citStr="Collins and Duffy, 2002" startWordPosition="3178" endWordPosition="3181">root multiple hyper-edges and each hyper-edge is independent to each other. Therefore, Algorithm 1 iterates each hyper-edge pairs with roots at 𝑣1 and 𝑣2 (line 3-4), and sums over (eq. (7) at line 9) each recursively-accumulated sub-kernel scores of subtree pairs extended from the hyper-edge pair 𝑒1, 𝑒2 (eq. (6) at line 8). Eq. (7) holds because the hyper-edges attached to the same node are independent to each other. Eq. (6) is very similar to the Rule 3 of tree kernel (see section 2) except its inputs are hyper-edges and its further expansion is based on forest nodes. Similar to tree kernel (Collins and Duffy, 2002), eq. (6) holds because a common subtree by extending from (𝑒1, 𝑒2) can be formed by taking the hyper-edge (𝑒1, 𝑒2), together with a choice at each of their leaf nodes of simply taking the non-terminal at the leaf node, or any one of the common subtrees with root at the leaf node. Thus there are 1 + Δ′ 𝑙𝑒𝑎𝑓 𝑒1, 𝑗 , 𝑙𝑒𝑎𝑓 𝑒2, 𝑗 possible choices at the jth leaf node. In total, there are Δ′′ 𝑒1, 𝑒2 (eq. (6)) common subtrees by extending from (𝑒1, 𝑒2) and Δ′ 𝑣1, 𝑣2 (eq. (7)) common subtrees with root at 𝑣1, 𝑣2 . Obviously Δ′ 𝑣1, 𝑣2 calculated by Algorithm 1 is a proper convolution kernel since it s</context>
<context position="21098" citStr="Collins and Duffy, 2002" startWordPosition="3784" endWordPosition="3787"> 𝑒2 Recall eq. (4), a fractional count consists of outside, inside and subtree probabilities. It is more straightforward to incorporate the outside and subtree probabilities since all the subtrees with roots at 𝑣1, 𝑣2 share the same outside probability and each hyper-edge pair is only visited one time. Thus we can integrate the two probabilities into Δ′ 𝑣1, 𝑣2 as follows. Δ′ 𝑣1, 𝑣2 = 𝜆 ∙ 𝛼 𝑣1 ∙ 𝛼 𝑣2 ∙𝑒1 𝑃 =𝑒2 𝑒1 ∙ 𝑃 𝑒2 ∙ Δ′′ 𝑒1, 𝑒2 (8) where, following tree kernel, a decay factor 𝜆(0 &lt; 𝜆 ≤ 1) is also introduced in order to make the kernel value less variable with respect to the subtree sizes (Collins and Duffy, 2002). It functions like multiplying each feature value by 𝜆𝑠𝑖𝑧𝑒𝑖, where 𝑠𝑖𝑧𝑒𝑖 is the number of hyper-edges in 𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑖. 8. Δ′′ 9. Δ′ 10. end if 11. end for 12. end for 880 The inside probability is only involved when a node does not need to be further expanded. The integer 1 at eq. (6) represents such case. So the inside probability is integrated into eq. (6) by replacing the integer 1 as follows. nl(e1 a(leaf (e1, j)) • a(leaf (e2, j))) ( ) D� (leaf (e1,j), leaf (e2,j))9 where in the last expression the two outside probabilities a(leaf (e1, j)) and a(leaf (e2, j)) are removed. This is because lea</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. Convolution Kernels for Natural Language. NIPS-2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing Word Lattice Translation.</title>
<date>2008</date>
<pages>2008</pages>
<contexts>
<context position="24732" citStr="Dyer et al., 2008" startWordPosition="4390" endWordPosition="4393">re vectors in the kernel-based model. There are a few other previous works done by generalizing convolution tree kernels (Kashima and Koyanagi, 2003; Moschitti, 2006; Zhang et al., 2007). However, all of these works limit themselves to single tree structure from modeling viewpoint in nature. From a broad viewpoint, as suggested by one reviewer of the paper, we can consider the forest kernel as an alternative solution proposed for the general problem of noisy inference pipelines (eg. speech translation by composition of FSTs, machine translation by translating over &apos;lattices&apos; of segmentations (Dyer et al., 2008) or using parse tree info for downstream applications in our cases) . Following this line, Bunescu (2008) and Finkel et al. (2006) are two typical related works done in reducing cascading noisy. However, our works are not overlapped with each other as there are two totally different solutions for the same general problem. In addition, the main motivation of this paper is also different from theirs. 4 Experiments Forest kernel has a broad application potential in NLP. In this section, we verify the effectiveness of the forest kernel on two NLP applications, semantic role labeling (SRL) (Gildea,</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan and Philip Resnik. 2008. Generalizing Word Lattice Translation. ACL-HLT-2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Solving the Problem of Cascading Errors: Approximate Bayesian Inference for Linguistic Annotation Pipelines.</title>
<date>2006</date>
<pages>2006</pages>
<contexts>
<context position="24862" citStr="Finkel et al. (2006)" startWordPosition="4413" endWordPosition="4416">a and Koyanagi, 2003; Moschitti, 2006; Zhang et al., 2007). However, all of these works limit themselves to single tree structure from modeling viewpoint in nature. From a broad viewpoint, as suggested by one reviewer of the paper, we can consider the forest kernel as an alternative solution proposed for the general problem of noisy inference pipelines (eg. speech translation by composition of FSTs, machine translation by translating over &apos;lattices&apos; of segmentations (Dyer et al., 2008) or using parse tree info for downstream applications in our cases) . Following this line, Bunescu (2008) and Finkel et al. (2006) are two typical related works done in reducing cascading noisy. However, our works are not overlapped with each other as there are two totally different solutions for the same general problem. In addition, the main motivation of this paper is also different from theirs. 4 Experiments Forest kernel has a broad application potential in NLP. In this section, we verify the effectiveness of the forest kernel on two NLP applications, semantic role labeling (SRL) (Gildea, 2002) and relation extraction (RE) (ACE, 2002-2006). In our experiments, SVM (Vapnik, 1998) is selected as our classifier and the</context>
</contexts>
<marker>Finkel, Manning, Ng, 2006</marker>
<rawString>Jenny Rose Finkel, Christopher D. Manning and Andrew Y. Ng. 2006. Solving the Problem of Cascading Errors: Approximate Bayesian Inference for Linguistic Annotation Pipelines. EMNLP-2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--3</pages>
<contexts>
<context position="2733" citStr="Freund and Schapire, 1999" startWordPosition="421" endWordPosition="424">ture) for a parse tree into a linear feature vector. Kernel-based machine learning method is a good way to overcome this problem. Kernel methods employ a kernel function, that must satisfy the properties of being symmetric and positive, to measure the similarity between two objects by computing implicitly the dot product of certain features of the input objects in high (or even infinite) dimensional feature spaces without enumerating all the features (Vapnik, 1998). Many learning algorithms, such as SVM (Vapnik, 1998), the Perceptron learning algorithm (Rosenblatt, 1962) and Voted Perceptron (Freund and Schapire, 1999), can work directly with kernels by replacing the dot product with a particular kernel function. This nice property of kernel methods, that implicitly calculates the dot product in a high-dimensional space over the original representations of objects, has made kernel methods an effective solution to modeling structured objects in NLP. In the context of parse tree, convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. The tree kernel ha</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277-296</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Guldea</author>
</authors>
<title>Probabilistic models of verbargument structure.</title>
<date>2002</date>
<pages>2002</pages>
<marker>Guldea, 2002</marker>
<rawString>D. Guldea. 2002. Probabilistic models of verbargument structure. COLING-2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Haussler</author>
</authors>
<title>Convolution Kernels on Discrete Structures.</title>
<date>1999</date>
<tech>Technical Report UCS-CRL-99-10,</tech>
<institution>University of California,</institution>
<location>Santa Cruz</location>
<contexts>
<context position="1675" citStr="Haussler, 1999" startWordPosition="256" endWordPosition="257">packed forest of parse trees are two widely used data structures to represent the syntactic structure information of sentences in natural language processing (NLP). The structured features embedded in a parse tree have been well explored together with different machine learning algorithms and proven very useful in many NLP applications (Collins and Duffy, 2002; Moschitti, 2004; Zhang et al., 2007). A forest (Tomita, 1987) compactly encodes an exponential number of parse trees. In this paper, we study how to effectively explore structured features embedded in a forest using convolution kernel (Haussler, 1999). As we know, feature-based machine learning methods are less effective in modeling highly structured objects (Vapnik, 1998), such as parse tree or semantic graph in NLP. This is due to the fact that it is usually very hard to represent structured objects using vectors of reasonable dimensions without losing too much information. For example, it is computationally infeasible to enumerate all subtree features (using subtree a feature) for a parse tree into a linear feature vector. Kernel-based machine learning method is a good way to overcome this problem. Kernel methods employ a kernel functio</context>
<context position="6539" citStr="Haussler (1999)" startWordPosition="1065" endWordPosition="1066">ity between two parse trees. Since the number of subtrees is exponential with the tree size, it is computationally infeasible to directly use the feature vector . To solve this computational issue, Collins and Duffy (2002) proposed the following tree kernel to calculate the dot product between the above high dimensional vectors implicitly. E # subtreetypei (T)  # subtreetypei (T2 ) i K(T,T2) = O(T ),O(T2)      =  E E Isubtreei (n1 ) E Isubtreei (n2 ) i n1eN1 n2eN2  2 Convolution Kernel over Parse Tree Convolution kernel was proposed as a concept of kernels for discrete structures by Haussler (1999) and related but independently conceived ideas on string kernels first presented in (Watkins, 1999). The framework defines the kernel function between input objects as the convolution of “subkernels”, i.e. the kernels for the decompositions (parts) of the input objects. The parse tree kernel (Collins and Duffy, 2002) is an instantiation of convolution kernel over syntactic parse trees. Given a parse tree, its features defined by a tree kernel are all of its subtree types and the value of a given feature is the number of the occurrences of the subtree in the parse tree. Fig. 1 illustrates a par</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>D. Haussler. 1999. Convolution Kernels on Discrete Structures. Technical Report UCS-CRL-99-10, University of California, Santa Cruz</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<pages>2008</pages>
<contexts>
<context position="14091" citStr="Huang, 2008" startWordPosition="2434" endWordPosition="2435">nce. Therefore, forest kernel should be more robust to parsing errors than tree kernel. In tree kernel, one occurrence of a subtree contributes 1 to the value of its corresponding feature (subtree type), so the feature value is an integer count. However, the case turns out very complicated in forest kernel. In a forest, each of its parse trees, when enumerated, has its own 878 probability. So one subtree extracted from different parse trees should have different fractional count with regard to the probabilities of different parse trees. Following the previous work (Charniak and Johnson, 2005; Huang, 2008), we define the fractional count of the occurrence of a subtree in a parse tree 𝑡𝑖 as 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑡𝑖 = 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒,𝑡𝑖|𝑓,𝑠 𝑜𝑡𝑒𝑟𝑤𝑖𝑠𝑒 0 𝑖𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∉ 𝑡𝑖 0 𝑖𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∉ 𝑡𝑖 = 𝑃 𝑡𝑖 |𝑓, 𝑠 𝑜𝑡𝑒𝑟𝑤𝑖𝑠𝑒 where we have 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑡𝑖 |𝑓, 𝑠 = 𝑃 𝑡𝑖 |𝑓, 𝑠 if 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∈ 𝑡𝑖. Then we define the fractional count of the occurrence of a subtree in a forest f as 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑓 = 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒|𝑓, 𝑠 = 𝑡𝑖 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑡𝑖 |𝑓, 𝑠 (2) = 𝑡𝑖 𝐼𝑠𝑢𝑏𝑡𝑟𝑒𝑒 𝑡𝑖 ∙ 𝑃 𝑡𝑖 |𝑓, 𝑠 where 𝐼𝑠𝑢𝑏𝑡𝑟𝑒𝑒 𝑡𝑖 is a binary function that is 1 iif the 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∈ 𝑡𝑖 and zero otherwise. Obviously, it needs exponential time to compute the above fractional counts. However,</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. ACL-2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karim Lari</author>
<author>Steve J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language.</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="10863" citStr="Lari and Young, 1990" startWordPosition="1849" endWordPosition="1852">terminal node in a forest is represented as a “label [start, end]”, where the “label” is its syntax category and “[start, end]” is the span of words it covers. As shown in Fig. 2, these two parse trees (𝑇1 and 𝑇2) can be represented as a single forest by sharing their common subtrees (such as NP[3,4] and PP[5,7]) and merging common non-terminal nodes covering the same span (such as VP[2,7], where there are two hyper-edges attach to it). Given the definition of forest, we introduce the concepts of inside probability β . and outside probability α(.) that are widely-used in parsing (Baker, 1979; Lari and Young, 1990) and are also to be used in our kernel calculation. β 𝑣 𝑝,𝑝 = 𝑃(𝑣 → 𝑆[𝑝]) β 𝑣 𝑝, 𝑞 = I 𝑃 𝑒 𝑒 𝑖𝑠 𝑎 𝑦𝑝𝑒𝑟 −𝑒𝑑𝑔𝑒 𝑎𝑡𝑡𝑎𝑐 𝑒𝑑 𝑡𝑜 𝑣 ∙ 𝛽 (𝑐𝑖 [𝑝𝑖, 𝑞𝑖]) 𝑐𝑖 𝑝𝑖,𝑞𝑖 𝑖𝑠 𝑎 𝑙𝑒𝑎𝑓 𝑛𝑜𝑑𝑒 𝑜𝑓 𝑒 α 𝑟𝑜𝑜𝑡(𝑓) = 1 α 𝑣 𝑝, 𝑞 = α 𝑟𝑜𝑜𝑡 𝑒 ∙ 𝑃 𝑒 𝑒 𝑖𝑠 𝑎 𝑦𝑝𝑒𝑟 − 𝑒𝑑𝑔𝑒 𝑎𝑛𝑑 𝑣 𝑖𝑠 𝑖𝑡𝑠 𝑜𝑛𝑒 𝑙𝑒𝑎𝑓 𝑛𝑜𝑑𝑒 ∙ 𝛽 (𝑐𝑖 [𝑝𝑖, 𝑞𝑖])) 𝑐𝑖 𝑝𝑖,𝑞𝑖 𝑖𝑠 𝑎 𝑐𝑖𝑙𝑑𝑟𝑒𝑛 𝑛𝑜𝑑𝑒 𝑜𝑓 𝑒 𝑒𝑥𝑐𝑒𝑝𝑡 𝑣 where 𝑣 is a forest node, 𝑆[𝑝] is the 𝑝𝑡 word of input sentence 𝑆, 𝑃 (𝑣 → 𝑆 [𝑝]) is the probability of the CFG rule 𝑣 → 𝑆[𝑝], 𝑟𝑜𝑜𝑡(.) returns the root node of input structure, [𝑝𝑖, 𝑞𝑖] is a sub-span of 𝑝, 𝑞 , being covered by 𝑐𝑖, and 𝑃 𝑒 is the PCFG probability of 𝑒. From these </context>
<context position="15095" citStr="Lari and Young, 1990" startWordPosition="2633" endWordPosition="2636"> |𝑓, 𝑠 (2) = 𝑡𝑖 𝐼𝑠𝑢𝑏𝑡𝑟𝑒𝑒 𝑡𝑖 ∙ 𝑃 𝑡𝑖 |𝑓, 𝑠 where 𝐼𝑠𝑢𝑏𝑡𝑟𝑒𝑒 𝑡𝑖 is a binary function that is 1 iif the 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∈ 𝑡𝑖 and zero otherwise. Obviously, it needs exponential time to compute the above fractional counts. However, due to the property of forest that compactly represents all the parse trees, the posterior probability of a subtree in a forest, 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 |𝑓, 𝑠 , can be easily computed in an Inside-Outside fashion as the product of three parts: the outside probability of its root node, the probabilities of parse hyperedges involved in the subtree, and the inside probabilities of its leaf nodes (Lari and Young, 1990; Mi and Huang, 2008). 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑓 = 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒|𝑓, 𝑠 (3) 𝛼𝛽(𝑠𝑢𝑏𝑡𝑟𝑒𝑒) = 𝛼𝛽(𝑟𝑜𝑜𝑡 𝑓 ) where 𝛼𝛽 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 = 𝛼 𝑟𝑜𝑜𝑡 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 (4) ∙ 𝑃 𝑒 ∙ 𝑒∈𝑠𝑢𝑏𝑡𝑟𝑒𝑒 𝛽 𝑣 𝑣∈𝑙𝑒𝑎𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 and 𝛼𝛽 𝑟𝑜𝑜𝑡 𝑓 = 𝛼 𝑟𝑜𝑜𝑡 𝑓 ∙ 𝛽 𝑟𝑜𝑜𝑡 𝑓 = 𝛽 𝑟𝑜𝑜𝑡 𝑓 where 𝛼 . and 𝛽 (. ) denote the outside and inside probabilities. They can be easily obtained using the equations introduced at section 3.1. Given a subtree, we can easily compute its fractional count (i.e. its feature value) directly using eq. (3) and (4) without the need of enumerating each parse trees as shown at eq. (2)1. Nonetheless, it is still computationally infeasible to directly use</context>
<context position="16952" citStr="Lari and Young, 1990" startWordPosition="2982" endWordPosition="2985">,𝑓2 =𝑣1 𝑣2∈𝑁2 Δ′ 𝑣1, 𝑣2 ∈𝑁1 where • 𝐼𝑒𝑞 ∙,∙ is a binary function that is 1 iif the input two subtrees are identical (i.e. they have the same typology and node labels) and zero otherwise; • 𝑐 ∙,∙ is the fractional count defined at eq. (3); • 𝑁1 and 𝑁2 are the sets of nodes in forests 𝑓1 and 𝑓2; • Δ′ 𝑣1, 𝑣2 returns the accumulated value of products between each two fractional counts of the common subtrees rooted at 𝑣1 and 𝑣2, i.e., Δ′ 𝑣1, 𝑣2 = 𝐼𝑒𝑞 𝑠𝑢𝑏𝑡𝑟𝑒𝑒1, 𝑠𝑢𝑏𝑡𝑟𝑒𝑒2 E 𝑟𝑜𝑜𝑡 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 1 =𝑣1 𝑟𝑜𝑜𝑡 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 2 =𝑣2 ∙ 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒1, 𝑓1 ∙ 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒2,𝑓2 1 It has been proven in parsing literatures (Baker, 1979; Lari and Young, 1990) that eq. (3) defined by Inside-Outside probabilities is exactly to compute the sum of those parse tree probabilities that cover the subtree of being considered as defined at eq. (2). 879 We next show that Δ′ 𝑣1, 𝑣2 can be computed recursively in a polynomial time as illustrated at Algorithm 1. To facilitate discussion, we temporarily ignore all fractional counts in Algorithm 1. Indeed, Algorithm 1 can be viewed as a natural extension of convolution kernel from over tree to over forest. In forest2, a node can root multiple hyper-edges and each hyper-edge is independent to each other. Therefore</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>Karim Lari and Steve J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language. 4(35–56)</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kashima</author>
<author>T Koyanagi</author>
</authors>
<date>2003</date>
<note>Kernels for SemiStructured Data. ICML-2003</note>
<contexts>
<context position="24262" citStr="Kashima and Koyanagi, 2003" startWordPosition="4317" endWordPosition="4320"> can help to address the data sparseness issue. To some degree, forest kernel can be viewed as a tree kernel with very powerful back-off mechanism. In addition, forest kernel is much more robust against parsing errors than tree kernel. Aiolli et al. (2006; 2007) propose using Direct Acyclic Graphs (DAG) as a compact representation of tree kernel-based models. This can largely reduce the computational burden and storage requirements by sharing the common structures and feature vectors in the kernel-based model. There are a few other previous works done by generalizing convolution tree kernels (Kashima and Koyanagi, 2003; Moschitti, 2006; Zhang et al., 2007). However, all of these works limit themselves to single tree structure from modeling viewpoint in nature. From a broad viewpoint, as suggested by one reviewer of the paper, we can consider the forest kernel as an alternative solution proposed for the general problem of noisy inference pipelines (eg. speech translation by composition of FSTs, machine translation by translating over &apos;lattices&apos; of segmentations (Dyer et al., 2008) or using parse tree info for downstream applications in our cases) . Following this line, Bunescu (2008) and Finkel et al. (2006)</context>
</contexts>
<marker>Kashima, Koyanagi, 2003</marker>
<rawString>H. Kashima and T. Koyanagi. 2003. Kernels for SemiStructured Data. ICML-2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and Hypergraphs.</title>
<date>2001</date>
<pages>2001</pages>
<contexts>
<context position="9501" citStr="Klein and Manning, 2001" startWordPosition="1610" endWordPosition="1613">n kernel over packed forest to address the above issues by exploring structured features embedded in a forest. 3 Convolution Kernel over Forest In this section, we first illustrate the concept of packed forest and then give a detailed discussion on the covered feature space, fractional count, feature value and the forest kernel function itself. 3.1 Packed forest of parse trees Informally, a packed parse forest, or (packed) forest in short, is a compact representation of all the derivations (i.e. parse trees) for a given sentence under context-free grammar (Tomita, 1987; Billot and Lang, 1989; Klein and Manning, 2001). It is the core data structure used in natural language parsing and other downstream NLP applications, such as syntax-based machine translation (Zhang et al., 2008; Zhang et al., 2009a). In parsing, a sentence corresponds to exponential number of parse trees with different tree probabilities, where a forest can compact all the parse trees by sharing their common subtrees in a bottom-up manner. Formally, a packed forest F can be described as a triple: F= &lt; V,E,S&gt; where Vis the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence nc 877 represented as an ordered word seque</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and Hypergraphs. IWPT-2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vecor Machine: learning with many relevant features.</title>
<date>1998</date>
<pages>1998</pages>
<contexts>
<context position="25767" citStr="Joachims, 1998" startWordPosition="4572" endWordPosition="4574">st kernel has a broad application potential in NLP. In this section, we verify the effectiveness of the forest kernel on two NLP applications, semantic role labeling (SRL) (Gildea, 2002) and relation extraction (RE) (ACE, 2002-2006). In our experiments, SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted to select the one with the D�� (e1, e2 ) = (fi(leaf(e1,j)) • fi (leaf (e2,j j=1 + re-formulize eq. (5) as Kf(f1,f2 ) =&lt; 95(f1 ), 95 (f2 ) &gt; V1EN1 L V2EN2 D V1, V2 881 largest margin as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and borrow the framework of the Tree Kernel Tools (Moschitti, 2004) to integrate our forest kernel into the SVMLight. We modify Charniak parser (Charniak, 2001) to output a packed forest. Following previous forest-based studies (Charniak and Johnson, 2005), we use the marginal probabilities of hyper-edges (i.e., the Viterbi-style inside-outside probabilities and set the pruning threshold as 8) for forest pruning. 4.1 Semantic role labeling Given a sentence and each predicate (either a target verb or a noun), SRL recognizes and maps all the constituents in the sentence into their corresponding</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text Categorization with Support Vecor Machine: learning with many relevant features. ECML-1998</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based Translation Rule Extraction.</title>
<date>2008</date>
<pages>2008</pages>
<contexts>
<context position="15116" citStr="Mi and Huang, 2008" startWordPosition="2637" endWordPosition="2640">𝑒𝑒 𝑡𝑖 ∙ 𝑃 𝑡𝑖 |𝑓, 𝑠 where 𝐼𝑠𝑢𝑏𝑡𝑟𝑒𝑒 𝑡𝑖 is a binary function that is 1 iif the 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 ∈ 𝑡𝑖 and zero otherwise. Obviously, it needs exponential time to compute the above fractional counts. However, due to the property of forest that compactly represents all the parse trees, the posterior probability of a subtree in a forest, 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 |𝑓, 𝑠 , can be easily computed in an Inside-Outside fashion as the product of three parts: the outside probability of its root node, the probabilities of parse hyperedges involved in the subtree, and the inside probabilities of its leaf nodes (Lari and Young, 1990; Mi and Huang, 2008). 𝑐 𝑠𝑢𝑏𝑡𝑟𝑒𝑒, 𝑓 = 𝑃 𝑠𝑢𝑏𝑡𝑟𝑒𝑒|𝑓, 𝑠 (3) 𝛼𝛽(𝑠𝑢𝑏𝑡𝑟𝑒𝑒) = 𝛼𝛽(𝑟𝑜𝑜𝑡 𝑓 ) where 𝛼𝛽 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 = 𝛼 𝑟𝑜𝑜𝑡 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 (4) ∙ 𝑃 𝑒 ∙ 𝑒∈𝑠𝑢𝑏𝑡𝑟𝑒𝑒 𝛽 𝑣 𝑣∈𝑙𝑒𝑎𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒 and 𝛼𝛽 𝑟𝑜𝑜𝑡 𝑓 = 𝛼 𝑟𝑜𝑜𝑡 𝑓 ∙ 𝛽 𝑟𝑜𝑜𝑡 𝑓 = 𝛽 𝑟𝑜𝑜𝑡 𝑓 where 𝛼 . and 𝛽 (. ) denote the outside and inside probabilities. They can be easily obtained using the equations introduced at section 3.1. Given a subtree, we can easily compute its fractional count (i.e. its feature value) directly using eq. (3) and (4) without the need of enumerating each parse trees as shown at eq. (2)1. Nonetheless, it is still computationally infeasible to directly use the feature vector 𝜙</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based Translation Rule Extraction. EMNLP-2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A Study on Convolution Kernels for Shallow Semantic Parsing.</title>
<date>2004</date>
<pages>2004</pages>
<contexts>
<context position="1439" citStr="Moschitti, 2004" startWordPosition="218" endWordPosition="219">el. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1 Introduction Parse tree and packed forest of parse trees are two widely used data structures to represent the syntactic structure information of sentences in natural language processing (NLP). The structured features embedded in a parse tree have been well explored together with different machine learning algorithms and proven very useful in many NLP applications (Collins and Duffy, 2002; Moschitti, 2004; Zhang et al., 2007). A forest (Tomita, 1987) compactly encodes an exponential number of parse trees. In this paper, we study how to effectively explore structured features embedded in a forest using convolution kernel (Haussler, 1999). As we know, feature-based machine learning methods are less effective in modeling highly structured objects (Vapnik, 1998), such as parse tree or semantic graph in NLP. This is due to the fact that it is usually very hard to represent structured objects using vectors of reasonable dimensions without losing too much information. For example, it is computational</context>
<context position="3458" citStr="Moschitti, 2004" startWordPosition="536" endWordPosition="537">perty of kernel methods, that implicitly calculates the dot product in a high-dimensional space over the original representations of objects, has made kernel methods an effective solution to modeling structured objects in NLP. In the context of parse tree, convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. The tree kernel has shown much success in many NLP applications like parsing (Collins and Duffy, 2002), semantic role labeling (Moschitti, 2004; Zhang et al., 2007), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006), question classification (Zhang and Lee, 2003) and machine translation (Zhang and Li, 2009), where the tree kernel is used to compute the similarity between two NLP application instances that are usually represented by parse trees. However, in those studies, the tree kernel only covers the features derived from single 1- 875 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 875–885, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computat</context>
<context position="25835" citStr="Moschitti, 2004" startWordPosition="4584" endWordPosition="4585">, we verify the effectiveness of the forest kernel on two NLP applications, semantic role labeling (SRL) (Gildea, 2002) and relation extraction (RE) (ACE, 2002-2006). In our experiments, SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted to select the one with the D�� (e1, e2 ) = (fi(leaf(e1,j)) • fi (leaf (e2,j j=1 + re-formulize eq. (5) as Kf(f1,f2 ) =&lt; 95(f1 ), 95 (f2 ) &gt; V1EN1 L V2EN2 D V1, V2 881 largest margin as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and borrow the framework of the Tree Kernel Tools (Moschitti, 2004) to integrate our forest kernel into the SVMLight. We modify Charniak parser (Charniak, 2001) to output a packed forest. Following previous forest-based studies (Charniak and Johnson, 2005), we use the marginal probabilities of hyper-edges (i.e., the Viterbi-style inside-outside probabilities and set the pruning threshold as 8) for forest pruning. 4.1 Semantic role labeling Given a sentence and each predicate (either a target verb or a noun), SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles, e.g., A0 for Agent, A1 for Patient ...) </context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A Study on Convolution Kernels for Shallow Semantic Parsing. ACL-2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Syntactic kernels for natural language learning: the semantic role labeling case.</title>
<date>2006</date>
<note>HLT-NAACL-2006 (short paper)</note>
<contexts>
<context position="24279" citStr="Moschitti, 2006" startWordPosition="4321" endWordPosition="4322">a sparseness issue. To some degree, forest kernel can be viewed as a tree kernel with very powerful back-off mechanism. In addition, forest kernel is much more robust against parsing errors than tree kernel. Aiolli et al. (2006; 2007) propose using Direct Acyclic Graphs (DAG) as a compact representation of tree kernel-based models. This can largely reduce the computational burden and storage requirements by sharing the common structures and feature vectors in the kernel-based model. There are a few other previous works done by generalizing convolution tree kernels (Kashima and Koyanagi, 2003; Moschitti, 2006; Zhang et al., 2007). However, all of these works limit themselves to single tree structure from modeling viewpoint in nature. From a broad viewpoint, as suggested by one reviewer of the paper, we can consider the forest kernel as an alternative solution proposed for the general problem of noisy inference pipelines (eg. speech translation by composition of FSTs, machine translation by translating over &apos;lattices&apos; of segmentations (Dyer et al., 2008) or using parse tree info for downstream applications in our cases) . Following this line, Bunescu (2008) and Finkel et al. (2006) are two typical </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Syntactic kernels for natural language learning: the semantic role labeling case. HLT-NAACL-2006 (short paper)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics.</journal>
<volume>31</volume>
<issue>1</issue>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics. 31(1)</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rosenblatt</author>
</authors>
<title>Principles of Neurodynamics: Perceptrons and the theory of brain mechanisms. Spartan Books,</title>
<date>1962</date>
<location>Washington D.C.</location>
<contexts>
<context position="2684" citStr="Rosenblatt, 1962" startWordPosition="416" endWordPosition="417">ll subtree features (using subtree a feature) for a parse tree into a linear feature vector. Kernel-based machine learning method is a good way to overcome this problem. Kernel methods employ a kernel function, that must satisfy the properties of being symmetric and positive, to measure the similarity between two objects by computing implicitly the dot product of certain features of the input objects in high (or even infinite) dimensional feature spaces without enumerating all the features (Vapnik, 1998). Many learning algorithms, such as SVM (Vapnik, 1998), the Perceptron learning algorithm (Rosenblatt, 1962) and Voted Perceptron (Freund and Schapire, 1999), can work directly with kernels by replacing the dot product with a particular kernel function. This nice property of kernel methods, that implicitly calculates the dot product in a high-dimensional space over the original representations of objects, has made kernel methods an effective solution to modeling structured objects in NLP. In the context of parse tree, convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic simil</context>
</contexts>
<marker>Rosenblatt, 1962</marker>
<rawString>F. Rosenblatt. 1962. Principles of Neurodynamics: Perceptrons and the theory of brain mechanisms. Spartan Books, Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>An Efficient AugmentedContext-Free Parsing Algorithm.</title>
<date>1987</date>
<journal>Computational Linguistics</journal>
<volume>13</volume>
<issue>1</issue>
<pages>31--46</pages>
<contexts>
<context position="1485" citStr="Tomita, 1987" startWordPosition="226" endWordPosition="227">, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1 Introduction Parse tree and packed forest of parse trees are two widely used data structures to represent the syntactic structure information of sentences in natural language processing (NLP). The structured features embedded in a parse tree have been well explored together with different machine learning algorithms and proven very useful in many NLP applications (Collins and Duffy, 2002; Moschitti, 2004; Zhang et al., 2007). A forest (Tomita, 1987) compactly encodes an exponential number of parse trees. In this paper, we study how to effectively explore structured features embedded in a forest using convolution kernel (Haussler, 1999). As we know, feature-based machine learning methods are less effective in modeling highly structured objects (Vapnik, 1998), such as parse tree or semantic graph in NLP. This is due to the fact that it is usually very hard to represent structured objects using vectors of reasonable dimensions without losing too much information. For example, it is computationally infeasible to enumerate all subtree feature</context>
<context position="9452" citStr="Tomita, 1987" startWordPosition="1604" endWordPosition="1605">n this paper, we present a convolution kernel over packed forest to address the above issues by exploring structured features embedded in a forest. 3 Convolution Kernel over Forest In this section, we first illustrate the concept of packed forest and then give a detailed discussion on the covered feature space, fractional count, feature value and the forest kernel function itself. 3.1 Packed forest of parse trees Informally, a packed parse forest, or (packed) forest in short, is a compact representation of all the derivations (i.e. parse trees) for a given sentence under context-free grammar (Tomita, 1987; Billot and Lang, 1989; Klein and Manning, 2001). It is the core data structure used in natural language parsing and other downstream NLP applications, such as syntax-based machine translation (Zhang et al., 2008; Zhang et al., 2009a). In parsing, a sentence corresponds to exponential number of parse trees with different tree probabilities, where a forest can compact all the parse trees by sharing their common subtrees in a bottom-up manner. Formally, a packed forest F can be described as a triple: F= &lt; V,E,S&gt; where Vis the set of non-terminal nodes, E is the set of hyper-edges and S is a sen</context>
</contexts>
<marker>Tomita, 1987</marker>
<rawString>Masaru Tomita. 1987. An Efficient AugmentedContext-Free Parsing Algorithm. Computational Linguistics 13(1-2): 31-46</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley</publisher>
<contexts>
<context position="1799" citStr="Vapnik, 1998" startWordPosition="273" endWordPosition="274"> in natural language processing (NLP). The structured features embedded in a parse tree have been well explored together with different machine learning algorithms and proven very useful in many NLP applications (Collins and Duffy, 2002; Moschitti, 2004; Zhang et al., 2007). A forest (Tomita, 1987) compactly encodes an exponential number of parse trees. In this paper, we study how to effectively explore structured features embedded in a forest using convolution kernel (Haussler, 1999). As we know, feature-based machine learning methods are less effective in modeling highly structured objects (Vapnik, 1998), such as parse tree or semantic graph in NLP. This is due to the fact that it is usually very hard to represent structured objects using vectors of reasonable dimensions without losing too much information. For example, it is computationally infeasible to enumerate all subtree features (using subtree a feature) for a parse tree into a linear feature vector. Kernel-based machine learning method is a good way to overcome this problem. Kernel methods employ a kernel function, that must satisfy the properties of being symmetric and positive, to measure the similarity between two objects by comput</context>
<context position="25424" citStr="Vapnik, 1998" startWordPosition="4504" endWordPosition="4505"> this line, Bunescu (2008) and Finkel et al. (2006) are two typical related works done in reducing cascading noisy. However, our works are not overlapped with each other as there are two totally different solutions for the same general problem. In addition, the main motivation of this paper is also different from theirs. 4 Experiments Forest kernel has a broad application potential in NLP. In this section, we verify the effectiveness of the forest kernel on two NLP applications, semantic role labeling (SRL) (Gildea, 2002) and relation extraction (RE) (ACE, 2002-2006). In our experiments, SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted to select the one with the D�� (e1, e2 ) = (fi(leaf(e1,j)) • fi (leaf (e2,j j=1 + re-formulize eq. (5) as Kf(f1,f2 ) =&lt; 95(f1 ), 95 (f2 ) &gt; V1EN1 L V2EN2 D V1, V2 881 largest margin as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and borrow the framework of the Tree Kernel Tools (Moschitti, 2004) to integrate our forest kernel into the SVMLight. We modify Charniak parser (Charniak, 2001) to output a packed forest. Following previous forest-based studies (Charniak and Johnson, 2005)</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Watkins</author>
</authors>
<title>Dynamic alignment kernels. In</title>
<date>1999</date>
<publisher>MIT Press</publisher>
<contexts>
<context position="6638" citStr="Watkins, 1999" startWordPosition="1079" endWordPosition="1080">omputationally infeasible to directly use the feature vector . To solve this computational issue, Collins and Duffy (2002) proposed the following tree kernel to calculate the dot product between the above high dimensional vectors implicitly. E # subtreetypei (T)  # subtreetypei (T2 ) i K(T,T2) = O(T ),O(T2)      =  E E Isubtreei (n1 ) E Isubtreei (n2 ) i n1eN1 n2eN2  2 Convolution Kernel over Parse Tree Convolution kernel was proposed as a concept of kernels for discrete structures by Haussler (1999) and related but independently conceived ideas on string kernels first presented in (Watkins, 1999). The framework defines the kernel function between input objects as the convolution of “subkernels”, i.e. the kernels for the decompositions (parts) of the input objects. The parse tree kernel (Collins and Duffy, 2002) is an instantiation of convolution kernel over syntactic parse trees. Given a parse tree, its features defined by a tree kernel are all of its subtree types and the value of a given feature is the number of the occurrences of the subtree in the parse tree. Fig. 1 illustrates a parse tree with all n1eN1 n2eN2 = E EA ( , ) n n 1 2 where N1 and N2 are the sets of nodes in trees T1</context>
</contexts>
<marker>Watkins, 1999</marker>
<rawString>C. Watkins. 1999. Dynamic alignment kernels. In A. J. Smola, B. Sch¨olkopf, P. Bartlett, and D. Schuurmans (Eds.), Advances in kernel methods. MIT Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<pages>2004</pages>
<contexts>
<context position="27915" citStr="Xue and Palmer, 2004" startWordPosition="4929" endWordPosition="4932">ching constituents using the Charniak parser (Charniak, 2001), and the number increases to 11.76% when using the Collins parser (Collins, 1999). In our method, we break the limitation of 1-best parse tree and regard each span rooted by a single forest node (i.e., a subforest with one or more roots) as a candidate argument. This largely reduces the unmatched arguments from 9.78% to 1.31% after forest pruning. However, it also results in a very large amount of argument candidates that is 5.6 times as many as that from 1-best tree. Fortunately, after the pre-processing stage of argument pruning (Xue and Palmer, 2004) 4 , although the 4 We extend (Xue and Palmer, 2004)’s argument pruning algorithm from tree-based to forest-based. The algorithm is very effective. It can prune out around 90% argument candidates in parse tree-based amount of unmatched argument increases a little bit to 3.1%, its generated total candidate amount decreases substantially to only 1.31 times of that from 1-best parse tree. This clearly shows the advantages of the forest-based method over treebased in SRL. The best-reported tree kernel method for SRL 𝐾𝑦𝑏𝑟𝑖𝑑 = 𝜃∙ 𝐾𝑝𝑎𝑡+ (1 − 𝜃) ∙ 𝐾𝑐𝑠 (0 ≤ 𝜃≤ 1), proposed by Che et al. (2006)5, is a</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. EMNLP-2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Kernel-Based Pronoun Resolution with Structured Syntactic Knowledge.</title>
<date>2006</date>
<pages>2006</pages>
<contexts>
<context position="3561" citStr="Yang et al., 2006" startWordPosition="550" endWordPosition="553"> the original representations of objects, has made kernel methods an effective solution to modeling structured objects in NLP. In the context of parse tree, convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. The tree kernel has shown much success in many NLP applications like parsing (Collins and Duffy, 2002), semantic role labeling (Moschitti, 2004; Zhang et al., 2007), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006), question classification (Zhang and Lee, 2003) and machine translation (Zhang and Li, 2009), where the tree kernel is used to compute the similarity between two NLP application instances that are usually represented by parse trees. However, in those studies, the tree kernel only covers the features derived from single 1- 875 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 875–885, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Figure 1. A parse tree and its 11 subtree features covered by convolution tree kernel</context>
</contexts>
<marker>Yang, Su, Tan, 2006</marker>
<rawString>Xiaofeng Yang, Jian Su and Chew Lim Tan. 2006. Kernel-Based Pronoun Resolution with Structured Syntactic Knowledge. COLING-ACL-2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>W Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<pages>2003</pages>
<contexts>
<context position="3608" citStr="Zhang and Lee, 2003" startWordPosition="556" endWordPosition="559"> made kernel methods an effective solution to modeling structured objects in NLP. In the context of parse tree, convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. The tree kernel has shown much success in many NLP applications like parsing (Collins and Duffy, 2002), semantic role labeling (Moschitti, 2004; Zhang et al., 2007), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006), question classification (Zhang and Lee, 2003) and machine translation (Zhang and Li, 2009), where the tree kernel is used to compute the similarity between two NLP application instances that are usually represented by parse trees. However, in those studies, the tree kernel only covers the features derived from single 1- 875 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 875–885, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Figure 1. A parse tree and its 11 subtree features covered by convolution tree kernel PP PP PP PP PP DT NN in bank IN in the in the </context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>Dell Zhang and W. Lee. 2003. Question classification using support vector machines. SIGIR-2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Aiti Aw and Chew Lim Tan.</title>
<date>2009</date>
<pages>2009</pages>
<contexts>
<context position="9685" citStr="Zhang et al., 2009" startWordPosition="1639" endWordPosition="1642">pt of packed forest and then give a detailed discussion on the covered feature space, fractional count, feature value and the forest kernel function itself. 3.1 Packed forest of parse trees Informally, a packed parse forest, or (packed) forest in short, is a compact representation of all the derivations (i.e. parse trees) for a given sentence under context-free grammar (Tomita, 1987; Billot and Lang, 1989; Klein and Manning, 2001). It is the core data structure used in natural language parsing and other downstream NLP applications, such as syntax-based machine translation (Zhang et al., 2008; Zhang et al., 2009a). In parsing, a sentence corresponds to exponential number of parse trees with different tree probabilities, where a forest can compact all the parse trees by sharing their common subtrees in a bottom-up manner. Formally, a packed forest F can be described as a triple: F= &lt; V,E,S&gt; where Vis the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence nc 877 represented as an ordered word sequence. A hyper-edge 𝑒 is a group of edges in a parse tree which connects a father node and its all child nodes, representing a CFG rule. A non-terminal node in a forest is represented as</context>
<context position="19638" citStr="Zhang et al., 2009" startWordPosition="3482" endWordPosition="3485">me complexity for computing 𝐾𝑓 𝑓1, 𝑓2 is 𝑂(|𝐸1 |∙ |𝐸2|) , where 𝐸1 and 𝐸2 are the set of hyper-edges in forests 𝑓1 and 𝑓2, respectively. Given a forest and the best parse trees, the number of hyperedges is only several times (normally &lt;=3 after pruning) than that of tree nodes in the parse tree3. 2 Tree can be viewed as a special case of forest with only one hyper-edge attached to each tree node. 3 Suppose there are K forest nodes in a forest, each node has M associated hyper-edges fan out and each hyper-edge has N children. Then the forest is capable 𝐾−1 of encoding 𝑀𝑁−1 parse trees at most (Zhang et al., 2009b). Algorithm 1. Input: 𝑓1, 𝑓2: two packed forests 𝑣1, 𝑣2: any two nodes of 𝑓1 and 𝑓2 Notation: 𝐼𝑒𝑞 ∙,∙ : defined at eq. (5) 𝑛𝑙 𝑒1 : number of leaf node of 𝑒1 𝑙𝑒𝑎𝑓 𝑒1, 𝑗 : the jth leaf node of 𝑒1 Output: Δ′ 𝑣1, 𝑣2 1. Δ′ 𝑣1, 𝑣2 = 0 2. if 𝑣1.𝑙𝑎𝑏𝑒𝑙 ≠ 𝑣2. 𝑙𝑎𝑏𝑒𝑙 exit 3. for each hyper-edge 𝑒1 attached to 𝑣1 do 4. for each hyper-edge 𝑒2 attached to 𝑣2 do 5. if 𝐼𝑒𝑞 𝑒1, 𝑒2 == 0 do 6. goto line 3 7. else do 𝑛𝑙 𝑒1 𝑒1, 𝑒2 = 𝑗=1 1 + Δ′ 𝑙𝑒𝑎𝑓 𝑒1, 𝑗 , 𝑙𝑒𝑎𝑓 𝑒2, 𝑗 (6) 𝑣1, 𝑣2 += Δ′′ 𝑒1, 𝑒2 (7) Same as tree kernel, forest kernel is running more efficiently in practice since only two nodes with the same label nee</context>
</contexts>
<marker>Zhang, Zhang, Li, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew Lim Tan. 2009a. Forest-based Tree Sequence to String Translation Model. ACLIJCNLP-2009</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
</authors>
<title>Fast Translation Rule Matching for Syntax-based Statistical Machine Translation.</title>
<date>2009</date>
<pages>2009</pages>
<contexts>
<context position="9685" citStr="Zhang et al., 2009" startWordPosition="1639" endWordPosition="1642">pt of packed forest and then give a detailed discussion on the covered feature space, fractional count, feature value and the forest kernel function itself. 3.1 Packed forest of parse trees Informally, a packed parse forest, or (packed) forest in short, is a compact representation of all the derivations (i.e. parse trees) for a given sentence under context-free grammar (Tomita, 1987; Billot and Lang, 1989; Klein and Manning, 2001). It is the core data structure used in natural language parsing and other downstream NLP applications, such as syntax-based machine translation (Zhang et al., 2008; Zhang et al., 2009a). In parsing, a sentence corresponds to exponential number of parse trees with different tree probabilities, where a forest can compact all the parse trees by sharing their common subtrees in a bottom-up manner. Formally, a packed forest F can be described as a triple: F= &lt; V,E,S&gt; where Vis the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence nc 877 represented as an ordered word sequence. A hyper-edge 𝑒 is a group of edges in a parse tree which connects a father node and its all child nodes, representing a CFG rule. A non-terminal node in a forest is represented as</context>
<context position="19638" citStr="Zhang et al., 2009" startWordPosition="3482" endWordPosition="3485">me complexity for computing 𝐾𝑓 𝑓1, 𝑓2 is 𝑂(|𝐸1 |∙ |𝐸2|) , where 𝐸1 and 𝐸2 are the set of hyper-edges in forests 𝑓1 and 𝑓2, respectively. Given a forest and the best parse trees, the number of hyperedges is only several times (normally &lt;=3 after pruning) than that of tree nodes in the parse tree3. 2 Tree can be viewed as a special case of forest with only one hyper-edge attached to each tree node. 3 Suppose there are K forest nodes in a forest, each node has M associated hyper-edges fan out and each hyper-edge has N children. Then the forest is capable 𝐾−1 of encoding 𝑀𝑁−1 parse trees at most (Zhang et al., 2009b). Algorithm 1. Input: 𝑓1, 𝑓2: two packed forests 𝑣1, 𝑣2: any two nodes of 𝑓1 and 𝑓2 Notation: 𝐼𝑒𝑞 ∙,∙ : defined at eq. (5) 𝑛𝑙 𝑒1 : number of leaf node of 𝑒1 𝑙𝑒𝑎𝑓 𝑒1, 𝑗 : the jth leaf node of 𝑒1 Output: Δ′ 𝑣1, 𝑣2 1. Δ′ 𝑣1, 𝑣2 = 0 2. if 𝑣1.𝑙𝑎𝑏𝑒𝑙 ≠ 𝑣2. 𝑙𝑎𝑏𝑒𝑙 exit 3. for each hyper-edge 𝑒1 attached to 𝑣1 do 4. for each hyper-edge 𝑒2 attached to 𝑣2 do 5. if 𝐼𝑒𝑞 𝑒1, 𝑒2 == 0 do 6. goto line 3 7. else do 𝑛𝑙 𝑒1 𝑒1, 𝑒2 = 𝑗=1 1 + Δ′ 𝑙𝑒𝑎𝑓 𝑒1, 𝑗 , 𝑙𝑒𝑎𝑓 𝑒2, 𝑗 (6) 𝑣1, 𝑣2 += Δ′′ 𝑒1, 𝑒2 (7) Same as tree kernel, forest kernel is running more efficiently in practice since only two nodes with the same label nee</context>
</contexts>
<marker>Zhang, Zhang, Li, Tan, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Haizhou Li and Chew Lim Tan. 2009b. Fast Translation Rule Matching for Syntax-based Statistical Machine Translation. EMNLP-2009</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>GuoDong Zhou</author>
</authors>
<title>A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features.</title>
<date>2006</date>
<pages>2006</pages>
<contexts>
<context position="3521" citStr="Zhang et al., 2006" startWordPosition="544" endWordPosition="547"> product in a high-dimensional space over the original representations of objects, has made kernel methods an effective solution to modeling structured objects in NLP. In the context of parse tree, convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. The tree kernel has shown much success in many NLP applications like parsing (Collins and Duffy, 2002), semantic role labeling (Moschitti, 2004; Zhang et al., 2007), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006), question classification (Zhang and Lee, 2003) and machine translation (Zhang and Li, 2009), where the tree kernel is used to compute the similarity between two NLP application instances that are usually represented by parse trees. However, in those studies, the tree kernel only covers the features derived from single 1- 875 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 875–885, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Figure 1. A parse tree and its 11 subtree fea</context>
<context position="30980" citStr="Zhang et al. (2006)" startWordPosition="5423" endWordPosition="5426"> span. 5 Kpath and Kcs are two standard convolution tree kernels to describe predicate-argument path substructures and argument syntactic substructures, respectively. 882 4.2 Relation extraction As a subtask of information extraction, relation extraction is to extract various semantic relations between entity pairs from text. For example, the sentence “Bill Gates is chairman and chief software architect of Microsoft Corporation” conveys the semantic relation “EMPLOYMENT.executive” between the entities “Bill Gates” (person) and “Microsoft Corporation” (company). We adopt the method reported in Zhang et al. (2006) as our baseline method as it reports the state-of-the-art performance using tree kernel-based composite kernel method for RE. We replace their tree kernels with our forest kernels and use the same experimental settings as theirs. We carry out the same five-fold cross validation experiment on the same subset of ACE 2004 data (LDC2005T09, ACE 2002-2004) as that in Zhang et al. (2006). The data contain 348 documents and 4400 relation instances. In SRL, constituents are used as the labeling units to form the labeled arguments. However, previous work (Zhang et al., 2006) shows that if we use compl</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su and GuoDong Zhou. 2006. A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features. COLING-ACL-2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>W Che</author>
<author>A Aw</author>
<author>C Tan</author>
<author>G Zhou</author>
<author>T Liu</author>
<author>S Li</author>
</authors>
<title>A Grammar-driven Convolution Tree Kernel for Semantic Role Classification.</title>
<date>2007</date>
<contexts>
<context position="1460" citStr="Zhang et al., 2007" startWordPosition="220" endWordPosition="223">results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1 Introduction Parse tree and packed forest of parse trees are two widely used data structures to represent the syntactic structure information of sentences in natural language processing (NLP). The structured features embedded in a parse tree have been well explored together with different machine learning algorithms and proven very useful in many NLP applications (Collins and Duffy, 2002; Moschitti, 2004; Zhang et al., 2007). A forest (Tomita, 1987) compactly encodes an exponential number of parse trees. In this paper, we study how to effectively explore structured features embedded in a forest using convolution kernel (Haussler, 1999). As we know, feature-based machine learning methods are less effective in modeling highly structured objects (Vapnik, 1998), such as parse tree or semantic graph in NLP. This is due to the fact that it is usually very hard to represent structured objects using vectors of reasonable dimensions without losing too much information. For example, it is computationally infeasible to enum</context>
<context position="3479" citStr="Zhang et al., 2007" startWordPosition="538" endWordPosition="541">ethods, that implicitly calculates the dot product in a high-dimensional space over the original representations of objects, has made kernel methods an effective solution to modeling structured objects in NLP. In the context of parse tree, convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. The tree kernel has shown much success in many NLP applications like parsing (Collins and Duffy, 2002), semantic role labeling (Moschitti, 2004; Zhang et al., 2007), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006), question classification (Zhang and Lee, 2003) and machine translation (Zhang and Li, 2009), where the tree kernel is used to compute the similarity between two NLP application instances that are usually represented by parse trees. However, in those studies, the tree kernel only covers the features derived from single 1- 875 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 875–885, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Fig</context>
<context position="24300" citStr="Zhang et al., 2007" startWordPosition="4323" endWordPosition="4326">e. To some degree, forest kernel can be viewed as a tree kernel with very powerful back-off mechanism. In addition, forest kernel is much more robust against parsing errors than tree kernel. Aiolli et al. (2006; 2007) propose using Direct Acyclic Graphs (DAG) as a compact representation of tree kernel-based models. This can largely reduce the computational burden and storage requirements by sharing the common structures and feature vectors in the kernel-based model. There are a few other previous works done by generalizing convolution tree kernels (Kashima and Koyanagi, 2003; Moschitti, 2006; Zhang et al., 2007). However, all of these works limit themselves to single tree structure from modeling viewpoint in nature. From a broad viewpoint, as suggested by one reviewer of the paper, we can consider the forest kernel as an alternative solution proposed for the general problem of noisy inference pipelines (eg. speech translation by composition of FSTs, machine translation by translating over &apos;lattices&apos; of segmentations (Dyer et al., 2008) or using parse tree info for downstream applications in our cases) . Following this line, Bunescu (2008) and Finkel et al. (2006) are two typical related works done in</context>
</contexts>
<marker>Zhang, Che, Aw, Tan, Zhou, Liu, Li, 2007</marker>
<rawString>Min Zhang, W. Che, A. Aw, C. Tan, G. Zhou, T. Liu and S. Li. 2007. A Grammar-driven Convolution Tree Kernel for Semantic Role Classification. ACL-2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
</authors>
<title>Hongfei Jiang, Aiti Aw,</title>
<date>2008</date>
<location>Haizhou Li, Chew Lim Tan</location>
<marker>Zhang, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan and Sheng Li. 2008. A Tree Sequence Alignment-based Tree-to-Tree Translation Model. ACL-2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Tree Kernel-based SVM with Structured Syntactic Knowledge for BTG-based Phrase Reordering.</title>
<date>2009</date>
<pages>2009</pages>
<contexts>
<context position="3653" citStr="Zhang and Li, 2009" startWordPosition="563" endWordPosition="566">modeling structured objects in NLP. In the context of parse tree, convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. The tree kernel has shown much success in many NLP applications like parsing (Collins and Duffy, 2002), semantic role labeling (Moschitti, 2004; Zhang et al., 2007), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006), question classification (Zhang and Lee, 2003) and machine translation (Zhang and Li, 2009), where the tree kernel is used to compute the similarity between two NLP application instances that are usually represented by parse trees. However, in those studies, the tree kernel only covers the features derived from single 1- 875 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 875–885, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Figure 1. A parse tree and its 11 subtree features covered by convolution tree kernel PP PP PP PP PP DT NN in bank IN in the in the bank IN DT NN IN the bank DT NN DT NN IN in I</context>
</contexts>
<marker>Zhang, Li, 2009</marker>
<rawString>Min Zhang and Haizhou Li. 2009. Tree Kernel-based SVM with Structured Syntactic Knowledge for BTG-based Phrase Reordering. EMNLP-2009</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>