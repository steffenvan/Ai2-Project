<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.99368">
Toward Completeness in Concept Extraction and Classification
</title>
<author confidence="0.834769">
Eduard Hovy and Zornitsa Kozareva
</author>
<affiliation confidence="0.770396">
USC Information Sciences Institute
</affiliation>
<address confidence="0.689778">
4676 Admiralty Way
Marina del Rey, CA 90292
</address>
<email confidence="0.996962">
hovy@isi.edu, zkozareva@gmail.com
</email>
<author confidence="0.996012">
Ellen Riloff
</author>
<affiliation confidence="0.916833">
School of Computing
University of Utah
Salt Lake City, UT 84112
</affiliation>
<email confidence="0.998696">
riloff@cs.utah.edu
</email>
<sectionHeader confidence="0.993903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999505038461538">
Many algorithms extract terms from text to-
gether with some kind of taxonomic clas-
sification (is-a) link. However, the general
approaches used today, and specifically the
methods of evaluating results, exhibit serious
shortcomings. Harvesting without focusing on
a specific conceptual area may deliver large
numbers of terms, but they are scattered over
an immense concept space, making Recall
judgments impossible. Regarding Precision,
simply judging the correctness of terms and
their individual classification links may pro-
vide high scores, but this doesn’t help with the
eventual assembly ofterms into a single coher-
ent taxonomy. Furthermore, since there is no
correct and complete gold standard to measure
against, most work invents some ad hoc evalu-
ation measure. We present an algorithm that is
more precise and complete than previous ones
for identifying from web text just those con-
cepts ‘below’ a given seed term. Comparing
the results to WordNet, we find that the algo-
rithm misses terms, but also that it learns many
new terms not in WordNet, and that it clas-
sifies them in ways acceptable to humans but
different from WordNet.
</bodyText>
<sectionHeader confidence="0.941834" genericHeader="keywords">
1 Collecting Information with Care
</sectionHeader>
<bodyText confidence="0.999889422222222">
Over the past few years, many algorithms have been
published on automatically harvesting terms and
their conceptual types from the web and/or other
large corpora (Etzioni et al., 2005; Pasca, 2007;
Banko et al., 2007; Yi and Niblack, 2005; Snow et
al., 2005). But several basic problems limit the even-
tual utility of the results.
First, there is no standard collection of facts
against which results can be measured. As we show
in this paper, WordNet (Fellbaum, 1998), the most
obvious contender because of its size and popularity,
is deficient in various ways: it is neither complete
nor is its taxonomic structure inarguably perfect. As
a result, alternative ad hoc measures are invented
that are not comparable. Second, simply harvesting
facts about an entity without regard to its actual sub-
sequent organization inflates Recall and Precision
evaluation scores: while it is correct that a jaguar
is a animal, mammal, toy, sports-team, car-make,
and operating-system, this information doesn’t help
to create a taxonomy that, for example, places mam-
mal and animal closer to one another than to some
of the others. ((Snow et al., 2005) is an exception
to this.) As a result, this work may give a mislead-
ing sense of progress. Third, entities are of differ-
ent formal types, and their taxonomic treatment is
consequently different: some are at the level of in-
stances (e.g., Michelangelo was a painter) and some
at the level of concepts (e.g., a painter is a human).
The goal of our research is to learn terms for en-
tities (objects) and their taxonomic organization si-
multaneously, from text. Our method is to use a
single surface-level pattern with several open posi-
tions. Filling them in different ways harvests differ-
ent kinds of information, and/or confirms this infor-
mation. We evaluate in two ways: against WordNet,
since that is a commonly available and popular re-
source, and also by asking humans to judge the re-
sults since WordNet is neither complete nor exhaus-
tively taxonomized.
In this paper, we describe experiments with two
rich and common portions of an entity taxonomy:
Animals and People. The claim of this paper is: It is
possible to learn terms automatically to populate a
targeted portion of a taxonomy (such as below An-
</bodyText>
<page confidence="0.961865">
948
</page>
<note confidence="0.996632">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 948–957,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.997349666666667">
imals or People) both at high precision compared
to WordNet and including additional correct ones as
well. We would like to also report on Recall rela-
tive to WordNet, but given the problems described
in Section 4, this turns out to be much harder than
would seem.
First, we need to define some basic terminology:
term: An English word (for our current purposes, a
noun or a proper name).
</bodyText>
<listItem confidence="0.974394166666667">
seed term: A word we use to initiate the algorithm.
concept: An item in the classification taxonomy we
are building. A concept may correspond to several
terms (singular form, plural form, the term’s syn-
onyms, etc.).
root concept: A concept at a fairly general (high)
</listItem>
<bodyText confidence="0.983813043478261">
level in the taxonomy, to which many others are
eventually learned to be subtypes/instances of.
basic-level concept: A concept at the ‘basic level’,
corresponding approximately to the Basic Level cat-
egories defined in Prototype Theory in Psychology
(Rosch, 1978). For our purposes, a concept corre-
sponding to the (proto)typical level of generality of
its type; that is, a dog, not a mammal or a dachshund;
a singer, not a human or an opera diva.
instance: An item in the classification taxonomy
that is more specific than a concept; only one exam-
ple of the instance exists in ‘the real world’ at any
time. For example, Michelangelo is an instance, as
well as Mazda Miata with license plate 3HCY687,
while Mazda Miata is not.
classification link: We use a single relation, that,
depending on its arguments, is either is a type of
(when both arguments are concepts), or is an in-
stance of or is an example of (when the first argu-
ment is an instance/example of the second).
Section 2 describes our method for harvesting;
Section 3 discusses related work; and Section 4 de-
scribes the experiments and the results.
</bodyText>
<sectionHeader confidence="0.97896" genericHeader="introduction">
2 Term and Relation Extraction using the
Doubly-Anchored Pattern
</sectionHeader>
<bodyText confidence="0.986518255319149">
Our goal is to develop a technique that automatically
‘fills in’ the concept space in the taxonomy below
any root concept, by harvesting terms through re-
peated web queries. We perform this in two alter-
nating stages.
Stage 1: Basic-level/Instance concept collec-
tion: We use the Doubly-Anchored Pattern DAP de-
veloped in (Kozareva et al., 2008):
DAP: [SeedTerm1] such as [SeedTerm2] and &lt;X&gt;
which learns a list of basic-level concepts or in-
stances (depending on whether SeedTerm2 ex-
presses a basic-level concept or an instance).1 DAP
is very reliable because it is instantiated with ex-
amples at both ‘ends’ of the space to be filled (the
higher-level (root) concept SeedTerm1 and a basic-
level term or instance (SeedTerm2)), which mutu-
ally disambiguate each other. For example, “pres-
idents” for SeedTerm1 can refer to the leader of a
country, corporation, or university, and “Ford” for
SeedTerm2 can refer to a car company, an automo-
bile pioneer, or a U.S. president. But when the two
terms co-occur in a text that matches the pattern
“Presidents such as Ford and &lt;X&gt;”, the text will
almost certainly refer to country presidents.
The first stage involves a series of repeated re-
placements of SeedTerm2 by newly-learned terms
in order to generate even more seed terms. That is,
each new basic-level concept or instance is rotated
into the pattern (becoming a new SeedTerm2) in a
bootstrapping cycle that Kozareva et al. called reck-
less bootstrapping. This procedure is implemented
as exhaustive breadth-first search, and iterates until
no new terms are harvested. The harvested terms are
incorporated in a Hyponym Pattern Linkage Graph
(HPLG) G = (V, E), where each vertex v ∈ V is
a candidate term and each edge (u, v) ∈ E indi-
cates that term v was generated by term u. A term
u is ranked by Out-Degree(u) = P∀(u,v)∈ E i (u,v) ,
which represents the weighted sum of u’s outgoing
edges normalized by the total number of other nodes
in the graph. Intuitively, a term ranks highly if it
is frequently discovering many different terms dur-
ing the reckless bootstrapping cycle. This method is
very productive, harvesting a constant stream of new
terms for basic-level concepts or instances when the
taxonomy below the initial root concept SeedTerm1
is extensive (such as for Animals or People).
</bodyText>
<footnote confidence="0.7984175">
1Strictly speaking, our lowest-level concepts can be in-
stances, basic-level concepts, or concepts below the basic level
(e.g., dachsund). But for the sake of simplicity we will refer to
our lowest-level terms as basic-level concepts and instances.
</footnote>
<page confidence="0.998536">
949
</page>
<bodyText confidence="0.977930671641791">
Stage 2: Intermediate level concept collection:
Going beyond (Kozareva et al., 2008), we next apply
the Doubly-Anchored Pattern in the ‘backward’ di-
rection (DAP−1), for any two seed terms represent-
ing basic-level concepts or instances:
DAP−1: &lt;X&gt; such as [SeedTerm1] and [SeedTerm2]
which harvests a set of concepts, most of them inter-
mediate between the basic level or instance and the
initial higher-level seed.
This second stage (DAP−1) has not yet been de-
scribed in the literature. It proceeds analogously.
For pairs of basic-level concepts or instances be-
low the root concept that were found during the first
stage, we instantiate DAP−1 and issue a new web
query. For example, if the term “cats” was harvested
by DAP in “Animals such as dogs and &lt;X&gt;”, then
the pair &lt; dogs, cats &gt; forms the new Web query
“&lt;X&gt; such as dogs and cats”. We extract up to 2
consecutive nouns from the &lt;X&gt; position.
This procedure yields a large number of discov-
ered concepts, but they cannot all be used for fur-
ther bootstrapping. In addition to practical limita-
tions (such as limits on web querying), many of them
are too general–more general than the initial root
concept–and could derail the bootstrapping process
by introducing terms that stray every further away
from the initial root concept. We therefore rank the
harvested terms based on the likelihood that they
will be productive if they are expanded in the next
cycle. Ranking is based on two criteria: (1) the con-
cept should be prolific (i.e., produce many lower-
level concepts) in order to keep the bootstrapping
process energized, and (2) the concept should be
subordinate to the root concept, so that the process
stays within the targeted part of the search space.
To perform ranking, we incorporate both the har-
vested concepts and the basic-level/instance pairs
into a Hypernym Relation Graph (HRG), which we
define as a bipartite graph HRG = (V, E) with two
types of vertices. One set of vertices represents the
concepts (the category vertices (Vc), and a second
set of vertices represents the basic-level/instance
pairs that produced the concepts (the member pair
vertices (Vmp)). We create an edge e(u, v) ∈ E
between u ∈ Vc and v ∈ Vmp when the con-
cept represented by u was harvested by the basic-
level/instance pair represented by v, with the weight
of the edge defined as the number of times that the
lower pair found the concept on the web.
We use the Hypernym Relation Graph to rank
the intermediate concepts based on each node’s In-
Degree, which is the sum of the weights on the
node’s incoming edges. Formally, In-Degree(u) =
Eb(u,v)EE w(u, v). Intuitively, a concept will be
ranked highly if it was harvested by many different
combinations of basic-level/instance terms.
However, this scoring function does not deter-
mine whether a concept is more or less general than
the initial root concept. For example, when har-
vesting animal categories, the system may learn the
word “species”, which is a very common term asso-
ciated with animals, but also applies to non-animals
such as plants. To prevent the inclusion of over-
general terms and constrain the search to remain
‘below’ the root concept, we apply a Concept Posi-
tioning Test (CPT): We issue the following two web
queries:
</bodyText>
<listItem confidence="0.952047">
(a) Concept such as RootConcept and &lt;X&gt;
(b) RootConcept such as Concept and &lt;X&gt;
</listItem>
<bodyText confidence="0.995135375">
If (b) returns more web hits than (a), then the con-
cept passes the test, otherwise it fails. The first (most
highly ranked) concept that passes CPT becomes the
new seed concept for the next bootstrapping cycle.
In principle, we could use all the concepts that pass
the CPT for bootstrapping2. However, for practical
reasons (primarily limitations on web querying), we
run the algorithm for 10 iterations.
</bodyText>
<sectionHeader confidence="0.999927" genericHeader="method">
3 Related Work
</sectionHeader>
<bodyText confidence="0.979590933333333">
Many algorithms have been developed to automat-
ically acquire semantic class members using a va-
riety of techniques, including co-occurrence statis-
tics (Riloff and Shepherd, 1997; Roark and Char-
niak, 1998), syntactic dependencies (Pantel and
Ravichandran, 2004), and lexico-syntactic patterns
(Riloff and Jones, 1999; Fleischman and Hovy,
2002; Thelen and Riloff, 2002).
The work most closely related to ours is that of
(Hearst, 1992) who introduced the idea of apply-
ing hyponym patterns to text, which explicitly iden-
tify a hyponym relation between two terms (e.g.,
2The number of ranked concepts that pass CPT changes in
each iteration. Also, the wildcard * is important for counts, as
can be verified with a quick experiment using Google.
</bodyText>
<page confidence="0.981205">
950
</page>
<bodyText confidence="0.999960857142858">
“such authors as &lt;X&gt;”). In recent years, sev-
eral researchers have followed up on this idea using
the web as a corpus. (Pasca, 2004) applies lexico-
syntactic hyponym patterns to the Web and use the
contexts around them for learning. KnowItAll (Et-
zioni et al., 2005) applies the hyponym patterns to
extract instances from the Web and ranks them by
relevance using mutual information. (Kozareva et
al., 2008) introduced a bootstrapping scheme using
the doubly-anchored pattern (DAP) that is guided
through graph ranking. This approach reported a
significant improvement from 5% to 18% over ap-
proaches using singly-anchored patterns like those
of (Pasca, 2004) and (Etzioni et al., 2005).
(Snow et al., 2005) describe a dependency path
based approach that generates a large number of
weak hypernym patterns using pairs of noun phrases
present in WordNet. They build a classifier using
the different hypernym patterns and find among the
highest precision patterns those of (Hearst, 1992).
Snow et al. report performance of 85% precision
at 10% recall and 25% precision at 30% recall for
5300 hand-tagged noun phrase pairs. (McNamee et
al., 2008) use the technique of (Snow et al., 2005)
to harvest the hypernyms of the proper names. The
average precision on 75 automatically detected cat-
egories is 53%. The discovered hypernyms were
intergrated in a Question Answering system which
showed an improvement of 9% when evaluated on a
TREC Question Answering data set.
Recently, (Ritter et al., 2009) reported hypernym
learning using (Hearst, 1992) patterns and manually
tagged common and proper nouns. All hypernym
candidates matching the pattern are acquired, and
the candidate terms are ranked by mutual informa-
tion. However, they evaluate the performance of
their hypernym algorithm by considering only the
top 5 hypernyms given a basic-level concept or in-
stance. They report 100% precision at 18% recall,
and 66% precision at 72% recall, considering only
the top-5 list. Necessarily, using all the results re-
turned will result in lower precision scores. In con-
trast to their approach, our aim is to first acquire au-
tomatically with minimal supervision the basic-level
concepts for given root concept. Thus, we almost
entirely eliminate the need for humans to provide
hyponym seeds. Second, we evaluate the perfor-
mance of our approach not by measuring the top-
ranked 5 hypernyms given a basic-level concept, but
considering all harvested hypernyms of the concept.
Unlike (Etzioni et al., 2005), (Pasca, 2007) and
(Snow et al., 2005), we learn both instances and con-
cepts simultaneously.
Some researchers have also worked on reorga-
nizing, augmenting, or extending semantic concepts
that already exist in manually built resources such
as WordNet (Widdows and Dorow, 2002; Snow et
al., 2005) or Wikipedia (Ponzetto and Strube, 2007).
Work in automated ontology construction has cre-
ated lexical hierarchies (Caraballo, 1999; Cimiano
and Volker, 2005; Mann, 2002), and learned seman-
tic relations such as meronymy (Berland and Char-
niak, 1999; Girju et al., 2003).
</bodyText>
<sectionHeader confidence="0.998593" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.998199677419355">
The root concepts discussed in this paper are An-
imals and People, because they head large taxo-
nomic structures that are well-represented in Word-
Net. Throughout these experiments, we used as the
initial SeedTerm2 lions for Animals and Madonna
for People (by specifically choosing a proper name
for People we force harvesting down to the level of
individual instances). To collect data, we submitted
the DAP patterns as web queries to Google, retrieved
the top 1000 web snippets per query, and kept only
the unique ones. In total, we collected 1.1 GB of
snippets for Animals and 1.5 GB for People. The
algorithm was allowed to run for 10 iterations.
The algorithm learns a staggering variety of terms
that is much more diverse than we had antici-
pated. In addition to many basic-level concepts or
instances, such as dog and Madonna respectively,
and many intermediate concepts, such as mammals,
pets, and predators, it also harvested categories that
clearly seemed useful, such as laboratory animals,
forest dwellers, and endangered species. Many other
harvested terms were more difficult to judge, includ-
ing bait, allergens, seafood, vectors, protein, and
pests. While these terms have an obvious relation-
ship to Animals, we have to determine whether they
are legitimate and valuable subconcepts of Animals.
A second issue involves relative terms that are
hard to define in an absolute sense, such as native
animals and large mammals.
A complete evaluation should answer the following
three questions:
</bodyText>
<page confidence="0.979443">
951
</page>
<listItem confidence="0.99775275">
• Precision: What is the correctness of the har-
vested concepts? (How many of them are sim-
ply wrong, given the root concept?)
• Recall: What is the coverage of the harvested
concepts? (How many are missing, below a
given root concept?)
• How correct is the taxonomic structure
learned?
</listItem>
<bodyText confidence="0.999935060606061">
Given the number and variety of terms obtained,
we initially decided that an automatic evaluation
against existing resources (such as WordNet or
something similar) would be inadequate because
they do not contain many of our harvested terms,
even though many of these terms are clearly sensi-
ble and potentially valuable. Indeed, the whole point
of our work is to learn concepts and taxonomies that
go above and beyond what is currently available.
However, it is necessary to compare with
something, and it is important not to skirt the issue
by conducting evaluations that measure subsets of
results, or that perhaps may mislead. We therefore
decided to compare our results against WordNet and
to have human annotators judge as many results as
we could afford (to obtain a measure of Precision
and the legitimate extensions beyond WordNet).
Unfortunately, it proved impossible to measure
Recall against WordNet, because this requires as-
certaining the number of synsets in WordNet be-
tween the root and its basic-level categories. This
requires human judgment, which we could not af-
ford. We plan to address this question in future
work. Also, assessing the correctness of the learned
taxonomy structure requires the manual assessment
of each classification link proposed by the system
that is not already in WordNet, a task also beyond
our budget to complete in full. Some results—for
just basic-level terms and intermediate concepts, but
not among intermediate-level concepts–are shown in
Section 4.3.
We provide Precision scores using the following
measures, where terms refers to the harvested terms:
</bodyText>
<equation confidence="0.43012">
NotInWN = #terms judged correct by human but
not in WordNet
</equation>
<bodyText confidence="0.999722888888889">
We conducted three sets of experiments. Ex-
periment 1 evaluates the results of using DAP to
learn basic-level concepts for Animals and instances
for People. Experiment 2 evaluates the results of
using DAP−1 to harvest intermediate concepts be-
tween each root concept and its basic-level concepts
or instances. Experiment 3 evaluates the taxonomy
structure that is produced via the links between the
instances and intermediate concepts.
</bodyText>
<subsectionHeader confidence="0.6221155">
4.1 Experiment 1: Basic-Level Concepts and
Instances
</subsectionHeader>
<bodyText confidence="0.999298875">
In this section we discuss the results of harvest-
ing the basic-level Animal concepts and People in-
stances. The bootstrapping algorithm ranks the har-
vested terms by their Out-Degree score and consid-
ers as correct only those with Out-Degree &gt; 0. In
ten iterations, the bootstrapping algorithm produced
913 Animal basic-level concepts and 1, 344 People
instances that passed this Out-Degree criterion.
</bodyText>
<subsubsectionHeader confidence="0.53086">
4.1.1 Human Evaluation
</subsubsectionHeader>
<bodyText confidence="0.9987745">
The harvested terms were labeled by human
judges as either correct or incorrect with respect to
the root concept. Table 1 shows the Precision of the
top-ranked N terms, with N shown in increments
of 100. Overall, the Animal terms yielded 71%
(649/913) Precision and the People terms yielded
95% Precision (1,271/1,344). Figure 1 shows that
higher-ranked Animal terms are more accurate than
lower-ranked terms, which indicates that the scor-
ing function did its job. For People terms, accuracy
was very high throughout the ranked list. Overall,
these results show that the bootstrapping algorithm
generates a large number of correct instances of high
quality.
</bodyText>
<subsubsectionHeader confidence="0.651749">
4.1.2 WordNet Evaluation
</subsubsectionHeader>
<bodyText confidence="0.999918285714286">
Table 1 shows a comparison of the harvested
terms against the terms present in WordNet.
Note that the Precision measured against WordNet
(PrWN) for People is dramatically different from
the Precision based on human judgments (PrH).
This can be explained by looking at the NotInWN
column, which shows that 48 correct Animal terms
</bodyText>
<figure confidence="0.873323272727273">
PrWN =
#terms harvested by system
#terms judged correct by human
PrH = #terms harvested by system
#terms found in WordNet
952
100 200 300 400 500 600 700 800 900
Rank
People Instances
200 400 600 800 1000 1200
Rank
</figure>
<figureCaption confidence="0.999935">
Figure 1: Ranked Basic-Concepts and Instances.
</figureCaption>
<bodyText confidence="0.997967111111111">
and 986 correct People instances are not present in
WordNet (primarily, for people, because WordNet
contains relatively few proper names). These results
show that there is substantial room for improvement
in WordNet’s coverage of these categories. For Ani-
mals, the precision measured against WordNet is ac-
tually higher than the precision measured by human
judges, which may indicate that the judges failed to
recognize some correct animal terms.
</bodyText>
<table confidence="0.996282666666667">
PrWN PrH NotInWN
Animal .79 .71 48
People .23 .95 986
</table>
<tableCaption confidence="0.999693">
Table 1: Instance Evaluation.
</tableCaption>
<subsectionHeader confidence="0.603676">
4.1.3 Evaluation against Prior Work
</subsectionHeader>
<bodyText confidence="0.999923630434783">
To assess how well our algorithm compares with
previous semantic class learning methods, we com-
pared our results to those of (Kozareva et al., 2008).
Our work was inspired by that approach–in fact, we
use that previous algorithm as the first step of our
bootstrapping process. The novelty of our approach
is the insertion of an additional bootstrapping stage
that iteratively learns new intermediate concepts us-
ing DAP−1 and the Concept Positioning Test, fol-
lowed by the subsequent use of the newly learned
intermediate concepts in DAP to expand the search
space beyond the original root concept. This leads
to the discovery of additional basic-level terms or in-
stances, which are then recycled in turn to discover
new intermediate concepts, and so on.
Consequently, we can compare the results pro-
duced by the first iteration of our algorithm (be-
fore intermediate concepts are learned) to those of
(Kozareva et al., 2008) for the Animal and People
categories, and then compare again after 10 boot-
strapping iterations of intermediate concept learn-
ing. Figure 2 shows the number of harvested con-
cepts for Animals and People after each bootstrap-
ping iteration. Bootstrapping with intermediate con-
cepts produces nearly 5 times as many basic-level
concepts and instances than (Kozareva et al., 2008)
obtain, while maintaining similar levels of precision.
The intermediate concepts help so much because
they steer the learning process into new (yet still cor-
rect) regions of the search space after each iteration.
For instance, in the first iteration, the pattern “ani-
mals such as lions and *” harvests about 350 basic-
level concepts, but only animals that are mentioned
in conjunction with lions are learned. Of these, an-
imals typically quite different from lions, such as
grass-eating kudu, are often not discovered.
However, in the second iteration, the intermediate
concept Herbivore is chosen for expansion. The pat-
tern “herbivore such as antelope and *” discovers
many additional animals, including kudu, that co-
occur with antelope but do not co-occur with lions.
Table 2 shows examples of the 10 top-ranked
basic-level concepts and instances that were learned
for 3 randomly-selected intermediate Animal and
People concepts (IConcepts) that were acquired dur-
ing bootstrapping. In the next section, we present an
</bodyText>
<figure confidence="0.993648782608696">
Animal Basic-level Concepts
Precision 1
Precision 0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<page confidence="0.538969">
953
</page>
<figure confidence="0.992915">
1 2 3 4 5 6 7 8 9 10
Iterations
</figure>
<figureCaption confidence="0.999965">
Figure 2: Learning Curves.
</figureCaption>
<bodyText confidence="0.97067">
evaluation of the intermediate concept terms.
</bodyText>
<subsectionHeader confidence="0.932268">
4.2 Experiment 2: Intermediate Concepts
</subsectionHeader>
<bodyText confidence="0.9998555">
In this section we discuss the results of harvesting
the intermediate-level concepts. Given the variety of
the harvested results, manual judgment of correct-
ness required an in-depth human annotation study.
We also compare our harvested results against the
concept terms in WordNet.
</bodyText>
<subsubsectionHeader confidence="0.516082">
4.2.1 Human Evaluation
</subsubsectionHeader>
<bodyText confidence="0.999994333333333">
We hired 4 annotators (undergraduates at a dif-
ferent institution) to judge the correctness of the in-
termediate concepts. We created detailed annota-
tion guidelines that define 14 annotation labels for
each of the Animal and People classes, as shown
in Table 3. The labels are clustered into 4 major
</bodyText>
<note confidence="0.831021090909091">
PEOPLE
IConcept Instances
Dictators: Adolf Hitler, Joseph Stalin, Benito Mussolini, Lenin,
Fidel Castro, Idi Amin, Slobodan Milosevic,
Hugo Chavez, Mao Zedong, Saddam Hussein
Celebrities: Madonna, Paris Hilton, Angelina Jolie, Britney ,
Spears, Tom Cruise, Cameron Diaz, Bono,
Oprah Winfrey, Jennifer Aniston, Kate Moss
Writers: William Shakespeare, James Joyce, Charles Dickens,
Leo Tolstoy, Goethe, Ralph Waldo Emerson,
Daniel Defoe, Jane Austen, Ernest Hemingway,
</note>
<table confidence="0.989838555555556">
Franz Kafka
ANIMAL
IConcept Basic-level Terms
Crustacean: shrimp, crabs, prawns, lobsters, crayfish, mysids,
decapods, marron, ostracods, yabbies
Primates: baboons, monkeys, chimpanzees, apes, marmosets,
chimps, orangutans, gibbons, tamarins, bonobos
Mammal: mice, whales, seals, dolphins, rats, deer, rabbits,
dogs, elephants, squirrels
</table>
<tableCaption confidence="0.998616">
Table 2: Learned People and Animals Terms.
</tableCaption>
<bodyText confidence="0.84769675">
types: Correct, Borderline, BasicConcept, and Not-
Concept. The details of our annotation guidelines,
the reasons for the intermediate labels, and the anno-
tation study can be found in (Kozareva et al., 2009).
</bodyText>
<sectionHeader confidence="0.481042" genericHeader="method">
ANIMAL
</sectionHeader>
<table confidence="0.999761967741936">
TYPE LABEL EXAMPLES
Correct GeneticAnimal reptile,mammal
BehavioralByFeeding predator, grazer
BehaviorByHabitat saltwater mammal
BehaviorSocialIndiv herding animal
BehaviorSocialGroup herd, pack
MorphologicalType cloven-hoofed animal
RoleOrFunction pet, parasite
Borderline NonRealAnimal dragons
EvaluativeTerm varmint,fox
OtherAnimal critter, fossil
BasicConcept BasicAnimal dog, hummingbird
NotConcept GeneralTerm model, catalyst
NotAnimal topic, favorite
GarbageTerm brates, mals
PEOPLE
TYPE LABEL EXAMPLES
Correct GeneticPerson Caucasian, Saxon
NonTransientEventRole stutterer, gourmand
TransientEventRole passenger, visitor
PersonState dwarf, schizophrenic
FamilyRelation aunt, mother
SocialRole fugitive, hero
NationOrTribe Bulgarian, Zulu
ReligiousAffiliation Catholic, atheist
Borderline NonRealPerson biblicalfigures
OtherPerson colleagues, couples
BasicConcept BasicPerson child, woman
RealPerson Barack Obama
NotConcept GeneralTerm image, figure
NotPerson books, events
</table>
<tableCaption confidence="0.999873">
Table 3: Intermediate Concept Annotation Labels
</tableCaption>
<bodyText confidence="0.999405">
We measured pairwise inter-annotator agreement
across the four labels using the Fleiss kappa (Fleiss,
1971). The K scores ranged from 0.61–0.71 for
Animals (average K=0.66) and from 0.51–0.70 for
People (average K=0.60). These agreement scores
seemed good enough to warrant using these human
judgments to estimate the accuracy of the algorithm.
The bootstrapping algorithm harvested 3, 549 An-
imal and 4,094 People intermediate concepts in ten
iterations. After In-Degree ranking was applied,
</bodyText>
<figure confidence="0.99710408">
3500
3000
2500
2000
1500
1000
500
0
Animal Intermediate Concepts
Animal Basic-level Concepts
#Items Learned
4000
3500
3000
2500
2000
1500
1000
500
0
1 2 3 4 5 6 7 8 9 10
Iterations
People Intermediate Concepts
People Instances
#Items Learned
</figure>
<page confidence="0.998068">
954
</page>
<bodyText confidence="0.999965794117647">
we chose a random sample of intermediate concepts
with frequency over 1, which was given to four hu-
man judges for annotation. Table 4 summarizes the
labels assigned by the four annotators (A1 – A4).
The top portion of Table 4 shows the results for all
the intermediate concepts (437 Animal terms and
296 People terms), and the bottom portion shows the
results only for the concepts that passed the Concept
Positioning Test (187 Animal terms and 139 People
terms). Accuracy is computed in two ways: Acc1 is
the percent of intermediate concepts labeled as Cor-
rect; Acc2 is the percent of intermediate concepts
labeled as either Correct or Borderline.
Without the CPT, accuracies range from 53–66%
for Animals and 75–85% for People. After ap-
plying the CPT, the accuracies increase to 71–84%
for animals and 82–94% for people. These results
confirm that the Concept Positioning Test is effec-
tive at removing many of the undesirable terms.
Overall, these results demonstrate that our algorithm
produced many high-quality intermediate concepts,
with good precision.
Figure 3 shows accuracy curves based on the
rankings of the intermediate concepts (based on In-
Degree scores). The CPT clearly improves accu-
racy even among the most highly ranked concepts.
For example, the Acc1 curves for animals show that
nearly 90% of the top 100 intermediate concepts
were correct after applying the CPT, whereas only
70% of the top 100 intermediate concepts were cor-
rect before. However, the CPT also eliminates many
desirable terms. For People, the accuracies are still
relatively high even without the CPT, and a much
larger set of intermediate concepts is learned.
</bodyText>
<table confidence="0.999706625">
Animals People
A1 A2 A3 A4 A1 A2 A3 A4
Correct 246 243 251 230 239 231 225 221
Borderline 42 26 22 29 12 10 6 4
BasicConcept 2 8 9 2 6 2 9 10
NotConcept 147 160 155 176 39 53 56 61
Acc1 .56 .56 .57 .53 .81 .78 .76 .75
Acc2 .66 .62 .62 .59 .85 .81 .78 .76
Animals after CPT People after CPT
A1 A2 A3 A4 A1 A2 A3 A4
Correct 146 133 144 141 126 126 114 116
Borderline 11 15 9 13 6 2 2 0
BasicConcept 2 8 9 2 0 1 7 7
NotConcept 28 31 25 31 7 10 16 16
Acc1 .78 .71 .77 .75 .91 .91 .82 .83
Acc2 .84 .79 .82 .82 .95 .92 .83 .83
</table>
<tableCaption confidence="0.996694">
Table 4: Human Intermediate Concept Evaluation.
</tableCaption>
<table confidence="0.555920333333333">
Animal Intermediate Concepts
50 100 150 200 250 300
Rank
</table>
<figureCaption confidence="0.97546">
Figure 3: Intermediate Concept Precision at Rank N.
</figureCaption>
<subsubsectionHeader confidence="0.867598">
4.2.2 WordNet Evaluation
</subsubsectionHeader>
<bodyText confidence="0.998157230769231">
We also compared the intermediate concepts har-
vested by the algorithm to the contents of WordNet.
The results are shown in Table 5. WordNet contains
20% of the Animal concepts and 51% of the People
concepts learned by our algorithm, which confirms
that many of these concepts were considered to be
valuable taxonomic terms by the WordNet develop-
ers. However, our human annotators judged 57%
of the Animal and 84% of the People concepts to
be correct, which suggests that our algorithm gen-
erates a substantial number of additional concepts
that could be used to enrich taxonomic structure in
WordNet.
</bodyText>
<figure confidence="0.999057909090909">
noCPT
noCPT a
withCPCT
withCPTC_&amp;
50 100 150 200 250 300 350 400
Rank
Precision 1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
noCPT
noCPT 6
withCR
withCPTCk
Precision 1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
People Intermediate Concepts
</figure>
<page confidence="0.989656">
955
</page>
<table confidence="0.991898666666667">
PrWN PrH NotInWN
Animal .20 (88/437) .57 (248/437) 204
People .51 (152/296) .85 (251/296) 108
</table>
<tableCaption confidence="0.998077">
Table 5: WordNet Intermediate Concept Evaluation.
</tableCaption>
<subsectionHeader confidence="0.956606">
4.3 Experiment 3: Taxonomic Links
</subsectionHeader>
<bodyText confidence="0.9998276">
In this section we evaluate the classification (taxon-
omy) that is learned by evaluating the links between
the intermediate concepts and the basic-level con-
cept/instance terms. That is, when our algorithm
claims that isa(X,Y), how often is X truly a subcon-
cept of Y? For example, isa(goat, herbivore) would
be correct, but isa(goat, bird) would not. Again,
since WordNet does not contain all the harvested
concepts, we conduct both a manual evaluation and
a comparison against WordNet.
</bodyText>
<subsubsectionHeader confidence="0.448527">
4.3.1 Manual and WordNet Evaluations
</subsubsectionHeader>
<bodyText confidence="0.9998163">
Creating and evaluating the full taxonomic struc-
ture between the root and the basic-level or instance
terms is future work. Here we evaluate simply the
accuracy of the taxonomic links between basic-level
concepts/instances and intermediate concepts as har-
vested, but not between intermediate concepts. For
each pair, we extracted all harvested links and deter-
mined whether the same links appear in WordNet.
The links were also given to human judges. Table 6
shows the results.
</bodyText>
<table confidence="0.996368666666667">
ISA PrWN PrH NotInWN
Animal .47(912/1940) .88 (1716/1940) 804
People .23 (318/908) .94 (857/908) 539
</table>
<tableCaption confidence="0.996341">
Table 6: WordNet Taxonomic Evaluation.
</tableCaption>
<bodyText confidence="0.999883">
The results show that WordNet lacks nearly half
of the taxonomic relations that were generated by
the algorithm: 804 Animal and 539 People links.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99999140625">
We describe a novel extension to the DAP approach
for discovering basic-level concepts or instances and
their superconcepts given an initial root concept. By
appropriate filling of different positions in DAP, the
algorithm alternates between ‘downward’ and ‘up-
ward’ learning. A key resulting benefit is that each
new intermediate-level term acquired restarts har-
vesting in a new region of the concept space, which
allows previously unseen concepts to be discovered
with each bootstrapping cycle.
We also introduce the Concept Positioning Test,
which serves to confirm that a harvested concept
falls into the desired part of the search space rela-
tive to either a superordinate or subordinate concept
in the growing taxonomy, before it is selected for
further harvesting using the DAP.
These algorithms can augment other term harvest-
ing algorithms recently reported. But in order to
compare different algorithms, it is important to com-
pare results to a standard. WordNet is our best can-
didate at present. But WordNet is incomplete. Our
results include a significantly large number of in-
stances of People (which WordNet does not claim
to cover), a number comparable to the results of (Et-
zioni et al., 2005; Pasca, 2007; Ritter et al., 2009).
Rather surprisingly, our results also include a large
number of basic-level and intermediate concepts for
Animals that are not present in WordNet, a category
WordNet is actually fairly complete about. These
numbers show clearly that it is important to conduct
manual evaluation of term harvesting algorithms in
addition to comparing to a standard resource.
</bodyText>
<sectionHeader confidence="0.998201" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99987925">
This research was supported in part by grants from
the National Science Foundation (NSF grant no. IIS-
0429360), and the Department of Homeland Se-
curity, ONR Grant numbers N0014-07-1-0152 and
N00014-07-1-0149. We are grateful to the anno-
tators at the University of Pittsburgh who helped
us evaluate this work: Jay Fischer, David Halpern,
Amir Hussain, and Taichi Nakatani.
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999781357142857">
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O.Etzioni. 2007. Open information extraction
from the web. In Proceedings of International Joint
Conference on Artificial Itelligence, pages 2670–2676.
M. Berland and E. Charniak. 1999. Finding Parts in Very
Large Corpora. In Proc. of the 37th Annual Meeting of
the Association for Computational Linguistics.
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proc. of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 120–126.
P. Cimiano and J. Volker. 2005. Towards large-scale,
open-domain and ontology-based named entity classi-
fication. In Proceeding of RANLP-05, pages 166–172.
</reference>
<page confidence="0.985718">
956
</page>
<reference confidence="0.999821337349397">
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artificial Intelligence,
165(1):91–134, June.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database (Language, Speech, and Communication).
May.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proceedings of the
COLING conference, August.
J.L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378–382.
R. Girju, A. Badulescu, and D. Moldovan. 2003. Learn-
ing semantic constraints for the automatic discovery of
part-whole relations. In HLT-NAACL.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In COLING, pages 539–545.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Seman-
tic class learning from the web with hyponym pat-
tern linkage graphs. In Proceedings ofACL-08: HLT,
pages 1048–1056. Association for Computational Lin-
guistics.
Z. Kozareva, E. Hovy, and E. Riloff. 2009. Learning and
evaluating the content and structure of a term taxon-
omy. In AAAI-09 Spring Symposium on Learning by
Reading and Learning to Read.
G. Mann. 2002. Fine-grained proper noun ontologies for
question answering. In COLING-02 on SEMANET,
pages 1–7.
P. McNamee, R. Snow, P. Schone, and J. Mayfield. 2008.
Learning named entity hyponyms for question answer-
ing. In Proceedings of the Third International Joint
Conference on Natural Language Processing.
P. Pantel and D. Ravichandran. 2004. Automatically la-
beling semantic classes. In HLT-NAACL, pages 321–
328.
M. Pasca. 2004. Acquisition of categorized named en-
tities for web search. In Proceedings of CIKM, pages
137–145.
M. Pasca. 2007. Weakly-supervised discovery of named
entities using web search queries. In CIKM, pages
683–690.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from wikipedia. In Proceedings of the 22nd
National COnference on Artificial Intelligence (AAAI-
07), pages 1440–1447.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117–124.
A. Ritter, S. Soderland, and O. Etzioni. 2009. What is
this, anyway: Automatic hypernym discovery. In Pro-
ceedings of AAAI-09 Spring Symposium on Learning
by Reading and Learning to Read, pages 88–93.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110–1116.
E. Rosch, 1978. Principles of Categorization, pages 27–
48.
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning
syntactic patterns for automatic hypernym discovery.
In NIPS.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing, pages 214–221.
D. Widdows and B. Dorow. 2002. A graph model for un-
supervised lexical acquisition. In Proceedings of the
19th international conference on Computational lin-
guistics, pages 1–7.
J. Yi and W. Niblack. 2005. Sentiment mining in web-
fountain. In ICDE ’05: Proceedings of the 21st In-
ternational Conference on Data Engineering, pages
1073–1083.
</reference>
<page confidence="0.997555">
957
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999943">Toward Completeness in Concept Extraction and Classification</title>
<author confidence="0.993091">Eduard Hovy</author>
<author confidence="0.993091">Zornitsa</author>
<affiliation confidence="0.997275">USC Information Sciences</affiliation>
<address confidence="0.989184">4676 Admiralty</address>
<author confidence="0.735602">Marina del Rey</author>
<author confidence="0.735602">CA</author>
<email confidence="0.99688">hovy@isi.edu,zkozareva@gmail.com</email>
<author confidence="0.994189">Ellen</author>
<affiliation confidence="0.9985335">School of University of</affiliation>
<address confidence="0.461863">Salt Lake City, UT</address>
<email confidence="0.999212">riloff@cs.utah.edu</email>
<abstract confidence="0.986214997757848">Many algorithms extract terms from text together with some kind of taxonomic classification (is-a) link. However, the general approaches used today, and specifically the methods of evaluating results, exhibit serious shortcomings. Harvesting without focusing on a specific conceptual area may deliver large numbers of terms, but they are scattered over an immense concept space, making Recall judgments impossible. Regarding Precision, simply judging the correctness of terms and their individual classification links may provide high scores, but this doesn’t help with the eventual assembly ofterms into a single coherent taxonomy. Furthermore, since there is no correct and complete gold standard to measure against, most work invents some ad hoc evaluation measure. We present an algorithm that is more precise and complete than previous ones for identifying from web text just those concepts ‘below’ a given seed term. Comparing the results to WordNet, we find that the algorithm misses terms, but also that it learns many new terms not in WordNet, and that it classifies them in ways acceptable to humans but different from WordNet. 1 Collecting Information with Care Over the past few years, many algorithms have been published on automatically harvesting terms and their conceptual types from the web and/or other large corpora (Etzioni et al., 2005; Pasca, 2007; Banko et al., 2007; Yi and Niblack, 2005; Snow et al., 2005). But several basic problems limit the eventual utility of the results. First, there is no standard collection of facts against which results can be measured. As we show in this paper, WordNet (Fellbaum, 1998), the most obvious contender because of its size and popularity, is deficient in various ways: it is neither complete nor is its taxonomic structure inarguably perfect. As a result, alternative ad hoc measures are invented that are not comparable. Second, simply harvesting facts about an entity without regard to its actual subsequent organization inflates Recall and Precision scores: while it is correct that a a mammal, toy, sports-team, this information doesn’t help create a taxonomy that, for example, places mamto one another than to some of the others. ((Snow et al., 2005) is an exception to this.) As a result, this work may give a misleading sense of progress. Third, entities are of different formal types, and their taxonomic treatment is consequently different: some are at the level of in- (e.g., was a and some the level of concepts (e.g., painter is a The goal of our research is to learn terms for entities (objects) and their taxonomic organization simultaneously, from text. Our method is to use a single surface-level pattern with several open positions. Filling them in different ways harvests different kinds of information, and/or confirms this information. We evaluate in two ways: against WordNet, since that is a commonly available and popular resource, and also by asking humans to judge the results since WordNet is neither complete nor exhaustively taxonomized. In this paper, we describe experiments with two rich and common portions of an entity taxonomy: and People. The claim of this paper is: is possible to learn terms automatically to populate a portion of a taxonomy (such as below An- 948 of the 2009 Conference on Empirical Methods in Natural Language pages 948–957, 6-7 August 2009. ACL and AFNLP imals or People) both at high precision compared to WordNet and including additional correct ones as We would like to also report on Recall relative to WordNet, but given the problems described in Section 4, this turns out to be much harder than would seem. First, we need to define some basic terminology: English word (for our current purposes, a noun or a proper name). term: word we use to initiate the algorithm. item in the classification taxonomy we are building. A concept may correspond to several terms (singular form, plural form, the term’s synonyms, etc.). concept: concept at a fairly general (high) level in the taxonomy, to which many others are eventually learned to be subtypes/instances of. concept: concept at the ‘basic level’, approximately to the Basic Level categories defined in Prototype Theory in Psychology (Rosch, 1978). For our purposes, a concept corresponding to the (proto)typical level of generality of type; that is, a not a a not a an item in the classification taxonomy is more specific than a concept; only one example of the instance exists in ‘the real world’ at any For example, an instance, as as Miata with license plate Miata not. link: use a single relation, that, on its arguments, is either a type of both arguments are concepts), or an inof an example of the first argument is an instance/example of the second). Section 2 describes our method for harvesting; Section 3 discusses related work; and Section 4 describes the experiments and the results. 2 Term and Relation Extraction using the Doubly-Anchored Pattern Our goal is to develop a technique that automatically ‘fills in’ the concept space in the taxonomy below any root concept, by harvesting terms through repeated web queries. We perform this in two alternating stages. Stage 1: Basic-level/Instance concept collecuse the Doubly-Anchored Pattern DAP developed in (Kozareva et al., 2008): such as [SeedTerm2] and which learns a list of basic-level concepts or instances (depending on whether SeedTerm2 exa basic-level concept or an DAP is very reliable because it is instantiated with examples at both ‘ends’ of the space to be filled (the higher-level (root) concept SeedTerm1 and a basiclevel term or instance (SeedTerm2)), which mutually disambiguate each other. For example, “presidents” for SeedTerm1 can refer to the leader of a country, corporation, or university, and “Ford” for SeedTerm2 can refer to a car company, an automobile pioneer, or a U.S. president. But when the two terms co-occur in a text that matches the pattern such as Ford and the text will almost certainly refer to country presidents. The first stage involves a series of repeated replacements of SeedTerm2 by newly-learned terms in order to generate even more seed terms. That is, each new basic-level concept or instance is rotated into the pattern (becoming a new SeedTerm2) in a cycle that Kozareva et al. called reck- This procedure is implemented as exhaustive breadth-first search, and iterates until no new terms are harvested. The harvested terms are in a Pattern Linkage Graph where each vertex candidate term and each edge indithat term generated by term A term ranked by = E i represents the weighted sum of edges normalized by the total number of other nodes in the graph. Intuitively, a term ranks highly if it is frequently discovering many different terms during the reckless bootstrapping cycle. This method is very productive, harvesting a constant stream of new terms for basic-level concepts or instances when the taxonomy below the initial root concept SeedTerm1 is extensive (such as for Animals or People). speaking, our lowest-level concepts can be instances, basic-level concepts, or concepts below the basic level But for the sake of simplicity we will refer to our lowest-level terms as basic-level concepts and instances. 949 Stage 2: Intermediate level concept collection: Going beyond (Kozareva et al., 2008), we next apply the Doubly-Anchored Pattern in the ‘backward’ difor any two seed terms representing basic-level concepts or instances: as [SeedTerm1] and [SeedTerm2] which harvests a set of concepts, most of them intermediate between the basic level or instance and the initial higher-level seed. second stage has not yet been described in the literature. It proceeds analogously. For pairs of basic-level concepts or instances below the root concept that were found during the first we instantiate issue a new web query. For example, if the term “cats” was harvested DAP in such as dogs and then pair dogs, cats &gt; the new Web query as dogs and We extract up to 2 nouns from the This procedure yields a large number of discovered concepts, but they cannot all be used for further bootstrapping. In addition to practical limitations (such as limits on web querying), many of them are too general–more general than the initial root concept–and could derail the bootstrapping process by introducing terms that stray every further away from the initial root concept. We therefore rank the harvested terms based on the likelihood that they will be productive if they are expanded in the next cycle. Ranking is based on two criteria: (1) the concept should be prolific (i.e., produce many lowerlevel concepts) in order to keep the bootstrapping process energized, and (2) the concept should be subordinate to the root concept, so that the process stays within the targeted part of the search space. To perform ranking, we incorporate both the harvested concepts and the basic-level/instance pairs a Relation Graph which we as a bipartite graph two types of vertices. One set of vertices represents the category vertices and a second set of vertices represents the basic-level/instance that produced the concepts member pair We create an edge the conrepresented by harvested by the basicpair represented by with the weight of the edge defined as the number of times that the lower pair found the concept on the web. We use the Hypernym Relation Graph to rank intermediate concepts based on each node’s Inwhich is the sum of the weights on the incoming edges. Formally, = Intuitively, a concept will be ranked highly if it was harvested by many different combinations of basic-level/instance terms. However, this scoring function does not determine whether a concept is more or less general than the initial root concept. For example, when harvesting animal categories, the system may learn the which is a very common term associated with animals, but also applies to non-animals such as plants. To prevent the inclusion of overgeneral terms and constrain the search to remain the root concept, we apply a Posi- Test We issue the following two web queries: Concept such as RootConcept and RootConcept such as Concept and If (b) returns more web hits than (a), then the concept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass CPT for However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applypatterns text, which explicitly identify a hyponym relation between two terms (e.g., number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. 950 authors as In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for learning. KnowItAll (Etzioni et al., 2005) applies the hyponym patterns to extract instances from the Web and ranks them by relevance using mutual information. (Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. This approach reported a significant improvement from 5% to 18% over approaches using singly-anchored patterns like those of (Pasca, 2004) and (Etzioni et al., 2005). (Snow et al., 2005) describe a dependency path based approach that generates a large number of weak hypernym patterns using pairs of noun phrases present in WordNet. They build a classifier using the different hypernym patterns and find among the highest precision patterns those of (Hearst, 1992). Snow et al. report performance of 85% precision at 10% recall and 25% precision at 30% recall for 5300 hand-tagged noun phrase pairs. (McNamee et al., 2008) use the technique of (Snow et al., 2005) to harvest the hypernyms of the proper names. The average precision on 75 automatically detected categories is 53%. The discovered hypernyms were intergrated in a Question Answering system which showed an improvement of 9% when evaluated on a TREC Question Answering data set. Recently, (Ritter et al., 2009) reported hypernym learning using (Hearst, 1992) patterns and manually tagged common and proper nouns. All hypernym candidates matching the pattern are acquired, and the candidate terms are ranked by mutual information. However, they evaluate the performance of their hypernym algorithm by considering only the top 5 hypernyms given a basic-level concept or instance. They report 100% precision at 18% recall, and 66% precision at 72% recall, considering only the top-5 list. Necessarily, using all the results returned will result in lower precision scores. In contrast to their approach, our aim is to first acquire automatically with minimal supervision the basic-level concepts for given root concept. Thus, we almost entirely eliminate the need for humans to provide hyponym seeds. Second, we evaluate the performance of our approach not by measuring the topranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in Word- Net. Throughout these experiments, we used as the SeedTerm2 Animals and for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query, and kept only the unique ones. In total, we collected 1.1 GB of snippets for Animals and 1.5 GB for People. The algorithm was allowed to run for 10 iterations. The algorithm learns a staggering variety of terms that is much more diverse than we had anticipated. In addition to many basic-level concepts or such as many intermediate concepts, such as and it also harvested categories that seemed useful, such as animals, and Many other harvested terms were more difficult to judge, includallergens, seafood, vectors, and While these terms have an obvious relationship to Animals, we have to determine whether they are legitimate and valuable subconcepts of Animals. A second issue involves relative terms that are to define in an absolute sense, such as A complete evaluation should answer the following three questions: 951 • Precision: What is the correctness of the harvested concepts? (How many of them are simply wrong, given the root concept?) • Recall: What is the coverage of the harvested concepts? (How many are missing, below a given root concept?) • How correct is the taxonomic structure learned? Given the number and variety of terms obtained, we initially decided that an automatic evaluation against existing resources (such as WordNet or something similar) would be inadequate because they do not contain many of our harvested terms, even though many of these terms are clearly sensible and potentially valuable. Indeed, the whole point of our work is to learn concepts and taxonomies that go above and beyond what is currently available. However, it is necessary to compare with and it is important not to skirt the issue by conducting evaluations that measure subsets of results, or that perhaps may mislead. We therefore to compare our results against WordNet to have human annotators judge as many results as we could afford (to obtain a measure of Precision and the legitimate extensions beyond WordNet). Unfortunately, it proved impossible to measure Recall against WordNet, because this requires ascertaining the number of synsets in WordNet between the root and its basic-level categories. This requires human judgment, which we could not afford. We plan to address this question in future work. Also, assessing the correctness of the learned taxonomy structure requires the manual assessment of each classification link proposed by the system that is not already in WordNet, a task also beyond our budget to complete in full. Some results—for just basic-level terms and intermediate concepts, but not among intermediate-level concepts–are shown in Section 4.3. We provide Precision scores using the following where to the harvested terms: judged correct by human but not in WordNet conducted three sets of experiments. Ex- 1 the results of using DAP to learn basic-level concepts for Animals and instances People. 2 the results of to harvest intermediate concepts between each root concept and its basic-level concepts instances. 3 the taxonomy structure that is produced via the links between the instances and intermediate concepts. 4.1 Experiment 1: Basic-Level Concepts and Instances In this section we discuss the results of harvesting the basic-level Animal concepts and People instances. The bootstrapping algorithm ranks the harterms by their and considas correct only those with In ten iterations, the bootstrapping algorithm produced basic-level concepts and that passed this 4.1.1 Human Evaluation The harvested terms were labeled by human judges as either correct or incorrect with respect to the root concept. Table 1 shows the Precision of the with in increments of 100. Overall, the Animal terms yielded 71% (649/913) Precision and the People terms yielded 95% Precision (1,271/1,344). Figure 1 shows that higher-ranked Animal terms are more accurate than lower-ranked terms, which indicates that the scoring function did its job. For People terms, accuracy was very high throughout the ranked list. Overall, these results show that the bootstrapping algorithm generates a large number of correct instances of high quality. 4.1.2 WordNet Evaluation Table 1 shows a comparison of the harvested terms against the terms present in WordNet. Note that the Precision measured against WordNet for People is dramatically different from Precision based on human judgments can be explained by looking at the which shows that Animal terms harvested by system judged correct by human harvested by system found in WordNet</abstract>
<address confidence="0.708533">952</address>
<phone confidence="0.686634">100 200 300 400 500 600 700 800 900</phone>
<author confidence="0.768484">Rank</author>
<affiliation confidence="0.773286">People Instances</affiliation>
<address confidence="0.805743">200 400 600 800 1000 1200</address>
<abstract confidence="0.93798164516129">Rank 1: Basic-Concepts and Instances. People instances are not present in WordNet (primarily, for people, because WordNet contains relatively few proper names). These results show that there is substantial room for improvement in WordNet’s coverage of these categories. For Animals, the precision measured against WordNet is actually higher than the precision measured by human judges, which may indicate that the judges failed to recognize some correct animal terms. NotInWN Animal .79 .71 48 People .23 .95 986 1: Evaluation. 4.1.3 Evaluation against Prior Work To assess how well our algorithm compares with previous semantic class learning methods, we compared our results to those of (Kozareva et al., 2008). Our work was inspired by that approach–in fact, we use that previous algorithm as the first step of our bootstrapping process. The novelty of our approach is the insertion of an additional bootstrapping stage that iteratively learns new intermediate concepts usand the Concept Positioning Test, followed by the subsequent use of the newly learned intermediate concepts in DAP to expand the search space beyond the original root concept. This leads to the discovery of additional basic-level terms or instances, which are then recycled in turn to discover new intermediate concepts, and so on. Consequently, we can compare the results produced by the first iteration of our algorithm (before intermediate concepts are learned) to those of (Kozareva et al., 2008) for the Animal and People categories, and then compare again after 10 bootstrapping iterations of intermediate concept learning. Figure 2 shows the number of harvested concepts for Animals and People after each bootstrapping iteration. Bootstrapping with intermediate concepts produces nearly 5 times as many basic-level concepts and instances than (Kozareva et al., 2008) obtain, while maintaining similar levels of precision. The intermediate concepts help so much because they steer the learning process into new (yet still correct) regions of the search space after each iteration. instance, in the first iteration, the pattern “anisuch as lions and *” about 350 basiclevel concepts, but only animals that are mentioned in conjunction with lions are learned. Of these, animals typically quite different from lions, such as grass-eating kudu, are often not discovered. However, in the second iteration, the intermediate concept Herbivore is chosen for expansion. The patsuch as antelope and *” additional animals, including that cowith do not co-occur with Table 2 shows examples of the 10 top-ranked basic-level concepts and instances that were learned for 3 randomly-selected intermediate Animal and concepts that were acquired during bootstrapping. In the next section, we present an</abstract>
<note confidence="0.81882025">Animal Basic-level Concepts Precision 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Precision 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 953</note>
<phone confidence="0.592382">1 2 3 4 5 6 7 8 9 10</phone>
<abstract confidence="0.964209">Iterations 2: Curves. evaluation of the intermediate concept terms. 4.2 Experiment 2: Intermediate Concepts In this section we discuss the results of harvesting the intermediate-level concepts. Given the variety of the harvested results, manual judgment of correctness required an in-depth human annotation study. We also compare our harvested results against the concept terms in WordNet. 4.2.1 Human Evaluation We hired 4 annotators (undergraduates at a different institution) to judge the correctness of the intermediate concepts. We created detailed annotation guidelines that define 14 annotation labels for each of the Animal and People classes, as shown in Table 3. The labels are clustered into 4 major PEOPLE</abstract>
<title confidence="0.818055">IConcept Instances</title>
<author confidence="0.922000333333333">Tom Cruise Spears</author>
<author confidence="0.922000333333333">Cameron Diaz</author>
<author confidence="0.922000333333333">Oprah Winfrey Bono</author>
<author confidence="0.922000333333333">Jennifer Aniston</author>
<author confidence="0.922000333333333">Kate Moss William Shakespeare</author>
<author confidence="0.922000333333333">James Joyce</author>
<author confidence="0.922000333333333">Charles Dickens</author>
<author confidence="0.922000333333333">Leo Tolstoy</author>
<author confidence="0.922000333333333">Ralph Waldo Emerson Goethe</author>
<author confidence="0.922000333333333">Daniel Defoe</author>
<author confidence="0.922000333333333">Jane Austen</author>
<author confidence="0.922000333333333">Ernest Hemingway</author>
<author confidence="0.922000333333333">Franz Kafka</author>
<abstract confidence="0.4045164">ANIMAL IConcept Basic-level Terms shrimp, crabs, prawns, lobsters, crayfish, mysids, decapods, marron, ostracods, yabbies baboons, monkeys, chimpanzees, apes, marmosets, chimps, orangutans, gibbons, tamarins, bonobos mice, whales, seals, dolphins, rats, deer, rabbits, dogs, elephants, squirrels 2: People and Animals Terms. Borderline, BasicConcept, and Not- The details of our annotation guidelines, the reasons for the intermediate labels, and the annotation study can be found in (Kozareva et al., 2009).</abstract>
<title confidence="0.8923622">ANIMAL TYPE LABEL EXAMPLES Correct GeneticAnimal BehavioralByFeeding BehaviorByHabitat BehaviorSocialIndiv BehaviorSocialGroup MorphologicalType RoleOrFunction saltwater mammal herding animal cloven-hoofed animal Borderline NonRealAnimal EvaluativeTerm OtherAnimal dragons BasicConcept BasicAnimal NotConcept GarbageTerm PEOPLE TYPE LABEL EXAMPLES Correct GeneticPerson NonTransientEventRole TransientEventRole PersonState FamilyRelation SocialRole NationOrTribe ReligiousAffiliation Caucasian, Saxon Borderline NonRealPerson OtherPerson biblicalfigures</title>
<author confidence="0.659212">BasicConcept BasicPerson RealPerson Barack Obama NotConcept GeneralTerm NotPerson</author>
<abstract confidence="0.983272">3: Concept Annotation Labels We measured pairwise inter-annotator agreement across the four labels using the Fleiss kappa (Fleiss, The ranged from 0.61–0.71 for (average and from 0.51–0.70 for (average These agreement scores seemed good enough to warrant using these human judgments to estimate the accuracy of the algorithm. bootstrapping algorithm harvested Anand intermediate concepts in ten After was applied,</abstract>
<address confidence="0.875888833333333">3500 3000 2500 2000 1500 1000</address>
<note confidence="0.647806">500 0</note>
<title confidence="0.6176985">Animal Intermediate Concepts Animal Basic-level Concepts</title>
<address confidence="0.895402">4000 3500 3000 2500 2000 1500 1000</address>
<note confidence="0.547623333333333">500 0 1 2 3 4 5 6 7 8 9 10</note>
<title confidence="0.921897666666667">Iterations People Intermediate Concepts People Instances</title>
<abstract confidence="0.984325057142857">954 we chose a random sample of intermediate concepts with frequency over 1, which was given to four human judges for annotation. Table 4 summarizes the assigned by the four annotators top portion of Table 4 shows the results for all intermediate concepts terms and terms), and the bottom portion shows the results only for the concepts that passed the Concept Test terms and Accuracy is computed in two ways: percent of intermediate concepts labeled as Corthe percent of intermediate concepts as either Without the CPT, accuracies range from 53–66% for Animals and 75–85% for People. After applying the CPT, the accuracies increase to 71–84% for animals and 82–94% for people. These results confirm that the Concept Positioning Test is effective at removing many of the undesirable terms. Overall, these results demonstrate that our algorithm produced many high-quality intermediate concepts, with good precision. Figure 3 shows accuracy curves based on the rankings of the intermediate concepts (based on In- Degree scores). The CPT clearly improves accuracy even among the most highly ranked concepts. example, the for animals show that nearly 90% of the top 100 intermediate concepts were correct after applying the CPT, whereas only 70% of the top 100 intermediate concepts were correct before. However, the CPT also eliminates many desirable terms. For People, the accuracies are still relatively high even without the CPT, and a much larger set of intermediate concepts is learned.</abstract>
<note confidence="0.881152875">Animals People Correct 246 243 251 230 239 231 225 221 Borderline 42 26 22 29 12 10 6 4 BasicConcept 2 8 9 2 6 2 9 10 NotConcept 147 160 155 176 39 53 56 61 Acc1 .56 .56 .57 .53 .81 .78 .76 .75 Acc2 .66 .62 .62 .59 .85 .81 .78 .76 Animals after CPT People after CPT Correct 146 133 144 141 126 126 114 116 Borderline 11 15 9 13 6 2 2 0 BasicConcept 2 8 9 2 0 1 7 7 NotConcept 28 31 25 31 7 10 16 16 Acc1 .78 .71 .77 .75 .91 .91 .82 .83 Acc2 .84 .79 .82 .82 .95 .92 .83 .83 4: Intermediate Concept Evaluation. Animal Intermediate Concepts</note>
<phone confidence="0.668633">50 100 150 200 250 300</phone>
<abstract confidence="0.879247625">Rank 3: Concept Precision at Rank N. 4.2.2 WordNet Evaluation We also compared the intermediate concepts harvested by the algorithm to the contents of WordNet. The results are shown in Table 5. WordNet contains 20% of the Animal concepts and 51% of the People concepts learned by our algorithm, which confirms that many of these concepts were considered to be valuable taxonomic terms by the WordNet developers. However, our human annotators judged 57% of the Animal and 84% of the People concepts to be correct, which suggests that our algorithm generates a substantial number of additional concepts that could be used to enrich taxonomic structure in WordNet.</abstract>
<email confidence="0.370099">noCPT</email>
<phone confidence="0.755717">50 100 150 200 250 300 350 400</phone>
<note confidence="0.861554">Rank Precision 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 noCPT Precision 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 People Intermediate Concepts 955 NotInWN Animal .20 (88/437) .57 (248/437) 204</note>
<date confidence="0.289168">People .51 (152/296) .85 (251/296) 108</date>
<abstract confidence="0.922258333333333">5: Intermediate Concept Evaluation. 4.3 Experiment 3: Taxonomic Links In this section we evaluate the classification (taxonomy) that is learned by evaluating the links between the intermediate concepts and the basic-level concept/instance terms. That is, when our algorithm that how often is a subconof For example, herbivore) correct, but bird) not. Again, since WordNet does not contain all the harvested concepts, we conduct both a manual evaluation and a comparison against WordNet. 4.3.1 Manual and WordNet Evaluations Creating and evaluating the full taxonomic structure between the root and the basic-level or instance terms is future work. Here we evaluate simply the accuracy of the taxonomic links between basic-level concepts/instances and intermediate concepts as harvested, but not between intermediate concepts. For each pair, we extracted all harvested links and determined whether the same links appear in WordNet. The links were also given to human judges. Table 6 shows the results. ISA NotInWN Animal .47(912/1940) .88 (1716/1940) 804 People .23 (318/908) .94 (857/908) 539 6: Taxonomic Evaluation. The results show that WordNet lacks nearly half of the taxonomic relations that were generated by the algorithm: 804 Animal and 539 People links. 5 Conclusion We describe a novel extension to the DAP approach for discovering basic-level concepts or instances and their superconcepts given an initial root concept. By appropriate filling of different positions in DAP, the algorithm alternates between ‘downward’ and ‘upward’ learning. A key resulting benefit is that each new intermediate-level term acquired restarts harvesting in a new region of the concept space, which allows previously unseen concepts to be discovered with each bootstrapping cycle. also introduce the Positioning which serves to confirm that a harvested concept falls into the desired part of the search space relative to either a superordinate or subordinate concept in the growing taxonomy, before it is selected for further harvesting using the DAP. These algorithms can augment other term harvesting algorithms recently reported. But in order to compare different algorithms, it is important to compare results to a standard. WordNet is our best candidate at present. But WordNet is incomplete. Our results include a significantly large number of instances of People (which WordNet does not claim to cover), a number comparable to the results of (Etzioni et al., 2005; Pasca, 2007; Ritter et al., 2009). Rather surprisingly, our results also include a large number of basic-level and intermediate concepts for Animals that are not present in WordNet, a category WordNet is actually fairly complete about. These numbers show clearly that it is important to conduct manual evaluation of term harvesting algorithms in addition to comparing to a standard resource. Acknowledgments This research was supported in part by grants from the National Science Foundation (NSF grant no. IIS- 0429360), and the Department of Homeland Security, ONR Grant numbers N0014-07-1-0152 and N00014-07-1-0149. We are grateful to the annotators at the University of Pittsburgh who helped us evaluate this work: Jay Fischer, David Halpern, Amir Hussain, and Taichi Nakatani.</abstract>
<title confidence="0.778508">References</title>
<author confidence="0.98209">M Banko</author>
<author confidence="0.98209">M Cafarella</author>
<author confidence="0.98209">S Soderland</author>
<author confidence="0.98209">M Broadhead</author>
<note confidence="0.810424857142857">and O.Etzioni. 2007. Open information extraction the web. In of International Joint on Artificial pages 2670–2676. M. Berland and E. Charniak. 1999. Finding Parts in Very Corpora. In of the 37th Annual Meeting of Association for Computational S. Caraballo. 1999. Automatic Acquisition of Hypernym-Labeled Noun Hierarchy from Text. In Proc. of the 37th Annual Meeting of the Association Computational pages 120–126. P. Cimiano and J. Volker. 2005. Towards large-scale, open-domain and ontology-based named entity classi- In of pages 166–172. 956</note>
<author confidence="0.327444">O Etzioni</author>
<author confidence="0.327444">M Cafarella</author>
<author confidence="0.327444">D Downey</author>
<author confidence="0.327444">A Popescu</author>
<abstract confidence="0.817581673913043">T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the an experimental study. 165(1):91–134, June. Fellbaum. 1998. An Electronic Lexical (Language, Speech, and May. M.B. Fleischman and E.H. Hovy. 2002. Fine grained of named entities. In of the August. J.L. Fleiss. 1971. Measuring nominal scale agreeamong many raters. 76(5):378–382. R. Girju, A. Badulescu, and D. Moldovan. 2003. Learning semantic constraints for the automatic discovery of relations. In M. Hearst. 1992. Automatic acquisition of hyponyms large text corpora. In pages 539–545. Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic class learning from the web with hyponym patlinkage graphs. In ofACL-08: pages 1048–1056. Association for Computational Linguistics. Z. Kozareva, E. Hovy, and E. Riloff. 2009. Learning and evaluating the content and structure of a term taxon- In Spring Symposium on Learning by and Learning to G. Mann. 2002. Fine-grained proper noun ontologies for answering. In on pages 1–7. P. McNamee, R. Snow, P. Schone, and J. Mayfield. 2008. Learning named entity hyponyms for question answer- In of the Third International Joint on Natural Language P. Pantel and D. Ravichandran. 2004. Automatically lasemantic classes. In pages 321– 328. M. Pasca. 2004. Acquisition of categorized named enfor web search. In of pages 137–145. M. Pasca. 2007. Weakly-supervised discovery of named using web search queries. In pages 683–690. S. Ponzetto and M. Strube. 2007. Deriving a large scale from wikipedia. In of the 22nd National COnference on Artificial Intelligence (AAAI-</abstract>
<note confidence="0.925019722222222">pages 1440–1447. E. Riloff and R. Jones. 1999. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. of the Sixteenth National Conference Artificial E. Riloff and J. Shepherd. 1997. A Corpus-Based Apfor Building Semantic Lexicons. In Proceedings of the Second Conference on Empirical Methods Natural Language pages 117–124. A. Ritter, S. Soderland, and O. Etzioni. 2009. What is anyway: Automatic hypernym discovery. In Proceedings of AAAI-09 Spring Symposium on Learning Reading and Learning to pages 88–93. B. Roark and E. Charniak. 1998. Noun-phrase Cooccurrence Statistics for Semi-automatic Semantic Construction. In of the 36th Annual Meeting of the Association for Computational pages 1110–1116. Rosch, 1978. of pages 27– 48. R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. M. Thelen and E. Riloff. 2002. A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pat- Contexts. In of the 2002 Conference on Empirical Methods in Natural Language Processpages 214–221. D. Widdows and B. Dorow. 2002. A graph model for unlexical acquisition. In of the 19th international conference on Computational linpages 1–7. J. Yi and W. Niblack. 2005. Sentiment mining in web- In ’05: Proceedings of the 21st In- Conference on Data pages 1073–1083. 957</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open information extraction from the web. In</title>
<date>2007</date>
<booktitle>Proceedings of International Joint Conference on Artificial Itelligence,</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="1702" citStr="Banko et al., 2007" startWordPosition="258" endWordPosition="261"> present an algorithm that is more precise and complete than previous ones for identifying from web text just those concepts ‘below’ a given seed term. Comparing the results to WordNet, we find that the algorithm misses terms, but also that it learns many new terms not in WordNet, and that it classifies them in ways acceptable to humans but different from WordNet. 1 Collecting Information with Care Over the past few years, many algorithms have been published on automatically harvesting terms and their conceptual types from the web and/or other large corpora (Etzioni et al., 2005; Pasca, 2007; Banko et al., 2007; Yi and Niblack, 2005; Snow et al., 2005). But several basic problems limit the eventual utility of the results. First, there is no standard collection of facts against which results can be measured. As we show in this paper, WordNet (Fellbaum, 1998), the most obvious contender because of its size and popularity, is deficient in various ways: it is neither complete nor is its taxonomic structure inarguably perfect. As a result, alternative ad hoc measures are invented that are not comparable. Second, simply harvesting facts about an entity without regard to its actual subsequent organization </context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, M. Cafarella, S. Soderland, M. Broadhead, and O.Etzioni. 2007. Open information extraction from the web. In Proceedings of International Joint Conference on Artificial Itelligence, pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Berland</author>
<author>E Charniak</author>
</authors>
<title>Finding Parts in Very Large Corpora.</title>
<date>1999</date>
<booktitle>In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="15747" citStr="Berland and Charniak, 1999" startWordPosition="2585" endWordPosition="2589">ut considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query, and kept only the unique ones. In total, we collected 1.1 GB of snippets for</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>M. Berland and E. Charniak. 1999. Finding Parts in Very Large Corpora. In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Caraballo</author>
</authors>
<title>Automatic Acquisition of a Hypernym-Labeled Noun Hierarchy from Text.</title>
<date>1999</date>
<booktitle>In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>120--126</pages>
<contexts>
<context position="15631" citStr="Caraballo, 1999" startWordPosition="2569" endWordPosition="2570">the performance of our approach not by measuring the topranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retriev</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>S. Caraballo. 1999. Automatic Acquisition of a Hypernym-Labeled Noun Hierarchy from Text. In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics, pages 120–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>J Volker</author>
</authors>
<title>Towards large-scale, open-domain and ontology-based named entity classification.</title>
<date>2005</date>
<booktitle>In Proceeding of RANLP-05,</booktitle>
<pages>166--172</pages>
<contexts>
<context position="15657" citStr="Cimiano and Volker, 2005" startWordPosition="2571" endWordPosition="2574">f our approach not by measuring the topranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippe</context>
</contexts>
<marker>Cimiano, Volker, 2005</marker>
<rawString>P. Cimiano and J. Volker. 2005. Towards large-scale, open-domain and ontology-based named entity classification. In Proceeding of RANLP-05, pages 166–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="1669" citStr="Etzioni et al., 2005" startWordPosition="252" endWordPosition="255"> some ad hoc evaluation measure. We present an algorithm that is more precise and complete than previous ones for identifying from web text just those concepts ‘below’ a given seed term. Comparing the results to WordNet, we find that the algorithm misses terms, but also that it learns many new terms not in WordNet, and that it classifies them in ways acceptable to humans but different from WordNet. 1 Collecting Information with Care Over the past few years, many algorithms have been published on automatically harvesting terms and their conceptual types from the web and/or other large corpora (Etzioni et al., 2005; Pasca, 2007; Banko et al., 2007; Yi and Niblack, 2005; Snow et al., 2005). But several basic problems limit the eventual utility of the results. First, there is no standard collection of facts against which results can be measured. As we show in this paper, WordNet (Fellbaum, 1998), the most obvious contender because of its size and popularity, is deficient in various ways: it is neither complete nor is its taxonomic structure inarguably perfect. As a result, alternative ad hoc measures are invented that are not comparable. Second, simply harvesting facts about an entity without regard to it</context>
<context position="12983" citStr="Etzioni et al., 2005" startWordPosition="2151" endWordPosition="2155">sely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. 950 “such authors as &lt;X&gt;”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for learning. KnowItAll (Etzioni et al., 2005) applies the hyponym patterns to extract instances from the Web and ranks them by relevance using mutual information. (Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. This approach reported a significant improvement from 5% to 18% over approaches using singly-anchored patterns like those of (Pasca, 2004) and (Etzioni et al., 2005). (Snow et al., 2005) describe a dependency path based approach that generates a large number of weak hypernym patterns using pairs of noun phrases present in WordNet. They build a </context>
<context position="15205" citStr="Etzioni et al., 2005" startWordPosition="2503" endWordPosition="2506">port 100% precision at 18% recall, and 66% precision at 72% recall, considering only the top-5 list. Necessarily, using all the results returned will result in lower precision scores. In contrast to their approach, our aim is to first acquire automatically with minimal supervision the basic-level concepts for given root concept. Thus, we almost entirely eliminate the need for humans to provide hyponym seeds. Second, we evaluate the performance of our approach not by measuring the topranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts disc</context>
<context position="33866" citStr="Etzioni et al., 2005" startWordPosition="5450" endWordPosition="5454">ed concept falls into the desired part of the search space relative to either a superordinate or subordinate concept in the growing taxonomy, before it is selected for further harvesting using the DAP. These algorithms can augment other term harvesting algorithms recently reported. But in order to compare different algorithms, it is important to compare results to a standard. WordNet is our best candidate at present. But WordNet is incomplete. Our results include a significantly large number of instances of People (which WordNet does not claim to cover), a number comparable to the results of (Etzioni et al., 2005; Pasca, 2007; Ritter et al., 2009). Rather surprisingly, our results also include a large number of basic-level and intermediate concepts for Animals that are not present in WordNet, a category WordNet is actually fairly complete about. These numbers show clearly that it is important to conduct manual evaluation of term harvesting algorithms in addition to comparing to a standard resource. Acknowledgments This research was supported in part by grants from the National Science Foundation (NSF grant no. IIS0429360), and the Department of Homeland Security, ONR Grant numbers N0014-07-1-0152 and </context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artificial Intelligence, 165(1):91–134, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<contexts>
<context position="1953" citStr="Fellbaum, 1998" startWordPosition="303" endWordPosition="304">y new terms not in WordNet, and that it classifies them in ways acceptable to humans but different from WordNet. 1 Collecting Information with Care Over the past few years, many algorithms have been published on automatically harvesting terms and their conceptual types from the web and/or other large corpora (Etzioni et al., 2005; Pasca, 2007; Banko et al., 2007; Yi and Niblack, 2005; Snow et al., 2005). But several basic problems limit the eventual utility of the results. First, there is no standard collection of facts against which results can be measured. As we show in this paper, WordNet (Fellbaum, 1998), the most obvious contender because of its size and popularity, is deficient in various ways: it is neither complete nor is its taxonomic structure inarguably perfect. As a result, alternative ad hoc measures are invented that are not comparable. Second, simply harvesting facts about an entity without regard to its actual subsequent organization inflates Recall and Precision evaluation scores: while it is correct that a jaguar is a animal, mammal, toy, sports-team, car-make, and operating-system, this information doesn’t help to create a taxonomy that, for example, places mammal and animal cl</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B Fleischman</author>
<author>E H Hovy</author>
</authors>
<title>Fine grained classification of named entities.</title>
<date>2002</date>
<booktitle>In Proceedings of the COLING conference,</booktitle>
<contexts>
<context position="12317" citStr="Fleischman and Hovy, 2002" startWordPosition="2037" endWordPosition="2040"> passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2. However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. 950 “such authors as &lt;X&gt;”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the c</context>
</contexts>
<marker>Fleischman, Hovy, 2002</marker>
<rawString>M.B. Fleischman and E.H. Hovy. 2002. Fine grained classification of named entities. In Proceedings of the COLING conference, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="27372" citStr="Fleiss, 1971" startWordPosition="4356" endWordPosition="4357">icPerson Caucasian, Saxon NonTransientEventRole stutterer, gourmand TransientEventRole passenger, visitor PersonState dwarf, schizophrenic FamilyRelation aunt, mother SocialRole fugitive, hero NationOrTribe Bulgarian, Zulu ReligiousAffiliation Catholic, atheist Borderline NonRealPerson biblicalfigures OtherPerson colleagues, couples BasicConcept BasicPerson child, woman RealPerson Barack Obama NotConcept GeneralTerm image, figure NotPerson books, events Table 3: Intermediate Concept Annotation Labels We measured pairwise inter-annotator agreement across the four labels using the Fleiss kappa (Fleiss, 1971). The K scores ranged from 0.61–0.71 for Animals (average K=0.66) and from 0.51–0.70 for People (average K=0.60). These agreement scores seemed good enough to warrant using these human judgments to estimate the accuracy of the algorithm. The bootstrapping algorithm harvested 3, 549 Animal and 4,094 People intermediate concepts in ten iterations. After In-Degree ranking was applied, 3500 3000 2500 2000 1500 1000 500 0 Animal Intermediate Concepts Animal Basic-level Concepts #Items Learned 4000 3500 3000 2500 2000 1500 1000 500 0 1 2 3 4 5 6 7 8 9 10 Iterations People Intermediate Concepts Peopl</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>J.L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Badulescu</author>
<author>D Moldovan</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of part-whole relations.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="15768" citStr="Girju et al., 2003" startWordPosition="2590" endWordPosition="2593"> hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query, and kept only the unique ones. In total, we collected 1.1 GB of snippets for Animals and 1.5 GB f</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>R. Girju, A. Badulescu, and D. Moldovan. 2003. Learning semantic constraints for the automatic discovery of part-whole relations. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In COLING,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="12408" citStr="Hearst, 1992" startWordPosition="2055" endWordPosition="2056">ll the concepts that pass the CPT for bootstrapping2. However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. 950 “such authors as &lt;X&gt;”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for learning. KnowItAll (Etzioni et al., 2005) applies the hyponym patt</context>
<context position="13701" citStr="Hearst, 1992" startWordPosition="2264" endWordPosition="2265">information. (Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. This approach reported a significant improvement from 5% to 18% over approaches using singly-anchored patterns like those of (Pasca, 2004) and (Etzioni et al., 2005). (Snow et al., 2005) describe a dependency path based approach that generates a large number of weak hypernym patterns using pairs of noun phrases present in WordNet. They build a classifier using the different hypernym patterns and find among the highest precision patterns those of (Hearst, 1992). Snow et al. report performance of 85% precision at 10% recall and 25% precision at 30% recall for 5300 hand-tagged noun phrase pairs. (McNamee et al., 2008) use the technique of (Snow et al., 2005) to harvest the hypernyms of the proper names. The average precision on 75 automatically detected categories is 53%. The discovered hypernyms were intergrated in a Question Answering system which showed an improvement of 9% when evaluated on a TREC Question Answering data set. Recently, (Ritter et al., 2009) reported hypernym learning using (Hearst, 1992) patterns and manually tagged common and pro</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In COLING, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Riloff</author>
<author>E Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>1048--1056</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6048" citStr="Kozareva et al., 2008" startWordPosition="989" endWordPosition="992">an example of (when the first argument is an instance/example of the second). Section 2 describes our method for harvesting; Section 3 discusses related work; and Section 4 describes the experiments and the results. 2 Term and Relation Extraction using the Doubly-Anchored Pattern Our goal is to develop a technique that automatically ‘fills in’ the concept space in the taxonomy below any root concept, by harvesting terms through repeated web queries. We perform this in two alternating stages. Stage 1: Basic-level/Instance concept collection: We use the Doubly-Anchored Pattern DAP developed in (Kozareva et al., 2008): DAP: [SeedTerm1] such as [SeedTerm2] and &lt;X&gt; which learns a list of basic-level concepts or instances (depending on whether SeedTerm2 expresses a basic-level concept or an instance).1 DAP is very reliable because it is instantiated with examples at both ‘ends’ of the space to be filled (the higher-level (root) concept SeedTerm1 and a basiclevel term or instance (SeedTerm2)), which mutually disambiguate each other. For example, “presidents” for SeedTerm1 can refer to the leader of a country, corporation, or university, and “Ford” for SeedTerm2 can refer to a car company, an automobile pioneer</context>
<context position="8328" citStr="Kozareva et al., 2008" startWordPosition="1367" endWordPosition="1370">vering many different terms during the reckless bootstrapping cycle. This method is very productive, harvesting a constant stream of new terms for basic-level concepts or instances when the taxonomy below the initial root concept SeedTerm1 is extensive (such as for Animals or People). 1Strictly speaking, our lowest-level concepts can be instances, basic-level concepts, or concepts below the basic level (e.g., dachsund). But for the sake of simplicity we will refer to our lowest-level terms as basic-level concepts and instances. 949 Stage 2: Intermediate level concept collection: Going beyond (Kozareva et al., 2008), we next apply the Doubly-Anchored Pattern in the ‘backward’ direction (DAP−1), for any two seed terms representing basic-level concepts or instances: DAP−1: &lt;X&gt; such as [SeedTerm1] and [SeedTerm2] which harvests a set of concepts, most of them intermediate between the basic level or instance and the initial higher-level seed. This second stage (DAP−1) has not yet been described in the literature. It proceeds analogously. For pairs of basic-level concepts or instances below the root concept that were found during the first stage, we instantiate DAP−1 and issue a new web query. For example, if</context>
<context position="13124" citStr="Kozareva et al., 2008" startWordPosition="2174" endWordPosition="2177">onym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. 950 “such authors as &lt;X&gt;”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for learning. KnowItAll (Etzioni et al., 2005) applies the hyponym patterns to extract instances from the Web and ranks them by relevance using mutual information. (Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. This approach reported a significant improvement from 5% to 18% over approaches using singly-anchored patterns like those of (Pasca, 2004) and (Etzioni et al., 2005). (Snow et al., 2005) describe a dependency path based approach that generates a large number of weak hypernym patterns using pairs of noun phrases present in WordNet. They build a classifier using the different hypernym patterns and find among the highest precision patterns those of (Hearst, 1992). Snow et al. report pe</context>
<context position="22130" citStr="Kozareva et al., 2008" startWordPosition="3605" endWordPosition="3608">WordNet contains relatively few proper names). These results show that there is substantial room for improvement in WordNet’s coverage of these categories. For Animals, the precision measured against WordNet is actually higher than the precision measured by human judges, which may indicate that the judges failed to recognize some correct animal terms. PrWN PrH NotInWN Animal .79 .71 48 People .23 .95 986 Table 1: Instance Evaluation. 4.1.3 Evaluation against Prior Work To assess how well our algorithm compares with previous semantic class learning methods, we compared our results to those of (Kozareva et al., 2008). Our work was inspired by that approach–in fact, we use that previous algorithm as the first step of our bootstrapping process. The novelty of our approach is the insertion of an additional bootstrapping stage that iteratively learns new intermediate concepts using DAP−1 and the Concept Positioning Test, followed by the subsequent use of the newly learned intermediate concepts in DAP to expand the search space beyond the original root concept. This leads to the discovery of additional basic-level terms or instances, which are then recycled in turn to discover new intermediate concepts, and so</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In Proceedings ofACL-08: HLT, pages 1048–1056. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Hovy</author>
<author>E Riloff</author>
</authors>
<title>Learning and evaluating the content and structure of a term taxonomy.</title>
<date>2009</date>
<booktitle>In AAAI-09 Spring Symposium on Learning by Reading and Learning to Read.</booktitle>
<contexts>
<context position="26227" citStr="Kozareva et al., 2009" startWordPosition="4237" endWordPosition="4240">foe, Jane Austen, Ernest Hemingway, Franz Kafka ANIMAL IConcept Basic-level Terms Crustacean: shrimp, crabs, prawns, lobsters, crayfish, mysids, decapods, marron, ostracods, yabbies Primates: baboons, monkeys, chimpanzees, apes, marmosets, chimps, orangutans, gibbons, tamarins, bonobos Mammal: mice, whales, seals, dolphins, rats, deer, rabbits, dogs, elephants, squirrels Table 2: Learned People and Animals Terms. types: Correct, Borderline, BasicConcept, and NotConcept. The details of our annotation guidelines, the reasons for the intermediate labels, and the annotation study can be found in (Kozareva et al., 2009). ANIMAL TYPE LABEL EXAMPLES Correct GeneticAnimal reptile,mammal BehavioralByFeeding predator, grazer BehaviorByHabitat saltwater mammal BehaviorSocialIndiv herding animal BehaviorSocialGroup herd, pack MorphologicalType cloven-hoofed animal RoleOrFunction pet, parasite Borderline NonRealAnimal dragons EvaluativeTerm varmint,fox OtherAnimal critter, fossil BasicConcept BasicAnimal dog, hummingbird NotConcept GeneralTerm model, catalyst NotAnimal topic, favorite GarbageTerm brates, mals PEOPLE TYPE LABEL EXAMPLES Correct GeneticPerson Caucasian, Saxon NonTransientEventRole stutterer, gourmand </context>
</contexts>
<marker>Kozareva, Hovy, Riloff, 2009</marker>
<rawString>Z. Kozareva, E. Hovy, and E. Riloff. 2009. Learning and evaluating the content and structure of a term taxonomy. In AAAI-09 Spring Symposium on Learning by Reading and Learning to Read.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
</authors>
<title>Fine-grained proper noun ontologies for question answering.</title>
<date>2002</date>
<booktitle>In COLING-02 on SEMANET,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="15670" citStr="Mann, 2002" startWordPosition="2575" endWordPosition="2576">uring the topranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query,</context>
</contexts>
<marker>Mann, 2002</marker>
<rawString>G. Mann. 2002. Fine-grained proper noun ontologies for question answering. In COLING-02 on SEMANET, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P McNamee</author>
<author>R Snow</author>
<author>P Schone</author>
<author>J Mayfield</author>
</authors>
<title>Learning named entity hyponyms for question answering.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="13859" citStr="McNamee et al., 2008" startWordPosition="2289" endWordPosition="2292">his approach reported a significant improvement from 5% to 18% over approaches using singly-anchored patterns like those of (Pasca, 2004) and (Etzioni et al., 2005). (Snow et al., 2005) describe a dependency path based approach that generates a large number of weak hypernym patterns using pairs of noun phrases present in WordNet. They build a classifier using the different hypernym patterns and find among the highest precision patterns those of (Hearst, 1992). Snow et al. report performance of 85% precision at 10% recall and 25% precision at 30% recall for 5300 hand-tagged noun phrase pairs. (McNamee et al., 2008) use the technique of (Snow et al., 2005) to harvest the hypernyms of the proper names. The average precision on 75 automatically detected categories is 53%. The discovered hypernyms were intergrated in a Question Answering system which showed an improvement of 9% when evaluated on a TREC Question Answering data set. Recently, (Ritter et al., 2009) reported hypernym learning using (Hearst, 1992) patterns and manually tagged common and proper nouns. All hypernym candidates matching the pattern are acquired, and the candidate terms are ranked by mutual information. However, they evaluate the per</context>
</contexts>
<marker>McNamee, Snow, Schone, Mayfield, 2008</marker>
<rawString>P. McNamee, R. Snow, P. Schone, and J. Mayfield. 2008. Learning named entity hyponyms for question answering. In Proceedings of the Third International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes. In</title>
<date>2004</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="12235" citStr="Pantel and Ravichandran, 2004" startWordPosition="2026" endWordPosition="2029">oncept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2. However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. 950 “such authors as &lt;X&gt;”). In recent years, several researchers have followed up on this idea using the web as a corpu</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>P. Pantel and D. Ravichandran. 2004. Automatically labeling semantic classes. In HLT-NAACL, pages 321– 328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
</authors>
<title>Acquisition of categorized named entities for web search.</title>
<date>2004</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>137--145</pages>
<contexts>
<context position="12851" citStr="Pasca, 2004" startWordPosition="2132" endWordPosition="2133">d lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. 950 “such authors as &lt;X&gt;”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for learning. KnowItAll (Etzioni et al., 2005) applies the hyponym patterns to extract instances from the Web and ranks them by relevance using mutual information. (Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. This approach reported a significant improvement from 5% to 18% over approaches using singly-anchored patterns like those of (Pasca, 2004) and (Etzioni et al., 2005). (Snow et al., 2005) describe a dependency path </context>
</contexts>
<marker>Pasca, 2004</marker>
<rawString>M. Pasca. 2004. Acquisition of categorized named entities for web search. In Proceedings of CIKM, pages 137–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
</authors>
<title>Weakly-supervised discovery of named entities using web search queries.</title>
<date>2007</date>
<booktitle>In CIKM,</booktitle>
<pages>683--690</pages>
<contexts>
<context position="1682" citStr="Pasca, 2007" startWordPosition="256" endWordPosition="257">n measure. We present an algorithm that is more precise and complete than previous ones for identifying from web text just those concepts ‘below’ a given seed term. Comparing the results to WordNet, we find that the algorithm misses terms, but also that it learns many new terms not in WordNet, and that it classifies them in ways acceptable to humans but different from WordNet. 1 Collecting Information with Care Over the past few years, many algorithms have been published on automatically harvesting terms and their conceptual types from the web and/or other large corpora (Etzioni et al., 2005; Pasca, 2007; Banko et al., 2007; Yi and Niblack, 2005; Snow et al., 2005). But several basic problems limit the eventual utility of the results. First, there is no standard collection of facts against which results can be measured. As we show in this paper, WordNet (Fellbaum, 1998), the most obvious contender because of its size and popularity, is deficient in various ways: it is neither complete nor is its taxonomic structure inarguably perfect. As a result, alternative ad hoc measures are invented that are not comparable. Second, simply harvesting facts about an entity without regard to its actual subs</context>
<context position="15220" citStr="Pasca, 2007" startWordPosition="2507" endWordPosition="2508">8% recall, and 66% precision at 72% recall, considering only the top-5 list. Necessarily, using all the results returned will result in lower precision scores. In contrast to their approach, our aim is to first acquire automatically with minimal supervision the basic-level concepts for given root concept. Thus, we almost entirely eliminate the need for humans to provide hyponym seeds. Second, we evaluate the performance of our approach not by measuring the topranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this p</context>
<context position="33879" citStr="Pasca, 2007" startWordPosition="5455" endWordPosition="5456">the desired part of the search space relative to either a superordinate or subordinate concept in the growing taxonomy, before it is selected for further harvesting using the DAP. These algorithms can augment other term harvesting algorithms recently reported. But in order to compare different algorithms, it is important to compare results to a standard. WordNet is our best candidate at present. But WordNet is incomplete. Our results include a significantly large number of instances of People (which WordNet does not claim to cover), a number comparable to the results of (Etzioni et al., 2005; Pasca, 2007; Ritter et al., 2009). Rather surprisingly, our results also include a large number of basic-level and intermediate concepts for Animals that are not present in WordNet, a category WordNet is actually fairly complete about. These numbers show clearly that it is important to conduct manual evaluation of term harvesting algorithms in addition to comparing to a standard resource. Acknowledgments This research was supported in part by grants from the National Science Foundation (NSF grant no. IIS0429360), and the Department of Homeland Security, ONR Grant numbers N0014-07-1-0152 and N00014-07-1-0</context>
</contexts>
<marker>Pasca, 2007</marker>
<rawString>M. Pasca. 2007. Weakly-supervised discovery of named entities using web search queries. In CIKM, pages 683–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Deriving a large scale taxonomy from wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd National COnference on Artificial Intelligence (AAAI07),</booktitle>
<pages>1440--1447</pages>
<contexts>
<context position="15541" citStr="Ponzetto and Strube, 2007" startWordPosition="2555" endWordPosition="2558">Thus, we almost entirely eliminate the need for humans to provide hyponym seeds. Second, we evaluate the performance of our approach not by measuring the topranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual in</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>S. Ponzetto and M. Strube. 2007. Deriving a large scale taxonomy from wikipedia. In Proceedings of the 22nd National COnference on Artificial Intelligence (AAAI07), pages 1440–1447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="12290" citStr="Riloff and Jones, 1999" startWordPosition="2033" endWordPosition="2036">hly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2. However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. 950 “such authors as &lt;X&gt;”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patter</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Shepherd</author>
</authors>
<title>A Corpus-Based Approach for Building Semantic Lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>117--124</pages>
<contexts>
<context position="12152" citStr="Riloff and Shepherd, 1997" startWordPosition="2015" endWordPosition="2018">cept such as Concept and &lt;X&gt; If (b) returns more web hits than (a), then the concept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2. However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. 950 “such authors as &lt;X&gt;”). In recen</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>E. Riloff and J. Shepherd. 1997. A Corpus-Based Approach for Building Semantic Lexicons. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 117–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>What is this, anyway: Automatic hypernym discovery.</title>
<date>2009</date>
<booktitle>In Proceedings of AAAI-09 Spring Symposium on Learning by Reading and Learning to Read,</booktitle>
<pages>88--93</pages>
<contexts>
<context position="14209" citStr="Ritter et al., 2009" startWordPosition="2346" endWordPosition="2349">fier using the different hypernym patterns and find among the highest precision patterns those of (Hearst, 1992). Snow et al. report performance of 85% precision at 10% recall and 25% precision at 30% recall for 5300 hand-tagged noun phrase pairs. (McNamee et al., 2008) use the technique of (Snow et al., 2005) to harvest the hypernyms of the proper names. The average precision on 75 automatically detected categories is 53%. The discovered hypernyms were intergrated in a Question Answering system which showed an improvement of 9% when evaluated on a TREC Question Answering data set. Recently, (Ritter et al., 2009) reported hypernym learning using (Hearst, 1992) patterns and manually tagged common and proper nouns. All hypernym candidates matching the pattern are acquired, and the candidate terms are ranked by mutual information. However, they evaluate the performance of their hypernym algorithm by considering only the top 5 hypernyms given a basic-level concept or instance. They report 100% precision at 18% recall, and 66% precision at 72% recall, considering only the top-5 list. Necessarily, using all the results returned will result in lower precision scores. In contrast to their approach, our aim is</context>
<context position="33901" citStr="Ritter et al., 2009" startWordPosition="5457" endWordPosition="5460">art of the search space relative to either a superordinate or subordinate concept in the growing taxonomy, before it is selected for further harvesting using the DAP. These algorithms can augment other term harvesting algorithms recently reported. But in order to compare different algorithms, it is important to compare results to a standard. WordNet is our best candidate at present. But WordNet is incomplete. Our results include a significantly large number of instances of People (which WordNet does not claim to cover), a number comparable to the results of (Etzioni et al., 2005; Pasca, 2007; Ritter et al., 2009). Rather surprisingly, our results also include a large number of basic-level and intermediate concepts for Animals that are not present in WordNet, a category WordNet is actually fairly complete about. These numbers show clearly that it is important to conduct manual evaluation of term harvesting algorithms in addition to comparing to a standard resource. Acknowledgments This research was supported in part by grants from the National Science Foundation (NSF grant no. IIS0429360), and the Department of Homeland Security, ONR Grant numbers N0014-07-1-0152 and N00014-07-1-0149. We are grateful t</context>
</contexts>
<marker>Ritter, Soderland, Etzioni, 2009</marker>
<rawString>A. Ritter, S. Soderland, and O. Etzioni. 2009. What is this, anyway: Automatic hypernym discovery. In Proceedings of AAAI-09 Spring Symposium on Learning by Reading and Learning to Read, pages 88–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Noun-phrase Cooccurrence Statistics for Semi-automatic Semantic Lexicon Construction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1110--1116</pages>
<contexts>
<context position="12179" citStr="Roark and Charniak, 1998" startWordPosition="2019" endWordPosition="2023">&gt; If (b) returns more web hits than (a), then the concept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2. However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. 950 “such authors as &lt;X&gt;”). In recent years, several researcher</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>B. Roark and E. Charniak. 1998. Noun-phrase Cooccurrence Statistics for Semi-automatic Semantic Lexicon Construction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 1110–1116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rosch</author>
</authors>
<title>Principles of Categorization,</title>
<date>1978</date>
<pages>27--48</pages>
<contexts>
<context position="4796" citStr="Rosch, 1978" startWordPosition="775" endWordPosition="776">nology: term: An English word (for our current purposes, a noun or a proper name). seed term: A word we use to initiate the algorithm. concept: An item in the classification taxonomy we are building. A concept may correspond to several terms (singular form, plural form, the term’s synonyms, etc.). root concept: A concept at a fairly general (high) level in the taxonomy, to which many others are eventually learned to be subtypes/instances of. basic-level concept: A concept at the ‘basic level’, corresponding approximately to the Basic Level categories defined in Prototype Theory in Psychology (Rosch, 1978). For our purposes, a concept corresponding to the (proto)typical level of generality of its type; that is, a dog, not a mammal or a dachshund; a singer, not a human or an opera diva. instance: An item in the classification taxonomy that is more specific than a concept; only one example of the instance exists in ‘the real world’ at any time. For example, Michelangelo is an instance, as well as Mazda Miata with license plate 3HCY687, while Mazda Miata is not. classification link: We use a single relation, that, depending on its arguments, is either is a type of (when both arguments are concepts</context>
</contexts>
<marker>Rosch, 1978</marker>
<rawString>E. Rosch, 1978. Principles of Categorization, pages 27– 48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2005</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1744" citStr="Snow et al., 2005" startWordPosition="266" endWordPosition="269"> and complete than previous ones for identifying from web text just those concepts ‘below’ a given seed term. Comparing the results to WordNet, we find that the algorithm misses terms, but also that it learns many new terms not in WordNet, and that it classifies them in ways acceptable to humans but different from WordNet. 1 Collecting Information with Care Over the past few years, many algorithms have been published on automatically harvesting terms and their conceptual types from the web and/or other large corpora (Etzioni et al., 2005; Pasca, 2007; Banko et al., 2007; Yi and Niblack, 2005; Snow et al., 2005). But several basic problems limit the eventual utility of the results. First, there is no standard collection of facts against which results can be measured. As we show in this paper, WordNet (Fellbaum, 1998), the most obvious contender because of its size and popularity, is deficient in various ways: it is neither complete nor is its taxonomic structure inarguably perfect. As a result, alternative ad hoc measures are invented that are not comparable. Second, simply harvesting facts about an entity without regard to its actual subsequent organization inflates Recall and Precision evaluation s</context>
<context position="13423" citStr="Snow et al., 2005" startWordPosition="2219" endWordPosition="2222"> idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for learning. KnowItAll (Etzioni et al., 2005) applies the hyponym patterns to extract instances from the Web and ranks them by relevance using mutual information. (Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. This approach reported a significant improvement from 5% to 18% over approaches using singly-anchored patterns like those of (Pasca, 2004) and (Etzioni et al., 2005). (Snow et al., 2005) describe a dependency path based approach that generates a large number of weak hypernym patterns using pairs of noun phrases present in WordNet. They build a classifier using the different hypernym patterns and find among the highest precision patterns those of (Hearst, 1992). Snow et al. report performance of 85% precision at 10% recall and 25% precision at 30% recall for 5300 hand-tagged noun phrase pairs. (McNamee et al., 2008) use the technique of (Snow et al., 2005) to harvest the hypernyms of the proper names. The average precision on 75 automatically detected categories is 53%. The di</context>
<context position="15244" citStr="Snow et al., 2005" startWordPosition="2510" endWordPosition="2513"> precision at 72% recall, considering only the top-5 list. Necessarily, using all the results returned will result in lower precision scores. In contrast to their approach, our aim is to first acquire automatically with minimal supervision the basic-level concepts for given root concept. Thus, we almost entirely eliminate the need for humans to provide hyponym seeds. Second, we evaluate the performance of our approach not by measuring the topranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and Peo</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pattern Contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>214--221</pages>
<contexts>
<context position="12343" citStr="Thelen and Riloff, 2002" startWordPosition="2041" endWordPosition="2044"> seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2. However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. 950 “such authors as &lt;X&gt;”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for le</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>M. Thelen and E. Riloff. 2002. A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pattern Contexts. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 214–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
<author>B Dorow</author>
</authors>
<title>A graph model for unsupervised lexical acquisition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="15480" citStr="Widdows and Dorow, 2002" startWordPosition="2545" endWordPosition="2548">ervision the basic-level concepts for given root concept. Thus, we almost entirely eliminate the need for humans to provide hyponym seeds. Second, we evaluate the performance of our approach not by measuring the topranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for </context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>D. Widdows and B. Dorow. 2002. A graph model for unsupervised lexical acquisition. In Proceedings of the 19th international conference on Computational linguistics, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yi</author>
<author>W Niblack</author>
</authors>
<title>Sentiment mining in webfountain.</title>
<date>2005</date>
<booktitle>In ICDE ’05: Proceedings of the 21st International Conference on Data Engineering,</booktitle>
<pages>1073--1083</pages>
<contexts>
<context position="1724" citStr="Yi and Niblack, 2005" startWordPosition="262" endWordPosition="265">m that is more precise and complete than previous ones for identifying from web text just those concepts ‘below’ a given seed term. Comparing the results to WordNet, we find that the algorithm misses terms, but also that it learns many new terms not in WordNet, and that it classifies them in ways acceptable to humans but different from WordNet. 1 Collecting Information with Care Over the past few years, many algorithms have been published on automatically harvesting terms and their conceptual types from the web and/or other large corpora (Etzioni et al., 2005; Pasca, 2007; Banko et al., 2007; Yi and Niblack, 2005; Snow et al., 2005). But several basic problems limit the eventual utility of the results. First, there is no standard collection of facts against which results can be measured. As we show in this paper, WordNet (Fellbaum, 1998), the most obvious contender because of its size and popularity, is deficient in various ways: it is neither complete nor is its taxonomic structure inarguably perfect. As a result, alternative ad hoc measures are invented that are not comparable. Second, simply harvesting facts about an entity without regard to its actual subsequent organization inflates Recall and Pr</context>
</contexts>
<marker>Yi, Niblack, 2005</marker>
<rawString>J. Yi and W. Niblack. 2005. Sentiment mining in webfountain. In ICDE ’05: Proceedings of the 21st International Conference on Data Engineering, pages 1073–1083.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>