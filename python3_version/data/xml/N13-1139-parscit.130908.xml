<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.6920115">
Optimal Data Set Selection: An Application to Grapheme-to-Phoneme
Conversion
</title>
<author confidence="0.965308">
Young-Bum Kim and Benjamin Snyder
</author>
<affiliation confidence="0.989706">
University of Wisconsin-Madison
</affiliation>
<email confidence="0.999381">
{ybkim,bsnyder}@cs.wisc.edu
</email>
<sectionHeader confidence="0.99683" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996485857142857">
In this paper we introduce the task of unla-
beled, optimal, data set selection. Given a
large pool of unlabeled examples, our goal is
to select a small subset to label, which will
yield a high performance supervised model
over the entire data set. Our first proposed
method, based on the rank-revealing QR ma-
trix factorization, selects a subset of words
which span the entire word-space effectively.
For our second method, we develop the con-
cept of feature coverage which we optimize
with a greedy algorithm. We apply these
methods to the task of grapheme-to-phoneme
prediction. Experiments over a data-set of 8
languages show that in all scenarios, our selec-
tion methods are effective at yielding a small,
but optimal set of labelled examples. When
fed into a state-of-the-art supervised model for
grapheme-to-phoneme prediction, our meth-
ods yield average error reductions of 20% over
randomly selected examples.
</bodyText>
<sectionHeader confidence="0.998885" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967177777778">
Over the last 15 years, supervised statistical learning
has become the dominant paradigm for building nat-
ural language technologies. While the accuracy of
supervised models can be high, expertly annotated
data sets exist for a small fraction of possible tasks,
genres, and languages. The would-be tool builder
is thus often faced with the prospect of annotating
data, using crowd-sourcing or domain experts. With
limited time and budget, the amount of data to be an-
notated might be small, especially in the prototyping
stage, when the exact specification of the prediction
task may still be in flux, and rapid prototypes are
desired.
In this paper, we propose the problem of unsuper-
vised, optimal data set selection. Formally, given
a large set X of n unlabeled examples, we must
select a subset S C X of size k« n to label.
Our goal is to select such a subset which, when
labeled, will yield a high performance supervised
model over the entire data set X. This task can be
thought of as a zero-stage version of active learn-
ing: we must choose a single batch of examples to
label, without the benefit of any prior labelled data
points. This problem definition avoids the practical
complexity of the active learning set-up (many it-
erations of learning and labeling), and ensures that
the labeled examples are not tied to one particular
model class or task, a well-known danger of active
learning (Settles, 2010). Alternatively, our methods
may be used to create the initial seed set for the ac-
tive learner.
Our initial testbed for optimal data set selec-
tion is the task of grapheme-to-phoneme conver-
sion. In this task, we are given an out-of-vocabulary
word, with the goal of predicting a sequence of
phonemes corresponding to its pronunciation. As
training data, we are given a pronunciation dic-
tionary listing words alongside corresponding se-
quences of phones, representing canonical pronun-
ciations of those words. Such dictionaries are used
as the final bridge between written and spoken lan-
guage for technologies that span this divide, such as
speech recognition, text-to-speech generation, and
speech-to-speech language translation. These dic-
tionaries are necessary: the pronunciation of words
</bodyText>
<page confidence="0.94351">
1196
</page>
<note confidence="0.4707025">
Proceedings of NAACL-HLT 2013, pages 1196–1205,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999955346153846">
continues to evolve after their written form has been
fixed, leading to a large number of rules and ir-
regularities. While large pronunciation dictionaries
of over 100,000 words exist for several major lan-
guages, these resources are entirely lacking for the
majority of the world’s languages. Our goal is to
automatically select a small but optimal subset of
words to be annotated with pronunciation data.
The main intuition behind our approach is that
the subset of selected data points should efficiently
cover the range of phenomena most commonly ob-
served across the pool of unlabeled examples. We
consider two methods. The first comes from a line
of research initiated by the numerical linear algebra
community (Golub, 1965) and taken up by computer
science theoreticians (Boutsidis et al., 2009), with
the name COLUMN SUBSET SELECTION PROBLEM
(CSSP). Given a matrix A, the goal of CSSP is to se-
lect a subset of k columns whose span most closely
captures the range of the full matrix. In particu-
lar, the matrix A formed by orthogonally project-
ing A onto the k-dimensional space spanned by the
selected columns should be a good approximation
to A. By defining AT to be our data matrix, whose
rows correspond to words and whose columns corre-
spond to features (character 4-grams), we can apply
the CSSP randomized algorithm of (Boutsidis et al.,
2009) on A to obtain a subset of k words which best
span the entire space of words.
Our second approach is based on a notion of fea-
ture coverage. We assume that the benefit of seeing
a feature f in a selected word bears some positive
relationship to the frequency of f in the unlabeled
pool. However, we further assume that the lion’s
share of benefit accrues the first few times that we
label a word with feature f, with the marginal util-
ity quickly tapering off as more such examples have
been labeled. We formalize this notion and provide
an exact greedy algorithm for selecting the k data
points with maximal feature coverage.
To assess the benefit of these methods, we ap-
ply them to a suite of 8 languages with pronunci-
ation dictionaries. We consider ranges from 500
to 2000 selected words and train a start-of-the-art
grapheme-to-phoneme prediction model (Bisani and
Ney, 2008). Our experiments show that both meth-
ods produce significant improvements in prediction
quality over randomly selected words, with our fea-
ture coverage method consistently outperforming
the randomized CSSP algorithm. Over the 8 lan-
guages, our method produces average reductions in
error of 20%.
</bodyText>
<sectionHeader confidence="0.954671" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.994444277777778">
Grapheme-to-phoneme Prediction The task of
grapheme-to-phoneme conversion has been consid-
ered in a variety of frameworks, including neural
networks (Sejnowski and Rosenberg, 1987), rule-
based FSA’s (Kaplan and Kay, 1994), and pronun-
ciation by analogy (Marchand and Damper, 2000).
Our goal here is not to compare these methods, so
we focus on the probabilistic joint-sequence model
of Bisani and Ney (2008). This model defines a
joint distribution over a grapheme sequence g E G*
and a phoneme sequence 0 E V, by way of an
unobserved co-segmentation sequence q. Each co-
segmentation unit qi is called a graphone and con-
sists of an aligned pair of zero or one graphemes and
zero or one phonemes: qi E GU{E}x4bUJeJ.1 The
probability of a joint grapheme-phoneme sequence
is then obtained by summing over all possible co-
segmentations:
</bodyText>
<equation confidence="0.9935355">
P(g, 0) = � P(q)
qES(g,φ)
</equation>
<bodyText confidence="0.9999275">
where S(g, 0) denotes the set of all graphone se-
quences which yield g and 0. The probability of a
graphone sequence of length K is defined using an
h-order Markov model with multinomial transitions:
</bodyText>
<equation confidence="0.998036333333333">
k+1
P(q) = P(qiJqi−h, ... , qi−1)
i=1
</equation>
<bodyText confidence="0.999974428571429">
where special start and end symbols are assumed for
qjG1 and qk+1, respectively.
To deal with the unobserved co-segmentation se-
quences, the authors develop an EM training regime
that avoids overfitting using a variety of smoothing
and initialization techniques. Their model produces
state-of-the-art or comparable accuracies across a
</bodyText>
<footnote confidence="0.884437">
1The model generalizes easily to graphones consisting of
more than one grapheme or phoneme, but in both (Bisani and
Ney, 2008) and our initial experiments we found that the 01-to-
01 model always performed best.
</footnote>
<page confidence="0.995921">
1197
</page>
<bodyText confidence="0.99997003125">
wide range of languages and data sets.2 We use the
publicly available code provided by the authors.3 In
all our experiments we set h = 4 (i.e. a 5-gram
model), as we found that accuracy tended to be flat
for h &gt; 4.
Active Learning for G2P Perhaps most closely
related to our work are the papers of Kominek and
Black (2006) and Dwyer and Kondrak (2009), both
of which use active learning to efficiently bootstrap
pronunciation dictionaries. In the former, the au-
thors develop an active learning word selection strat-
egy for inducing pronunciation rules. In fact, their
greedy n-gram selection strategy shares some of
the some intuition as our second data set selection
method, but they were unable to achieve any accu-
racy gains over randomly selected words without ac-
tive learning.
Dwyer and Kondrak use a Query-by-Bagging
active learning strategy over decision tree learn-
ers. They find that their active learning strategy
produces higher accuracy across 5 of the 6 lan-
guages that they explored (English being the ex-
ception). They extract further performance gains
through various refinements to their model. Even
so, we found that the Bisani and Ney grapheme-to-
phoneme (G2P) model (Bisani and Ney, 2008) al-
ways achieved higher accuracy, even when trained
on random words. Furthermore, the relative gains
that we observe using our optimal data set selection
strategies (without any active learning) are much
larger than the relative gains of active learning found
in their study.
</bodyText>
<subsectionHeader confidence="0.951505">
Data Set Selection and Active Learning
</subsectionHeader>
<bodyText confidence="0.90898925">
Eck et al (2005) developed a method for train-
ing compact Machine Translation systems by
selecting a subset of sentences with high n-gram
coverage. Their selection criterion essentially cor-
responds to our feature coverage selection method
using coverage function cove (see Section 3.2). As
our results will show, the use of a geometric feature
discount (cov3) provided better results in our task.
Otherwise, we are not aware of previous work
2We note that the discriminative model of Jiampojamarn and
Kondrak (2010) outperforms the Bisani and Ney model by an
average of about 0.75 percentage points across five data sets.
</bodyText>
<footnote confidence="0.8146015">
3http://www-i6.informatik.rwth-aachen.
de/web/Software/g2p.html
</footnote>
<bodyText confidence="0.999890909090909">
proposing optimal data set selection as a general re-
search problem. Of course, active learning strategies
can be employed for this task by starting with a small
random seed of examples and incrementally adding
small batches. Unfortunately, this can lead to data-
sets that are biased to work well for one particular
class of models and task, but may otherwise perform
worse than a random set of examples (Settles, 2010,
Section 6.6). Furthermore the active learning set-
up can be prohibitively tedious and slow. To illus-
trate, Dwyer and Kondrak (2009) used 190 iterations
of active learning to arrive at 2,000 words. Each
iteration involves bootstrapping 10 different sam-
ples, and training 10 corresponding learners. Thus,
in total, the underlying prediction model is trained
1,900 times. In contrast, our selection methods are
fast, can select any number of data points in a sin-
gle step, and are not tied to a particular prediction
task or model. Furthermore, these methods can be
combined with active learning in selecting the initial
seed set.
Unsupervised Feature Selection Finally, we note
that CSSP and related spectral methods have been
applied to the problem of unsupervised feature se-
lection (Stoppiglia et al., 2003; Mao, 2005; Wolf and
Shashua, 2005; Zhao and Liu, 2007; Boutsidis et al.,
2008). These methods are related to dimensionality
reduction techniques such as Principal Components
Analysis (PCA), but instead of truncating features in
the eigenbasis representation (where each feature is
a linear combination of all the original features), the
goal is to remove dimensions in the standard basis,
leading to a compact set of interpretable features. As
long as the discarded features can be well approxi-
mated by a (linear) function of the selected features,
the loss of information will be minimal.
Our first method for optimal data-set creation ap-
plies a randomized CSSP approach to the transpose
of the data matrix, AT. Equivalently, it selects the
optimal k rows of A for embedding the full set of un-
labeled examples. We use a recently developed ran-
domized algorithm (Boutsidis et al., 2009), and an
underlying rank-revealing QR factorization (Golub,
1965).
</bodyText>
<page confidence="0.949874">
1198
</page>
<figure confidence="0.998834">
(a) (b) (c)
</figure>
<figureCaption confidence="0.999465">
Figure 1: Various versions of the feature coverage function. Panel (a) shows cov1 (Equation 5). Panel (b) shows cove
(Equation 6). Panel (c) shows cov3 (Equation 7) with discount factor q = 1.2.
</figureCaption>
<sectionHeader confidence="0.953943" genericHeader="method">
3 Two Methods for Optimal Data Set
Selection
</sectionHeader>
<bodyText confidence="0.9999864375">
In this section we detail our two proposed methods
for optimal data set selection. The key intuition is
that we would like to pick a subset of data points
which broadly and efficiently cover the features of
the full range of data points. We assume a large pool
X of n unlabeled examples, and our goal is to se-
lect a subset S C X of size k « n for labeling.
We assume that each data point x E X is a vec-
tor of m feature values. Our first method applies to
any real or complex feature space, while our second
method is specialized for binary features. We will
use the (n x m) matrix A to denote our unlabeled
data: each row is a data point and each column is
a feature. In all our experiments, we used the pres-
ence (1) or absence (0) of each character 4-gram as
our set of features.
</bodyText>
<subsectionHeader confidence="0.999967">
3.1 Method 1: Row Subset Selection
</subsectionHeader>
<bodyText confidence="0.999771666666667">
To motivate this method, first consider the task of
finding a rank k approximation to the data matrix A.
The SVD decomposition yields:
</bodyText>
<equation confidence="0.495911">
A = UEV T
</equation>
<listItem confidence="0.982244">
• U is (n x n) orthogonal and its columns form
the eigenvectors of AAT
• V is (m x m) orthogonal and its columns form
the eigenvectors of ATA
• E is (n x m) diagonal, and its diagonal entries
are the singular values of A (the square roots of
the eigenvalues of both AAT and AT A).
</listItem>
<bodyText confidence="0.9701615">
To obtain a rank k approximation to A, we start by
rewriting the SVD decomposition as a sum:
</bodyText>
<equation confidence="0.993312">
QiuivT (1)
i
</equation>
<bodyText confidence="0.999744444444444">
where p = min(m, n), Qi is the ith diagonal entry of
E, ui is the ith column of U, and vi is the ith column
of V . To obtain a rank k approximation to A, we
simply truncate the sum in equation 1 to its first k
terms, yielding Ak. To evaluate the quality of this
approximation, we can measure the Frobenius norm
of the residual matrix ||A − Ak||F.4 The Eckart-
Young theorem (Eckart and Young, 1936) states that
Ak is optimal in the following sense:
</bodyText>
<equation confidence="0.89494">
Ak = argmin ||A − �A||F (2)
</equation>
<bodyText confidence="0.960199666666667">
A s.t. rank(A)=k
In other words, truncated SVD gives the best rank
k approximation to A in terms of minimizing the
Frobenius norm of the residual matrix. In CSSP,
the goal is similar, with the added constraint that the
approximation to A must be obtained by projecting
onto the subspace spanned by a k-subset of the orig-
inal rows of A.5 Formally, the goal is to produce a
(k x m) matrix 5 formed from rows of A, such that
</bodyText>
<equation confidence="0.943463">
||A − A5+5||F (3)
</equation>
<footnote confidence="0.8265836">
4The Frobenius norm M F is defined as the entry-wise L2
��norm: i,j m2 ij
5Though usually framed in terms of column selection, we
switch to row selection here as our goal is to select data points
rather than features.
</footnote>
<equation confidence="0.9025636">
A = � P
i=1
1199
is minimized over all (n ) possible choices for S.
k
</equation>
<bodyText confidence="0.983797333333333">
Here S+ is the (m x k) Moore-Penrose pseudo-
inverse of S, and S+S gives the orthogonal projec-
tor onto the rowspace of S. In other words, our goal
is to select k data points which serve as a good ap-
proximate basis for all the data points. Since AS+S
can be at most rank k, the constraint considered here
is stricter than that of Equation 1, so the truncated
SVD Ak gives a lower bound on the residual.
Boutsidis et al (2009) develop a randomized algo-
rithm that produces a submatrix S (consisting of k
rows of A) which, with high probability, achieves a
residual bound of:
</bodyText>
<equation confidence="0.967702">
�
||A − AS+S||F &lt; O(k log k)||A − Ak||F (4)
</equation>
<bodyText confidence="0.9991946">
in running time O(min{mn2, m2n}). The algo-
rithm proceeds in three steps: first by computing the
SVD of A, then by randomly sampling O(k log k)
rows of A with importance weights carefully com-
puted from the SVD, and then applying a determin-
istic rank-revealing QR factorization (Golub, 1965)
to select k of the sampled rows. To give some in-
tuition, we now provide some background on rank
revealing factorizations.
Rank revealing QR / LQ (RRQR) Every real
(nxm) matrix can be factored as A = LQ, where Q
is (mxm) orthogonal and L is (nxm) lower trian-
gular.6 It is important to notice that in this triangular
factorization, each successive row of A introduces
exactly one new basis vector from Q. We can thus
represent row i as a linear combination of the first
i − 1 rows along with the ith row of Q.
A rank-revealing factorization is one which dis-
plays the numerical rank of the matrix — defined to
be the singular value index r such that
</bodyText>
<equation confidence="0.734438">
σr » σr+1 = O(c)
</equation>
<bodyText confidence="0.999606">
for machine precision c. In the case of the LQ
factorization, our goal is to order the rows of A
such that each successive row has decreasing rep-
resentational importance as a basis for the future
rows. More formally, If there exists a row permu-
tation II such that IIA has a triangular factorization
</bodyText>
<footnote confidence="0.975848666666667">
6We replace the standard upper triangular QR factorization
with an equivalent lower triangular factorization LQ to focus
intuition on the rowspace of A.
</footnote>
<table confidence="0.999939111111111">
Language Training Test Total
Dutch 11,622 104,589 116,211
English 11209 100891 112100
French 2,748 24,721 27,469
Frisian 6,198 55,778 61,976
German 4,942 44,460 49,402
Italian 7,529 79,133 86,662
Norwegian 4,172 37,541 41,713
Spanish 3,150 28,341 31,491
</table>
<tableCaption confidence="0.8714395">
Table 1: Pronunciation dictionary size for each of the lan-
guages.
</tableCaption>
<equation confidence="0.9685965">
IIA = LQ with L = L L11 0
L21 L22 J
</equation>
<bodyText confidence="0.976695333333333">
J where the small-
est singular value of L11 is much greater than the
spectral norm of L22, which is itself almost zero:
</bodyText>
<equation confidence="0.998521">
σmin(L11) » ||L22||2 = O(6)
</equation>
<bodyText confidence="0.99975635">
then we say that IIA = LQ is a rank-revealing LQ
factorization. Both L11 and L22 will be lower tri-
angular matrices and if L11 is (r x r) then A has
numerical rank r (Hong and Pan, 1992).
Implementation In our implementation of the
CSSP algorithm, we first prune away 4-gram fea-
tures that appear in fewer than 3 words, then com-
pute the SVD of the pruned data matrix using
the PROPACK package,7 which efficiently handles
sparse matrixes. After sampling k log k words from
A (with sampling weights calculated from the top-k
singular vectors), we form a submatrix B consist-
ing of the sampled words. We then use the RRQR
implementation from ACM Algorithm 782 (Bischof
and Quintana-Ort´ı, 1998) (routine DGEQPX) to
compute IIB = LQ. We finally select the first k
rows of IIB as our optimal data set. Even for our
largest data sets (English and Dutch), this entire pro-
cedure runs in less than an hour on a 3.4Ghz quad-
core i7 desktop with 32 GB of RAM.
</bodyText>
<subsectionHeader confidence="0.99929">
3.2 Method 2: Feature Coverage Maximization
</subsectionHeader>
<bodyText confidence="0.9994022">
In our previous approach, we adopted a general
method for approximating a matrix with a subset of
rows (or columns). Here we develop a novel objec-
tive function with the specific aim of optimal data set
selection. Our key assumption is that the benefit of
</bodyText>
<footnote confidence="0.972408">
7http://soi.stanford.edu/˜rmunk/PROPACK/
</footnote>
<page confidence="0.976098">
1200
</page>
<bodyText confidence="0.999699384615385">
seeing a new feature f in a selected data point bears
a positive relationship to the frequency of f in the
unlabeled pool of words. However, we further as-
sume that the lion’s share of benefit accrues quickly,
with the marginal utility quickly tapering off as we
label more and more examples with feature f. Note
that for this method, we assume a boolean feature
space.
To formalize this intuition, we will define the cov-
erage of a selected (k x m) submatrix S consisting
of rows of A, with respect to a feature index j. For il-
lustration purposes, we will list three alternative def-
initions:
</bodyText>
<equation confidence="0.9997262">
cov1(S; j) = ||sj||1 (5)
cov2(S; j) = ||aj||1 ff(||sj||1 &gt; 0) (6)
cov3(S; j) = ||aj||1 − ||aj||1 ��||sj||1 &lt; ||aj||1)
�||�j||1
(7)
</equation>
<bodyText confidence="0.904756928571429">
In all cases, sj refers the jth column of S, aj refers
the jth column of A, ff(�) is a 0-1 indicator function,
and ,q is a scalar discount factor.8
Figure 1 provides an intuitive explanation of these
functions: cov1 simply counts the number of se-
lected data points with boolean feature j. Thus, full
coverage (||aj||: the entire number of data points
with the feature) is only achieved when all data
points with the feature are selected. cov2 lies at the
opposite extreme. Even a single selected data point
with feature j triggers coverage of the entire feature.
Finally, cov3 is designed so that the coverage scales
monotonically as additional data points with feature
j are selected. The first selected data point will cap-
ture all but 1 � of the total coverage, and each further
selected data point will capture all but � 1 of what-
ever coverage remains. Essentially, the coverage for
a feature scales as a geometric series in the number
of selected examples having that feature.
To ensure that the total coverage (1 aj||1) is
achieved when all the data points are selected, we
add an indicator function for the case of ||cj||1 =
||aj||1 .9
8Chosen to be 5 in all our experiments. We experimented
with several values between 2 and 10, without significant dif-
ferences in results.
9Otherwise, the geometric coverage function would con-
verge to ||aj ||only as ||cj ||- oc.
</bodyText>
<table confidence="0.999834181818182">
500 Words 2000 Words
RAND CSSP FEAT RAND CSSP FEAT
Dut 48.2 50.8 59.3 69.8 75.0 77.8
Eng 25.4 26.5 29.5 40.3 40.1 42.8
Fra 66.9 69.2 72.1 81.2 82.0 84.8
Fri 42.7 48.0 53.6 62.2 65.3 68.5
Ger 55.2 58.6 65.0 74.2 78.6 80.8
Ita 80.6 82.8 82.8 85.3 86.1 86.8
Nor 48.1 49.5 55.0 66.1 69.9 71.6
Spa 90.7 96.8 95.0 98.1 98.4 99.0
avg 57.2 60.3 64.0 72.2 74.4 76.5
</table>
<tableCaption confidence="0.999305">
Table 2: Test word accuracy across the 8 languages for
</tableCaption>
<bodyText confidence="0.835836125">
randomly selected words (RAND), CSSP matrix subset
selection (CSSP), and Feature Coverage Maximization
(FEAT). We show results for 500 and 2000 word train-
ing sets.
Setting our feature coverage function to cov3, we
can now define the overall feature coverage of the
selected points as:
where ||A||1 is the L1 entrywise matrix norm,
</bodyText>
<equation confidence="0.703852">
E
</equation>
<bodyText confidence="0.977668666666667">
i�j |Aij|, which ensures that 0 &lt; coverage(S) &lt;
1 with equality only achieved when S = A, i.e.
when all data points have been selected.
We provide a brief sketch of our optimization al-
gorithm: To pick the subset S of k words which
optimizes Equation 8, we incrementally build opti-
mal subsets S&apos; C S of size k&apos; &lt; k. At each stage,
we keep track of the unclaimed coverage associated
with each feature j:
</bodyText>
<equation confidence="0.873043">
unclaimed(j) = ||aj||1 − cov3(S&apos;; j)
</equation>
<bodyText confidence="0.995257555555555">
To add a new word, we scan through the pool of re-
maining words, and calculate the additional cover-
age that selecting word w would achieve:
unclaimed (j) ( 77
q − 1 /
We greedily select the word which adds the most
coverage, remove it from the pool, and update the
unclaimed feature coverages. It is easy to show that
this greedy algorithm is globally optimal.
</bodyText>
<equation confidence="0.950440333333333">
1
coverage(S) =
||
A||1 Y_cov3(S; j) (8) j
�(w) = �
feature j in w
</equation>
<page confidence="0.973414">
1201
</page>
<figure confidence="0.987454593220339">
English
85
80
70
75
French
98
96
94
90
92
Spanish
84
87
86
85
83
81
80
82
Italian
40
35
30
25
500 1000 1500 2000
500 1000 1500 2000
500 1000 1500 2000
500 1000 1500 2000
German
70
65
60
50
55
Norwegian
80
60
50
75
70
65
55
Dutch
45
70
65
60
50
55
Frisian
500 1000 1500 2000 500 1000 1500 2000 500 1000 1500 2000 500 1000 1500 2000
Feat Coverage RRQR Random
80
75
70
65
60
55
</figure>
<figureCaption confidence="0.9979365">
Figure 2: Test word accuracy across the 8 languages for (1) feature coverage, (2) CSSP matrix subset selection, (3)
and randomly selected words.
</figureCaption>
<sectionHeader confidence="0.976134" genericHeader="evaluation">
4 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.974478">
To test the effectiveness of the two proposed data
set selection methods, we conduct grapheme-to-
phoneme prediction experiments across a test suite
of 8 languages: Dutch, English, French, Frisian,
German, Italian, Norwegian, and Spanish. The data
was obtained from the PASCAL Letter-to-Phoneme
Conversion Challenge,10 and was processed to
match the setup of Dwyer and Kondrak (2009).
The data comes from a range of sources, includ-
ing CELEX for Dutch and German (Baayen et al.,
1995), BRULEX for French (Mousty et al., 1990),
CMUDict for English,11 the Italian Festival Dictio-
nary (Cosi et al., 2000), as well as pronunciation dic-
tionaries for Spanish, Norwegian, and Frisian (orig-
inal provenance not clear).
As Table 1 shows, the size of the dictionaries
ranges from 31,491 words (Spanish) up to 116,211
words (Dutch). We follow the PASCAL challenge
training and test folds, treating the training set as our
pool of words to be selected for labeling.
Results We consider training subsets of sizes 500,
1000, 1500, and 2000. For our baseline, we train the
</bodyText>
<footnote confidence="0.994787">
10http://pascallin.ecs.soton.ac.uk/
Challenges/PRONALSYL/
11http://www.speech.cs.cmu.edu/cgi-bin/
cmudict
</footnote>
<bodyText confidence="0.998623035714286">
G2P model (Bisani and Ney, 2008) on randomly se-
lected words of each size, and average the results
over 10 runs. We follow the same procedure for
our two data set selection methods. Figure 2 plots
the word prediction accuracy for all three meth-
ods across the eight languages with varying training
sizes, while Table 2 provides corresponding numer-
ical results. We see that in all scenarios the two data
set selection strategies fare better than random sub-
sets of words.
In all but one case, the feature coverage method
yields the best performance (with the exception of
Spanish trained with 500 words, where the CSSP
yields the best results). Feature coverage achieves
average error reduction of 20% over the randomly
selected training words across the different lan-
guages and training set sizes.
Coverage variants We also experimented with
the other versions of the feature coverage function
discussed in Section 3.2 (see Figure 1). While cove
tended to perform quite poorly (usually worse than
random), cove — which gives full credit for each
feature the first time it is seen — yields results just
slightly worse than the CSSP matrix method on av-
erage, and always better than random. In the 2000
word scenario, for example, cove achieves average
accuracy of 74.0, just a bit below the 74.4 accuracy
of the CSSP method. It is also possible that more
</bodyText>
<page confidence="0.977699">
1202
</page>
<table confidence="0.999738">
RAND CSSP FEAT SVD
Fra 0.66 0.62 0.65 0.51
Fry 0.75 0.72 0.75 0.6
Ger 0.71 0.67 0.71 0.55
Ita 0.64 0.61 0.67 0.49
Nor 0.7 0.61 0.64 0.5
Spa 0.65 0.67 0.68 0.53
avg 0.69 0.65 0.68 0.53
</table>
<tableCaption confidence="0.97726775">
Table 3: Residual matrix norm across 6 languages for
randomly selected words (RAND), CSSP matrix subset
selection (CSSP), feature coverage maximization (FEAT),
and the rank k SVD (SVD). Lower is better.
</tableCaption>
<table confidence="0.999348">
RAND CSSP FEAT
Dut 0.66 0.72 0.81
Eng 0.52 0.58 0.69
Fra 0.68 0.74 0.81
Fry 0.7 0.79 0.84
Ger 0.68 0.74 0.81
Ita 0.79 0.84 0.9
Nor 0.7 0.79 0.84
Spa 0.67 0.75 0.8
avg 0.68 0.74 0.81
</table>
<tableCaption confidence="0.995222">
Table 4: Feature coverage across the 8 languages for ran-
</tableCaption>
<bodyText confidence="0.951941736842105">
domly selected words (RAND), CSSP matrix subset selec-
tion (CSSP), and feature coverage maximization (FEAT).
Higher is better.
careful tuning of the discount factor q of cov3 would
yield further gains.
Optimization Analysis Both the CSSP and fea-
ture coverage methods have clearly defined objec-
tive functions — formulated in Equations 3 and 8,
respectively. We can therefore ask how well each
methods fares in optimizing either one of the two
objectives.
First we consider the objective of the CSSP al-
gorithm: to find k data points which can accurately
embed the entire data matrix. Once the data points
are selected, we compute the orthogonal projection
of the data matrix onto the submatrix, obtaining an
approximation matrix A. We can then measure the
residual norm as a fraction of the original matrix
norm:
</bodyText>
<equation confidence="0.304725">
(9)
||A||F
</equation>
<bodyText confidence="0.9011195">
As noted in Section 3.1, truncated SVD minimizes
the residual over all rank k matrices, so we can com-
</bodyText>
<table confidence="0.995581909090909">
CSSP FEAT FEAT-SLS
fettered internationalization rating
exceptionally underestimating overs
gellert schellinger nation
daughtry barristers scherman
blowed constellations olinger
harmonium complementing anderson
cassini bergerman inter
rupees characteristically stated
tewksbury heatherington press
ley overstated conner
</table>
<tableCaption confidence="0.990039333333333">
Table 5: Top 10 words selected by CSSP, feature cov-
erage (FEAT), and feature coverage with stratified length
sampling (FEAT-SLS)
</tableCaption>
<bodyText confidence="0.999993484848485">
pare our three methods — random selections, CSSP,
and feature coverage — all of which select k exam-
ples as a basis, against the lower bound given by
SVD. Table 3 shows the result of this analysis for
k = 2000 (Note that we were unable to compute
the projection matrices for English and Dutch due
to the size of the data and memory limitations). As
expected, SVD fares the best, with CSSP as a some-
what distant second. On average, feature coverage
seems to do a bit better than random.
A similar analysis for the feature coverage objec-
tive function is shown in Table 4. Unsurprisingly,
this objective is best optimized by the feature cov-
erage method. Interestingly though, CSSP seems
to perform about halfway between random and the
feature coverage method. This makes some sense,
as good basis data points will tend to have frequent
features, while at the same time being maximally
spread out from one another. We also note that
the poor coverage result for English in Table 4 mir-
rors its overall poor performance in the G2P predic-
tion task – not only are the phoneme labels unpre-
dictable, but the input data itself is wild and hard to
compress.
Stratified length sampling As Table 5 shows,
the top 10 words selected by the feature coverage
method are mostly long and unusual, averaging 13.3
characters in length. In light of the potential an-
notation burden, we developed a stratified sampling
strategy to ensure typical word lengths. Before se-
lecting each new word, we first sample a word length
according to the empirical word length distribution.
We then choose among words of the sampled length
</bodyText>
<figure confidence="0.382776">
||A − A||F
</figure>
<page confidence="0.922663">
1203
</page>
<bodyText confidence="0.999816666666667">
according to the feature coverage criterion. This re-
sults in more typical words of average length, with
only a very small drop in performance.
</bodyText>
<sectionHeader confidence="0.979042" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999804238095238">
In this paper we proposed the task of optimal data
set selection in the unsupervised setting. In contrast
to active learning, our methods do not require re-
peated training of multiple models and iterative an-
notations. Since the methods are unsupervised, they
also avoid tying the selected data set to a particular
model class (or even task).
We proposed two methods for optimally select-
ing a small subset of examples for labeling. The
first uses techniques developed by the numerical lin-
ear algebra and theory communities for approximat-
ing matrices with subsets of columns or rows. For
our second method, we developed a novel notion
of feature coverage. Experiments on the task of
grapheme-to-phoneme prediction across eight lan-
guages show that our method yields performance
improvements in all scenarios, averaging 20% re-
duction in error. For future work, we intend to apply
the data set selection strategies to other NLP tasks,
such as the optimal selection of sentences for tag-
ging and parsing.
</bodyText>
<sectionHeader confidence="0.998436" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999972">
The authors thank the reviewers and acknowledge
support by the NSF (grant IIS-1116676) and a re-
search gift from Google. Any opinions, findings, or
conclusions are those of the authors, and do not nec-
essarily reflect the views of the NSF.
</bodyText>
<sectionHeader confidence="0.998834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999460378787879">
RH Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The celex lexical database (version release 2)[cd-rom].
Philadelphia, PA: Linguistic Data Consortium, Uni-
versity of Pennsylvania.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434–451, 5.
C.H. Bischof and G. Quintana-Orti. 1998. Algorithm
782: codes for rank-revealing qr factorizations of
dense matrices. ACM Transactions on Mathematical
Software (TOMS), 24(2):254–257.
C. Boutsidis, M.W. Mahoney, and P. Drineas. 2008. Un-
supervised feature selection for principal components
analysis. In Proceeding of the 14th ACM SIGKDD
international conference on Knowledge discovery and
data mining, pages 61–69.
C. Boutsidis, M. W. Mahoney, and P. Drineas. 2009.
An improved approximation algorithm for the column
subset selection problem. In Proceedings of the twen-
tieth Annual ACM-SIAM Symposium on Discrete Al-
gorithms, pages 968–977. Society for Industrial and
Applied Mathematics.
P. Cosi, R. Gretter, and F. Tesser. 2000. Festival
parla italiano. Proceedings of GFS2000, Giornate del
Gruppo di Fonetica Sperimentale, Padova.
K. Dwyer and G. Kondrak. 2009. Reducing the anno-
tation effort for letter-to-phoneme conversion. In Pro-
ceedings of the ACL, pages 127–135. Association for
Computational Linguistics.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine translation
based on n-gram coverage. In Proceedings of the Ma-
chine Translation Summit X.
C. Eckart and G. Young. 1936. The approximation of
one matrix by another of lower rank. Psychometrika,
1(3):211–218.
G. Golub. 1965. Numerical methods for solving lin-
ear least squares problems. Numerische Mathematik,
7(3):206–216.
Yoo Pyo Hong and C-T Pan. 1992. Rank-revealing
factorizations and the singular value decomposition.
Mathematics of Computation, 58(197):213–232.
S. Jiampojamarn and G. Kondrak. 2010. Letter-phoneme
alignment: An exploration. In Proceedings of the
ACL, pages 780–788. Association for Computational
Linguistics.
R.M. Kaplan and M. Kay. 1994. Regular models of
phonological rule systems. Computational linguistics,
20(3):331–378.
J. Kominek and A. W. Black. 2006. Learning pronunci-
ation dictionaries: language complexity and word se-
lection strategies. In Proceedings of the NAACL, pages
232–239. Association for Computational Linguistics.
K.Z. Mao. 2005. Identifying critical variables of prin-
cipal components for unsupervised feature selection.
Systems, Man, and Cybernetics, Part B: Cybernetics,
IEEE Transactions on, 35(2):339–344.
Y. Marchand and R.I. Damper. 2000. A multistrategy ap-
proach to improving pronunciation by analogy. Com-
putational Linguistics, 26(2):195–219.
P. Mousty, M. Radeau, et al. 1990. Brulex. une base de
donn´ees lexicales informatis´ee pour le franc¸ais ´ecrit et
parl´e. L’ann´ee psychologique, 90(4):551–566.
T.J. Sejnowski and C.R. Rosenberg. 1987. Parallel net-
works that learn to pronounce english text. Complex
systems, 1(1):145–168.
</reference>
<page confidence="0.885733">
1204
</page>
<reference confidence="0.999537214285714">
Burr Settles. 2010. Active learning literature survey.
Technical Report TR1648, Department of Computer
Sciences, University of Wisconsin-Madison.
H. Stoppiglia, G. Dreyfus, R. Dubois, and Y. Oussar.
2003. Ranking a random feature for variable and fea-
ture selection. The Journal of Machine Learning Re-
search, 3:1399–1414.
L. Wolf and A. Shashua. 2005. Feature selection for un-
supervised and supervised inference: The emergence
of sparsity in a weight-based approach. The Journal
of Machine Learning Research, 6:1855–1887.
Z. Zhao and H. Liu. 2007. Spectral feature selection for
supervised and unsupervised learning. In Proceedings
of the ICML, pages 1151–1157.
</reference>
<page confidence="0.99087">
1205
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981009">
<title confidence="0.999268">Optimal Data Set Selection: An Application to Grapheme-to-Phoneme Conversion</title>
<author confidence="0.993396">Young-Bum Kim</author>
<author confidence="0.993396">Benjamin</author>
<affiliation confidence="0.992512">University of</affiliation>
<abstract confidence="0.999779727272727">In this paper we introduce the task of unlabeled, optimal, data set selection. Given a large pool of unlabeled examples, our goal is to select a small subset to label, which will yield a high performance supervised model over the entire data set. Our first proposed method, based on the rank-revealing QR matrix factorization, selects a subset of words which span the entire word-space effectively. For our second method, we develop the conof coverage we optimize with a greedy algorithm. We apply these methods to the task of grapheme-to-phoneme prediction. Experiments over a data-set of 8 languages show that in all scenarios, our selection methods are effective at yielding a small, but optimal set of labelled examples. When fed into a state-of-the-art supervised model for grapheme-to-phoneme prediction, our methods yield average error reductions of 20% over randomly selected examples.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>RH Baayen</author>
<author>R Piepenbrock</author>
<author>L Gulikers</author>
</authors>
<title>The celex lexical database (version release 2)[cd-rom].</title>
<date>1995</date>
<institution>Linguistic Data Consortium, University of Pennsylvania.</institution>
<location>Philadelphia, PA:</location>
<contexts>
<context position="23354" citStr="Baayen et al., 1995" startWordPosition="4037" endWordPosition="4040">ss the 8 languages for (1) feature coverage, (2) CSSP matrix subset selection, (3) and randomly selected words. 4 Experiments and Analysis To test the effectiveness of the two proposed data set selection methods, we conduct grapheme-tophoneme prediction experiments across a test suite of 8 languages: Dutch, English, French, Frisian, German, Italian, Norwegian, and Spanish. The data was obtained from the PASCAL Letter-to-Phoneme Conversion Challenge,10 and was processed to match the setup of Dwyer and Kondrak (2009). The data comes from a range of sources, including CELEX for Dutch and German (Baayen et al., 1995), BRULEX for French (Mousty et al., 1990), CMUDict for English,11 the Italian Festival Dictionary (Cosi et al., 2000), as well as pronunciation dictionaries for Spanish, Norwegian, and Frisian (original provenance not clear). As Table 1 shows, the size of the dictionaries ranges from 31,491 words (Spanish) up to 116,211 words (Dutch). We follow the PASCAL challenge training and test folds, treating the training set as our pool of words to be selected for labeling. Results We consider training subsets of sizes 500, 1000, 1500, and 2000. For our baseline, we train the 10http://pascallin.ecs.soto</context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1995</marker>
<rawString>RH Baayen, R. Piepenbrock, and L. Gulikers. 1995. The celex lexical database (version release 2)[cd-rom]. Philadelphia, PA: Linguistic Data Consortium, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Bisani</author>
<author>Hermann Ney</author>
</authors>
<title>Jointsequence models for grapheme-to-phoneme conversion.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>5</issue>
<pages>5</pages>
<contexts>
<context position="5671" citStr="Bisani and Ney, 2008" startWordPosition="920" endWordPosition="923">y of f in the unlabeled pool. However, we further assume that the lion’s share of benefit accrues the first few times that we label a word with feature f, with the marginal utility quickly tapering off as more such examples have been labeled. We formalize this notion and provide an exact greedy algorithm for selecting the k data points with maximal feature coverage. To assess the benefit of these methods, we apply them to a suite of 8 languages with pronunciation dictionaries. We consider ranges from 500 to 2000 selected words and train a start-of-the-art grapheme-to-phoneme prediction model (Bisani and Ney, 2008). Our experiments show that both methods produce significant improvements in prediction quality over randomly selected words, with our feature coverage method consistently outperforming the randomized CSSP algorithm. Over the 8 languages, our method produces average reductions in error of 20%. 2 Background Grapheme-to-phoneme Prediction The task of grapheme-to-phoneme conversion has been considered in a variety of frameworks, including neural networks (Sejnowski and Rosenberg, 1987), rulebased FSA’s (Kaplan and Kay, 1994), and pronunciation by analogy (Marchand and Damper, 2000). Our goal here</context>
<context position="7530" citStr="Bisani and Ney, 2008" startWordPosition="1218" endWordPosition="1221"> The probability of a graphone sequence of length K is defined using an h-order Markov model with multinomial transitions: k+1 P(q) = P(qiJqi−h, ... , qi−1) i=1 where special start and end symbols are assumed for qjG1 and qk+1, respectively. To deal with the unobserved co-segmentation sequences, the authors develop an EM training regime that avoids overfitting using a variety of smoothing and initialization techniques. Their model produces state-of-the-art or comparable accuracies across a 1The model generalizes easily to graphones consisting of more than one grapheme or phoneme, but in both (Bisani and Ney, 2008) and our initial experiments we found that the 01-to01 model always performed best. 1197 wide range of languages and data sets.2 We use the publicly available code provided by the authors.3 In all our experiments we set h = 4 (i.e. a 5-gram model), as we found that accuracy tended to be flat for h &gt; 4. Active Learning for G2P Perhaps most closely related to our work are the papers of Kominek and Black (2006) and Dwyer and Kondrak (2009), both of which use active learning to efficiently bootstrap pronunciation dictionaries. In the former, the authors develop an active learning word selection st</context>
<context position="8821" citStr="Bisani and Ney, 2008" startWordPosition="1434" endWordPosition="1437">selection strategy shares some of the some intuition as our second data set selection method, but they were unable to achieve any accuracy gains over randomly selected words without active learning. Dwyer and Kondrak use a Query-by-Bagging active learning strategy over decision tree learners. They find that their active learning strategy produces higher accuracy across 5 of the 6 languages that they explored (English being the exception). They extract further performance gains through various refinements to their model. Even so, we found that the Bisani and Ney grapheme-tophoneme (G2P) model (Bisani and Ney, 2008) always achieved higher accuracy, even when trained on random words. Furthermore, the relative gains that we observe using our optimal data set selection strategies (without any active learning) are much larger than the relative gains of active learning found in their study. Data Set Selection and Active Learning Eck et al (2005) developed a method for training compact Machine Translation systems by selecting a subset of sentences with high n-gram coverage. Their selection criterion essentially corresponds to our feature coverage selection method using coverage function cove (see Section 3.2).</context>
<context position="24065" citStr="Bisani and Ney, 2008" startWordPosition="4142" endWordPosition="4145">tionary (Cosi et al., 2000), as well as pronunciation dictionaries for Spanish, Norwegian, and Frisian (original provenance not clear). As Table 1 shows, the size of the dictionaries ranges from 31,491 words (Spanish) up to 116,211 words (Dutch). We follow the PASCAL challenge training and test folds, treating the training set as our pool of words to be selected for labeling. Results We consider training subsets of sizes 500, 1000, 1500, and 2000. For our baseline, we train the 10http://pascallin.ecs.soton.ac.uk/ Challenges/PRONALSYL/ 11http://www.speech.cs.cmu.edu/cgi-bin/ cmudict G2P model (Bisani and Ney, 2008) on randomly selected words of each size, and average the results over 10 runs. We follow the same procedure for our two data set selection methods. Figure 2 plots the word prediction accuracy for all three methods across the eight languages with varying training sizes, while Table 2 provides corresponding numerical results. We see that in all scenarios the two data set selection strategies fare better than random subsets of words. In all but one case, the feature coverage method yields the best performance (with the exception of Spanish trained with 500 words, where the CSSP yields the best r</context>
</contexts>
<marker>Bisani, Ney, 2008</marker>
<rawString>Maximilian Bisani and Hermann Ney. 2008. Jointsequence models for grapheme-to-phoneme conversion. Speech Communication, 50(5):434–451, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Bischof</author>
<author>G Quintana-Orti</author>
</authors>
<title>Algorithm 782: codes for rank-revealing qr factorizations of dense matrices.</title>
<date>1998</date>
<journal>ACM Transactions on Mathematical Software (TOMS),</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Bischof, Quintana-Orti, 1998</marker>
<rawString>C.H. Bischof and G. Quintana-Orti. 1998. Algorithm 782: codes for rank-revealing qr factorizations of dense matrices. ACM Transactions on Mathematical Software (TOMS), 24(2):254–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Boutsidis</author>
<author>M W Mahoney</author>
<author>P Drineas</author>
</authors>
<title>Unsupervised feature selection for principal components analysis.</title>
<date>2008</date>
<booktitle>In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>61--69</pages>
<contexts>
<context position="11123" citStr="Boutsidis et al., 2008" startWordPosition="1796" endWordPosition="1799">raining 10 corresponding learners. Thus, in total, the underlying prediction model is trained 1,900 times. In contrast, our selection methods are fast, can select any number of data points in a single step, and are not tied to a particular prediction task or model. Furthermore, these methods can be combined with active learning in selecting the initial seed set. Unsupervised Feature Selection Finally, we note that CSSP and related spectral methods have been applied to the problem of unsupervised feature selection (Stoppiglia et al., 2003; Mao, 2005; Wolf and Shashua, 2005; Zhao and Liu, 2007; Boutsidis et al., 2008). These methods are related to dimensionality reduction techniques such as Principal Components Analysis (PCA), but instead of truncating features in the eigenbasis representation (where each feature is a linear combination of all the original features), the goal is to remove dimensions in the standard basis, leading to a compact set of interpretable features. As long as the discarded features can be well approximated by a (linear) function of the selected features, the loss of information will be minimal. Our first method for optimal data-set creation applies a randomized CSSP approach to the</context>
</contexts>
<marker>Boutsidis, Mahoney, Drineas, 2008</marker>
<rawString>C. Boutsidis, M.W. Mahoney, and P. Drineas. 2008. Unsupervised feature selection for principal components analysis. In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 61–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Boutsidis</author>
<author>M W Mahoney</author>
<author>P Drineas</author>
</authors>
<title>An improved approximation algorithm for the column subset selection problem.</title>
<date>2009</date>
<journal>Society for Industrial and Applied Mathematics.</journal>
<booktitle>In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,</booktitle>
<pages>968--977</pages>
<contexts>
<context position="4241" citStr="Boutsidis et al., 2009" startWordPosition="667" endWordPosition="670"> exist for several major languages, these resources are entirely lacking for the majority of the world’s languages. Our goal is to automatically select a small but optimal subset of words to be annotated with pronunciation data. The main intuition behind our approach is that the subset of selected data points should efficiently cover the range of phenomena most commonly observed across the pool of unlabeled examples. We consider two methods. The first comes from a line of research initiated by the numerical linear algebra community (Golub, 1965) and taken up by computer science theoreticians (Boutsidis et al., 2009), with the name COLUMN SUBSET SELECTION PROBLEM (CSSP). Given a matrix A, the goal of CSSP is to select a subset of k columns whose span most closely captures the range of the full matrix. In particular, the matrix A formed by orthogonally projecting A onto the k-dimensional space spanned by the selected columns should be a good approximation to A. By defining AT to be our data matrix, whose rows correspond to words and whose columns correspond to features (character 4-grams), we can apply the CSSP randomized algorithm of (Boutsidis et al., 2009) on A to obtain a subset of k words which best s</context>
<context position="11930" citStr="Boutsidis et al., 2009" startWordPosition="1926" endWordPosition="1929">re each feature is a linear combination of all the original features), the goal is to remove dimensions in the standard basis, leading to a compact set of interpretable features. As long as the discarded features can be well approximated by a (linear) function of the selected features, the loss of information will be minimal. Our first method for optimal data-set creation applies a randomized CSSP approach to the transpose of the data matrix, AT. Equivalently, it selects the optimal k rows of A for embedding the full set of unlabeled examples. We use a recently developed randomized algorithm (Boutsidis et al., 2009), and an underlying rank-revealing QR factorization (Golub, 1965). 1198 (a) (b) (c) Figure 1: Various versions of the feature coverage function. Panel (a) shows cov1 (Equation 5). Panel (b) shows cove (Equation 6). Panel (c) shows cov3 (Equation 7) with discount factor q = 1.2. 3 Two Methods for Optimal Data Set Selection In this section we detail our two proposed methods for optimal data set selection. The key intuition is that we would like to pick a subset of data points which broadly and efficiently cover the features of the full range of data points. We assume a large pool X of n unlabele</context>
<context position="15227" citStr="Boutsidis et al (2009)" startWordPosition="2569" endWordPosition="2572">y framed in terms of column selection, we switch to row selection here as our goal is to select data points rather than features. A = � P i=1 1199 is minimized over all (n ) possible choices for S. k Here S+ is the (m x k) Moore-Penrose pseudoinverse of S, and S+S gives the orthogonal projector onto the rowspace of S. In other words, our goal is to select k data points which serve as a good approximate basis for all the data points. Since AS+S can be at most rank k, the constraint considered here is stricter than that of Equation 1, so the truncated SVD Ak gives a lower bound on the residual. Boutsidis et al (2009) develop a randomized algorithm that produces a submatrix S (consisting of k rows of A) which, with high probability, achieves a residual bound of: � ||A − AS+S||F &lt; O(k log k)||A − Ak||F (4) in running time O(min{mn2, m2n}). The algorithm proceeds in three steps: first by computing the SVD of A, then by randomly sampling O(k log k) rows of A with importance weights carefully computed from the SVD, and then applying a deterministic rank-revealing QR factorization (Golub, 1965) to select k of the sampled rows. To give some intuition, we now provide some background on rank revealing factorizatio</context>
</contexts>
<marker>Boutsidis, Mahoney, Drineas, 2009</marker>
<rawString>C. Boutsidis, M. W. Mahoney, and P. Drineas. 2009. An improved approximation algorithm for the column subset selection problem. In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 968–977. Society for Industrial and Applied Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cosi</author>
<author>R Gretter</author>
<author>F Tesser</author>
</authors>
<title>Festival parla italiano.</title>
<date>2000</date>
<booktitle>Proceedings of GFS2000, Giornate del Gruppo di Fonetica Sperimentale,</booktitle>
<location>Padova.</location>
<contexts>
<context position="23471" citStr="Cosi et al., 2000" startWordPosition="4056" endWordPosition="4059">iments and Analysis To test the effectiveness of the two proposed data set selection methods, we conduct grapheme-tophoneme prediction experiments across a test suite of 8 languages: Dutch, English, French, Frisian, German, Italian, Norwegian, and Spanish. The data was obtained from the PASCAL Letter-to-Phoneme Conversion Challenge,10 and was processed to match the setup of Dwyer and Kondrak (2009). The data comes from a range of sources, including CELEX for Dutch and German (Baayen et al., 1995), BRULEX for French (Mousty et al., 1990), CMUDict for English,11 the Italian Festival Dictionary (Cosi et al., 2000), as well as pronunciation dictionaries for Spanish, Norwegian, and Frisian (original provenance not clear). As Table 1 shows, the size of the dictionaries ranges from 31,491 words (Spanish) up to 116,211 words (Dutch). We follow the PASCAL challenge training and test folds, treating the training set as our pool of words to be selected for labeling. Results We consider training subsets of sizes 500, 1000, 1500, and 2000. For our baseline, we train the 10http://pascallin.ecs.soton.ac.uk/ Challenges/PRONALSYL/ 11http://www.speech.cs.cmu.edu/cgi-bin/ cmudict G2P model (Bisani and Ney, 2008) on ra</context>
</contexts>
<marker>Cosi, Gretter, Tesser, 2000</marker>
<rawString>P. Cosi, R. Gretter, and F. Tesser. 2000. Festival parla italiano. Proceedings of GFS2000, Giornate del Gruppo di Fonetica Sperimentale, Padova.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dwyer</author>
<author>G Kondrak</author>
</authors>
<title>Reducing the annotation effort for letter-to-phoneme conversion.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>127--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7970" citStr="Dwyer and Kondrak (2009)" startWordPosition="1300" endWordPosition="1303">oduces state-of-the-art or comparable accuracies across a 1The model generalizes easily to graphones consisting of more than one grapheme or phoneme, but in both (Bisani and Ney, 2008) and our initial experiments we found that the 01-to01 model always performed best. 1197 wide range of languages and data sets.2 We use the publicly available code provided by the authors.3 In all our experiments we set h = 4 (i.e. a 5-gram model), as we found that accuracy tended to be flat for h &gt; 4. Active Learning for G2P Perhaps most closely related to our work are the papers of Kominek and Black (2006) and Dwyer and Kondrak (2009), both of which use active learning to efficiently bootstrap pronunciation dictionaries. In the former, the authors develop an active learning word selection strategy for inducing pronunciation rules. In fact, their greedy n-gram selection strategy shares some of the some intuition as our second data set selection method, but they were unable to achieve any accuracy gains over randomly selected words without active learning. Dwyer and Kondrak use a Query-by-Bagging active learning strategy over decision tree learners. They find that their active learning strategy produces higher accuracy acros</context>
<context position="10369" citStr="Dwyer and Kondrak (2009)" startWordPosition="1675" endWordPosition="1678"> data sets. 3http://www-i6.informatik.rwth-aachen. de/web/Software/g2p.html proposing optimal data set selection as a general research problem. Of course, active learning strategies can be employed for this task by starting with a small random seed of examples and incrementally adding small batches. Unfortunately, this can lead to datasets that are biased to work well for one particular class of models and task, but may otherwise perform worse than a random set of examples (Settles, 2010, Section 6.6). Furthermore the active learning setup can be prohibitively tedious and slow. To illustrate, Dwyer and Kondrak (2009) used 190 iterations of active learning to arrive at 2,000 words. Each iteration involves bootstrapping 10 different samples, and training 10 corresponding learners. Thus, in total, the underlying prediction model is trained 1,900 times. In contrast, our selection methods are fast, can select any number of data points in a single step, and are not tied to a particular prediction task or model. Furthermore, these methods can be combined with active learning in selecting the initial seed set. Unsupervised Feature Selection Finally, we note that CSSP and related spectral methods have been applied</context>
<context position="23254" citStr="Dwyer and Kondrak (2009)" startWordPosition="4018" endWordPosition="4021">00 2000 500 1000 1500 2000 Feat Coverage RRQR Random 80 75 70 65 60 55 Figure 2: Test word accuracy across the 8 languages for (1) feature coverage, (2) CSSP matrix subset selection, (3) and randomly selected words. 4 Experiments and Analysis To test the effectiveness of the two proposed data set selection methods, we conduct grapheme-tophoneme prediction experiments across a test suite of 8 languages: Dutch, English, French, Frisian, German, Italian, Norwegian, and Spanish. The data was obtained from the PASCAL Letter-to-Phoneme Conversion Challenge,10 and was processed to match the setup of Dwyer and Kondrak (2009). The data comes from a range of sources, including CELEX for Dutch and German (Baayen et al., 1995), BRULEX for French (Mousty et al., 1990), CMUDict for English,11 the Italian Festival Dictionary (Cosi et al., 2000), as well as pronunciation dictionaries for Spanish, Norwegian, and Frisian (original provenance not clear). As Table 1 shows, the size of the dictionaries ranges from 31,491 words (Spanish) up to 116,211 words (Dutch). We follow the PASCAL challenge training and test folds, treating the training set as our pool of words to be selected for labeling. Results We consider training su</context>
</contexts>
<marker>Dwyer, Kondrak, 2009</marker>
<rawString>K. Dwyer and G. Kondrak. 2009. Reducing the annotation effort for letter-to-phoneme conversion. In Proceedings of the ACL, pages 127–135. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Low cost portability for statistical machine translation based on n-gram coverage.</title>
<date>2005</date>
<booktitle>In Proceedings of the Machine Translation Summit X.</booktitle>
<contexts>
<context position="9152" citStr="Eck et al (2005)" startWordPosition="1487" endWordPosition="1490">egy produces higher accuracy across 5 of the 6 languages that they explored (English being the exception). They extract further performance gains through various refinements to their model. Even so, we found that the Bisani and Ney grapheme-tophoneme (G2P) model (Bisani and Ney, 2008) always achieved higher accuracy, even when trained on random words. Furthermore, the relative gains that we observe using our optimal data set selection strategies (without any active learning) are much larger than the relative gains of active learning found in their study. Data Set Selection and Active Learning Eck et al (2005) developed a method for training compact Machine Translation systems by selecting a subset of sentences with high n-gram coverage. Their selection criterion essentially corresponds to our feature coverage selection method using coverage function cove (see Section 3.2). As our results will show, the use of a geometric feature discount (cov3) provided better results in our task. Otherwise, we are not aware of previous work 2We note that the discriminative model of Jiampojamarn and Kondrak (2010) outperforms the Bisani and Ney model by an average of about 0.75 percentage points across five data s</context>
</contexts>
<marker>Eck, Vogel, Waibel, 2005</marker>
<rawString>Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Low cost portability for statistical machine translation based on n-gram coverage. In Proceedings of the Machine Translation Summit X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Eckart</author>
<author>G Young</author>
</authors>
<title>The approximation of one matrix by another of lower rank.</title>
<date>1936</date>
<journal>Psychometrika,</journal>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="13998" citStr="Eckart and Young, 1936" startWordPosition="2329" endWordPosition="2332">gonal, and its diagonal entries are the singular values of A (the square roots of the eigenvalues of both AAT and AT A). To obtain a rank k approximation to A, we start by rewriting the SVD decomposition as a sum: QiuivT (1) i where p = min(m, n), Qi is the ith diagonal entry of E, ui is the ith column of U, and vi is the ith column of V . To obtain a rank k approximation to A, we simply truncate the sum in equation 1 to its first k terms, yielding Ak. To evaluate the quality of this approximation, we can measure the Frobenius norm of the residual matrix ||A − Ak||F.4 The EckartYoung theorem (Eckart and Young, 1936) states that Ak is optimal in the following sense: Ak = argmin ||A − �A||F (2) A s.t. rank(A)=k In other words, truncated SVD gives the best rank k approximation to A in terms of minimizing the Frobenius norm of the residual matrix. In CSSP, the goal is similar, with the added constraint that the approximation to A must be obtained by projecting onto the subspace spanned by a k-subset of the original rows of A.5 Formally, the goal is to produce a (k x m) matrix 5 formed from rows of A, such that ||A − A5+5||F (3) 4The Frobenius norm M F is defined as the entry-wise L2 ��norm: i,j m2 ij 5Though</context>
</contexts>
<marker>Eckart, Young, 1936</marker>
<rawString>C. Eckart and G. Young. 1936. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Golub</author>
</authors>
<title>Numerical methods for solving linear least squares problems.</title>
<date>1965</date>
<journal>Numerische Mathematik,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="4169" citStr="Golub, 1965" startWordPosition="658" endWordPosition="659"> While large pronunciation dictionaries of over 100,000 words exist for several major languages, these resources are entirely lacking for the majority of the world’s languages. Our goal is to automatically select a small but optimal subset of words to be annotated with pronunciation data. The main intuition behind our approach is that the subset of selected data points should efficiently cover the range of phenomena most commonly observed across the pool of unlabeled examples. We consider two methods. The first comes from a line of research initiated by the numerical linear algebra community (Golub, 1965) and taken up by computer science theoreticians (Boutsidis et al., 2009), with the name COLUMN SUBSET SELECTION PROBLEM (CSSP). Given a matrix A, the goal of CSSP is to select a subset of k columns whose span most closely captures the range of the full matrix. In particular, the matrix A formed by orthogonally projecting A onto the k-dimensional space spanned by the selected columns should be a good approximation to A. By defining AT to be our data matrix, whose rows correspond to words and whose columns correspond to features (character 4-grams), we can apply the CSSP randomized algorithm of </context>
<context position="11995" citStr="Golub, 1965" startWordPosition="1936" endWordPosition="1937">oal is to remove dimensions in the standard basis, leading to a compact set of interpretable features. As long as the discarded features can be well approximated by a (linear) function of the selected features, the loss of information will be minimal. Our first method for optimal data-set creation applies a randomized CSSP approach to the transpose of the data matrix, AT. Equivalently, it selects the optimal k rows of A for embedding the full set of unlabeled examples. We use a recently developed randomized algorithm (Boutsidis et al., 2009), and an underlying rank-revealing QR factorization (Golub, 1965). 1198 (a) (b) (c) Figure 1: Various versions of the feature coverage function. Panel (a) shows cov1 (Equation 5). Panel (b) shows cove (Equation 6). Panel (c) shows cov3 (Equation 7) with discount factor q = 1.2. 3 Two Methods for Optimal Data Set Selection In this section we detail our two proposed methods for optimal data set selection. The key intuition is that we would like to pick a subset of data points which broadly and efficiently cover the features of the full range of data points. We assume a large pool X of n unlabeled examples, and our goal is to select a subset S C X of size k « </context>
<context position="15708" citStr="Golub, 1965" startWordPosition="2656" endWordPosition="2657">dered here is stricter than that of Equation 1, so the truncated SVD Ak gives a lower bound on the residual. Boutsidis et al (2009) develop a randomized algorithm that produces a submatrix S (consisting of k rows of A) which, with high probability, achieves a residual bound of: � ||A − AS+S||F &lt; O(k log k)||A − Ak||F (4) in running time O(min{mn2, m2n}). The algorithm proceeds in three steps: first by computing the SVD of A, then by randomly sampling O(k log k) rows of A with importance weights carefully computed from the SVD, and then applying a deterministic rank-revealing QR factorization (Golub, 1965) to select k of the sampled rows. To give some intuition, we now provide some background on rank revealing factorizations. Rank revealing QR / LQ (RRQR) Every real (nxm) matrix can be factored as A = LQ, where Q is (mxm) orthogonal and L is (nxm) lower triangular.6 It is important to notice that in this triangular factorization, each successive row of A introduces exactly one new basis vector from Q. We can thus represent row i as a linear combination of the first i − 1 rows along with the ith row of Q. A rank-revealing factorization is one which displays the numerical rank of the matrix — def</context>
</contexts>
<marker>Golub, 1965</marker>
<rawString>G. Golub. 1965. Numerical methods for solving linear least squares problems. Numerische Mathematik, 7(3):206–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoo Pyo Hong</author>
<author>C-T Pan</author>
</authors>
<title>Rank-revealing factorizations and the singular value decomposition.</title>
<date>1992</date>
<journal>Mathematics of Computation,</journal>
<volume>58</volume>
<issue>197</issue>
<contexts>
<context position="17513" citStr="Hong and Pan, 1992" startWordPosition="2984" endWordPosition="2987">09 100891 112100 French 2,748 24,721 27,469 Frisian 6,198 55,778 61,976 German 4,942 44,460 49,402 Italian 7,529 79,133 86,662 Norwegian 4,172 37,541 41,713 Spanish 3,150 28,341 31,491 Table 1: Pronunciation dictionary size for each of the languages. IIA = LQ with L = L L11 0 L21 L22 J J where the smallest singular value of L11 is much greater than the spectral norm of L22, which is itself almost zero: σmin(L11) » ||L22||2 = O(6) then we say that IIA = LQ is a rank-revealing LQ factorization. Both L11 and L22 will be lower triangular matrices and if L11 is (r x r) then A has numerical rank r (Hong and Pan, 1992). Implementation In our implementation of the CSSP algorithm, we first prune away 4-gram features that appear in fewer than 3 words, then compute the SVD of the pruned data matrix using the PROPACK package,7 which efficiently handles sparse matrixes. After sampling k log k words from A (with sampling weights calculated from the top-k singular vectors), we form a submatrix B consisting of the sampled words. We then use the RRQR implementation from ACM Algorithm 782 (Bischof and Quintana-Ort´ı, 1998) (routine DGEQPX) to compute IIB = LQ. We finally select the first k rows of IIB as our optimal d</context>
</contexts>
<marker>Hong, Pan, 1992</marker>
<rawString>Yoo Pyo Hong and C-T Pan. 1992. Rank-revealing factorizations and the singular value decomposition. Mathematics of Computation, 58(197):213–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jiampojamarn</author>
<author>G Kondrak</author>
</authors>
<title>Letter-phoneme alignment: An exploration.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>780--788</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9650" citStr="Jiampojamarn and Kondrak (2010)" startWordPosition="1564" endWordPosition="1567"> much larger than the relative gains of active learning found in their study. Data Set Selection and Active Learning Eck et al (2005) developed a method for training compact Machine Translation systems by selecting a subset of sentences with high n-gram coverage. Their selection criterion essentially corresponds to our feature coverage selection method using coverage function cove (see Section 3.2). As our results will show, the use of a geometric feature discount (cov3) provided better results in our task. Otherwise, we are not aware of previous work 2We note that the discriminative model of Jiampojamarn and Kondrak (2010) outperforms the Bisani and Ney model by an average of about 0.75 percentage points across five data sets. 3http://www-i6.informatik.rwth-aachen. de/web/Software/g2p.html proposing optimal data set selection as a general research problem. Of course, active learning strategies can be employed for this task by starting with a small random seed of examples and incrementally adding small batches. Unfortunately, this can lead to datasets that are biased to work well for one particular class of models and task, but may otherwise perform worse than a random set of examples (Settles, 2010, Section 6.6</context>
</contexts>
<marker>Jiampojamarn, Kondrak, 2010</marker>
<rawString>S. Jiampojamarn and G. Kondrak. 2010. Letter-phoneme alignment: An exploration. In Proceedings of the ACL, pages 780–788. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>M Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="6198" citStr="Kaplan and Kay, 1994" startWordPosition="995" endWordPosition="998">words and train a start-of-the-art grapheme-to-phoneme prediction model (Bisani and Ney, 2008). Our experiments show that both methods produce significant improvements in prediction quality over randomly selected words, with our feature coverage method consistently outperforming the randomized CSSP algorithm. Over the 8 languages, our method produces average reductions in error of 20%. 2 Background Grapheme-to-phoneme Prediction The task of grapheme-to-phoneme conversion has been considered in a variety of frameworks, including neural networks (Sejnowski and Rosenberg, 1987), rulebased FSA’s (Kaplan and Kay, 1994), and pronunciation by analogy (Marchand and Damper, 2000). Our goal here is not to compare these methods, so we focus on the probabilistic joint-sequence model of Bisani and Ney (2008). This model defines a joint distribution over a grapheme sequence g E G* and a phoneme sequence 0 E V, by way of an unobserved co-segmentation sequence q. Each cosegmentation unit qi is called a graphone and consists of an aligned pair of zero or one graphemes and zero or one phonemes: qi E GU{E}x4bUJeJ.1 The probability of a joint grapheme-phoneme sequence is then obtained by summing over all possible cosegmen</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>R.M. Kaplan and M. Kay. 1994. Regular models of phonological rule systems. Computational linguistics, 20(3):331–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kominek</author>
<author>A W Black</author>
</authors>
<title>Learning pronunciation dictionaries: language complexity and word selection strategies.</title>
<date>2006</date>
<booktitle>In Proceedings of the NAACL,</booktitle>
<pages>232--239</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7941" citStr="Kominek and Black (2006)" startWordPosition="1295" endWordPosition="1298">on techniques. Their model produces state-of-the-art or comparable accuracies across a 1The model generalizes easily to graphones consisting of more than one grapheme or phoneme, but in both (Bisani and Ney, 2008) and our initial experiments we found that the 01-to01 model always performed best. 1197 wide range of languages and data sets.2 We use the publicly available code provided by the authors.3 In all our experiments we set h = 4 (i.e. a 5-gram model), as we found that accuracy tended to be flat for h &gt; 4. Active Learning for G2P Perhaps most closely related to our work are the papers of Kominek and Black (2006) and Dwyer and Kondrak (2009), both of which use active learning to efficiently bootstrap pronunciation dictionaries. In the former, the authors develop an active learning word selection strategy for inducing pronunciation rules. In fact, their greedy n-gram selection strategy shares some of the some intuition as our second data set selection method, but they were unable to achieve any accuracy gains over randomly selected words without active learning. Dwyer and Kondrak use a Query-by-Bagging active learning strategy over decision tree learners. They find that their active learning strategy p</context>
</contexts>
<marker>Kominek, Black, 2006</marker>
<rawString>J. Kominek and A. W. Black. 2006. Learning pronunciation dictionaries: language complexity and word selection strategies. In Proceedings of the NAACL, pages 232–239. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Z Mao</author>
</authors>
<title>Identifying critical variables of principal components for unsupervised feature selection.</title>
<date>2005</date>
<journal>Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on,</journal>
<volume>35</volume>
<issue>2</issue>
<contexts>
<context position="11054" citStr="Mao, 2005" startWordPosition="1786" endWordPosition="1787">tion involves bootstrapping 10 different samples, and training 10 corresponding learners. Thus, in total, the underlying prediction model is trained 1,900 times. In contrast, our selection methods are fast, can select any number of data points in a single step, and are not tied to a particular prediction task or model. Furthermore, these methods can be combined with active learning in selecting the initial seed set. Unsupervised Feature Selection Finally, we note that CSSP and related spectral methods have been applied to the problem of unsupervised feature selection (Stoppiglia et al., 2003; Mao, 2005; Wolf and Shashua, 2005; Zhao and Liu, 2007; Boutsidis et al., 2008). These methods are related to dimensionality reduction techniques such as Principal Components Analysis (PCA), but instead of truncating features in the eigenbasis representation (where each feature is a linear combination of all the original features), the goal is to remove dimensions in the standard basis, leading to a compact set of interpretable features. As long as the discarded features can be well approximated by a (linear) function of the selected features, the loss of information will be minimal. Our first method fo</context>
</contexts>
<marker>Mao, 2005</marker>
<rawString>K.Z. Mao. 2005. Identifying critical variables of principal components for unsupervised feature selection. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, 35(2):339–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Marchand</author>
<author>R I Damper</author>
</authors>
<title>A multistrategy approach to improving pronunciation by analogy.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="6256" citStr="Marchand and Damper, 2000" startWordPosition="1004" endWordPosition="1007">e prediction model (Bisani and Ney, 2008). Our experiments show that both methods produce significant improvements in prediction quality over randomly selected words, with our feature coverage method consistently outperforming the randomized CSSP algorithm. Over the 8 languages, our method produces average reductions in error of 20%. 2 Background Grapheme-to-phoneme Prediction The task of grapheme-to-phoneme conversion has been considered in a variety of frameworks, including neural networks (Sejnowski and Rosenberg, 1987), rulebased FSA’s (Kaplan and Kay, 1994), and pronunciation by analogy (Marchand and Damper, 2000). Our goal here is not to compare these methods, so we focus on the probabilistic joint-sequence model of Bisani and Ney (2008). This model defines a joint distribution over a grapheme sequence g E G* and a phoneme sequence 0 E V, by way of an unobserved co-segmentation sequence q. Each cosegmentation unit qi is called a graphone and consists of an aligned pair of zero or one graphemes and zero or one phonemes: qi E GU{E}x4bUJeJ.1 The probability of a joint grapheme-phoneme sequence is then obtained by summing over all possible cosegmentations: P(g, 0) = � P(q) qES(g,φ) where S(g, 0) denotes t</context>
</contexts>
<marker>Marchand, Damper, 2000</marker>
<rawString>Y. Marchand and R.I. Damper. 2000. A multistrategy approach to improving pronunciation by analogy. Computational Linguistics, 26(2):195–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mousty</author>
<author>M Radeau</author>
</authors>
<title>Brulex. une base de donn´ees lexicales informatis´ee pour le franc¸ais ´ecrit et parl´e. L’ann´ee psychologique,</title>
<date>1990</date>
<pages>90--4</pages>
<marker>Mousty, Radeau, 1990</marker>
<rawString>P. Mousty, M. Radeau, et al. 1990. Brulex. une base de donn´ees lexicales informatis´ee pour le franc¸ais ´ecrit et parl´e. L’ann´ee psychologique, 90(4):551–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J Sejnowski</author>
<author>C R Rosenberg</author>
</authors>
<title>Parallel networks that learn to pronounce english text.</title>
<date>1987</date>
<journal>Complex systems,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="6158" citStr="Sejnowski and Rosenberg, 1987" startWordPosition="988" endWordPosition="991">es. We consider ranges from 500 to 2000 selected words and train a start-of-the-art grapheme-to-phoneme prediction model (Bisani and Ney, 2008). Our experiments show that both methods produce significant improvements in prediction quality over randomly selected words, with our feature coverage method consistently outperforming the randomized CSSP algorithm. Over the 8 languages, our method produces average reductions in error of 20%. 2 Background Grapheme-to-phoneme Prediction The task of grapheme-to-phoneme conversion has been considered in a variety of frameworks, including neural networks (Sejnowski and Rosenberg, 1987), rulebased FSA’s (Kaplan and Kay, 1994), and pronunciation by analogy (Marchand and Damper, 2000). Our goal here is not to compare these methods, so we focus on the probabilistic joint-sequence model of Bisani and Ney (2008). This model defines a joint distribution over a grapheme sequence g E G* and a phoneme sequence 0 E V, by way of an unobserved co-segmentation sequence q. Each cosegmentation unit qi is called a graphone and consists of an aligned pair of zero or one graphemes and zero or one phonemes: qi E GU{E}x4bUJeJ.1 The probability of a joint grapheme-phoneme sequence is then obtain</context>
</contexts>
<marker>Sejnowski, Rosenberg, 1987</marker>
<rawString>T.J. Sejnowski and C.R. Rosenberg. 1987. Parallel networks that learn to pronounce english text. Complex systems, 1(1):145–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Active learning literature survey.</title>
<date>2010</date>
<tech>Technical Report TR1648,</tech>
<institution>Department of Computer Sciences, University of Wisconsin-Madison.</institution>
<contexts>
<context position="2518" citStr="Settles, 2010" startWordPosition="405" endWordPosition="406">S C X of size k« n to label. Our goal is to select such a subset which, when labeled, will yield a high performance supervised model over the entire data set X. This task can be thought of as a zero-stage version of active learning: we must choose a single batch of examples to label, without the benefit of any prior labelled data points. This problem definition avoids the practical complexity of the active learning set-up (many iterations of learning and labeling), and ensures that the labeled examples are not tied to one particular model class or task, a well-known danger of active learning (Settles, 2010). Alternatively, our methods may be used to create the initial seed set for the active learner. Our initial testbed for optimal data set selection is the task of grapheme-to-phoneme conversion. In this task, we are given an out-of-vocabulary word, with the goal of predicting a sequence of phonemes corresponding to its pronunciation. As training data, we are given a pronunciation dictionary listing words alongside corresponding sequences of phones, representing canonical pronunciations of those words. Such dictionaries are used as the final bridge between written and spoken language for technol</context>
<context position="10237" citStr="Settles, 2010" startWordPosition="1656" endWordPosition="1657">ojamarn and Kondrak (2010) outperforms the Bisani and Ney model by an average of about 0.75 percentage points across five data sets. 3http://www-i6.informatik.rwth-aachen. de/web/Software/g2p.html proposing optimal data set selection as a general research problem. Of course, active learning strategies can be employed for this task by starting with a small random seed of examples and incrementally adding small batches. Unfortunately, this can lead to datasets that are biased to work well for one particular class of models and task, but may otherwise perform worse than a random set of examples (Settles, 2010, Section 6.6). Furthermore the active learning setup can be prohibitively tedious and slow. To illustrate, Dwyer and Kondrak (2009) used 190 iterations of active learning to arrive at 2,000 words. Each iteration involves bootstrapping 10 different samples, and training 10 corresponding learners. Thus, in total, the underlying prediction model is trained 1,900 times. In contrast, our selection methods are fast, can select any number of data points in a single step, and are not tied to a particular prediction task or model. Furthermore, these methods can be combined with active learning in sele</context>
</contexts>
<marker>Settles, 2010</marker>
<rawString>Burr Settles. 2010. Active learning literature survey. Technical Report TR1648, Department of Computer Sciences, University of Wisconsin-Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Stoppiglia</author>
<author>G Dreyfus</author>
<author>R Dubois</author>
<author>Y Oussar</author>
</authors>
<title>Ranking a random feature for variable and feature selection.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1399</pages>
<contexts>
<context position="11043" citStr="Stoppiglia et al., 2003" startWordPosition="1782" endWordPosition="1785">t 2,000 words. Each iteration involves bootstrapping 10 different samples, and training 10 corresponding learners. Thus, in total, the underlying prediction model is trained 1,900 times. In contrast, our selection methods are fast, can select any number of data points in a single step, and are not tied to a particular prediction task or model. Furthermore, these methods can be combined with active learning in selecting the initial seed set. Unsupervised Feature Selection Finally, we note that CSSP and related spectral methods have been applied to the problem of unsupervised feature selection (Stoppiglia et al., 2003; Mao, 2005; Wolf and Shashua, 2005; Zhao and Liu, 2007; Boutsidis et al., 2008). These methods are related to dimensionality reduction techniques such as Principal Components Analysis (PCA), but instead of truncating features in the eigenbasis representation (where each feature is a linear combination of all the original features), the goal is to remove dimensions in the standard basis, leading to a compact set of interpretable features. As long as the discarded features can be well approximated by a (linear) function of the selected features, the loss of information will be minimal. Our firs</context>
</contexts>
<marker>Stoppiglia, Dreyfus, Dubois, Oussar, 2003</marker>
<rawString>H. Stoppiglia, G. Dreyfus, R. Dubois, and Y. Oussar. 2003. Ranking a random feature for variable and feature selection. The Journal of Machine Learning Research, 3:1399–1414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wolf</author>
<author>A Shashua</author>
</authors>
<title>Feature selection for unsupervised and supervised inference: The emergence of sparsity in a weight-based approach.</title>
<date>2005</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>6--1855</pages>
<contexts>
<context position="11078" citStr="Wolf and Shashua, 2005" startWordPosition="1788" endWordPosition="1791">es bootstrapping 10 different samples, and training 10 corresponding learners. Thus, in total, the underlying prediction model is trained 1,900 times. In contrast, our selection methods are fast, can select any number of data points in a single step, and are not tied to a particular prediction task or model. Furthermore, these methods can be combined with active learning in selecting the initial seed set. Unsupervised Feature Selection Finally, we note that CSSP and related spectral methods have been applied to the problem of unsupervised feature selection (Stoppiglia et al., 2003; Mao, 2005; Wolf and Shashua, 2005; Zhao and Liu, 2007; Boutsidis et al., 2008). These methods are related to dimensionality reduction techniques such as Principal Components Analysis (PCA), but instead of truncating features in the eigenbasis representation (where each feature is a linear combination of all the original features), the goal is to remove dimensions in the standard basis, leading to a compact set of interpretable features. As long as the discarded features can be well approximated by a (linear) function of the selected features, the loss of information will be minimal. Our first method for optimal data-set creat</context>
</contexts>
<marker>Wolf, Shashua, 2005</marker>
<rawString>L. Wolf and A. Shashua. 2005. Feature selection for unsupervised and supervised inference: The emergence of sparsity in a weight-based approach. The Journal of Machine Learning Research, 6:1855–1887.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhao</author>
<author>H Liu</author>
</authors>
<title>Spectral feature selection for supervised and unsupervised learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the ICML,</booktitle>
<pages>1151--1157</pages>
<contexts>
<context position="11098" citStr="Zhao and Liu, 2007" startWordPosition="1792" endWordPosition="1795">erent samples, and training 10 corresponding learners. Thus, in total, the underlying prediction model is trained 1,900 times. In contrast, our selection methods are fast, can select any number of data points in a single step, and are not tied to a particular prediction task or model. Furthermore, these methods can be combined with active learning in selecting the initial seed set. Unsupervised Feature Selection Finally, we note that CSSP and related spectral methods have been applied to the problem of unsupervised feature selection (Stoppiglia et al., 2003; Mao, 2005; Wolf and Shashua, 2005; Zhao and Liu, 2007; Boutsidis et al., 2008). These methods are related to dimensionality reduction techniques such as Principal Components Analysis (PCA), but instead of truncating features in the eigenbasis representation (where each feature is a linear combination of all the original features), the goal is to remove dimensions in the standard basis, leading to a compact set of interpretable features. As long as the discarded features can be well approximated by a (linear) function of the selected features, the loss of information will be minimal. Our first method for optimal data-set creation applies a random</context>
</contexts>
<marker>Zhao, Liu, 2007</marker>
<rawString>Z. Zhao and H. Liu. 2007. Spectral feature selection for supervised and unsupervised learning. In Proceedings of the ICML, pages 1151–1157.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>