<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000250">
<title confidence="0.998282">
Lexical Selection in the Process of Language Generation
</title>
<author confidence="0.985224">
James Pustejovsky
</author>
<affiliation confidence="0.8057245">
Department of Computer Science
Brandeis University
</affiliation>
<bodyText confidence="0.72547">
Waltham, MA 02254
617-736-2709
jamespabrandeis.csnet-relay
</bodyText>
<sectionHeader confidence="0.981947" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.987801222222222">
In this paper we argue that lexical selection plays a more
important role in the generation process than has com-
monly been assumed. To stress the importance of lexical-
semantic input to generation, we explore the distinction
and treatment of generating open and closed class lexical
items, and suggest an additional classification of the lat-
ter into discourse-oriented and proposition-oriented items.
Finally, we discuss how lexical selection is influenced by
thematic (focus) information in the input.
</bodyText>
<sectionHeader confidence="0.998774" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999160451612904">
There is a consensus among computational linguists
that a comprehensive analyzer for natural language must
have the capability for robust lexical disambiguation,
i.e., its central task is to select appropriate meanings of
lexical items in the input and come up with a non contra-
dictory, unambiguous representation of both the proposi-
tional and the non-propositional meaning of the input text.
The task of a natural language generator is, in some sense,
the opposite task of rendering an unambiguous meaning
in a natural language. The main task here is to to per-
form principled selection of a) lexical items and b) the
syntactic structure for input constituents, based on lexical
semantic, pragmatic and discourse clues available in the
input. In this paper we will discuss the problem of lexical
selection.
The problem of selecting lexical items in the pro-
cess of natural language generation has not received as
much attention as the problems associated with express-
ing explicit grammatical knowledge and control. In most
of the generation systems, lexical selection could not be
a primary concern due to the overwhelming complexity
of the generation problem itself. Thus, MUMBLE con-
centrates on grammar-intensive control decisions (McDon-
ald and Pustejovsky, 1985a) and some stylistic considera-
tions (McDonald and Pustejovsky, 1985b); TEXT (McKe-
own, 1985) stresses the strategical level of control decisions
about the overall textual shape of the generation output.
&apos;KAMP (Appelt, 1985) emphasizes the role that dynamic
planning plays in controlling the process of generation,
and specifically, of referring expressions; NIGEL (Mann
and Matthiessen, 1983) derives its control structures from
</bodyText>
<subsectionHeader confidence="0.817266">
Sergei Nirenburg
</subsectionHeader>
<bodyText confidence="0.983192909090909">
Computer Science Department
Carnegie-Mellon University
Pittsburgh, PA. 15213
412-268-3823
sergeiOcad.cs.cmu.edu
the choice systems of systemic grammar, concentrating on
grammatical knowledge without fully realizing the &apos;deli-
cate&apos; choices between elements of what systemicists call
lexis (e.g., Halliday, 1961). Thus, the survey in Cumming
(1986) deals predominantly with the grammatical aspects
of the lexicon.
We discuss here the problem of lexical selection and
explore the types of control knowledge that are neces-
sary for it. In particular, we propose different control
strategies and epistemological foundations for the selec-
tion of members of a) open-class and b) closed-class lex-
ical items. One of the most important aspects of control
knowledge our generator employs for lexical selection is the
non-propositional information (including knowledge about
focus and discourse cohesion markers). Our generation
system incorporates the discourse and textual knowledge
provided by TEXT as well as the power of MUMBLE&apos;s
grammatical constraints and adds principled lexical selec-
tion (based on a large semantic knowledge base) and a
control structure capitalizing on the inherent flexibility of
distributed architectures. 3 The specific innovations dis-
cussed in this paper are:
Derr and McKeown, 1984 and McKeown. 1985, however,
discuss thematic information, i.e. focus, as a basis for the selec-
tion of anaphoric pronouns. This is a fruitful direction, and we
attempt to extend it for treatment of additional discourse-based
phenomena.
2 Rubinoff (1986) is one attempt at integrating the tex-
tual component of TEXT with the grammar of MUMBLE. This
interesting idea leads to a significant improvement in the perfor-
mance of sentence production. Our approach differs from this
effort in two important repsects. First, in Rubinoff&apos;s system the
output of TEXT serves as the input to MUMBLE, resulting in
a cascaded process. We propose a distributed control where the
separate knowledge sources contribute to the control when they
can, opportunistically. Secondly, we view the generation process
as the product of many more components than the number pro-
posed in current generators. For a detailed discussion of these
see Nirenburg and Pustejovsky, in preparation.
</bodyText>
<page confidence="0.9948">
201
</page>
<listItem confidence="0.739289916666667">
1. We attach importance to the question of what the input
to a generator should be, both as regards its content and
its form; thus, we maintain that discourse and pragmatic
information is absolutely essential in order for the genera- .
tor to be able to handle a large class of lexical phenomena;
we distinguish two sources of knowledge for lexical selec-
tion, one discourse and pragmatics-based, the other lexical
semantic.
2. We argue that lexical selection is not just a side effect of
grammatical decisions but rather acts to flexibly constrain
concurrent and later generation decisions of either lexical
or grammatical type.
</listItem>
<bodyText confidence="0.995396574468085">
For comparison, MUMBLE lexical selections are per-
formed after some grammatical constraints have been used
to determine the surface syntactic structure; this type of
control of the generation process does not seem optimal
or sufficient for all generation tasks, although it may be
appropriate for on-line generation models; ; we argue that
the decision process is greatly enhanced by making lexical
choices early on in the process. Note that the above does
not presuppose that the control structure for generation is
to be like cascaded transducers; in fact, the actual system
that we are building based on these principles, features a
distributed architecture that supports non-rigid decision
making (it follows that the lexical and grammatical deci-
sions are not explicitly ordered with respect to each other).
This architecture is discussed in detail in Nirenburg and
Pustejovsky, in preparation.
3. We introduce an important distinction between open-
class and closed-class lexical items in the way they are rep-
resented as well as the way they are processed by our gen-
erator; our computational, processing-oriented paradigm
has led us to develop a finer classification of the closed-
class items than that traditionally acknowledged in the
psycholinguistic literature; thus, we distinguish between
discourse oriented closed-class (DOCC) items and propo-
sition oriented ones (POCC);
4. We upgrade the importance of knowledge about focus
in the sentence to be generated so that it becomes one of
the prime heuristics for controlling the entire generation
process, including both lexical selection and grammatical
phrasing.
5. We suggest a comprehensive design for the concept lex-
icon component used by the generator, which is perceived
as a combination of a general-purpose semantic knowl-
edge base describing a subject domain (a subworld) and
a generation-specific lexicon (indexed by concepts in this
knowledge base) that consists of a large set of discrimi-
nation nets with semantic and pragmatic tests on their
nodes.
These discrimination nets are distinct from the choo-
sers in NIGEL&apos;s choice systems, where grammatical knowl-
edge is not systematically separated from the lexical se-
mantic knowledge (for a discussion of problems inherent
in this approach see McDonald, Vaughan and Pustejovsky,
1986); the pragmatic nature of some of the tests, as well
as the fine level of detail of knowledge representation is
what distinguishes our approach from previous conceptual
generators, notably PHRED (Jacobs, 1985)).
</bodyText>
<sectionHeader confidence="0.931265" genericHeader="method">
2. Input to Generation
</sectionHeader>
<bodyText confidence="0.9998668">
As in McKeown (1985,1986) the input to the pro-
cess of generation includes information about the discourse
within which the proposition is to be generated. In our sys-
tem the following static knowledge sources constitute the
input to generation:
</bodyText>
<listItem confidence="0.7952702">
1. A representation of the meaning of the text to be gener-
ated, chunked into proposition-size modules, each of which
carries its own set of contextual values; (cf. TRANSLA-
TOR, Nirenburg et al., 1986, 1987);
2. the semantic knowledge base (concept lexicon) that
</listItem>
<bodyText confidence="0.9609672">
contains information about the types of concepts (objects
(mental, physical and perceptual) and processes (states
and actions)) in the subject domain, represented with the
help of the description module (DRL) of the TRANSLA-
TOR knowledge representation language. The organiza-
tional basis for the semantic knowledge base is an empir-
ically derived set of inheritance networks (isa, made-of,
belongs-to, has-as-part, etc.).
3. The specific lexicon for generation, which takes the form
of a set of discrimination nets, whose leaves are marked
with lexical units or lexical gaps and whose non-leaf nodes
contain discrimination criteria that for open-class items are
derived from selectional restrictions, in the sense of Katz
and Fodor (1963) or Chomsky (1965), as modified by the
ideas of preference semantics (Wilks, 1975, 1978). Note
that most closed-class items have a special status in this
generation lexicon: the discrimination nets for them are
indexed not by concepts in the concept lexicon, but rather
by the types of values in certain (mostly, nonpropositional)
slots in input frames;
</bodyText>
<listItem confidence="0.6037154">
4. The history of processing, structured along the lines of
the episodic memory organization suggested by Kolodner
(1984) and including the feedback of the results of actual
lexical choices during the generation of previous sentences
in a text.
</listItem>
<page confidence="0.996146">
202
</page>
<sectionHeader confidence="0.972869" genericHeader="method">
3. Lexical Classes
</sectionHeader>
<bodyText confidence="0.999242619047619">
The distinction between the open- and closed-class
lexical units has proved an important one in psychology
and psycholinguistics. The manner in which retrieval of el-
ements from these two classes operates is taken as evidence
for a particular mental lexicon structure. A recent pro-
posal (Morrow, 1986) goes even further to explain some of
our discourse processing capabilities in terms of the prop-
erties of some closed-class lexical items. It is interesting
that for this end Morrow assumes, quite uncritically, the
standard division between closed- and open-class lexical
categories: &apos;Open-class categories include content words,
such as nouns, verbs and adjectives... Closed-class cate-
gories include function words, such as articles and prepo-
sitions...&apos; (op. cit., p. 423). We do not elaborate on the
definition of the open-class lexical items. We have, how-
ever, found it useful to actually define a particular subset
of closed-class items as being discourse-oriented, distinct
from those closed-claw items whose processing does not
depend on discourse knowledge.
A more complete list of closed-class lexical items
will include the following:
</bodyText>
<listItem confidence="0.995235384615385">
• determiners and demonstratives (a, the, this, that);
• quantifiers (most, every, each, all of);
• pronouns (he, her, its);
• deictic terms and indexicals (here, now, I, there);
• prepositions (on, during, against);
• parentheticals and attitudinals (as a matter of fact,
on the contrary);
• conjunctions, including discontinuous ones (and, be-
cause, neither...nor);
• primary verbs (do, have, be);
• modal verbs (shall, might, ought to);
• wh-words (who, why, how);
• expletives (no, yes, maybe).
</listItem>
<bodyText confidence="0.999494333333333">
We have concluded that the above is not a homoge-
neous list; its members can be characterized on the basis of
what knowledge sources are used to evaluate them in the
generation process. We have established two such distinct
knowledge sources: purely propositional information and
contextual and discourse knowledge. Those closed-class
items that are assigned a denotation only in the context
of an utterance will be termed discourse-oriented closed
class (DOCC) items; this includes determiners, pronouns,
indexicals, and temporal prepositions. Those contributing
to the propositional content of the utterance will be called
proposition-oriented closed-class (POCC) items. These in-
clude modals, locative and function prepositions, and pri-
mary verbs.
According to this classification, the `definiteness
effect* (that is, whether a definite or an intefinite noun
phrase is selected for generation) is distinct from general
quantification, which appears to be decided on the basis
of propositional factors. Note that prepositions no longer
form a natural class of simple closed-class items. For ex-
ample, in (1) the preposition before unites two entities con-
nected through a discourse marker. In (2) the choice of the
preposition on is determined by information contained in
the propositional content of the sentence.
</bodyText>
<listItem confidence="0.996856">
(1) John ate breakfast before leaving for work.
(2) John sat on the bed.
</listItem>
<bodyText confidence="0.999628">
We will now suggest a set of processing heuristics for
the lexical selection of a member from each lexical class.
This classification entails that the lexicon for generation
will contain only open-class lexical items, because the rest
of the lexical items do not have an independent epistemo-
logical status, outside the context of an utterance. The
selection of closed-class items, therefore, comes as a result
of the use of the various control heuristics that guide the
process of generation. In other words, they are incorpo-
rated in the procedural knowledge rather than the static
knowledge.
</bodyText>
<sectionHeader confidence="0.676144" genericHeader="method">
4.0 Lexical Selection
</sectionHeader>
<subsectionHeader confidence="0.999976">
4.1 Selection of Open-Class Items
</subsectionHeader>
<bodyText confidence="0.99983652173913">
A significant problem in lexical selection of open-
class items is how well the concept to be generated matches
the desired lexical output. In other words, the input to
generate in English the concept `son&apos;s wife&apos;s mother&apos; will
find no single lexical item covering the entire expression. In
Russian, however, this meaning is covered by a single word
&apos;Evatja.&apos; This illustrates the general problem of lexical
gaps and bears on the question of how strongly the con-
ceptual representation is influenced by the native tongue of
the knowledge-engineer. The representation must be com-
prehensive yet flexible enough to accommodate this kind
of problem. The processor, on the other hand, must be
constructed so that it can accommodate lexical gaps by
being able to build the most appropriate phrase to insert
in the slot for which no single lexical unit can be selected
(perhaps, along the lines of McDonald and Pustejovsky,
1985a).
To illustrate the knowledge that bears upon the
choice of an open-class lexical item, let us trace the process
of lexical selection of one of the words from the list: desk,
table, dining table, coffee table, utility table. Suppose, dur-
ing a run of our generator we have already generated the
following partial sentence:
</bodyText>
<listItem confidence="0.688068">
(3) John bought a
</listItem>
<figureCaption confidence="0.9291745">
and the pending input is as partially shown in Figures 1-3.
Figure 1 contains the instance of a concept to be generated.
</figureCaption>
<page confidence="0.972093">
203
</page>
<figure confidence="0.982648105263158">
(sto134
(instance-of eta)
(color black)
(size amalt)
(height average)
(mass average)
(nade-of Steel)
(location-of eat))
Figure I
(stol
(isa furniture)
(color black brown yellow white)
(size small average)
(height low average high)
(mass less-than-average average)
(made-of wood plastic steel)
(location-of eat write sew work)
(has-as-part ( leg leg leg (leg) top)
(topology 011 (top leg)))
</figure>
<figureCaption confidence="0.860811">
Figure 2
Figure 2 contains the representation of the corresponding
type in the semantic knowledge base. Figure 3 contains an
excerpt from the English generation lexicon, which is the
discrimination net for the concept in Figure 2.
case location-of of
eat: case height of
</figureCaption>
<figure confidence="0.880033166666667">
lov: coffee table
average: dining table
write: desk
sew: sewing table
saw: workbench
otherwise: table
</figure>
<figureCaption confidence="0.954731">
Figure 3
</figureCaption>
<bodyText confidence="0.9993589">
In order to select the appropriate lexicalization the
generator has to traverse the discrimination net, having
first found the answers to tests on its nodes in the repre-
sentation of the concept token (in Figure 1). In addition,
the latter representation is compared with the represen-
tation of the concept type and if non-default values are
found in some slots, then the result of the generation will
be a noun phrase with the above noun as its head and a
number of adjectival modifiers. Thus, in our example, the
generator will produce &apos;black steel dining table&apos;.
</bodyText>
<subsectionHeader confidence="0.999609">
4.2 Selection of POCC Items
</subsectionHeader>
<bodyText confidence="0.998535333333333">
Now let us discuss the process of generating a propo-
sition oriented lexical item. The example we will use here
is that of the function preposition to. The observation
here is that if to is a POCC item, the information required
for generating it should be contained within the proposi-
tional content of the input representation; no contextual
information should be necessary for the lexical decision.
Assume that we wish to generate sentence (1) where we
are focussing on the selection of to.
</bodyText>
<table confidence="0.8414591">
(1) John walked to the store.
If the input to the generator is
(walk
(Actor John)
(Location &apos;,here&apos;)
(Soares 2)
(Goal store23)
(Time past2)
(intention 0)
(Direction store23))
</table>
<bodyText confidence="0.9938355">
then the only information necessary to generate the prepo-
sition is the case role for the goal, store. Notice that a
change in the lexicalization of this attribute would only
arise with a different input to the generator. Thus, if the
goal were unspecified, we might generate (2) instead of (1);
but here the propositional content is different.
</bodyText>
<listItem confidence="0.619259">
(2) John walked towards the store.
</listItem>
<bodyText confidence="0.997249">
In the complete paper we will discuss the generation of two
other DOCC items; namely, quantifiers and primary verbs,
such as do and have.
</bodyText>
<subsectionHeader confidence="0.9907785">
4.2 Selection of DOCC Items:
Generating a discourse anaphor
</subsectionHeader>
<bodyText confidence="0.999852545454545">
Suppose we wish to generate an anaphoric pronoun
for an NP in a discourse where its antecedent was men-
tioned in a previous sentence. We illustrate this in Figure
2. Unlike open-class items, pronominals are not going to
be directly associated with concepts in the semantic kn-
woledge base. Rather, they are generated as a result of
decisions involving contextual knowledge, the beliefs of the
speaker and hearer, and previous utterances. Suppose, we
have already generated (4) and the next sentence to be
generated also refers to the same individual and informs
us that John was at his father&apos;s for two days.
</bodyText>
<listItem confidence="0.9791575">
(1) John, visited his father.
(2) He, stayed for two days.
</listItem>
<bodyText confidence="0.9728695">
Immediate focus information, in the sense of Gross (1979)
interacts with a history of the previous sentence struc-
tures to determine a strategy for selecting the appropriate
anaphor. Thus, selecting the appropriate pronoun is an
attached procedure. The heuristic for discourse-directed
pronominalization is as follows:
</bodyText>
<page confidence="0.996631">
204
</page>
<bodyText confidence="0.950817928571429">
IF: (1) the input for the generation of a sentence
includes an instance of an object present in a recent
input; and
(2) the the previous instance of this object (the po-
tential antecedent) is in the topic position; and
(3) there are few intervening potential antecedents;
and
(4) there is no focus shift in the space between the
occurrence of the antecedent and the current object
instance
THEN: realize the current instance of that object as a pro-
noun; consult the grammatical knowledge source for
the proper gender, number and case form of the pro-
noun.
In McDonald and Pustejovsky (1985b) a heursitic
was given for deciding when to generate a full NP and
when a pronoun. This decision was fully integrated into
the grammatical decisions made by MUMBLE in terms of
realization-classes, and was no different from the decision
to make a sentence active or passive. Here, we are separat-
ing discourse information from linguistic knowledge. Our
system is closer to McKeown&apos;s (1985, 1986) TEXT system,
where discourse information acts to constrain the control
regimen for linguistic generation. We extend McKeown&apos;s
idea, however, in that we view the process of lexical selec-
tion as a constraining factor in general. In the complete
paper, we illustrate how this works with other discourse
oriented closed-class items.
</bodyText>
<sectionHeader confidence="0.984323" genericHeader="method">
5. The Role of Focus in Lexical Selection
</sectionHeader>
<bodyText confidence="0.999977833333333">
As witnessed in the previous section, focus is an im-
portant factor in the generation of discourse anaphors. In
this section we demonstrate that focus plays an important
role in selecting non-discourse items as well. Suppose your
generator has to describe a financial transaction as a result
of which
</bodyText>
<listItem confidence="0.989043333333333">
(1) Bill is the owner of a car that previously belonged
to John, and
(2) John is richer by $2,000.
</listItem>
<bodyText confidence="0.979541571428571">
Assuming your generator is capable of representing the
grammatical structure of the resulting -English sentence,
it still faces an important decision of how to express lexi-
cally the actual transaction relation. Its choice is to either
use buy or sell as the main predicate, leading to either (1)
or (2), or to use a non-perspective phrasing where neither
verb is used.
</bodyText>
<listItem confidence="0.8641805">
(1) Bill bought a car from John for S2,000.
(2) John sold a car to Bill for $2,000.
</listItem>
<bodyText confidence="0.999561777777778">
We distinguish the following major contributing factors for
selecting one verb over the other; (1) the intended perspec-
tive of the situation, (2) the emphasis of one activity rather
than another, (3) the focus being on a particular individ-
ual, and (4) previous lexicalizations of the concept.
These observations are captured by allowing focus
to operate over several expression including event-types
such as transfer. Thus, the variables at play for focus in-
clude:
</bodyText>
<listItem confidence="0.999181142857143">
• end-of-transfer,
• beginning-of-transfer,
• activity-of-transfer,
• goal-of-object,
• source-of-object,
• goal-of-money,
• source-of-money.
</listItem>
<bodyText confidence="0.999495714285714">
That is, lexicalization depends on which expressions are in
focus. For example, if John is the immediate focus (as in
McKeown (1985)) and beginning-of-transfer is the current-
focus, the generator will lexicalize from the perspective of
the selling, namely (2). Given a different focus configura-
tion in the input to the generator, the selection would be
different and another verb would be generated.
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.996857692307692">
In this paper we have argued that lexical selection
is an important contributing factor to the process of gen-
eration, and not just a side effect of grammatical deci-
sions. Furthermore, we claim that open-class items are
not only conceptually different from closed-class items, but
are processed differently as well. Closed class items have
no epistemological status other than procedural attach-
ments to conceptual and discourse information. Related to
this, we discovered an interesting distinction between two
types of closed-class items, distinguished by the knowledge
sources necessary to generate them; discourse oriented and
proposition-oriented. Finally, we extend the importance of
focus information for directing the generation process.
</bodyText>
<page confidence="0.998071">
205
</page>
<sectionHeader confidence="0.998165" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999790595744681">
[11 Appelt, Douglas Planning English Sentences, Cam-
bridge U. Preys.
[2] Chomsky, Noam Aspects on the Theory of Syntax,
MIT Press.
[3] Cumming, Susanna, &apos;A Guide to Lexical Acquisi-
tion in the JANUS System&apos; ISI Research Report
ISI/RR-85-162, Information Sciences Institute, Ma-
rina del Rey, California, 1986a.
[4] Cumming, Sussana, &apos;The Distribution of Lexical
Information in Text Generation&apos;, presented for Work-
shop on Automating the Lexicon, Pisa, 1986b.
[5] Derr, K. and K. McKeown &apos;Focus in Generation,
COLING 1984
[6] Dowty, David R., Word Meaning and Montague
Grammar, D. Reidel, Dordrecht, Holland, 1979.
[7] Halliday, M.A.K. &apos;Options and functions in the En-
glish clause&apos;. Brno Studies in English 8, 82-88.
[8] Jacobs, Paul S., &amp;quot;PHRED: A Generator for Nat-
ural Language Interface&apos;, Computational Linguis-
tics, Volume 11, Number 4, 1985.
[9] Katz, Jerrold and Jerry A. Fodor, &apos;The Structure of
a Semantic Theory&apos;, Language Vol 39, pp.170-210,
1963.
[10] Mann, William and Matthiessen, °NIGEL: a Sys-
temic Grammar for Text Generation&apos;, in Freddie
(ed.), Systemic Perspectives on Discourse, Ablex.
[11] McDonald, David and James Pustejovsky, &apos;Descrip-
tion directed Natural Language Generation&apos; Pro-
ceedings of IJCAI-85. Kaufmann.
[12] McDonald, David and James Pustejovsky, &apos;A Com- •
putational Theory of Prose Style for Natural Lan-
guage Generation, Proceedings of the European ACL,
University of Geneva, 1985.
1131 McKeown, Kathy Tezt Generation, Cambridge Uni-
versity Press.
[14] McKeown, Kathy, &amp;quot;Stratagies and Constraints for
Generating Natural Language Text*, in Bolc and
McDonald, 1987.
[15] Morrow &apos;The Processing of Closed Class Lexical
Items&apos;, in Cognitive Science 10.4, 1986.
[16] Nirenburg, Sergei, Victor Raskin, and Allen Tucker,
&apos;The Structure of Interlingua in TRANSLATOR&apos;,
in Nirenburg (ed.) Machine Translation: Theoret-
ical and Methodological Issues, Cambridge Univer-
sity Press. 1987.
[17] Wilke, Yorick &apos;Preference Semantics,&apos; Artificial In-
telligence, 1975.
</reference>
<page confidence="0.998894">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.896066">
<title confidence="0.999926">Lexical Selection in the Process of Language Generation</title>
<author confidence="0.999821">James Pustejovsky</author>
<affiliation confidence="0.999752">Department of Computer Science Brandeis University</affiliation>
<address confidence="0.999986">Waltham, MA 02254</address>
<phone confidence="0.980695">617-736-2709</phone>
<email confidence="0.985393">jamespabrandeis.csnet-relay</email>
<abstract confidence="0.9905584">In this paper we argue that lexical selection plays a more important role in the generation process than has commonly been assumed. To stress the importance of lexicalsemantic input to generation, we explore the distinction treatment of generating class items, and suggest an additional classification of the latinto Finally, we discuss how lexical selection is influenced by in the input.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Douglas Planning English Sentences, Cambridge U.</title>
<publisher>Preys.</publisher>
<marker></marker>
<rawString> [11 Appelt, Douglas Planning English Sentences, Cambridge U. Preys.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects on the Theory of Syntax,</title>
<publisher>MIT Press.</publisher>
<marker>[2]</marker>
<rawString>Chomsky, Noam Aspects on the Theory of Syntax, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susanna Cumming</author>
</authors>
<title>A Guide to Lexical Acquisition in the JANUS System&apos;</title>
<date>1986</date>
<tech>ISI Research Report ISI/RR-85-162,</tech>
<institution>Information Sciences Institute, Marina del Rey,</institution>
<location>California,</location>
<marker>[3]</marker>
<rawString>Cumming, Susanna, &apos;A Guide to Lexical Acquisition in the JANUS System&apos; ISI Research Report ISI/RR-85-162, Information Sciences Institute, Marina del Rey, California, 1986a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sussana Cumming</author>
</authors>
<title>The Distribution of Lexical Information in Text Generation&apos;, presented for Workshop on Automating the Lexicon,</title>
<date>1986</date>
<location>Pisa,</location>
<marker>[4]</marker>
<rawString>Cumming, Sussana, &apos;The Distribution of Lexical Information in Text Generation&apos;, presented for Workshop on Automating the Lexicon, Pisa, 1986b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Derr</author>
<author>K McKeown</author>
</authors>
<title>Focus in Generation,</title>
<date>1984</date>
<location>COLING</location>
<marker>[5]</marker>
<rawString>Derr, K. and K. McKeown &apos;Focus in Generation, COLING 1984</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Dowty</author>
<author>Word Meaning</author>
<author>Montague Grammar</author>
<author>D Reidel</author>
</authors>
<date>1979</date>
<location>Dordrecht, Holland,</location>
<marker>[6]</marker>
<rawString>Dowty, David R., Word Meaning and Montague Grammar, D. Reidel, Dordrecht, Holland, 1979.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M A K Halliday</author>
</authors>
<title>Options and functions in the English clause&apos;.</title>
<journal>Brno Studies in English</journal>
<volume>8</volume>
<pages>82--88</pages>
<marker>[7]</marker>
<rawString>Halliday, M.A.K. &apos;Options and functions in the English clause&apos;. Brno Studies in English 8, 82-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul S Jacobs</author>
</authors>
<title>PHRED: A Generator for Natural Language Interface&apos;,</title>
<date>1985</date>
<journal>Computational Linguistics, Volume</journal>
<volume>11</volume>
<marker>[8]</marker>
<rawString>Jacobs, Paul S., &amp;quot;PHRED: A Generator for Natural Language Interface&apos;, Computational Linguistics, Volume 11, Number 4, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold Katz</author>
<author>Jerry A Fodor</author>
</authors>
<title>The Structure of a Semantic Theory&apos;,</title>
<date>1963</date>
<journal>Language Vol</journal>
<volume>39</volume>
<pages>170--210</pages>
<marker>[9]</marker>
<rawString>Katz, Jerrold and Jerry A. Fodor, &apos;The Structure of a Semantic Theory&apos;, Language Vol 39, pp.170-210, 1963.</rawString>
</citation>
<citation valid="false">
<authors>
<author>William Mann</author>
<author>°NIGEL Matthiessen</author>
</authors>
<title>a Systemic Grammar for Text Generation&apos;,</title>
<booktitle>Systemic Perspectives on Discourse, Ablex.</booktitle>
<editor>in Freddie (ed.),</editor>
<marker>[10]</marker>
<rawString>Mann, William and Matthiessen, °NIGEL: a Systemic Grammar for Text Generation&apos;, in Freddie (ed.), Systemic Perspectives on Discourse, Ablex.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David McDonald</author>
<author>James Pustejovsky</author>
</authors>
<title>Description directed Natural Language Generation&apos;</title>
<booktitle>Proceedings of IJCAI-85.</booktitle>
<publisher>Kaufmann.</publisher>
<marker>[11]</marker>
<rawString>McDonald, David and James Pustejovsky, &apos;Description directed Natural Language Generation&apos; Proceedings of IJCAI-85. Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McDonald</author>
<author>James Pustejovsky</author>
</authors>
<title>A Com- • putational Theory of Prose Style for Natural Language Generation,</title>
<date>1985</date>
<booktitle>Proceedings of the European ACL,</booktitle>
<pages>1131</pages>
<publisher>University Press.</publisher>
<institution>University of Geneva,</institution>
<marker>[12]</marker>
<rawString>McDonald, David and James Pustejovsky, &apos;A Com- • putational Theory of Prose Style for Natural Language Generation, Proceedings of the European ACL, University of Geneva, 1985. 1131 McKeown, Kathy Tezt Generation, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathy McKeown</author>
</authors>
<title>Stratagies and Constraints for Generating Natural Language Text*,</title>
<date>1987</date>
<booktitle>in Bolc and McDonald,</booktitle>
<marker>[14]</marker>
<rawString>McKeown, Kathy, &amp;quot;Stratagies and Constraints for Generating Natural Language Text*, in Bolc and McDonald, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morrow</author>
</authors>
<title>The Processing of Closed Class Lexical Items&apos;,</title>
<date>1986</date>
<journal>in Cognitive Science</journal>
<volume>10</volume>
<marker>[15]</marker>
<rawString>Morrow &apos;The Processing of Closed Class Lexical Items&apos;, in Cognitive Science 10.4, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergei Nirenburg</author>
<author>Victor Raskin</author>
<author>Allen Tucker</author>
</authors>
<title>The Structure of Interlingua in TRANSLATOR&apos;,</title>
<date>1987</date>
<booktitle>in Nirenburg (ed.) Machine Translation: Theoretical and Methodological Issues,</booktitle>
<publisher>University Press.</publisher>
<location>Cambridge</location>
<marker>[16]</marker>
<rawString>Nirenburg, Sergei, Victor Raskin, and Allen Tucker, &apos;The Structure of Interlingua in TRANSLATOR&apos;, in Nirenburg (ed.) Machine Translation: Theoretical and Methodological Issues, Cambridge University Press. 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilke</author>
</authors>
<title>Yorick &apos;Preference Semantics,&apos;</title>
<date>1975</date>
<journal>Artificial Intelligence,</journal>
<marker>[17]</marker>
<rawString>Wilke, Yorick &apos;Preference Semantics,&apos; Artificial Intelligence, 1975.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>