<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000448">
<title confidence="0.977073">
Fast Mapping in Word Learning: What Probabilities Tell Us
</title>
<author confidence="0.958838">
Afra Alishahi and Afsaneh Fazly and Suzanne Stevenson
</author>
<affiliation confidence="0.9991765">
Department of Computer Science
University of Toronto
</affiliation>
<email confidence="0.99754">
{afra,afsaneh,suzanne}@cs.toronto.edu
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997608">
Children can determine the meaning of a
new word from hearing it used in a familiar
context—an ability often referred to as fast
mapping. In this paper, we study fast map-
ping in the context of a general probabilistic
model of word learning. We use our model
to simulate fast mapping experiments on chil-
dren, such as referent selection and retention.
The word learning model can perform these
tasks through an inductive interpretation of
the acquired probabilities. Our results suggest
that fast mapping occurs as a natural conse-
quence of learning more words, and provides
explanations for the (occasionally contradic-
tory) child experimental data.
</bodyText>
<sectionHeader confidence="0.98103" genericHeader="method">
1 Fast Mapping
</sectionHeader>
<bodyText confidence="0.999915722222222">
An average six-year-old child knows over 14, 000
words, most of which s/he has learned from hearing
other people use them in ambiguous contexts (Carey,
1978). Children are thus assumed to be equipped with
powerful mechanisms for performing such a complex
task so efficiently. One interesting ability children as
young as two years of age show is that of correctly and
immediately mapping a novel word to a novel object
in the presence of other familiar objects. The term
“fast mapping” was first used by Carey and Bartlett
(1978) to refer to this phenomenon.
Carey and Bartlett’s goal was to examine how much
children learn about a word when presented in an am-
biguous context, as opposed to concentrated teaching.
They used an unfamiliar name (chromium) to refer to
an unfamiliar color (olive green), and then asked
a group of four-year-old children to select an object
from among a set, upon hearing a sentence explicitly
</bodyText>
<note confidence="0.717732">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.945638333333333">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
</footnote>
<bodyText confidence="0.9994698">
asking for the object of the new color, as in: bring
the chromium tray, not the blue one. Children were
generally good at performing this “referent selection”
task. In a production task performed six weeks later,
when children had to use the name of the new color,
they showed signs of having learned something about
the new color name, but were not successful at pro-
ducing it. On the basis of these findings, Carey and
Bartlett suggest that fast mapping and word learning
are two distinct, yet related, processes.
Extending Carey and Bartlett’s work, much re-
search has concentrated on providing an explanation
for fast mapping, and on examining its role in word
learning. These studies also show that children are
generally good at referent selection, given a novel tar-
get. However, there is not consistent evidence regard-
ing whether children actually learn the novel word
from one or a few such exposures (retention). For
example, whereas the children in the experiments of
Golinkoff et al. (1992) and Halberda (2006) showed
signs of nearly-perfect retention of the fast-mapped
words, those in the studies reported by Horst and
Samuelson (2008) did not (all participating children
were close in age range).
There are also many speculations about the possible
causes of fast mapping. Some researchers consider
it as a sign of a specialized (innate) mechanism for
word learning. Markman and Wachtel (1988), for ex-
ample, argue that children fast map because they ex-
pect each object to have only one name (mutual exclu-
sivity). Golinkoff et al. (1992) attribute fast mapping
to a (hard-coded) bias towards mapping novel names
to nameless object categories. Some even suggest a
change in children’s learning mechanisms, at around
the time they start to show evidence of fast mapping
(which coincides with a sudden burst in their vocab-
ulary), e.g., from associative to referential (Gopnik
and Meltzoff, 1987; Reznick and Goldfield, 1992). In
contrast, others see fast mapping as a phenomenon
that arises from more general processes of learning
</bodyText>
<page confidence="0.986829">
57
</page>
<note confidence="0.881882">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 57–64
Manchester, August 2008
</note>
<bodyText confidence="0.998604333333334">
and/or communication, which also underlie the im-
pressive rate of lexical acquisition in children (e.g.,
Clark, 1990; Diesendruck and Markson, 2001; Regier,
2005; Horst et al., 2006; Halberda, 2006).
In our previous work (Fazly et al., 2008), we pre-
sented a word learning model which proposes a prob-
abilistic interpretation of cross-situational learning,
and bootstraps its own partially-learned knowledge of
the word meanings to accelerate word learning over
time. We have shown that the model can learn reason-
able word–meaning associations from child-directed
data, and that it accounts for observed learning pat-
terns in children, such as vocabulary spurt, without
requiring a developmental change in the underlying
learning mechanism. Here, we use this computational
model to investigate fast mapping and its relation to
word learning. Specifically, we take a close look at
the onset of fast mapping in our model by simulat-
ing some of the psychological experiments mentioned
above. We examine the behaviour of the model in var-
ious referent selection and retention tasks, and pro-
vide explanations for the (occasionally contradictory)
experimental results reported in the literature. We also
study the effect of exposure to more input on the per-
formance of the model in fast mapping.
Our results suggest that fast mapping can be ex-
plained as an induction process over the acquired as-
sociations between words and meanings. Our model
learns these associations in the form of probabilities
within a unified framework; however, we argue that
different interpretations of such probabilities may be
involved in choosing the referent of a familiar as op-
posed to a novel target word (as noted by Halberda,
2006). Moreover, the overall behaviour of our model
confirms that the probabilistic bootstrapping approach
to word learning naturally leads to the onset of fast
mapping in the course of lexical development, with-
out hard-coding any specialized learning mechanism
into the model to account for this phenomenon.
</bodyText>
<sectionHeader confidence="0.606222" genericHeader="method">
2 Overview of the Computational Model
</sectionHeader>
<bodyText confidence="0.999949">
This section summarizes the model presented in Fa-
zly et al. (2008). Our word learning algorithm is an
adaptation of the IBM translation model proposed by
Brown et al. (1993). However, our model is incre-
mental, and does not require a batch process over the
entire data.
</bodyText>
<subsectionHeader confidence="0.995215">
2.1 Utterance and Meaning Representations
</subsectionHeader>
<bodyText confidence="0.97719968">
The input to our word learning model consists of a set
of utterance–scene pairs that link an observed scene
(what the child perceives) to the utterance that de-
scribes it (what the child hears). We represent each
utterance as a sequence of words, and the correspond-
ing scene as a set of meaning symbols. To simulate
referential uncertainty (i.e., the case where the child
perceives aspects of the scene that are unrelated to the
perceived utterance), we include additional symbols
in the representation of the scene, e.g.:
Utterance: Joe rolled the ball
Scene: {joe, roll, the, ball, mommy, hand, talk}
In Section 3.1, we explain how the utterances and
the corresponding semantic symbols are selected, and
how we add referential uncertainty.
Given a corpus of such utterance–scene pairs, our
model learns the meaning of each word w as a prob-
ability distribution, p(.|w), over the semantic sym-
bols appearing in the corpus. In this representation,
p(m|w) is the probability of a symbol m being the
meaning of a word w. In the absence of any prior
knowledge, all symbols are equally likely to be the
meaning of a word. Hence, prior to receiving any us-
ages of a given word, the model assumes a uniform
distribution over semantic symbols as its meaning.
</bodyText>
<subsectionHeader confidence="0.999003">
2.2 Meaning Probabilities
</subsectionHeader>
<bodyText confidence="0.968224357142857">
Our model combines probabilistic interpretations of
cross-situational learning (Quine, 1960) and of a
variation of the principle of contrast (Clark, 1990),
through an interaction between two types of prob-
abilistic knowledge acquired and refined over time.
Given an utterance–scene pair received at time t, i.e.,
(U(t), S(t)), the model first calculates an alignment
probability a for each w E U(t) and each m E S(t),
using the meaning probabilities p(.|w) of all the
words in the utterance prior to this time. The model
then revises the meaning of the words in U(t) by in-
corporating the alignment probabilities for the current
input pair. This process is repeated for all the input
pairs, one at a time.
Step 1: Calculating the alignment probabilities.
We estimate the alignment probabilities of words
and meaning symbols based on a localized version
of the principle of contrast: that a meaning sym-
bol in a scene is likely to be highly associated with
only one of the words in the corresponding utter-
ance.1 For a symbol m E S(t) and a word w E U(t),
the higher the probability of m being the meaning
of w (according to p(m|w)), the more likely it is
that m is aligned with w in the current input. In
other words, a(w|m, U(t), S(t)) is proportional to
p(t−1)(m|w). In addition, if there is strong evidence
that m is the meaning of another word in U(t)—
i.e., if p(t−1)(m|w′) is high for some w′ E U(t) other
</bodyText>
<footnote confidence="0.997047333333333">
1Note that this differs from what is widely known as the prin-
ciple of contrast (Clark, 1990), in that the latter assumes contrast
across the entire vocabulary rather than within an utterance.
</footnote>
<page confidence="0.99935">
58
</page>
<bodyText confidence="0.98048">
than w—the likelihood of aligning m to w should de-
crease. Combining these two requirements:
</bodyText>
<equation confidence="0.999043666666667">
a(w|m, U(t), S(t)) = p(t−1)(m|w) (1)
E p(t−1)(m|w′)
w′∈U(t)
</equation>
<bodyText confidence="0.9998444375">
Due to referential uncertainty, some of the meaning
symbols in the scene might not have a counterpart
in the utterance. To accommodate for such cases, a
dummy word is added to each utterance before the
alignment probabilities are calculated, in order to let
a meaning symbol not be (strongly) aligned with any
of the words in the current utterance.
Step 2: Updating the word meanings. We need to
update the probabilities p(.|w) for all words w E U(t),
based on the evidence from the current input pair re-
flected in the alignment probabilities. We thus add
the current alignment probabilities for w and the sym-
bols m E S(t) to the accumulated evidence from prior
co-occurrences of w and m. We summarize this
cross-situational evidence in the form of an associa-
tion score, which is updated incrementally:
</bodyText>
<equation confidence="0.974533">
assoc(t)(w, m) = assoc(t−1)(w, m) +
a(w|m, U(t), S(t)) (2)
</equation>
<bodyText confidence="0.999937666666667">
where assoc(t−1)(w, m) is zero if w and m have not
co-occurred before. The association score of a word
and a symbol is basically a weighted sum of their co-
occurrence counts.
The model then uses these association scores to up-
date the meaning of the words in the current input:
</bodyText>
<equation confidence="0.9911775">
E (3)
Mj ∈M
</equation>
<bodyText confidence="0.996885304347826">
where M is the set of all symbols encountered prior to
or at time t, 0 is the expected number of symbol types,
and A is a small smoothing factor. The denominator is
a normalization factor to get valid probabilities. This
formulation results in a uniform probability of 1/0
over all m E M for a novel word w, and a probability
smaller than A for a meaning symbol m that has not
been previously seen with a familiar word w.
Our model updates the meaning of a word ev-
ery time it is heard in an utterance. The strength
of learning of a word at time t is reflected in
p(t)(m = mw|w), where mw is the “correct” mean-
ing of w: for a learned word w, the probability dis-
tribution p(.|w) is highly skewed towards the correct
meaning mw, and therefore hearing w will trigger the
retrieval of the meaning mw.2
2An input-generation lexicon contains the correct meaning for
each word, as described in Section 3.1. Note that the model does
not have access to this lexicon for learning; it is used only for
input generation and evaluation.
From this point on, we simply use p(m|w) (omit-
ting the superscript (t)) to refer to the meaning prob-
ability of m for w at the present time of learning.
</bodyText>
<subsectionHeader confidence="0.997648">
2.3 Referent Probabilities
</subsectionHeader>
<bodyText confidence="0.9997144">
The meaning probability p(m|w) is used to retrieve
the most probable meaning for w among all the possi-
ble meaning symbols m. However, in the referent se-
lection tasks performed by children, the subject is of-
ten forced to select the referent of a target word from
among a limited set of objects, even when the mean-
ing of the target word has not been accurately learned
yet. For our model to perform such tasks, it has to de-
cide how likely it is for a target word w to refer to a
particular object m, based on its previous knowledge
about the mapping between m and w (i.e., p(m|w)),
as well as the mapping between m and other words in
the lexicon.3
The likelihood of using a particular name w to refer
to a given object m is calculated as:
</bodyText>
<equation confidence="0.973844">
rf (w|m) = p(w|m)
p(m|w) - p(w)
= p(m)
p(m|w) - p(w) =(4)
Ew′∈V p(m |w′) - p(w′)
</equation>
<bodyText confidence="0.995256">
where V is the set of all words that the model has seen
so far, and p(w) is the relative frequency of w:
</bodyText>
<equation confidence="0.9229605">
= freq(w) (5)
p(w ) Ew′∈V freq(w′)
</equation>
<bodyText confidence="0.999954333333333">
The referent of a target word w among the present ob-
jects, therefore, will be the object m with the highest
referent probability rf (w|m).
</bodyText>
<sectionHeader confidence="0.996402" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.994561">
3.1 The Input Corpora
</subsectionHeader>
<bodyText confidence="0.999986333333333">
We extract utterances from the Manchester corpus
(Theakston et al., 2001) in the CHILDES database
(MacWhinney, 2000). This corpus contains tran-
scripts of conversations with children between the
ages of 1; 8 and 3; 0 (years;months). We use the
mother’s speech from transcripts of 6 children, re-
move punctuation and lemmatize the words, and con-
catenate the corresponding sessions as input data.
There is no semantic representation of the corre-
sponding scenes available from CHILDES. There-
fore, we automatically construct a scene representa-
tion for each utterance, as a set containing the seman-
tic referents of the words in that utterance. We get
these from an input-generation lexicon that contains
a symbol associated with each word as its semantic
</bodyText>
<footnote confidence="0.55078">
3All through the paper, we use m as both the meaning and the
referent of a word w.
</footnote>
<equation confidence="0.999798333333333">
assoc(t)(m, w) + A
p(t)(m|w) =
assoc(t)(mj, w) + 0 x A
</equation>
<page confidence="0.984125">
59
</page>
<bodyText confidence="0.999980153846154">
referent. We use every other sentence from the orig-
inal corpus, preserving their chronological order. To
simulate referential uncertainty in the input, we then
pair each sentence with its own scene representation
as well as that of the following sentence in the origi-
nal corpus. (Note that the latter sentence is not used
as an utterance in our input.) The extra semantic sym-
bols that are added to each utterance thus correspond
to meaningful semantic representations, as opposed
to randomly selected symbols. In the resulting corpus
of 92, 239 input pairs, each utterance is, on average,
paired with 78% extra meaning symbols, reflecting a
high degree of referential uncertianty.
</bodyText>
<subsectionHeader confidence="0.99626">
3.2 The Model Parameters
</subsectionHeader>
<bodyText confidence="0.999966071428571">
We set the parameters of our learning algorithm using
a development data set which is similar to our training
and test data, but is selected from a non-overlapping
portion of the Manchester corpus. The expected num-
ber of symbols, 0 in Eq. (3), is set to 8500 based on
the total number of distinct symbols extracted for the
development data. Therefore, the default probability
of a symbol for a novel word will be 1/8500. A famil-
iar word, on the other hand, has been seen with some
symbols before. Therefore, the probability of a previ-
ously unseen symbol for it (which, based on Eq. (3),
has an upper bound of A) must be less than the default
probability mentioned above. Accordingly, we set A
to 10−5.
</bodyText>
<subsectionHeader confidence="0.998171">
3.3 The Training Procedure
</subsectionHeader>
<bodyText confidence="0.999997">
In the next section, we report results from the com-
putational simulation of our model for a number of
experiments. All of the simulations use the same pa-
rameter settings (as described in the previous section),
but different input: in each simulation, a random por-
tion of 1000 utterance–scene pairs is selected from
the input corpus, and incrementally processed by the
model. The size of the training corpus is chosen arbi-
trarily to reflect a sample point in learning, and further
experiments have shown that increasing this number
does not change the pattern observed in the results. In
order to avoid behaviour that is specific to a particu-
lar sequence of input items, the reported results in the
next section are averaged over 10 simulations.
</bodyText>
<sectionHeader confidence="0.980416" genericHeader="method">
4 Experimental Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.974571">
4.1 Referent Selection
</subsectionHeader>
<bodyText confidence="0.999990237288136">
In a typical word learning scenario, the child faces
a scene where a number of familiar and unfamiliar
objects are present. The child then hears a sentence,
which describes (some part of) the scene, and is com-
posed of familiar and novel words (e.g., hearing Joe is
eating a cheem, where cheem is a previously unseen
fruit). In such a setting, our model aligns the objects
in the scene with the words in the utterance based on
its acquired knowledge of word meanings, and then
updates the meanings of the words accordingly. The
model can align a familiar word with its referent with
high confidence, since the previously learned mean-
ing probability of the familiar object given the famil-
iar word, or p(mIw), is much higher than the meaning
probability of the same object given any other word in
the sentence. In a similar fashion, the model can eas-
ily align a novel word in the sentence with a novel
object in the scene, because the meaning probability
of the novel object given the novel word (1/0, ac-
cording to Eq. (3)) is higher than the meaning proba-
bility of that object for any previously heard word in
the sentence (the latter probability is smaller than A in
Eq. (3), as explained in Section 3.2).
Earlier fast mapping experiments on children as-
sumed that it is such a contrast between the familiar
and novel words in the same sentence that helps chil-
dren select the correct target object in a referent selec-
tion task. For example, in Carey and Bartlett’s (1978)
experiment, to introduce a novel word–meaning as-
sociation (e.g., chromium–olive), the authors use
both the familiar and the novel words in one sentence
(bring me the chromium tray, not the blue one.). How-
ever, further experiments show that children can suc-
cessfully select the correct referent even if such a con-
trast is not present in the sentence. Many researchers
have performed experiments where young subjects
are forced to choose between a novel and a familiar
object upon hearing a request, such as give me the
ball (familiar target), or give me the dax (novel tar-
get). In all of the reported experimental results, chil-
dren can readily pick the correct referent for a famil-
iar or a novel target word in such a setting (Golinkoff
et al., 1992; Halberda and Goldman, 2008; Halberda,
2006; Horst and Samuelson, 2008).
However, Halberda’s eye-tracking experiments on
both adults and pre-schoolers suggest that the pro-
cesses involved for referent selection in the familiar
target situation may be different from those in the
novel target situation. In the latter situation, subjects
appear to systematically reject the familiar object as
the referent of the novel name before mapping the
novel object to the novel name. In the familiar target
situation, however, there is no need to reject the novel
distractor object, because the subject already knows
the referent of the target.
The difference between these two conditions can be
explained in terms of the meaning and referent proba-
bilities of our model explained in Section 2. In a typi-
cal referent selection experiment, the child is asked to
</bodyText>
<page confidence="0.992563">
60
</page>
<bodyText confidence="0.990695153846154">
“get the ball” while facing a ball and a novel object
(dax). We assume that the child knows the meaning
of verbs and determiners such as get and the, therefore
we simplify the familiar target condition in the form
of the following input item:
ball (FAMILIAR TARGET)
{ball, dax}
A familiar word such as ball has a meaning prob-
ability highly skewed towards its correct meaning.
That is, upon hearing ball, the model can confidently
retrieve its meaning ball, which is the one with
the highest probability p(m|ball) among all possible
meanings m. In such a case, if ball is present in the
scene, the model can easily pick it as the referent of
the familiar target name, without processing the other
objects in the scene.
Now consider the condition where a novel target
name is used in the presence of a familiar and a pre-
viously unseen object:
dax (NOVEL TARGET)
{ball, dax}
Since this is the first time the model has heard the
word dax, both meanings ball and dax are equally
likely because p(.|dax) is uniform. Thus the mean-
ing probabilities cannot be solely used for selecting
the referent of dax, and the model has to perform
some kind of induction on the potential referents in
the scene based on what it has learned about each
of them. The model can infer the referent of dax
by comparing the referent probabilities rf (dax |ball)
and rf (dax |dax) from Eq. (4) after processing the in-
put item. Since ball has strong associations with an-
other word ball, its referent probability for the novel
name dax is much lower than the referent probability
of dax, which does not have strong associations with
any of the words in the learned lexicon.
We simulate the process of referent selection in our
model as follows. We train the model as described
in Section 3.3. We then present the model with one
more input item, which represents either the FAMIL-
IAR TARGET or the NOVEL TARGET condition. For
each condition, we compare the meaning probability
p(object|target) for both familiar and novel objects
in the scene (see Table 1, top panel). In the FA-
MILIAR TARGET condition, the model demonstrates
a strong preference towards choosing the familiar ob-
ject as the referent, whereas in the NOVEL TARGET
condition, the model shows no preference towards any
of the objects based on the meaning probabilities of
the target word. Therefore, for the NOVEL TARGET
condition, we also compare the referent probabilities
rf (target|object) for both objects after processing
</bodyText>
<tableCaption confidence="0.9871425">
Table 1: Referent selection in FAMILIAR and NOVEL
TARGET conditions.
</tableCaption>
<table confidence="0.985572857142857">
UPON HEARING THE TARGET WORD
Condition p(ball|target) p(dax|target)
FAMILIAR TARGET 0.843 ±0.056 K 0.0001
NOVEL TARGET 0.0001 ±0.00 0.0001 ±0.00
AFTER PERFORMING INDUCTION
Condition rf (target|ball) rf (target|dax)
NOVEL TARGET 0.127 ±0.127 0.993 ±0.002
</table>
<bodyText confidence="0.99936175">
the input item as a training pair, simulating the in-
duction process that humans go through to select the
referent in such cases. This time, the model shows a
strong preference towards the novel object as the ref-
erent of the target word (see Table 1, bottom panel).
Our results confirm that in both conditions, the model
consistently selects the correct referent for the target
word across all the simulations.
</bodyText>
<subsectionHeader confidence="0.917608">
4.2 Retention
</subsectionHeader>
<bodyText confidence="0.9559345">
As discussed in the previous section, results from
the human experiments as well as our computational
simulations show that the referent of a novel target
word can be selected based on the previous knowl-
edge about the present objects and their names. How-
ever, the success of a subject in a referent selection
task does not necessarily mean that the child/model
has learned the meaning of the novel word based on
that one trial. In order to better understand what and
how much children learn about a novel word from a
single ambiguous exposure, some studies have per-
formed retention trials after the referent selection ex-
periments. Often, various referent selection trials are
performed in one session, where in each trial a novel
object–name pair is introduced among familiar ob-
jects. Some of the recently introduced objects are
then put together in one last trial, and the subjects
are asked to choose the correct referent for one of the
(recently heard) novel target words. The majority of
the reported experiments show that children can suc-
cessfully perform the retention task (Golinkoff et al.,
1992; Halberda and Goldman, 2008; Halberda, 2006).
We simulate a similar retention experiment by
training the model as usual. We further present the
model with two experimental training items similar to
the one used in the NOVEL TARGET condition in the
previous section, with different familiar and novel ob-
jects and words in each input:
dax (REFERENT SELECTION TRIAL 1)
{ball, dax}
cheem (REFERENT SELECTION TRIAL 2)
{pen,cheem}
</bodyText>
<page confidence="0.999485">
61
</page>
<tableCaption confidence="0.822583">
Table 2: Retention of a novel target word from a set
of novel objects.
</tableCaption>
<table confidence="0.986967333333333">
2-OBJECT RETENTION TRIAL
rf (dax|dax) rf (dax|cheem)
0.996 ±0.001 0.501 ±0.068
3-OBJECT RETENTION TRIAL
rf (dax|dax) rf (dax|cheem) rf (dax|lukk)
0.995 ±0.001 0.407 ±0.062 0.990 ±0.001
</table>
<bodyText confidence="0.969607837209303">
The training session is followed by a retention trial,
where the two novel objects used in the previous ex-
perimental inputs are paired with one of the novel tar-
get words:
dax (2-OBJECT RETENTION TRIAL)
{cheem,dax}
After processing the retention input, we com-
pare the referent probabilities rf (dax |cheem) and
rf (dax |dax) to see if the model can choose the cor-
rect novel object in response to the target word dax.
The top panel in Table 2 summarizes the results of this
experiment. The model consistently shows a strong
preference towards the correct novel object as the ref-
erent of the novel target word across all simulations.
Unlike studies on referent selection, experimental
results for retention have not been consistent across
various studies. Horst and Samuelson (2008) per-
form experiments with two-year-old children involv-
ing both referent selection and retention, and report
that their subjects perform very poorly at the retention
task. One factor that discriminates the experimental
setup of Horst and Samuelson from others (e.g., Hal-
berda, 2006) is that, in their retention trials, they put
together two recently observed novel objects with a
third novel object that has not been seen in any of the
experimental sessions before. The authors do not at-
tribute their contradictory results to the presence of
this third object, but this factor can in fact affect the
performance considerably. We simulate this condition
by using the same input items for referent selection
trials as in the previous simulation, but we replace the
retention trial with the following:
dax (3-OBJECT RETENTION TRIAL)
{cheem,dax,lukk}
The third object, lukk, has not been seen by the
model before. Results under the new condition are re-
ported in the bottom panel of Table 2. As can be seen,
the model shows a strong tendency towards the cor-
rect novel referent dax for the novel target dax, com-
pared to the other recently seen novel object cheem.
However, the probability of the unseen object lukk
is also very high for the target word dax. That is be-
cause the model cannot use any previously acquired
</bodyText>
<figure confidence="0.988047">
0
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
time of first exposure x 104
</figure>
<figureCaption confidence="0.9621615">
Figure 1: Number of usages needed to learn a word,
as a function of the word’s age of exposure.
</figureCaption>
<bodyText confidence="0.9998462">
knowledge about lukk (i.e., associating it with an-
other word) to rule it out as a referent for dax. These
results show that introducing a new object for the first
time in a retention trial considerably increases the dif-
ficulty of the task. This can explain the contradictory
results reported in the literature: when the referent
probabilities are not informative, other factors may
influence the outcome of the experiment, such as the
amount of training received for a novel word–object,
or a possible delay between training and test sessions.
</bodyText>
<subsectionHeader confidence="0.998537">
4.3 The Effect of Exposure to More Input
</subsectionHeader>
<bodyText confidence="0.999987043478261">
The fast mapping ability observed in children implies
that once children have learned a repository of words,
they can easily link novel words to novel objects in a
familiar context based only on a few exposures. We
examine this effect in our model: we train the model
on 20, 000 input pairs, looking at the relation between
the time of first exposure to a word, and the number
of usages that the model needs for learning that word.
Figure 1 plots this for words that have been learned at
some point during the training.4 We can see that the
model shows clear fast mapping behaviour—that is,
words received later in time, on average, require fewer
usages to be learned. These results show that our
model exhibits fast mapping patterns once it has been
exposed to enough word usages, and that no change
in the underlying learning mechanism is needed.5
The effect of exposure to more input on fast map-
ping can be described in terms of context familiarity:
the more input the model has processed so far, the
more likely it is that the context of the usage of a novel
word (the other words in the sentence and the objects
in the scene) is familiar to the model. This pattern
has been studied through a number of experiments on
</bodyText>
<footnote confidence="0.997157666666667">
4We consider a word w as learned if the meaning probability
p(m.|w) is higher than a certain threshold 0. For this experi-
ment, we set 0 = 0.70.
5In Fazly et al. (2008), we reported a variation of this exper-
iment, where we used a smaller training set, and also a different
semantic representation for word meanings.
</footnote>
<page confidence="0.590557">
35
</page>
<figure confidence="0.993579">
number of usages needed to learn
30
25
20
15
10
5
</figure>
<page confidence="0.995537">
62
</page>
<bodyText confidence="0.999778192307692">
children. For example, Gershkoff-Stowe and Hahn
(2007) taught 16- to 18-month-olds the names of 24
unfamiliar objects over 12 training sessions, where
unfamiliar objects were presented with varying fre-
quency. Data were compared to a control group of
children who were exposed to the same experimen-
tal words at the first and last sessions only. Their re-
sults show that for children in the experimental group,
extended practice with a novel set of words led to
the rapid acquisition of a second set of low-practice
words. Children in the control group did not show the
same lexical advantage.
Inspired by Gershkoff-Stowe and Hahn (2007), we
perform an experiment to study the effect of con-
text familiarity on fast mapping in our model. We
choose two sets of words, CONTEXT (containing 20
words) and TARGET (containing 10 words), to con-
duct a referent selection task as follows. First, we
train our model on a sequence of utterance–scene
pairs constructed from the set CONTEXT U TARGET,
as follows: the unified set is randomly shuffled and
divided into two subsets, words in each subset are
put together to form an utterance, and the meanings
of the words in that utterance are put together to
form the corresponding scene. We repeat this process
twice, so that each word appears in exactly two input
pairs. We train our model on the constructed pairs.6
Next, we perform a referent selection task on each
word in the TARGET set: we pair each target word
w with the meaning of 10 randomly selected words
from CONTEXT U TARGET, including the meaning of
the target word itself (mw), and have the model pro-
cess this test pair. We compare the referent probabil-
ity of w and each m E CONTEXT U TARGET to see
whether the model can correctly map the target word
to its referent. We call this setting the LOW TRAIN-
ING condition.
In the above setting, the context words in the ref-
erent selection trials are as new to the model as the
target words. We thus repeat this experiment with
a familiar context: we first train the model over in-
put pairs that are randomly constructed from words
in CONTEXT only, using the same training proce-
dure as described above. This context-familiarization
process is followed by a similar training session on
CONTEXT U TARGET, and a test session on target
words, similar to the previous condition. Again, we
count the number of correct mappings between a tar-
get word and its referent based on the referent proba-
bilities. We call this setting the HIGH TRAINING con-
dition. Table 3 shows the results for both conditions.
It can be seen that the accuracy of finding the referent
</bodyText>
<footnote confidence="0.944264">
6Unlike in previous experiments, here we do not use child-
directed data as we want to control the familiarity of the context.
</footnote>
<tableCaption confidence="0.989186">
Table 3: Average number of correct mappings and the
referent probabilities of target words for two condi-
tions, LOW and HIGH TRAINING.
</tableCaption>
<table confidence="0.997855">
Condition Correct mappings P(target|mtarget)
LOW TRAINING %54 0.216±0.04
HIGH TRAINING %90 0.494±0.79
</table>
<bodyText confidence="0.999413">
for a target word, as well as the referent probability of
a target word for its correct meaning, increase as a re-
sult of more training on the context. In other words, a
more familiar context helps the model perform better
in a fast mapping task.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="method">
5 Related Computational Models
</sectionHeader>
<bodyText confidence="0.999990714285714">
The rule-based model of Siskind (1996), and the con-
nectionist model proposed by Regier (2005), both
show that learning gets easier as the model is exposed
to more input—that is, words heard later are learned
faster. These findings confirm that fast mapping may
simply be a result of learning more words, and that
no explicit change in the underlying learning mech-
anism is needed. However, these studies do not ex-
amine various aspects of fast mapping, such as ref-
erent selection and retention. Horst et al. (2006) ex-
plicitly test fast mapping in their connectionist model
of word learning by performing referent selection and
retention tasks. The behaviour of their model matches
the child experimental data reported in a study by the
same authors (Horst and Samuelson, 2008), but not
that of the contradictory findings of other similar ex-
periments. Moreover, the model’s learning capacity
is limited, and the fast mapping experiments are per-
formed on a very small vocabulary. Frank et al. (2007)
examine fast mapping in their Bayesian model by test-
ing its performance in a novel target referent selection
task. However, the experiment is performed on an ar-
tifical corpus. Moreover, since the learning algorithm
is non-incremental, the success of the model in refer-
ent selection is determined implicitly: each possible
word–meaning mapping from the test input is added
to the current lexicon, and the consistency of the new
lexicon is checked against the training corpus.
</bodyText>
<sectionHeader confidence="0.998972" genericHeader="conclusions">
6 Discussion and Concluding Remarks
</sectionHeader>
<bodyText confidence="0.9999845">
We have used a general computational model of word
learning (first introduced in Fazly et al., 2008) to study
fast mapping. Our model learns a probabilistic asso-
ciation between a word and its meaning, from expo-
sure to word usages in naturalistic contexts. We have
shown that these probabilities can be used to simu-
late various fast mapping experiments performed on
children, such as referent selection and retention. Our
</bodyText>
<page confidence="0.998155">
63
</page>
<bodyText confidence="0.999989685714286">
experimental results suggest that fast mapping can be
explained as an induction process over the acquired
associations between words and objects. In that sense,
fast mapping is a general cognitive ability, and not
a hard-coded, specialized mechanism of word learn-
ing.7 In addition, our results confirm that the onset
of fast mapping is a natural consequence of learning
more words, which in turn accelerates the learning of
new words. This bootstrapping approach results in a
rapid pace of vocabulary acquisition in children, with-
out requiring a developmental change in the underly-
ing learning mechanism.
Results of the referent selection experiments show
that our model can successfully find the referent of
a novel target word in a familiar context. Moreover,
our retention experiments show that the model can
map a recently heard novel word to its recently seen
novel referent (among other novel objects) after only
one exposure. However, the strength of the associa-
tion of a novel pair after one exposure shows a no-
table difference compared to the association between
a “typical” familiar word and its meaning.8 This is
consistent with what is commonly assumed in the lit-
erature: even though children learn something about
a word from only one exposure, they often need more
exposure to reliably learn its meaning (Carey, 1978).
Various kinds of experiments have been performed to
examine how strongly children learn novel words in-
troduced to them in experimental settings. For exam-
ple, children are persuaded to produce a fast-mapped
word, or to use the novel word to refer to objects
that are from the same category as its original refer-
ent (e.g., Golinkoff et al., 1992; Horst and Samuelson,
2008). We intend to look at these new tasks in our fu-
ture research.
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9985836875">
Behrend, Douglas A., Jason Scofield, and Erica E.
Kleinknecht 2001. Beyond fast mapping: Young chil-
dren’s extensions of novel words and novel facts. De-
velopmental Psychology, 37(5):698–705.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer 1993. The mathematics
of statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263–311.
Carey, Susan 1978. The child as word learner. In Halle, M.,
J. Bresnan, and G. A. Miller, editors, Linguistic Theory
and Psychological Reality. The MIT Press.
Carey, Susan and Elsa Bartlett 1978. Acquiring a single
new word. Papers and reports on Child Language De-
velopment, 15:17–29.
7In fact, similar fast mapping effects have been studied in con-
texts other than language. For example, Behrend et al. (2001) re-
port on children’s fast mapping of novel facts about novel objects.
8After processing 1000 input pairs, the average meaning prob-
ability of familiar words (those with frequency higher than 10) is
0.77, whereas that of the novel word after one exposure is 0.64.
Clark, Eve 1990. On the pragmatics of contrast. Journal
of Child Language, 17:417–431.
Diesendruck, Gil and Lori Markson 2001. Children’s
avoidance of lexical overlap: A pragmatic account. De-
velopmental Psychology, 37(5):630–641.
Fazly, Afsaneh, Afra Alishahi, and Suzanne Steven-
son 2008. A probabilistic incremental model of word
learning in the presence of referential uncertainty. In
Proceedings of the 30th Annual Conference of the Cog-
nitive Science Society.
Frank, Michael C., Noah D. Goodman, and Joshua B.
Tenenbaum 2007. A bayesian framework for cross-
situational word-learning. In Advances in Neural Infor-
mation Processing Systems, volume 20.
Gershkoff-Stowe, Lisa and Erin R. Hahn 2007. Fast map-
ping skills in the developing lexicon. Journal of Speech,
Language, and Hearing Research, 50:682–697.
Golinkoff, Roberta Michnick, Kathy Hirsh-Pasek,
Leslie M. Bailey, and Neil R. Wegner 1992. Young
children and adults use lexical principles to learn new
nouns. Developmental Psychology, 28(1):99–108.
Gopnik, Alison and Andrew Meltzoff 1987. The develop-
ment of categorization in the second year and its relation
to other cognitive and linguistic developments. Child
Development, 58(6):1523–1531.
Halberda, Justin 2006. Is this a dax which I see before
me? use of the logical argument disjunctive syllogism
supports word-learning in children and adults. Cognitive
Psychology, 53:310–344.
Halberda, Justin and Julie Goldman 2008. One-trial learn-
ing in 2-year-olds: Children learn new nouns in 3 sec-
onds flat. (in submission).
Horst, Jessica S., Bob McMurray, and Larissa K. Samuel-
son 2006. Online processing is essential for learning:
Understanding fast mapping and word learning in a dy-
namic connectionist architecture. In Proc. of CogSci’06.
Horst, Jessica S. and Larissa K. Samuelson 2008. Fast
mapping but poor retention by 24-month-old infants. In-
fancy, 13(2):128–157.
MacWhinney, B. 2000. The CHILDES Project: Tools for
Analyzing Talk, volume 2: The Database. MahWah, NJ:
Lawrence Erlbaum Associates, third edition.
Markman, Ellen M. and Gwyn F. Wachtel 1988. Children’s
use of mutual exclusivity to constrain the meanings of
words. Cognitive Psychology, 20:121–157.
Quine, W.V.O. 1960. Word and Object. Cambridge, MA:
MIT Press.
Regier, Terry 2005. The emergence of words: Atten-
tional learning in form and meaning. Cognitive Science,
29:819–865.
Reznick, J. Steven and Beverly A. Goldfield 1992. Rapid
change in lexical development in comprehension and
production. Developmental Psychology, 28(3):406–413.
Siskind, Jeffery Mark 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61:39–91.
Theakston, A. L., E. V. Lieven, J. M. Pine, and C. F. Row-
land 2001. The role of performance limitations in the
acquisition of verb-argument structure: An alternative
account. Journal of Child Language, 28:127–152.
</reference>
<page confidence="0.999419">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000226">
<title confidence="0.99975">Fast Mapping in Word Learning: What Probabilities Tell Us</title>
<author confidence="0.997174">Alishahi Fazly</author>
<affiliation confidence="0.998099">Department of Computer University of</affiliation>
<abstract confidence="0.9938521">Children can determine the meaning of a new word from hearing it used in a familiar ability often referred to as In this paper, we study fast mapping in the context of a general probabilistic model of word learning. We use our model to simulate fast mapping experiments on children, such as referent selection and retention. The word learning model can perform these tasks through an inductive interpretation of the acquired probabilities. Our results suggest that fast mapping occurs as a natural consequence of learning more words, and provides explanations for the (occasionally contradictory) child experimental data. 1 Fast Mapping average six-year-old child knows over words, most of which s/he has learned from hearing other people use them in ambiguous contexts (Carey, 1978). Children are thus assumed to be equipped with powerful mechanisms for performing such a complex task so efficiently. One interesting ability children as young as two years of age show is that of correctly and immediately mapping a novel word to a novel object in the presence of other familiar objects. The term “fast mapping” was first used by Carey and Bartlett (1978) to refer to this phenomenon. Carey and Bartlett’s goal was to examine how much children learn about a word when presented in an ambiguous context, as opposed to concentrated teaching. used an unfamiliar name to refer to unfamiliar color and then asked a group of four-year-old children to select an object from among a set, upon hearing a sentence explicitly Licensed under the Commons Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. for the object of the new color, as in: chromium tray, not the blue Children were generally good at performing this “referent selection” task. In a production task performed six weeks later, when children had to use the name of the new color, they showed signs of having learned something about the new color name, but were not successful at producing it. On the basis of these findings, Carey and Bartlett suggest that fast mapping and word learning are two distinct, yet related, processes. Extending Carey and Bartlett’s work, much research has concentrated on providing an explanation for fast mapping, and on examining its role in word learning. These studies also show that children are generally good at referent selection, given a novel target. However, there is not consistent evidence regarding whether children actually learn the novel word from one or a few such exposures (retention). For example, whereas the children in the experiments of Golinkoff et al. (1992) and Halberda (2006) showed signs of nearly-perfect retention of the fast-mapped words, those in the studies reported by Horst and Samuelson (2008) did not (all participating children were close in age range). There are also many speculations about the possible causes of fast mapping. Some researchers consider it as a sign of a specialized (innate) mechanism for word learning. Markman and Wachtel (1988), for example, argue that children fast map because they expect each object to have only one name (mutual exclusivity). Golinkoff et al. (1992) attribute fast mapping to a (hard-coded) bias towards mapping novel names to nameless object categories. Some even suggest a change in children’s learning mechanisms, at around the time they start to show evidence of fast mapping (which coincides with a sudden burst in their vocabulary), e.g., from associative to referential (Gopnik and Meltzoff, 1987; Reznick and Goldfield, 1992). In contrast, others see fast mapping as a phenomenon that arises from more general processes of learning 57</abstract>
<address confidence="0.849364">2008: Proceedings of the 12th Conference on Computational Natural Language Learning, Manchester, August 2008</address>
<abstract confidence="0.996949705382437">and/or communication, which also underlie the impressive rate of lexical acquisition in children (e.g., Clark, 1990; Diesendruck and Markson, 2001; Regier, 2005; Horst et al., 2006; Halberda, 2006). In our previous work (Fazly et al., 2008), we presented a word learning model which proposes a probabilistic interpretation of cross-situational learning, and bootstraps its own partially-learned knowledge of the word meanings to accelerate word learning over time. We have shown that the model can learn reasonable word–meaning associations from child-directed data, and that it accounts for observed learning patterns in children, such as vocabulary spurt, without requiring a developmental change in the underlying learning mechanism. Here, we use this computational model to investigate fast mapping and its relation to word learning. Specifically, we take a close look at the onset of fast mapping in our model by simulating some of the psychological experiments mentioned above. We examine the behaviour of the model in various referent selection and retention tasks, and provide explanations for the (occasionally contradictory) experimental results reported in the literature. We also study the effect of exposure to more input on the performance of the model in fast mapping. Our results suggest that fast mapping can be explained as an induction process over the acquired associations between words and meanings. Our model learns these associations in the form of probabilities within a unified framework; however, we argue that different interpretations of such probabilities may be involved in choosing the referent of a familiar as opposed to a novel target word (as noted by Halberda, 2006). Moreover, the overall behaviour of our model confirms that the probabilistic bootstrapping approach to word learning naturally leads to the onset of fast mapping in the course of lexical development, without hard-coding any specialized learning mechanism into the model to account for this phenomenon. 2 Overview of the Computational Model This section summarizes the model presented in Fazly et al. (2008). Our word learning algorithm is an adaptation of the IBM translation model proposed by Brown et al. (1993). However, our model is incremental, and does not require a batch process over the entire data. 2.1 Utterance and Meaning Representations The input to our word learning model consists of a set of utterance–scene pairs that link an observed scene (what the child perceives) to the utterance that describes it (what the child hears). We represent each as a sequence of words, and the corresponding scene as a set of meaning symbols. To simulate uncertainty the case where the child perceives aspects of the scene that are unrelated to the perceived utterance), we include additional symbols in the representation of the scene, e.g.: rolled the ball In Section 3.1, we explain how the utterances and the corresponding semantic symbols are selected, and how we add referential uncertainty. Given a corpus of such utterance–scene pairs, our learns the meaning of each word a probdistribution, over the semantic symbols appearing in the corpus. In this representation, the probability of a symbol the of a word In the absence of any prior knowledge, all symbols are equally likely to be the meaning of a word. Hence, prior to receiving any usages of a given word, the model assumes a uniform distribution over semantic symbols as its meaning. 2.2 Meaning Probabilities Our model combines probabilistic interpretations of cross-situational learning (Quine, 1960) and of a variation of the principle of contrast (Clark, 1990), through an interaction between two types of probabilistic knowledge acquired and refined over time. an utterance–scene pair received at time i.e., the model first calculates an alignment each and each the meaning probabilities all the words in the utterance prior to this time. The model revises the meaning of the words in incorporating the alignment probabilities for the current input pair. This process is repeated for all the input pairs, one at a time. Step 1: Calculating the alignment probabilities. We estimate the alignment probabilities of words and meaning symbols based on a localized version of the principle of contrast: that a meaning symbol in a scene is likely to be highly associated with one of the words the corresponding utter- For a symbol and a word higher the probability of the meaning to the more likely it is aligned with the current input. In words, proportional to In addition, if there is strong evidence the meaning of another word in if high for some E that this differs from what is widely known as the principle of contrast (Clark, 1990), in that the latter assumes contrast across the entire vocabulary rather than within an utterance. 58 likelihood of aligning decrease. Combining these two requirements: = Due to referential uncertainty, some of the meaning symbols in the scene might not have a counterpart in the utterance. To accommodate for such cases, a dummy word is added to each utterance before the alignment probabilities are calculated, in order to let a meaning symbol not be (strongly) aligned with any of the words in the current utterance. 2: Updating the word meanings. need to the probabilities all words based on the evidence from the current input pair reflected in the alignment probabilities. We thus add current alignment probabilities for the symthe accumulated evidence from prior of We summarize this cross-situational evidence in the form of an association score, which is updated incrementally: = + zero if not co-occurred before. The association score of a word and a symbol is basically a weighted sum of their cooccurrence counts. The model then uses these association scores to update the meaning of the words in the current input: the set of all symbols encountered prior to at time the expected number of symbol types, a small smoothing factor. The denominator is a normalization factor to get valid probabilities. This results in a uniform probability of all M a novel word and a probability than a meaning symbol has not previously seen with a familiar word Our model updates the meaning of a word every time it is heard in an utterance. The strength learning of a word at time reflected in where the “correct” meanof for a learned word the probability dishighly skewed towards the correct and therefore hearing trigger the of the meaning input-generation lexicon contains the correct meaning for each word, as described in Section 3.1. Note that the model does not have access to this lexicon for learning; it is used only for input generation and evaluation. this point on, we simply use (omitthe superscript to refer to the meaning probof the present time of learning. 2.3 Referent Probabilities meaning probability used to retrieve most probable meaning for all the possimeaning symbols However, in the referent selection tasks performed by children, the subject is often forced to select the referent of a target word from among a limited set of objects, even when the meaning of the target word has not been accurately learned yet. For our model to perform such tasks, it has to dehow likely it is for a target word refer to a object based on its previous knowledge the mapping between well as the mapping between other words in likelihood of using a particular name refer a given object calculated as: = - - the set of all words that the model has seen far, and the relative frequency of referent of a target word the present obtherefore, will be the object the highest probability 3 Experimental Setup 3.1 The Input Corpora We extract utterances from the Manchester corpus (Theakston et al., 2001) in the CHILDES database (MacWhinney, 2000). This corpus contains transcripts of conversations with children between the of 8 0 We use the speech from transcripts of remove punctuation and lemmatize the words, and concatenate the corresponding sessions as input data. There is no semantic representation of the corresponding scenes available from CHILDES. Therefore, we automatically construct a scene representation for each utterance, as a set containing the semantic referents of the words in that utterance. We get these from an input-generation lexicon that contains a symbol associated with each word as its semantic through the paper, we use both the meaning and the of a word + = + 59 referent. We use every other sentence from the original corpus, preserving their chronological order. To simulate referential uncertainty in the input, we then pair each sentence with its own scene representation as well as that of the following sentence in the original corpus. (Note that the latter sentence is not used as an utterance in our input.) The extra semantic symbols that are added to each utterance thus correspond to meaningful semantic representations, as opposed to randomly selected symbols. In the resulting corpus pairs, each utterance is, on average, with meaning symbols, reflecting a high degree of referential uncertianty. 3.2 The Model Parameters We set the parameters of our learning algorithm using a development data set which is similar to our training and test data, but is selected from a non-overlapping portion of the Manchester corpus. The expected numof symbols, Eq. (3), is set to on the total number of distinct symbols extracted for the development data. Therefore, the default probability a symbol for a novel word will be A familiar word, on the other hand, has been seen with some symbols before. Therefore, the probability of a previously unseen symbol for it (which, based on Eq. (3), an upper bound of must be less than the default mentioned above. Accordingly, we set 3.3 The Training Procedure In the next section, we report results from the computational simulation of our model for a number of experiments. All of the simulations use the same parameter settings (as described in the previous section), but different input: in each simulation, a random porof pairs is selected from the input corpus, and incrementally processed by the model. The size of the training corpus is chosen arbitrarily to reflect a sample point in learning, and further experiments have shown that increasing this number does not change the pattern observed in the results. In order to avoid behaviour that is specific to a particular sequence of input items, the reported results in the section are averaged over 4 Experimental Results and Analysis 4.1 Referent Selection In a typical word learning scenario, the child faces a scene where a number of familiar and unfamiliar objects are present. The child then hears a sentence, which describes (some part of) the scene, and is comof familiar and novel words (e.g., hearing is a where a previously unseen fruit). In such a setting, our model aligns the objects in the scene with the words in the utterance based on its acquired knowledge of word meanings, and then updates the meanings of the words accordingly. The model can align a familiar word with its referent with high confidence, since the previously learned meaning probability of the familiar object given the familword, or is much higher than the meaning probability of the same object given any other word in the sentence. In a similar fashion, the model can easily align a novel word in the sentence with a novel object in the scene, because the meaning probability the novel object given the novel word according to Eq. (3)) is higher than the meaning probability of that object for any previously heard word in sentence (the latter probability is smaller than Eq. (3), as explained in Section 3.2). Earlier fast mapping experiments on children assumed that it is such a contrast between the familiar and novel words in the same sentence that helps children select the correct target object in a referent selection task. For example, in Carey and Bartlett’s (1978) experiment, to introduce a novel word–meaning as- (e.g., the authors use both the familiar and the novel words in one sentence me the chromium tray, not the blue However, further experiments show that children can successfully select the correct referent even if such a contrast is not present in the sentence. Many researchers have performed experiments where young subjects are forced to choose between a novel and a familiar upon hearing a request, such as me the target), or me the dax target). In all of the reported experimental results, children can readily pick the correct referent for a familiar or a novel target word in such a setting (Golinkoff et al., 1992; Halberda and Goldman, 2008; Halberda, 2006; Horst and Samuelson, 2008). However, Halberda’s eye-tracking experiments on both adults and pre-schoolers suggest that the processes involved for referent selection in the familiar target situation may be different from those in the novel target situation. In the latter situation, subjects appear to systematically reject the familiar object as the referent of the novel name before mapping the novel object to the novel name. In the familiar target situation, however, there is no need to reject the novel distractor object, because the subject already knows the referent of the target. The difference between these two conditions can be explained in terms of the meaning and referent probabilities of our model explained in Section 2. In a typical referent selection experiment, the child is asked to 60 the while facing a a novel object We assume that the child knows the meaning verbs and determiners such as therefore we simplify the familiar target condition in the form of the following input item: familiar word such as a meaning probability highly skewed towards its correct meaning. is, upon hearing the model can confidently its meaning which is the one with highest probability all possible In such a case, if present in the scene, the model can easily pick it as the referent of the familiar target name, without processing the other objects in the scene. Now consider the condition where a novel target name is used in the presence of a familiar and a previously unseen object: Since this is the first time the model has heard the both meanings equally because uniform. Thus the meaning probabilities cannot be solely used for selecting referent of and the model has to perform some kind of induction on the potential referents in the scene based on what it has learned about each them. The model can infer the referent of comparing the referent probabilities Eq. (4) after processing the initem. Since strong associations with anword its referent probability for the novel much lower than the referent probability which does not have strong associations with any of the words in the learned lexicon. We simulate the process of referent selection in our model as follows. We train the model as described in Section 3.3. We then present the model with one input item, which represents either the the For each condition, we compare the meaning probability both familiar and novel objects the scene (see Table 1, top panel). In the the model demonstrates a strong preference towards choosing the familiar obas the referent, whereas in the condition, the model shows no preference towards any of the objects based on the meaning probabilities of target word. Therefore, for the condition, we also compare the referent probabilities both objects after processing 1: Referent selection in</abstract>
<title confidence="0.8509325">HEARING THE TARGET WORD Condition PERFORMING INDUCTION Condition</title>
<abstract confidence="0.993007896296296">the input item as a training pair, simulating the induction process that humans go through to select the referent in such cases. This time, the model shows a strong preference towards the novel object as the referent of the target word (see Table 1, bottom panel). Our results confirm that in both conditions, the model consistently selects the correct referent for the target word across all the simulations. 4.2 Retention As discussed in the previous section, results from the human experiments as well as our computational simulations show that the referent of a novel target word can be selected based on the previous knowledge about the present objects and their names. However, the success of a subject in a referent selection task does not necessarily mean that the child/model meaning of the novel word based on that one trial. In order to better understand what and how much children learn about a novel word from a single ambiguous exposure, some studies have performed retention trials after the referent selection experiments. Often, various referent selection trials are performed in one session, where in each trial a novel object–name pair is introduced among familiar objects. Some of the recently introduced objects are then put together in one last trial, and the subjects are asked to choose the correct referent for one of the (recently heard) novel target words. The majority of the reported experiments show that children can successfully perform the retention task (Golinkoff et al., 1992; Halberda and Goldman, 2008; Halberda, 2006). We simulate a similar retention experiment by training the model as usual. We further present the model with two experimental training items similar to one used in the in the previous section, with different familiar and novel objects and words in each input: 61 Table 2: Retention of a novel target word from a set of novel objects. The training session is followed by a retention trial, where the two novel objects used in the previous experimental inputs are paired with one of the novel target words: After processing the retention input, we comthe referent probabilities see if the model can choose the cornovel object in response to the target word The top panel in Table 2 summarizes the results of this experiment. The model consistently shows a strong preference towards the correct novel object as the referent of the novel target word across all simulations. Unlike studies on referent selection, experimental results for retention have not been consistent across various studies. Horst and Samuelson (2008) perform experiments with two-year-old children involving both referent selection and retention, and report that their subjects perform very poorly at the retention task. One factor that discriminates the experimental setup of Horst and Samuelson from others (e.g., Halberda, 2006) is that, in their retention trials, they put together two recently observed novel objects with a third novel object that has not been seen in any of the experimental sessions before. The authors do not attribute their contradictory results to the presence of this third object, but this factor can in fact affect the performance considerably. We simulate this condition by using the same input items for referent selection trials as in the previous simulation, but we replace the retention trial with the following: third object, has not been seen by the model before. Results under the new condition are reported in the bottom panel of Table 2. As can be seen, the model shows a strong tendency towards the cornovel referent the novel target comto the other recently seen novel object the probability of the unseen object also very high for the target word That is because the model cannot use any previously acquired 0 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 of first exposure Figure 1: Number of usages needed to learn a word, as a function of the word’s age of exposure. about associating it with anword) to rule it out as a referent for These results show that introducing a new object for the first time in a retention trial considerably increases the difficulty of the task. This can explain the contradictory results reported in the literature: when the referent probabilities are not informative, other factors may influence the outcome of the experiment, such as the amount of training received for a novel word–object, or a possible delay between training and test sessions. 4.3 The Effect of Exposure to More Input The fast mapping ability observed in children implies that once children have learned a repository of words, they can easily link novel words to novel objects in a familiar context based only on a few exposures. We examine this effect in our model: we train the model pairs, looking at the relation between the time of first exposure to a word, and the number of usages that the model needs for learning that word. Figure 1 plots this for words that have been learned at point during the We can see that the model shows clear fast mapping behaviour—that is, words received later in time, on average, require fewer usages to be learned. These results show that our model exhibits fast mapping patterns once it has been exposed to enough word usages, and that no change the underlying learning mechanism is The effect of exposure to more input on fast mapping can be described in terms of context familiarity: the more input the model has processed so far, the more likely it is that the context of the usage of a novel word (the other words in the sentence and the objects in the scene) is familiar to the model. This pattern has been studied through a number of experiments on consider a word learned if the meaning probability higher than a certain threshold For this experiwe set Fazly et al. (2008), we reported a variation of this experiment, where we used a smaller training set, and also a different semantic representation for word meanings. 35 number of usages needed to learn 30 25 20 15 10 5 62 children. For example, Gershkoff-Stowe and Hahn taught to the names of objects over sessions, where unfamiliar objects were presented with varying frequency. Data were compared to a control group of children who were exposed to the same experimental words at the first and last sessions only. Their results show that for children in the experimental group, extended practice with a novel set of words led to the rapid acquisition of a second set of low-practice words. Children in the control group did not show the same lexical advantage. Inspired by Gershkoff-Stowe and Hahn (2007), we perform an experiment to study the effect of context familiarity on fast mapping in our model. We two sets of words, and to conduct a referent selection task as follows. First, we train our model on a sequence of utterance–scene constructed from the set as follows: the unified set is randomly shuffled and divided into two subsets, words in each subset are put together to form an utterance, and the meanings of the words in that utterance are put together to form the corresponding scene. We repeat this process twice, so that each word appears in exactly two input We train our model on the constructed Next, we perform a referent selection task on each in the we pair each target word the meaning of selected words including the meaning of target word itself and have the model process this test pair. We compare the referent probabilof each see whether the model can correctly map the target word its referent. We call this setting the In the above setting, the context words in the referent selection trials are as new to the model as the target words. We thus repeat this experiment with a familiar context: we first train the model over input pairs that are randomly constructed from words using the same training procedure as described above. This context-familiarization process is followed by a similar training session on and a test session on target words, similar to the previous condition. Again, we count the number of correct mappings between a target word and its referent based on the referent proba- We call this setting the condition. Table 3 shows the results for both conditions. It can be seen that the accuracy of finding the referent in previous experiments, here we do not use childdirected data as we want to control the familiarity of the context. Table 3: Average number of correct mappings and the referent probabilities of target words for two condi- Condition Correct mappings %54 %90 for a target word, as well as the referent probability of a target word for its correct meaning, increase as a result of more training on the context. In other words, a more familiar context helps the model perform better in a fast mapping task. 5 Related Computational Models The rule-based model of Siskind (1996), and the connectionist model proposed by Regier (2005), both show that learning gets easier as the model is exposed to more input—that is, words heard later are learned faster. These findings confirm that fast mapping may simply be a result of learning more words, and that no explicit change in the underlying learning mechanism is needed. However, these studies do not examine various aspects of fast mapping, such as referent selection and retention. Horst et al. (2006) explicitly test fast mapping in their connectionist model of word learning by performing referent selection and retention tasks. The behaviour of their model matches the child experimental data reported in a study by the same authors (Horst and Samuelson, 2008), but not that of the contradictory findings of other similar experiments. Moreover, the model’s learning capacity is limited, and the fast mapping experiments are performed on a very small vocabulary. Frank et al. (2007) examine fast mapping in their Bayesian model by testing its performance in a novel target referent selection task. However, the experiment is performed on an artifical corpus. Moreover, since the learning algorithm is non-incremental, the success of the model in referent selection is determined implicitly: each possible word–meaning mapping from the test input is added to the current lexicon, and the consistency of the new lexicon is checked against the training corpus. 6 Discussion and Concluding Remarks We have used a general computational model of word learning (first introduced in Fazly et al., 2008) to study fast mapping. Our model learns a probabilistic association between a word and its meaning, from exposure to word usages in naturalistic contexts. We have shown that these probabilities can be used to simulate various fast mapping experiments performed on children, such as referent selection and retention. Our 63 experimental results suggest that fast mapping can be explained as an induction process over the acquired associations between words and objects. In that sense, fast mapping is a general cognitive ability, and not a hard-coded, specialized mechanism of word learn- In addition, our results confirm that the onset of fast mapping is a natural consequence of learning more words, which in turn accelerates the learning of new words. This bootstrapping approach results in a rapid pace of vocabulary acquisition in children, without requiring a developmental change in the underlying learning mechanism. Results of the referent selection experiments show that our model can successfully find the referent of a novel target word in a familiar context. Moreover, our retention experiments show that the model can map a recently heard novel word to its recently seen novel referent (among other novel objects) after only one exposure. However, the strength of the association of a novel pair after one exposure shows a notable difference compared to the association between “typical” familiar word and its This is consistent with what is commonly assumed in the literature: even though children learn something about a word from only one exposure, they often need more exposure to reliably learn its meaning (Carey, 1978). Various kinds of experiments have been performed to examine how strongly children learn novel words introduced to them in experimental settings. For example, children are persuaded to produce a fast-mapped word, or to use the novel word to refer to objects that are from the same category as its original referent (e.g., Golinkoff et al., 1992; Horst and Samuelson, 2008). We intend to look at these new tasks in our future research.</abstract>
<note confidence="0.865877230769231">References Behrend, Douglas A., Jason Scofield, and Erica E. Kleinknecht 2001. Beyond fast mapping: Young chilextensions of novel words and novel facts. De- 37(5):698–705. Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer 1993. The mathematics of statistical machine translation: Parameter estimation. 19(2):263–311. Carey, Susan 1978. The child as word learner. In Halle, M., Bresnan, and G. A. Miller, editors, Theory Psychological The MIT Press. Carey, Susan and Elsa Bartlett 1978. Acquiring a single</note>
<abstract confidence="0.912027888888889">word. and reports on Child Language De- 15:17–29. fact, similar fast mapping effects have been studied in contexts other than language. For example, Behrend et al. (2001) report on children’s fast mapping of novel facts about novel objects. processing pairs, the average meaning probability of familiar words (those with frequency higher than 10) is 0.77, whereas that of the novel word after one exposure is 0.64. Eve 1990. On the pragmatics of contrast.</abstract>
<note confidence="0.924701375">Child 17:417–431. Diesendruck, Gil and Lori Markson 2001. Children’s of lexical overlap: A pragmatic account. De- 37(5):630–641. Fazly, Afsaneh, Afra Alishahi, and Suzanne Stevenson 2008. A probabilistic incremental model of word learning in the presence of referential uncertainty. In Proceedings of the 30th Annual Conference of the Cog- Science Frank, Michael C., Noah D. Goodman, and Joshua B. Tenenbaum 2007. A bayesian framework for crossword-learning. In in Neural Infor- Processing volume 20. Gershkoff-Stowe, Lisa and Erin R. Hahn 2007. Fast mapskills in the developing lexicon. of Speech, and Hearing 50:682–697.</note>
<title confidence="0.419759">Golinkoff, Roberta Michnick, Kathy Hirsh-Pasek,</title>
<author confidence="0.845863">Leslie M Bailey</author>
<author confidence="0.845863">Neil R Wegner</author>
<abstract confidence="0.816552853658537">children and adults use lexical principles to learn new 28(1):99–108. Gopnik, Alison and Andrew Meltzoff 1987. The development of categorization in the second year and its relation other cognitive and linguistic developments. 58(6):1523–1531. Halberda, Justin 2006. Is this a dax which I see before me? use of the logical argument disjunctive syllogism word-learning in children and adults. 53:310–344. Halberda, Justin and Julie Goldman 2008. One-trial learning in 2-year-olds: Children learn new nouns in 3 seconds flat. (in submission). Horst, Jessica S., Bob McMurray, and Larissa K. Samuelson 2006. Online processing is essential for learning: Understanding fast mapping and word learning in a dyconnectionist architecture. In of Horst, Jessica S. and Larissa K. Samuelson 2008. Fast but poor retention by 24-month-old infants. In- 13(2):128–157. B. 2000. CHILDES Project: Tools for volume 2: The Database. MahWah, NJ: Lawrence Erlbaum Associates, third edition. Markman, Ellen M. and Gwyn F. Wachtel 1988. Children’s use of mutual exclusivity to constrain the meanings of 20:121–157. W.V.O. 1960. and Cambridge, MA: MIT Press. Regier, Terry 2005. The emergence of words: Attenlearning in form and meaning. 29:819–865. Reznick, J. Steven and Beverly A. Goldfield 1992. Rapid change in lexical development in comprehension and 28(3):406–413. Siskind, Jeffery Mark 1996. A computational study of cross-situational techniques for learning word-tomappings. 61:39–91. Theakston, A. L., E. V. Lieven, J. M. Pine, and C. F. Rowland 2001. The role of performance limitations in the acquisition of verb-argument structure: An alternative of Child 28:127–152.</abstract>
<intro confidence="0.847331">64</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas A Behrend</author>
<author>Jason Scofield</author>
<author>Erica E Kleinknecht</author>
</authors>
<title>Beyond fast mapping: Young children’s extensions of novel words and novel facts.</title>
<date>2001</date>
<journal>Developmental Psychology,</journal>
<volume>37</volume>
<issue>5</issue>
<marker>Behrend, Scofield, Kleinknecht, 2001</marker>
<rawString>Behrend, Douglas A., Jason Scofield, and Erica E. Kleinknecht 2001. Beyond fast mapping: Young children’s extensions of novel words and novel facts. Developmental Psychology, 37(5):698–705.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="6362" citStr="Brown et al. (1993)" startWordPosition="1000" endWordPosition="1003">ed in choosing the referent of a familiar as opposed to a novel target word (as noted by Halberda, 2006). Moreover, the overall behaviour of our model confirms that the probabilistic bootstrapping approach to word learning naturally leads to the onset of fast mapping in the course of lexical development, without hard-coding any specialized learning mechanism into the model to account for this phenomenon. 2 Overview of the Computational Model This section summarizes the model presented in Fazly et al. (2008). Our word learning algorithm is an adaptation of the IBM translation model proposed by Brown et al. (1993). However, our model is incremental, and does not require a batch process over the entire data. 2.1 Utterance and Meaning Representations The input to our word learning model consists of a set of utterance–scene pairs that link an observed scene (what the child perceives) to the utterance that describes it (what the child hears). We represent each utterance as a sequence of words, and the corresponding scene as a set of meaning symbols. To simulate referential uncertainty (i.e., the case where the child perceives aspects of the scene that are unrelated to the perceived utterance), we include a</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Carey</author>
</authors>
<title>The child as word learner.</title>
<date>1978</date>
<booktitle>Linguistic Theory and Psychological Reality.</booktitle>
<editor>In Halle, M., J. Bresnan, and G. A. Miller, editors,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="1031" citStr="Carey, 1978" startWordPosition="157" endWordPosition="158">abilistic model of word learning. We use our model to simulate fast mapping experiments on children, such as referent selection and retention. The word learning model can perform these tasks through an inductive interpretation of the acquired probabilities. Our results suggest that fast mapping occurs as a natural consequence of learning more words, and provides explanations for the (occasionally contradictory) child experimental data. 1 Fast Mapping An average six-year-old child knows over 14, 000 words, most of which s/he has learned from hearing other people use them in ambiguous contexts (Carey, 1978). Children are thus assumed to be equipped with powerful mechanisms for performing such a complex task so efficiently. One interesting ability children as young as two years of age show is that of correctly and immediately mapping a novel word to a novel object in the presence of other familiar objects. The term “fast mapping” was first used by Carey and Bartlett (1978) to refer to this phenomenon. Carey and Bartlett’s goal was to examine how much children learn about a word when presented in an ambiguous context, as opposed to concentrated teaching. They used an unfamiliar name (chromium) to </context>
</contexts>
<marker>Carey, 1978</marker>
<rawString>Carey, Susan 1978. The child as word learner. In Halle, M., J. Bresnan, and G. A. Miller, editors, Linguistic Theory and Psychological Reality. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Carey</author>
<author>Elsa Bartlett</author>
</authors>
<title>Acquiring a single new word. Papers and reports on Child Language Development,</title>
<date>1978</date>
<pages>15--17</pages>
<contexts>
<context position="1403" citStr="Carey and Bartlett (1978)" startWordPosition="218" endWordPosition="221">vides explanations for the (occasionally contradictory) child experimental data. 1 Fast Mapping An average six-year-old child knows over 14, 000 words, most of which s/he has learned from hearing other people use them in ambiguous contexts (Carey, 1978). Children are thus assumed to be equipped with powerful mechanisms for performing such a complex task so efficiently. One interesting ability children as young as two years of age show is that of correctly and immediately mapping a novel word to a novel object in the presence of other familiar objects. The term “fast mapping” was first used by Carey and Bartlett (1978) to refer to this phenomenon. Carey and Bartlett’s goal was to examine how much children learn about a word when presented in an ambiguous context, as opposed to concentrated teaching. They used an unfamiliar name (chromium) to refer to an unfamiliar color (olive green), and then asked a group of four-year-old children to select an object from among a set, upon hearing a sentence explicitly © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. asking for the object of the</context>
</contexts>
<marker>Carey, Bartlett, 1978</marker>
<rawString>Carey, Susan and Elsa Bartlett 1978. Acquiring a single new word. Papers and reports on Child Language Development, 15:17–29.</rawString>
</citation>
<citation valid="true">
<title>7In fact, similar fast mapping effects have been studied in contexts other than language. For example,</title>
<date>2001</date>
<journal>Behrend</journal>
<marker>2001</marker>
<rawString>7In fact, similar fast mapping effects have been studied in contexts other than language. For example, Behrend et al. (2001) report on children’s fast mapping of novel facts about novel objects.</rawString>
</citation>
<citation valid="false">
<title>8After processing 1000 input pairs, the average meaning probability of familiar words (those with frequency higher than 10) is 0.77, whereas that of the novel word after one exposure is 0.64.</title>
<marker></marker>
<rawString>8After processing 1000 input pairs, the average meaning probability of familiar words (those with frequency higher than 10) is 0.77, whereas that of the novel word after one exposure is 0.64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eve Clark</author>
</authors>
<title>On the pragmatics of contrast.</title>
<date>1990</date>
<journal>Journal of Child Language,</journal>
<pages>17--417</pages>
<contexts>
<context position="4259" citStr="Clark, 1990" startWordPosition="670" endWordPosition="671"> change in children’s learning mechanisms, at around the time they start to show evidence of fast mapping (which coincides with a sudden burst in their vocabulary), e.g., from associative to referential (Gopnik and Meltzoff, 1987; Reznick and Goldfield, 1992). In contrast, others see fast mapping as a phenomenon that arises from more general processes of learning 57 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 57–64 Manchester, August 2008 and/or communication, which also underlie the impressive rate of lexical acquisition in children (e.g., Clark, 1990; Diesendruck and Markson, 2001; Regier, 2005; Horst et al., 2006; Halberda, 2006). In our previous work (Fazly et al., 2008), we presented a word learning model which proposes a probabilistic interpretation of cross-situational learning, and bootstraps its own partially-learned knowledge of the word meanings to accelerate word learning over time. We have shown that the model can learn reasonable word–meaning associations from child-directed data, and that it accounts for observed learning patterns in children, such as vocabulary spurt, without requiring a developmental change in the underlyin</context>
<context position="7928" citStr="Clark, 1990" startWordPosition="1260" endWordPosition="1261">of each word w as a probability distribution, p(.|w), over the semantic symbols appearing in the corpus. In this representation, p(m|w) is the probability of a symbol m being the meaning of a word w. In the absence of any prior knowledge, all symbols are equally likely to be the meaning of a word. Hence, prior to receiving any usages of a given word, the model assumes a uniform distribution over semantic symbols as its meaning. 2.2 Meaning Probabilities Our model combines probabilistic interpretations of cross-situational learning (Quine, 1960) and of a variation of the principle of contrast (Clark, 1990), through an interaction between two types of probabilistic knowledge acquired and refined over time. Given an utterance–scene pair received at time t, i.e., (U(t), S(t)), the model first calculates an alignment probability a for each w E U(t) and each m E S(t), using the meaning probabilities p(.|w) of all the words in the utterance prior to this time. The model then revises the meaning of the words in U(t) by incorporating the alignment probabilities for the current input pair. This process is repeated for all the input pairs, one at a time. Step 1: Calculating the alignment probabilities. W</context>
<context position="9274" citStr="Clark, 1990" startWordPosition="1502" endWordPosition="1503">meaning symbol in a scene is likely to be highly associated with only one of the words in the corresponding utterance.1 For a symbol m E S(t) and a word w E U(t), the higher the probability of m being the meaning of w (according to p(m|w)), the more likely it is that m is aligned with w in the current input. In other words, a(w|m, U(t), S(t)) is proportional to p(t−1)(m|w). In addition, if there is strong evidence that m is the meaning of another word in U(t)— i.e., if p(t−1)(m|w′) is high for some w′ E U(t) other 1Note that this differs from what is widely known as the principle of contrast (Clark, 1990), in that the latter assumes contrast across the entire vocabulary rather than within an utterance. 58 than w—the likelihood of aligning m to w should decrease. Combining these two requirements: a(w|m, U(t), S(t)) = p(t−1)(m|w) (1) E p(t−1)(m|w′) w′∈U(t) Due to referential uncertainty, some of the meaning symbols in the scene might not have a counterpart in the utterance. To accommodate for such cases, a dummy word is added to each utterance before the alignment probabilities are calculated, in order to let a meaning symbol not be (strongly) aligned with any of the words in the current utteran</context>
</contexts>
<marker>Clark, 1990</marker>
<rawString>Clark, Eve 1990. On the pragmatics of contrast. Journal of Child Language, 17:417–431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gil Diesendruck</author>
<author>Lori Markson</author>
</authors>
<title>Children’s avoidance of lexical overlap: A pragmatic account.</title>
<date>2001</date>
<journal>Developmental Psychology,</journal>
<volume>37</volume>
<issue>5</issue>
<contexts>
<context position="4290" citStr="Diesendruck and Markson, 2001" startWordPosition="672" endWordPosition="675">ildren’s learning mechanisms, at around the time they start to show evidence of fast mapping (which coincides with a sudden burst in their vocabulary), e.g., from associative to referential (Gopnik and Meltzoff, 1987; Reznick and Goldfield, 1992). In contrast, others see fast mapping as a phenomenon that arises from more general processes of learning 57 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 57–64 Manchester, August 2008 and/or communication, which also underlie the impressive rate of lexical acquisition in children (e.g., Clark, 1990; Diesendruck and Markson, 2001; Regier, 2005; Horst et al., 2006; Halberda, 2006). In our previous work (Fazly et al., 2008), we presented a word learning model which proposes a probabilistic interpretation of cross-situational learning, and bootstraps its own partially-learned knowledge of the word meanings to accelerate word learning over time. We have shown that the model can learn reasonable word–meaning associations from child-directed data, and that it accounts for observed learning patterns in children, such as vocabulary spurt, without requiring a developmental change in the underlying learning mechanism. Here, we </context>
</contexts>
<marker>Diesendruck, Markson, 2001</marker>
<rawString>Diesendruck, Gil and Lori Markson 2001. Children’s avoidance of lexical overlap: A pragmatic account. Developmental Psychology, 37(5):630–641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Afra Alishahi</author>
<author>Suzanne Stevenson</author>
</authors>
<title>A probabilistic incremental model of word learning in the presence of referential uncertainty.</title>
<date>2008</date>
<booktitle>In Proceedings of the 30th Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="4384" citStr="Fazly et al., 2008" startWordPosition="688" endWordPosition="691">cides with a sudden burst in their vocabulary), e.g., from associative to referential (Gopnik and Meltzoff, 1987; Reznick and Goldfield, 1992). In contrast, others see fast mapping as a phenomenon that arises from more general processes of learning 57 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 57–64 Manchester, August 2008 and/or communication, which also underlie the impressive rate of lexical acquisition in children (e.g., Clark, 1990; Diesendruck and Markson, 2001; Regier, 2005; Horst et al., 2006; Halberda, 2006). In our previous work (Fazly et al., 2008), we presented a word learning model which proposes a probabilistic interpretation of cross-situational learning, and bootstraps its own partially-learned knowledge of the word meanings to accelerate word learning over time. We have shown that the model can learn reasonable word–meaning associations from child-directed data, and that it accounts for observed learning patterns in children, such as vocabulary spurt, without requiring a developmental change in the underlying learning mechanism. Here, we use this computational model to investigate fast mapping and its relation to word learning. Sp</context>
<context position="6255" citStr="Fazly et al. (2008)" startWordPosition="981" endWordPosition="985">n a unified framework; however, we argue that different interpretations of such probabilities may be involved in choosing the referent of a familiar as opposed to a novel target word (as noted by Halberda, 2006). Moreover, the overall behaviour of our model confirms that the probabilistic bootstrapping approach to word learning naturally leads to the onset of fast mapping in the course of lexical development, without hard-coding any specialized learning mechanism into the model to account for this phenomenon. 2 Overview of the Computational Model This section summarizes the model presented in Fazly et al. (2008). Our word learning algorithm is an adaptation of the IBM translation model proposed by Brown et al. (1993). However, our model is incremental, and does not require a batch process over the entire data. 2.1 Utterance and Meaning Representations The input to our word learning model consists of a set of utterance–scene pairs that link an observed scene (what the child perceives) to the utterance that describes it (what the child hears). We represent each utterance as a sequence of words, and the corresponding scene as a set of meaning symbols. To simulate referential uncertainty (i.e., the case </context>
<context position="28442" citStr="Fazly et al. (2008)" startWordPosition="4803" endWordPosition="4806">ord usages, and that no change in the underlying learning mechanism is needed.5 The effect of exposure to more input on fast mapping can be described in terms of context familiarity: the more input the model has processed so far, the more likely it is that the context of the usage of a novel word (the other words in the sentence and the objects in the scene) is familiar to the model. This pattern has been studied through a number of experiments on 4We consider a word w as learned if the meaning probability p(m.|w) is higher than a certain threshold 0. For this experiment, we set 0 = 0.70. 5In Fazly et al. (2008), we reported a variation of this experiment, where we used a smaller training set, and also a different semantic representation for word meanings. 35 number of usages needed to learn 30 25 20 15 10 5 62 children. For example, Gershkoff-Stowe and Hahn (2007) taught 16- to 18-month-olds the names of 24 unfamiliar objects over 12 training sessions, where unfamiliar objects were presented with varying frequency. Data were compared to a control group of children who were exposed to the same experimental words at the first and last sessions only. Their results show that for children in the experime</context>
<context position="33483" citStr="Fazly et al., 2008" startWordPosition="5659" endWordPosition="5662"> (2007) examine fast mapping in their Bayesian model by testing its performance in a novel target referent selection task. However, the experiment is performed on an artifical corpus. Moreover, since the learning algorithm is non-incremental, the success of the model in referent selection is determined implicitly: each possible word–meaning mapping from the test input is added to the current lexicon, and the consistency of the new lexicon is checked against the training corpus. 6 Discussion and Concluding Remarks We have used a general computational model of word learning (first introduced in Fazly et al., 2008) to study fast mapping. Our model learns a probabilistic association between a word and its meaning, from exposure to word usages in naturalistic contexts. We have shown that these probabilities can be used to simulate various fast mapping experiments performed on children, such as referent selection and retention. Our 63 experimental results suggest that fast mapping can be explained as an induction process over the acquired associations between words and objects. In that sense, fast mapping is a general cognitive ability, and not a hard-coded, specialized mechanism of word learning.7 In addi</context>
</contexts>
<marker>Fazly, Alishahi, Stevenson, 2008</marker>
<rawString>Fazly, Afsaneh, Afra Alishahi, and Suzanne Stevenson 2008. A probabilistic incremental model of word learning in the presence of referential uncertainty. In Proceedings of the 30th Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Noah D Goodman</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>A bayesian framework for crosssituational word-learning.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>20</volume>
<contexts>
<context position="32871" citStr="Frank et al. (2007)" startWordPosition="5562" endWordPosition="5565">ed. However, these studies do not examine various aspects of fast mapping, such as referent selection and retention. Horst et al. (2006) explicitly test fast mapping in their connectionist model of word learning by performing referent selection and retention tasks. The behaviour of their model matches the child experimental data reported in a study by the same authors (Horst and Samuelson, 2008), but not that of the contradictory findings of other similar experiments. Moreover, the model’s learning capacity is limited, and the fast mapping experiments are performed on a very small vocabulary. Frank et al. (2007) examine fast mapping in their Bayesian model by testing its performance in a novel target referent selection task. However, the experiment is performed on an artifical corpus. Moreover, since the learning algorithm is non-incremental, the success of the model in referent selection is determined implicitly: each possible word–meaning mapping from the test input is added to the current lexicon, and the consistency of the new lexicon is checked against the training corpus. 6 Discussion and Concluding Remarks We have used a general computational model of word learning (first introduced in Fazly e</context>
</contexts>
<marker>Frank, Goodman, Tenenbaum, 2007</marker>
<rawString>Frank, Michael C., Noah D. Goodman, and Joshua B. Tenenbaum 2007. A bayesian framework for crosssituational word-learning. In Advances in Neural Information Processing Systems, volume 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Gershkoff-Stowe</author>
<author>Erin R Hahn</author>
</authors>
<title>Fast mapping skills in the developing lexicon.</title>
<date>2007</date>
<journal>Journal of Speech, Language, and Hearing Research,</journal>
<pages>50--682</pages>
<contexts>
<context position="28700" citStr="Gershkoff-Stowe and Hahn (2007)" startWordPosition="4848" endWordPosition="4851">it is that the context of the usage of a novel word (the other words in the sentence and the objects in the scene) is familiar to the model. This pattern has been studied through a number of experiments on 4We consider a word w as learned if the meaning probability p(m.|w) is higher than a certain threshold 0. For this experiment, we set 0 = 0.70. 5In Fazly et al. (2008), we reported a variation of this experiment, where we used a smaller training set, and also a different semantic representation for word meanings. 35 number of usages needed to learn 30 25 20 15 10 5 62 children. For example, Gershkoff-Stowe and Hahn (2007) taught 16- to 18-month-olds the names of 24 unfamiliar objects over 12 training sessions, where unfamiliar objects were presented with varying frequency. Data were compared to a control group of children who were exposed to the same experimental words at the first and last sessions only. Their results show that for children in the experimental group, extended practice with a novel set of words led to the rapid acquisition of a second set of low-practice words. Children in the control group did not show the same lexical advantage. Inspired by Gershkoff-Stowe and Hahn (2007), we perform an expe</context>
</contexts>
<marker>Gershkoff-Stowe, Hahn, 2007</marker>
<rawString>Gershkoff-Stowe, Lisa and Erin R. Hahn 2007. Fast mapping skills in the developing lexicon. Journal of Speech, Language, and Hearing Research, 50:682–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberta Michnick Golinkoff</author>
<author>Kathy Hirsh-Pasek</author>
<author>Leslie M Bailey</author>
<author>Neil R Wegner</author>
</authors>
<title>Young children and adults use lexical principles to learn new nouns.</title>
<date>1992</date>
<journal>Developmental Psychology,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="2974" citStr="Golinkoff et al. (1992)" startWordPosition="468" endWordPosition="471">. On the basis of these findings, Carey and Bartlett suggest that fast mapping and word learning are two distinct, yet related, processes. Extending Carey and Bartlett’s work, much research has concentrated on providing an explanation for fast mapping, and on examining its role in word learning. These studies also show that children are generally good at referent selection, given a novel target. However, there is not consistent evidence regarding whether children actually learn the novel word from one or a few such exposures (retention). For example, whereas the children in the experiments of Golinkoff et al. (1992) and Halberda (2006) showed signs of nearly-perfect retention of the fast-mapped words, those in the studies reported by Horst and Samuelson (2008) did not (all participating children were close in age range). There are also many speculations about the possible causes of fast mapping. Some researchers consider it as a sign of a specialized (innate) mechanism for word learning. Markman and Wachtel (1988), for example, argue that children fast map because they expect each object to have only one name (mutual exclusivity). Golinkoff et al. (1992) attribute fast mapping to a (hard-coded) bias towa</context>
<context position="18367" citStr="Golinkoff et al., 1992" startWordPosition="3102" endWordPosition="3105">vel words in one sentence (bring me the chromium tray, not the blue one.). However, further experiments show that children can successfully select the correct referent even if such a contrast is not present in the sentence. Many researchers have performed experiments where young subjects are forced to choose between a novel and a familiar object upon hearing a request, such as give me the ball (familiar target), or give me the dax (novel target). In all of the reported experimental results, children can readily pick the correct referent for a familiar or a novel target word in such a setting (Golinkoff et al., 1992; Halberda and Goldman, 2008; Halberda, 2006; Horst and Samuelson, 2008). However, Halberda’s eye-tracking experiments on both adults and pre-schoolers suggest that the processes involved for referent selection in the familiar target situation may be different from those in the novel target situation. In the latter situation, subjects appear to systematically reject the familiar object as the referent of the novel name before mapping the novel object to the novel name. In the familiar target situation, however, there is no need to reject the novel distractor object, because the subject already</context>
<context position="23527" citStr="Golinkoff et al., 1992" startWordPosition="3960" endWordPosition="3963">ren learn about a novel word from a single ambiguous exposure, some studies have performed retention trials after the referent selection experiments. Often, various referent selection trials are performed in one session, where in each trial a novel object–name pair is introduced among familiar objects. Some of the recently introduced objects are then put together in one last trial, and the subjects are asked to choose the correct referent for one of the (recently heard) novel target words. The majority of the reported experiments show that children can successfully perform the retention task (Golinkoff et al., 1992; Halberda and Goldman, 2008; Halberda, 2006). We simulate a similar retention experiment by training the model as usual. We further present the model with two experimental training items similar to the one used in the NOVEL TARGET condition in the previous section, with different familiar and novel objects and words in each input: dax (REFERENT SELECTION TRIAL 1) {ball, dax} cheem (REFERENT SELECTION TRIAL 2) {pen,cheem} 61 Table 2: Retention of a novel target word from a set of novel objects. 2-OBJECT RETENTION TRIAL rf (dax|dax) rf (dax|cheem) 0.996 ±0.001 0.501 ±0.068 3-OBJECT RETENTION TR</context>
</contexts>
<marker>Golinkoff, Hirsh-Pasek, Bailey, Wegner, 1992</marker>
<rawString>Golinkoff, Roberta Michnick, Kathy Hirsh-Pasek, Leslie M. Bailey, and Neil R. Wegner 1992. Young children and adults use lexical principles to learn new nouns. Developmental Psychology, 28(1):99–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alison Gopnik</author>
<author>Andrew Meltzoff</author>
</authors>
<title>The development of categorization in the second year and its relation to other cognitive and linguistic developments.</title>
<date>1987</date>
<journal>Child Development,</journal>
<volume>58</volume>
<issue>6</issue>
<contexts>
<context position="3877" citStr="Gopnik and Meltzoff, 1987" startWordPosition="612" endWordPosition="615">apping. Some researchers consider it as a sign of a specialized (innate) mechanism for word learning. Markman and Wachtel (1988), for example, argue that children fast map because they expect each object to have only one name (mutual exclusivity). Golinkoff et al. (1992) attribute fast mapping to a (hard-coded) bias towards mapping novel names to nameless object categories. Some even suggest a change in children’s learning mechanisms, at around the time they start to show evidence of fast mapping (which coincides with a sudden burst in their vocabulary), e.g., from associative to referential (Gopnik and Meltzoff, 1987; Reznick and Goldfield, 1992). In contrast, others see fast mapping as a phenomenon that arises from more general processes of learning 57 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 57–64 Manchester, August 2008 and/or communication, which also underlie the impressive rate of lexical acquisition in children (e.g., Clark, 1990; Diesendruck and Markson, 2001; Regier, 2005; Horst et al., 2006; Halberda, 2006). In our previous work (Fazly et al., 2008), we presented a word learning model which proposes a probabilistic interpretation of cross-s</context>
</contexts>
<marker>Gopnik, Meltzoff, 1987</marker>
<rawString>Gopnik, Alison and Andrew Meltzoff 1987. The development of categorization in the second year and its relation to other cognitive and linguistic developments. Child Development, 58(6):1523–1531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Halberda</author>
</authors>
<title>Is this a dax which I see before me? use of the logical argument disjunctive syllogism supports word-learning in children and adults. Cognitive Psychology,</title>
<date>2006</date>
<pages>53--310</pages>
<contexts>
<context position="2994" citStr="Halberda (2006)" startWordPosition="473" endWordPosition="474">ings, Carey and Bartlett suggest that fast mapping and word learning are two distinct, yet related, processes. Extending Carey and Bartlett’s work, much research has concentrated on providing an explanation for fast mapping, and on examining its role in word learning. These studies also show that children are generally good at referent selection, given a novel target. However, there is not consistent evidence regarding whether children actually learn the novel word from one or a few such exposures (retention). For example, whereas the children in the experiments of Golinkoff et al. (1992) and Halberda (2006) showed signs of nearly-perfect retention of the fast-mapped words, those in the studies reported by Horst and Samuelson (2008) did not (all participating children were close in age range). There are also many speculations about the possible causes of fast mapping. Some researchers consider it as a sign of a specialized (innate) mechanism for word learning. Markman and Wachtel (1988), for example, argue that children fast map because they expect each object to have only one name (mutual exclusivity). Golinkoff et al. (1992) attribute fast mapping to a (hard-coded) bias towards mapping novel na</context>
<context position="4341" citStr="Halberda, 2006" startWordPosition="682" endWordPosition="683">ow evidence of fast mapping (which coincides with a sudden burst in their vocabulary), e.g., from associative to referential (Gopnik and Meltzoff, 1987; Reznick and Goldfield, 1992). In contrast, others see fast mapping as a phenomenon that arises from more general processes of learning 57 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 57–64 Manchester, August 2008 and/or communication, which also underlie the impressive rate of lexical acquisition in children (e.g., Clark, 1990; Diesendruck and Markson, 2001; Regier, 2005; Horst et al., 2006; Halberda, 2006). In our previous work (Fazly et al., 2008), we presented a word learning model which proposes a probabilistic interpretation of cross-situational learning, and bootstraps its own partially-learned knowledge of the word meanings to accelerate word learning over time. We have shown that the model can learn reasonable word–meaning associations from child-directed data, and that it accounts for observed learning patterns in children, such as vocabulary spurt, without requiring a developmental change in the underlying learning mechanism. Here, we use this computational model to investigate fast ma</context>
<context position="5847" citStr="Halberda, 2006" startWordPosition="921" endWordPosition="922">lanations for the (occasionally contradictory) experimental results reported in the literature. We also study the effect of exposure to more input on the performance of the model in fast mapping. Our results suggest that fast mapping can be explained as an induction process over the acquired associations between words and meanings. Our model learns these associations in the form of probabilities within a unified framework; however, we argue that different interpretations of such probabilities may be involved in choosing the referent of a familiar as opposed to a novel target word (as noted by Halberda, 2006). Moreover, the overall behaviour of our model confirms that the probabilistic bootstrapping approach to word learning naturally leads to the onset of fast mapping in the course of lexical development, without hard-coding any specialized learning mechanism into the model to account for this phenomenon. 2 Overview of the Computational Model This section summarizes the model presented in Fazly et al. (2008). Our word learning algorithm is an adaptation of the IBM translation model proposed by Brown et al. (1993). However, our model is incremental, and does not require a batch process over the en</context>
<context position="18411" citStr="Halberda, 2006" startWordPosition="3110" endWordPosition="3111">y, not the blue one.). However, further experiments show that children can successfully select the correct referent even if such a contrast is not present in the sentence. Many researchers have performed experiments where young subjects are forced to choose between a novel and a familiar object upon hearing a request, such as give me the ball (familiar target), or give me the dax (novel target). In all of the reported experimental results, children can readily pick the correct referent for a familiar or a novel target word in such a setting (Golinkoff et al., 1992; Halberda and Goldman, 2008; Halberda, 2006; Horst and Samuelson, 2008). However, Halberda’s eye-tracking experiments on both adults and pre-schoolers suggest that the processes involved for referent selection in the familiar target situation may be different from those in the novel target situation. In the latter situation, subjects appear to systematically reject the familiar object as the referent of the novel name before mapping the novel object to the novel name. In the familiar target situation, however, there is no need to reject the novel distractor object, because the subject already knows the referent of the target. The diffe</context>
<context position="23572" citStr="Halberda, 2006" startWordPosition="3968" endWordPosition="3969"> exposure, some studies have performed retention trials after the referent selection experiments. Often, various referent selection trials are performed in one session, where in each trial a novel object–name pair is introduced among familiar objects. Some of the recently introduced objects are then put together in one last trial, and the subjects are asked to choose the correct referent for one of the (recently heard) novel target words. The majority of the reported experiments show that children can successfully perform the retention task (Golinkoff et al., 1992; Halberda and Goldman, 2008; Halberda, 2006). We simulate a similar retention experiment by training the model as usual. We further present the model with two experimental training items similar to the one used in the NOVEL TARGET condition in the previous section, with different familiar and novel objects and words in each input: dax (REFERENT SELECTION TRIAL 1) {ball, dax} cheem (REFERENT SELECTION TRIAL 2) {pen,cheem} 61 Table 2: Retention of a novel target word from a set of novel objects. 2-OBJECT RETENTION TRIAL rf (dax|dax) rf (dax|cheem) 0.996 ±0.001 0.501 ±0.068 3-OBJECT RETENTION TRIAL rf (dax|dax) rf (dax|cheem) rf (dax|lukk)</context>
<context position="25272" citStr="Halberda, 2006" startWordPosition="4240" endWordPosition="4242">arizes the results of this experiment. The model consistently shows a strong preference towards the correct novel object as the referent of the novel target word across all simulations. Unlike studies on referent selection, experimental results for retention have not been consistent across various studies. Horst and Samuelson (2008) perform experiments with two-year-old children involving both referent selection and retention, and report that their subjects perform very poorly at the retention task. One factor that discriminates the experimental setup of Horst and Samuelson from others (e.g., Halberda, 2006) is that, in their retention trials, they put together two recently observed novel objects with a third novel object that has not been seen in any of the experimental sessions before. The authors do not attribute their contradictory results to the presence of this third object, but this factor can in fact affect the performance considerably. We simulate this condition by using the same input items for referent selection trials as in the previous simulation, but we replace the retention trial with the following: dax (3-OBJECT RETENTION TRIAL) {cheem,dax,lukk} The third object, lukk, has not bee</context>
</contexts>
<marker>Halberda, 2006</marker>
<rawString>Halberda, Justin 2006. Is this a dax which I see before me? use of the logical argument disjunctive syllogism supports word-learning in children and adults. Cognitive Psychology, 53:310–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Halberda</author>
<author>Julie Goldman</author>
</authors>
<title>One-trial learning in 2-year-olds: Children learn new nouns in 3 seconds flat. (in submission).</title>
<date>2008</date>
<contexts>
<context position="18395" citStr="Halberda and Goldman, 2008" startWordPosition="3106" endWordPosition="3109">e (bring me the chromium tray, not the blue one.). However, further experiments show that children can successfully select the correct referent even if such a contrast is not present in the sentence. Many researchers have performed experiments where young subjects are forced to choose between a novel and a familiar object upon hearing a request, such as give me the ball (familiar target), or give me the dax (novel target). In all of the reported experimental results, children can readily pick the correct referent for a familiar or a novel target word in such a setting (Golinkoff et al., 1992; Halberda and Goldman, 2008; Halberda, 2006; Horst and Samuelson, 2008). However, Halberda’s eye-tracking experiments on both adults and pre-schoolers suggest that the processes involved for referent selection in the familiar target situation may be different from those in the novel target situation. In the latter situation, subjects appear to systematically reject the familiar object as the referent of the novel name before mapping the novel object to the novel name. In the familiar target situation, however, there is no need to reject the novel distractor object, because the subject already knows the referent of the t</context>
<context position="23555" citStr="Halberda and Goldman, 2008" startWordPosition="3964" endWordPosition="3967">word from a single ambiguous exposure, some studies have performed retention trials after the referent selection experiments. Often, various referent selection trials are performed in one session, where in each trial a novel object–name pair is introduced among familiar objects. Some of the recently introduced objects are then put together in one last trial, and the subjects are asked to choose the correct referent for one of the (recently heard) novel target words. The majority of the reported experiments show that children can successfully perform the retention task (Golinkoff et al., 1992; Halberda and Goldman, 2008; Halberda, 2006). We simulate a similar retention experiment by training the model as usual. We further present the model with two experimental training items similar to the one used in the NOVEL TARGET condition in the previous section, with different familiar and novel objects and words in each input: dax (REFERENT SELECTION TRIAL 1) {ball, dax} cheem (REFERENT SELECTION TRIAL 2) {pen,cheem} 61 Table 2: Retention of a novel target word from a set of novel objects. 2-OBJECT RETENTION TRIAL rf (dax|dax) rf (dax|cheem) 0.996 ±0.001 0.501 ±0.068 3-OBJECT RETENTION TRIAL rf (dax|dax) rf (dax|che</context>
</contexts>
<marker>Halberda, Goldman, 2008</marker>
<rawString>Halberda, Justin and Julie Goldman 2008. One-trial learning in 2-year-olds: Children learn new nouns in 3 seconds flat. (in submission).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jessica S Horst</author>
<author>Bob McMurray</author>
<author>Larissa K Samuelson</author>
</authors>
<title>Online processing is essential for learning: Understanding fast mapping and word learning in a dynamic connectionist architecture.</title>
<date>2006</date>
<booktitle>In Proc. of CogSci’06.</booktitle>
<contexts>
<context position="4324" citStr="Horst et al., 2006" startWordPosition="678" endWordPosition="681">ime they start to show evidence of fast mapping (which coincides with a sudden burst in their vocabulary), e.g., from associative to referential (Gopnik and Meltzoff, 1987; Reznick and Goldfield, 1992). In contrast, others see fast mapping as a phenomenon that arises from more general processes of learning 57 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 57–64 Manchester, August 2008 and/or communication, which also underlie the impressive rate of lexical acquisition in children (e.g., Clark, 1990; Diesendruck and Markson, 2001; Regier, 2005; Horst et al., 2006; Halberda, 2006). In our previous work (Fazly et al., 2008), we presented a word learning model which proposes a probabilistic interpretation of cross-situational learning, and bootstraps its own partially-learned knowledge of the word meanings to accelerate word learning over time. We have shown that the model can learn reasonable word–meaning associations from child-directed data, and that it accounts for observed learning patterns in children, such as vocabulary spurt, without requiring a developmental change in the underlying learning mechanism. Here, we use this computational model to in</context>
<context position="32388" citStr="Horst et al. (2006)" startWordPosition="5485" endWordPosition="5488">ore familiar context helps the model perform better in a fast mapping task. 5 Related Computational Models The rule-based model of Siskind (1996), and the connectionist model proposed by Regier (2005), both show that learning gets easier as the model is exposed to more input—that is, words heard later are learned faster. These findings confirm that fast mapping may simply be a result of learning more words, and that no explicit change in the underlying learning mechanism is needed. However, these studies do not examine various aspects of fast mapping, such as referent selection and retention. Horst et al. (2006) explicitly test fast mapping in their connectionist model of word learning by performing referent selection and retention tasks. The behaviour of their model matches the child experimental data reported in a study by the same authors (Horst and Samuelson, 2008), but not that of the contradictory findings of other similar experiments. Moreover, the model’s learning capacity is limited, and the fast mapping experiments are performed on a very small vocabulary. Frank et al. (2007) examine fast mapping in their Bayesian model by testing its performance in a novel target referent selection task. H</context>
</contexts>
<marker>Horst, McMurray, Samuelson, 2006</marker>
<rawString>Horst, Jessica S., Bob McMurray, and Larissa K. Samuelson 2006. Online processing is essential for learning: Understanding fast mapping and word learning in a dynamic connectionist architecture. In Proc. of CogSci’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jessica S Horst</author>
<author>Larissa K Samuelson</author>
</authors>
<title>Fast mapping but poor retention by 24-month-old infants.</title>
<date>2008</date>
<journal>Infancy,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="3121" citStr="Horst and Samuelson (2008)" startWordPosition="490" endWordPosition="493">ding Carey and Bartlett’s work, much research has concentrated on providing an explanation for fast mapping, and on examining its role in word learning. These studies also show that children are generally good at referent selection, given a novel target. However, there is not consistent evidence regarding whether children actually learn the novel word from one or a few such exposures (retention). For example, whereas the children in the experiments of Golinkoff et al. (1992) and Halberda (2006) showed signs of nearly-perfect retention of the fast-mapped words, those in the studies reported by Horst and Samuelson (2008) did not (all participating children were close in age range). There are also many speculations about the possible causes of fast mapping. Some researchers consider it as a sign of a specialized (innate) mechanism for word learning. Markman and Wachtel (1988), for example, argue that children fast map because they expect each object to have only one name (mutual exclusivity). Golinkoff et al. (1992) attribute fast mapping to a (hard-coded) bias towards mapping novel names to nameless object categories. Some even suggest a change in children’s learning mechanisms, at around the time they start </context>
<context position="18439" citStr="Horst and Samuelson, 2008" startWordPosition="3112" endWordPosition="3115">one.). However, further experiments show that children can successfully select the correct referent even if such a contrast is not present in the sentence. Many researchers have performed experiments where young subjects are forced to choose between a novel and a familiar object upon hearing a request, such as give me the ball (familiar target), or give me the dax (novel target). In all of the reported experimental results, children can readily pick the correct referent for a familiar or a novel target word in such a setting (Golinkoff et al., 1992; Halberda and Goldman, 2008; Halberda, 2006; Horst and Samuelson, 2008). However, Halberda’s eye-tracking experiments on both adults and pre-schoolers suggest that the processes involved for referent selection in the familiar target situation may be different from those in the novel target situation. In the latter situation, subjects appear to systematically reject the familiar object as the referent of the novel name before mapping the novel object to the novel name. In the familiar target situation, however, there is no need to reject the novel distractor object, because the subject already knows the referent of the target. The difference between these two cond</context>
<context position="24991" citStr="Horst and Samuelson (2008)" startWordPosition="4197" endWordPosition="4200">vel target words: dax (2-OBJECT RETENTION TRIAL) {cheem,dax} After processing the retention input, we compare the referent probabilities rf (dax |cheem) and rf (dax |dax) to see if the model can choose the correct novel object in response to the target word dax. The top panel in Table 2 summarizes the results of this experiment. The model consistently shows a strong preference towards the correct novel object as the referent of the novel target word across all simulations. Unlike studies on referent selection, experimental results for retention have not been consistent across various studies. Horst and Samuelson (2008) perform experiments with two-year-old children involving both referent selection and retention, and report that their subjects perform very poorly at the retention task. One factor that discriminates the experimental setup of Horst and Samuelson from others (e.g., Halberda, 2006) is that, in their retention trials, they put together two recently observed novel objects with a third novel object that has not been seen in any of the experimental sessions before. The authors do not attribute their contradictory results to the presence of this third object, but this factor can in fact affect the p</context>
<context position="32650" citStr="Horst and Samuelson, 2008" startWordPosition="5526" endWordPosition="5529">xposed to more input—that is, words heard later are learned faster. These findings confirm that fast mapping may simply be a result of learning more words, and that no explicit change in the underlying learning mechanism is needed. However, these studies do not examine various aspects of fast mapping, such as referent selection and retention. Horst et al. (2006) explicitly test fast mapping in their connectionist model of word learning by performing referent selection and retention tasks. The behaviour of their model matches the child experimental data reported in a study by the same authors (Horst and Samuelson, 2008), but not that of the contradictory findings of other similar experiments. Moreover, the model’s learning capacity is limited, and the fast mapping experiments are performed on a very small vocabulary. Frank et al. (2007) examine fast mapping in their Bayesian model by testing its performance in a novel target referent selection task. However, the experiment is performed on an artifical corpus. Moreover, since the learning algorithm is non-incremental, the success of the model in referent selection is determined implicitly: each possible word–meaning mapping from the test input is added to the</context>
</contexts>
<marker>Horst, Samuelson, 2008</marker>
<rawString>Horst, Jessica S. and Larissa K. Samuelson 2008. Fast mapping but poor retention by 24-month-old infants. Infancy, 13(2):128–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analyzing Talk, volume 2: The Database. MahWah, NJ: Lawrence Erlbaum Associates, third edition.</title>
<date>2000</date>
<contexts>
<context position="13137" citStr="MacWhinney, 2000" startWordPosition="2205" endWordPosition="2206">icon.3 The likelihood of using a particular name w to refer to a given object m is calculated as: rf (w|m) = p(w|m) p(m|w) - p(w) = p(m) p(m|w) - p(w) =(4) Ew′∈V p(m |w′) - p(w′) where V is the set of all words that the model has seen so far, and p(w) is the relative frequency of w: = freq(w) (5) p(w ) Ew′∈V freq(w′) The referent of a target word w among the present objects, therefore, will be the object m with the highest referent probability rf (w|m). 3 Experimental Setup 3.1 The Input Corpora We extract utterances from the Manchester corpus (Theakston et al., 2001) in the CHILDES database (MacWhinney, 2000). This corpus contains transcripts of conversations with children between the ages of 1; 8 and 3; 0 (years;months). We use the mother’s speech from transcripts of 6 children, remove punctuation and lemmatize the words, and concatenate the corresponding sessions as input data. There is no semantic representation of the corresponding scenes available from CHILDES. Therefore, we automatically construct a scene representation for each utterance, as a set containing the semantic referents of the words in that utterance. We get these from an input-generation lexicon that contains a symbol associated</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>MacWhinney, B. 2000. The CHILDES Project: Tools for Analyzing Talk, volume 2: The Database. MahWah, NJ: Lawrence Erlbaum Associates, third edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Markman</author>
<author>Gwyn F Wachtel</author>
</authors>
<title>Children’s use of mutual exclusivity to constrain the meanings of words.</title>
<date>1988</date>
<pages>20--121</pages>
<publisher>Cognitive Psychology,</publisher>
<contexts>
<context position="3380" citStr="Markman and Wachtel (1988)" startWordPosition="531" endWordPosition="534">ver, there is not consistent evidence regarding whether children actually learn the novel word from one or a few such exposures (retention). For example, whereas the children in the experiments of Golinkoff et al. (1992) and Halberda (2006) showed signs of nearly-perfect retention of the fast-mapped words, those in the studies reported by Horst and Samuelson (2008) did not (all participating children were close in age range). There are also many speculations about the possible causes of fast mapping. Some researchers consider it as a sign of a specialized (innate) mechanism for word learning. Markman and Wachtel (1988), for example, argue that children fast map because they expect each object to have only one name (mutual exclusivity). Golinkoff et al. (1992) attribute fast mapping to a (hard-coded) bias towards mapping novel names to nameless object categories. Some even suggest a change in children’s learning mechanisms, at around the time they start to show evidence of fast mapping (which coincides with a sudden burst in their vocabulary), e.g., from associative to referential (Gopnik and Meltzoff, 1987; Reznick and Goldfield, 1992). In contrast, others see fast mapping as a phenomenon that arises from m</context>
</contexts>
<marker>Markman, Wachtel, 1988</marker>
<rawString>Markman, Ellen M. and Gwyn F. Wachtel 1988. Children’s use of mutual exclusivity to constrain the meanings of words. Cognitive Psychology, 20:121–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W V O Quine</author>
</authors>
<title>Word and Object.</title>
<date>1960</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="7866" citStr="Quine, 1960" startWordPosition="1249" endWordPosition="1250">s of such utterance–scene pairs, our model learns the meaning of each word w as a probability distribution, p(.|w), over the semantic symbols appearing in the corpus. In this representation, p(m|w) is the probability of a symbol m being the meaning of a word w. In the absence of any prior knowledge, all symbols are equally likely to be the meaning of a word. Hence, prior to receiving any usages of a given word, the model assumes a uniform distribution over semantic symbols as its meaning. 2.2 Meaning Probabilities Our model combines probabilistic interpretations of cross-situational learning (Quine, 1960) and of a variation of the principle of contrast (Clark, 1990), through an interaction between two types of probabilistic knowledge acquired and refined over time. Given an utterance–scene pair received at time t, i.e., (U(t), S(t)), the model first calculates an alignment probability a for each w E U(t) and each m E S(t), using the meaning probabilities p(.|w) of all the words in the utterance prior to this time. The model then revises the meaning of the words in U(t) by incorporating the alignment probabilities for the current input pair. This process is repeated for all the input pairs, one</context>
</contexts>
<marker>Quine, 1960</marker>
<rawString>Quine, W.V.O. 1960. Word and Object. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Regier</author>
</authors>
<title>The emergence of words: Attentional learning in form and meaning.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<pages>29--819</pages>
<contexts>
<context position="4304" citStr="Regier, 2005" startWordPosition="676" endWordPosition="677">t around the time they start to show evidence of fast mapping (which coincides with a sudden burst in their vocabulary), e.g., from associative to referential (Gopnik and Meltzoff, 1987; Reznick and Goldfield, 1992). In contrast, others see fast mapping as a phenomenon that arises from more general processes of learning 57 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 57–64 Manchester, August 2008 and/or communication, which also underlie the impressive rate of lexical acquisition in children (e.g., Clark, 1990; Diesendruck and Markson, 2001; Regier, 2005; Horst et al., 2006; Halberda, 2006). In our previous work (Fazly et al., 2008), we presented a word learning model which proposes a probabilistic interpretation of cross-situational learning, and bootstraps its own partially-learned knowledge of the word meanings to accelerate word learning over time. We have shown that the model can learn reasonable word–meaning associations from child-directed data, and that it accounts for observed learning patterns in children, such as vocabulary spurt, without requiring a developmental change in the underlying learning mechanism. Here, we use this compu</context>
<context position="31969" citStr="Regier (2005)" startWordPosition="5415" endWordPosition="5416">ntext. Table 3: Average number of correct mappings and the referent probabilities of target words for two conditions, LOW and HIGH TRAINING. Condition Correct mappings P(target|mtarget) LOW TRAINING %54 0.216±0.04 HIGH TRAINING %90 0.494±0.79 for a target word, as well as the referent probability of a target word for its correct meaning, increase as a result of more training on the context. In other words, a more familiar context helps the model perform better in a fast mapping task. 5 Related Computational Models The rule-based model of Siskind (1996), and the connectionist model proposed by Regier (2005), both show that learning gets easier as the model is exposed to more input—that is, words heard later are learned faster. These findings confirm that fast mapping may simply be a result of learning more words, and that no explicit change in the underlying learning mechanism is needed. However, these studies do not examine various aspects of fast mapping, such as referent selection and retention. Horst et al. (2006) explicitly test fast mapping in their connectionist model of word learning by performing referent selection and retention tasks. The behaviour of their model matches the child expe</context>
</contexts>
<marker>Regier, 2005</marker>
<rawString>Regier, Terry 2005. The emergence of words: Attentional learning in form and meaning. Cognitive Science, 29:819–865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Steven Reznick</author>
<author>Beverly A Goldfield</author>
</authors>
<title>Rapid change in lexical development in comprehension and production.</title>
<date>1992</date>
<journal>Developmental Psychology,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="3907" citStr="Reznick and Goldfield, 1992" startWordPosition="616" endWordPosition="619">nsider it as a sign of a specialized (innate) mechanism for word learning. Markman and Wachtel (1988), for example, argue that children fast map because they expect each object to have only one name (mutual exclusivity). Golinkoff et al. (1992) attribute fast mapping to a (hard-coded) bias towards mapping novel names to nameless object categories. Some even suggest a change in children’s learning mechanisms, at around the time they start to show evidence of fast mapping (which coincides with a sudden burst in their vocabulary), e.g., from associative to referential (Gopnik and Meltzoff, 1987; Reznick and Goldfield, 1992). In contrast, others see fast mapping as a phenomenon that arises from more general processes of learning 57 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 57–64 Manchester, August 2008 and/or communication, which also underlie the impressive rate of lexical acquisition in children (e.g., Clark, 1990; Diesendruck and Markson, 2001; Regier, 2005; Horst et al., 2006; Halberda, 2006). In our previous work (Fazly et al., 2008), we presented a word learning model which proposes a probabilistic interpretation of cross-situational learning, and boots</context>
</contexts>
<marker>Reznick, Goldfield, 1992</marker>
<rawString>Reznick, J. Steven and Beverly A. Goldfield 1992. Rapid change in lexical development in comprehension and production. Developmental Psychology, 28(3):406–413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffery Mark Siskind</author>
</authors>
<title>A computational study of cross-situational techniques for learning word-tomeaning mappings.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--39</pages>
<contexts>
<context position="31914" citStr="Siskind (1996)" startWordPosition="5406" endWordPosition="5407">ted data as we want to control the familiarity of the context. Table 3: Average number of correct mappings and the referent probabilities of target words for two conditions, LOW and HIGH TRAINING. Condition Correct mappings P(target|mtarget) LOW TRAINING %54 0.216±0.04 HIGH TRAINING %90 0.494±0.79 for a target word, as well as the referent probability of a target word for its correct meaning, increase as a result of more training on the context. In other words, a more familiar context helps the model perform better in a fast mapping task. 5 Related Computational Models The rule-based model of Siskind (1996), and the connectionist model proposed by Regier (2005), both show that learning gets easier as the model is exposed to more input—that is, words heard later are learned faster. These findings confirm that fast mapping may simply be a result of learning more words, and that no explicit change in the underlying learning mechanism is needed. However, these studies do not examine various aspects of fast mapping, such as referent selection and retention. Horst et al. (2006) explicitly test fast mapping in their connectionist model of word learning by performing referent selection and retention tas</context>
</contexts>
<marker>Siskind, 1996</marker>
<rawString>Siskind, Jeffery Mark 1996. A computational study of cross-situational techniques for learning word-tomeaning mappings. Cognition, 61:39–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Theakston</author>
<author>E V Lieven</author>
<author>J M Pine</author>
<author>C F Rowland</author>
</authors>
<title>The role of performance limitations in the acquisition of verb-argument structure: An alternative account.</title>
<date>2001</date>
<journal>Journal of Child Language,</journal>
<pages>28--127</pages>
<contexts>
<context position="13094" citStr="Theakston et al., 2001" startWordPosition="2197" endWordPosition="2200"> the mapping between m and other words in the lexicon.3 The likelihood of using a particular name w to refer to a given object m is calculated as: rf (w|m) = p(w|m) p(m|w) - p(w) = p(m) p(m|w) - p(w) =(4) Ew′∈V p(m |w′) - p(w′) where V is the set of all words that the model has seen so far, and p(w) is the relative frequency of w: = freq(w) (5) p(w ) Ew′∈V freq(w′) The referent of a target word w among the present objects, therefore, will be the object m with the highest referent probability rf (w|m). 3 Experimental Setup 3.1 The Input Corpora We extract utterances from the Manchester corpus (Theakston et al., 2001) in the CHILDES database (MacWhinney, 2000). This corpus contains transcripts of conversations with children between the ages of 1; 8 and 3; 0 (years;months). We use the mother’s speech from transcripts of 6 children, remove punctuation and lemmatize the words, and concatenate the corresponding sessions as input data. There is no semantic representation of the corresponding scenes available from CHILDES. Therefore, we automatically construct a scene representation for each utterance, as a set containing the semantic referents of the words in that utterance. We get these from an input-generatio</context>
</contexts>
<marker>Theakston, Lieven, Pine, Rowland, 2001</marker>
<rawString>Theakston, A. L., E. V. Lieven, J. M. Pine, and C. F. Rowland 2001. The role of performance limitations in the acquisition of verb-argument structure: An alternative account. Journal of Child Language, 28:127–152.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>