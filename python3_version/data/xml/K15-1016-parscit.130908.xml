<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000860">
<title confidence="0.999159">
Instance Selection Improves Cross-Lingual Model Training for
Fine-Grained Sentiment Analysis
</title>
<author confidence="0.999528">
Roman Klinger*$ Philipp Cimiano$
</author>
<affiliation confidence="0.9975885">
*Institute for Natural Language Processing $Semantic Computing Group, CIT-EC
University of Stuttgart Bielefeld University
</affiliation>
<address confidence="0.943917">
70569 Stuttgart, Germany 33615 Bielefeld, Germany
</address>
<email confidence="0.77383">
roman.klinger@ims.uni-stuttgart.de
cimiano@cit-ec.uni-bielefeld.de
</email>
<sectionHeader confidence="0.980021" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999811">
Scarcity of annotated corpora for many
languages is a bottleneck for training fine-
grained sentiment analysis models that can
tag aspects and subjective phrases. We pro-
pose to exploit statistical machine transla-
tion to alleviate the need for training data
by projecting annotated data in a source
language to a target language such that a
supervised fine-grained sentiment analysis
system can be trained. To avoid a nega-
tive influence of poor-quality translations,
we propose a filtering approach based on
machine translation quality estimation mea-
sures to select only high-quality sentence
pairs for projection. We evaluate on the
language pair German/English on a corpus
of product reviews annotated for both lan-
guages and compare to in-target-language
training. Projection without any filtering
leads to 23 % F1 in the task of detecting
aspect phrases, compared to 41 % F1 for
in-target-language training. Our approach
obtains up to 47 % F1. Further, we show
that the detection of subjective phrases is
competitive to in-target-language training
without filtering.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997822314285714">
An important task in fine-grained sentiment anal-
ysis and opinion mining is the extraction of men-
tioned aspects, evaluative subjective phrases and
the relation between them. For instance, in the
sentence
“I really like the display but the battery
seems weak to me.”
the task is to detect evaluative (subjective) phrases
(in this example “really like” and “seems weak”)
and aspects (“display” and “battery”) as well as
their relation (that “really like” refers to “display”
and “seems weak” refers to “battery”).
Annotating data for learning a model to extract
such detailed information is a tedious and time-
consuming task. Therefore, given the scarcity of
such annotated corpora in most languages, it is in-
teresting to generate models which can be applied
on languages without manually created training
data. In this paper, we perform annotation pro-
jection, which is one of the two main categories
for cross-language model induction (next to direct
model transfer (Agi´c et al., 2014)).
Figure 1 shows an example of a sentence to-
gether with its automatically derived translation
(source language on top, target language on bot-
tom) and the alignment between both. Such an
alignment can be used to project annotations across
languages, e. g., from a source to target language,
to produce data to train a system for the target lan-
guage. As shown in the example, translation errors
as well as alignment errors can occur. When using
a projection-based approach, the performance of a
system on the target language crucially depends on
the quality of the translation and the alignment. In
this paper we address two questions:
</bodyText>
<listItem confidence="0.999021714285714">
• What is the performance on the task when
training data for the source language is pro-
jected into a target language, compared to an
approach where training data for the target
language is available?
• Can the performance be increased by selecting
only high-quality translations and alignments?
</listItem>
<bodyText confidence="0.967033">
Towards answering these questions, we present the
following contributions:
</bodyText>
<page confidence="0.988687">
153
</page>
<note confidence="0.9857055">
Proceedings of the 19th Conference on Computational Language Learning, pages 153–163,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.9146734">
Es gibt mit Sicherheit bessere Maschinen , aber die bietet das beste Preis-Leistungs-Verhaltnis .
There are certainly better
machines
, but o↵ers the best
price-performance ratio .
</figure>
<figureCaption confidence="0.993023">
Figure 1: Example for the projection of an annotation from the source language to the target language.
</figureCaption>
<bodyText confidence="0.965638">
The translation has been generated with the Google translate API (https://cloud.google.com/translate/).
The alignment is induced with FastAlign (Dyer et al., 2013).
</bodyText>
<listItem confidence="0.981988666666667">
• We propose to use a supervised approach to in-
duce a fine-grained sentiment analysis model
to predict aspect and subjective phrases on
some target language, given training data in
some source language. This approach relies
on automatic translation of source training
data and projection of annotations to the tar-
get language data.
• We present an instance selection method that
</listItem>
<bodyText confidence="0.97131824">
only selects sentences with a certain trans-
lation quality. For this, we incorporate dif-
ferent measures of translation and alignment
confidence. We show that such an instance
selection method leads to increased perfor-
mance compared to a system without instance
selection for the prediction of aspects. Re-
markably, for the prediction of aspects the
performance is comparable to an upper base-
line using manually annotated target language
data for training (we refer to the latter setting
as in-target-language training).
• In contrast, for the prediction of subjective
phrases, we show that, while a competitive re-
sult compared to target language training can
be observed when training with the projected
training data, there is no beneficial effect of
the filtering.
In the following, we describe our methodology
in detail, including the description of the machine
translation, annotation projection, and quality esti-
mation methods (Section 2), and present the evalu-
ation on manually annotated data (Section 3). Re-
lated work is discussed in Section 4. We conclude
with Section 5 and mention promising future steps.
</bodyText>
<sectionHeader confidence="0.998523" genericHeader="introduction">
2 Methods
</sectionHeader>
<subsectionHeader confidence="0.6899055">
2.1 Supervised Model for Aspect and
Subjective Phrase Detection
</subsectionHeader>
<bodyText confidence="0.999980634146342">
We use a supervised model induced from training
data to detect aspect phrases, subjective (evaluative)
phrases and their relations. The structure follows
the proposed pipeline approach by Klinger and
Cimiano (2013).1 However, in contrast to their
work, we focus on the detection of phrases only,
and exploit the detection of relations only during
inference, such that the detection of relations has
an effect on the detection of phrases, but is not
evaluated directly.
The phrase detection follows the idea of semi-
Markov conditional random fields (Sarawagi and
Cohen, 2004; Yang and Cardie, 2012) and models
phrases as spans over tokens as variables. Factor
templates for spans of type aspect and subjective
take into account token strings, prefixes, suffixes,
the inclusion of digits, and part-of-speech tags,
both as full string and as bigrams, for the spans
and their vicinity. In addition, the length of the
span is modeled by cumulative binning. The rela-
tion template indicates how close an aspect is to a
subjective phrase based on token distance and on
the length of the shortest path in the dependency
tree. The edge names of the shortest path are also
included as features. It is further checked if no
other noun than the aspect is close to the subjective
phrase.
Inference during training and testing is done via
Markov Chain Monte Carlo (MCMC). In each sam-
pling step (with options of adding a span, removing
a span, adding an aspect as target to a subjective
phrase), the respective factors lead to an associated
model score. The model parameters are adapted
based on sample rank (Wick et al., 2011) using an
objective function which computes the fraction of
correctly predicted tokens in a span. For details
on the model configuration and its implementation
in FACTORIE (McCallum et al., 2009), we refer to
the description in the original paper (Klinger and
Cimiano, 2013). The objective function to evaluate
a span r during training is
</bodyText>
<equation confidence="0.793161333333333">
Jr n gJ
JgJ
1https://bitbucket.org/rklinger/jfsa
f(r) = max
g∈s
α - Jr\gJ ,
</equation>
<page confidence="0.998733">
154
</page>
<bodyText confidence="0.998263">
where g is the set of all gold spans, and |r n g|
is the number of tokens shared by gold and pre-
dicted span and |r\g |the number of predicted to-
kens which are not part of the gold span. The
parameter α is set to 0.1 as in the original paper.2.
The objective for the predictions in a whole sen-
tence s containing spans is f(s) = Er∈s f(r).
This model does not take into account language-
specific features and can therefore be trained for
different languages. In the following, we explain
our procedure for inducing a model for a target
language for which no annotations are available.
</bodyText>
<subsectionHeader confidence="0.999109">
2.2 Statistical Machine Translation and
Annotation Projection
</subsectionHeader>
<bodyText confidence="0.99992509375">
Annotating textual corpora with fine-grained sen-
timent information is a time-consuming and there-
fore costly process. In order to adapt a model to a
new domain and to a new language, corresponding
training data is needed. In order to circumvent the
need for additional training data when addressing
a new language, we project training data automati-
cally from a source to a target language. As input
to our approach we require a corpus annotated for
some source language and a translation from the
source to a target language. As the availability
of a parallel training corpus cannot be assumed
in general, we use statistical machine translation
(SMT) methods, relying on phrase-based transla-
tion models that use large amounts of parallel data
for training (Koehn, 2010).
While using an open-source system such as
Moses3 would have been an option, we note that
the quality would be limited by whether the sys-
tem can be trained on a representative corpus. A
standard dataset that SMT systems are trained on
is EuroParl (Koehn, 2005). EuroParl covers 21 lan-
guages and contains 1.920.209 sentences for the
pair German/English. The corpus includes only 4
sentences with the term “toaster”, 12 with “knives”
(mostly in the context of violence), 6 with “dish-
washer” (in the context of regulations) and 0 with
“trash can”. The terms “camera” and “display” are
more frequent, with 208 and 1186 mentions, respec-
tively, but they never occur together.4 The corpus
is thus not representative for product reviews as we
consider in this paper.
</bodyText>
<footnote confidence="0.905441333333333">
2Note that the learning is independent from the actual value
for all 0 &lt; α &lt; (max c i
9E orpus I 9I)
3www.statmt.org/moses/
4These example domains are taken from the USAGE cor-
pus (Klinger and Cimiano, 2014), which is used in Section 3.
</footnote>
<bodyText confidence="0.999875185185185">
Thus, we opt for using a closed translation sys-
tem that is trained on larger amounts of data, that is
Google Translate, through the available API5. The
alignment is then computed as a post processing
step relying on FastAlign (Dyer et al., 2013), a
reparametrization of IBM Model 2 with a reduced
set of parameters. It is trained in an unsupervised
fashion via expectation maximization.
Projecting the annotations from the source to the
target language works as follows: given an anno-
tated sentence in the source language si, ... , sn
and some translation of this sentence ti, ... , tm
into the target language, we induce an inductive
mapping a : [1... n] —* [1... m] using FastAlign.
For a source language phrase si,j = si, ... , sj we
refer by a(si,j) to the set of tokens that some to-
ken in si,j has been aligned to, that is: a(si,j) =
Ui&lt;k&lt;j{a(k)}. Note that the tokens in a(si,j) are
not necessarily consecutive, therefore the annota-
tion in the target language is defined as the minimal
sequence including all tokens tk E a(si,j), i. e., the
most left and most right tokens define the span of
the target annotation.
This procedure leads to the same number of span
annotations in source and target language with the
only exception that we exclude projected annota-
tions for which |n − m |&gt; 10.
</bodyText>
<subsectionHeader confidence="0.99911">
2.3 Quality Estimation-based Instance
Filtering
</subsectionHeader>
<bodyText confidence="0.996854916666667">
The performance of an approach relying on pro-
jection of training data from a source to a target
language and using this automatically projected
data to train a supervised model crucially depends
on the quality of the translations and alignments.
In order to reduce the impact of spurious transla-
tions, we filter out low-quality sentence pairs. To
estimate this quality, we take three measures into
consideration (following approaches described by
Shah and Specia (2014), in addition to a manual
assessment of the translation quality as an upper
baseline):
</bodyText>
<listItem confidence="0.517307571428571">
1. The probability of the sentence in the source
language given a language model build on
unannotated text in the source language (mea-
suring if the language to be translated is typi-
cal, referred to as Source LM).
2. The probability of the machine translated sen-
tence given a language model built on unanno-
</listItem>
<footnote confidence="0.971141">
5https://cloud.google.com/translate/
</footnote>
<page confidence="0.992401">
155
</page>
<table confidence="0.9994852">
# reviews
en de
coffee machine 75 108
cutlery 49 72
microwave 100 100
toaster 100 4
trash can 100 99
vacuum cleaner 51 140
washing machine 49 88
dish washer 98 0
</table>
<tableCaption confidence="0.78776">
Table 1: Frequencies of the corpus used in our
experiments (Klinger and Cimiano, 2014).
</tableCaption>
<bodyText confidence="0.9992734">
tated text in the target language (measuring if
the translation is typical, referred to as Target
LM).
3. The likelihood that the alignment is correct,
directly computed on the basis of the align-
ment probability (referred to as Alignment):
p(e I f) = Hi=1m p(ei I f, m, n), where e
and f are source and target sentences and m
and n denote the sentence lengths (Dyer et al.,
2013, Eq. 1f.).
For building the language models, we employ the
toolkit SRILM (Stolcke, 2002; Stolcke et al., 2011).
The likelihood for the alignment as well as the
language model probability are normalized by the
number of tokens in the sentence.
</bodyText>
<sectionHeader confidence="0.999858" genericHeader="background">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999943">
3.1 Corpus and Setting
</subsectionHeader>
<bodyText confidence="0.999988">
The proposed approach is evaluated on the lan-
guage pair German/English in both directions (pro-
jecting German annotations into an automatically
generated English corpus and testing on English
annotations and vice versa). As a resource, we
use the recently published USAGE corpus (Klinger
and Cimiano, 2014), which consists of 622 En-
glish and 611 German product reviews from http:
//www.amazon.com/ and http://www.amazon.de/,
respectively. The reviews are on coffee machines,
cutlery sets, microwaves, toasters, trash cans, vac-
uum cleaners, washing machines, and dish washers.
Frequencies of entries in the corpus are summa-
rized in Table 1. Each review has been annotated
by two annotators. We take into account the data
generated by the first annotator in this work to
avoid the design of an aggregation procedure. The
corpus is unbalanced between the product classes.
The average numbers of annotated aspects in each
review in the German corpus (10.4) is smaller than
in English (13.7). The average number of sub-
jective phrases is more similar with 8.6 and 8.3,
respectively. The total number of aspects is 8545
for English and 6340 in German, the number of
subjective phrases is 5321 and 5086, respectively.
The experiments are performed in a leave-one-
domain-out setting, e. g., testing on coffee machine
reviews is based on a model trained on all other
products except coffee machines. This holds for
the cross-language and the in-target-language train-
ing results and leads therefore to comparable set-
tings. We use exact match precision, recall and
F1-measure for evaluation. However, it should be
noted that partial matching scores are also com-
monly applied in fine-grained sentiment analysis
due to the fact that boundaries of annotations can
differ substantially between annotators. For sim-
plicity, we limit ourselves to the more strict evalua-
tion measure.
The language models are trained on 7,413,774
German and 9,650,633 English sentences sampled
from Amazon product reviews concerning the prod-
uct categories in the USAGE corpus. The FastAlign
model is trained on the EuroParl corpus and the
automatically translated USAGE corpus in both di-
rections (German translated to English and English
translated to German).
</bodyText>
<subsectionHeader confidence="0.931762">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999954529411765">
We evaluate and compare the impact of the three
automatic quality estimation methods and compare
them to a manual sentence-based judgement for
the language projection from German to English
(testing on English). The manual judgement was
performed by assigning values ranging from 0 (not
understandable), over 1 and 2 (slightly understand-
able) to 8 (some flaws in translation), 9 (minor
flaws in translation) to 10 (perfect translation).6
Figure 2 shows the results for all four methods
(including manual quality assessment) from Ger-
man to English for the product category of coffee
machines compared to in-target-language training
results. The x-axis corresponds to different values
for the filtering threshold. Thus, when increasing
the threshold, the number of sentences used for
training decreases. For all quality estimation meth-
</bodyText>
<footnote confidence="0.9089835">
6This annotated data is available at http://www.
romanklinger.de/translation-quality-review-corpus
</footnote>
<page confidence="0.989999">
156
</page>
<figure confidence="0.99947795">
0.5
F� measure
0.4
0.3
0.2
0.1
0
0 2 4 6 8 10
Manual
0.5
0.4
0.3
0.2
0.1
0
-5 -4.5 -4 -3.5 -3 -2.5 -2 -1.5 -1
Alignment
F� measure
0.5
0.4
0.3
0.2
0.1
0
-8 -7 -6 -5 -4 -3 -2 -1 0
Source LM
0.5
0.4
0.3
0.2
0.1
0
-8 -7 -6 -5 -4 -3 -2 -1
Target LM
F� measure
F� measure
cross aspect
cross subjective
inlang. aspect
inlang. subjective
</figure>
<figureCaption confidence="0.968297">
Figure 2: Complete results for the reviews for coffee machines for the projection direction German to
English.
</figureCaption>
<bodyText confidence="0.99781125925926">
ods except for the language model for the source
language, the performance on the target language
increases significantly for the prediction of aspect
phrases. The English in-target-language training
performance represents an upper baseline, result-
ing from training the system on manually annotated
data for the target language (F1 = 0.42). Without
any instance filtering, relying on all automatically
produced translations to induce projected annota-
tions for the target language, an F1 measure of
0.21 is reached. With filtering based on the manu-
ally assigned translation quality estimation, a result
of F1 = 0.43 is reached. Using the alignment as
quality score for filtering, the best result obtained
is F1 = 0.48. However, results start decreasing
from this threshold value on, which is likely due
to the fact that the positive effect of instance filter-
ing is outweighed by the performance drop due to
training with a smaller dataset. The filtering based
on the target language model leads to F1 = 0.42,
while the source language model cannot filter the
training instances such that the performance in-
creases over the initial value.
Surprisingly, instance filtering has no impact on
the detection of subjective phrases. Without any
filtering, for the prediction of subjective phrases we
get an F1 of 0.42, which is close to the performance
of in-target-language training of F1 = 0.49. For
the case of phrase detection, the difference between
training with all data (21%) and in-target-language
training (42%) is considerably higher. Decreas-
ing the size of the training set by filtering only
decreases the F1 measure.
Figure 3 shows the macro-average results sum-
marizing the different domains in precision, recall
and F1 measure. The thresholds for the filtering
have been fixed to the best result of the product
coffee machine for all products. The manual qual-
ity estimation as well as the alignment and target
language model lead to comparable (or superior)
results compared to target language training for
aspect detection. This holds for nearly all prod-
uct domains, only for trash cans and cutlery the
performance achieved by filtering is slightly lower
for the direction German-to-English. The initial
performance for the whole data set is on average
higher for the projection direction English to Ger-
man; therefore the values for the source language
model are comparably higher than for the other
projection direction.
For the aspect detection, all filtering methods
except using the source language model lead to an
improvement over the baseline (without filtering)
that is significant according to a Welch t-test (α =
</bodyText>
<page confidence="0.949324">
157
</page>
<figure confidence="0.974499144230769">
German to English, Aspects
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
�
0.43 0.450.44 0.43
�
0.41
0.39
0.38
0.35
0.31
0.52
� �
0.47
0.61
0.57 0.57
0.23
0.23
0.15
0.15
Manual Alignment Source LM Target LM in-target-lang no filter
English to German, Aspects
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.57
0.64 0.66 0.61
0.58
0.27
0.36
0.20
0.31
0.32
0.40
�
0.25
0.37
�
0.19
0.29
Alignment Source LM Target LM in-target-lan no filter
German to English, Subjective
0.7
0.56
�
0.48
0.47
0.40
0.40
0.40
0.40
0.40
0.39
0.34
0.34
0.34
0.34
0.34
0.48 0.47 0.48 0.49
0.6
0.5
0.4
0.3
0.2
0.1
0
Manual Alignment Source LM Target LM in-target-lang no filter
������� �� ������� ����������
���
0.42
0.53 �
0.49 0.50 0.50 0.49
0.47
0.34
0.35
0.35
0.35
0.26
0.27
0.27
0.27
���
���
���
���
���
���
�
�������
��������� �������
������ �� ����
������ �� ��������������
������ �� �����
</figure>
<figureCaption confidence="0.851037">
Figure 3: Macro-average results of cross-language training over the different domains showing precision,
recall and F1 measure. Significant differences of the F1 results to the no-filter-baseline are marked with a
star (Welch t-test comparing the different separate domain results, p &lt; 0.05).
</figureCaption>
<bodyText confidence="0.999777636363636">
0.05). For English to German, in-target-language
training and the target language model filtering
provide improved results over the baseline that are
significant. For subjective phrase detection, the
in-language training is significantly better than the
baseline.
It is notable that in all experiments the model’s
performance without filtering is mainly limited in
recall, which drops the performance in F1. In-
stance filtering therefore has mainly an effect on
the recall.
</bodyText>
<subsectionHeader confidence="0.973101">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.99999496875">
Our results show that an approach based on au-
tomatic training data projection across languages
is feasible and provides competitive results com-
pared to training on manually annotated target lan-
guage data. We have in particular quantified the
loss of performance of such an automatic approach
compared to using a system trained on manually
annotated data in the target language. We have
shown that the performance of aspect detection
of a system using all available projected training
data yields a drop of ≈ 50 % in F1-measure com-
pared to a model trained using manually annotated
data in the target language. The instance filter-
ing approaches in which only the sentences with
highest quality are selected to project training data
to the target language using a threshold has a sig-
nificant positive impact on the performance of a
model trained on automatically projected training
data when predicting aspect phrases on the target
language, increasing results from 23 % to 47 % on
average. Our approach relying on instance filtering
comes close to results produced by a system trained
on manually annotated data for the target language
on the task of predicting aspect phrases.
In contrast to these results for the aspect phrase
recognition, the impact of filtering training in-
stances is negligible for the detection of subjective
phrases. The highest performance is achieved with
the full set of training instances. Therefore, it may
be concluded that for aspect name detection, high
quality training is crucial. For subjective phrase
detection, a greater training set is crucial. In con-
trast to aspect recognition, the drop in subjective
phrase recognition performance is comparatively
low when training on all instances.
Filtering translations by manually provided trans-
lation scores (as an upper baseline for the filtering)
yields comparable results to using the alignment
and the language model on the target language. Us-
ing the language model on the source language for
filtering does not lead to any improvement. Predict-
ing the quality of translation relying on the proba-
bility of the source sentence via a source language
model therefore seems not to be a viable approach
on the task in question. Using the target language
model as a filter leads to the most consistent results
and is therefore to be preferred over the source
language model and the alignment score.
Including more presumably noisy instances by
using a smaller filtering threshold leads to a de-
creased recall throughout all methods in aspect de-
tection and to a lesser extent for subjective phrase
detection. The precision is affected to a smaller
degree. This can as well be observed in the number
of predictions the models based on different thresh-
olds generate: While the number of true positive
aspects for the coffee machine subdomain is 1100,
only 221 are predicted with a threshold of the man-
ual quality assignment of 0. However, a treshold
of 9 leads to 560 predictions and a threshold of 10
to 1291. This effect can be observed for subjective
phrases as well. It increases from 465 to 827 while
the gold number is 676. These observations hold
for all filtering methods analogously.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999794074074074">
In-target-language training approaches for fine-
grained sentiment analysis include those targeting
the extraction of phrases or modelling it as text
classification (Choi et al., 2010; Johansson and
Moschitti, 2011; Yang and Cardie, 2012; Hu and
Liu, 2004; Li et al., 2010; Popescu and Etzioni,
2005; Jakob and Gurevych, 2010b). Such models
are typically trained or optimized on manually an-
notated data (Klinger and Cimiano, 2013; Yang and
Cardie, 2012; Jakob and Gurevych, 2010a; Zhang
et al., 2011). The necessary data, at least containing
fine-grained annotations for aspects and subjective
phrases instead of only an overall polarity score,
are mainly available for the English language to a
sufficient extent. Popular corpora used for training
are for instance the J.D. Power and Associates Sen-
timent Corpora (Kessler et al., 2010) or the MPQA
corpora (Wilson and Wiebe, 2005).
Non-English resources are scarce. Examples are
a YouTube corpus consisting of English and Italian
comments (Uryupina et al., 2014), a not publicly
available German Amazon review corpus of 270
sentences (Boland et al., 2013), in addition to the
USAGE corpus (Klinger and Cimiano, 2014) we
have used in this work, consisting of German and
English reviews. The (non-fine-grained annotated)
Spanish TASS corpus consists of Twitter messages
(Saralegi and Vicente, 2012). The “Multilingual
Subjectivity Analysis Gold Standard Data Set” fo-
cuses on subjectivity in the news domain (Balahur
and Steinberger, 2009). A Chinese corpus anno-
tated at the aspect and subjective phrase level is
described by Zhao et al. (2014).
There has not been too much work on ap-
proaches to transfer a model either directly or via
annotation projection in the area of sentiment anal-
ysis. One example is based on sentence level anno-
tations which are automatically translated to yield
a resource in another language. This approach has
been proven to work well across several languages
(Banea et al., 2010; Mihalcea et al., 2007; Balahur
and Turchi, 2014). Recent work approached multi-
lingual opinion mining on the above-mentioned
multi-lingual Youtube corpus with tree kernels pre-
dicting the polarity of a comment and whether it
concerns the product or the video in which the
product is featured. (Severyn et al., 2015). Brooke
et al. (2009) compare dictionary and classification
transfer from English to Spanish in a similar classi-
fication setting.
While cross-lingual annotation projection has
been investigated in the context of polarity com-
putation, we are only aware of two approaches
exploiting cross-lingual annotation projection on
</bodyText>
<page confidence="0.997583">
159
</page>
<bodyText confidence="0.999945445652174">
the task of identifying aspects specifically with an
evaluation on manually annotated data in more than
one language. The CLOpinionMiner (Zhou et al.,
2015) uses an English data set which is transfered
to Chinese. Models are further improved by co-
training. Xu et al. (2013) perform self-training
based on a projected corpus from English to Chi-
nese to detect opinion holders. Due to the lack
of existing manually annotated resources, to our
knowledge no cross-language projection approach
for fine-grained annotation at the level of aspect
and subjective phrases has been proposed before.
The projection of annotated data sets has been
investigated in a variety of applications. Early work
includes an approach to the projection of part-of-
speech tags and noun phrases (Yarowsky et al.,
2001; Yarowsky and Ngai, 2001) and parsing infor-
mation (Hwa et al., 2005) on a parallel corpus. Es-
pecially in syntactic and semantic parsing, heuris-
tics to remove or correct spuriously projected an-
notations have been developed (Pad´o and Lapata,
2009; Agi´c et al., 2014). It is typical for these
approaches to be applied on existing parallel cor-
pora (one counter example is the work by Basili et
al. (2009) who perform postprocessing of machine
translated resources to improve the annotation for
training semantic role labeling models). In cases
in which no such parallel resources are available
containing pertinent annotations, models can be
transfered after training. Early work includes a
cross-lingual parser adaption (Zeman and Resnik,
2008). A recent example is the projection of a
metaphor detection model using a bilingual dictio-
nary (Tsvetkov et al., 2014). A combination of
model transfer and annotation projection for depen-
dency parsing has been proposed by Kozhevnikov
and Titov (2014).
To improve quality of the overall corpus of pro-
jected annotations, the selection of data points for
dependency parsing has been studied (Søgaard,
2011). Similarly, Axelrod et al. (2011) improve
the average quality of machine translation systems
by selection of promising training examples and
show that such a selection approach has a positive
impact. Related to the latter, a generic instance
weighting scheme has been proposed for domain
adaptation (Jiang and Zhai, 2007).
Other work has attempted to exploit information
available in multiple languages to induce a model
for a language for which sufficient training data is
not available. For instance, universal tag sets take
advantage of annotations that are aligned across
languages (Snyder et al., 2008). Delexicalization
allows for applying a model to other languages
(McDonald et al., 2011).
Focusing on cross-lingual sentiment analysis,
joint training of classification models on multiple
languages shows an improvement over separated
models. Balahur and Turchi (2014) analyzed the
impact of using different machine translation ap-
proaches in such settings. Differences in sentiment
expressions have been analyzed between English
and Dutch (Bal et al., 2011). Co-training with non-
annotated corpora has been shown to yield good
results for Chinese (Wan, 2009). Ghorbel (2012)
analyzed the impact of automatic translation on
sentiment analysis.
Finally, SentiWordNet has been used for multi-
lingual sentiment analysis (Denecke, 2008). Build-
ing dictionaries for languages with scarce resources
can be supported by bootstrapping approaches
(Banea et al., 2008).
Estimating the quality of machine translation can
be understood as a ranking problem and thus be
modeled as regression or classification. An impor-
tant research focus is on investigating the impact of
different features on predicting translation quality.
For instance, sentence length, the output probabil-
ity, number of unknown words of a target language
as well as parsing-based features have been used
(Avramidis et al., 2011). The alignment context
can also be taken into account (Bach et al., 2011).
An overview on confidence measures for machine
translation is for instance provided by Ueffing et al.
(2003). The impact of different features has been
analyzed by Shah et al. (2013). A complete system
and framework for quality estimation (including
a list of possible features) is QuEst (Specia et al.,
2013).
For an overview of other cross-lingual applica-
tions and methods, we refer to Bikel and Zitouni
(2012).
</bodyText>
<sectionHeader confidence="0.979155" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99996525">
We have presented an approach that alleviates the
need of training data for a target language when
adapting a fine-grained sentiment analysis system
to a new language. Our approach relies on training
data available for a source language and on auto-
matic machine translation, in particular statistical
methods, to project training data to the target lan-
guage, thus creating a training corpus on which
</bodyText>
<page confidence="0.991878">
160
</page>
<bodyText confidence="0.999977018518519">
a supervised sentiment analysis approach can be
trained on. We have in particular shown that our
results are competitive to training with manually
annotated data in the target language, both for the
prediction of aspect phrases as well as subjective
phrases. We have further shown that performance
for aspect detection can be almost doubled by es-
timating the quality of translations and selecting
only the translations with highest quality for train-
ing. Such an effect cannot be observed in the pre-
diction of subjective phrases, which nevertheless
delivers results comparable to training using tar-
get language data using all automatically projected
training data. Predicting translation quality by both
the alignment probability and the target language
model probability have been shown to deliver good
results, while an approach exploiting source lan-
guage model probability does not perform well.
Our hypothesis for the failure of translation filter-
ing for the prediction of subjective phrases is that
translation quality for subjective phrases is gener-
ally higher as their coverage in standard parallel
corpora is reasonable and they are often domain-
independent. A further possible explanation is that
subjective phrases have a more complex structure
(for instance, their average length is 2.38 tokens
in English and 2.57 tokens in German, while the
aspect length is 1.6 and 1.3, respectively). There-
fore, translation as well as filtering might be more
challenging. These hypotheses should be verified
and investigated further in future work.
Further work should also be devoted to the in-
vestigation of other quality estimation procedures,
in particular combinations of those investigated in
this paper. Preliminary experiments have shown
that the correlation between the filters incorporated
in this paper is low. Thus, their combination could
indeed have an additional impact. Similarly, the
projection quality can be affected by the transla-
tion itself and by the alignment. These two aspects
should be analyzed separately.
In addition, instead of Boolean filtering (using
an instance or not), weighting the impact of the in-
stance in the learning procedure might be beneficial
as lower-quality instances can still be taken into
account, although with a lower impact proportional
to their corresponding score or probability.
In addition to the presented approach of pro-
jecting annotations, a comparison to directly trans-
ferring a trained model across languages would
allow for a deeper understanding of the processes
involved. Finally, it is an important and promis-
ing step to apply the presented methods on other
languages.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999808142857143">
We thank Nils Reiter, John McCrae, and Matthias
Hartung for fruitful discussions. Thanks to Lu-
cia Specia for her comments on quality estimation
methods in our work. We thank Chris Dyer for
his help with FastAlign. Thanks to the anonymous
reviewers for their comments. This research was
partially supported by the “It’s OWL” project (“In-
telligent Technical Systems Ostwestfalen-Lippe”,
http://www.its-owl.de/), a leading-edge cluster of
the German Ministry of Education and Research
and by the Cluster of Excellence Cognitive Interac-
tion Technology ‘CITEC’ (EXC 277) at Bielefeld
University, which is funded by the German Re-
search Foundation (DFG).
</bodyText>
<sectionHeader confidence="0.993289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.905174333333333">
ˇZeljko Agi´c, J¨org Tiedemann, Danijela Merkler, Si-
mon Krek, Kaja Dobrovoljc, and Sara Moze. 2014.
Cross-lingual dependency parsing of related lan-
guages with rich morphosyntactic tagsets. In Work-
shop on Language Technology for Closely Related
Languages and Language Variants, EMNLP.
Eleftherios Avramidis, Maja Popovi´c, David Vilar, and
Aljoscha Burchardt. 2011. Evaluate with confi-
dence estimation: Machine ranking of translation
outputs using grammatical features. In Workshop on
Statistical Machine Translation, ACL.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In EMNLP.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011.
Goodness: A method for measuring machine trans-
lation confidence. In ACL-HLT.
Daniella Bal, Malissa Bal, Arthur van Bunningen,
Alexander Hogenboom, Frederik Hogenboom, and
Flavius Frasincar. 2011. Sentiment analysis with
a multilingual pipeline. In Web Information System
Engineering WISE 2011.
Alexandra Balahur and Ralf Steinberger. 2009. Re-
thinking sentiment analysis in the news: from theory
to practice and back. In Proceeding of Workshop on
Opinion Mining and Sentiment Analysis (WOMSA).
Alexandra Balahur and Marco Turchi. 2014. Compar-
ative experiments using supervised learning and ma-
chine translation for multilingual sentiment analysis.
Computer Speech &amp; Language, 28(1).
</reference>
<page confidence="0.991502">
161
</page>
<reference confidence="0.999762669811321">
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In LREC.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2010. Multilingual subjectivity: Are more lan-
guages better? In COLING.
Roberto Basili, Diego De Cao, Danilo Croce, Bonaven-
tura Coppola, and Alessandro Moschitti. 2009.
Cross-language frame semantics transfer in bilin-
gual corpora. In CICLING, volume 5449 of Lecture
Notes in Computer Science. Springer Berlin Heidel-
berg.
Daniel Bikel and Imed Zitouni, editors. 2012. Multi-
lingual Natural Language Processing Applications:
From Theory to Practice. IBM Press, 1st edition.
Katarina Boland, Andias Wira-Alam, and Reinhard
Messerschmidt. 2013. Creating an annotated corpus
for sentiment analysis of german product reviews.
Technical Report 2013/05, GESIS Institute.
Julian Brooke, Milan Tofiloski, and Maite Taboada.
2009. Cross-linguistic sentiment analysis: From
english to spanish. In RANLP, Borovets, Bulgaria,
September. Association for Computational Linguis-
tics.
Yoonjung Choi, Seongchan Kim, and Sung-Hyon
Myaeng. 2010. Detecting Opinions and their Opin-
ion Targets in NTCIR-8. In NTCIR8.
Kerstin Denecke. 2008. Using sentiwordnet for multi-
lingual sentiment analysis. In ICDEW.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In NAACL.
Hatem Ghorbel. 2012. Experiments in cross-lingual
sentiment analysis in discussion forums. In Karl
Aberer, Andreas Flache, Wander Jager, Ling Liu, Jie
Tang, and Christophe Guret, editors, Social Infor-
matics, volume 7710 of Lecture Notes in Computer
Science. Springer Berlin Heidelberg.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In ACM SIGKDD.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3), September.
Niklas Jakob and Iryna Gurevych. 2010a. Extracting
opinion targets in a single- and cross-domain setting
with conditional random fields. In EMNLP.
Niklas Jakob and Iryna Gurevych. 2010b. Using
anaphora resolution to improve opinion target iden-
tification in movie reviews. In ACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in nlp. In ACL.
Richard Johansson and Alessandro Moschitti. 2011.
Extracting opinion expressions and their polarities:
exploration of pipelines and joint models. In ACL-
HLT.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA
Sentment Corpus for the Automotive Domain. In
Proc. od the 4th International AAAI Conference on
Weblogs and Social Media Data Workshop Chal-
lenge (ICWSM-DWC 2010).
Roman Klinger and Philipp Cimiano. 2013. Bi-
directional inter-dependencies of subjective expres-
sions and targets and their value for a joint model.
In ACL.
Roman Klinger and Philipp Cimiano. 2014. The usage
review corpus for fine grained multi lingual opinion
analysis. In LREC.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit.
AAMT.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
Mikhail Kozhevnikov and Ivan Titov. 2014. Cross-
lingual model transfer using feature representation
projection. In ACL.
Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010.
Sentiment analysis with global topics and local de-
pendency. In AAAI.
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via
imperatively defined factor graphs. In NIPS.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In EMNLP.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In ACL.
Sebastian Pad´o and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research (JAIR),
36.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
HLT-EMNLP.
Xabier Saralegi and I˜naki San Vicente. 2012. Tass:
Detecting sentiments in spanish tweets. In Work-
shop on Sentiment Analisis at SEPLN (TASS). SE-
PLN, 09/2012.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In NIPS.
</reference>
<page confidence="0.974476">
162
</page>
<reference confidence="0.999769092105263">
Aliaksei Severyn, , Alessandro Moschitti, Olga
Uryupina, Barbara Plank, and Katja Filippova. 2015.
Multi-lingual opinion mining on youtube. Informa-
tion Processing and Management. in press.
Kashif Shah and Lucia Specia. 2014. Quality esti-
mation for translation selection. In Conference of
the European Association for Machine Translation,
EAMT, Dubrovnik, Croatia.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An investigation on the effectiveness of features for
translation quality estimation. In Machine Transla-
tion Summit XIV.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay. 2008. Unsupervised multilin-
gual learning for POS tagging. In EMNLP.
Anders Søgaard. 2011. Datapoint selection for cross-
language adaptation of dependency parsers. In ACL-
HLT.
Lucia Specia, Kashif Shah, Jose G.C. de Souza, and
Trevor Cohn. 2013. Quest - a translation quality
estimation framework. In ACL.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at sixteen: Update and
outlook. In Proceedings IEEE Automatic Speech
Recognition and Understanding Workshop.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings International Con-
ference on Spoken Language Processing.
Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman,
Eric Nyberg, and Chris Dyer. 2014. Metaphor de-
tection with cross-lingual model transfer. In ACL.
Nicola Ueffing, Klaus Macherey, and Hermann Ney.
2003. Confidence measures for statistical machine
translation. In In Proc. MT Summit IX.
Olga Uryupina, Barbara Plank, Aliaksei Severyn,
Agata Rotondi, and Alessandro Moschitti. 2014.
Sentube: A corpus for sentiment analysis on youtube
social media. In LREC.
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Keh-Yih Su, Jian Su, and
Janyce Wiebe, editors, ACL/IJCNLP.
M. Wick, K. Rohanimanesh, K. Bellare, A. Culotta,
and A. McCallum. 2011. SampleRank: Training
factor graphs with atomic gradients. In ICML.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In CorpusAnno, ACL.
Ruifeng Xu, Lin Gui, Jun Xu, Qin Lu, and Kam-Fai
Wong. 2013. Cross lingual opinion holder extrac-
tion based on multi-kernel svms and transfer learn-
ing. World Wide Web Journal.
Bishan Yang and Claire Cardie. 2012. Extracting opin-
ion expressions with semi-markov conditional ran-
dom fields. In EMNLP-CoNLL.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In NAACL.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora. In
HLT.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In IJCNLP-08 Workshop on NLP for Less
Privileged Languages.
Qi Zhang, Yuanbin Wu, Yan Wu, and Xuanjing Huang.
2011. Opinion mining with sentiment graph. In
Proceedings of the 2011 IEEE/WIC/ACM Interna-
tional Conferences on Web Intelligence and Intelli-
gent Agent Technology - Volume 01.
Y. Zhao, B. Qin, and T. Liu. 2014. Creating a fine-
grained corpus for chinese sentiment analysis. Intel-
ligent Systems, IEEE, PP(99).
Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2015.
Clopinionminer: Opinion target extraction in a cross-
language scenario. IEEE/ACM Transactions on Au-
dio, Speech &amp; Language Processing, 23(4).
</reference>
<page confidence="0.999103">
163
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.440549">
<title confidence="0.948602">Instance Selection Improves Cross-Lingual Model Training Fine-Grained Sentiment Analysis</title>
<author confidence="0.504793">for Natural Language Processing Computing Group</author>
<author confidence="0.504793">CIT-EC</author>
<affiliation confidence="0.999978">University of Stuttgart Bielefeld University</affiliation>
<address confidence="0.999216">70569 Stuttgart, Germany 33615 Bielefeld,</address>
<email confidence="0.9478945">roman.klinger@ims.uni-stuttgart.decimiano@cit-ec.uni-bielefeld.de</email>
<abstract confidence="0.999350888888889">Scarcity of annotated corpora for many languages is a bottleneck for training finegrained sentiment analysis models that can tag aspects and subjective phrases. We propose to exploit statistical machine translation to alleviate the need for training data by projecting annotated data in a source language to a target language such that a supervised fine-grained sentiment analysis system can be trained. To avoid a negative influence of poor-quality translations, we propose a filtering approach based on machine translation quality estimation measures to select only high-quality sentence pairs for projection. We evaluate on the language pair German/English on a corpus of product reviews annotated for both languages and compare to in-target-language training. Projection without any filtering to 23 % the task of detecting phrases, compared to 41 % in-target-language training. Our approach up to 47 % Further, we show that the detection of subjective phrases is competitive to in-target-language training without filtering.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ˇZeljko Agi´c</author>
<author>J¨org Tiedemann</author>
<author>Danijela Merkler</author>
<author>Simon Krek</author>
<author>Kaja Dobrovoljc</author>
<author>Sara Moze</author>
</authors>
<title>Cross-lingual dependency parsing of related languages with rich morphosyntactic tagsets.</title>
<date>2014</date>
<booktitle>In Workshop on Language Technology for Closely Related Languages and Language Variants, EMNLP.</booktitle>
<marker>Agi´c, Tiedemann, Merkler, Krek, Dobrovoljc, Moze, 2014</marker>
<rawString>ˇZeljko Agi´c, J¨org Tiedemann, Danijela Merkler, Simon Krek, Kaja Dobrovoljc, and Sara Moze. 2014. Cross-lingual dependency parsing of related languages with rich morphosyntactic tagsets. In Workshop on Language Technology for Closely Related Languages and Language Variants, EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleftherios Avramidis</author>
<author>Maja Popovi´c</author>
<author>David Vilar</author>
<author>Aljoscha Burchardt</author>
</authors>
<title>Evaluate with confidence estimation: Machine ranking of translation outputs using grammatical features.</title>
<date>2011</date>
<booktitle>In Workshop on Statistical Machine Translation, ACL.</booktitle>
<marker>Avramidis, Popovi´c, Vilar, Burchardt, 2011</marker>
<rawString>Eleftherios Avramidis, Maja Popovi´c, David Vilar, and Aljoscha Burchardt. 2011. Evaluate with confidence estimation: Machine ranking of translation outputs using grammatical features. In Workshop on Statistical Machine Translation, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="28831" citStr="Axelrod et al. (2011)" startWordPosition="4646" endWordPosition="4649">arallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows </context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Fei Huang</author>
<author>Yaser Al-Onaizan</author>
</authors>
<title>Goodness: A method for measuring machine translation confidence.</title>
<date>2011</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="30762" citStr="Bach et al., 2011" startWordPosition="4934" endWordPosition="4937"> dictionaries for languages with scarce resources can be supported by bootstrapping approaches (Banea et al., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into account (Bach et al., 2011). An overview on confidence measures for machine translation is for instance provided by Ueffing et al. (2003). The impact of different features has been analyzed by Shah et al. (2013). A complete system and framework for quality estimation (including a list of possible features) is QuEst (Specia et al., 2013). For an overview of other cross-lingual applications and methods, we refer to Bikel and Zitouni (2012). 5 Conclusion and Future Work We have presented an approach that alleviates the need of training data for a target language when adapting a fine-grained sentiment analysis system to a n</context>
</contexts>
<marker>Bach, Huang, Al-Onaizan, 2011</marker>
<rawString>Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011. Goodness: A method for measuring machine translation confidence. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniella Bal</author>
<author>Malissa Bal</author>
<author>Arthur van Bunningen</author>
</authors>
<title>Alexander Hogenboom, Frederik Hogenboom, and Flavius Frasincar.</title>
<date>2011</date>
<booktitle>In Web Information System Engineering WISE</booktitle>
<marker>Bal, Bal, van Bunningen, 2011</marker>
<rawString>Daniella Bal, Malissa Bal, Arthur van Bunningen, Alexander Hogenboom, Frederik Hogenboom, and Flavius Frasincar. 2011. Sentiment analysis with a multilingual pipeline. In Web Information System Engineering WISE 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
<author>Ralf Steinberger</author>
</authors>
<title>Rethinking sentiment analysis in the news: from theory to practice and back.</title>
<date>2009</date>
<booktitle>In Proceeding of Workshop on Opinion Mining and Sentiment Analysis (WOMSA).</booktitle>
<contexts>
<context position="25752" citStr="Balahur and Steinberger, 2009" startWordPosition="4159" endWordPosition="4162">(Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al., 2013), in addition to the USAGE corpus (Klinger and Cimiano, 2014) we have used in this work, consisting of German and English reviews. The (non-fine-grained annotated) Spanish TASS corpus consists of Twitter messages (Saralegi and Vicente, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been too much work on approaches to transfer a model either directly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et al., 2010; Mihalcea et al., 2007; Balahur and Turchi, 2014). Recent work approached multilingual opinion mining on the above-mentioned multi-</context>
</contexts>
<marker>Balahur, Steinberger, 2009</marker>
<rawString>Alexandra Balahur and Ralf Steinberger. 2009. Rethinking sentiment analysis in the news: from theory to practice and back. In Proceeding of Workshop on Opinion Mining and Sentiment Analysis (WOMSA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
<author>Marco Turchi</author>
</authors>
<title>Comparative experiments using supervised learning and machine translation for multilingual sentiment analysis.</title>
<date>2014</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="26270" citStr="Balahur and Turchi, 2014" startWordPosition="4248" endWordPosition="4251">Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been too much work on approaches to transfer a model either directly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et al., 2010; Mihalcea et al., 2007; Balahur and Turchi, 2014). Recent work approached multilingual opinion mining on the above-mentioned multi-lingual Youtube corpus with tree kernels predicting the polarity of a comment and whether it concerns the product or the video in which the product is featured. (Severyn et al., 2015). Brooke et al. (2009) compare dictionary and classification transfer from English to Spanish in a similar classification setting. While cross-lingual annotation projection has been investigated in the context of polarity computation, we are only aware of two approaches exploiting cross-lingual annotation projection on 159 the task o</context>
<context position="29673" citStr="Balahur and Turchi (2014)" startWordPosition="4770" endWordPosition="4773">g scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of automatic translation on sentiment analysis. Finally, SentiWordNet has been used for multilingual sentiment analysis (Denecke, 2008). Building dictionaries for languages with scarce resources can be supported by bootstrapping approaches (Banea et al., 2008). Estimating t</context>
</contexts>
<marker>Balahur, Turchi, 2014</marker>
<rawString>Alexandra Balahur and Marco Turchi. 2014. Comparative experiments using supervised learning and machine translation for multilingual sentiment analysis. Computer Speech &amp; Language, 28(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
</authors>
<title>A bootstrapping method for building subjectivity lexicons for languages with scarce resources.</title>
<date>2008</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="30259" citStr="Banea et al., 2008" startWordPosition="4855" endWordPosition="4858">models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of automatic translation on sentiment analysis. Finally, SentiWordNet has been used for multilingual sentiment analysis (Denecke, 2008). Building dictionaries for languages with scarce resources can be supported by bootstrapping approaches (Banea et al., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into account (Bach et al., 2011). An overview on confidence measures for machine translation is for instance provided by Ueffing </context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, 2008</marker>
<rawString>Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2008. A bootstrapping method for building subjectivity lexicons for languages with scarce resources. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Multilingual subjectivity: Are more languages better?</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="26220" citStr="Banea et al., 2010" startWordPosition="4240" endWordPosition="4243">nte, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been too much work on approaches to transfer a model either directly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et al., 2010; Mihalcea et al., 2007; Balahur and Turchi, 2014). Recent work approached multilingual opinion mining on the above-mentioned multi-lingual Youtube corpus with tree kernels predicting the polarity of a comment and whether it concerns the product or the video in which the product is featured. (Severyn et al., 2015). Brooke et al. (2009) compare dictionary and classification transfer from English to Spanish in a similar classification setting. While cross-lingual annotation projection has been investigated in the context of polarity computation, we are only aware of two approaches exploiting cro</context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, 2010</marker>
<rawString>Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2010. Multilingual subjectivity: Are more languages better? In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Diego De Cao</author>
<author>Danilo Croce</author>
<author>Bonaventura Coppola</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Cross-language frame semantics transfer in bilingual corpora.</title>
<date>2009</date>
<booktitle>In CICLING,</booktitle>
<volume>5449</volume>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Basili, De Cao, Croce, Coppola, Moschitti, 2009</marker>
<rawString>Roberto Basili, Diego De Cao, Danilo Croce, Bonaventura Coppola, and Alessandro Moschitti. 2009. Cross-language frame semantics transfer in bilingual corpora. In CICLING, volume 5449 of Lecture Notes in Computer Science. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bikel</author>
<author>Imed Zitouni</author>
<author>editors</author>
</authors>
<date>2012</date>
<booktitle>Multilingual Natural Language Processing Applications: From Theory to Practice. IBM Press, 1st edition.</booktitle>
<marker>Bikel, Zitouni, editors, 2012</marker>
<rawString>Daniel Bikel and Imed Zitouni, editors. 2012. Multilingual Natural Language Processing Applications: From Theory to Practice. IBM Press, 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katarina Boland</author>
<author>Andias Wira-Alam</author>
<author>Reinhard Messerschmidt</author>
</authors>
<title>Creating an annotated corpus for sentiment analysis of german product reviews.</title>
<date>2013</date>
<tech>Technical Report 2013/05,</tech>
<institution>GESIS Institute.</institution>
<contexts>
<context position="25371" citStr="Boland et al., 2013" startWordPosition="4103" endWordPosition="4106"> et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al., 2013), in addition to the USAGE corpus (Klinger and Cimiano, 2014) we have used in this work, consisting of German and English reviews. The (non-fine-grained annotated) Spanish TASS corpus consists of Twitter messages (Saralegi and Vicente, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been too much work on approaches to transfer a model either directly or via annotation projection </context>
</contexts>
<marker>Boland, Wira-Alam, Messerschmidt, 2013</marker>
<rawString>Katarina Boland, Andias Wira-Alam, and Reinhard Messerschmidt. 2013. Creating an annotated corpus for sentiment analysis of german product reviews. Technical Report 2013/05, GESIS Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Maite Taboada</author>
</authors>
<title>Cross-linguistic sentiment analysis: From english to spanish.</title>
<date>2009</date>
<booktitle>In RANLP, Borovets,</booktitle>
<location>Bulgaria,</location>
<contexts>
<context position="26557" citStr="Brooke et al. (2009)" startWordPosition="4294" endWordPosition="4297">ctly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et al., 2010; Mihalcea et al., 2007; Balahur and Turchi, 2014). Recent work approached multilingual opinion mining on the above-mentioned multi-lingual Youtube corpus with tree kernels predicting the polarity of a comment and whether it concerns the product or the video in which the product is featured. (Severyn et al., 2015). Brooke et al. (2009) compare dictionary and classification transfer from English to Spanish in a similar classification setting. While cross-lingual annotation projection has been investigated in the context of polarity computation, we are only aware of two approaches exploiting cross-lingual annotation projection on 159 the task of identifying aspects specifically with an evaluation on manually annotated data in more than one language. The CLOpinionMiner (Zhou et al., 2015) uses an English data set which is transfered to Chinese. Models are further improved by cotraining. Xu et al. (2013) perform self-training b</context>
</contexts>
<marker>Brooke, Tofiloski, Taboada, 2009</marker>
<rawString>Julian Brooke, Milan Tofiloski, and Maite Taboada. 2009. Cross-linguistic sentiment analysis: From english to spanish. In RANLP, Borovets, Bulgaria, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoonjung Choi</author>
<author>Seongchan Kim</author>
<author>Sung-Hyon Myaeng</author>
</authors>
<title>Detecting Opinions and their Opinion Targets in NTCIR-8.</title>
<date>2010</date>
<booktitle>In NTCIR8.</booktitle>
<contexts>
<context position="24448" citStr="Choi et al., 2010" startWordPosition="3958" endWordPosition="3961"> the number of true positive aspects for the coffee machine subdomain is 1100, only 221 are predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power an</context>
</contexts>
<marker>Choi, Kim, Myaeng, 2010</marker>
<rawString>Yoonjung Choi, Seongchan Kim, and Sung-Hyon Myaeng. 2010. Detecting Opinions and their Opinion Targets in NTCIR-8. In NTCIR8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kerstin Denecke</author>
</authors>
<title>Using sentiwordnet for multilingual sentiment analysis.</title>
<date>2008</date>
<booktitle>In ICDEW.</booktitle>
<contexts>
<context position="30134" citStr="Denecke, 2008" startWordPosition="4839" endWordPosition="4840">l sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of automatic translation on sentiment analysis. Finally, SentiWordNet has been used for multilingual sentiment analysis (Denecke, 2008). Building dictionaries for languages with scarce resources can be supported by bootstrapping approaches (Banea et al., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into</context>
</contexts>
<marker>Denecke, 2008</marker>
<rawString>Kerstin Denecke. 2008. Using sentiwordnet for multilingual sentiment analysis. In ICDEW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of ibm model 2. In NAACL.</title>
<date>2013</date>
<contexts>
<context position="4064" citStr="Dyer et al., 2013" startWordPosition="606" endWordPosition="609">3 Proceedings of the 19th Conference on Computational Language Learning, pages 153–163, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics Es gibt mit Sicherheit bessere Maschinen , aber die bietet das beste Preis-Leistungs-Verhaltnis . There are certainly better machines , but o↵ers the best price-performance ratio . Figure 1: Example for the projection of an annotation from the source language to the target language. The translation has been generated with the Google translate API (https://cloud.google.com/translate/). The alignment is induced with FastAlign (Dyer et al., 2013). • We propose to use a supervised approach to induce a fine-grained sentiment analysis model to predict aspect and subjective phrases on some target language, given training data in some source language. This approach relies on automatic translation of source training data and projection of annotations to the target language data. • We present an instance selection method that only selects sentences with a certain translation quality. For this, we incorporate different measures of translation and alignment confidence. We show that such an instance selection method leads to increased performan</context>
<context position="10314" citStr="Dyer et al., 2013" startWordPosition="1642" endWordPosition="1645">s, respectively, but they never occur together.4 The corpus is thus not representative for product reviews as we consider in this paper. 2Note that the learning is independent from the actual value for all 0 &lt; α &lt; (max c i 9E orpus I 9I) 3www.statmt.org/moses/ 4These example domains are taken from the USAGE corpus (Klinger and Cimiano, 2014), which is used in Section 3. Thus, we opt for using a closed translation system that is trained on larger amounts of data, that is Google Translate, through the available API5. The alignment is then computed as a post processing step relying on FastAlign (Dyer et al., 2013), a reparametrization of IBM Model 2 with a reduced set of parameters. It is trained in an unsupervised fashion via expectation maximization. Projecting the annotations from the source to the target language works as follows: given an annotated sentence in the source language si, ... , sn and some translation of this sentence ti, ... , tm into the target language, we induce an inductive mapping a : [1... n] —* [1... m] using FastAlign. For a source language phrase si,j = si, ... , sj we refer by a(si,j) to the set of tokens that some token in si,j has been aligned to, that is: a(si,j) = Ui&lt;k&lt;j</context>
<context position="12944" citStr="Dyer et al., 2013" startWordPosition="2097" endWordPosition="2100">n de coffee machine 75 108 cutlery 49 72 microwave 100 100 toaster 100 4 trash can 100 99 vacuum cleaner 51 140 washing machine 49 88 dish washer 98 0 Table 1: Frequencies of the corpus used in our experiments (Klinger and Cimiano, 2014). tated text in the target language (measuring if the translation is typical, referred to as Target LM). 3. The likelihood that the alignment is correct, directly computed on the basis of the alignment probability (referred to as Alignment): p(e I f) = Hi=1m p(ei I f, m, n), where e and f are source and target sentences and m and n denote the sentence lengths (Dyer et al., 2013, Eq. 1f.). For building the language models, we employ the toolkit SRILM (Stolcke, 2002; Stolcke et al., 2011). The likelihood for the alignment as well as the language model probability are normalized by the number of tokens in the sentence. 3 Experiments 3.1 Corpus and Setting The proposed approach is evaluated on the language pair German/English in both directions (projecting German annotations into an automatically generated English corpus and testing on English annotations and vice versa). As a resource, we use the recently published USAGE corpus (Klinger and Cimiano, 2014), which consis</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of ibm model 2. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hatem Ghorbel</author>
</authors>
<title>Experiments in cross-lingual sentiment analysis in discussion forums.</title>
<date>2012</date>
<journal>Social Informatics,</journal>
<booktitle>of Lecture Notes in Computer Science.</booktitle>
<volume>7710</volume>
<editor>In Karl Aberer, Andreas Flache, Wander Jager, Ling Liu, Jie Tang, and Christophe Guret, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="29978" citStr="Ghorbel (2012)" startWordPosition="4818" endWordPosition="4819">ed across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of automatic translation on sentiment analysis. Finally, SentiWordNet has been used for multilingual sentiment analysis (Denecke, 2008). Building dictionaries for languages with scarce resources can be supported by bootstrapping approaches (Banea et al., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number o</context>
</contexts>
<marker>Ghorbel, 2012</marker>
<rawString>Hatem Ghorbel. 2012. Experiments in cross-lingual sentiment analysis in discussion forums. In Karl Aberer, Andreas Flache, Wander Jager, Ling Liu, Jie Tang, and Christophe Guret, editors, Social Informatics, volume 7710 of Lecture Notes in Computer Science. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In ACM SIGKDD.</booktitle>
<contexts>
<context position="24520" citStr="Hu and Liu, 2004" startWordPosition="3970" endWordPosition="3973">1100, only 221 are predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpor</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In ACM SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="27718" citStr="Hwa et al., 2005" startWordPosition="4473" endWordPosition="4476"> by cotraining. Xu et al. (2013) perform self-training based on a projected corpus from English to Chinese to detect opinion holders. Due to the lack of existing manually annotated resources, to our knowledge no cross-language projection approach for fine-grained annotation at the level of aspect and subjective phrases has been proposed before. The projection of annotated data sets has been investigated in a variety of applications. Early work includes an approach to the projection of part-ofspeech tags and noun phrases (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and parsing information (Hwa et al., 2005) on a parallel corpus. Especially in syntactic and semantic parsing, heuristics to remove or correct spuriously projected annotations have been developed (Pad´o and Lapata, 2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. E</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Nat. Lang. Eng., 11(3), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niklas Jakob</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting opinion targets in a single- and cross-domain setting with conditional random fields.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="24590" citStr="Jakob and Gurevych, 2010" startWordPosition="3982" endWordPosition="3985">ality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples</context>
</contexts>
<marker>Jakob, Gurevych, 2010</marker>
<rawString>Niklas Jakob and Iryna Gurevych. 2010a. Extracting opinion targets in a single- and cross-domain setting with conditional random fields. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niklas Jakob</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using anaphora resolution to improve opinion target identification in movie reviews.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="24590" citStr="Jakob and Gurevych, 2010" startWordPosition="3982" endWordPosition="3985">ality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples</context>
</contexts>
<marker>Jakob, Gurevych, 2010</marker>
<rawString>Niklas Jakob and Iryna Gurevych. 2010b. Using anaphora resolution to improve opinion target identification in movie reviews. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="29119" citStr="Jiang and Zhai, 2007" startWordPosition="4690" endWordPosition="4693"> et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machin</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Extracting opinion expressions and their polarities: exploration of pipelines and joint models.</title>
<date>2011</date>
<booktitle>In ACLHLT.</booktitle>
<contexts>
<context position="24479" citStr="Johansson and Moschitti, 2011" startWordPosition="3962" endWordPosition="3965"> positive aspects for the coffee machine subdomain is 1100, only 221 are predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora </context>
</contexts>
<marker>Johansson, Moschitti, 2011</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2011. Extracting opinion expressions and their polarities: exploration of pipelines and joint models. In ACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason S Kessler</author>
<author>Miriam Eckert</author>
<author>Lyndsie Clark</author>
<author>Nicolas Nicolov</author>
</authors>
<title>ICWSM JDPA Sentment Corpus for the Automotive Domain.</title>
<date>2010</date>
<booktitle>In Proc. od the 4th International AAAI Conference on Weblogs and Social Media Data Workshop Challenge (ICWSM-DWC</booktitle>
<publisher>The</publisher>
<contexts>
<context position="25101" citStr="Kessler et al., 2010" startWordPosition="4061" endWordPosition="4064"> Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al., 2013), in addition to the USAGE corpus (Klinger and Cimiano, 2014) we have used in this work, consisting of German and English reviews. The (non-fine-grained annotated) Spanish TASS corpus consists of Twitter messages (Saralegi and Vicente, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity</context>
</contexts>
<marker>Kessler, Eckert, Clark, Nicolov, 2010</marker>
<rawString>Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and Nicolas Nicolov. 2010. The 2010 ICWSM JDPA Sentment Corpus for the Automotive Domain. In Proc. od the 4th International AAAI Conference on Weblogs and Social Media Data Workshop Challenge (ICWSM-DWC 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Klinger</author>
<author>Philipp Cimiano</author>
</authors>
<title>Bidirectional inter-dependencies of subjective expressions and targets and their value for a joint model.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5846" citStr="Klinger and Cimiano (2013)" startWordPosition="882" endWordPosition="885">lowing, we describe our methodology in detail, including the description of the machine translation, annotation projection, and quality estimation methods (Section 2), and present the evaluation on manually annotated data (Section 3). Related work is discussed in Section 4. We conclude with Section 5 and mention promising future steps. 2 Methods 2.1 Supervised Model for Aspect and Subjective Phrase Detection We use a supervised model induced from training data to detect aspect phrases, subjective (evaluative) phrases and their relations. The structure follows the proposed pipeline approach by Klinger and Cimiano (2013).1 However, in contrast to their work, we focus on the detection of phrases only, and exploit the detection of relations only during inference, such that the detection of relations has an effect on the detection of phrases, but is not evaluated directly. The phrase detection follows the idea of semiMarkov conditional random fields (Sarawagi and Cohen, 2004; Yang and Cardie, 2012) and models phrases as spans over tokens as variables. Factor templates for spans of type aspect and subjective take into account token strings, prefixes, suffixes, the inclusion of digits, and part-of-speech tags, bot</context>
<context position="7516" citStr="Klinger and Cimiano, 2013" startWordPosition="1161" endWordPosition="1164">ective phrase. Inference during training and testing is done via Markov Chain Monte Carlo (MCMC). In each sampling step (with options of adding a span, removing a span, adding an aspect as target to a subjective phrase), the respective factors lead to an associated model score. The model parameters are adapted based on sample rank (Wick et al., 2011) using an objective function which computes the fraction of correctly predicted tokens in a span. For details on the model configuration and its implementation in FACTORIE (McCallum et al., 2009), we refer to the description in the original paper (Klinger and Cimiano, 2013). The objective function to evaluate a span r during training is Jr n gJ JgJ 1https://bitbucket.org/rklinger/jfsa f(r) = max g∈s α - Jr\gJ , 154 where g is the set of all gold spans, and |r n g| is the number of tokens shared by gold and predicted span and |r\g |the number of predicted tokens which are not part of the gold span. The parameter α is set to 0.1 as in the original paper.2. The objective for the predictions in a whole sentence s containing spans is f(s) = Er∈s f(r). This model does not take into account languagespecific features and can therefore be trained for different languages.</context>
<context position="24694" citStr="Klinger and Cimiano, 2013" startWordPosition="3998" endWordPosition="4001"> This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly</context>
</contexts>
<marker>Klinger, Cimiano, 2013</marker>
<rawString>Roman Klinger and Philipp Cimiano. 2013. Bidirectional inter-dependencies of subjective expressions and targets and their value for a joint model. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Klinger</author>
<author>Philipp Cimiano</author>
</authors>
<title>The usage review corpus for fine grained multi lingual opinion analysis.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="10039" citStr="Klinger and Cimiano, 2014" startWordPosition="1593" endWordPosition="1596">rman/English. The corpus includes only 4 sentences with the term “toaster”, 12 with “knives” (mostly in the context of violence), 6 with “dishwasher” (in the context of regulations) and 0 with “trash can”. The terms “camera” and “display” are more frequent, with 208 and 1186 mentions, respectively, but they never occur together.4 The corpus is thus not representative for product reviews as we consider in this paper. 2Note that the learning is independent from the actual value for all 0 &lt; α &lt; (max c i 9E orpus I 9I) 3www.statmt.org/moses/ 4These example domains are taken from the USAGE corpus (Klinger and Cimiano, 2014), which is used in Section 3. Thus, we opt for using a closed translation system that is trained on larger amounts of data, that is Google Translate, through the available API5. The alignment is then computed as a post processing step relying on FastAlign (Dyer et al., 2013), a reparametrization of IBM Model 2 with a reduced set of parameters. It is trained in an unsupervised fashion via expectation maximization. Projecting the annotations from the source to the target language works as follows: given an annotated sentence in the source language si, ... , sn and some translation of this senten</context>
<context position="12564" citStr="Klinger and Cimiano, 2014" startWordPosition="2027" endWordPosition="2030"> quality as an upper baseline): 1. The probability of the sentence in the source language given a language model build on unannotated text in the source language (measuring if the language to be translated is typical, referred to as Source LM). 2. The probability of the machine translated sentence given a language model built on unanno5https://cloud.google.com/translate/ 155 # reviews en de coffee machine 75 108 cutlery 49 72 microwave 100 100 toaster 100 4 trash can 100 99 vacuum cleaner 51 140 washing machine 49 88 dish washer 98 0 Table 1: Frequencies of the corpus used in our experiments (Klinger and Cimiano, 2014). tated text in the target language (measuring if the translation is typical, referred to as Target LM). 3. The likelihood that the alignment is correct, directly computed on the basis of the alignment probability (referred to as Alignment): p(e I f) = Hi=1m p(ei I f, m, n), where e and f are source and target sentences and m and n denote the sentence lengths (Dyer et al., 2013, Eq. 1f.). For building the language models, we employ the toolkit SRILM (Stolcke, 2002; Stolcke et al., 2011). The likelihood for the alignment as well as the language model probability are normalized by the number of </context>
<context position="25432" citStr="Klinger and Cimiano, 2014" startWordPosition="4113" endWordPosition="4116"> fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al., 2013), in addition to the USAGE corpus (Klinger and Cimiano, 2014) we have used in this work, consisting of German and English reviews. The (non-fine-grained annotated) Spanish TASS corpus consists of Twitter messages (Saralegi and Vicente, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been too much work on approaches to transfer a model either directly or via annotation projection in the area of sentiment analysis. One example is based on se</context>
</contexts>
<marker>Klinger, Cimiano, 2014</marker>
<rawString>Roman Klinger and Philipp Cimiano. 2014. The usage review corpus for fine grained multi lingual opinion analysis. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: the tenth Machine Translation Summit. AAMT.</booktitle>
<contexts>
<context position="9334" citStr="Koehn, 2005" startWordPosition="1476" endWordPosition="1477">require a corpus annotated for some source language and a translation from the source to a target language. As the availability of a parallel training corpus cannot be assumed in general, we use statistical machine translation (SMT) methods, relying on phrase-based translation models that use large amounts of parallel data for training (Koehn, 2010). While using an open-source system such as Moses3 would have been an option, we note that the quality would be limited by whether the system can be trained on a representative corpus. A standard dataset that SMT systems are trained on is EuroParl (Koehn, 2005). EuroParl covers 21 languages and contains 1.920.209 sentences for the pair German/English. The corpus includes only 4 sentences with the term “toaster”, 12 with “knives” (mostly in the context of violence), 6 with “dishwasher” (in the context of regulations) and 0 with “trash can”. The terms “camera” and “display” are more frequent, with 208 and 1186 mentions, respectively, but they never occur together.4 The corpus is thus not representative for product reviews as we consider in this paper. 2Note that the learning is independent from the actual value for all 0 &lt; α &lt; (max c i 9E orpus I 9I) </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the tenth Machine Translation Summit. AAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="9073" citStr="Koehn, 2010" startWordPosition="1430" endWordPosition="1431">in and to a new language, corresponding training data is needed. In order to circumvent the need for additional training data when addressing a new language, we project training data automatically from a source to a target language. As input to our approach we require a corpus annotated for some source language and a translation from the source to a target language. As the availability of a parallel training corpus cannot be assumed in general, we use statistical machine translation (SMT) methods, relying on phrase-based translation models that use large amounts of parallel data for training (Koehn, 2010). While using an open-source system such as Moses3 would have been an option, we note that the quality would be limited by whether the system can be trained on a representative corpus. A standard dataset that SMT systems are trained on is EuroParl (Koehn, 2005). EuroParl covers 21 languages and contains 1.920.209 sentences for the pair German/English. The corpus includes only 4 sentences with the term “toaster”, 12 with “knives” (mostly in the context of violence), 6 with “dishwasher” (in the context of regulations) and 0 with “trash can”. The terms “camera” and “display” are more frequent, wi</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press, New York, NY, USA, 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Kozhevnikov</author>
<author>Ivan Titov</author>
</authors>
<title>Crosslingual model transfer using feature representation projection.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28644" citStr="Kozhevnikov and Titov (2014)" startWordPosition="4617" endWordPosition="4620"> is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which </context>
</contexts>
<marker>Kozhevnikov, Titov, 2014</marker>
<rawString>Mikhail Kozhevnikov and Ivan Titov. 2014. Crosslingual model transfer using feature representation projection. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Sentiment analysis with global topics and local dependency.</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="24537" citStr="Li et al., 2010" startWordPosition="3974" endWordPosition="3977"> predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wie</context>
</contexts>
<marker>Li, Huang, Zhu, 2010</marker>
<rawString>Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010. Sentiment analysis with global topics and local dependency. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Karl Schultz</author>
<author>Sameer Singh</author>
</authors>
<title>FACTORIE: Probabilistic programming via imperatively defined factor graphs.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7437" citStr="McCallum et al., 2009" startWordPosition="1148" endWordPosition="1151">It is further checked if no other noun than the aspect is close to the subjective phrase. Inference during training and testing is done via Markov Chain Monte Carlo (MCMC). In each sampling step (with options of adding a span, removing a span, adding an aspect as target to a subjective phrase), the respective factors lead to an associated model score. The model parameters are adapted based on sample rank (Wick et al., 2011) using an objective function which computes the fraction of correctly predicted tokens in a span. For details on the model configuration and its implementation in FACTORIE (McCallum et al., 2009), we refer to the description in the original paper (Klinger and Cimiano, 2013). The objective function to evaluate a span r during training is Jr n gJ JgJ 1https://bitbucket.org/rklinger/jfsa f(r) = max g∈s α - Jr\gJ , 154 where g is the set of all gold spans, and |r n g| is the number of tokens shared by gold and predicted span and |r\g |the number of predicted tokens which are not part of the gold span. The parameter α is set to 0.1 as in the original paper.2. The objective for the predictions in a whole sentence s containing spans is f(s) = Er∈s f(r). This model does not take into account </context>
</contexts>
<marker>McCallum, Schultz, Singh, 2009</marker>
<rawString>Andrew McCallum, Karl Schultz, and Sameer Singh. 2009. FACTORIE: Probabilistic programming via imperatively defined factor graphs. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="29494" citStr="McDonald et al., 2011" startWordPosition="4747" endWordPosition="4750">ranslation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of automatic translation on sentiment analysis. Finally, SentiWordNet has been used for multili</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carmen Banea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning multilingual subjective language via cross-lingual projections.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="26243" citStr="Mihalcea et al., 2007" startWordPosition="4244" endWordPosition="4247">tilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been too much work on approaches to transfer a model either directly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et al., 2010; Mihalcea et al., 2007; Balahur and Turchi, 2014). Recent work approached multilingual opinion mining on the above-mentioned multi-lingual Youtube corpus with tree kernels predicting the polarity of a comment and whether it concerns the product or the video in which the product is featured. (Severyn et al., 2015). Brooke et al. (2009) compare dictionary and classification transfer from English to Spanish in a similar classification setting. While cross-lingual annotation projection has been investigated in the context of polarity computation, we are only aware of two approaches exploiting cross-lingual annotation p</context>
</contexts>
<marker>Mihalcea, Banea, Wiebe, 2007</marker>
<rawString>Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007. Learning multilingual subjective language via cross-lingual projections. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Crosslingual annotation projection for semantic roles.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<volume>36</volume>
<marker>Pad´o, Lapata, 2009</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2009. Crosslingual annotation projection for semantic roles. Journal of Artificial Intelligence Research (JAIR), 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP.</booktitle>
<contexts>
<context position="24564" citStr="Popescu and Etzioni, 2005" startWordPosition="3978" endWordPosition="3981"> threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English reso</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xabier Saralegi</author>
<author>I˜naki San Vicente</author>
</authors>
<title>Tass: Detecting sentiments in spanish tweets.</title>
<date>2012</date>
<booktitle>In Workshop on Sentiment Analisis at SEPLN (TASS). SEPLN,</booktitle>
<pages>09--2012</pages>
<contexts>
<context position="25612" citStr="Saralegi and Vicente, 2012" startWordPosition="4139" endWordPosition="4142">ar corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al., 2013), in addition to the USAGE corpus (Klinger and Cimiano, 2014) we have used in this work, consisting of German and English reviews. The (non-fine-grained annotated) Spanish TASS corpus consists of Twitter messages (Saralegi and Vicente, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been too much work on approaches to transfer a model either directly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et a</context>
</contexts>
<marker>Saralegi, Vicente, 2012</marker>
<rawString>Xabier Saralegi and I˜naki San Vicente. 2012. Tass: Detecting sentiments in spanish tweets. In Workshop on Sentiment Analisis at SEPLN (TASS). SEPLN, 09/2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="6204" citStr="Sarawagi and Cohen, 2004" startWordPosition="940" endWordPosition="943">ed Model for Aspect and Subjective Phrase Detection We use a supervised model induced from training data to detect aspect phrases, subjective (evaluative) phrases and their relations. The structure follows the proposed pipeline approach by Klinger and Cimiano (2013).1 However, in contrast to their work, we focus on the detection of phrases only, and exploit the detection of relations only during inference, such that the detection of relations has an effect on the detection of phrases, but is not evaluated directly. The phrase detection follows the idea of semiMarkov conditional random fields (Sarawagi and Cohen, 2004; Yang and Cardie, 2012) and models phrases as spans over tokens as variables. Factor templates for spans of type aspect and subjective take into account token strings, prefixes, suffixes, the inclusion of digits, and part-of-speech tags, both as full string and as bigrams, for the spans and their vicinity. In addition, the length of the span is modeled by cumulative binning. The relation template indicates how close an aspect is to a subjective phrase based on token distance and on the length of the shortest path in the dependency tree. The edge names of the shortest path are also included as</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2004. Semimarkov conditional random fields for information extraction. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Olga Uryupina</author>
<author>Barbara Plank</author>
<author>Katja Filippova</author>
</authors>
<title>Multi-lingual opinion mining on youtube. Information Processing and Management.</title>
<date>2015</date>
<note>in press.</note>
<marker>Moschitti, Uryupina, Plank, Filippova, 2015</marker>
<rawString>Aliaksei Severyn, , Alessandro Moschitti, Olga Uryupina, Barbara Plank, and Katja Filippova. 2015. Multi-lingual opinion mining on youtube. Information Processing and Management. in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
<author>Lucia Specia</author>
</authors>
<title>Quality estimation for translation selection.</title>
<date>2014</date>
<booktitle>In Conference of the European Association for Machine Translation,</booktitle>
<location>EAMT, Dubrovnik, Croatia.</location>
<contexts>
<context position="11883" citStr="Shah and Specia (2014)" startWordPosition="1910" endWordPosition="1913">nd target language with the only exception that we exclude projected annotations for which |n − m |&gt; 10. 2.3 Quality Estimation-based Instance Filtering The performance of an approach relying on projection of training data from a source to a target language and using this automatically projected data to train a supervised model crucially depends on the quality of the translations and alignments. In order to reduce the impact of spurious translations, we filter out low-quality sentence pairs. To estimate this quality, we take three measures into consideration (following approaches described by Shah and Specia (2014), in addition to a manual assessment of the translation quality as an upper baseline): 1. The probability of the sentence in the source language given a language model build on unannotated text in the source language (measuring if the language to be translated is typical, referred to as Source LM). 2. The probability of the machine translated sentence given a language model built on unanno5https://cloud.google.com/translate/ 155 # reviews en de coffee machine 75 108 cutlery 49 72 microwave 100 100 toaster 100 4 trash can 100 99 vacuum cleaner 51 140 washing machine 49 88 dish washer 98 0 Table</context>
</contexts>
<marker>Shah, Specia, 2014</marker>
<rawString>Kashif Shah and Lucia Specia. 2014. Quality estimation for translation selection. In Conference of the European Association for Machine Translation, EAMT, Dubrovnik, Croatia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>An investigation on the effectiveness of features for translation quality estimation.</title>
<date>2013</date>
<booktitle>In Machine Translation Summit XIV.</booktitle>
<contexts>
<context position="30946" citStr="Shah et al. (2013)" startWordPosition="4964" endWordPosition="4967">a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into account (Bach et al., 2011). An overview on confidence measures for machine translation is for instance provided by Ueffing et al. (2003). The impact of different features has been analyzed by Shah et al. (2013). A complete system and framework for quality estimation (including a list of possible features) is QuEst (Specia et al., 2013). For an overview of other cross-lingual applications and methods, we refer to Bikel and Zitouni (2012). 5 Conclusion and Future Work We have presented an approach that alleviates the need of training data for a target language when adapting a fine-grained sentiment analysis system to a new language. Our approach relies on training data available for a source language and on automatic machine translation, in particular statistical methods, to project training data to t</context>
</contexts>
<marker>Shah, Cohn, Specia, 2013</marker>
<rawString>Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. An investigation on the effectiveness of features for translation quality estimation. In Machine Translation Summit XIV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for POS tagging.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="29405" citStr="Snyder et al., 2008" startWordPosition="4734" endWordPosition="4737">gaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of aut</context>
</contexts>
<marker>Snyder, Naseem, Eisenstein, Barzilay, 2008</marker>
<rawString>Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and Regina Barzilay. 2008. Unsupervised multilingual learning for POS tagging. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Datapoint selection for crosslanguage adaptation of dependency parsers.</title>
<date>2011</date>
<booktitle>In ACLHLT.</booktitle>
<contexts>
<context position="28797" citStr="Søgaard, 2011" startWordPosition="4643" endWordPosition="4644">In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard. 2011. Datapoint selection for crosslanguage adaptation of dependency parsers. In ACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Kashif Shah</author>
<author>Jose G C de Souza</author>
<author>Trevor Cohn</author>
</authors>
<title>Quest - a translation quality estimation framework.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<marker>Specia, Shah, de Souza, Cohn, 2013</marker>
<rawString>Lucia Specia, Kashif Shah, Jose G.C. de Souza, and Trevor Cohn. 2013. Quest - a translation quality estimation framework. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jing Zheng</author>
<author>Wen Wang</author>
<author>Victor Abrash</author>
</authors>
<title>SRILM at sixteen: Update and outlook.</title>
<date>2011</date>
<booktitle>In Proceedings IEEE Automatic Speech Recognition and Understanding Workshop.</booktitle>
<contexts>
<context position="13055" citStr="Stolcke et al., 2011" startWordPosition="2115" endWordPosition="2118"> 140 washing machine 49 88 dish washer 98 0 Table 1: Frequencies of the corpus used in our experiments (Klinger and Cimiano, 2014). tated text in the target language (measuring if the translation is typical, referred to as Target LM). 3. The likelihood that the alignment is correct, directly computed on the basis of the alignment probability (referred to as Alignment): p(e I f) = Hi=1m p(ei I f, m, n), where e and f are source and target sentences and m and n denote the sentence lengths (Dyer et al., 2013, Eq. 1f.). For building the language models, we employ the toolkit SRILM (Stolcke, 2002; Stolcke et al., 2011). The likelihood for the alignment as well as the language model probability are normalized by the number of tokens in the sentence. 3 Experiments 3.1 Corpus and Setting The proposed approach is evaluated on the language pair German/English in both directions (projecting German annotations into an automatically generated English corpus and testing on English annotations and vice versa). As a resource, we use the recently published USAGE corpus (Klinger and Cimiano, 2014), which consists of 622 English and 611 German product reviews from http: //www.amazon.com/ and http://www.amazon.de/, respec</context>
</contexts>
<marker>Stolcke, Zheng, Wang, Abrash, 2011</marker>
<rawString>Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. SRILM at sixteen: Update and outlook. In Proceedings IEEE Automatic Speech Recognition and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="13032" citStr="Stolcke, 2002" startWordPosition="2113" endWordPosition="2114">cuum cleaner 51 140 washing machine 49 88 dish washer 98 0 Table 1: Frequencies of the corpus used in our experiments (Klinger and Cimiano, 2014). tated text in the target language (measuring if the translation is typical, referred to as Target LM). 3. The likelihood that the alignment is correct, directly computed on the basis of the alignment probability (referred to as Alignment): p(e I f) = Hi=1m p(ei I f, m, n), where e and f are source and target sentences and m and n denote the sentence lengths (Dyer et al., 2013, Eq. 1f.). For building the language models, we employ the toolkit SRILM (Stolcke, 2002; Stolcke et al., 2011). The likelihood for the alignment as well as the language model probability are normalized by the number of tokens in the sentence. 3 Experiments 3.1 Corpus and Setting The proposed approach is evaluated on the language pair German/English in both directions (projecting German annotations into an automatically generated English corpus and testing on English annotations and vice versa). As a resource, we use the recently published USAGE corpus (Klinger and Cimiano, 2014), which consists of 622 English and 611 German product reviews from http: //www.amazon.com/ and http:/</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Leonid Boytsov</author>
<author>Anatole Gershman</author>
<author>Eric Nyberg</author>
<author>Chris Dyer</author>
</authors>
<title>Metaphor detection with cross-lingual model transfer.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28512" citStr="Tsvetkov et al., 2014" startWordPosition="4597" endWordPosition="4600"> 2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai</context>
</contexts>
<marker>Tsvetkov, Boytsov, Gershman, Nyberg, Dyer, 2014</marker>
<rawString>Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, Eric Nyberg, and Chris Dyer. 2014. Metaphor detection with cross-lingual model transfer. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Klaus Macherey</author>
<author>Hermann Ney</author>
</authors>
<title>Confidence measures for statistical machine translation. In</title>
<date>2003</date>
<booktitle>In Proc. MT Summit IX.</booktitle>
<contexts>
<context position="30872" citStr="Ueffing et al. (2003)" startWordPosition="4951" endWordPosition="4954">., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into account (Bach et al., 2011). An overview on confidence measures for machine translation is for instance provided by Ueffing et al. (2003). The impact of different features has been analyzed by Shah et al. (2013). A complete system and framework for quality estimation (including a list of possible features) is QuEst (Specia et al., 2013). For an overview of other cross-lingual applications and methods, we refer to Bikel and Zitouni (2012). 5 Conclusion and Future Work We have presented an approach that alleviates the need of training data for a target language when adapting a fine-grained sentiment analysis system to a new language. Our approach relies on training data available for a source language and on automatic machine tra</context>
</contexts>
<marker>Ueffing, Macherey, Ney, 2003</marker>
<rawString>Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003. Confidence measures for statistical machine translation. In In Proc. MT Summit IX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
<author>Barbara Plank</author>
<author>Aliaksei Severyn</author>
<author>Agata Rotondi</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Sentube: A corpus for sentiment analysis on youtube social media.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="25278" citStr="Uryupina et al., 2014" startWordPosition="4088" endWordPosition="4091">otated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al., 2013), in addition to the USAGE corpus (Klinger and Cimiano, 2014) we have used in this work, consisting of German and English reviews. The (non-fine-grained annotated) Spanish TASS corpus consists of Twitter messages (Saralegi and Vicente, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been </context>
</contexts>
<marker>Uryupina, Plank, Severyn, Rotondi, Moschitti, 2014</marker>
<rawString>Olga Uryupina, Barbara Plank, Aliaksei Severyn, Agata Rotondi, and Alessandro Moschitti. 2014. Sentube: A corpus for sentiment analysis on youtube social media. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<editor>In Keh-Yih Su, Jian Su, and Janyce Wiebe, editors,</editor>
<publisher>ACL/IJCNLP.</publisher>
<contexts>
<context position="29962" citStr="Wan, 2009" startWordPosition="4816" endWordPosition="4817">at are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of automatic translation on sentiment analysis. Finally, SentiWordNet has been used for multilingual sentiment analysis (Denecke, 2008). Building dictionaries for languages with scarce resources can be supported by bootstrapping approaches (Banea et al., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output proba</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Xiaojun Wan. 2009. Co-training for cross-lingual sentiment classification. In Keh-Yih Su, Jian Su, and Janyce Wiebe, editors, ACL/IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wick</author>
<author>K Rohanimanesh</author>
<author>K Bellare</author>
<author>A Culotta</author>
<author>A McCallum</author>
</authors>
<title>SampleRank: Training factor graphs with atomic gradients.</title>
<date>2011</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="7242" citStr="Wick et al., 2011" startWordPosition="1118" endWordPosition="1121"> an aspect is to a subjective phrase based on token distance and on the length of the shortest path in the dependency tree. The edge names of the shortest path are also included as features. It is further checked if no other noun than the aspect is close to the subjective phrase. Inference during training and testing is done via Markov Chain Monte Carlo (MCMC). In each sampling step (with options of adding a span, removing a span, adding an aspect as target to a subjective phrase), the respective factors lead to an associated model score. The model parameters are adapted based on sample rank (Wick et al., 2011) using an objective function which computes the fraction of correctly predicted tokens in a span. For details on the model configuration and its implementation in FACTORIE (McCallum et al., 2009), we refer to the description in the original paper (Klinger and Cimiano, 2013). The objective function to evaluate a span r during training is Jr n gJ JgJ 1https://bitbucket.org/rklinger/jfsa f(r) = max g∈s α - Jr\gJ , 154 where g is the set of all gold spans, and |r n g| is the number of tokens shared by gold and predicted span and |r\g |the number of predicted tokens which are not part of the gold s</context>
</contexts>
<marker>Wick, Rohanimanesh, Bellare, Culotta, McCallum, 2011</marker>
<rawString>M. Wick, K. Rohanimanesh, K. Bellare, A. Culotta, and A. McCallum. 2011. SampleRank: Training factor graphs with atomic gradients. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
</authors>
<title>Annotating attributions and private states.</title>
<date>2005</date>
<booktitle>In CorpusAnno, ACL.</booktitle>
<contexts>
<context position="25146" citStr="Wilson and Wiebe, 2005" startWordPosition="4069" endWordPosition="4072">i et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al., 2013), in addition to the USAGE corpus (Klinger and Cimiano, 2014) we have used in this work, consisting of German and English reviews. The (non-fine-grained annotated) Spanish TASS corpus consists of Twitter messages (Saralegi and Vicente, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger,</context>
</contexts>
<marker>Wilson, Wiebe, 2005</marker>
<rawString>Theresa Wilson and Janyce Wiebe. 2005. Annotating attributions and private states. In CorpusAnno, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifeng Xu</author>
<author>Lin Gui</author>
<author>Jun Xu</author>
<author>Qin Lu</author>
<author>Kam-Fai Wong</author>
</authors>
<title>Cross lingual opinion holder extraction based on multi-kernel svms and transfer learning. World Wide Web Journal.</title>
<date>2013</date>
<contexts>
<context position="27133" citStr="Xu et al. (2013)" startWordPosition="4381" endWordPosition="4384">eryn et al., 2015). Brooke et al. (2009) compare dictionary and classification transfer from English to Spanish in a similar classification setting. While cross-lingual annotation projection has been investigated in the context of polarity computation, we are only aware of two approaches exploiting cross-lingual annotation projection on 159 the task of identifying aspects specifically with an evaluation on manually annotated data in more than one language. The CLOpinionMiner (Zhou et al., 2015) uses an English data set which is transfered to Chinese. Models are further improved by cotraining. Xu et al. (2013) perform self-training based on a projected corpus from English to Chinese to detect opinion holders. Due to the lack of existing manually annotated resources, to our knowledge no cross-language projection approach for fine-grained annotation at the level of aspect and subjective phrases has been proposed before. The projection of annotated data sets has been investigated in a variety of applications. Early work includes an approach to the projection of part-ofspeech tags and noun phrases (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and parsing information (Hwa et al., 2005) on a parallel </context>
</contexts>
<marker>Xu, Gui, Xu, Lu, Wong, 2013</marker>
<rawString>Ruifeng Xu, Lin Gui, Jun Xu, Qin Lu, and Kam-Fai Wong. 2013. Cross lingual opinion holder extraction based on multi-kernel svms and transfer learning. World Wide Web Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Extracting opinion expressions with semi-markov conditional random fields.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="6228" citStr="Yang and Cardie, 2012" startWordPosition="944" endWordPosition="947">bjective Phrase Detection We use a supervised model induced from training data to detect aspect phrases, subjective (evaluative) phrases and their relations. The structure follows the proposed pipeline approach by Klinger and Cimiano (2013).1 However, in contrast to their work, we focus on the detection of phrases only, and exploit the detection of relations only during inference, such that the detection of relations has an effect on the detection of phrases, but is not evaluated directly. The phrase detection follows the idea of semiMarkov conditional random fields (Sarawagi and Cohen, 2004; Yang and Cardie, 2012) and models phrases as spans over tokens as variables. Factor templates for spans of type aspect and subjective take into account token strings, prefixes, suffixes, the inclusion of digits, and part-of-speech tags, both as full string and as bigrams, for the spans and their vicinity. In addition, the length of the span is modeled by cumulative binning. The relation template indicates how close an aspect is to a subjective phrase based on token distance and on the length of the shortest path in the dependency tree. The edge names of the shortest path are also included as features. It is further</context>
<context position="24502" citStr="Yang and Cardie, 2012" startWordPosition="3966" endWordPosition="3969">e machine subdomain is 1100, only 221 are predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) </context>
</contexts>
<marker>Yang, Cardie, 2012</marker>
<rawString>Bishan Yang and Claire Cardie. 2012. Extracting opinion expressions with semi-markov conditional random fields. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora. In NAACL.</title>
<date>2001</date>
<contexts>
<context position="27675" citStr="Yarowsky and Ngai, 2001" startWordPosition="4465" endWordPosition="4468">transfered to Chinese. Models are further improved by cotraining. Xu et al. (2013) perform self-training based on a projected corpus from English to Chinese to detect opinion holders. Due to the lack of existing manually annotated resources, to our knowledge no cross-language projection approach for fine-grained annotation at the level of aspect and subjective phrases has been proposed before. The projection of annotated data sets has been investigated in a variety of applications. Early work includes an approach to the projection of part-ofspeech tags and noun phrases (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and parsing information (Hwa et al., 2005) on a parallel corpus. Especially in syntactic and semantic parsing, heuristics to remove or correct spuriously projected annotations have been developed (Pad´o and Lapata, 2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations,</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora. In HLT.</title>
<date>2001</date>
<contexts>
<context position="27649" citStr="Yarowsky et al., 2001" startWordPosition="4461" endWordPosition="4464">lish data set which is transfered to Chinese. Models are further improved by cotraining. Xu et al. (2013) perform self-training based on a projected corpus from English to Chinese to detect opinion holders. Due to the lack of existing manually annotated resources, to our knowledge no cross-language projection approach for fine-grained annotation at the level of aspect and subjective phrases has been proposed before. The projection of annotated data sets has been investigated in a variety of applications. Early work includes an approach to the projection of part-ofspeech tags and noun phrases (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and parsing information (Hwa et al., 2005) on a parallel corpus. Especially in syntactic and semantic parsing, heuristics to remove or correct spuriously projected annotations have been developed (Pad´o and Lapata, 2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available contain</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Philip Resnik</author>
</authors>
<title>Crosslanguage parser adaptation between related languages.</title>
<date>2008</date>
<booktitle>In IJCNLP-08 Workshop on NLP for Less Privileged Languages.</booktitle>
<contexts>
<context position="28393" citStr="Zeman and Resnik, 2008" startWordPosition="4577" endWordPosition="4580">emantic parsing, heuristics to remove or correct spuriously projected annotations have been developed (Pad´o and Lapata, 2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive imp</context>
</contexts>
<marker>Zeman, Resnik, 2008</marker>
<rawString>Daniel Zeman and Philip Resnik. 2008. Crosslanguage parser adaptation between related languages. In IJCNLP-08 Workshop on NLP for Less Privileged Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhang</author>
<author>Yuanbin Wu</author>
<author>Yan Wu</author>
<author>Xuanjing Huang</author>
</authors>
<title>Opinion mining with sentiment graph.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Volume</booktitle>
<volume>01</volume>
<contexts>
<context position="24765" citStr="Zhang et al., 2011" startWordPosition="4010" endWordPosition="4013">m 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al.,</context>
</contexts>
<marker>Zhang, Wu, Wu, Huang, 2011</marker>
<rawString>Qi Zhang, Yuanbin Wu, Yan Wu, and Xuanjing Huang. 2011. Opinion mining with sentiment graph. In Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Volume 01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhao</author>
<author>B Qin</author>
<author>T Liu</author>
</authors>
<title>Creating a finegrained corpus for chinese sentiment analysis. Intelligent Systems,</title>
<date>2014</date>
<pages>99</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="25857" citStr="Zhao et al. (2014)" startWordPosition="4178" endWordPosition="4181">talian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al., 2013), in addition to the USAGE corpus (Klinger and Cimiano, 2014) we have used in this work, consisting of German and English reviews. The (non-fine-grained annotated) Spanish TASS corpus consists of Twitter messages (Saralegi and Vicente, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been too much work on approaches to transfer a model either directly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et al., 2010; Mihalcea et al., 2007; Balahur and Turchi, 2014). Recent work approached multilingual opinion mining on the above-mentioned multi-lingual Youtube corpus with tree kernels predicting the polarity of a comment and whether it concerns the</context>
</contexts>
<marker>Zhao, Qin, Liu, 2014</marker>
<rawString>Y. Zhao, B. Qin, and T. Liu. 2014. Creating a finegrained corpus for chinese sentiment analysis. Intelligent Systems, IEEE, PP(99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinjie Zhou</author>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Clopinionminer: Opinion target extraction in a crosslanguage scenario.</title>
<date>2015</date>
<journal>IEEE/ACM Transactions on Audio, Speech &amp; Language Processing,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="27016" citStr="Zhou et al., 2015" startWordPosition="4360" endWordPosition="4363">cting the polarity of a comment and whether it concerns the product or the video in which the product is featured. (Severyn et al., 2015). Brooke et al. (2009) compare dictionary and classification transfer from English to Spanish in a similar classification setting. While cross-lingual annotation projection has been investigated in the context of polarity computation, we are only aware of two approaches exploiting cross-lingual annotation projection on 159 the task of identifying aspects specifically with an evaluation on manually annotated data in more than one language. The CLOpinionMiner (Zhou et al., 2015) uses an English data set which is transfered to Chinese. Models are further improved by cotraining. Xu et al. (2013) perform self-training based on a projected corpus from English to Chinese to detect opinion holders. Due to the lack of existing manually annotated resources, to our knowledge no cross-language projection approach for fine-grained annotation at the level of aspect and subjective phrases has been proposed before. The projection of annotated data sets has been investigated in a variety of applications. Early work includes an approach to the projection of part-ofspeech tags and no</context>
</contexts>
<marker>Zhou, Wan, Xiao, 2015</marker>
<rawString>Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2015. Clopinionminer: Opinion target extraction in a crosslanguage scenario. IEEE/ACM Transactions on Audio, Speech &amp; Language Processing, 23(4).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>