<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001268">
<title confidence="0.9981475">
Utilizing Dependency Language Models for Graph-based Dependency
Parsing Models
</title>
<author confidence="0.997525">
Wenliang Chen, Min Zhang�, and Haizhou Li
</author>
<affiliation confidence="0.992893">
Human Language Technology, Institute for Infocomm Research, Singapore
</affiliation>
<email confidence="0.995853">
{wechen, mzhang, hli}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.995632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991688">
Most previous graph-based parsing models in-
crease decoding complexity when they use
high-order features due to exact-inference de-
coding. In this paper, we present an approach
to enriching high-order feature representations
for graph-based dependency parsing models
using a dependency language model and beam
search. The dependency language model is
built on a large-amount of additional auto-
parsed data that is processed by a baseline
parser. Based on the dependency language
model, we represent a set of features for the
parsing model. Finally, the features are effi-
ciently integrated into the parsing model dur-
ing decoding using beam search. Our ap-
proach has two advantages. Firstly we utilize
rich high-order features defined over a view
of large scope and additional large raw cor-
pus. Secondly our approach does not increase
the decoding complexity. We evaluate the pro-
posed approach on English and Chinese data.
The experimental results show that our new
parser achieves the best accuracy on the Chi-
nese data and comparable accuracy with the
best known systems on the English data.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986085829268293">
In recent years, there are many data-driven mod-
els that have been proposed for dependency parsing
(McDonald and Nivre, 2007). Among them, graph-
based dependency parsing models have achieved
state-of-the-art performance for a wide range of lan-
guages as shown in recent CoNLL shared tasks
*Corresponding author
(Buchholz and Marsi, 2006; Nivre et al., 2007).
In the graph-based models, dependency parsing is
treated as a structured prediction problem in which
the graphs are usually represented as factored struc-
tures. The information of the factored structures de-
cides the features that the models can utilize. There
are several previous studies that exploit high-order
features that lead to significant improvements.
McDonald et al. (2005) and Covington (2001)
develop models that represent first-order features
over a single arc in graphs. By extending the first-
order model, McDonald and Pereira (2006) and Car-
reras (2007) exploit second-order features over two
adjacent arcs in second-order models. Koo and
Collins (2010) further propose a third-order model
that uses third-order features. These models utilize
higher-order feature representations and achieve bet-
ter performance than the first-order models. But this
achievement is at the cost of the higher decoding
complexity, from O(n2) to O(n4), where n is the
length of the input sentence. Thus, it is very hard to
develop higher-order models further in this way.
How to enrich high-order feature representations
without increasing the decoding complexity for
graph-based models becomes a very challenging
problem in the dependency parsing task. In this pa-
per, we solve this issue by enriching the feature rep-
resentations for a graph-based model using a depen-
dency language model (DLM) (Shen et al., 2008).
The N-gram DLM has the ability to predict the next
child based on the N-1 immediate previous children
and their head (Shen et al., 2008). The basic idea
behind is that we use the DLM to evaluate whether a
valid dependency tree (McDonald and Nivre, 2007)
</bodyText>
<page confidence="0.990247">
213
</page>
<note confidence="0.985736">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 213–222,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99987445">
is well-formed from a view of large scope. The pars-
ing model searches for the final dependency trees
by considering the original scores and the scores of
DLM.
In our approach, the DLM is built on a large
amount of auto-parsed data, which is processed
by an original first-order parser (McDonald et al.,
2005). We represent the features based on the DLM.
The DLM-based features can capture the N-gram in-
formation of the parent-children structures for the
parsing model. Then, they are integrated directly
in the decoding algorithms using beam-search. Our
new parsing model can utilize rich high-order fea-
ture representations but without increasing the com-
plexity.
To demonstrate the effectiveness of the proposed
approach, we conduct experiments on English and
Chinese data. The results indicate that the approach
greatly improves the accuracy. In summary, we
make the following contributions:
</bodyText>
<listItem confidence="0.964167">
• We utilize the dependency language model to
enhance the graph-based parsing model. The
DLM-based features are integrated directly into
the beam-search decoder.
• The new parsing model uses the rich high-order
features defined over a view of large scope and
and additional large raw corpus, but without in-
creasing the decoding complexity.
• Our parser achieves the best accuracy on the
Chinese data and comparable accuracy with the
best known systems on the English data.
</listItem>
<sectionHeader confidence="0.942939" genericHeader="introduction">
2 Dependency language model
</sectionHeader>
<bodyText confidence="0.989096923076923">
Language models play a very important role for sta-
tistical machine translation (SMT). The standard N-
gram based language model predicts the next word
based on the N−1 immediate previous words. How-
ever, the traditional N-gram language model can
not capture long-distance word relations. To over-
come this problem, Shen et al. (2008) proposed a
dependency language model (DLM) to exploit long-
distance word relations for SMT. The N-gram DLM
predicts the next child of a head based on the N − 1
immediate previous children and the head itself. In
this paper, we define a DLM, which is similar to the
one of Shen et al. (2008), to score entire dependency
trees.
An input sentence is denoted by x =
(x0, x1, ..., xi, ..., xn), where x0 = ROOT and
does not depend on any other token in x and each
token xi refers to a word. Let y be a depen-
dency tree for x and H(y) be a set that includes the
words that have at least one dependent. For each
xh E H(y), we have a dependency structure Dh =
(xLk, ...xL1, xh, xR1...xRm), where xLk, ...xL1 are
the children on the left side from the farthest to the
nearest and xR1...xRm are the children on the right
side from the nearest to the farthest. Probability
P(Dh) is defined as follows:
</bodyText>
<equation confidence="0.999783">
P(Dh) = PL(Dh) X PR(Dh) (1)
</equation>
<bodyText confidence="0.99944575">
Here PL and PR are left and right side generative
probabilities respectively. Suppose, we use a N-
gram dependency language model. PL is defined as
follows:
</bodyText>
<equation confidence="0.99993825">
PL(Dh) ,: PLc(xL1|xh)
XPLc(xL2|xL1, xh)
X... (2)
XPLc(xLk|xL(k−1), ..., xL(k−N+1), xh)
</equation>
<bodyText confidence="0.99990475">
where the approximation is based on the nth order
Markov assumption. The right side probability is
similar. For a dependency tree, we calculate the
probability as follows:
</bodyText>
<equation confidence="0.997725">
P(y) = 11 P(Dh) (3)
xhEH(y)
</equation>
<bodyText confidence="0.9986975">
In this paper, we use a linear model to calculate
the scores for the parsing models (defined in Section
3.1). Accordingly, we reform Equation 3. We define
fDLM as a high-dimensional feature representation
which is based on arbitrary features of PLc, PRc and
x. Then, the DLM score of tree y is in turn computed
as the inner product of fDLM with a corresponding
weight vector wDLM.
</bodyText>
<equation confidence="0.995036">
scoreDLM(y) = fDLM �wDLM (4)
</equation>
<sectionHeader confidence="0.971619" genericHeader="method">
3 Parsing with dependency language
model
</sectionHeader>
<bodyText confidence="0.997606">
In this section, we propose a parsing model which
includes the dependency language model by extend-
ing the model of McDonald et al. (2005).
</bodyText>
<page confidence="0.97709">
214
</page>
<equation confidence="0.589611">
score(w, x, g)+scoreDLM (y))
�
(
gEy
</equation>
<subsectionHeader confidence="0.998278">
3.1 Graph-based parsing model
</subsectionHeader>
<bodyText confidence="0.999253266666667">
The graph-based parsing model aims to search for
the maximum spanning tree (MST) in a graph (Mc-
Donald et al., 2005). We write (xi, xj) E y
if there is a dependency in tree y from word xi
to word xj (xi is the head and xj is the depen-
dent). A graph, denoted by Gx, consists of a set
of nodes Vx = {x0, x1, ..., xi, ..., xn1 and a set of
arcs (edges) Ex = {(xi, xj)Ii =� j, xi E Vx, xj E
(Vx − x0)1, where the nodes in Vx are the words
in x. Let T(Gx) be the set of all the subgraphs of
Gx that are valid dependency trees (McDonald and
Nivre, 2007) for sentence x.
The formulation defines the score of a depen-
dency tree y E T(Gx) to be the sum of the edge
scores,
</bodyText>
<equation confidence="0.9576365">
s(x, y) = � score(w, x, g) (5)
gEy
</equation>
<bodyText confidence="0.999759875">
where g is a spanning subgraph of y. g can be a
single dependency or adjacent dependencies. Then
y is represented as a set of factors. The model
scores each factor using a weight vector w that con-
tains the weights for the features to be learned dur-
ing training using the Margin Infused Relaxed Algo-
rithm (MIRA) (Crammer and Singer, 2003; McDon-
ald and Pereira, 2006). The scoring function is
</bodyText>
<equation confidence="0.844366">
score(w, x, g) = f(x, g) · w (6)
</equation>
<bodyText confidence="0.999927428571429">
where f(x, g) is a high-dimensional feature repre-
sentation which is based on arbitrary features of g
and x.
The parsing model finds a maximum spanning
tree (MST), which is the highest scoring tree in
T(Gx). The task of the decoding algorithm for a
given sentence x is to find y*,
</bodyText>
<equation confidence="0.991838666666667">
y* = arg max
yET (Gx) �s(x, y) = arg max score(w, x, g)
yET(Gx) gEy
</equation>
<subsectionHeader confidence="0.997851">
3.2 Add DLM scores
</subsectionHeader>
<bodyText confidence="0.99998">
In our approach, we consider the scores of the DLM
when searching for the maximum spanning tree.
Then for a given sentence x, we find y*DLM,
</bodyText>
<equation confidence="0.9905475">
y*DLM = arg max
yET (Gx)
</equation>
<bodyText confidence="0.986544125">
After adding the DLM scores, the new parsing
model can capture richer information. Figure 1 illus-
trates the changes. In the original first-order parsing
model, we only utilize the information of single arc
(xh, xL(k−1)) for xL(k−1) as shown in Figure 1-(a).
If we use 3-gram DLM, we can utilize the additional
information of the two previous children (nearer to
xh than xL(k−1)): xL(k−2) and xL(k−3) as shown in
</bodyText>
<figureCaption confidence="0.97125">
Figure 1-(b).
Figure 1: Adding the DLM scores to the parsing model
</figureCaption>
<subsectionHeader confidence="0.997453">
3.3 DLM-based feature templates
</subsectionHeader>
<bodyText confidence="0.999996933333333">
We define DLM-based features for Dh =
(xLk, ...xL1, xh, xR1...xRm). For each child xch on
the left side, we have PLc(xchJHIS), where HIS
refers to the N − 1 immediate previous right chil-
dren and head xh. Similarly, we have PRc(xchIHIS)
for each child on the right side. Let Pu(xchIHIS)
(Pu(ch) in short) be one of the above probabilities.
We use the map function 4)(Pu(ch)) to obtain the
predefined discrete value (defined in Section 5.3).
The feature templates are outlined in Table 1, where
TYPE refers to one of the types:PL or PR, h pos
refers to the part-of-speech tag of xh, h word refers
to the lexical form of xh, ch pos refers to the part-of-
speech tag of xch, and ch word refers to the lexical
form of xch.
</bodyText>
<sectionHeader confidence="0.995505" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.997992">
In this section, we turn to the problem of adding the
DLM in the decoding algorithm. We propose two
ways: (1) Rescoring, in which we rescore the K-
best list with the DLM-based features; (2) Intersect,
</bodyText>
<page confidence="0.997394">
215
</page>
<construct confidence="0.995126142857143">
&lt; 4)(P�(ch)), TYPE &gt;
&lt; 4)(P�(ch)), TYPE, h pos &gt;
&lt; 4)(P�(ch)), TYPE, h word &gt;
&lt; 4)(P�(ch)), TYPE, ch pos &gt;
&lt; 4)(P�(ch)), TYPE, ch word &gt;
&lt; 4)(P�(ch)), TYPE, h pos, ch pos &gt;
&lt; 4)(P�(ch)), TYPE, h word, ch word &gt;
</construct>
<tableCaption confidence="0.997618">
Table 1: DLM-based feature templates
</tableCaption>
<bodyText confidence="0.999651">
in which we add the DLM-based features in the de-
coding algorithm directly.
</bodyText>
<subsectionHeader confidence="0.992169">
4.1 Rescoring
</subsectionHeader>
<bodyText confidence="0.999978">
We add the DLM-based features into the decoding
procedure by using the rescoring technique used in
(Shen et al., 2008). We can use an original parser
to produce the K-best list. This method has the po-
tential to be very fast. However, because the perfor-
mance of this method is restricted to the K-best list,
we may have to set K to a high number in order to
find the best parsing tree (with DLM) or a tree ac-
ceptably close to the best (Shen et al., 2008).
</bodyText>
<subsectionHeader confidence="0.90672">
4.2 Intersect
</subsectionHeader>
<bodyText confidence="0.999982069767442">
Then, we add the DLM-based features in the decod-
ing algorithm directly. The DLM-based features are
generated online during decoding.
For our parser, we use the decoding algorithm
of McDonald et al. (2005). The algorithm was ex-
tensions of the parsing algorithm of (Eisner, 1996),
which was a modified version of the CKY chart
parsing algorithm. Here, we describe how to add
the DLM-based features in the first-order algorithm.
The second-order and higher-order algorithms can
be extended by the similar way.
The parsing algorithm independently parses the
left and right dependents of a word and combines
them later. There are two types of chart items (Mc-
Donald and Pereira, 2006): 1) a complete item in
which the words are unable to accept more depen-
dents in a certain direction; and 2) an incomplete
item in which the words can accept more dependents
in a certain direction. In the algorithm, we create
both types of chart items with two directions for all
the word pairs in a given sentence. The direction of
a dependency is from the head to the dependent. The
right (left) direction indicates the dependent is on the
right (left) side of the head. Larger chart items are
created from pairs of smaller ones in a bottom-up
style. In the following figures, complete items are
represented by triangles and incomplete items are
represented by trapezoids. Figure 2 illustrates the
cubic parsing actions of the algorithm (Eisner, 1996)
in the right direction, where s, r, and t refer to the
start and end indices of the chart items. In Figure
2-(a), all the items on the left side are complete and
the algorithm creates the incomplete item (trapezoid
on the right side) of s – t. This action builds a de-
pendency relation from s to t. In Figure 2-(b), the
item of s – r is incomplete and the item of r – t is
complete. Then the algorithm creates the complete
item of s – t. In this action, all the children of r are
generated. In Figure 2, the longer vertical edge in a
triangle or a trapezoid corresponds to the subroot of
the structure (spanning chart). For example, s is the
subroot of the span s – t in Figure 2-(a). For the left
direction case, the actions are similar.
</bodyText>
<figureCaption confidence="0.994234">
Figure 2: Cubic parsing actions of Eisner (Eisner, 1996)
</figureCaption>
<bodyText confidence="0.9999486">
Then, we add the DLM-based features into the
parsing actions. Because the parsing algorithm is
in the bottom-up style, the nearer children are gen-
erated earlier than the farther ones of the same head.
Thus, we calculate the left or right side probabil-
ity for a new child when a new dependency rela-
tion is built. For Figure 2-(a), we add the features of
PR,(xtIHIS). Figure 3 shows the structure, where
cRs refers to the current children (nearer than xt) of
xs. In the figure, HIS includes cRs and xs.
</bodyText>
<figureCaption confidence="0.98569">
Figure 3: Add DLM-based features in cubic parsing
</figureCaption>
<page confidence="0.993187">
216
</page>
<bodyText confidence="0.999964">
We use beam search to choose the one having the
overall best score as the final parse, where K spans
are built at each step (Zhang and Clark, 2008). At
each step, we perform the parsing actions in the cur-
rent beam and then choose the best K resulting spans
for the next step. The time complexity of the new de-
coding algorithm is O(Kn3) while the original one
is O(n3), where n is the length of the input sentence.
With the rich feature set in Table 1, the running time
of Intersect is longer than the time of Rescoring. But
Intersect considers more combination of spans with
the DLM-based features than Rescoring that is only
given a K-best list.
</bodyText>
<sectionHeader confidence="0.996127" genericHeader="method">
5 Implementation Details
</sectionHeader>
<subsectionHeader confidence="0.981406">
5.1 Baseline parser
</subsectionHeader>
<bodyText confidence="0.999910142857143">
We implement our parsers based on the MSTParser1,
a freely available implementation of the graph-based
model proposed by (McDonald and Pereira, 2006).
We train a first-order parser on the training data (de-
scribed in Section 6.1) with the features defined in
McDonald et al. (2005). We call this first-order
parser Baseline parser.
</bodyText>
<subsectionHeader confidence="0.99607">
5.2 Build dependency language models
</subsectionHeader>
<bodyText confidence="0.999978222222222">
We use a large amount of unannotated data to build
the dependency language model. We first perform
word segmentation (if needed) and part-of-speech
tagging. After that, we obtain the word-segmented
sentences with the part-of-speech tags. Then the
sentences are parsed by the Baseline parser. Finally,
we obtain the auto-parsed data.
Given the dependency trees, we estimate the prob-
ability distribution by relative frequency:
</bodyText>
<equation confidence="0.982266666666667">
count(xch, HIS)
Pu(xch|HIS) = ExI count(x&apos; HIS) (7)
ch ch,
</equation>
<bodyText confidence="0.9949055">
No smoothing is performed because we use the
mapping function for the feature representations.
</bodyText>
<subsectionHeader confidence="0.997733">
5.3 Mapping function
</subsectionHeader>
<bodyText confidence="0.9999318">
We can define different mapping functions for the
feature representations. Here, we use a simple way.
First, the probabilities are sorted in decreasing order.
Let No(Pu(ch)) be the position number of Pu(ch)
in the sorted list. The mapping function is:
</bodyText>
<footnote confidence="0.798102">
1http://mstparser.sourceforge.net
</footnote>
<equation confidence="0.8896715">
PH if No(P� (ch)) &lt; TOP10
&lt;IIl PM if TOP10 &lt; No(P�(ch)) &lt; TOP30
PL ifTOP30 &lt; No(P�(ch))
PO if P�(ch)) = 0
</equation>
<bodyText confidence="0.9999905">
where TOP10 and TOP 30 refer to the position num-
bers of top 10% and top 30% respectively. The num-
bers, 10% and 30%, are tuned on the development
sets in the experiments.
</bodyText>
<sectionHeader confidence="0.999642" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9997525">
We conducted experiments on English and Chinese
data.
</bodyText>
<subsectionHeader confidence="0.997286">
6.1 Data sets
</subsectionHeader>
<bodyText confidence="0.999780533333333">
For English, we used the Penn Treebank (Marcus et
al., 1993) in our experiments. We created a stan-
dard data split: sections 2-21 for training, section
22 for development, and section 23 for testing. Tool
“Penn2Malt”2 was used to convert the data into de-
pendency structures using a standard set of head
rules (Yamada and Matsumoto, 2003). Following
the work of (Koo et al., 2008), we used the MX-
POST (Ratnaparkhi, 1996) tagger trained on training
data to provide part-of-speech tags for the develop-
ment and the test set, and used 10-way jackknifing
to generate part-of-speech tags for the training set.
For the unannotated data, we used the BLLIP corpus
(Charniak et al., 2000) that contains about 43 million
words of WSJ text.3 We used the MXPOST tagger
trained on training data to assign part-of-speech tags
and used the Baseline parser to process the sentences
of the BLLIP corpus.
For Chinese, we used the Chinese Treebank
(CTB) version 4.04 in the experiments. We also used
the “Penn2Malt” tool to convert the data and cre-
ated a data split: files 1-270 and files 400-931 for
training, files 271-300 for testing, and files 301-325
for development. We used gold standard segmenta-
tion and part-of-speech tags in the CTB. The data
partition and part-of-speech settings were chosen to
match previous work (Chen et al., 2008; Yu et al.,
2008; Chen et al., 2009). For the unannotated data,
we used the XIN CMN portion of Chinese Giga-
word5 Version 2.0 (LDC2009T14) (Huang, 2009),
</bodyText>
<footnote confidence="0.998495833333333">
2http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html
3We ensured that the text used for extracting subtrees did not
include the sentences of the Penn Treebank.
4http://www.cis.upenn.edu/˜chinese/.
5We excluded the sentences of the CTB data from the Giga-
word data
</footnote>
<equation confidence="0.955159">
41(Pu(ch)) =
</equation>
<page confidence="0.995405">
217
</page>
<bodyText confidence="0.999983875">
which has approximately 311 million words whose
segmentation and POS tags are given. We discarded
the annotations due to the differences in annotation
policy between CTB and this corpus. We used the
MMA system (Kruengkrai et al., 2009) trained on
the training data to perform word segmentation and
POS tagging and used the Baseline parser to parse
all the sentences in the data.
</bodyText>
<subsectionHeader confidence="0.899838">
6.2 Features for basic and enhanced parsers
</subsectionHeader>
<bodyText confidence="0.99979285">
The previous studies have defined four types of
features: (FT1) the first-order features defined in
McDonald et al. (2005), (FT2SB) the second-order
parent-siblings features defined in McDonald and
Pereira (2006), (FT2GC) the second-order parent-
child-grandchild features defined in Carreras (2007),
and (FT3) the third-order features defined in (Koo
and Collins, 2010).
We used the first- and second-order parsers of
the MSTParser as the basic parsers. Then we en-
hanced them with other higher-order features us-
ing beam-search. Table 2 shows the feature set-
tings of the systems, where MST1/2 refers to the ba-
sic first-/second-order parser and MSTB1/2 refers to
the enhanced first-/second-order parser. MSTB1 and
MSTB2 used the same feature setting, but used dif-
ferent order models. This resulted in the difference
of using FT2SB (beam-search in MSTB1 vs exact-
inference in MSTB2). We used these four parsers as
the Baselines in the experiments.
</bodyText>
<tableCaption confidence="0.963738">
Table 2: Baseline parsers
</tableCaption>
<bodyText confidence="0.999983">
We measured the parser quality by the unlabeled
attachment score (UAS), i.e., the percentage of to-
kens (excluding all punctuation tokens) with the cor-
rect HEAD. In the following experiments, we used
“Inter” to refer to the parser with Intersect, and
“Rescore” to refer to the parser with Rescoring.
</bodyText>
<subsectionHeader confidence="0.997579">
6.3 Development experiments
</subsectionHeader>
<bodyText confidence="0.9997308">
Since the setting of K (for beam search) affects our
parsers, we studied its influence on the development
set for English. We added the DLM-based features
to MST1. Figure 4 shows the UAS curves on the
development set, where K is beam size for Inter-
sect and K-best for Rescoring, the X-axis represents
K, and the Y-axis represents the UAS scores. The
parsing performance generally increased as the K
increased. The parser with Intersect always outper-
formed the one with Rescoring.
</bodyText>
<equation confidence="0.594469">
1 2 4 8 16
K
</equation>
<figureCaption confidence="0.998746">
Figure 4: The influence of K on the development data
</figureCaption>
<tableCaption confidence="0.976294">
Table 3: The parsing times on the development set (sec-
onds for all the sentences)
</tableCaption>
<bodyText confidence="0.978971454545454">
Table 3 shows the parsing times of Intersect on
the development set for English. By comparing the
curves of Figure 4, we can see that, while using
larger K reduced the parsing speed, it improved the
performance of our parsers. In the rest of the ex-
periments, we set K=8 in order to obtain the high
accuracy with reasonable speed and used Intersect
to add the DLM-based features.
N 0 1 2 3 4
English 91.30 91.87 92.52 92.72 92.72
Chinese 87.36 87.96 89.33 89.92 90.40
</bodyText>
<tableCaption confidence="0.989381">
Table 4: Effect of different N-gram DLMs
</tableCaption>
<bodyText confidence="0.9999747">
Then, we studied the effect of adding different N-
gram DLMs to MST1. Table 4 shows the results.
From the table, we found that the parsing perfor-
mance roughly increased as the N increased. When
N=3 and N=4, the parsers obtained the same scores
for English. For Chinese, the parser obtained the
best score when N=4. Note that the size of the Chi-
nese unannotated data was larger than that of En-
glish. In the rest of the experiments, we used 3-gram
for English and 4-gram for Chinese.
</bodyText>
<figure confidence="0.988357823529412">
System
MST1
Features
(FT1)
MSTB1
(FT1)+(FT2SB+FT2GC+FT3)
MST2
MSTB2
(FT1+FT2SB)
(FT1+FT2SB)+(FT2GC+FT3)
UAS
0.928
0.926
0.924
0.922
0.918
0.916
0.914
0.912
0.92
Rescore
Inter
4
2
1
K
16
8
English
462.3
351.9
247.4
157.1
578.2
</figure>
<page confidence="0.9913">
218
</page>
<subsectionHeader confidence="0.979534">
6.4 Main results on English data
</subsectionHeader>
<bodyText confidence="0.975792090909091">
We evaluated the systems on the testing data for En-
glish. The results are shown in Table 5, where -
DLM refers to adding the DLM-based features to the
Baselines. The parsers using the DLM-based fea-
tures consistently outperformed the Baselines. For
the basic models (MST1/2), we obtained absolute
improvements of 0.94 and 0.63 points respectively.
For the enhanced models (MSTB1/2), we found that
there were 0.63 and 0.66 points improvements re-
spectively. The improvements were significant in
McNemar’s Test (p &lt; 10−5)(Nivre et al., 2004).
</bodyText>
<table confidence="0.9993376">
Order1 UAS Order2 UAS
MST1 90.95 MST2 91.71
MST-DLM1 91.89 MST-DLM2 92.34
MSTB1 91.92 MSTB2 92.10
MSTB-DLM1 92.55 MSTB-DLM2 92.76
</table>
<tableCaption confidence="0.998896">
Table 5: Main results for English
</tableCaption>
<subsectionHeader confidence="0.979316">
6.5 Main results on Chinese data
</subsectionHeader>
<bodyText confidence="0.999352461538462">
The results are shown in Table 6, where the abbrevi-
ations used are the same as those in Table 5. As in
the English experiments, the parsers using the DLM-
based features consistently outperformed the Base-
lines. For the basic models (MST1/2), we obtained
absolute improvements of 4.28 and 3.51 points re-
spectively. For the enhanced models (MSTB1/2),
we got 3.00 and 2.93 points improvements respec-
tively. We obtained large improvements on the Chi-
nese data. The reasons may be that we use the very
large amount of data and 4-gram DLM that captures
high-order information. The improvements were
significant in McNemar’s Test (p &lt; 10−7).
</bodyText>
<table confidence="0.9995586">
Order1 UAS Order2 UAS
MST1 86.38 MST2 88.11
MST-DLM1 90.66 MST-DLM2 91.62
MSTB1 88.38 MSTB2 88.66
MSTB-DLM1 91.38 MSTB-DLM2 91.59
</table>
<tableCaption confidence="0.997986">
Table 6: Main results for Chinese
</tableCaption>
<subsectionHeader confidence="0.998925">
6.6 Compare with previous work on English
</subsectionHeader>
<bodyText confidence="0.999875464285714">
Table 7 shows the performance of the graph-based
systems that were compared, where McDonald06
refers to the second-order parser of McDonald
and Pereira (2006), Koo08-standard refers to the
second-order parser with the features defined in
Koo et al. (2008), Koo10-model1 refers to the
third-order parser with model1 of Koo and Collins
(2010), Koo08-dep2c refers to the second-order
parser with cluster-based features of (Koo et al.,
2008), Suzuki09 refers to the parser of Suzuki et
al. (2009), Chen09-ord2s refers to the second-order
parser with subtree-based features of Chen et al.
(2009), and Zhou11 refers to the second-order parser
with web-derived selectional preference features of
Zhou et al. (2011).
The results showed that our MSTB-DLM2 ob-
tained the comparable accuracy with the previous
state-of-the-art systems. Koo10-model1 (Koo and
Collins, 2010) used the third-order features and
achieved the best reported result among the super-
vised parsers. Suzuki2009 (Suzuki et al., 2009) re-
ported the best reported result by combining a Semi-
supervised Structured Conditional Model (Suzuki
and Isozaki, 2008) with the method of (Koo et al.,
2008). However, their decoding complexities were
higher than ours and we believe that the performance
of our parser can be further enhanced by integrating
their methods with our parser.
</bodyText>
<table confidence="0.999683666666667">
Type System UAS Cost
McDonald06 91.5
G Koo08-standard 92.02
Koo10-model1 93.04 O(n4)
Koo08-dep2c 93.16 O(n4)
S Suzuki09 93.79 O(n4)
Chen09-ord2s 92.51
Zhou11 92.64
D MSTB-DLM2 92.76 O(Kn3)
</table>
<tableCaption confidence="0.99484175">
Table 7: Relevant results for English. G denotes the su-
pervised graph-based parsers, S denotes the graph-based
parsers with semi-supervised methods, D denotes our
new parsers
</tableCaption>
<subsectionHeader confidence="0.998784">
6.7 Compare with previous work on Chinese
</subsectionHeader>
<bodyText confidence="0.996281571428572">
Table 8 shows the comparative results, where
Chen08 refers to the parser of (Chen et al., 2008),
Yu08 refers to the parser of (Yu et al., 2008), Zhao09
refers to the parser of (Zhao et al., 2009), and
Chen09-ord2s refers to the second-order parser with
subtree-based features of Chen et al. (2009). The
results showed that our score for this data was the
</bodyText>
<page confidence="0.997621">
219
</page>
<bodyText confidence="0.929988">
best reported so far and significantly higher than the
previous scores.
</bodyText>
<table confidence="0.852161166666667">
System UAS
Chen08 86.52
Yu08 87.26
Zhao09 87.0
Chen09-ord2s 89.43
MSTB-DLM2 91.59
</table>
<tableCaption confidence="0.997901">
Table 8: Relevant results for Chinese
</tableCaption>
<sectionHeader confidence="0.987372" genericHeader="method">
7 Analysis
</sectionHeader>
<bodyText confidence="0.999956222222222">
Dependency parsers tend to perform worse on heads
which have many children. Here, we studied the ef-
fect of DLM-based features for this structure. We
calculated the number of children for each head and
listed the accuracy changes for different numbers.
We compared the MST-DLM1 and MST1 systems
on the English data. The accuracy is the percentage
of heads having all the correct children.
Figure 5 shows the results for English, where the
X-axis represents the number of children, the Y-
axis represents the accuracies, OURS refers to MST-
DLM1, and Baseline refers to MST1. For example,
for heads having two children, Baseline obtained
89.04% accuracy while OURS obtained 89.32%.
From the figure, we found that OURS achieved bet-
ter performance consistently in all cases and when
the larger the number of children became, the more
significant the performance improvement was.
</bodyText>
<figure confidence="0.8563575">
1 2 3 4 5 6 7 8 9 10
Number of children
</figure>
<figureCaption confidence="0.998113">
Figure 5: Improvement relative to numbers of children
</figureCaption>
<sectionHeader confidence="0.999744" genericHeader="method">
8 Related work
</sectionHeader>
<bodyText confidence="0.999912483870968">
Several previous studies related to our work have
been conducted.
Koo et al. (2008) used a clustering algorithm to
produce word clusters on a large amount of unan-
notated data and represented new features based on
the clusters for dependency parsing models. Chen
et al. (2009) proposed an approach that extracted
partial tree structures from a large amount of data
and used them as the additional features to im-
prove dependency parsing. They approaches were
still restricted in a small number of arcs in the
graphs. Suzuki et al. (2009) presented a semi-
supervised learning approach. They extended a
Semi-supervised Structured Conditional Model (SS-
SCM)(Suzuki and Isozaki, 2008) to the dependency
parsing problem and combined their method with
the approach of Koo et al. (2008). In future work,
we may consider apply their methods on our parsers
to improve further.
Another group of methods are the co-
training/self-training techniques. McClosky et
al. (2006) presented a self-training approach for
phrase structure parsing. Sagae and Tsujii (2007)
used the co-training technique to improve perfor-
mance. First, two parsers were used to parse the
sentences in unannotated data. Then they selected
some sentences which have the same trees produced
by those two parsers. They retrained a parser on
newly parsed sentences and the original labeled
data. We are able to use the output of our systems
for co-training/self-training techniques.
</bodyText>
<sectionHeader confidence="0.996767" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999931">
We have presented an approach to utilizing the de-
pendency language model to improve graph-based
dependency parsing. We represent new features
based on the dependency language model and in-
tegrate them in the decoding algorithm directly us-
ing beam-search. Our approach enriches the feature
representations but without increasing the decoding
complexity. When tested on both English and Chi-
nese data, our parsers provided very competitive per-
formance compared with the best systems on the En-
glish data and achieved the best performance on the
Chinese data in the literature.
</bodyText>
<sectionHeader confidence="0.995699" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.7614805">
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
</reference>
<figure confidence="0.9973298">
Accuracy
0.9
0.8
0.7
0.6
0.5
0.4
1
Baseline
OURS
</figure>
<page confidence="0.961058">
220
</page>
<reference confidence="0.99928585046729">
CoILL-X. SIGNLL.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoILL Shared Task Session of EMILP-CoILL 2007,
pages 957–961, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP 1987-
89 WSJ Corpus Release 1, LDC2000T43. Linguistic
Data Consortium.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto,
Yujie Zhang, and Hitoshi Isahara. 2008. Dependency
parsing with short dependency relations in unlabeled
data. In Proceedings ofIJCILP 2008.
Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings ofEMILP 2009, pages 570–579, Singapore,
August.
Michael A. Covington. 2001. A dundamental algorithm
for dependency parsing. In Proceedings of the 39th
Annual ACM Southeast Conference, pages 95–102.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951–991.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLIIG1996, pages 340–345.
Chu-Ren Huang. 2009. Tagged Chinese Gigaword Ver-
sion 2.0, LDC2009T14. Linguistic Data Consortium.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL
2010, pages 1–11, Uppsala, Sweden, July. Association
for Computational Linguistics.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceedings
ofACL-08: HLT, Columbus, Ohio, June.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL-IJCILP2009, pages
513–521, Suntec, Singapore, August. Association for
Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguisticss, 19(2):313–330.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of Coling-ACL, pages 337–344.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMILP-CoILL, pages 122–131.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings ofEACL 2006, pages 81–88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91–98.
Association for Computational Linguistics.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proc. of CoILL 2004, pages
49–56.
J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the CoILL Shared Task Session ofEMILP-CoILL
2007, pages 915–932.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMILP
1996, pages 133–142.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser ensem-
bles. In Proceedings of the CoILL Shared Task Ses-
sion of EMILP-CoILL 2007, pages 1044–1050.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings ofACL-08: HLT, pages 577–585, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings ofACL-08: HLT,
pages 665–673, Columbus, Ohio, June. Association
for Computational Linguistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proceedings of EMILP2009, pages 551–560, Sin-
gapore, August. Association for Computational Lin-
guistics.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings ofIWPT 2003, pages 195–206.
K. Yu, D. Kawahara, and S. Kurohashi. 2008. Chi-
nese dependency parsing with large scale automati-
cally constructed case structures. In Proceedings of
Coling 2008, pages 1049–1056, Manchester, UK, Au-
gust.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing. In Proceedings of EMILP
2008, pages 562–571, Honolulu, Hawaii, October.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing us-
</reference>
<page confidence="0.969789">
221
</page>
<reference confidence="0.991712444444444">
ing a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55–63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting web-derived selectional preference to im-
prove statistical dependency parsing. In Proceedings
of ACL-HLT2011, pages 1556–1565, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.997978">
222
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911065">
<title confidence="0.99981">Utilizing Dependency Language Models for Graph-based Parsing Models</title>
<author confidence="0.983242">Min Chen</author>
<author confidence="0.983242">Haizhou Li</author>
<affiliation confidence="0.976912">Human Language Technology, Institute for Infocomm Research,</affiliation>
<email confidence="0.959132">mzhang,</email>
<abstract confidence="0.999435846153846">Most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding. In this paper, we present an approach to enriching high-order feature representations for graph-based dependency parsing models using a dependency language model and beam search. The dependency language model is built on a large-amount of additional autoparsed data that is processed by a baseline parser. Based on the dependency language model, we represent a set of features for the parsing model. Finally, the features are efficiently integrated into the parsing model during decoding using beam search. Our approach has two advantages. Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoILL-X. SIGNLL.</booktitle>
<contexts>
<context position="1675" citStr="Buchholz and Marsi, 2006" startWordPosition="249" endWordPosition="252">does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data. 1 Introduction In recent years, there are many data-driven models that have been proposed for dependency parsing (McDonald and Nivre, 2007). Among them, graphbased dependency parsing models have achieved state-of-the-art performance for a wide range of languages as shown in recent CoNLL shared tasks *Corresponding author (Buchholz and Marsi, 2006; Nivre et al., 2007). In the graph-based models, dependency parsing is treated as a structured prediction problem in which the graphs are usually represented as factored structures. The information of the factored structures decides the features that the models can utilize. There are several previous studies that exploit high-order features that lead to significant improvements. McDonald et al. (2005) and Covington (2001) develop models that represent first-order features over a single arc in graphs. By extending the firstorder model, McDonald and Pereira (2006) and Carreras (2007) exploit se</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoILL-X. SIGNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoILL Shared Task Session of EMILP-CoILL</booktitle>
<pages>957--961</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2264" citStr="Carreras (2007)" startWordPosition="340" endWordPosition="342"> (Buchholz and Marsi, 2006; Nivre et al., 2007). In the graph-based models, dependency parsing is treated as a structured prediction problem in which the graphs are usually represented as factored structures. The information of the factored structures decides the features that the models can utilize. There are several previous studies that exploit high-order features that lead to significant improvements. McDonald et al. (2005) and Covington (2001) develop models that represent first-order features over a single arc in graphs. By extending the firstorder model, McDonald and Pereira (2006) and Carreras (2007) exploit second-order features over two adjacent arcs in second-order models. Koo and Collins (2010) further propose a third-order model that uses third-order features. These models utilize higher-order feature representations and achieve better performance than the first-order models. But this achievement is at the cost of the higher decoding complexity, from O(n2) to O(n4), where n is the length of the input sentence. Thus, it is very hard to develop higher-order models further in this way. How to enrich high-order feature representations without increasing the decoding complexity for graph-</context>
<context position="18764" citStr="Carreras (2007)" startWordPosition="3196" endWordPosition="3197">otations due to the differences in annotation policy between CTB and this corpus. We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. 6.2 Features for basic and enhanced parsers The previous studies have defined four types of features: (FT1) the first-order features defined in McDonald et al. (2005), (FT2SB) the second-order parent-siblings features defined in McDonald and Pereira (2006), (FT2GC) the second-order parentchild-grandchild features defined in Carreras (2007), and (FT3) the third-order features defined in (Koo and Collins, 2010). We used the first- and second-order parsers of the MSTParser as the basic parsers. Then we enhanced them with other higher-order features using beam-search. Table 2 shows the feature settings of the systems, where MST1/2 refers to the basic first-/second-order parser and MSTB1/2 refers to the enhanced first-/second-order parser. MSTB1 and MSTB2 used the same feature setting, but used different order models. This resulted in the difference of using FT2SB (beam-search in MSTB1 vs exactinference in MSTB2). We used these four</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of the CoILL Shared Task Session of EMILP-CoILL 2007, pages 957–961, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Don Blaheta</author>
<author>Niyu Ge</author>
<author>Keith Hall</author>
<author>John Hale</author>
<author>Mark Johnson</author>
</authors>
<date>2000</date>
<booktitle>BLLIP 1987-89 WSJ Corpus Release 1, LDC2000T43. Linguistic Data Consortium.</booktitle>
<contexts>
<context position="16964" citStr="Charniak et al., 2000" startWordPosition="2913" endWordPosition="2916"> 1993) in our experiments. We created a standard data split: sections 2-21 for training, section 22 for development, and section 23 for testing. Tool “Penn2Malt”2 was used to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). Following the work of (Koo et al., 2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and used 10-way jackknifing to generate part-of-speech tags for the training set. For the unannotated data, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ text.3 We used the MXPOST tagger trained on training data to assign part-of-speech tags and used the Baseline parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were ch</context>
</contexts>
<marker>Charniak, Blaheta, Ge, Hall, Hale, Johnson, 2000</marker>
<rawString>Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall, John Hale, and Mark Johnson. 2000. BLLIP 1987-89 WSJ Corpus Release 1, LDC2000T43. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Daisuke Kawahara</author>
<author>Kiyotaka Uchimoto</author>
<author>Yujie Zhang</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Dependency parsing with short dependency relations in unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings ofIJCILP</booktitle>
<contexts>
<context position="17610" citStr="Chen et al., 2008" startWordPosition="3021" endWordPosition="3024">on words of WSJ text.3 We used the MXPOST tagger trained on training data to assign part-of-speech tags and used the Baseline parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008; Chen et al., 2009). For the unannotated data, we used the XIN CMN portion of Chinese Gigaword5 Version 2.0 (LDC2009T14) (Huang, 2009), 2http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html 3We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 4http://www.cis.upenn.edu/˜chinese/. 5We excluded the sentences of the CTB data from the Gigaword data 41(Pu(ch)) = 217 which has approximately 311 million words whose segmentation and POS tags are given. We discarded the annotations due to the differences in annotation policy between </context>
<context position="25047" citStr="Chen et al., 2008" startWordPosition="4225" endWordPosition="4228"> we believe that the performance of our parser can be further enhanced by integrating their methods with our parser. Type System UAS Cost McDonald06 91.5 G Koo08-standard 92.02 Koo10-model1 93.04 O(n4) Koo08-dep2c 93.16 O(n4) S Suzuki09 93.79 O(n4) Chen09-ord2s 92.51 Zhou11 92.64 D MSTB-DLM2 92.76 O(Kn3) Table 7: Relevant results for English. G denotes the supervised graph-based parsers, S denotes the graph-based parsers with semi-supervised methods, D denotes our new parsers 6.7 Compare with previous work on Chinese Table 8 shows the comparative results, where Chen08 refers to the parser of (Chen et al., 2008), Yu08 refers to the parser of (Yu et al., 2008), Zhao09 refers to the parser of (Zhao et al., 2009), and Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009). The results showed that our score for this data was the 219 best reported so far and significantly higher than the previous scores. System UAS Chen08 86.52 Yu08 87.26 Zhao09 87.0 Chen09-ord2s 89.43 MSTB-DLM2 91.59 Table 8: Relevant results for Chinese 7 Analysis Dependency parsers tend to perform worse on heads which have many children. Here, we studied the effect of DLM-based features for thi</context>
</contexts>
<marker>Chen, Kawahara, Uchimoto, Zhang, Isahara, 2008</marker>
<rawString>Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto, Yujie Zhang, and Hitoshi Isahara. 2008. Dependency parsing with short dependency relations in unlabeled data. In Proceedings ofIJCILP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving dependency parsing with subtrees from auto-parsed data.</title>
<date>2009</date>
<booktitle>In Proceedings ofEMILP 2009,</booktitle>
<pages>570--579</pages>
<location>Singapore,</location>
<contexts>
<context position="17647" citStr="Chen et al., 2009" startWordPosition="3029" endWordPosition="3032">XPOST tagger trained on training data to assign part-of-speech tags and used the Baseline parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008; Chen et al., 2009). For the unannotated data, we used the XIN CMN portion of Chinese Gigaword5 Version 2.0 (LDC2009T14) (Huang, 2009), 2http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html 3We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 4http://www.cis.upenn.edu/˜chinese/. 5We excluded the sentences of the CTB data from the Gigaword data 41(Pu(ch)) = 217 which has approximately 311 million words whose segmentation and POS tags are given. We discarded the annotations due to the differences in annotation policy between CTB and this corpus. We used the MMA </context>
<context position="23806" citStr="Chen et al. (2009)" startWordPosition="4036" endWordPosition="4039">th previous work on English Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al. (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al., 2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009), and Zhou11 refers to the second-order parser with web-derived selectional preference features of Zhou et al. (2011). The results showed that our MSTB-DLM2 obtained the comparable accuracy with the previous state-of-the-art systems. Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers. Suzuki2009 (Suzuki et al., 2009) reported the best reported result by combining a Semisupervised Structured Conditional Model (Suzuki and Isozaki, 2008) with the method of (Koo et al., 2008). However, their decoding complexities we</context>
<context position="25249" citStr="Chen et al. (2009)" startWordPosition="4260" endWordPosition="4263">Koo08-dep2c 93.16 O(n4) S Suzuki09 93.79 O(n4) Chen09-ord2s 92.51 Zhou11 92.64 D MSTB-DLM2 92.76 O(Kn3) Table 7: Relevant results for English. G denotes the supervised graph-based parsers, S denotes the graph-based parsers with semi-supervised methods, D denotes our new parsers 6.7 Compare with previous work on Chinese Table 8 shows the comparative results, where Chen08 refers to the parser of (Chen et al., 2008), Yu08 refers to the parser of (Yu et al., 2008), Zhao09 refers to the parser of (Zhao et al., 2009), and Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009). The results showed that our score for this data was the 219 best reported so far and significantly higher than the previous scores. System UAS Chen08 86.52 Yu08 87.26 Zhao09 87.0 Chen09-ord2s 89.43 MSTB-DLM2 91.59 Table 8: Relevant results for Chinese 7 Analysis Dependency parsers tend to perform worse on heads which have many children. Here, we studied the effect of DLM-based features for this structure. We calculated the number of children for each head and listed the accuracy changes for different numbers. We compared the MST-DLM1 and MST1 systems on the English data. The accuracy is the </context>
<context position="26769" citStr="Chen et al. (2009)" startWordPosition="4514" endWordPosition="4517">% accuracy while OURS obtained 89.32%. From the figure, we found that OURS achieved better performance consistently in all cases and when the larger the number of children became, the more significant the performance improvement was. 1 2 3 4 5 6 7 8 9 10 Number of children Figure 5: Improvement relative to numbers of children 8 Related work Several previous studies related to our work have been conducted. Koo et al. (2008) used a clustering algorithm to produce word clusters on a large amount of unannotated data and represented new features based on the clusters for dependency parsing models. Chen et al. (2009) proposed an approach that extracted partial tree structures from a large amount of data and used them as the additional features to improve dependency parsing. They approaches were still restricted in a small number of arcs in the graphs. Suzuki et al. (2009) presented a semisupervised learning approach. They extended a Semi-supervised Structured Conditional Model (SSSCM)(Suzuki and Isozaki, 2008) to the dependency parsing problem and combined their method with the approach of Koo et al. (2008). In future work, we may consider apply their methods on our parsers to improve further. Another gro</context>
</contexts>
<marker>Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Improving dependency parsing with subtrees from auto-parsed data. In Proceedings ofEMILP 2009, pages 570–579, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>A dundamental algorithm for dependency parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual ACM Southeast Conference,</booktitle>
<pages>95--102</pages>
<contexts>
<context position="2101" citStr="Covington (2001)" startWordPosition="315" endWordPosition="316">based dependency parsing models have achieved state-of-the-art performance for a wide range of languages as shown in recent CoNLL shared tasks *Corresponding author (Buchholz and Marsi, 2006; Nivre et al., 2007). In the graph-based models, dependency parsing is treated as a structured prediction problem in which the graphs are usually represented as factored structures. The information of the factored structures decides the features that the models can utilize. There are several previous studies that exploit high-order features that lead to significant improvements. McDonald et al. (2005) and Covington (2001) develop models that represent first-order features over a single arc in graphs. By extending the firstorder model, McDonald and Pereira (2006) and Carreras (2007) exploit second-order features over two adjacent arcs in second-order models. Koo and Collins (2010) further propose a third-order model that uses third-order features. These models utilize higher-order feature representations and achieve better performance than the first-order models. But this achievement is at the cost of the higher decoding complexity, from O(n2) to O(n4), where n is the length of the input sentence. Thus, it is v</context>
</contexts>
<marker>Covington, 2001</marker>
<rawString>Michael A. Covington. 2001. A dundamental algorithm for dependency parsing. In Proceedings of the 39th Annual ACM Southeast Conference, pages 95–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--951</pages>
<contexts>
<context position="8334" citStr="Crammer and Singer, 2003" startWordPosition="1392" endWordPosition="1395">odes in Vx are the words in x. Let T(Gx) be the set of all the subgraphs of Gx that are valid dependency trees (McDonald and Nivre, 2007) for sentence x. The formulation defines the score of a dependency tree y E T(Gx) to be the sum of the edge scores, s(x, y) = � score(w, x, g) (5) gEy where g is a spanning subgraph of y. g can be a single dependency or adjacent dependencies. Then y is represented as a set of factors. The model scores each factor using a weight vector w that contains the weights for the features to be learned during training using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; McDonald and Pereira, 2006). The scoring function is score(w, x, g) = f(x, g) · w (6) where f(x, g) is a high-dimensional feature representation which is based on arbitrary features of g and x. The parsing model finds a maximum spanning tree (MST), which is the highest scoring tree in T(Gx). The task of the decoding algorithm for a given sentence x is to find y*, y* = arg max yET (Gx) �s(x, y) = arg max score(w, x, g) yET(Gx) gEy 3.2 Add DLM scores In our approach, we consider the scores of the DLM when searching for the maximum spanning tree. Then for a given sentence x, we find y*DLM, y*DL</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. J. Mach. Learn. Res., 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLIIG1996,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="11480" citStr="Eisner, 1996" startWordPosition="1970" endWordPosition="1971"> original parser to produce the K-best list. This method has the potential to be very fast. However, because the performance of this method is restricted to the K-best list, we may have to set K to a high number in order to find the best parsing tree (with DLM) or a tree acceptably close to the best (Shen et al., 2008). 4.2 Intersect Then, we add the DLM-based features in the decoding algorithm directly. The DLM-based features are generated online during decoding. For our parser, we use the decoding algorithm of McDonald et al. (2005). The algorithm was extensions of the parsing algorithm of (Eisner, 1996), which was a modified version of the CKY chart parsing algorithm. Here, we describe how to add the DLM-based features in the first-order algorithm. The second-order and higher-order algorithms can be extended by the similar way. The parsing algorithm independently parses the left and right dependents of a word and combines them later. There are two types of chart items (McDonald and Pereira, 2006): 1) a complete item in which the words are unable to accept more dependents in a certain direction; and 2) an incomplete item in which the words can accept more dependents in a certain direction. In</context>
<context position="13428" citStr="Eisner, 1996" startWordPosition="2319" endWordPosition="2320">es the incomplete item (trapezoid on the right side) of s – t. This action builds a dependency relation from s to t. In Figure 2-(b), the item of s – r is incomplete and the item of r – t is complete. Then the algorithm creates the complete item of s – t. In this action, all the children of r are generated. In Figure 2, the longer vertical edge in a triangle or a trapezoid corresponds to the subroot of the structure (spanning chart). For example, s is the subroot of the span s – t in Figure 2-(a). For the left direction case, the actions are similar. Figure 2: Cubic parsing actions of Eisner (Eisner, 1996) Then, we add the DLM-based features into the parsing actions. Because the parsing algorithm is in the bottom-up style, the nearer children are generated earlier than the farther ones of the same head. Thus, we calculate the left or right side probability for a new child when a new dependency relation is built. For Figure 2-(a), we add the features of PR,(xtIHIS). Figure 3 shows the structure, where cRs refers to the current children (nearer than xt) of xs. In the figure, HIS includes cRs and xs. Figure 3: Add DLM-based features in cubic parsing 216 We use beam search to choose the one having </context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLIIG1996, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-Ren Huang</author>
</authors>
<title>Tagged Chinese Gigaword Version 2.0, LDC2009T14. Linguistic Data Consortium.</title>
<date>2009</date>
<contexts>
<context position="17762" citStr="Huang, 2009" startWordPosition="3050" endWordPosition="3051"> of the BLLIP corpus. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008; Chen et al., 2009). For the unannotated data, we used the XIN CMN portion of Chinese Gigaword5 Version 2.0 (LDC2009T14) (Huang, 2009), 2http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html 3We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 4http://www.cis.upenn.edu/˜chinese/. 5We excluded the sentences of the CTB data from the Gigaword data 41(Pu(ch)) = 217 which has approximately 311 million words whose segmentation and POS tags are given. We discarded the annotations due to the differences in annotation policy between CTB and this corpus. We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used</context>
</contexts>
<marker>Huang, 2009</marker>
<rawString>Chu-Ren Huang. 2009. Tagged Chinese Gigaword Version 2.0, LDC2009T14. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010,</booktitle>
<pages>1--11</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2364" citStr="Koo and Collins (2010)" startWordPosition="353" endWordPosition="356">g is treated as a structured prediction problem in which the graphs are usually represented as factored structures. The information of the factored structures decides the features that the models can utilize. There are several previous studies that exploit high-order features that lead to significant improvements. McDonald et al. (2005) and Covington (2001) develop models that represent first-order features over a single arc in graphs. By extending the firstorder model, McDonald and Pereira (2006) and Carreras (2007) exploit second-order features over two adjacent arcs in second-order models. Koo and Collins (2010) further propose a third-order model that uses third-order features. These models utilize higher-order feature representations and achieve better performance than the first-order models. But this achievement is at the cost of the higher decoding complexity, from O(n2) to O(n4), where n is the length of the input sentence. Thus, it is very hard to develop higher-order models further in this way. How to enrich high-order feature representations without increasing the decoding complexity for graph-based models becomes a very challenging problem in the dependency parsing task. In this paper, we so</context>
<context position="18835" citStr="Koo and Collins, 2010" startWordPosition="3205" endWordPosition="3208"> and this corpus. We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. 6.2 Features for basic and enhanced parsers The previous studies have defined four types of features: (FT1) the first-order features defined in McDonald et al. (2005), (FT2SB) the second-order parent-siblings features defined in McDonald and Pereira (2006), (FT2GC) the second-order parentchild-grandchild features defined in Carreras (2007), and (FT3) the third-order features defined in (Koo and Collins, 2010). We used the first- and second-order parsers of the MSTParser as the basic parsers. Then we enhanced them with other higher-order features using beam-search. Table 2 shows the feature settings of the systems, where MST1/2 refers to the basic first-/second-order parser and MSTB1/2 refers to the enhanced first-/second-order parser. MSTB1 and MSTB2 used the same feature setting, but used different order models. This resulted in the difference of using FT2SB (beam-search in MSTB1 vs exactinference in MSTB2). We used these four parsers as the Baselines in the experiments. Table 2: Baseline parsers</context>
<context position="23556" citStr="Koo and Collins (2010)" startWordPosition="3998" endWordPosition="4001">nformation. The improvements were significant in McNemar’s Test (p &lt; 10−7). Order1 UAS Order2 UAS MST1 86.38 MST2 88.11 MST-DLM1 90.66 MST-DLM2 91.62 MSTB1 88.38 MSTB2 88.66 MSTB-DLM1 91.38 MSTB-DLM2 91.59 Table 6: Main results for Chinese 6.6 Compare with previous work on English Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al. (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al., 2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009), and Zhou11 refers to the second-order parser with web-derived selectional preference features of Zhou et al. (2011). The results showed that our MSTB-DLM2 obtained the comparable accuracy with the previous state-of-the-art systems. Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the s</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of ACL 2010, pages 1–11, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<location>Columbus, Ohio,</location>
<contexts>
<context position="16666" citStr="Koo et al., 2008" startWordPosition="2864" endWordPosition="2867">nd TOP 30 refer to the position numbers of top 10% and top 30% respectively. The numbers, 10% and 30%, are tuned on the development sets in the experiments. 6 Experiments We conducted experiments on English and Chinese data. 6.1 Data sets For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments. We created a standard data split: sections 2-21 for training, section 22 for development, and section 23 for testing. Tool “Penn2Malt”2 was used to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). Following the work of (Koo et al., 2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and used 10-way jackknifing to generate part-of-speech tags for the training set. For the unannotated data, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ text.3 We used the MXPOST tagger trained on training data to assign part-of-speech tags and used the Baseline parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used </context>
<context position="23471" citStr="Koo et al. (2008)" startWordPosition="3985" endWordPosition="3988">t we use the very large amount of data and 4-gram DLM that captures high-order information. The improvements were significant in McNemar’s Test (p &lt; 10−7). Order1 UAS Order2 UAS MST1 86.38 MST2 88.11 MST-DLM1 90.66 MST-DLM2 91.62 MSTB1 88.38 MSTB2 88.66 MSTB-DLM1 91.38 MSTB-DLM2 91.59 Table 6: Main results for Chinese 6.6 Compare with previous work on English Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al. (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al., 2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009), and Zhou11 refers to the second-order parser with web-derived selectional preference features of Zhou et al. (2011). The results showed that our MSTB-DLM2 obtained the comparable accuracy with the previous state-of-the-art systems. Koo10-model1 (Koo and Collins, </context>
<context position="26577" citStr="Koo et al. (2008)" startWordPosition="4482" endWordPosition="4485">epresents the number of children, the Yaxis represents the accuracies, OURS refers to MSTDLM1, and Baseline refers to MST1. For example, for heads having two children, Baseline obtained 89.04% accuracy while OURS obtained 89.32%. From the figure, we found that OURS achieved better performance consistently in all cases and when the larger the number of children became, the more significant the performance improvement was. 1 2 3 4 5 6 7 8 9 10 Number of children Figure 5: Improvement relative to numbers of children 8 Related work Several previous studies related to our work have been conducted. Koo et al. (2008) used a clustering algorithm to produce word clusters on a large amount of unannotated data and represented new features based on the clusters for dependency parsing models. Chen et al. (2009) proposed an approach that extracted partial tree structures from a large amount of data and used them as the additional features to improve dependency parsing. They approaches were still restricted in a small number of arcs in the graphs. Suzuki et al. (2009) presented a semisupervised learning approach. They extended a Semi-supervised Structured Conditional Model (SSSCM)(Suzuki and Isozaki, 2008) to the</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL-08: HLT, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCILP2009,</booktitle>
<pages>513--521</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="18279" citStr="Kruengkrai et al., 2009" startWordPosition="3122" endWordPosition="3125">e unannotated data, we used the XIN CMN portion of Chinese Gigaword5 Version 2.0 (LDC2009T14) (Huang, 2009), 2http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html 3We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 4http://www.cis.upenn.edu/˜chinese/. 5We excluded the sentences of the CTB data from the Gigaword data 41(Pu(ch)) = 217 which has approximately 311 million words whose segmentation and POS tags are given. We discarded the annotations due to the differences in annotation policy between CTB and this corpus. We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. 6.2 Features for basic and enhanced parsers The previous studies have defined four types of features: (FT1) the first-order features defined in McDonald et al. (2005), (FT2SB) the second-order parent-siblings features defined in McDonald and Pereira (2006), (FT2GC) the second-order parentchild-grandchild features defined in Carreras (2007), and (FT3) the third-order features defined in (Koo and Collins, 2010). We used the first- and second-order parser</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In Proceedings of ACL-IJCILP2009, pages 513–521, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguisticss,</title>
<date>1993</date>
<contexts>
<context position="16348" citStr="Marcus et al., 1993" startWordPosition="2810" endWordPosition="2813">use a simple way. First, the probabilities are sorted in decreasing order. Let No(Pu(ch)) be the position number of Pu(ch) in the sorted list. The mapping function is: 1http://mstparser.sourceforge.net PH if No(P� (ch)) &lt; TOP10 &lt;IIl PM if TOP10 &lt; No(P�(ch)) &lt; TOP30 PL ifTOP30 &lt; No(P�(ch)) PO if P�(ch)) = 0 where TOP10 and TOP 30 refer to the position numbers of top 10% and top 30% respectively. The numbers, 10% and 30%, are tuned on the development sets in the experiments. 6 Experiments We conducted experiments on English and Chinese data. 6.1 Data sets For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments. We created a standard data split: sections 2-21 for training, section 22 for development, and section 23 for testing. Tool “Penn2Malt”2 was used to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). Following the work of (Koo et al., 2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and used 10-way jackknifing to generate part-of-speech tags for the training set. For the unannotated data, we used the BLLIP corpus (Charni</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguisticss, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of Coling-ACL,</booktitle>
<pages>337--344</pages>
<contexts>
<context position="27450" citStr="McClosky et al. (2006)" startWordPosition="4621" endWordPosition="4624">s from a large amount of data and used them as the additional features to improve dependency parsing. They approaches were still restricted in a small number of arcs in the graphs. Suzuki et al. (2009) presented a semisupervised learning approach. They extended a Semi-supervised Structured Conditional Model (SSSCM)(Suzuki and Isozaki, 2008) to the dependency parsing problem and combined their method with the approach of Koo et al. (2008). In future work, we may consider apply their methods on our parsers to improve further. Another group of methods are the cotraining/self-training techniques. McClosky et al. (2006) presented a self-training approach for phrase structure parsing. Sagae and Tsujii (2007) used the co-training technique to improve performance. First, two parsers were used to parse the sentences in unannotated data. Then they selected some sentences which have the same trees produced by those two parsers. They retrained a parser on newly parsed sentences and the original labeled data. We are able to use the output of our systems for co-training/self-training techniques. 9 Conclusion We have presented an approach to utilizing the dependency language model to improve graph-based dependency par</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006. Reranking and self-training for parser adaptation. In Proceedings of Coling-ACL, pages 337–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>J Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of EMILP-CoILL,</booktitle>
<pages>122--131</pages>
<contexts>
<context position="1466" citStr="McDonald and Nivre, 2007" startWordPosition="218" endWordPosition="221">g model during decoding using beam search. Our approach has two advantages. Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data. 1 Introduction In recent years, there are many data-driven models that have been proposed for dependency parsing (McDonald and Nivre, 2007). Among them, graphbased dependency parsing models have achieved state-of-the-art performance for a wide range of languages as shown in recent CoNLL shared tasks *Corresponding author (Buchholz and Marsi, 2006; Nivre et al., 2007). In the graph-based models, dependency parsing is treated as a structured prediction problem in which the graphs are usually represented as factored structures. The information of the factored structures decides the features that the models can utilize. There are several previous studies that exploit high-order features that lead to significant improvements. McDonald</context>
<context position="3358" citStr="McDonald and Nivre, 2007" startWordPosition="514" endWordPosition="517"> models further in this way. How to enrich high-order feature representations without increasing the decoding complexity for graph-based models becomes a very challenging problem in the dependency parsing task. In this paper, we solve this issue by enriching the feature representations for a graph-based model using a dependency language model (DLM) (Shen et al., 2008). The N-gram DLM has the ability to predict the next child based on the N-1 immediate previous children and their head (Shen et al., 2008). The basic idea behind is that we use the DLM to evaluate whether a valid dependency tree (McDonald and Nivre, 2007) 213 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 213–222, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics is well-formed from a view of large scope. The parsing model searches for the final dependency trees by considering the original scores and the scores of DLM. In our approach, the DLM is built on a large amount of auto-parsed data, which is processed by an original first-order parser (McDonald et al., 2005). We represent the features based on the DLM. The DLM-based features can capture the N-gram i</context>
<context position="7847" citStr="McDonald and Nivre, 2007" startWordPosition="1297" endWordPosition="1300">(2005). 214 score(w, x, g)+scoreDLM (y)) � ( gEy 3.1 Graph-based parsing model The graph-based parsing model aims to search for the maximum spanning tree (MST) in a graph (McDonald et al., 2005). We write (xi, xj) E y if there is a dependency in tree y from word xi to word xj (xi is the head and xj is the dependent). A graph, denoted by Gx, consists of a set of nodes Vx = {x0, x1, ..., xi, ..., xn1 and a set of arcs (edges) Ex = {(xi, xj)Ii =� j, xi E Vx, xj E (Vx − x0)1, where the nodes in Vx are the words in x. Let T(Gx) be the set of all the subgraphs of Gx that are valid dependency trees (McDonald and Nivre, 2007) for sentence x. The formulation defines the score of a dependency tree y E T(Gx) to be the sum of the edge scores, s(x, y) = � score(w, x, g) (5) gEy where g is a spanning subgraph of y. g can be a single dependency or adjacent dependencies. Then y is represented as a set of factors. The model scores each factor using a weight vector w that contains the weights for the features to be learned during training using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; McDonald and Pereira, 2006). The scoring function is score(w, x, g) = f(x, g) · w (6) where f(x, g) is a high-d</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>R. McDonald and J. Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proceedings of EMILP-CoILL, pages 122–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings ofEACL</booktitle>
<pages>81--88</pages>
<contexts>
<context position="2244" citStr="McDonald and Pereira (2006)" startWordPosition="335" endWordPosition="338">ared tasks *Corresponding author (Buchholz and Marsi, 2006; Nivre et al., 2007). In the graph-based models, dependency parsing is treated as a structured prediction problem in which the graphs are usually represented as factored structures. The information of the factored structures decides the features that the models can utilize. There are several previous studies that exploit high-order features that lead to significant improvements. McDonald et al. (2005) and Covington (2001) develop models that represent first-order features over a single arc in graphs. By extending the firstorder model, McDonald and Pereira (2006) and Carreras (2007) exploit second-order features over two adjacent arcs in second-order models. Koo and Collins (2010) further propose a third-order model that uses third-order features. These models utilize higher-order feature representations and achieve better performance than the first-order models. But this achievement is at the cost of the higher decoding complexity, from O(n2) to O(n4), where n is the length of the input sentence. Thus, it is very hard to develop higher-order models further in this way. How to enrich high-order feature representations without increasing the decoding c</context>
<context position="8363" citStr="McDonald and Pereira, 2006" startWordPosition="1396" endWordPosition="1400">n x. Let T(Gx) be the set of all the subgraphs of Gx that are valid dependency trees (McDonald and Nivre, 2007) for sentence x. The formulation defines the score of a dependency tree y E T(Gx) to be the sum of the edge scores, s(x, y) = � score(w, x, g) (5) gEy where g is a spanning subgraph of y. g can be a single dependency or adjacent dependencies. Then y is represented as a set of factors. The model scores each factor using a weight vector w that contains the weights for the features to be learned during training using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; McDonald and Pereira, 2006). The scoring function is score(w, x, g) = f(x, g) · w (6) where f(x, g) is a high-dimensional feature representation which is based on arbitrary features of g and x. The parsing model finds a maximum spanning tree (MST), which is the highest scoring tree in T(Gx). The task of the decoding algorithm for a given sentence x is to find y*, y* = arg max yET (Gx) �s(x, y) = arg max score(w, x, g) yET(Gx) gEy 3.2 Add DLM scores In our approach, we consider the scores of the DLM when searching for the maximum spanning tree. Then for a given sentence x, we find y*DLM, y*DLM = arg max yET (Gx) After ad</context>
<context position="11881" citStr="McDonald and Pereira, 2006" startWordPosition="2032" endWordPosition="2036"> algorithm directly. The DLM-based features are generated online during decoding. For our parser, we use the decoding algorithm of McDonald et al. (2005). The algorithm was extensions of the parsing algorithm of (Eisner, 1996), which was a modified version of the CKY chart parsing algorithm. Here, we describe how to add the DLM-based features in the first-order algorithm. The second-order and higher-order algorithms can be extended by the similar way. The parsing algorithm independently parses the left and right dependents of a word and combines them later. There are two types of chart items (McDonald and Pereira, 2006): 1) a complete item in which the words are unable to accept more dependents in a certain direction; and 2) an incomplete item in which the words can accept more dependents in a certain direction. In the algorithm, we create both types of chart items with two directions for all the word pairs in a given sentence. The direction of a dependency is from the head to the dependent. The right (left) direction indicates the dependent is on the right (left) side of the head. Larger chart items are created from pairs of smaller ones in a bottom-up style. In the following figures, complete items are rep</context>
<context position="14825" citStr="McDonald and Pereira, 2006" startWordPosition="2564" endWordPosition="2567">and then choose the best K resulting spans for the next step. The time complexity of the new decoding algorithm is O(Kn3) while the original one is O(n3), where n is the length of the input sentence. With the rich feature set in Table 1, the running time of Intersect is longer than the time of Rescoring. But Intersect considers more combination of spans with the DLM-based features than Rescoring that is only given a K-best list. 5 Implementation Details 5.1 Baseline parser We implement our parsers based on the MSTParser1, a freely available implementation of the graph-based model proposed by (McDonald and Pereira, 2006). We train a first-order parser on the training data (described in Section 6.1) with the features defined in McDonald et al. (2005). We call this first-order parser Baseline parser. 5.2 Build dependency language models We use a large amount of unannotated data to build the dependency language model. We first perform word segmentation (if needed) and part-of-speech tagging. After that, we obtain the word-segmented sentences with the part-of-speech tags. Then the sentences are parsed by the Baseline parser. Finally, we obtain the auto-parsed data. Given the dependency trees, we estimate the prob</context>
<context position="18679" citStr="McDonald and Pereira (2006)" startWordPosition="3184" endWordPosition="3187">s approximately 311 million words whose segmentation and POS tags are given. We discarded the annotations due to the differences in annotation policy between CTB and this corpus. We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. 6.2 Features for basic and enhanced parsers The previous studies have defined four types of features: (FT1) the first-order features defined in McDonald et al. (2005), (FT2SB) the second-order parent-siblings features defined in McDonald and Pereira (2006), (FT2GC) the second-order parentchild-grandchild features defined in Carreras (2007), and (FT3) the third-order features defined in (Koo and Collins, 2010). We used the first- and second-order parsers of the MSTParser as the basic parsers. Then we enhanced them with other higher-order features using beam-search. Table 2 shows the feature settings of the systems, where MST1/2 refers to the basic first-/second-order parser and MSTB1/2 refers to the enhanced first-/second-order parser. MSTB1 and MSTB2 used the same feature setting, but used different order models. This resulted in the difference</context>
<context position="23374" citStr="McDonald and Pereira (2006)" startWordPosition="3970" endWordPosition="3973">oints improvements respectively. We obtained large improvements on the Chinese data. The reasons may be that we use the very large amount of data and 4-gram DLM that captures high-order information. The improvements were significant in McNemar’s Test (p &lt; 10−7). Order1 UAS Order2 UAS MST1 86.38 MST2 88.11 MST-DLM1 90.66 MST-DLM2 91.62 MSTB1 88.38 MSTB2 88.66 MSTB-DLM1 91.38 MSTB-DLM2 91.59 Table 6: Main results for Chinese 6.6 Compare with previous work on English Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al. (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al., 2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009), and Zhou11 refers to the second-order parser with web-derived selectional preference features of Zhou et al. (2011). The results showed that our MSTB-DLM2 obtained th</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings ofEACL 2006, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL 2005,</booktitle>
<pages>91--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2080" citStr="McDonald et al. (2005)" startWordPosition="310" endWordPosition="313">e, 2007). Among them, graphbased dependency parsing models have achieved state-of-the-art performance for a wide range of languages as shown in recent CoNLL shared tasks *Corresponding author (Buchholz and Marsi, 2006; Nivre et al., 2007). In the graph-based models, dependency parsing is treated as a structured prediction problem in which the graphs are usually represented as factored structures. The information of the factored structures decides the features that the models can utilize. There are several previous studies that exploit high-order features that lead to significant improvements. McDonald et al. (2005) and Covington (2001) develop models that represent first-order features over a single arc in graphs. By extending the firstorder model, McDonald and Pereira (2006) and Carreras (2007) exploit second-order features over two adjacent arcs in second-order models. Koo and Collins (2010) further propose a third-order model that uses third-order features. These models utilize higher-order feature representations and achieve better performance than the first-order models. But this achievement is at the cost of the higher decoding complexity, from O(n2) to O(n4), where n is the length of the input se</context>
<context position="3865" citStr="McDonald et al., 2005" startWordPosition="595" endWordPosition="598">The basic idea behind is that we use the DLM to evaluate whether a valid dependency tree (McDonald and Nivre, 2007) 213 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 213–222, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics is well-formed from a view of large scope. The parsing model searches for the final dependency trees by considering the original scores and the scores of DLM. In our approach, the DLM is built on a large amount of auto-parsed data, which is processed by an original first-order parser (McDonald et al., 2005). We represent the features based on the DLM. The DLM-based features can capture the N-gram information of the parent-children structures for the parsing model. Then, they are integrated directly in the decoding algorithms using beam-search. Our new parsing model can utilize rich high-order feature representations but without increasing the complexity. To demonstrate the effectiveness of the proposed approach, we conduct experiments on English and Chinese data. The results indicate that the approach greatly improves the accuracy. In summary, we make the following contributions: • We utilize th</context>
<context position="7228" citStr="McDonald et al. (2005)" startWordPosition="1165" endWordPosition="1168">lows: P(y) = 11 P(Dh) (3) xhEH(y) In this paper, we use a linear model to calculate the scores for the parsing models (defined in Section 3.1). Accordingly, we reform Equation 3. We define fDLM as a high-dimensional feature representation which is based on arbitrary features of PLc, PRc and x. Then, the DLM score of tree y is in turn computed as the inner product of fDLM with a corresponding weight vector wDLM. scoreDLM(y) = fDLM �wDLM (4) 3 Parsing with dependency language model In this section, we propose a parsing model which includes the dependency language model by extending the model of McDonald et al. (2005). 214 score(w, x, g)+scoreDLM (y)) � ( gEy 3.1 Graph-based parsing model The graph-based parsing model aims to search for the maximum spanning tree (MST) in a graph (McDonald et al., 2005). We write (xi, xj) E y if there is a dependency in tree y from word xi to word xj (xi is the head and xj is the dependent). A graph, denoted by Gx, consists of a set of nodes Vx = {x0, x1, ..., xi, ..., xn1 and a set of arcs (edges) Ex = {(xi, xj)Ii =� j, xi E Vx, xj E (Vx − x0)1, where the nodes in Vx are the words in x. Let T(Gx) be the set of all the subgraphs of Gx that are valid dependency trees (McDona</context>
<context position="11407" citStr="McDonald et al. (2005)" startWordPosition="1956" endWordPosition="1959">cedure by using the rescoring technique used in (Shen et al., 2008). We can use an original parser to produce the K-best list. This method has the potential to be very fast. However, because the performance of this method is restricted to the K-best list, we may have to set K to a high number in order to find the best parsing tree (with DLM) or a tree acceptably close to the best (Shen et al., 2008). 4.2 Intersect Then, we add the DLM-based features in the decoding algorithm directly. The DLM-based features are generated online during decoding. For our parser, we use the decoding algorithm of McDonald et al. (2005). The algorithm was extensions of the parsing algorithm of (Eisner, 1996), which was a modified version of the CKY chart parsing algorithm. Here, we describe how to add the DLM-based features in the first-order algorithm. The second-order and higher-order algorithms can be extended by the similar way. The parsing algorithm independently parses the left and right dependents of a word and combines them later. There are two types of chart items (McDonald and Pereira, 2006): 1) a complete item in which the words are unable to accept more dependents in a certain direction; and 2) an incomplete item</context>
<context position="14956" citStr="McDonald et al. (2005)" startWordPosition="2587" endWordPosition="2590">inal one is O(n3), where n is the length of the input sentence. With the rich feature set in Table 1, the running time of Intersect is longer than the time of Rescoring. But Intersect considers more combination of spans with the DLM-based features than Rescoring that is only given a K-best list. 5 Implementation Details 5.1 Baseline parser We implement our parsers based on the MSTParser1, a freely available implementation of the graph-based model proposed by (McDonald and Pereira, 2006). We train a first-order parser on the training data (described in Section 6.1) with the features defined in McDonald et al. (2005). We call this first-order parser Baseline parser. 5.2 Build dependency language models We use a large amount of unannotated data to build the dependency language model. We first perform word segmentation (if needed) and part-of-speech tagging. After that, we obtain the word-segmented sentences with the part-of-speech tags. Then the sentences are parsed by the Baseline parser. Finally, we obtain the auto-parsed data. Given the dependency trees, we estimate the probability distribution by relative frequency: count(xch, HIS) Pu(xch|HIS) = ExI count(x&apos; HIS) (7) ch ch, No smoothing is performed be</context>
<context position="18589" citStr="McDonald et al. (2005)" startWordPosition="3173" endWordPosition="3176">cluded the sentences of the CTB data from the Gigaword data 41(Pu(ch)) = 217 which has approximately 311 million words whose segmentation and POS tags are given. We discarded the annotations due to the differences in annotation policy between CTB and this corpus. We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. 6.2 Features for basic and enhanced parsers The previous studies have defined four types of features: (FT1) the first-order features defined in McDonald et al. (2005), (FT2SB) the second-order parent-siblings features defined in McDonald and Pereira (2006), (FT2GC) the second-order parentchild-grandchild features defined in Carreras (2007), and (FT3) the third-order features defined in (Koo and Collins, 2010). We used the first- and second-order parsers of the MSTParser as the basic parsers. Then we enhanced them with other higher-order features using beam-search. Table 2 shows the feature settings of the systems, where MST1/2 refers to the basic first-/second-order parser and MSTB1/2 refers to the enhanced first-/second-order parser. MSTB1 and MSTB2 used </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL 2005, pages 91–98. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proc. of CoILL</booktitle>
<pages>49--56</pages>
<contexts>
<context position="22179" citStr="Nivre et al., 2004" startWordPosition="3775" endWordPosition="3778">K 16 8 English 462.3 351.9 247.4 157.1 578.2 218 6.4 Main results on English data We evaluated the systems on the testing data for English. The results are shown in Table 5, where - DLM refers to adding the DLM-based features to the Baselines. The parsers using the DLM-based features consistently outperformed the Baselines. For the basic models (MST1/2), we obtained absolute improvements of 0.94 and 0.63 points respectively. For the enhanced models (MSTB1/2), we found that there were 0.63 and 0.66 points improvements respectively. The improvements were significant in McNemar’s Test (p &lt; 10−5)(Nivre et al., 2004). Order1 UAS Order2 UAS MST1 90.95 MST2 91.71 MST-DLM1 91.89 MST-DLM2 92.34 MSTB1 91.92 MSTB2 92.10 MSTB-DLM1 92.55 MSTB-DLM2 92.76 Table 5: Main results for English 6.5 Main results on Chinese data The results are shown in Table 6, where the abbreviations used are the same as those in Table 5. As in the English experiments, the parsers using the DLMbased features consistently outperformed the Baselines. For the basic models (MST1/2), we obtained absolute improvements of 4.28 and 3.51 points respectively. For the enhanced models (MSTB1/2), we got 3.00 and 2.93 points improvements respectively.</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based dependency parsing. In Proc. of CoILL 2004, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoILL Shared Task Session ofEMILP-CoILL</booktitle>
<pages>915--932</pages>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoILL Shared Task Session ofEMILP-CoILL 2007, pages 915–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of EMILP</booktitle>
<pages>133--142</pages>
<contexts>
<context position="16706" citStr="Ratnaparkhi, 1996" startWordPosition="2873" endWordPosition="2874"> of top 10% and top 30% respectively. The numbers, 10% and 30%, are tuned on the development sets in the experiments. 6 Experiments We conducted experiments on English and Chinese data. 6.1 Data sets For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments. We created a standard data split: sections 2-21 for training, section 22 for development, and section 23 for testing. Tool “Penn2Malt”2 was used to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). Following the work of (Koo et al., 2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and used 10-way jackknifing to generate part-of-speech tags for the training set. For the unannotated data, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ text.3 We used the MXPOST tagger trained on training data to assign part-of-speech tags and used the Baseline parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of EMILP 1996, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>J Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with LR models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoILL Shared Task Session of EMILP-CoILL</booktitle>
<pages>1044--1050</pages>
<contexts>
<context position="27539" citStr="Sagae and Tsujii (2007)" startWordPosition="4633" endWordPosition="4636">ency parsing. They approaches were still restricted in a small number of arcs in the graphs. Suzuki et al. (2009) presented a semisupervised learning approach. They extended a Semi-supervised Structured Conditional Model (SSSCM)(Suzuki and Isozaki, 2008) to the dependency parsing problem and combined their method with the approach of Koo et al. (2008). In future work, we may consider apply their methods on our parsers to improve further. Another group of methods are the cotraining/self-training techniques. McClosky et al. (2006) presented a self-training approach for phrase structure parsing. Sagae and Tsujii (2007) used the co-training technique to improve performance. First, two parsers were used to parse the sentences in unannotated data. Then they selected some sentences which have the same trees produced by those two parsers. They retrained a parser on newly parsed sentences and the original labeled data. We are able to use the output of our systems for co-training/self-training techniques. 9 Conclusion We have presented an approach to utilizing the dependency language model to improve graph-based dependency parsing. We represent new features based on the dependency language model and integrate them</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>K. Sagae and J. Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In Proceedings of the CoILL Shared Task Session of EMILP-CoILL 2007, pages 1044–1050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>577--585</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3103" citStr="Shen et al., 2008" startWordPosition="468" endWordPosition="471">tions and achieve better performance than the first-order models. But this achievement is at the cost of the higher decoding complexity, from O(n2) to O(n4), where n is the length of the input sentence. Thus, it is very hard to develop higher-order models further in this way. How to enrich high-order feature representations without increasing the decoding complexity for graph-based models becomes a very challenging problem in the dependency parsing task. In this paper, we solve this issue by enriching the feature representations for a graph-based model using a dependency language model (DLM) (Shen et al., 2008). The N-gram DLM has the ability to predict the next child based on the N-1 immediate previous children and their head (Shen et al., 2008). The basic idea behind is that we use the DLM to evaluate whether a valid dependency tree (McDonald and Nivre, 2007) 213 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 213–222, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics is well-formed from a view of large scope. The parsing model searches for the final dependency trees by considering the original scores and the sc</context>
<context position="5280" citStr="Shen et al. (2008)" startWordPosition="814" endWordPosition="817">eatures defined over a view of large scope and and additional large raw corpus, but without increasing the decoding complexity. • Our parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data. 2 Dependency language model Language models play a very important role for statistical machine translation (SMT). The standard Ngram based language model predicts the next word based on the N−1 immediate previous words. However, the traditional N-gram language model can not capture long-distance word relations. To overcome this problem, Shen et al. (2008) proposed a dependency language model (DLM) to exploit longdistance word relations for SMT. The N-gram DLM predicts the next child of a head based on the N − 1 immediate previous children and the head itself. In this paper, we define a DLM, which is similar to the one of Shen et al. (2008), to score entire dependency trees. An input sentence is denoted by x = (x0, x1, ..., xi, ..., xn), where x0 = ROOT and does not depend on any other token in x and each token xi refers to a word. Let y be a dependency tree for x and H(y) be a set that includes the words that have at least one dependent. For e</context>
<context position="10852" citStr="Shen et al., 2008" startWordPosition="1853" endWordPosition="1856">f adding the DLM in the decoding algorithm. We propose two ways: (1) Rescoring, in which we rescore the Kbest list with the DLM-based features; (2) Intersect, 215 &lt; 4)(P�(ch)), TYPE &gt; &lt; 4)(P�(ch)), TYPE, h pos &gt; &lt; 4)(P�(ch)), TYPE, h word &gt; &lt; 4)(P�(ch)), TYPE, ch pos &gt; &lt; 4)(P�(ch)), TYPE, ch word &gt; &lt; 4)(P�(ch)), TYPE, h pos, ch pos &gt; &lt; 4)(P�(ch)), TYPE, h word, ch word &gt; Table 1: DLM-based feature templates in which we add the DLM-based features in the decoding algorithm directly. 4.1 Rescoring We add the DLM-based features into the decoding procedure by using the rescoring technique used in (Shen et al., 2008). We can use an original parser to produce the K-best list. This method has the potential to be very fast. However, because the performance of this method is restricted to the K-best list, we may have to set K to a high number in order to find the best parsing tree (with DLM) or a tree acceptably close to the best (Shen et al., 2008). 4.2 Intersect Then, we add the DLM-based features in the decoding algorithm directly. The DLM-based features are generated online during decoding. For our parser, we use the decoding algorithm of McDonald et al. (2005). The algorithm was extensions of the parsing</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings ofACL-08: HLT, pages 577–585, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>665--673</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="24327" citStr="Suzuki and Isozaki, 2008" startWordPosition="4112" endWordPosition="4115">(2009), Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009), and Zhou11 refers to the second-order parser with web-derived selectional preference features of Zhou et al. (2011). The results showed that our MSTB-DLM2 obtained the comparable accuracy with the previous state-of-the-art systems. Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers. Suzuki2009 (Suzuki et al., 2009) reported the best reported result by combining a Semisupervised Structured Conditional Model (Suzuki and Isozaki, 2008) with the method of (Koo et al., 2008). However, their decoding complexities were higher than ours and we believe that the performance of our parser can be further enhanced by integrating their methods with our parser. Type System UAS Cost McDonald06 91.5 G Koo08-standard 92.02 Koo10-model1 93.04 O(n4) Koo08-dep2c 93.16 O(n4) S Suzuki09 93.79 O(n4) Chen09-ord2s 92.51 Zhou11 92.64 D MSTB-DLM2 92.76 O(Kn3) Table 7: Relevant results for English. G denotes the supervised graph-based parsers, S denotes the graph-based parsers with semi-supervised methods, D denotes our new parsers 6.7 Compare with </context>
<context position="27170" citStr="Suzuki and Isozaki, 2008" startWordPosition="4575" endWordPosition="4579">een conducted. Koo et al. (2008) used a clustering algorithm to produce word clusters on a large amount of unannotated data and represented new features based on the clusters for dependency parsing models. Chen et al. (2009) proposed an approach that extracted partial tree structures from a large amount of data and used them as the additional features to improve dependency parsing. They approaches were still restricted in a small number of arcs in the graphs. Suzuki et al. (2009) presented a semisupervised learning approach. They extended a Semi-supervised Structured Conditional Model (SSSCM)(Suzuki and Isozaki, 2008) to the dependency parsing problem and combined their method with the approach of Koo et al. (2008). In future work, we may consider apply their methods on our parsers to improve further. Another group of methods are the cotraining/self-training techniques. McClosky et al. (2006) presented a self-training approach for phrase structure parsing. Sagae and Tsujii (2007) used the co-training technique to improve performance. First, two parsers were used to parse the sentences in unannotated data. Then they selected some sentences which have the same trees produced by those two parsers. They retrai</context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. In Proceedings ofACL-08: HLT, pages 665–673, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An empirical study of semi-supervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of EMILP2009,</booktitle>
<pages>551--560</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23708" citStr="Suzuki et al. (2009)" startWordPosition="4022" endWordPosition="4025">1 88.38 MSTB2 88.66 MSTB-DLM1 91.38 MSTB-DLM2 91.59 Table 6: Main results for Chinese 6.6 Compare with previous work on English Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al. (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al., 2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009), and Zhou11 refers to the second-order parser with web-derived selectional preference features of Zhou et al. (2011). The results showed that our MSTB-DLM2 obtained the comparable accuracy with the previous state-of-the-art systems. Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers. Suzuki2009 (Suzuki et al., 2009) reported the best reported result by combining a Semisupervised Structured Conditional Model (Suzuki</context>
<context position="27029" citStr="Suzuki et al. (2009)" startWordPosition="4558" endWordPosition="4561">ber of children Figure 5: Improvement relative to numbers of children 8 Related work Several previous studies related to our work have been conducted. Koo et al. (2008) used a clustering algorithm to produce word clusters on a large amount of unannotated data and represented new features based on the clusters for dependency parsing models. Chen et al. (2009) proposed an approach that extracted partial tree structures from a large amount of data and used them as the additional features to improve dependency parsing. They approaches were still restricted in a small number of arcs in the graphs. Suzuki et al. (2009) presented a semisupervised learning approach. They extended a Semi-supervised Structured Conditional Model (SSSCM)(Suzuki and Isozaki, 2008) to the dependency parsing problem and combined their method with the approach of Koo et al. (2008). In future work, we may consider apply their methods on our parsers to improve further. Another group of methods are the cotraining/self-training techniques. McClosky et al. (2006) presented a self-training approach for phrase structure parsing. Sagae and Tsujii (2007) used the co-training technique to improve performance. First, two parsers were used to pa</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An empirical study of semi-supervised structured conditional models for dependency parsing. In Proceedings of EMILP2009, pages 551–560, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings ofIWPT</booktitle>
<pages>195--206</pages>
<contexts>
<context position="16624" citStr="Yamada and Matsumoto, 2003" startWordPosition="2856" endWordPosition="2859">ifTOP30 &lt; No(P�(ch)) PO if P�(ch)) = 0 where TOP10 and TOP 30 refer to the position numbers of top 10% and top 30% respectively. The numbers, 10% and 30%, are tuned on the development sets in the experiments. 6 Experiments We conducted experiments on English and Chinese data. 6.1 Data sets For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments. We created a standard data split: sections 2-21 for training, section 22 for development, and section 23 for testing. Tool “Penn2Malt”2 was used to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). Following the work of (Koo et al., 2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and used 10-way jackknifing to generate part-of-speech tags for the training set. For the unannotated data, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ text.3 We used the MXPOST tagger trained on training data to assign part-of-speech tags and used the Baseline parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank (CTB) vers</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings ofIWPT 2003, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yu</author>
<author>D Kawahara</author>
<author>S Kurohashi</author>
</authors>
<title>Chinese dependency parsing with large scale automatically constructed case structures.</title>
<date>2008</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>1049--1056</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="17627" citStr="Yu et al., 2008" startWordPosition="3025" endWordPosition="3028">t.3 We used the MXPOST tagger trained on training data to assign part-of-speech tags and used the Baseline parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008; Chen et al., 2009). For the unannotated data, we used the XIN CMN portion of Chinese Gigaword5 Version 2.0 (LDC2009T14) (Huang, 2009), 2http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html 3We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 4http://www.cis.upenn.edu/˜chinese/. 5We excluded the sentences of the CTB data from the Gigaword data 41(Pu(ch)) = 217 which has approximately 311 million words whose segmentation and POS tags are given. We discarded the annotations due to the differences in annotation policy between CTB and this corp</context>
<context position="25095" citStr="Yu et al., 2008" startWordPosition="4235" endWordPosition="4238"> be further enhanced by integrating their methods with our parser. Type System UAS Cost McDonald06 91.5 G Koo08-standard 92.02 Koo10-model1 93.04 O(n4) Koo08-dep2c 93.16 O(n4) S Suzuki09 93.79 O(n4) Chen09-ord2s 92.51 Zhou11 92.64 D MSTB-DLM2 92.76 O(Kn3) Table 7: Relevant results for English. G denotes the supervised graph-based parsers, S denotes the graph-based parsers with semi-supervised methods, D denotes our new parsers 6.7 Compare with previous work on Chinese Table 8 shows the comparative results, where Chen08 refers to the parser of (Chen et al., 2008), Yu08 refers to the parser of (Yu et al., 2008), Zhao09 refers to the parser of (Zhao et al., 2009), and Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009). The results showed that our score for this data was the 219 best reported so far and significantly higher than the previous scores. System UAS Chen08 86.52 Yu08 87.26 Zhao09 87.0 Chen09-ord2s 89.43 MSTB-DLM2 91.59 Table 8: Relevant results for Chinese 7 Analysis Dependency parsers tend to perform worse on heads which have many children. Here, we studied the effect of DLM-based features for this structure. We calculated the number of childre</context>
</contexts>
<marker>Yu, Kawahara, Kurohashi, 2008</marker>
<rawString>K. Yu, D. Kawahara, and S. Kurohashi. 2008. Chinese dependency parsing with large scale automatically constructed case structures. In Proceedings of Coling 2008, pages 1049–1056, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transitionbased dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of EMILP</booktitle>
<pages>562--571</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="14131" citStr="Zhang and Clark, 2008" startWordPosition="2445" endWordPosition="2448">g algorithm is in the bottom-up style, the nearer children are generated earlier than the farther ones of the same head. Thus, we calculate the left or right side probability for a new child when a new dependency relation is built. For Figure 2-(a), we add the features of PR,(xtIHIS). Figure 3 shows the structure, where cRs refers to the current children (nearer than xt) of xs. In the figure, HIS includes cRs and xs. Figure 3: Add DLM-based features in cubic parsing 216 We use beam search to choose the one having the overall best score as the final parse, where K spans are built at each step (Zhang and Clark, 2008). At each step, we perform the parsing actions in the current beam and then choose the best K resulting spans for the next step. The time complexity of the new decoding algorithm is O(Kn3) while the original one is O(n3), where n is the length of the input sentence. With the rich feature set in Table 1, the running time of Intersect is longer than the time of Rescoring. But Intersect considers more combination of spans with the DLM-based features than Rescoring that is only given a K-best list. 5 Implementation Details 5.1 Baseline parser We implement our parsers based on the MSTParser1, a fre</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Y. Zhang and S. Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transitionbased dependency parsing. In Proceedings of EMILP 2008, pages 562–571, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Yan Song</author>
<author>Chunyu Kit</author>
<author>Guodong Zhou</author>
</authors>
<title>Cross language dependency parsing using a bilingual lexicon.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP2009,</booktitle>
<pages>55--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="25147" citStr="Zhao et al., 2009" startWordPosition="4245" endWordPosition="4248">with our parser. Type System UAS Cost McDonald06 91.5 G Koo08-standard 92.02 Koo10-model1 93.04 O(n4) Koo08-dep2c 93.16 O(n4) S Suzuki09 93.79 O(n4) Chen09-ord2s 92.51 Zhou11 92.64 D MSTB-DLM2 92.76 O(Kn3) Table 7: Relevant results for English. G denotes the supervised graph-based parsers, S denotes the graph-based parsers with semi-supervised methods, D denotes our new parsers 6.7 Compare with previous work on Chinese Table 8 shows the comparative results, where Chen08 refers to the parser of (Chen et al., 2008), Yu08 refers to the parser of (Yu et al., 2008), Zhao09 refers to the parser of (Zhao et al., 2009), and Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009). The results showed that our score for this data was the 219 best reported so far and significantly higher than the previous scores. System UAS Chen08 86.52 Yu08 87.26 Zhao09 87.0 Chen09-ord2s 89.43 MSTB-DLM2 91.59 Table 8: Relevant results for Chinese 7 Analysis Dependency parsers tend to perform worse on heads which have many children. Here, we studied the effect of DLM-based features for this structure. We calculated the number of children for each head and listed the accuracy changes for </context>
</contexts>
<marker>Zhao, Song, Kit, Zhou, 2009</marker>
<rawString>Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou. 2009. Cross language dependency parsing using a bilingual lexicon. In Proceedings of ACLIJCNLP2009, pages 55–63, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
<author>Kang Liu</author>
<author>Li Cai</author>
</authors>
<title>Exploiting web-derived selectional preference to improve statistical dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT2011,</booktitle>
<pages>1556--1565</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="23923" citStr="Zhou et al. (2011)" startWordPosition="4053" endWordPosition="4056">ld06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al. (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al., 2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09-ord2s refers to the second-order parser with subtree-based features of Chen et al. (2009), and Zhou11 refers to the second-order parser with web-derived selectional preference features of Zhou et al. (2011). The results showed that our MSTB-DLM2 obtained the comparable accuracy with the previous state-of-the-art systems. Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers. Suzuki2009 (Suzuki et al., 2009) reported the best reported result by combining a Semisupervised Structured Conditional Model (Suzuki and Isozaki, 2008) with the method of (Koo et al., 2008). However, their decoding complexities were higher than ours and we believe that the performance of our parser can be further enhanced by integrating their me</context>
</contexts>
<marker>Zhou, Zhao, Liu, Cai, 2011</marker>
<rawString>Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve statistical dependency parsing. In Proceedings of ACL-HLT2011, pages 1556–1565, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>