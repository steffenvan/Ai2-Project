<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002309">
<title confidence="0.997587">
Putting the User in the Loop: Interactive Maximal Marginal Relevance for
Query-Focused Summarization
</title>
<author confidence="0.998688">
Jimmy Lin, Nitin Madnani, and Bonnie J. Dorr
</author>
<affiliation confidence="0.999081">
University of Maryland
</affiliation>
<address confidence="0.880054">
College Park, MD 20742, USA
</address>
<email confidence="0.999187">
jimmylin@umd.edu, {nmadnani,bonnie}@umiacs.umd.edu
</email>
<sectionHeader confidence="0.990795" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.581598692307692">
This work represents an initial attempt to
move beyond “single-shot” summarization to
interactive summarization. We present an ex-
tension to the classic Maximal Marginal Rel-
evance (MMR) algorithm that places a user
“in the loop” to assist in candidate selec-
tion. Experiments in the complex interac-
tive Question Answering (ciQA) task at TREC
2007 show that interactively-constructed re-
sponses are significantly higher in quality than
automatically-generated ones. This novel al-
gorithm provides a starting point for future
work on interactive summarization.
</bodyText>
<sectionHeader confidence="0.998099" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.966059766666667">
Document summarization, as captured in modern
comparative evaluations such as TAC and DUC, is
mostly conceived as a “one-shot” task. However, re-
searchers have long known that information seeking
is an iterative activity, which suggests that an inter-
active approach might be worth exploring.
This paper present a simple extension of a well-
known algorithm, Maximal Marginal Relevance
(MMR) (Goldstein et al., 2000), that places the user
in the loop. MMR is an iterative algorithm, where
at each step a candidate extract c (e.g., a sentence) is
assigned the following score:
ARel(q, c) − (1 − A) max Sim(s, c)
SEs
The score consists of two components: the rele-
vance of the candidate c with respect to the query
q (Rel) and the similarity of the candidate c to each
extract s in the current summary 5 (Sim). The maxi-
mum score from these similarity comparisons is sub-
tracted from the relevance score, subjected to a tun-
ing parameter that controls the emphasis on rele-
vance and anti-redundancy. Scores are recomputed
after each step and the algorithm iterates until a stop-
ping criterion has been met (e.g., length quota).
We propose a simple extension to MMR: at each
step, we interactively ask the user to select the best
sentence for inclusion in the summary. That is, in-
stead of the system automatically selecting the can-
didate with the highest score, it presents the user
with a ranked list of candidates for selection.
</bodyText>
<sectionHeader confidence="0.845487" genericHeader="method">
2 Complex, Interactive QA
</sectionHeader>
<bodyText confidence="0.996845210526316">
One obstacle to assessing the effectiveness of in-
teractive summarization algorithms is the lack of a
suitable evaluation vehicle. Given the convergence
of complex QA and summarization (particularly the
query-focused variant) in recent years, we found an
appropriate evaluation vehicle in the ciQA (com-
plex, interactive Question Answering) task at TREC
2007 (Dang et al., 2007).
Information needs in the ciQA task, called top-
ics, consist of two parts: the question template and
the narrative. The question template is a stylized in-
formation need that has a fixed structure and free
slots whose instantiation varies across different top-
ics. The narrative is unstructured prose that elabo-
rates on the information need. For the evaluation,
NIST assessors developed 30 topics, grouped into
five templates. See Figure 1 for an example.
Participants in the task were able to deploy fully-
functional web-based QA systems, with which the
</bodyText>
<page confidence="0.984771">
305
</page>
<subsubsectionHeader confidence="0.560608">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 305–308,
</subsubsectionHeader>
<subsectionHeader confidence="0.244453">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.895208571428571">
Template: What evidence is there for transport of
[drugs] from [Mexico] to [the U.S.]?
Narrative: The analyst would like to know of efforts
to curtail the transport of drugs from Mexico to the
U.S. Specifically, the analyst would like to know of
the success of the efforts by local or international au-
thorities.
</bodyText>
<figureCaption confidence="0.997704">
Figure 1: Example topic from the TREC 2007 ciQA task.
</figureCaption>
<bodyText confidence="0.99976225">
NIST assessors interacted (serving as surrogates for
users). Upon receiving the topics, participants first
submitted an initial run. During a pre-arranged pe-
riod of time shortly thereafter, each assessor was
given five minutes to interact with the participant’s
system, live over the web. After this interaction pe-
riod, participants submitted a final run, which had
presumably gained the benefit of user interaction.
By comparing initial and final runs, it was possible
to quantify the effect of the interaction.
The target corpus was AQUAINT-2, which con-
sists of around 970k documents totaling 2.5 GB.
System responses consisted of multi-line answers
and were evaluated using the “nugget” methodol-
ogy with the “nugget pyramid” extension (Lin and
Demner-Fushman, 2006).
</bodyText>
<sectionHeader confidence="0.993744" genericHeader="method">
3 Experiment Design
</sectionHeader>
<bodyText confidence="0.9999775">
This section describes our experiments for the
TREC 2007 ciQA task. In summary: the initial run
was generated automatically using standard MMR.
The web-based interactions consisted of iterations of
interactive MMR, where the user selected the best
candidate extract at each step. The final run con-
sisted of the output of interactive MMR padded with
automatically-generated output.
Sentence extracts were used as the basic re-
sponse unit. For each topic, the top 100 documents
were retrieved from the AQUAINT-2 collection with
Lucene, using the topic template verbatim as the
query. Neither the template structure nor the narra-
tive text were exploited. All documents were then
broken into individual sentences, which served as
the pool of candidates. The relevance of each sen-
tence was computed as the sum of the inverse doc-
ument frequencies of matching terms from the topic
template. Redundancy was computed as the cosine
similarity between the current answer (consisting of
</bodyText>
<figureCaption confidence="0.833473">
Figure 2: Screenshot of the interface for interactive
</figureCaption>
<bodyText confidence="0.992808096774194">
MMR, which shows the current topic (A), the current an-
swer (B), and a ranked list of document extracts (C).
all previously-selected sentences) and the current
candidate. The relevance and redundancy scores
were then normalized and combined (A = 0.8). For
the initial run, the MMR algorithm iterated until 25
candidates had been selected.
For interactive MMR, a screenshot of the web-
based system is shown in Figure 2. The interface
consists of three elements: at the top (label A) is the
current topic; in the middle (label B) is the current
answer, containing user selections from previous it-
erations; the bottom area (label C) shows a ranked
list of candidate sentences ordered by MMR score.
At each iteration, the user is asked to select one can-
didate by clicking the “Add to answer” button next
to that candidate. The selected candidate is then
added to the current answer. Ten answer candidates
are shown per page. Clicking on a button labeled
“Show more candidates” at the bottom of the page
(not shown in the screenshot) displays the next ten
candidates. In the ciQA 2007 evaluation, NIST as-
sessors engaged with this interface for the entire al-
lotted five minute interaction period. Note that this
simple interface was designed only to assess the ef-
fectiveness of interactive MMR, and not intended to
represent an actual interactive system.
To prevent users from seeing the same sentences
repeatedly once a candidate selection has been
recorded, we divide the scores of all candidates
ranked higher than the selected candidate by two (an
</bodyText>
<page confidence="0.993083">
306
</page>
<figure confidence="0.473683">
Length of ciQA 2007 Final Answers: Number of Extracts
</figure>
<figureCaption confidence="0.996505333333333">
Figure 3: Per-topic lengths of final run in terms of num-
ber of extracts. Bars show contribution from interactive
MMR (darker) and “padding” (lighter).
</figureCaption>
<bodyText confidence="0.999970307692308">
arbitrary constant). For example, if the user clicked
on candidate five, scores for candidates one through
four are cut in half. Previous studies have shown
that users generally examine ranked lists in order, so
the lack of a selection can be interpreted as negative
feedback (Joachims et al., 2007).
The answers constructed interactively were sub-
mitted to NIST as the final (post-interaction) run.
However, since these answers were significantly
shorter than the initial run (given the short interac-
tion period), the responses were “padded” by run-
ning additional iterations of automatic MMR until a
length quota of 4000 characters had been achieved.
</bodyText>
<sectionHeader confidence="0.999671" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999981055555556">
First, we present descriptive statistics of the final
run submitted to NIST. Lengths of the answers on
a per-topic basis are shown in Figure 3 in terms of
number of extracts: darker bars show the number of
manually-selected extracts for each topic during the
five-minute interaction period (i.e., the number of in-
teractive MMR iterations). The average across all
topics was 6.5 iterations, shown by the horizontal
line; the average length of answers (all user selec-
tions) was 1186 characters. The average rank of the
user selection was 4.9, and the user selected the top
ranking sentence 28% of the time. Note that the in-
teraction period included system processing as well
as delays caused by network traffic. The number of
extracts contained in the padding is shown by the
lighter gray portions of the bars. For topic 68, the
system did not record any user interactions (possi-
bly resulting from a network glitch).
</bodyText>
<figure confidence="0.8302545">
TREC 2007 ciQA: interactive vs. non-interactive MMR
non MMR
-interactive
interactive
sig., p&lt;0.05
MMR
0 500 1000 1500 2000 2500 3000 3500 4000
Length of Answer (non-whitespace characters)
</figure>
<figureCaption confidence="0.99968">
Figure 4: Weighted recall at different length increments,
comparing interactive and non-interactive MMR.
</figureCaption>
<bodyText confidence="0.999942205882353">
The official metric for the ciQA task was F-
measure, but a disadvantage of this single-point met-
ric is that it doesn’t account for answers of vary-
ing lengths. An alternative proposed by Lin (2007)
and used as the secondary metric in the evalua-
tion is recall-by-length plots, which characterize
weighted nugget recall at varying length incre-
ments. Weighted recall captures how much rele-
vant information is contained in the system response
(weighted by each nugget’s importance, with an up-
per bound of one). Responses that achieve higher
nugget recall at shorter length increments are desir-
able in providing concise, informative answers.
Recall-by-length plots for both the initial run
(non-interactive MMR) and final run (interactive
MMR with padding) are shown in Figure 4, in length
increments of 1000 characters. The vertical dotted
line denotes the average length of interactive MMR
answers (without padding). Taking length as a proxy
for time, one natural interpretation of this plot is how
quickly users are able to “learn” about the topic of
interest under the two conditions.
We see that interactive MMR yields higher
weighted recall at all length increments. The
Wilcoxon signed-rank test was applied to assess the
statistical significance of the differences in weighted
recall at each length increment. Solid circles in the
graph represent improvements that are statistically
significant (p &lt; 0.05). Furthermore, in the 700–
1000 character range, weighted recall is significantly
higher for interactive MMR at the 99% level.
Viewing weighted recall as a proxy for answer
quality, interactive MMR yields responses that are
significantly better than non-interactive MMR at
</bodyText>
<figure confidence="0.99818092">
56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85
Topic
Number of Extracts
40
35
30
25
20
15
10
5
0
complete answer
interactive MMR
mean, interactive MMR
Weighted Recall 0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
</figure>
<page confidence="0.991847">
307
</page>
<bodyText confidence="0.999336979166667">
a range of length increments. This is an impor-
tant finding, since effective interaction techniques
that require little training and work well in limited-
duration settings are quite elusive. Often, user in-
put actually makes answers worse. Results from
both ciQA 2006 and ciQA 2007 show that, overall,
F-measure improved little between initial and final
runs. Although it is widely accepted that user feed-
back can enhance interactive IR, effective interac-
tion techniques to exploit this feedback are by no
means obvious.
To better understand the characteristics of interac-
tive MMR, it is helpful to compare our experiments
with the ciQA task-wide baseline. As a reference
for all participants, the organizers of the task sub-
mitted a pair of runs to help calibrate effectiveness.
According to Dang et al. (2007), the first run was
prepared by submitting the question template ver-
batim as a query to Lucene to retrieve the top 20
documents. These documents were then tokenized
into individual sentences. Sentences that contained
at least one non-stopword from the question were re-
tained and returned as the initial run (up to a quota
of 5,000 characters). Sentence order within each
document and across the ranked list was preserved.
The interaction associated with this run asked the as-
sessor for relevance judgments on each of the sen-
tences. Three options were given: “relevant”, “not
relevant”, and “no opinion”. The final run was pre-
pared by removing sentences judged not relevant.
Other evidence suggests that the task-wide sen-
tence retrieval algorithm represents a strong base-
line. Similar algorithms performed well in other
complex QA tasks—in TREC 2003, a sentence re-
trieval variant beat all but one run on definition ques-
tions (Voorhees, 2003). The sentence retrieval base-
line also performed well in ciQA 2006.
The MMR runs are compared to the task-wide
reference runs in Figure 5: diamonds denote the
sentence retrieval baseline and triangles mark the
manual sentence selection final run. The manual
sentence selection run outperforms the sentence re-
trieval baseline (as expected), but its weighted recall
is still below that of interactive MMR across almost
all length increments. The weighted recall of inter-
active MMR is significantly better at 1000 characters
(at the 95% level), but nowhere else. So, the bottom
line is: for limited-duration interactions, interactive
</bodyText>
<note confidence="0.792752">
TREC 2007 ciQA: MMR vs. task-wide baseline
</note>
<figure confidence="0.982902">
0 500 1000 1500 2000 2500 3000 3500 4000
Length of Answer (non-whitespace characters)
</figure>
<figureCaption confidence="0.991529">
Figure 5: Weighted recall at different length increments,
comparing MMR with the task-wide baseline.
</figureCaption>
<bodyText confidence="0.880738">
MMR is more effective than simply asking for rele-
vance judgments, but not significantly so.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99994475">
We present an interactive extension of the Maximal
Marginal Relevance algorithm for query-focused
summarization. Results from the TREC 2007 ciQA
task demonstrate it is a simple yet effective tech-
nique for involving users in interactively construct-
ing responses to complex information needs. These
results provide a starting point for future work in in-
teractive summarization.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99958">
This work was supported in part by NLM/NIH. The
first author would like to thank Esther and Kiri for
their loving support.
</bodyText>
<sectionHeader confidence="0.999272" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996108125">
H. Dang, J. Lin, and D. Kelly. 2007. Overview of the
TREC 2007 question answering track. TREC 2007.
J. Goldstein, V. Mittal, J. Carbonell, and J. Callan. 2000.
Creating and evaluating multi-document sentence ex-
tract summaries. CIKM 2000.
T. Joachims, L. Granka, B. Pan, H. Hembrooke,
F. Radlinski, and G. Gay. 2007. Evaluating the ac-
curacy of implicit feedback from clicks and query re-
formulations in Web search. TOIS, 25(2):1–27.
J. Lin and D. Demner-Fushman. 2006. Will pyramids
built of nuggets topple over? HLT/NAACL 2006.
J. Lin. 2007. Is question answering better than informa-
tion retrieval? Towards a task-based evaluation frame-
work for question series. HLT/NAACL 2007.
E. Voorhees. 2003. Overview of the TREC 2003 ques-
tion answering track. TREC 2003.
</reference>
<figure confidence="0.99357">
Weighted Recall
0.45
0.35
0.25
0.15
0.05
0.4
0.3
0.2
0.1
0
non-interactive MMR
interactive MMR
sentence retrieval baseline
manual sentence selection
</figure>
<page confidence="0.969992">
308
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.982743">
<title confidence="0.999624">Putting the User in the Loop: Interactive Maximal Marginal Relevance for Query-Focused Summarization</title>
<author confidence="0.994543">Nitin Madnani Lin</author>
<author confidence="0.994543">J</author>
<affiliation confidence="0.999906">University of</affiliation>
<address confidence="0.998612">College Park, MD 20742,</address>
<abstract confidence="0.999273142857143">This work represents an initial attempt to move beyond “single-shot” summarization to We present an extension to the classic Maximal Marginal Relevance (MMR) algorithm that places a user “in the loop” to assist in candidate selection. Experiments in the complex interactive Question Answering (ciQA) task at TREC 2007 show that interactively-constructed responses are significantly higher in quality than automatically-generated ones. This novel algorithm provides a starting point for future work on interactive summarization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Dang</author>
<author>J Lin</author>
<author>D Kelly</author>
</authors>
<title>question answering track. TREC</title>
<date>2007</date>
<journal>Overview of the TREC</journal>
<contexts>
<context position="2647" citStr="Dang et al., 2007" startWordPosition="413" endWordPosition="416">elect the best sentence for inclusion in the summary. That is, instead of the system automatically selecting the candidate with the highest score, it presents the user with a ranked list of candidates for selection. 2 Complex, Interactive QA One obstacle to assessing the effectiveness of interactive summarization algorithms is the lack of a suitable evaluation vehicle. Given the convergence of complex QA and summarization (particularly the query-focused variant) in recent years, we found an appropriate evaluation vehicle in the ciQA (complex, interactive Question Answering) task at TREC 2007 (Dang et al., 2007). Information needs in the ciQA task, called topics, consist of two parts: the question template and the narrative. The question template is a stylized information need that has a fixed structure and free slots whose instantiation varies across different topics. The narrative is unstructured prose that elaborates on the information need. For the evaluation, NIST assessors developed 30 topics, grouped into five templates. See Figure 1 for an example. Participants in the task were able to deploy fullyfunctional web-based QA systems, with which the 305 Human Language Technologies: The 2010 Annual</context>
<context position="11965" citStr="Dang et al. (2007)" startWordPosition="1920" endWordPosition="1923">Often, user input actually makes answers worse. Results from both ciQA 2006 and ciQA 2007 show that, overall, F-measure improved little between initial and final runs. Although it is widely accepted that user feedback can enhance interactive IR, effective interaction techniques to exploit this feedback are by no means obvious. To better understand the characteristics of interactive MMR, it is helpful to compare our experiments with the ciQA task-wide baseline. As a reference for all participants, the organizers of the task submitted a pair of runs to help calibrate effectiveness. According to Dang et al. (2007), the first run was prepared by submitting the question template verbatim as a query to Lucene to retrieve the top 20 documents. These documents were then tokenized into individual sentences. Sentences that contained at least one non-stopword from the question were retained and returned as the initial run (up to a quota of 5,000 characters). Sentence order within each document and across the ranked list was preserved. The interaction associated with this run asked the assessor for relevance judgments on each of the sentences. Three options were given: “relevant”, “not relevant”, and “no opinio</context>
</contexts>
<marker>Dang, Lin, Kelly, 2007</marker>
<rawString>H. Dang, J. Lin, and D. Kelly. 2007. Overview of the TREC 2007 question answering track. TREC 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldstein</author>
<author>V Mittal</author>
<author>J Carbonell</author>
<author>J Callan</author>
</authors>
<title>Creating and evaluating multi-document sentence extract summaries. CIKM</title>
<date>2000</date>
<contexts>
<context position="1239" citStr="Goldstein et al., 2000" startWordPosition="174" endWordPosition="177">ractively-constructed responses are significantly higher in quality than automatically-generated ones. This novel algorithm provides a starting point for future work on interactive summarization. 1 Introduction Document summarization, as captured in modern comparative evaluations such as TAC and DUC, is mostly conceived as a “one-shot” task. However, researchers have long known that information seeking is an iterative activity, which suggests that an interactive approach might be worth exploring. This paper present a simple extension of a wellknown algorithm, Maximal Marginal Relevance (MMR) (Goldstein et al., 2000), that places the user in the loop. MMR is an iterative algorithm, where at each step a candidate extract c (e.g., a sentence) is assigned the following score: ARel(q, c) − (1 − A) max Sim(s, c) SEs The score consists of two components: the relevance of the candidate c with respect to the query q (Rel) and the similarity of the candidate c to each extract s in the current summary 5 (Sim). The maximum score from these similarity comparisons is subtracted from the relevance score, subjected to a tuning parameter that controls the emphasis on relevance and anti-redundancy. Scores are recomputed a</context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Callan, 2000</marker>
<rawString>J. Goldstein, V. Mittal, J. Carbonell, and J. Callan. 2000. Creating and evaluating multi-document sentence extract summaries. CIKM 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
<author>L Granka</author>
<author>B Pan</author>
<author>H Hembrooke</author>
<author>F Radlinski</author>
<author>G Gay</author>
</authors>
<title>Evaluating the accuracy of implicit feedback from clicks and query reformulations in Web search.</title>
<date>2007</date>
<journal>TOIS,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="7637" citStr="Joachims et al., 2007" startWordPosition="1214" endWordPosition="1217">on has been recorded, we divide the scores of all candidates ranked higher than the selected candidate by two (an 306 Length of ciQA 2007 Final Answers: Number of Extracts Figure 3: Per-topic lengths of final run in terms of number of extracts. Bars show contribution from interactive MMR (darker) and “padding” (lighter). arbitrary constant). For example, if the user clicked on candidate five, scores for candidates one through four are cut in half. Previous studies have shown that users generally examine ranked lists in order, so the lack of a selection can be interpreted as negative feedback (Joachims et al., 2007). The answers constructed interactively were submitted to NIST as the final (post-interaction) run. However, since these answers were significantly shorter than the initial run (given the short interaction period), the responses were “padded” by running additional iterations of automatic MMR until a length quota of 4000 characters had been achieved. 4 Results and Discussion First, we present descriptive statistics of the final run submitted to NIST. Lengths of the answers on a per-topic basis are shown in Figure 3 in terms of number of extracts: darker bars show the number of manually-selected</context>
</contexts>
<marker>Joachims, Granka, Pan, Hembrooke, Radlinski, Gay, 2007</marker>
<rawString>T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and G. Gay. 2007. Evaluating the accuracy of implicit feedback from clicks and query reformulations in Web search. TOIS, 25(2):1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Demner-Fushman</author>
</authors>
<title>Will pyramids built of nuggets topple over? HLT/NAACL</title>
<date>2006</date>
<contexts>
<context position="4535" citStr="Lin and Demner-Fushman, 2006" startWordPosition="709" endWordPosition="712"> a pre-arranged period of time shortly thereafter, each assessor was given five minutes to interact with the participant’s system, live over the web. After this interaction period, participants submitted a final run, which had presumably gained the benefit of user interaction. By comparing initial and final runs, it was possible to quantify the effect of the interaction. The target corpus was AQUAINT-2, which consists of around 970k documents totaling 2.5 GB. System responses consisted of multi-line answers and were evaluated using the “nugget” methodology with the “nugget pyramid” extension (Lin and Demner-Fushman, 2006). 3 Experiment Design This section describes our experiments for the TREC 2007 ciQA task. In summary: the initial run was generated automatically using standard MMR. The web-based interactions consisted of iterations of interactive MMR, where the user selected the best candidate extract at each step. The final run consisted of the output of interactive MMR padded with automatically-generated output. Sentence extracts were used as the basic response unit. For each topic, the top 100 documents were retrieved from the AQUAINT-2 collection with Lucene, using the topic template verbatim as the quer</context>
</contexts>
<marker>Lin, Demner-Fushman, 2006</marker>
<rawString>J. Lin and D. Demner-Fushman. 2006. Will pyramids built of nuggets topple over? HLT/NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
</authors>
<title>Is question answering better than information retrieval? Towards a task-based evaluation framework for question series. HLT/NAACL</title>
<date>2007</date>
<contexts>
<context position="9417" citStr="Lin (2007)" startWordPosition="1503" endWordPosition="1504">ns of the bars. For topic 68, the system did not record any user interactions (possibly resulting from a network glitch). TREC 2007 ciQA: interactive vs. non-interactive MMR non MMR -interactive interactive sig., p&lt;0.05 MMR 0 500 1000 1500 2000 2500 3000 3500 4000 Length of Answer (non-whitespace characters) Figure 4: Weighted recall at different length increments, comparing interactive and non-interactive MMR. The official metric for the ciQA task was Fmeasure, but a disadvantage of this single-point metric is that it doesn’t account for answers of varying lengths. An alternative proposed by Lin (2007) and used as the secondary metric in the evaluation is recall-by-length plots, which characterize weighted nugget recall at varying length increments. Weighted recall captures how much relevant information is contained in the system response (weighted by each nugget’s importance, with an upper bound of one). Responses that achieve higher nugget recall at shorter length increments are desirable in providing concise, informative answers. Recall-by-length plots for both the initial run (non-interactive MMR) and final run (interactive MMR with padding) are shown in Figure 4, in length increments o</context>
</contexts>
<marker>Lin, 2007</marker>
<rawString>J. Lin. 2007. Is question answering better than information retrieval? Towards a task-based evaluation framework for question series. HLT/NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<contexts>
<context position="12905" citStr="Voorhees, 2003" startWordPosition="2074" endWordPosition="2075">5,000 characters). Sentence order within each document and across the ranked list was preserved. The interaction associated with this run asked the assessor for relevance judgments on each of the sentences. Three options were given: “relevant”, “not relevant”, and “no opinion”. The final run was prepared by removing sentences judged not relevant. Other evidence suggests that the task-wide sentence retrieval algorithm represents a strong baseline. Similar algorithms performed well in other complex QA tasks—in TREC 2003, a sentence retrieval variant beat all but one run on definition questions (Voorhees, 2003). The sentence retrieval baseline also performed well in ciQA 2006. The MMR runs are compared to the task-wide reference runs in Figure 5: diamonds denote the sentence retrieval baseline and triangles mark the manual sentence selection final run. The manual sentence selection run outperforms the sentence retrieval baseline (as expected), but its weighted recall is still below that of interactive MMR across almost all length increments. The weighted recall of interactive MMR is significantly better at 1000 characters (at the 95% level), but nowhere else. So, the bottom line is: for limited-dura</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>E. Voorhees. 2003. Overview of the TREC 2003 question answering track. TREC 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>