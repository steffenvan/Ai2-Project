<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001899">
<title confidence="0.9990985">
Adaptive Referring Expression Generation in Spoken Dialogue Systems:
Evaluation with Real Users
</title>
<author confidence="0.996609">
Srinivasan Janarthanam
</author>
<affiliation confidence="0.9984285">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.998457">
s.janarthanam@ed.ac.uk
</email>
<sectionHeader confidence="0.997385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975655172414">
We present new results from a real-user
evaluation of a data-driven approach to
learning user-adaptive referring expres-
sion generation (REG) policies for spoken
dialogue systems. Referring expressions
can be difficult to understand in techni-
cal domains where users may not know
the technical ‘jargon’ names of the do-
main entities. In such cases, dialogue sys-
tems must be able to model the user’s (lex-
ical) domain knowledge and use appro-
priate referring expressions. We present
a reinforcement learning (RL) framework
in which the system learns REG policies
which can adapt to unknown users on-
line. For real users of such a system, we
show that in comparison to an adaptive
hand-coded baseline policy, the learned
policy performs significantly better, with
a 20.8% average increase in adaptation ac-
curacy, 12.6% decrease in time taken, and
a 15.1% increase in task completion rate.
The learned policy also has a significantly
better subjective rating from users. This is
because the learned policies adapt online
to changing evidence about the user’s do-
main expertise. We also discuss the issue
of evaluation in simulation versus evalua-
tion with real users.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915875">
We present new results from an evaluation with
real users, for a reinforcement learning (Sutton
and Barto, 1998) framework to learn user-adaptive
referring expression generation policies from data-
driven user simulations. Such a policy allows the
system to choose appropriate expressions to re-
fer to domain entities in a dialogue setting. For
instance, in a technical support conversation, the
</bodyText>
<author confidence="0.29139">
Oliver Lemon
</author>
<affiliation confidence="0.435134333333333">
Interaction Lab
Mathematics and Computer Science
Heriot-Watt University
</affiliation>
<email confidence="0.956303">
o.lemon@hw.ac.uk
</email>
<bodyText confidence="0.8223032">
Jargon: Please plug one end of the broadband
cable into the broadband filter.
Descriptive: Please plug one end of the thin
white cable with grey ends into the
small white box.
</bodyText>
<tableCaption confidence="0.880559">
Table 1: Referring expression examples for 2 enti-
ties (from the corpus)
</tableCaption>
<bodyText confidence="0.999524580645161">
system could choose to use more technical terms
with an expert user, or to use more descriptive and
general expressions with novice users, and a mix
of the two with intermediate users of various sorts
(see examples in Table 1).
In natural human-human conversations, dia-
logue partners learn about each other and adapt
their language to suit their domain expertise (Is-
sacs and Clark, 1987). This kind of adaptation
is called Alignment through Audience
Design (Clark and Murphy, 1982; Bell, 1984).
We assume that users are mostly unknown to
the system and therefore that a spoken dialogue
system (SDS) must be capable of observing the
user’s dialogue behaviour, modelling his/her do-
main knowledge, and adapting accordingly, just
like human interlocutors. Therefore unlike sys-
tems that use static user models, our system has to
dynamically model the user’s domain knowledge
in order to adapt during the conversation.
We present a corpus-driven framework for
learning a user-adaptive REG policy from a small
corpus of non-adaptive human-machine interac-
tion. We show that the learned policy performs
better than a simple hand-coded adaptive policy
in terms of accuracy of adaptation, dialogue time
and task completion rate when evaluated with real
users in a wizarded study.
In section 2, we present some of the related
work. Section 3 and section 4 describe the dia-
logue system framework and the user simulation
</bodyText>
<note confidence="0.6228275">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 124–131,
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</note>
<page confidence="0.998485">
124
</page>
<bodyText confidence="0.999094333333333">
model. In section 5, we present the training and in
section 6, we present the evaluation for different
REG policies with real users.
</bodyText>
<sectionHeader confidence="0.999697" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999746243902439">
Rule-based and supervised learning approaches
have been proposed to learn and adapt during
conversations dynamically. Such systems learn
from a user at the start and later adapt to the do-
main knowledge of the user. However, they either
require expensive expert knowledge resources to
hand-code the inference rules (Cawsey, 1993) or a
large corpus of expert-layperson interaction from
which adaptive strategies can be learned and mod-
elled, using methods such as Bayesian networks
(Akiba and Tanaka, 1994). In contrast, we present
an approach that learns in the absence of these
expensive resources. It is also not clear how su-
pervised approaches choose between when to seek
more information and when to adapt. In this study,
we show that using reinforcement learning this de-
cision is learned automatically.
Reinforcement Learning (RL) has been suc-
cessfully used for learning dialogue management
policies since (Levin et al., 1997). The learned
policies allow the dialogue manager to optimally
choose appropriate dialogue acts such as instruc-
tions, confirmation requests, and so on, under
uncertain noise or other environment conditions.
There have been recent efforts to learn infor-
mation presentation and recommendation strate-
gies using reinforcement learning (Hernandez et
al., 2003; Rieser and Lemon, 2009; Rieser and
Lemon, 2010), and joint optimisation of Dialogue
Management and NLG using hierarchical RL has
been proposed by (Lemon, 2010). In addition,
we present a framework to learn to choose appro-
priate referring expressions based on a user’s do-
main knowledge. Following a proof-of-concept
study using a hand-coded rule-based user simu-
lation (Janarthanam and Lemon, 2009c), we pre-
viously showed that adaptive REG policies can
be learned using an RL framework with data-
driven user simulations and that such policies per-
form better than simple hand-coded policies (Ja-
narthanam and Lemon, 2010).
</bodyText>
<sectionHeader confidence="0.976572" genericHeader="method">
3 The Dialogue System
</sectionHeader>
<bodyText confidence="0.999873333333333">
In this section, we describe the different modules
of the dialogue system. The interaction between
the different modules is shown in figure 1 (in
learning mode). The dialogue system presents the
user with instructions to setup a broadband con-
nection at home. In the Wizard of Oz setup, the
system and the user interact using speech. How-
ever, in our machine learning setup, they interact at
the abstract level of dialogue actions and referring
expressions. Our objective is to learn to choose
the appropriate referring expressions to refer to the
domain entities in the instructions.
</bodyText>
<figureCaption confidence="0.997755">
Figure 1: System User Interaction (learning)
</figureCaption>
<subsectionHeader confidence="0.999408">
3.1 Dialogue Manager
</subsectionHeader>
<bodyText confidence="0.999978222222222">
The dialogue manager identifies the next dialogue
act (As,t where t denotes turn, s denotes system)
to give to the user based on the dialogue man-
agement policy 7rdm. The dialogue management
is coded in the form of a finite state machine. In
this dialogue task, the system provides instructions
to either observe or manipulate the environment.
When users ask for clarifications on referring ex-
pressions, the system clarifies (provide clar) by
giving information to enable the user to associate
the expression with the intended referent. When
the user responds in any other way, the instruc-
tion is simply repeated. The dialogue manager
is also responsible for updating and managing the
system state Ss,t (see section 3.2). The system in-
teracts with the user by passing both the system
action As,t and the referring expressions RECs,t
(see section 3.3).
</bodyText>
<subsectionHeader confidence="0.998695">
3.2 The dialogue state
</subsectionHeader>
<bodyText confidence="0.999703833333333">
The dialogue state Ss,t is a set of variables that
represent the current state of the conversation. In
our study, in addition to maintaining an overall di-
alogue state, the system maintains a user model
UMs,t which records the initial domain knowl-
edge of the user. It is a dynamic model that starts
</bodyText>
<page confidence="0.997794">
125
</page>
<bodyText confidence="0.99997768">
with a state where the system does not have any
knowledge about the user. Since the model is up-
dated according to the user’s behaviour, it may be
inaccurate if the user’s behaviour is itself uncer-
tain. Hence, the user model used in this system is
not always an accurate model of the user’s knowl-
edge and reflects a level of uncertainty about the
user.
Each jargon referring expression x is repre-
sented by a three-valued variable in the dialogue
state: user knows x. The three values that each
variable takes are yes, no, not sure. The vari-
ables are updated using a simple user model up-
date algorithm after the user’s response each turn.
Initially each variable is set to not sure. If the
user responds to an instruction containing the re-
ferring expression x with a clarification request,
then user knows x is set to no. Similarly, if
the user responds with appropriate information to
the system’s instruction, the dialogue manager sets
user knows x is set to yes. Only the user’s ini-
tial knowledge is recorded. This is based on the
assumption that an estimate of the user’s initial
knowledge helps to predict the user’s knowledge
of the rest of the referring expressions.
</bodyText>
<subsectionHeader confidence="0.999236">
3.3 REG module
</subsectionHeader>
<bodyText confidence="0.978543121212121">
The REG module is a part of the NLG module
whose task is to identify the list of domain enti-
ties to be referred to and to choose the appropriate
referring expression for each of the domain enti-
ties for each given dialogue act. In this study, we
focus only on the production of appropriate refer-
ring expressions to refer to domain entities men-
tioned in the dialogue act. It chooses between
the two types of referring expressions - jargon
and descriptive. For example, the domain entity
broadband filter can be referred to using the jar-
gon expression “broadband filter” or using the de-
scriptive expression “small white box”1. Although
adaptation is the primary goal, it should be noted
that in order to get an idea of the user the system
is dealing with, it needs to seek information using
jargon expressions.
The REG module operates in two modes - learn-
ing and evaluation. In the learning mode, the REG
module is the learning agent. The REG module
learns to associate dialogue states with optimal re-
ferring expressions. This is represented by a REG
1We will use italicised forms to represent the domain enti-
ties (e.g. broadband filter) and double quotes to represent the
referring expressions (e.g. “broadband filter”).
policy 7rTeg : UMs,t —* RECs,t, which maps
the states of the dialogue (user model) to opti-
mal referring expressions. The referring expres-
sion choices RECs,t is a set of pairs identifying
the referent R and the type of expression T used in
the current system utterance. For instance, the pair
(broadband filter, desc) represents the descriptive
expression “small white box”.
</bodyText>
<equation confidence="0.86016">
RECs,t = {(R1, Ti), ..., (Rn, Tn)}
</equation>
<bodyText confidence="0.99989225">
In the evaluation mode, a trained REG policy in-
teracts with unknown users. It consults the learned
policy 7rTeg to choose the referring expressions
based on the current user model.
</bodyText>
<sectionHeader confidence="0.995582" genericHeader="method">
4 User Simulations
</sectionHeader>
<bodyText confidence="0.999985789473684">
In this section, we present user simulation mod-
els that simulate the dialogue behaviour of a real
human user. Several user simulation models have
been proposed for use in reinforcement learning
of dialogue policies (Georgila et al., 2005; Schatz-
mann et al., 2006; Schatzmann et al., 2007; Ai and
Litman, 2007). However, they are suited only for
learning dialogue management policies, and not
natural language generation policies. In particular,
our model is the first to be sensitive to a system’s
choices of referring expressions. Earlier, we pre-
sented a two-tier simulation trained on data pre-
cisely for REG policy learning (Janarthanam and
Lemon, 2009a). However, it is not suited for train-
ing on small corpus like the one we have at our
disposal. In contrast to the earlier model, we now
condition the clarification requests on the referent
class rather than the referent itself to handle the
data sparsity problem.
</bodyText>
<subsectionHeader confidence="0.999644">
4.1 Corpus-driven action selection model
</subsectionHeader>
<bodyText confidence="0.999991533333333">
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a user
action Au,t (u denoting user). This can either be a
clarification request (cr) or an instruction response
(ir). The US produces a clarification request cr
based on the class of the referent C(Ri), type of
the referring expression Ti, and the current domain
knowledge of the user for the referring expression
DKu,t(Ri,Ti). Domain entities whose jargon ex-
pressions raised clarification requests in the cor-
pus were listed and those that had more than the
mean number of clarification requests were clas-
sified as difficult and others as easy enti-
ties (for example, power adaptor is easy - all
</bodyText>
<page confidence="0.994079">
126
</page>
<bodyText confidence="0.97317325">
users understood this expression, broadband filter
is difficult). Clarification requests are pro-
duced using the following model.
P(Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri, Ti))
where (Ri, Ti) E RECs,t
One should note that the actual literal expres-
sion is not used in the transaction. Only the entity
that it is referring to (Ri) and its type (Ti) are used.
However, the above model simulates the process
of interpreting and resolving the expression and
identifying the domain entity of interest in the in-
struction. The user identification of the entity is
signified when there is no clarification request pro-
duced (i.e. Au,t = none). When no clarification
request is produced, the environment action EAu,t
is generated using the following model.
</bodyText>
<equation confidence="0.980754">
P(EAu,t|As,t) if Au,t! = cr(Ri,Ti)
</equation>
<bodyText confidence="0.9985658">
Finally, the user action is an instruction re-
sponse which is determined by the system ac-
tion As,t. Instruction responses can be ei-
ther provide info, acknowledgement or other
based on the system’s instruction.
</bodyText>
<equation confidence="0.985185">
P(Au,t = ir|EAu,t, As,t)
</equation>
<bodyText confidence="0.999984083333333">
All the above models were trained on our cor-
pus data using maximum likelihood estimation and
smoothed using a variant of Witten-Bell discount-
ing. The corpus contained dialogues between
a non-adaptive dialogue system and real users.
According to the data, clarification requests are
much more likely when jargon expressions are
used to refer to the referents that belong to the
difficult class and which the user doesn’t
know about. When the system uses expressions
that the user knows, the user generally responds to
the instruction given by the system.
</bodyText>
<subsectionHeader confidence="0.978045">
4.2 User Domain knowledge
</subsectionHeader>
<bodyText confidence="0.999974923076923">
The user domain knowledge is initially set to one
of several models at the start of every conver-
sation. The models range from novices to ex-
perts which were identified from the corpus using
k-means clustering. A novice user knows only
“power adaptor”, an expert knows all the jargon
expressions and intermediate users know some.
We assume that users can interpret the descriptive
expressions and resolve their references. There-
fore, they are not explicitly represented. We only
code the user’s knowledge of jargon expressions
using boolean variables representing whether the
user knows the expression or not.
</bodyText>
<subsectionHeader confidence="0.995039">
4.3 Corpus
</subsectionHeader>
<bodyText confidence="0.9992709">
We trained the action selection model on a small
corpus of 12 non-adaptive dialogues between real
users and a dialogue system. There were six
dialogues in which users interacted with a sys-
tem using just jargon expressions and six with a
system using descriptive expressions. For more
discussions on our user simulation models and
the corpus, please refer to (Janarthanam and
Lemon, 2009b; Janarthanam and Lemon, 2009a;
Janarthanam and Lemon, 2010).
</bodyText>
<sectionHeader confidence="0.995948" genericHeader="method">
5 Training
</sectionHeader>
<bodyText confidence="0.999976142857143">
The REG module was trained (operated in learn-
ing mode) using the above simulations to learn
REG policies that select referring expressions
based on the user expertise in the domain. In
this section, we discuss how to code the learning
agent’s goals as reward. We then discuss how the
reward function is used to train the learning agent.
</bodyText>
<subsectionHeader confidence="0.976217">
5.1 Reward function
</subsectionHeader>
<bodyText confidence="0.999993684210527">
We designed a reward function for the goal of
adapting to each user’s domain knowledge. We
present the Adaptation Accuracy score (AA) that
calculates how accurately the agent chose the ap-
propriate expressions for each referent r, with re-
spect to the user’s knowledge. So, when the user
knows the jargon expression for r, the appropri-
ate expression to use is jargon, and if s/he doesn’t
know the jargon, a descriptive expression is appro-
priate. Although the user’s domain knowledge is
dynamically changing due to learning, we base ap-
propriateness on the initial state, because our ob-
jective is to adapt to the initial state of the user
DKu,initial. However, in reality, designers might
want their system to account for user’s chang-
ing knowledge as well. We calculate accuracy
per referent RAr and then calculate the overall
mean adaptation accuracy (AA) over all referents
as shown below.
</bodyText>
<equation confidence="0.96205275">
RAr = #(appropriate expressions(r))
#(instances(r))
AdaptationAccuracyAA = 1
#(r)�rRAr
</equation>
<subsectionHeader confidence="0.995585">
5.2 Learning
</subsectionHeader>
<bodyText confidence="0.9995158">
The REG module was trained in learning mode us-
ing the above reward function using the SHAR-
SHA reinforcement learning algorithm (with lin-
ear function approximation) (Shapiro and Langley,
2002). This is a hierarchical variant of SARSA,
</bodyText>
<page confidence="0.994937">
127
</page>
<bodyText confidence="0.999991568181818">
which is an on-policy learning algorithm that up-
dates the current behaviour policy (see (Sutton
and Barto, 1998)). The training produced approx.
5000 dialogues. The user simulation was cali-
brated to produce three types of users: Novice,
Intermediate and Expert, randomly but with equal
probability.
Initially, the REG policy chooses randomly be-
tween the referring expression types for each do-
main entity in the system utterance, irrespective
of the user model state. Once the referring ex-
pressions are chosen, the system presents the user
simulation with both the dialogue act and refer-
ring expression choices. The choice of referring
expression affects the user’s dialogue behaviour.
For instance, choosing a jargon expression could
evoke a clarification request from the user, based
on which, the dialogue manager updates the inter-
nal user model (UM,,t) with the new information
that the user is ignorant of the particular expres-
sion. It should be noted that using a jargon expres-
sion is an information seeking move which enables
the REG module to estimate the user’s knowledge
level. The same process is repeated for every dia-
logue instruction. At the end of the dialogue, the
system is rewarded based on its choices of refer-
ring expressions. If the system chooses jargon ex-
pressions for novice users or descriptive expres-
sions for expert users, penalties are incurred and if
the system chooses REs appropriately, the reward
is high. On the one hand, those actions that fetch
more reward are reinforced, and on the other hand,
the agent tries out new state-action combinations
to explore the possibility of greater rewards. Over
time, it stops exploring new state-action combina-
tions and exploits those actions that contribute to
higher reward. The REG module learns to choose
the appropriate referring expressions based on the
user model in order to maximize the overall adap-
tation accuracy. Figure 2 shows how the agent
learns using the data-driven (Learned DS) during
training. It can be seen in the figure 2 that towards
the end the curve plateaus, signifying that learning
has converged.
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.998391">
In this section, we present the details of the eval-
uation process, the baseline policy, the metrics
used, and the results. In a recent study, we eval-
uated the learned policy and several hand-coded
baselines with simulated users and found that
</bodyText>
<figureCaption confidence="0.998618">
Figure 2: Learning curve - Training
</figureCaption>
<bodyText confidence="0.9997915">
the Learned-DS policy produced higher adapta-
tion accuracy than other policies (Janarthanam and
Lemon, 2010). An interesting issue for research
in this area is to what extent evaluation results ob-
tained in simulated environments transfer to eval-
uations with real users (Lemon et al., 2006).
</bodyText>
<subsectionHeader confidence="0.997586">
6.1 Baseline system
</subsectionHeader>
<bodyText confidence="0.999985666666667">
In order to compare the performance of the learned
policy with a baseline, a simple rule-based policy
was built. This baseline was chosen because it per-
formed better in simulation, compared to a vari-
ety of other baselines (Janarthanam and Lemon,
2010). It uses jargon for all referents by default
and provides clarifications when requested. It ex-
ploits the user model in subsequent references af-
ter the user’s knowledge of the expression has
been set to either yes or no. Therefore, although
it is a simple policy, it adapts to a certain extent
(‘locally’). We refer to this policy as the ‘Jargon-
adapt’ policy. It should be noted that this policy
was built in the absence of expert domain knowl-
edge and/or an expert-layperson corpus.
</bodyText>
<subsectionHeader confidence="0.996292">
6.2 Process
</subsectionHeader>
<bodyText confidence="0.999941272727273">
We evaluated the two policies with real users.
36 university students from different backgrounds
(e.g. Arts, Humanities, Medicine and Engineer-
ing) participated in the evaluation. 17 users were
given a system with Jargon-adapt policy and 19
users interacted with a system with Learned-DS
policy. Each user was given a pre-task recognition
test to record his/her initial domain knowledge.
The experimenter read out a list of technical terms
and the user was asked to point out to the domain
entities laid out in front of them. They were then
</bodyText>
<page confidence="0.995959">
128
</page>
<bodyText confidence="0.999937692307692">
given one of the two systems - learned or base-
line, to interact with. Following the system in-
structions, they then attempted to set up the broad-
band connection. When the dialogue had ended,
the user was given a post-task test where the recog-
nition test was repeated and their responses were
recorded. The user’s broadband connection setup
was manually examined for task completion (i.e.
the percentage of correct connections that they had
made in their final set-up). The user was given the
task completion results and was then given a user
satisfaction questionnaire to evaluate the features
of the system based on the conversation.
All users interacted with a wizarded system em-
ploying one of the two REG policies (see figure
3). The user’s responses were intercepted by a hu-
man interpreter (or “wizard”) and were annotated
as dialogue acts, to which the automated dialogue
manager responded with a system dialogue action
(the dialogue policy was fixed). The wizards were
not aware of the policy used by the system. The
respective policies chose only the referring expres-
sions to generate the system utterance for the given
dialogue action. The system utterances were con-
verted to speech by a speech synthesizer (Cere-
proc) and were played to the user.
</bodyText>
<figureCaption confidence="0.995347">
Figure 3: Wizarded Dialogue System
</figureCaption>
<subsectionHeader confidence="0.998846">
6.3 Metrics
</subsectionHeader>
<bodyText confidence="0.9993941">
In addition to the adaptation accuracy mentioned
in section 5.1, we also measure other parame-
ters from the conversation in order to show how
learned adaptive policies compare with other poli-
cies on other dimensions. We also measure the
learning effect on the users as (normalised) learn-
ing gain (LG) produced by using unknown jargon
expressions. This is calculated using the pre- and
post-test scores for the user domain knowledge
(DK,,) as follows.
</bodyText>
<table confidence="0.996660166666667">
Metrics Jargon-adapt Learned-DS
AA 63.91 84.72 **
LG 0.59 0.61
DT 7.86 6.98 *
TC 84.7 99.8 **
* Statistical significance (p &lt; 0.05).
</table>
<tableCaption confidence="0.897819">
** Statistical significance (p &lt; 0.001).
Table 2: Evaluation with real users
</tableCaption>
<author confidence="0.4572">
Learning Gain LG = P�st−P��
</author>
<equation confidence="0.818981">
�−P ��
</equation>
<bodyText confidence="0.9993245">
Dialogue time (DT) is the actual time taken for
the user to complete the task. We measured task
completion (TC) by examining the user’s broad-
band setup after the task was completed (i.e. the
percentage of correct connections that they had
made in their final set-up).
</bodyText>
<subsectionHeader confidence="0.725622">
6.4 Results
</subsectionHeader>
<bodyText confidence="0.99998534375">
We compare the performance of the two strategies
on real users using objective parameters and sub-
jective feedback scores. Tests for statistical sig-
nificance were done using Mann-Whitney test for
2 independent samples (due to non-parametric na-
ture of the data).
Table 2 presents the mean accuracy of adap-
tation (AA), learning gain (LG), dialogue time
(DT), and task completion (TC), produced by the
two strategies. The Learned-DS strategy pro-
duced more accurate adaptation than the Jargon-
adapt strategy (p&lt;0.001, U=9.0, r=-0.81). Higher
accuracy of adaptation (AA) of the Learned-DS
strategy translates to less dialogue time (U=73.0,
p&lt;0.05, r=-0.46) and higher task completion
(U=47.5, p&lt;0.001, r=-0.72) than the Jargon-adapt
policy. However, there was no significant differ-
ence in learning gain (LG).
Table 3 presents how the users subjectively
scored on a agreement scale of 1 to 4 (with 1
meaning ‘strongly disagree’), different features of
the system based on their conversations with the
two different strategies. Users’ feedback on dif-
ferent features of the systems were not very differ-
ent from each other. However, users did feel that
it was easier to identify domain objects with the
Learned-DS strategy than the Jargon-adapt strat-
egy (U=104.0, p&lt;0.05, r=-0.34). To our knowl-
edge, this is the first study to show a significant
improvement in real user ratings for a learned pol-
icy in spoken dialogue systems (normally, objec-
tive metrics show an improvement, but not subjec-
</bodyText>
<page confidence="0.996094">
129
</page>
<table confidence="0.8099588">
Feedback questions Jargon-adapt Learned-DS
Q1. Quality of voice 3.11 3.36
Q2. Had to ask too many questions 2.23 1.89
Q3. System adapted very well 3.41 3.58
Q4. Easy to identify objects 2.94 3.37 *
Q5. Right amount of dialogue time 3.23 3.26
Q6. Learned useful terms 2.94 3.05
Q7. Conversation was easy 3.17 3.42
Q8. Future use 3.22 3.47
* Statistical significance (p &lt; 0.05).
</table>
<tableCaption confidence="0.997726">
Table 3: Real user feedback
</tableCaption>
<bodyText confidence="0.726832">
tive scores (Lemon et al., 2006)).
</bodyText>
<subsectionHeader confidence="0.910838">
6.5 Analysis
</subsectionHeader>
<bodyText confidence="0.999996327272728">
The results show that the Learned-DS strategy is
significantly better than the hand-coded Jargon-
Adapt policy in terms of adaptation accuracy, di-
alogue time, and task completion rate. The ini-
tial knowledge of the users (mean pre-task recog-
nition score) of the two groups were not signifi-
cantly different from each other (Jargon-adapt =
7.33, Learned-DS = 7.45). Hence there is no bias
on the user’s pre-task score towards any strategy.
While the Learned-DS system adapts well to its
users globally, the Jargon-adapt system adapted
only locally. This led to higher task completion
rate and lower dialogue time.
The Learned-DS strategy enabled the system to
adapt using the dependencies that it learned dur-
ing the training phase. For instance, when the user
asked for clarification on some referring expres-
sions (e.g. “ethernet cable”), it used descriptive
expressions for domain objects like ethernet light
and ethernet socket. Such adaptation across ref-
erents enabled the Learned-DS strategy to score
better than the Jargon-adapt strategy. Since the
agent starts the conversation with no knowledge
about the user, it learned to use information seek-
ing moves (use jargon) at appropriate moments,
although they may be inappropriate. But since it
was trained to maximize the adaptation accuracy,
the agent also learned to restrict such moves and
start predicting the user’s domain knowledge as
soon as possible. By learning to trade-off between
information-seeking and adaptation, the Learned-
DS policy produced a higher adaptation with real
users with different domain knowledge levels.
The users however did not generally rate the
two policies differently. However, they did rate
it (significantly) easier to identify objects when
using the learned policy. For the other ratings,
users seemed to be not able to recognize the nu-
ances in the way the system adapted to them. They
could have been satisfied with the fact that the sys-
tem adapted better (Q3). This adaptation and the
fact that the system offered help when the users
were confused in interpreting the technical terms,
could have led the users to score the system well in
terms of future use (Q8), dialogue time (Q5), and
ease of conversation (Q7), but in common with ex-
periments in dialogue management (Lemon et al.,
2006) it seems that users find it difficult to evaluate
these improvements subjectively. The users were
given only one of the two strategies and therefore
were not in a position to compare the two strate-
gies and judge which one is better. Results in table
3 lead us to conclude that perhaps users need to
compare two or more strategies in order to judge
the strategies better.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999853125">
We presented new results from an evaluation with
real users. In this study, we have shown that user-
adaptive REG policies can be learned using an RL
framework and data-driven user simulations. It
learned to trade off between adaptive moves and
information seeking moves automatically to max-
imize the overall adaptation accuracy. The learned
policy started the conversation with information
seeking moves, learned a little about the user, and
started adapting dynamically as the conversation
progressed. We also showed that the learned pol-
icy performs better than a reasonable hand-coded
policy with real users in terms of accuracy of adap-
tation, dialogue time, task completion, and a sub-
jective evaluation. Finally, this paper provides
further evidence that evaluation results obtained
</bodyText>
<page confidence="0.989491">
130
</page>
<bodyText confidence="0.999920153846154">
in simulated environments can transfer reliably to
evaluations with real users (Lemon et al., 2006).
Whether the learned policy would perform bet-
ter than a hand-coded policy which was painstak-
ingly crafted by a domain expert (or learned us-
ing supervised methods from an expert-layperson
corpus) is an interesting question that needs fur-
ther exploration. Also, it would also be interesting
to make the learned policy account for the user’s
learning behaviour and adapt accordingly. We also
believe that this framework can be extended to in-
clude other decisions in NLG besides REG (Deth-
lefs and Cuayahuitl, 2010).
</bodyText>
<sectionHeader confidence="0.999023" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999992833333333">
The research leading to these results has received
funding from the European Community’s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 216594 (CLASSiC project
www.classic-project.org) and from the
EPSRC, project no. EP/G069840/1.
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849924050633">
H. Ai and D. Litman. 2007. Knowledge consistent
user simulations for dialog systems. In Proceedings
of Interspeech 2007, Antwerp, Belgium.
T. Akiba and H. Tanaka. 1994. A Bayesian approach
for User Modelling in Dialogue Systems. In Pro-
ceedings of the 15th conference on Computational
Linguistics - Volume 2, Kyoto.
A. Bell. 1984. Language style as audience design.
Language in Society, 13(2):145–204.
A. Cawsey. 1993. User Modelling in Interactive Ex-
planations. User Modeling and User-Adapted Inter-
action, 3(3):221–247.
H. H. Clark and G. L. Murphy. 1982. Audience de-
sign in meaning and reference. In J. F. LeNy and
W. Kintsch, editors, Language and comprehension.
Amsterdam: North-Holland.
N. Dethlefs and H. Cuayahuitl. 2010. Hierarchical Re-
inforcement Learning for Adaptive Text Generation.
In Proc. INLG 2010.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning User Simulations for Information State
Update Dialogue Systems. In Proc of Eu-
rospeech/Interspeech.
F. Hernandez, E. Gaudioso, and J. G. Boticario. 2003.
A Multiagent Approach to Obtain Open and Flexible
User Models in Adaptive Learning Communities. In
User Modeling 2003, volume 2702/2003 of LNCS.
Springer, Berlin / Heidelberg.
E. A. Issacs and H. H. Clark. 1987. References in
conversations between experts and novices. Journal
of Experimental Psychology: General, 116:26–37.
S. Janarthanam and O. Lemon. 2009a. A Two-tier
User Simulation Model for Reinforcement Learning
of Adaptive Referring Expression Generation Poli-
cies. In Proc. SigDial’09.
S. Janarthanam and O. Lemon. 2009b. A Wizard-of-
Oz environment to study Referring Expression Gen-
eration in a Situated Spoken Dialogue Task. In Proc.
ENLG’09.
S. Janarthanam and O. Lemon. 2009c. Learning Lexi-
cal Alignment Policies for Generating Referring Ex-
pressions for Spoken Dialogue Systems. In Proc.
ENLG’09.
S. Janarthanam and O. Lemon. 2010. Learning to
Adapt to Unknown Users: Referring Expression
Generation in Spoken Dialogue Systems. In Proc.
ACL’10.
O. Lemon, Georgila. K., and J. Henderson. 2006.
Evaluating Effectiveness and Portability of Rein-
forcement Learned Dialogue Strategies with real
users: the TALK TownInfo Evaluation. In
IEEE/ACL Spoken Language Technology.
O. Lemon. 2010. Learning what to say and how to say
it: joint optimization of spoken dialogue manage-
ment and Natural Language Generation. Computer
Speech and Language. (to appear).
E. Levin, R. Pieraccini, and W. Eckert. 1997. Learn-
ing Dialogue Strategies within the Markov Decision
Process Framework. In Proc. of ASRU97.
V. Rieser and O. Lemon. 2009. Natural Language
Generation as Planning Under Uncertainty for Spo-
ken Dialogue Systems. In Proc. EACL’09.
V. Rieser and O. Lemon. 2010. Optimising informa-
tion presentation for spoken dialogue systems. In
Proc. ACL. (to appear).
J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J.
Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement Learning of
Dialogue Management Strategies. Knowledge Engi-
neering Review, pages 97–126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. J. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proc of HLT/NAACL 2007.
D. Shapiro and P. Langley. 2002. Separating skills
from preference: Using learning to program by re-
ward. In Proc. ICML-02.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
</reference>
<page confidence="0.998309">
131
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.919887">
<title confidence="0.999757">Adaptive Referring Expression Generation in Spoken Dialogue Evaluation with Real Users</title>
<author confidence="0.996952">Srinivasan</author>
<affiliation confidence="0.9991215">School of University of</affiliation>
<email confidence="0.989414">s.janarthanam@ed.ac.uk</email>
<abstract confidence="0.997788266666667">We present new results from a real-user evaluation of a data-driven approach to learning user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the system learns REG policies which can adapt to unknown users online. For real users of such a system, we show that in comparison to an adaptive hand-coded baseline policy, the learned policy performs significantly better, with a 20.8% average increase in adaptation accuracy, 12.6% decrease in time taken, and a 15.1% increase in task completion rate. The learned policy also has a significantly better subjective rating from users. This is because the learned policies adapt online to changing evidence about the user’s domain expertise. We also discuss the issue of evaluation in simulation versus evaluation with real users.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Ai</author>
<author>D Litman</author>
</authors>
<title>Knowledge consistent user simulations for dialog systems.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="10967" citStr="Ai and Litman, 2007" startWordPosition="1769" endWordPosition="1772">band filter, desc) represents the descriptive expression “small white box”. RECs,t = {(R1, Ti), ..., (Rn, Tn)} In the evaluation mode, a trained REG policy interacts with unknown users. It consults the learned policy 7rTeg to choose the referring expressions based on the current user model. 4 User Simulations In this section, we present user simulation models that simulate the dialogue behaviour of a real human user. Several user simulation models have been proposed for use in reinforcement learning of dialogue policies (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007; Ai and Litman, 2007). However, they are suited only for learning dialogue management policies, and not natural language generation policies. In particular, our model is the first to be sensitive to a system’s choices of referring expressions. Earlier, we presented a two-tier simulation trained on data precisely for REG policy learning (Janarthanam and Lemon, 2009a). However, it is not suited for training on small corpus like the one we have at our disposal. In contrast to the earlier model, we now condition the clarification requests on the referent class rather than the referent itself to handle the data sparsit</context>
</contexts>
<marker>Ai, Litman, 2007</marker>
<rawString>H. Ai and D. Litman. 2007. Knowledge consistent user simulations for dialog systems. In Proceedings of Interspeech 2007, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Akiba</author>
<author>H Tanaka</author>
</authors>
<title>A Bayesian approach for User Modelling in Dialogue Systems.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th conference on Computational Linguistics - Volume 2,</booktitle>
<location>Kyoto.</location>
<contexts>
<context position="4396" citStr="Akiba and Tanaka, 1994" startWordPosition="679" endWordPosition="682">ent the training and in section 6, we present the evaluation for different REG policies with real users. 2 Related work Rule-based and supervised learning approaches have been proposed to learn and adapt during conversations dynamically. Such systems learn from a user at the start and later adapt to the domain knowledge of the user. However, they either require expensive expert knowledge resources to hand-code the inference rules (Cawsey, 1993) or a large corpus of expert-layperson interaction from which adaptive strategies can be learned and modelled, using methods such as Bayesian networks (Akiba and Tanaka, 1994). In contrast, we present an approach that learns in the absence of these expensive resources. It is also not clear how supervised approaches choose between when to seek more information and when to adapt. In this study, we show that using reinforcement learning this decision is learned automatically. Reinforcement Learning (RL) has been successfully used for learning dialogue management policies since (Levin et al., 1997). The learned policies allow the dialogue manager to optimally choose appropriate dialogue acts such as instructions, confirmation requests, and so on, under uncertain noise </context>
</contexts>
<marker>Akiba, Tanaka, 1994</marker>
<rawString>T. Akiba and H. Tanaka. 1994. A Bayesian approach for User Modelling in Dialogue Systems. In Proceedings of the 15th conference on Computational Linguistics - Volume 2, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bell</author>
</authors>
<title>Language style as audience design.</title>
<date>1984</date>
<journal>Language in Society,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="2605" citStr="Bell, 1984" startWordPosition="401" endWordPosition="402">able with grey ends into the small white box. Table 1: Referring expression examples for 2 entities (from the corpus) system could choose to use more technical terms with an expert user, or to use more descriptive and general expressions with novice users, and a mix of the two with intermediate users of various sorts (see examples in Table 1). In natural human-human conversations, dialogue partners learn about each other and adapt their language to suit their domain expertise (Issacs and Clark, 1987). This kind of adaptation is called Alignment through Audience Design (Clark and Murphy, 1982; Bell, 1984). We assume that users are mostly unknown to the system and therefore that a spoken dialogue system (SDS) must be capable of observing the user’s dialogue behaviour, modelling his/her domain knowledge, and adapting accordingly, just like human interlocutors. Therefore unlike systems that use static user models, our system has to dynamically model the user’s domain knowledge in order to adapt during the conversation. We present a corpus-driven framework for learning a user-adaptive REG policy from a small corpus of non-adaptive human-machine interaction. We show that the learned policy performs</context>
</contexts>
<marker>Bell, 1984</marker>
<rawString>A. Bell. 1984. Language style as audience design. Language in Society, 13(2):145–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cawsey</author>
</authors>
<title>User Modelling in Interactive Explanations. User Modeling and User-Adapted Interaction,</title>
<date>1993</date>
<pages>3--3</pages>
<contexts>
<context position="4221" citStr="Cawsey, 1993" startWordPosition="654" endWordPosition="655">course and Dialogue, pages 124–131, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 124 model. In section 5, we present the training and in section 6, we present the evaluation for different REG policies with real users. 2 Related work Rule-based and supervised learning approaches have been proposed to learn and adapt during conversations dynamically. Such systems learn from a user at the start and later adapt to the domain knowledge of the user. However, they either require expensive expert knowledge resources to hand-code the inference rules (Cawsey, 1993) or a large corpus of expert-layperson interaction from which adaptive strategies can be learned and modelled, using methods such as Bayesian networks (Akiba and Tanaka, 1994). In contrast, we present an approach that learns in the absence of these expensive resources. It is also not clear how supervised approaches choose between when to seek more information and when to adapt. In this study, we show that using reinforcement learning this decision is learned automatically. Reinforcement Learning (RL) has been successfully used for learning dialogue management policies since (Levin et al., 1997</context>
</contexts>
<marker>Cawsey, 1993</marker>
<rawString>A. Cawsey. 1993. User Modelling in Interactive Explanations. User Modeling and User-Adapted Interaction, 3(3):221–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>G L Murphy</author>
</authors>
<title>Audience design in meaning and reference.</title>
<date>1982</date>
<editor>In J. F. LeNy and W. Kintsch, editors,</editor>
<publisher>North-Holland.</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="2592" citStr="Clark and Murphy, 1982" startWordPosition="397" endWordPosition="400"> end of the thin white cable with grey ends into the small white box. Table 1: Referring expression examples for 2 entities (from the corpus) system could choose to use more technical terms with an expert user, or to use more descriptive and general expressions with novice users, and a mix of the two with intermediate users of various sorts (see examples in Table 1). In natural human-human conversations, dialogue partners learn about each other and adapt their language to suit their domain expertise (Issacs and Clark, 1987). This kind of adaptation is called Alignment through Audience Design (Clark and Murphy, 1982; Bell, 1984). We assume that users are mostly unknown to the system and therefore that a spoken dialogue system (SDS) must be capable of observing the user’s dialogue behaviour, modelling his/her domain knowledge, and adapting accordingly, just like human interlocutors. Therefore unlike systems that use static user models, our system has to dynamically model the user’s domain knowledge in order to adapt during the conversation. We present a corpus-driven framework for learning a user-adaptive REG policy from a small corpus of non-adaptive human-machine interaction. We show that the learned po</context>
</contexts>
<marker>Clark, Murphy, 1982</marker>
<rawString>H. H. Clark and G. L. Murphy. 1982. Audience design in meaning and reference. In J. F. LeNy and W. Kintsch, editors, Language and comprehension. Amsterdam: North-Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Dethlefs</author>
<author>H Cuayahuitl</author>
</authors>
<title>Hierarchical Reinforcement Learning for Adaptive Text Generation.</title>
<date>2010</date>
<booktitle>In Proc. INLG</booktitle>
<marker>Dethlefs, Cuayahuitl, 2010</marker>
<rawString>N. Dethlefs and H. Cuayahuitl. 2010. Hierarchical Reinforcement Learning for Adaptive Text Generation. In Proc. INLG 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Georgila</author>
<author>J Henderson</author>
<author>O Lemon</author>
</authors>
<title>Learning User Simulations for Information State Update Dialogue Systems.</title>
<date>2005</date>
<booktitle>In Proc of Eurospeech/Interspeech.</booktitle>
<contexts>
<context position="10895" citStr="Georgila et al., 2005" startWordPosition="1756" endWordPosition="1759">ion T used in the current system utterance. For instance, the pair (broadband filter, desc) represents the descriptive expression “small white box”. RECs,t = {(R1, Ti), ..., (Rn, Tn)} In the evaluation mode, a trained REG policy interacts with unknown users. It consults the learned policy 7rTeg to choose the referring expressions based on the current user model. 4 User Simulations In this section, we present user simulation models that simulate the dialogue behaviour of a real human user. Several user simulation models have been proposed for use in reinforcement learning of dialogue policies (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007; Ai and Litman, 2007). However, they are suited only for learning dialogue management policies, and not natural language generation policies. In particular, our model is the first to be sensitive to a system’s choices of referring expressions. Earlier, we presented a two-tier simulation trained on data precisely for REG policy learning (Janarthanam and Lemon, 2009a). However, it is not suited for training on small corpus like the one we have at our disposal. In contrast to the earlier model, we now condition the clarification requests on the r</context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2005</marker>
<rawString>K. Georgila, J. Henderson, and O. Lemon. 2005. Learning User Simulations for Information State Update Dialogue Systems. In Proc of Eurospeech/Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Hernandez</author>
<author>E Gaudioso</author>
<author>J G Boticario</author>
</authors>
<title>A Multiagent Approach to Obtain Open and Flexible User Models in Adaptive Learning Communities. In User Modeling</title>
<date>2003</date>
<volume>2702</volume>
<publisher>Springer,</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="5176" citStr="Hernandez et al., 2003" startWordPosition="797" endWordPosition="800">hen to seek more information and when to adapt. In this study, we show that using reinforcement learning this decision is learned automatically. Reinforcement Learning (RL) has been successfully used for learning dialogue management policies since (Levin et al., 1997). The learned policies allow the dialogue manager to optimally choose appropriate dialogue acts such as instructions, confirmation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthana</context>
</contexts>
<marker>Hernandez, Gaudioso, Boticario, 2003</marker>
<rawString>F. Hernandez, E. Gaudioso, and J. G. Boticario. 2003. A Multiagent Approach to Obtain Open and Flexible User Models in Adaptive Learning Communities. In User Modeling 2003, volume 2702/2003 of LNCS. Springer, Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Issacs</author>
<author>H H Clark</author>
</authors>
<title>References in conversations between experts and novices.</title>
<date>1987</date>
<journal>Journal of Experimental Psychology: General,</journal>
<pages>116--26</pages>
<contexts>
<context position="2499" citStr="Issacs and Clark, 1987" startWordPosition="382" endWordPosition="386">se plug one end of the broadband cable into the broadband filter. Descriptive: Please plug one end of the thin white cable with grey ends into the small white box. Table 1: Referring expression examples for 2 entities (from the corpus) system could choose to use more technical terms with an expert user, or to use more descriptive and general expressions with novice users, and a mix of the two with intermediate users of various sorts (see examples in Table 1). In natural human-human conversations, dialogue partners learn about each other and adapt their language to suit their domain expertise (Issacs and Clark, 1987). This kind of adaptation is called Alignment through Audience Design (Clark and Murphy, 1982; Bell, 1984). We assume that users are mostly unknown to the system and therefore that a spoken dialogue system (SDS) must be capable of observing the user’s dialogue behaviour, modelling his/her domain knowledge, and adapting accordingly, just like human interlocutors. Therefore unlike systems that use static user models, our system has to dynamically model the user’s domain knowledge in order to adapt during the conversation. We present a corpus-driven framework for learning a user-adaptive REG poli</context>
</contexts>
<marker>Issacs, Clark, 1987</marker>
<rawString>E. A. Issacs and H. H. Clark. 1987. References in conversations between experts and novices. Journal of Experimental Psychology: General, 116:26–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>A Two-tier User Simulation Model for Reinforcement Learning of Adaptive Referring Expression Generation Policies. In</title>
<date>2009</date>
<booktitle>Proc. SigDial’09.</booktitle>
<contexts>
<context position="5573" citStr="Janarthanam and Lemon, 2009" startWordPosition="858" endWordPosition="861">mation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 The Dialogue System In this section, we describe the different modules of the dialogue system. The interaction between the different modules is shown in figure 1 (in learning mode). The dialogue system presents the user with instructions to setup a broadband connection at home. In the Wizard of Oz setup, the system and the user interact using speech. However, in our machin</context>
<context position="11312" citStr="Janarthanam and Lemon, 2009" startWordPosition="1822" endWordPosition="1825"> user simulation models that simulate the dialogue behaviour of a real human user. Several user simulation models have been proposed for use in reinforcement learning of dialogue policies (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007; Ai and Litman, 2007). However, they are suited only for learning dialogue management policies, and not natural language generation policies. In particular, our model is the first to be sensitive to a system’s choices of referring expressions. Earlier, we presented a two-tier simulation trained on data precisely for REG policy learning (Janarthanam and Lemon, 2009a). However, it is not suited for training on small corpus like the one we have at our disposal. In contrast to the earlier model, we now condition the clarification requests on the referent class rather than the referent itself to handle the data sparsity problem. 4.1 Corpus-driven action selection model The user simulation (US) receives the system action As,t and its referring expression choices RECs,t at each turn. The US responds with a user action Au,t (u denoting user). This can either be a clarification request (cr) or an instruction response (ir). The US produces a clarification reques</context>
<context position="14953" citStr="Janarthanam and Lemon, 2009" startWordPosition="2412" endWordPosition="2415"> expressions and resolve their references. Therefore, they are not explicitly represented. We only code the user’s knowledge of jargon expressions using boolean variables representing whether the user knows the expression or not. 4.3 Corpus We trained the action selection model on a small corpus of 12 non-adaptive dialogues between real users and a dialogue system. There were six dialogues in which users interacted with a system using just jargon expressions and six with a system using descriptive expressions. For more discussions on our user simulation models and the corpus, please refer to (Janarthanam and Lemon, 2009b; Janarthanam and Lemon, 2009a; Janarthanam and Lemon, 2010). 5 Training The REG module was trained (operated in learning mode) using the above simulations to learn REG policies that select referring expressions based on the user expertise in the domain. In this section, we discuss how to code the learning agent’s goals as reward. We then discuss how the reward function is used to train the learning agent. 5.1 Reward function We designed a reward function for the goal of adapting to each user’s domain knowledge. We present the Adaptation Accuracy score (AA) that calculates how accurately the </context>
</contexts>
<marker>Janarthanam, Lemon, 2009</marker>
<rawString>S. Janarthanam and O. Lemon. 2009a. A Two-tier User Simulation Model for Reinforcement Learning of Adaptive Referring Expression Generation Policies. In Proc. SigDial’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>A Wizard-ofOz environment to study Referring Expression Generation in a Situated Spoken Dialogue Task. In</title>
<date>2009</date>
<booktitle>Proc. ENLG’09.</booktitle>
<contexts>
<context position="5573" citStr="Janarthanam and Lemon, 2009" startWordPosition="858" endWordPosition="861">mation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 The Dialogue System In this section, we describe the different modules of the dialogue system. The interaction between the different modules is shown in figure 1 (in learning mode). The dialogue system presents the user with instructions to setup a broadband connection at home. In the Wizard of Oz setup, the system and the user interact using speech. However, in our machin</context>
<context position="11312" citStr="Janarthanam and Lemon, 2009" startWordPosition="1822" endWordPosition="1825"> user simulation models that simulate the dialogue behaviour of a real human user. Several user simulation models have been proposed for use in reinforcement learning of dialogue policies (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007; Ai and Litman, 2007). However, they are suited only for learning dialogue management policies, and not natural language generation policies. In particular, our model is the first to be sensitive to a system’s choices of referring expressions. Earlier, we presented a two-tier simulation trained on data precisely for REG policy learning (Janarthanam and Lemon, 2009a). However, it is not suited for training on small corpus like the one we have at our disposal. In contrast to the earlier model, we now condition the clarification requests on the referent class rather than the referent itself to handle the data sparsity problem. 4.1 Corpus-driven action selection model The user simulation (US) receives the system action As,t and its referring expression choices RECs,t at each turn. The US responds with a user action Au,t (u denoting user). This can either be a clarification request (cr) or an instruction response (ir). The US produces a clarification reques</context>
<context position="14953" citStr="Janarthanam and Lemon, 2009" startWordPosition="2412" endWordPosition="2415"> expressions and resolve their references. Therefore, they are not explicitly represented. We only code the user’s knowledge of jargon expressions using boolean variables representing whether the user knows the expression or not. 4.3 Corpus We trained the action selection model on a small corpus of 12 non-adaptive dialogues between real users and a dialogue system. There were six dialogues in which users interacted with a system using just jargon expressions and six with a system using descriptive expressions. For more discussions on our user simulation models and the corpus, please refer to (Janarthanam and Lemon, 2009b; Janarthanam and Lemon, 2009a; Janarthanam and Lemon, 2010). 5 Training The REG module was trained (operated in learning mode) using the above simulations to learn REG policies that select referring expressions based on the user expertise in the domain. In this section, we discuss how to code the learning agent’s goals as reward. We then discuss how the reward function is used to train the learning agent. 5.1 Reward function We designed a reward function for the goal of adapting to each user’s domain knowledge. We present the Adaptation Accuracy score (AA) that calculates how accurately the </context>
</contexts>
<marker>Janarthanam, Lemon, 2009</marker>
<rawString>S. Janarthanam and O. Lemon. 2009b. A Wizard-ofOz environment to study Referring Expression Generation in a Situated Spoken Dialogue Task. In Proc. ENLG’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>Learning Lexical Alignment Policies for Generating Referring Expressions for Spoken Dialogue Systems.</title>
<date>2009</date>
<booktitle>In Proc. ENLG’09.</booktitle>
<contexts>
<context position="5573" citStr="Janarthanam and Lemon, 2009" startWordPosition="858" endWordPosition="861">mation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 The Dialogue System In this section, we describe the different modules of the dialogue system. The interaction between the different modules is shown in figure 1 (in learning mode). The dialogue system presents the user with instructions to setup a broadband connection at home. In the Wizard of Oz setup, the system and the user interact using speech. However, in our machin</context>
<context position="11312" citStr="Janarthanam and Lemon, 2009" startWordPosition="1822" endWordPosition="1825"> user simulation models that simulate the dialogue behaviour of a real human user. Several user simulation models have been proposed for use in reinforcement learning of dialogue policies (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007; Ai and Litman, 2007). However, they are suited only for learning dialogue management policies, and not natural language generation policies. In particular, our model is the first to be sensitive to a system’s choices of referring expressions. Earlier, we presented a two-tier simulation trained on data precisely for REG policy learning (Janarthanam and Lemon, 2009a). However, it is not suited for training on small corpus like the one we have at our disposal. In contrast to the earlier model, we now condition the clarification requests on the referent class rather than the referent itself to handle the data sparsity problem. 4.1 Corpus-driven action selection model The user simulation (US) receives the system action As,t and its referring expression choices RECs,t at each turn. The US responds with a user action Au,t (u denoting user). This can either be a clarification request (cr) or an instruction response (ir). The US produces a clarification reques</context>
<context position="14953" citStr="Janarthanam and Lemon, 2009" startWordPosition="2412" endWordPosition="2415"> expressions and resolve their references. Therefore, they are not explicitly represented. We only code the user’s knowledge of jargon expressions using boolean variables representing whether the user knows the expression or not. 4.3 Corpus We trained the action selection model on a small corpus of 12 non-adaptive dialogues between real users and a dialogue system. There were six dialogues in which users interacted with a system using just jargon expressions and six with a system using descriptive expressions. For more discussions on our user simulation models and the corpus, please refer to (Janarthanam and Lemon, 2009b; Janarthanam and Lemon, 2009a; Janarthanam and Lemon, 2010). 5 Training The REG module was trained (operated in learning mode) using the above simulations to learn REG policies that select referring expressions based on the user expertise in the domain. In this section, we discuss how to code the learning agent’s goals as reward. We then discuss how the reward function is used to train the learning agent. 5.1 Reward function We designed a reward function for the goal of adapting to each user’s domain knowledge. We present the Adaptation Accuracy score (AA) that calculates how accurately the </context>
</contexts>
<marker>Janarthanam, Lemon, 2009</marker>
<rawString>S. Janarthanam and O. Lemon. 2009c. Learning Lexical Alignment Policies for Generating Referring Expressions for Spoken Dialogue Systems. In Proc. ENLG’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems.</title>
<date>2010</date>
<booktitle>In Proc. ACL’10.</booktitle>
<contexts>
<context position="5794" citStr="Janarthanam and Lemon, 2010" startWordPosition="893" endWordPosition="897"> al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 The Dialogue System In this section, we describe the different modules of the dialogue system. The interaction between the different modules is shown in figure 1 (in learning mode). The dialogue system presents the user with instructions to setup a broadband connection at home. In the Wizard of Oz setup, the system and the user interact using speech. However, in our machine learning setup, they interact at the abstract level of dialogue actions and referring expressions. Our objective is to learn to choose the appropriate referring expressions to refer to the domain entities in the instruc</context>
<context position="15014" citStr="Janarthanam and Lemon, 2010" startWordPosition="2420" endWordPosition="2423">re not explicitly represented. We only code the user’s knowledge of jargon expressions using boolean variables representing whether the user knows the expression or not. 4.3 Corpus We trained the action selection model on a small corpus of 12 non-adaptive dialogues between real users and a dialogue system. There were six dialogues in which users interacted with a system using just jargon expressions and six with a system using descriptive expressions. For more discussions on our user simulation models and the corpus, please refer to (Janarthanam and Lemon, 2009b; Janarthanam and Lemon, 2009a; Janarthanam and Lemon, 2010). 5 Training The REG module was trained (operated in learning mode) using the above simulations to learn REG policies that select referring expressions based on the user expertise in the domain. In this section, we discuss how to code the learning agent’s goals as reward. We then discuss how the reward function is used to train the learning agent. 5.1 Reward function We designed a reward function for the goal of adapting to each user’s domain knowledge. We present the Adaptation Accuracy score (AA) that calculates how accurately the agent chose the appropriate expressions for each referent r, </context>
<context position="19105" citStr="Janarthanam and Lemon, 2010" startWordPosition="3081" endWordPosition="3084">overall adaptation accuracy. Figure 2 shows how the agent learns using the data-driven (Learned DS) during training. It can be seen in the figure 2 that towards the end the curve plateaus, signifying that learning has converged. 6 Evaluation In this section, we present the details of the evaluation process, the baseline policy, the metrics used, and the results. In a recent study, we evaluated the learned policy and several hand-coded baselines with simulated users and found that Figure 2: Learning curve - Training the Learned-DS policy produced higher adaptation accuracy than other policies (Janarthanam and Lemon, 2010). An interesting issue for research in this area is to what extent evaluation results obtained in simulated environments transfer to evaluations with real users (Lemon et al., 2006). 6.1 Baseline system In order to compare the performance of the learned policy with a baseline, a simple rule-based policy was built. This baseline was chosen because it performed better in simulation, compared to a variety of other baselines (Janarthanam and Lemon, 2010). It uses jargon for all referents by default and provides clarifications when requested. It exploits the user model in subsequent references afte</context>
</contexts>
<marker>Janarthanam, Lemon, 2010</marker>
<rawString>S. Janarthanam and O. Lemon. 2010. Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems. In Proc. ACL’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K</author>
<author>J Henderson</author>
</authors>
<title>Evaluating Effectiveness and Portability of Reinforcement Learned Dialogue Strategies with real users: the TALK TownInfo Evaluation.</title>
<date>2006</date>
<booktitle>In IEEE/ACL Spoken Language Technology.</booktitle>
<marker>K, Henderson, 2006</marker>
<rawString>O. Lemon, Georgila. K., and J. Henderson. 2006. Evaluating Effectiveness and Portability of Reinforcement Learned Dialogue Strategies with real users: the TALK TownInfo Evaluation. In IEEE/ACL Spoken Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
</authors>
<title>Learning what to say and how to say it: joint optimization of spoken dialogue management and Natural Language Generation. Computer Speech and Language.</title>
<date>2010</date>
<note>(to appear).</note>
<contexts>
<context position="5225" citStr="Lemon, 2010" startWordPosition="807" endWordPosition="808">dy, we show that using reinforcement learning this decision is learned automatically. Reinforcement Learning (RL) has been successfully used for learning dialogue management policies since (Levin et al., 1997). The learned policies allow the dialogue manager to optimally choose appropriate dialogue acts such as instructions, confirmation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 The Dialogue System In this</context>
<context position="15014" citStr="Lemon, 2010" startWordPosition="2422" endWordPosition="2423">y represented. We only code the user’s knowledge of jargon expressions using boolean variables representing whether the user knows the expression or not. 4.3 Corpus We trained the action selection model on a small corpus of 12 non-adaptive dialogues between real users and a dialogue system. There were six dialogues in which users interacted with a system using just jargon expressions and six with a system using descriptive expressions. For more discussions on our user simulation models and the corpus, please refer to (Janarthanam and Lemon, 2009b; Janarthanam and Lemon, 2009a; Janarthanam and Lemon, 2010). 5 Training The REG module was trained (operated in learning mode) using the above simulations to learn REG policies that select referring expressions based on the user expertise in the domain. In this section, we discuss how to code the learning agent’s goals as reward. We then discuss how the reward function is used to train the learning agent. 5.1 Reward function We designed a reward function for the goal of adapting to each user’s domain knowledge. We present the Adaptation Accuracy score (AA) that calculates how accurately the agent chose the appropriate expressions for each referent r, </context>
<context position="19105" citStr="Lemon, 2010" startWordPosition="3083" endWordPosition="3084">on accuracy. Figure 2 shows how the agent learns using the data-driven (Learned DS) during training. It can be seen in the figure 2 that towards the end the curve plateaus, signifying that learning has converged. 6 Evaluation In this section, we present the details of the evaluation process, the baseline policy, the metrics used, and the results. In a recent study, we evaluated the learned policy and several hand-coded baselines with simulated users and found that Figure 2: Learning curve - Training the Learned-DS policy produced higher adaptation accuracy than other policies (Janarthanam and Lemon, 2010). An interesting issue for research in this area is to what extent evaluation results obtained in simulated environments transfer to evaluations with real users (Lemon et al., 2006). 6.1 Baseline system In order to compare the performance of the learned policy with a baseline, a simple rule-based policy was built. This baseline was chosen because it performed better in simulation, compared to a variety of other baselines (Janarthanam and Lemon, 2010). It uses jargon for all referents by default and provides clarifications when requested. It exploits the user model in subsequent references afte</context>
</contexts>
<marker>Lemon, 2010</marker>
<rawString>O. Lemon. 2010. Learning what to say and how to say it: joint optimization of spoken dialogue management and Natural Language Generation. Computer Speech and Language. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
<author>W Eckert</author>
</authors>
<date>1997</date>
<booktitle>Learning Dialogue Strategies within the Markov Decision Process Framework. In Proc. of ASRU97.</booktitle>
<contexts>
<context position="4822" citStr="Levin et al., 1997" startWordPosition="747" endWordPosition="750">les (Cawsey, 1993) or a large corpus of expert-layperson interaction from which adaptive strategies can be learned and modelled, using methods such as Bayesian networks (Akiba and Tanaka, 1994). In contrast, we present an approach that learns in the absence of these expensive resources. It is also not clear how supervised approaches choose between when to seek more information and when to adapt. In this study, we show that using reinforcement learning this decision is learned automatically. Reinforcement Learning (RL) has been successfully used for learning dialogue management policies since (Levin et al., 1997). The learned policies allow the dialogue manager to optimally choose appropriate dialogue acts such as instructions, confirmation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expres</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 1997</marker>
<rawString>E. Levin, R. Pieraccini, and W. Eckert. 1997. Learning Dialogue Strategies within the Markov Decision Process Framework. In Proc. of ASRU97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems.</title>
<date>2009</date>
<booktitle>In Proc. EACL’09.</booktitle>
<contexts>
<context position="5200" citStr="Rieser and Lemon, 2009" startWordPosition="801" endWordPosition="804">tion and when to adapt. In this study, we show that using reinforcement learning this decision is learned automatically. Reinforcement Learning (RL) has been successfully used for learning dialogue management policies since (Levin et al., 1997). The learned policies allow the dialogue manager to optimally choose appropriate dialogue acts such as instructions, confirmation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 Th</context>
</contexts>
<marker>Rieser, Lemon, 2009</marker>
<rawString>V. Rieser and O. Lemon. 2009. Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems. In Proc. EACL’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Optimising information presentation for spoken dialogue systems.</title>
<date>2010</date>
<booktitle>In Proc. ACL.</booktitle>
<note>(to appear).</note>
<contexts>
<context position="5225" citStr="Rieser and Lemon, 2010" startWordPosition="805" endWordPosition="808">In this study, we show that using reinforcement learning this decision is learned automatically. Reinforcement Learning (RL) has been successfully used for learning dialogue management policies since (Levin et al., 1997). The learned policies allow the dialogue manager to optimally choose appropriate dialogue acts such as instructions, confirmation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 The Dialogue System In this</context>
</contexts>
<marker>Rieser, Lemon, 2010</marker>
<rawString>V. Rieser and O. Lemon. 2010. Optimising information presentation for spoken dialogue systems. In Proc. ACL. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>M N Stuttle</author>
<author>S J Young</author>
</authors>
<title>A Survey of Statistical User Simulation Techniques for Reinforcement Learning of Dialogue Management Strategies. Knowledge Engineering Review,</title>
<date>2006</date>
<pages>97--126</pages>
<contexts>
<context position="10920" citStr="Schatzmann et al., 2006" startWordPosition="1760" endWordPosition="1764">nt system utterance. For instance, the pair (broadband filter, desc) represents the descriptive expression “small white box”. RECs,t = {(R1, Ti), ..., (Rn, Tn)} In the evaluation mode, a trained REG policy interacts with unknown users. It consults the learned policy 7rTeg to choose the referring expressions based on the current user model. 4 User Simulations In this section, we present user simulation models that simulate the dialogue behaviour of a real human user. Several user simulation models have been proposed for use in reinforcement learning of dialogue policies (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007; Ai and Litman, 2007). However, they are suited only for learning dialogue management policies, and not natural language generation policies. In particular, our model is the first to be sensitive to a system’s choices of referring expressions. Earlier, we presented a two-tier simulation trained on data precisely for REG policy learning (Janarthanam and Lemon, 2009a). However, it is not suited for training on small corpus like the one we have at our disposal. In contrast to the earlier model, we now condition the clarification requests on the referent class rather than</context>
</contexts>
<marker>Schatzmann, Weilhammer, Stuttle, Young, 2006</marker>
<rawString>J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J. Young. 2006. A Survey of Statistical User Simulation Techniques for Reinforcement Learning of Dialogue Management Strategies. Knowledge Engineering Review, pages 97–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Weilhammer</author>
<author>H Ye</author>
<author>S J Young</author>
</authors>
<title>Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In</title>
<date>2007</date>
<booktitle>Proc of HLT/NAACL</booktitle>
<contexts>
<context position="10945" citStr="Schatzmann et al., 2007" startWordPosition="1765" endWordPosition="1768">instance, the pair (broadband filter, desc) represents the descriptive expression “small white box”. RECs,t = {(R1, Ti), ..., (Rn, Tn)} In the evaluation mode, a trained REG policy interacts with unknown users. It consults the learned policy 7rTeg to choose the referring expressions based on the current user model. 4 User Simulations In this section, we present user simulation models that simulate the dialogue behaviour of a real human user. Several user simulation models have been proposed for use in reinforcement learning of dialogue policies (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007; Ai and Litman, 2007). However, they are suited only for learning dialogue management policies, and not natural language generation policies. In particular, our model is the first to be sensitive to a system’s choices of referring expressions. Earlier, we presented a two-tier simulation trained on data precisely for REG policy learning (Janarthanam and Lemon, 2009a). However, it is not suited for training on small corpus like the one we have at our disposal. In contrast to the earlier model, we now condition the clarification requests on the referent class rather than the referent itself to h</context>
</contexts>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and S. J. Young. 2007. Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In Proc of HLT/NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shapiro</author>
<author>P Langley</author>
</authors>
<title>Separating skills from preference: Using learning to program by reward.</title>
<date>2002</date>
<booktitle>In Proc. ICML-02.</booktitle>
<contexts>
<context position="16563" citStr="Shapiro and Langley, 2002" startWordPosition="2671" endWordPosition="2674">initial state, because our objective is to adapt to the initial state of the user DKu,initial. However, in reality, designers might want their system to account for user’s changing knowledge as well. We calculate accuracy per referent RAr and then calculate the overall mean adaptation accuracy (AA) over all referents as shown below. RAr = #(appropriate expressions(r)) #(instances(r)) AdaptationAccuracyAA = 1 #(r)�rRAr 5.2 Learning The REG module was trained in learning mode using the above reward function using the SHARSHA reinforcement learning algorithm (with linear function approximation) (Shapiro and Langley, 2002). This is a hierarchical variant of SARSA, 127 which is an on-policy learning algorithm that updates the current behaviour policy (see (Sutton and Barto, 1998)). The training produced approx. 5000 dialogues. The user simulation was calibrated to produce three types of users: Novice, Intermediate and Expert, randomly but with equal probability. Initially, the REG policy chooses randomly between the referring expression types for each domain entity in the system utterance, irrespective of the user model state. Once the referring expressions are chosen, the system presents the user simulation wit</context>
</contexts>
<marker>Shapiro, Langley, 2002</marker>
<rawString>D. Shapiro and P. Langley. 2002. Separating skills from preference: Using learning to program by reward. In Proc. ICML-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sutton</author>
<author>A Barto</author>
</authors>
<title>Reinforcement Learning.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1481" citStr="Sutton and Barto, 1998" startWordPosition="223" endWordPosition="226">an adaptive hand-coded baseline policy, the learned policy performs significantly better, with a 20.8% average increase in adaptation accuracy, 12.6% decrease in time taken, and a 15.1% increase in task completion rate. The learned policy also has a significantly better subjective rating from users. This is because the learned policies adapt online to changing evidence about the user’s domain expertise. We also discuss the issue of evaluation in simulation versus evaluation with real users. 1 Introduction We present new results from an evaluation with real users, for a reinforcement learning (Sutton and Barto, 1998) framework to learn user-adaptive referring expression generation policies from datadriven user simulations. Such a policy allows the system to choose appropriate expressions to refer to domain entities in a dialogue setting. For instance, in a technical support conversation, the Oliver Lemon Interaction Lab Mathematics and Computer Science Heriot-Watt University o.lemon@hw.ac.uk Jargon: Please plug one end of the broadband cable into the broadband filter. Descriptive: Please plug one end of the thin white cable with grey ends into the small white box. Table 1: Referring expression examples fo</context>
<context position="16722" citStr="Sutton and Barto, 1998" startWordPosition="2697" endWordPosition="2700">or user’s changing knowledge as well. We calculate accuracy per referent RAr and then calculate the overall mean adaptation accuracy (AA) over all referents as shown below. RAr = #(appropriate expressions(r)) #(instances(r)) AdaptationAccuracyAA = 1 #(r)�rRAr 5.2 Learning The REG module was trained in learning mode using the above reward function using the SHARSHA reinforcement learning algorithm (with linear function approximation) (Shapiro and Langley, 2002). This is a hierarchical variant of SARSA, 127 which is an on-policy learning algorithm that updates the current behaviour policy (see (Sutton and Barto, 1998)). The training produced approx. 5000 dialogues. The user simulation was calibrated to produce three types of users: Novice, Intermediate and Expert, randomly but with equal probability. Initially, the REG policy chooses randomly between the referring expression types for each domain entity in the system utterance, irrespective of the user model state. Once the referring expressions are chosen, the system presents the user simulation with both the dialogue act and referring expression choices. The choice of referring expression affects the user’s dialogue behaviour. For instance, choosing a ja</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>R. Sutton and A. Barto. 1998. Reinforcement Learning. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>