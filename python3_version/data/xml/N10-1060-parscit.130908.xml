<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.031790">
<title confidence="0.99342">
“cba to check the spelling”
Investigating Parser Performance on Discussion Forum Posts
</title>
<author confidence="0.994716">
Jennifer Foster
</author>
<affiliation confidence="0.994079666666667">
National Centre for Language Technology
School of Computing
Dublin City University
</affiliation>
<email confidence="0.994285">
jfoster@computing.dcu.ie
</email>
<sectionHeader confidence="0.995597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9984421">
We evaluate the Berkeley parser on text from
an online discussion forum. We evaluate the
parser output with and without gold tokens
and spellings (using Sparseval and Parseval),
and we compile a list of problematic phenom-
ena for this domain. The Parseval f-score for a
small development set is 77.56. This increases
to 80.27 when we apply a set of simple trans-
formations to the input sentences and to the
Wall Street Journal (WSJ) training sections.
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995775">
Parsing techniques have recently become efficient
enough for parsers to be used as part of a pipeline in
a variety of tasks. Another recent development is the
rise of user-generated content in the form of blogs,
wikis and discussion forums. Thus, it is both inter-
esting and necessary to investigate the performance
of NLP tools trained on edited text when applied to
unedited Web 2.0 text. McClosky et al. (2006) re-
port a Parseval f-score decrease of 5% when a WSJ-
trained parser is applied to Brown corpus sentences.
In this paper, we move even further from the WSJ by
investigating the performance of the Berkeley parser
(Petrov et al., 2006) on user-generated content.
We create gold standard phrase structure trees for
the posts on two threads of the same online dis-
cussion forum. We then parse the sentences in
one thread, our development set, with the Berke-
ley parser under three conditions: 1) when it per-
forms its own tokenisation, 2) when it is provided
with gold tokens and 3) when misspellings in the in-
put have been corrected. A qualitative evaluation is
then carried out on parser output under the third con-
dition. Based on this evaluation, we identify some
“low-hanging fruit” which we attempt to handle ei-
ther by transforming the input sentence or by trans-
forming the WSJ training material. The success of
these transformations is evaluated on our develop-
ment and test sets, with encouraging results.
</bodyText>
<sectionHeader confidence="0.976432" genericHeader="method">
2 Parser Evaluation Experiments
</sectionHeader>
<bodyText confidence="0.997302263157895">
Data Our data consists of sentences that occur on
the BBC Sport 606 Football discussion forum. The
posts on this forum are quite varied, ranging from
throwaway comments to more considered essay-like
contributions. The development set consists of 42
posts (185 sentences) on a thread discussing a con-
troversial refereeing decision in a soccer match.1
The test set is made up of 40 posts (170 sentences)
on a thread discussing a player’s behaviour in the
same match.2 The average sentence length in the
development set is 18 words and the test set 15
words. Tokenisation and spelling correction were
carried out by hand on the sentences in both sets.3
They were then parsed using Bikel’s parser (Bikel,
2004) and corrected by hand using the Penn Tree-
bank Bracketing Guidelines (Bies et al., 1995).
Parser The Berkeley parser is an unlexicalised
phrase structure parser which learns a latent vari-
able PCFG by iteratively splitting the treebank non-
</bodyText>
<footnote confidence="0.99152">
1http://www.bbc.co.uk/dna/606/F15264075?
thread=7065503&amp;show=50
2http://www.bbc.co.uk/dna/606/F15265997?
thread=7066196&amp;show=50
3Note that abbreviated forms such as cos which are typical
of computer-mediated communication are not corrected.
</footnote>
<page confidence="0.939851">
381
</page>
<subsubsectionHeader confidence="0.557569">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 381–384,
</subsubsectionHeader>
<subsectionHeader confidence="0.269146">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.993082181818182">
terminals, estimating rule probabilities for the new
grammar using EM and merging the less useful
splits. We train a PCFG from WSJ2-21 by carrying
out five cycles of the split-merge process (SM5).
Tokenisation and Spelling Effects In the first ex-
periment, the parser is given the original devel-
opment set sentences which contain spelling mis-
takes and which have not been tokenised. We ask
the parser to perform its own tokenisation. In the
second experiment, the parser is given the hand-
tokenised sentences which still contain spelling mis-
takes. These are corrected for the third experiment.
Since the yields of the parser output and gold trees
are not guaranteed to match exactly, we cannot use
the evalb implementation of the Parseval evalua-
tion metrics. Instead we use Sparseval (Roark et al.,
2006), which was designed to be used to evaluate the
parsing of spoken data and can handle this situation.
An unaligned dependency evaluation is carried out:
head-finding rules are used to convert a phrase struc-
ture tree into a dependency graph. Precision and re-
call are calculated over the dependencies
The Sparseval results are shown in Table 1. For
the purposes of comparison, the WSJ23 perfor-
mance is displayed in the top row. We can see that
performance suffers when the parser performs its
own tokenisation. A reason for this is the under-use
of apostrophes in the forum data, with the result that
words such as didnt and im remain untokenised and
are tagged by the parser as common nouns:
(NP (NP (DT the) (NNS refs)) (SBAR (S (NP (NN didnt))
(VP want to make it to obvious))))
To properly see the effect of the 39 spelling errors
on parsing accuracy, we factor out the mismatches
between the correctly spelled words in the reference
set and their incorrectly spelled equivalents. We do
this by evaluating against a version of the gold stan-
dard which contains the original misspellings (third
row). We can see that the effect of spelling errors
is quite small. The Berkeley parser’s mechanism
for handling unknown words makes use of suffix in-
formation and it is able to ignore many of the con-
tent word spelling errors. It is the errors in function
words that appear to cause a greater problem:
</bodyText>
<construct confidence="0.78879">
(NP (DT the) (JJ zealous) (NNS fans) (NN whpo) (NN
care) (JJR more) )
</construct>
<table confidence="0.999486833333333">
Test Set R P F
WSJ 23 88.66 88.66 88.66
Football 68.49 70.74 69.60
Football Gold Tokens 71.54 73.25 72.39
Ft Gold Tok (misspelled gold) 73.49 75.25 74.36
Football Gold Tokens+Spell 73.94 75.59 74.76
</table>
<tableCaption confidence="0.976413">
Table 1: Sparseval scores for Berkeley SM5
</tableCaption>
<table confidence="0.999890666666667">
Test Set R P F
WSJ 23 88.88 89.46 89.17
Football Gold Tokens+Spell 78.15 76.97 77.56
</table>
<tableCaption confidence="0.998865">
Table 2: Parseval scores for Berkeley SM5
</tableCaption>
<bodyText confidence="0.999514692307692">
Gold Tokens and Spelling Leaving aside the
problems of automatic tokenisation and spelling cor-
rection, we focus on the results of the third experi-
ment. The Parseval results are given in Table 2. Note
that the performance degradation is quite large, more
than has been reported for the Charniak parser on
the Brown corpus. We examine the parser output for
each sentence in the development set. The phenom-
ena which lead the parser astray are listed in Table 3.
One problem is coordination which is difficult for
parsers on in-domain data but which is exacerbated
here by the omission of conjunctions, the use of a
comma as a conjunction and the tendency towards
unlike constituent coordination.
Parser Comparison We test the lexicalised Char-
niak parser plus reranker (Charniak and Johnson,
2005) on the development set sentences. We also
test the Berkeley parser with an SM6 grammar. The
f-scores are shown in Table 4. The parser achiev-
ing the highest score on WSJ23, namely, the C&amp;J
reranking parser, also achieves the highest score on
our development set. The difference between the
two Berkeley grammars supports the claim that an
SM6 grammar overfits to the WSJ (Petrov and Klein,
2007). However, the differences between the four
parser/grammar configurations are small.
</bodyText>
<table confidence="0.9996888">
Parser WSJ23 Football
Berkeley SM5 89.17 77.56
Berkeley SM6 89.56 77.01
Charniak First-Stage 89.13 77.13
C &amp; J Reranking 91.33 78.33
</table>
<tableCaption confidence="0.998999">
Table 4: Cross-parser and cross-grammar comparison
</tableCaption>
<page confidence="0.954002">
382
</page>
<table confidence="0.999930461538462">
Problematic Phenomena Examples
Idioms/Fixed Expressions Spot on
(S (VP (VB Spot) (PP (IN on))) (. .))
Acronyms lmao
(S (NP (PRP you))
(VP (VBZ have) (RB n’t) (VP (VBN done)
(NP (ADVP (RB that) (RB once)) (DT this) (NN season))
(NP (NN lmao)))))
Missing subject Does n’t change the result though
(SQ (VBZ Does) (RB n’t) (NP (NN change))
(NP (DT the) (NN result)) (ADVP (RB though)) (. !))
Lowercase proper nouns paul scholes
(NP (JJ paul) (NNS scholes))
Coordination Very even game and it’s sad that...
(S (ADVP (RB Very))
(NP (NP (JJ even) (NN game)) (CC and) (NP (PRP it)))
(VP (VBZ ’s) (ADJP (JJ sad)) (SBAR (IN that)...
Adverb/Adjective Confusion when playing bad
(SBAR (WHADVP (WRB when))
(S (VP (VBG playing) (ADJP (JJ bad)))))
CAPS LOCK IS ON YOU GOT BEATENBYTHE BETTER TEAM
(S (NP (PRP YOU)) (VP (VBP GOT) (NP (NNP BEATEN)
(NNP BY) (NNP THE) (NNP BETTER) (NNP TEAM))))
cos instead of because or it was cos you lost
(VP (VBD was) (ADJP (NN cos)
(SBAR (S (NP (PRP you)) (VP (VBD lost))))))
</table>
<tableCaption confidence="0.99992">
Table 3: Phenomena which lead the parser astray. The output of the parser is given for each example.
</tableCaption>
<sectionHeader confidence="0.998379" genericHeader="method">
3 Initial Improvements
</sectionHeader>
<bodyText confidence="0.999901944444444">
Parsing performance on noisy data can be improved
by transforming the input data so that it resembles
the parser’s training data (Aw et al., 2006), trans-
forming the training data so that it resembles the in-
put data (van der Plas et al., 2009), applying semi-
supervised techniques such as the self-training pro-
tocol used by McClosky et al. (2006), and changing
the parser internals, e.g. adapting the parser’s un-
known word model to take into account variation in
capitalisation and function word misspelling.4
We focus on the first two approaches and attempt
to transform both the input data and the WSJ training
material. The transformations that we experiment
with are shown in Table 5. The treebank transfor-
mations are performed in such a way that their fre-
quency distribution mirrors their distribution in the
development data. We remove discourse-marking
acronyms such as lol5 from the input sentence, but
</bodyText>
<footnote confidence="0.968581142857143">
4Even when spelling errors have been corrected, unknown
words are still an issue: 8.5% of the words in the football devel-
opment set do not occur in WSJ2-21, compared to 3.6% of the
words in WSJ23.
5In a study of teenage instant messaging, Tagliamonte and
Dennis (2008) found that forms such as lol are not as ubiquitous
as is commonly perceived. Although only occurring a couple of
</footnote>
<bodyText confidence="0.998905307692308">
do not attempt to handle acronyms which are inte-
grated into the sentence.6
We examine the effect of each transformation on
development set parsing performance and discard
those which do not improve performance. We keep
all the input sentence transformations and those tree-
bank transformations which affect lexical rules, i.e.
changing the endings on adverbs and changing the
first character of proper nouns. The treebank trans-
formations which delete subject pronouns and co-
ordinating conjunctions are not as effective. They
work in individual cases, e.g. the original analysis
of the sentence Will be here all day is
</bodyText>
<equation confidence="0.804877666666667">
(S (NP (NNP Will)) (VP be here all day) (. .))
After applying the treebank transformation, it is
(S (VP (MD Will) (VP be here all day)) (..))
</equation>
<bodyText confidence="0.97933775">
Their overall effect is, however, negative. It is likely
that, for complex phenomena such as coordination
and subject ellipsis, the development set is still too
small to inform how much of and in what way the
original treebank should be transformed. The results
of applying the effective transformations to the de-
velopment set and the test set are shown in Table 6.
times in our data, they are problematic for the parser.
</bodyText>
<footnote confidence="0.99573">
6An example is: your loss to Wigan would be more scrutu-
nized (cba to check spelling) than it has been this year
</footnote>
<page confidence="0.994694">
383
</page>
<table confidence="0.999533533333333">
Input Sentence
cos → because
Sentences consisting of all uppercase characters converted to standard capitalisation
DEAL WITH IT → Deal with it
Remove certain acronyms
lol → e
Treebank
Delete subject noun phrases when the subject is a pronoun
(S (NP (PRP It)) (VP (VBD arrived)... −→ (S (VP (VBD arrived)...
Delete or replace conjunctions with a comma (for sentence coordination)
(S ...) (CC and) (S ...) −→ (S ...) (, ,) (S ...) OR (S ...) (CC and) (S ...) −→ (S ...) (S ...)
Delete ly from adverbs
(VP (VBD arrived) (ADVP (RB quickly))) −→ (VP (VBD arrived) (ADVP (RB quick)))
Replace uppercase first character in proper nouns
(NP (NP (NNP Warner) (POS ’s)) (NNprice)) −→ (NP (NP (NNP warner) (POS ’s)) (NNprice))
</table>
<tableCaption confidence="0.974907">
Table 5: Input Sentence and Treebank Transformations
</tableCaption>
<table confidence="0.9999128">
Configuration Recall Precision F-Score
Baseline Dev 78.15 76.97 77.56
Transformed Dev 80.83 79.73 80.27
Baseline Test 77.61 79.14 78.37
Transformed Test 80.10 79.77 79.93
</table>
<tableCaption confidence="0.999362">
Table 6: Effect of transformations on dev and test set
</tableCaption>
<bodyText confidence="0.999878333333333">
The recall and precision improvements on the devel-
opment set are statistically significant (p &lt; 0.02), as
is the recall improvement on the test set (p &lt; 0.05).
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999978466666667">
Ongoing research on the problem of parsing
unedited informal text has been presented. At the
moment, because of the small size of the data sets
and the variety of writing styles in the development
set, only tentative conclusions can be drawn. How-
ever, even this small data set reveals clear problems
for WSJ-trained parsers: the handling of long co-
ordinated sentences (particularly in the presence of
erratic punctuation usage), domain-specific fixed ex-
pressions and unknown words. We have presented
some preliminary experimental results using simple
transformations to both the input sentence and the
parser’s training material. Treebank transformations
need to be more thoroughly explored with use made
of the Switchboard corpus as well as the WSJ.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.954039">
Thanks to the reviewers and to Emmet O´ Briain,
Deirdre Hogan, Adam Bermingham, Joel Tetreault.
</bodyText>
<sectionHeader confidence="0.996292" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999183914285714">
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normali-
sation. In Proceedings of the 21st COLING/44th ACL.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
style, Penn Treebank Project. Technical Report Tech
Report MS-CIS-95-06, University of Pennsylvania.
Daniel Bikel. 2004. Intricacies of Collins Parsing Model.
Computational Linguistics, 30(4):479–511.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of the 21st COLING/44th ACL.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings ofHLT
NAACL 2007.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact and inter-
pretable tree annotation. In Proceedings of the 21st
COLING and the 44th ACL.
Brian Roark, Mary Harper, Eugene Charniak, Bonnie
Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, Mari
Ostendorf, John Hale, Anna Krasnyanskaya, Matthew
Lease, Izhak Shafran, Matthew Snover, Robin Stewart,
and Lisa Yung. 2006. SParseval: Evaluation metrics
for parsing speech. In Proceedings ofLREC.
Sali A. Tagliamonte and Derek Dennis. 2008. Linguis-
tic ruin? LOL! Instant messaging and teen language.
American Speech, 83(1).
Lonneke van der Plas, James Henderson, and Paola
Merlo. 2009. Domain adaptation with artificial data
for semantic parsing of speech. In Proceedings ofHLT
NAACL 2009, Companion Volume: Short Papers.
</reference>
<page confidence="0.998941">
384
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.697876">
<title confidence="0.9331645">cba to check the spelling” Investigating Parser Performance on Discussion Forum Posts</title>
<author confidence="0.986028">Jennifer</author>
<affiliation confidence="0.948580333333333">National Centre for Language School of Dublin City</affiliation>
<email confidence="0.950335">jfoster@computing.dcu.ie</email>
<abstract confidence="0.997918">We evaluate the Berkeley parser on text from an online discussion forum. We evaluate the parser output with and without gold tokens and spellings (using Sparseval and Parseval), and we compile a list of problematic phenomena for this domain. The Parseval f-score for a small development set is 77.56. This increases to 80.27 when we apply a set of simple transformations to the input sentences and to the Wall Street Journal (WSJ) training sections.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AiTi Aw</author>
<author>Min Zhang</author>
<author>Juan Xiao</author>
<author>Jian Su</author>
</authors>
<title>A phrase-based statistical model for SMS text normalisation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st COLING/44th ACL.</booktitle>
<contexts>
<context position="8882" citStr="Aw et al., 2006" startWordPosition="1463" endWordPosition="1466"> when playing bad (SBAR (WHADVP (WRB when)) (S (VP (VBG playing) (ADJP (JJ bad))))) CAPS LOCK IS ON YOU GOT BEATENBYTHE BETTER TEAM (S (NP (PRP YOU)) (VP (VBP GOT) (NP (NNP BEATEN) (NNP BY) (NNP THE) (NNP BETTER) (NNP TEAM)))) cos instead of because or it was cos you lost (VP (VBD was) (ADJP (NN cos) (SBAR (S (NP (PRP you)) (VP (VBD lost)))))) Table 3: Phenomena which lead the parser astray. The output of the parser is given for each example. 3 Initial Improvements Parsing performance on noisy data can be improved by transforming the input data so that it resembles the parser’s training data (Aw et al., 2006), transforming the training data so that it resembles the input data (van der Plas et al., 2009), applying semisupervised techniques such as the self-training protocol used by McClosky et al. (2006), and changing the parser internals, e.g. adapting the parser’s unknown word model to take into account variation in capitalisation and function word misspelling.4 We focus on the first two approaches and attempt to transform both the input data and the WSJ training material. The transformations that we experiment with are shown in Table 5. The treebank transformations are performed in such a way th</context>
</contexts>
<marker>Aw, Zhang, Xiao, Su, 2006</marker>
<rawString>AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for SMS text normalisation. In Proceedings of the 21st COLING/44th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
</authors>
<title>Bracketing guidelines for Treebank II style, Penn Treebank Project.</title>
<date>1995</date>
<tech>Technical Report Tech Report MS-CIS-95-06,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2926" citStr="Bies et al., 1995" startWordPosition="479" endWordPosition="482"> considered essay-like contributions. The development set consists of 42 posts (185 sentences) on a thread discussing a controversial refereeing decision in a soccer match.1 The test set is made up of 40 posts (170 sentences) on a thread discussing a player’s behaviour in the same match.2 The average sentence length in the development set is 18 words and the test set 15 words. Tokenisation and spelling correction were carried out by hand on the sentences in both sets.3 They were then parsed using Bikel’s parser (Bikel, 2004) and corrected by hand using the Penn Treebank Bracketing Guidelines (Bies et al., 1995). Parser The Berkeley parser is an unlexicalised phrase structure parser which learns a latent variable PCFG by iteratively splitting the treebank non1http://www.bbc.co.uk/dna/606/F15264075? thread=7065503&amp;show=50 2http://www.bbc.co.uk/dna/606/F15265997? thread=7066196&amp;show=50 3Note that abbreviated forms such as cos which are typical of computer-mediated communication are not corrected. 381 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 381–384, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics termina</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, 1995</marker>
<rawString>Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre. 1995. Bracketing guidelines for Treebank II style, Penn Treebank Project. Technical Report Tech Report MS-CIS-95-06, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins Parsing Model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="2838" citStr="Bikel, 2004" startWordPosition="466" endWordPosition="467"> The posts on this forum are quite varied, ranging from throwaway comments to more considered essay-like contributions. The development set consists of 42 posts (185 sentences) on a thread discussing a controversial refereeing decision in a soccer match.1 The test set is made up of 40 posts (170 sentences) on a thread discussing a player’s behaviour in the same match.2 The average sentence length in the development set is 18 words and the test set 15 words. Tokenisation and spelling correction were carried out by hand on the sentences in both sets.3 They were then parsed using Bikel’s parser (Bikel, 2004) and corrected by hand using the Penn Treebank Bracketing Guidelines (Bies et al., 1995). Parser The Berkeley parser is an unlexicalised phrase structure parser which learns a latent variable PCFG by iteratively splitting the treebank non1http://www.bbc.co.uk/dna/606/F15264075? thread=7065503&amp;show=50 2http://www.bbc.co.uk/dna/606/F15265997? thread=7066196&amp;show=50 3Note that abbreviated forms such as cos which are typical of computer-mediated communication are not corrected. 381 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 381–384, Los </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel Bikel. 2004. Intricacies of Collins Parsing Model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Course-tofine n-best-parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="6949" citStr="Charniak and Johnson, 2005" startWordPosition="1134" endWordPosition="1137">esults are given in Table 2. Note that the performance degradation is quite large, more than has been reported for the Charniak parser on the Brown corpus. We examine the parser output for each sentence in the development set. The phenomena which lead the parser astray are listed in Table 3. One problem is coordination which is difficult for parsers on in-domain data but which is exacerbated here by the omission of conjunctions, the use of a comma as a conjunction and the tendency towards unlike constituent coordination. Parser Comparison We test the lexicalised Charniak parser plus reranker (Charniak and Johnson, 2005) on the development set sentences. We also test the Berkeley parser with an SM6 grammar. The f-scores are shown in Table 4. The parser achieving the highest score on WSJ23, namely, the C&amp;J reranking parser, also achieves the highest score on our development set. The difference between the two Berkeley grammars supports the claim that an SM6 grammar overfits to the WSJ (Petrov and Klein, 2007). However, the differences between the four parser/grammar configurations are small. Parser WSJ23 Football Berkeley SM5 89.17 77.56 Berkeley SM6 89.56 77.01 Charniak First-Stage 89.13 77.13 C &amp; J Reranking</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Course-tofine n-best-parsing and maxent discriminative reranking. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st COLING/44th ACL.</booktitle>
<contexts>
<context position="1097" citStr="McClosky et al. (2006)" startWordPosition="171" endWordPosition="174">f-score for a small development set is 77.56. This increases to 80.27 when we apply a set of simple transformations to the input sentences and to the Wall Street Journal (WSJ) training sections. 1 Introduction Parsing techniques have recently become efficient enough for parsers to be used as part of a pipeline in a variety of tasks. Another recent development is the rise of user-generated content in the form of blogs, wikis and discussion forums. Thus, it is both interesting and necessary to investigate the performance of NLP tools trained on edited text when applied to unedited Web 2.0 text. McClosky et al. (2006) report a Parseval f-score decrease of 5% when a WSJtrained parser is applied to Brown corpus sentences. In this paper, we move even further from the WSJ by investigating the performance of the Berkeley parser (Petrov et al., 2006) on user-generated content. We create gold standard phrase structure trees for the posts on two threads of the same online discussion forum. We then parse the sentences in one thread, our development set, with the Berkeley parser under three conditions: 1) when it performs its own tokenisation, 2) when it is provided with gold tokens and 3) when misspellings in the i</context>
<context position="9080" citStr="McClosky et al. (2006)" startWordPosition="1498" endWordPosition="1501">NP THE) (NNP BETTER) (NNP TEAM)))) cos instead of because or it was cos you lost (VP (VBD was) (ADJP (NN cos) (SBAR (S (NP (PRP you)) (VP (VBD lost)))))) Table 3: Phenomena which lead the parser astray. The output of the parser is given for each example. 3 Initial Improvements Parsing performance on noisy data can be improved by transforming the input data so that it resembles the parser’s training data (Aw et al., 2006), transforming the training data so that it resembles the input data (van der Plas et al., 2009), applying semisupervised techniques such as the self-training protocol used by McClosky et al. (2006), and changing the parser internals, e.g. adapting the parser’s unknown word model to take into account variation in capitalisation and function word misspelling.4 We focus on the first two approaches and attempt to transform both the input data and the WSJ training material. The transformations that we experiment with are shown in Table 5. The treebank transformations are performed in such a way that their frequency distribution mirrors their distribution in the development data. We remove discourse-marking acronyms such as lol5 from the input sentence, but 4Even when spelling errors have bee</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In Proceedings of the 21st COLING/44th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings ofHLT NAACL</booktitle>
<contexts>
<context position="7344" citStr="Petrov and Klein, 2007" startWordPosition="1201" endWordPosition="1204"> by the omission of conjunctions, the use of a comma as a conjunction and the tendency towards unlike constituent coordination. Parser Comparison We test the lexicalised Charniak parser plus reranker (Charniak and Johnson, 2005) on the development set sentences. We also test the Berkeley parser with an SM6 grammar. The f-scores are shown in Table 4. The parser achieving the highest score on WSJ23, namely, the C&amp;J reranking parser, also achieves the highest score on our development set. The difference between the two Berkeley grammars supports the claim that an SM6 grammar overfits to the WSJ (Petrov and Klein, 2007). However, the differences between the four parser/grammar configurations are small. Parser WSJ23 Football Berkeley SM5 89.17 77.56 Berkeley SM6 89.56 77.01 Charniak First-Stage 89.13 77.13 C &amp; J Reranking 91.33 78.33 Table 4: Cross-parser and cross-grammar comparison 382 Problematic Phenomena Examples Idioms/Fixed Expressions Spot on (S (VP (VB Spot) (PP (IN on))) (. .)) Acronyms lmao (S (NP (PRP you)) (VP (VBZ have) (RB n’t) (VP (VBN done) (NP (ADVP (RB that) (RB once)) (DT this) (NN season)) (NP (NN lmao))))) Missing subject Does n’t change the result though (SQ (VBZ Does) (RB n’t) (NP (NN </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings ofHLT NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st COLING and the 44th ACL.</booktitle>
<contexts>
<context position="1328" citStr="Petrov et al., 2006" startWordPosition="212" endWordPosition="215"> recently become efficient enough for parsers to be used as part of a pipeline in a variety of tasks. Another recent development is the rise of user-generated content in the form of blogs, wikis and discussion forums. Thus, it is both interesting and necessary to investigate the performance of NLP tools trained on edited text when applied to unedited Web 2.0 text. McClosky et al. (2006) report a Parseval f-score decrease of 5% when a WSJtrained parser is applied to Brown corpus sentences. In this paper, we move even further from the WSJ by investigating the performance of the Berkeley parser (Petrov et al., 2006) on user-generated content. We create gold standard phrase structure trees for the posts on two threads of the same online discussion forum. We then parse the sentences in one thread, our development set, with the Berkeley parser under three conditions: 1) when it performs its own tokenisation, 2) when it is provided with gold tokens and 3) when misspellings in the input have been corrected. A qualitative evaluation is then carried out on parser output under the third condition. Based on this evaluation, we identify some “low-hanging fruit” which we attempt to handle either by transforming the</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact and interpretable tree annotation. In Proceedings of the 21st COLING and the 44th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Mary Harper</author>
<author>Eugene Charniak</author>
<author>Bonnie Dorr</author>
<author>Mark Johnson</author>
<author>Jeremy G Kahn</author>
</authors>
<title>SParseval: Evaluation metrics for parsing speech.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC.</booktitle>
<location>Yang Liu, Mari Ostendorf, John Hale, Anna Krasnyanskaya, Matthew Lease, Izhak Shafran, Matthew Snover, Robin</location>
<contexts>
<context position="4321" citStr="Roark et al., 2006" startWordPosition="682" endWordPosition="685">rocess (SM5). Tokenisation and Spelling Effects In the first experiment, the parser is given the original development set sentences which contain spelling mistakes and which have not been tokenised. We ask the parser to perform its own tokenisation. In the second experiment, the parser is given the handtokenised sentences which still contain spelling mistakes. These are corrected for the third experiment. Since the yields of the parser output and gold trees are not guaranteed to match exactly, we cannot use the evalb implementation of the Parseval evaluation metrics. Instead we use Sparseval (Roark et al., 2006), which was designed to be used to evaluate the parsing of spoken data and can handle this situation. An unaligned dependency evaluation is carried out: head-finding rules are used to convert a phrase structure tree into a dependency graph. Precision and recall are calculated over the dependencies The Sparseval results are shown in Table 1. For the purposes of comparison, the WSJ23 performance is displayed in the top row. We can see that performance suffers when the parser performs its own tokenisation. A reason for this is the under-use of apostrophes in the forum data, with the result that w</context>
</contexts>
<marker>Roark, Harper, Charniak, Dorr, Johnson, Kahn, 2006</marker>
<rawString>Brian Roark, Mary Harper, Eugene Charniak, Bonnie Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, Mari Ostendorf, John Hale, Anna Krasnyanskaya, Matthew Lease, Izhak Shafran, Matthew Snover, Robin Stewart, and Lisa Yung. 2006. SParseval: Evaluation metrics for parsing speech. In Proceedings ofLREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sali A Tagliamonte</author>
<author>Derek Dennis</author>
</authors>
<title>Linguistic ruin? LOL! Instant messaging and teen language.</title>
<date>2008</date>
<journal>American Speech,</journal>
<volume>83</volume>
<issue>1</issue>
<contexts>
<context position="9913" citStr="Tagliamonte and Dennis (2008)" startWordPosition="1634" endWordPosition="1637"> attempt to transform both the input data and the WSJ training material. The transformations that we experiment with are shown in Table 5. The treebank transformations are performed in such a way that their frequency distribution mirrors their distribution in the development data. We remove discourse-marking acronyms such as lol5 from the input sentence, but 4Even when spelling errors have been corrected, unknown words are still an issue: 8.5% of the words in the football development set do not occur in WSJ2-21, compared to 3.6% of the words in WSJ23. 5In a study of teenage instant messaging, Tagliamonte and Dennis (2008) found that forms such as lol are not as ubiquitous as is commonly perceived. Although only occurring a couple of do not attempt to handle acronyms which are integrated into the sentence.6 We examine the effect of each transformation on development set parsing performance and discard those which do not improve performance. We keep all the input sentence transformations and those treebank transformations which affect lexical rules, i.e. changing the endings on adverbs and changing the first character of proper nouns. The treebank transformations which delete subject pronouns and coordinating co</context>
</contexts>
<marker>Tagliamonte, Dennis, 2008</marker>
<rawString>Sali A. Tagliamonte and Derek Dennis. 2008. Linguistic ruin? LOL! Instant messaging and teen language. American Speech, 83(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
</authors>
<title>Domain adaptation with artificial data for semantic parsing of speech.</title>
<date>2009</date>
<booktitle>In Proceedings ofHLT NAACL 2009, Companion Volume: Short Papers.</booktitle>
<marker>van der Plas, Henderson, Merlo, 2009</marker>
<rawString>Lonneke van der Plas, James Henderson, and Paola Merlo. 2009. Domain adaptation with artificial data for semantic parsing of speech. In Proceedings ofHLT NAACL 2009, Companion Volume: Short Papers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>