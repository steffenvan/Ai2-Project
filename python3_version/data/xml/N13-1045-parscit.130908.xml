<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.036552">
<title confidence="0.998284">
Using a Supertagged Dependency Language Model to Select
a Good Translation in System Combination
</title>
<author confidence="0.999716">
Wei-Yun Ma Kathleen McKeown
</author>
<affiliation confidence="0.9999345">
Department of Computer Science Department of Computer Science
Columbia University Columbia University
</affiliation>
<address confidence="0.989562">
New York, NY 10027, USA New York, NY 10027, USA
</address>
<email confidence="0.99948">
ma@cs.columbia.edu kathy@cs.columbia.edu
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9983759375">
We present a novel, structured language
model - Supertagged Dependency Language
Model to model the syntactic dependencies
between words. The goal is to identify
ungrammatical hypotheses from a set of
candidate translations in a MT system
combination framework and help select the
best translation candidates using a variety of
sentence-level features. We use a two-step
mechanism based on constituent parsing and
elementary tree extraction to obtain supertags
and their dependency relations. Our
experiments show that the structured language
model provides significant improvement in
the framework of sentence-level system
combination.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999346224489796">
In recent years, there has been a burgeoning
interest in incorporating syntactic structure into
Statistical machine translation (SMT) models (e..g,
Galley et al., 2006; DeNeefe and Knight 2009;
Quirk et al., 2005). In addition to modeling
syntactic structure in the decoding process, a
methodology for candidate translation selection
has also emerged. This methodology first generates
multiple candidate translations followed by
rescoring using global sentence-level syntactic
features to select the final translation. The
advantage of this methodology is that it allows for
easy integration of complex syntactic features that
would be too expensive to use during the decoding
process. The methodology is usually applied in two
scenarios: one is as part of an n-best reranking
(Och et al., 2004; Hasan et al., 2006), where n-best
candidate translations are generated through a
decoding process. The other is translation selection
or reranking (Hildebrand and Vogel 2008;
Callison-Burch et al., 2012), where candidate
translations are generated by different decoding
processes or different decoders.
This paper belongs to the latter; the goal is to
identify ungrammatical hypotheses from given
candidate translations using grammatical
knowledge in the target language that expresses
syntactic dependencies between words. To achieve
that, we propose a novel Structured Language
Model (SLM) - Supertagged Dependency
Language Model (SDLM) to model the syntactic
dependencies between words. Supertag (Bangalore
and Joshi, 1999) is an elementary syntactic
structure based on Lexicalized Tree Adjoining
Grammar (LTAG). Traditional supertagged n-gram
LM predicts the next supertag based on the
immediate words to the left with supertags, so it
can not explicitly model long-distance dependency
relations. In contrast, SDLM predicts the next
supertag using the words with supertags on which
it syntactically depend, and these words could be
anywhere and arbitrarily far apart in a sentence. A
candidate translation’s grammatical degree or
“fluency” can be measured by simply calculating
the SDLM likelihood of the supertagged
dependency structure that spans the entire sentence.
To obtain the supertagged dependency structure,
the most intuitive way is through a LTAG parser
(Schabes et al., 1988). However, this could be very
</bodyText>
<page confidence="0.991629">
433
</page>
<subsectionHeader confidence="0.293852">
Proceedings of NAACL-HLT 2013, pages 433–438,
</subsectionHeader>
<bodyText confidence="0.9861635">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
slow as it has time complexity of O(n6). Instead
we propose an alternative mechanism in this paper:
first we use a constituent parser1 of O(n3) ~ O(n5)
to obtain the parse of a sentence, and then we
extract elementary trees with dependencies from
the parse in linear time. Aside from the
consideration of time complexity, another
motivation of this two-step mechanism is that
compared with LTAG parsing, the mechanism is
more flexible for defining syntactic structures of
elementary trees for our needs. Because those
structures are defined only within the elementary
tree extractor, we can easily adjust the definition of
those structures within the extractor and avoid
redesigning or retraining our constituent parser.
We experiment with sentence-level translation
combination of five different translation systems;
the goal is for the system to select the best
translation for each input source sentence among
the translations provided by the five systems. The
results show a significant improvement of 1.45
Bleu score over the best single MT system and
0.72 Bleu score over a baseline sentence-level
combination system of using consensus and n-
gram LM.
</bodyText>
<sectionHeader confidence="0.999878" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9995235">
Och et al., (2004) investigated various syntactic
feature functions to rerank the n-best candidate
translations. Most features are syntactically
motivated and based on alignment information
between the source sentence and the target
translation. The results are rather disappointing.
Only the non-syntactic IBM model 1 yielded
significant improvement. All other tree-based
feature functions had only a very small effect on
the performance.
In contrast to (Och et al., 2004)’s bilingual
syntax features, Hasan et al., (2006) focused on
monolingual syntax features in n-best reranking.
They also investigated the effect of directly using
the log-likelihood of the output of a HMM-based
supertagger, and found it did not improve
performance significantly. It is worth noticing that
this log-likelihood is based on supertagged n-gram
</bodyText>
<footnote confidence="0.7098122">
1 Stanford parser (http://nlp.stanford.edu/software/lex-
parser.shtml). We use its PCFG version of O(n3) for SDLM
training of part of Gigaword in addition to Treebank and use
its factor version of O(n5) to calculate the SDLM likelihood of
translations.
</footnote>
<bodyText confidence="0.9999765625">
LM, which is one type of class-based n-gram LM,
so it does not model explicit syntactic
dependencies between words in contrast to the
work we describe in this paper. Hardmeier et al.,
(2012) use tree kernels over constituency and
dependency parse trees for either the input or
output sentences to identify constructions that are
difficult to translate in the source language, and
doubtful syntactic structures in the output language.
The tree fragments extracted by their tree kernels
are similar to our elementary trees but they only
regard them as the individual inputs of support
vector machine regression while binary relations of
our elementary trees are considered in a
formulation of a structural language model.
Outside the field of candidate translation
selection, Hassan et al., (2007) proposed a phrase-
based SMT model that integrates supertags into the
target side of the translation model and the target
n-gram LM. Two kinds of supertags are employed:
those from LTAG and Combinatory Categorial
Grannar (CCG), and both yield similar
improvements. They found that using both or
either of the supertag-based translation model and
supertagged LM can achieve significant
improvement. Again, the supertagged LM is a
class-based n-gram LM and does not model
explicit syntactic dependencies during decoding.
In the field of MT system combination, word-
level confusion network decoding is one of the
most successful approaches (Matusov et al., 2006;
Rosti et al., 2007; He et al. 2008; Karakos et al.
2008; Sim et al. 2007; Xu et al. 2011). It is capable
of generating brand new translations but it is
difficult to consider more complex syntax such as
dependency LM during decoding since it adds one
word at a time while a dependency based LM must
parse a complete sentence. Typically, a confusion
network approach selects one translation as the
best and uses this as the backbone for the
confusion network. The work we present here
could provide a more sophisticated mechanism for
selecting the backbone. Alternatively, one can
enhance confusion network models by
collaborating with a sentence-level combination
model which uses complex syntax to re-rank n-best
outputs of a confusion network model. This kind of
collaboration is one of our future works.
</bodyText>
<page confidence="0.998484">
434
</page>
<sectionHeader confidence="0.990189" genericHeader="method">
3 LTAG and Supertag
</sectionHeader>
<bodyText confidence="0.999959416666667">
LTAG (Joshi et al., 1975; Schabes et al., 1988) is a
formal tree rewriting formalism, which consists of
a set of elementary trees, corresponding to minimal
linguistic structures that localize dependencies,
including long-distance dependencies, such as
predicate-argument structure. Each elementary tree
is associated with at least one lexical item on its
frontier. The lexical item associated with an
elementary tree is called the anchor in that tree; an
elementary tree thus serves as a description of
syntactic constraints of the anchor. The elementary
syntactic structures of elementary trees are called
supertags (Bangalore and Joshi, 1999), in order to
distinguish them from the standard part-of-speech
tags. Some examples are provided in figure 1 (b).
Elementary trees are divided into initial and
auxiliary trees. Initial trees are those for which all
non-terminal nodes on the frontier are substitutable.
Auxiliary trees are defined as initial trees, except
that exactly one frontier, non-terminal node must
be a foot node, with the same label as the root node.
Two operations - substitution and adjunction - are
provided in LTAG to combine elementary trees
into a derived tree.
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="method">
4 SDLM
</sectionHeader>
<bodyText confidence="0.999969451612903">
Our goal is to use SDLM to calculate the
grammaticality of translated sentences. We do this
by calculating the likelihood of the supertagged
dependency structure that spans the entire sentence
using SDLM. To obtain the supertagged
dependency linkage, the most intuitive way is
through a LTAG parser (Schabes et al., 1988).
However, this could be very slow as it has time
complexity of O(n6). Another possibility is to
follow the procedure in (Joshi and Srinivas 1994,
Bangalore and Joshi, 1999): use a HMM-based
supertagger to assign words with supertags,
followed by derivation of a shallow parse in linear
time based on only the supertags to obtain the
dependencies. But since this approach uses only
the local context, in (Joshi and Srinivas 1994), they
also proposed another greedy algorithm based on
supertagged dependency probabilities to gradually
select the path with the maximum path probability
to extend to the remaining directions in the
dependency list.
In contrast to the LTAG parsing and
supertagging-based approaches, we propose an
alternative mechanism: first we use a state-of-the-
art constituent parser to obtain the parse of a
sentence, and then we extract elementary trees with
dependencies from the parse to assign each word
with an elementary tree. The second step is similar
to the approach used in extracting elementary trees
from the TreeBank (Xia, 1999; Chen and Vijay-
Shanker, 2000).
</bodyText>
<subsectionHeader confidence="0.988881">
4.1 Elementary Tree Extraction
</subsectionHeader>
<bodyText confidence="0.999794625">
We use an elementary tree extractor, a
modification of (Chen and Vijay-Shanker, 2000),
to serve our purpose. Heuristic rules were used to
distinguish arguments from adjuncts, and the
extraction process can be regarded as a process that
gradually decomposes a constituent parse to
multiple elementary trees and records substitutions
and adjunctions. From elementary trees, we can
obtain supertags by only considering syntactic
structure and ignoring anchor words. Take the
sentence – “The hungry boys ate dinner” as an
example; the constituent parse and extracted
supertags are shown in Figure 1.
In Figure 1 (b), dotted lines represent the
operations of substitution and adjunction. Note that
each word in a translated sentence would be
assigned exactly one elementary syntactic structure
which is associated with a unique supertag id for
the whole corpus. Different anchor words could
own the same elementary syntactic structure and
would be assigned the same supertag id, such as
“ α1 ” for “boys” and “dinner”. For our corpus,
around 1700 different elementary syntactic
structures (1700 supertag ids) are extracted.
</bodyText>
<figureCaption confidence="0.989383">
Figure 1. (a) Parse of “The hungry boys ate dinner”
</figureCaption>
<page confidence="0.994036">
435
</page>
<figureCaption confidence="0.998056">
Figure 1. (b) Extracted elementary trees
</figureCaption>
<subsectionHeader confidence="0.980706">
4.2 Model
</subsectionHeader>
<bodyText confidence="0.998554304347826">
Bangalore and Joshi (1999) gave a concise
description for dependencies between supertags:
“A supertag is dependent on another supertag if the
former substitutes or adjoins into the latter”.
Following this description, for the example in
Figure 1 (b), supertags of “the” and “hungry” are
dependent on the supertag of “boys”, and supertags
of “boys” and “dinner” are dependent on the
supertag of “ate”. These dependencies between
supertags also provide the dependencies between
anchor words.
Since the syntactic constraints for each word in
its context are decided and described through its
supertag, the likelihood of SDLM for a sentence
could also be regarded as the degree of violations
of the syntactic constraints on all words in the
sentence. Consider a sentence S = w1 w2 ...wn with
corresponding supertags T = t1 t2 ...tn. We use di=j
to represent the dependency relations for words or
supertags. For example, d3 = 5 means that w3
depends on w5 or t3 depends on t5. We propose five
different bigram SDLM as follows and evaluate
their effects in section 5.
</bodyText>
<equation confidence="0.854205666666667">
11 P(witi  |w t )
d d
i i
</equation>
<bodyText confidence="0.997722">
smoothing. Take Figure 1 (b) as an example; if
using model (1), the SDLM likelihood of “The
hungry boys ate dinner” is
</bodyText>
<equation confidence="0.9999715">
P(the, fl1  |boys,α1) * P(hungry, fl2  |boys,α1) * P(boys,α1  |ate,α2) *
P(dinner,α1  |ate,α2) * P(ate,α
</equation>
<bodyText confidence="0.999891333333333">
In our experiment on sentence-level translation
combination, we use a log-linear model to integrate
all features including SDLM models. The
corresponding weights are trained discriminatively
for Bleu score using Minimum Error Rate Training
(MERT).
</bodyText>
<sectionHeader confidence="0.998821" genericHeader="method">
5 Experiment
</sectionHeader>
<bodyText confidence="0.99693684">
Our experiments are conducted and reported on the
Chinese-English dataset from NIST 2008
(LDC2010T01). It consists of four human
reference translations and corresponding machine
translations for the NIST Open MT08 test set,
which consists of newswire and web data. The test
set contains 105 documents with 1312 sentences
and output from 23 machine translation systems.
Each system provides the top one translation
hypothesis for every sentence. We further divide
the NIST Open MT08 test set into the tuning set
and test set for our experiment of sentence-level
translation combination. We divided the 1312
sentences into tuning data of 524 sentences and the
test set of 788 sentences. Out of 23 MT systems,
we manually select the top five MT systems as our
MT systems for our combination experiment.
In terms of SDLM training, since the size of
TreeBank-extracted elementary trees is much
smaller compared to most practical n-gram LMs
trained from the Gigaword corpus, we also extract
elementary trees from automatically-generated
parses of part of the Gigaword corpus (around one-
year newswire of “afp_eng” in Gigaword 4) in
addition to TreeBank-extracted elementary trees.
</bodyText>
<figure confidence="0.98273403125">
DT
NP*
anchor
word
JJ NP*
NN
anchor
word
ate
anchor
word:
the
hungry
boys
dinner
supertag id: fl1 fl2 α1 α2 α1
elementary
syntactic
structure
(supertag):
NP
anchor anchor
word word VB
NP
NP
NP1 ↓ VP
S
NP2↓
anchor
word
NP
NN
</figure>
<equation confidence="0.974912705882353">
i
) P(ti  |tdi )P(wi  |ti )
i i
11P(w t11
i
i ti I wd, d
SDLM model(1)
SDLM model(2)
|
2
root)
11 P(ti  |tdi ) SDLM model(3) 5.1 Feature Functions
i
11 P(wi  |ti) SDLM model(4)
i
11 P(wi  |wd ) SDLM model(5) i
i
</equation>
<bodyText confidence="0.999149555555556">
SDLM model (2) is the approximation form of
model (1); model (3) and (4) are individual terms
of model (2); model (5) models word dependencies
based on elementary tree dependencies. The
estimation of the probabilities is done using
maximum likelihood estimations with Laplace
For the baseline combination system, we use the
following feature functions in the log-linear model
to calculate the score of a system translation.
</bodyText>
<listItem confidence="0.99424775">
• Sentence consensus based on Translation Edit
Ratio (TER)
• Gigaword-trained 3-gram LM and word
penalty
</listItem>
<page confidence="0.998826">
436
</page>
<bodyText confidence="0.9837985">
many other NLP tasks, such as speech recognition
and natural language generation.
</bodyText>
<sectionHeader confidence="0.857533" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999455">
For testing SDLM, in additional to all features
that the baseline combination system uses, we add
single or multiple SDLM models in the log-linear
model, and each SDLM model has its own weight.
</bodyText>
<subsectionHeader confidence="0.992751">
5.2 Result
</subsectionHeader>
<bodyText confidence="0.999665333333333">
From table 1, we can see that the combination of
SDLM model 3, 4 and 5 yields the best
performance, which is better than the best MT
system by Bleu of 1.45, TER of 0.67 and
METEOR of 1.25, and also better than the baseline
combination system by Bleu of 0.72, TER of 0.25
and METEOR of 0.44. Compared with SDLM
model 5, which represents a type of word
dependency LM without labels, the results show
that adding appropriate syntactic “labels” (here,
they are “supertags”) on word dependencies brings
benefits.
</bodyText>
<table confidence="0.998598">
Bleu TER METEOR
Best MT system 30.16 55.45 54.43
baseline 30.89 55.03 55.24
baseline+ model 1 31.29 54.99 55.63
baseline+ model 2 31.25 55.23 55.37
baseline+ model 3 31.25 55.06 55.40
baseline+ model 4 31.44 54.70 55.54
baseline+ model 5 31.39 55.15 55.68
baseline+ model 3+ 31.61 54.78 55.68
model 4+ model 5
</table>
<tableCaption confidence="0.999896">
Table 1. Result of Sentence-level Translation Combination
</tableCaption>
<sectionHeader confidence="0.998801" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999741896551724">
In this paper we presented Supertagged
Dependency Language Model for explicitly
modeling syntactic dependencies of the words of
translated sentences. Our goal is to select the most
grammatical translation from candidate translations.
To obtain the supertagged dependency structure of
a translation candidate, a two-step mechanism
based on constituent parsing and elementary tree
extraction is also proposed. SDLM shows its
effectiveness in the scenario of translation
selection.
There are several avenues for future work: we
have focused on bigram dependencies in our
models; extension to more than two dependent
elementary trees is straightforward. It would also
be worth investigating the performance of using
our sentence-level model to re-rank n-best outputs
of a confusion network model. And in terms of
applications, SDLM can be directly applied to
We would like to thank Owen Rambow for
providing the elementary tree extractor and also
thank the anonymous reviewers for their helpful
comments. This work is supported by the National
Science Foundation via Grant No. 0910778
entitled “Richer Representations for Machine
Translation”. All views expressed in this paper are
those of the authors and do not necessarily
represent the view of the National Science
Foundation.
</bodyText>
<sectionHeader confidence="0.999381" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997839189189189">
Srinivas Bangalore and Aravind K. Joshi. 1999.
Supertagging: An approach to almost parsing.
Computational Linguistics, 25(2):237–265.
John Chen and K. Vijay-Shanker. 2000. Automated
extraction of TAGs from the Penn treebank. In
Proceedings of the Sixth International Workshop on
Parsing Technologies
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical
Machine Translation. In Proceedings of WMT12.
Steve DeNeefe and Kevin Knight. 2009 Synchronous
Tree Adjoining Machine Translation. In Proceedings
of EMNLP
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, Ignacio Thayer.
2006. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. In Proceedings of
the Annual Meeting of the Association for
Computational Linguistics
Christian Hardmeier, Joakim Nivre and Jörg Tiedemann.
2012. Tree Kernels for Machine Translation Quality
Estimation. In Proceedings of WMT12
S. Hasan, O. Bender, and H. Ney. 2006. Reranking
translation hypotheses using structural properties. In
Proceedings of the EACL&apos;06 Workshop on Learning
Structured Information in Natural Language
Applications
Hany Hassan , Khalil Sima&apos;an and Andy Way. 2007.
Supertagged Phrase-Based Statistical Machine
Translation. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based
hypothesis alignment for computing outputs from
machine translation systems. In Proceedings of
EMNLP
</reference>
<page confidence="0.983861">
437
</page>
<reference confidence="0.999558566037736">
Almut Silja Hildebrand and Stephan Vogel. 2008.
Combination of machine translation systems via
hypothesis selection from combined n-best lists. In
Proceedings of the Eighth Conference of the
Association for Machine Translation in the Americas
Aravind K. Joshi and B. Srinivas. 1994. Disambiguation
of super parts of speech (or supertags): Almost
parsing. In Proceedings of the 15th International
Conference on Computational Linguistics
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.
1975. Tree Adjunct Grammars. Journal of Computer
and System Science, 10:136–163.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney
2006. Computing consensus translation from multiple
machine translation systems using enhanced
hypotheses alignment. In Proceedings of EACL
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004 A
smorgasbord of features for statistical machine
translation. In Proceedings of the Meeting of the
North American chapter of the Association for
Computational Linguistics
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation
system combination using ITG-based alignments. In
Proceedings of ACL-HLT
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically
Informed Phrasal SMT, In Proceedings of the
Association for Computational Linguistics
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system
combination for machine translation. In Proceedings
of ACL
Yves Schabes, Anne Abeille and Aravind K. Joshi.
1988. Parsing strategies with &apos;lexicalized&apos; grammars:
Application to Tree Adjoining Grammars. In
Proceedings of the 12th International Conference on
Computational Linguistics
K.C. Sim, W.J. Byrne, M.J.F. Gales, H. Sahbi and P.C.
Woodland .2007. Consensus Network Decoding for
Statistical Machine Translation System Combination.
In Proceedings of ICASSP
Fei Xia. 1999. Extracting Tree Adjoining Grammars
from Bracketed Corpora. In Proceedings of the 5th
Natural Language Processing Pacific Rim
Symposium (NLPRS-1999)
Daguang Xu, Yuan Cao, Damianos Karakos. 2011.
Description of the JHU System Combination Scheme
for WMT 2011. In Proceedings of the Sixth
Workshop on Statistical Machine Translation
</reference>
<page confidence="0.998269">
438
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.864543">
<title confidence="0.9976475">Using a Supertagged Dependency Language Model to a Good Translation in System Combination</title>
<author confidence="0.999965">Wei-Yun Ma Kathleen McKeown</author>
<affiliation confidence="0.999913">Department of Computer Science Department of Computer Science Columbia University Columbia University</affiliation>
<address confidence="0.999637">New York, NY 10027, USA New York, NY 10027, USA</address>
<email confidence="0.999795">ma@cs.columbia.edukathy@cs.columbia.edu</email>
<abstract confidence="0.992084235294118">We present a novel, structured language model - Supertagged Dependency Language Model to model the syntactic dependencies between words. The goal is to identify ungrammatical hypotheses from a set of candidate translations in a MT system combination framework and help select the best translation candidates using a variety of sentence-level features. We use a two-step mechanism based on constituent parsing and elementary tree extraction to obtain supertags and their dependency relations. Our experiments show that the structured language model provides significant improvement in the framework of sentence-level system combination.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="2498" citStr="Bangalore and Joshi, 1999" startWordPosition="348" endWordPosition="351">he other is translation selection or reranking (Hildebrand and Vogel 2008; Callison-Burch et al., 2012), where candidate translations are generated by different decoding processes or different decoders. This paper belongs to the latter; the goal is to identify ungrammatical hypotheses from given candidate translations using grammatical knowledge in the target language that expresses syntactic dependencies between words. To achieve that, we propose a novel Structured Language Model (SLM) - Supertagged Dependency Language Model (SDLM) to model the syntactic dependencies between words. Supertag (Bangalore and Joshi, 1999) is an elementary syntactic structure based on Lexicalized Tree Adjoining Grammar (LTAG). Traditional supertagged n-gram LM predicts the next supertag based on the immediate words to the left with supertags, so it can not explicitly model long-distance dependency relations. In contrast, SDLM predicts the next supertag using the words with supertags on which it syntactically depend, and these words could be anywhere and arbitrarily far apart in a sentence. A candidate translation’s grammatical degree or “fluency” can be measured by simply calculating the SDLM likelihood of the supertagged depen</context>
<context position="8606" citStr="Bangalore and Joshi, 1999" startWordPosition="1285" endWordPosition="1288">, 1975; Schabes et al., 1988) is a formal tree rewriting formalism, which consists of a set of elementary trees, corresponding to minimal linguistic structures that localize dependencies, including long-distance dependencies, such as predicate-argument structure. Each elementary tree is associated with at least one lexical item on its frontier. The lexical item associated with an elementary tree is called the anchor in that tree; an elementary tree thus serves as a description of syntactic constraints of the anchor. The elementary syntactic structures of elementary trees are called supertags (Bangalore and Joshi, 1999), in order to distinguish them from the standard part-of-speech tags. Some examples are provided in figure 1 (b). Elementary trees are divided into initial and auxiliary trees. Initial trees are those for which all non-terminal nodes on the frontier are substitutable. Auxiliary trees are defined as initial trees, except that exactly one frontier, non-terminal node must be a foot node, with the same label as the root node. Two operations - substitution and adjunction - are provided in LTAG to combine elementary trees into a derived tree. 4 SDLM Our goal is to use SDLM to calculate the grammatic</context>
<context position="11849" citStr="Bangalore and Joshi (1999)" startWordPosition="1797" endWordPosition="1800">represent the operations of substitution and adjunction. Note that each word in a translated sentence would be assigned exactly one elementary syntactic structure which is associated with a unique supertag id for the whole corpus. Different anchor words could own the same elementary syntactic structure and would be assigned the same supertag id, such as “ α1 ” for “boys” and “dinner”. For our corpus, around 1700 different elementary syntactic structures (1700 supertag ids) are extracted. Figure 1. (a) Parse of “The hungry boys ate dinner” 435 Figure 1. (b) Extracted elementary trees 4.2 Model Bangalore and Joshi (1999) gave a concise description for dependencies between supertags: “A supertag is dependent on another supertag if the former substitutes or adjoins into the latter”. Following this description, for the example in Figure 1 (b), supertags of “the” and “hungry” are dependent on the supertag of “boys”, and supertags of “boys” and “dinner” are dependent on the supertag of “ate”. These dependencies between supertags also provide the dependencies between anchor words. Since the syntactic constraints for each word in its context are decided and described through its supertag, the likelihood of SDLM for </context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Automated extraction of TAGs from the Penn treebank.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth International Workshop on Parsing Technologies</booktitle>
<contexts>
<context position="10682" citStr="Chen and Vijay-Shanker, 2000" startWordPosition="1615" endWordPosition="1618">ability to extend to the remaining directions in the dependency list. In contrast to the LTAG parsing and supertagging-based approaches, we propose an alternative mechanism: first we use a state-of-theart constituent parser to obtain the parse of a sentence, and then we extract elementary trees with dependencies from the parse to assign each word with an elementary tree. The second step is similar to the approach used in extracting elementary trees from the TreeBank (Xia, 1999; Chen and VijayShanker, 2000). 4.1 Elementary Tree Extraction We use an elementary tree extractor, a modification of (Chen and Vijay-Shanker, 2000), to serve our purpose. Heuristic rules were used to distinguish arguments from adjuncts, and the extraction process can be regarded as a process that gradually decomposes a constituent parse to multiple elementary trees and records substitutions and adjunctions. From elementary trees, we can obtain supertags by only considering syntactic structure and ignoring anchor words. Take the sentence – “The hungry boys ate dinner” as an example; the constituent parse and extracted supertags are shown in Figure 1. In Figure 1 (b), dotted lines represent the operations of substitution and adjunction. No</context>
</contexts>
<marker>Chen, Vijay-Shanker, 2000</marker>
<rawString>John Chen and K. Vijay-Shanker. 2000. Automated extraction of TAGs from the Penn treebank. In Proceedings of the Sixth International Workshop on Parsing Technologies</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
</authors>
<title>Radu Soricut and Lucia Specia.</title>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of WMT12.</booktitle>
<contexts>
<context position="1975" citStr="Callison-Burch et al., 2012" startWordPosition="277" endWordPosition="280">rst generates multiple candidate translations followed by rescoring using global sentence-level syntactic features to select the final translation. The advantage of this methodology is that it allows for easy integration of complex syntactic features that would be too expensive to use during the decoding process. The methodology is usually applied in two scenarios: one is as part of an n-best reranking (Och et al., 2004; Hasan et al., 2006), where n-best candidate translations are generated through a decoding process. The other is translation selection or reranking (Hildebrand and Vogel 2008; Callison-Burch et al., 2012), where candidate translations are generated by different decoding processes or different decoders. This paper belongs to the latter; the goal is to identify ungrammatical hypotheses from given candidate translations using grammatical knowledge in the target language that expresses syntactic dependencies between words. To achieve that, we propose a novel Structured Language Model (SLM) - Supertagged Dependency Language Model (SDLM) to model the syntactic dependencies between words. Supertag (Bangalore and Joshi, 1999) is an elementary syntactic structure based on Lexicalized Tree Adjoining Gra</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of WMT12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous Tree Adjoining Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="1168" citStr="DeNeefe and Knight 2009" startWordPosition="158" endWordPosition="161">ns in a MT system combination framework and help select the best translation candidates using a variety of sentence-level features. We use a two-step mechanism based on constituent parsing and elementary tree extraction to obtain supertags and their dependency relations. Our experiments show that the structured language model provides significant improvement in the framework of sentence-level system combination. 1 Introduction In recent years, there has been a burgeoning interest in incorporating syntactic structure into Statistical machine translation (SMT) models (e..g, Galley et al., 2006; DeNeefe and Knight 2009; Quirk et al., 2005). In addition to modeling syntactic structure in the decoding process, a methodology for candidate translation selection has also emerged. This methodology first generates multiple candidate translations followed by rescoring using global sentence-level syntactic features to select the final translation. The advantage of this methodology is that it allows for easy integration of complex syntactic features that would be too expensive to use during the decoding process. The methodology is usually applied in two scenarios: one is as part of an n-best reranking (Och et al., 20</context>
</contexts>
<marker>DeNeefe, Knight, 2009</marker>
<rawString>Steve DeNeefe and Kevin Knight. 2009 Synchronous Tree Adjoining Machine Translation. In Proceedings of EMNLP</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable Inference and Training of ContextRich Syntactic Translation Models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics</booktitle>
<contexts>
<context position="1143" citStr="Galley et al., 2006" startWordPosition="154" endWordPosition="157"> candidate translations in a MT system combination framework and help select the best translation candidates using a variety of sentence-level features. We use a two-step mechanism based on constituent parsing and elementary tree extraction to obtain supertags and their dependency relations. Our experiments show that the structured language model provides significant improvement in the framework of sentence-level system combination. 1 Introduction In recent years, there has been a burgeoning interest in incorporating syntactic structure into Statistical machine translation (SMT) models (e..g, Galley et al., 2006; DeNeefe and Knight 2009; Quirk et al., 2005). In addition to modeling syntactic structure in the decoding process, a methodology for candidate translation selection has also emerged. This methodology first generates multiple candidate translations followed by rescoring using global sentence-level syntactic features to select the final translation. The advantage of this methodology is that it allows for easy integration of complex syntactic features that would be too expensive to use during the decoding process. The methodology is usually applied in two scenarios: one is as part of an n-best </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, Ignacio Thayer. 2006. Scalable Inference and Training of ContextRich Syntactic Translation Models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Joakim Nivre</author>
<author>Jörg Tiedemann</author>
</authors>
<title>Tree Kernels for Machine Translation Quality Estimation.</title>
<date>2012</date>
<booktitle>In Proceedings of WMT12</booktitle>
<contexts>
<context position="5869" citStr="Hardmeier et al., (2012)" startWordPosition="858" endWordPosition="861">elihood of the output of a HMM-based supertagger, and found it did not improve performance significantly. It is worth noticing that this log-likelihood is based on supertagged n-gram 1 Stanford parser (http://nlp.stanford.edu/software/lexparser.shtml). We use its PCFG version of O(n3) for SDLM training of part of Gigaword in addition to Treebank and use its factor version of O(n5) to calculate the SDLM likelihood of translations. LM, which is one type of class-based n-gram LM, so it does not model explicit syntactic dependencies between words in contrast to the work we describe in this paper. Hardmeier et al., (2012) use tree kernels over constituency and dependency parse trees for either the input or output sentences to identify constructions that are difficult to translate in the source language, and doubtful syntactic structures in the output language. The tree fragments extracted by their tree kernels are similar to our elementary trees but they only regard them as the individual inputs of support vector machine regression while binary relations of our elementary trees are considered in a formulation of a structural language model. Outside the field of candidate translation selection, Hassan et al., (</context>
</contexts>
<marker>Hardmeier, Nivre, Tiedemann, 2012</marker>
<rawString>Christian Hardmeier, Joakim Nivre and Jörg Tiedemann. 2012. Tree Kernels for Machine Translation Quality Estimation. In Proceedings of WMT12</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hasan</author>
<author>O Bender</author>
<author>H Ney</author>
</authors>
<title>Reranking translation hypotheses using structural properties.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL&apos;06 Workshop on Learning Structured Information in Natural Language Applications</booktitle>
<contexts>
<context position="1791" citStr="Hasan et al., 2006" startWordPosition="252" endWordPosition="255">irk et al., 2005). In addition to modeling syntactic structure in the decoding process, a methodology for candidate translation selection has also emerged. This methodology first generates multiple candidate translations followed by rescoring using global sentence-level syntactic features to select the final translation. The advantage of this methodology is that it allows for easy integration of complex syntactic features that would be too expensive to use during the decoding process. The methodology is usually applied in two scenarios: one is as part of an n-best reranking (Och et al., 2004; Hasan et al., 2006), where n-best candidate translations are generated through a decoding process. The other is translation selection or reranking (Hildebrand and Vogel 2008; Callison-Burch et al., 2012), where candidate translations are generated by different decoding processes or different decoders. This paper belongs to the latter; the goal is to identify ungrammatical hypotheses from given candidate translations using grammatical knowledge in the target language that expresses syntactic dependencies between words. To achieve that, we propose a novel Structured Language Model (SLM) - Supertagged Dependency La</context>
<context position="5121" citStr="Hasan et al., (2006)" startWordPosition="743" endWordPosition="746">ver a baseline sentence-level combination system of using consensus and ngram LM. 2 Related Work Och et al., (2004) investigated various syntactic feature functions to rerank the n-best candidate translations. Most features are syntactically motivated and based on alignment information between the source sentence and the target translation. The results are rather disappointing. Only the non-syntactic IBM model 1 yielded significant improvement. All other tree-based feature functions had only a very small effect on the performance. In contrast to (Och et al., 2004)’s bilingual syntax features, Hasan et al., (2006) focused on monolingual syntax features in n-best reranking. They also investigated the effect of directly using the log-likelihood of the output of a HMM-based supertagger, and found it did not improve performance significantly. It is worth noticing that this log-likelihood is based on supertagged n-gram 1 Stanford parser (http://nlp.stanford.edu/software/lexparser.shtml). We use its PCFG version of O(n3) for SDLM training of part of Gigaword in addition to Treebank and use its factor version of O(n5) to calculate the SDLM likelihood of translations. LM, which is one type of class-based n-gra</context>
</contexts>
<marker>Hasan, Bender, Ney, 2006</marker>
<rawString>S. Hasan, O. Bender, and H. Ney. 2006. Reranking translation hypotheses using structural properties. In Proceedings of the EACL&apos;06 Workshop on Learning Structured Information in Natural Language Applications</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima&apos;an</author>
<author>Andy Way</author>
</authors>
<title>Supertagged Phrase-Based Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics</booktitle>
<marker>Sima&apos;an, Way, 2007</marker>
<rawString>Hany Hassan , Khalil Sima&apos;an and Andy Way. 2007. Supertagged Phrase-Based Statistical Machine Translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-hmm-based hypothesis alignment for computing outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="7166" citStr="He et al. 2008" startWordPosition="1059" endWordPosition="1062">de of the translation model and the target n-gram LM. Two kinds of supertags are employed: those from LTAG and Combinatory Categorial Grannar (CCG), and both yield similar improvements. They found that using both or either of the supertag-based translation model and supertagged LM can achieve significant improvement. Again, the supertagged LM is a class-based n-gram LM and does not model explicit syntactic dependencies during decoding. In the field of MT system combination, wordlevel confusion network decoding is one of the most successful approaches (Matusov et al., 2006; Rosti et al., 2007; He et al. 2008; Karakos et al. 2008; Sim et al. 2007; Xu et al. 2011). It is capable of generating brand new translations but it is difficult to consider more complex syntax such as dependency LM during decoding since it adds one word at a time while a dependency based LM must parse a complete sentence. Typically, a confusion network approach selects one translation as the best and uses this as the backbone for the confusion network. The work we present here could provide a more sophisticated mechanism for selecting the backbone. Alternatively, one can enhance confusion network models by collaborating with </context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-hmm-based hypothesis alignment for computing outputs from machine translation systems. In Proceedings of EMNLP</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Combination of machine translation systems via hypothesis selection from combined n-best lists.</title>
<date>2008</date>
<booktitle>In Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas</booktitle>
<contexts>
<context position="1945" citStr="Hildebrand and Vogel 2008" startWordPosition="273" endWordPosition="276">merged. This methodology first generates multiple candidate translations followed by rescoring using global sentence-level syntactic features to select the final translation. The advantage of this methodology is that it allows for easy integration of complex syntactic features that would be too expensive to use during the decoding process. The methodology is usually applied in two scenarios: one is as part of an n-best reranking (Och et al., 2004; Hasan et al., 2006), where n-best candidate translations are generated through a decoding process. The other is translation selection or reranking (Hildebrand and Vogel 2008; Callison-Burch et al., 2012), where candidate translations are generated by different decoding processes or different decoders. This paper belongs to the latter; the goal is to identify ungrammatical hypotheses from given candidate translations using grammatical knowledge in the target language that expresses syntactic dependencies between words. To achieve that, we propose a novel Structured Language Model (SLM) - Supertagged Dependency Language Model (SDLM) to model the syntactic dependencies between words. Supertag (Bangalore and Joshi, 1999) is an elementary syntactic structure based on </context>
</contexts>
<marker>Hildebrand, Vogel, 2008</marker>
<rawString>Almut Silja Hildebrand and Stephan Vogel. 2008. Combination of machine translation systems via hypothesis selection from combined n-best lists. In Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>B Srinivas</author>
</authors>
<title>Disambiguation of super parts of speech (or supertags): Almost parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics</booktitle>
<contexts>
<context position="9622" citStr="Joshi and Srinivas 1994" startWordPosition="1450" endWordPosition="1453">the same label as the root node. Two operations - substitution and adjunction - are provided in LTAG to combine elementary trees into a derived tree. 4 SDLM Our goal is to use SDLM to calculate the grammaticality of translated sentences. We do this by calculating the likelihood of the supertagged dependency structure that spans the entire sentence using SDLM. To obtain the supertagged dependency linkage, the most intuitive way is through a LTAG parser (Schabes et al., 1988). However, this could be very slow as it has time complexity of O(n6). Another possibility is to follow the procedure in (Joshi and Srinivas 1994, Bangalore and Joshi, 1999): use a HMM-based supertagger to assign words with supertags, followed by derivation of a shallow parse in linear time based on only the supertags to obtain the dependencies. But since this approach uses only the local context, in (Joshi and Srinivas 1994), they also proposed another greedy algorithm based on supertagged dependency probabilities to gradually select the path with the maximum path probability to extend to the remaining directions in the dependency list. In contrast to the LTAG parsing and supertagging-based approaches, we propose an alternative mechan</context>
</contexts>
<marker>Joshi, Srinivas, 1994</marker>
<rawString>Aravind K. Joshi and B. Srinivas. 1994. Disambiguation of super parts of speech (or supertags): Almost parsing. In Proceedings of the 15th International Conference on Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Leon S Levy</author>
<author>Masako Takahashi</author>
</authors>
<title>Tree Adjunct Grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Science,</journal>
<pages>10--136</pages>
<contexts>
<context position="7986" citStr="Joshi et al., 1975" startWordPosition="1195" endWordPosition="1198">e it adds one word at a time while a dependency based LM must parse a complete sentence. Typically, a confusion network approach selects one translation as the best and uses this as the backbone for the confusion network. The work we present here could provide a more sophisticated mechanism for selecting the backbone. Alternatively, one can enhance confusion network models by collaborating with a sentence-level combination model which uses complex syntax to re-rank n-best outputs of a confusion network model. This kind of collaboration is one of our future works. 434 3 LTAG and Supertag LTAG (Joshi et al., 1975; Schabes et al., 1988) is a formal tree rewriting formalism, which consists of a set of elementary trees, corresponding to minimal linguistic structures that localize dependencies, including long-distance dependencies, such as predicate-argument structure. Each elementary tree is associated with at least one lexical item on its frontier. The lexical item associated with an elementary tree is called the anchor in that tree; an elementary tree thus serves as a description of syntactic constraints of the anchor. The elementary syntactic structures of elementary trees are called supertags (Bangal</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Aravind K. Joshi, Leon S. Levy, and Masako Takahashi. 1975. Tree Adjunct Grammars. Journal of Computer and System Science, 10:136–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="7130" citStr="Matusov et al., 2006" startWordPosition="1051" endWordPosition="1054">at integrates supertags into the target side of the translation model and the target n-gram LM. Two kinds of supertags are employed: those from LTAG and Combinatory Categorial Grannar (CCG), and both yield similar improvements. They found that using both or either of the supertag-based translation model and supertagged LM can achieve significant improvement. Again, the supertagged LM is a class-based n-gram LM and does not model explicit syntactic dependencies during decoding. In the field of MT system combination, wordlevel confusion network decoding is one of the most successful approaches (Matusov et al., 2006; Rosti et al., 2007; He et al. 2008; Karakos et al. 2008; Sim et al. 2007; Xu et al. 2011). It is capable of generating brand new translations but it is difficult to consider more complex syntax such as dependency LM during decoding since it adds one word at a time while a dependency based LM must parse a complete sentence. Typically, a confusion network approach selects one translation as the best and uses this as the backbone for the confusion network. The work we present here could provide a more sophisticated mechanism for selecting the backbone. Alternatively, one can enhance confusion n</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing, and Hermann Ney 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Proceedings of EACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In Proceedings of the Meeting of the North American chapter of the Association for Computational Linguistics</booktitle>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004 A smorgasbord of features for statistical machine translation. In Proceedings of the Meeting of the North American chapter of the Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damianos Karakos</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
<author>Markus Dreyer</author>
</authors>
<title>Machine translation system combination using ITG-based alignments.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT</booktitle>
<contexts>
<context position="7187" citStr="Karakos et al. 2008" startWordPosition="1063" endWordPosition="1066">ation model and the target n-gram LM. Two kinds of supertags are employed: those from LTAG and Combinatory Categorial Grannar (CCG), and both yield similar improvements. They found that using both or either of the supertag-based translation model and supertagged LM can achieve significant improvement. Again, the supertagged LM is a class-based n-gram LM and does not model explicit syntactic dependencies during decoding. In the field of MT system combination, wordlevel confusion network decoding is one of the most successful approaches (Matusov et al., 2006; Rosti et al., 2007; He et al. 2008; Karakos et al. 2008; Sim et al. 2007; Xu et al. 2011). It is capable of generating brand new translations but it is difficult to consider more complex syntax such as dependency LM during decoding since it adds one word at a time while a dependency based LM must parse a complete sentence. Typically, a confusion network approach selects one translation as the best and uses this as the backbone for the confusion network. The work we present here could provide a more sophisticated mechanism for selecting the backbone. Alternatively, one can enhance confusion network models by collaborating with a sentence-level comb</context>
</contexts>
<marker>Karakos, Eisner, Khudanpur, Dreyer, 2008</marker>
<rawString>Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, and Markus Dreyer. 2008. Machine translation system combination using ITG-based alignments. In Proceedings of ACL-HLT</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT,</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics</booktitle>
<contexts>
<context position="1189" citStr="Quirk et al., 2005" startWordPosition="162" endWordPosition="165">tion framework and help select the best translation candidates using a variety of sentence-level features. We use a two-step mechanism based on constituent parsing and elementary tree extraction to obtain supertags and their dependency relations. Our experiments show that the structured language model provides significant improvement in the framework of sentence-level system combination. 1 Introduction In recent years, there has been a burgeoning interest in incorporating syntactic structure into Statistical machine translation (SMT) models (e..g, Galley et al., 2006; DeNeefe and Knight 2009; Quirk et al., 2005). In addition to modeling syntactic structure in the decoding process, a methodology for candidate translation selection has also emerged. This methodology first generates multiple candidate translations followed by rescoring using global sentence-level syntactic features to select the final translation. The advantage of this methodology is that it allows for easy integration of complex syntactic features that would be too expensive to use during the decoding process. The methodology is usually applied in two scenarios: one is as part of an n-best reranking (Och et al., 2004; Hasan et al., 200</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT, In Proceedings of the Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="7150" citStr="Rosti et al., 2007" startWordPosition="1055" endWordPosition="1058">s into the target side of the translation model and the target n-gram LM. Two kinds of supertags are employed: those from LTAG and Combinatory Categorial Grannar (CCG), and both yield similar improvements. They found that using both or either of the supertag-based translation model and supertagged LM can achieve significant improvement. Again, the supertagged LM is a class-based n-gram LM and does not model explicit syntactic dependencies during decoding. In the field of MT system combination, wordlevel confusion network decoding is one of the most successful approaches (Matusov et al., 2006; Rosti et al., 2007; He et al. 2008; Karakos et al. 2008; Sim et al. 2007; Xu et al. 2011). It is capable of generating brand new translations but it is difficult to consider more complex syntax such as dependency LM during decoding since it adds one word at a time while a dependency based LM must parse a complete sentence. Typically, a confusion network approach selects one translation as the best and uses this as the backbone for the confusion network. The work we present here could provide a more sophisticated mechanism for selecting the backbone. Alternatively, one can enhance confusion network models by col</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard Schwartz. 2007. Improved word-level system combination for machine translation. In Proceedings of ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Anne Abeille</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing strategies with &apos;lexicalized&apos; grammars: Application to Tree Adjoining Grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics</booktitle>
<contexts>
<context position="3264" citStr="Schabes et al., 1988" startWordPosition="462" endWordPosition="465">ertag based on the immediate words to the left with supertags, so it can not explicitly model long-distance dependency relations. In contrast, SDLM predicts the next supertag using the words with supertags on which it syntactically depend, and these words could be anywhere and arbitrarily far apart in a sentence. A candidate translation’s grammatical degree or “fluency” can be measured by simply calculating the SDLM likelihood of the supertagged dependency structure that spans the entire sentence. To obtain the supertagged dependency structure, the most intuitive way is through a LTAG parser (Schabes et al., 1988). However, this could be very 433 Proceedings of NAACL-HLT 2013, pages 433–438, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics slow as it has time complexity of O(n6). Instead we propose an alternative mechanism in this paper: first we use a constituent parser1 of O(n3) ~ O(n5) to obtain the parse of a sentence, and then we extract elementary trees with dependencies from the parse in linear time. Aside from the consideration of time complexity, another motivation of this two-step mechanism is that compared with LTAG parsing, the mechanism is more flexible fo</context>
<context position="8009" citStr="Schabes et al., 1988" startWordPosition="1199" endWordPosition="1202">t a time while a dependency based LM must parse a complete sentence. Typically, a confusion network approach selects one translation as the best and uses this as the backbone for the confusion network. The work we present here could provide a more sophisticated mechanism for selecting the backbone. Alternatively, one can enhance confusion network models by collaborating with a sentence-level combination model which uses complex syntax to re-rank n-best outputs of a confusion network model. This kind of collaboration is one of our future works. 434 3 LTAG and Supertag LTAG (Joshi et al., 1975; Schabes et al., 1988) is a formal tree rewriting formalism, which consists of a set of elementary trees, corresponding to minimal linguistic structures that localize dependencies, including long-distance dependencies, such as predicate-argument structure. Each elementary tree is associated with at least one lexical item on its frontier. The lexical item associated with an elementary tree is called the anchor in that tree; an elementary tree thus serves as a description of syntactic constraints of the anchor. The elementary syntactic structures of elementary trees are called supertags (Bangalore and Joshi, 1999), i</context>
<context position="9477" citStr="Schabes et al., 1988" startWordPosition="1425" endWordPosition="1428">are substitutable. Auxiliary trees are defined as initial trees, except that exactly one frontier, non-terminal node must be a foot node, with the same label as the root node. Two operations - substitution and adjunction - are provided in LTAG to combine elementary trees into a derived tree. 4 SDLM Our goal is to use SDLM to calculate the grammaticality of translated sentences. We do this by calculating the likelihood of the supertagged dependency structure that spans the entire sentence using SDLM. To obtain the supertagged dependency linkage, the most intuitive way is through a LTAG parser (Schabes et al., 1988). However, this could be very slow as it has time complexity of O(n6). Another possibility is to follow the procedure in (Joshi and Srinivas 1994, Bangalore and Joshi, 1999): use a HMM-based supertagger to assign words with supertags, followed by derivation of a shallow parse in linear time based on only the supertags to obtain the dependencies. But since this approach uses only the local context, in (Joshi and Srinivas 1994), they also proposed another greedy algorithm based on supertagged dependency probabilities to gradually select the path with the maximum path probability to extend to the</context>
</contexts>
<marker>Schabes, Abeille, Joshi, 1988</marker>
<rawString>Yves Schabes, Anne Abeille and Aravind K. Joshi. 1988. Parsing strategies with &apos;lexicalized&apos; grammars: Application to Tree Adjoining Grammars. In Proceedings of the 12th International Conference on Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Sim</author>
<author>W J Byrne</author>
<author>M J F Gales</author>
<author>H Sahbi</author>
<author>P C Woodland</author>
</authors>
<title>Consensus Network Decoding for Statistical Machine Translation System Combination.</title>
<date>2007</date>
<booktitle>In Proceedings of ICASSP</booktitle>
<contexts>
<context position="7204" citStr="Sim et al. 2007" startWordPosition="1067" endWordPosition="1070">arget n-gram LM. Two kinds of supertags are employed: those from LTAG and Combinatory Categorial Grannar (CCG), and both yield similar improvements. They found that using both or either of the supertag-based translation model and supertagged LM can achieve significant improvement. Again, the supertagged LM is a class-based n-gram LM and does not model explicit syntactic dependencies during decoding. In the field of MT system combination, wordlevel confusion network decoding is one of the most successful approaches (Matusov et al., 2006; Rosti et al., 2007; He et al. 2008; Karakos et al. 2008; Sim et al. 2007; Xu et al. 2011). It is capable of generating brand new translations but it is difficult to consider more complex syntax such as dependency LM during decoding since it adds one word at a time while a dependency based LM must parse a complete sentence. Typically, a confusion network approach selects one translation as the best and uses this as the backbone for the confusion network. The work we present here could provide a more sophisticated mechanism for selecting the backbone. Alternatively, one can enhance confusion network models by collaborating with a sentence-level combination model whi</context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>K.C. Sim, W.J. Byrne, M.J.F. Gales, H. Sahbi and P.C. Woodland .2007. Consensus Network Decoding for Statistical Machine Translation System Combination. In Proceedings of ICASSP</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Extracting Tree Adjoining Grammars from Bracketed Corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium (NLPRS-1999)</booktitle>
<contexts>
<context position="10534" citStr="Xia, 1999" startWordPosition="1595" endWordPosition="1596">d another greedy algorithm based on supertagged dependency probabilities to gradually select the path with the maximum path probability to extend to the remaining directions in the dependency list. In contrast to the LTAG parsing and supertagging-based approaches, we propose an alternative mechanism: first we use a state-of-theart constituent parser to obtain the parse of a sentence, and then we extract elementary trees with dependencies from the parse to assign each word with an elementary tree. The second step is similar to the approach used in extracting elementary trees from the TreeBank (Xia, 1999; Chen and VijayShanker, 2000). 4.1 Elementary Tree Extraction We use an elementary tree extractor, a modification of (Chen and Vijay-Shanker, 2000), to serve our purpose. Heuristic rules were used to distinguish arguments from adjuncts, and the extraction process can be regarded as a process that gradually decomposes a constituent parse to multiple elementary trees and records substitutions and adjunctions. From elementary trees, we can obtain supertags by only considering syntactic structure and ignoring anchor words. Take the sentence – “The hungry boys ate dinner” as an example; the consti</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fei Xia. 1999. Extracting Tree Adjoining Grammars from Bracketed Corpora. In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium (NLPRS-1999)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daguang Xu</author>
<author>Yuan Cao</author>
<author>Damianos Karakos</author>
</authors>
<title>Description of the JHU System Combination Scheme for WMT</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation</booktitle>
<contexts>
<context position="7221" citStr="Xu et al. 2011" startWordPosition="1071" endWordPosition="1074">Two kinds of supertags are employed: those from LTAG and Combinatory Categorial Grannar (CCG), and both yield similar improvements. They found that using both or either of the supertag-based translation model and supertagged LM can achieve significant improvement. Again, the supertagged LM is a class-based n-gram LM and does not model explicit syntactic dependencies during decoding. In the field of MT system combination, wordlevel confusion network decoding is one of the most successful approaches (Matusov et al., 2006; Rosti et al., 2007; He et al. 2008; Karakos et al. 2008; Sim et al. 2007; Xu et al. 2011). It is capable of generating brand new translations but it is difficult to consider more complex syntax such as dependency LM during decoding since it adds one word at a time while a dependency based LM must parse a complete sentence. Typically, a confusion network approach selects one translation as the best and uses this as the backbone for the confusion network. The work we present here could provide a more sophisticated mechanism for selecting the backbone. Alternatively, one can enhance confusion network models by collaborating with a sentence-level combination model which uses complex s</context>
</contexts>
<marker>Xu, Cao, Karakos, 2011</marker>
<rawString>Daguang Xu, Yuan Cao, Damianos Karakos. 2011. Description of the JHU System Combination Scheme for WMT 2011. In Proceedings of the Sixth Workshop on Statistical Machine Translation</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>