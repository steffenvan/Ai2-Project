<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000364">
<title confidence="0.994739">
Grounded Unsupervised Semantic Parsing
</title>
<author confidence="0.814655">
Hoifung Poon
</author>
<affiliation confidence="0.6528075">
One Microsoft Way
Microsoft Research
</affiliation>
<address confidence="0.935029">
Redmond, WA 98052, USA
</address>
<email confidence="0.998489">
hoifung@microsoft.com
</email>
<sectionHeader confidence="0.994801" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879588235294">
We present the first unsupervised ap-
proach for semantic parsing that rivals
the accuracy of supervised approaches
in translating natural-language questions
to database queries. Our GUSP system
produces a semantic parse by annotat-
ing the dependency-tree nodes and edges
with latent states, and learns a proba-
bilistic grammar using EM. To compen-
sate for the lack of example annotations
or question-answer pairs, GUSP adopts
a novel grounded-learning approach to
leverage database for indirect supervision.
On the challenging ATIS dataset, GUSP
attained an accuracy of 84%, effectively
tying with the best published results by su-
pervised approaches.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999876948275863">
Semantic parsing maps text to a formal mean-
ing representation such as logical forms or struc-
tured queries. Recently, there has been a bur-
geoning interest in developing machine-learning
approaches for semantic parsing (Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Mooney, 2007; Kwiatkowski et al., 2011), but
the predominant paradigm uses supervised learn-
ing, which requires example annotations that are
costly to obtain. More recently, several grounded-
learning approaches have been proposed to alle-
viate the annotation burden (Chen and Mooney,
2008; Kim and Mooney, 2010; B¨orschinger et al.,
2011; Clarke et al., 2010; Liang et al., 2011). In
particular, Clarke et al. (2010) and Liang et al.
(2011) proposed methods to learn from question-
answer pairs alone, which represents a significant
advance. However, although these methods exon-
erate annotators from mastering specialized logi-
cal forms, finding the answers for complex ques-
tions still requires non-trivial effort. 1
Poon &amp; Domingos (2009, 2010) proposed the
USP system for unsupervised semantic parsing,
which learns a parser by recursively clustering
and composing synonymous expressions. While
their approach completely obviates the need for di-
rect supervision, their target logic forms are self-
induced clusters, which do not align with existing
database or ontology. As a result, USP can not be
used directly to answer complex questions against
an existing database. More importantly, it misses
the opportunity to leverage database for indirect
supervision.
In this paper, we present the GUSP system,
which combines unsupervised semantic parsing
with grounded learning from a database. GUSP
starts with the dependency tree of a sentence and
produces a semantic parse by annotating the nodes
and edges with latent semantic states derived from
the database. Given a set of natural-language
questions and a database, GUSP learns a prob-
abilistic semantic grammar using EM. To com-
pensate for the lack of direct supervision, GUSP
constrains the search space using the database
schema, and bootstraps learning using lexical
scores computed from the names and values of
database elements.
Unlike previous grounded-learning approaches,
GUSP does not require ambiguous annotations
or oracle answers, but rather focuses on lever-
aging database contents that are readily avail-
able. Unlike USP, GUSP predetermines the tar-
get logical forms based on the database schema,
which alleviates the difficulty in learning and en-
sures that the output semantic parses can be di-
rectly used in querying the database. To handle
syntax-semantics mismatch, GUSP introduces a
novel dependency-based meaning representation
</bodyText>
<footnote confidence="0.995819666666667">
1Clarke et al. (2010) and Liang et al. (2011) used the
annotated logical forms to compute answers for their experi-
ments.
</footnote>
<page confidence="0.951156">
933
</page>
<note confidence="0.9149595">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 933–943,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999721526315789">
by augmenting the state space to represent seman-
tic relations beyond immediate dependency neigh-
borhood. This representation also factorizes over
nodes and edges, enabling linear-time exact infer-
ence in GUSP.
We evaluated GUSP on end-to-end question
answering using the ATIS dataset for semantic
parsing (Zettlemoyer and Collins, 2007). Com-
pared to other standard datasets such as GEO and
JOBS, ATIS features a database that is an order
of magnitude larger in the numbers of relations
and instances, as well as a more irregular lan-
guage (ATIS questions were derived from spo-
ken dialogs). Despite these challenges, GUSP
attains an accuracy of 84% in end-to-end ques-
tion answering, effectively tying with the state-
of-the-art supervised approaches (85% by Zettle-
moyer &amp; Collins (2007), 83% by Kwiatkowski et
al. (2011)).
</bodyText>
<sectionHeader confidence="0.997926" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.996331">
2.1 Semantic Parsing
</subsectionHeader>
<bodyText confidence="0.999985035714286">
The goal of semantic parsing is to map text to
a complete and detailed meaning representation
(Mooney, 2007). This is in contrast with semantic
role labeling (Carreras and Marquez, 2004) and in-
formation extraction (Banko et al., 2007; Poon and
Domingos, 2007), which have a more restricted
goal of identifying local semantic roles or extract-
ing selected information slots.
The standard language for meaning representa-
tion is first-order logic or a sublanguage, such as
FunQL (Kate et al., 2005; Clarke et al., 2010) and
lambda calculus (Zettlemoyer and Collins, 2005;
Zettlemoyer and Collins, 2007). Poon &amp; Domin-
gos (2009, 2010) induce a meaning representa-
tion by clustering synonymous lambda-calculus
forms stemming from partitions of dependency
trees. More recently, Liang et al. (2011) proposed
DCS for dependency-based compositional seman-
tics, which represents a semantic parse as a tree
with nodes representing database elements and op-
erations, and edges representing relational joins.
In this paper, we focus on semantic parsing
for natural-language interface to database (Grosz
et al., 1987). In this problem setting, a natural-
language question is first translated into a mean-
ing representation by semantic parsing, and then
converted into a structured query such as SQL to
obtain answer from the database.
</bodyText>
<subsectionHeader confidence="0.999163">
2.2 Unsupervised Semantic Parsing
</subsectionHeader>
<bodyText confidence="0.999921022727273">
Unsupervised semantic parsing was first proposed
by Poon &amp; Domingos (2009, 2010) with their
USP system. USP defines a probabilistic model
over the dependency tree and semantic parse us-
ing Markov logic (Domingos and Lowd, 2009),
and recursively clusters and composes synony-
mous dependency treelets using a hard EM-like
procedure. Since USP uses nonlocal features (e.g.,
the argument-number feature) and operates over
partitions, exact inference is intractable, and USP
resorts to a greedy approach to find the MAP parse
by searching over partitions. Titov &amp; Klementiev
(2011) proposed a Bayesian version of USP and
Titov &amp; Klementiev (2012) adapted it for seman-
tic role induction. In USP, the meaning is repre-
sented by self-induced clusters. Therefore, to an-
swer complex questions against a database, it re-
quires an additional ontology matching step to re-
solve USP clusters with database elements.
Popescu et al. (2003, 2004) proposed the PRE-
CISE system, which does not require labeled ex-
amples and can be directly applied to question
answering with a database. The PRECISE sys-
tem, however, requires substantial amount of engi-
neering, including a domain-specific lexicon that
specifies the synonyms for names and values of
database elements, a restricted set of potential in-
terpretations for domain verbs and prepositions, as
well as a set of domain questions with manually la-
beled POS tags for retraining the tagger and parser.
It also focuses on the subset of easy questions (“se-
mantically tractable” questions), and sidesteps the
problem of dealing with complex and nested struc-
tures, as well as ambiguous interpretations. Re-
markably, while PRECISE can be very accurate
on easy questions, it does not try to learn from
these interpretations. In contrast, Goldwasser et
al. (2011) proposed a self-supervised approach,
which iteratively chose high-confidence parses to
retrain the parser. Their system, however, still
required a lexicon manually constructed for the
given domain. Moreover, it was only applied to
a small domain (a subset of GEO), and the result
still trailed supervised systems by a wide margin.
</bodyText>
<subsectionHeader confidence="0.998558">
2.3 Grounded Learning for Semantic Parsing
</subsectionHeader>
<bodyText confidence="0.996893">
Grounded learning is motivated by alleviating the
burden of direct supervision via interaction with
the world, where the indirect supervision may
take the form as ambiguous annotations (Chen
</bodyText>
<page confidence="0.995856">
934
</page>
<figure confidence="0.839032">
E:flight:R
</figure>
<figureCaption confidence="0.992731">
Figure 1: End-to-end question answering by
</figureCaption>
<bodyText confidence="0.997294878787879">
GUSP for sentence get flight from toronto to san
diego stopping in dtw. Top: the dependency tree
of the sentence is annotated with latent semantic
states by GUSP. For brevity, we omit the edge
states. Raising occurs from flight to get and sink-
ing occurs from get to diego. Bottom: the seman-
tic tree is deterministically converted into SQL to
obtain answer from the database.
and Mooney, 2008; Kim and Mooney, 2010;
B¨orschinger et al., 2011) or example question-
answer pairs (Clarke et al., 2010; Liang et al.,
2011). In general, however, such supervision is
not always available or easy to obtain. In con-
trast, databases are often abundantly available, es-
pecially for important domains.
The database community has considerable
amount of work on leveraging databases in various
tasks such as entity resolution, schema matching,
and others. To the best of our knowledge, this ap-
proach is still underexplored in the NLP commu-
nity. One notable exception is distant supervision
(Mintz et al., 2009; Riedel et al., 2010; Hoffmann
et al., 2011; Krishnamurthy and Mitchell, 2012;
Heck et al., 2013), which used database instances
to derive training examples for relation extraction.
This approach, however, still has considerable lim-
itations. For example, it only handles binary rela-
tions, and the quality of the training examples is
inherently noisy and hard to control. Moreover,
this approach is not applicable to the question-
answering setting considered in this paper, since
entity pairs in questions need not correspond to
valid relational instances in the database.
</bodyText>
<sectionHeader confidence="0.936054" genericHeader="method">
3 Grounded Unsupervised Semantic
</sectionHeader>
<subsectionHeader confidence="0.534354">
Parsing
</subsectionHeader>
<bodyText confidence="0.999725571428572">
In this section, we present the GUSP system for
grounded unsupervised semantic parsing. GUSP
is unsupervised and does not require example log-
ical forms or question-answer pairs. Figure 1
shows an example of end-to-end question answer-
ing using GUSP. GUSP produces a semantic parse
of the question by annotating its dependency tree
with latent semantic states. The semantic tree
can then be deterministically converted into SQL
to obtain answer from the database. Given a
set of natural-language questions and a database,
GUSP learns a probabilistic semantic grammar us-
ing EM.
To compensate for the lack of annotated ex-
amples, GUSP derives indirect supervision from
a novel combination of three key sources. First,
GUSP leverages the target database to constrain
the search space. Specifically, it defines the se-
mantic states based on the database schema, and
derives lexical-trigger scores from database ele-
ments to bootstrap learning.
Second, in contrast to most existing approaches
for semantic parsing, GUSP starts directly from
dependency trees and focuses on translating them
into semantic parses. While syntax may not al-
ways align perfectly with semantics, it is still
highly informative about the latter. In particular,
dependency edges are often indicative of semantic
relations. On the other hand, syntax and semantic
often diverge, and synactic parsing errors abound.
To combat this problem, GUSP introduces a novel
dependency-based meaning representation with an
augmented state space to account for semantic re-
lations that are nonlocal in the dependency tree.
GUSP’s approach of starting directly from de-
pendency tree is inspired by USP. However, GUSP
uses a different meaning representation defined
over individual nodes and edges, rather than par-
titions, which enables linear-time exact inference.
GUSP also handles complex linguistic phenomena
and syntax-semantics mismatch by explicitly aug-
menting the state space, whereas USP’s capability
in handling such phenomena is indirect and more
limited.
GUSP represents meaning by a semantic tree,
which is similar to DCS (Liang et al., 2011). Their
approach to semantic parsing, however, differs
from GUSP in that it induced the semantic tree di-
rectly from a sentence, rather than starting from
</bodyText>
<figure confidence="0.990529923076923">
get
flight from to
E:flight
V:city.name:C
in
dtw
V:airport.code
diego
toronto
V:city.name + E:flight
V:city.name
E:flight_stop
san stopping
</figure>
<page confidence="0.992993">
935
</page>
<bodyText confidence="0.999990848484848">
a dependency tree and annotating it. Their ap-
proach alleviates some complexity in the mean-
ing representation for handling syntax-semantics
mismatch, but it has to search over a much larger
search space involving exponentially many candi-
date trees. This might partially explain why it has
not yet been scaled up to the ATIS dataset.
Finally, GUSP recognizes that certain aspects
in semantic parsing may not be worth learn-
ing using precious annotated examples. These
are domain-independent and closed-class expres-
sions, such as times and dates (e.g., before 5pm
and July seventeenth), logical connectives (e.g.,
and, or, not), and numerics (e.g., 200 dol-
lars). GUSP preprocesses the text to detect such
expressions and restricts their interpretation to
database elements of compatible types (e.g., be-
fore 5pm vs. flight.departure time or
flight.arrival time). Short of training ex-
amples, GUSP also resolves quantifier scoping
ambiguities deterministically by a fixed ordering.
For example, in the phrase cheapest flight to Seat-
tle, the scope of cheapest can be either flight or
flight to seattle. GUSP always chooses to apply
the superlative at last, amounting to choosing the
most restricted scope (flight to seattle), which is
usually the correct interpretation.
In the remainder of this section, we first formal-
ize the problem setting and introduce the GUSP
meaning representation. We then present the
GUSP model and learning and inference algo-
rithms. Finally, we describe how to convert a
GUSP semantic parse into SQL.
</bodyText>
<subsectionHeader confidence="0.999334">
3.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.999983615384615">
Let d be a dependency tree, N(d) and E(d) be
its nodes and edges. In GUSP, a semantic parse
of d is an assignment z : N(d) U E(d) → S
that maps its nodes and edges to semantic states
in S. For example, in the example in Figure 1,
z(flight) = E : flight. At the core of GUSP
is a joint probability distribution PB(d, z) over the
dependency tree and the semantic parse. Seman-
tic parsing in GUSP amounts to finding the most
probable parse z* = arg maxz PB(d, z). Given
a set of sentences and their dependency trees D,
learning in GUSP maximizes the log-likelihood of
D while summing out the latent parses z:
</bodyText>
<equation confidence="0.883605333333333">
θ* = arg max log PB(D)
�= arg max
d∈D
</equation>
<subsectionHeader confidence="0.999772">
3.2 Simple Semantic States
</subsectionHeader>
<bodyText confidence="0.9884846875">
Node states GUSP creates a state E:X (E short
for entity) for each database entity X (i.e., a
database table), a state P:Y (P short for prop-
erty) and V:Y (V short for value) for each database
attribute Y (i.e., a database column). Node
states are assigned to dependency nodes. Intu-
itively, they represent database entities, proper-
ties, and values. For example, the ATIS do-
main contains entities such as flight and fare,
which may contain properties such as the depar-
ture time flight.departure time or ticket
price fare.one direction cost. The men-
tions of entities and properties are represented
by entity and property states, whereas constants
such as 9:25am or 120 dollars are repre-
sented by value states. In the semantic parse in
Figure 1, for example, flight is assigned to en-
tity state E:flight, where toronto is assigned
to value state V:city.name. There is a special
node state NULL, which signifies that the subtree
headed by the word contributes no meaning to the
semantic parse (e.g., an auxilliary verb).
Edge states GUSP creates an edge state for
each valid relational join paths connecting two
node states. Edge states are assigned to de-
pendency edges. GUSP enforces the constraints
that the node states of the dependency par-
ent and child must agree with the node states
in the edge state. For example, E:flight-
-V:flight.departure time represents a
natural join between the flight entity and the prop-
erty value departure time. For a dependency edge
e : a → b, the assignment to E:flight-
-V:flight.departure time signifies that
a represents a flight entity, and b represents the
value of its departure time. An edge state may
also represent a relational path consisting of a
serial of joins. For example, Zettlemoyer and
Collins (2007) used a predicate from(f,c) to
signify that flight f starts from city c. In the ATIS
database, however, this amounts to a path of three
joins:
flight.from airport-airport
airport-airport service
airport service-city
In GUSP, this is represented by the edge
state flight-flight.from airport-
-airport-airport service-city.
</bodyText>
<table confidence="0.316529333333333">
� PB(d, z)
log
z
</table>
<page confidence="0.991561">
936
</page>
<bodyText confidence="0.999853642857143">
GUSP only creates edge states for relational join
paths up to length four, as longer paths rarely
correspond to meaningful semantic relations.
Composition To handle compositions such as
American Airlines and New York City, it helps
to distinguish the head words (Airlines and City)
from the rest. In GUSP, this is handled by intro-
ducing, for each node state such as E:airline,
a new node state such as E:airline:C, where
C signifies composition. For example, in Figure
1, diego is assigned to V:city.name, whereas
san is assigned to V:city.name:C, since san
diego forms a single meaning unit, and should be
translated into SQL as a whole.
</bodyText>
<subsectionHeader confidence="0.977326">
3.3 Domain-Independent States
</subsectionHeader>
<bodyText confidence="0.999738863636364">
These are for handling special linguistic phenom-
ena that are not domain-specific, such as negation,
superlatives, and quantifiers.
Operator states GUSP create node states for
the logical and comparison operators (OR, AND,
NOT, MORE, LESS, EQ). Additionally, to han-
dle the cases when prepositions and logical
connectives are collapsed into the label of a
dependency edge, as in Stanford dependency,
GUSP introduces an edge state for each triple
of an operator and two node states, such as
E:flight-AND-E:fare.
Quantifier states GUSP creates a node state for
each of the standard SQL functions: argmin,
argmax, count, sum. Additionally, it cre-
ates a node state for each pair of compatible func-
tion and property. For example, argmin can
be applied to any numeric property, in particular
flight.departure time, and so the node
state P:flight.departure time:argmin
is created and can be assigned to superlatives such
as earliest.
</bodyText>
<subsectionHeader confidence="0.993297">
3.4 Complex Semantic States
</subsectionHeader>
<bodyText confidence="0.999643916666667">
For sentences with a correct dependency tree and
well-aligned syntax and semantics, the simple se-
mantic states suffice for annotating the correct se-
mantic parse. However, in complex sentences,
syntax and semantic often diverge, either due to
their differing goals or simply stemming from syn-
tactic parsing errors. In Figure 1, the dependency
tree contains multiple errors: from toronto and to
san diego are mistakenly attached to get, which
has no literal meaning here; stopping in dtw is also
wrongly attached to diego rather than flight. An-
notating such a tree with only simple states will
lead to incorrect semantic parses, e.g., by joining
V:city:san diego with V:airport:dtw
via E:airport service, rather than join-
ing E:flight with V:airport:dtw via
E:flight stop.
To overcome these challenges, GUSP intro-
duces three types of complex states to handle
syntax-semantics divergence. Figure 1 shows the
correct semantic parse for the above sentence us-
ing the complex states.
Raising For each simple node state N, GUSP
creates a “raised” state N:R (R short for raised). A
raised state signifies a word that has little or none
of its own meaning, but effectively takes one of its
child states to be its own (“raises”). Correspond-
ingly, GUSP creates a “raising” edge state N-R-N,
which signifies that the parent is a raised state and
its meaning is derived from the dependency child
of state N. For all other children, the parent be-
haves just as state N. For example, in Figure 1, get
is assigned to the raised state E:flight:R, and
the edge between get and flight is assigned to the
raising edge state E:flight-R-E:flight.
Sinking For simple node states A, B and an
edge state E connecting the two, GUSP creates
a “sinking” node state A+E+B:S (S for sinking).
When a node n is assigned to such a sinking state,
n can behave as either A or B for its children
(i.e., the edge states can connect to either one),
and n’s parent must be of state B. In Figure 1,
for example, diego is assigned to a sinking state
V:city.name + E:flight (the edge state is
omitted for brevity). E:flight comes from its
parent get. For child san, diego behaves as in state
V:city.name, and their edge state is a simple
compositional join. For the other child stopping,
diego behaves as in state E:flight, and their
edge state is a relational join connecting flight
with flight stop. Effectively, this connects
stopping with get and eventually with flight (due to
raising), virtually correcting the syntax-semantics
mismatch stemming from attachment errors.
Implicit For simple node states A, B and an
edge state E connecting the two, GUSP also cre-
ates a node state A+E+B:I (I for implicit) with
the “implicit” state B. In natural languages, an en-
tity is often introduced implicitly, which the reader
infers from shared world knowledge. For example,
</bodyText>
<page confidence="0.988614">
937
</page>
<bodyText confidence="0.999977875">
to obtain the correct semantic parse for Give me
the fare from Seattle to Boston, one needs to infer
the existence of a flight entity, as in Give me the
fare (of a flight) from Seattle to Boston. Implicit
states offer candidates for addressing such needs.
As in sinking, child nodes have access to either of
the two simple states, but the implicit state is not
visible to the parent node.
</bodyText>
<subsectionHeader confidence="0.739627">
3.5 Lexical-Trigger Scores
</subsectionHeader>
<bodyText confidence="0.999979">
GUSP uses the database elements to automatically
derive a simple scoring scheme for lexical triggers.
If a database element has a name of k words, each
word is assigned score 1/k for the corresponding
node state. Similarly for property values and value
node states. In a sentence, if a word w triggers a
node state with score s, its dependency children
and left and right neighbors all get a trigger score
of 0.1·s for the same state. To score relevant words
not appearing in the database (due to incomplete-
ness of the database or lexical variations), GUSP
uses DASH (Pantel et al., 2009) to provide addi-
tional word-pair scoring based on lexical distribu-
tional similarity computed over general text cor-
pora (Wikipedia in this case). In the case of multi-
ple score assignments for the same word, the max-
imum score is used.
For multi-word values of property Y , and for
a dependency edge connecting two collocated
words, GUSP assigns a score 1.0 to the edge state
joining the value node state V:Y to its composi-
tion state V:Y:C, as well as the edge state joining
two composition states V:Y:C.
GUSP also uses a domain-independent list of
superlatives with the corresponding data types and
polarity (e.g., first, last, earliest, latest, cheapest)
and assigns a trigger score of 1.0 for each prop-
erty of a compatible data type (e.g., cheapest for
properties of type MONEY).
</bodyText>
<subsectionHeader confidence="0.937296">
3.6 The GUSP Model
</subsectionHeader>
<bodyText confidence="0.999787846153846">
In a nutshell, the GUSP model resembles a tree-
HMM, which models the emission of words and
dependencies by node and edge states, as well as
transition between an edge state and the parent
and child node states. In preliminary experiments
on the development set, we found that the naive
model (with multinomials as conditional probabil-
ities) did not perform well in EM. We thus chose
to apply feature-rich EM (Berg-Kirkpatrick et al.,
2010) in GUSP, which enabled the use of more
generalizable features. Specifically, GUSP defines
a probability distribution over dependency tree d
and semantic parse z by
</bodyText>
<equation confidence="0.9930705">
1 �
Pθ(d, z) =
Z exp
i
</equation>
<bodyText confidence="0.992765380952381">
where fi and wi are features and their weights, and
Z is the normalization constant that sums over all
possible d, z (over the same unlabeled tree). The
features of GUSP are as follows:
Lexical-trigger scores These are implemented
as emission features with fixed weights. For ex-
ample, given a token t that triggers node state
N with score s, there is a corresponding features
1(lemma = t, state = N) with weight α·s, where
α is a parameter.
Emission features for node states GUSP uses
two templates for emission of node states: for
raised states, 1(token = ·), i.e., the emission
weights for all raised states are tied; for non-raised
states, 1(lemma = ·, state = N).
Emission features for edge states GUSP uses
the following templates for emission of edge
states:
Child node state is NULL, dependency= ·;
Edge state is RAISING, dependency= ·;
Parent node state is same as the child node state,
</bodyText>
<equation confidence="0.655204">
dependency= ·;
</equation>
<bodyText confidence="0.9995044">
Otherwise, parent node state= ·, child node
state= ·, edge state type= ·, dependency= ·.
Transition features GUSP uses the following
templates for transition features, which are similar
to the edge emission features except for the depen-
dency label:
Child node state is NULL;
Edge state is RAISING;
Parent node state is same as the child node state;
Otherwise, parent node state= ·, child node
state= ·, edge state type= ·.
Complexity Prior To favor simple semantic
parses, GUSP imposes an exponential prior with
weight β on nodes states that are not null or raised,
and on each relational join in an edge state.
</bodyText>
<subsectionHeader confidence="0.998172">
3.7 Learning and Inference
</subsectionHeader>
<bodyText confidence="0.9999705">
Since the GUSP model factors over nodes and
edges, learning and inference can be done ef-
ficiently using EM and dynamic programming.
Specifically, the MAP parse and expectations can
</bodyText>
<equation confidence="0.754727">
fi(d, z) · wi(d, z)
</equation>
<page confidence="0.987708">
938
</page>
<bodyText confidence="0.999211375">
be computed by tree-Viterbi and inside-outside
(Petrov and Klein, 2008). The parameters can be
estimated by feature-rich EM (Berg-Kirkpatrick et
al., 2010).
Because the Viterbi and inside-outside are ap-
plied to a fixed tree (i.e., the input dependency
tree), their running times are only linear in the sen-
tence length in GUSP.
</bodyText>
<subsectionHeader confidence="0.99629">
3.8 Query Generation
</subsectionHeader>
<bodyText confidence="0.998209263888889">
Given a semantic parse, GUSP generates the SQL
by a depth-first traversal that recursively computes
the denotation of a node from the denotations of its
children and its node state and edge states. Each
denotation is a structured query that contains: a
list of entities for projection (corresponding to
the FROM statement in SQL); a computation tree
where the leaves are simple joins or value compar-
isons, and the internal nodes are logical or quan-
tifier operators (the WHERE statement); the salient
database elements (the SELECT statement). Be-
low, we illustrate this procedure using the seman-
tic parse in Figure 1 as a running example.
Value node state GUSP creates a semantic ob-
ject of the given type with a unique index and
the word constant. For example, the denotation
for node toronto is a city.name object with a
unique index and constant “toronto”. The unique
index is necessary in case the SQL involves mul-
tiple instances of the same entity. For example,
the SQL in Figure 1 involves two instances of the
entity city, corresponding to the departure and
arrival cities, respectively. By default, such a se-
mantic object will be translated into an equality
constraint, such as city.name = toronto.
Entity or property node state GUSP creates a
semantic object of the given type with a unique re-
lation index. For example, the denotation for node
flight is simply a flight object with a unique in-
dex. By default, such an object will contribute to
the list of entities in SQL projection (the FROM
statement), but not any constraints.
NULL state GUSP returns an empty denotation.
Simple edge state GUSP appends the child de-
notation to that of the parent, and appends equal-
ity constraints corresponding to the relational join
path. In the case of composition, such as the join
between diego and san, GUSP simply keeps the
parent object, while adding to it the words from
the child. In the case of a more complex join,
such as that between stopping and dtw, GUSP adds
the relational constraints that join flight stop
with airport:
flight stop.stop airport = airport.airport id.
Raising edge state GUSP simply takes the child
denotation and sets that to the parent.
Implicit and sinking states GUSP maintains
two separate denotations for the two simple states
in the complex state, and processes their respec-
tive edge states accordingly. For example, the
node diego contains two denotations, one for
V:city.name, and one for E:flight, with
the corresponding child being san and stopping,
respectively.
Domain-independent states For comparator
states such as MORE or LESS, GUSP changes the
default equality constraints to an inequality one,
such as flight.depart time &lt; 600 for before
6am. For logical connectives, GUSP combines the
projection and constraints accordingly. For quan-
tifier states, GUSP applies the given function to
the query.
Resolve scoping ambiguities GUSP delays ap-
plying quantifiers until the child semantic object
differs from the parent one or when reaching the
root. GUSP employs the following fixed ordering
in evaluating quantifiers and operators: superla-
tives and other quantifiers are evaluated at last
(i.e., after evaluating all other joins or operators
for the given object), whereas negation is evalu-
ated first, conjunctions and disjunctions are evalu-
ated in their order of appearance.
</bodyText>
<sectionHeader confidence="0.997832" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.985557">
4.1 Task
</subsectionHeader>
<bodyText confidence="0.999996083333333">
We evaluated GUSP on the ATIS travel planning
domain, which has been studied in He &amp; Young
(2005, 2006) and adapted for evaluating semantic
parsing by Zettlemoyer &amp; Collins (2007) (hence-
forth ZC07). The ZC07 dataset contains annotated
logical forms for each sentence, which we do not
use. Since our goal is not to produce a specific log-
ical form, we directly evaluate on the end-to-end
task of translating questions into database queries
and measure question-answering accuracy. The
ATIS distrbution contains the original SQL anno-
tations, which we used to compute gold answers
</bodyText>
<page confidence="0.997003">
939
</page>
<bodyText confidence="0.999915857142857">
for evaluation only. The dataset is split into train-
ing, development, and test, containing 4500, 478,
and 449 sentences, respectively. We used the de-
velopment set for initial development and tuning
hyperparameters. At test time, we ran GUSP over
the test set to learn a semantic parser and output
the MAP parses.2
</bodyText>
<subsectionHeader confidence="0.997229">
4.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999974285714286">
The ATIS sentences were originally derived from
spoken dialog and were therefore in lower cases.
Since case information is important for parsers
and taggers, we first truecased the sentences us-
ing DASH (Pantel et al., 2009), which stores the
case for each phrase in Wikipedia.
We then ran the sentences through SPLAT, a
state-of-the-art NLP toolkit (Quirk et al., 2012), to
conduct tokenization, part-of-speech tagging, and
constituency parsing. Since SPLAT does not out-
put dependency trees, we ran the Stanford parser
over SPLAT parses to generate the dependency
trees in Stanford dependency (de Marneffe et al.,
2006).
</bodyText>
<subsectionHeader confidence="0.998337">
4.3 Systems
</subsectionHeader>
<bodyText confidence="0.996855791666666">
For the GUSP system, we set the hyperparame-
ters from initial experiments on the development
set, and used them in all subsequent experiments.
Specifically, we set α = 50 and β = −0.1, and
ran three iterations of feature-rich EM with an L2
prior of 10 over the feature weights.
To evaluate the importance of complex states,
we considered two versions of GUSP : GUSP-
SIMPLE and GUSP-FULL, where GUSP-
SIMPLE only admits simple states, whereas
GUSP-FULL admits all states.
During development, we found that some
questions are inherently ambiguous that can-
not be solved except with some domain
knowledge or labeled examples. In Sec-
tion 3.2, we discuss an edge state that joins
a flight with its starting city: flight-
-flight.from airport-airport-
-airport service-city. The ATIS
database also contains another path of the same
length: flight-flight.from airport-
-airport-ground service-city. The
only difference is that air service is replaced
by ground service. In some occasions, the
</bodyText>
<footnote confidence="0.963844">
2This doesn’t lead to overfitting since we did not use any
labeled information in the test set.
</footnote>
<tableCaption confidence="0.982733">
Table 1: Comparison of semantic parsing accu-
</tableCaption>
<bodyText confidence="0.944949285714286">
racy on the ATIS test dataset. Both ZC07 and
FUBL used annotated logical forms in training,
whereas GUSP-FULL and GUSP++ did not. The
numbers for GUSP-FULL and GUSP++ are end-
to-end question answering accuracy, whereas the
numbers for ZC07 and FUBL are recall on exact
match in logical forms.
</bodyText>
<table confidence="0.9936424">
Accuracy
ZC07 84.6
FUBL 82.8
GUSP-FULL 74.8
GUSP++ 83.5
</table>
<bodyText confidence="0.999255928571428">
answers are identical whereas in others they are
different. Without other information, neither the
complexity prior nor EM can properly discrimi-
nate one against another. (Note that this ambiguity
is not present in the ZC07 logical forms, which
use a single predicate from(f,c) for the entire
relation paths. In other words, to translate ZC07
logical forms into SQL, one also needs to decide
on which path to use.)
Another type of domain-specific ambigui-
ties involves sentences such as give me in-
formation on flights after 4pm on wednesday.
There is no obvious information to disam-
biguate between flight.departure time
and flight.arrival time for 4pm.
Such ambiguities suggest opportunities for in-
teractive learning,3 but this is clearly out of
the scope of this paper. Instead, we incor-
porated a simple disambiguation feature with a
small weight of 0.01 that fires over the sim-
ple states of flight.departure time and
airport service. We named the resulting
system GUSP++.
To gauge the difficulty of the task and the qual-
ity of lexical-trigger scores, we also considered
a deterministic baseline LEXICAL, which com-
puted semantic parses using lexical-trigger scores
alone.
</bodyText>
<footnote confidence="0.5765894">
3For example, after eliminating other much less likely
alternatives, the system can present to the user with both
choices and let the user to choose the correct one. The im-
plicit feedback signal can then be used to train the system for
future disambiguation.
</footnote>
<page confidence="0.979832">
940
</page>
<note confidence="0.6938715">
removing RAISING dropped accuracy by almost
8 points.
</note>
<tableCaption confidence="0.8723345">
Table 2: Comparison of question answering accu-
racy in ablation experiments.
</tableCaption>
<table confidence="0.998430625">
Accuracy
LEXICAL 33.9
GUSP-SIMPLE 66.5
GUSP-FULL 74.8
GUSP++ 83.5
− RAISING 75.7
− SINKING 77.5
− IMPLICIT 76.2
</table>
<subsectionHeader confidence="0.584633">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999991588235294">
We first compared the results of GUSP-FULL and
GUSP++ with ZC07 and FUBL (Kwiatkowski et
al., 2011).4 Note that ZC07 and FUBL were eval-
uated on exact match in logical forms. We used
their recall numbers which are the percentages of
sentences with fully correct logical forms. Given
that the questions are quite specific and generally
admit nonzero number of answers, the question-
answer accuracy should be quite comparable with
these numbers.
Table 1 shows the comparison. Surprisingly,
even without the additional disambiguation fea-
ture, GUSP-FULL already attained an accuracy
broadly in range with supervised results. With the
feature, GUSP++ effectively tied with the best
supervised approach.
To evaluate the importance of various compo-
nents in GUSP, we conducted ablation test to com-
pare the variants of GUSP. Table 2 shows the re-
sults. LEXICAL can parse more than one third
of the sentences correctly, which is quite remark-
able in itself, considering that it only used the lex-
ical scores. On the other hand, roughly two-third
of the sentences cannot be correctly parsed in this
way, suggesting that the lexical scores are noisy
and ambiguous. In comparison, all GUSP variants
achieved significant gains over LEXICAL. Addi-
tionally, GUSP-FULL substantially outperformed
GUSP-SIMPLE, highlighting the challenges of
syntax-semantics mismatch in ATIS, and demon-
strating the importance and effectiveness of com-
plex states for handling such mismatch. All three
types of complex states produced significant con-
tributions. For example, compared to GUSP++,
</bodyText>
<footnote confidence="0.936055666666667">
4We should note that while the more recent system of
FUBL slightly trails ZC07, it is language-independent and
can parse questions in multiple languages.
</footnote>
<subsectionHeader confidence="0.847665">
4.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999939294117647">
Upon manual inspection, many of the remaining
errors are due to syntactic parsing errors that are
too severe to fix. This is partly due to the fact that
ATIS sentences are out of domain compared to
the newswired text on which the syntactic parsers
were trained. For example, show, list were regu-
larly parsed as nouns, whereas round (as in round
trip) were often parsed as a verb and northwest
were parsed as an auxilliary verb. Another reason
is that ATIS sentences are typically less formal or
grammatical, which exacerbates the difficulty in
parsing. In this paper, we used the 1-best depen-
dency tree to produce semantic parse. An interest-
ing future direction is to consider joint syntactic-
semantic parsing, using k-best trees or even the
parse forest as input and reranking the top parse
using semantic information.5
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999534375">
This paper introduces grounded unsupervised
semantic parsing, which leverages available
database for indirect supervision and uses a
grounded meaning representation to account for
syntax-semantics mismatch in dependency-based
semantic parsing. The resulting GUSP system is
the first unsupervised approach to attain an accu-
racy comparable to the best supervised systems in
translating complex natural-language questions to
database queries.
Directions for future work include: joint
syntactic-semantic parsing, developing better fea-
tures for learning; interactive learning in a dialog
setting; generalizing distant supervision; applica-
tion to knowledge extraction from database-rich
domains such as biomedical sciences.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99994125">
We would like to thank Kristina Toutanova, Chris
Quirk, Luke Zettlemoyer, and Yoav Artzi for use-
ful discussions, and Patrick Pantel and Michael
Gammon for help with the datasets.
</bodyText>
<footnote confidence="0.8886676">
5Note that this is still different from the currently predom-
inant approaches in semantic parsing, which learn to parse
both syntax and semantics by training from the semantic
parsing datasets alone, which are considerably smaller com-
pared to resources available for syntactic parsing.
</footnote>
<page confidence="0.995181">
941
</page>
<sectionHeader confidence="0.983244" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99984585046729">
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the Twentieth International Joint Con-
ference on Artificial Intelligence, pages 2670–2676,
Hyderabad, India. AAAI Press.
Taylor Berg-Kirkpatrick, John DeNero, and Dan Klein.
2010. Painless unsupervised learning with features.
In Proceedings of Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Benjamin B¨orschinger, Bevan K. Jones, and Mark
Johnson. 2011. Reducing grounded learning tasks
to grammatical inference. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing.
Xavier Carreras and Luis Marquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role la-
beling. In Proceedings of the Eighth Conference on
Computational Natural Language Learning, pages
89–97, Boston, MA. ACL.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language ac-
quisition. In ICML-08.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
world’s response. In Proceedings of the 2010 Con-
ference on Natural Language Learning.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation, pages 449–
454, Genoa, Italy. ELRA.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Morgan &amp; Claypool, San Rafael, CA.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proceedings of the Forty Ninth
Annual Meeting of the Association for Computa-
tional Linguistics.
B.J. Grosz, D. Appelt, P. Martin, and F. Pereira. 1987.
Team: An experiment in the design of transportable
natural language interfaces. Artificial Intelligence,
32:173–243.
Yulan He and Steve Young. 2005. Semantic process-
ing using the hidden vector state model. In Com-
puter Speech and Language.
Yulan He and Steve Young. 2006. Spoken lan-
guage understanding using the hidden vector state
model. In Speech Communication Special Issue on
Spoken Language understanding for Conversational
Systems.
Larry Heck, Dilek Hakkani-Tur, and Gokhan Tur.
2013. Leveraging knowledge graphs for web-scale
unsupervised semantic parsing. In Proceedings of
the Interspeech 2013.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the Forty Ninth Annual Meeting of the Association
for Computational Linguistics.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005.
Learning to transform natural to formal languages.
In Proceedings of the Twentieth National Confer-
ence on Artificial Intelligence.
Joohyun Kim and Raymond J. Mooney. 2010. Gen-
erative alignment and semantic parsing for learning
from ambiguous supervision. In COLING10.
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
EMNLP-12.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in ccg grammar induction for semantic parsing.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the Forty Ninth Annual Meet-
ing of the Association for Computational Linguis-
tics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Forty Seventh Annual Meeting of the Association for
Computational Linguistics.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In Proceedings of the Eighth International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 311–324, Mexico City,
Mexico. Springer.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing.
Slav Petrov and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. In NIPS-08.
Hoifung Poon and Pedro Domingos. 2007. Joint in-
ference in information extraction. In Proceedings of
the Twenty Second National Conference on Artificial
Intelligence, pages 913–918, Vancouver, Canada.
AAAI Press.
</reference>
<page confidence="0.977329">
942
</page>
<reference confidence="0.999572085106383">
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1–10, Singapore. ACL.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontological induction from text. In Proceed-
ings of the Forty Eighth Annual Meeting of the As-
sociation for Computational Linguistics, pages 296–
305, Uppsala, Sweden. ACL.
Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003. Towards a theory of natural language inter-
faces to databases. In IUI-03.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,
David Ko, and Alexander Yates. 2004. Modern
natural language interfaces to databases: Compos-
ing statistical parsing with semantic tractability. In
COLING-04.
Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami
Suzuki, Kristina Toutanova, Michael Gamon, Wen-
tau Yih, and Lucy Vanderwende. 2012. MSR
SPLAT, a language analysis toolkit. In Proceedings
of NAACL HLT 2012 Demonstration Session.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the Sixteen Euro-
pean Conference on Machine Learning.
Ivan Titov and Alexandre Klementiev. 2011. A
bayesian model for unsupervised semantic parsing.
In Proceedings of the Forty Ninth Annual Meeting of
the Association for Computational Linguistics.
Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the Conference of the
European Chapter of the Association for Computa-
tional Linguistics.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammers. In Proceedings of the Twenty First
Conference on Uncertainty in Artificial Intelligence,
pages 658–666, Edinburgh, Scotland. AUAI Press.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing
to logical form. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
</reference>
<page confidence="0.999136">
943
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.413026">
<title confidence="0.920133">Grounded Unsupervised Semantic Parsing Hoifung One Microsoft</title>
<affiliation confidence="0.654359">Microsoft</affiliation>
<address confidence="0.998844">Redmond, WA 98052, USA</address>
<email confidence="0.999712">hoifung@microsoft.com</email>
<abstract confidence="0.994559277777778">We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using EM. To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2670--2676</pages>
<publisher>AAAI Press.</publisher>
<location>Hyderabad,</location>
<contexts>
<context position="4870" citStr="Banko et al., 2007" startWordPosition="730" endWordPosition="733">bers of relations and instances, as well as a more irregular language (ATIS questions were derived from spoken dialogs). Despite these challenges, GUSP attains an accuracy of 84% in end-to-end question answering, effectively tying with the stateof-the-art supervised approaches (85% by Zettlemoyer &amp; Collins (2007), 83% by Kwiatkowski et al. (2011)). 2 Background 2.1 Semantic Parsing The goal of semantic parsing is to map text to a complete and detailed meaning representation (Mooney, 2007). This is in contrast with semantic role labeling (Carreras and Marquez, 2004) and information extraction (Banko et al., 2007; Poon and Domingos, 2007), which have a more restricted goal of identifying local semantic roles or extracting selected information slots. The standard language for meaning representation is first-order logic or a sublanguage, such as FunQL (Kate et al., 2005; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). Poon &amp; Domingos (2009, 2010) induce a meaning representation by clustering synonymous lambda-calculus forms stemming from partitions of dependency trees. More recently, Liang et al. (2011) proposed DCS for dependency-based compositio</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence, pages 2670–2676, Hyderabad, India. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23384" citStr="Berg-Kirkpatrick et al., 2010" startWordPosition="3727" endWordPosition="3730">e.g., first, last, earliest, latest, cheapest) and assigns a trigger score of 1.0 for each property of a compatible data type (e.g., cheapest for properties of type MONEY). 3.6 The GUSP Model In a nutshell, the GUSP model resembles a treeHMM, which models the emission of words and dependencies by node and edge states, as well as transition between an edge state and the parent and child node states. In preliminary experiments on the development set, we found that the naive model (with multinomials as conditional probabilities) did not perform well in EM. We thus chose to apply feature-rich EM (Berg-Kirkpatrick et al., 2010) in GUSP, which enabled the use of more generalizable features. Specifically, GUSP defines a probability distribution over dependency tree d and semantic parse z by 1 � Pθ(d, z) = Z exp i where fi and wi are features and their weights, and Z is the normalization constant that sums over all possible d, z (over the same unlabeled tree). The features of GUSP are as follows: Lexical-trigger scores These are implemented as emission features with fixed weights. For example, given a token t that triggers node state N with score s, there is a corresponding features 1(lemma = t, state = N) with weight </context>
<context position="25481" citStr="Berg-Kirkpatrick et al., 2010" startWordPosition="4083" endWordPosition="4086">de state= ·, child node state= ·, edge state type= ·. Complexity Prior To favor simple semantic parses, GUSP imposes an exponential prior with weight β on nodes states that are not null or raised, and on each relational join in an edge state. 3.7 Learning and Inference Since the GUSP model factors over nodes and edges, learning and inference can be done efficiently using EM and dynamic programming. Specifically, the MAP parse and expectations can fi(d, z) · wi(d, z) 938 be computed by tree-Viterbi and inside-outside (Petrov and Klein, 2008). The parameters can be estimated by feature-rich EM (Berg-Kirkpatrick et al., 2010). Because the Viterbi and inside-outside are applied to a fixed tree (i.e., the input dependency tree), their running times are only linear in the sentence length in GUSP. 3.8 Query Generation Given a semantic parse, GUSP generates the SQL by a depth-first traversal that recursively computes the denotation of a node from the denotations of its children and its node state and edge states. Each denotation is a structured query that contains: a list of entities for projection (corresponding to the FROM statement in SQL); a computation tree where the leaves are simple joins or value comparisons, a</context>
</contexts>
<marker>Berg-Kirkpatrick, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin B¨orschinger</author>
<author>Bevan K Jones</author>
<author>Mark Johnson</author>
</authors>
<title>Reducing grounded learning tasks to grammatical inference.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>B¨orschinger, Jones, Johnson, 2011</marker>
<rawString>Benjamin B¨orschinger, Bevan K. Jones, and Mark Johnson. 2011. Reducing grounded learning tasks to grammatical inference. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Luis Marquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eighth Conference on Computational Natural Language Learning,</booktitle>
<pages>89--97</pages>
<publisher>ACL.</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="4823" citStr="Carreras and Marquez, 2004" startWordPosition="722" endWordPosition="725">database that is an order of magnitude larger in the numbers of relations and instances, as well as a more irregular language (ATIS questions were derived from spoken dialogs). Despite these challenges, GUSP attains an accuracy of 84% in end-to-end question answering, effectively tying with the stateof-the-art supervised approaches (85% by Zettlemoyer &amp; Collins (2007), 83% by Kwiatkowski et al. (2011)). 2 Background 2.1 Semantic Parsing The goal of semantic parsing is to map text to a complete and detailed meaning representation (Mooney, 2007). This is in contrast with semantic role labeling (Carreras and Marquez, 2004) and information extraction (Banko et al., 2007; Poon and Domingos, 2007), which have a more restricted goal of identifying local semantic roles or extracting selected information slots. The standard language for meaning representation is first-order logic or a sublanguage, such as FunQL (Kate et al., 2005; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). Poon &amp; Domingos (2009, 2010) induce a meaning representation by clustering synonymous lambda-calculus forms stemming from partitions of dependency trees. More recently, Liang et al. (201</context>
</contexts>
<marker>Carreras, Marquez, 2004</marker>
<rawString>Xavier Carreras and Luis Marquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of the Eighth Conference on Computational Natural Language Learning, pages 89–97, Boston, MA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to sportscast: A test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In ICML-08.</booktitle>
<contexts>
<context position="1370" citStr="Chen and Mooney, 2008" startWordPosition="194" endWordPosition="197">lished results by supervised approaches. 1 Introduction Semantic parsing maps text to a formal meaning representation such as logical forms or structured queries. Recently, there has been a burgeoning interest in developing machine-learning approaches for semantic parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Mooney, 2007; Kwiatkowski et al., 2011), but the predominant paradigm uses supervised learning, which requires example annotations that are costly to obtain. More recently, several groundedlearning approaches have been proposed to alleviate the annotation burden (Chen and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011; Clarke et al., 2010; Liang et al., 2011). In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from questionanswer pairs alone, which represents a significant advance. However, although these methods exonerate annotators from mastering specialized logical forms, finding the answers for complex questions still requires non-trivial effort. 1 Poon &amp; Domingos (2009, 2010) proposed the USP system for unsupervised semantic parsing, which learns a parser by recursively clustering and composing synonymous expressions. </context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2008. Learning to sportscast: A test of grounded language acquisition. In ICML-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from world’s response.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="1440" citStr="Clarke et al., 2010" startWordPosition="206" endWordPosition="209"> maps text to a formal meaning representation such as logical forms or structured queries. Recently, there has been a burgeoning interest in developing machine-learning approaches for semantic parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Mooney, 2007; Kwiatkowski et al., 2011), but the predominant paradigm uses supervised learning, which requires example annotations that are costly to obtain. More recently, several groundedlearning approaches have been proposed to alleviate the annotation burden (Chen and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011; Clarke et al., 2010; Liang et al., 2011). In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from questionanswer pairs alone, which represents a significant advance. However, although these methods exonerate annotators from mastering specialized logical forms, finding the answers for complex questions still requires non-trivial effort. 1 Poon &amp; Domingos (2009, 2010) proposed the USP system for unsupervised semantic parsing, which learns a parser by recursively clustering and composing synonymous expressions. While their approach completely obviates the need for direct supervisi</context>
<context position="3495" citStr="Clarke et al. (2010)" startWordPosition="517" endWordPosition="520">s learning using lexical scores computed from the names and values of database elements. Unlike previous grounded-learning approaches, GUSP does not require ambiguous annotations or oracle answers, but rather focuses on leveraging database contents that are readily available. Unlike USP, GUSP predetermines the target logical forms based on the database schema, which alleviates the difficulty in learning and ensures that the output semantic parses can be directly used in querying the database. To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al. (2010) and Liang et al. (2011) used the annotated logical forms to compute answers for their experiments. 933 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 933–943, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics by augmenting the state space to represent semantic relations beyond immediate dependency neighborhood. This representation also factorizes over nodes and edges, enabling linear-time exact inference in GUSP. We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettl</context>
<context position="5152" citStr="Clarke et al., 2010" startWordPosition="775" endWordPosition="778">5% by Zettlemoyer &amp; Collins (2007), 83% by Kwiatkowski et al. (2011)). 2 Background 2.1 Semantic Parsing The goal of semantic parsing is to map text to a complete and detailed meaning representation (Mooney, 2007). This is in contrast with semantic role labeling (Carreras and Marquez, 2004) and information extraction (Banko et al., 2007; Poon and Domingos, 2007), which have a more restricted goal of identifying local semantic roles or extracting selected information slots. The standard language for meaning representation is first-order logic or a sublanguage, such as FunQL (Kate et al., 2005; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). Poon &amp; Domingos (2009, 2010) induce a meaning representation by clustering synonymous lambda-calculus forms stemming from partitions of dependency trees. More recently, Liang et al. (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins. In this paper, we focus on semantic parsing for natural-language interface to database (Grosz et al., 1987). In this problem</context>
<context position="8884" citStr="Clarke et al., 2010" startWordPosition="1357" endWordPosition="1360">irect supervision may take the form as ambiguous annotations (Chen 934 E:flight:R Figure 1: End-to-end question answering by GUSP for sentence get flight from toronto to san diego stopping in dtw. Top: the dependency tree of the sentence is annotated with latent semantic states by GUSP. For brevity, we omit the edge states. Raising occurs from flight to get and sinking occurs from get to diego. Bottom: the semantic tree is deterministically converted into SQL to obtain answer from the database. and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011) or example questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). In general, however, such supervision is not always available or easy to obtain. In contrast, databases are often abundantly available, especially for important domains. The database community has considerable amount of work on leveraging databases in various tasks such as entity resolution, schema matching, and others. To the best of our knowledge, this approach is still underexplored in the NLP community. One notable exception is distant supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012; Heck et al., 2013), wh</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from world’s response. In Proceedings of the 2010 Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation,</booktitle>
<pages>449--454</pages>
<location>Genoa, Italy. ELRA.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the Fifth International Conference on Language Resources and Evaluation, pages 449– 454, Genoa, Italy. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
<author>Daniel Lowd</author>
</authors>
<title>Markov Logic: An Interface Layer for Artificial Intelligence. Morgan &amp; Claypool,</title>
<date>2009</date>
<location>San Rafael, CA.</location>
<contexts>
<context position="6211" citStr="Domingos and Lowd, 2009" startWordPosition="934" endWordPosition="937">and edges representing relational joins. In this paper, we focus on semantic parsing for natural-language interface to database (Grosz et al., 1987). In this problem setting, a naturallanguage question is first translated into a meaning representation by semantic parsing, and then converted into a structured query such as SQL to obtain answer from the database. 2.2 Unsupervised Semantic Parsing Unsupervised semantic parsing was first proposed by Poon &amp; Domingos (2009, 2010) with their USP system. USP defines a probabilistic model over the dependency tree and semantic parse using Markov logic (Domingos and Lowd, 2009), and recursively clusters and composes synonymous dependency treelets using a hard EM-like procedure. Since USP uses nonlocal features (e.g., the argument-number feature) and operates over partitions, exact inference is intractable, and USP resorts to a greedy approach to find the MAP parse by searching over partitions. Titov &amp; Klementiev (2011) proposed a Bayesian version of USP and Titov &amp; Klementiev (2012) adapted it for semantic role induction. In USP, the meaning is represented by self-induced clusters. Therefore, to answer complex questions against a database, it requires an additional </context>
</contexts>
<marker>Domingos, Lowd, 2009</marker>
<rawString>Pedro Domingos and Daniel Lowd. 2009. Markov Logic: An Interface Layer for Artificial Intelligence. Morgan &amp; Claypool, San Rafael, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Roi Reichart</author>
<author>James Clarke</author>
<author>Dan Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Forty Ninth Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7766" citStr="Goldwasser et al. (2011)" startWordPosition="1180" endWordPosition="1183">ific lexicon that specifies the synonyms for names and values of database elements, a restricted set of potential interpretations for domain verbs and prepositions, as well as a set of domain questions with manually labeled POS tags for retraining the tagger and parser. It also focuses on the subset of easy questions (“semantically tractable” questions), and sidesteps the problem of dealing with complex and nested structures, as well as ambiguous interpretations. Remarkably, while PRECISE can be very accurate on easy questions, it does not try to learn from these interpretations. In contrast, Goldwasser et al. (2011) proposed a self-supervised approach, which iteratively chose high-confidence parses to retrain the parser. Their system, however, still required a lexicon manually constructed for the given domain. Moreover, it was only applied to a small domain (a subset of GEO), and the result still trailed supervised systems by a wide margin. 2.3 Grounded Learning for Semantic Parsing Grounded learning is motivated by alleviating the burden of direct supervision via interaction with the world, where the indirect supervision may take the form as ambiguous annotations (Chen 934 E:flight:R Figure 1: End-to-en</context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth. 2011. Confidence driven unsupervised semantic parsing. In Proceedings of the Forty Ninth Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>D Appelt</author>
<author>P Martin</author>
<author>F Pereira</author>
</authors>
<title>Team: An experiment in the design of transportable natural language interfaces.</title>
<date>1987</date>
<journal>Artificial Intelligence,</journal>
<pages>32--173</pages>
<contexts>
<context position="5735" citStr="Grosz et al., 1987" startWordPosition="859" endWordPosition="862">e et al., 2005; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). Poon &amp; Domingos (2009, 2010) induce a meaning representation by clustering synonymous lambda-calculus forms stemming from partitions of dependency trees. More recently, Liang et al. (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins. In this paper, we focus on semantic parsing for natural-language interface to database (Grosz et al., 1987). In this problem setting, a naturallanguage question is first translated into a meaning representation by semantic parsing, and then converted into a structured query such as SQL to obtain answer from the database. 2.2 Unsupervised Semantic Parsing Unsupervised semantic parsing was first proposed by Poon &amp; Domingos (2009, 2010) with their USP system. USP defines a probabilistic model over the dependency tree and semantic parse using Markov logic (Domingos and Lowd, 2009), and recursively clusters and composes synonymous dependency treelets using a hard EM-like procedure. Since USP uses nonloc</context>
</contexts>
<marker>Grosz, Appelt, Martin, Pereira, 1987</marker>
<rawString>B.J. Grosz, D. Appelt, P. Martin, and F. Pereira. 1987. Team: An experiment in the design of transportable natural language interfaces. Artificial Intelligence, 32:173–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Steve Young</author>
</authors>
<title>Semantic processing using the hidden vector state model.</title>
<date>2005</date>
<booktitle>In Computer Speech and Language.</booktitle>
<contexts>
<context position="29107" citStr="He &amp; Young (2005" startWordPosition="4679" endWordPosition="4682">unction to the query. Resolve scoping ambiguities GUSP delays applying quantifiers until the child semantic object differs from the parent one or when reaching the root. GUSP employs the following fixed ordering in evaluating quantifiers and operators: superlatives and other quantifiers are evaluated at last (i.e., after evaluating all other joins or operators for the given object), whereas negation is evaluated first, conjunctions and disjunctions are evaluated in their order of appearance. 4 Experiments 4.1 Task We evaluated GUSP on the ATIS travel planning domain, which has been studied in He &amp; Young (2005, 2006) and adapted for evaluating semantic parsing by Zettlemoyer &amp; Collins (2007) (henceforth ZC07). The ZC07 dataset contains annotated logical forms for each sentence, which we do not use. Since our goal is not to produce a specific logical form, we directly evaluate on the end-to-end task of translating questions into database queries and measure question-answering accuracy. The ATIS distrbution contains the original SQL annotations, which we used to compute gold answers 939 for evaluation only. The dataset is split into training, development, and test, containing 4500, 478, and 449 sente</context>
</contexts>
<marker>He, Young, 2005</marker>
<rawString>Yulan He and Steve Young. 2005. Semantic processing using the hidden vector state model. In Computer Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Steve Young</author>
</authors>
<title>Spoken language understanding using the hidden vector state model. In Speech Communication Special Issue on Spoken Language understanding for Conversational Systems.</title>
<date>2006</date>
<marker>He, Young, 2006</marker>
<rawString>Yulan He and Steve Young. 2006. Spoken language understanding using the hidden vector state model. In Speech Communication Special Issue on Spoken Language understanding for Conversational Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Heck</author>
<author>Dilek Hakkani-Tur</author>
<author>Gokhan Tur</author>
</authors>
<title>Leveraging knowledge graphs for web-scale unsupervised semantic parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the Interspeech</booktitle>
<contexts>
<context position="9480" citStr="Heck et al., 2013" startWordPosition="1452" endWordPosition="1455">s (Clarke et al., 2010; Liang et al., 2011). In general, however, such supervision is not always available or easy to obtain. In contrast, databases are often abundantly available, especially for important domains. The database community has considerable amount of work on leveraging databases in various tasks such as entity resolution, schema matching, and others. To the best of our knowledge, this approach is still underexplored in the NLP community. One notable exception is distant supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012; Heck et al., 2013), which used database instances to derive training examples for relation extraction. This approach, however, still has considerable limitations. For example, it only handles binary relations, and the quality of the training examples is inherently noisy and hard to control. Moreover, this approach is not applicable to the questionanswering setting considered in this paper, since entity pairs in questions need not correspond to valid relational instances in the database. 3 Grounded Unsupervised Semantic Parsing In this section, we present the GUSP system for grounded unsupervised semantic parsin</context>
</contexts>
<marker>Heck, Hakkani-Tur, Tur, 2013</marker>
<rawString>Larry Heck, Dilek Hakkani-Tur, and Gokhan Tur. 2013. Leveraging knowledge graphs for web-scale unsupervised semantic parsing. In Proceedings of the Interspeech 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledge-based weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the Forty Ninth Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9426" citStr="Hoffmann et al., 2011" startWordPosition="1444" endWordPosition="1447">B¨orschinger et al., 2011) or example questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). In general, however, such supervision is not always available or easy to obtain. In contrast, databases are often abundantly available, especially for important domains. The database community has considerable amount of work on leveraging databases in various tasks such as entity resolution, schema matching, and others. To the best of our knowledge, this approach is still underexplored in the NLP community. One notable exception is distant supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012; Heck et al., 2013), which used database instances to derive training examples for relation extraction. This approach, however, still has considerable limitations. For example, it only handles binary relations, and the quality of the training examples is inherently noisy and hard to control. Moreover, this approach is not applicable to the questionanswering setting considered in this paper, since entity pairs in questions need not correspond to valid relational instances in the database. 3 Grounded Unsupervised Semantic Parsing In this section, we present the</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the Forty Ninth Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twentieth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="5130" citStr="Kate et al., 2005" startWordPosition="771" endWordPosition="774">vised approaches (85% by Zettlemoyer &amp; Collins (2007), 83% by Kwiatkowski et al. (2011)). 2 Background 2.1 Semantic Parsing The goal of semantic parsing is to map text to a complete and detailed meaning representation (Mooney, 2007). This is in contrast with semantic role labeling (Carreras and Marquez, 2004) and information extraction (Banko et al., 2007; Poon and Domingos, 2007), which have a more restricted goal of identifying local semantic roles or extracting selected information slots. The standard language for meaning representation is first-order logic or a sublanguage, such as FunQL (Kate et al., 2005; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). Poon &amp; Domingos (2009, 2010) induce a meaning representation by clustering synonymous lambda-calculus forms stemming from partitions of dependency trees. More recently, Liang et al. (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins. In this paper, we focus on semantic parsing for natural-language interface to database (Grosz et al., </context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural to formal languages. In Proceedings of the Twentieth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Generative alignment and semantic parsing for learning from ambiguous supervision.</title>
<date>2010</date>
<booktitle>In COLING10.</booktitle>
<contexts>
<context position="1392" citStr="Kim and Mooney, 2010" startWordPosition="198" endWordPosition="201">vised approaches. 1 Introduction Semantic parsing maps text to a formal meaning representation such as logical forms or structured queries. Recently, there has been a burgeoning interest in developing machine-learning approaches for semantic parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Mooney, 2007; Kwiatkowski et al., 2011), but the predominant paradigm uses supervised learning, which requires example annotations that are costly to obtain. More recently, several groundedlearning approaches have been proposed to alleviate the annotation burden (Chen and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011; Clarke et al., 2010; Liang et al., 2011). In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from questionanswer pairs alone, which represents a significant advance. However, although these methods exonerate annotators from mastering specialized logical forms, finding the answers for complex questions still requires non-trivial effort. 1 Poon &amp; Domingos (2009, 2010) proposed the USP system for unsupervised semantic parsing, which learns a parser by recursively clustering and composing synonymous expressions. While their approach c</context>
<context position="8803" citStr="Kim and Mooney, 2010" startWordPosition="1344" endWordPosition="1347">ing the burden of direct supervision via interaction with the world, where the indirect supervision may take the form as ambiguous annotations (Chen 934 E:flight:R Figure 1: End-to-end question answering by GUSP for sentence get flight from toronto to san diego stopping in dtw. Top: the dependency tree of the sentence is annotated with latent semantic states by GUSP. For brevity, we omit the edge states. Raising occurs from flight to get and sinking occurs from get to diego. Bottom: the semantic tree is deterministically converted into SQL to obtain answer from the database. and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011) or example questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). In general, however, such supervision is not always available or easy to obtain. In contrast, databases are often abundantly available, especially for important domains. The database community has considerable amount of work on leveraging databases in various tasks such as entity resolution, schema matching, and others. To the best of our knowledge, this approach is still underexplored in the NLP community. One notable exception is distant supervision (Mintz et al., 2009; Riedel et al., 2010</context>
</contexts>
<marker>Kim, Mooney, 2010</marker>
<rawString>Joohyun Kim and Raymond J. Mooney. 2010. Generative alignment and semantic parsing for learning from ambiguous supervision. In COLING10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Weakly supervised training of semantic parsers.</title>
<date>2012</date>
<booktitle>In EMNLP-12.</booktitle>
<contexts>
<context position="9460" citStr="Krishnamurthy and Mitchell, 2012" startWordPosition="1448" endWordPosition="1451">11) or example questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). In general, however, such supervision is not always available or easy to obtain. In contrast, databases are often abundantly available, especially for important domains. The database community has considerable amount of work on leveraging databases in various tasks such as entity resolution, schema matching, and others. To the best of our knowledge, this approach is still underexplored in the NLP community. One notable exception is distant supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012; Heck et al., 2013), which used database instances to derive training examples for relation extraction. This approach, however, still has considerable limitations. For example, it only handles binary relations, and the quality of the training examples is inherently noisy and hard to control. Moreover, this approach is not applicable to the questionanswering setting considered in this paper, since entity pairs in questions need not correspond to valid relational instances in the database. 3 Grounded Unsupervised Semantic Parsing In this section, we present the GUSP system for grounded unsuperv</context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Jayant Krishnamurthy and Tom M. Mitchell. 2012. Weakly supervised training of semantic parsers. In EMNLP-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Lexical generalization in ccg grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1124" citStr="Kwiatkowski et al., 2011" startWordPosition="158" endWordPosition="161">lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches. 1 Introduction Semantic parsing maps text to a formal meaning representation such as logical forms or structured queries. Recently, there has been a burgeoning interest in developing machine-learning approaches for semantic parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Mooney, 2007; Kwiatkowski et al., 2011), but the predominant paradigm uses supervised learning, which requires example annotations that are costly to obtain. More recently, several groundedlearning approaches have been proposed to alleviate the annotation burden (Chen and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011; Clarke et al., 2010; Liang et al., 2011). In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from questionanswer pairs alone, which represents a significant advance. However, although these methods exonerate annotators from mastering specialized logical forms, finding</context>
<context position="4600" citStr="Kwiatkowski et al. (2011)" startWordPosition="687" endWordPosition="690">inference in GUSP. We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). Compared to other standard datasets such as GEO and JOBS, ATIS features a database that is an order of magnitude larger in the numbers of relations and instances, as well as a more irregular language (ATIS questions were derived from spoken dialogs). Despite these challenges, GUSP attains an accuracy of 84% in end-to-end question answering, effectively tying with the stateof-the-art supervised approaches (85% by Zettlemoyer &amp; Collins (2007), 83% by Kwiatkowski et al. (2011)). 2 Background 2.1 Semantic Parsing The goal of semantic parsing is to map text to a complete and detailed meaning representation (Mooney, 2007). This is in contrast with semantic role labeling (Carreras and Marquez, 2004) and information extraction (Banko et al., 2007; Poon and Domingos, 2007), which have a more restricted goal of identifying local semantic roles or extracting selected information slots. The standard language for meaning representation is first-order logic or a sublanguage, such as FunQL (Kate et al., 2005; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2</context>
<context position="33807" citStr="Kwiatkowski et al., 2011" startWordPosition="5433" endWordPosition="5436">s alone. 3For example, after eliminating other much less likely alternatives, the system can present to the user with both choices and let the user to choose the correct one. The implicit feedback signal can then be used to train the system for future disambiguation. 940 removing RAISING dropped accuracy by almost 8 points. Table 2: Comparison of question answering accuracy in ablation experiments. Accuracy LEXICAL 33.9 GUSP-SIMPLE 66.5 GUSP-FULL 74.8 GUSP++ 83.5 − RAISING 75.7 − SINKING 77.5 − IMPLICIT 76.2 4.4 Results We first compared the results of GUSP-FULL and GUSP++ with ZC07 and FUBL (Kwiatkowski et al., 2011).4 Note that ZC07 and FUBL were evaluated on exact match in logical forms. We used their recall numbers which are the percentages of sentences with fully correct logical forms. Given that the questions are quite specific and generally admit nonzero number of answers, the questionanswer accuracy should be quite comparable with these numbers. Table 1 shows the comparison. Surprisingly, even without the additional disambiguation feature, GUSP-FULL already attained an accuracy broadly in range with supervised results. With the feature, GUSP++ effectively tied with the best supervised approach. To </context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical generalization in ccg grammar induction for semantic parsing. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Forty Ninth Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1461" citStr="Liang et al., 2011" startWordPosition="210" endWordPosition="213">l meaning representation such as logical forms or structured queries. Recently, there has been a burgeoning interest in developing machine-learning approaches for semantic parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Mooney, 2007; Kwiatkowski et al., 2011), but the predominant paradigm uses supervised learning, which requires example annotations that are costly to obtain. More recently, several groundedlearning approaches have been proposed to alleviate the annotation burden (Chen and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011; Clarke et al., 2010; Liang et al., 2011). In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from questionanswer pairs alone, which represents a significant advance. However, although these methods exonerate annotators from mastering specialized logical forms, finding the answers for complex questions still requires non-trivial effort. 1 Poon &amp; Domingos (2009, 2010) proposed the USP system for unsupervised semantic parsing, which learns a parser by recursively clustering and composing synonymous expressions. While their approach completely obviates the need for direct supervision, their target logi</context>
<context position="3519" citStr="Liang et al. (2011)" startWordPosition="522" endWordPosition="525">scores computed from the names and values of database elements. Unlike previous grounded-learning approaches, GUSP does not require ambiguous annotations or oracle answers, but rather focuses on leveraging database contents that are readily available. Unlike USP, GUSP predetermines the target logical forms based on the database schema, which alleviates the difficulty in learning and ensures that the output semantic parses can be directly used in querying the database. To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al. (2010) and Liang et al. (2011) used the annotated logical forms to compute answers for their experiments. 933 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 933–943, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics by augmenting the state space to represent semantic relations beyond immediate dependency neighborhood. This representation also factorizes over nodes and edges, enabling linear-time exact inference in GUSP. We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007</context>
<context position="5425" citStr="Liang et al. (2011)" startWordPosition="814" endWordPosition="817">nd Marquez, 2004) and information extraction (Banko et al., 2007; Poon and Domingos, 2007), which have a more restricted goal of identifying local semantic roles or extracting selected information slots. The standard language for meaning representation is first-order logic or a sublanguage, such as FunQL (Kate et al., 2005; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). Poon &amp; Domingos (2009, 2010) induce a meaning representation by clustering synonymous lambda-calculus forms stemming from partitions of dependency trees. More recently, Liang et al. (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins. In this paper, we focus on semantic parsing for natural-language interface to database (Grosz et al., 1987). In this problem setting, a naturallanguage question is first translated into a meaning representation by semantic parsing, and then converted into a structured query such as SQL to obtain answer from the database. 2.2 Unsupervised Semantic Parsing Unsupervised semantic parsing was first </context>
<context position="8905" citStr="Liang et al., 2011" startWordPosition="1361" endWordPosition="1364"> take the form as ambiguous annotations (Chen 934 E:flight:R Figure 1: End-to-end question answering by GUSP for sentence get flight from toronto to san diego stopping in dtw. Top: the dependency tree of the sentence is annotated with latent semantic states by GUSP. For brevity, we omit the edge states. Raising occurs from flight to get and sinking occurs from get to diego. Bottom: the semantic tree is deterministically converted into SQL to obtain answer from the database. and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011) or example questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). In general, however, such supervision is not always available or easy to obtain. In contrast, databases are often abundantly available, especially for important domains. The database community has considerable amount of work on leveraging databases in various tasks such as entity resolution, schema matching, and others. To the best of our knowledge, this approach is still underexplored in the NLP community. One notable exception is distant supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012; Heck et al., 2013), which used database ins</context>
<context position="12095" citStr="Liang et al., 2011" startWordPosition="1847" endWordPosition="1850">d state space to account for semantic relations that are nonlocal in the dependency tree. GUSP’s approach of starting directly from dependency tree is inspired by USP. However, GUSP uses a different meaning representation defined over individual nodes and edges, rather than partitions, which enables linear-time exact inference. GUSP also handles complex linguistic phenomena and syntax-semantics mismatch by explicitly augmenting the state space, whereas USP’s capability in handling such phenomena is indirect and more limited. GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al., 2011). Their approach to semantic parsing, however, differs from GUSP in that it induced the semantic tree directly from a sentence, rather than starting from get flight from to E:flight V:city.name:C in dtw V:airport.code diego toronto V:city.name + E:flight V:city.name E:flight_stop san stopping 935 a dependency tree and annotating it. Their approach alleviates some complexity in the meaning representation for handling syntax-semantics mismatch, but it has to search over a much larger search space involving exponentially many candidate trees. This might partially explain why it has not yet been s</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the Forty Ninth Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Forty Seventh Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9382" citStr="Mintz et al., 2009" startWordPosition="1436" endWordPosition="1439"> and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011) or example questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). In general, however, such supervision is not always available or easy to obtain. In contrast, databases are often abundantly available, especially for important domains. The database community has considerable amount of work on leveraging databases in various tasks such as entity resolution, schema matching, and others. To the best of our knowledge, this approach is still underexplored in the NLP community. One notable exception is distant supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012; Heck et al., 2013), which used database instances to derive training examples for relation extraction. This approach, however, still has considerable limitations. For example, it only handles binary relations, and the quality of the training examples is inherently noisy and hard to control. Moreover, this approach is not applicable to the questionanswering setting considered in this paper, since entity pairs in questions need not correspond to valid relational instances in the database. 3 Grounded Unsupervised Sema</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Forty Seventh Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eighth International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>311--324</pages>
<publisher>Springer.</publisher>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="1097" citStr="Mooney, 2007" startWordPosition="156" endWordPosition="157">nsate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches. 1 Introduction Semantic parsing maps text to a formal meaning representation such as logical forms or structured queries. Recently, there has been a burgeoning interest in developing machine-learning approaches for semantic parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Mooney, 2007; Kwiatkowski et al., 2011), but the predominant paradigm uses supervised learning, which requires example annotations that are costly to obtain. More recently, several groundedlearning approaches have been proposed to alleviate the annotation burden (Chen and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011; Clarke et al., 2010; Liang et al., 2011). In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from questionanswer pairs alone, which represents a significant advance. However, although these methods exonerate annotators from mastering special</context>
<context position="4745" citStr="Mooney, 2007" startWordPosition="712" endWordPosition="713">o other standard datasets such as GEO and JOBS, ATIS features a database that is an order of magnitude larger in the numbers of relations and instances, as well as a more irregular language (ATIS questions were derived from spoken dialogs). Despite these challenges, GUSP attains an accuracy of 84% in end-to-end question answering, effectively tying with the stateof-the-art supervised approaches (85% by Zettlemoyer &amp; Collins (2007), 83% by Kwiatkowski et al. (2011)). 2 Background 2.1 Semantic Parsing The goal of semantic parsing is to map text to a complete and detailed meaning representation (Mooney, 2007). This is in contrast with semantic role labeling (Carreras and Marquez, 2004) and information extraction (Banko et al., 2007; Poon and Domingos, 2007), which have a more restricted goal of identifying local semantic roles or extracting selected information slots. The standard language for meaning representation is first-order logic or a sublanguage, such as FunQL (Kate et al., 2005; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). Poon &amp; Domingos (2009, 2010) induce a meaning representation by clustering synonymous lambda-calculus forms </context>
</contexts>
<marker>Mooney, 2007</marker>
<rawString>Raymond J. Mooney. 2007. Learning for semantic parsing. In Proceedings of the Eighth International Conference on Computational Linguistics and Intelligent Text Processing, pages 311–324, Mexico City, Mexico. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative loglinear grammars with latent variables.</title>
<date>2008</date>
<booktitle>In NIPS-08.</booktitle>
<contexts>
<context position="25397" citStr="Petrov and Klein, 2008" startWordPosition="4071" endWordPosition="4074">SING; Parent node state is same as the child node state; Otherwise, parent node state= ·, child node state= ·, edge state type= ·. Complexity Prior To favor simple semantic parses, GUSP imposes an exponential prior with weight β on nodes states that are not null or raised, and on each relational join in an edge state. 3.7 Learning and Inference Since the GUSP model factors over nodes and edges, learning and inference can be done efficiently using EM and dynamic programming. Specifically, the MAP parse and expectations can fi(d, z) · wi(d, z) 938 be computed by tree-Viterbi and inside-outside (Petrov and Klein, 2008). The parameters can be estimated by feature-rich EM (Berg-Kirkpatrick et al., 2010). Because the Viterbi and inside-outside are applied to a fixed tree (i.e., the input dependency tree), their running times are only linear in the sentence length in GUSP. 3.8 Query Generation Given a semantic parse, GUSP generates the SQL by a depth-first traversal that recursively computes the denotation of a node from the denotations of its children and its node state and edge states. Each denotation is a structured query that contains: a list of entities for projection (corresponding to the FROM statement i</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008. Discriminative loglinear grammars with latent variables. In NIPS-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint inference in information extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twenty Second National Conference on Artificial Intelligence,</booktitle>
<pages>913--918</pages>
<publisher>AAAI Press.</publisher>
<location>Vancouver, Canada.</location>
<contexts>
<context position="4896" citStr="Poon and Domingos, 2007" startWordPosition="734" endWordPosition="737">d instances, as well as a more irregular language (ATIS questions were derived from spoken dialogs). Despite these challenges, GUSP attains an accuracy of 84% in end-to-end question answering, effectively tying with the stateof-the-art supervised approaches (85% by Zettlemoyer &amp; Collins (2007), 83% by Kwiatkowski et al. (2011)). 2 Background 2.1 Semantic Parsing The goal of semantic parsing is to map text to a complete and detailed meaning representation (Mooney, 2007). This is in contrast with semantic role labeling (Carreras and Marquez, 2004) and information extraction (Banko et al., 2007; Poon and Domingos, 2007), which have a more restricted goal of identifying local semantic roles or extracting selected information slots. The standard language for meaning representation is first-order logic or a sublanguage, such as FunQL (Kate et al., 2005; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). Poon &amp; Domingos (2009, 2010) induce a meaning representation by clustering synonymous lambda-calculus forms stemming from partitions of dependency trees. More recently, Liang et al. (2011) proposed DCS for dependency-based compositional semantics, which repre</context>
</contexts>
<marker>Poon, Domingos, 2007</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2007. Joint inference in information extraction. In Proceedings of the Twenty Second National Conference on Artificial Intelligence, pages 913–918, Vancouver, Canada. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1817" citStr="Poon &amp; Domingos (2009" startWordPosition="264" endWordPosition="267">example annotations that are costly to obtain. More recently, several groundedlearning approaches have been proposed to alleviate the annotation burden (Chen and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011; Clarke et al., 2010; Liang et al., 2011). In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from questionanswer pairs alone, which represents a significant advance. However, although these methods exonerate annotators from mastering specialized logical forms, finding the answers for complex questions still requires non-trivial effort. 1 Poon &amp; Domingos (2009, 2010) proposed the USP system for unsupervised semantic parsing, which learns a parser by recursively clustering and composing synonymous expressions. While their approach completely obviates the need for direct supervision, their target logic forms are selfinduced clusters, which do not align with existing database or ontology. As a result, USP can not be used directly to answer complex questions against an existing database. More importantly, it misses the opportunity to leverage database for indirect supervision. In this paper, we present the GUSP system, which combines unsupervised seman</context>
<context position="5258" citStr="Poon &amp; Domingos (2009" startWordPosition="790" endWordPosition="794">The goal of semantic parsing is to map text to a complete and detailed meaning representation (Mooney, 2007). This is in contrast with semantic role labeling (Carreras and Marquez, 2004) and information extraction (Banko et al., 2007; Poon and Domingos, 2007), which have a more restricted goal of identifying local semantic roles or extracting selected information slots. The standard language for meaning representation is first-order logic or a sublanguage, such as FunQL (Kate et al., 2005; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). Poon &amp; Domingos (2009, 2010) induce a meaning representation by clustering synonymous lambda-calculus forms stemming from partitions of dependency trees. More recently, Liang et al. (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins. In this paper, we focus on semantic parsing for natural-language interface to database (Grosz et al., 1987). In this problem setting, a naturallanguage question is first translated into a meaning representation by semantic parsing</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1–10, Singapore. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised ontological induction from text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Forty Eighth Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>296--305</pages>
<publisher>ACL.</publisher>
<location>Uppsala,</location>
<marker>Poon, Domingos, 2010</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2010. Unsupervised ontological induction from text. In Proceedings of the Forty Eighth Annual Meeting of the Association for Computational Linguistics, pages 296– 305, Uppsala, Sweden. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
<author>Henry Kautz</author>
</authors>
<title>Towards a theory of natural language interfaces to databases.</title>
<date>2003</date>
<booktitle>In IUI-03.</booktitle>
<contexts>
<context position="6902" citStr="Popescu et al. (2003" startWordPosition="1042" endWordPosition="1045">using a hard EM-like procedure. Since USP uses nonlocal features (e.g., the argument-number feature) and operates over partitions, exact inference is intractable, and USP resorts to a greedy approach to find the MAP parse by searching over partitions. Titov &amp; Klementiev (2011) proposed a Bayesian version of USP and Titov &amp; Klementiev (2012) adapted it for semantic role induction. In USP, the meaning is represented by self-induced clusters. Therefore, to answer complex questions against a database, it requires an additional ontology matching step to resolve USP clusters with database elements. Popescu et al. (2003, 2004) proposed the PRECISE system, which does not require labeled examples and can be directly applied to question answering with a database. The PRECISE system, however, requires substantial amount of engineering, including a domain-specific lexicon that specifies the synonyms for names and values of database elements, a restricted set of potential interpretations for domain verbs and prepositions, as well as a set of domain questions with manually labeled POS tags for retraining the tagger and parser. It also focuses on the subset of easy questions (“semantically tractable” questions), and</context>
</contexts>
<marker>Popescu, Etzioni, Kautz, 2003</marker>
<rawString>Ana-Maria Popescu, Oren Etzioni, and Henry Kautz. 2003. Towards a theory of natural language interfaces to databases. In IUI-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Alex Armanasu</author>
<author>Oren Etzioni</author>
<author>David Ko</author>
<author>Alexander Yates</author>
</authors>
<title>Modern natural language interfaces to databases: Composing statistical parsing with semantic tractability.</title>
<date>2004</date>
<booktitle>In COLING-04.</booktitle>
<marker>Popescu, Armanasu, Etzioni, Ko, Yates, 2004</marker>
<rawString>Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko, and Alexander Yates. 2004. Modern natural language interfaces to databases: Composing statistical parsing with semantic tractability. In COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Pallavi Choudhury</author>
<author>Jianfeng Gao</author>
<author>Hisami Suzuki</author>
<author>Kristina Toutanova</author>
<author>Michael Gamon</author>
<author>Wentau Yih</author>
<author>Lucy Vanderwende</author>
</authors>
<title>MSR SPLAT, a language analysis toolkit.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL HLT 2012 Demonstration Session.</booktitle>
<contexts>
<context position="30293" citStr="Quirk et al., 2012" startWordPosition="4869" endWordPosition="4872">ntaining 4500, 478, and 449 sentences, respectively. We used the development set for initial development and tuning hyperparameters. At test time, we ran GUSP over the test set to learn a semantic parser and output the MAP parses.2 4.2 Preprocessing The ATIS sentences were originally derived from spoken dialog and were therefore in lower cases. Since case information is important for parsers and taggers, we first truecased the sentences using DASH (Pantel et al., 2009), which stores the case for each phrase in Wikipedia. We then ran the sentences through SPLAT, a state-of-the-art NLP toolkit (Quirk et al., 2012), to conduct tokenization, part-of-speech tagging, and constituency parsing. Since SPLAT does not output dependency trees, we ran the Stanford parser over SPLAT parses to generate the dependency trees in Stanford dependency (de Marneffe et al., 2006). 4.3 Systems For the GUSP system, we set the hyperparameters from initial experiments on the development set, and used them in all subsequent experiments. Specifically, we set α = 50 and β = −0.1, and ran three iterations of feature-rich EM with an L2 prior of 10 over the feature weights. To evaluate the importance of complex states, we considered</context>
</contexts>
<marker>Quirk, Choudhury, Gao, Suzuki, Toutanova, Gamon, Yih, Vanderwende, 2012</marker>
<rawString>Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami Suzuki, Kristina Toutanova, Michael Gamon, Wentau Yih, and Lucy Vanderwende. 2012. MSR SPLAT, a language analysis toolkit. In Proceedings of NAACL HLT 2012 Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Sixteen European Conference on Machine Learning.</booktitle>
<contexts>
<context position="9403" citStr="Riedel et al., 2010" startWordPosition="1440" endWordPosition="1443">im and Mooney, 2010; B¨orschinger et al., 2011) or example questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). In general, however, such supervision is not always available or easy to obtain. In contrast, databases are often abundantly available, especially for important domains. The database community has considerable amount of work on leveraging databases in various tasks such as entity resolution, schema matching, and others. To the best of our knowledge, this approach is still underexplored in the NLP community. One notable exception is distant supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012; Heck et al., 2013), which used database instances to derive training examples for relation extraction. This approach, however, still has considerable limitations. For example, it only handles binary relations, and the quality of the training examples is inherently noisy and hard to control. Moreover, this approach is not applicable to the questionanswering setting considered in this paper, since entity pairs in questions need not correspond to valid relational instances in the database. 3 Grounded Unsupervised Semantic Parsing In this </context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the Sixteen European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A bayesian model for unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Forty Ninth Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6559" citStr="Titov &amp; Klementiev (2011)" startWordPosition="985" endWordPosition="988">om the database. 2.2 Unsupervised Semantic Parsing Unsupervised semantic parsing was first proposed by Poon &amp; Domingos (2009, 2010) with their USP system. USP defines a probabilistic model over the dependency tree and semantic parse using Markov logic (Domingos and Lowd, 2009), and recursively clusters and composes synonymous dependency treelets using a hard EM-like procedure. Since USP uses nonlocal features (e.g., the argument-number feature) and operates over partitions, exact inference is intractable, and USP resorts to a greedy approach to find the MAP parse by searching over partitions. Titov &amp; Klementiev (2011) proposed a Bayesian version of USP and Titov &amp; Klementiev (2012) adapted it for semantic role induction. In USP, the meaning is represented by self-induced clusters. Therefore, to answer complex questions against a database, it requires an additional ontology matching step to resolve USP clusters with database elements. Popescu et al. (2003, 2004) proposed the PRECISE system, which does not require labeled examples and can be directly applied to question answering with a database. The PRECISE system, however, requires substantial amount of engineering, including a domain-specific lexicon that</context>
</contexts>
<marker>Titov, Klementiev, 2011</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2011. A bayesian model for unsupervised semantic parsing. In Proceedings of the Forty Ninth Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A bayesian approach to unsupervised semantic role induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6624" citStr="Titov &amp; Klementiev (2012)" startWordPosition="996" endWordPosition="999">emantic parsing was first proposed by Poon &amp; Domingos (2009, 2010) with their USP system. USP defines a probabilistic model over the dependency tree and semantic parse using Markov logic (Domingos and Lowd, 2009), and recursively clusters and composes synonymous dependency treelets using a hard EM-like procedure. Since USP uses nonlocal features (e.g., the argument-number feature) and operates over partitions, exact inference is intractable, and USP resorts to a greedy approach to find the MAP parse by searching over partitions. Titov &amp; Klementiev (2011) proposed a Bayesian version of USP and Titov &amp; Klementiev (2012) adapted it for semantic role induction. In USP, the meaning is represented by self-induced clusters. Therefore, to answer complex questions against a database, it requires an additional ontology matching step to resolve USP clusters with database elements. Popescu et al. (2003, 2004) proposed the PRECISE system, which does not require labeled examples and can be directly applied to question answering with a database. The PRECISE system, however, requires substantial amount of engineering, including a domain-specific lexicon that specifies the synonyms for names and values of database elements</context>
</contexts>
<marker>Titov, Klementiev, 2012</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2012. A bayesian approach to unsupervised semantic role induction. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammers.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twenty First Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>658--666</pages>
<publisher>AUAI Press.</publisher>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="1052" citStr="Zettlemoyer and Collins, 2005" startWordPosition="148" endWordPosition="151"> states, and learns a probabilistic grammar using EM. To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches. 1 Introduction Semantic parsing maps text to a formal meaning representation such as logical forms or structured queries. Recently, there has been a burgeoning interest in developing machine-learning approaches for semantic parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Mooney, 2007; Kwiatkowski et al., 2011), but the predominant paradigm uses supervised learning, which requires example annotations that are costly to obtain. More recently, several groundedlearning approaches have been proposed to alleviate the annotation burden (Chen and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011; Clarke et al., 2010; Liang et al., 2011). In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from questionanswer pairs alone, which represents a significant advance. However, although these method</context>
<context position="5203" citStr="Zettlemoyer and Collins, 2005" startWordPosition="782" endWordPosition="785"> Kwiatkowski et al. (2011)). 2 Background 2.1 Semantic Parsing The goal of semantic parsing is to map text to a complete and detailed meaning representation (Mooney, 2007). This is in contrast with semantic role labeling (Carreras and Marquez, 2004) and information extraction (Banko et al., 2007; Poon and Domingos, 2007), which have a more restricted goal of identifying local semantic roles or extracting selected information slots. The standard language for meaning representation is first-order logic or a sublanguage, such as FunQL (Kate et al., 2005; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). Poon &amp; Domingos (2009, 2010) induce a meaning representation by clustering synonymous lambda-calculus forms stemming from partitions of dependency trees. More recently, Liang et al. (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins. In this paper, we focus on semantic parsing for natural-language interface to database (Grosz et al., 1987). In this problem setting, a naturallanguage question is first trans</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammers. In Proceedings of the Twenty First Conference on Uncertainty in Artificial Intelligence, pages 658–666, Edinburgh, Scotland. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed ccg grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="1083" citStr="Zettlemoyer and Collins, 2007" startWordPosition="152" endWordPosition="155">stic grammar using EM. To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches. 1 Introduction Semantic parsing maps text to a formal meaning representation such as logical forms or structured queries. Recently, there has been a burgeoning interest in developing machine-learning approaches for semantic parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Mooney, 2007; Kwiatkowski et al., 2011), but the predominant paradigm uses supervised learning, which requires example annotations that are costly to obtain. More recently, several groundedlearning approaches have been proposed to alleviate the annotation burden (Chen and Mooney, 2008; Kim and Mooney, 2010; B¨orschinger et al., 2011; Clarke et al., 2010; Liang et al., 2011). In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from questionanswer pairs alone, which represents a significant advance. However, although these methods exonerate annotators from mas</context>
<context position="4120" citStr="Zettlemoyer and Collins, 2007" startWordPosition="607" endWordPosition="610">2010) and Liang et al. (2011) used the annotated logical forms to compute answers for their experiments. 933 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 933–943, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics by augmenting the state space to represent semantic relations beyond immediate dependency neighborhood. This representation also factorizes over nodes and edges, enabling linear-time exact inference in GUSP. We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). Compared to other standard datasets such as GEO and JOBS, ATIS features a database that is an order of magnitude larger in the numbers of relations and instances, as well as a more irregular language (ATIS questions were derived from spoken dialogs). Despite these challenges, GUSP attains an accuracy of 84% in end-to-end question answering, effectively tying with the stateof-the-art supervised approaches (85% by Zettlemoyer &amp; Collins (2007), 83% by Kwiatkowski et al. (2011)). 2 Background 2.1 Semantic Parsing The goal of semantic parsing is to map text to a complete and detailed meaning repr</context>
<context position="16353" citStr="Zettlemoyer and Collins (2007)" startWordPosition="2556" endWordPosition="2559">es. Edge states are assigned to dependency edges. GUSP enforces the constraints that the node states of the dependency parent and child must agree with the node states in the edge state. For example, E:flight-V:flight.departure time represents a natural join between the flight entity and the property value departure time. For a dependency edge e : a → b, the assignment to E:flight-V:flight.departure time signifies that a represents a flight entity, and b represents the value of its departure time. An edge state may also represent a relational path consisting of a serial of joins. For example, Zettlemoyer and Collins (2007) used a predicate from(f,c) to signify that flight f starts from city c. In the ATIS database, however, this amounts to a path of three joins: flight.from airport-airport airport-airport service airport service-city In GUSP, this is represented by the edge state flight-flight.from airport-airport-airport service-city. � PB(d, z) log z 936 GUSP only creates edge states for relational join paths up to length four, as longer paths rarely correspond to meaningful semantic relations. Composition To handle compositions such as American Airlines and New York City, it helps to distinguish the head wor</context>
<context position="4566" citStr="Zettlemoyer &amp; Collins (2007)" startWordPosition="680" endWordPosition="684">nd edges, enabling linear-time exact inference in GUSP. We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). Compared to other standard datasets such as GEO and JOBS, ATIS features a database that is an order of magnitude larger in the numbers of relations and instances, as well as a more irregular language (ATIS questions were derived from spoken dialogs). Despite these challenges, GUSP attains an accuracy of 84% in end-to-end question answering, effectively tying with the stateof-the-art supervised approaches (85% by Zettlemoyer &amp; Collins (2007), 83% by Kwiatkowski et al. (2011)). 2 Background 2.1 Semantic Parsing The goal of semantic parsing is to map text to a complete and detailed meaning representation (Mooney, 2007). This is in contrast with semantic role labeling (Carreras and Marquez, 2004) and information extraction (Banko et al., 2007; Poon and Domingos, 2007), which have a more restricted goal of identifying local semantic roles or extracting selected information slots. The standard language for meaning representation is first-order logic or a sublanguage, such as FunQL (Kate et al., 2005; Clarke et al., 2010) and lambda ca</context>
<context position="29190" citStr="Zettlemoyer &amp; Collins (2007)" startWordPosition="4691" endWordPosition="4694">quantifiers until the child semantic object differs from the parent one or when reaching the root. GUSP employs the following fixed ordering in evaluating quantifiers and operators: superlatives and other quantifiers are evaluated at last (i.e., after evaluating all other joins or operators for the given object), whereas negation is evaluated first, conjunctions and disjunctions are evaluated in their order of appearance. 4 Experiments 4.1 Task We evaluated GUSP on the ATIS travel planning domain, which has been studied in He &amp; Young (2005, 2006) and adapted for evaluating semantic parsing by Zettlemoyer &amp; Collins (2007) (henceforth ZC07). The ZC07 dataset contains annotated logical forms for each sentence, which we do not use. Since our goal is not to produce a specific logical form, we directly evaluate on the end-to-end task of translating questions into database queries and measure question-answering accuracy. The ATIS distrbution contains the original SQL annotations, which we used to compute gold answers 939 for evaluation only. The dataset is split into training, development, and test, containing 4500, 478, and 449 sentences, respectively. We used the development set for initial development and tuning </context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2007. Online learning of relaxed ccg grammars for parsing to logical form. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>