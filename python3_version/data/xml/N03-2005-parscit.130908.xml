<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000602">
<title confidence="0.839777">
Story Link Detection and New Event Detection are Asymmetric
</title>
<author confidence="0.783394">
Francine Chen
</author>
<affiliation confidence="0.663806">
PARC
</affiliation>
<address confidence="0.8439345">
3333 Coyote Hill Rd
Palo Alto, CA 94304
</address>
<email confidence="0.99755">
fchen@parc.com
</email>
<note confidence="0.453245">
Ayman Farahat
PARC
</note>
<address confidence="0.8452275">
3333 Coyote Hill Rd
Palo Alto, CA 94304
</address>
<email confidence="0.997962">
farahat@parc.com
</email>
<author confidence="0.889525">
Thorsten Brants
</author>
<affiliation confidence="0.750373">
PARC
</affiliation>
<address confidence="0.8451045">
3333 Coyote Hill Rd
Palo Alto, CA 94304
</address>
<email confidence="0.998508">
thorsten@brants.net
</email>
<sectionHeader confidence="0.995627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968666666667">
Story link detection has been regarded as a
core technology for other Topic Detection and
Tracking tasks such as new event detection. In
this paper we analyze story link detection and
new event detection in a retrieval framework
and examine the effect of a number of tech-
niques, including part of speech tagging, new
similarity measures, and an expanded stop list,
on the performance of the two detection tasks.
We present experimental results that show that
the utility of the techniques on the two tasks
differs, as is consistent with our analysis.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996917826086956">
Topic Detection and Tracking (TDT) research is spon-
sored by the DARPA TIDES program. The research has
five tasks related to organizing streams of data such as
newswire and broadcast news (Wayne, 2000). A link
detection (LNK) system detects whether two stories are
“linked”, or discuss the same event. A story about a plane
crash and another story about the funeral of the crash vic-
tims are considered to be linked. In contrast, a story about
hurricane Andrew and a story about hurricane Agnes are
not linked because they are two different events. A new
event detection (NED) system detects when a story dis-
cusses a previously unseen event. Link detection is con-
sidered to be a core technology for new event detection
and the other tasks.
Several groups are performing research on the TDT
tasks of link detection and new event detection (e.g.,
(Carbonell et al., 2001) (Allan et al., 2000)). In this pa-
per, we compare the link detection and new event detec-
tion tasks in an information retrieval framework, examin-
ing the criteria for improving a NED system based on a
LNK system, and give specific directions for improving
each system separately. We also investigate the utility of
a number of techniques for improving the systems.
</bodyText>
<sectionHeader confidence="0.820009" genericHeader="method">
2 Common Processing and Models
</sectionHeader>
<bodyText confidence="0.97241168">
The Link Detection and New Event Detection systems
that we developed for TDT2002 share many process-
ing steps in common. This includes preprocessing
to tokenize the data, recognize abbreviations, normal-
ize abbreviations, remove stop-words, replace spelled-
out numbers by digits, add part-of--speech tags, replace
the tokens by their stems, and then generating term-
frequency vectors. Document frequency counts are in-
crementally updated as new sources of stories are pre-
sented to the system. Additionally, separate source-
specific counts are used, so that, for example, the
term frequencies for the New York Times are com-
puted separately from stories from CNN. The source-
specific, incremental, document frequency counts are
used to compute a TF-IDF term vector for each story.
Stories are compared using either the cosine distance
or Hellinger
distance for
terms in documents and . To help compensate for
stylistic differences between various sources, e.g., news
paper vs. broadcast news, translation errors, and auto-
matic speech recognition errors (Allan et al., 1999), we
subtract the average observed similarity values, in similar
spirit to the use of thresholds conditioned on the sources
(Carbonell et al., 2001)
</bodyText>
<sectionHeader confidence="0.988064" genericHeader="method">
3 New Event Detection
</sectionHeader>
<bodyText confidence="0.8629624375">
In order to decide whether a new document describes a
new event, it is compared to all previous documents and
the document with highest similarity is identified. If
the score exceeds a thresh-
LNK − Hellinger vs. Cosine
old , then there is no sufficiently similar previous doc- 1
ument, and is classified as a new event. 0.9
0.8
4 Link Detection
0.7
In order to decide whether a pair of stories and
are linked, we compute the similarity between the two
documents using the cosine and Hellinger metrics. The
similarity metrics are combined using a support vector
machine and the margin is used as a confidence measure
that is thresholded.
</bodyText>
<figure confidence="0.9914506">
0.2
0.6
CDF 0.5
0.4
0.3
</figure>
<bodyText confidence="0.55753675">
on cos
off cos
on hell
off hell
</bodyText>
<sectionHeader confidence="0.941094" genericHeader="method">
5 Evaluation Metric
</sectionHeader>
<bodyText confidence="0.9967364">
TDT system evaluation is based on the number of false
alarms and misses produced by a system. In link detec-
tion, the system should detect linked story pairs; in new
event detection, the system should detect new stories. A
detection cost
</bodyText>
<equation confidence="0.884454">
(1)
</equation>
<bodyText confidence="0.993624454545455">
is computed where the costs and are set to 1
and 0.1, respectively. and are the computed
miss and false alarm probabilities. and are
the a priori target and non-target probabilities, set to 0.02
and 0.98, respectively. The detection cost is normalized
by dividing by min so that a
perfect system scores 0, and a random baseline scores 1.
Equal weight is given to each topic by accumulating error
probabilities separately for each topic and then averaged.
The minimum detection cost is the decision cost when the
decision threshold is set to the optimal confidence score.
</bodyText>
<sectionHeader confidence="0.955661" genericHeader="method">
6 Differences between LNK and NED
</sectionHeader>
<bodyText confidence="0.999831428571429">
The conditions for false alarms and misses are reversed
for the LNK and NED tasks. In the LNK task, incor-
rectly flagging two stories as being on the same event is
considered a false alarm. In contrast, in the NED task, in-
correctly flagging two stories as being on the same event
will cause a true first story to be missed. Conversely, in-
correctly labeling two stories that are on the same event
as not linked is a miss, but for the NED task, incorrectly
labeling two stories on the same event as not linked may
result in a false alarm.
In this section, we analyze the utility of a number of
techniques for the LNK and NED tasks in an information
retrieval framework. The detection cost in Eqn. 1 assigns
a higher cost to false alarms since
and . A LNK system should
minimize false alarms by identifying only linked stories,
which results in high precision for LNK. In contrast, a
NED system will minimize false alarms by identifying all
stories that are linked, which translates to high recall for
LNK. Based on this observation, we investigated a num-
ber of precision and recall enhancing techniques for the
</bodyText>
<figure confidence="0.980769666666667">
0.1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
score
</figure>
<figureCaption confidence="0.98036025">
Figure 1: CDF for cosine and Hellinger similarity on the
LNK task for on-topic and off-topic pairs.
Figure 2: CDF for cosine and Hellinger similarity on the
NED task for on-topic and off-topic pairs.
</figureCaption>
<bodyText confidence="0.999438">
LNK and NED systems, namely, part-of-speech tagging,
an expanded stoplist, and normalizing abbreviations and
transforming spelled out numbers into numbers. We also
investigated the use of different similarity measures.
</bodyText>
<subsectionHeader confidence="0.995142">
6.1 Similarity Measures
</subsectionHeader>
<bodyText confidence="0.9538129">
The systems developed for TDT primarily use cosine
similarity as the similarity measure. In work on text seg-
mentation (Brants et al., 2002), better performance was
observed with the Hellinger measure. Table 1 shows
that for LNK, the system based on cosine similarity per-
formed better; in contrast, for NED, the system based on
Hellinger similarity performed better.
The LNK task requires high precision, which corre-
sponds to a large separation between the on-topic and
off-topic distributions, as shown for the cosine metric in
</bodyText>
<figureCaption confidence="0.971398">
Figure 1. The NED task requires high recall (low CDF
</figureCaption>
<figure confidence="0.9981599">
0
NED − Hellinger vs. Cosine
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
Hellinger on−topic
Hellinger off−topic
cosine on−topic
cosine off−topic
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Similarity
CDF(Similarity)
0.2
0.1
</figure>
<tableCaption confidence="0.782024333333333">
Table 1: Effect of different similarity measures on topic-
weighted minimum normalized detection costs on the
TDT 2002 dry run data.
</tableCaption>
<table confidence="0.998809666666667">
System Cosine Hellinger Change(%)
LNK 0.3180 0.3777 -0.0597(-18.8)
NED 0.7059 0.5873 +0.1186(+16.3)
</table>
<tableCaption confidence="0.998351">
Table 2: Effect of using part-of-speech on minimum nor-
malized detection costs on the TDT 2002 dry run data.
</tableCaption>
<table confidence="0.987102">
System PoS PoS Change (%)
LNK 0.3180 0.3334 -0.0154 ( %)
NED 0.6403 0.5873 +0.0530 ( %)
</table>
<bodyText confidence="0.997187833333333">
values for on-topic). Figure 2, which is based on pairs
that contain the current story and its most similar story in
the story history, shows a greater separation in this region
with the Hellinger metric. For example, at 10% recall, the
Hellinger metric has 71% false alarm rate as compared to
75% for the cosine metric.
</bodyText>
<subsectionHeader confidence="0.999732">
6.2 Part-of-Speech (PoS) Tagging
</subsectionHeader>
<bodyText confidence="0.999993625">
To reduce confusion among some word senses, we tagged
the terms as one of five categories: adjective, noun,
proper nouns, verb, or other, and then combined the stem
and part-of-speech to create a “tagged term”. For exam-
ple, ‘N train’ represents the term ‘train’ when used as a
noun. The LNK and NED systems were tested using the
tagged terms. Table 2 shows the opposite effect PoS tag-
ging has on LNK and NED.
</bodyText>
<subsectionHeader confidence="0.99478">
6.3 Stop Words
</subsectionHeader>
<bodyText confidence="0.999694166666667">
The broadcast news documents in the TDT collection
have been transcribed using Automatic Speech Recog-
nition (ASR). There are systematic differences between
ASR and manually transcribed text. For example “30”
will be spelled out as “thirty” and ‘CNN” is represented
as three separate tokens “C”, “N”, and “N”. To handle
these differences, an “ASR stoplist” was created by iden-
tifying terms with statistically different distributions in a
parallel corpus of manually and automatically transcribed
documents, the TDT2 corpus. Table 3 shows that use of
an ASR stoplist on the topic-weighted minimum detec-
tion costs improves results for LNK but not for NED.
We also performed “enhanced preprocessing” to nor-
malize abbreviations and transform spelled-out numbers
into numerals, which improves both precision and re-
call. Table 3 shows that enhanced preprocessing exhibits
worse performance than the ASR stoplist for Link Detec-
tion, but yields best results for New Event Detection.
</bodyText>
<tableCaption confidence="0.897675666666667">
Table 3: Effect of using an “ASR stoplist” and “enhanced
preprocessing” for handling ASR differences on the TDT
2001 evaluation data.
</tableCaption>
<table confidence="0.99896925">
ASRstop No Yes No
Preproc Std Std Enh
LNK 0.312 0.299 (+4.4%) 0.301 (+3.3%)
NED 0.606 0.641 (-5.5%) 0.587 (+3.1%)
</table>
<sectionHeader confidence="0.9712" genericHeader="evaluation">
7 Summary and Conclusions
</sectionHeader>
<bodyText confidence="0.999985846153846">
We have presented a comparison of story link detection
and new event detection in a retrieval framework, show-
ing that the two tasks are asymmetric in the optimiza-
tion of precision and recall. We performed experiments
comparing the effect of several techniques on the perfor-
mance of LNK and NED systems. Although many of the
processing techniques used by our systems are the same,
the results of our experiments indicate that some tech-
niques affect the performance of LNK and NED systems
differently. These differences may be due in part to the
asymmetry in the tasks and the corresponding differences
in whether improving precision or recall for the link task
is more important.
</bodyText>
<sectionHeader confidence="0.998763" genericHeader="conclusions">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99847">
We thank James Allan of UMass for suggesting that pre-
cision and recall may partially explain the asymmetry of
LNK and NED.
</bodyText>
<sectionHeader confidence="0.999227" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997288631578947">
James Allan, Hubert Jin, Martin Rajman, Charles Wayne,
Dan Gildea, Victor Lavrenko, Rose Hoberman, and
David Caputo. 1999. Topic-based novelty detection.
Summer workshop final report, Center for Language
and Speech Processing, Johns Hopkins University.
James Allan, Victor Lavrenko, and Hubert Jin. 2000.
First story detection in TDT is hard. In CIKM, pages
374–381.
Thorsten Brants, Francine Chen, and Ioannis Tsochan-
taridis. 2002. Topic-based document segmentation
with probabilistic latent semantic analysis. In CIKM,
pages 211–218, McLean, VA.
Jaime Carbonell, Yiming Yang, Ralf Brown, Chun Jin,
and Jian Zhang. 2001. Cmu tdt report. Slides at the
TDT-2001 meeting, CMU.
Charles Wayne. 2000. Multilingual topic detection
and tracking: Successful research enabled by corpora
and evaluation. In LREC, pages 1487–1494, Athens,
Greece.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.063582">
<title confidence="0.734385">Story Link Detection and New Event Detection are Asymmetric Francine</title>
<address confidence="0.806264">3333 Coyote Hill Palo Alto, CA</address>
<email confidence="0.999423">fchen@parc.com</email>
<affiliation confidence="0.423955">Ayman</affiliation>
<address confidence="0.844497">3333 Coyote Hill Palo Alto, CA</address>
<email confidence="0.999614">farahat@parc.com</email>
<author confidence="0.942745">Thorsten</author>
<address confidence="0.7769225">3333 Coyote Hill Palo Alto, CA</address>
<email confidence="0.968088">thorsten@brants.net</email>
<abstract confidence="0.998898230769231">Story link detection has been regarded as a core technology for other Topic Detection and Tracking tasks such as new event detection. In this paper we analyze story link detection and new event detection in a retrieval framework and examine the effect of a number of techniques, including part of speech tagging, new similarity measures, and an expanded stop list, on the performance of the two detection tasks. We present experimental results that show that the utility of the techniques on the two tasks differs, as is consistent with our analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Hubert Jin</author>
<author>Martin Rajman</author>
<author>Charles Wayne</author>
<author>Dan Gildea</author>
<author>Victor Lavrenko</author>
<author>Rose Hoberman</author>
<author>David Caputo</author>
</authors>
<title>Topic-based novelty detection. Summer workshop final report,</title>
<date>1999</date>
<institution>Center for Language and Speech Processing, Johns Hopkins University.</institution>
<contexts>
<context position="3191" citStr="Allan et al., 1999" startWordPosition="512" endWordPosition="515">es of stories are presented to the system. Additionally, separate sourcespecific counts are used, so that, for example, the term frequencies for the New York Times are computed separately from stories from CNN. The sourcespecific, incremental, document frequency counts are used to compute a TF-IDF term vector for each story. Stories are compared using either the cosine distance or Hellinger distance for terms in documents and . To help compensate for stylistic differences between various sources, e.g., news paper vs. broadcast news, translation errors, and automatic speech recognition errors (Allan et al., 1999), we subtract the average observed similarity values, in similar spirit to the use of thresholds conditioned on the sources (Carbonell et al., 2001) 3 New Event Detection In order to decide whether a new document describes a new event, it is compared to all previous documents and the document with highest similarity is identified. If the score exceeds a threshLNK − Hellinger vs. Cosine old , then there is no sufficiently similar previous doc- 1 ument, and is classified as a new event. 0.9 0.8 4 Link Detection 0.7 In order to decide whether a pair of stories and are linked, we compute the simil</context>
</contexts>
<marker>Allan, Jin, Rajman, Wayne, Gildea, Lavrenko, Hoberman, Caputo, 1999</marker>
<rawString>James Allan, Hubert Jin, Martin Rajman, Charles Wayne, Dan Gildea, Victor Lavrenko, Rose Hoberman, and David Caputo. 1999. Topic-based novelty detection. Summer workshop final report, Center for Language and Speech Processing, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Victor Lavrenko</author>
<author>Hubert Jin</author>
</authors>
<title>First story detection in TDT is hard.</title>
<date>2000</date>
<booktitle>In CIKM,</booktitle>
<pages>374--381</pages>
<contexts>
<context position="1754" citStr="Allan et al., 2000" startWordPosition="288" endWordPosition="291">ed”, or discuss the same event. A story about a plane crash and another story about the funeral of the crash victims are considered to be linked. In contrast, a story about hurricane Andrew and a story about hurricane Agnes are not linked because they are two different events. A new event detection (NED) system detects when a story discusses a previously unseen event. Link detection is considered to be a core technology for new event detection and the other tasks. Several groups are performing research on the TDT tasks of link detection and new event detection (e.g., (Carbonell et al., 2001) (Allan et al., 2000)). In this paper, we compare the link detection and new event detection tasks in an information retrieval framework, examining the criteria for improving a NED system based on a LNK system, and give specific directions for improving each system separately. We also investigate the utility of a number of techniques for improving the systems. 2 Common Processing and Models The Link Detection and New Event Detection systems that we developed for TDT2002 share many processing steps in common. This includes preprocessing to tokenize the data, recognize abbreviations, normalize abbreviations, remove </context>
</contexts>
<marker>Allan, Lavrenko, Jin, 2000</marker>
<rawString>James Allan, Victor Lavrenko, and Hubert Jin. 2000. First story detection in TDT is hard. In CIKM, pages 374–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Francine Chen</author>
<author>Ioannis Tsochantaridis</author>
</authors>
<title>Topic-based document segmentation with probabilistic latent semantic analysis.</title>
<date>2002</date>
<booktitle>In CIKM,</booktitle>
<pages>211--218</pages>
<location>McLean, VA.</location>
<contexts>
<context position="6660" citStr="Brants et al., 2002" startWordPosition="1112" endWordPosition="1115">2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 score Figure 1: CDF for cosine and Hellinger similarity on the LNK task for on-topic and off-topic pairs. Figure 2: CDF for cosine and Hellinger similarity on the NED task for on-topic and off-topic pairs. LNK and NED systems, namely, part-of-speech tagging, an expanded stoplist, and normalizing abbreviations and transforming spelled out numbers into numbers. We also investigated the use of different similarity measures. 6.1 Similarity Measures The systems developed for TDT primarily use cosine similarity as the similarity measure. In work on text segmentation (Brants et al., 2002), better performance was observed with the Hellinger measure. Table 1 shows that for LNK, the system based on cosine similarity performed better; in contrast, for NED, the system based on Hellinger similarity performed better. The LNK task requires high precision, which corresponds to a large separation between the on-topic and off-topic distributions, as shown for the cosine metric in Figure 1. The NED task requires high recall (low CDF 0 NED − Hellinger vs. Cosine 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 Hellinger on−topic Hellinger off−topic cosine on−topic cosine off−topic 0 0 0.1 0.2 0.3 0.4 0.5 0.6</context>
</contexts>
<marker>Brants, Chen, Tsochantaridis, 2002</marker>
<rawString>Thorsten Brants, Francine Chen, and Ioannis Tsochantaridis. 2002. Topic-based document segmentation with probabilistic latent semantic analysis. In CIKM, pages 211–218, McLean, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Yiming Yang</author>
<author>Ralf Brown</author>
<author>Chun Jin</author>
<author>Jian Zhang</author>
</authors>
<date>2001</date>
<booktitle>Cmu tdt report. Slides at the TDT-2001 meeting, CMU.</booktitle>
<contexts>
<context position="1733" citStr="Carbonell et al., 2001" startWordPosition="284" endWordPosition="287">her two stories are “linked”, or discuss the same event. A story about a plane crash and another story about the funeral of the crash victims are considered to be linked. In contrast, a story about hurricane Andrew and a story about hurricane Agnes are not linked because they are two different events. A new event detection (NED) system detects when a story discusses a previously unseen event. Link detection is considered to be a core technology for new event detection and the other tasks. Several groups are performing research on the TDT tasks of link detection and new event detection (e.g., (Carbonell et al., 2001) (Allan et al., 2000)). In this paper, we compare the link detection and new event detection tasks in an information retrieval framework, examining the criteria for improving a NED system based on a LNK system, and give specific directions for improving each system separately. We also investigate the utility of a number of techniques for improving the systems. 2 Common Processing and Models The Link Detection and New Event Detection systems that we developed for TDT2002 share many processing steps in common. This includes preprocessing to tokenize the data, recognize abbreviations, normalize a</context>
<context position="3339" citStr="Carbonell et al., 2001" startWordPosition="535" endWordPosition="538"> the New York Times are computed separately from stories from CNN. The sourcespecific, incremental, document frequency counts are used to compute a TF-IDF term vector for each story. Stories are compared using either the cosine distance or Hellinger distance for terms in documents and . To help compensate for stylistic differences between various sources, e.g., news paper vs. broadcast news, translation errors, and automatic speech recognition errors (Allan et al., 1999), we subtract the average observed similarity values, in similar spirit to the use of thresholds conditioned on the sources (Carbonell et al., 2001) 3 New Event Detection In order to decide whether a new document describes a new event, it is compared to all previous documents and the document with highest similarity is identified. If the score exceeds a threshLNK − Hellinger vs. Cosine old , then there is no sufficiently similar previous doc- 1 ument, and is classified as a new event. 0.9 0.8 4 Link Detection 0.7 In order to decide whether a pair of stories and are linked, we compute the similarity between the two documents using the cosine and Hellinger metrics. The similarity metrics are combined using a support vector machine and the m</context>
</contexts>
<marker>Carbonell, Yang, Brown, Jin, Zhang, 2001</marker>
<rawString>Jaime Carbonell, Yiming Yang, Ralf Brown, Chun Jin, and Jian Zhang. 2001. Cmu tdt report. Slides at the TDT-2001 meeting, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Wayne</author>
</authors>
<title>Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.</title>
<date>2000</date>
<booktitle>In LREC,</booktitle>
<pages>1487--1494</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="1066" citStr="Wayne, 2000" startWordPosition="171" endWordPosition="172">lyze story link detection and new event detection in a retrieval framework and examine the effect of a number of techniques, including part of speech tagging, new similarity measures, and an expanded stop list, on the performance of the two detection tasks. We present experimental results that show that the utility of the techniques on the two tasks differs, as is consistent with our analysis. 1 Introduction Topic Detection and Tracking (TDT) research is sponsored by the DARPA TIDES program. The research has five tasks related to organizing streams of data such as newswire and broadcast news (Wayne, 2000). A link detection (LNK) system detects whether two stories are “linked”, or discuss the same event. A story about a plane crash and another story about the funeral of the crash victims are considered to be linked. In contrast, a story about hurricane Andrew and a story about hurricane Agnes are not linked because they are two different events. A new event detection (NED) system detects when a story discusses a previously unseen event. Link detection is considered to be a core technology for new event detection and the other tasks. Several groups are performing research on the TDT tasks of lin</context>
</contexts>
<marker>Wayne, 2000</marker>
<rawString>Charles Wayne. 2000. Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation. In LREC, pages 1487–1494, Athens, Greece.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>