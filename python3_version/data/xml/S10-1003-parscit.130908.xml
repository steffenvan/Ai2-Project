<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003794">
<title confidence="0.6889765">
SemEval-2010 Task 3: Cross-Lingual Word Sense
Disambiguation
</title>
<note confidence="0.649883666666667">
Els Lefever1,2 and Veronique Hoste1,2
1 LT3, Language and Translation Technology Team, University College Ghent, Belgium
2Department of Applied Mathematics and Computer Science, Ghent University, Belgium
</note>
<email confidence="0.989745">
{Els.Lefever,Veronique.Hoste}@hogent.be
</email>
<sectionHeader confidence="0.987102" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999765">
The goal of this task is to evaluate
the feasibility of multilingual WSD on
a newly developed multilingual lexi-
cal sample data set. Participants were
asked to automatically determine the
contextually appropriate translation of
a given English noun in five languages,
viz. Dutch, German, Italian, Spanish
and French. This paper reports on the
sixteen submissions from the five dif-
ferent participating teams.
</bodyText>
<sectionHeader confidence="0.993323" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999684436363636">
Word Sense Disambiguation, the task of se-
lecting the correct sense of an ambiguous word
in a given context, is a well-researched NLP
problem (see for example Agirre and Edmonds
(2006) and Navigli (2009)), largely boosted
by the various Senseval and SemEval editions.
The SemEval-2010 Cross-lingual Word Sense
Disambiguation task focuses on two bottle-
necks in current WSD research, namely the
scarcity of sense inventories and sense-tagged
corpora (especially for languages other than
English) and the growing tendency to eval-
uate the performance of WSD systems in a
real application such as machine translation
and cross-language information retrieval (see
for example Agirre et al. (2007)).
The Cross-lingual WSD task aims at the de-
velopment of a multilingual data set to test the
feasibility of multilingual WSD. Many studies
have already shown the validity of this cross-
lingual evidence idea (Gale et al., 1993; Ide et
al., 2002; Ng et al., 2003; Apidianaki, 2009),
but until now no benchmark data sets have
been available. For the SemEval-2010 compe-
tition we developed (i) a sense inventory in
which the sense distinctions were extracted
from the multilingual corpus Europarli and
(ii) a data set in which the ambiguous words
were annotated with the senses from the mul-
tilingual sense inventory. The Cross-Lingual
WSD task is a lexical sample task for English
nouns, in which the word senses are made up of
the translations in five languages, viz. Dutch,
French, Italian, Spanish and German. Both
the sense inventory and the annotated data
set were constructed for a sample of 25 nouns.
The data set was divided into a trial set of 5
ambiguous nouns and a test set of 20 nouns.
The participants had to automatically deter-
mine the contextually appropriate translation
for a given English noun in each or a subset
of the five target languages. Only translations
present in Europarl were considered as valid
translations.
The remainder of this article is organized as
follows. Section 2 focuses on the task descrip-
tion and gives a short overview of the construc-
tion of the sense inventory and the annotation
of the benchmark data set with the senses from
the multilingual sense inventory. Section 3
clarifies the scoring metrics and presents two
frequency-based baselines. The participating
systems are presented in Section 4, while the
results of the task are discussed in Section 5.
Section 6 concludes this paper.
</bodyText>
<sectionHeader confidence="0.940323" genericHeader="method">
2 Task setup
</sectionHeader>
<subsectionHeader confidence="0.983739">
2.1 Data sets
</subsectionHeader>
<bodyText confidence="0.999716857142857">
Two types of data sets were used in the
Cross-lingual WSD task: (a) a parallel corpus
on the basis of which the gold standard sense
inventory was created and (b) a collection of
English sentences containing the lexical sam-
ple words annotated with their contextually
appropriate translations in five languages.
</bodyText>
<footnote confidence="0.389306">
lhttp://www.statmt.org/europarl/
</footnote>
<page confidence="0.960976">
15
</page>
<bodyText confidence="0.966170730769231">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 15–20,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
Below, we provide a short summary of the
complete data construction process. For a
more detailed description, we refer to Lefever
and Hoste (2009; 2010).
The gold standard sense inventory was
derived from the Europarl parallel corpus2,
which is extracted from the proceedings of the
European Parliament (Koehn, 2005). We se-
lected 6 languages from the 11 European lan-
guages represented in the corpus, viz. English
(our target language), Dutch, French, Ger-
man, Italian and Spanish. All data were al-
ready sentence-aligned using a tool based on
the Gale and Church (1991) algorithm, which
was part of the Europarl corpus. We only con-
sidered the 1-1 sentence alignments between
English and the five other languages. These
sentence alignments were made available to
the task participants for the five trial words.
The sense inventory extracted from the paral-
lel data set (Section 2.2) was used to annotate
the sentences in the trial set and the test set,
which were extracted from the JRC-ACQUIS
Multilingual Parallel Corpus3 and BNC4.
</bodyText>
<subsectionHeader confidence="0.998857">
2.2 Creation of the sense inventory
</subsectionHeader>
<bodyText confidence="0.999280727272727">
Two steps were taken to obtain a multilingual
sense inventory: (1) word alignment on the
sentences to find the set of possible transla-
tions for the set of ambiguous nouns and (2)
clustering by meaning (per target word) of the
resulting translations.
GIZA++ (Och and Ney, 2003) was used to
generate the initial word alignments, which
were manually verified by certified translators
in all six involved languages. The human an-
notators were asked to assign a “NULL” link
to words for which no valid translation could
be identified. Furthermore, they were also
asked to provide extra information on com-
pound translations (e.g. the Dutch word In-
vesteringsbank as a translation of the English
multiword Investment Bank), fuzzy links, or
target words with a different PoS (e.g. the verb
to bank).
The manually verified translations were
clustered by meaning by one annotator. In
order to do so, the translations were linked
</bodyText>
<footnote confidence="0.999937">
2http://www.statmt.org/europarl/
3http://wt.jrc.it/lt/Acquis/
4http://www.natcorp.ox.ac.uk/
</footnote>
<bodyText confidence="0.997935647058824">
across languages on the basis of unique
sentence IDs. After the selection of all
unique translation combinations, the transla-
tions were grouped into clusters. The clus-
ters were organized in two levels, in which
the top level reflects the main sense categories
(e.g. for the word coach we have (1) (sports)
manager, (2) bus, (3) carriage and (4) part
of a train), and the subclusters represent the
finer sense distinctions. Translations that cor-
respond to English multiword units were iden-
tified and in case of non-apparent compounds,
i.e. compounds which are not marked with a
“-”, the different compound parts were sepa-
rated by §§ in the clustering file (e.g. the Ger-
man Post§§kutsche). All clustered translations
were also manually lemmatized.
</bodyText>
<subsectionHeader confidence="0.992658">
2.3 Sense annotation of the test data
</subsectionHeader>
<bodyText confidence="0.999878764705882">
The resulting sense inventory was used to an-
notate the sentences in the trial set (20 sen-
tences per ambiguous word) and the test set
(50 sentences per ambiguous word). In total,
1100 sentences were annotated. The annota-
tors were asked to (a) pick the contextually ap-
propriate sense cluster and to (b) choose their
three preferred translations from this cluster.
In case they were not able to find three ap-
propriate translations, they were also allowed
to provide fewer. These potentially differ-
ent translations were used to assign frequency
weights (shown in example (2)) to the gold
standard translations per sentence. The ex-
ample (1) below shows the annotation result in
both German and Dutch for an English source
sentence containing coach.
</bodyText>
<figureCaption confidence="0.691223">
(1) SENTENCE 12. STRANGELY , the na-
tional coach of the Irish teams down the
years has had little direct contact with the
four provincial coaches.
German 1: Nationaltrainer
German 2: Trainer
German 3: Coach
Dutch 1: trainer
Dutch 2: coach
Dutch 3: voetbaltrainer
</figureCaption>
<bodyText confidence="0.999643">
For each instance, the gold standard that
results from the manual annotation contains
a set of translations that are enriched with
</bodyText>
<page confidence="0.98319">
16
</page>
<bodyText confidence="0.999888083333333">
frequency information. The format of both
the input file and gold standard is similar to
the format that will be used for the Sem-
Eval Cross-Lingual Lexical Substitution task
(Sinha and Mihalcea, 2009). The following
example illustrates the six-language gold stan-
dard format for the trial sentence in (1). The
first field contains the target word, PoS-tag
and language code, the second field contains
the sentence ID and the third field contains the
gold standard translations in the target lan-
guage, enriched with their frequency weight:
</bodyText>
<construct confidence="0.989883571428572">
(2) coach.n.nl 12 :: coach 3; speler-trainer 1;
trainer 3; voetbaltrainer 1;
coach.n.fr 12 :: capitaine 1; entraineur 3;
coach.n.de 12 :: Coach 1; FuBbaltrainer 1;
Nationaltrainer 2; Trainer 3;
coach.n.it 12 :: allenatore 3;
coach.n.es 12 :: entrenador 3;
</construct>
<bodyText confidence="0.999114666666667">
an associated frequency (freqres). In order to
assign frequency weights to our gold standard
translations, we asked our human annotators
to indicate their top 3 translations, which en-
ables us to also obtain meaningful associated
frequencies (freqres) viz. “1” in case a transla-
tion is picked by 1 annotator, “2” if picked by
two annotators and “3” if chosen by all three
annotators.
Best result evaluation For the best re-
sult evaluation, systems can propose as many
guesses as the system believes are correct, but
the resulting score is divided by the number of
guesses. In this way, systems that output a lot
of guesses are not favoured.
</bodyText>
<equation confidence="0.9995131">
P ai:iEA
|A|
Pres∈ai fre9res
|ai |
|Hi |(1)
Prec =
Rec =
Pres∈ai fre9res
|ai |
|Hi |(2)
</equation>
<sectionHeader confidence="0.968262" genericHeader="method">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.990252">
3.1 Scoring
</subsectionHeader>
<bodyText confidence="0.99997">
To score the participating systems, we use an
evaluation scheme which is inspired by the
English lexical substitution task in SemEval
2007 (McCarthy and Navigli, 2007). We per-
form both a best result evaluation and a more
relaxed evaluation for the top five results. The
evaluation is performed using precision and re-
call (Prec and Rec in the equations below),
and Mode precision (MP) and Mode recall
(MR), where we calculate precision and re-
call against the translation that is preferred by
the majority of annotators, provided that one
translation is more frequent than the others.
For the precision and recall formula we use
the following variables. Let H be the set of
annotators, T the set of test items and hi the
set of responses for an item i ∈ T for annota-
tor h ∈ H. For each i ∈ T we calculate the
mode (mi) which corresponds to the transla-
tion with the highest frequency weight. For
a detailed overview of the MP and MR cal-
culations, we refer to McCarthy and Navigli
(2007). Let A be the set of items from T (and
TM) where the system provides at least one
answer and ai : i ∈ A the set of guesses from
the system for item i. For each i, we calculate
the multiset union (Hi) for all hi for all h ∈ H
and for each unique type (res) in Hi that has
</bodyText>
<equation confidence="0.9686005">
P ai:iET
|T|
</equation>
<bodyText confidence="0.9016172">
Out-of-five (Oof) evaluation For the
more relaxed evaluation, systems can propose
up to five guesses. For this evaluation, the
resulting score is not divided by the number
of guesses.
</bodyText>
<equation confidence="0.942659">
PPP res∈ai freqres
ai:iEA |Hi|
|A |(3)
P res∈ai freqres
ai:iET |Hi|
|T |(4)
</equation>
<subsectionHeader confidence="0.983647">
3.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999816">
We produced two frequency-based baselines:
</bodyText>
<listItem confidence="0.883208125">
1. For the Best result evaluation, we select
the most frequent lemmatized translation
that results from the automated word
alignment process (GIZA++).
2. For the Out-of-five or more relaxed eval-
uation, we select the five most fre-
quent (lemmatized) translations that re-
sult from the GIZA++ alignment.
</listItem>
<bodyText confidence="0.99782825">
Table 1 shows the baselines for the Best
evaluation, while Table 2 gives an overview
per language of the baselines for the Out-of-
five evaluation.
</bodyText>
<equation confidence="0.9831685">
Prec =
Rec =
</equation>
<page confidence="0.996396">
17
</page>
<table confidence="0.9997775">
Prec Rec MP MR
Spanish 18.36 18.36 23.38 23.38
French 20.71 20.71 15.21 15.21
Italian 14.03 14.03 11.23 11.23
Dutch 15.69 15.69 8.71 8.71
German 13.16 13.16 6.95 6.95
</table>
<tableCaption confidence="0.834376">
Table 1: Best Baselines
</tableCaption>
<table confidence="0.999891666666667">
Prec Rec MP MR
Spanish 48.41 48.41 42.62 42.62
French 45.99 45.99 36.45 36.45
Italian 34.51 34.51 29.70 29.70
Dutch 37.43 37.43 24.58 24.58
German 32.89 32.89 29.80 29.80
</table>
<tableCaption confidence="0.993622">
Table 2: Out-of-five Baselines
</tableCaption>
<sectionHeader confidence="0.982359" genericHeader="method">
4 Systems
</sectionHeader>
<bodyText confidence="0.999936157894737">
We received sixteen submissions from five dif-
ferent participating teams. One group tack-
led all five target languages, whereas the other
groups focused on four (one team), two (one
team) or one (two teams) target language(s).
For both the best and the Out-of-five evalua-
tion tasks, there were between three and seven
participating systems per language.
The OWNS system identifies the nearest
neighbors of the test instances from the train-
ing data using a pairwise similarity measure
(weighted sum of the word overlap and se-
mantic overlap between two sentences). They
use WordNet similarity measures as an ad-
ditional information source, while the other
teams merely rely on parallel corpora to ex-
tract all lexical information. The UvT-WSD
systems use a k-nearest neighbour classifier
in the form of one word expert per lemma–
Part-of-Speech pair to be disambiguated. The
classifier takes as input a variety of local
and global context features. Both the FCC-
WSD and T3-COLEUR systems use bilingual
translation probability tables that are derived
from the Europarl corpus. The FCC-WSD
system uses a Naive Bayes classifier, while
the T3-COLEUR system uses an unsupervised
graph-based method. Finally, the UHD sys-
tems build for each target word a multilin-
gual co-occurrence graph based on the target
word’s aligned contexts found in parallel cor-
pora. The cross-lingual nodes are first linked
by translation edges, that are labeled with the
translations of the target word in the corre-
sponding contexts. The graph is transformed
into a minimum spanning tree which is used
to select the most relevant words in context to
disambiguate a given test instance.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9994515">
For the system evaluation results, we show
precision (Prec), recall (Rec), Mode precision
(MP) and Mode recall (MR). We ranked all
system results according to recall, as was done
for the Lexical Substitution task. Table 3
shows the system ranking on the best task,
while Table 4 shows the results for the Oof
task.
</bodyText>
<table confidence="0.999904034482759">
Prec Rec MP MR
Spanish
UvT-v 23.42 24.98 24.98 24.98
UvT-g 19.92 19.92 24.17 24.17
T3-COLEUR 19.78 19.59 24.59 24.59
UHD-1 20.48 16.33 28.48 22.19
UHD-2 20.2 16.09 28.18 22.65
FCC-WSD1 15.09 15.09 14.31 14.31
FCC-WSD3 14.43 14.43 13.41 13.41
French
T3-COLEUR 21.96 21.73 16.15 15.93
UHD-2 20.93 16.65 17.78 14.15
UHD-1 20.22 16.21 17.59 14.56
OWNS2 16.05 16.05 14.21 14.21
OWNS1 16.05 16.05 14.21 14.21
OWNS3 12.53 12.53 14.21 14.21
OWNS4 10.49 10.49 14.21 14.21
Italian
T3-COLEUR 15.55 15.4 10.2 10.12
UHD-2 16.28 13.03 14.89 9.46
UHD-1 15.94 12.78 12.34 8.48
Dutch
UvT-v 17.7 17.7 12.05 12.05
UvT-g 15.93 15.93 10.54 10.54
T3-COLEUR 10.71 10.56 6.18 6.16
German
T3-COLEUR 13.79 13.63 8.1 8.1
UHD-1 12.2 9.32 11.05 7.78
UHD-2 12.03 9.23 12.91 9.22
</table>
<tableCaption confidence="0.994604">
Table 3: Best System Results
</tableCaption>
<bodyText confidence="0.789321">
Beating the baseline seems to be quite chal-
lenging for this WSD task. While the best sys-
tems outperform the baseline for the best task,
</bodyText>
<page confidence="0.996887">
18
</page>
<table confidence="0.999946481481481">
Prec Rec MP MR
Spanish
UvT-g 43.12 43.12 43.94 43.94
UvT-v 42.17 42.17 40.62 40.62
FCC-WSD2 40.76 40.76 44.84 44.84
FCC-WSD4 38.46 38.46 39.49 39.49
T3-COLEUR 35.84 35.46 39.01 38.78
UHD-1 38.78 31.81 40.68 32.38
UHD-2 37.74 31.3 39.09 32.05
French
T3-COLEUR 49.44 48.96 42.13 41.77
OWNS1 43.11 43.11 38.29 38.29
OWNS2 38.74 38.74 37.73 37.73
UHD-1 39.06 32 37.00 26.79
UHD-2 37.92 31.38 37.66 27.08
Italian
T3-COLEUR 40.7 40.34 38.99 38.70
UHD-1 33.72 27.49 27.54 21.81
UHD-2 32.68 27.42 29.82 23.20
Dutch
UvT-v 34.95 34.95 24.62 24.62
UvT-g 34.92 34.92 19.72 19.72
T3-COLEUR 21.47 21.27 12.05 12.03
German
T3-COLEUR 33.21 32.82 33.60 33.56
UHD-1 27.62 22.82 25.68 21.16
UHD-2 27.24 22.55 27.19 22.30
</table>
<tableCaption confidence="0.999161">
Table 4: Out-of-five System Results
</tableCaption>
<bodyText confidence="0.99984786">
this is not always the case for the Out-of-five
task. This is not surprising though, as the Oof
baseline contains the five most frequent Eu-
roparl translations. As a consequence, these
translations usually contain the most frequent
translations from different sense clusters, and
in addition they also contain the most generic
translation that often covers multiple senses of
the target word.
The best results are achieved by the UvT-
WSD (Spanish, Dutch) and ColEur (French,
Italian and German) systems. An interest-
ing feature that these systems have in com-
mon, is that they extract all lexical informa-
tion from the parallel corpus at hand, and do
not need any additional data sources. As a
consequence, the systems can easily be applied
to other languages as well. This is clearly il-
lustrated by the ColEur system, that partici-
pated for all supported languages, and outper-
formed the other systems for three of the five
languages.
In general, we notice that Spanish and
French have the highest scores, followed by
Italian, whereas Dutch and German seem to be
more challenging. The same observation can
be made for both the Oof and Best results,
except for Italian that performs worse than
Dutch for the latter. However, given the low
participation rate for Italian, we do not have
sufficient information to explain this different
behaviour on the two tasks. The discrepancy
between the performance figures for Spanish
and French on the one hand, and German and
Dutch on the other hand, seems more readily
explicable. A likely explanation could be the
number of classes (or translations) the systems
have to choose from. As both Dutch and Ger-
man are characterized by a rich compound-
ing system, these compound translations also
result in a higher number of different trans-
lations. Figure 1 illustrates this by listing
the number of different translations (or classes
in the context of WSD) for all trial and test
words. As a result, the broader set of trans-
lations makes the WSD task, that consists
in choosing the most appropriate translation
from all possible translations for a given in-
stance, more complicated for Dutch and Ger-
man.
</bodyText>
<sectionHeader confidence="0.987872" genericHeader="conclusions">
6 Concluding remarks
</sectionHeader>
<bodyText confidence="0.99998585">
We believe that the Cross-lingual Word Sense
Disambiguation task is an interesting contri-
bution to the domain, as it attempts to ad-
dress two WSD problems which have received
a lot of attention lately, namely (1) the scarcity
of hand-crafted sense inventories and sense-
tagged corpora and (2) the need to make WSD
more suited for practical applications.
The system results lead to the following ob-
servations. Firstly, languages which make ex-
tensive use of single word compounds seem
harder to tackle, which is also reflected in the
baseline scores. A possible explanation for
this phenomenon could lie in the number of
translations the systems have to choose from.
Secondly, it is striking that the systems with
the highest performance solely rely on paral-
lel corpora as a source of information. This
would seem very promising for future multi-
lingual WSD research; by eliminating the need
</bodyText>
<page confidence="0.998523">
19
</page>
<figureCaption confidence="0.977686">
Figure 1: Number of different translations per word for Dutch, French, Spanish, Italian and
German.
</figureCaption>
<bodyText confidence="0.999047666666667">
for external information sources, these sys-
tems present a more flexible and language-
independent approach to WSD.
</bodyText>
<sectionHeader confidence="0.997396" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999948540983607">
E. Agirre and P. Edmonds, editors. 2006. Word
Sense Disambiguation. Text, Speech and Lan-
guage Technology. Springer, Dordrecht.
E. Agirre, B. Magnini, O. Lopez de Lacalle,
A. Otegi, G. Rigau, and P. Vossen. 2007.
Semeval-2007 task01: Evaluating wsd on cross-
language information retrieval. In Proceedings
of CLEF 2007 Workshop, pp. 908 - 917. ISSN:
1818-8044. ISBN: 2-912335-31-0.
M. Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual wsd and lexical selection in
translation. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association
for Computational Linguistics (EACL), Athens,
Greece.
W.A. Gale and K.W. Church. 1991. A program
for aligning sentences in bilingual corpora. In
Computational Linguistics, pages 177–184.
W.A. Gale, K.W. Church, and D. Yarowsky. 1993.
A method for disambiguating word senses in a
large corpus. In Computers and the Humanities,
volume 26, pages 415–439.
N. Ide, T. Erjavec, and D. Tufis. 2002. Sense dis-
crimination with parallel corpora. In Proceed-
ings of ACL Workshop on Word Sense Disam-
biguation: Recent Successes and Future Direc-
tions, pages 54–60.
P. Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings
of the MT Summit.
E. Lefever and V. Hoste. 2009. Semeval-2010
task 3: Cross-lingual word sense disambigua-
tion. In Proceedings of the NAACL-HLT 2009
Workshop: SEW-2009 - Semantic Evaluations,
pages 82–87, Boulder, Colorado.
E. Lefever and V. Hoste. 2010. Construction of a
benchmark data set for cross-lingual word sense
disambiguation. In Proceedings of the seventh
international conference on Language Resources
and Evaluation., Malta.
D. McCarthy and R. Navigli. 2007. Semeval-2007
task 10: English lexical substitution task. In
Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007), pages
48–53, Prague, Czech Republic.
R. Navigli. 2009. Word sense disambiguation: a
survey. In ACM Computing Surveys, volume 41,
pages 1–69.
H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploit-
ing parallel texts for word sense disambiguation:
An empirical study. In Proceedings of the 41st
Annual Meeting of the Association for Compu-
tational Linguistics, pages 455–462, Santa Cruz.
F.J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
McCarthy D. Sinha, R. D. and R. Mihalcea. 2009.
Semeval-2010 task 2: Cross-lingual lexical sub-
stitution. In Proceedings of the NAACL-HLT
2009 Workshop: SEW-2009 - Semantic Evalua-
tions, Boulder, Colorado.
</reference>
<page confidence="0.994901">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.235291">
<title confidence="0.6675118">SemEval-2010 Task 3: Cross-Lingual Word Sense Disambiguation Veronique Language and Translation Technology Team, University College Ghent, Belgium of Applied Mathematics and Computer Science, Ghent University, Belgium</title>
<abstract confidence="0.998475833333333">The goal of this task is to evaluate the feasibility of multilingual WSD on a newly developed multilingual lexical sample data set. Participants were asked to automatically determine the contextually appropriate translation of a given English noun in five languages, viz. Dutch, German, Italian, Spanish and French. This paper reports on the sixteen submissions from the five different participating teams.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>P Edmonds</author>
<author>editors</author>
</authors>
<date>2006</date>
<booktitle>Word Sense Disambiguation. Text, Speech and Language Technology.</booktitle>
<publisher>Springer,</publisher>
<location>Dordrecht.</location>
<marker>Agirre, Edmonds, editors, 2006</marker>
<rawString>E. Agirre and P. Edmonds, editors. 2006. Word Sense Disambiguation. Text, Speech and Language Technology. Springer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>B Magnini</author>
<author>O Lopez de Lacalle</author>
<author>A Otegi</author>
<author>G Rigau</author>
<author>P Vossen</author>
</authors>
<title>Semeval-2007 task01: Evaluating wsd on crosslanguage information retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of CLEF 2007 Workshop,</booktitle>
<pages>908--917</pages>
<marker>Agirre, Magnini, de Lacalle, Otegi, Rigau, Vossen, 2007</marker>
<rawString>E. Agirre, B. Magnini, O. Lopez de Lacalle, A. Otegi, G. Rigau, and P. Vossen. 2007. Semeval-2007 task01: Evaluating wsd on crosslanguage information retrieval. In Proceedings of CLEF 2007 Workshop, pp. 908 - 917. ISSN: 1818-8044. ISBN: 2-912335-31-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Apidianaki</author>
</authors>
<title>Data-driven semantic analysis for multilingual wsd and lexical selection in translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="1703" citStr="Apidianaki, 2009" startWordPosition="250" endWordPosition="251">necks in current WSD research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other than English) and the growing tendency to evaluate the performance of WSD systems in a real application such as machine translation and cross-language information retrieval (see for example Agirre et al. (2007)). The Cross-lingual WSD task aims at the development of a multilingual data set to test the feasibility of multilingual WSD. Many studies have already shown the validity of this crosslingual evidence idea (Gale et al., 1993; Ide et al., 2002; Ng et al., 2003; Apidianaki, 2009), but until now no benchmark data sets have been available. For the SemEval-2010 competition we developed (i) a sense inventory in which the sense distinctions were extracted from the multilingual corpus Europarli and (ii) a data set in which the ambiguous words were annotated with the senses from the multilingual sense inventory. The Cross-Lingual WSD task is a lexical sample task for English nouns, in which the word senses are made up of the translations in five languages, viz. Dutch, French, Italian, Spanish and German. Both the sense inventory and the annotated data set were constructed fo</context>
</contexts>
<marker>Apidianaki, 2009</marker>
<rawString>M. Apidianaki. 2009. Data-driven semantic analysis for multilingual wsd and lexical selection in translation. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1991</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>177--184</pages>
<contexts>
<context position="4270" citStr="Gale and Church (1991)" startWordPosition="662" endWordPosition="665">2010. c�2010 Association for Computational Linguistics Below, we provide a short summary of the complete data construction process. For a more detailed description, we refer to Lefever and Hoste (2009; 2010). The gold standard sense inventory was derived from the Europarl parallel corpus2, which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus, viz. English (our target language), Dutch, French, German, Italian and Spanish. All data were already sentence-aligned using a tool based on the Gale and Church (1991) algorithm, which was part of the Europarl corpus. We only considered the 1-1 sentence alignments between English and the five other languages. These sentence alignments were made available to the task participants for the five trial words. The sense inventory extracted from the parallel data set (Section 2.2) was used to annotate the sentences in the trial set and the test set, which were extracted from the JRC-ACQUIS Multilingual Parallel Corpus3 and BNC4. 2.2 Creation of the sense inventory Two steps were taken to obtain a multilingual sense inventory: (1) word alignment on the sentences to</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>W.A. Gale and K.W. Church. 1991. A program for aligning sentences in bilingual corpora. In Computational Linguistics, pages 177–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
<author>D Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus.</title>
<date>1993</date>
<booktitle>In Computers and the Humanities,</booktitle>
<volume>26</volume>
<pages>415--439</pages>
<contexts>
<context position="1649" citStr="Gale et al., 1993" startWordPosition="238" endWordPosition="241">l Word Sense Disambiguation task focuses on two bottlenecks in current WSD research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other than English) and the growing tendency to evaluate the performance of WSD systems in a real application such as machine translation and cross-language information retrieval (see for example Agirre et al. (2007)). The Cross-lingual WSD task aims at the development of a multilingual data set to test the feasibility of multilingual WSD. Many studies have already shown the validity of this crosslingual evidence idea (Gale et al., 1993; Ide et al., 2002; Ng et al., 2003; Apidianaki, 2009), but until now no benchmark data sets have been available. For the SemEval-2010 competition we developed (i) a sense inventory in which the sense distinctions were extracted from the multilingual corpus Europarli and (ii) a data set in which the ambiguous words were annotated with the senses from the multilingual sense inventory. The Cross-Lingual WSD task is a lexical sample task for English nouns, in which the word senses are made up of the translations in five languages, viz. Dutch, French, Italian, Spanish and German. Both the sense in</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1993</marker>
<rawString>W.A. Gale, K.W. Church, and D. Yarowsky. 1993. A method for disambiguating word senses in a large corpus. In Computers and the Humanities, volume 26, pages 415–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>T Erjavec</author>
<author>D Tufis</author>
</authors>
<title>Sense discrimination with parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>54--60</pages>
<contexts>
<context position="1667" citStr="Ide et al., 2002" startWordPosition="242" endWordPosition="245">iguation task focuses on two bottlenecks in current WSD research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other than English) and the growing tendency to evaluate the performance of WSD systems in a real application such as machine translation and cross-language information retrieval (see for example Agirre et al. (2007)). The Cross-lingual WSD task aims at the development of a multilingual data set to test the feasibility of multilingual WSD. Many studies have already shown the validity of this crosslingual evidence idea (Gale et al., 1993; Ide et al., 2002; Ng et al., 2003; Apidianaki, 2009), but until now no benchmark data sets have been available. For the SemEval-2010 competition we developed (i) a sense inventory in which the sense distinctions were extracted from the multilingual corpus Europarli and (ii) a data set in which the ambiguous words were annotated with the senses from the multilingual sense inventory. The Cross-Lingual WSD task is a lexical sample task for English nouns, in which the word senses are made up of the translations in five languages, viz. Dutch, French, Italian, Spanish and German. Both the sense inventory and the an</context>
</contexts>
<marker>Ide, Erjavec, Tufis, 2002</marker>
<rawString>N. Ide, T. Erjavec, and D. Tufis. 2002. Sense discrimination with parallel corpora. In Proceedings of ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 54–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the MT Summit.</booktitle>
<contexts>
<context position="4019" citStr="Koehn, 2005" startWordPosition="621" endWordPosition="622">words annotated with their contextually appropriate translations in five languages. lhttp://www.statmt.org/europarl/ 15 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 15–20, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Below, we provide a short summary of the complete data construction process. For a more detailed description, we refer to Lefever and Hoste (2009; 2010). The gold standard sense inventory was derived from the Europarl parallel corpus2, which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus, viz. English (our target language), Dutch, French, German, Italian and Spanish. All data were already sentence-aligned using a tool based on the Gale and Church (1991) algorithm, which was part of the Europarl corpus. We only considered the 1-1 sentence alignments between English and the five other languages. These sentence alignments were made available to the task participants for the five trial words. The sense inventory extracted from the parallel data set (Section 2.2) was used to annotate the sentences in</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Lefever</author>
<author>V Hoste</author>
</authors>
<title>Semeval-2010 task 3: Cross-lingual word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL-HLT 2009 Workshop: SEW-2009 - Semantic Evaluations,</booktitle>
<pages>82--87</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="3848" citStr="Lefever and Hoste (2009" startWordPosition="594" endWordPosition="597">s-lingual WSD task: (a) a parallel corpus on the basis of which the gold standard sense inventory was created and (b) a collection of English sentences containing the lexical sample words annotated with their contextually appropriate translations in five languages. lhttp://www.statmt.org/europarl/ 15 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 15–20, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Below, we provide a short summary of the complete data construction process. For a more detailed description, we refer to Lefever and Hoste (2009; 2010). The gold standard sense inventory was derived from the Europarl parallel corpus2, which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus, viz. English (our target language), Dutch, French, German, Italian and Spanish. All data were already sentence-aligned using a tool based on the Gale and Church (1991) algorithm, which was part of the Europarl corpus. We only considered the 1-1 sentence alignments between English and the five other languages. These sentence alignments were mad</context>
</contexts>
<marker>Lefever, Hoste, 2009</marker>
<rawString>E. Lefever and V. Hoste. 2009. Semeval-2010 task 3: Cross-lingual word sense disambiguation. In Proceedings of the NAACL-HLT 2009 Workshop: SEW-2009 - Semantic Evaluations, pages 82–87, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Lefever</author>
<author>V Hoste</author>
</authors>
<title>Construction of a benchmark data set for cross-lingual word sense disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the seventh international conference on Language Resources and Evaluation.,</booktitle>
<marker>Lefever, Hoste, 2010</marker>
<rawString>E. Lefever and V. Hoste. 2010. Construction of a benchmark data set for cross-lingual word sense disambiguation. In Proceedings of the seventh international conference on Language Resources and Evaluation., Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Navigli</author>
</authors>
<title>Semeval-2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9415" citStr="McCarthy and Navigli, 2007" startWordPosition="1496" endWordPosition="1499">by 1 annotator, “2” if picked by two annotators and “3” if chosen by all three annotators. Best result evaluation For the best result evaluation, systems can propose as many guesses as the system believes are correct, but the resulting score is divided by the number of guesses. In this way, systems that output a lot of guesses are not favoured. P ai:iEA |A| Pres∈ai fre9res |ai | |Hi |(1) Prec = Rec = Pres∈ai fre9res |ai | |Hi |(2) 3 Evaluation 3.1 Scoring To score the participating systems, we use an evaluation scheme which is inspired by the English lexical substitution task in SemEval 2007 (McCarthy and Navigli, 2007). We perform both a best result evaluation and a more relaxed evaluation for the top five results. The evaluation is performed using precision and recall (Prec and Rec in the equations below), and Mode precision (MP) and Mode recall (MR), where we calculate precision and recall against the translation that is preferred by the majority of annotators, provided that one translation is more frequent than the others. For the precision and recall formula we use the following variables. Let H be the set of annotators, T the set of test items and hi the set of responses for an item i ∈ T for annotator</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>D. McCarthy and R. Navigli. 2007. Semeval-2007 task 10: English lexical substitution task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 48–53, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
</authors>
<title>Word sense disambiguation: a survey. In</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<pages>1--69</pages>
<contexts>
<context position="938" citStr="Navigli (2009)" startWordPosition="131" endWordPosition="132">f this task is to evaluate the feasibility of multilingual WSD on a newly developed multilingual lexical sample data set. Participants were asked to automatically determine the contextually appropriate translation of a given English noun in five languages, viz. Dutch, German, Italian, Spanish and French. This paper reports on the sixteen submissions from the five different participating teams. 1 Introduction Word Sense Disambiguation, the task of selecting the correct sense of an ambiguous word in a given context, is a well-researched NLP problem (see for example Agirre and Edmonds (2006) and Navigli (2009)), largely boosted by the various Senseval and SemEval editions. The SemEval-2010 Cross-lingual Word Sense Disambiguation task focuses on two bottlenecks in current WSD research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other than English) and the growing tendency to evaluate the performance of WSD systems in a real application such as machine translation and cross-language information retrieval (see for example Agirre et al. (2007)). The Cross-lingual WSD task aims at the development of a multilingual data set to test the feasibility of multi</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>R. Navigli. 2009. Word sense disambiguation: a survey. In ACM Computing Surveys, volume 41, pages 1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>B Wang</author>
<author>Y S Chan</author>
</authors>
<title>Exploiting parallel texts for word sense disambiguation: An empirical study.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>455--462</pages>
<location>Santa Cruz.</location>
<contexts>
<context position="1684" citStr="Ng et al., 2003" startWordPosition="246" endWordPosition="249">ses on two bottlenecks in current WSD research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other than English) and the growing tendency to evaluate the performance of WSD systems in a real application such as machine translation and cross-language information retrieval (see for example Agirre et al. (2007)). The Cross-lingual WSD task aims at the development of a multilingual data set to test the feasibility of multilingual WSD. Many studies have already shown the validity of this crosslingual evidence idea (Gale et al., 1993; Ide et al., 2002; Ng et al., 2003; Apidianaki, 2009), but until now no benchmark data sets have been available. For the SemEval-2010 competition we developed (i) a sense inventory in which the sense distinctions were extracted from the multilingual corpus Europarli and (ii) a data set in which the ambiguous words were annotated with the senses from the multilingual sense inventory. The Cross-Lingual WSD task is a lexical sample task for English nouns, in which the word senses are made up of the translations in five languages, viz. Dutch, French, Italian, Spanish and German. Both the sense inventory and the annotated data set </context>
</contexts>
<marker>Ng, Wang, Chan, 2003</marker>
<rawString>H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploiting parallel texts for word sense disambiguation: An empirical study. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 455–462, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5045" citStr="Och and Ney, 2003" startWordPosition="790" endWordPosition="793">e alignments were made available to the task participants for the five trial words. The sense inventory extracted from the parallel data set (Section 2.2) was used to annotate the sentences in the trial set and the test set, which were extracted from the JRC-ACQUIS Multilingual Parallel Corpus3 and BNC4. 2.2 Creation of the sense inventory Two steps were taken to obtain a multilingual sense inventory: (1) word alignment on the sentences to find the set of possible translations for the set of ambiguous nouns and (2) clustering by meaning (per target word) of the resulting translations. GIZA++ (Och and Ney, 2003) was used to generate the initial word alignments, which were manually verified by certified translators in all six involved languages. The human annotators were asked to assign a “NULL” link to words for which no valid translation could be identified. Furthermore, they were also asked to provide extra information on compound translations (e.g. the Dutch word Investeringsbank as a translation of the English multiword Investment Bank), fuzzy links, or target words with a different PoS (e.g. the verb to bank). The manually verified translations were clustered by meaning by one annotator. In orde</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F.J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>McCarthy D Sinha</author>
<author>R D</author>
<author>R Mihalcea</author>
</authors>
<title>Semeval-2010 task 2: Cross-lingual lexical substitution.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL-HLT 2009 Workshop: SEW-2009 - Semantic Evaluations,</booktitle>
<location>Boulder, Colorado.</location>
<marker>Sinha, D, Mihalcea, 2009</marker>
<rawString>McCarthy D. Sinha, R. D. and R. Mihalcea. 2009. Semeval-2010 task 2: Cross-lingual lexical substitution. In Proceedings of the NAACL-HLT 2009 Workshop: SEW-2009 - Semantic Evaluations, Boulder, Colorado.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>