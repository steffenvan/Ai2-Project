<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001503">
<title confidence="0.974811">
Online EM for Unsupervised Models
</title>
<author confidence="0.999614">
Percy Liang Dan Klein
</author>
<affiliation confidence="0.9986395">
Computer Science Division, EECS Department
University of California at Berkeley
</affiliation>
<address confidence="0.798838">
Berkeley, CA 94720
</address>
<email confidence="0.996716">
1pliang,kleinj@cs.berkeley.edu
</email>
<sectionHeader confidence="0.998566" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9951632">
The (batch) EM algorithm plays an important
role in unsupervised induction, but it some-
times suffers from slow convergence. In this
paper, we show that online variants (1) provide
significant speedups and (2) can even find bet-
ter solutions than those found by batch EM.
We support these findings on four unsuper-
vised tasks: part-of-speech tagging, document
classification, word segmentation, and word
alignment.
</bodyText>
<sectionHeader confidence="0.99952" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995486">
In unsupervised NLP tasks such as tagging, parsing,
and alignment, one wishes to induce latent linguistic
structures from raw text. Probabilistic modeling has
emerged as a dominant paradigm for these problems,
and the EM algorithm has been a driving force for
learning models in a simple and intuitive manner.
However, on some tasks, EM can converge
slowly. For instance, on unsupervised part-of-
speech tagging, EM requires over 100 iterations to
reach its peak performance on the Wall-Street Jour-
nal (Johnson, 2007). The slowness of EM is mainly
due to its batch nature: Parameters are updated only
once after each pass through the data. When param-
eter estimates are still rough or if there is high redun-
dancy in the data, computing statistics on the entire
dataset just to make one update can be wasteful.
In this paper, we investigate two flavors of on-
line EM—incremental EM (Neal and Hinton, 1998)
and stepwise EM (Sato and Ishii, 2000; Capp´e and
Moulines, 2009), both of which involve updating pa-
rameters after each example or after a mini-batch
(subset) of examples. Online algorithms have the
potential to speed up learning by making updates
more frequently. However, these updates can be
seen as noisy approximations to the full batch up-
date, and this noise can in fact impede learning.
This tradeoff between speed and stability is famil-
iar to online algorithms for convex supervised learn-
ing problems—e.g., Perceptron, MIRA, stochastic
gradient, etc. Unsupervised learning raises two ad-
ditional issues: (1) Since the EM objective is non-
convex, we often get convergence to different local
optima of varying quality; and (2) we evaluate on
accuracy metrics which are at best loosely correlated
with the EM likelihood objective (Liang and Klein,
2008). We will see that these issues can lead to sur-
prising results.
In Section 4, we present a thorough investigation
of online EM, mostly focusing on stepwise EM since
it dominates incremental EM. For stepwise EM, we
find that choosing a good stepsize and mini-batch
size is important but can fortunately be done ade-
quately without supervision. With a proper choice,
stepwise EM reaches the same performance as batch
EM, but much more quickly. Moreover, it can even
surpass the performance of batch EM. Our results
are particularly striking on part-of-speech tagging:
Batch EM crawls to an accuracy of 57.3% after 100
iterations, whereas stepwise EM shoots up to 65.4%
after just two iterations.
</bodyText>
<sectionHeader confidence="0.874234" genericHeader="method">
2 Tasks, models, and datasets
</sectionHeader>
<bodyText confidence="0.995770666666667">
In this paper, we focus on unsupervised induction
via probabilistic modeling. In particular, we define
a probabilistic model p(x, z; θ) of the input x (e.g.,
</bodyText>
<page confidence="0.974869">
611
</page>
<note confidence="0.890888">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 611–619,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9991248">
a sentence) and hidden output z (e.g., a parse tree)
with parameters θ (e.g., rule probabilities). Given a
set of unlabeled examples x(1), ... , x(n), the stan-
dard training objective is to maximize the marginal
log-likelihood of these examples:
</bodyText>
<equation confidence="0.978609333333333">
n
`(θ) = log p(x(i); θ). (1)
i=1
</equation>
<bodyText confidence="0.99190115942029">
A trained model θˆ is then evaluated on the accuracy
of its predictions: argmaxZ p(z  |x(i); ˆθ) against the
true output z(i); the exact evaluation metric depends
on the task. What makes unsupervised induction
hard at best and ill-defined at worst is that the train-
ing objective (1) does not depend on the true outputs
at all.
We ran experiments on four tasks described be-
low. Two of these tasks—part-of-speech tagging and
document classification—are “clustering” tasks. For
these, the output z consists of labels; for evalua-
tion, we map each predicted label to the true label
that maximizes accuracy. The other two tasks—
segmentation and alignment—only involve unla-
beled combinatorial structures, which can be eval-
uated directly.
Part-of-speech tagging For each sentence x =
(x1, ... , x`), represented as a sequence of words, we
wish to predict the corresponding sequence of part-
of-speech (POS) tags z = (z1, ... , z`). We used
a simple bigram HMM trained on the Wall Street
Journal (WSJ) portion of the Penn Treebank (49208
sentences, 45 tags). No tagging dictionary was used.
We evaluated using per-position accuracy.
Document classification For each document x =
(x1, ... , x`) consisting of ` words,1 we wish to pre-
dict the document class z E 11,. .. , 20j. Each doc-
ument x is modeled as a bag of words drawn inde-
pendently given the class z. We used the 20 News-
groups dataset (18828 documents, 20 classes). We
evaluated on class accuracy.
Word segmentation For each sentence x =
(x1, ... , x`), represented as a sequence of English
phonemes or Chinese characters without spaces
separating the words, we would like to predict
1We removed the 50 most common words and words that
occurred fewer than 5 times.
a segmentation of the sequence into words z =
(z1, ... , z|,|), where each segment (word) zi is a
contiguous subsequence of 1, ... , `. Since the naive
unigram model has a degenerate maximum likeli-
hood solution that makes each sentence a separate
word, we incorporate a penalty for longer segments:
p(x, z; θ) . ri|, |k=1 p(xzk; θ)e−|zk|β, where β &gt; 1
determines the strength of the penalty. For English,
we used β = 1.6; Chinese, β = 2.5. To speed up in-
ference, we restricted the maximum segment length
to 10 for English and 5 for Chinese.
We applied this model on the Bernstein-Ratner
corpus from the CHILDES database used in
Goldwater et al. (2006) (9790 sentences) and
the Academia Sinica (AS) corpus from the first
SIGHAN Chinese word segmentation bakeoff (we
used the first 100K sentences). We evaluated using
F1 on word tokens.
To the best of our knowledge, our penalized uni-
gram model is new and actually beats the more com-
plicated model of Johnson (2008) 83.5% to 78%,
which had been the best published result on this task.
Word alignment For each pair of translated sen-
tences x = (e1,... , ene, f1, ... , fnf), we wish to
predict the word alignments z E 10, 1jnenf. We
trained two IBM model 1s using agreement-based
learning (Liang et al., 2008). We used the first
30K sentence pairs of the English-French Hansards
data from the NAACL 2003 Shared Task, 447+37
of which were hand-aligned (Och and Ney, 2003).
We evaluated using the standard alignment error rate
(AER).
</bodyText>
<sectionHeader confidence="0.988286" genericHeader="method">
3 EM algorithms
</sectionHeader>
<bodyText confidence="0.999536846153846">
Given a probabilistic model p(x, z; θ) and unla-
beled examples x(1), ... , x(n), recall we would like
to maximize the marginal likelihood of the data
(1). Let φ(x, z) denote a mapping from a fully-
labeled example (x, z) to a vector of sufficient statis-
tics (counts in the case of multinomials) for the
model. For example, one component of this vec-
tor for HMMs would be the number of times state
7 emits the word “house” in sentence x with state
sequence z. Given a vector of sufficient statistics µ,
let θ(µ) denote the maximum likelihood estimate. In
our case, θ(µ) are simply probabilities obtained by
normalizing each block of counts. This closed-form
</bodyText>
<page confidence="0.941924">
612
</page>
<equation confidence="0.982692117647059">
Batch EM
µ ← initialization
for each iteration t = 1, ... ,T:
µ0 ← 0
for each example i = 1, ... ,n:
s0i ← E. p(z  |x(i); θ(µ)) φ(x(i), z) [inference]
µ0 ←µ0 + s0 [accumulate new]
i
µ ← µ0 [replace old with new]
Incremental EM (iEM)
si ← initialization for i = 1, ... n
µ ← En
i=1 si
for each iteration t = 1, ... ,T:
for each example i = 1, ... , n in random order:
s0i ← E. p(z  |x(i); θ(µ)) φ(x(i), z) [inference]
µ ← µ + s0i − si; si ← s0i [replace old with new]
</equation>
<bodyText confidence="0.995291">
solution is one of the features that makes EM (both
batch and online) attractive.
</bodyText>
<subsectionHeader confidence="0.999683">
3.1 Batch EM
</subsectionHeader>
<bodyText confidence="0.999951428571429">
In the (batch) EM algorithm, we alternate between
the E-step and the M-step. In the E-step, we com-
pute the expected sufficient statistics µ0 across all
the examples based on the posterior over z under the
current parameters θ(µ). In all our models, this step
can be done via a dynamic program (for example,
forward-backward for POS tagging).
In the M-step, we use these sufficient statistics
µ0 to re-estimate the parameters. Since the M-step
is trivial, we represent it implicitly by θ(·) in order
to concentrate on the computation of the sufficient
statistics. This focus will be important for online
EM, so writing batch EM in this way accentuates
the parallel between batch and online.
</bodyText>
<subsectionHeader confidence="0.992936">
3.2 Online EM
</subsectionHeader>
<bodyText confidence="0.9999395">
To obtain an online EM algorithm, we store a sin-
gle set of sufficient statistics µ and update it after
processing each example. For the i-th example, we
compute sufficient statistics s0i. There are two main
variants of online EM algorithms which differ in ex-
actly how the new s0i is incorporated into µ.
The first is incremental EM (iEM) (Neal and Hin-
ton, 1998), in which we not only keep track of µ but
also the sufficient statistics s1, ... , sn for each ex-
ample (µ = Eni=1 si). When we process example i,
we subtract out the old si and add the new s0i.
Sato and Ishii (2000) developed another variant,
later generalized by Capp´e and Moulines (2009),
which we call stepwise EM (sEM). In sEM, we in-
terpolate between µ and s0i based on a stepsize ηk (k
is the number of updates made to µ so far).
The two algorithms are motivated in different
ways. Recall that the log-likelihood can be lower
</bodyText>
<equation confidence="0.942974444444444">
Stepwise EM (sEM)
µ ← initialization; k = 0
for each iteration t = 1, ... , T:
for each example i = 1, ... , n in random order:
s0i ← E. p(z  |x(i); θ(µ)) φ(x(i), z) [inference]
µ ← (1−ηk)µ + ηks0i; k ← k+1 [towards new]
bounded as follows (Neal and Hinton, 1998):
`(θ) ≥ L(q1, ... , qn, θ) (2)
]qi(z  |x(i)) log p(x(i), z; θ) + H(qi) ,
</equation>
<bodyText confidence="0.99931324">
where H(qi) is the entropy of the distribution qi(z |
x(i)). Batch EM alternates between optimizing L
with respect to q1, ... , qn in the E-step (represented
implicitly via sufficient statistics µ0) and with re-
spect to θ in the M-step. Incremental EM alternates
between optimizing with respect to a single qi and θ.
Stepwise EM is motivated from the stochastic ap-
proximation literature, where we think of approxi-
mating the update µ0 in batch EM with a single sam-
ple s0i. Since one sample is a bad approximation,
we interpolate between s0i and the current µ. Thus,
sEM can be seen as stochastic gradient in the space
of sufficient statistics.
Stepsize reduction power α Stepwise EM leaves
open the choice of the stepsize ηk. Standard results
from the stochastic approximation literature state
that E∞k=0 ηk = ∞ and E∞k=0 η2k &lt; ∞ are suffi-
cient to guarantee convergence to a local optimum.
In particular, if we take ηk = (k + 2)−α, then any
0.5 &lt; α ≤ 1 is valid. The smaller the α, the larger
the updates, and the more quickly we forget (decay)
our old sufficient statistics. This can lead to swift
progress but also generates instability.
Mini-batch size m We can add some stability
to sEM by updating on multiple examples at once
</bodyText>
<equation confidence="0.997719">
def =
� �
n
i=1
</equation>
<page confidence="0.987063">
613
</page>
<bodyText confidence="0.9926365">
instead of just one. In particular, partition the
n examples into mini-batches of size m and run
sEM, treating each mini-batch as a single exam-
ple. Formally, for each i = 0, m, 2m, 3m, ... , first
compute the sufficient statistics s0i+1, ... , s0i+m on
x(i+1), ... , x(i+m) and then update µ using s0i+1 +
· · ·+ s0i+m. The larger the m, the less frequent
the updates, but the more stable they are. In this
way, mini-batches interpolate between a pure online
(m = 1) and a pure batch (m = n) algorithm.2
Fast implementation Due to sparsity in NLP, the
sufficient statistics of an example s0i are nonzero for
a small fraction of its components. For iEM, the
time required to update µ with s0i depends only on
the number of nonzero components of s0i. However,
the sEM update is µ ← (1−ηk)µ+ηks0i, and a naive
implementation would take time proportional to the
total number of components. The key to a more effi-
cient solution is to note that θ(µ) is invariant to scal-
ing of µ. Therefore, we can store S =µ
</bodyText>
<equation confidence="0.874904833333333">
Q
j&lt;k(1−ηj)
instead of µ and make the following sparse update:
S ← S +Q ηk
j≤k(1−ηj)s0 i, taking comfort in the fact
that θ(µ) = θ(S).
</equation>
<bodyText confidence="0.999838428571428">
For both iEM and sEM, we also need to efficiently
compute θ(µ). We can do this by maintaining the
normalizer for each multinomial block (sum of the
components in the block). This extra maintenance
only doubles the number of updates we have to make
but allows us to fetch any component of θ(µ) in con-
stant time by dividing out the normalizer.
</bodyText>
<subsectionHeader confidence="0.992956">
3.3 Incremental versus stepwise EM
</subsectionHeader>
<bodyText confidence="0.999816909090909">
Incremental EM increases L monotonically after
each update by virtue of doing coordinate-wise as-
cent and thus is guaranteed to converge to a local
optimum of both L and ` (Neal and Hinton, 1998).
However, ` is not guaranteed to increase after each
update. Stepwise EM might not increase either L or
` after each update, but it is guaranteed to converge
to a local optimum of ` given suitable conditions on
the stepsize discussed earlier.
Incremental and stepwise EM actually coincide
under the following setting (Capp´e and Moulines,
</bodyText>
<footnote confidence="0.544186">
2Note that running sEM with m = n is similar but not
equivalent to batch EM since old sufficient statistics are still
interpolated rather than replaced.
</footnote>
<bodyText confidence="0.99993525">
2009): If we set (α, m) = (1, 1) for sEM and ini-
tialize all si = 0 for iEM, then both algorithms make
the same updates on the first pass through the data.
They diverge thereafter as iEM subtracts out old sis,
while sEM does not even remember them.
One weakness of iEM is that its memory require-
ments grow linearly with the number of examples
due to storing s1, ... , sn. For large datasets, these
sis might not even fit in memory, and resorting to
physical disk would be very slow. In contrast, the
memory usage of sEM does not depend on n.
The relationship between iEM and sEM (with
m = 1) is analogous to the one between exponen-
tiated gradient (Collins et al., 2008) and stochastic
gradient for supervised learning of log-linear mod-
els. The former maintains the sufficient statistics of
each example and subtracts out old ones whereas the
latter does not. In the supervised case, the added sta-
bility of exponentiated gradient tends to yield bet-
ter performance. For the unsupervised case, we will
see empirically that remembering the old sufficient
statistics offers no benefit, and much better perfor-
mance can be obtained by properly setting (α, m)
for sEM (Section 4).
</bodyText>
<sectionHeader confidence="0.99981" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.974089727272727">
We now present our empirical results for batch EM
and online EM (iEM and sEM) on the four tasks de-
scribed in Section 2: part-of-speech tagging, docu-
ment classification, word segmentation (English and
Chinese), and word alignment.
We used the following protocol for all experi-
ments: We initialized the parameters to a neutral set-
ting plus noise to break symmetries.3 Training was
performed for 20 iterations.4 No parameter smooth-
ing was used. All runs used a fixed random seed for
initializing the parameters and permuting the exam-
ples at the beginning of each iteration. We report two
performance metrics: log-likelihood normalized by
the number of examples and the task-specific accu-
racy metric (see Section 2). All numbers are taken
from the final iteration.
3Specifically, for each block of multinomial probabilities
B1, ... , BK, we set Bk a exp{10−3(1 + ak)}, where ak ∼
U[0, 1]. Exception: for batch EM on POS tagging, we used 1
instead of 10−3; more noise worked better.
4Exception: for batch EM on POS tagging, 100 iterations
was needed to get satisfactory performance.
</bodyText>
<page confidence="0.995012">
614
</page>
<bodyText confidence="0.999884">
Stepwise EM (sEM) requires setting two
optimization parameters: the stepsize reduc-
tion power α and the mini-batch size m (see
Section 3.2). As Section 4.3 will show, these
two parameters can have a large impact on
performance. As a default rule of thumb, we
chose (α, m) E {0.5, 0.6, 0.7, 0.8, 0.9,1.0} x
{1, 3,10, 30,100, 300,1K, 3K,10K} to maximize
log-likelihood; let sEM` denote stepwise EM with
this setting. Note that this setting requires no labeled
data. We will also consider fixing (α, m) = (1, 1)
(sEMi) and choosing (α, m) to maximize accuracy
(sEMa).
In the results to follow, we first demonstrate that
online EM is faster (Section 4.1) and sometimes
leads to higher accuracies (Section 4.2). Next, we
explore the effect of the optimization parameters
(α, m) (Section 4.3), briefly revisiting the connec-
tion between incremental and stepwise EM. Finally,
we show the stability of our results under different
random seeds (Section 4.4).
</bodyText>
<subsectionHeader confidence="0.964043">
4.1 Speed
</subsectionHeader>
<bodyText confidence="0.999993">
One of the principal motivations for online EM
is speed, and indeed we found this motivation to
be empirically well-justified. Figure 1 shows that,
across all five datasets, sEM` converges to a solution
with at least comparable log-likelihood and accuracy
with respect to batch EM, but sEM` does it anywhere
from about 2 (word alignment) to 10 (POS tagging)
times faster. This supports our intuition that more
frequent updates lead to faster convergence. At the
same time, note that the other two online EM vari-
ants in Figure 1 (iEM and sEMi) are prone to catas-
trophic failure. See Section 4.3 for further discus-
sion on this issue.
</bodyText>
<subsectionHeader confidence="0.93658">
4.2 Performance
</subsectionHeader>
<bodyText confidence="0.999878">
It is fortunate but perhaps not surprising that step-
wise EM is faster than batch EM. But Figure 1 also
shows that, somewhat surprisingly, sEM` can actu-
ally converge to a solution with higher accuracy, in
particular on POS tagging and document classifica-
tion. To further explore the accuracy-increasing po-
tential of sEM, consider choosing (α, m) to maxi-
mize accuracy (sEMa). Unlike sEM`, sEMa does re-
quire labeled data. In practice, (α, m) can be tuned
</bodyText>
<table confidence="0.9994215">
EM sEM` sEMa α` m` αa ma
POS 57.3 59.6 66.7 0.7 3 0.5 3
DOC 39.1 47.8 49.9 0.8 1K 0.5 3K
SEG(en) 80.5 80.7 83.5 0.7 1K 1.0 100
SEG(ch) 78.2 77.2 78.1 0.6 10K 1.0 10K
ALIGN 78.8 78.9 78.9 0.7 10K 0.7 10K
</table>
<tableCaption confidence="0.9940165">
Table 1: Accuracy of batch EM and stepwise EM, where
the optimization parameters (α, m) are tuned to either
maximize log-likelihood (sEMP) or accuracy (sEM.).
With an appropriate setting of (α, m), stepwise EM out-
performs batch EM significantly on POS tagging and
document classification.
</tableCaption>
<bodyText confidence="0.998974571428571">
on a small labeled set along with any model hyper-
parameters.
Table 1 shows that sEMa improves the accuracy
compared to batch EM even more than sEM`. The
result for POS is most vivid: After one iteration of
batch EM, the accuracy is only at 24.0% whereas
sEMa is already at 54.5%, and after two iterations,
at 65.4%. Not only is this orders of magnitude faster
than batch EM, batch EM only reaches 57.3% after
100 iterations.
We get a similarly striking result for document
classification, but the results for word segmentation
and word alignment are more modest. A full un-
derstanding of this phenomenon is left as an open
problem, but we will comment on one difference be-
tween the tasks where sEM improves accuracy and
the tasks where it doesn’t. The former are “clus-
tering” tasks (POS tagging and document classifi-
cation), while the latter are “structural” tasks (word
segmentation and word alignment). Learning of
clustering models centers around probabilities over
words given a latent cluster label, whereas in struc-
tural models, there are no cluster labels, and it is
the combinatorial structure (the segmentations and
alignments) that drives the learning.
Likelihood versus accuracy From Figure 1, we
see that stepwise EM (sEM`) can outperform batch
EM in both likelihood and accuracy. This suggests
that stepwise EM is better at avoiding local minima,
perhaps leveraging its stochasticity to its advantage.
However, on POS tagging, tuning sEM to maxi-
mize accuracy (sEMa) results in a slower increase
in likelihood: compare sEMa in Figure 2 with sEM`
in Figure 1(a). This shouldn’t surprise us too much
given that likelihood and accuracy are only loosely
</bodyText>
<page confidence="0.990565">
615
</page>
<figure confidence="0.996639702970297">
20 40 60 80
iterations
20 40 60 80
iterations
-5.9
1.0
-6.9
0.8
log-likelihood
accuracy
-7.8
0.6
0.4
-8.8
EM
sEMi
sEM`
0.2
-9.8
accuracy
0.8
0.6
0.4
0.2
1.0
2 4 6 8 10
iterations
log-likelihood
-7.8
-8.3
-8.8
-9.3
-9.8
2 4 6 8 10
iterations
EM
iEM
sEMi
sEM`
(a) POS tagging (b) Document classification
F1
0.8
0.6
0.4
0.2
1.0
2 4 6 8 10
iterations
log-likelihood
-4.0
-4.2
-4.4
-4.6
-4.8
2 4 6 8 10
iterations
EM
iEM
sEMi
sEM`
F1
0.8
0.6
0.4
0.2
1.0
2 4 6 8 10
iterations
log-likelihood
-7.2
-7.8
-8.4
-8.9
-9.5
2 4 6 8 10
iterations
EM
iEM
sEMi
sEM`
(c) Word segmentation (English) (d) Word segmentation (Chinese)
1 − AER
0.8
0.6
0.4
0.2
1.0
2 4 6 8 10
iterations
log-likelihood
-10.9
-5.0
-6.5
-7.9
-9.4
2 4 6 8 10
iterations
EM
iEM
sEMi
sEM`
</figure>
<table confidence="0.877589875">
accuracy log-likelihood
EM sEM` EM sEM`
POS 57.3 59.6 -6.03 -6.08
DOC 39.1 47.8 -7.96 -7.88
SEG(en) 80.5 80.7 -4.11 -4.11
SEG(ch) 78.2 77.2 -7.27 -7.28
ALIGN 78.8 78.9 -5.05 -5.12
(e) Word alignment (f) Results after convergence
</table>
<figureCaption confidence="0.995606">
Figure 1: Accuracy and log-likelihood plots for batch EM, incremental EM, and stepwise EM across all five datasets.
sEMP outperforms batch EM in terms of convergence speed and even accuracy and likelihood; iEM and sEMi fail in
some cases. We did not run iEM on POS tagging due to memory limitations; we expect the performance would be
similar to sEMi, which is not very encouraging (Section 4.3).
</figureCaption>
<bodyText confidence="0.994487">
correlated (Liang and Klein, 2008). But it does sug-
gest that stepwise EM is injecting a bias that favors
accuracy over likelihood—a bias not at all reflected
in the training objective.
We can create a hybrid (sEMa+EM) that com-
bines the strengths of both sEMa and EM: First run
sEMa for 5 iterations, which quickly takes us to a
part of the parameter space yielding good accura-
cies; then run EM, which quickly improves the like-
lihood. Fortunately, accuracy does not degrade as
likelihood increases (Figure 2).
</bodyText>
<subsectionHeader confidence="0.999823">
4.3 Varying the optimization parameters
</subsectionHeader>
<bodyText confidence="0.999763142857143">
Recall that stepwise EM requires setting two opti-
mization parameters: the stepsize reduction power α
and the mini-batch size m. We now explore the ef-
fect of (α, m) on likelihood and accuracy.
As mentioned in Section 3.2, larger mini-batches
(increasing m) stabilize parameter updates, while
larger stepsizes (decreasing α) provide swifter
</bodyText>
<page confidence="0.995069">
616
</page>
<figure confidence="0.981734529411765">
DOC accuracy POS DOC ALIGN
α\m 1 3 10 30 100 300 1K 3K 10K
0.5 5.4 5.4 5.5 5.6 6.0 25.7 48.8 49.9 44.6
0.6 5.4 5.4 5.6 5.6 22.3 36.1 48.7 49.3 44.2
0.7 5.5 5.5 5.6 11.1 39.9 43.3 48.1 49.0 43.5
0.8 5.6 5.6 6.0 21.7 47.3 45.0 47.8 49.5 42.8
0.9 5.8 6.0 13.4 32.4 48.7 48.4 46.4 49.4 42.4
1.0 6.2 11.8 19.6 35.2 47.6 49.5 47.5 49.3 41.7
DOC log-likelihood
SEG(en) SEG(ch)
α\m 1 3 10 30 100 300 1K 3K 10K
0.5 -8.875 -8.71 -8.61 -8.555 -8.505 -8.172 -7.92 -7.906 -7.916
0.6 -8.604 -8.575 -8.54 -8.524 -8.235 -8.041 -7.898 -7.901 -7.916
0.7 -8.541 -8.533 -8.531 -8.354 -8.023 -7.943 -7.886 -7.896 -7.918
0.8 -8.519 -8.506 -8.493 -8.228 -7.933 -7.896 -7.883 -7.89 -7.922
0.9 -8.505 -8.486 -8.283 -8.106 -7.91 -7.889 -7.889 -7.891 -7.927
1.0 -8.471 -8.319 -8.204 -8.052 -7.919 -7.889 -7.892 -7.896 -7.937
</figure>
<figureCaption confidence="0.99760375">
Figure 3: Effect of optimization parameters (stepsize reduction power α and mini-batch size m) on accuracy and
likelihood. Numerical results are shown for document classification. In the interest of space, the results for each task
are compressed into two gray scale images, one for accuracy (top) and one for log-likelihood (bottom), where darker
shades represent larger values. Bold (red) numbers denote the best α for a given m.
</figureCaption>
<figure confidence="0.974798">
20 40 60 80 20 40 60 80
iterations iterations
</figure>
<figureCaption confidence="0.9993332">
Figure 2: sEMa quickly obtains higher accuracy than
batch EM but suffers from a slower increase in likeli-
hood. The hybrid sEMa+EM (5 iterations of EMa fol-
lowed by batch EM) increases both accuracy and likeli-
hood sharply.
</figureCaption>
<bodyText confidence="0.999859352941177">
progress. Remember that since we are dealing with a
nonconvex objective, the choice of stepsize not only
influences how fast we converge, but also the quality
of the solution that we converge to.
Figure 3 shows the interaction between α and m
in terms of likelihood and accuracy. In general, the
best (α, m) depends on the task and dataset. For ex-
ample, for document classification, larger m is criti-
cal for good performance; for POS tagging, it is bet-
ter to use smaller values of α and m.
Fortunately, there is a range of permissible set-
tings (corresponding to the dark regions in Figure 3)
that lead to reasonable performance. Furthermore,
the settings that perform well on likelihood gener-
ally correspond to ones that perform well on accu-
racy, which justifies using sEMt.
A final observation is that as we use larger mini-
batches (larger m), decreasing the stepsize more
gradually (smaller α) leads to better performance.
Intuitively, updates become more reliable with larger
m, so we can afford to trust them more and incorpo-
rate them more aggressively.
Stepwise versus incremental EM In Section 3.2,
we mentioned that incremental EM can be made
equivalent to stepwise EM with α = 1 and m = 1
(sEMz). Figure 1 provides the empirical support:
iEM and sEMz have very similar training curves.
Therefore, keeping around the old sufficient statis-
tics does not provide any advantage and still requires
a substantial storage cost. As mentioned before, set-
ting (α, m) properly is crucial. While we could sim-
ulate mini-batches with iEM by updating multiple
coordinates simultaneously, iEM is not capable of
exploiting the behavior of α &lt; 1.
</bodyText>
<subsectionHeader confidence="0.998341">
4.4 Varying the random seed
</subsectionHeader>
<bodyText confidence="0.9997934">
All our results thus far represent single runs with a
fixed random seed. We now investigate the impact
of randomness on our results. Recall that we use
randomness for two purposes: (1) initializing the
parameters (affects both batch EM and online EM),
</bodyText>
<figure confidence="0.996228866666667">
accuracy
0.8
0.6
0.4
0.2
1.0
log-likelihood
-11.0
-12.7
-5.9
-7.6
-9.3
EM
sEMa
sEMa+EM
</figure>
<page confidence="0.948702">
617
</page>
<table confidence="0.992191857142857">
accuracy log-likelihood
EM sEMt EM sEMt
POS 56.2 ±1.36 58.8 ±0.73, 1.41 −6.01 −6.09
DOC 41.2 ±1.97 51.4 ±0.97, 2.82 −7.93 −7.88
SEG(en) 80.5 ±0.0 81.0 ±0.0, 0.42 −4.1 −4.1
SEG(ch) 78.2 ±0.0 77.2 ±0.0, 0.04 −7.26 −7.27
ALIGN 79.0 ±0.14 78.8 ±0.14, 0.25 −5.04 −5.11
</table>
<tableCaption confidence="0.997167">
Table 2: Mean and standard deviation over different ran-
</tableCaption>
<bodyText confidence="0.971264464285714">
dom seeds. For EM and sEM, the first number after f
is the standard deviation due to different initializations
of the parameters. For sEM, the second number is the
standard deviation due to different permutations of the
examples. Standard deviation for log-likelihoods are all
&lt; 0.01 and therefore left out due to lack of space.
and (2) permuting the examples at the beginning of
each iteration (affects only online EM).
To separate these two purposes, we used two
different seeds, Si E 11, 2, 3, 4, 5} and Sp E
11, 2, 3, 4, 51 for initializing and permuting, respec-
tively. Let X be a random variable denoting either
log-likelihood or accuracy. We define the variance
due to initialization as var(E(X  |Si)) (E averages
over Sp for each fixed Si) and the variance due to
permutation as E(var(X  |Si)) (E averages over Si).
These two variances provide an additive decompo-
sition of the total variance: var(X) = var(E(X |
Si)) + E(var(X  |Si)).
Table 2 summarizes the results across the 5 tri-
als for EM and 25 for sEMt. Since we used a very
small amount of noise to initialize the parameters,
the variance due to initialization is systematically
smaller than the variance due to permutation. sEMt
is less sensitive to initialization than EM, but addi-
tional variance is created by randomly permuting the
examples. Overall, the accuracy of sEMt is more
variable than that of EM, but not by a large amount.
</bodyText>
<sectionHeader confidence="0.997911" genericHeader="evaluation">
5 Discussion and related work
</sectionHeader>
<bodyText confidence="0.99997435">
As datasets increase in size, the demand for online
algorithms has grown in recent years. One sees
this clear trend in the supervised NLP literature—
examples include the Perceptron algorithm for tag-
ging (Collins, 2002), MIRA for dependency parsing
(McDonald et al., 2005), exponentiated gradient al-
gorithms (Collins et al., 2008), stochastic gradient
for constituency parsing (Finkel et al., 2008), just
to name a few. Empirically, online methods are of-
ten faster by an order of magnitude (Collins et al.,
2008), and it has been argued on theoretical grounds
that the fast, approximate nature of online meth-
ods is a good fit given that we are interested in test
performance, not the training objective (Bottou and
Bousquet, 2008; Shalev-Shwartz and Srebro, 2008).
However, in the unsupervised NLP literature, on-
line methods are rarely seen,5 and when they are,
incremental EM is the dominant variant (Gildea and
Hofmann, 1999; Kuo et al., 2008). Indeed, as we
have shown, applying online EM does require some
care, and some variants (including incremental EM)
can fail catastrophically in face of local optima.
Stepwise EM provides finer control via its optimiza-
tion parameters and has proven quite successful.
One family of methods that resembles incremen-
tal EM includes collapsed samplers for Bayesian
models—for example, Goldwater et al. (2006) and
Goldwater and Griffiths (2007). These samplers
keep track of a sample of the latent variables for
each example, akin to the sufficient statistics that we
store in incremental EM. In contrast, stepwise EM
does not require this storage and operates more in
the spirit of a truly online algorithm.
Besides speed, online algorithms are of interest
for two additional reasons. First, in some applica-
tions, we receive examples sequentially and would
like to estimate a model in real-time, e.g., in the clus-
tering of news articles. Second, since humans learn
sequentially, studying online EM might suggest new
connections to cognitive mechanisms.
</bodyText>
<sectionHeader confidence="0.999657" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9988919">
We have explored online EM on four tasks and
demonstrated how to use stepwise EM to overcome
the dangers of stochasticity and reap the benefits of
frequent updates and fast learning. We also discov-
ered that stepwise EM can actually improve accu-
racy, a phenomenon worthy of further investigation.
This paper makes some progress on elucidating the
properties of online EM. With this increased under-
standing, online EM, like its batch cousin, could be-
come a mainstay for unsupervised learning.
</bodyText>
<footnote confidence="0.756984">
5Other types of learning methods have been employed
successfully, for example, Venkataraman (2001) and Seginer
(2007).
</footnote>
<page confidence="0.992397">
618
</page>
<sectionHeader confidence="0.998263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833362318841">
L. Bottou and O. Bousquet. 2008. The tradeoffs of large
scale learning. In Advances in Neural Information
Processing Systems (NIPS).
O. Capp´e and E. Moulines. 2009. Online expectation-
maximization algorithm for latent data models. Jour-
nal of the Royal Statistics Society: Series B (Statistical
Methodology), 71.
M. Collins, A. Globerson, T. Koo, X. Carreras, and
P. Bartlett. 2008. Exponentiated gradient algorithms
for conditional random fields and max-margin Markov
networks. Journal of Machine Learning Research, 9.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
Perceptron algorithms. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
J. R. Finkel, A. Kleeman, and C. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
In Human Language Technology and Association for
Computational Linguistics (HLT/ACL).
D. Gildea and T. Hofmann. 1999. Topic-based language
models using EM. In Eurospeech.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics (ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Con-
textual dependencies in unsupervised word segmenta-
tion. In International Conference on Computational
Linguistics and Association for Computational Lin-
guistics (COLING/ACL).
M. Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers? In Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP/CoNLL).
M. Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguistic
structure. In Human Language Technology and As-
sociation for Computational Linguistics (HLT/ACL),
pages 398–406.
J. Kuo, H. Li, and C. Lin. 2008. Mining transliterations
from web query results: An incremental approach. In
Sixth SIGHAN Workshop on Chinese Language Pro-
cessing.
P. Liang and D. Klein. 2008. Analyzing the errors
of unsupervised learning. In Human Language Tech-
nology and Association for Computational Linguistics
(HLT/ACL).
P. Liang, D. Klein, and M. I. Jordan. 2008. Agreement-
based learning. In Advances in Neural Information
Processing Systems (NIPS).
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Association for Computational Linguistics (ACL).
R. Neal and G. Hinton. 1998. A view of the EM algo-
rithm that justifies incremental, sparse, and other vari-
ants. In Learning in Graphical Models.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29:19–51.
M. Sato and S. Ishii. 2000. On-line EM algorithm for the
normalized Gaussian network. Neural Computation,
12:407–432.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In Association for Computational Linguistics (ACL).
S. Shalev-Shwartz and N. Srebro. 2008. SVM optimiza-
tion: Inverse dependence on training set size. In Inter-
national Conference on Machine Learning (ICML).
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27:351–372.
</reference>
<page confidence="0.99866">
619
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.617931">
<title confidence="0.999617">Online EM for Unsupervised Models</title>
<author confidence="0.999998">Percy Liang Dan Klein</author>
<affiliation confidence="0.9953625">Computer Science Division, EECS University of California at</affiliation>
<address confidence="0.701523">Berkeley, CA</address>
<email confidence="0.959737">1pliang,kleinj@cs.berkeley.edu</email>
<abstract confidence="0.989791">The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence. In this paper, we show that online variants (1) provide speedups and (2) can even find betthan those found by batch EM. We support these findings on four unsupervised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bottou</author>
<author>O Bousquet</author>
</authors>
<title>The tradeoffs of large scale learning.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="28266" citStr="Bottou and Bousquet, 2008" startWordPosition="4904" endWordPosition="4907">clear trend in the supervised NLP literature— examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are often faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models—for example, Gold</context>
</contexts>
<marker>Bottou, Bousquet, 2008</marker>
<rawString>L. Bottou and O. Bousquet. 2008. The tradeoffs of large scale learning. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Capp´e</author>
<author>E Moulines</author>
</authors>
<title>Online expectationmaximization algorithm for latent data models.</title>
<date>2009</date>
<journal>Journal of the Royal Statistics Society: Series B (Statistical Methodology),</journal>
<volume>71</volume>
<marker>Capp´e, Moulines, 2009</marker>
<rawString>O. Capp´e and E. Moulines. 2009. Online expectationmaximization algorithm for latent data models. Journal of the Royal Statistics Society: Series B (Statistical Methodology), 71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>A Globerson</author>
<author>T Koo</author>
<author>X Carreras</author>
<author>P Bartlett</author>
</authors>
<title>Exponentiated gradient algorithms for conditional random fields and max-margin Markov networks.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<contexts>
<context position="14210" citStr="Collins et al., 2008" startWordPosition="2501" endWordPosition="2504">itialize all si = 0 for iEM, then both algorithms make the same updates on the first pass through the data. They diverge thereafter as iEM subtracts out old sis, while sEM does not even remember them. One weakness of iEM is that its memory requirements grow linearly with the number of examples due to storing s1, ... , sn. For large datasets, these sis might not even fit in memory, and resorting to physical disk would be very slow. In contrast, the memory usage of sEM does not depend on n. The relationship between iEM and sEM (with m = 1) is analogous to the one between exponentiated gradient (Collins et al., 2008) and stochastic gradient for supervised learning of log-linear models. The former maintains the sufficient statistics of each example and subtracts out old ones whereas the latter does not. In the supervised case, the added stability of exponentiated gradient tends to yield better performance. For the unsupervised case, we will see empirically that remembering the old sufficient statistics offers no benefit, and much better performance can be obtained by properly setting (α, m) for sEM (Section 4). 4 Experiments We now present our empirical results for batch EM and online EM (iEM and sEM) on t</context>
<context position="27867" citStr="Collins et al., 2008" startWordPosition="4837" endWordPosition="4840">ly smaller than the variance due to permutation. sEMt is less sensitive to initialization than EM, but additional variance is created by randomly permuting the examples. Overall, the accuracy of sEMt is more variable than that of EM, but not by a large amount. 5 Discussion and related work As datasets increase in size, the demand for online algorithms has grown in recent years. One sees this clear trend in the supervised NLP literature— examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are often faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Ku</context>
</contexts>
<marker>Collins, Globerson, Koo, Carreras, Bartlett, 2008</marker>
<rawString>M. Collins, A. Globerson, T. Koo, X. Carreras, and P. Bartlett. 2008. Exponentiated gradient algorithms for conditional random fields and max-margin Markov networks. Journal of Machine Learning Research, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with Perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="27756" citStr="Collins, 2002" startWordPosition="4823" endWordPosition="4824">y small amount of noise to initialize the parameters, the variance due to initialization is systematically smaller than the variance due to permutation. sEMt is less sensitive to initialization than EM, but additional variance is created by randomly permuting the examples. Overall, the accuracy of sEMt is more variable than that of EM, but not by a large amount. 5 Discussion and related work As datasets increase in size, the demand for online algorithms has grown in recent years. One sees this clear trend in the supervised NLP literature— examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are often faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online me</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with Perceptron algorithms. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>A Kleeman</author>
<author>C Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Human Language Technology and Association for Computational Linguistics (HLT/ACL).</booktitle>
<contexts>
<context position="27935" citStr="Finkel et al., 2008" startWordPosition="4846" endWordPosition="4849">ve to initialization than EM, but additional variance is created by randomly permuting the examples. Overall, the accuracy of sEMt is more variable than that of EM, but not by a large amount. 5 Discussion and related work As datasets increase in size, the demand for online algorithms has grown in recent years. One sees this clear trend in the supervised NLP literature— examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are often faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does r</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>J. R. Finkel, A. Kleeman, and C. Manning. 2008. Efficient, feature-based, conditional random field parsing. In Human Language Technology and Association for Computational Linguistics (HLT/ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>T Hofmann</author>
</authors>
<title>Topic-based language models using EM.</title>
<date>1999</date>
<booktitle>In Eurospeech.</booktitle>
<contexts>
<context position="28463" citStr="Gildea and Hofmann, 1999" startWordPosition="4934" endWordPosition="4937">ithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are often faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models—for example, Goldwater et al. (2006) and Goldwater and Griffiths (2007). These samplers keep track of a sample of the latent variables for each example, akin to the sufficient statistics that we store in incrementa</context>
</contexts>
<marker>Gildea, Hofmann, 1999</marker>
<rawString>D. Gildea and T. Hofmann. 1999. Topic-based language models using EM. In Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="28920" citStr="Goldwater and Griffiths (2007)" startWordPosition="5004" endWordPosition="5007">bro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models—for example, Goldwater et al. (2006) and Goldwater and Griffiths (2007). These samplers keep track of a sample of the latent variables for each example, akin to the sufficient statistics that we store in incremental EM. In contrast, stepwise EM does not require this storage and operates more in the spirit of a truly online algorithm. Besides speed, online algorithms are of interest for two additional reasons. First, in some applications, we receive examples sequentially and would like to estimate a model in real-time, e.g., in the clustering of news articles. Second, since humans learn sequentially, studying online EM might suggest new connections to cognitive me</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>S. Goldwater and T. Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
<author>M Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL).</booktitle>
<contexts>
<context position="6097" citStr="Goldwater et al. (2006)" startWordPosition="1003" endWordPosition="1006">ce into words z = (z1, ... , z|,|), where each segment (word) zi is a contiguous subsequence of 1, ... , `. Since the naive unigram model has a degenerate maximum likelihood solution that makes each sentence a separate word, we incorporate a penalty for longer segments: p(x, z; θ) . ri|, |k=1 p(xzk; θ)e−|zk|β, where β &gt; 1 determines the strength of the penalty. For English, we used β = 1.6; Chinese, β = 2.5. To speed up inference, we restricted the maximum segment length to 10 for English and 5 for Chinese. We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al. (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences). We evaluated using F1 on word tokens. To the best of our knowledge, our penalized unigram model is new and actually beats the more complicated model of Johnson (2008) 83.5% to 78%, which had been the best published result on this task. Word alignment For each pair of translated sentences x = (e1,... , ene, f1, ... , fnf), we wish to predict the word alignments z E 10, 1jnenf. We trained two IBM model 1s using agreement-based learning (Liang et al., 2</context>
<context position="28885" citStr="Goldwater et al. (2006)" startWordPosition="4999" endWordPosition="5002">2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models—for example, Goldwater et al. (2006) and Goldwater and Griffiths (2007). These samplers keep track of a sample of the latent variables for each example, akin to the sufficient statistics that we store in incremental EM. In contrast, stepwise EM does not require this storage and operates more in the spirit of a truly online algorithm. Besides speed, online algorithms are of interest for two additional reasons. First, in some applications, we receive examples sequentially and would like to estimate a model in real-time, e.g., in the clustering of news articles. Second, since humans learn sequentially, studying online EM might sugg</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>S. Goldwater, T. Griffiths, and M. Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL).</booktitle>
<contexts>
<context position="1137" citStr="Johnson, 2007" startWordPosition="168" endWordPosition="169">-of-speech tagging, document classification, word segmentation, and word alignment. 1 Introduction In unsupervised NLP tasks such as tagging, parsing, and alignment, one wishes to induce latent linguistic structures from raw text. Probabilistic modeling has emerged as a dominant paradigm for these problems, and the EM algorithm has been a driving force for learning models in a simple and intuitive manner. However, on some tasks, EM can converge slowly. For instance, on unsupervised part-ofspeech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). The slowness of EM is mainly due to its batch nature: Parameters are updated only once after each pass through the data. When parameter estimates are still rough or if there is high redundancy in the data, computing statistics on the entire dataset just to make one update can be wasteful. In this paper, we investigate two flavors of online EM—incremental EM (Neal and Hinton, 1998) and stepwise EM (Sato and Ishii, 2000; Capp´e and Moulines, 2009), both of which involve updating parameters after each example or after a mini-batch (subset) of examples. Online algorithms have the potential to sp</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>M. Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Human Language Technology and Association for Computational Linguistics (HLT/ACL),</booktitle>
<pages>398--406</pages>
<contexts>
<context position="6409" citStr="Johnson (2008)" startWordPosition="1058" endWordPosition="1059"> 1 determines the strength of the penalty. For English, we used β = 1.6; Chinese, β = 2.5. To speed up inference, we restricted the maximum segment length to 10 for English and 5 for Chinese. We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al. (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences). We evaluated using F1 on word tokens. To the best of our knowledge, our penalized unigram model is new and actually beats the more complicated model of Johnson (2008) 83.5% to 78%, which had been the best published result on this task. Word alignment For each pair of translated sentences x = (e1,... , ene, f1, ... , fnf), we wish to predict the word alignments z E 10, 1jnenf. We trained two IBM model 1s using agreement-based learning (Liang et al., 2008). We used the first 30K sentence pairs of the English-French Hansards data from the NAACL 2003 Shared Task, 447+37 of which were hand-aligned (Och and Ney, 2003). We evaluated using the standard alignment error rate (AER). 3 EM algorithms Given a probabilistic model p(x, z; θ) and unlabeled examples x(1), .</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>M. Johnson. 2008. Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure. In Human Language Technology and Association for Computational Linguistics (HLT/ACL), pages 398–406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kuo</author>
<author>H Li</author>
<author>C Lin</author>
</authors>
<title>Mining transliterations from web query results: An incremental approach.</title>
<date>2008</date>
<booktitle>In Sixth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="28482" citStr="Kuo et al., 2008" startWordPosition="4938" endWordPosition="4941">8), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are often faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models—for example, Goldwater et al. (2006) and Goldwater and Griffiths (2007). These samplers keep track of a sample of the latent variables for each example, akin to the sufficient statistics that we store in incremental EM. In contrast, </context>
</contexts>
<marker>Kuo, Li, Lin, 2008</marker>
<rawString>J. Kuo, H. Li, and C. Lin. 2008. Mining transliterations from web query results: An incremental approach. In Sixth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Analyzing the errors of unsupervised learning.</title>
<date>2008</date>
<booktitle>In Human Language Technology and Association for Computational Linguistics (HLT/ACL).</booktitle>
<contexts>
<context position="2379" citStr="Liang and Klein, 2008" startWordPosition="371" endWordPosition="374">aking updates more frequently. However, these updates can be seen as noisy approximations to the full batch update, and this noise can in fact impede learning. This tradeoff between speed and stability is familiar to online algorithms for convex supervised learning problems—e.g., Perceptron, MIRA, stochastic gradient, etc. Unsupervised learning raises two additional issues: (1) Since the EM objective is nonconvex, we often get convergence to different local optima of varying quality; and (2) we evaluate on accuracy metrics which are at best loosely correlated with the EM likelihood objective (Liang and Klein, 2008). We will see that these issues can lead to surprising results. In Section 4, we present a thorough investigation of online EM, mostly focusing on stepwise EM since it dominates incremental EM. For stepwise EM, we find that choosing a good stepsize and mini-batch size is important but can fortunately be done adequately without supervision. With a proper choice, stepwise EM reaches the same performance as batch EM, but much more quickly. Moreover, it can even surpass the performance of batch EM. Our results are particularly striking on part-of-speech tagging: Batch EM crawls to an accuracy of 5</context>
<context position="21429" citStr="Liang and Klein, 2008" startWordPosition="3738" endWordPosition="3741">` POS 57.3 59.6 -6.03 -6.08 DOC 39.1 47.8 -7.96 -7.88 SEG(en) 80.5 80.7 -4.11 -4.11 SEG(ch) 78.2 77.2 -7.27 -7.28 ALIGN 78.8 78.9 -5.05 -5.12 (e) Word alignment (f) Results after convergence Figure 1: Accuracy and log-likelihood plots for batch EM, incremental EM, and stepwise EM across all five datasets. sEMP outperforms batch EM in terms of convergence speed and even accuracy and likelihood; iEM and sEMi fail in some cases. We did not run iEM on POS tagging due to memory limitations; we expect the performance would be similar to sEMi, which is not very encouraging (Section 4.3). correlated (Liang and Klein, 2008). But it does suggest that stepwise EM is injecting a bias that favors accuracy over likelihood—a bias not at all reflected in the training objective. We can create a hybrid (sEMa+EM) that combines the strengths of both sEMa and EM: First run sEMa for 5 iterations, which quickly takes us to a part of the parameter space yielding good accuracies; then run EM, which quickly improves the likelihood. Fortunately, accuracy does not degrade as likelihood increases (Figure 2). 4.3 Varying the optimization parameters Recall that stepwise EM requires setting two optimization parameters: the stepsize re</context>
</contexts>
<marker>Liang, Klein, 2008</marker>
<rawString>P. Liang and D. Klein. 2008. Analyzing the errors of unsupervised learning. In Human Language Technology and Association for Computational Linguistics (HLT/ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
<author>M I Jordan</author>
</authors>
<title>Agreementbased learning.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="6701" citStr="Liang et al., 2008" startWordPosition="1111" endWordPosition="1114">r et al. (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences). We evaluated using F1 on word tokens. To the best of our knowledge, our penalized unigram model is new and actually beats the more complicated model of Johnson (2008) 83.5% to 78%, which had been the best published result on this task. Word alignment For each pair of translated sentences x = (e1,... , ene, f1, ... , fnf), we wish to predict the word alignments z E 10, 1jnenf. We trained two IBM model 1s using agreement-based learning (Liang et al., 2008). We used the first 30K sentence pairs of the English-French Hansards data from the NAACL 2003 Shared Task, 447+37 of which were hand-aligned (Och and Ney, 2003). We evaluated using the standard alignment error rate (AER). 3 EM algorithms Given a probabilistic model p(x, z; θ) and unlabeled examples x(1), ... , x(n), recall we would like to maximize the marginal likelihood of the data (1). Let φ(x, z) denote a mapping from a fullylabeled example (x, z) to a vector of sufficient statistics (counts in the case of multinomials) for the model. For example, one component of this vector for HMMs wou</context>
</contexts>
<marker>Liang, Klein, Jordan, 2008</marker>
<rawString>P. Liang, D. Klein, and M. I. Jordan. 2008. Agreementbased learning. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="27809" citStr="McDonald et al., 2005" startWordPosition="4829" endWordPosition="4832">ameters, the variance due to initialization is systematically smaller than the variance due to permutation. sEMt is less sensitive to initialization than EM, but additional variance is created by randomly permuting the examples. Overall, the accuracy of sEMt is more variable than that of EM, but not by a large amount. 5 Discussion and related work As datasets increase in size, the demand for online algorithms has grown in recent years. One sees this clear trend in the supervised NLP literature— examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are often faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incrementa</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Neal</author>
<author>G Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1998</date>
<booktitle>In Learning in Graphical Models.</booktitle>
<contexts>
<context position="1522" citStr="Neal and Hinton, 1998" startWordPosition="235" endWordPosition="238"> a simple and intuitive manner. However, on some tasks, EM can converge slowly. For instance, on unsupervised part-ofspeech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). The slowness of EM is mainly due to its batch nature: Parameters are updated only once after each pass through the data. When parameter estimates are still rough or if there is high redundancy in the data, computing statistics on the entire dataset just to make one update can be wasteful. In this paper, we investigate two flavors of online EM—incremental EM (Neal and Hinton, 1998) and stepwise EM (Sato and Ishii, 2000; Capp´e and Moulines, 2009), both of which involve updating parameters after each example or after a mini-batch (subset) of examples. Online algorithms have the potential to speed up learning by making updates more frequently. However, these updates can be seen as noisy approximations to the full batch update, and this noise can in fact impede learning. This tradeoff between speed and stability is familiar to online algorithms for convex supervised learning problems—e.g., Perceptron, MIRA, stochastic gradient, etc. Unsupervised learning raises two additio</context>
<context position="9222" citStr="Neal and Hinton, 1998" startWordPosition="1573" endWordPosition="1577">is trivial, we represent it implicitly by θ(·) in order to concentrate on the computation of the sufficient statistics. This focus will be important for online EM, so writing batch EM in this way accentuates the parallel between batch and online. 3.2 Online EM To obtain an online EM algorithm, we store a single set of sufficient statistics µ and update it after processing each example. For the i-th example, we compute sufficient statistics s0i. There are two main variants of online EM algorithms which differ in exactly how the new s0i is incorporated into µ. The first is incremental EM (iEM) (Neal and Hinton, 1998), in which we not only keep track of µ but also the sufficient statistics s1, ... , sn for each example (µ = Eni=1 si). When we process example i, we subtract out the old si and add the new s0i. Sato and Ishii (2000) developed another variant, later generalized by Capp´e and Moulines (2009), which we call stepwise EM (sEM). In sEM, we interpolate between µ and s0i based on a stepsize ηk (k is the number of updates made to µ so far). The two algorithms are motivated in different ways. Recall that the log-likelihood can be lower Stepwise EM (sEM) µ ← initialization; k = 0 for each iteration t = </context>
<context position="13049" citStr="Neal and Hinton, 1998" startWordPosition="2291" endWordPosition="2294">s0 i, taking comfort in the fact that θ(µ) = θ(S). For both iEM and sEM, we also need to efficiently compute θ(µ). We can do this by maintaining the normalizer for each multinomial block (sum of the components in the block). This extra maintenance only doubles the number of updates we have to make but allows us to fetch any component of θ(µ) in constant time by dividing out the normalizer. 3.3 Incremental versus stepwise EM Incremental EM increases L monotonically after each update by virtue of doing coordinate-wise ascent and thus is guaranteed to converge to a local optimum of both L and ` (Neal and Hinton, 1998). However, ` is not guaranteed to increase after each update. Stepwise EM might not increase either L or ` after each update, but it is guaranteed to converge to a local optimum of ` given suitable conditions on the stepsize discussed earlier. Incremental and stepwise EM actually coincide under the following setting (Capp´e and Moulines, 2Note that running sEM with m = n is similar but not equivalent to batch EM since old sufficient statistics are still interpolated rather than replaced. 2009): If we set (α, m) = (1, 1) for sEM and initialize all si = 0 for iEM, then both algorithms make the s</context>
</contexts>
<marker>Neal, Hinton, 1998</marker>
<rawString>R. Neal and G. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Learning in Graphical Models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="6862" citStr="Och and Ney, 2003" startWordPosition="1138" endWordPosition="1141">e evaluated using F1 on word tokens. To the best of our knowledge, our penalized unigram model is new and actually beats the more complicated model of Johnson (2008) 83.5% to 78%, which had been the best published result on this task. Word alignment For each pair of translated sentences x = (e1,... , ene, f1, ... , fnf), we wish to predict the word alignments z E 10, 1jnenf. We trained two IBM model 1s using agreement-based learning (Liang et al., 2008). We used the first 30K sentence pairs of the English-French Hansards data from the NAACL 2003 Shared Task, 447+37 of which were hand-aligned (Och and Ney, 2003). We evaluated using the standard alignment error rate (AER). 3 EM algorithms Given a probabilistic model p(x, z; θ) and unlabeled examples x(1), ... , x(n), recall we would like to maximize the marginal likelihood of the data (1). Let φ(x, z) denote a mapping from a fullylabeled example (x, z) to a vector of sufficient statistics (counts in the case of multinomials) for the model. For example, one component of this vector for HMMs would be the number of times state 7 emits the word “house” in sentence x with state sequence z. Given a vector of sufficient statistics µ, let θ(µ) denote the maxi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sato</author>
<author>S Ishii</author>
</authors>
<title>On-line EM algorithm for the normalized Gaussian network.</title>
<date>2000</date>
<journal>Neural Computation,</journal>
<pages>12--407</pages>
<contexts>
<context position="1560" citStr="Sato and Ishii, 2000" startWordPosition="242" endWordPosition="245"> on some tasks, EM can converge slowly. For instance, on unsupervised part-ofspeech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). The slowness of EM is mainly due to its batch nature: Parameters are updated only once after each pass through the data. When parameter estimates are still rough or if there is high redundancy in the data, computing statistics on the entire dataset just to make one update can be wasteful. In this paper, we investigate two flavors of online EM—incremental EM (Neal and Hinton, 1998) and stepwise EM (Sato and Ishii, 2000; Capp´e and Moulines, 2009), both of which involve updating parameters after each example or after a mini-batch (subset) of examples. Online algorithms have the potential to speed up learning by making updates more frequently. However, these updates can be seen as noisy approximations to the full batch update, and this noise can in fact impede learning. This tradeoff between speed and stability is familiar to online algorithms for convex supervised learning problems—e.g., Perceptron, MIRA, stochastic gradient, etc. Unsupervised learning raises two additional issues: (1) Since the EM objective</context>
<context position="9438" citStr="Sato and Ishii (2000)" startWordPosition="1620" endWordPosition="1623">lel between batch and online. 3.2 Online EM To obtain an online EM algorithm, we store a single set of sufficient statistics µ and update it after processing each example. For the i-th example, we compute sufficient statistics s0i. There are two main variants of online EM algorithms which differ in exactly how the new s0i is incorporated into µ. The first is incremental EM (iEM) (Neal and Hinton, 1998), in which we not only keep track of µ but also the sufficient statistics s1, ... , sn for each example (µ = Eni=1 si). When we process example i, we subtract out the old si and add the new s0i. Sato and Ishii (2000) developed another variant, later generalized by Capp´e and Moulines (2009), which we call stepwise EM (sEM). In sEM, we interpolate between µ and s0i based on a stepsize ηk (k is the number of updates made to µ so far). The two algorithms are motivated in different ways. Recall that the log-likelihood can be lower Stepwise EM (sEM) µ ← initialization; k = 0 for each iteration t = 1, ... , T: for each example i = 1, ... , n in random order: s0i ← E. p(z |x(i); θ(µ)) φ(x(i), z) [inference] µ ← (1−ηk)µ + ηks0i; k ← k+1 [towards new] bounded as follows (Neal and Hinton, 1998): `(θ) ≥ L(q1, ... , </context>
</contexts>
<marker>Sato, Ishii, 2000</marker>
<rawString>M. Sato and S. Ishii. 2000. On-line EM algorithm for the normalized Gaussian network. Neural Computation, 12:407–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Seginer</author>
</authors>
<title>Fast unsupervised incremental parsing.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<marker>Seginer, 2007</marker>
<rawString>Y. Seginer. 2007. Fast unsupervised incremental parsing. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shalev-Shwartz</author>
<author>N Srebro</author>
</authors>
<title>SVM optimization: Inverse dependence on training set size.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="28300" citStr="Shalev-Shwartz and Srebro, 2008" startWordPosition="4908" endWordPosition="4911">ed NLP literature— examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are often faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models—for example, Goldwater et al. (2006) and Goldwater </context>
</contexts>
<marker>Shalev-Shwartz, Srebro, 2008</marker>
<rawString>S. Shalev-Shwartz and N. Srebro. 2008. SVM optimization: Inverse dependence on training set size. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Venkataraman</author>
</authors>
<title>A statistical model for word discovery in transcribed speech.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--351</pages>
<marker>Venkataraman, 2001</marker>
<rawString>A. Venkataraman. 2001. A statistical model for word discovery in transcribed speech. Computational Linguistics, 27:351–372.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>