<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.135319">
<title confidence="0.9960255">
Multi-Domain Sentiment Relevance Classification with Automatic
Representation Learning
</title>
<author confidence="0.986113">
Christian Scheible Hinrich Sch¨utze
</author>
<affiliation confidence="0.977988666666667">
Institut f¨ur Maschinelle Sprachverarbeitung Center for Information
University of Stuttgart and Language Processing
scheibcn@ims.uni-stuttgart.de University of Munich
</affiliation>
<sectionHeader confidence="0.979297" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999799125">
Sentiment relevance (SR) aims at identify-
ing content that does not contribute to sen-
timent analysis. Previously, automatic SR
classification has been studied in a limited
scope, using a single domain and feature
augmentation techniques that require large
hand-crafted databases. In this paper, we
present experiments on SR classification
with automatically learned feature repre-
sentations on multiple domains. We show
that a combination of transfer learning and
in-task supervision using features learned
unsupervisedly by the stacked denoising
autoencoder significantly outperforms a
bag-of-words baseline for in-domain and
cross-domain classification.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943596491228">
Many approaches to sentiment analysis rely on
term-based clues to detect the polarity of sentences
or documents, using the bag-of-words (BoW)
model (Wang and Manning, 2012). One drawback
of this approach is that the polarity of a clue is
often treated as fixed, which can be problematic
when content is not intended to contribute to the
polarity of the entity but contains a term with a
known lexical non-neutral polarity.
For example, movie reviews often have plot
summaries which contain subjective descriptions,
e.g., “April loves her new home and friends.”, con-
taining “loves”, commonly a subjective positive
term. Other domains contain different types of
nonrelevant content: Music reviews may contain
track listings, product reviews on retail platforms
contain complaints that do not concern the prod-
uct, e.g., about shipping and handling. Filtering
such nonrelevant content can help to improve sen-
timent analysis (Pang and Lee, 2004). Sentiment
relevance (Scheible and Sch¨utze, 2013; Taboada
et al., 2009; T¨ackstr¨om and McDonald, 2011) for-
malizes this distinction: Content that contributes
to the overall sentiment of a document is said to
be sentiment relevant (SR), other content is senti-
ment nonrelevant (SNR).
The main bottleneck in automatic SR classifi-
cation is the lack of annotated data. On the sen-
tence level, it has been attempted for the movie
review domain (Scheible and Sch¨utze, 2013) on
a manually annotated dataset that covers around
3,500 sentences. The sentiment analysis data by
T¨ackstr¨om and McDonald (2011) contains SR an-
notations for five product review domains, four of
which have fewer than 1,000 annotated examples.
As the amount of labeled data is low, we adopt
transfer learning (TL, (Thrun, 1995)), which has
been used before for SR classification. In this
setup, we train a classifier on a different task, using
subjectivity-labeled data – for which a large num-
ber of annotated examples is available – and ap-
ply it for SR classification. To enable knowledge
transfer between the tasks, feature space augmen-
tation has been proposed. For this purpose, we em-
ploy automatic representation learning, using the
stacked denoising autoencoder (SDA, (Vincent et
al., 2010)) which has been applied successfully to
other domain adaptation problems such as cross-
domain sentiment analysis (Glorot et al., 2011).
In this paper, we present experiments on both
multi-domain and cross-domain SR classification.
We show that compared to the in-domain base-
line, TL with SDA features increases F1 by 6.8%
on average. We find that domain adaptation using
TL with the SDA compensates for strong domain
shifts, reducing the average classification transfer
loss by 12.7%.
</bodyText>
<sectionHeader confidence="0.944244" genericHeader="method">
2 Stacked Denoising Autoencoders
</sectionHeader>
<bodyText confidence="0.996843">
The stacked denoising autoencoder (SDA, (Vin-
cent et al., 2010)) is a neural network (NN) model
for unsupervised feature representation learning.
</bodyText>
<page confidence="0.93562">
200
</page>
<note confidence="0.680199">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 200–204,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999517727272727">
An autoencoder takes an input vector x, uses an
NN layer with a (possibly) nonlinear activation
function to generate a hidden feature representa-
tion h. A second NN layer reconstructs x at the
output, minimizing the error.
Denoising autoencoders reconstruct x from a
corrupted version of the input, ˜x. As the model
learns to be robust to noise, the representations are
expected to generalize better. For discrete data,
masking noise is a natural choice, where each in-
put unit is randomly set to 0 with probability p.
Autoencoders can be stacked by using the hi
produced by the ith autoencoder as the input to
the (i+1)th one, yielding the representation hi+1.
The h of the topmost autoencoder is the final rep-
resentation output by the SDA. We let k-SDA de-
note a stack of k denoising autoencoders.
Chen et al. (2012) introduced a marginalized
closed-form version, the mSDA. We opt for this
version as it is faster to train and allows us to use
the full feature space which would be inefficient
with iterative backpropagation training.
</bodyText>
<sectionHeader confidence="0.988244" genericHeader="method">
3 Task and Experimental Setup
</sectionHeader>
<bodyText confidence="0.999677037037037">
The task in this paper is multi- and cross-domain
SR classification. Two aspects motivate our work:
First, we need to address the sparse data situa-
tion. Second, we are interested in how cross-
domain effects influence SR classification. We
classify SR in three different setups: in-domain
(ID), in which we take the training and test data
from the same domain; domain adaptation (DA),
where training and test data are from different do-
mains; and transfer learning (TL), where we use a
much larger amount of data from a different but re-
lated task. To improve the generalization capabili-
ties of the models, we use representations learned
by the SDA. We will next describe our classifica-
tion setup in more detail.
Data We use the following datasets for our ex-
periments. Table 1 shows statistics on the datasets.
CINEMA: The movie SR data (CINEMA)
by Scheible and Sch¨utze (2013) contains SR-
annotated sentences for the movie review domain.
Ambiguous sentences are marked as unknown; we
exclude them.
PRODUCTS: The multi-domain product data
(PRODUCTS) by T¨ackstr¨om and McDonald (2011)
contains labeled sentences from five Amazon.com
product review domains: BOOKS, DVDS, electron-
ics (EL), MUSIC, and video games (VG). This
</bodyText>
<table confidence="0.9994158">
Dataset #doc #sent #SR #SNR
CINEMA 125 3,487 2,759 728
PRODUCTS 294 3,836 2,689 1,147
–BOOKS 59 739 424 315
–DVDS 59 799 524 275
–ELECTRONICS 57 628 491 137
–MUSIC 59 638 448 190
–VIDEOGAMES 60 1032 802 230
P&amp;L – 10,000 5,000 5,000
UNLAB 7,500 68,927 – –
</table>
<tableCaption confidence="0.999798">
Table 1: Dataset statistics
</tableCaption>
<bodyText confidence="0.999943825">
dataset differs from CINEMA firstly in the product
domains (except obviously for DVDS which also
covers movies). Secondly, the data was collected
from a retail site, which introduces further facets
of sentiment nonrelevance, as discussed above.
Thirdly, the annotation style has no unknown cat-
egory: ambiguous examples are marked as SR.
P&amp;L: The subjectivity data (P&amp;L) by Pang and
Lee (2004) serves as our cross-task training data
for transfer learning. The dataset was heuristically
created for subjectivity detection on the movie do-
main by sampling snippets from Rotten Tomatoes
as subjective and sentences from IMDb plot sum-
maries as objective examples.
UNLAB: To improve generalization on PROD-
UCTS, we use additional unlabeled sentences
(UNLAB) for SDA training. We extract the sen-
tences of 1,500 randomly selected documents for
each of the five domains from the Amazon.com
review data by Jindal and Liu (2008).
SDA setup We train the SDA with 10,000 hid-
den units and tanh nonlinearity on the BoW fea-
tures of all available data as the input. We opti-
mize the noise level p with 2-fold cross-validation
on the in-domain training folds.
Classification setup We perform SR classifica-
tion with a linear support vector machine (SVM)
using LIBLINEAR (Chang and Lin, 2011). We
perform 2-fold cross-validation for all training
data but P&amp;L. We report overall macro-averaged
F1 over both folds. The feature representation for
the SVM is either bag of words (BoW) or the k-
SDA output. Unlike Chen et al. (2012), we do not
use concatenations of BoW and SDA vectors as
we found them to perform worse.
Evaluation As class distributions are heavily
skewed, we use macro-averaged F1(s, t) (train-
ing on s and evaluating on t) as the basic eval-
uation measure. We evaluate DA with transfer
loss, the difference in F1 of a classifier CL with
</bodyText>
<page confidence="0.983857">
201
</page>
<table confidence="0.999947363636364">
Features Setup CINEMA BOOKS DVDS EL MUSIC VG 0
1 Majority BL – 39.6 28.9 32.6 39.2 35.1 39.0 35.7
2 BoW ID 74.0 57.5 49.8 55.1 55.5 55.0 58.4
3 1-SDA ID 73.6 55.3 48.4 43.8 41.8 44.1 52.6
4 2-SDA ID 76.0 54.5 52.5 43.9 41.2 46.7 53.6
5 BoW TL 71.5 60.7 60.2 50.3 55.1 53.2 59.6
6 1-SDA TL 73.3 62.9 60.6 59.0 59.9 57.0 63.1
7 2-SDA TL 76.2 62.9 65.8 59.7 59.9 60.5 64.9
8 BoW ID+TL 76.6 63.5 61.7 52.4 56.7 57.0 62.3
9 1-SDA ID+TL 79.0 62.7 62.1 57.7 57.8 57.4 63.9
10 2-SDA ID+TL 80.4 62.7 65.2 59.0 58.7 58.9 65.2
</table>
<tableCaption confidence="0.97643">
Table 2: Macro-averaged F1 (%) evaluating on each test domain on both folds. 0 = row mean. Bold:
best result in each column and results in that column not significantly different from it.
</tableCaption>
<bodyText confidence="0.87709">
respect to the in-domain baseline BL: L(s, t) =
</bodyText>
<equation confidence="0.968678">
F (BL)
1 (t, t)−Fl CL) (s, t). L is negative if the clas-
</equation>
<bodyText confidence="0.99872025">
sifier surpasses the baseline. As a statistical sig-
nificance test (indicated by † in the text), we use
approximate randomization (Noreen, 1989) with
10,000 iterations at p &lt; 0.05.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999517409090909">
In-Domain Classification (ID) Table 2 shows
macro-averaged F1 for different SR models. We
first turn to fully supervised SR classification with
bag-of-words (BoW) features using ID training
(line 2). While the results for CINEMA are high,
on par with the reported results in related work,
they are low for the PRODUCTS data. This is not
surprising as the SVM is trained with fewer than
600 examples on each domain. Also, no unknown
category exists in the latter dataset. While am-
biguous examples on CINEMA are annotated as un-
known, they receive an SR label on PRODUCTS.
Thus, many examples are ambiguous and thus dif-
ficult to classify. SDA features worsen results
significantly† (lines 3–4) on all domains except
CINEMA and DVDS due to data sparsity. They are
the two most homogeneous domains where plot
descriptions make up a large part of the SNR con-
tent. On many domains, there is no single proto-
typical type of SNR which could be learned from
a small amount of training data.
Transfer Learning (TL) TL with training on
P&amp;L and evaluation on CINEMA/PRODUCTS with
BoW features (line 5) performs slightly worse than
ID classification, except on BOOKS and DVDS
where we see strong improvements. This result
is easy to explain: Both BOOKS and DVDS contain
SNR descriptions of narratives, which are covered
well in P&amp;L. This distinction is less helpful on
domains like EL where SNR content is different,
so we achieve worse results even with the much
larger P&amp;L data.
We find that 1-SDA (line 6) already performs
significantly† better than the ID baseline on all do-
mains except CINEMA which has a much larger
amount of ID training data available than the other
domains (approx. 1700 sentences vs. fewer than
600). Using stacking, 2-SDA (line 7) improves
the results on three domains significantly,† and
performs on par with the ID classifier on CIN-
EMA. We found that stack depths of k &gt; 2 do
not significantly† increase performance.
Finally, we try a combination of ID and TL
(ID+TL), training on both P&amp;L and the respective
ID training fold of CINEMA/PRODUCTS. The re-
sults for this experiment are shown in lines 8–10
in Table 2. Comparing BoW models, we beat both
ID and TL across all domains (lines 2 and 5). With
SDA features, we are able to beat ID for CINEMA.
The results on the other domains are comparable
to plain TL. This is a promising result, showing
that with SDA features, ID+TL performs as well
as or better than plain TL. This property could be
exploited for domains where labeled data is not
available. We will show below that SDA features
become important when we apply ID+TL to do-
main adaptation.
We also conducted experiments using only the
5,000 most frequent features but found that the
SDA does not generalize well from this input rep-
resentation, particularly on EL and MUSIC. This
confirms that in SR, rare features make an im-
portant contribution (such as named entities in the
movie domain).
Domain Adaptation (DA) We now evaluate the
task in a DA setting, comparing the ID and ID+TL
</bodyText>
<page confidence="0.988049">
202
</page>
<figure confidence="0.973624808510638">
BoW ID 2−SDA ID BOW ID+TL 2−SDA ID+TL
25
20
15
10
5
0
Transfer Loss (%)
−5
−10
−15
CC
BC
DC
EC
MC
VC
CB
BB
DB
EB
MB
VB
CD
BD
DD
ED
MD
VD
CE
BE
DE
EE
ME
VE
CM
BM
DM
EM
MM
VM
CV
BV
DV
EV
MV
VV
</figure>
<figureCaption confidence="0.705361333333333">
Figure 1: Transfer losses (%) for DA. Training-test pairs grouped by target domain and abbreviated by
first letter (e.g., CD: training on CINEMA, evaluating on DVDS). In-domain results shown for comparison
to Table 2.
</figureCaption>
<bodyText confidence="0.999088911764706">
setups with BoW and 2-SDA features. We mea-
sure the transfer losses we suffer from training on
one domain and evaluating on another (Figure 1).
The overall picture is the same as above: ID+TL
2-SDA models perform best. In the baseline BoW
ID setup, domain shifts have a strong influence on
the results. The combination of out-of-domain and
out-of-task data in ID+TL keeps losses uniformly
low. 2-SDA features lower almost all losses fur-
ther. On average, 2-SDA ID+TL reduces trans-
fer loss by 12.7 points compared to the baseline
(Table 3). As expected, pairings of thematically
strongly related domains (e.g., BOOKS and DVDS)
have lower losses in all setups.
The biggest challenge is the strong domain shift
between the CINEMA and PRODUCTS domains
(concerning mainly the retail aspects). With BoW
ID, losses on CINEMA reach up to 25 points, and
using CINEMA for training causes high losses for
PRODUCTS in most cases. Our key result is that
the ID+TL 2-SDA setup successfully compensates
for these problems, reducing the losses below 0.
Losses across the PRODUCTS domains are less
pronounced. The DVDS baseline classifier has the
lowest F1 (cf. Table 2) and shows the highest im-
provements in domain adaptation: BoW models of
other domains perform better than the in-domain
classifier. Analyzing the DVDS model shows over-
fitting to specific movie terms which occur fre-
quently across each review in the training data.
SNR content in movies is mostly concerned with
named entity types which cannot easily be learned
from BoW representations. Out-of-domain mod-
els are less specialized and perform better than in-
</bodyText>
<table confidence="0.86572">
BoW 2-SDA
ID 6.7 8.9
ID+TL -1.8 -6.0
</table>
<tableCaption confidence="0.915096">
Table 3: Mean transfer losses (%) for the different
training data and feature representation setups. In-
domain results not included.
</tableCaption>
<bodyText confidence="0.999654666666667">
domain models. TL and SDA increase the cover-
age of movie terms and provide better generaliza-
tion, which improves performance further.
BOOKS is the most challenging domain in all se-
tups. It is particularly heterogeneous, containing
both fiction and non-fiction reviews which feature
different SNR aspects. Both results illustrate that
domain effects depend on how diverse SNR con-
tent is within the domain.
Overall, the results show that ID+TL leads to a
successful compensation of cross-domain effects.
SDA features improve the results significantly† for
ID+TL. In particular, we find that the SDA suc-
cessfully compensates for the strong domain shift
between CINEMA and PRODUCTS.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999742111111111">
We presented experiments on multi- and cross-
domain sentiment relevance classification. We
showed that transfer learning (TL) using stacked
denoising autoencoder (SDA) representations sig-
nificantly increases performance by 6.8% F1 for
in-domain classification. Moreover, the average
transfer loss in domain adaptation is reduced by
12.7 percentage points where the SDA features
compensate for strong domain shifts.
</bodyText>
<page confidence="0.998414">
203
</page>
<sectionHeader confidence="0.995849" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999840886792453">
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(ACM TIST), 2(3):1–27.
Minmin Chen, Zhixiang Xu, Kilian Weinberger, and
Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In Proceedings of the
29th International Conference on Machine Learning
(ICML).
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML), pages 513–520.
Nitin Jindal and Bing Liu. 2008. Opinion spam
and analysis. In Proceedings of the International
Conference on Web Search and Web Data Mining
(WSDM), pages 219–230.
Eric W. Noreen. 1989. Computer Intensive Methods
for Hypothesis Testing: An Introduction. Wiley.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL), pages 271–278.
Christian Scheible and Hinrich Sch¨utze. 2013. Senti-
ment relevance. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 954–963.
Maite Taboada, Julian Brooke, and Manfred Stede.
2009. Genre-based paragraph classification for sen-
timent analysis. In Proceedings of the SIGDIAL
2009 Conference, pages 62–70.
Oscar T¨ackstr¨om and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 569–574.
Sebastian Thrun. 1995. Is learning the n-th thing any
easier than learning the first? In Proceedings of Ad-
vances in Neural Information Processing Systems 8
(NIPS), pages 640–646.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. The Journal of Machine Learn-
ing Research (JMLR), 11:3371–3408.
Sida Wang and Christopher Manning. 2012. Baselines
and bigrams: Simple, good sentiment and topic clas-
sification. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 90–94.
</reference>
<page confidence="0.998892">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.836847">
<title confidence="0.999915">Multi-Domain Sentiment Relevance Classification with Representation Learning</title>
<author confidence="0.99886">Christian Scheible Hinrich Sch¨utze</author>
<affiliation confidence="0.957924666666667">Institut f¨ur Maschinelle Sprachverarbeitung Center for Information University of Stuttgart and Language Processing of Munich</affiliation>
<abstract confidence="0.997013823529412">Sentiment relevance (SR) aims at identifying content that does not contribute to sentiment analysis. Previously, automatic SR classification has been studied in a limited scope, using a single domain and feature augmentation techniques that require large hand-crafted databases. In this paper, we present experiments on SR classification with automatically learned feature representations on multiple domains. We show that a combination of transfer learning and in-task supervision using features learned unsupervisedly by the stacked denoising autoencoder significantly outperforms a bag-of-words baseline for in-domain and cross-domain classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (ACM TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="7883" citStr="Chang and Lin, 2011" startWordPosition="1227" endWordPosition="1230">xamples. UNLAB: To improve generalization on PRODUCTS, we use additional unlabeled sentences (UNLAB) for SDA training. We extract the sentences of 1,500 randomly selected documents for each of the five domains from the Amazon.com review data by Jindal and Liu (2008). SDA setup We train the SDA with 10,000 hidden units and tanh nonlinearity on the BoW features of all available data as the input. We optimize the noise level p with 2-fold cross-validation on the in-domain training folds. Classification setup We perform SR classification with a linear support vector machine (SVM) using LIBLINEAR (Chang and Lin, 2011). We perform 2-fold cross-validation for all training data but P&amp;L. We report overall macro-averaged F1 over both folds. The feature representation for the SVM is either bag of words (BoW) or the kSDA output. Unlike Chen et al. (2012), we do not use concatenations of BoW and SDA vectors as we found them to perform worse. Evaluation As class distributions are heavily skewed, we use macro-averaged F1(s, t) (training on s and evaluating on t) as the basic evaluation measure. We evaluate DA with transfer loss, the difference in F1 of a classifier CL with 201 Features Setup CINEMA BOOKS DVDS EL MUS</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology (ACM TIST), 2(3):1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minmin Chen</author>
<author>Zhixiang Xu</author>
<author>Kilian Weinberger</author>
<author>Fei Sha</author>
</authors>
<title>Marginalized denoising autoencoders for domain adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="4865" citStr="Chen et al. (2012)" startWordPosition="732" endWordPosition="735">t, minimizing the error. Denoising autoencoders reconstruct x from a corrupted version of the input, ˜x. As the model learns to be robust to noise, the representations are expected to generalize better. For discrete data, masking noise is a natural choice, where each input unit is randomly set to 0 with probability p. Autoencoders can be stacked by using the hi produced by the ith autoencoder as the input to the (i+1)th one, yielding the representation hi+1. The h of the topmost autoencoder is the final representation output by the SDA. We let k-SDA denote a stack of k denoising autoencoders. Chen et al. (2012) introduced a marginalized closed-form version, the mSDA. We opt for this version as it is faster to train and allows us to use the full feature space which would be inefficient with iterative backpropagation training. 3 Task and Experimental Setup The task in this paper is multi- and cross-domain SR classification. Two aspects motivate our work: First, we need to address the sparse data situation. Second, we are interested in how crossdomain effects influence SR classification. We classify SR in three different setups: in-domain (ID), in which we take the training and test data from the same </context>
<context position="8117" citStr="Chen et al. (2012)" startWordPosition="1267" endWordPosition="1270"> data by Jindal and Liu (2008). SDA setup We train the SDA with 10,000 hidden units and tanh nonlinearity on the BoW features of all available data as the input. We optimize the noise level p with 2-fold cross-validation on the in-domain training folds. Classification setup We perform SR classification with a linear support vector machine (SVM) using LIBLINEAR (Chang and Lin, 2011). We perform 2-fold cross-validation for all training data but P&amp;L. We report overall macro-averaged F1 over both folds. The feature representation for the SVM is either bag of words (BoW) or the kSDA output. Unlike Chen et al. (2012), we do not use concatenations of BoW and SDA vectors as we found them to perform worse. Evaluation As class distributions are heavily skewed, we use macro-averaged F1(s, t) (training on s and evaluating on t) as the basic evaluation measure. We evaluate DA with transfer loss, the difference in F1 of a classifier CL with 201 Features Setup CINEMA BOOKS DVDS EL MUSIC VG 0 1 Majority BL – 39.6 28.9 32.6 39.2 35.1 39.0 35.7 2 BoW ID 74.0 57.5 49.8 55.1 55.5 55.0 58.4 3 1-SDA ID 73.6 55.3 48.4 43.8 41.8 44.1 52.6 4 2-SDA ID 76.0 54.5 52.5 43.9 41.2 46.7 53.6 5 BoW TL 71.5 60.7 60.2 50.3 55.1 53.2 </context>
</contexts>
<marker>Chen, Xu, Weinberger, Sha, 2012</marker>
<rawString>Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei Sha. 2012. Marginalized denoising autoencoders for domain adaptation. In Proceedings of the 29th International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML),</booktitle>
<pages>513--520</pages>
<contexts>
<context position="3306" citStr="Glorot et al., 2011" startWordPosition="483" endWordPosition="486">(TL, (Thrun, 1995)), which has been used before for SR classification. In this setup, we train a classifier on a different task, using subjectivity-labeled data – for which a large number of annotated examples is available – and apply it for SR classification. To enable knowledge transfer between the tasks, feature space augmentation has been proposed. For this purpose, we employ automatic representation learning, using the stacked denoising autoencoder (SDA, (Vincent et al., 2010)) which has been applied successfully to other domain adaptation problems such as crossdomain sentiment analysis (Glorot et al., 2011). In this paper, we present experiments on both multi-domain and cross-domain SR classification. We show that compared to the in-domain baseline, TL with SDA features increases F1 by 6.8% on average. We find that domain adaptation using TL with the SDA compensates for strong domain shifts, reducing the average classification transfer loss by 12.7%. 2 Stacked Denoising Autoencoders The stacked denoising autoencoder (SDA, (Vincent et al., 2010)) is a neural network (NN) model for unsupervised feature representation learning. 200 Proceedings of the 14th Conference of the European Chapter of the A</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th International Conference on Machine Learning (ICML), pages 513–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Web Search and Web Data Mining (WSDM),</booktitle>
<pages>219--230</pages>
<contexts>
<context position="7529" citStr="Jindal and Liu (2008)" startWordPosition="1166" endWordPosition="1169">gory: ambiguous examples are marked as SR. P&amp;L: The subjectivity data (P&amp;L) by Pang and Lee (2004) serves as our cross-task training data for transfer learning. The dataset was heuristically created for subjectivity detection on the movie domain by sampling snippets from Rotten Tomatoes as subjective and sentences from IMDb plot summaries as objective examples. UNLAB: To improve generalization on PRODUCTS, we use additional unlabeled sentences (UNLAB) for SDA training. We extract the sentences of 1,500 randomly selected documents for each of the five domains from the Amazon.com review data by Jindal and Liu (2008). SDA setup We train the SDA with 10,000 hidden units and tanh nonlinearity on the BoW features of all available data as the input. We optimize the noise level p with 2-fold cross-validation on the in-domain training folds. Classification setup We perform SR classification with a linear support vector machine (SVM) using LIBLINEAR (Chang and Lin, 2011). We perform 2-fold cross-validation for all training data but P&amp;L. We report overall macro-averaged F1 over both folds. The feature representation for the SVM is either bag of words (BoW) or the kSDA output. Unlike Chen et al. (2012), we do not </context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>Nitin Jindal and Bing Liu. 2008. Opinion spam and analysis. In Proceedings of the International Conference on Web Search and Web Data Mining (WSDM), pages 219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Hypothesis Testing: An Introduction.</title>
<date>1989</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="9395" citStr="Noreen, 1989" startWordPosition="1516" endWordPosition="1517">2 62.9 65.8 59.7 59.9 60.5 64.9 8 BoW ID+TL 76.6 63.5 61.7 52.4 56.7 57.0 62.3 9 1-SDA ID+TL 79.0 62.7 62.1 57.7 57.8 57.4 63.9 10 2-SDA ID+TL 80.4 62.7 65.2 59.0 58.7 58.9 65.2 Table 2: Macro-averaged F1 (%) evaluating on each test domain on both folds. 0 = row mean. Bold: best result in each column and results in that column not significantly different from it. respect to the in-domain baseline BL: L(s, t) = F (BL) 1 (t, t)−Fl CL) (s, t). L is negative if the classifier surpasses the baseline. As a statistical significance test (indicated by † in the text), we use approximate randomization (Noreen, 1989) with 10,000 iterations at p &lt; 0.05. 4 Experiments In-Domain Classification (ID) Table 2 shows macro-averaged F1 for different SR models. We first turn to fully supervised SR classification with bag-of-words (BoW) features using ID training (line 2). While the results for CINEMA are high, on par with the reported results in related work, they are low for the PRODUCTS data. This is not surprising as the SVM is trained with fewer than 600 examples on each domain. Also, no unknown category exists in the latter dataset. While ambiguous examples on CINEMA are annotated as unknown, they receive an S</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Hypothesis Testing: An Introduction. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>271--278</pages>
<contexts>
<context position="1908" citStr="Pang and Lee, 2004" startWordPosition="262" endWordPosition="265">ntribute to the polarity of the entity but contains a term with a known lexical non-neutral polarity. For example, movie reviews often have plot summaries which contain subjective descriptions, e.g., “April loves her new home and friends.”, containing “loves”, commonly a subjective positive term. Other domains contain different types of nonrelevant content: Music reviews may contain track listings, product reviews on retail platforms contain complaints that do not concern the product, e.g., about shipping and handling. Filtering such nonrelevant content can help to improve sentiment analysis (Pang and Lee, 2004). Sentiment relevance (Scheible and Sch¨utze, 2013; Taboada et al., 2009; T¨ackstr¨om and McDonald, 2011) formalizes this distinction: Content that contributes to the overall sentiment of a document is said to be sentiment relevant (SR), other content is sentiment nonrelevant (SNR). The main bottleneck in automatic SR classification is the lack of annotated data. On the sentence level, it has been attempted for the movie review domain (Scheible and Sch¨utze, 2013) on a manually annotated dataset that covers around 3,500 sentences. The sentiment analysis data by T¨ackstr¨om and McDonald (2011) </context>
<context position="7006" citStr="Pang and Lee (2004)" startWordPosition="1084" endWordPosition="1087">RODUCTS 294 3,836 2,689 1,147 –BOOKS 59 739 424 315 –DVDS 59 799 524 275 –ELECTRONICS 57 628 491 137 –MUSIC 59 638 448 190 –VIDEOGAMES 60 1032 802 230 P&amp;L – 10,000 5,000 5,000 UNLAB 7,500 68,927 – – Table 1: Dataset statistics dataset differs from CINEMA firstly in the product domains (except obviously for DVDS which also covers movies). Secondly, the data was collected from a retail site, which introduces further facets of sentiment nonrelevance, as discussed above. Thirdly, the annotation style has no unknown category: ambiguous examples are marked as SR. P&amp;L: The subjectivity data (P&amp;L) by Pang and Lee (2004) serves as our cross-task training data for transfer learning. The dataset was heuristically created for subjectivity detection on the movie domain by sampling snippets from Rotten Tomatoes as subjective and sentences from IMDb plot summaries as objective examples. UNLAB: To improve generalization on PRODUCTS, we use additional unlabeled sentences (UNLAB) for SDA training. We extract the sentences of 1,500 randomly selected documents for each of the five domains from the Amazon.com review data by Jindal and Liu (2008). SDA setup We train the SDA with 10,000 hidden units and tanh nonlinearity o</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL), pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Scheible</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Sentiment relevance.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>954--963</pages>
<marker>Scheible, Sch¨utze, 2013</marker>
<rawString>Christian Scheible and Hinrich Sch¨utze. 2013. Sentiment relevance. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 954–963.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Manfred Stede</author>
</authors>
<title>Genre-based paragraph classification for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference,</booktitle>
<pages>62--70</pages>
<contexts>
<context position="1980" citStr="Taboada et al., 2009" startWordPosition="272" endWordPosition="275"> lexical non-neutral polarity. For example, movie reviews often have plot summaries which contain subjective descriptions, e.g., “April loves her new home and friends.”, containing “loves”, commonly a subjective positive term. Other domains contain different types of nonrelevant content: Music reviews may contain track listings, product reviews on retail platforms contain complaints that do not concern the product, e.g., about shipping and handling. Filtering such nonrelevant content can help to improve sentiment analysis (Pang and Lee, 2004). Sentiment relevance (Scheible and Sch¨utze, 2013; Taboada et al., 2009; T¨ackstr¨om and McDonald, 2011) formalizes this distinction: Content that contributes to the overall sentiment of a document is said to be sentiment relevant (SR), other content is sentiment nonrelevant (SNR). The main bottleneck in automatic SR classification is the lack of annotated data. On the sentence level, it has been attempted for the movie review domain (Scheible and Sch¨utze, 2013) on a manually annotated dataset that covers around 3,500 sentences. The sentiment analysis data by T¨ackstr¨om and McDonald (2011) contains SR annotations for five product review domains, four of which h</context>
</contexts>
<marker>Taboada, Brooke, Stede, 2009</marker>
<rawString>Maite Taboada, Julian Brooke, and Manfred Stede. 2009. Genre-based paragraph classification for sentiment analysis. In Proceedings of the SIGDIAL 2009 Conference, pages 62–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
</authors>
<title>Semisupervised latent variable models for sentence-level sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>569--574</pages>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>Oscar T¨ackstr¨om and Ryan McDonald. 2011. Semisupervised latent variable models for sentence-level sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 569–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Thrun</author>
</authors>
<title>Is learning the n-th thing any easier than learning the first?</title>
<date>1995</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems 8 (NIPS),</booktitle>
<pages>640--646</pages>
<contexts>
<context position="2704" citStr="Thrun, 1995" startWordPosition="390" endWordPosition="391"> of a document is said to be sentiment relevant (SR), other content is sentiment nonrelevant (SNR). The main bottleneck in automatic SR classification is the lack of annotated data. On the sentence level, it has been attempted for the movie review domain (Scheible and Sch¨utze, 2013) on a manually annotated dataset that covers around 3,500 sentences. The sentiment analysis data by T¨ackstr¨om and McDonald (2011) contains SR annotations for five product review domains, four of which have fewer than 1,000 annotated examples. As the amount of labeled data is low, we adopt transfer learning (TL, (Thrun, 1995)), which has been used before for SR classification. In this setup, we train a classifier on a different task, using subjectivity-labeled data – for which a large number of annotated examples is available – and apply it for SR classification. To enable knowledge transfer between the tasks, feature space augmentation has been proposed. For this purpose, we employ automatic representation learning, using the stacked denoising autoencoder (SDA, (Vincent et al., 2010)) which has been applied successfully to other domain adaptation problems such as crossdomain sentiment analysis (Glorot et al., 201</context>
</contexts>
<marker>Thrun, 1995</marker>
<rawString>Sebastian Thrun. 1995. Is learning the n-th thing any easier than learning the first? In Proceedings of Advances in Neural Information Processing Systems 8 (NIPS), pages 640–646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Vincent</author>
<author>Hugo Larochelle</author>
<author>Isabelle Lajoie</author>
<author>Yoshua Bengio</author>
<author>Pierre-Antoine Manzagol</author>
</authors>
<title>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research (JMLR),</journal>
<pages>11--3371</pages>
<contexts>
<context position="3172" citStr="Vincent et al., 2010" startWordPosition="463" endWordPosition="466">view domains, four of which have fewer than 1,000 annotated examples. As the amount of labeled data is low, we adopt transfer learning (TL, (Thrun, 1995)), which has been used before for SR classification. In this setup, we train a classifier on a different task, using subjectivity-labeled data – for which a large number of annotated examples is available – and apply it for SR classification. To enable knowledge transfer between the tasks, feature space augmentation has been proposed. For this purpose, we employ automatic representation learning, using the stacked denoising autoencoder (SDA, (Vincent et al., 2010)) which has been applied successfully to other domain adaptation problems such as crossdomain sentiment analysis (Glorot et al., 2011). In this paper, we present experiments on both multi-domain and cross-domain SR classification. We show that compared to the in-domain baseline, TL with SDA features increases F1 by 6.8% on average. We find that domain adaptation using TL with the SDA compensates for strong domain shifts, reducing the average classification transfer loss by 12.7%. 2 Stacked Denoising Autoencoders The stacked denoising autoencoder (SDA, (Vincent et al., 2010)) is a neural networ</context>
</contexts>
<marker>Vincent, Larochelle, Lajoie, Bengio, Manzagol, 2010</marker>
<rawString>Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. 2010. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research (JMLR), 11:3371–3408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>90--94</pages>
<contexts>
<context position="1140" citStr="Wang and Manning, 2012" startWordPosition="142" endWordPosition="145">tation techniques that require large hand-crafted databases. In this paper, we present experiments on SR classification with automatically learned feature representations on multiple domains. We show that a combination of transfer learning and in-task supervision using features learned unsupervisedly by the stacked denoising autoencoder significantly outperforms a bag-of-words baseline for in-domain and cross-domain classification. 1 Introduction Many approaches to sentiment analysis rely on term-based clues to detect the polarity of sentences or documents, using the bag-of-words (BoW) model (Wang and Manning, 2012). One drawback of this approach is that the polarity of a clue is often treated as fixed, which can be problematic when content is not intended to contribute to the polarity of the entity but contains a term with a known lexical non-neutral polarity. For example, movie reviews often have plot summaries which contain subjective descriptions, e.g., “April loves her new home and friends.”, containing “loves”, commonly a subjective positive term. Other domains contain different types of nonrelevant content: Music reviews may contain track listings, product reviews on retail platforms contain compl</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida Wang and Christopher Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 90–94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>