<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.649011">
Every sensible extended top-down tree transducer
is a multi bottom-up tree transducer
</title>
<author confidence="0.561133">
Andreas Maletti*
</author>
<affiliation confidence="0.374245">
Institute for Natural Language Processing, Universität Stuttgart
</affiliation>
<address confidence="0.354946">
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
</address>
<email confidence="0.785178">
andreas.maletti@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.973654" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998432454545454">
A tree transformation is sensible if the size of
each output tree is uniformly bounded by a
linear function in the size of the correspond-
ing input tree. Every sensible tree transfor-
mation computed by an arbitrary weighted ex-
tended top-down tree transducer can also be
computed by a weighted multi bottom-up tree
transducer. This further motivates weighted
multi bottom-up tree transducers as suitable
translation models for syntax-based machine
translation.
</bodyText>
<sectionHeader confidence="0.992457" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999780222222222">
Several different translation models are used in
syntax-based statistical machine translation. Koehn
(2010) presents an introduction to statistical ma-
chine translation, and Knight (2007) presents an
overview of syntax-based statistical machine trans-
lation. The oldest and best-studied tree transfor-
mation device is the top-down tree transducer of
Rounds (1970) and Thatcher (1970). Gécseg and
Steinby (1984) and Fülöp and Vogler (2009) present
the existing results on the unweighted and weighted
model, respectively. Knight (2007) promotes the
use of weighted extended top-down tree transduc-
ers (XTOP), which have also been implemented in
the toolkit TIBURON by May and Knight (2006)
[more detail is reported by May (2010)]. In the con-
text of bimorphisms, Arnold and Dauchet (1976) in-
vestigated XTOP, and Lilin (1978) and Arnold and
Dauchet (1982) investigated multi bottom-up tree
</bodyText>
<note confidence="0.726277">
*The author was supported by the German Research Foun-
dation (DFG) grant MA 4959/1-1.
</note>
<bodyText confidence="0.999122088235294">
transducers (MBOT) [as k-morphisms]. Recently,
weighted XTOP and MBOT, which are the cen-
tral devices in this contribution, were investigated
by Maletti (2011a) in the context of statistical ma-
chine translation.
Several tree transformation devices are used as
translation models in statistical machine translation.
Chiang (2007) uses synchronous context-free gram-
mars, which force translations to be very similar
as observed by Eisner (2003) and Shieber (2004).
This deficiency is overcome by synchronous tree
substitution grammars, which are state-less linear
and nondeleting XTOP. Recently, Maletti (2010b)
proposed MBOT, and Zhang et al. (2008b) and
Sun et al. (2009) proposed the even more powerful
synchronous tree-sequence substitution grammars.
Those two models allow certain translation discon-
tinuities, and the former device also offers computa-
tional benefits over linear and nondeleting XTOP as
argued by Maletti (2010b).
The simplicity of XTOP makes them very appeal-
ing as translation models. In 2010 the ATANLP par-
ticipants [workshop at ACL] identified ‘copying’ as
the most exciting and promising feature of XTOP,
but unrestricted copying can lead to an undesirable
explosion of the size of the translation. According
to Engelfriet and Maneth (2003) a tree transforma-
tion has linear size-increase if the size of each output
tree is linearly bounded by the size of its correspond-
ing input tree. The author believes that this is a very
sensible restriction that intuitively makes sense and
at the same time suitably limits the copying power
of XTOP.
We show that every sensible tree transformation
</bodyText>
<page confidence="0.485338">
263
</page>
<note confidence="0.9703175">
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 263–273,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9932191">
that can be computed by an XTOP can also be com-
puted by an MBOT. For example, linear XTOP (i.e.,
no copying) compute only sensible tree transforma-
tions, and Maletti (2008) shows that for each linear
XTOP there exists an equivalent MBOT. Here, we
do not make any restrictions on the XTOP besides
some sanity conditions (see Section 3). In particu-
lar, we consider copying XTOP. If we accept the re-
striction to linear size-increase tree transformation,
then our main result further motivates MBOT as a
suitable translation model for syntax-based machine
translation because MBOT can implement each rea-
sonable (even copying) XTOP. In addition, our re-
sult allows us to show that each reasonable XTOP
preserves regularity under backward application. As
demonstrated by May et al. (2010) backward appli-
cation is the standard application of XTOP in the
machine translation pipeline, and preservation of
regularity is the essential property for several of the
evaluation algorithms of May et al. (2010).
</bodyText>
<sectionHeader confidence="0.948524" genericHeader="introduction">
2 Notation
</sectionHeader>
<bodyText confidence="0.9999295">
We start by introducing our notation for trees, whose
nodes are labeled by elements of an alphabet E and
a set V . However, only leaves can be labeled by
elements of V . For every set T, we let
</bodyText>
<equation confidence="0.981234">
E(T) = {Q(t1, ... , tk)  |Q ∈ E, t1, ... , tk ∈ T} ,
</equation>
<bodyText confidence="0.97944705">
which contains all trees with a E-labeled root
and direct successors in T. The set TΣ(V ) of
E-trees with V-leaves is the smallest set T such that
V ∪ E(T) ⊆ T. We use X = {x1, x2,... } as a set
of formal variables.
Each node of the tree t ∈ TΣ(V ) is identified by
a position p ∈ N+, which is a sequence of posi-
tive integers. The root is at position E (the empty
string), and the position ip with i ∈ N+ and p ∈ N+*
is the position p in the i-th direct subtree. The
set pos(t) contains all positions of t, and the size
of t is |t |= |pos(t)|. For each p ∈ pos(t), the label
of t at p is t(p). Given a set L ⊆ E ∪ V of labels, we
let posL(t) = {p ∈ pos(t)  |t(p) ∈ L} be the posi-
tions with L-labels. We write posl(t) for pos{lt(t)
for each l ∈ L. Finally, we write t[u]g, for the tree
obtained from t by replacing the subtree at position p
by the tree u ∈ TΣ(V ).
The following notions refer to the variables X.
The tree t ∈ TΣ(V ) [potentially V ∩ X = ∅] is
</bodyText>
<note confidence="0.813965">
SE
VP2
VBD21
ran211 away221
</note>
<figureCaption confidence="0.834882666666667">
Figure 1: The tree t (with positions indicated as super-
scripts) is linear and var(t) = {x2}. The tree t[He]111 is
the same tree with x2 replaced by ‘He’.
</figureCaption>
<equation confidence="0.999723666666667">
linear if every x ∈ X occurs at most once in t (i.e.,
|posx(t) |≤ 1). Moreover,
var(t) = {x ∈ X  |posx(t) =6 ∅}
</equation>
<bodyText confidence="0.979937916666667">
contains the variables that occur in t. A substitu-
tion 0 is a mapping 0: X → TΣ(V ). When applied
to t, it returns the tree t0, which is obtained from t
by replacing all occurrences of x ∈ X in t by 0(x).
Our notions for trees are illustrated in Figure 1.
Finally, we present weighted tree grammars
(WTG) as defined by Fülöp and Vogler (2009), who
defined it for arbitrary semirings as weight struc-
tures. In contrast, our weights are always nonneg-
ative reals, which form the semiring (R+, +, ·, 0,1)
and are used in probabilistic grammars. For each
weight assignment f : T → R+, we let
</bodyText>
<equation confidence="0.994333">
supp(f) = {t ∈ T  |f(t) =6 0} .
</equation>
<bodyText confidence="0.998664">
WTG offer an efficient representation of weighted
forests (i.e., set of weighted trees), which is even
more efficient than the packed forests of Mi et al.
(2008) because they can be minimized efficiently us-
ing an algorithm of Maletti and Quernheim (2011).
In particular, WTG can share more than equivalent
subtrees and can even represent infinite sets of trees.
A WTG is a system G = (Q, E, q0, P, wt) with
</bodyText>
<listItem confidence="0.975925571428571">
• a finite set Q of states (nonterminals),
• an alphabet E of symbols,
• a starting state q0 ∈ Q,
• a finite set P of productions q → r, where
q ∈ Q and r ∈ TΣ(Q) \ Q, and
• a mapping wt: P → R+ that assigns produc-
tion weights.
</listItem>
<bodyText confidence="0.639014666666667">
Without loss of generality, we assume that we can
distinguish states and symbols (i.e., Q ∩ E = ∅).
For all �, C ∈ TΣ(Q) and a production p = q → r,
</bodyText>
<equation confidence="0.6525412">
NP1
PP11
x111
2
RB22
</equation>
<page confidence="0.556893">
264
</page>
<figureCaption confidence="0.972145">
Figure 2: Example rotation. In principle, such rotations
are required in the translation from English to Arabic.
</figureCaption>
<bodyText confidence="0.96816">
we write ξ ==&gt;.G ζ if ξ = ξ[q]p and ζ = ξ[r]p, where
p is the lexicographically least element of posQ(ξ).
The WTG G generates the weighted tree lan-
guage LG: TE H R+ such that
</bodyText>
<equation confidence="0.9991715">
LG(t) = E wt(ρ1) · ... · wt(ρn)
nEN,p1,...,pnEP
P1 Pn
40�G ···�G t
</equation>
<bodyText confidence="0.9790945">
for every t E TE. Each such language is regular, and
Reg(E) contains all those languages over the alpha-
bet E. A thorough introduction to tree languages is
presented by Gécseg and Steinby (1984) and Géc-
seg and Steinby (1997) for the unweighted case and
by FUlöp and Vogler (2009) for the weighted case.
</bodyText>
<sectionHeader confidence="0.972302" genericHeader="method">
3 Extended top-down tree transducers
</sectionHeader>
<bodyText confidence="0.999558">
We start by introducing the main model of this
contribution. Extended top-down tree transducers
(XTOP) are a generalization of the top-down tree
transducers (TOP) of Rounds (1970) and Thatcher
(1970). XTOP allow rules with several (non-state
and non-variable) symbols in the left-hand side (as
in the rule of Figure 3), whereas a TOP rule contains
exactly one symbol in the left-hand side. Shieber
(2004) and Knight (2007) identified that this exten-
sion is essential for many NLP applications because
without it linear (i.e., non-copying) cannot compute
rotations (see Figure 2). In the form of bimorphisms
XTOP were investigated by Arnold and Dauchet
(1976) and Arnold and Dauchet (1982) in the 1970s,
and Knight (2007) invigorated research.
As demonstrated by Graehl et al. (2009) the
most general XTOP model includes copying, dele-
tion, and regular look-ahead in the spirit of En-
gelfriet (1977). More powerful models (such as
synchronous tree-sequence substitution grammars
and multi bottom-up tree transducers) can handle
translation discontinuities naturally as evidenced
by Zhang et al. (2008a) and Maletti (2011b), but
</bodyText>
<equation confidence="0.9768775">
q0
S
x1 VP
x2 x3
</equation>
<figureCaption confidence="0.991643">
Figure 3: Example XTOP rule by Graehl et al. (2008).
</figureCaption>
<bodyText confidence="0.999335444444444">
XTOP need copying and deletion to handle them.
Copying essentially allows an XTOP to translate
certain parts of the input several times and was iden-
tified by the ATANLP 2010 participants as one of the
most interesting and promising features of XTOP.
Currently, the look-ahead feature is not used in ma-
chine translation, but we need it later on in the theo-
retical development.
Given an alphabet Q and a set T, we let
</bodyText>
<equation confidence="0.723937">
Q[T]={q(t) I q E Q,t E TI,
</equation>
<bodyText confidence="0.999869714285714">
in which the root always has exactly one succes-
sor from T in contrast to Q(T). We treat elements
of Q[TE(V )] as special trees of TEuQ(V ). More-
over, we let —1E(t) = 1 for every t E TE. XTOP
with regular look-ahead (XTOPR) were also stud-
ied by Knight and Graehl (2005) and Graehl et al.
(2008). Formally, an XTOPR is a system
</bodyText>
<equation confidence="0.7817625">
M = (Q, E, A, q0, R, c, wt)
with
</equation>
<listItem confidence="0.9997018">
• a finite set Q of states,
• alphabets E and A of input and output symbols,
• a starting state q0 E Q,
• a finite set R of rules of the form ` H r with
linear ` E Q[TE(X)] and r E TA(Q[var(`)]),
• c: R x X H Reg(E) assigns a regular look-
ahead to each deleted variable of a rule [i.e.,
c(` H r, x) = —1E for all ` H r E R and
x E X \ (var(`) \ var(r))], and
• wt: R H R+ assigns rule weights.
</listItem>
<bodyText confidence="0.992249857142857">
The XTOPR M is linear [respectively, nondeleting]
if r is linear [respectively, var(`) = var(r)] for ev-
ery rule ` H—r E R. It has no look-ahead (XTOP)
if c(ρ, x) = 1E for all ρ E R and x E X. Figure 3
shows a rule of a linear and nondeleting XTOP.
The look-ahead can be used to restrict rule appli-
cations. It can inspect subtrees that are deleted by a
</bodyText>
<figure confidence="0.730197866666667">
S
t1 VP
t2 t3
S
t2 t1 t3
H
S
H
qNP
qNP
x3
qVB
x2
x1
265
</figure>
<figureCaption confidence="0.999792">
Figure 4: Rewrite step using rule ρ of Figure 3.
</figureCaption>
<bodyText confidence="0.998118652173913">
rule application, so for each rule ρ = ` → r, we let
del(ρ) = var(`) \ var(r) be the set of deleted vari-
ables in ρ. If we suppose that a variable x ∈ del(ρ)
matches to an input subtree t, then the weight of the
look-ahead c(ρ,x)(t), which we also write cρ,x(t),
is applied to the derivation. If it is 0, then this look-
ahead essentially prohibits the application of ρ. It is
important that the look-ahead is regular (i.e., there
exists a WTG accepting it). The toolkit TIBURON
by May and Knight (2006) implements XTOP to-
gether with a number of essential operations. Look-
ahead is not implemented in TIBURON, but it can
be simulated using a composition of two XTOP, in
which the first XTOP performs the look-ahead and
marks the results, so that the second XTOP can ac-
cess the look-ahead information.
As for WTG the semantics for the XTOPR
M = (Q, E, A, I, R, c, wt) is presented using
rewriting. Without loss of generality, we again sup-
pose that Q ∩ (E ∪ A) = ∅. Let ξ, ζ ∈ TA(Q[TE]),
w ∈ ][R+, and ρ = ` → r be a rule of R. We write
ξ ⇒ρ,wM ζ if there exists a substitution θ: X → TE
such that
</bodyText>
<listItem confidence="0.999662">
• ξ = ξ[`θ]p,
• ζ = ξ[rθ]p, and
• w = wt(ρ) · Hx∈del(ρ) cρ,x(xθ),
</listItem>
<bodyText confidence="0.998848769230769">
where p ∈ posQ(ξ) is the lexicographically least
Q-labeled position in ξ. Figure 4 illustrates a deriva-
tion step.
The XTOPR M computes a weighted tree trans-
formation by applying rewrite steps to the tree qo(t),
where t ∈ TE is the input tree, until an output
tree u ∈ TA has been produced. The weight of a
particular derivation is obtained by multiplying the
weights of the rewrite steps. The weight of the trans-
formation from t to u is obtained by summing all
weights of the derivations from qo(t) to u. For-
mally1, the weighted tree transformation computed
by M in state q ∈ Q is
</bodyText>
<equation confidence="0.9981708">
w1 · ... · wn (1)
n∈N` ,ρ1,...,ρn∈R
q(t)⇒ρ1,w1···⇒ρn,wn
M u
M
</equation>
<bodyText confidence="0.992711333333334">
for every t ∈ TE and u ∈ TA. The XTOPR M
computes the weighted tree transformation τq�M. Two
XTOPR M and N are equivalent, if τM = τN.
The sum (1) can be infinite, which we avoid by
simply requiring that all our XTOPR are produc-
ing, which means that r ∈/ Q[X] for every rule
` → r ∈ R.2 In a producing XTOPR each rule ap-
plication produces at least one output symbol, which
limits the number n of rule applications to the size of
the output tree u. A detailed exposition to XTOPR is
presented by Arnold and Dauchet (1982) and Graehl
et al. (2009) for the unweighted case and by Fülöp
and Vogler (2009) for the weighted case.
Example 1. Let Mex = (Q, E, E, q, R, c, wt) be the
nondeleting XTOP with
</bodyText>
<listItem confidence="0.999838">
• Q = {q},
• E = {σ, γ, α},
• the two rules
</listItem>
<equation confidence="0.9925815">
q(α) → α (ρ)
q(γ(x1)) → σ(q(x1),q(x1)) (ρ0)
</equation>
<listItem confidence="0.9982775">
• trivial look-ahead (i.e., c(ρ, x) = �1E), and
• wt(ρ) = 2 and wt(ρ0) = 1.
</listItem>
<bodyText confidence="0.999863222222222">
The XTOPR Mex computes the tree transformation
that turns the input tree γn(α) into the fully balanced
binary tree u of the same height with weight 2(2n).
An example derivation is presented in Figure 5.
Unrestricted copying (as in Example 1) yields
very undesirable phenomena and is most likely not
needed in the machine translation task. In fact, it
is almost universally agreed that a translation model
should be “linear-size increase”, which means that
</bodyText>
<footnote confidence="0.817676">
1There is an additional restriction that is discussed in the
next paragraph.
2This is a convenience requirement. We can use other con-
ditions on the XTOPR or the used weight structures to guarantee
</footnote>
<figure confidence="0.998668802816902">
a well-defined semantics.
qVB
qNP
qNP
u
S
t2
t1
t3
u
qo
S
ρ,.5
⇒M
VP
t1
t2 t3
τM(t, u) =
q
266
σ
σ
σ
q
⇒ρ0,1
Mex
⇒ρ0,1
Mex
⇒ρ0,1
Mex
q
q
γ
γ
q
γ
q
γ
q
γ
α
α
α
α
α
α
⇒ρ,2
Mex
σ σ
⇒ρ,2 ρ,2
Mex Mex
q
q
α
α
q
α
q
α
σ
q
α
σ
α q
α
q
α
σ σ
σ
σ σ
α α α α
</figure>
<figureCaption confidence="0.999929">
Figure 5: Example derivation using the XTOP Mex with weight 13 · 24 = 16.
</figureCaption>
<bodyText confidence="0.998508419354839">
the size of each output tree should be linearly
bounded in the size of the corresponding input tree
according to Aho and Ullman (1971) and Engelfriet
and Maneth (2003).
Definition 2. A mapping τ : TE × To → R+ is
linear-size increase if there exists an integer n ∈ N
such that |u |≤ n · |t |for all (t, u) ∈ supp(τ).
An XTOPR M is sensible if τM is linear-size in-
crease.
‘Sensible’ is not a syntactic property of an
XTOPR as it does not depend on the actual rules,
but only on its computed weighted tree transforma-
tion. The XTOP Mex of Example 1 is not sensible
because |u |= 2|t |− 1 for every (t, u) ∈ τMex. In-
tuitively, the number of times that Mex can use the
copying rule ρ0 is not uniformly bounded.
We need an auxiliary result in the main part.
Let τ : TE × To → R+ be a weighted tree
transformation. We need the weighted tree lan-
guage τ−1(u): TE → R+ of input trees weighted
by their translation weight to a given output
tree u ∈ To. Formally, (τ−1(u))(t) = τ(t, u) for
every t ∈ TE.
Theorem 3. For every producing XTOPR M and
output tree u0 ∈ To, the weighted tree lan-
guage τ−1M (u0) is regular.
Proof sketch. We use some properties that are only
defined in the next sections (for proof economy). It
is recommended to skip this proof on the first read-
ing and revisit it later. Maletti (2010a) shows that
we can construct an XTOPR M0 such that
</bodyText>
<equation confidence="0.976577">
�
τM(t, u) if u0 = u
τM0(t, u) =
0 otherwise
</equation>
<bodyText confidence="0.99984136">
for every t ∈ TE and u ∈ To. This operation is
called ‘output product’ by Maletti (2010a). The ob-
tained XTOPR M0 is also producing, so we know
that M0 can take at most |u0 |rewrite steps to de-
rive u0. Since M0 can only produce the output
tree u0, this also limits the total number of rule appli-
cations in any successful derivation. Consequently,
M0 can only apply a copying rule at most |u0 |times,
which shows that M0 is finitely copying (see Def-
inition 8). By Theorem 11 we can implement M0
by an equivalent MBOT M00 (i.e., τM00 = τM0;
see Section 5), for which we know by Theorem 14
of Maletti (2011a) that τ−1M00(u) = τ−1M0(u) is regu-
lar.
Finally, let us illustrate the overall structure of our
arguments to show that every sensible XTOPR can
be implemented by an equivalent MBOT. We first
normalize the given XTOPR such that the seman-
tic property ‘sensible’ yields a syntactic property
called ‘finitely copying’ (see Section 4). In a second
step, we show that each finitely copying XTOPR can
be implemented by an equivalent MBOT (see Sec-
tion 5). Figure 6 illustrates these steps towards our
main result. In the final section, we derive some con-
sequences from our main result (see Section 6).
</bodyText>
<sectionHeader confidence="0.631086" genericHeader="method">
4 From sensible to finite copying
</sectionHeader>
<bodyText confidence="0.999921733333333">
First, we adjust a normal form of Engelfriet and
Maneth (2003) to our needs. This section bor-
rows heavily from Aho and Ullman (1971) and En-
gelfriet and Maneth (2003), where “sensible”
(unweighted) deterministic macro tree transduc-
ers (MAC) [see Engelfriet and Vogler (1985)] are
considered. Our setting is simpler on the one hand
because XTOPR do not have context parameters
as MAC, but more difficult on the other hand be-
cause we consider nondeterministic and weighted
transducers.
Intuitively, a sensible XTOPR cannot copy a lot
since the size of each output tree is linearly bounded
in the size of the corresponding input tree. However,
the actual presentation of the XTOPR M might con-
</bodyText>
<figure confidence="0.9627115">
267
sensible XTOPR
sensible proper XTOPR
finitely copying XTOPR
</figure>
<figureCaption confidence="0.942237">
linear and nondeleting MBOT
Figure 6: Overview of the proof steps.
</figureCaption>
<bodyText confidence="0.99985185">
tain rules that allow unbounded copying. This un-
bounded copying might not manifest due to the look-
ahead restrictions or due to the fact that those rules
cannot be used in a successful derivation. The pur-
pose of the normal form is the elimination of those
artifacts. To this end, we eliminate all states (except
the initial state) that can only produce finitely many
outputs. Such a state can simply be replaced by one
of the output trees that it can produce and an ad-
ditional look-ahead that checks whether the current
input tree indeed allows that translation (and inserts
the correct translation weight).
Normalized XTOPR are called ‘proper’, and we
define this property next. For the rest of this section,
let M = (Q, E, A, q0, R, c, wt) be the considered
sensible XTOPR. Without loss of generality, we as-
sume that the state q0 does not occur in the right-
hand sides of rules. Moreover, we write ξ ==&gt;.∗M ζ if
there exist nonzero weights w1, ... ,wn E R+ \ {0}
and rules ρ1, ... , ρn E R with
</bodyText>
<equation confidence="0.986066333333333">
ξ ==&gt;.ρ�,w�
M · · · ==&gt;.ρ�,w�
M ζ .
</equation>
<bodyText confidence="0.9922702">
In essence,ξ ==&gt;.∗M ζ means that M can transform ξ
into ζ (in the unweighted setting).
Definition 4. A state q E Q is proper if there are in-
finitely many u0 E TA such that there exists a deriva-
tion
</bodyText>
<equation confidence="0.769054">
q0(t) ==&gt;.∗M ξ[q(s)]p ==&gt;.∗M u[u0]p
</equation>
<bodyText confidence="0.998994185185185">
where s, t E TE are input trees, ξ E TA(Q[TE]),
p E pos(ξ), and u E TA is an output tree.
The derivation in Definition 4 is illustrated in Fig-
ure 7. In other words, a proper state is reachable
from the initial state and can transform infinitely
many input trees into infinitely many output trees.
The latter is an immediate consequence of Defini-
tion 4 since each input tree can be transformed into
only finitely many output trees due to sensibility.
The restriction includes the look-ahead (because we
require that the rewrite step weights are nonzero),
which might further restrict the input trees.
Example 5. The state q of the XTOP Mex is proper
because we already demonstrated that it can trans-
form infinitely many input trees into infinitely many
output trees.
The XTOPR M is proper if all its states except
the initial state q0 are proper. Next, we show that
each XTOPR can be transformed into an equivalent
proper XTOPR using a simplified version of the con-
struction of Lemma 5.4 by Engelfriet and Maneth
(2003). Mind that we generally assume that all con-
sidered XTOPR are producing.
Theorem 6. For every XTOPR there exists an equiv-
alent proper XTOPR.
Proof sketch. The construction is iterative. Sup-
pose that M is not yet proper. Then there exists
a state q E Q, which can produce only finitely
many outputs U. It can be decided whether a state
is proper using Theorem 4.5 of Drewes and Engel-
friet (1998), and in case it is proper, the set U can
also be computed effectively. The cited theorem ap-
plies to unweighted XTOPR, but it can be applied
also in our setting because ==&gt;.∗M in Definition 4 dis-
regards weights. Now we consider each u E U in-
dividually. Clearly, (τqM)−1(u) is regular by The-
orem 3. For each u and each occurrence of q in
the right-hand side of a rule ρ E R of M, we cre-
ate a copy ρ0 of ρ, in which the selected occur-
rence of q(x) is replaced by u and the new look-
ahead is c(ρ0, x) = c(ρ, x) · (τqM)−1(u), which re-
stricts the input tree appropriately and includes the
adjustment of the weights. Since regular weighted
tree languages are closed under HADAMARD prod-
ucts [see Fülöp and Vogler (2009)], the look-ahead
c(ρ, x) · (τqM)−1(u) is again regular.
Essentially, we precompute the action of q as
much as possible, and immediately output one of
the finitely many output trees, check that the input
tree has the required shape using the look-ahead,
and charge the weight for the precomputed trans-
formation again using the look-ahead. This pro-
cess is done for each occurrence, so if a rule con-
tains two occurrences of q, then the process must be
</bodyText>
<page confidence="0.734845">
268
</page>
<figureCaption confidence="0.999721">
Figure 7: Illustration of the derivation in Definition 4.
</figureCaption>
<figure confidence="0.9985941">
q0
...
...
t ==&gt;.∗M
. ..
q
s
==&gt;.∗ M
u0
...
</figure>
<bodyText confidence="0.998476277777778">
done twice to this rule. In this way, we eventually
purge all occurrences of q from the right-hand sides
of rules of M without changing the computed trans-
formation. Since q =� q0 and q is now unreachable,
it is useless and can be deleted, which removes one
non-proper state. This process is repeated until all
states except the initial state q0 are proper.
Clearly, the construction of Theorem 6 applied
to a sensible XTOPR M yields a sensible proper
XTOPR M0 since the property ‘sensible’ refers to
the computed transformation and τM = τMS. Let us
illustrate the construction on a small example.
Example 7. Let ρ be the rule displayed in Figure 3,
and let us assume that the state qVB is not proper.
Moreover, suppose that qVB can yield the output
tree u and that we already computed the translation
options that yield u. Let t1, ... , tn E TE be those
translation options. Then we create the copy ρ0
</bodyText>
<equation confidence="0.91967">
q0(S(x1, VP(x2, x3))) —* S(u, qNP(x1), qNP(x3))
of the rule ρ with look-ahead c0(ρ0, x) such that
�
cρ,x(t) if x =� x2
c0 ρ�,x(t) = τqVB
M (t, u) if x = x2
</equation>
<bodyText confidence="0.984216254237289">
In general, there can be infinitely many input
trees ti that translate to a selected output tree u, so
we cannot simply replace the variable in the left-
hand side by all the options for the input tree. This
is the reason why we use the look-ahead because the
set τ−1M (u) is a regular weighted tree language.
From now on, we assume that the XTOPR M is
proper. Next, we want to invoke Theorem 7.1 of En-
gelfriet and Maneth (2003) to show that a proper
sensible XTOPR is finitely copying. Engelfriet and
Maneth (2003) present a formal definition of finite
copying, but we only present a high-level descrip-
tion of it.
Definition 8. The XTOPR M is finitely copying if
there is a copying bound n E N such that no input
subtree is copied more than n times in any derivation
q(t) ==&gt;.∗M u with q E Q, t E TE, and u E TA.
Example 9. The XTOP of Example 1 is not finitely
copying as the input subtree α is copied 2n times if
the input tree is γn(α). Clearly, this shows that there
is no uniform bound on the number of copies.
It is worth noting that the properties ‘sensible’ and
‘finitely copying’ are essentially unweighted prop-
erties. They largely disregard the weights and a
weighted XTOPR does have one of those properties
if and only if its associated unweighted XTOPR has
it. We now use this tight connection to lift Theo-
rem 7.1 of Engelfriet and Maneth (2003) from the
unweighted (and deterministic) case to the weighted
(and nondeterministic) case.
Theorem 10. If a proper XTOPR is sensible, then it
is finitely copying.
Proof. Let M be the input XTOPR. Since M is sen-
sible, its associated unweighted XTOPR N, which
is obtained by setting all weights to 1 and comput-
ing in the BOOLEAN semiring, is sensible. Conse-
quently, N is finitely copying by Theorem 7.1 of En-
gelfriet and Maneth (2003). Thus, also M is finitely
copying, which concludes the proof. We remark
that Theorem 7.1 of Engelfriet and Maneth (2003)
only applies to deterministic XTOPR, but the essen-
tial pumping argument, which is Lemma 6.2 of En-
gelfriet and Maneth (2003) also works for nonde-
terministic XTOPR. Essentially, the pumping argu-
ment shows the contraposition. If M is not finitely
copying, then M can copy a certain subtree an arbi-
trarily often. Due to the properness of M, all these
copies have an impact on the output tree, which
yields that its size grows beyond any uniform lin-
ear bound, which in turn demonstrates that M is not
sensible.
.
269
We showed that each sensible XTOPR can be im-
plemented by a finitely copying XTOPR via the con-
struction of the proper normal form. This approach
actually yields a characterization because finitely
copying XTOPR are trivially sensible by Theo-
rem 4.19 of Engelfriet and Maneth (2003).
</bodyText>
<sectionHeader confidence="0.503544" genericHeader="method">
5 From finite copying to an MBOT
</sectionHeader>
<bodyText confidence="0.997643">
We complete the argument by showing how to im-
plement a finitely copying XTOPR by a weighted
multi bottom-up tree transducer (MBOT). First, we
recall the MBOT, which was introduced by Arnold
and Dauchet (1982) and Lilin (1978) in the un-
weighted case. Engelfriet et al. (2009) give an En-
glish presentation. We present the linear and non-
deleting MBOT of Engelfriet et al. (2009).
A weighted multi bottom-up tree transducer is a
system M = (Q, E, A, F, R, wt) with
</bodyText>
<listItem confidence="0.999858">
• an alphabet Q of states,
• alphabets E and A of input and output symbols,
• a set F C_ Q of final states,
• a finite set R of rules of the form E —* r where
E E TΣ(Q(X)) and r E Q(TΔ(X)) are linear
and var(E) = var(r), and
• wt: R —* R+ assigning rule weights.
</listItem>
<bodyText confidence="0.999751555555556">
We now use TΣ(Q(X)) and Q(TΔ(X)) instead of
TΣ(Q[X]) and Q[TΔ(X)], which highlights the dif-
ference between XTOPR and MBOT. First, MBOT
are a bottom-up device, which yields that E and A
as well as E and r exchange their place. More impor-
tantly, MBOT can use states with more than 1 suc-
cessor (e.g, Q(X) instead of Q[X]). An example
rule is displayed in Figure 8.
Let M = (Q, E, A, F, R, wt) be an MBOT such
that Qn(EUA) = 0.3 We require that r E� Q(X) for
each rule E —* r E R to guarantee finite derivations
and thus a well-defined semantics.4 As before, we
present a rewrite semantics. Let �, C E TΣ(Q(TΔ)),
and let p = E —* r be a rule. We write � ==&gt;.ρM C
if there exists a substitution O: X —* TΔ such that
� = �[EO]p and C = �[rO]p, where p E pos(�) be is
the lexicographically least reducible position in �. A
rewrite step is illustrated in Figure 8.
</bodyText>
<footnote confidence="0.36249525">
3This restriction can always be achieved by renaming the
states.
4Again this could have been achieved with the help of other
conditions on the MBOT or the used weight structure.
</footnote>
<bodyText confidence="0.998797">
The weighted tree transformation computed by M
in state q E Q is
</bodyText>
<equation confidence="0.997939333333333">
q 1`TM(t, u1 ··· uk) wt(p1) · ... · wt(pn)
nEN,ρ1,...,ρnER
t�M ···�M q(u1,...,uk)
</equation>
<bodyText confidence="0.998581634146341">
for all t E TΣ and u1, ... , uk E TΔ. The semantics
of M is TM(t, u) = EqEF TqM(t, u) for all t E TΣ
and u E TΔ.
We move to the last step for our main result,
in which we show how to implement each finitely
copying XTOPR by an MBOT using a weighted ver-
sion of the construction in Lemma 15 of Maletti
(2008). The computational benefits (binarization,
composition, efficient parsing, etc.) of MBOT over
XTOPR are described by Maletti (2011a).
Theorem 11. Every finitely copying XTOPR can be
implemented by an MBOT.
Proof sketch. We plan to utilize Theorem 18 of En-
gelfriet et al. (2009), which proves the same state-
ment in the unweighted and deterministic case.
Again, the weights are not problematic, but we need
to remove the nondeterminism before we can apply
it. This is achieved by a decomposition into two
XTOPR. The first XTOPR annotates the input tree
with the rules that the second XTOPR is supposed to
use. Thus, the first XTOPR remains nondeterminis-
tic, but the second XTOPR, which simply executes
the annotated rules, is now deterministic. This stan-
dard approach due to Engelfriet (1975) is used in
many similar constructions.
Suppose that n is a copying bound for the input
XTOPR M, which means that no more than n rules
are applied to each input symbol. The first XTOPR
is actually a nondeterministic linear and nondeleting
XTOP that annotates each input tree symbol with ex-
actly n rules of M that are consistent with the state
behavior of M. Moreover, the annotation also pre-
scribes with which of n rules the processing should
continue at each subtree. Since we know all the rules
that will potentially be applied for a certain symbol,
we can make the assignment such that no annotated
rule is used twice in the same derivation. The de-
tails for this construction can be found in Lemma 15
of Maletti (2008).
In this way, we obtain a weighted linear and non-
deleting XTOP M1, which includes the look-ahead,
</bodyText>
<figure confidence="0.996593233333333">
270
qNP
VP
qVB
x1
qNP
x3 x4
x2
�
x2 x1 x3
S
qS
x4
qNP
u1
qVB
S
u2
t
VP
u3 u4
qNP
P
==&gt;.M
u2 u1 u3
S
qS
t
u4
S
</figure>
<figureCaption confidence="0.999922">
Figure 8: Example MBOT rule ρ [left] and its use in a rewrite step [right].
</figureCaption>
<bodyText confidence="0.999977541666667">
and an unweighted deterministic XTOP M2. Only
the weight and look-ahead of rules that are actu-
ally executed are applied (e.g., although we anno-
tate n rules at the root symbol, we only execute the
first rule and thus only apply its weight and look-
ahead). The look-ahead of different rules is either
resolved (i.e., pushed to the next rules) or multi-
plied using the HADAMARD product [see Fülöp and
Vogler (2009)], which preserves regularity. This
process is also used by Seemann et al. (2012). Now
we can use Theorem 4 of Maletti (2011a) to obtain
an MBOT N1 that is equivalent to M1. Similarly,
we can use Theorem 18 of Engelfriet et al. (2009)
to obtain an MBOT N2 that is equivalent to M2.
Since MBOT are closed under composition by The-
orem 23 of Engelfriet et al. (2009), we can compose
N1 and N2 to obtain a single MBOT N that is equiv-
alent to M.
Corollary 12. For every sensible producing XTOPR
there exists an equivalent MBOT.
Proof. Theorem 6 shows that there exists an equiva-
lent proper XTOPR, which must be finitely copying
by Theorem 10. This last fact allows us to construct
an equivalent MBOT by Theorem 11.
</bodyText>
<sectionHeader confidence="0.775966" genericHeader="method">
6 Preservation of regularity
</sectionHeader>
<bodyText confidence="0.99962136">
Finally, we present an application of Corollary 12 to
solve an open problem. The translation model is of-
ten used in a backwards manner in a machine trans-
lation system as demonstrated, for example, by May
et al. (2010), which means that an output tree is sup-
plied and the corresponding input trees are sought.
This starting output tree is typically the best parse
of the string that we want to translate. However, in-
stead of a single tree, we want to use all parses of
this sentence together with their parse scores. Those
parses form a regular weighted tree language, and
applying them backwards to the translation model
yields another weighted tree language L of corre-
sponding input trees. For an efficient representation
and efficient modification algorithms (such a k-best
extraction) we would like L to be regular. However,
Fülöp et al. (2011) demonstrate that the backward
application of a regular weighted tree language to
an XTOPR is not necessarily regular. The counterex-
ample uses a variant of the XTOP of Example 1 and
is thus not sensible. Theorem 14 of Maletti (2011a)
shows that MBOT preserve regularity under back-
ward application.
Corollary 13. Sensible XTOPR preserve regularity
under backward application.
</bodyText>
<sectionHeader confidence="0.85616" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999822333333333">
We demonstrated that each sensible XTOPR can be
implemented by an MBOT. The latter formalism of-
fers many computational advantages, so that the au-
thor believes that MBOT should be used instead of
XTOP. We used real number weights, but the author
believes that our results carry over to at least all zero-
sum and zero-divisor free semirings [see Hebisch
and Weinert (1998) and Golan (1999)], which are
semirings such that (i) a + b = 0 implies a = 0 and
(ii) a · b = 0 implies 0 E {a, b}. Whether our results
hold in other semirings (such as the semiring of all
reals where −1 + 1 = 0) remains an open question.
</bodyText>
<page confidence="0.77096">
271
</page>
<sectionHeader confidence="0.974422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996933068965518">
Alfred V. Aho and Jeffrey D. Ullman. 1971. Transla-
tions on a context-free grammar. Inform. and Control,
19(5):439–475.
André Arnold and Max Dauchet. 1976. Bi-transductions
de forêts. In Proc. 3th Int. Coll. Automata, Languages
and Programming, pages 74–86. University of Edin-
burgh.
André Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d’arbres. Theoret. Comput. Sci.,
20(1):33–93.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201–228.
Frank Drewes and Joost Engelfriet. 1998. Decidability
of the finiteness of ranges of tree transductions. In-
form. and Comput., 145(1):1–50.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. 41st Ann.
Meeting Association for Computational Linguistics,
pages 205–208. Association for Computational Lin-
guistics.
Joost Engelfriet and Sebastian Maneth. 2003. Macro tree
translations of linear size increase are MSO definable.
SIAM J. Comput., 32(4):950–1006.
Joost Engelfriet and Heiko Vogler. 1985. Macro tree
transducers. J. Comput. System Sci., 31(1):71–146.
Joost Engelfriet, Eric Lilin, and Andreas Maletti. 2009.
Extended multi bottom-up tree transducers — compo-
sition and decomposition. Acta Inform., 46(8):561–
590.
Joost Engelfriet. 1975. Bottom-up and top-down tree
transformations — a comparison. Math. Systems The-
ory, 9(3):198–231.
Joost Engelfriet. 1977. Top-down tree transducers
with regular look-ahead. Math. Systems Theory,
10(1):289–303.
Zoltán Fülöp and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, EATCS Monographs on Theo-
ret. Comput. Sci., chapter 9, pages 313–403. Springer.
Zoltán Fülöp, Andreas Maletti, and Heiko Vogler. 2011.
Weighted extended tree transducers. Fundam. Inform.,
111(2):163–202.
Ferenc Gécseg and Magnus Steinby. 1984. Tree Au-
tomata. Akadémiai Kiadó, Budapest.
Ferenc Gécseg and Magnus Steinby. 1997. Tree lan-
guages. In Grzegorz Rozenberg and Arto Salomaa,
editors, Handbook of Formal Languages, volume 3,
chapter 1, pages 1–68. Springer.
Jonathan S. Golan. 1999. Semirings and their Applica-
tions. Kluwer Academic, Dordrecht.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Comput. Linguist.,
34(3):391–427.
Jonathan Graehl, Mark Hopkins, Kevin Knight, and An-
dreas Maletti. 2009. The power of extended top-down
tree transducers. SIAM J. Comput., 39(2):410–430.
Udo Hebisch and Hanns J. Weinert. 1998. Semirings—
Algebraic Theory and Applications in Computer Sci-
ence. World Scientific.
Kevin Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Proc. 6th Int. Conf. Computational Lin-
guistics and Intelligent Text Processing, volume 3406
of LNCS, pages 1–24. Springer.
Kevin Knight. 2007. Capturing practical natu-
ral language transformations. Machine Translation,
21(2):121–133.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Eric Lilin. 1978. Une généralisation des transduc-
teurs d’états finis d’arbres: les S-transducteurs. Thèse
3ème cycle, Université de Lille.
Andreas Maletti and Daniel Quernheim. 2011. Pushing
for weighted tree automata. In Proc. 36th Int. Symp.
Mathematical Foundations of Computer Science, vol-
ume 6907 of LNCS, pages 460–471. Springer.
Andreas Maletti. 2008. Compositions of extended top-
down tree transducers. Inform. and Comput., 206(9–
10):1187–1196.
Andreas Maletti. 2010a. Input and output products
for weighted extended top-down tree transducers. In
Proc. 14th Int. Conf. Developments in Language The-
ory, volume 6224 of LNCS, pages 316–327. Springer.
Andreas Maletti. 2010b. Why synchronous tree substitu-
tion grammars? In Proc. Human Language Technolo-
gies: Conf. North American Chapter of the ACL, pages
876–884. Association for Computational Linguistics.
Andreas Maletti. 2011a. An alternative to synchronous
tree substitution grammars. J. Natur. Lang. Engrg.,
17(2):221–242.
Andreas Maletti. 2011b. How to train your multi bottom-
up tree transducer. In Proc. 49th Ann. Meeting Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 825–834. Association for
Computational Linguistics.
Jonathan May and Kevin Knight. 2006. Tiburon: A
weighted tree automata toolkit. In Proc. 11th Int.
Conf. Implementation and Application of Automata,
volume 4094 of LNCS, pages 102–113. Springer.
Jonathan May, Kevin Knight, and Heiko Vogler. 2010.
Efficient inference through cascades of weighted tree
transducers. In Proc. 48th Ann. Meeting Association
for Computational Linguistics, pages 1058–1066. As-
sociation for Computational Linguistics.
272
Jonathan May. 2010. Weighted Tree Automata and
Transducers for Syntactic Natural Language Process-
ing. Ph.D. thesis, University of Southern California,
Los Angeles.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. 46th Ann. Meeting Associ-
ation for Computational Linguistics, pages 192–199.
Association for Computational Linguistics.
William C. Rounds. 1970. Mappings and grammars on
trees. Math. Systems Theory, 4(3):257–287.
Nina Seemann, Daniel Quernheim, Fabienne Braune, and
Andreas Maletti. 2012. Preservation of recognizabil-
ity for weighted linear extended top-down tree trans-
ducers. In Proc. 2nd Workshop Applications of Tree
Automata in Natural Language Processing, pages 1–
10. Association for Computational Linguistics.
Stuart M. Shieber. 2004. Synchronous grammars as tree
transducers. In Proc. 7th Int. Workshop Tree Adjoining
Grammars and Related Formalisms, pages 88–95.
Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A
non-contiguous tree sequence alignment-based model
for statistical machine translation. In Proc. 47th Ann.
Meeting Association for Computational Linguistics,
pages 914–922. Association for Computational Lin-
guistics.
James W. Thatcher. 1970. Generalized sequential ma-
chine maps. J. Comput. System Sci., 4(4):339–367.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008a. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. 46th Ann. Meeting Association for Compu-
tational Linguistics, pages 559–567. Association for
Computational Linguistics.
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and
Sheng Li. 2008b. Grammar comparison study for
translational equivalence modeling and statistical ma-
chine translation. In Proc. 22nd Int. Conf. Computa-
tional Linguistics, pages 1097–1104. Association for
Computational Linguistics.
</reference>
<page confidence="0.954637">
273
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.585868">
<note confidence="0.7249165">Every sensible extended top-down tree transducer is a multi bottom-up tree transducer Institute for Natural Language Processing, Universität Pfaffenwaldring 5b, 70569 Stuttgart,</note>
<email confidence="0.97912">andreas.maletti@ims.uni-stuttgart.de</email>
<abstract confidence="0.995148916666667">A tree transformation is sensible if the size of each output tree is uniformly bounded by a linear function in the size of the corresponding input tree. Every sensible tree transformation computed by an arbitrary weighted extended top-down tree transducer can also be computed by a weighted multi bottom-up tree transducer. This further motivates weighted multi bottom-up tree transducers as suitable translation models for syntax-based machine translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Translations on a context-free grammar.</title>
<date>1971</date>
<journal>Inform. and Control,</journal>
<volume>19</volume>
<issue>5</issue>
<contexts>
<context position="14775" citStr="Aho and Ullman (1971)" startWordPosition="2724" endWordPosition="2727">tion that is discussed in the next paragraph. 2This is a convenience requirement. We can use other conditions on the XTOPR or the used weight structures to guarantee a well-defined semantics. qVB qNP qNP u S t2 t1 t3 u qo S ρ,.5 ⇒M VP t1 t2 t3 τM(t, u) = q 266 σ σ σ q ⇒ρ0,1 Mex ⇒ρ0,1 Mex ⇒ρ0,1 Mex q q γ γ q γ q γ q γ α α α α α α ⇒ρ,2 Mex σ σ ⇒ρ,2 ρ,2 Mex Mex q q α α q α q α σ q α σ α q α q α σ σ σ σ σ α α α α Figure 5: Example derivation using the XTOP Mex with weight 13 · 24 = 16. the size of each output tree should be linearly bounded in the size of the corresponding input tree according to Aho and Ullman (1971) and Engelfriet and Maneth (2003). Definition 2. A mapping τ : TE × To → R+ is linear-size increase if there exists an integer n ∈ N such that |u |≤ n · |t |for all (t, u) ∈ supp(τ). An XTOPR M is sensible if τM is linear-size increase. ‘Sensible’ is not a syntactic property of an XTOPR as it does not depend on the actual rules, but only on its computed weighted tree transformation. The XTOP Mex of Example 1 is not sensible because |u |= 2|t |− 1 for every (t, u) ∈ τMex. Intuitively, the number of times that Mex can use the copying rule ρ0 is not uniformly bounded. We need an auxiliary result </context>
<context position="17401" citStr="Aho and Ullman (1971)" startWordPosition="3226" endWordPosition="3229"> can be implemented by an equivalent MBOT. We first normalize the given XTOPR such that the semantic property ‘sensible’ yields a syntactic property called ‘finitely copying’ (see Section 4). In a second step, we show that each finitely copying XTOPR can be implemented by an equivalent MBOT (see Section 5). Figure 6 illustrates these steps towards our main result. In the final section, we derive some consequences from our main result (see Section 6). 4 From sensible to finite copying First, we adjust a normal form of Engelfriet and Maneth (2003) to our needs. This section borrows heavily from Aho and Ullman (1971) and Engelfriet and Maneth (2003), where “sensible” (unweighted) deterministic macro tree transducers (MAC) [see Engelfriet and Vogler (1985)] are considered. Our setting is simpler on the one hand because XTOPR do not have context parameters as MAC, but more difficult on the other hand because we consider nondeterministic and weighted transducers. Intuitively, a sensible XTOPR cannot copy a lot since the size of each output tree is linearly bounded in the size of the corresponding input tree. However, the actual presentation of the XTOPR M might con267 sensible XTOPR sensible proper XTOPR fin</context>
</contexts>
<marker>Aho, Ullman, 1971</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1971. Translations on a context-free grammar. Inform. and Control, 19(5):439–475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André Arnold</author>
<author>Max Dauchet</author>
</authors>
<title>Bi-transductions de forêts.</title>
<date>1976</date>
<booktitle>In Proc. 3th Int. Coll. Automata, Languages and Programming,</booktitle>
<pages>74--86</pages>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1513" citStr="Arnold and Dauchet (1976)" startWordPosition="212" endWordPosition="215">translation, and Knight (2007) presents an overview of syntax-based statistical machine translation. The oldest and best-studied tree transformation device is the top-down tree transducer of Rounds (1970) and Thatcher (1970). Gécseg and Steinby (1984) and Fülöp and Vogler (2009) present the existing results on the unweighted and weighted model, respectively. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) investigated XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as</context>
<context position="8816" citStr="Arnold and Dauchet (1976)" startWordPosition="1521" endWordPosition="1524">model of this contribution. Extended top-down tree transducers (XTOP) are a generalization of the top-down tree transducers (TOP) of Rounds (1970) and Thatcher (1970). XTOP allow rules with several (non-state and non-variable) symbols in the left-hand side (as in the rule of Figure 3), whereas a TOP rule contains exactly one symbol in the left-hand side. Shieber (2004) and Knight (2007) identified that this extension is essential for many NLP applications because without it linear (i.e., non-copying) cannot compute rotations (see Figure 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (2009) the most general XTOP model includes copying, deletion, and regular look-ahead in the spirit of Engelfriet (1977). More powerful models (such as synchronous tree-sequence substitution grammars and multi bottom-up tree transducers) can handle translation discontinuities naturally as evidenced by Zhang et al. (2008a) and Maletti (2011b), but q0 S x1 VP x2 x3 Figure 3: Example XTOP rule by Graehl et al. (2008). XTOP need copying and deletion to handle them. Copying essentia</context>
</contexts>
<marker>Arnold, Dauchet, 1976</marker>
<rawString>André Arnold and Max Dauchet. 1976. Bi-transductions de forêts. In Proc. 3th Int. Coll. Automata, Languages and Programming, pages 74–86. University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André Arnold</author>
<author>Max Dauchet</author>
</authors>
<title>Morphismes et bimorphismes d’arbres.</title>
<date>1982</date>
<journal>Theoret. Comput. Sci.,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="1579" citStr="Arnold and Dauchet (1982)" startWordPosition="223" endWordPosition="226">d statistical machine translation. The oldest and best-studied tree transformation device is the top-down tree transducer of Rounds (1970) and Thatcher (1970). Gécseg and Steinby (1984) and Fülöp and Vogler (2009) present the existing results on the unweighted and weighted model, respectively. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) investigated XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as observed by Eisner (2003) and Shieber (2004). This deficiency is </context>
<context position="8846" citStr="Arnold and Dauchet (1982)" startWordPosition="1526" endWordPosition="1529">tended top-down tree transducers (XTOP) are a generalization of the top-down tree transducers (TOP) of Rounds (1970) and Thatcher (1970). XTOP allow rules with several (non-state and non-variable) symbols in the left-hand side (as in the rule of Figure 3), whereas a TOP rule contains exactly one symbol in the left-hand side. Shieber (2004) and Knight (2007) identified that this extension is essential for many NLP applications because without it linear (i.e., non-copying) cannot compute rotations (see Figure 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (2009) the most general XTOP model includes copying, deletion, and regular look-ahead in the spirit of Engelfriet (1977). More powerful models (such as synchronous tree-sequence substitution grammars and multi bottom-up tree transducers) can handle translation discontinuities naturally as evidenced by Zhang et al. (2008a) and Maletti (2011b), but q0 S x1 VP x2 x3 Figure 3: Example XTOP rule by Graehl et al. (2008). XTOP need copying and deletion to handle them. Copying essentially allows an XTOP to translat</context>
<context position="13326" citStr="Arnold and Dauchet (1982)" startWordPosition="2418" endWordPosition="2421">sformation computed by M in state q ∈ Q is w1 · ... · wn (1) n∈N` ,ρ1,...,ρn∈R q(t)⇒ρ1,w1···⇒ρn,wn M u M for every t ∈ TE and u ∈ TA. The XTOPR M computes the weighted tree transformation τq�M. Two XTOPR M and N are equivalent, if τM = τN. The sum (1) can be infinite, which we avoid by simply requiring that all our XTOPR are producing, which means that r ∈/ Q[X] for every rule ` → r ∈ R.2 In a producing XTOPR each rule application produces at least one output symbol, which limits the number n of rule applications to the size of the output tree u. A detailed exposition to XTOPR is presented by Arnold and Dauchet (1982) and Graehl et al. (2009) for the unweighted case and by Fülöp and Vogler (2009) for the weighted case. Example 1. Let Mex = (Q, E, E, q, R, c, wt) be the nondeleting XTOP with • Q = {q}, • E = {σ, γ, α}, • the two rules q(α) → α (ρ) q(γ(x1)) → σ(q(x1),q(x1)) (ρ0) • trivial look-ahead (i.e., c(ρ, x) = �1E), and • wt(ρ) = 2 and wt(ρ0) = 1. The XTOPR Mex computes the tree transformation that turns the input tree γn(α) into the fully balanced binary tree u of the same height with weight 2(2n). An example derivation is presented in Figure 5. Unrestricted copying (as in Example 1) yields very undes</context>
<context position="26035" citStr="Arnold and Dauchet (1982)" startWordPosition="4795" endWordPosition="4798"> size grows beyond any uniform linear bound, which in turn demonstrates that M is not sensible. . 269 We showed that each sensible XTOPR can be implemented by a finitely copying XTOPR via the construction of the proper normal form. This approach actually yields a characterization because finitely copying XTOPR are trivially sensible by Theorem 4.19 of Engelfriet and Maneth (2003). 5 From finite copying to an MBOT We complete the argument by showing how to implement a finitely copying XTOPR by a weighted multi bottom-up tree transducer (MBOT). First, we recall the MBOT, which was introduced by Arnold and Dauchet (1982) and Lilin (1978) in the unweighted case. Engelfriet et al. (2009) give an English presentation. We present the linear and nondeleting MBOT of Engelfriet et al. (2009). A weighted multi bottom-up tree transducer is a system M = (Q, E, A, F, R, wt) with • an alphabet Q of states, • alphabets E and A of input and output symbols, • a set F C_ Q of final states, • a finite set R of rules of the form E —* r where E E TΣ(Q(X)) and r E Q(TΔ(X)) are linear and var(E) = var(r), and • wt: R —* R+ assigning rule weights. We now use TΣ(Q(X)) and Q(TΔ(X)) instead of TΣ(Q[X]) and Q[TΔ(X)], which highlights </context>
</contexts>
<marker>Arnold, Dauchet, 1982</marker>
<rawString>André Arnold and Max Dauchet. 1982. Morphismes et bimorphismes d’arbres. Theoret. Comput. Sci., 20(1):33–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2026" citStr="Chiang (2007)" startWordPosition="289" endWordPosition="290">6) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) investigated XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as observed by Eisner (2003) and Shieber (2004). This deficiency is overcome by synchronous tree substitution grammars, which are state-less linear and nondeleting XTOP. Recently, Maletti (2010b) proposed MBOT, and Zhang et al. (2008b) and Sun et al. (2009) proposed the even more powerful synchronous tree-sequence substitution grammars. Those two models allow certain translation discontinuities, and the former device also offers computational benefits over linear and nondeleting XTOP as argued by Maletti (2010</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Drewes</author>
<author>Joost Engelfriet</author>
</authors>
<title>Decidability of the finiteness of ranges of tree transductions.</title>
<date>1998</date>
<journal>Inform. and Comput.,</journal>
<volume>145</volume>
<issue>1</issue>
<contexts>
<context position="20765" citStr="Drewes and Engelfriet (1998)" startWordPosition="3827" endWordPosition="3831">per if all its states except the initial state q0 are proper. Next, we show that each XTOPR can be transformed into an equivalent proper XTOPR using a simplified version of the construction of Lemma 5.4 by Engelfriet and Maneth (2003). Mind that we generally assume that all considered XTOPR are producing. Theorem 6. For every XTOPR there exists an equivalent proper XTOPR. Proof sketch. The construction is iterative. Suppose that M is not yet proper. Then there exists a state q E Q, which can produce only finitely many outputs U. It can be decided whether a state is proper using Theorem 4.5 of Drewes and Engelfriet (1998), and in case it is proper, the set U can also be computed effectively. The cited theorem applies to unweighted XTOPR, but it can be applied also in our setting because ==&gt;.∗M in Definition 4 disregards weights. Now we consider each u E U individually. Clearly, (τqM)−1(u) is regular by Theorem 3. For each u and each occurrence of q in the right-hand side of a rule ρ E R of M, we create a copy ρ0 of ρ, in which the selected occurrence of q(x) is replaced by u and the new lookahead is c(ρ0, x) = c(ρ, x) · (τqM)−1(u), which restricts the input tree appropriately and includes the adjustment of the</context>
</contexts>
<marker>Drewes, Engelfriet, 1998</marker>
<rawString>Frank Drewes and Joost Engelfriet. 1998. Decidability of the finiteness of ranges of tree transductions. Inform. and Comput., 145(1):1–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proc. 41st Ann. Meeting Association for Computational Linguistics,</booktitle>
<pages>205--208</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2139" citStr="Eisner (2003)" startWordPosition="306" endWordPosition="307"> XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as observed by Eisner (2003) and Shieber (2004). This deficiency is overcome by synchronous tree substitution grammars, which are state-less linear and nondeleting XTOP. Recently, Maletti (2010b) proposed MBOT, and Zhang et al. (2008b) and Sun et al. (2009) proposed the even more powerful synchronous tree-sequence substitution grammars. Those two models allow certain translation discontinuities, and the former device also offers computational benefits over linear and nondeleting XTOP as argued by Maletti (2010b). The simplicity of XTOP makes them very appealing as translation models. In 2010 the ATANLP participants [work</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. 41st Ann. Meeting Association for Computational Linguistics, pages 205–208. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joost Engelfriet</author>
<author>Sebastian Maneth</author>
</authors>
<title>Macro tree translations of linear size increase are MSO definable.</title>
<date>2003</date>
<journal>SIAM J. Comput.,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="2960" citStr="Engelfriet and Maneth (2003)" startWordPosition="427" endWordPosition="430">et al. (2008b) and Sun et al. (2009) proposed the even more powerful synchronous tree-sequence substitution grammars. Those two models allow certain translation discontinuities, and the former device also offers computational benefits over linear and nondeleting XTOP as argued by Maletti (2010b). The simplicity of XTOP makes them very appealing as translation models. In 2010 the ATANLP participants [workshop at ACL] identified ‘copying’ as the most exciting and promising feature of XTOP, but unrestricted copying can lead to an undesirable explosion of the size of the translation. According to Engelfriet and Maneth (2003) a tree transformation has linear size-increase if the size of each output tree is linearly bounded by the size of its corresponding input tree. The author believes that this is a very sensible restriction that intuitively makes sense and at the same time suitably limits the copying power of XTOP. We show that every sensible tree transformation 263 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 263–273, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics that can be computed by a</context>
<context position="14808" citStr="Engelfriet and Maneth (2003)" startWordPosition="2729" endWordPosition="2732">the next paragraph. 2This is a convenience requirement. We can use other conditions on the XTOPR or the used weight structures to guarantee a well-defined semantics. qVB qNP qNP u S t2 t1 t3 u qo S ρ,.5 ⇒M VP t1 t2 t3 τM(t, u) = q 266 σ σ σ q ⇒ρ0,1 Mex ⇒ρ0,1 Mex ⇒ρ0,1 Mex q q γ γ q γ q γ q γ α α α α α α ⇒ρ,2 Mex σ σ ⇒ρ,2 ρ,2 Mex Mex q q α α q α q α σ q α σ α q α q α σ σ σ σ σ α α α α Figure 5: Example derivation using the XTOP Mex with weight 13 · 24 = 16. the size of each output tree should be linearly bounded in the size of the corresponding input tree according to Aho and Ullman (1971) and Engelfriet and Maneth (2003). Definition 2. A mapping τ : TE × To → R+ is linear-size increase if there exists an integer n ∈ N such that |u |≤ n · |t |for all (t, u) ∈ supp(τ). An XTOPR M is sensible if τM is linear-size increase. ‘Sensible’ is not a syntactic property of an XTOPR as it does not depend on the actual rules, but only on its computed weighted tree transformation. The XTOP Mex of Example 1 is not sensible because |u |= 2|t |− 1 for every (t, u) ∈ τMex. Intuitively, the number of times that Mex can use the copying rule ρ0 is not uniformly bounded. We need an auxiliary result in the main part. Let τ : TE × To</context>
<context position="17331" citStr="Engelfriet and Maneth (2003)" startWordPosition="3213" endWordPosition="3216">rate the overall structure of our arguments to show that every sensible XTOPR can be implemented by an equivalent MBOT. We first normalize the given XTOPR such that the semantic property ‘sensible’ yields a syntactic property called ‘finitely copying’ (see Section 4). In a second step, we show that each finitely copying XTOPR can be implemented by an equivalent MBOT (see Section 5). Figure 6 illustrates these steps towards our main result. In the final section, we derive some consequences from our main result (see Section 6). 4 From sensible to finite copying First, we adjust a normal form of Engelfriet and Maneth (2003) to our needs. This section borrows heavily from Aho and Ullman (1971) and Engelfriet and Maneth (2003), where “sensible” (unweighted) deterministic macro tree transducers (MAC) [see Engelfriet and Vogler (1985)] are considered. Our setting is simpler on the one hand because XTOPR do not have context parameters as MAC, but more difficult on the other hand because we consider nondeterministic and weighted transducers. Intuitively, a sensible XTOPR cannot copy a lot since the size of each output tree is linearly bounded in the size of the corresponding input tree. However, the actual presentatio</context>
<context position="20371" citStr="Engelfriet and Maneth (2003)" startWordPosition="3756" endWordPosition="3759">ed into only finitely many output trees due to sensibility. The restriction includes the look-ahead (because we require that the rewrite step weights are nonzero), which might further restrict the input trees. Example 5. The state q of the XTOP Mex is proper because we already demonstrated that it can transform infinitely many input trees into infinitely many output trees. The XTOPR M is proper if all its states except the initial state q0 are proper. Next, we show that each XTOPR can be transformed into an equivalent proper XTOPR using a simplified version of the construction of Lemma 5.4 by Engelfriet and Maneth (2003). Mind that we generally assume that all considered XTOPR are producing. Theorem 6. For every XTOPR there exists an equivalent proper XTOPR. Proof sketch. The construction is iterative. Suppose that M is not yet proper. Then there exists a state q E Q, which can produce only finitely many outputs U. It can be decided whether a state is proper using Theorem 4.5 of Drewes and Engelfriet (1998), and in case it is proper, the set U can also be computed effectively. The cited theorem applies to unweighted XTOPR, but it can be applied also in our setting because ==&gt;.∗M in Definition 4 disregards wei</context>
<context position="23516" citStr="Engelfriet and Maneth (2003)" startWordPosition="4347" endWordPosition="4351">anslation options. Then we create the copy ρ0 q0(S(x1, VP(x2, x3))) —* S(u, qNP(x1), qNP(x3)) of the rule ρ with look-ahead c0(ρ0, x) such that � cρ,x(t) if x =� x2 c0 ρ�,x(t) = τqVB M (t, u) if x = x2 In general, there can be infinitely many input trees ti that translate to a selected output tree u, so we cannot simply replace the variable in the lefthand side by all the options for the input tree. This is the reason why we use the look-ahead because the set τ−1M (u) is a regular weighted tree language. From now on, we assume that the XTOPR M is proper. Next, we want to invoke Theorem 7.1 of Engelfriet and Maneth (2003) to show that a proper sensible XTOPR is finitely copying. Engelfriet and Maneth (2003) present a formal definition of finite copying, but we only present a high-level description of it. Definition 8. The XTOPR M is finitely copying if there is a copying bound n E N such that no input subtree is copied more than n times in any derivation q(t) ==&gt;.∗M u with q E Q, t E TE, and u E TA. Example 9. The XTOP of Example 1 is not finitely copying as the input subtree α is copied 2n times if the input tree is γn(α). Clearly, this shows that there is no uniform bound on the number of copies. It is worth</context>
<context position="24878" citStr="Engelfriet and Maneth (2003)" startWordPosition="4595" endWordPosition="4599">s and a weighted XTOPR does have one of those properties if and only if its associated unweighted XTOPR has it. We now use this tight connection to lift Theorem 7.1 of Engelfriet and Maneth (2003) from the unweighted (and deterministic) case to the weighted (and nondeterministic) case. Theorem 10. If a proper XTOPR is sensible, then it is finitely copying. Proof. Let M be the input XTOPR. Since M is sensible, its associated unweighted XTOPR N, which is obtained by setting all weights to 1 and computing in the BOOLEAN semiring, is sensible. Consequently, N is finitely copying by Theorem 7.1 of Engelfriet and Maneth (2003). Thus, also M is finitely copying, which concludes the proof. We remark that Theorem 7.1 of Engelfriet and Maneth (2003) only applies to deterministic XTOPR, but the essential pumping argument, which is Lemma 6.2 of Engelfriet and Maneth (2003) also works for nondeterministic XTOPR. Essentially, the pumping argument shows the contraposition. If M is not finitely copying, then M can copy a certain subtree an arbitrarily often. Due to the properness of M, all these copies have an impact on the output tree, which yields that its size grows beyond any uniform linear bound, which in turn demonstra</context>
</contexts>
<marker>Engelfriet, Maneth, 2003</marker>
<rawString>Joost Engelfriet and Sebastian Maneth. 2003. Macro tree translations of linear size increase are MSO definable. SIAM J. Comput., 32(4):950–1006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joost Engelfriet</author>
<author>Heiko Vogler</author>
</authors>
<title>Macro tree transducers.</title>
<date>1985</date>
<journal>J. Comput. System Sci.,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="17542" citStr="Engelfriet and Vogler (1985)" startWordPosition="3246" endWordPosition="3249">actic property called ‘finitely copying’ (see Section 4). In a second step, we show that each finitely copying XTOPR can be implemented by an equivalent MBOT (see Section 5). Figure 6 illustrates these steps towards our main result. In the final section, we derive some consequences from our main result (see Section 6). 4 From sensible to finite copying First, we adjust a normal form of Engelfriet and Maneth (2003) to our needs. This section borrows heavily from Aho and Ullman (1971) and Engelfriet and Maneth (2003), where “sensible” (unweighted) deterministic macro tree transducers (MAC) [see Engelfriet and Vogler (1985)] are considered. Our setting is simpler on the one hand because XTOPR do not have context parameters as MAC, but more difficult on the other hand because we consider nondeterministic and weighted transducers. Intuitively, a sensible XTOPR cannot copy a lot since the size of each output tree is linearly bounded in the size of the corresponding input tree. However, the actual presentation of the XTOPR M might con267 sensible XTOPR sensible proper XTOPR finitely copying XTOPR linear and nondeleting MBOT Figure 6: Overview of the proof steps. tain rules that allow unbounded copying. This unbounde</context>
</contexts>
<marker>Engelfriet, Vogler, 1985</marker>
<rawString>Joost Engelfriet and Heiko Vogler. 1985. Macro tree transducers. J. Comput. System Sci., 31(1):71–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joost Engelfriet</author>
<author>Eric Lilin</author>
<author>Andreas Maletti</author>
</authors>
<title>Extended multi bottom-up tree transducers — composition and decomposition.</title>
<date>2009</date>
<journal>Acta Inform.,</journal>
<volume>46</volume>
<issue>8</issue>
<pages>590</pages>
<contexts>
<context position="26101" citStr="Engelfriet et al. (2009)" startWordPosition="4807" endWordPosition="4810">ates that M is not sensible. . 269 We showed that each sensible XTOPR can be implemented by a finitely copying XTOPR via the construction of the proper normal form. This approach actually yields a characterization because finitely copying XTOPR are trivially sensible by Theorem 4.19 of Engelfriet and Maneth (2003). 5 From finite copying to an MBOT We complete the argument by showing how to implement a finitely copying XTOPR by a weighted multi bottom-up tree transducer (MBOT). First, we recall the MBOT, which was introduced by Arnold and Dauchet (1982) and Lilin (1978) in the unweighted case. Engelfriet et al. (2009) give an English presentation. We present the linear and nondeleting MBOT of Engelfriet et al. (2009). A weighted multi bottom-up tree transducer is a system M = (Q, E, A, F, R, wt) with • an alphabet Q of states, • alphabets E and A of input and output symbols, • a set F C_ Q of final states, • a finite set R of rules of the form E —* r where E E TΣ(Q(X)) and r E Q(TΔ(X)) are linear and var(E) = var(r), and • wt: R —* R+ assigning rule weights. We now use TΣ(Q(X)) and Q(TΔ(X)) instead of TΣ(Q[X]) and Q[TΔ(X)], which highlights the difference between XTOPR and MBOT. First, MBOT are a bottom-up</context>
<context position="28317" citStr="Engelfriet et al. (2009)" startWordPosition="5246" endWordPosition="5250"> nEN,ρ1,...,ρnER t�M ···�M q(u1,...,uk) for all t E TΣ and u1, ... , uk E TΔ. The semantics of M is TM(t, u) = EqEF TqM(t, u) for all t E TΣ and u E TΔ. We move to the last step for our main result, in which we show how to implement each finitely copying XTOPR by an MBOT using a weighted version of the construction in Lemma 15 of Maletti (2008). The computational benefits (binarization, composition, efficient parsing, etc.) of MBOT over XTOPR are described by Maletti (2011a). Theorem 11. Every finitely copying XTOPR can be implemented by an MBOT. Proof sketch. We plan to utilize Theorem 18 of Engelfriet et al. (2009), which proves the same statement in the unweighted and deterministic case. Again, the weights are not problematic, but we need to remove the nondeterminism before we can apply it. This is achieved by a decomposition into two XTOPR. The first XTOPR annotates the input tree with the rules that the second XTOPR is supposed to use. Thus, the first XTOPR remains nondeterministic, but the second XTOPR, which simply executes the annotated rules, is now deterministic. This standard approach due to Engelfriet (1975) is used in many similar constructions. Suppose that n is a copying bound for the input</context>
<context position="30479" citStr="Engelfriet et al. (2009)" startWordPosition="5645" endWordPosition="5648">eterministic XTOP M2. Only the weight and look-ahead of rules that are actually executed are applied (e.g., although we annotate n rules at the root symbol, we only execute the first rule and thus only apply its weight and lookahead). The look-ahead of different rules is either resolved (i.e., pushed to the next rules) or multiplied using the HADAMARD product [see Fülöp and Vogler (2009)], which preserves regularity. This process is also used by Seemann et al. (2012). Now we can use Theorem 4 of Maletti (2011a) to obtain an MBOT N1 that is equivalent to M1. Similarly, we can use Theorem 18 of Engelfriet et al. (2009) to obtain an MBOT N2 that is equivalent to M2. Since MBOT are closed under composition by Theorem 23 of Engelfriet et al. (2009), we can compose N1 and N2 to obtain a single MBOT N that is equivalent to M. Corollary 12. For every sensible producing XTOPR there exists an equivalent MBOT. Proof. Theorem 6 shows that there exists an equivalent proper XTOPR, which must be finitely copying by Theorem 10. This last fact allows us to construct an equivalent MBOT by Theorem 11. 6 Preservation of regularity Finally, we present an application of Corollary 12 to solve an open problem. The translation mo</context>
</contexts>
<marker>Engelfriet, Lilin, Maletti, 2009</marker>
<rawString>Joost Engelfriet, Eric Lilin, and Andreas Maletti. 2009. Extended multi bottom-up tree transducers — composition and decomposition. Acta Inform., 46(8):561– 590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joost Engelfriet</author>
</authors>
<title>Bottom-up and top-down tree transformations — a comparison.</title>
<date>1975</date>
<journal>Math. Systems Theory,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="28830" citStr="Engelfriet (1975)" startWordPosition="5334" endWordPosition="5335">R can be implemented by an MBOT. Proof sketch. We plan to utilize Theorem 18 of Engelfriet et al. (2009), which proves the same statement in the unweighted and deterministic case. Again, the weights are not problematic, but we need to remove the nondeterminism before we can apply it. This is achieved by a decomposition into two XTOPR. The first XTOPR annotates the input tree with the rules that the second XTOPR is supposed to use. Thus, the first XTOPR remains nondeterministic, but the second XTOPR, which simply executes the annotated rules, is now deterministic. This standard approach due to Engelfriet (1975) is used in many similar constructions. Suppose that n is a copying bound for the input XTOPR M, which means that no more than n rules are applied to each input symbol. The first XTOPR is actually a nondeterministic linear and nondeleting XTOP that annotates each input tree symbol with exactly n rules of M that are consistent with the state behavior of M. Moreover, the annotation also prescribes with which of n rules the processing should continue at each subtree. Since we know all the rules that will potentially be applied for a certain symbol, we can make the assignment such that no annotate</context>
</contexts>
<marker>Engelfriet, 1975</marker>
<rawString>Joost Engelfriet. 1975. Bottom-up and top-down tree transformations — a comparison. Math. Systems Theory, 9(3):198–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joost Engelfriet</author>
</authors>
<title>Top-down tree transducers with regular look-ahead.</title>
<date>1977</date>
<journal>Math. Systems Theory,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="9054" citStr="Engelfriet (1977)" startWordPosition="1561" endWordPosition="1563">-hand side (as in the rule of Figure 3), whereas a TOP rule contains exactly one symbol in the left-hand side. Shieber (2004) and Knight (2007) identified that this extension is essential for many NLP applications because without it linear (i.e., non-copying) cannot compute rotations (see Figure 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (2009) the most general XTOP model includes copying, deletion, and regular look-ahead in the spirit of Engelfriet (1977). More powerful models (such as synchronous tree-sequence substitution grammars and multi bottom-up tree transducers) can handle translation discontinuities naturally as evidenced by Zhang et al. (2008a) and Maletti (2011b), but q0 S x1 VP x2 x3 Figure 3: Example XTOP rule by Graehl et al. (2008). XTOP need copying and deletion to handle them. Copying essentially allows an XTOP to translate certain parts of the input several times and was identified by the ATANLP 2010 participants as one of the most interesting and promising features of XTOP. Currently, the look-ahead feature is not used in ma</context>
</contexts>
<marker>Engelfriet, 1977</marker>
<rawString>Joost Engelfriet. 1977. Top-down tree transducers with regular look-ahead. Math. Systems Theory, 10(1):289–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoltán Fülöp</author>
<author>Heiko Vogler</author>
</authors>
<title>Weighted tree automata and tree transducers.</title>
<date>2009</date>
<booktitle>Handbook of Weighted Automata, EATCS Monographs on Theoret. Comput. Sci., chapter 9,</booktitle>
<pages>313--403</pages>
<editor>In Manfred Droste, Werner Kuich, and Heiko Vogler, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1167" citStr="Fülöp and Vogler (2009)" startWordPosition="157" endWordPosition="160">ghted multi bottom-up tree transducer. This further motivates weighted multi bottom-up tree transducers as suitable translation models for syntax-based machine translation. 1 Introduction Several different translation models are used in syntax-based statistical machine translation. Koehn (2010) presents an introduction to statistical machine translation, and Knight (2007) presents an overview of syntax-based statistical machine translation. The oldest and best-studied tree transformation device is the top-down tree transducer of Rounds (1970) and Thatcher (1970). Gécseg and Steinby (1984) and Fülöp and Vogler (2009) present the existing results on the unweighted and weighted model, respectively. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) investigated XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBO</context>
<context position="6380" citStr="Fülöp and Vogler (2009)" startWordPosition="1073" endWordPosition="1076"> ran211 away221 Figure 1: The tree t (with positions indicated as superscripts) is linear and var(t) = {x2}. The tree t[He]111 is the same tree with x2 replaced by ‘He’. linear if every x ∈ X occurs at most once in t (i.e., |posx(t) |≤ 1). Moreover, var(t) = {x ∈ X |posx(t) =6 ∅} contains the variables that occur in t. A substitution 0 is a mapping 0: X → TΣ(V ). When applied to t, it returns the tree t0, which is obtained from t by replacing all occurrences of x ∈ X in t by 0(x). Our notions for trees are illustrated in Figure 1. Finally, we present weighted tree grammars (WTG) as defined by Fülöp and Vogler (2009), who defined it for arbitrary semirings as weight structures. In contrast, our weights are always nonnegative reals, which form the semiring (R+, +, ·, 0,1) and are used in probabilistic grammars. For each weight assignment f : T → R+, we let supp(f) = {t ∈ T |f(t) =6 0} . WTG offer an efficient representation of weighted forests (i.e., set of weighted trees), which is even more efficient than the packed forests of Mi et al. (2008) because they can be minimized efficiently using an algorithm of Maletti and Quernheim (2011). In particular, WTG can share more than equivalent subtrees and can ev</context>
<context position="13406" citStr="Fülöp and Vogler (2009)" startWordPosition="2433" endWordPosition="2436">)⇒ρ1,w1···⇒ρn,wn M u M for every t ∈ TE and u ∈ TA. The XTOPR M computes the weighted tree transformation τq�M. Two XTOPR M and N are equivalent, if τM = τN. The sum (1) can be infinite, which we avoid by simply requiring that all our XTOPR are producing, which means that r ∈/ Q[X] for every rule ` → r ∈ R.2 In a producing XTOPR each rule application produces at least one output symbol, which limits the number n of rule applications to the size of the output tree u. A detailed exposition to XTOPR is presented by Arnold and Dauchet (1982) and Graehl et al. (2009) for the unweighted case and by Fülöp and Vogler (2009) for the weighted case. Example 1. Let Mex = (Q, E, E, q, R, c, wt) be the nondeleting XTOP with • Q = {q}, • E = {σ, γ, α}, • the two rules q(α) → α (ρ) q(γ(x1)) → σ(q(x1),q(x1)) (ρ0) • trivial look-ahead (i.e., c(ρ, x) = �1E), and • wt(ρ) = 2 and wt(ρ0) = 1. The XTOPR Mex computes the tree transformation that turns the input tree γn(α) into the fully balanced binary tree u of the same height with weight 2(2n). An example derivation is presented in Figure 5. Unrestricted copying (as in Example 1) yields very undesirable phenomena and is most likely not needed in the machine translation task. </context>
<context position="21476" citStr="Fülöp and Vogler (2009)" startWordPosition="3967" endWordPosition="3970">m applies to unweighted XTOPR, but it can be applied also in our setting because ==&gt;.∗M in Definition 4 disregards weights. Now we consider each u E U individually. Clearly, (τqM)−1(u) is regular by Theorem 3. For each u and each occurrence of q in the right-hand side of a rule ρ E R of M, we create a copy ρ0 of ρ, in which the selected occurrence of q(x) is replaced by u and the new lookahead is c(ρ0, x) = c(ρ, x) · (τqM)−1(u), which restricts the input tree appropriately and includes the adjustment of the weights. Since regular weighted tree languages are closed under HADAMARD products [see Fülöp and Vogler (2009)], the look-ahead c(ρ, x) · (τqM)−1(u) is again regular. Essentially, we precompute the action of q as much as possible, and immediately output one of the finitely many output trees, check that the input tree has the required shape using the look-ahead, and charge the weight for the precomputed transformation again using the look-ahead. This process is done for each occurrence, so if a rule contains two occurrences of q, then the process must be 268 Figure 7: Illustration of the derivation in Definition 4. q0 ... ... t ==&gt;.∗M . .. q s ==&gt;.∗ M u0 ... done twice to this rule. In this way, we eve</context>
<context position="30245" citStr="Fülöp and Vogler (2009)" startWordPosition="5602" endWordPosition="5605">ch includes the look-ahead, 270 qNP VP qVB x1 qNP x3 x4 x2 � x2 x1 x3 S qS x4 qNP u1 qVB S u2 t VP u3 u4 qNP P ==&gt;.M u2 u1 u3 S qS t u4 S Figure 8: Example MBOT rule ρ [left] and its use in a rewrite step [right]. and an unweighted deterministic XTOP M2. Only the weight and look-ahead of rules that are actually executed are applied (e.g., although we annotate n rules at the root symbol, we only execute the first rule and thus only apply its weight and lookahead). The look-ahead of different rules is either resolved (i.e., pushed to the next rules) or multiplied using the HADAMARD product [see Fülöp and Vogler (2009)], which preserves regularity. This process is also used by Seemann et al. (2012). Now we can use Theorem 4 of Maletti (2011a) to obtain an MBOT N1 that is equivalent to M1. Similarly, we can use Theorem 18 of Engelfriet et al. (2009) to obtain an MBOT N2 that is equivalent to M2. Since MBOT are closed under composition by Theorem 23 of Engelfriet et al. (2009), we can compose N1 and N2 to obtain a single MBOT N that is equivalent to M. Corollary 12. For every sensible producing XTOPR there exists an equivalent MBOT. Proof. Theorem 6 shows that there exists an equivalent proper XTOPR, which mu</context>
</contexts>
<marker>Fülöp, Vogler, 2009</marker>
<rawString>Zoltán Fülöp and Heiko Vogler. 2009. Weighted tree automata and tree transducers. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, EATCS Monographs on Theoret. Comput. Sci., chapter 9, pages 313–403. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoltán Fülöp</author>
<author>Andreas Maletti</author>
<author>Heiko Vogler</author>
</authors>
<title>Weighted extended tree transducers.</title>
<date>2011</date>
<journal>Fundam. Inform.,</journal>
<volume>111</volume>
<issue>2</issue>
<contexts>
<context position="31831" citStr="Fülöp et al. (2011)" startWordPosition="5881" endWordPosition="5884">ns that an output tree is supplied and the corresponding input trees are sought. This starting output tree is typically the best parse of the string that we want to translate. However, instead of a single tree, we want to use all parses of this sentence together with their parse scores. Those parses form a regular weighted tree language, and applying them backwards to the translation model yields another weighted tree language L of corresponding input trees. For an efficient representation and efficient modification algorithms (such a k-best extraction) we would like L to be regular. However, Fülöp et al. (2011) demonstrate that the backward application of a regular weighted tree language to an XTOPR is not necessarily regular. The counterexample uses a variant of the XTOP of Example 1 and is thus not sensible. Theorem 14 of Maletti (2011a) shows that MBOT preserve regularity under backward application. Corollary 13. Sensible XTOPR preserve regularity under backward application. Conclusion We demonstrated that each sensible XTOPR can be implemented by an MBOT. The latter formalism offers many computational advantages, so that the author believes that MBOT should be used instead of XTOP. We used real </context>
</contexts>
<marker>Fülöp, Maletti, Vogler, 2011</marker>
<rawString>Zoltán Fülöp, Andreas Maletti, and Heiko Vogler. 2011. Weighted extended tree transducers. Fundam. Inform., 111(2):163–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferenc Gécseg</author>
<author>Magnus Steinby</author>
</authors>
<title>Tree Automata. Akadémiai Kiadó,</title>
<date>1984</date>
<location>Budapest.</location>
<contexts>
<context position="1139" citStr="Gécseg and Steinby (1984)" startWordPosition="152" endWordPosition="155"> can also be computed by a weighted multi bottom-up tree transducer. This further motivates weighted multi bottom-up tree transducers as suitable translation models for syntax-based machine translation. 1 Introduction Several different translation models are used in syntax-based statistical machine translation. Koehn (2010) presents an introduction to statistical machine translation, and Knight (2007) presents an overview of syntax-based statistical machine translation. The oldest and best-studied tree transformation device is the top-down tree transducer of Rounds (1970) and Thatcher (1970). Gécseg and Steinby (1984) and Fülöp and Vogler (2009) present the existing results on the unweighted and weighted model, respectively. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) investigated XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (MBOT) [as k-morphisms]. Rec</context>
<context position="8012" citStr="Gécseg and Steinby (1984)" startWordPosition="1393" endWordPosition="1396">). For all �, C ∈ TΣ(Q) and a production p = q → r, NP1 PP11 x111 2 RB22 264 Figure 2: Example rotation. In principle, such rotations are required in the translation from English to Arabic. we write ξ ==&gt;.G ζ if ξ = ξ[q]p and ζ = ξ[r]p, where p is the lexicographically least element of posQ(ξ). The WTG G generates the weighted tree language LG: TE H R+ such that LG(t) = E wt(ρ1) · ... · wt(ρn) nEN,p1,...,pnEP P1 Pn 40�G ···�G t for every t E TE. Each such language is regular, and Reg(E) contains all those languages over the alphabet E. A thorough introduction to tree languages is presented by Gécseg and Steinby (1984) and Gécseg and Steinby (1997) for the unweighted case and by FUlöp and Vogler (2009) for the weighted case. 3 Extended top-down tree transducers We start by introducing the main model of this contribution. Extended top-down tree transducers (XTOP) are a generalization of the top-down tree transducers (TOP) of Rounds (1970) and Thatcher (1970). XTOP allow rules with several (non-state and non-variable) symbols in the left-hand side (as in the rule of Figure 3), whereas a TOP rule contains exactly one symbol in the left-hand side. Shieber (2004) and Knight (2007) identified that this extension </context>
</contexts>
<marker>Gécseg, Steinby, 1984</marker>
<rawString>Ferenc Gécseg and Magnus Steinby. 1984. Tree Automata. Akadémiai Kiadó, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferenc Gécseg</author>
<author>Magnus Steinby</author>
</authors>
<title>Tree languages.</title>
<date>1997</date>
<booktitle>In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>1--68</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="8042" citStr="Gécseg and Steinby (1997)" startWordPosition="1398" endWordPosition="1402">production p = q → r, NP1 PP11 x111 2 RB22 264 Figure 2: Example rotation. In principle, such rotations are required in the translation from English to Arabic. we write ξ ==&gt;.G ζ if ξ = ξ[q]p and ζ = ξ[r]p, where p is the lexicographically least element of posQ(ξ). The WTG G generates the weighted tree language LG: TE H R+ such that LG(t) = E wt(ρ1) · ... · wt(ρn) nEN,p1,...,pnEP P1 Pn 40�G ···�G t for every t E TE. Each such language is regular, and Reg(E) contains all those languages over the alphabet E. A thorough introduction to tree languages is presented by Gécseg and Steinby (1984) and Gécseg and Steinby (1997) for the unweighted case and by FUlöp and Vogler (2009) for the weighted case. 3 Extended top-down tree transducers We start by introducing the main model of this contribution. Extended top-down tree transducers (XTOP) are a generalization of the top-down tree transducers (TOP) of Rounds (1970) and Thatcher (1970). XTOP allow rules with several (non-state and non-variable) symbols in the left-hand side (as in the rule of Figure 3), whereas a TOP rule contains exactly one symbol in the left-hand side. Shieber (2004) and Knight (2007) identified that this extension is essential for many NLP appl</context>
</contexts>
<marker>Gécseg, Steinby, 1997</marker>
<rawString>Ferenc Gécseg and Magnus Steinby. 1997. Tree languages. In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages, volume 3, chapter 1, pages 1–68. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan S Golan</author>
</authors>
<title>Semirings and their Applications.</title>
<date>1999</date>
<publisher>Kluwer Academic,</publisher>
<location>Dordrecht.</location>
<marker>Golan, 1999</marker>
<rawString>Jonathan S. Golan. 1999. Semirings and their Applications. Kluwer Academic, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Jonathan May</author>
</authors>
<title>Training tree transducers.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="9351" citStr="Graehl et al. (2008)" startWordPosition="1606" endWordPosition="1609">re 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (2009) the most general XTOP model includes copying, deletion, and regular look-ahead in the spirit of Engelfriet (1977). More powerful models (such as synchronous tree-sequence substitution grammars and multi bottom-up tree transducers) can handle translation discontinuities naturally as evidenced by Zhang et al. (2008a) and Maletti (2011b), but q0 S x1 VP x2 x3 Figure 3: Example XTOP rule by Graehl et al. (2008). XTOP need copying and deletion to handle them. Copying essentially allows an XTOP to translate certain parts of the input several times and was identified by the ATANLP 2010 participants as one of the most interesting and promising features of XTOP. Currently, the look-ahead feature is not used in machine translation, but we need it later on in the theoretical development. Given an alphabet Q and a set T, we let Q[T]={q(t) I q E Q,t E TI, in which the root always has exactly one successor from T in contrast to Q(T). We treat elements of Q[TE(V )] as special trees of TEuQ(V ). Moreover, we le</context>
</contexts>
<marker>Graehl, Knight, May, 2008</marker>
<rawString>Jonathan Graehl, Kevin Knight, and Jonathan May. 2008. Training tree transducers. Comput. Linguist., 34(3):391–427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Andreas Maletti</author>
</authors>
<title>The power of extended top-down tree transducers.</title>
<date>2009</date>
<journal>SIAM J. Comput.,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="8940" citStr="Graehl et al. (2009)" startWordPosition="1541" endWordPosition="1544"> of Rounds (1970) and Thatcher (1970). XTOP allow rules with several (non-state and non-variable) symbols in the left-hand side (as in the rule of Figure 3), whereas a TOP rule contains exactly one symbol in the left-hand side. Shieber (2004) and Knight (2007) identified that this extension is essential for many NLP applications because without it linear (i.e., non-copying) cannot compute rotations (see Figure 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (2009) the most general XTOP model includes copying, deletion, and regular look-ahead in the spirit of Engelfriet (1977). More powerful models (such as synchronous tree-sequence substitution grammars and multi bottom-up tree transducers) can handle translation discontinuities naturally as evidenced by Zhang et al. (2008a) and Maletti (2011b), but q0 S x1 VP x2 x3 Figure 3: Example XTOP rule by Graehl et al. (2008). XTOP need copying and deletion to handle them. Copying essentially allows an XTOP to translate certain parts of the input several times and was identified by the ATANLP 2010 participants </context>
<context position="13351" citStr="Graehl et al. (2009)" startWordPosition="2423" endWordPosition="2426">ate q ∈ Q is w1 · ... · wn (1) n∈N` ,ρ1,...,ρn∈R q(t)⇒ρ1,w1···⇒ρn,wn M u M for every t ∈ TE and u ∈ TA. The XTOPR M computes the weighted tree transformation τq�M. Two XTOPR M and N are equivalent, if τM = τN. The sum (1) can be infinite, which we avoid by simply requiring that all our XTOPR are producing, which means that r ∈/ Q[X] for every rule ` → r ∈ R.2 In a producing XTOPR each rule application produces at least one output symbol, which limits the number n of rule applications to the size of the output tree u. A detailed exposition to XTOPR is presented by Arnold and Dauchet (1982) and Graehl et al. (2009) for the unweighted case and by Fülöp and Vogler (2009) for the weighted case. Example 1. Let Mex = (Q, E, E, q, R, c, wt) be the nondeleting XTOP with • Q = {q}, • E = {σ, γ, α}, • the two rules q(α) → α (ρ) q(γ(x1)) → σ(q(x1),q(x1)) (ρ0) • trivial look-ahead (i.e., c(ρ, x) = �1E), and • wt(ρ) = 2 and wt(ρ0) = 1. The XTOPR Mex computes the tree transformation that turns the input tree γn(α) into the fully balanced binary tree u of the same height with weight 2(2n). An example derivation is presented in Figure 5. Unrestricted copying (as in Example 1) yields very undesirable phenomena and is m</context>
</contexts>
<marker>Graehl, Hopkins, Knight, Maletti, 2009</marker>
<rawString>Jonathan Graehl, Mark Hopkins, Kevin Knight, and Andreas Maletti. 2009. The power of extended top-down tree transducers. SIAM J. Comput., 39(2):410–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Hebisch</author>
<author>Hanns J Weinert</author>
</authors>
<date>1998</date>
<booktitle>Semirings— Algebraic Theory and Applications in Computer Science. World Scientific.</booktitle>
<marker>Hebisch, Weinert, 1998</marker>
<rawString>Udo Hebisch and Hanns J. Weinert. 1998. Semirings— Algebraic Theory and Applications in Computer Science. World Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>An overview of probabilistic tree transducers for natural language processing.</title>
<date>2005</date>
<booktitle>In Proc. 6th Int. Conf. Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>3406</volume>
<pages>1--24</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10064" citStr="Knight and Graehl (2005)" startWordPosition="1741" endWordPosition="1744">anslate certain parts of the input several times and was identified by the ATANLP 2010 participants as one of the most interesting and promising features of XTOP. Currently, the look-ahead feature is not used in machine translation, but we need it later on in the theoretical development. Given an alphabet Q and a set T, we let Q[T]={q(t) I q E Q,t E TI, in which the root always has exactly one successor from T in contrast to Q(T). We treat elements of Q[TE(V )] as special trees of TEuQ(V ). Moreover, we let —1E(t) = 1 for every t E TE. XTOP with regular look-ahead (XTOPR) were also studied by Knight and Graehl (2005) and Graehl et al. (2008). Formally, an XTOPR is a system M = (Q, E, A, q0, R, c, wt) with • a finite set Q of states, • alphabets E and A of input and output symbols, • a starting state q0 E Q, • a finite set R of rules of the form ` H r with linear ` E Q[TE(X)] and r E TA(Q[var(`)]), • c: R x X H Reg(E) assigns a regular lookahead to each deleted variable of a rule [i.e., c(` H r, x) = —1E for all ` H r E R and x E X \ (var(`) \ var(r))], and • wt: R H R+ assigns rule weights. The XTOPR M is linear [respectively, nondeleting] if r is linear [respectively, var(`) = var(r)] for every rule ` H—</context>
</contexts>
<marker>Knight, Graehl, 2005</marker>
<rawString>Kevin Knight and Jonathan Graehl. 2005. An overview of probabilistic tree transducers for natural language processing. In Proc. 6th Int. Conf. Computational Linguistics and Intelligent Text Processing, volume 3406 of LNCS, pages 1–24. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Capturing practical natural language transformations.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="918" citStr="Knight (2007)" startWordPosition="122" endWordPosition="123">of each output tree is uniformly bounded by a linear function in the size of the corresponding input tree. Every sensible tree transformation computed by an arbitrary weighted extended top-down tree transducer can also be computed by a weighted multi bottom-up tree transducer. This further motivates weighted multi bottom-up tree transducers as suitable translation models for syntax-based machine translation. 1 Introduction Several different translation models are used in syntax-based statistical machine translation. Koehn (2010) presents an introduction to statistical machine translation, and Knight (2007) presents an overview of syntax-based statistical machine translation. The oldest and best-studied tree transformation device is the top-down tree transducer of Rounds (1970) and Thatcher (1970). Gécseg and Steinby (1984) and Fülöp and Vogler (2009) present the existing results on the unweighted and weighted model, respectively. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) inve</context>
<context position="8580" citStr="Knight (2007)" startWordPosition="1487" endWordPosition="1488">ges is presented by Gécseg and Steinby (1984) and Gécseg and Steinby (1997) for the unweighted case and by FUlöp and Vogler (2009) for the weighted case. 3 Extended top-down tree transducers We start by introducing the main model of this contribution. Extended top-down tree transducers (XTOP) are a generalization of the top-down tree transducers (TOP) of Rounds (1970) and Thatcher (1970). XTOP allow rules with several (non-state and non-variable) symbols in the left-hand side (as in the rule of Figure 3), whereas a TOP rule contains exactly one symbol in the left-hand side. Shieber (2004) and Knight (2007) identified that this extension is essential for many NLP applications because without it linear (i.e., non-copying) cannot compute rotations (see Figure 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (2009) the most general XTOP model includes copying, deletion, and regular look-ahead in the spirit of Engelfriet (1977). More powerful models (such as synchronous tree-sequence substitution grammars and multi bottom-up tree transducers) can hand</context>
</contexts>
<marker>Knight, 2007</marker>
<rawString>Kevin Knight. 2007. Capturing practical natural language transformations. Machine Translation, 21(2):121–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="839" citStr="Koehn (2010)" startWordPosition="111" endWordPosition="112">i@ims.uni-stuttgart.de Abstract A tree transformation is sensible if the size of each output tree is uniformly bounded by a linear function in the size of the corresponding input tree. Every sensible tree transformation computed by an arbitrary weighted extended top-down tree transducer can also be computed by a weighted multi bottom-up tree transducer. This further motivates weighted multi bottom-up tree transducers as suitable translation models for syntax-based machine translation. 1 Introduction Several different translation models are used in syntax-based statistical machine translation. Koehn (2010) presents an introduction to statistical machine translation, and Knight (2007) presents an overview of syntax-based statistical machine translation. The oldest and best-studied tree transformation device is the top-down tree transducer of Rounds (1970) and Thatcher (1970). Gécseg and Steinby (1984) and Fülöp and Vogler (2009) present the existing results on the unweighted and weighted model, respectively. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reporte</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Lilin</author>
</authors>
<title>Une généralisation des transducteurs d’états finis d’arbres: les S-transducteurs. Thèse 3ème cycle, Université de Lille.</title>
<date>1978</date>
<contexts>
<context position="1549" citStr="Lilin (1978)" startWordPosition="220" endWordPosition="221">ew of syntax-based statistical machine translation. The oldest and best-studied tree transformation device is the top-down tree transducer of Rounds (1970) and Thatcher (1970). Gécseg and Steinby (1984) and Fülöp and Vogler (2009) present the existing results on the unweighted and weighted model, respectively. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) investigated XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as observed by Eisner (2003) and Shieb</context>
<context position="26052" citStr="Lilin (1978)" startWordPosition="4800" endWordPosition="4801"> linear bound, which in turn demonstrates that M is not sensible. . 269 We showed that each sensible XTOPR can be implemented by a finitely copying XTOPR via the construction of the proper normal form. This approach actually yields a characterization because finitely copying XTOPR are trivially sensible by Theorem 4.19 of Engelfriet and Maneth (2003). 5 From finite copying to an MBOT We complete the argument by showing how to implement a finitely copying XTOPR by a weighted multi bottom-up tree transducer (MBOT). First, we recall the MBOT, which was introduced by Arnold and Dauchet (1982) and Lilin (1978) in the unweighted case. Engelfriet et al. (2009) give an English presentation. We present the linear and nondeleting MBOT of Engelfriet et al. (2009). A weighted multi bottom-up tree transducer is a system M = (Q, E, A, F, R, wt) with • an alphabet Q of states, • alphabets E and A of input and output symbols, • a set F C_ Q of final states, • a finite set R of rules of the form E —* r where E E TΣ(Q(X)) and r E Q(TΔ(X)) are linear and var(E) = var(r), and • wt: R —* R+ assigning rule weights. We now use TΣ(Q(X)) and Q(TΔ(X)) instead of TΣ(Q[X]) and Q[TΔ(X)], which highlights the difference be</context>
</contexts>
<marker>Lilin, 1978</marker>
<rawString>Eric Lilin. 1978. Une généralisation des transducteurs d’états finis d’arbres: les S-transducteurs. Thèse 3ème cycle, Université de Lille.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
<author>Daniel Quernheim</author>
</authors>
<title>Pushing for weighted tree automata.</title>
<date>2011</date>
<booktitle>In Proc. 36th Int. Symp. Mathematical Foundations of Computer Science,</booktitle>
<volume>6907</volume>
<pages>460--471</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6909" citStr="Maletti and Quernheim (2011)" startWordPosition="1168" endWordPosition="1171">in Figure 1. Finally, we present weighted tree grammars (WTG) as defined by Fülöp and Vogler (2009), who defined it for arbitrary semirings as weight structures. In contrast, our weights are always nonnegative reals, which form the semiring (R+, +, ·, 0,1) and are used in probabilistic grammars. For each weight assignment f : T → R+, we let supp(f) = {t ∈ T |f(t) =6 0} . WTG offer an efficient representation of weighted forests (i.e., set of weighted trees), which is even more efficient than the packed forests of Mi et al. (2008) because they can be minimized efficiently using an algorithm of Maletti and Quernheim (2011). In particular, WTG can share more than equivalent subtrees and can even represent infinite sets of trees. A WTG is a system G = (Q, E, q0, P, wt) with • a finite set Q of states (nonterminals), • an alphabet E of symbols, • a starting state q0 ∈ Q, • a finite set P of productions q → r, where q ∈ Q and r ∈ TΣ(Q) \ Q, and • a mapping wt: P → R+ that assigns production weights. Without loss of generality, we assume that we can distinguish states and symbols (i.e., Q ∩ E = ∅). For all �, C ∈ TΣ(Q) and a production p = q → r, NP1 PP11 x111 2 RB22 264 Figure 2: Example rotation. In principle, suc</context>
</contexts>
<marker>Maletti, Quernheim, 2011</marker>
<rawString>Andreas Maletti and Daniel Quernheim. 2011. Pushing for weighted tree automata. In Proc. 36th Int. Symp. Mathematical Foundations of Computer Science, volume 6907 of LNCS, pages 460–471. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Compositions of extended topdown tree transducers. Inform. and Comput.,</title>
<date>2008</date>
<pages>10--1187</pages>
<contexts>
<context position="3706" citStr="Maletti (2008)" startWordPosition="548" endWordPosition="549"> input tree. The author believes that this is a very sensible restriction that intuitively makes sense and at the same time suitably limits the copying power of XTOP. We show that every sensible tree transformation 263 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 263–273, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics that can be computed by an XTOP can also be computed by an MBOT. For example, linear XTOP (i.e., no copying) compute only sensible tree transformations, and Maletti (2008) shows that for each linear XTOP there exists an equivalent MBOT. Here, we do not make any restrictions on the XTOP besides some sanity conditions (see Section 3). In particular, we consider copying XTOP. If we accept the restriction to linear size-increase tree transformation, then our main result further motivates MBOT as a suitable translation model for syntax-based machine translation because MBOT can implement each reasonable (even copying) XTOP. In addition, our result allows us to show that each reasonable XTOP preserves regularity under backward application. As demonstrated by May et a</context>
<context position="28039" citStr="Maletti (2008)" startWordPosition="5206" endWordPosition="5207"> can always be achieved by renaming the states. 4Again this could have been achieved with the help of other conditions on the MBOT or the used weight structure. The weighted tree transformation computed by M in state q E Q is q 1`TM(t, u1 ··· uk) wt(p1) · ... · wt(pn) nEN,ρ1,...,ρnER t�M ···�M q(u1,...,uk) for all t E TΣ and u1, ... , uk E TΔ. The semantics of M is TM(t, u) = EqEF TqM(t, u) for all t E TΣ and u E TΔ. We move to the last step for our main result, in which we show how to implement each finitely copying XTOPR by an MBOT using a weighted version of the construction in Lemma 15 of Maletti (2008). The computational benefits (binarization, composition, efficient parsing, etc.) of MBOT over XTOPR are described by Maletti (2011a). Theorem 11. Every finitely copying XTOPR can be implemented by an MBOT. Proof sketch. We plan to utilize Theorem 18 of Engelfriet et al. (2009), which proves the same statement in the unweighted and deterministic case. Again, the weights are not problematic, but we need to remove the nondeterminism before we can apply it. This is achieved by a decomposition into two XTOPR. The first XTOPR annotates the input tree with the rules that the second XTOPR is supposed</context>
<context position="29551" citStr="Maletti (2008)" startWordPosition="5464" endWordPosition="5465">s that no more than n rules are applied to each input symbol. The first XTOPR is actually a nondeterministic linear and nondeleting XTOP that annotates each input tree symbol with exactly n rules of M that are consistent with the state behavior of M. Moreover, the annotation also prescribes with which of n rules the processing should continue at each subtree. Since we know all the rules that will potentially be applied for a certain symbol, we can make the assignment such that no annotated rule is used twice in the same derivation. The details for this construction can be found in Lemma 15 of Maletti (2008). In this way, we obtain a weighted linear and nondeleting XTOP M1, which includes the look-ahead, 270 qNP VP qVB x1 qNP x3 x4 x2 � x2 x1 x3 S qS x4 qNP u1 qVB S u2 t VP u3 u4 qNP P ==&gt;.M u2 u1 u3 S qS t u4 S Figure 8: Example MBOT rule ρ [left] and its use in a rewrite step [right]. and an unweighted deterministic XTOP M2. Only the weight and look-ahead of rules that are actually executed are applied (e.g., although we annotate n rules at the root symbol, we only execute the first rule and thus only apply its weight and lookahead). The look-ahead of different rules is either resolved (i.e., p</context>
</contexts>
<marker>Maletti, 2008</marker>
<rawString>Andreas Maletti. 2008. Compositions of extended topdown tree transducers. Inform. and Comput., 206(9– 10):1187–1196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Input and output products for weighted extended top-down tree transducers.</title>
<date>2010</date>
<booktitle>In Proc. 14th Int. Conf. Developments in Language Theory,</booktitle>
<volume>6224</volume>
<pages>316--327</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2304" citStr="Maletti (2010" startWordPosition="328" endWordPosition="329">59/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as observed by Eisner (2003) and Shieber (2004). This deficiency is overcome by synchronous tree substitution grammars, which are state-less linear and nondeleting XTOP. Recently, Maletti (2010b) proposed MBOT, and Zhang et al. (2008b) and Sun et al. (2009) proposed the even more powerful synchronous tree-sequence substitution grammars. Those two models allow certain translation discontinuities, and the former device also offers computational benefits over linear and nondeleting XTOP as argued by Maletti (2010b). The simplicity of XTOP makes them very appealing as translation models. In 2010 the ATANLP participants [workshop at ACL] identified ‘copying’ as the most exciting and promising feature of XTOP, but unrestricted copying can lead to an undesirable explosion of the size of th</context>
<context position="15940" citStr="Maletti (2010" startWordPosition="2959" endWordPosition="2960"> uniformly bounded. We need an auxiliary result in the main part. Let τ : TE × To → R+ be a weighted tree transformation. We need the weighted tree language τ−1(u): TE → R+ of input trees weighted by their translation weight to a given output tree u ∈ To. Formally, (τ−1(u))(t) = τ(t, u) for every t ∈ TE. Theorem 3. For every producing XTOPR M and output tree u0 ∈ To, the weighted tree language τ−1M (u0) is regular. Proof sketch. We use some properties that are only defined in the next sections (for proof economy). It is recommended to skip this proof on the first reading and revisit it later. Maletti (2010a) shows that we can construct an XTOPR M0 such that � τM(t, u) if u0 = u τM0(t, u) = 0 otherwise for every t ∈ TE and u ∈ To. This operation is called ‘output product’ by Maletti (2010a). The obtained XTOPR M0 is also producing, so we know that M0 can take at most |u0 |rewrite steps to derive u0. Since M0 can only produce the output tree u0, this also limits the total number of rule applications in any successful derivation. Consequently, M0 can only apply a copying rule at most |u0 |times, which shows that M0 is finitely copying (see Definition 8). By Theorem 11 we can implement M0 by an equ</context>
</contexts>
<marker>Maletti, 2010</marker>
<rawString>Andreas Maletti. 2010a. Input and output products for weighted extended top-down tree transducers. In Proc. 14th Int. Conf. Developments in Language Theory, volume 6224 of LNCS, pages 316–327. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Why synchronous tree substitution grammars?</title>
<date>2010</date>
<booktitle>In Proc. Human Language Technologies: Conf. North American Chapter of the ACL,</booktitle>
<pages>876--884</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2304" citStr="Maletti (2010" startWordPosition="328" endWordPosition="329">59/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as observed by Eisner (2003) and Shieber (2004). This deficiency is overcome by synchronous tree substitution grammars, which are state-less linear and nondeleting XTOP. Recently, Maletti (2010b) proposed MBOT, and Zhang et al. (2008b) and Sun et al. (2009) proposed the even more powerful synchronous tree-sequence substitution grammars. Those two models allow certain translation discontinuities, and the former device also offers computational benefits over linear and nondeleting XTOP as argued by Maletti (2010b). The simplicity of XTOP makes them very appealing as translation models. In 2010 the ATANLP participants [workshop at ACL] identified ‘copying’ as the most exciting and promising feature of XTOP, but unrestricted copying can lead to an undesirable explosion of the size of th</context>
<context position="15940" citStr="Maletti (2010" startWordPosition="2959" endWordPosition="2960"> uniformly bounded. We need an auxiliary result in the main part. Let τ : TE × To → R+ be a weighted tree transformation. We need the weighted tree language τ−1(u): TE → R+ of input trees weighted by their translation weight to a given output tree u ∈ To. Formally, (τ−1(u))(t) = τ(t, u) for every t ∈ TE. Theorem 3. For every producing XTOPR M and output tree u0 ∈ To, the weighted tree language τ−1M (u0) is regular. Proof sketch. We use some properties that are only defined in the next sections (for proof economy). It is recommended to skip this proof on the first reading and revisit it later. Maletti (2010a) shows that we can construct an XTOPR M0 such that � τM(t, u) if u0 = u τM0(t, u) = 0 otherwise for every t ∈ TE and u ∈ To. This operation is called ‘output product’ by Maletti (2010a). The obtained XTOPR M0 is also producing, so we know that M0 can take at most |u0 |rewrite steps to derive u0. Since M0 can only produce the output tree u0, this also limits the total number of rule applications in any successful derivation. Consequently, M0 can only apply a copying rule at most |u0 |times, which shows that M0 is finitely copying (see Definition 8). By Theorem 11 we can implement M0 by an equ</context>
</contexts>
<marker>Maletti, 2010</marker>
<rawString>Andreas Maletti. 2010b. Why synchronous tree substitution grammars? In Proc. Human Language Technologies: Conf. North American Chapter of the ACL, pages 876–884. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>An alternative to synchronous tree substitution grammars.</title>
<date>2011</date>
<journal>J. Natur. Lang. Engrg.,</journal>
<volume>17</volume>
<issue>2</issue>
<contexts>
<context position="1856" citStr="Maletti (2011" startWordPosition="266" endWordPosition="267">vely. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) investigated XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as observed by Eisner (2003) and Shieber (2004). This deficiency is overcome by synchronous tree substitution grammars, which are state-less linear and nondeleting XTOP. Recently, Maletti (2010b) proposed MBOT, and Zhang et al. (2008b) and Sun et al. (2009) proposed the even more powerful synchronous tree-sequence substitution grammars. Those </context>
<context position="9275" citStr="Maletti (2011" startWordPosition="1591" endWordPosition="1592">hout it linear (i.e., non-copying) cannot compute rotations (see Figure 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (2009) the most general XTOP model includes copying, deletion, and regular look-ahead in the spirit of Engelfriet (1977). More powerful models (such as synchronous tree-sequence substitution grammars and multi bottom-up tree transducers) can handle translation discontinuities naturally as evidenced by Zhang et al. (2008a) and Maletti (2011b), but q0 S x1 VP x2 x3 Figure 3: Example XTOP rule by Graehl et al. (2008). XTOP need copying and deletion to handle them. Copying essentially allows an XTOP to translate certain parts of the input several times and was identified by the ATANLP 2010 participants as one of the most interesting and promising features of XTOP. Currently, the look-ahead feature is not used in machine translation, but we need it later on in the theoretical development. Given an alphabet Q and a set T, we let Q[T]={q(t) I q E Q,t E TI, in which the root always has exactly one successor from T in contrast to Q(T). </context>
<context position="16640" citStr="Maletti (2011" startWordPosition="3098" endWordPosition="3099"> otherwise for every t ∈ TE and u ∈ To. This operation is called ‘output product’ by Maletti (2010a). The obtained XTOPR M0 is also producing, so we know that M0 can take at most |u0 |rewrite steps to derive u0. Since M0 can only produce the output tree u0, this also limits the total number of rule applications in any successful derivation. Consequently, M0 can only apply a copying rule at most |u0 |times, which shows that M0 is finitely copying (see Definition 8). By Theorem 11 we can implement M0 by an equivalent MBOT M00 (i.e., τM00 = τM0; see Section 5), for which we know by Theorem 14 of Maletti (2011a) that τ−1M00(u) = τ−1M0(u) is regular. Finally, let us illustrate the overall structure of our arguments to show that every sensible XTOPR can be implemented by an equivalent MBOT. We first normalize the given XTOPR such that the semantic property ‘sensible’ yields a syntactic property called ‘finitely copying’ (see Section 4). In a second step, we show that each finitely copying XTOPR can be implemented by an equivalent MBOT (see Section 5). Figure 6 illustrates these steps towards our main result. In the final section, we derive some consequences from our main result (see Section 6). 4 Fro</context>
<context position="28170" citStr="Maletti (2011" startWordPosition="5223" endWordPosition="5224">r the used weight structure. The weighted tree transformation computed by M in state q E Q is q 1`TM(t, u1 ··· uk) wt(p1) · ... · wt(pn) nEN,ρ1,...,ρnER t�M ···�M q(u1,...,uk) for all t E TΣ and u1, ... , uk E TΔ. The semantics of M is TM(t, u) = EqEF TqM(t, u) for all t E TΣ and u E TΔ. We move to the last step for our main result, in which we show how to implement each finitely copying XTOPR by an MBOT using a weighted version of the construction in Lemma 15 of Maletti (2008). The computational benefits (binarization, composition, efficient parsing, etc.) of MBOT over XTOPR are described by Maletti (2011a). Theorem 11. Every finitely copying XTOPR can be implemented by an MBOT. Proof sketch. We plan to utilize Theorem 18 of Engelfriet et al. (2009), which proves the same statement in the unweighted and deterministic case. Again, the weights are not problematic, but we need to remove the nondeterminism before we can apply it. This is achieved by a decomposition into two XTOPR. The first XTOPR annotates the input tree with the rules that the second XTOPR is supposed to use. Thus, the first XTOPR remains nondeterministic, but the second XTOPR, which simply executes the annotated rules, is now de</context>
<context position="30369" citStr="Maletti (2011" startWordPosition="5626" endWordPosition="5627">4 S Figure 8: Example MBOT rule ρ [left] and its use in a rewrite step [right]. and an unweighted deterministic XTOP M2. Only the weight and look-ahead of rules that are actually executed are applied (e.g., although we annotate n rules at the root symbol, we only execute the first rule and thus only apply its weight and lookahead). The look-ahead of different rules is either resolved (i.e., pushed to the next rules) or multiplied using the HADAMARD product [see Fülöp and Vogler (2009)], which preserves regularity. This process is also used by Seemann et al. (2012). Now we can use Theorem 4 of Maletti (2011a) to obtain an MBOT N1 that is equivalent to M1. Similarly, we can use Theorem 18 of Engelfriet et al. (2009) to obtain an MBOT N2 that is equivalent to M2. Since MBOT are closed under composition by Theorem 23 of Engelfriet et al. (2009), we can compose N1 and N2 to obtain a single MBOT N that is equivalent to M. Corollary 12. For every sensible producing XTOPR there exists an equivalent MBOT. Proof. Theorem 6 shows that there exists an equivalent proper XTOPR, which must be finitely copying by Theorem 10. This last fact allows us to construct an equivalent MBOT by Theorem 11. 6 Preservation</context>
<context position="32062" citStr="Maletti (2011" startWordPosition="5923" endWordPosition="5924">of this sentence together with their parse scores. Those parses form a regular weighted tree language, and applying them backwards to the translation model yields another weighted tree language L of corresponding input trees. For an efficient representation and efficient modification algorithms (such a k-best extraction) we would like L to be regular. However, Fülöp et al. (2011) demonstrate that the backward application of a regular weighted tree language to an XTOPR is not necessarily regular. The counterexample uses a variant of the XTOP of Example 1 and is thus not sensible. Theorem 14 of Maletti (2011a) shows that MBOT preserve regularity under backward application. Corollary 13. Sensible XTOPR preserve regularity under backward application. Conclusion We demonstrated that each sensible XTOPR can be implemented by an MBOT. The latter formalism offers many computational advantages, so that the author believes that MBOT should be used instead of XTOP. We used real number weights, but the author believes that our results carry over to at least all zerosum and zero-divisor free semirings [see Hebisch and Weinert (1998) and Golan (1999)], which are semirings such that (i) a + b = 0 implies a = </context>
</contexts>
<marker>Maletti, 2011</marker>
<rawString>Andreas Maletti. 2011a. An alternative to synchronous tree substitution grammars. J. Natur. Lang. Engrg., 17(2):221–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>How to train your multi bottomup tree transducer.</title>
<date>2011</date>
<booktitle>In Proc. 49th Ann. Meeting Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>825--834</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1856" citStr="Maletti (2011" startWordPosition="266" endWordPosition="267">vely. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) investigated XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as observed by Eisner (2003) and Shieber (2004). This deficiency is overcome by synchronous tree substitution grammars, which are state-less linear and nondeleting XTOP. Recently, Maletti (2010b) proposed MBOT, and Zhang et al. (2008b) and Sun et al. (2009) proposed the even more powerful synchronous tree-sequence substitution grammars. Those </context>
<context position="9275" citStr="Maletti (2011" startWordPosition="1591" endWordPosition="1592">hout it linear (i.e., non-copying) cannot compute rotations (see Figure 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (2009) the most general XTOP model includes copying, deletion, and regular look-ahead in the spirit of Engelfriet (1977). More powerful models (such as synchronous tree-sequence substitution grammars and multi bottom-up tree transducers) can handle translation discontinuities naturally as evidenced by Zhang et al. (2008a) and Maletti (2011b), but q0 S x1 VP x2 x3 Figure 3: Example XTOP rule by Graehl et al. (2008). XTOP need copying and deletion to handle them. Copying essentially allows an XTOP to translate certain parts of the input several times and was identified by the ATANLP 2010 participants as one of the most interesting and promising features of XTOP. Currently, the look-ahead feature is not used in machine translation, but we need it later on in the theoretical development. Given an alphabet Q and a set T, we let Q[T]={q(t) I q E Q,t E TI, in which the root always has exactly one successor from T in contrast to Q(T). </context>
<context position="16640" citStr="Maletti (2011" startWordPosition="3098" endWordPosition="3099"> otherwise for every t ∈ TE and u ∈ To. This operation is called ‘output product’ by Maletti (2010a). The obtained XTOPR M0 is also producing, so we know that M0 can take at most |u0 |rewrite steps to derive u0. Since M0 can only produce the output tree u0, this also limits the total number of rule applications in any successful derivation. Consequently, M0 can only apply a copying rule at most |u0 |times, which shows that M0 is finitely copying (see Definition 8). By Theorem 11 we can implement M0 by an equivalent MBOT M00 (i.e., τM00 = τM0; see Section 5), for which we know by Theorem 14 of Maletti (2011a) that τ−1M00(u) = τ−1M0(u) is regular. Finally, let us illustrate the overall structure of our arguments to show that every sensible XTOPR can be implemented by an equivalent MBOT. We first normalize the given XTOPR such that the semantic property ‘sensible’ yields a syntactic property called ‘finitely copying’ (see Section 4). In a second step, we show that each finitely copying XTOPR can be implemented by an equivalent MBOT (see Section 5). Figure 6 illustrates these steps towards our main result. In the final section, we derive some consequences from our main result (see Section 6). 4 Fro</context>
<context position="28170" citStr="Maletti (2011" startWordPosition="5223" endWordPosition="5224">r the used weight structure. The weighted tree transformation computed by M in state q E Q is q 1`TM(t, u1 ··· uk) wt(p1) · ... · wt(pn) nEN,ρ1,...,ρnER t�M ···�M q(u1,...,uk) for all t E TΣ and u1, ... , uk E TΔ. The semantics of M is TM(t, u) = EqEF TqM(t, u) for all t E TΣ and u E TΔ. We move to the last step for our main result, in which we show how to implement each finitely copying XTOPR by an MBOT using a weighted version of the construction in Lemma 15 of Maletti (2008). The computational benefits (binarization, composition, efficient parsing, etc.) of MBOT over XTOPR are described by Maletti (2011a). Theorem 11. Every finitely copying XTOPR can be implemented by an MBOT. Proof sketch. We plan to utilize Theorem 18 of Engelfriet et al. (2009), which proves the same statement in the unweighted and deterministic case. Again, the weights are not problematic, but we need to remove the nondeterminism before we can apply it. This is achieved by a decomposition into two XTOPR. The first XTOPR annotates the input tree with the rules that the second XTOPR is supposed to use. Thus, the first XTOPR remains nondeterministic, but the second XTOPR, which simply executes the annotated rules, is now de</context>
<context position="30369" citStr="Maletti (2011" startWordPosition="5626" endWordPosition="5627">4 S Figure 8: Example MBOT rule ρ [left] and its use in a rewrite step [right]. and an unweighted deterministic XTOP M2. Only the weight and look-ahead of rules that are actually executed are applied (e.g., although we annotate n rules at the root symbol, we only execute the first rule and thus only apply its weight and lookahead). The look-ahead of different rules is either resolved (i.e., pushed to the next rules) or multiplied using the HADAMARD product [see Fülöp and Vogler (2009)], which preserves regularity. This process is also used by Seemann et al. (2012). Now we can use Theorem 4 of Maletti (2011a) to obtain an MBOT N1 that is equivalent to M1. Similarly, we can use Theorem 18 of Engelfriet et al. (2009) to obtain an MBOT N2 that is equivalent to M2. Since MBOT are closed under composition by Theorem 23 of Engelfriet et al. (2009), we can compose N1 and N2 to obtain a single MBOT N that is equivalent to M. Corollary 12. For every sensible producing XTOPR there exists an equivalent MBOT. Proof. Theorem 6 shows that there exists an equivalent proper XTOPR, which must be finitely copying by Theorem 10. This last fact allows us to construct an equivalent MBOT by Theorem 11. 6 Preservation</context>
<context position="32062" citStr="Maletti (2011" startWordPosition="5923" endWordPosition="5924">of this sentence together with their parse scores. Those parses form a regular weighted tree language, and applying them backwards to the translation model yields another weighted tree language L of corresponding input trees. For an efficient representation and efficient modification algorithms (such a k-best extraction) we would like L to be regular. However, Fülöp et al. (2011) demonstrate that the backward application of a regular weighted tree language to an XTOPR is not necessarily regular. The counterexample uses a variant of the XTOP of Example 1 and is thus not sensible. Theorem 14 of Maletti (2011a) shows that MBOT preserve regularity under backward application. Corollary 13. Sensible XTOPR preserve regularity under backward application. Conclusion We demonstrated that each sensible XTOPR can be implemented by an MBOT. The latter formalism offers many computational advantages, so that the author believes that MBOT should be used instead of XTOP. We used real number weights, but the author believes that our results carry over to at least all zerosum and zero-divisor free semirings [see Hebisch and Weinert (1998) and Golan (1999)], which are semirings such that (i) a + b = 0 implies a = </context>
</contexts>
<marker>Maletti, 2011</marker>
<rawString>Andreas Maletti. 2011b. How to train your multi bottomup tree transducer. In Proc. 49th Ann. Meeting Association for Computational Linguistics: Human Language Technologies, pages 825–834. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>Tiburon: A weighted tree automata toolkit.</title>
<date>2006</date>
<booktitle>In Proc. 11th Int. Conf. Implementation and Application of Automata,</booktitle>
<volume>4094</volume>
<pages>102--113</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1415" citStr="May and Knight (2006)" startWordPosition="195" endWordPosition="198">statistical machine translation. Koehn (2010) presents an introduction to statistical machine translation, and Knight (2007) presents an overview of syntax-based statistical machine translation. The oldest and best-studied tree transformation device is the top-down tree transducer of Rounds (1970) and Thatcher (1970). Gécseg and Steinby (1984) and Fülöp and Vogler (2009) present the existing results on the unweighted and weighted model, respectively. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) investigated XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Ch</context>
<context position="11505" citStr="May and Knight (2006)" startWordPosition="2055" endWordPosition="2058">re deleted by a S t1 VP t2 t3 S t2 t1 t3 H S H qNP qNP x3 qVB x2 x1 265 Figure 4: Rewrite step using rule ρ of Figure 3. rule application, so for each rule ρ = ` → r, we let del(ρ) = var(`) \ var(r) be the set of deleted variables in ρ. If we suppose that a variable x ∈ del(ρ) matches to an input subtree t, then the weight of the look-ahead c(ρ,x)(t), which we also write cρ,x(t), is applied to the derivation. If it is 0, then this lookahead essentially prohibits the application of ρ. It is important that the look-ahead is regular (i.e., there exists a WTG accepting it). The toolkit TIBURON by May and Knight (2006) implements XTOP together with a number of essential operations. Lookahead is not implemented in TIBURON, but it can be simulated using a composition of two XTOP, in which the first XTOP performs the look-ahead and marks the results, so that the second XTOP can access the look-ahead information. As for WTG the semantics for the XTOPR M = (Q, E, A, I, R, c, wt) is presented using rewriting. Without loss of generality, we again suppose that Q ∩ (E ∪ A) = ∅. Let ξ, ζ ∈ TA(Q[TE]), w ∈ ][R+, and ρ = ` → r be a rule of R. We write ξ ⇒ρ,wM ζ if there exists a substitution θ: X → TE such that • ξ = ξ[</context>
</contexts>
<marker>May, Knight, 2006</marker>
<rawString>Jonathan May and Kevin Knight. 2006. Tiburon: A weighted tree automata toolkit. In Proc. 11th Int. Conf. Implementation and Application of Automata, volume 4094 of LNCS, pages 102–113. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
<author>Heiko Vogler</author>
</authors>
<title>Efficient inference through cascades of weighted tree transducers.</title>
<date>2010</date>
<booktitle>In Proc. 48th Ann. Meeting Association for Computational Linguistics,</booktitle>
<pages>1058--1066</pages>
<contexts>
<context position="4315" citStr="May et al. (2010)" startWordPosition="643" endWordPosition="646">i (2008) shows that for each linear XTOP there exists an equivalent MBOT. Here, we do not make any restrictions on the XTOP besides some sanity conditions (see Section 3). In particular, we consider copying XTOP. If we accept the restriction to linear size-increase tree transformation, then our main result further motivates MBOT as a suitable translation model for syntax-based machine translation because MBOT can implement each reasonable (even copying) XTOP. In addition, our result allows us to show that each reasonable XTOP preserves regularity under backward application. As demonstrated by May et al. (2010) backward application is the standard application of XTOP in the machine translation pipeline, and preservation of regularity is the essential property for several of the evaluation algorithms of May et al. (2010). 2 Notation We start by introducing our notation for trees, whose nodes are labeled by elements of an alphabet E and a set V . However, only leaves can be labeled by elements of V . For every set T, we let E(T) = {Q(t1, ... , tk) |Q ∈ E, t1, ... , tk ∈ T} , which contains all trees with a E-labeled root and direct successors in T. The set TΣ(V ) of E-trees with V-leaves is the smalle</context>
<context position="31201" citStr="May et al. (2010)" startWordPosition="5776" endWordPosition="5779">Engelfriet et al. (2009), we can compose N1 and N2 to obtain a single MBOT N that is equivalent to M. Corollary 12. For every sensible producing XTOPR there exists an equivalent MBOT. Proof. Theorem 6 shows that there exists an equivalent proper XTOPR, which must be finitely copying by Theorem 10. This last fact allows us to construct an equivalent MBOT by Theorem 11. 6 Preservation of regularity Finally, we present an application of Corollary 12 to solve an open problem. The translation model is often used in a backwards manner in a machine translation system as demonstrated, for example, by May et al. (2010), which means that an output tree is supplied and the corresponding input trees are sought. This starting output tree is typically the best parse of the string that we want to translate. However, instead of a single tree, we want to use all parses of this sentence together with their parse scores. Those parses form a regular weighted tree language, and applying them backwards to the translation model yields another weighted tree language L of corresponding input trees. For an efficient representation and efficient modification algorithms (such a k-best extraction) we would like L to be regular</context>
</contexts>
<marker>May, Knight, Vogler, 2010</marker>
<rawString>Jonathan May, Kevin Knight, and Heiko Vogler. 2010. Efficient inference through cascades of weighted tree transducers. In Proc. 48th Ann. Meeting Association for Computational Linguistics, pages 1058–1066. Association for Computational Linguistics. 272</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
</authors>
<title>Weighted Tree Automata and Transducers for Syntactic Natural Language Processing.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern</institution>
<location>California, Los Angeles.</location>
<contexts>
<context position="1454" citStr="May (2010)" startWordPosition="204" endWordPosition="205">ents an introduction to statistical machine translation, and Knight (2007) presents an overview of syntax-based statistical machine translation. The oldest and best-studied tree transformation device is the top-down tree transducer of Rounds (1970) and Thatcher (1970). Gécseg and Steinby (1984) and Fülöp and Vogler (2009) present the existing results on the unweighted and weighted model, respectively. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) investigated XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-fr</context>
</contexts>
<marker>May, 2010</marker>
<rawString>Jonathan May. 2010. Weighted Tree Automata and Transducers for Syntactic Natural Language Processing. Ph.D. thesis, University of Southern California, Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proc. 46th Ann. Meeting Association for Computational Linguistics,</booktitle>
<pages>192--199</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6816" citStr="Mi et al. (2008)" startWordPosition="1153" endWordPosition="1156">ing all occurrences of x ∈ X in t by 0(x). Our notions for trees are illustrated in Figure 1. Finally, we present weighted tree grammars (WTG) as defined by Fülöp and Vogler (2009), who defined it for arbitrary semirings as weight structures. In contrast, our weights are always nonnegative reals, which form the semiring (R+, +, ·, 0,1) and are used in probabilistic grammars. For each weight assignment f : T → R+, we let supp(f) = {t ∈ T |f(t) =6 0} . WTG offer an efficient representation of weighted forests (i.e., set of weighted trees), which is even more efficient than the packed forests of Mi et al. (2008) because they can be minimized efficiently using an algorithm of Maletti and Quernheim (2011). In particular, WTG can share more than equivalent subtrees and can even represent infinite sets of trees. A WTG is a system G = (Q, E, q0, P, wt) with • a finite set Q of states (nonterminals), • an alphabet E of symbols, • a starting state q0 ∈ Q, • a finite set P of productions q → r, where q ∈ Q and r ∈ TΣ(Q) \ Q, and • a mapping wt: P → R+ that assigns production weights. Without loss of generality, we assume that we can distinguish states and symbols (i.e., Q ∩ E = ∅). For all �, C ∈ TΣ(Q) and a</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proc. 46th Ann. Meeting Association for Computational Linguistics, pages 192–199. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Rounds</author>
</authors>
<title>Mappings and grammars on trees.</title>
<date>1970</date>
<booktitle>Math. Systems Theory,</booktitle>
<pages>4--3</pages>
<contexts>
<context position="1092" citStr="Rounds (1970)" startWordPosition="147" endWordPosition="148">d extended top-down tree transducer can also be computed by a weighted multi bottom-up tree transducer. This further motivates weighted multi bottom-up tree transducers as suitable translation models for syntax-based machine translation. 1 Introduction Several different translation models are used in syntax-based statistical machine translation. Koehn (2010) presents an introduction to statistical machine translation, and Knight (2007) presents an overview of syntax-based statistical machine translation. The oldest and best-studied tree transformation device is the top-down tree transducer of Rounds (1970) and Thatcher (1970). Gécseg and Steinby (1984) and Fülöp and Vogler (2009) present the existing results on the unweighted and weighted model, respectively. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) investigated XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 495</context>
<context position="8337" citStr="Rounds (1970)" startWordPosition="1447" endWordPosition="1448">guage LG: TE H R+ such that LG(t) = E wt(ρ1) · ... · wt(ρn) nEN,p1,...,pnEP P1 Pn 40�G ···�G t for every t E TE. Each such language is regular, and Reg(E) contains all those languages over the alphabet E. A thorough introduction to tree languages is presented by Gécseg and Steinby (1984) and Gécseg and Steinby (1997) for the unweighted case and by FUlöp and Vogler (2009) for the weighted case. 3 Extended top-down tree transducers We start by introducing the main model of this contribution. Extended top-down tree transducers (XTOP) are a generalization of the top-down tree transducers (TOP) of Rounds (1970) and Thatcher (1970). XTOP allow rules with several (non-state and non-variable) symbols in the left-hand side (as in the rule of Figure 3), whereas a TOP rule contains exactly one symbol in the left-hand side. Shieber (2004) and Knight (2007) identified that this extension is essential for many NLP applications because without it linear (i.e., non-copying) cannot compute rotations (see Figure 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (20</context>
</contexts>
<marker>Rounds, 1970</marker>
<rawString>William C. Rounds. 1970. Mappings and grammars on trees. Math. Systems Theory, 4(3):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Seemann</author>
<author>Daniel Quernheim</author>
<author>Fabienne Braune</author>
<author>Andreas Maletti</author>
</authors>
<title>Preservation of recognizability for weighted linear extended top-down tree transducers.</title>
<date>2012</date>
<booktitle>In Proc. 2nd Workshop Applications of Tree Automata in Natural Language Processing,</booktitle>
<pages>pages</pages>
<contexts>
<context position="30326" citStr="Seemann et al. (2012)" startWordPosition="5615" endWordPosition="5618">1 qVB S u2 t VP u3 u4 qNP P ==&gt;.M u2 u1 u3 S qS t u4 S Figure 8: Example MBOT rule ρ [left] and its use in a rewrite step [right]. and an unweighted deterministic XTOP M2. Only the weight and look-ahead of rules that are actually executed are applied (e.g., although we annotate n rules at the root symbol, we only execute the first rule and thus only apply its weight and lookahead). The look-ahead of different rules is either resolved (i.e., pushed to the next rules) or multiplied using the HADAMARD product [see Fülöp and Vogler (2009)], which preserves regularity. This process is also used by Seemann et al. (2012). Now we can use Theorem 4 of Maletti (2011a) to obtain an MBOT N1 that is equivalent to M1. Similarly, we can use Theorem 18 of Engelfriet et al. (2009) to obtain an MBOT N2 that is equivalent to M2. Since MBOT are closed under composition by Theorem 23 of Engelfriet et al. (2009), we can compose N1 and N2 to obtain a single MBOT N that is equivalent to M. Corollary 12. For every sensible producing XTOPR there exists an equivalent MBOT. Proof. Theorem 6 shows that there exists an equivalent proper XTOPR, which must be finitely copying by Theorem 10. This last fact allows us to construct an eq</context>
</contexts>
<marker>Seemann, Quernheim, Braune, Maletti, 2012</marker>
<rawString>Nina Seemann, Daniel Quernheim, Fabienne Braune, and Andreas Maletti. 2012. Preservation of recognizability for weighted linear extended top-down tree transducers. In Proc. 2nd Workshop Applications of Tree Automata in Natural Language Processing, pages 1– 10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Synchronous grammars as tree transducers.</title>
<date>2004</date>
<booktitle>In Proc. 7th Int. Workshop Tree Adjoining Grammars and Related Formalisms,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="2158" citStr="Shieber (2004)" startWordPosition="309" endWordPosition="310">1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (MBOT) [as k-morphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as observed by Eisner (2003) and Shieber (2004). This deficiency is overcome by synchronous tree substitution grammars, which are state-less linear and nondeleting XTOP. Recently, Maletti (2010b) proposed MBOT, and Zhang et al. (2008b) and Sun et al. (2009) proposed the even more powerful synchronous tree-sequence substitution grammars. Those two models allow certain translation discontinuities, and the former device also offers computational benefits over linear and nondeleting XTOP as argued by Maletti (2010b). The simplicity of XTOP makes them very appealing as translation models. In 2010 the ATANLP participants [workshop at ACL] identi</context>
<context position="8562" citStr="Shieber (2004)" startWordPosition="1484" endWordPosition="1485">tion to tree languages is presented by Gécseg and Steinby (1984) and Gécseg and Steinby (1997) for the unweighted case and by FUlöp and Vogler (2009) for the weighted case. 3 Extended top-down tree transducers We start by introducing the main model of this contribution. Extended top-down tree transducers (XTOP) are a generalization of the top-down tree transducers (TOP) of Rounds (1970) and Thatcher (1970). XTOP allow rules with several (non-state and non-variable) symbols in the left-hand side (as in the rule of Figure 3), whereas a TOP rule contains exactly one symbol in the left-hand side. Shieber (2004) and Knight (2007) identified that this extension is essential for many NLP applications because without it linear (i.e., non-copying) cannot compute rotations (see Figure 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (2009) the most general XTOP model includes copying, deletion, and regular look-ahead in the spirit of Engelfriet (1977). More powerful models (such as synchronous tree-sequence substitution grammars and multi bottom-up tree tra</context>
</contexts>
<marker>Shieber, 2004</marker>
<rawString>Stuart M. Shieber. 2004. Synchronous grammars as tree transducers. In Proc. 7th Int. Workshop Tree Adjoining Grammars and Related Formalisms, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Sun</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
</authors>
<title>A non-contiguous tree sequence alignment-based model for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. 47th Ann. Meeting Association for Computational Linguistics,</booktitle>
<pages>914--922</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2368" citStr="Sun et al. (2009)" startWordPosition="338" endWordPosition="341">ted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as observed by Eisner (2003) and Shieber (2004). This deficiency is overcome by synchronous tree substitution grammars, which are state-less linear and nondeleting XTOP. Recently, Maletti (2010b) proposed MBOT, and Zhang et al. (2008b) and Sun et al. (2009) proposed the even more powerful synchronous tree-sequence substitution grammars. Those two models allow certain translation discontinuities, and the former device also offers computational benefits over linear and nondeleting XTOP as argued by Maletti (2010b). The simplicity of XTOP makes them very appealing as translation models. In 2010 the ATANLP participants [workshop at ACL] identified ‘copying’ as the most exciting and promising feature of XTOP, but unrestricted copying can lead to an undesirable explosion of the size of the translation. According to Engelfriet and Maneth (2003) a tree </context>
</contexts>
<marker>Sun, Zhang, Tan, 2009</marker>
<rawString>Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A non-contiguous tree sequence alignment-based model for statistical machine translation. In Proc. 47th Ann. Meeting Association for Computational Linguistics, pages 914–922. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Thatcher</author>
</authors>
<title>Generalized sequential machine maps.</title>
<date>1970</date>
<journal>J. Comput. System Sci.,</journal>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="1112" citStr="Thatcher (1970)" startWordPosition="150" endWordPosition="151">n tree transducer can also be computed by a weighted multi bottom-up tree transducer. This further motivates weighted multi bottom-up tree transducers as suitable translation models for syntax-based machine translation. 1 Introduction Several different translation models are used in syntax-based statistical machine translation. Koehn (2010) presents an introduction to statistical machine translation, and Knight (2007) presents an overview of syntax-based statistical machine translation. The oldest and best-studied tree transformation device is the top-down tree transducer of Rounds (1970) and Thatcher (1970). Gécseg and Steinby (1984) and Fülöp and Vogler (2009) present the existing results on the unweighted and weighted model, respectively. Knight (2007) promotes the use of weighted extended top-down tree transducers (XTOP), which have also been implemented in the toolkit TIBURON by May and Knight (2006) [more detail is reported by May (2010)]. In the context of bimorphisms, Arnold and Dauchet (1976) investigated XTOP, and Lilin (1978) and Arnold and Dauchet (1982) investigated multi bottom-up tree *The author was supported by the German Research Foundation (DFG) grant MA 4959/1-1. transducers (</context>
<context position="8357" citStr="Thatcher (1970)" startWordPosition="1450" endWordPosition="1451">such that LG(t) = E wt(ρ1) · ... · wt(ρn) nEN,p1,...,pnEP P1 Pn 40�G ···�G t for every t E TE. Each such language is regular, and Reg(E) contains all those languages over the alphabet E. A thorough introduction to tree languages is presented by Gécseg and Steinby (1984) and Gécseg and Steinby (1997) for the unweighted case and by FUlöp and Vogler (2009) for the weighted case. 3 Extended top-down tree transducers We start by introducing the main model of this contribution. Extended top-down tree transducers (XTOP) are a generalization of the top-down tree transducers (TOP) of Rounds (1970) and Thatcher (1970). XTOP allow rules with several (non-state and non-variable) symbols in the left-hand side (as in the rule of Figure 3), whereas a TOP rule contains exactly one symbol in the left-hand side. Shieber (2004) and Knight (2007) identified that this extension is essential for many NLP applications because without it linear (i.e., non-copying) cannot compute rotations (see Figure 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (2009) the most general</context>
</contexts>
<marker>Thatcher, 1970</marker>
<rawString>James W. Thatcher. 1970. Generalized sequential machine maps. J. Comput. System Sci., 4(4):339–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proc. 46th Ann. Meeting Association for Computational Linguistics,</booktitle>
<pages>559--567</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2344" citStr="Zhang et al. (2008" startWordPosition="333" endWordPosition="336">rphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as observed by Eisner (2003) and Shieber (2004). This deficiency is overcome by synchronous tree substitution grammars, which are state-less linear and nondeleting XTOP. Recently, Maletti (2010b) proposed MBOT, and Zhang et al. (2008b) and Sun et al. (2009) proposed the even more powerful synchronous tree-sequence substitution grammars. Those two models allow certain translation discontinuities, and the former device also offers computational benefits over linear and nondeleting XTOP as argued by Maletti (2010b). The simplicity of XTOP makes them very appealing as translation models. In 2010 the ATANLP participants [workshop at ACL] identified ‘copying’ as the most exciting and promising feature of XTOP, but unrestricted copying can lead to an undesirable explosion of the size of the translation. According to Engelfriet a</context>
<context position="9255" citStr="Zhang et al. (2008" startWordPosition="1586" endWordPosition="1589"> applications because without it linear (i.e., non-copying) cannot compute rotations (see Figure 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (2009) the most general XTOP model includes copying, deletion, and regular look-ahead in the spirit of Engelfriet (1977). More powerful models (such as synchronous tree-sequence substitution grammars and multi bottom-up tree transducers) can handle translation discontinuities naturally as evidenced by Zhang et al. (2008a) and Maletti (2011b), but q0 S x1 VP x2 x3 Figure 3: Example XTOP rule by Graehl et al. (2008). XTOP need copying and deletion to handle them. Copying essentially allows an XTOP to translate certain parts of the input several times and was identified by the ATANLP 2010 participants as one of the most interesting and promising features of XTOP. Currently, the look-ahead feature is not used in machine translation, but we need it later on in the theoretical development. Given an alphabet Q and a set T, we let Q[T]={q(t) I q E Q,t E TI, in which the root always has exactly one successor from T i</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008a. A tree sequence alignment-based tree-to-tree translation model. In Proc. 46th Ann. Meeting Association for Computational Linguistics, pages 559–567. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Haizhou Li</author>
<author>Aiti Aw</author>
<author>Sheng Li</author>
</authors>
<title>Grammar comparison study for translational equivalence modeling and statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. 22nd Int. Conf. Computational Linguistics,</booktitle>
<pages>1097--1104</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2344" citStr="Zhang et al. (2008" startWordPosition="333" endWordPosition="336">rphisms]. Recently, weighted XTOP and MBOT, which are the central devices in this contribution, were investigated by Maletti (2011a) in the context of statistical machine translation. Several tree transformation devices are used as translation models in statistical machine translation. Chiang (2007) uses synchronous context-free grammars, which force translations to be very similar as observed by Eisner (2003) and Shieber (2004). This deficiency is overcome by synchronous tree substitution grammars, which are state-less linear and nondeleting XTOP. Recently, Maletti (2010b) proposed MBOT, and Zhang et al. (2008b) and Sun et al. (2009) proposed the even more powerful synchronous tree-sequence substitution grammars. Those two models allow certain translation discontinuities, and the former device also offers computational benefits over linear and nondeleting XTOP as argued by Maletti (2010b). The simplicity of XTOP makes them very appealing as translation models. In 2010 the ATANLP participants [workshop at ACL] identified ‘copying’ as the most exciting and promising feature of XTOP, but unrestricted copying can lead to an undesirable explosion of the size of the translation. According to Engelfriet a</context>
<context position="9255" citStr="Zhang et al. (2008" startWordPosition="1586" endWordPosition="1589"> applications because without it linear (i.e., non-copying) cannot compute rotations (see Figure 2). In the form of bimorphisms XTOP were investigated by Arnold and Dauchet (1976) and Arnold and Dauchet (1982) in the 1970s, and Knight (2007) invigorated research. As demonstrated by Graehl et al. (2009) the most general XTOP model includes copying, deletion, and regular look-ahead in the spirit of Engelfriet (1977). More powerful models (such as synchronous tree-sequence substitution grammars and multi bottom-up tree transducers) can handle translation discontinuities naturally as evidenced by Zhang et al. (2008a) and Maletti (2011b), but q0 S x1 VP x2 x3 Figure 3: Example XTOP rule by Graehl et al. (2008). XTOP need copying and deletion to handle them. Copying essentially allows an XTOP to translate certain parts of the input several times and was identified by the ATANLP 2010 participants as one of the most interesting and promising features of XTOP. Currently, the look-ahead feature is not used in machine translation, but we need it later on in the theoretical development. Given an alphabet Q and a set T, we let Q[T]={q(t) I q E Q,t E TI, in which the root always has exactly one successor from T i</context>
</contexts>
<marker>Zhang, Jiang, Li, Aw, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and Sheng Li. 2008b. Grammar comparison study for translational equivalence modeling and statistical machine translation. In Proc. 22nd Int. Conf. Computational Linguistics, pages 1097–1104. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>