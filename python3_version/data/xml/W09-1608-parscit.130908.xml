<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993587">
An Approach to Text Summarization
</title>
<author confidence="0.957633">
Sankar K Sobha L
</author>
<affiliation confidence="0.896673">
AU-KBC Research Centre AU-KBC Research Centre
MIT Campus, Anna University MIT Campus, Anna University
</affiliation>
<address confidence="0.88595">
Chennai- 44. Chennai- 44.
</address>
<email confidence="0.874651">
sankar@au-kbc.org sobha@au-kbc.org
</email>
<sectionHeader confidence="0.997529" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999001875">
We propose an efficient text summarization
technique that involves two basic opera-
tions. The first operation involves finding
coherent chunks in the document and the
second operation involves ranking the text
in the individual coherent chunks and pick-
ing the sentences that rank above a given
threshold. The coherent chunks are formed
by exploiting the lexical relationship be-
tween adjacent sentences in the document.
Occurrence of words through repetition or
relatedness by sense relation plays a major
role in forming a cohesive tie. The pro-
posed text ranking approach is based on a
graph theoretic ranking model applied to
text summarization task.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999907818181818">
Automated summarization is an important area in
NLP research. A variety of automated summariza-
tion schemes have been proposed recently. NeATS
(Lin and Hovy, 2002) is a sentence position, term
frequency, topic signature and term clustering
based approach and MEAD (Radev et al., 2004) is
a centroid based approach. Iterative graph based
Ranking algorithms, such as Kleinberg’s HITS
algorithm (Kleinberg, 1999) and Google’s Page-
Rank (Brin and Page, 1998) have been traditionally
and successfully used in web-link analysis, social
networks and more recently in text processing ap-
plications (Mihalcea and Tarau, 2004), (Mihalcea
et al., 2004), (Erkan and Radev, 2004) and (Mihal-
cea, 2004). These iterative approaches have a high
time complexity and are practically slow in dy-
namic summarization. Proposals are also made for
coherence based automated summarization system
(Silber and McCoy, 2000).
We propose a novel text summarization tech-
nique that involves two basic operations, namely
finding coherent chunks in the document and rank-
ing the text in the individual coherent chunks
formed.
For finding coherent chunks in the document, we
propose a set of rules that identifies the connection
between adjacent sentences in the document. The
connected sentences that are picked based on the
rules form coherent chunks in the document. For
text ranking, we propose an automatic and unsu-
pervised graph based ranking algorithm that gives
improved results when compared to other ranking
algorithms. The formation of coherent chunks
greatly improves the amount of information of the
text picked for subsequent ranking and hence the
quality of text summarization.
The proposed text ranking technique employs a
hybrid approach involving two phases; the first
phase employs word frequency statistics and the
second phase involves a word position and string
pattern based weighing algorithm to find the
weight of the sentence. A fast running time is
achieved by using a compression hash on each sen-
tence.
</bodyText>
<page confidence="0.996134">
53
</page>
<note confidence="0.9565585">
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 53–60,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9929044">
This paper is organized as follows: section 2
discusses lexical cohesion, section 3 discusses the
text ranking algorithm and section 4 describes the
summarization by combining lexical cohesion and
summarization.
</bodyText>
<sectionHeader confidence="0.977202" genericHeader="introduction">
2 Lexical Cohesion
</sectionHeader>
<bodyText confidence="0.999903916666667">
Coherence in linguistics makes the text semantical-
ly meaningful. It is achieved through semantic fea-
tures such as the use of deictic (a deictic is an
expression which shows the direction. ex: that,
this.), anaphoric (a referent which requires an ante-
cedent in front. ex: he, she, it.), cataphoric (a refe-
rent which requires an antecedent at the back.),
lexical relation and proper noun repeating elements
(Morris and Hirst, 1991). Robert De Beaugrande
and Wolfgang U. Dressler define coherence as a
“continuity of senses” and “the mutual access and
relevance within a configuration of concepts and
relations” (Beaugrande and Dressler, 1981). Thus a
text gives meaning as a result of union of meaning
or senses in the text.
The coherence cues present in a sentence are di-
rectly visible when we go through the flow of the
document. Our approach aims to achieve this ob-
jective with linguistic and heuristic information.
The identification of semantic neighborhood, oc-
currence of words through repetition or relatedness
by sense relation namely synonyms, hyponyms and
hypernym, plays a major role in forming a cohesive
tie (Miller et al., 1990).
</bodyText>
<subsectionHeader confidence="0.873876">
2.1 Rules for finding Coherent chunks
</subsectionHeader>
<bodyText confidence="0.9999212">
When parsing through a document, the relationship
among adjacent sentences is determined by the
continuity that exists between them.
We define the following set of rules to find co-
herent chunks in the document.
</bodyText>
<subsectionHeader confidence="0.582657">
Rule 1
</subsectionHeader>
<bodyText confidence="0.962092307692308">
The presence of connectives (such as accordingly,
again, also, besides) in present sentence indicates
the connectedness of the present sentence with the
previous sentence. When such connectives are
found, the adjacent sentences form coherent
chunks.
Rule 2
A 3rd person pronominal in a given sentence refers
to the antecedent in the previous sentence, in such
a way that the given sentence gives the complete
meaning with respect to the previous sentence.
When such adjacent sentences are found, they form
coherent chunks.
</bodyText>
<subsectionHeader confidence="0.53133">
Rule 3
</subsectionHeader>
<bodyText confidence="0.961689">
The reappearance of VERs in adjacent sentences
is an indication of connectedness. When such adja-
cent sentences are found, they form coherent
chunks.
Rule 4
An ontology relationship between words across
sentences can be used to find semantically related
words across adjacent sentences that appear in the
document. The appearance of related words is an
indication of its coherence and hence forms cohe-
rent chunks.
All the above rules are applied incrementally to
achieve the complete set of coherent chunks.
</bodyText>
<subsectionHeader confidence="0.976371">
2.1.1 Connecting Word
</subsectionHeader>
<bodyText confidence="0.996549863636364">
The ACE Corpus was used for studying the cohe-
rence patterns between adjacent sentences of the
document. From our analysis, we picked out a set
of keywords such that, the appearance of these
keywords at the beginning of the sentence provide
a strong lexical tie with the previous sentence.
The appearance of the keywords “accordingly,
again, also, besides, hence, henceforth, however,
incidentally, meanwhile, moreover, namely, never-
theless, otherwise, that is, then, therefore, thus,
and, but, or, yet, so, once, so that, than, that, till,
whenever, whereas and wherever”, at the begin-
ning of the present sentence was found to be highly
coherent with the previous sentence.
Linguistically a sentence cannot start with the
above words without any related introduction in
the previous sentence.
Furthermore, the appearance of the keywords
“consequently, finally, furthermore”, at the begin-
ning or middle of the present sentence was found
to be highly cohesive with the previous sentence.
Example 1
</bodyText>
<page confidence="0.982194">
54
</page>
<listItem confidence="0.815568">
1. a The train was late.
1. b However I managed to reach the wedding
on time.
</listItem>
<bodyText confidence="0.998001166666667">
In Example 4, the pronominal he in the second sen-
tence refers to the antecedent Ravi in the first sen-
tence.
In Example 1, the connecting word however binds
with the situation of the train being late.
Example 2
</bodyText>
<listItem confidence="0.979100666666667">
1. a The cab driver was late.
1. b The bike tyre was punctured.
1. c The train was late.
1 .d Finally, I managed to arrive at the wed-
ding on time by calling a cab.
Example 3
1. a The cab driver was late.
1. b The bike tyre was punctured.
1. c The train was late.
1. d I could not wait any more; I finally ma-
naged to reach the wedding on time by calling a
cab.
</listItem>
<bodyText confidence="0.9993692">
In Example 2, the connecting word finally binds
with the situation of him being delayed. Similarly,
in Example 3, the connecting word finally, though
it comes in the middle of the sentence, it still binds
with the situation of him being delayed.
</bodyText>
<subsectionHeader confidence="0.644693">
2.1.2 Pronominals
</subsectionHeader>
<bodyText confidence="0.983884842105263">
In this approach we have a set of pronominals
which establishes coherence in the text. From our
analysis, it was observed that if the pronominals
“he, she, it, they, her, his, hers, its, their, theirs”,
appear in the present sentence; its antecedent may
be in the same or previous sentence.
It is also found that if the pronominal is not pos-
sessive (i.e. the antecedent appears in the previous
sentence or previous clause), then the present sen-
tence and the previous sentences are connected.
However, if the pronominal is possessive then it
behaves like reflexives such as “himself”, “herself”
which has subject as its antecedent. Hence the pos-
sibility of connecting it with the previous sentence
is very unlikely. Though pronominal resolution
cannot be done at a window size of 2 alone, still
we are looking at window size 2 alone to pick
guaranteed connected sentences.
Example 4
</bodyText>
<listItem confidence="0.93997">
1. a Ravi is a good boy.
1. b He always speaks the truth.
Example 5
1. a He is the one who got the first prize.
</listItem>
<bodyText confidence="0.982408666666667">
In example 5 the pronominal he is possessive and
it doesn’t need an antecedent to convey the mean-
ing.
</bodyText>
<subsectionHeader confidence="0.597431">
2.1.3 NERs Reappearance
</subsectionHeader>
<bodyText confidence="0.960624333333333">
Two adjacent sentences are said to be coherent
when both the sentences contain one or more reap-
pearing nouns.
</bodyText>
<figure confidence="0.789159">
Example 6
1. a Ravi is a good boy.
1. b Ravi scored good marks in exams.
Example 7
1. a The car race starts at noon.
1. b Any car is allowed to participate.
</figure>
<bodyText confidence="0.735751666666667">
Example 6 and Example 7 demonstrates the cohe-
rence between the two sentences through reappear-
ing nouns.
</bodyText>
<subsectionHeader confidence="0.474237">
2.1.4 Thesaurus Relationships
</subsectionHeader>
<bodyText confidence="0.93169525">
WordNet covers most of the sense relationships.
To find the semantic neighborhood between adja-
cent sentences, most of the lexical relationships
such as synonyms, hyponyms, hypernyms, mero-
nyms, holonyms and gradation can be used (Fell-
baum 1998). Hence, semantically related terms are
captured through this process.
Example 8
</bodyText>
<listItem confidence="0.9948835">
1. a The bicycle has two wheels.
1. b The wheels provide speed and stability.
</listItem>
<bodyText confidence="0.914439">
In Example 8, bicycle and wheels are related
through bicycle is the holonym of wheels.
</bodyText>
<subsectionHeader confidence="0.999914">
2.2 Coherence Finding Algorithm
</subsectionHeader>
<bodyText confidence="0.99312425">
The algorithm is carried out in four phases. Initial-
ly, each of the 4 cohesion rules is individually ap-
plied over the given document to give coherent
chunks. Next, the coherent chunks obtained in each
</bodyText>
<page confidence="0.992991">
55
</page>
<bodyText confidence="0.58210925">
averaged to obtain the final weight metric. Sen-
tences are sorted in non ascending order of weight.
phases are merged together to give the global cohe-
rent chunks in the document.
</bodyText>
<figure confidence="0.987263142857143">
Input Text
Connecting Word
Possessive Pronoun
Noun Reappearance
Thesaurus Relationships
Coherent Chunks
.
</figure>
<subsectionHeader confidence="0.788428">
3.1 Graph
</subsectionHeader>
<bodyText confidence="0.821228333333333">
Let G (V, E) be a weighted undirected complete
graph, where V is set of vertices and E is set of
weighted edges.
</bodyText>
<figure confidence="0.945827">
S1
S6
S3
S4
S2
S5
</figure>
<figureCaption confidence="0.9939605">
Figure 2: A complete undirected graph
Figure 1: Flow of Coherence chunker
Figure 1, shows the flow and rule positions in the
coherence chunk identification module.
</figureCaption>
<subsectionHeader confidence="0.97588">
2.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999953714285714">
One way to evaluate the coherence finding algo-
rithm is to compare against human judgments
made by readers, evaluating against text pre
marked by authors and to see the improved result
in the computational task. In this paper we will see
the computational method to see the improved re-
sult.
</bodyText>
<sectionHeader confidence="0.991416" genericHeader="method">
3 Text Ranking
</sectionHeader>
<bodyText confidence="0.999908307692308">
The proposed graph based text ranking algorithm
consists of three steps: (1) Word Frequency Analy-
sis; (2) A word positional and string pattern based
weight calculation algorithm; (3) Ranking the sen-
tences by normalizing the results of step (1) and
(2).
The algorithm is carried out in two phases. The
weight metric obtained at the end of each phase is
In figure 2, the vertices in graph G represent the set
of all sentences in the given document. Each sen-
tence in G is related to every other sentence
through the set of weighted edges in the complete
graph.
</bodyText>
<subsectionHeader confidence="0.995393">
3.2 Phase 1
</subsectionHeader>
<bodyText confidence="0.953819333333333">
Let the set of all sentences in document S= {si  |1 ≤
i ≤ n}, where n is the number of sentences in S.
The sentence weight (SW) for each sentence is cal-
culated by average affinity weight of words in it.
For a sentence si= {wj  |1 ≤ j ≤ mi} where mi is the
number of words in sentence si, (1 ≤ i ≤ n) the af-
finity weight AW of a word wj is calculated as fol-
lows:
IsEqual (wj, wk)
</bodyText>
<sectionHeader confidence="0.41409" genericHeader="method">
AW
WC ( S)
</sectionHeader>
<bodyText confidence="0.99989">
where S is the set of all sentences in the given
document, wk is a word in S, WC (S) is the total
number of words in S and function IsEqual(x, y)
returns an integer count 1 if x and y are equal else
integer count 0 is returned by the function.
</bodyText>
<equation confidence="0.65513475">
∑
∀wk∈
S
(1)
56
(w)=(c1ak-1+c2ak-2 +c3ak-3+...+cka0)mod p (3)
w=
c2, c3 ...
</equation>
<bodyText confidence="0.809722416666667">
is the ordered set of
ASCII equivalents of alphabets in w and k the total
number of alphabets in w. The choice of a=2 per-
mits the exponentiations and term wise multiplica-
tions in equation 3 to be binary shift operations on
a micro processor, thereby speeding up the hash
computation over the text. Any lexicographically
ordered bijective map from character to integer
may be used to generate set w. The recommenda-
tion to use ASCII equivalents is solely for imple-
mentation convenience. Set
where
</bodyText>
<equation confidence="0.768776">
{c1,
ck}
p = 26 (for English), to
</equation>
<bodyText confidence="0.958265">
cover the sample space of the set of alphabets un-
der consideration.
Compute H (w) for each word in sentence si to
obtain the hashed set
</bodyText>
<equation confidence="0.977413125">
H s _ H w H w H wm
( i ) { ( 1 ), ( 2)... ( i) } (4)
Next, invert
each element in the set H (si) back
to its ASCII equivalent to obtain the set
H s _ H c H c H c m
ˆ ( ˆ i ) { ( ˆ 1), ( ˆ 2)... ( ˆ i) } (5)
ˆ
</equation>
<bodyText confidence="0.988387785714286">
Then, concatenate the elements in set H(si) to
string sˆi;
sˆi
si.
s.
where
is the compressed
representation of sentence
The hash operations
are carried out to reduce the computational com-
plexity in phase 2, by compressing the sentences
and at the same time retaining their structural
properties, specifically word frequency, word posi-
tion and sentence pattern
</bodyText>
<subsectionHeader confidence="0.414894">
3.4 Levenshtein Distance
3.3 Compression hash
</subsectionHeader>
<bodyText confidence="0.8460255">
A fast compression hash function over word w is
given as follows:
H
obtain the
Levenshtein distance (LD) between two strings
string1 and string2 is a metric that is used to find
the number of operations required to convert
string1 to string2 or vice versa; where the set of
possible operations on the character is insert ion,
deletion, or substitution.
The LD algorithm is illustrated by the following
example
</bodyText>
<equation confidence="0.866885">
LD (ROLL, ROLE) is 1
LD (SATURDAY, SUNDAY) is 3
</equation>
<bodyText confidence="0.85078952173913">
Next, we find the sentence weight SW (si) of
each sentence si (1 ≤ i ≤ n) as follows:
SW (si)_ 1∑ AW w (2)
( j)
mi ∀wj∈si
At the end of phase 1, the graph vert
ices hold
the sentence weight as illustrated in figure 4.
[1]&amp;quot;The whole show is dreadful,&amp;quot; she cried, com-
ing out of the menagerie of M. Martin.
[2]She had just been looking at that daring specu-
lator &amp;quot;working with his hyena&amp;quot; to speak in the
style of the
[3]&amp;quot;By what means,&amp;quot; she continued, &amp;quot;can he have
tamed these animals to such a point as to be cer-
tain of their affection for.&amp;quot;
[4]&amp;quot;What seems to you a problem,&amp;quot; said I, inter-
rupting, &amp;quot;is really quite natural.&amp;quot;
[5]&amp;quot;Oh!&amp;quot; she cried, letting an incredulous smile
wander over her lips.
[6]&amp;quot;You think that beasts are wholly without pas-
sions?&amp;quot; Quite the reverse; we can
|XML |xmlLoc_3 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_no bi_xmlSFBIA_new bi_xmlPara_continue
</bodyText>
<figure confidence="0.704647">
program.
|XML |xmlLoc_2 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_no bi_xmlSFBIA_new bi_xmlPara_continue
communicate to
them all the vices arising in our own state of civi-
lization.
</figure>
<figureCaption confidence="0.983272">
Figure 2: Sample text taken for the ranking
process.
Figure 4: Sample graph of Sentence weight calcu-
lation in phase 1
</figureCaption>
<page confidence="0.994295">
57
</page>
<subsectionHeader confidence="0.976134">
3.5 Levenshtein Similarity Weight
</subsectionHeader>
<bodyText confidence="0.995039625">
Consider two strings, string1 and string2 where ls1
is the length of string1 and ls2 be the length of
string2. Compute MaxLen=maximum (ls1, ls2).
Then LSW between string1 and string2 is the dif-
ference between MaxLen and LD, divided by Max-
Len. Clearly, LSW lies in the interval 0 to 1. In case
of a perfect match between two words, its LSW is 1
and in case of a total mismatch, its LSW is 0. In all
other cases, 0 &lt; LSW &lt;1. The LSW metric is illu-
strated by the following example.
LSW (ABC, ABC) =1
LSW (ABC, XYZ) =0
LSW (ABCD, EFD) =0.25
Hence, to find the Levenshtein similarity
weight, first find the Levenshtein distance LD us-
ing which LSW is calculated by the equation
</bodyText>
<equation confidence="0.958691">
MaxLen (si, sj) − LD(si, sj)
MaxLen(si, sj) (6)
</equation>
<bodyText confidence="0.999816">
where, sˆi and sˆj are the concatenated string out-
puts of equation 5.
</bodyText>
<subsectionHeader confidence="0.967379">
3.6 Phase 2
</subsectionHeader>
<bodyText confidence="0.928241263157895">
Let S = {si  |1 ≤ i ≤ n} be the set of all sentences in
the given document; where n is the number of sen-
tences in S. Further, si = {wj  |1 ≤ j ≤ m}, where m
is the number of words in sentence si.
Figure 5: Sample graph for Sentence weight calcu-
lation in phase 2
dsi E S ,find ˆ ( ˆ i ) { ( ˆ 1 ), ( ˆ 2 )... ( ˆ i) }
H s = H c H c H c m
using equation 3 and 4. Then, concatenate the ele-
ˆ
ments in set H(si) to obtain the string sˆi ; where sˆi
is the compressed representation of sentence si.
Each string sˆi ; 1 ≤ i ≤ n is represented as the
vertex of the complete graph as in figure 5
ˆ
and S={sˆi|1 &lt;— i &lt;— n} . For the graph in figure 5,
find the Levenshtein similarity weight LSW be-
tween every vertex using equation 6. Find vertex
weight (VW) for each string sˆi ; 1 ≤ l ≤ n by
</bodyText>
<equation confidence="0.974986">
VW (sl) = 1 ∑ LSW (sl, si)
n dsi # sl E S (7)
</equation>
<sectionHeader confidence="0.957252" genericHeader="method">
4 Text Ranking
</sectionHeader>
<bodyText confidence="0.501192">
The rank of sentence si; 1 ≤ i ≤ n is computed as
</bodyText>
<equation confidence="0.929079625">
SW s VW s
( ) ( ˆ )
i + i
Rank s
( )
i = ;1 &lt;— &lt;— (8)
i n
2
</equation>
<bodyText confidence="0.999931214285714">
where, SW (si) is calculated by equation 2 of
phase 1 and VW (sˆi) is found using equation 7 of
phase 2. Arrange the sentences si; 1 ≤ i ≤ n, in non
increasing order of their ranks.
SW (si) in phase 1 holds the sentence affinity in
terms of word frequency and is used to determine
the significance of the sentence in the overall rak-
ing scheme. VW (sˆi) in phase 2 helps in the overall
ranking by determining largest common subse-
quences and other smaller subsequences then as-
signing weights to it using LSW. Further, since
named entities are represented as strings, repeated
occurrences are weighed efficiently by LSW, the-
reby giving it a relevant ranking position.
</bodyText>
<sectionHeader confidence="0.998074" genericHeader="method">
5 Summarization
</sectionHeader>
<bodyText confidence="0.9993506">
Summarization is done by applying text ranking
over the global coherent chunks in the document.
The sentences whose weight is above the threshold
is picked and rearranged in the order in which the
sentences appeared in the original document.
</bodyText>
<equation confidence="0.727534">
LSW (si, sj) =
</equation>
<page confidence="0.995441">
58
</page>
<sectionHeader confidence="0.999511" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.93330096">
The ROUGE evaluation toolkit is employed to
evaluate the proposed algorithm. ROUGE, an au-
tomated summarization evaluation package based
on Ngram statistics, is found to be highly corre-
lated with human evaluations (Lin and Hovy,
2003a).
The evaluations are reported in ROUGE-1 me-
trics, which seeks unigram matches between the
generated and the reference summaries. The
ROUGE-1 metric is found to have high correlation
with human judgments at a 95% confidence level
and hence used for evaluation. (Mihalcea and Ta-
rau, 2004) a graph based ranking model with
Rouge score 0.4904, (Mihalcea, 2004) Graph-
based Ranking Algorithms for Sentence Extrac-
tion, Applied to Text Summarization with Rouge
score 0.5023.
Table 1 shows the ROUGE Score of 567 news
articles provided during the Document Under-
standing Evaluations 2002(DUC, 2002) using the
proposed algorithm without the inclusion of cohe-
rence chunker module.
Score
ROUGE-1 0.5103
ROUGE-L 0.4863
</bodyText>
<tableCaption confidence="0.897184666666667">
Table 1: ROUGE Score for the news article
summarization task without coherence
chunker, calculated across 567 articles.
</tableCaption>
<bodyText confidence="0.942413">
Comparatively Table 2, which is the the
ROUGE score for summary including the cohe-
rence chunker module gives better result.
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.9999045">
Text extraction is considered to be the important
and foremost process in summarization. Intuitive-
ly, a hash based approach to graph based ranking
algorithm for text ranking works well on the task
of extractive summarization. A notable study re-
port on usefulness and limitations of automatic
sentence extraction is reported in (Lin and Hovy,
2003b), which emphasizes the need for efficient
algorithms for sentence ranking and summariza-
tion.
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999966642857143">
In this paper, we propose a coherence chunker
module and a hash based approach to graph based
ranking algorithm for text ranking. In specific, we
propose a novel approach for graph based text
ranking, with improved results comparative to ex-
isting ranking algorithms. The architecture of the
algorithm helps the ranking process to be done in a
time efficient way. This approach succeeds in
grabbing the coherent sentences based on the lin-
guistic and heuristic rules; whereas other super-
vised ranking systems do this process by training
the summary collection. This makes the proposed
algorithm highly portable to other domains and
languages.
</bodyText>
<sectionHeader confidence="0.989247" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.994461">
Table 2 shows the ROUGE Score of 567 news
articles provided during the Document Under-
standing Evaluations 2002(DUC, 2002) using the
proposed algorithm after the inclusion of cohe-
rence chunker module.
</bodyText>
<reference confidence="0.921968285714286">
Score
ROUGE-1 0.5312
ROUGE-L 0.4978
Table 2: ROUGE Score for the news article
summarization task with coherence chunker,
calculated across 567 articles.
ACE Corpus. NIST 2008 Automatic Content Extraction
Evaluation(ACE08).
http://www.itl.nist.gov/iad/mig/tests/ace/2008/
Brin and L. Page. 1998. The anatomy of a large scale
hypertextualWeb search engine. Computer Networks
and ISDN Systems, 30 (1 – 7).
Erkan and D. Radev. 2004. Lexpagerank: Prestige in
multi document text summarization. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Barcelona, Spain, July.
Fellbaum, C., ed. WordNet: An electronic lexical data-
base. MIT Press, Cambridge (1998).
Kleinberg. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, 46(5):604-
632.
</reference>
<page confidence="0.982918">
59
</page>
<reference confidence="0.999846051282052">
Lin and E.H. Hovy. From Single to Multi document
Summarization: A Prototype System and its Evalua-
tion. In Proceedings of ACL-2002.
Lin and E.H. Hovy. 2003a. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of Human Language Technology Confe-
rence (HLT-NAACL 2003), Edmonton, Canada, May.
Lin and E.H. Hovy. 2003b. The potential and limitations
of sentence extraction for summarization. In Pro-
ceedings of the HLT/NAACL Workshop on Automatic
Summarization, Edmonton, Canada, May.
Mihalcea. 2004. Graph-based ranking algorithms for
sentence extraction, applied to text summarization. In
Proceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2004)
(companion volume), Barcelona, Spain.
Mihalcea and P. Tarau. 2004. TextRank - bringing order
into texts. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2004), Barcelona, Spain.
Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on
semantic networks, with application to word sense
disambiguation. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics
(COLING 2004), Geneva, Switzerland.
Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D.,
Miller, K. J. Introduction to WordNet: An on-line
lexical database. Journal of Lexicography (1990).
Morris, J., Hirst, G. Lexical cohesion computed by the-
saural relations as an indicator of the structure of
text. Computational Linguistics (1991).
Radev, H. Y. Jing, M. Stys and D. Tam. Centroid-based
summarization of multiple documents. Information
Processing and Management, 40: 919-938, 2004.
Robert de Beaugrande and Wolfgang Dressler. Intro-
duction to Text Linguistics. Longman, 1981.
Silber, H. G., McCoy, K. F. Efficient text summariza-
tion using lexical chains. In Proceedings of Intelli-
gent User Interfaces. (2000).
</reference>
<page confidence="0.998412">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.916958">
<title confidence="0.999901">An Approach to Text Summarization</title>
<author confidence="0.991032">Sankar K Sobha L</author>
<affiliation confidence="0.997065">AU-KBC Research Centre AU-KBC Research Centre MIT Campus, Anna University MIT Campus, Anna University</affiliation>
<address confidence="0.998061">Chennai- 44. Chennai- 44.</address>
<email confidence="0.941636">sankar@au-kbc.orgsobha@au-kbc.org</email>
<abstract confidence="0.99929505882353">We propose an efficient text summarization technique that involves two basic operations. The first operation involves finding coherent chunks in the document and the second operation involves ranking the text in the individual coherent chunks and picking the sentences that rank above a given threshold. The coherent chunks are formed by exploiting the lexical relationship between adjacent sentences in the document. Occurrence of words through repetition or relatedness by sense relation plays a major role in forming a cohesive tie. The proposed text ranking approach is based on a graph theoretic ranking model applied to text summarization task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>for the news article summarization task with coherence chunker, calculated across 567 articles.</title>
<booktitle>Score ROUGE-1 0.5312 ROUGE-L 0.4978 Table 2: ROUGE Score</booktitle>
<marker></marker>
<rawString>Score ROUGE-1 0.5312 ROUGE-L 0.4978 Table 2: ROUGE Score for the news article summarization task with coherence chunker, calculated across 567 articles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Automatic Content Extraction Evaluation(ACE08).</title>
<date>2008</date>
<marker>NIST, 2008</marker>
<rawString>ACE Corpus. NIST 2008 Automatic Content Extraction Evaluation(ACE08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large scale hypertextualWeb search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="1341" citStr="Brin and Page, 1998" startWordPosition="199" endWordPosition="202">major role in forming a cohesive tie. The proposed text ranking approach is based on a graph theoretic ranking model applied to text summarization task. 1 Introduction Automated summarization is an important area in NLP research. A variety of automated summarization schemes have been proposed recently. NeATS (Lin and Hovy, 2002) is a sentence position, term frequency, topic signature and term clustering based approach and MEAD (Radev et al., 2004) is a centroid based approach. Iterative graph based Ranking algorithms, such as Kleinberg’s HITS algorithm (Kleinberg, 1999) and Google’s PageRank (Brin and Page, 1998) have been traditionally and successfully used in web-link analysis, social networks and more recently in text processing applications (Mihalcea and Tarau, 2004), (Mihalcea et al., 2004), (Erkan and Radev, 2004) and (Mihalcea, 2004). These iterative approaches have a high time complexity and are practically slow in dynamic summarization. Proposals are also made for coherence based automated summarization system (Silber and McCoy, 2000). We propose a novel text summarization technique that involves two basic operations, namely finding coherent chunks in the document and ranking the text in the </context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Brin and L. Page. 1998. The anatomy of a large scale hypertextualWeb search engine. Computer Networks and ISDN Systems, 30 (1 – 7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erkan</author>
<author>D Radev</author>
</authors>
<title>Lexpagerank: Prestige in multi document text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1552" citStr="Erkan and Radev, 2004" startWordPosition="230" endWordPosition="233">rea in NLP research. A variety of automated summarization schemes have been proposed recently. NeATS (Lin and Hovy, 2002) is a sentence position, term frequency, topic signature and term clustering based approach and MEAD (Radev et al., 2004) is a centroid based approach. Iterative graph based Ranking algorithms, such as Kleinberg’s HITS algorithm (Kleinberg, 1999) and Google’s PageRank (Brin and Page, 1998) have been traditionally and successfully used in web-link analysis, social networks and more recently in text processing applications (Mihalcea and Tarau, 2004), (Mihalcea et al., 2004), (Erkan and Radev, 2004) and (Mihalcea, 2004). These iterative approaches have a high time complexity and are practically slow in dynamic summarization. Proposals are also made for coherence based automated summarization system (Silber and McCoy, 2000). We propose a novel text summarization technique that involves two basic operations, namely finding coherent chunks in the document and ranking the text in the individual coherent chunks formed. For finding coherent chunks in the document, we propose a set of rules that identifies the connection between adjacent sentences in the document. The connected sentences that a</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Erkan and D. Radev. 2004. Lexpagerank: Prestige in multi document text summarization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
<author>ed</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge</location>
<marker>Fellbaum, ed, 1998</marker>
<rawString>Fellbaum, C., ed. WordNet: An electronic lexical database. MIT Press, Cambridge (1998).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>Journal of the ACM,</journal>
<pages>46--5</pages>
<contexts>
<context position="1297" citStr="Kleinberg, 1999" startWordPosition="193" endWordPosition="194">r relatedness by sense relation plays a major role in forming a cohesive tie. The proposed text ranking approach is based on a graph theoretic ranking model applied to text summarization task. 1 Introduction Automated summarization is an important area in NLP research. A variety of automated summarization schemes have been proposed recently. NeATS (Lin and Hovy, 2002) is a sentence position, term frequency, topic signature and term clustering based approach and MEAD (Radev et al., 2004) is a centroid based approach. Iterative graph based Ranking algorithms, such as Kleinberg’s HITS algorithm (Kleinberg, 1999) and Google’s PageRank (Brin and Page, 1998) have been traditionally and successfully used in web-link analysis, social networks and more recently in text processing applications (Mihalcea and Tarau, 2004), (Mihalcea et al., 2004), (Erkan and Radev, 2004) and (Mihalcea, 2004). These iterative approaches have a high time complexity and are practically slow in dynamic summarization. Proposals are also made for coherence based automated summarization system (Silber and McCoy, 2000). We propose a novel text summarization technique that involves two basic operations, namely finding coherent chunks </context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>Kleinberg. 1999. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):604-632.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lin</author>
<author>E H Hovy</author>
</authors>
<title>From Single to Multi document Summarization: A Prototype System and its Evaluation.</title>
<booktitle>In Proceedings of ACL-2002.</booktitle>
<marker>Lin, Hovy, </marker>
<rawString>Lin and E.H. Hovy. From Single to Multi document Summarization: A Prototype System and its Evaluation. In Proceedings of ACL-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology Conference (HLT-NAACL 2003),</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="18048" citStr="Lin and Hovy, 2003" startWordPosition="3156" endWordPosition="3159">eated occurrences are weighed efficiently by LSW, thereby giving it a relevant ranking position. 5 Summarization Summarization is done by applying text ranking over the global coherent chunks in the document. The sentences whose weight is above the threshold is picked and rearranged in the order in which the sentences appeared in the original document. LSW (si, sj) = 58 6 Evaluation The ROUGE evaluation toolkit is employed to evaluate the proposed algorithm. ROUGE, an automated summarization evaluation package based on Ngram statistics, is found to be highly correlated with human evaluations (Lin and Hovy, 2003a). The evaluations are reported in ROUGE-1 metrics, which seeks unigram matches between the generated and the reference summaries. The ROUGE-1 metric is found to have high correlation with human judgments at a 95% confidence level and hence used for evaluation. (Mihalcea and Tarau, 2004) a graph based ranking model with Rouge score 0.4904, (Mihalcea, 2004) Graphbased Ranking Algorithms for Sentence Extraction, Applied to Text Summarization with Rouge score 0.5023. Table 1 shows the ROUGE Score of 567 news articles provided during the Document Understanding Evaluations 2002(DUC, 2002) using th</context>
<context position="19361" citStr="Lin and Hovy, 2003" startWordPosition="3360" endWordPosition="3363">UGE-L 0.4863 Table 1: ROUGE Score for the news article summarization task without coherence chunker, calculated across 567 articles. Comparatively Table 2, which is the the ROUGE score for summary including the coherence chunker module gives better result. 7 Related Work Text extraction is considered to be the important and foremost process in summarization. Intuitively, a hash based approach to graph based ranking algorithm for text ranking works well on the task of extractive summarization. A notable study report on usefulness and limitations of automatic sentence extraction is reported in (Lin and Hovy, 2003b), which emphasizes the need for efficient algorithms for sentence ranking and summarization. 8 Conclusions In this paper, we propose a coherence chunker module and a hash based approach to graph based ranking algorithm for text ranking. In specific, we propose a novel approach for graph based text ranking, with improved results comparative to existing ranking algorithms. The architecture of the algorithm helps the ranking process to be done in a time efficient way. This approach succeeds in grabbing the coherent sentences based on the linguistic and heuristic rules; whereas other supervised </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin and E.H. Hovy. 2003a. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of Human Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin</author>
<author>E H Hovy</author>
</authors>
<title>The potential and limitations of sentence extraction for summarization.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT/NAACL Workshop on Automatic Summarization,</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="18048" citStr="Lin and Hovy, 2003" startWordPosition="3156" endWordPosition="3159">eated occurrences are weighed efficiently by LSW, thereby giving it a relevant ranking position. 5 Summarization Summarization is done by applying text ranking over the global coherent chunks in the document. The sentences whose weight is above the threshold is picked and rearranged in the order in which the sentences appeared in the original document. LSW (si, sj) = 58 6 Evaluation The ROUGE evaluation toolkit is employed to evaluate the proposed algorithm. ROUGE, an automated summarization evaluation package based on Ngram statistics, is found to be highly correlated with human evaluations (Lin and Hovy, 2003a). The evaluations are reported in ROUGE-1 metrics, which seeks unigram matches between the generated and the reference summaries. The ROUGE-1 metric is found to have high correlation with human judgments at a 95% confidence level and hence used for evaluation. (Mihalcea and Tarau, 2004) a graph based ranking model with Rouge score 0.4904, (Mihalcea, 2004) Graphbased Ranking Algorithms for Sentence Extraction, Applied to Text Summarization with Rouge score 0.5023. Table 1 shows the ROUGE Score of 567 news articles provided during the Document Understanding Evaluations 2002(DUC, 2002) using th</context>
<context position="19361" citStr="Lin and Hovy, 2003" startWordPosition="3360" endWordPosition="3363">UGE-L 0.4863 Table 1: ROUGE Score for the news article summarization task without coherence chunker, calculated across 567 articles. Comparatively Table 2, which is the the ROUGE score for summary including the coherence chunker module gives better result. 7 Related Work Text extraction is considered to be the important and foremost process in summarization. Intuitively, a hash based approach to graph based ranking algorithm for text ranking works well on the task of extractive summarization. A notable study report on usefulness and limitations of automatic sentence extraction is reported in (Lin and Hovy, 2003b), which emphasizes the need for efficient algorithms for sentence ranking and summarization. 8 Conclusions In this paper, we propose a coherence chunker module and a hash based approach to graph based ranking algorithm for text ranking. In specific, we propose a novel approach for graph based text ranking, with improved results comparative to existing ranking algorithms. The architecture of the algorithm helps the ranking process to be done in a time efficient way. This approach succeeds in grabbing the coherent sentences based on the linguistic and heuristic rules; whereas other supervised </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin and E.H. Hovy. 2003b. The potential and limitations of sentence extraction for summarization. In Proceedings of the HLT/NAACL Workshop on Automatic Summarization, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihalcea</author>
</authors>
<title>Graph-based ranking algorithms for sentence extraction, applied to text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1573" citStr="Mihalcea, 2004" startWordPosition="235" endWordPosition="237">ty of automated summarization schemes have been proposed recently. NeATS (Lin and Hovy, 2002) is a sentence position, term frequency, topic signature and term clustering based approach and MEAD (Radev et al., 2004) is a centroid based approach. Iterative graph based Ranking algorithms, such as Kleinberg’s HITS algorithm (Kleinberg, 1999) and Google’s PageRank (Brin and Page, 1998) have been traditionally and successfully used in web-link analysis, social networks and more recently in text processing applications (Mihalcea and Tarau, 2004), (Mihalcea et al., 2004), (Erkan and Radev, 2004) and (Mihalcea, 2004). These iterative approaches have a high time complexity and are practically slow in dynamic summarization. Proposals are also made for coherence based automated summarization system (Silber and McCoy, 2000). We propose a novel text summarization technique that involves two basic operations, namely finding coherent chunks in the document and ranking the text in the individual coherent chunks formed. For finding coherent chunks in the document, we propose a set of rules that identifies the connection between adjacent sentences in the document. The connected sentences that are picked based on th</context>
<context position="18407" citStr="Mihalcea, 2004" startWordPosition="3215" endWordPosition="3216">, sj) = 58 6 Evaluation The ROUGE evaluation toolkit is employed to evaluate the proposed algorithm. ROUGE, an automated summarization evaluation package based on Ngram statistics, is found to be highly correlated with human evaluations (Lin and Hovy, 2003a). The evaluations are reported in ROUGE-1 metrics, which seeks unigram matches between the generated and the reference summaries. The ROUGE-1 metric is found to have high correlation with human judgments at a 95% confidence level and hence used for evaluation. (Mihalcea and Tarau, 2004) a graph based ranking model with Rouge score 0.4904, (Mihalcea, 2004) Graphbased Ranking Algorithms for Sentence Extraction, Applied to Text Summarization with Rouge score 0.5023. Table 1 shows the ROUGE Score of 567 news articles provided during the Document Understanding Evaluations 2002(DUC, 2002) using the proposed algorithm without the inclusion of coherence chunker module. Score ROUGE-1 0.5103 ROUGE-L 0.4863 Table 1: ROUGE Score for the news article summarization task without coherence chunker, calculated across 567 articles. Comparatively Table 2, which is the the ROUGE score for summary including the coherence chunker module gives better result. 7 Relat</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>Mihalcea. 2004. Graph-based ranking algorithms for sentence extraction, applied to text summarization. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004) (companion volume), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>TextRank - bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1502" citStr="Mihalcea and Tarau, 2004" startWordPosition="222" endWordPosition="225">ntroduction Automated summarization is an important area in NLP research. A variety of automated summarization schemes have been proposed recently. NeATS (Lin and Hovy, 2002) is a sentence position, term frequency, topic signature and term clustering based approach and MEAD (Radev et al., 2004) is a centroid based approach. Iterative graph based Ranking algorithms, such as Kleinberg’s HITS algorithm (Kleinberg, 1999) and Google’s PageRank (Brin and Page, 1998) have been traditionally and successfully used in web-link analysis, social networks and more recently in text processing applications (Mihalcea and Tarau, 2004), (Mihalcea et al., 2004), (Erkan and Radev, 2004) and (Mihalcea, 2004). These iterative approaches have a high time complexity and are practically slow in dynamic summarization. Proposals are also made for coherence based automated summarization system (Silber and McCoy, 2000). We propose a novel text summarization technique that involves two basic operations, namely finding coherent chunks in the document and ranking the text in the individual coherent chunks formed. For finding coherent chunks in the document, we propose a set of rules that identifies the connection between adjacent sentenc</context>
<context position="18337" citStr="Mihalcea and Tarau, 2004" startWordPosition="3201" endWordPosition="3205">d in the order in which the sentences appeared in the original document. LSW (si, sj) = 58 6 Evaluation The ROUGE evaluation toolkit is employed to evaluate the proposed algorithm. ROUGE, an automated summarization evaluation package based on Ngram statistics, is found to be highly correlated with human evaluations (Lin and Hovy, 2003a). The evaluations are reported in ROUGE-1 metrics, which seeks unigram matches between the generated and the reference summaries. The ROUGE-1 metric is found to have high correlation with human judgments at a 95% confidence level and hence used for evaluation. (Mihalcea and Tarau, 2004) a graph based ranking model with Rouge score 0.4904, (Mihalcea, 2004) Graphbased Ranking Algorithms for Sentence Extraction, Applied to Text Summarization with Rouge score 0.5023. Table 1 shows the ROUGE Score of 567 news articles provided during the Document Understanding Evaluations 2002(DUC, 2002) using the proposed algorithm without the inclusion of coherence chunker module. Score ROUGE-1 0.5103 ROUGE-L 0.4863 Table 1: ROUGE Score for the news article summarization task without coherence chunker, calculated across 567 articles. Comparatively Table 2, which is the the ROUGE score for summa</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Mihalcea and P. Tarau. 2004. TextRank - bringing order into texts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tarau Mihalcea</author>
<author>E Figa</author>
</authors>
<title>PageRank on semantic networks, with application to word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004),</booktitle>
<location>Geneva, Switzerland.</location>
<marker>Mihalcea, Figa, 2004</marker>
<rawString>Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on semantic networks, with application to word sense disambiguation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K J Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>Journal of Lexicography</journal>
<contexts>
<context position="4433" citStr="Miller et al., 1990" startWordPosition="678" endWordPosition="681">” and “the mutual access and relevance within a configuration of concepts and relations” (Beaugrande and Dressler, 1981). Thus a text gives meaning as a result of union of meaning or senses in the text. The coherence cues present in a sentence are directly visible when we go through the flow of the document. Our approach aims to achieve this objective with linguistic and heuristic information. The identification of semantic neighborhood, occurrence of words through repetition or relatedness by sense relation namely synonyms, hyponyms and hypernym, plays a major role in forming a cohesive tie (Miller et al., 1990). 2.1 Rules for finding Coherent chunks When parsing through a document, the relationship among adjacent sentences is determined by the continuity that exists between them. We define the following set of rules to find coherent chunks in the document. Rule 1 The presence of connectives (such as accordingly, again, also, besides) in present sentence indicates the connectedness of the present sentence with the previous sentence. When such connectives are found, the adjacent sentences form coherent chunks. Rule 2 A 3rd person pronominal in a given sentence refers to the antecedent in the previous </context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D., Miller, K. J. Introduction to WordNet: An on-line lexical database. Journal of Lexicography (1990).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics</journal>
<contexts>
<context position="3722" citStr="Morris and Hirst, 1991" startWordPosition="563" endWordPosition="566">ection 2 discusses lexical cohesion, section 3 discusses the text ranking algorithm and section 4 describes the summarization by combining lexical cohesion and summarization. 2 Lexical Cohesion Coherence in linguistics makes the text semantically meaningful. It is achieved through semantic features such as the use of deictic (a deictic is an expression which shows the direction. ex: that, this.), anaphoric (a referent which requires an antecedent in front. ex: he, she, it.), cataphoric (a referent which requires an antecedent at the back.), lexical relation and proper noun repeating elements (Morris and Hirst, 1991). Robert De Beaugrande and Wolfgang U. Dressler define coherence as a “continuity of senses” and “the mutual access and relevance within a configuration of concepts and relations” (Beaugrande and Dressler, 1981). Thus a text gives meaning as a result of union of meaning or senses in the text. The coherence cues present in a sentence are directly visible when we go through the flow of the document. Our approach aims to achieve this objective with linguistic and heuristic information. The identification of semantic neighborhood, occurrence of words through repetition or relatedness by sense rela</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, J., Hirst, G. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics (1991).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Y Jing Radev</author>
<author>M Stys</author>
<author>D Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<journal>Information Processing and Management,</journal>
<volume>40</volume>
<pages>919--938</pages>
<contexts>
<context position="1172" citStr="Radev et al., 2004" startWordPosition="174" endWordPosition="177">rmed by exploiting the lexical relationship between adjacent sentences in the document. Occurrence of words through repetition or relatedness by sense relation plays a major role in forming a cohesive tie. The proposed text ranking approach is based on a graph theoretic ranking model applied to text summarization task. 1 Introduction Automated summarization is an important area in NLP research. A variety of automated summarization schemes have been proposed recently. NeATS (Lin and Hovy, 2002) is a sentence position, term frequency, topic signature and term clustering based approach and MEAD (Radev et al., 2004) is a centroid based approach. Iterative graph based Ranking algorithms, such as Kleinberg’s HITS algorithm (Kleinberg, 1999) and Google’s PageRank (Brin and Page, 1998) have been traditionally and successfully used in web-link analysis, social networks and more recently in text processing applications (Mihalcea and Tarau, 2004), (Mihalcea et al., 2004), (Erkan and Radev, 2004) and (Mihalcea, 2004). These iterative approaches have a high time complexity and are practically slow in dynamic summarization. Proposals are also made for coherence based automated summarization system (Silber and McCo</context>
</contexts>
<marker>Radev, Stys, Tam, 2004</marker>
<rawString>Radev, H. Y. Jing, M. Stys and D. Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40: 919-938, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert de Beaugrande</author>
<author>Wolfgang Dressler</author>
</authors>
<title>Introduction to Text Linguistics.</title>
<date>1981</date>
<publisher>Longman,</publisher>
<marker>de Beaugrande, Dressler, 1981</marker>
<rawString>Robert de Beaugrande and Wolfgang Dressler. Introduction to Text Linguistics. Longman, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H G Silber</author>
<author>K F McCoy</author>
</authors>
<title>Efficient text summarization using lexical chains.</title>
<date>2000</date>
<booktitle>In Proceedings of Intelligent User Interfaces.</booktitle>
<contexts>
<context position="1780" citStr="Silber and McCoy, 2000" startWordPosition="264" endWordPosition="267">v et al., 2004) is a centroid based approach. Iterative graph based Ranking algorithms, such as Kleinberg’s HITS algorithm (Kleinberg, 1999) and Google’s PageRank (Brin and Page, 1998) have been traditionally and successfully used in web-link analysis, social networks and more recently in text processing applications (Mihalcea and Tarau, 2004), (Mihalcea et al., 2004), (Erkan and Radev, 2004) and (Mihalcea, 2004). These iterative approaches have a high time complexity and are practically slow in dynamic summarization. Proposals are also made for coherence based automated summarization system (Silber and McCoy, 2000). We propose a novel text summarization technique that involves two basic operations, namely finding coherent chunks in the document and ranking the text in the individual coherent chunks formed. For finding coherent chunks in the document, we propose a set of rules that identifies the connection between adjacent sentences in the document. The connected sentences that are picked based on the rules form coherent chunks in the document. For text ranking, we propose an automatic and unsupervised graph based ranking algorithm that gives improved results when compared to other ranking algorithms. T</context>
</contexts>
<marker>Silber, McCoy, 2000</marker>
<rawString>Silber, H. G., McCoy, K. F. Efficient text summarization using lexical chains. In Proceedings of Intelligent User Interfaces. (2000).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>