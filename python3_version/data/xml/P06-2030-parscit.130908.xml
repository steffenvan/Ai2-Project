<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.997577">
Using Bilingual Comparable Corpora and Semi-supervised Clustering for
Topic Tracking
</title>
<author confidence="0.961917">
Fumiyo Fukumoto
</author>
<affiliation confidence="0.981598666666667">
Interdisciplinary Graduate
School of Medicine and Engineering
Univ. of Yamanashi
</affiliation>
<email confidence="0.998606">
fukumoto@yamanashi.ac.jp
</email>
<author confidence="0.984156">
Yoshimi Suzuki
</author>
<affiliation confidence="0.988626666666667">
Interdisciplinary Graduate
School of Medicine and Engineering
Univ. of Yamanashi
</affiliation>
<email confidence="0.998829">
ysuzuki@yamanashi.ac.jp
</email>
<sectionHeader confidence="0.997388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983615384615">
We address the problem dealing with
skewed data, and propose a method for
estimating effective training stories for the
topic tracking task. For a small number of
labelled positive stories, we extract story
pairs which consist of positive and its as-
sociated stories from bilingual comparable
corpora. To overcome the problem of a
large number of labelled negative stories,
we classify them into some clusters. This
is done by using k-means with EM. The
results on the TDT corpora show the ef-
fectiveness of the method.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973366666667">
With the exponential growth of information on the
Internet, it is becoming increasingly difficult to
find and organize relevant materials. Topic Track-
ing defined by the TDT project is a research area
to attack the problem. It starts from a few sample
stories and finds all subsequent stories that discuss
the target topic. Here, a topic in the TDT con-
text is something that happens at a specific place
and time associated with some specific actions. A
wide range of statistical and ML techniques have
been applied to topic tracking(Carbonell et. al,
1999; Oard, 1999; Franz, 2001; Larkey, 2004).
The main task of these techniques is to tune the
parameters or the threshold to produce optimal re-
sults. However, parameter tuning is a tricky issue
for tracking(Yang, 2000) because the number of
initial positive training stories is very small (one
to four), and topics are localized in space and time.
For example, ‘Taipei Mayoral Elections’ and ‘U.S.
Mid-term Elections’ are topics, but ‘Elections’ is
not a topic. Therefore, the system needs to esti-
mate whether or not the test stories are the same
topic with few information about the topic. More-
over, the training data is skewed data, i.e. there
is a large number of labelled negative stories com-
pared to positive ones. The system thus needs to
balance the amount of positive and negative train-
ing stories not to hamper the accuracy of estima-
tion.
In this paper, we propose a method for esti-
mating efficient training stories for topic track-
ing. For a small number of labelled positive sto-
ries, we use bilingual comparable corpora (TDT1-
3 English and Japanese newspapers, Mainichi and
Yomiuri Shimbun). Our hypothesis using bilin-
gual corpora is that many of the broadcasting sta-
tion from one country report local events more fre-
quently and in more detail than overseas’ broad-
casting stations, even if it is a world-wide famous
ones. Let us take a look at some topic from
the TDT corpora. A topic, ‘Kobe Japan quake’
from the TDT1 is a world-wide famous one, and
89 stories are included in the TDT1. However,
Mainichi and Yomiuri Japanese newspapers have
much more stories from the same period of time,
i.e. 5,029 and 4,883 stories for each. These obser-
vations show that it is crucial to investigate the use
of bilingual comparable corpora based on the NL
techniques in terms of collecting more information
about some specific topics. We extract Japanese
stories which are relevant to the positive English
stories using English-Japanese bilingual corpora,
together with the EDR bilingual dictionary. The
associated story is the result of alignment of a
Japanese term association with an English term as-
sociation.
For a large number of labelled negative sto-
ries, we classify them into some clusters us-
ing labelled positive stories. We used a semi-
supervised clustering technique which combines
</bodyText>
<page confidence="0.96764">
231
</page>
<note confidence="0.7241745">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 231–238,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999262">
labeled and unlabeled stories during clustering.
Our goal for semi-supervised clustering is to clas-
sify negative stories into clusters where each clus-
ter is meaningful in terms of class distribution
provided by one cluster of positive training sto-
ries. We introduce k-means clustering that can be
viewed as instances of the EM algorithm, and clas-
sify negative stories into clusters. In general, the
number of clusters k for the k-means algorithm is
not given beforehand. We thus use the Bayesian
Information Criterion (BIC) as the splitting crite-
rion, and select the proper number for k.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999955303571429">
Most of the work which addresses the small num-
ber of positive training stories applies statistical
techniques based on word distribution and ML
techniques. Allan et. al explored on-line adaptive
filtering approaches based on the threshold strat-
egy to tackle the problem(Allan et. al, 1998). The
basic idea behind their work is that stories closer
together in the stream are more likely to discuss re-
lated topics than stories further apart. The method
is based on unsupervised learning techniques ex-
cept for its incremental nature. When a tracking
query is first created from the Nt training stories,
it is also given a threshold. During the tracking
phase, if a story 5 scores over that threshold, 5
is regarded to be relevant and the query is regen-
erated as if 5 were among the Nt training sto-
ries. This method was tested using the TDT1 cor-
pus and it was found that the adaptive approach
is highly successful. But adding more than four
training stories provided only little help, although
in their approach, 12 training stories were added.
The method proposed in this paper is similar to
Allan’s method, however our method for collect-
ing relevant stories is based on story pairs which
are extracted from bilingual comparable corpora.
The methods for finding bilingual story pairs
are well studied in the cross-language IR task,
or MT systems/bilingual lexicons(Dagan, 1997).
Much of the previous work uses cosine similar-
ity between story term vectors with some weight-
ing techniques(Allan et. al, 1998) such as TF-IDF,
or cross-language similarities of terms. However,
most of them rely on only two stories in question
to estimate whether or not they are about the same
topic. We use multiple-links among stories to
produce optimal results.
In the TDT tracking task, classifying negative
stories into meaningful groups is also an im-
portant issue to track topics, since a large num-
ber of labelled negative stories are available in
the TDT context. Basu et. al. proposed a
method using k-means clustering with the EM al-
gorithm, where labeled data provides prior infor-
mation about the conditional distribution of hid-
den category labels(Basu, 2002). They reported
that the method outperformed the standard random
seeding and COP-k-means(Wagstaff, 2001). Our
method shares the basic idea with Basu et. al. An
important difference with their method is that our
method does not require the number of clusters k
in advance, since it is determined during cluster-
ing. We use the BIC as the splitting criterion, and
estimate the proper number for k. It is an impor-
tant feature because in the tracking task, no knowl-
edge of the number of topics in the negative train-
ing stories is available.
</bodyText>
<sectionHeader confidence="0.995766" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.999873666666667">
The system consists of four procedures: extracting
bilingual story pairs, extracting monolingual story
pairs, clustering negative stories, and tracking.
</bodyText>
<subsectionHeader confidence="0.999805">
3.1 Extracting Bilingual Story Pairs
</subsectionHeader>
<bodyText confidence="0.999852958333333">
We extract story pairs which consist of positive
English story and its associated Japanese stories
using the TDT English and Mainichi and Yomi-
uri Japanese corpora. To address the optimal pos-
itive English and their associated Japanese stories,
we combine the output of similarities(multiple-
links). The idea comes from speech recognition
where two outputs are combined to yield a better
result in average. Fig.1 illustrates multiple-links.
The TDT English corpus consists of training and
test stories. Training stories are further divided
into positive(black box) and negative stories(doted
box). Arrows in Fig.1 refer to an edge with simi-
larity value between stories. In Fig.1, for example,
whether the story J2 discusses the target topic, and
is related to E1 or not is determined by not only the
value of similarity between E1 and J2, but also the
similarities between J2 and J4, E1 and J4.
Extracting story pairs is summarized as follows:
Let initial positive training stories E1, · · ·, Em be
initial node, and each Japanese stories J1, · · ·, Jm,
be node or terminal node in the graph G. We cal-
culate cosine similarities between Et(1 &lt; i &lt; m)
and Jj(1 &lt; j &lt; m&apos;)1. In a similar way, we calcu-
</bodyText>
<footnote confidence="0.549202">
1m&apos; refers to the difference of dates between English and
</footnote>
<page confidence="0.977116">
232
</page>
<figure confidence="0.420139">
TDT English corpus
</figure>
<figureCaption confidence="0.999859">
Figure 1: Multiple-links among stories
</figureCaption>
<bodyText confidence="0.954999111111111">
late similarities between Jk and Jl(1 &lt; k, l &lt; m&apos;).
If the value of similarity between nodes is larger
than a certain threshold, we connect them by an
edge(bold arrow in Fig.1). Next, we delete an edge
which is not a constituent of maximal connected
sub-graph(doted arrow in Fig.1). After eliminat-
ing edges, we extract pairs of initial positive En-
glish story Ei and Japanese story Jj as a linked
story pair, and add associated Japanese story Jj
to the training stories. In Fig.1, E1, J2, and J4
are extracted. The procedure for calculating co-
sine similarities between Ei and Jj consists of two
sub-steps: extracting terms, and estimating bilin-
gual term correspondences.
Extracting terms
The first step to calculate similarity between
Ei and Jj is to align a Japanese term with its
associated English term using the bilingual dic-
tionary, EDR. However, this naive method suf-
fers from frequent failure due to incompleteness
of the bilingual dictionary. Let us take a look at
the Mainichi Japanese newspaper stories. The to-
tal number of terms(words) from Oct. 1, 1998 to
Dec. 31, 1998, was 528,726. Of these, 370,013
terms are not included in the EDR bilingual dic-
tionary. For example, ’ (Endeavour)’
which is a key term for the topic ‘Shuttle Endeav-
our mission for space station’ from the TDT3 cor-
pus is not included in the EDR bilingual dictio-
nary. New terms which fail to segment by dur-
ing a morphological analysis are also a problem in
calculating similarities between stories in mono-
lingual data. For example, a proper noun ‘
’(Tokyo Metropolitan Univ.) is divided into
three terms, ‘ ’ (Metropolitan), ‘ (Univ.)’,
Japanese story pairs.
</bodyText>
<tableCaption confidence="0.950566">
Table 1: tE and tJ matrix
</tableCaption>
<equation confidence="0.950621">
tE
tE E siE tE E� siE
</equation>
<bodyText confidence="0.9994533125">
and ‘ (Tokyo)’. To tackle these problems, we
conducted term extraction from a large collection
of English and Japanese corpora. There are several
techniques for term extraction(Chen, 1996). We
used n-gram model with Church-Gale smoothing,
since Chen reported that it outperforms all existing
methods on bigram models produced from large
training data. The length of the extracted terms
does not have a fixed range2. We thus applied the
normalization strategy which is shown in Eq.(1)
to each length of the terms to bring the probabil-
ity value into the range [0,1]. We extracted terms
whose probability value is greater than a certain
threshold. Words from the TDT English(Japanese
newspaper) corpora are identified if they match the
extracted terms.
</bodyText>
<equation confidence="0.7596505">
simold − simmin
simnew = (1)
simmax − simmin
Bilingual term correspondences
</equation>
<bodyText confidence="0.989926695652174">
The second step to calculate similarity between
Ei and Jj is to estimate bilingual term correspon-
dences using χ2 statistics. We estimated bilingual
term correspondences with a large collection of
English and Japanese data. More precisely, let Ei
be an English story (1 &lt; i &lt; n), where n is the
number of stories in the collection, and SiJ denote
the set of Japanese stories with cosine similarities
higher than a certain threshold value θ: SiJ _ {Jj
 |cos(Ei, Jj) ≥ θ}. Then, we concatenate con-
stituent Japanese stories of SiJ into one story S�iJ ,
and construct a pseudo-parallel corpus PPCEJ of
English and Japanese stories: PPCEJ _ { { Ei,
S�i J }  |Si J :7� 0 }. Suppose that there are two crite-
ria, monolingual term tE in English story and tJ in
Japanese story. We can determine whether or not a
particular term belongs to a particular story. Con-
sequently, terms are divided into four classes, as
shown in Table 1. Based on the contingency table
of co-occurence frequencies of tE and tJ, we esti-
mate bilingual term correspondences according to
the statistical measure χ2.
2We set at most five noun words.
</bodyText>
<figure confidence="0.943274">
training stories
E1 E2 E3
time lines
test stories
edge(E1,J1)
edge(E1,J4)
J1 J2 J3 J4 J5 J6
Jm’
time lines
edge(J2,J4)
Mainichi and Yomiuri Japanese corpora topic
not topic
tJ tJ E S�i J
tJ E� S�iJ
a
c
b
d
(ad − bc)2
χ2(tE, tJ) =
(a + b)(a + c)(b + d)(c + d) (2)
</figure>
<page confidence="0.993958">
233
</page>
<bodyText confidence="0.999969714285714">
We extract term tJ as a pair of tE which satisfies
maximum value of χ2, i.e. maxtJ∈TJ χ2(tE,tJ),
where TJ = {tJ  |χ2(tE,tJ)}. For the extracted En-
glish and Japanese term pairs, we conducted semi-
automatic acquisition, i.e. we manually selected
bilingual term pairs, since our source data is not
a clean parallel corpus, but an artificially gener-
ated noisy pseudo-parallel corpus, it is difficult to
compile bilingual terms full-automatically(Dagan,
1997). Finally, we align a Japanese term with its
associated English term using the selected bilin-
gual term correspondences, and again calculate
cosine similarities between Japanese and English
stories.
</bodyText>
<subsectionHeader confidence="0.998752">
3.2 Extracting Monolingual Story Pairs
</subsectionHeader>
<bodyText confidence="0.99996">
We noted above that our source data is not a clean
parallel corpus. Thus the difference of dates be-
tween bilingual stories is one of the key factors to
improve the performance of extracting story pairs,
i.e. stories closer together in the timeline are more
likely to discuss related subjects. We therefore ap-
plied a method for extracting bilingual story pairs
from stories closer in the timelines. However, this
often hampers our basic motivation for using bilin-
gual corpora: bilingual corpora helps to collect
more information about the target topic. We there-
fore extracted monolingual(Japanese) story pairs
and added them to the training stories. Extract-
ing Japanese monolingual story pairs is quite sim-
ple: Let Jj (1 G j G m&apos;) be the extracted Japanese
story in the procedure, extracting bilingual story
pairs. We calculate cosine similarities between Jj
and Jk(1 G k G n). If the value of similarity be-
tween them is larger than a certain threshold, we
add Jk to the training stories.
</bodyText>
<subsectionHeader confidence="0.999726">
3.3 Clustering Negative Stories
</subsectionHeader>
<bodyText confidence="0.9998947">
Our method for classifying negative stories into
some clusters is based on Basu et. al.’s
method(Basu, 2002) which uses k-means with the
EM algorithm. K-means is a clustering algo-
rithm based on iterative relocation that partitions
a dataset into the number of k clusters, locally
minimizing the average squared distance between
the data points and the cluster centers(centroids).
Suppose we classify X = { x1, · · ·, xN}, xi E
Rd into k clusters: one is the cluster which con-
sists of positive stories, and other k-1 clusters
consist of negative stories. Here, which clusters
does each negative story belong to? The EM is
a method of finding the maximum-likelihood es-
timate(MLE) of the parameters of an underlying
distribution from a set of observed data that has
missing value. K-means is essentially an EM on
a mixture of k Gaussians under certain assump-
tions. In the standard k-means without any initial
supervision, the k-means are chosen randomly in
the initial M-step and the stories are assigned to
the nearest means in the subsequent E-step. For
positive training stories, the initial labels are kept
unchanged throughout the algorithm, whereas the
conditional distribution for the negative stories are
re-estimated at every E-step. We select the num-
ber of k initial stories: one is the cluster center of
positive stories, and other k-1 stories are negative
stories which have the top k-1 smallest value be-
tween the negative story and the cluster center of
positive stories. In Basu et. al’s method, the num-
ber of k is given by a user. However, for negative
training stories, the number of clusters is not given
beforehand. We thus developed an algorithm for
estimating k. It goes into action after each run of
k means3, making decisions about which sets of
clusters should be chosen in order to better fit the
data. The splitting decision is done by comput-
ing the Bayesian Information Criterion which is
shown in Eq.(3).
</bodyText>
<equation confidence="0.997686">
BIC(k = l) = �lll(X) − pl2 · log N (3)
</equation>
<bodyText confidence="0.999685222222222">
where �lll(X) is the log-likelihood of X according
to the number of k is l, N is the total number of
training stories, and pl is the number of parame-
ters in k = l. We set pl to the sum of k class prob-
abilities, �km=1�ll(Xm) , the number of n · k cen-
troid coordinates, and the MLE for the variance,
p2. Here, n is the number of dimensions. p2, un-
der the identical spherical Gaussian assumption,
is:
</bodyText>
<equation confidence="0.872494">
(xi − μi)2 (4)
</equation>
<bodyText confidence="0.9972375">
where μi denotes i-th partition center. The proba-
bilities are:
</bodyText>
<equation confidence="0.990206666666667">
P(xi) = RiN · 1
√2πPn exp(− 1
2P2  ||xi − μi ||2) (5)
</equation>
<footnote confidence="0.64195125">
Ri is the number of stories that have μi as their
closest centroid. The log-likelihood of ll(X)
3We set the maximum number of k to 100 in the experi-
ment.
</footnote>
<equation confidence="0.929905333333333">
P2 = 1
N − k�
i
</equation>
<page confidence="0.989224">
234
</page>
<figureCaption confidence="0.799093333333333">
minimum distance between test data and the center of gravity
Figure 2: Each cluster and a test story
is log Hi P(xi). It is taken at the maximum-
likelihood point(story), and thus, focusing just on
the set X,,t C X which belongs to the centroid m
and plugging in the MLE yields:
</figureCaption>
<equation confidence="0.99970425">
ll(Xm) = −Rm2 log(2π) − Rm · n
2 log(�ρ2) − Rm − k
2
+Rm log Rm − Rm log N (1 ≤ m ≤ k) (6)
</equation>
<bodyText confidence="0.9997865">
We choose the number of k whose value of BIC
is highest.
</bodyText>
<subsectionHeader confidence="0.95602">
3.4 Tracking
</subsectionHeader>
<bodyText confidence="0.999977545454546">
Each story is represented as a vector of terms
with tf· idf weights in an n dimensional space,
where n is the number of terms in the collection.
Whether or not each test story is positive is judged
using the distance (measured by cosine similarity)
between a vector representation of the test story
and each centroid g of the clusters. Fig.2 illus-
trates each cluster and a test story in the tracking
procedure. Fig.2 shows that negative training sto-
ries are classified into three groups. The centroid
g for each cluster is calculated as follows:
</bodyText>
<equation confidence="0.970915">
b = (g1, ··· ,gn) = (1
p
</equation>
<bodyText confidence="0.999986888888889">
where xis (1 &lt; j &lt; n) is the tf·idf weighted value
of term j in the story xi. The test story is judged
by using these centroids. If the value of cosine
similarity between the test story and the centroid
with positive stories is smallest among others, the
test story is declared to be positive. In Fig.2, the
test story is regarded as negative, since the value
between them is smallest. This procedure, is re-
peated until the last test story is judged.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99981">
4.1 Creating Japanese Corpus
</subsectionHeader>
<bodyText confidence="0.999989961538461">
We chose the TDT3 English corpora as our gold
standard corpora. TDT3 consists of 34,600 sto-
ries with 60 manually identified topics. We then
created Japanese corpora (Mainichi and Yomiuri
newspapers) to evaluate the method. We annotated
the total number of 66,420 stories from Oct.1, to
Dec.31, 1998, against the 60 topics. Each story
was labelled according to whether the story dis-
cussed the topic or not. Not all the topics were
present in the Japanese corpora. We therefore col-
lected 1 topic from the TDT1 and 2 topics from the
TDT2, each of which occurred in Japan, and added
them in the experiment. TDT1 is collected from
the same period of dates as the TDT3, and the first
story of ‘Kobe Japan Quake’ topic starts from Jan.
16th. We annotated 174,384 stories of Japanese
corpora from the same period for the topic. Ta-
ble 2 shows 24 topics which are included in the
Japanese corpora. ‘TDT’ refers to the evaluation
data, TDT1, 2, or 3. ‘ID’ denotes topic number de-
fined by the TDT. ‘OnT.’(On-Topic) refers to the
number of stories discussing the topic. Bold font
stands for the topic which happened in Japan. The
evaluation of annotation is made by three humans.
The classification is determined to be correct if the
majority of three human judges agree.
</bodyText>
<subsectionHeader confidence="0.965241">
4.2 Experiments Set Up
</subsectionHeader>
<bodyText confidence="0.999975818181818">
The English data we used for extracting terms
is Reuters’96 corpus(806,791 stories) including
TDT1 and TDT3 corpora. The Japanese data
was 1,874,947 stories from 14 years(from 1991
to 2004) Mainichi newspapers(1,499,936 stories),
and 3 years(1994, 1995, and 1998) Yomiuri
newspapers(375,011 stories). All Japanese sto-
ries were tagged by the morphological analysis
Chasen(Matsumoto, 1997). English stories were
tagged by a part-of-speech tagger(Schmid, 1995),
and stop word removal. We applied n-gram model
with Church-Gale smoothing to noun words, and
selected terms whose probabilities are higher than
a certain threshold4. As a result, we obtained
338,554 Japanese and 130,397 English terms. We
used the EDR bilingual dictionary, and translated
Japanese terms into English. Some of the words
had no translation. For these, we estimated term
correspondences. Each story is represented as a
vector of terms with tf·idf weights. We calcu-
lated story similarities and extracted story pairs
between positive and its associated stories5. In
</bodyText>
<footnote confidence="0.9424976">
4The threshold value for both English and Japanese was
0.800. It was empirically determined.
5The threshold value for bilingual story pair was 0.65, and
that for monolingual was 0.48. The difference of dates be-
tween bilingual stories was ±4.
</footnote>
<bodyText confidence="0.774019333333333">
center of gravity
cluster of positive training data
test data cluster of negative training data
</bodyText>
<equation confidence="0.957688375">
1
p
xi1, ·· · ,
xin)(7)
�p
i=1
�p
i=1
</equation>
<page confidence="0.999413">
235
</page>
<tableCaption confidence="0.992119">
Table 2: Topic Name
</tableCaption>
<table confidence="0.559789">
TDT ID Topic name OnT. TDT ID Topic name OnT.
1 15 Kobe Japan quake 9,912
2 31015 Japan Apology to Korea 28 2 31023 Kyoto Energy Protocol 40
3 30001 Cambodian government coalition 48 3 30003 Pinochet trial 165
3 30006 NBA labor disputes 44 3 30014 Nigerian gas line fire 6
3 30017 North Korean food shortages 23 3 30018 Tony Blair visits China in Oct. 7
3 30022 Chinese dissidents sentenced 21 3 30030 Taipei Mayoral elections 353
3 30031 Shuttle Endeavour mission for space station 17 3 30033 Euro Introduced 152
3 30034 Indonesia-East Timor conflict 34 3 30038 Olympic bribery scandal 35
3 30041 Jiang’s Historic Visit to Japan 111 3 30042 PanAm lockerbie bombing trial 13
3 30047 Space station module Zarya launched 30 3 30048 IMF bailout of Brazil 28
3 30049 North Korean nuclear facility? 111 3 30050 U.S. Mid-term elections 123
3 30053 Clinton’s Gaza trip 74 3 30055 D’Alema’s new Italian government 37
3 30057 India train derailment 12
</table>
<bodyText confidence="0.999019285714286">
the tracking, we used the extracted terms together
with all verbs, adjectives, and numbers, and repre-
sented each story as a vector of these with tf·idf
weights.
We set the evaluation measures used in the TDT
benchmark evaluations. ‘Miss’ denotes Miss rate,
which is the ratio of the stories that were judged
as YES but were not evaluated as such for the run
in question. ‘F/A’ shows false alarm rate, which is
the ratio of the stories judged as NO but were eval-
uated as YES. The DET curve plots misses and
false alarms, and better performance is indicated
by curves more to the lower left of the graph. The
detection cost function(CDet) is defined by Eq.(8).
</bodyText>
<equation confidence="0.99957575">
CDet = (CMiss * PMiss * PTarget +
CFa * PFa * (1 − PTarget))
PMiss = #Misses/#Targets
PFa = #FalseAlarms/#NonTargets (8)
</equation>
<bodyText confidence="0.999373571428571">
CMiss, CFa, and PTarget are the costs of a missed
detection, false alarm, and priori probability of
finding a target, respectively. CMiss, CFa, and
PTarget are usually set to 10, 1, and 0.02, respec-
tively. The normalized cost function is defined by
Eq.(9), and lower cost scores indicate better per-
formance.
</bodyText>
<equation confidence="0.9992845">
(CDet)Norm = CDet/MIN(CMiss * PTarget, CFa
*(1 − PTarget)) (9)
</equation>
<subsectionHeader confidence="0.999436">
4.3 Basic Results
</subsectionHeader>
<bodyText confidence="0.999520625">
Table 3 summaries the tracking results. MIN
denotes MIN(CDet)Norm which is the value of
(CDet)Norm at the best possible threshold. Nt
is the number of initial positive training stories.
We recall that we used subset of the topics de-
fined by the TDT. We thus implemented Allan’s
method(Allan et. al, 1998) which is similar to
our method, and compared the results. It is based
</bodyText>
<figure confidence="0.9312485">
.01 .02 .05 0.1 0.2 0.5 1 2 5 10 20 40 60 80 90
False Alarm Probability (in %)
</figure>
<figureCaption confidence="0.999616">
Figure 3: Tracking result(23 topics)
</figureCaption>
<bodyText confidence="0.999925047619048">
on a tracking query which is created from the top
10 most commonly occurring features in the Nt
stories, with weight equal to the number of times
the term occurred in those stories multiplied by its
incremental idf value. They used a shallow tag-
ger and selected all nouns, verbs, adjectives, and
numbers. We added the extracted terms to these
part-of-speech words to make their results compa-
rable with the results by our method. ‘Baseline’
in Table 3 shows the best result with their method
among varying threshold values of similarity be-
tween queries and test stories. We can see that the
performance of our method was competitive to the
baseline at every Nt value.
Fig.3 shows DET curves by both our method
and Allan’s method(baseline) for 23 topics from
the TDT2 and 3. Fig.4 illustrates the results for 3
topics from TDT2 and 3 which occurred in Japan.
To make some comparison possible, only the Nt =
4 is given for each. Both Figs. show that we have
an advantage using bilingual comparable corpora.
</bodyText>
<subsectionHeader confidence="0.997818">
4.4 The Effect of Story Pairs
</subsectionHeader>
<bodyText confidence="0.999907">
The contribution of the extracted story pairs, es-
pecially the use of two types of story pairs, bilin-
gual and monolingual, is best explained by look-
ing at the two results: (i) the tracking results with
two types of story pairs, with only English and
</bodyText>
<figure confidence="0.997597428571429">
Miss Probability (in %)
90
80
60
40
20
10
5
2
1
random performance
With
story pairs
Baseline
</figure>
<page confidence="0.997435">
236
</page>
<tableCaption confidence="0.9942">
Table 3: Basic results
</tableCaption>
<table confidence="0.944634428571429">
TDT1 (Kobe Japan Quake)
Baseline Bilingual corpora &amp; clustering
Nt Miss F/A Recall Precision F MIN Nt Miss F/A Recall Precision F MIN
1 27% .15% 73% 67% .70 .055 1 10% .42% 90% 74% .81 .023
2 20% .12% 80% 73% .76 .042 2 6% .27% 93% 76% .83 .013
4 9% .09% 91% 80% .85 .039 4 5% .18% 96% 81% .88 .012
TDT2 &amp; TDT3(23 topics)
Baseline Bilingual corpora &amp; clustering
Nt Miss F/A Recall Precision F MIN Nt Miss F/A Recall Precision F MIN
1 41% .17% 59% 60% .60 .089 1 29% .25% 71% 54% .61 .059
2 40% .16% 60% 62% .61 .072 2 27% .25% 73% 55% .63 .054
4 29% .12% 71% 72% .71 .057 4 20% .13% 80% 73% .76 .041
.01 .02 .05 0.1 0.2 0.5 1 2 5 10 20 40 60 80 90
False Alarm Probability (in %)
</table>
<figureCaption confidence="0.982466">
Figure 4: 3 topics concerning to Japan
</figureCaption>
<figure confidence="0.9561845">
.01 .02 .05 0.1 0.2 0.5 1 2 5 10 20 40 60 80 90
False Alarm Probability (in %)
</figure>
<figureCaption confidence="0.999906">
Figure 5: With and without story pairs
</figureCaption>
<bodyText confidence="0.999583125">
Japanese stories in question, and without story
pairs, and (ii) the results of story pairs by vary-
ing values of Nt. Fig.5 illustrates DET curves for
23 topics, Nt=4.
As can be clearly seen from Fig.5, the re-
sult with story pairs improves the overall perfor-
mance, especially the result with two types of
story pairs was better than that with only English
</bodyText>
<tableCaption confidence="0.998655">
Table 4: Performance of story pairs(24 topics)
</tableCaption>
<table confidence="0.9692354">
Nt Two types of story pairs J-E story pairs
Rec. Prec. F Rec. Prec. F
1 30% 82% .439 28% 80% .415
2 36% 85% .506 33% 82% .471
4 45% 88% .595 42% 79% .548
</table>
<bodyText confidence="0.99987529032258">
and Japanese stories in question. Table 4 shows
the performance of story pairs which consist of
positive and its associated story. Each result de-
notes micro-averaged scores. ‘Rec.’ is the ratio
of correct story pair assignments by the system di-
vided by the total number of correct assignments.
‘Prec.’ is the ratio of correct story pair assign-
ments by the system divided by the total number
of system’s assignments. Table 4 shows that the
system with two types of story pairs correctly ex-
tracted stories related to the target topic even for a
small number of positive training stories, since the
ratio of Prec. in Nt = 1 is 0.82. However, each re-
call value in Table 4 is low. One solution is to use
an incremental approach, i.e. by repeating story
pairs extraction, new story pairs that are not ex-
tracted previously may be extracted. This is a rich
space for further exploration.
The effect of story pairs for the tracking task
also depends on the performance of bilingual term
correspondences. We obtained 1,823 English and
Japanese term pairs in all when a period of days
was ±4. Fig.6 illustrates the result using differ-
ent period of days(±1 to ±10). For example, ‘±1’
shows that the difference of dates between English
and Japanese story pairs is less than ±1. Y-axis
shows the precision which is the ratio of correct
term pairs by the system divided by the total num-
ber of system’s assignments. Fig.6 shows that the
difference of dates between bilingual story pairs,
affects the overall performance.
</bodyText>
<subsectionHeader confidence="0.966028">
4.5 The Effect of k-means with EM
</subsectionHeader>
<bodyText confidence="0.999878">
The contribution of k-means with EM for classi-
fying negative stories is explained by looking at
the result without classifying negative stories. We
calculated the centroid using all negative training
stories, and a test story is judged to be negative or
</bodyText>
<figure confidence="0.924817583333334">
Miss Probability (in %)
90
80
60
40
20
10
5
2
1
random performance
With story pairs(Japan)
Baseline(Japan)
Miss Probability (in %)
90
80
60
40
20
10
5
2
1
random performance
two types of story pairs
With only J-E story pairs
Without story pairs
237
Prec. (%)
39.8 53.0
18.3 37.2 34.0 33.7 32.0
20.8 19.6
1.42
㫧㪈㪻㪸㫐 㫧㪉 㫧㪊 㫧㪋 㫧㪌 㫧㪍 㫧㪎 㫧㪏 㫧㪐 㫧㪈㪇
.01 .02 .05 0.1 0.2 0.5 1 2 5 10 20 40 60 80 90
False Alarm Probability (in %)
</figure>
<figureCaption confidence="0.999986">
Figure 7: BIC v.s. fixed k for k-means with EM
</figureCaption>
<bodyText confidence="0.9999454">
positive by calculating cosine similarities between
the test story and each centroid of negative and
positive stories. Further, to examine the effect of
using the BIC, we compared with choosing a pre-
defined k, i.e. k=10, 50, and 100. Fig.7 illustrates
part of the result for k=100. We can see that the
method without classifying negative stories(k=0)
does not perform as well and results in a high miss
rate. This result is not surprising, because the size
of negative training stories is large compared with
that of positive ones, and therefore, the test story is
erroneously judged as NO. Furthermore, the result
indicates that we need to run BIC, as the result was
better than the results with choosing any number
of pre-defined k, i.e. k=10, 50, and 100. We also
found that there was no correlation between the
number of negative training stories for each of the
24 topics and the number of clusters k obtained by
the BIC. The minimum number of clusters k was
44, and the maximum was 100.
</bodyText>
<sectionHeader confidence="0.999597" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999975727272727">
In this paper, we addressed the issue of the differ-
ence in sizes between positive and negative train-
ing stories for the tracking task, and investigated
the use of bilingual comparable corpora and semi-
supervised clustering. The empirical results were
encouraging. Future work includes (i) extend-
ing the method to an incremental approach for
extracting story pairs, (ii) comparing our cluster-
ing method with the other existing methods such
as X-means(Pelleg, 2000), and (iii) applying the
method to the TDT4 for quantitative evaluation.
</bodyText>
<sectionHeader confidence="0.998901" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.77786375">
This work was supported by the Grant-in-aid for
the JSPS, Support Center for Advanced Telecom-
munications Technology Research, and Interna-
tional Communications Foundation.
</bodyText>
<sectionHeader confidence="0.991109" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999143414634146">
J.Allan and R.Papka and V.Lavrenko, On-line new event
detection and tracking, Proc. of the DARPA Workshop,
1998.
J.Allan and V.Lavrenko and R.Nallapti, UMass at TDT
2002, Proc. of TDT Workshop, 2002.
S.Basu and A.Banerjee and R.Mooney, Semi-supervised
clustering by seeding, Proc. of ICML’02, 2002.
J.Carbonell et. al, CMU report on TDT-2: segmentation,
detection and tracking, Proc. of the DARPA Workshop,
1999.
S.F.Chen and J.Goodman, An empirical study of smoothing
techniques for language modeling, Proc. of the ACL’96,
pp. 310-318, 1996.
N.Collier and H.Hirakawa and A.Kumano, Machine trans-
lation vs. dictionary term translation - a comparison for
English-Japanese news article alignment, Proc. of COL-
ING’02, pp. 263-267, 2002.
I.Dagan and K.Church, Termight: Coordinating humans and
machines in bilingual terminology acquisition, Journal of
MT, Vol. 20, No. 1, pp. 89-107, 1997.
M.Franz and J.S.McCarley, Unsupervised and supervised
clustering for topic tracking, Proc. of SIGIR’01, pp. 310-
317, 2001.
L.S.Larkey et. al, Language-specific model in multilingual
topic tracking, Proc. of SIGIR’04, pp. 402-409, 2004.
Y.Matsumoto et. al, Japanese morphological analysis system
chasen manual, NAIST Technical Report, 1997.
D.W.Oard, Topic tracking with the PRISE information re-
trieval system, Proc. of the DARPA Workshop, pp. 94-
101, 1999.
D.Pelleg and A.Moore, X-means: Extending K-means with
efficient estimation of the number of clusters, Proc. of
ICML’00, pp. 727-734, 2000.
H.Schmid, Improvements in part-of-speech tagging with an
application to german, Proc. of the EACL SIGDAT Work-
shop, 1995.
K.Wagstaff et. al, Constrained K-means clustering with
background knowledge, Proc. of ICML’01, pp. 577-584,
2001.
Y.Yang et. al, Improving text categorization methods for
event tracking, Proc. of SIGIR’00, pp. 65-72, 2000.
</reference>
<figureCaption confidence="0.989497">
Figure 6: Prec. with different period of days
</figureCaption>
<figure confidence="0.998568421052632">
90
80
60
40
20
10
5
2
1
Miss Probability (in %)
Random Performance
BIC (with classifying)
k=0
k=100
㪏㪇
㪍㪇
㪋㪇
㪉㪇
㪇
</figure>
<page confidence="0.947461">
238
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.741739">
<title confidence="0.997501">Using Bilingual Comparable Corpora and Semi-supervised Clustering for Topic Tracking</title>
<author confidence="0.904089">Fumiyo Fukumoto</author>
<affiliation confidence="0.954384">Interdisciplinary Graduate School of Medicine and Engineering Univ. of Yamanashi</affiliation>
<email confidence="0.979992">fukumoto@yamanashi.ac.jp</email>
<author confidence="0.977804">Yoshimi Suzuki</author>
<affiliation confidence="0.993102">Interdisciplinary Graduate School of Medicine and Engineering Univ. of Yamanashi</affiliation>
<email confidence="0.973838">ysuzuki@yamanashi.ac.jp</email>
<abstract confidence="0.997192214285714">We address the problem dealing with and propose a method for estimating effective training stories for the topic tracking task. For a small number of labelled positive stories, we extract story pairs which consist of positive and its associated stories from bilingual comparable corpora. To overcome the problem of a large number of labelled negative stories, we classify them into some clusters. This done by using with EM. The results on the TDT corpora show the effectiveness of the method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
</authors>
<title>R.Papka and V.Lavrenko, On-line new event detection and tracking,</title>
<date>1998</date>
<booktitle>Proc. of the DARPA Workshop,</booktitle>
<marker>Allan, 1998</marker>
<rawString>J.Allan and R.Papka and V.Lavrenko, On-line new event detection and tracking, Proc. of the DARPA Workshop, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
</authors>
<title>V.Lavrenko and R.Nallapti, UMass at TDT</title>
<date>2002</date>
<booktitle>Proc. of TDT Workshop,</booktitle>
<marker>Allan, 2002</marker>
<rawString>J.Allan and V.Lavrenko and R.Nallapti, UMass at TDT 2002, Proc. of TDT Workshop, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Basu</author>
</authors>
<title>A.Banerjee and R.Mooney, Semi-supervised clustering by seeding,</title>
<date>2002</date>
<booktitle>Proc. of ICML’02,</booktitle>
<contexts>
<context position="6613" citStr="Basu, 2002" startWordPosition="1069" endWordPosition="1070">age similarities of terms. However, most of them rely on only two stories in question to estimate whether or not they are about the same topic. We use multiple-links among stories to produce optimal results. In the TDT tracking task, classifying negative stories into meaningful groups is also an important issue to track topics, since a large number of labelled negative stories are available in the TDT context. Basu et. al. proposed a method using k-means clustering with the EM algorithm, where labeled data provides prior information about the conditional distribution of hidden category labels(Basu, 2002). They reported that the method outperformed the standard random seeding and COP-k-means(Wagstaff, 2001). Our method shares the basic idea with Basu et. al. An important difference with their method is that our method does not require the number of clusters k in advance, since it is determined during clustering. We use the BIC as the splitting criterion, and estimate the proper number for k. It is an important feature because in the tracking task, no knowledge of the number of topics in the negative training stories is available. 3 System Description The system consists of four procedures: ext</context>
<context position="14378" citStr="Basu, 2002" startWordPosition="2379" endWordPosition="2380">nformation about the target topic. We therefore extracted monolingual(Japanese) story pairs and added them to the training stories. Extracting Japanese monolingual story pairs is quite simple: Let Jj (1 G j G m&apos;) be the extracted Japanese story in the procedure, extracting bilingual story pairs. We calculate cosine similarities between Jj and Jk(1 G k G n). If the value of similarity between them is larger than a certain threshold, we add Jk to the training stories. 3.3 Clustering Negative Stories Our method for classifying negative stories into some clusters is based on Basu et. al.’s method(Basu, 2002) which uses k-means with the EM algorithm. K-means is a clustering algorithm based on iterative relocation that partitions a dataset into the number of k clusters, locally minimizing the average squared distance between the data points and the cluster centers(centroids). Suppose we classify X = { x1, · · ·, xN}, xi E Rd into k clusters: one is the cluster which consists of positive stories, and other k-1 clusters consist of negative stories. Here, which clusters does each negative story belong to? The EM is a method of finding the maximum-likelihood estimate(MLE) of the parameters of an underl</context>
</contexts>
<marker>Basu, 2002</marker>
<rawString>S.Basu and A.Banerjee and R.Mooney, Semi-supervised clustering by seeding, Proc. of ICML’02, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
</authors>
<title>CMU report on TDT-2: segmentation, detection and tracking,</title>
<date>1999</date>
<booktitle>Proc. of the DARPA Workshop,</booktitle>
<marker>Carbonell, 1999</marker>
<rawString>J.Carbonell et. al, CMU report on TDT-2: segmentation, detection and tracking, Proc. of the DARPA Workshop, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling,</title>
<date>1996</date>
<booktitle>Proc. of the ACL’96,</booktitle>
<pages>310--318</pages>
<marker>Chen, Goodman, 1996</marker>
<rawString>S.F.Chen and J.Goodman, An empirical study of smoothing techniques for language modeling, Proc. of the ACL’96, pp. 310-318, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Collier</author>
</authors>
<title>H.Hirakawa and A.Kumano, Machine translation vs. dictionary term translation - a comparison for English-Japanese news article alignment,</title>
<date>2002</date>
<booktitle>Proc. of COLING’02,</booktitle>
<pages>263--267</pages>
<marker>Collier, 2002</marker>
<rawString>N.Collier and H.Hirakawa and A.Kumano, Machine translation vs. dictionary term translation - a comparison for English-Japanese news article alignment, Proc. of COLING’02, pp. 263-267, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>Termight K Church</author>
</authors>
<title>Coordinating humans and machines in bilingual terminology acquisition,</title>
<date>1997</date>
<journal>Journal of MT,</journal>
<volume>20</volume>
<pages>89--107</pages>
<marker>Dagan, Church, 1997</marker>
<rawString>I.Dagan and K.Church, Termight: Coordinating humans and machines in bilingual terminology acquisition, Journal of MT, Vol. 20, No. 1, pp. 89-107, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Franz</author>
<author>J S McCarley</author>
</authors>
<title>Unsupervised and supervised clustering for topic tracking,</title>
<date>2001</date>
<booktitle>Proc. of SIGIR’01,</booktitle>
<pages>310--317</pages>
<marker>Franz, McCarley, 2001</marker>
<rawString>M.Franz and J.S.McCarley, Unsupervised and supervised clustering for topic tracking, Proc. of SIGIR’01, pp. 310-317, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Larkey</author>
</authors>
<title>Language-specific model in multilingual topic tracking,</title>
<date>2004</date>
<booktitle>Proc. of SIGIR’04,</booktitle>
<pages>402--409</pages>
<contexts>
<context position="1463" citStr="Larkey, 2004" startWordPosition="222" endWordPosition="223">od. 1 Introduction With the exponential growth of information on the Internet, it is becoming increasingly difficult to find and organize relevant materials. Topic Tracking defined by the TDT project is a research area to attack the problem. It starts from a few sample stories and finds all subsequent stories that discuss the target topic. Here, a topic in the TDT context is something that happens at a specific place and time associated with some specific actions. A wide range of statistical and ML techniques have been applied to topic tracking(Carbonell et. al, 1999; Oard, 1999; Franz, 2001; Larkey, 2004). The main task of these techniques is to tune the parameters or the threshold to produce optimal results. However, parameter tuning is a tricky issue for tracking(Yang, 2000) because the number of initial positive training stories is very small (one to four), and topics are localized in space and time. For example, ‘Taipei Mayoral Elections’ and ‘U.S. Mid-term Elections’ are topics, but ‘Elections’ is not a topic. Therefore, the system needs to estimate whether or not the test stories are the same topic with few information about the topic. Moreover, the training data is skewed data, i.e. the</context>
</contexts>
<marker>Larkey, 2004</marker>
<rawString>L.S.Larkey et. al, Language-specific model in multilingual topic tracking, Proc. of SIGIR’04, pp. 402-409, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumoto</author>
</authors>
<title>Japanese morphological analysis system chasen manual, NAIST</title>
<date>1997</date>
<tech>Technical Report,</tech>
<contexts>
<context position="20113" citStr="Matsumoto, 1997" startWordPosition="3406" endWordPosition="3407">ic. Bold font stands for the topic which happened in Japan. The evaluation of annotation is made by three humans. The classification is determined to be correct if the majority of three human judges agree. 4.2 Experiments Set Up The English data we used for extracting terms is Reuters’96 corpus(806,791 stories) including TDT1 and TDT3 corpora. The Japanese data was 1,874,947 stories from 14 years(from 1991 to 2004) Mainichi newspapers(1,499,936 stories), and 3 years(1994, 1995, and 1998) Yomiuri newspapers(375,011 stories). All Japanese stories were tagged by the morphological analysis Chasen(Matsumoto, 1997). English stories were tagged by a part-of-speech tagger(Schmid, 1995), and stop word removal. We applied n-gram model with Church-Gale smoothing to noun words, and selected terms whose probabilities are higher than a certain threshold4. As a result, we obtained 338,554 Japanese and 130,397 English terms. We used the EDR bilingual dictionary, and translated Japanese terms into English. Some of the words had no translation. For these, we estimated term correspondences. Each story is represented as a vector of terms with tf·idf weights. We calculated story similarities and extracted story pairs </context>
</contexts>
<marker>Matsumoto, 1997</marker>
<rawString>Y.Matsumoto et. al, Japanese morphological analysis system chasen manual, NAIST Technical Report, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W Oard</author>
</authors>
<title>Topic tracking with the PRISE information retrieval system,</title>
<date>1999</date>
<booktitle>Proc. of the DARPA Workshop,</booktitle>
<pages>94--101</pages>
<contexts>
<context position="1435" citStr="Oard, 1999" startWordPosition="218" endWordPosition="219">effectiveness of the method. 1 Introduction With the exponential growth of information on the Internet, it is becoming increasingly difficult to find and organize relevant materials. Topic Tracking defined by the TDT project is a research area to attack the problem. It starts from a few sample stories and finds all subsequent stories that discuss the target topic. Here, a topic in the TDT context is something that happens at a specific place and time associated with some specific actions. A wide range of statistical and ML techniques have been applied to topic tracking(Carbonell et. al, 1999; Oard, 1999; Franz, 2001; Larkey, 2004). The main task of these techniques is to tune the parameters or the threshold to produce optimal results. However, parameter tuning is a tricky issue for tracking(Yang, 2000) because the number of initial positive training stories is very small (one to four), and topics are localized in space and time. For example, ‘Taipei Mayoral Elections’ and ‘U.S. Mid-term Elections’ are topics, but ‘Elections’ is not a topic. Therefore, the system needs to estimate whether or not the test stories are the same topic with few information about the topic. Moreover, the training d</context>
</contexts>
<marker>Oard, 1999</marker>
<rawString>D.W.Oard, Topic tracking with the PRISE information retrieval system, Proc. of the DARPA Workshop, pp. 94-101, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pelleg</author>
</authors>
<title>A.Moore, X-means: Extending K-means with efficient estimation of the number of clusters,</title>
<date>2000</date>
<booktitle>Proc. of ICML’00,</booktitle>
<pages>727--734</pages>
<marker>Pelleg, 2000</marker>
<rawString>D.Pelleg and A.Moore, X-means: Extending K-means with efficient estimation of the number of clusters, Proc. of ICML’00, pp. 727-734, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Improvements in part-of-speech tagging with an application to german,</title>
<date>1995</date>
<booktitle>Proc. of the EACL SIGDAT Workshop,</booktitle>
<contexts>
<context position="20183" citStr="Schmid, 1995" startWordPosition="3415" endWordPosition="3416">n of annotation is made by three humans. The classification is determined to be correct if the majority of three human judges agree. 4.2 Experiments Set Up The English data we used for extracting terms is Reuters’96 corpus(806,791 stories) including TDT1 and TDT3 corpora. The Japanese data was 1,874,947 stories from 14 years(from 1991 to 2004) Mainichi newspapers(1,499,936 stories), and 3 years(1994, 1995, and 1998) Yomiuri newspapers(375,011 stories). All Japanese stories were tagged by the morphological analysis Chasen(Matsumoto, 1997). English stories were tagged by a part-of-speech tagger(Schmid, 1995), and stop word removal. We applied n-gram model with Church-Gale smoothing to noun words, and selected terms whose probabilities are higher than a certain threshold4. As a result, we obtained 338,554 Japanese and 130,397 English terms. We used the EDR bilingual dictionary, and translated Japanese terms into English. Some of the words had no translation. For these, we estimated term correspondences. Each story is represented as a vector of terms with tf·idf weights. We calculated story similarities and extracted story pairs between positive and its associated stories5. In 4The threshold value </context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>H.Schmid, Improvements in part-of-speech tagging with an application to german, Proc. of the EACL SIGDAT Workshop, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wagstaff</author>
</authors>
<title>Constrained K-means clustering with background knowledge,</title>
<date>2001</date>
<booktitle>Proc. of ICML’01,</booktitle>
<pages>577--584</pages>
<contexts>
<context position="6717" citStr="Wagstaff, 2001" startWordPosition="1082" endWordPosition="1083">ether or not they are about the same topic. We use multiple-links among stories to produce optimal results. In the TDT tracking task, classifying negative stories into meaningful groups is also an important issue to track topics, since a large number of labelled negative stories are available in the TDT context. Basu et. al. proposed a method using k-means clustering with the EM algorithm, where labeled data provides prior information about the conditional distribution of hidden category labels(Basu, 2002). They reported that the method outperformed the standard random seeding and COP-k-means(Wagstaff, 2001). Our method shares the basic idea with Basu et. al. An important difference with their method is that our method does not require the number of clusters k in advance, since it is determined during clustering. We use the BIC as the splitting criterion, and estimate the proper number for k. It is an important feature because in the tracking task, no knowledge of the number of topics in the negative training stories is available. 3 System Description The system consists of four procedures: extracting bilingual story pairs, extracting monolingual story pairs, clustering negative stories, and trac</context>
</contexts>
<marker>Wagstaff, 2001</marker>
<rawString>K.Wagstaff et. al, Constrained K-means clustering with background knowledge, Proc. of ICML’01, pp. 577-584, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
</authors>
<title>Improving text categorization methods for event tracking,</title>
<date>2000</date>
<booktitle>Proc. of SIGIR’00,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="1638" citStr="Yang, 2000" startWordPosition="251" endWordPosition="252">ned by the TDT project is a research area to attack the problem. It starts from a few sample stories and finds all subsequent stories that discuss the target topic. Here, a topic in the TDT context is something that happens at a specific place and time associated with some specific actions. A wide range of statistical and ML techniques have been applied to topic tracking(Carbonell et. al, 1999; Oard, 1999; Franz, 2001; Larkey, 2004). The main task of these techniques is to tune the parameters or the threshold to produce optimal results. However, parameter tuning is a tricky issue for tracking(Yang, 2000) because the number of initial positive training stories is very small (one to four), and topics are localized in space and time. For example, ‘Taipei Mayoral Elections’ and ‘U.S. Mid-term Elections’ are topics, but ‘Elections’ is not a topic. Therefore, the system needs to estimate whether or not the test stories are the same topic with few information about the topic. Moreover, the training data is skewed data, i.e. there is a large number of labelled negative stories compared to positive ones. The system thus needs to balance the amount of positive and negative training stories not to hampe</context>
</contexts>
<marker>Yang, 2000</marker>
<rawString>Y.Yang et. al, Improving text categorization methods for event tracking, Proc. of SIGIR’00, pp. 65-72, 2000.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>