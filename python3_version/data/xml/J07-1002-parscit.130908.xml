<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000906">
<title confidence="0.975201">
Squibs and Discussions
Identifying Sources of Disagreement:
Generalizability Theory in Manual
Annotation Studies
</title>
<author confidence="0.998855">
Petra Saskia Bayerl* Karsten Ingmar Paul†
</author>
<affiliation confidence="0.999084">
University of Oklahoma University of Erlangen-Nuremberg
</affiliation>
<bodyText confidence="0.98651">
Many annotation projects have shown that the quality of manual annotations often is not as
good as would be desirable for reliable data analysis. Identifying the main sources responsible
for poor annotation quality must thus be a major concern. Generalizability theory is a valuable
tool for this purpose, because it allows for the differentiation and detailed analysis offactors that
influence annotation quality. In this article we will present basic concepts of Generalizability
Theory and give an example for its application based on published data.
</bodyText>
<sectionHeader confidence="0.99518" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999324428571429">
Manual annotations are still a major source of information in many small- and large-
scale projects in diverse areas of corpus and computational linguistics. Often, however,
manual annotations are not reliable enough for a given application. Measures must
be taken to increase and secure the consistency of linguistic annotations, if analyses
and applications are not to suffer from low data quality. Because a multitude of fac-
tors may be responsible for inadequate reliability, a method is needed that is able to
simultaneously consider a variety of probable factors and indicate those that are mainly
responsible for low reliability in a given case. Generalizability Theory, or G-Theory
(Cronbach et al. 1972), is a methodological framework specifically designed for this
purpose. Because it is not restricted to any type of data or study design, it can be of
great use in any kind of manual annotation project that needs to systematically identify
sources of annotator disagreement. In this article we provide an outline of the approach
and its basic assumptions and demonstrate its application based on an annotation study
done by Shriberg and Lof (1991).1
</bodyText>
<footnote confidence="0.630717428571429">
* Department of Psychology, Tulsa Graduate College, 4502 East 41st Street, Tulsa, OK 74135, USA.
† Chair of Psychology, especially Organizational and Social Psychology (Prof. Dr. Moser), Lange Gasse 20,
90403 Nuremberg, Germany.
1 A more comprehensive introduction to G-Theory is provided in a longer version of this article, which is
available from the authors.
The work for this article was done at the Department for Applied and Computational Linguistics,
Justus-Liebig-University Giessen, Germany.
</footnote>
<note confidence="0.5368435">
© 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 1
</note>
<sectionHeader confidence="0.749211" genericHeader="method">
2. The G-Theory Approach
</sectionHeader>
<bodyText confidence="0.999744166666667">
Reliability in G-Theory is defined by the amount of variation or variance observed in
annotations; the lower the total variance in the data, the higher is its reliability. G-Theory
further assumes that data reliability is influenced by several independent factors or
facets, which are, individually as well as in interaction, responsible for the observed
variation.2 Sources of variation might be idiosyncratic behaviors of individual annota-
tors or external influences like alterations in the tools used for annotations, increasing
time pressure, removal or adding of rewards, or changes in the annotation scheme. Each
of these influences can lead to systematic changes in an annotator’s behavior and so
to higher disagreement among annotators. According to G-Theory each possible facet,
annotator, tools, rewards, and so on, will have its own independent impact on the quality,
that is, reliability, of annotations. The task of a G-study is to isolate the influence of
single facets and determine the degree of their impact.
</bodyText>
<subsectionHeader confidence="0.997673">
2.1 Basic G-Study Designs
</subsectionHeader>
<bodyText confidence="0.99998825">
The main distinction with respect to G-study designs is the choice between a crossed
and a (partially) nested design. In crossed designs measurements are obtained for
each possible combination of facet values. Given two facets, items and coders, each
individual item (phrase, phone, gesture, etc.) is annotated by all possible coders, so
that each value of the item facet is measured on every value of the coders facet. Nested
designs, in contrast, only measure a subset of possible combinations of facet values,
for instance, when limited resources determine that only some of the coders annotate
the same objects on more than one occasion. In general, fully crossed designs require
a higher number of observations, but also provide more information. To obtain a full
picture of possible influences crossed designs should therefore be preferred. For a
detailed discussion of G-study designs, including unbalanced designs or missing data,
and random and fixed facets, see, for example, Brennan (2001).
</bodyText>
<subsectionHeader confidence="0.99942">
2.2 Estimating Variance Components
</subsectionHeader>
<bodyText confidence="0.9975705">
In fully crossed designs the total variance in the data is a result of individual facets as
well as their interactions. Because G-Theory assumes independence of facets, effects of
components are additive. Given three facets a, b, c, the total variance σ2(Xabc) therefore
is calculated as
</bodyText>
<equation confidence="0.9453505">
σ2(Xabc) = σ2 a + σ2 b + σ2 c + σ2 ab + σ2 ac + σ2 bc + σ2 (1)
abc,e
</equation>
<bodyText confidence="0.993909">
where σ2 refers to variance and the subscripts to the name of one or more facets.
The subscript e in the last variance component denotes error variance. In nested de-
signs some facets cannot be determined as independent terms due to their confounding
with other facets. For instance, in a nested design with three factors a, b, c, different
2 This definition of reliability differs from the traditional true-score-model of classical reliability theory
(Spearman 1904) and can be considered a modern approach to the question of consistency or
’dependability’ of data measurement. A discussion of the conceptual differences is beyond the scope
of this article. Information on this topic may be found in Thompson (2002) or Matt (2001).
</bodyText>
<page confidence="0.971917">
4
</page>
<note confidence="0.450814">
Bayerl and Paul Identifying Sources of Disagreement
</note>
<bodyText confidence="0.9998208">
values of c may be associated with different values of b. Here the effect for c will be
confounded both with bc and the residual term abc,e so that no independent term for
the c facet can be obtained. Instead of the seven variance components in the crossed
design only five variance components can be calculated, again stressing the fact that
nested designs provide less information than fully crossed designs
</bodyText>
<equation confidence="0.886533">
σ2(Xabc) = σ2a + σ2b + σ2ab + σ2c,cb + σ2 (2)
ac,abc,e
</equation>
<bodyText confidence="0.9992015">
For information on the mathematical foundation of G-Theory and the derivation of
estimates see Cronbach et al. (1972) and Brennan (2001).
</bodyText>
<subsectionHeader confidence="0.997175">
2.3 Interpreting Variance Components
</subsectionHeader>
<bodyText confidence="0.9999898">
Based on the assumption that the total variance is a sum of single variance components,
the total variance is 100%. The relative magnitude of each component with respect to
the total variance is an indicator of the individual contribution of this component with
respect to overall (un)reliability. A facet explaining 60% of the total variance would thus
be considered a major source of variation in contrast to a minor facet explaining only 5%
of the variance. For instance, given that the coder facet is the largest facet, variation can
be explained through systematic differences in the annotation behavior of individual
coders—for example, annotators differ in their tendency to set prosodic boundaries in
utterances leading to systematic differences in the number of boundaries placed. In this
case retraining of annotators to reach a more comparable behavior would be advisable.
A high schema component indicates that there is systematic variation in the use of
categories, whereas a high coder–schema interaction indicates systematic differences in
annotators’ use of these categories; for example, coders annotating rhetorical (RST)
relations could differ in the frequency with which they use individual relations such as
’background’, ’concession’, ’evidence’, and so forth, pointing to possible problems with
the interpretation of rhetorical relations and their application. Variation mainly due to
the item facet indicates that certain materials are harder to annotate than others. Such a
result would imply retraining or elimination of overly difficult material. In consequence,
the identification of distinct sources of variation should lead to specifically designed
steps for improvement.
</bodyText>
<sectionHeader confidence="0.886058" genericHeader="method">
3. A Re-Analysis of Shriberg and Lof (1991)
</sectionHeader>
<bodyText confidence="0.999972666666667">
As an illustration for the application of G-Theory we reanalyzed data provided by
Shriberg and Lof (1991), who studied the accuracy of broad and narrow phonetic tran-
scriptions. In Set A of their study they investigated four facets: annotation scheme (type
of consonant, C), granularity (broad vs. narrow transcription G), material (continuous
speech vs. articulation test, M), and annotation team (T). Data in Set A were given as
agreement percentages. Our G-study results are shown in Table 1.
Traditionally, reliability concerns focus on disagreements among individual anno-
tators assuming that variation is due to incommensurable annotator behavior. In our
case, however, the team facet explains only a very small percentage of variance both
as an individual factor and in interaction with other factors. This suggests that the
four annotation teams are comparable in their annotation quality. The major factors
responsible for the observed variance are granularity and type of consonants. Material
</bodyText>
<page confidence="0.952123">
5
</page>
<table confidence="0.581365">
Computational Linguistics Volume 33, Number 1
</table>
<tableCaption confidence="0.994941">
Table 1
</tableCaption>
<table confidence="0.9698136">
G-Study results for Shriberg and Lof (1991), Table 8, Set A.
Effect df Variance Percentage of
components estimates total variance
Consonant (C) 23 234.86877 25.70
Granularity (G) 1 312.80278 34.23
Team (T) 3 3.70906 0.41
Material (M) 1 0.0 [−3.08672]* –
CG 23 99.18526 10.85
CT 69 0.0 [−8.25984]* –
CM 23 45.80498 5.01
GT 3 0.0 [−1.12263]* –
GM 1 0.0 [−6.05138]* –
TM 3 0.0 [−1.74740]* –
CGT 69 3.84207 0.42
CGM 23 111.61108 12.12
CTM 69 57.64646 6.31
GTM 3 6.04318 0.66
CGTM,e 36 38.23065 4.18
913.74429 99.99
* Values set zero, original negative estimates in brackets.
</table>
<subsectionHeader confidence="0.640101">
For the analysis the GENOVA program as described in Brennan (2001) was used.
</subsectionHeader>
<bodyText confidence="0.999770842105263">
does not exhibit a substantial individual influence on reliability, but becomes relevant
in the CGM-interaction. Our G-study therefore reveals that unreliability in Shriberg and
Lof’s data is caused not by idiosyncrasies of individuals, but due to the characteristics
of the task, namely, granularity and scheme.
Having identified the critical facets, it might now be interesting to look at the values
of these facets that are especially prone to produce disagreement. Because we operated
with agreement data, this information can be easily obtained from the data entered into
the analysis. Because neither team nor material are major sources for variance, we only
have to examine the values for granularity and consonants. Due to the same reason
we can base the comparison on mean values over teams and material types. For the
granularity facet we find overall lower agreement in narrow transcriptions (64.15%)
compared to broad transcriptions (89.46%). On the consonant facet we can differentiate
critical phonemes such as /6/ or /f/ from uncritical ones (e. g., /j/, /b/). Interpreting
the CG-interaction in this light, disagreement on consonants in narrow transcriptions
seems to be comparably higher than in broad transcriptions. Implications from this
study would be that the selection of annotators and the training of annotation teams are
successful in producing comparable results. For high reliability, however, transcriptions
should be done on a broad level with specific training for difficult consonants and some
special care for material from articulation tests (see CGM-interaction).
</bodyText>
<sectionHeader confidence="0.936077" genericHeader="method">
4. Practical Considerations in Planning G-Studies
</sectionHeader>
<bodyText confidence="0.999506666666667">
In planning and conducting a G-study some deliberation is necessary to achieve inter-
pretable results. Foremost, the overall quality of the G-study depends on the choice of
factors that completely and accurately represent the situation of the annotators. As it is
</bodyText>
<page confidence="0.998145">
6
</page>
<note confidence="0.707579">
Bayerl and Paul Identifying Sources of Disagreement
</note>
<bodyText confidence="0.999950285714285">
quite easy to overlook relevant but rather inconspicuous factors like minor changes in
the annotation tool or increasing time pressure due to upcoming project deadlines, the
choice of correct facets relies heavily on the experience and knowledge of the researcher.
The statistical results, however, will give indications for likely misspecifications of facets
by showing a high error or rest variance σe for the tested model. Theoretically, the
number of facets that can be included in a G-study is unlimited. Having more than four
or five facets in one study might make the final interpretation overly complex, however.
Even though there is no minimum necessary number of observations, missing data due
to a low number of observations pose a problem for model interpretation. Approaches
to deal with such unbalanced designs are given by Brennan (2001) and Chiu and Wolfe
(2003). Additionally, there is no clear-cut rule when a component might be considered
’too small’ to be of importance. As a rule of thumb a component of less than 8% might
be considered ’small’, but the decision remains one of ’relative importance’ depending
on the distribution of explained variance across components.
</bodyText>
<sectionHeader confidence="0.804955" genericHeader="evaluation">
5. G-Theory and Agreement Indices
</sectionHeader>
<bodyText confidence="0.999979136363636">
Two well-known measures for capturing the quality of manual annotations are agree-
ment percentages and the kappa statistic (Cohen 1960; Carletta 1996; Eugenio and
Glass 2004). Both measures provide a “summary index” (Agresti 1992) that expresses
the degree of (dis)agreement among coders. Where the calculation of percentages and
kappa provides a measure for overall reliability (or reliability indices for individual
facets), G-Theory has been designed to analyze multiple possible influencing factors in
a single run and to compare the relative importance of components among each other.
Shriberg and Lof (1991), for instance, compare narrow and broad transcriptions using
graphics that show the agreement percentages for each consonant for both transcrip-
tion types, arguing that overall narrow transcriptions seem unreliable. Based on their
experience with the study context they further assumed that these differences were not
due to annotator training, behaviors, or experience. They did not provide any direct
evidence, however. The G-study presented in this article could prove both assumptions
in a single run. It further allows us to investigate the interactions and mutual influences
of these factors, thus clearly exceeding the possibilities of summary statistics. As we
have seen in the example, G-Theory, however, does not provide answers as to which
values of a facet are responsible for higher or lower reliability. This information must
be obtained by a review of the data. Agreement indices and G-Theory should thus
not be seen as competing, but rather as complementary, approaches. Kappa can serve
as a first approximation to the degree of disagreement present in the data, whereas
G-Theory in a second step investigates the underlying reasons of inadequate reliability
and subsequently guides efforts to improve reliability.
</bodyText>
<sectionHeader confidence="0.997997" genericHeader="conclusions">
6. Final Remarks
</sectionHeader>
<bodyText confidence="0.999949142857143">
Generalizability theory is a valuable approach for identifying problematic areas in
annotation projects. The investigation of multiple facets at the same time can provide a
clearer understanding of reasons underlying insufficient annotation quality and subse-
quently offer avenues to its improvement. In this article we could not give more than
a passing glance over the possibilities provided by the G-Theory approach. For the
interested reader, Shavelson and Webb (1981) give a good introduction into the material.
Further references are provided throughout the article and in the reference section.
</bodyText>
<page confidence="0.998154">
7
</page>
<note confidence="0.643353">
Computational Linguistics Volume 33, Number 1
</note>
<sectionHeader confidence="0.953002" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995902537037037">
Agresti, Alan. 1992. Modelling patterns
of agreement and disagreement.
Statistical Methods in Medical Research,
1(2):201–218.
Brennan, Robert L. 2001. Generalizability
Theory. Statistics for Social Science and
Public Policy. Springer Verlag, New York.
Carletta, Jean. 1996. Assessing agreement
on classification tasks: The kappa
statistic. Computational Linguistics,
22(2):249–254.
Chiu, Christopher W. T. and Edward W.
Wolfe. 2003. A method for analyzing
sparse data matrices in the generalizability
theory framework. Applied Psychological
Measurement, 26(3):321–338.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20(1):37–46.
Cronbach, Lee J., Goldine C. Gleser, Harinder
Nanda, and Nageswari Rajaratnam.
1972. The Dependability of Behavioral
Measurements: Theory of Generalizability
for Scores and Profiles. John Wiley &amp; Sons,
Inc., New York.
Eugenio, Barbara Di and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95–101.
Matt, Georg E. 2001. Generalizabilty theory.
In Neil J. Smelser and Paul B. Baltes,
editors, International Encyclopedia of the
Social and Behavioral Sciences. Elsevier,
Oxford.
Shavelson, Richard J. and Noreen M. Webb.
1981. Generalizability Theory: A Primer. Sage
Publications, Newbury Park, London,
New Delhi.
Shriberg, Lawrence D. and Gregory L. Lof.
1991. Reliability studies in broad and
narrow phonetic transcription. Clinical
Linguistics and Phonetics, 5(3):225–279.
Spearman, Charles. 1904. The proof and
measurement of association between two
things. American Journal of Psychology,
15:72–101.
Thompson, Bruce. 2002. A brief introduction
to Generalizability Theory. In Bruce
Thompson, editor, Score Reliability:
Contemporary Thinking on Reliability Issues.
Sage Publication, Thousand Oaks, CA,
pages 43–58.
Statistical Software
• GENOVA, urGENOVA, mGENOVA: Available online at
http://www.education.uiowa.edu/casma/GenovaPrograms.htm
</reference>
<page confidence="0.998209">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.761887">
<title confidence="0.9983885">Squibs and Discussions Identifying Sources of Disagreement: Generalizability Theory in Manual Annotation Studies</title>
<author confidence="0.972636">Saskia Ingmar</author>
<affiliation confidence="0.989799">University of Oklahoma University of Erlangen-Nuremberg</affiliation>
<abstract confidence="0.963401833333333">Many annotation projects have shown that the quality of manual annotations often is not as good as would be desirable for reliable data analysis. Identifying the main sources responsible for poor annotation quality must thus be a major concern. Generalizability theory is a valuable tool for this purpose, because it allows for the differentiation and detailed analysis offactors that influence annotation quality. In this article we will present basic concepts of Generalizability Theory and give an example for its application based on published data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan Agresti</author>
</authors>
<title>Modelling patterns of agreement and disagreement.</title>
<date>1992</date>
<journal>Statistical Methods in Medical Research,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="13306" citStr="Agresti 1992" startWordPosition="2061" endWordPosition="2062">nan (2001) and Chiu and Wolfe (2003). Additionally, there is no clear-cut rule when a component might be considered ’too small’ to be of importance. As a rule of thumb a component of less than 8% might be considered ’small’, but the decision remains one of ’relative importance’ depending on the distribution of explained variance across components. 5. G-Theory and Agreement Indices Two well-known measures for capturing the quality of manual annotations are agreement percentages and the kappa statistic (Cohen 1960; Carletta 1996; Eugenio and Glass 2004). Both measures provide a “summary index” (Agresti 1992) that expresses the degree of (dis)agreement among coders. Where the calculation of percentages and kappa provides a measure for overall reliability (or reliability indices for individual facets), G-Theory has been designed to analyze multiple possible influencing factors in a single run and to compare the relative importance of components among each other. Shriberg and Lof (1991), for instance, compare narrow and broad transcriptions using graphics that show the agreement percentages for each consonant for both transcription types, arguing that overall narrow transcriptions seem unreliable. B</context>
</contexts>
<marker>Agresti, 1992</marker>
<rawString>Agresti, Alan. 1992. Modelling patterns of agreement and disagreement. Statistical Methods in Medical Research, 1(2):201–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert L Brennan</author>
</authors>
<title>Generalizability Theory. Statistics for Social Science and Public Policy.</title>
<date>2001</date>
<publisher>Springer Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="4614" citStr="Brennan (2001)" startWordPosition="698" endWordPosition="699"> every value of the coders facet. Nested designs, in contrast, only measure a subset of possible combinations of facet values, for instance, when limited resources determine that only some of the coders annotate the same objects on more than one occasion. In general, fully crossed designs require a higher number of observations, but also provide more information. To obtain a full picture of possible influences crossed designs should therefore be preferred. For a detailed discussion of G-study designs, including unbalanced designs or missing data, and random and fixed facets, see, for example, Brennan (2001). 2.2 Estimating Variance Components In fully crossed designs the total variance in the data is a result of individual facets as well as their interactions. Because G-Theory assumes independence of facets, effects of components are additive. Given three facets a, b, c, the total variance σ2(Xabc) therefore is calculated as σ2(Xabc) = σ2 a + σ2 b + σ2 c + σ2 ab + σ2 ac + σ2 bc + σ2 (1) abc,e where σ2 refers to variance and the subscripts to the name of one or more facets. The subscript e in the last variance component denotes error variance. In nested designs some facets cannot be determined as</context>
<context position="6395" citStr="Brennan (2001)" startWordPosition="1001" endWordPosition="1002">Disagreement values of c may be associated with different values of b. Here the effect for c will be confounded both with bc and the residual term abc,e so that no independent term for the c facet can be obtained. Instead of the seven variance components in the crossed design only five variance components can be calculated, again stressing the fact that nested designs provide less information than fully crossed designs σ2(Xabc) = σ2a + σ2b + σ2ab + σ2c,cb + σ2 (2) ac,abc,e For information on the mathematical foundation of G-Theory and the derivation of estimates see Cronbach et al. (1972) and Brennan (2001). 2.3 Interpreting Variance Components Based on the assumption that the total variance is a sum of single variance components, the total variance is 100%. The relative magnitude of each component with respect to the total variance is an indicator of the individual contribution of this component with respect to overall (un)reliability. A facet explaining 60% of the total variance would thus be considered a major source of variation in contrast to a minor facet explaining only 5% of the variance. For instance, given that the coder facet is the largest facet, variation can be explained through sy</context>
<context position="9874" citStr="Brennan (2001)" startWordPosition="1536" endWordPosition="1537">iberg and Lof (1991), Table 8, Set A. Effect df Variance Percentage of components estimates total variance Consonant (C) 23 234.86877 25.70 Granularity (G) 1 312.80278 34.23 Team (T) 3 3.70906 0.41 Material (M) 1 0.0 [−3.08672]* – CG 23 99.18526 10.85 CT 69 0.0 [−8.25984]* – CM 23 45.80498 5.01 GT 3 0.0 [−1.12263]* – GM 1 0.0 [−6.05138]* – TM 3 0.0 [−1.74740]* – CGT 69 3.84207 0.42 CGM 23 111.61108 12.12 CTM 69 57.64646 6.31 GTM 3 6.04318 0.66 CGTM,e 36 38.23065 4.18 913.74429 99.99 * Values set zero, original negative estimates in brackets. For the analysis the GENOVA program as described in Brennan (2001) was used. does not exhibit a substantial individual influence on reliability, but becomes relevant in the CGM-interaction. Our G-study therefore reveals that unreliability in Shriberg and Lof’s data is caused not by idiosyncrasies of individuals, but due to the characteristics of the task, namely, granularity and scheme. Having identified the critical facets, it might now be interesting to look at the values of these facets that are especially prone to produce disagreement. Because we operated with agreement data, this information can be easily obtained from the data entered into the analysis</context>
<context position="12703" citStr="Brennan (2001)" startWordPosition="1967" endWordPosition="1968">owledge of the researcher. The statistical results, however, will give indications for likely misspecifications of facets by showing a high error or rest variance σe for the tested model. Theoretically, the number of facets that can be included in a G-study is unlimited. Having more than four or five facets in one study might make the final interpretation overly complex, however. Even though there is no minimum necessary number of observations, missing data due to a low number of observations pose a problem for model interpretation. Approaches to deal with such unbalanced designs are given by Brennan (2001) and Chiu and Wolfe (2003). Additionally, there is no clear-cut rule when a component might be considered ’too small’ to be of importance. As a rule of thumb a component of less than 8% might be considered ’small’, but the decision remains one of ’relative importance’ depending on the distribution of explained variance across components. 5. G-Theory and Agreement Indices Two well-known measures for capturing the quality of manual annotations are agreement percentages and the kappa statistic (Cohen 1960; Carletta 1996; Eugenio and Glass 2004). Both measures provide a “summary index” (Agresti 19</context>
</contexts>
<marker>Brennan, 2001</marker>
<rawString>Brennan, Robert L. 2001. Generalizability Theory. Statistics for Social Science and Public Policy. Springer Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="13225" citStr="Carletta 1996" startWordPosition="2049" endWordPosition="2050">interpretation. Approaches to deal with such unbalanced designs are given by Brennan (2001) and Chiu and Wolfe (2003). Additionally, there is no clear-cut rule when a component might be considered ’too small’ to be of importance. As a rule of thumb a component of less than 8% might be considered ’small’, but the decision remains one of ’relative importance’ depending on the distribution of explained variance across components. 5. G-Theory and Agreement Indices Two well-known measures for capturing the quality of manual annotations are agreement percentages and the kappa statistic (Cohen 1960; Carletta 1996; Eugenio and Glass 2004). Both measures provide a “summary index” (Agresti 1992) that expresses the degree of (dis)agreement among coders. Where the calculation of percentages and kappa provides a measure for overall reliability (or reliability indices for individual facets), G-Theory has been designed to analyze multiple possible influencing factors in a single run and to compare the relative importance of components among each other. Shriberg and Lof (1991), for instance, compare narrow and broad transcriptions using graphics that show the agreement percentages for each consonant for both t</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, Jean. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher W T Chiu</author>
<author>Edward W Wolfe</author>
</authors>
<title>A method for analyzing sparse data matrices in the generalizability theory framework.</title>
<date>2003</date>
<journal>Applied Psychological Measurement,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="12729" citStr="Chiu and Wolfe (2003)" startWordPosition="1970" endWordPosition="1973">archer. The statistical results, however, will give indications for likely misspecifications of facets by showing a high error or rest variance σe for the tested model. Theoretically, the number of facets that can be included in a G-study is unlimited. Having more than four or five facets in one study might make the final interpretation overly complex, however. Even though there is no minimum necessary number of observations, missing data due to a low number of observations pose a problem for model interpretation. Approaches to deal with such unbalanced designs are given by Brennan (2001) and Chiu and Wolfe (2003). Additionally, there is no clear-cut rule when a component might be considered ’too small’ to be of importance. As a rule of thumb a component of less than 8% might be considered ’small’, but the decision remains one of ’relative importance’ depending on the distribution of explained variance across components. 5. G-Theory and Agreement Indices Two well-known measures for capturing the quality of manual annotations are agreement percentages and the kappa statistic (Cohen 1960; Carletta 1996; Eugenio and Glass 2004). Both measures provide a “summary index” (Agresti 1992) that expresses the deg</context>
</contexts>
<marker>Chiu, Wolfe, 2003</marker>
<rawString>Chiu, Christopher W. T. and Edward W. Wolfe. 2003. A method for analyzing sparse data matrices in the generalizability theory framework. Applied Psychological Measurement, 26(3):321–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="13210" citStr="Cohen 1960" startWordPosition="2047" endWordPosition="2048">m for model interpretation. Approaches to deal with such unbalanced designs are given by Brennan (2001) and Chiu and Wolfe (2003). Additionally, there is no clear-cut rule when a component might be considered ’too small’ to be of importance. As a rule of thumb a component of less than 8% might be considered ’small’, but the decision remains one of ’relative importance’ depending on the distribution of explained variance across components. 5. G-Theory and Agreement Indices Two well-known measures for capturing the quality of manual annotations are agreement percentages and the kappa statistic (Cohen 1960; Carletta 1996; Eugenio and Glass 2004). Both measures provide a “summary index” (Agresti 1992) that expresses the degree of (dis)agreement among coders. Where the calculation of percentages and kappa provides a measure for overall reliability (or reliability indices for individual facets), G-Theory has been designed to analyze multiple possible influencing factors in a single run and to compare the relative importance of components among each other. Shriberg and Lof (1991), for instance, compare narrow and broad transcriptions using graphics that show the agreement percentages for each conso</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, Jacob. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee J Cronbach</author>
<author>Goldine C Gleser</author>
<author>Harinder Nanda</author>
<author>Nageswari Rajaratnam</author>
</authors>
<title>The Dependability of Behavioral Measurements: Theory of Generalizability for Scores and Profiles.</title>
<date>1972</date>
<publisher>John Wiley &amp; Sons, Inc.,</publisher>
<location>New York.</location>
<contexts>
<context position="1490" citStr="Cronbach et al. 1972" startWordPosition="217" endWordPosition="220"> projects in diverse areas of corpus and computational linguistics. Often, however, manual annotations are not reliable enough for a given application. Measures must be taken to increase and secure the consistency of linguistic annotations, if analyses and applications are not to suffer from low data quality. Because a multitude of factors may be responsible for inadequate reliability, a method is needed that is able to simultaneously consider a variety of probable factors and indicate those that are mainly responsible for low reliability in a given case. Generalizability Theory, or G-Theory (Cronbach et al. 1972), is a methodological framework specifically designed for this purpose. Because it is not restricted to any type of data or study design, it can be of great use in any kind of manual annotation project that needs to systematically identify sources of annotator disagreement. In this article we provide an outline of the approach and its basic assumptions and demonstrate its application based on an annotation study done by Shriberg and Lof (1991).1 * Department of Psychology, Tulsa Graduate College, 4502 East 41st Street, Tulsa, OK 74135, USA. † Chair of Psychology, especially Organizational and </context>
<context position="6376" citStr="Cronbach et al. (1972)" startWordPosition="996" endWordPosition="999">aul Identifying Sources of Disagreement values of c may be associated with different values of b. Here the effect for c will be confounded both with bc and the residual term abc,e so that no independent term for the c facet can be obtained. Instead of the seven variance components in the crossed design only five variance components can be calculated, again stressing the fact that nested designs provide less information than fully crossed designs σ2(Xabc) = σ2a + σ2b + σ2ab + σ2c,cb + σ2 (2) ac,abc,e For information on the mathematical foundation of G-Theory and the derivation of estimates see Cronbach et al. (1972) and Brennan (2001). 2.3 Interpreting Variance Components Based on the assumption that the total variance is a sum of single variance components, the total variance is 100%. The relative magnitude of each component with respect to the total variance is an indicator of the individual contribution of this component with respect to overall (un)reliability. A facet explaining 60% of the total variance would thus be considered a major source of variation in contrast to a minor facet explaining only 5% of the variance. For instance, given that the coder facet is the largest facet, variation can be e</context>
</contexts>
<marker>Cronbach, Gleser, Nanda, Rajaratnam, 1972</marker>
<rawString>Cronbach, Lee J., Goldine C. Gleser, Harinder Nanda, and Nageswari Rajaratnam. 1972. The Dependability of Behavioral Measurements: Theory of Generalizability for Scores and Profiles. John Wiley &amp; Sons, Inc., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di and Michael Glass Eugenio</author>
</authors>
<title>The kappa statistic: A second look.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<marker>Eugenio, 2004</marker>
<rawString>Eugenio, Barbara Di and Michael Glass. 2004. The kappa statistic: A second look. Computational Linguistics, 30(1):95–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georg E Matt</author>
</authors>
<title>Generalizabilty theory.</title>
<date>2001</date>
<booktitle>International Encyclopedia of the Social and Behavioral Sciences.</booktitle>
<editor>In Neil J. Smelser and Paul B. Baltes, editors,</editor>
<publisher>Elsevier,</publisher>
<location>Oxford.</location>
<contexts>
<context position="5738" citStr="Matt (2001)" startWordPosition="889" endWordPosition="890">component denotes error variance. In nested designs some facets cannot be determined as independent terms due to their confounding with other facets. For instance, in a nested design with three factors a, b, c, different 2 This definition of reliability differs from the traditional true-score-model of classical reliability theory (Spearman 1904) and can be considered a modern approach to the question of consistency or ’dependability’ of data measurement. A discussion of the conceptual differences is beyond the scope of this article. Information on this topic may be found in Thompson (2002) or Matt (2001). 4 Bayerl and Paul Identifying Sources of Disagreement values of c may be associated with different values of b. Here the effect for c will be confounded both with bc and the residual term abc,e so that no independent term for the c facet can be obtained. Instead of the seven variance components in the crossed design only five variance components can be calculated, again stressing the fact that nested designs provide less information than fully crossed designs σ2(Xabc) = σ2a + σ2b + σ2ab + σ2c,cb + σ2 (2) ac,abc,e For information on the mathematical foundation of G-Theory and the derivation o</context>
</contexts>
<marker>Matt, 2001</marker>
<rawString>Matt, Georg E. 2001. Generalizabilty theory. In Neil J. Smelser and Paul B. Baltes, editors, International Encyclopedia of the Social and Behavioral Sciences. Elsevier, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard J Shavelson</author>
<author>Noreen M Webb</author>
</authors>
<title>Generalizability Theory: A Primer.</title>
<date>1981</date>
<publisher>Sage Publications,</publisher>
<location>Newbury Park, London, New Delhi.</location>
<marker>Shavelson, Webb, 1981</marker>
<rawString>Shavelson, Richard J. and Noreen M. Webb. 1981. Generalizability Theory: A Primer. Sage Publications, Newbury Park, London, New Delhi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence D Shriberg</author>
<author>Gregory L Lof</author>
</authors>
<title>Reliability studies in broad and narrow phonetic transcription.</title>
<date>1991</date>
<journal>Clinical Linguistics and Phonetics,</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="1937" citStr="Shriberg and Lof (1991)" startWordPosition="290" endWordPosition="293">er a variety of probable factors and indicate those that are mainly responsible for low reliability in a given case. Generalizability Theory, or G-Theory (Cronbach et al. 1972), is a methodological framework specifically designed for this purpose. Because it is not restricted to any type of data or study design, it can be of great use in any kind of manual annotation project that needs to systematically identify sources of annotator disagreement. In this article we provide an outline of the approach and its basic assumptions and demonstrate its application based on an annotation study done by Shriberg and Lof (1991).1 * Department of Psychology, Tulsa Graduate College, 4502 East 41st Street, Tulsa, OK 74135, USA. † Chair of Psychology, especially Organizational and Social Psychology (Prof. Dr. Moser), Lange Gasse 20, 90403 Nuremberg, Germany. 1 A more comprehensive introduction to G-Theory is provided in a longer version of this article, which is available from the authors. The work for this article was done at the Department for Applied and Computational Linguistics, Justus-Liebig-University Giessen, Germany. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 1 </context>
<context position="8179" citStr="Shriberg and Lof (1991)" startWordPosition="1263" endWordPosition="1266"> rhetorical (RST) relations could differ in the frequency with which they use individual relations such as ’background’, ’concession’, ’evidence’, and so forth, pointing to possible problems with the interpretation of rhetorical relations and their application. Variation mainly due to the item facet indicates that certain materials are harder to annotate than others. Such a result would imply retraining or elimination of overly difficult material. In consequence, the identification of distinct sources of variation should lead to specifically designed steps for improvement. 3. A Re-Analysis of Shriberg and Lof (1991) As an illustration for the application of G-Theory we reanalyzed data provided by Shriberg and Lof (1991), who studied the accuracy of broad and narrow phonetic transcriptions. In Set A of their study they investigated four facets: annotation scheme (type of consonant, C), granularity (broad vs. narrow transcription G), material (continuous speech vs. articulation test, M), and annotation team (T). Data in Set A were given as agreement percentages. Our G-study results are shown in Table 1. Traditionally, reliability concerns focus on disagreements among individual annotators assuming that var</context>
<context position="13689" citStr="Shriberg and Lof (1991)" startWordPosition="2115" endWordPosition="2118">nt Indices Two well-known measures for capturing the quality of manual annotations are agreement percentages and the kappa statistic (Cohen 1960; Carletta 1996; Eugenio and Glass 2004). Both measures provide a “summary index” (Agresti 1992) that expresses the degree of (dis)agreement among coders. Where the calculation of percentages and kappa provides a measure for overall reliability (or reliability indices for individual facets), G-Theory has been designed to analyze multiple possible influencing factors in a single run and to compare the relative importance of components among each other. Shriberg and Lof (1991), for instance, compare narrow and broad transcriptions using graphics that show the agreement percentages for each consonant for both transcription types, arguing that overall narrow transcriptions seem unreliable. Based on their experience with the study context they further assumed that these differences were not due to annotator training, behaviors, or experience. They did not provide any direct evidence, however. The G-study presented in this article could prove both assumptions in a single run. It further allows us to investigate the interactions and mutual influences of these factors, t</context>
</contexts>
<marker>Shriberg, Lof, 1991</marker>
<rawString>Shriberg, Lawrence D. and Gregory L. Lof. 1991. Reliability studies in broad and narrow phonetic transcription. Clinical Linguistics and Phonetics, 5(3):225–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Spearman</author>
</authors>
<title>The proof and measurement of association between two things.</title>
<date>1904</date>
<journal>American Journal of Psychology,</journal>
<pages>15--72</pages>
<contexts>
<context position="5474" citStr="Spearman 1904" startWordPosition="846" endWordPosition="847">n three facets a, b, c, the total variance σ2(Xabc) therefore is calculated as σ2(Xabc) = σ2 a + σ2 b + σ2 c + σ2 ab + σ2 ac + σ2 bc + σ2 (1) abc,e where σ2 refers to variance and the subscripts to the name of one or more facets. The subscript e in the last variance component denotes error variance. In nested designs some facets cannot be determined as independent terms due to their confounding with other facets. For instance, in a nested design with three factors a, b, c, different 2 This definition of reliability differs from the traditional true-score-model of classical reliability theory (Spearman 1904) and can be considered a modern approach to the question of consistency or ’dependability’ of data measurement. A discussion of the conceptual differences is beyond the scope of this article. Information on this topic may be found in Thompson (2002) or Matt (2001). 4 Bayerl and Paul Identifying Sources of Disagreement values of c may be associated with different values of b. Here the effect for c will be confounded both with bc and the residual term abc,e so that no independent term for the c facet can be obtained. Instead of the seven variance components in the crossed design only five varian</context>
</contexts>
<marker>Spearman, 1904</marker>
<rawString>Spearman, Charles. 1904. The proof and measurement of association between two things. American Journal of Psychology, 15:72–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Thompson</author>
</authors>
<title>A brief introduction to Generalizability Theory.</title>
<date>2002</date>
<booktitle>Score Reliability: Contemporary Thinking on Reliability Issues. Sage Publication, Thousand Oaks, CA,</booktitle>
<pages>43--58</pages>
<editor>In Bruce Thompson, editor,</editor>
<contexts>
<context position="5723" citStr="Thompson (2002)" startWordPosition="886" endWordPosition="887"> the last variance component denotes error variance. In nested designs some facets cannot be determined as independent terms due to their confounding with other facets. For instance, in a nested design with three factors a, b, c, different 2 This definition of reliability differs from the traditional true-score-model of classical reliability theory (Spearman 1904) and can be considered a modern approach to the question of consistency or ’dependability’ of data measurement. A discussion of the conceptual differences is beyond the scope of this article. Information on this topic may be found in Thompson (2002) or Matt (2001). 4 Bayerl and Paul Identifying Sources of Disagreement values of c may be associated with different values of b. Here the effect for c will be confounded both with bc and the residual term abc,e so that no independent term for the c facet can be obtained. Instead of the seven variance components in the crossed design only five variance components can be calculated, again stressing the fact that nested designs provide less information than fully crossed designs σ2(Xabc) = σ2a + σ2b + σ2ab + σ2c,cb + σ2 (2) ac,abc,e For information on the mathematical foundation of G-Theory and t</context>
</contexts>
<marker>Thompson, 2002</marker>
<rawString>Thompson, Bruce. 2002. A brief introduction to Generalizability Theory. In Bruce Thompson, editor, Score Reliability: Contemporary Thinking on Reliability Issues. Sage Publication, Thousand Oaks, CA, pages 43–58.</rawString>
</citation>
<citation valid="false">
<institution>Statistical Software</institution>
<marker></marker>
<rawString>Statistical Software</rawString>
</citation>
<citation valid="false">
<authors>
<author>urGENOVA GENOVA</author>
</authors>
<note>mGENOVA: Available online at http://www.education.uiowa.edu/casma/GenovaPrograms.htm</note>
<marker>GENOVA, </marker>
<rawString>• GENOVA, urGENOVA, mGENOVA: Available online at http://www.education.uiowa.edu/casma/GenovaPrograms.htm</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>