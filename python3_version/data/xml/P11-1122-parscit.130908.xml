<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000652">
<title confidence="0.935784">
Crowdsourcing Translation: Professional Quality from Non-Professionals
</title>
<author confidence="0.889325">
Omar F. Zaidan and Chris Callison-Burch
</author>
<affiliation confidence="0.975442">
Dept. of Computer Science, Johns Hopkins University
</affiliation>
<address confidence="0.76702">
Baltimore, MD 21218, USA
</address>
<email confidence="0.999682">
{ozaidan,ccb}@cs.jhu.edu
</email>
<sectionHeader confidence="0.997402" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902166666667">
Naively collecting translations by crowd-
sourcing the task to non-professional trans-
lators yields disfluent, low-quality results if
no quality control is exercised. We demon-
strate a variety of mechanisms that increase
the translation quality to near professional lev-
els. Specifically, we solicit redundant transla-
tions and edits to them, and automatically se-
lect the best output among them. We propose a
set of features that model both the translations
and the translators, such as country of resi-
dence, LM perplexity of the translation, edit
rate from the other translations, and (option-
ally) calibration against professional transla-
tors. Using these features to score the col-
lected translations, we are able to discriminate
between acceptable and unacceptable transla-
tions. We recreate the NIST 2009 Urdu-to-
English evaluation set with Mechanical Turk,
and quantitatively show that our models are
able to select translations within the range of
quality that we expect from professional trans-
lators. The total cost is more than an order of
magnitude lower than professional translation.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989466666667">
In natural language processing research, translations
are most often used in statistical machine translation
(SMT), where systems are trained using bilingual
sentence-aligned parallel corpora. SMT owes its ex-
istence to data like the Canadian Hansards (which by
law must be published in both French and English).
SMT can be applied to any language pair for which
there is sufficient data, and it has been shown to pro-
duce state-of-the-art results for language pairs like
Arabic–English, where there is ample data. How-
ever, large bilingual parallel corpora exist for rela-
tively few languages pairs.
There are various options for creating new train-
ing resources for new language pairs. These include
harvesting the web for translations or comparable
corpora (Resnik and Smith, 2003; Munteanu and
Marcu, 2005; Smith et al., 2010; Uszkoreit et al.,
2010), improving SMT models so that they are bet-
ter suited to the low resource setting (Al-Onaizan
et al., 2002; Probst et al., 2002; Oard et al., 2003;
Niessen and Ney, 2004), or designing models that
are capable of learning translations from monolin-
gual corpora (Rapp, 1995; Fung and Yee, 1998;
Schafer and Yarowsky, 2002; Haghighi et al., 2008).
Relatively little consideration is given to the idea of
simply hiring translators to create parallel data, be-
cause it would seem to be prohibitively expensive.
For example, Germann (2001) estimated the cost
of hiring professional translators to create a Tamil-
English corpus at $0.36/word. At that rate, translat-
ing enough data to build even a small parallel corpus
like the LDC’s 1.5 million word Urdu–English cor-
pus would exceed half a million dollars.
In this paper we examine the idea of creating low
cost translations via crowdscouring. We use Ama-
zon’s Mechanical Turk to hire a large group of non-
professional translators, and have them recreate an
Urdu–English evaluation set at a fraction of the cost
of professional translators. The original dataset al-
ready has professionally-produced reference trans-
lations, which allows us to objectively and quantita-
tively compare the quality of professional and non-
professional translations. Although many of the in-
dividual non-expert translators produce low-quality,
disfluent translations, we show that it is possible to
</bodyText>
<page confidence="0.901506">
1220
</page>
<note confidence="0.982061823529412">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1220–1229,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
Urdu source Professional LDC Translation Non-Professional Mechanical Turk Translation
!&amp;quot;#$&amp;quot;% &amp;&apos; ()*&amp;quot;+*, &amp;-,./%, 0#1 234 5, 0#1 1994 Signs of human life of ancient people have been Signs of human livings have been found in many caves
67&amp;quot;89: ;2&lt; &amp;=&amp;quot;&gt; &amp;*&amp;quot;1 &amp;*,?@ A&amp;quot;B C&apos;D 8 E&amp;quot;FG8?H= )&gt; discovered in several caves of Atapuerca. In 1994, in Attapure. In 1994, the remains of pre-historic man,
ÔI&amp;quot;+*, &amp;*&amp;quot;%’ &amp;JK8 ?+#B &amp;LJ8, )1)&lt; 0#MJ&gt; 0#NO &amp;&apos; several homo antecessor fossils i.e. pioneer human which are believed to be 800,000 years old were
were uncovered in this region, which are supposed to discovered and they were named `Home Antecessor&apos;
P&amp;quot;#O &amp;quot;8: Q&amp;quot;* &amp;quot;&apos; be 800,000 years old. Previously, 600,000 years old meaning `The Founding Man&apos;. Prior to that 6 lac years
&amp;+J-&amp;quot;B 0#MJ&gt; I&amp;quot;+*, 2*,?@ C&apos;D 6 RG$ 2B 5, ancestors, called homo hudlabar [sic] in scientific old humans, named as Homogenisens in scientific
term, were supposed to be the most ancient terms,were believed to be the oldest dwellers of this
5, ;2&lt; &amp;quot;=&amp;quot;&gt; &amp;quot;M&apos; S+J#&gt;?GTU#&lt; )1)&lt; 0#1 VW3X, inhabitants of the region.Archeologists are of the view area. Archaeological experts say that evidence is found
P2Y= 2=&amp;quot;&gt; 2*&amp;quot;1 &amp;Z-&amp;quot;&lt;9 [8?= \8.$ 2&apos; 234 that they have gathered evidence that the people of that proves that the inhabitants of this area used
2+8, 0#M*, ]&apos; 2&lt; &amp;quot;JM&apos; &amp;quot;&apos; [8?&lt;&amp;quot;1 2&apos; ]^8.$ this region had also been using fabricated tools. molded tools. The ground where these digs took place
_9&amp;quot;`a
2&apos; 234 5, ]&apos; 2&lt; &amp;quot;/bc ]/@ 2B [&gt; 0#&lt; 2b1 .&lt;,)d On the basis of the level at which this excavation was has been claimed to be the oldest known European
carried out, the French news agency [AFP] has termed discovery of civilization, as announced by the French
P2Y= 2=?&apos; A&amp;quot;^K/B, &amp;Y% 9,ef, 2-)&lt; 2#&apos; &amp;-Wgh i)T it the oldest European discovery. News Agency.
</note>
<figureCaption confidence="0.9954765">
Figure 1: A comparison of professional translations provided by the LDC to non-professional translations created on
Mechanical Turk.
</figureCaption>
<bodyText confidence="0.999932875">
get high quality translations in aggregate by solicit-
ing multiple translations, redundantly editing them,
and then selecting the best of the bunch.
To select the best translation, we use a machine-
learning-inspired approach that assigns a score to
each translation we collect. The scores discrimi-
nate acceptable translations from those that are not
(and competent translators from those who are not).
The scoring is based on a set of informative, intu-
itive, and easy-to-compute features. These include
country of residence, number of years speaking En-
glish, LM perplexity of the translation, edit rate from
the other translations, and (optionally) calibration
against professional translators, with the weights set
using a small set of gold standard data from profes-
sional translators.
</bodyText>
<sectionHeader confidence="0.82491" genericHeader="method">
2 Crowdsourcing Translation to
</sectionHeader>
<subsectionHeader confidence="0.721664">
Non-Professionals
</subsectionHeader>
<bodyText confidence="0.998722529411765">
To collect crowdsourced translations, we use Ama-
zon’s Mechanical Turk (MTurk), an online market-
place designed to pay people small sums of money
to complete Human Intelligence Tasks (or HITs) –
tasks that are difficult for computers but easy for
people. Example HITs range from labeling images
to moderating blog comments to providing feedback
on relevance of results for search queries. Anyone
with an Amazon account can either submit HITs or
work on HITs that were submitted by others. Work-
ers are referred to as “Turkers”, and designers of
HITs as “Requesters.” A Requester specifies the re-
ward to be paid for each completed item, sometimes
as low as $0.01. Turkers are free to select whichever
HITs interest them, and to bypass HITs they find un-
interesting or which they deem pay too little.
The advantages of Mechanical Turk include:
</bodyText>
<listItem confidence="0.993296142857143">
• zero overhead for hiring workers
• a large, low-cost labor force
• easy micropayment system
• short turnaround time, as tasks get completed
in parallel by many individuals
• access to foreign markets with native speakers
of many rare languages
</listItem>
<bodyText confidence="0.99886676">
One downside is that Amazon does not provide
any personal information about Turkers. (Each
Turker is identifiable only through an anonymous
ID like A23KO2TP7I4KK2.) In particular, no in-
formation is available about a worker’s educational
background, skills, or even native language(s). This
makes it difficult to determine if a Turker is qualified
to complete a translation task.
Therefore, soliciting translations from anony-
mous non-professionals carries a significant risk of
poor translation quality. Whereas hiring a profes-
sional translator ensures a degree of quality and
care, it is not very difficult to find bad translations
provided by Turkers. One Urdu headline, profes-
sionally translated as Barack Obama: America Will
Adopt a New Iran Strategy, was rendered disfluently
by a Turker as Barak Obam will do a new policy
with Iran. Another translated it with snarky sar-
casm: Barak Obama and America weave new evil
strategies against Iran. Figure 1 gives more typical
translation examples. The translations often reflect
non-native English, but are generally done conscien-
tiously (in spite of the relatively small payment).
To improve the accuracy of noisy labels from non-
experts, most existing quality control mechanisms
</bodyText>
<page confidence="0.983188">
1221
</page>
<bodyText confidence="0.999928615384615">
employ some form of voting, assuming a discrete
set of possible labels. This is not the case for trans-
lations, where the ‘labels’ are full sentences. When
dealing with such a structured output, the space of
possible outputs is diverse and complex. We there-
fore need a different approach for quality control.
That is precisely the focus of this work: to propose,
and evaluate, such quality control mechanisms.
In the next section, we discuss reproducing the
Urdu-to-English 2009 NIST evaluation set. We then
describe a principled approach to discriminate good
translations from bad ones, given a set of redundant
translations for the same source sentence.
</bodyText>
<sectionHeader confidence="0.999883" genericHeader="method">
3 Datasets
</sectionHeader>
<subsectionHeader confidence="0.998467">
3.1 The Urdu-to-English 2009 NIST
Evaluation Set
</subsectionHeader>
<bodyText confidence="0.999857571428571">
We translated the Urdu side of the Urdu–English test
set of the 2009 NIST MT Evaluation Workshop. The
set consists of 1,792 Urdu sentences from a vari-
ety of news and online sources. The set includes
four different reference translations for each source
sentence, produced by professional translation agen-
cies. NIST contracted the LDC to oversee the trans-
lation process and perform quality control.
This particular dataset, with its multiple reference
translations, is very useful because we can measure
the quality range for professional translators, which
gives us an idea of whether or not the crowdsourced
translations approach the quality of a professional
translator.
</bodyText>
<subsectionHeader confidence="0.999085">
3.2 Translation HIT design
</subsectionHeader>
<bodyText confidence="0.999964238095238">
We solicited English translations for the Urdu sen-
tences in the NIST dataset. Amazon has enabled
payments in rupees, which has attracted a large de-
mographic of workers from India (Ipeirotis, 2010).
Although it does not yet have s direct payment in
Pakistan’s local currency, we found that a large con-
tingent of our workers are located in Pakistan.
Our HIT involved showing the worker a sequence
of Urdu sentences, and asking them to provide an
English translation for each one. The screen also
included a brief set of instructions, and a short ques-
tionnaire section. The reward was set at $0.10 per
translation, or roughly $0.005 per word.
In our first collection effort, we solicited only one
translation per Urdu sentence. After confirming that
the task is feasible due to the large pool of work-
ers willing and able to provide translations, we car-
ried out a second collection effort, this time solicit-
ing three translations per Urdu sentence (from three
distinct translators). The interface was also slightly
modified, in the following ways:
</bodyText>
<listItem confidence="0.987538125">
• Instead of asking Turkers to translate a full doc-
ument (as in our first pass), we instead split the
data set into groups of 10 sentences per HIT.
• We converted the Urdu sentences into images
so that Turkers could not cheat by copying-and-
pasting the Urdu text into an MT system.
• We collected information about each worker’s
geographic location, using a JavaScript plugin.
</listItem>
<bodyText confidence="0.997181428571429">
The translations from the first pass were of notice-
ably low quality, most likely due to Turkers using
automatic translation systems. That is why we used
images instead of text in our second pass, which
yielded significant improvements. That said, we do
not discard the translations from the first pass, and
we do include them in our experiments.
</bodyText>
<subsectionHeader confidence="0.999563">
3.3 Post-editing and Ranking HITs
</subsectionHeader>
<bodyText confidence="0.999963136363636">
In addition to collecting four translations per source
sentence, we also collected post-edited versions
of the translations, as well as ranking judgments
about their quality.
Figure 2 gives examples of the unedited transla-
tions that we collected in the translation pass. These
typically contain many simple mistakes like mis-
spellings, typos, and awkward word choice. We
posted another MTurk task where we asked workers
to edit the translations into more fluent and gram-
matical sentences. We restrict the task to US-based
workers to increase the likelihood that they would be
native English speakers.
We also asked US-based Turkers to rank the trans-
lations. We presented the translations in groups of
four, and the annotator’s task was to rank the sen-
tences by fluency, from best to worst (allowing ties).
We collected redundant annotations in these two
tasks as well. Each translation is edited three times
(by three distinct editors). We solicited only one edit
per translation from our first pass translation effort.
So, in total, we had 10 post-edited translations for
</bodyText>
<page confidence="0.726943">
1222
</page>
<bodyText confidence="0.999926307692308">
Avoiding dieting to prevent abstention from dieting in Abstain from decrease eating in In order to be safer from flu
from flu order to avoid Flu order to escape from flue quit dieting
This research of American This research from the This research of American According to the American
scientists came in front after American Scientists have scientists was shown after Scientist this research has come
experimenting on mice. come up after the many experiments on mouses. out after much
experiments on rats. experimentations on rats.
Experiments proved that mice in has been proven from It was proved by experiments Experimentaions have proved
on a lower calorie diet had experiments that rats put on the low calories eaters that those rats on less calories
comparatively less ability to diet with less calories had less mouses had low defending diet have developed a tendency
fight the flu virus. ability to resist the Flu virus. power for flue in ratio. of not overcoming the flu virus.
research has proven this old Research disproved the old The research proved this old This Research has proved the
myth wrong that its better to axiom that &amp;quot; It is better to talk that decrease eating is very old saying wrong that it is
fast during fever. fast during fever&amp;quot; useful in fever. good to starve while in fever.
</bodyText>
<figureCaption confidence="0.987112333333333">
Figure 2: We redundantly translate each source sentence by soliciting multiple translations from different Turkers.
These translations are put through a subsequent editing set, where multiple edited versions are produced. We select
the best translation from the set using features that predict the quality of each translation and each translator.
</figureCaption>
<bodyText confidence="0.999028">
each source sentence (plus the four original transla-
tions). In the ranking task, we collected judgments
from five distinct workers for each translation group.
</bodyText>
<subsectionHeader confidence="0.959877">
3.4 Data Collection Cost
</subsectionHeader>
<bodyText confidence="0.9993915">
We paid a reward of $0.10 to translate a sentence,
$0.25 to edit a set of ten sentences, and $0.06 to rank
a set of four translation groups. Therefore, we had
the following costs:
</bodyText>
<listItem confidence="0.999929333333333">
• Translation cost: $716.80
• Editing cost: $447.50
• Ranking cost: $134.40
</listItem>
<bodyText confidence="0.998256666666667">
(If not done redundantly, those values would be
$179.20, $44.75, and $26.88, respectively.)
Adding Amazon’s 10% fee, this brings the grand
total to under $1,500, spent to collect 7,000+ transla-
tions, 17,000+ edited translations, and 35,000+ rank
labels.1 We also use about 10% of the existing pro-
fessional references in most of our experiments (see
4.2 and 4.3). If we estimate the cost at $0.30/word,
that would roughly be an additional $1,000.
</bodyText>
<subsectionHeader confidence="0.949291">
3.5 MTurk Participation
</subsectionHeader>
<bodyText confidence="0.999955571428571">
52 different Turkers took part in the translation task,
each translating 138 sentences on average. In the
editing task, 320 Turkers participated, averaging 56
sentences each. In the ranking task, 245 Turkers par-
ticipated, averaging 9.1 HITs each, or 146 rank la-
bels (since each ranking HIT involved judging 16
translations, in groups of four).
</bodyText>
<footnote confidence="0.946517">
1Data URL: www.cs.jhu.edu/˜ozaidan/RCLMT.
</footnote>
<sectionHeader confidence="0.999187" genericHeader="method">
4 Quality Control Model
</sectionHeader>
<bodyText confidence="0.999991444444445">
Our approach to building a translation set from
the available data is to select, for each Urdu sen-
tence, the one translation that our model believes
to be the best out of the available translations. We
evaluate various selection techniques by compar-
ing the selected Turker translations against existing
professionally-produced translations. The more the
selected translations resemble the professional trans-
lations, the higher the quality.
</bodyText>
<subsectionHeader confidence="0.969585">
4.1 Features Used to Select Best Translations
</subsectionHeader>
<bodyText confidence="0.9999792">
Our model selects one of the 14 English options gen-
erated by Turkers. For a source sentence si, our
model assigns a score to each sentence in the set
of available translations {ti,1, ...ti,141. The chosen
translation is the highest scoring translation:
</bodyText>
<equation confidence="0.88763775">
tr(si) = tri,j* s.t. j* = argmax score(ti,j) (1)
j
where score(.) is the dot product:
score(ti,j) def = IV· �f(ti,j) (2)
</equation>
<bodyText confidence="0.999633285714286">
Here, w is the model’s weight vector (tuned as
described below in 4.2), and f� is a translation’s cor-
responding feature vector. Each feature is a function
computed from the English sentence string, the Urdu
sentence string, the workers (translators, editors, and
rankers), and/or the rank labels. We use 21 features,
categorized into the following three sets.
</bodyText>
<page confidence="0.89921">
1223
</page>
<bodyText confidence="0.999407">
Sentence-level (6 features). Most of the Turk-
ers performing our task were native Urdu speakers
whose second language was English, and they do not
always produce natural-sounding English sentences.
Therefore, the first set of features attempt to discrim-
inate good English sentences from bad ones.
</bodyText>
<listItem confidence="0.86073636">
• Language model features: each sentence is
assigned a log probability and per-word per-
plexity score, using a 5-gram language model
trained on the English Gigaword corpus.
• Sentence length features: a good translation
tends to be comparable in length to the source
sentence, whereas an overly short or long trans-
lation is probably bad. We add two features that
are the ratios of the two lengths (one penalizes
short sentences and one penalizes long ones).
• Web n-gram match percentage: we assign a
score to each sentence based on the percentage
of the n-grams (up to length 5) in the transla-
tion that exist in the Google N-Gram Database.
• Web n-gram geometric average: we calculate
the average over the different n-gram match
percentages (similar to the way BLEU is com-
puted). We add three features corresponding to
max n-gram lengths of 3, 4, and 5.
• Edit rate to other translations: a bad translation
is likely not to be very similar to other transla-
tions, since there are many more ways a trans-
lation can be bad than for it to be good. So, we
compute the average edit rate distance from the
other translations (using the TER metric).
Worker-level (12 features). We add worker-level
features that evaluate a translation based on who pro-
vided it.
• Aggregate features: for each sentence-level
feature above, we have a corresponding feature
computed over all of that worker’s translations.
• Language abilities: we ask workers to provide
information about their language abilities. We
have a binary feature indicating whether Urdu
is their native language, and a feature for how
long they have spoken it. We add a pair of
equivalent features for English.
• Worker location: two binary features reflect a
worker’s location, one to indicate if they are lo-
cated in Pakistan, and one to indicate if they are
located in India.
Ranking (3 features). The third set of features is
based on the ranking labels we collected (see 3.3).
• Average rank: the average of the five rank la-
bels provided for this translation.
• Is-Best percentage: how often the translation
was top-ranked among the four translations.
• Is-Better percentage: how often the translation
was judged as the better translation, over all
pairwise comparisons extracted from the ranks.
</listItem>
<bodyText confidence="0.99965275">
Other features (not investigated here) could in-
clude source-target information, such as translation
model scores or the number of source words trans-
lated correctly according to a bilingual dictionary.
</bodyText>
<subsectionHeader confidence="0.960392">
4.2 Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.999990631578948">
Once features are computed for the sentences, we
must set the model’s weight vector w. Naturally, the
weights should be chosen so that good translations
get high scores, and bad translations get low scores.
We optimize translation quality against a small sub-
set (10%) of reference (professional) translations.
To tune the weight vector, we use the linear search
method of Och (2003), which is the basis of Min-
imum Error Rate Training (MERT). MERT is an
iterative algorithm used to tune parameters of an
MT system, which operates by iteratively generating
new candidate translations and adjusting the weights
to give good translations a high score, then regener-
ating new candidates based on the updated weights,
etc. In our work, the set of candidate translations is
fixed (the 14 English sentences for each source sen-
tence), and therefore iterating the procedure is not
applicable. We use the Z-MERT software package
(Zaidan, 2009) to perform the search.
</bodyText>
<subsectionHeader confidence="0.993146">
4.3 The Worker Calibration Feature
</subsectionHeader>
<bodyText confidence="0.999992428571429">
Since we use a small portion of the reference trans-
lations to perform weight tuning, we can also use
that data to compute another worker-specific fea-
ture. Namely, we can evaluate the competency of
each worker by scoring their translations against the
reference translations. We then use that feature for
every translation given by that worker. The intuition
</bodyText>
<page confidence="0.97354">
1224
</page>
<bodyText confidence="0.999893">
is that workers known to produce good translations
are likely to continue to produce good translations,
and the opposite is likely true as well.
</bodyText>
<subsectionHeader confidence="0.997591">
4.4 Evaluation Strategy
</subsectionHeader>
<bodyText confidence="0.999989266666667">
To measure the quality of the translations, we make
use of the existing professional translations. Since
we have four professional translation sets, we can
calculate the BLEU score (Papineni et al., 2002) for
one professional translator P1 using the other three
P2,3,4 as a reference set. We repeat the process four
times, scoring each professional translator against
the others, to calculate the expected range of profes-
sional quality translation. We can see how a trans-
lation set T (chosen by our model) compares to this
range by calculating T’s BLEU scores against the
same four sets of three reference translations. We
will evaluate different strategies for selecting such
a set T, and see how much each improves on the
BLEU score, compared to randomly picking from
among the Turker translations.
We also evaluate Turker translation quality by us-
ing them as reference sets to score various submis-
sions to the NIST MT evaluation. Specifically, we
measure the correlation (using Pearson’s r) between
BLEU scores of MT systems measured against non-
professional translations, and BLEU scores mea-
sured against professional translations. Since the
main purpose of the NIST dataset was to compare
MT systems against each other, this is a more di-
rect fitness-for-task measure. We chose the middle 6
systems (in terms of performance) submitted to the
NIST evaluation, out of 12, as those systems were
fairly close to each other, with less than 2 BLEU
points separating them.2
</bodyText>
<sectionHeader confidence="0.996602" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999795714285714">
We establish the performance of professional trans-
lators, calculate oracle upper bounds on Turker
translation quality, and carry out a set of experiments
that demonstrate the effectiveness of our model and
that determine which features are most helpful.
Each number reported in this section is an average
of four numbers, corresponding to the four possible
</bodyText>
<footnote confidence="0.855771666666667">
2Using all 12 systems artificially inflates correlation, due to
the vast differences between the systems. For instance, the top
system outperforms the bottom system by 15 BLEU points!
</footnote>
<bodyText confidence="0.9998665">
ways of choosing 3 of the 4 reference sets. Further-
more, each of those 4 numbers is itself based on a
five-fold cross validation, where 80% of the data is
used to compute feature values, and 20% used for
evaluation. The 80% portion is used to compute the
aggregate worker-level features. For the worker cal-
ibration feature, we utilize the references for 10% of
the data (which is within the 80% portion).
</bodyText>
<subsectionHeader confidence="0.8963765">
5.1 Translation Quality: BLEU Scores
Compared to Professionals
</subsectionHeader>
<bodyText confidence="0.999887810810811">
We first evaluated the reference sets against each
other, in order to quantify the concept of “profes-
sional quality”. On average, evaluating one refer-
ence set against the other three gives a BLEU score
of 42.38 (Figure 3). A Turker set of translations
scores 28.13 on average, which highlights the loss in
quality when collecting translations from amateurs.
To make the gap clearer, the output of a state-of-
the-art machine translation system (the syntax-based
variant of Joshua; Li et al. (2010)) achieves a score
of 26.91, a mere 1.22 worse than the Turkers.
We perform two oracle experiments to determine
if there exist high-quality Turker translations in the
first place. The first oracle operates on the segment
level: for each source segment, choose from the four
translations the one that scores highest against the
reference sentence. The second oracle operates on
the worker level: for each source segment, choose
from the four translations the one provided by the
worker whose translations (over all sentences) score
the highest. The two oracles achieve BLEU scores
of 43.75 and 40.64, respectively – well within the
range of professional translators.
We examined two voting-inspired methods, since
taking a majority vote usually works well when deal-
ing with MTurk data. The first selects the translation
with the minimum average TER (Snover et al., 2006)
against the other three translations, since that would
be a ‘consensus’ translation. The second method se-
lects the translation that received the best average
rank, using the rank labels assigned by other Turkers
(see 3.3). These approaches achieve BLEU scores of
34.41 and 36.64, respectively.
The main set of experiments evaluated the fea-
tures from 4.1 and 4.3. We applied our approach
using each of the four feature types: sentence fea-
tures, Turker features, rank features, and the cali-
</bodyText>
<page confidence="0.884631">
1225
</page>
<figure confidence="0.950323333333333">
42.38 26.91 28.13 43.75 40.64 34.41 36.64 34.95 35.79 37.14 37.82 39.06
Reference Joshua Turker Oracle Oracle Lowest Best Sentence Turker Rank Calibration All
(ave.) (synta�) (ave.) (segment) (Turker) TER rank features features features feature features
</figure>
<figureCaption confidence="0.995092">
Figure 3: BLEU scores for different selection methods, measured against the reference sets. Each score is an average
of four BLEU scores, each calculated against three LDC reference translations. The five right-most bars are colored
in orange to indicate selection over a set that includes both original translations as well as edited versions of them.
</figureCaption>
<figure confidence="0.993204333333333">
BLEU 45
40
35
30
25
20
</figure>
<bodyText confidence="0.9835655">
bration feature. That yielded BLEU scores ranging
from 34.95 to 37.82. With all features combined, we
achieve a higher score of 39.06, which is within the
range of scores for the professional translators.
</bodyText>
<subsectionHeader confidence="0.9887265">
5.2 Fitness for a Task: Correlation With
Professionals When Ranking MT Systems
</subsectionHeader>
<bodyText confidence="0.999869615384615">
We evaluated the selection methods by measuring
correlation with the references, in terms of BLEU
scores assigned to outputs of MT systems. The re-
sults, in Table 1, tell a fairly similar story as eval-
uating with BLEU: references and oracles naturally
perform very well, and the loss in quality when se-
lecting arbitrary Turker translations is largely elimi-
nated using our selection strategy.
Interestingly, when using the Joshua output as
a reference set, the performance is quite abysmal.
Even though its BLEU score is comparable to the
Turker translations, it cannot be used to distinguish
closely matched MT systems from each other.3
</bodyText>
<sectionHeader confidence="0.975973" genericHeader="conclusions">
6 Analysis
</sectionHeader>
<bodyText confidence="0.9998756">
The oracles indicate that there is usually an accept-
able translation from the Turkers for any given sen-
tence. Since the oracles select from a small group of
only 4 translations per source segment, they are not
overly optimistic, and rather reflect the true potential
of the collected translations.
The results indicate that, although some features
are more useful than others, much of the benefit
from combining all the features can be obtained
from any one set of features, with the benefit of
</bodyText>
<footnote confidence="0.963815">
3It should be noted that the Joshua system was not one of
the six MT systems we scored in the correlation experiments.
</footnote>
<figureCaption confidence="0.9999995">
Figure 4: BLEU scores for the five right-most setups from
Figure 3, constrained over the original translations.
</figureCaption>
<bodyText confidence="0.995939538461538">
adding more features being somewhat orthogonal.
Finally, we performed a series of experiments ex-
ploring the calibration feature, varying the amount
of gold-standard references from 10% all the way up
to 80%. As expected, the performance improved as
more references were used to calibrate the transla-
tors (Figure 5). What’s particularly important about
this experiment is that it shows the added benefit
of the other features: We would have to use 30%–
40% of the references to get the same benefit ob-
tained from combining the non-calibration features
and only 10% for the calibration feature (dashed line
in the Figure; BLEU = 39.06).
</bodyText>
<subsectionHeader confidence="0.999502">
6.1 Cost Reduction
</subsectionHeader>
<bodyText confidence="0.99994625">
While the combined cost of our data collection ef-
fort ($2,500; see 3.4) is quite low considering the
amount of collected data, it would be more attractive
if the cost could be reduced further without losing
much in translation quality. To that end, we inves-
tigated lowering cost along two dimensions: elimi-
nating the need for professional translations, and de-
creasing the amount of edited translations.
</bodyText>
<figure confidence="0.969530142857143">
Sentence Turker Rank Calibration All
features features features feature features
BLEU
45
40
25
20
35
30
34.71
35.45
37.14
37.22
37.96
</figure>
<page confidence="0.990142">
1226
</page>
<tableCaption confidence="0.971961">
Table 1: Correlation (f std. dev.) for different selection
methods, compared against the reference sets.
</tableCaption>
<table confidence="0.999846076923077">
Selection Method Pearson’s r2
Reference (ave.) 0.81 f 0.07
Joshua (syntax) 0.08 f 0.09
Turker (ave.) 0.60 f 0.17
Oracle (segment) 0.81 f 0.09
Oracle (Turker) 0.79 f 0.10
Lowest TER 0.50 f 0.26
Best rank 0.74 f 0.17
Sentence features 0.56 f 0.21
Turker features 0.59 f 0.19
Rank features 0.75 f 0.14
Calibration feature 0.76 f 0.13
All features 0.77 f 0.11
</table>
<figureCaption confidence="0.8083565">
Figure 5: The effect of varying the amount of calibra-
tion data (and using only the calibration feature). The
10% point (BLEU = 37.82) and the dashed line (BLEU =
39.06) correspond to the two right-most bars of Figure 3.
</figureCaption>
<figure confidence="0.979476285714286">
BLEU
40.5
40.0
39.5
39.0
38.5
38.0
37.5
37.0
0 20 40 60 80 100
% References Used for Calibration
10%}other features
(i.e. &amp;quot;All features&amp;quot;
from Figure 3)
</figure>
<bodyText confidence="0.999880891304348">
The professional translations are used in our ap-
proach for computing the worker calibration feature
(subsection 4.3) and for tuning the weights of the
other features. We use a relatively small amount
for this purpose, but we investigate a different setup
whereby no professional translations are used at all.
This eliminates the worker calibration feature, but,
perhaps more critically, the feature weights must be
set in a different fashion, since we cannot optimize
BLEU on reference data anymore. Instead, we use
the rank labels (from 3.3) as a proxy for BLEU, and
set the weights so that better ranked translations re-
ceive higher scores.
Note that the rank features will also be excluded
in this setup, since they are perfect predictors of rank
labels. On the one hand, this means no rank labels
need to be collected, other than for a small set used
for weight tuning, further reducing the cost of data
collection. However, this leads to a significant drop
in performance, yielding a BLEU score of 34.86.
Another alternative for cost reduction would be to
reduce the number of collected edited translations.
To that end, we first investigate completely eliminat-
ing the editing phase, and considering only unedited
translations. In other words, the selection will be
over a group of four English sentences rather than
14 sentences. Completely eliminating the edited
translations has an adverse effect, as expected (Fig-
ure 4). Another option, rather than eliminating the
editing phase altogether, would be to consider the
edited translations of only the translation receiving
the best rank labels. This would reflect a data col-
lection process whereby the editing task is delayed
until after the rank labels are collected, with the rank
labels used to determine which translations are most
promising to post-edit (in addition to using the rank
labels for the ranking features). Using this approach
enables us to greatly reduce the number of edited
translations collected, while maintaining good per-
formance, obtaining a BLEU score of 38.67.
It is therefore our recommendation that crowd-
sourced translation efforts adhere to the follow-
ing pipeline: collect multiple translations for each
source sentence, collect rank labels for the transla-
tions, and finally collect edited versions of the top
ranked translations.
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="references">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999946642857143">
Dawid and Skene (1979) investigated filtering
annotations using the EM algorithm, estimating
annotator-specific error rates in the context of patient
medical records. Snow et al. (2008) were among the
first to use MTurk to obtain data for several NLP
tasks, such as textual entailment and word sense dis-
ambiguation. Their approach, based on majority
voting, had a component for annotator bias correc-
tion. They showed that for such tasks, a few non-
expert labels usually suffice.
Whitehill et al. (2009) proposed a probabilistic
model to filter labels from non-experts, in the con-
text of an image labeling task. Their system genera-
tively models image difficulty, as well as noisy, even
</bodyText>
<page confidence="0.883986">
1227
</page>
<bodyText confidence="0.932943">
adversarial, annotators. They apply their method to nation techniques to the redundant translations.
simulated labels rather than real-life labels. • Modify our editing step to collect an annotated
Callison-Burch (2009) proposed several ways to corpus of English as a second language errors.
evaluate MT output on MTurk. One such method • Calibrate against good Turkers, instead of pro-
was to collect reference translations to score MT fessionals, once they have been identified.
output. It was only a pilot study (50 sentences in • Predict whether it is necessary to solicit another
each of several languages), but it showed the pos- translation instead of collecting a fixed number.
sibility of obtaining high-quality translations from • Analyze how much quality matters if our goal
non-professionals. As a followup, Bloodgood and is to train a statistical translation system.
Callison-Burch (2010) solicited a single translation Acknowledgments
of the NIST Urdu-to-English dataset we used. Their This research was supported by the Human Lan-
evaluation was similar to our correlation experi- guage Technology Center of Excellence, by gifts
ments, examining how well the collected transla- from Google and Microsoft, and by the DARPA
tions agreed with the professional translations when GALE program under Contract No. HR0011-06-2-
evaluating three MT systems. 0001. The views and findings are the authors’ alone.
That paper appeared in a NAACL 2010 workshop We would like to thank Ben Bederson, Philip
organized by Callison-Burch and Dredze (2010), fo- Resnik, and Alain D´esilets for organizing work-
cusing on MTurk as a source of data for speech and shops focused on crowdsourcing translation (Bed-
language tasks. Two relevant papers from that work- erson and Resnik, 2010; D´esilets, 2010). We are
shop were by Ambati and Vogel (2010), focusing on grateful for the feedback of workshop participants,
the design of the translation HIT, and by Irvine and which helped shape this research.
Klementiev (2010), who created translation lexicons References
between English and 42 rare languages. Yaser Al-Onaizan, Ulrich Germann, Ulf Hermjakob,
Resnik et al. (2010) explore a very interesting Kevin Knight, Philipp Koehn, Daniel Marcu, and
way of creating translations on MTurk, relying only Kenji Yamada. 2002. Translation with scarce bilin-
on monolingual speakers. Speakers of the target gual resources. Machine Translation, 17(1), March.
language iteratively identified problems in machine Vamshi Ambati and Stephan Vogel. 2010. Can crowds
translation output, and speakers of the source lan- build parallel corpora for machine translation systems?
guage paraphrased the corresponding source por- In Proceedings of the NAACL HLT Workshop on Cre-
tion. The paraphrased source would then be re- ating Speech and Language Data With Amazon’s Me-
translated to produce a different translation, hope- chanical Turk, pages 62–65.
fully more coherent than the original. Ben Bederson and Philip Resnik. 2010. Workshop on
8 Conclusion and Future Work crowdsourcing and translation. http://www.cs.
We have demonstrated that it is possible to ob- umd.edu/hcil/monotrans/workshop/.
tain high-quality translations from non-professional Michael Bloodgood and Chris Callison-Burch. 2010.
translators, and that the cost is an order of magni- Using Mechanical Turk to build machine translation
tude cheaper than professional translation. We be- evaluation sets. In Proceedings of the NAACL HLT
lieve that crowdsourcing can play a pivotal role in Workshop on Creating Speech and Language Data
future efforts to create parallel translation datasets. With Amazon’s Mechanical Turk, pages 208–211.
Beyond the cost and scalability, crowdsourcing pro- Chris Callison-Burch and Mark Dredze. 2010. Creating
vides access to languages that currently fall outside speech and language data with Amazon’s Mechanical
the scope of statistical machine translation research. Turk. In Proceedings of the NAACL HLT Workshop on
We have begun an ongoing effort to collect transla- Creating Speech and Language Data With Amazon’s
tions for several low resource languages, including Mechanical Turk, pages 1–12.
Tamil, Yoruba, and dialectal Arabic. We plan to: Chris Callison-Burch. 2009. Fast, cheap, and creative:
• Investigate improvements from system combi- Evaluating translation quality using Amazon’s Me-
1228
</bodyText>
<reference confidence="0.999354990654205">
chanical Turk. In Proceedings of EMNLP, pages 286–
295.
A. P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the EM
algorithm. Applied Statistics, 28(1):20–28.
Alain D´esilets. 2010. AMTA 2010 workshop on collabo-
rative translation: technology, crowdsourcing, and the
translator perspective. http://bit.ly/gPnqR2.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach for
translating new words from nonparallel, comparable
texts. In Proceedings of ACL/CoLing.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang for
the buck can we expect? In ACL 2001 Workshop on
Data-Driven Machine Translation, Toulouse, France.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
ACL/HLT.
Panos Ipeirotis. 2010. New demographics of Mechanical
Turk. http://behind-the-enemy-lines.
blogspot.com/2010/03/
new-demographics-of-mechanical-turk.
html.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Proceedings of the NAACL HLT
Workshop on Creating Speech and Language Data
With Amazon’s Mechanical Turk, pages 108–113.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with syn-
tax, semirings, discriminative training and other good-
ies. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR, pages
133–137.
Dragos Munteanu and Daniel Marcu. 2005. Improving
machine translation performance by exploiting compa-
rable corpora. Computational Linguistics, 31(4):477–
504, December.
Sonja Niessen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntatic analysis. Computational Linguistics,
30(2):181–204.
Doug Oard, David Doermann, Bonnie Dorr, Daqing He,
Phillip Resnik, William Byrne, Sanjeeve Khudanpur,
David Yarowsky, Anton Leuski, Philipp Koehn, and
Kevin Knight. 2003. Desperately seeking Cebuano.
In Proceedings of HLT/NAACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings ofACL,
pages 160–167.
Kishore Papineni, Salim Poukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311–318.
Katharina Probst, Lori Levin, Erik Peterson, Alon Lavie,
and Jamie Carbonell. 2002. MT for minority lan-
guages using elicitation-based learning of syntactic
transfer rules. Machine Translation, 17(4).
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of ACL.
Philip Resnik and Noah Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, 29(3):349–
380, September.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alex Quinn, and Benjamin Bederson. 2010. Improv-
ing translation via targeted paraphrasing. In Proceed-
ings of EMNLP, pages 127–137.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In Conference on Natural Lan-
guage Learning-2002, pages 146–152.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 403–411, Los An-
geles, California, June. Association for Computational
Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAssociation forMachine Translation
in the Americas (AMTA).
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast – but is it
good? Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP, pages
254–263.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel document
mining for machine translation. In Proc. of the In-
ternational Conference on Computational Linguistics
(COLING).
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels from
labelers of unknown expertise. In Proceedings of
NIPS, pages 2035–2043.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
</reference>
<page confidence="0.995043">
1229
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984506">
<title confidence="0.999742">Crowdsourcing Translation: Professional Quality from Non-Professionals</title>
<author confidence="0.999545">F Zaidan</author>
<affiliation confidence="0.999237">Dept. of Computer Science, Johns Hopkins</affiliation>
<address confidence="0.998312">Baltimore, MD 21218,</address>
<abstract confidence="0.99947772">Naively collecting translations by crowdsourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-to- English evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators. The total cost is more than an order of magnitude lower than professional translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>chanical Turk</author>
</authors>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>286--295</pages>
<marker>Turk, </marker>
<rawString>chanical Turk. In Proceedings of EMNLP, pages 286– 295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dawid</author>
<author>A M Skene</author>
</authors>
<title>Maximum likelihood estimation of observer error-rates using the EM algorithm.</title>
<date>1979</date>
<journal>Applied Statistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="32665" citStr="Dawid and Skene (1979)" startWordPosition="5259" endWordPosition="5262">he rank labels used to determine which translations are most promising to post-edit (in addition to using the rank labels for the ranking features). Using this approach enables us to greatly reduce the number of edited translations collected, while maintaining good performance, obtaining a BLEU score of 38.67. It is therefore our recommendation that crowdsourced translation efforts adhere to the following pipeline: collect multiple translations for each source sentence, collect rank labels for the translations, and finally collect edited versions of the top ranked translations. 7 Related Work Dawid and Skene (1979) investigated filtering annotations using the EM algorithm, estimating annotator-specific error rates in the context of patient medical records. Snow et al. (2008) were among the first to use MTurk to obtain data for several NLP tasks, such as textual entailment and word sense disambiguation. Their approach, based on majority voting, had a component for annotator bias correction. They showed that for such tasks, a few nonexpert labels usually suffice. Whitehill et al. (2009) proposed a probabilistic model to filter labels from non-experts, in the context of an image labeling task. Their system</context>
</contexts>
<marker>Dawid, Skene, 1979</marker>
<rawString>A. P. Dawid and A. M. Skene. 1979. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied Statistics, 28(1):20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alain D´esilets</author>
</authors>
<title>AMTA 2010 workshop on collaborative translation: technology, crowdsourcing, and the translator perspective.</title>
<date>2010</date>
<note>http://bit.ly/gPnqR2.</note>
<marker>D´esilets, 2010</marker>
<rawString>Alain D´esilets. 2010. AMTA 2010 workshop on collaborative translation: technology, crowdsourcing, and the translator perspective. http://bit.ly/gPnqR2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An ir approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL/CoLing.</booktitle>
<contexts>
<context position="2462" citStr="Fung and Yee, 1998" startWordPosition="372" endWordPosition="375"> large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exceed half a million dollars. In this paper we examine the idea of creating low cost translations via crowdscouring. We us</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An ir approach for translating new words from nonparallel, comparable texts. In Proceedings of ACL/CoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
</authors>
<title>Building a statistical machine translation system from scratch: How much bang for the buck can we expect?</title>
<date>2001</date>
<booktitle>In ACL 2001 Workshop on Data-Driven Machine Translation,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="2703" citStr="Germann (2001)" startWordPosition="411" endWordPosition="412"> Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exceed half a million dollars. In this paper we examine the idea of creating low cost translations via crowdscouring. We use Amazon’s Mechanical Turk to hire a large group of nonprofessional translators, and have them recreate an Urdu–English evaluation set at a fraction of the cost of professional translators. The original dataset already has professionally-pro</context>
</contexts>
<marker>Germann, 2001</marker>
<rawString>Ulrich Germann. 2001. Building a statistical machine translation system from scratch: How much bang for the buck can we expect? In ACL 2001 Workshop on Data-Driven Machine Translation, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<contexts>
<context position="2514" citStr="Haghighi et al., 2008" startWordPosition="380" endWordPosition="383">tively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exceed half a million dollars. In this paper we examine the idea of creating low cost translations via crowdscouring. We use Amazon’s Mechanical Turk to hire a large group of </context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panos Ipeirotis</author>
</authors>
<title>New demographics of Mechanical Turk.</title>
<date>2010</date>
<note>http://behind-the-enemy-lines. blogspot.com/2010/03/</note>
<contexts>
<context position="10468" citStr="Ipeirotis, 2010" startWordPosition="1646" endWordPosition="1647">ssional translation agencies. NIST contracted the LDC to oversee the translation process and perform quality control. This particular dataset, with its multiple reference translations, is very useful because we can measure the quality range for professional translators, which gives us an idea of whether or not the crowdsourced translations approach the quality of a professional translator. 3.2 Translation HIT design We solicited English translations for the Urdu sentences in the NIST dataset. Amazon has enabled payments in rupees, which has attracted a large demographic of workers from India (Ipeirotis, 2010). Although it does not yet have s direct payment in Pakistan’s local currency, we found that a large contingent of our workers are located in Pakistan. Our HIT involved showing the worker a sequence of Urdu sentences, and asking them to provide an English translation for each one. The screen also included a brief set of instructions, and a short questionnaire section. The reward was set at $0.10 per translation, or roughly $0.005 per word. In our first collection effort, we solicited only one translation per Urdu sentence. After confirming that the task is feasible due to the large pool of wor</context>
</contexts>
<marker>Ipeirotis, 2010</marker>
<rawString>Panos Ipeirotis. 2010. New demographics of Mechanical Turk. http://behind-the-enemy-lines. blogspot.com/2010/03/</rawString>
</citation>
<citation valid="false">
<note>new-demographics-of-mechanical-turk. html.</note>
<marker></marker>
<rawString>new-demographics-of-mechanical-turk. html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Using Mechanical Turk to annotate lexicons for less commonly used languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk,</booktitle>
<pages>108--113</pages>
<marker>Irvine, Klementiev, 2010</marker>
<rawString>Ann Irvine and Alexandre Klementiev. 2010. Using Mechanical Turk to annotate lexicons for less commonly used languages. In Proceedings of the NAACL HLT Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk, pages 108–113.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Ann Irvine</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Ziyuan Wang</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua 2.0: A toolkit for parsing-based machine translation with syntax, semirings, discriminative training and other goodies.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>133--137</pages>
<contexts>
<context position="24634" citStr="Li et al. (2010)" startWordPosition="3953" endWordPosition="3956">es for 10% of the data (which is within the 80% portion). 5.1 Translation Quality: BLEU Scores Compared to Professionals We first evaluated the reference sets against each other, in order to quantify the concept of “professional quality”. On average, evaluating one reference set against the other three gives a BLEU score of 42.38 (Figure 3). A Turker set of translations scores 28.13 on average, which highlights the loss in quality when collecting translations from amateurs. To make the gap clearer, the output of a state-ofthe-art machine translation system (the syntax-based variant of Joshua; Li et al. (2010)) achieves a score of 26.91, a mere 1.22 worse than the Turkers. We perform two oracle experiments to determine if there exist high-quality Turker translations in the first place. The first oracle operates on the segment level: for each source segment, choose from the four translations the one that scores highest against the reference sentence. The second oracle operates on the worker level: for each source segment, choose from the four translations the one provided by the worker whose translations (over all sentences) score the highest. The two oracles achieve BLEU scores of 43.75 and 40.64, </context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Irvine, Khudanpur, Schwartz, Thornton, Wang, Weese, Zaidan, 2010</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Ann Irvine, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Ziyuan Wang, Jonathan Weese, and Omar Zaidan. 2010. Joshua 2.0: A toolkit for parsing-based machine translation with syntax, semirings, discriminative training and other goodies. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 133–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting comparable corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<pages>504</pages>
<contexts>
<context position="2127" citStr="Munteanu and Marcu, 2005" startWordPosition="314" endWordPosition="317">lel corpora. SMT owes its existence to data like the Canadian Hansards (which by law must be published in both French and English). SMT can be applied to any language pair for which there is sufficient data, and it has been shown to produce state-of-the-art results for language pairs like Arabic–English, where there is ample data. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of h</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting comparable corpora. Computational Linguistics, 31(4):477– 504, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Niessen</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical machine translation with scarce resources using morpho-syntatic analysis.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="2342" citStr="Niessen and Ney, 2004" startWordPosition="353" endWordPosition="356">s been shown to produce state-of-the-art results for language pairs like Arabic–English, where there is ample data. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exc</context>
</contexts>
<marker>Niessen, Ney, 2004</marker>
<rawString>Sonja Niessen and Hermann Ney. 2004. Statistical machine translation with scarce resources using morpho-syntatic analysis. Computational Linguistics, 30(2):181–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Oard</author>
<author>David Doermann</author>
<author>Bonnie Dorr</author>
<author>Daqing He</author>
<author>Phillip Resnik</author>
<author>William Byrne</author>
<author>Sanjeeve Khudanpur</author>
<author>David Yarowsky</author>
<author>Anton Leuski</author>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Desperately seeking Cebuano.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="2318" citStr="Oard et al., 2003" startWordPosition="349" endWordPosition="352">ent data, and it has been shown to produce state-of-the-art results for language pairs like Arabic–English, where there is ample data. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–</context>
</contexts>
<marker>Oard, Doermann, Dorr, He, Resnik, Byrne, Khudanpur, Yarowsky, Leuski, Koehn, Knight, 2003</marker>
<rawString>Doug Oard, David Doermann, Bonnie Dorr, Daqing He, Phillip Resnik, William Byrne, Sanjeeve Khudanpur, David Yarowsky, Anton Leuski, Philipp Koehn, and Kevin Knight. 2003. Desperately seeking Cebuano. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="20497" citStr="Och (2003)" startWordPosition="3285" endWordPosition="3286">anks. Other features (not investigated here) could include source-target information, such as translation model scores or the number of source words translated correctly according to a bilingual dictionary. 4.2 Parameter Tuning Once features are computed for the sentences, we must set the model’s weight vector w. Naturally, the weights should be chosen so that good translations get high scores, and bad translations get low scores. We optimize translation quality against a small subset (10%) of reference (professional) translations. To tune the weight vector, we use the linear search method of Och (2003), which is the basis of Minimum Error Rate Training (MERT). MERT is an iterative algorithm used to tune parameters of an MT system, which operates by iteratively generating new candidate translations and adjusting the weights to give good translations a high score, then regenerating new candidates based on the updated weights, etc. In our work, the set of candidate translations is fixed (the 14 English sentences for each source sentence), and therefore iterating the procedure is not applicable. We use the Z-MERT software package (Zaidan, 2009) to perform the search. 4.3 The Worker Calibration </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings ofACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Poukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="21841" citStr="Papineni et al., 2002" startWordPosition="3499" endWordPosition="3502">compute another worker-specific feature. Namely, we can evaluate the competency of each worker by scoring their translations against the reference translations. We then use that feature for every translation given by that worker. The intuition 1224 is that workers known to produce good translations are likely to continue to produce good translations, and the opposite is likely true as well. 4.4 Evaluation Strategy To measure the quality of the translations, we make use of the existing professional translations. Since we have four professional translation sets, we can calculate the BLEU score (Papineni et al., 2002) for one professional translator P1 using the other three P2,3,4 as a reference set. We repeat the process four times, scoring each professional translator against the others, to calculate the expected range of professional quality translation. We can see how a translation set T (chosen by our model) compares to this range by calculating T’s BLEU scores against the same four sets of three reference translations. We will evaluate different strategies for selecting such a set T, and see how much each improves on the BLEU score, compared to randomly picking from among the Turker translations. We </context>
</contexts>
<marker>Papineni, Poukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Poukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina Probst</author>
<author>Lori Levin</author>
<author>Erik Peterson</author>
<author>Alon Lavie</author>
<author>Jamie Carbonell</author>
</authors>
<title>MT for minority languages using elicitation-based learning of syntactic transfer rules.</title>
<date>2002</date>
<journal>Machine Translation,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="2299" citStr="Probst et al., 2002" startWordPosition="345" endWordPosition="348">hich there is sufficient data, and it has been shown to produce state-of-the-art results for language pairs like Arabic–English, where there is ample data. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5</context>
</contexts>
<marker>Probst, Levin, Peterson, Lavie, Carbonell, 2002</marker>
<rawString>Katharina Probst, Lori Levin, Erik Peterson, Alon Lavie, and Jamie Carbonell. 2002. MT for minority languages using elicitation-based learning of syntactic transfer rules. Machine Translation, 17(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2442" citStr="Rapp, 1995" startWordPosition="370" endWordPosition="371">ta. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exceed half a million dollars. In this paper we examine the idea of creating low cost translations via </context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<pages>380</pages>
<contexts>
<context position="2101" citStr="Resnik and Smith, 2003" startWordPosition="310" endWordPosition="313">l sentence-aligned parallel corpora. SMT owes its existence to data like the Canadian Hansards (which by law must be published in both French and English). SMT can be applied to any language pair for which there is sufficient data, and it has been shown to produce state-of-the-art results for language pairs like Arabic–English, where there is ample data. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (200</context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah Smith. 2003. The web as a parallel corpus. Computational Linguistics, 29(3):349– 380, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Olivia Buzek</author>
<author>Chang Hu</author>
<author>Yakov Kronrod</author>
<author>Alex Quinn</author>
<author>Benjamin Bederson</author>
</authors>
<title>Improving translation via targeted paraphrasing.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>127--137</pages>
<contexts>
<context position="35491" citStr="Resnik et al. (2010)" startWordPosition="5696" endWordPosition="5699">e (2010), fo- Resnik, and Alain D´esilets for organizing workcusing on MTurk as a source of data for speech and shops focused on crowdsourcing translation (Bedlanguage tasks. Two relevant papers from that work- erson and Resnik, 2010; D´esilets, 2010). We are shop were by Ambati and Vogel (2010), focusing on grateful for the feedback of workshop participants, the design of the translation HIT, and by Irvine and which helped shape this research. Klementiev (2010), who created translation lexicons References between English and 42 rare languages. Yaser Al-Onaizan, Ulrich Germann, Ulf Hermjakob, Resnik et al. (2010) explore a very interesting Kevin Knight, Philipp Koehn, Daniel Marcu, and way of creating translations on MTurk, relying only Kenji Yamada. 2002. Translation with scarce bilinon monolingual speakers. Speakers of the target gual resources. Machine Translation, 17(1), March. language iteratively identified problems in machine Vamshi Ambati and Stephan Vogel. 2010. Can crowds translation output, and speakers of the source lan- build parallel corpora for machine translation systems? guage paraphrased the corresponding source por- In Proceedings of the NAACL HLT Workshop on Cretion. The paraphrase</context>
</contexts>
<marker>Resnik, Buzek, Hu, Kronrod, Quinn, Bederson, 2010</marker>
<rawString>Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod, Alex Quinn, and Benjamin Bederson. 2010. Improving translation via targeted paraphrasing. In Proceedings of EMNLP, pages 127–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing translation lexicons via diverse similarity measures and bridge languages.</title>
<date>2002</date>
<booktitle>In Conference on Natural Language Learning-2002,</booktitle>
<pages>146--152</pages>
<contexts>
<context position="2490" citStr="Schafer and Yarowsky, 2002" startWordPosition="376" endWordPosition="379">allel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exceed half a million dollars. In this paper we examine the idea of creating low cost translations via crowdscouring. We use Amazon’s Mechanical Turk t</context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Charles Schafer and David Yarowsky. 2002. Inducing translation lexicons via diverse similarity measures and bridge languages. In Conference on Natural Language Learning-2002, pages 146–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason R Smith</author>
<author>Chris Quirk</author>
<author>Kristina Toutanova</author>
</authors>
<title>Extracting parallel sentences from comparable corpora using document level alignment.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>403--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="2147" citStr="Smith et al., 2010" startWordPosition="318" endWordPosition="321">existence to data like the Canadian Hansards (which by law must be published in both French and English). SMT can be applied to any language pair for which there is sufficient data, and it has been shown to produce state-of-the-art results for language pairs like Arabic–English, where there is ample data. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional t</context>
</contexts>
<marker>Smith, Quirk, Toutanova, 2010</marker>
<rawString>Jason R. Smith, Chris Quirk, and Kristina Toutanova. 2010. Extracting parallel sentences from comparable corpora using document level alignment. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 403–411, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation forMachine Translation in the Americas (AMTA).</booktitle>
<contexts>
<context position="25503" citStr="Snover et al., 2006" startWordPosition="4091" endWordPosition="4094">nt, choose from the four translations the one that scores highest against the reference sentence. The second oracle operates on the worker level: for each source segment, choose from the four translations the one provided by the worker whose translations (over all sentences) score the highest. The two oracles achieve BLEU scores of 43.75 and 40.64, respectively – well within the range of professional translators. We examined two voting-inspired methods, since taking a majority vote usually works well when dealing with MTurk data. The first selects the translation with the minimum average TER (Snover et al., 2006) against the other three translations, since that would be a ‘consensus’ translation. The second method selects the translation that received the best average rank, using the rank labels assigned by other Turkers (see 3.3). These approaches achieve BLEU scores of 34.41 and 36.64, respectively. The main set of experiments evaluated the features from 4.1 and 4.3. We applied our approach using each of the four feature types: sentence features, Turker features, rank features, and the cali1225 42.38 26.91 28.13 43.75 40.64 34.41 36.64 34.95 35.79 37.14 37.82 39.06 Reference Joshua Turker Oracle Ora</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation forMachine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast – but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast – but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP, pages 254–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Jay M Ponte</author>
<author>Ashok C Popat</author>
<author>Moshe Dubiner</author>
</authors>
<title>Large scale parallel document mining for machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="2172" citStr="Uszkoreit et al., 2010" startWordPosition="322" endWordPosition="325">ke the Canadian Hansards (which by law must be published in both French and English). SMT can be applied to any language pair for which there is sufficient data, and it has been shown to produce state-of-the-art results for language pairs like Arabic–English, where there is ample data. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a Ta</context>
</contexts>
<marker>Uszkoreit, Ponte, Popat, Dubiner, 2010</marker>
<rawString>Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and Moshe Dubiner. 2010. Large scale parallel document mining for machine translation. In Proc. of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Whitehill</author>
<author>Paul Ruvolo</author>
<author>Tingfan Wu</author>
<author>Jacob Bergsma</author>
<author>Javier Movellan</author>
</authors>
<title>Whose vote should count more: Optimal integration of labels from labelers of unknown expertise.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2035--2043</pages>
<contexts>
<context position="33144" citStr="Whitehill et al. (2009)" startWordPosition="5335" endWordPosition="5338">lect rank labels for the translations, and finally collect edited versions of the top ranked translations. 7 Related Work Dawid and Skene (1979) investigated filtering annotations using the EM algorithm, estimating annotator-specific error rates in the context of patient medical records. Snow et al. (2008) were among the first to use MTurk to obtain data for several NLP tasks, such as textual entailment and word sense disambiguation. Their approach, based on majority voting, had a component for annotator bias correction. They showed that for such tasks, a few nonexpert labels usually suffice. Whitehill et al. (2009) proposed a probabilistic model to filter labels from non-experts, in the context of an image labeling task. Their system generatively models image difficulty, as well as noisy, even 1227 adversarial, annotators. They apply their method to nation techniques to the redundant translations. simulated labels rather than real-life labels. • Modify our editing step to collect an annotated Callison-Burch (2009) proposed several ways to corpus of English as a second language errors. evaluate MT output on MTurk. One such method • Calibrate against good Turkers, instead of prowas to collect reference tr</context>
</contexts>
<marker>Whitehill, Ruvolo, Wu, Bergsma, Movellan, 2009</marker>
<rawString>Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier Movellan. 2009. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Proceedings of NIPS, pages 2035–2043.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<contexts>
<context position="21046" citStr="Zaidan, 2009" startWordPosition="3374" endWordPosition="3375">the weight vector, we use the linear search method of Och (2003), which is the basis of Minimum Error Rate Training (MERT). MERT is an iterative algorithm used to tune parameters of an MT system, which operates by iteratively generating new candidate translations and adjusting the weights to give good translations a high score, then regenerating new candidates based on the updated weights, etc. In our work, the set of candidate translations is fixed (the 14 English sentences for each source sentence), and therefore iterating the procedure is not applicable. We use the Z-MERT software package (Zaidan, 2009) to perform the search. 4.3 The Worker Calibration Feature Since we use a small portion of the reference translations to perform weight tuning, we can also use that data to compute another worker-specific feature. Namely, we can evaluate the competency of each worker by scoring their translations against the reference translations. We then use that feature for every translation given by that worker. The intuition 1224 is that workers known to produce good translations are likely to continue to produce good translations, and the opposite is likely true as well. 4.4 Evaluation Strategy To measur</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>