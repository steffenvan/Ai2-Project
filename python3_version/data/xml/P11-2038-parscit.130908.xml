<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010850">
<title confidence="0.998171">
Judging Grammaticality with Tree Substitution Grammar Derivations
</title>
<author confidence="0.99737">
Matt Post
</author>
<affiliation confidence="0.9382795">
Human Language Technology Center of Excellence
Johns Hopkins University
</affiliation>
<address confidence="0.473445">
Baltimore, MD 21211
</address>
<sectionHeader confidence="0.977896" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998165">
In this paper, we show that local features com-
puted from the derivations of tree substitution
grammars — such as the identify of particu-
lar fragments, and a count of large and small
fragments — are useful in binary grammatical
classification tasks. Such features outperform
n-gram features and various model scores by
a wide margin. Although they fall short of
the performance of the hand-crafted feature
set of Charniak and Johnson (2005) developed
for parse tree reranking, they do so with an
order of magnitude fewer features. Further-
more, since the TSGs employed are learned
in a Bayesian setting, the use of their deriva-
tions can be viewed as the automatic discov-
ery of tree patterns useful for classification.
On the BLLIP dataset, we achieve an accuracy
of 89.9% in discriminating between grammat-
ical text and samples from an n-gram language
model.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964125">
The task of a language model is to provide a measure
of the grammaticality of a sentence. Language mod-
els are useful in a variety of settings, for both human
and machine output; for example, in the automatic
grading of essays, or in guiding search in a machine
translation system. Language modeling has proved
to be quite difficult. The simplest models, n-grams,
are self-evidently poor models of language, unable
to (easily) capture or enforce long-distance linguis-
tic phenomena. However, they are easy to train, are
long-studied and well understood, and can be ef-
ficiently incorporated into search procedures, such
as for machine translation. As a result, the output
of such text generation systems is often very poor
grammatically, even if it is understandable.
Since grammaticality judgments are a matter of
the syntax of a language, the obvious approach for
modeling grammaticality is to start with the exten-
sive work produced over the past two decades in
the field of parsing. This paper demonstrates the
utility of local features derived from the fragments
of tree substitution grammar derivations. Follow-
ing Cherry and Quirk (2008), we conduct experi-
ments in a classification setting, where the task is to
distinguish between real text and “pseudo-negative”
text obtained by sampling from a trigram language
model (Okanohara and Tsujii, 2007). Our primary
points of comparison are the latent SVM training
of Cherry and Quirk (2008), mentioned above, and
the extensive set of local and nonlocal feature tem-
plates developed by Charniak and Johnson (2005)
for parse tree reranking. In contrast to this latter set
of features, the feature sets from TSG derivations
require no engineering; instead, they are obtained
directly from the identity of the fragments used in
the derivation, plus simple statistics computed over
them. Since these fragments are in turn learned au-
tomatically from a Treebank with a Bayesian model,
their usefulness here suggests a greater potential for
adapting to other languages and datasets.
</bodyText>
<sectionHeader confidence="0.878911" genericHeader="method">
2 Tree substitution grammars
</sectionHeader>
<bodyText confidence="0.975499">
Tree substitution grammars (Joshi and Schabes,
1997) generalize context-free grammars by allow-
ing nonterminals to rewrite as tree fragments of ar-
bitrary size, instead of as only a sequence of one or
</bodyText>
<page confidence="0.975041">
217
</page>
<note confidence="0.847254">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 217–222,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<table confidence="0.987469428571429">
S
NPV
dataset training devel. test
Treebank 3,836 2,690 3,398
91,954 65,474 79,998
BLLIP 100,000 6,000 6,000
2,596,508 155,247 156,353
</table>
<tableCaption confidence="0.8936625">
Table 1: The number of sentences (first line) and words
(second line) using for training, development, and test-
ing of the classifier. Each set of sentences is evenly split
between positive and negative examples.
</tableCaption>
<bodyText confidence="0.97375525">
4. The Charniak parser (Charniak, 2000), run in
language modeling mode
The parsing models for both datasets were built from
sections 2 - 21 of the WSJ portion of the Treebank.
These models were used to score or parse the train-
ing, development, and test data for the classifier.
From the output, we extract the following feature
sets used in the classifier.
</bodyText>
<listItem confidence="0.983603">
• Sentence length (l).
</listItem>
<bodyText confidence="0.9983875">
and 25.6 and 26.2 for the BLLIP data.
Each dataset is divided into training, develop-
ment, and test sets. For the Treebank, we trained
the n-gram language model on sections 2 - 21. The
classifier then used sections 0, 24, and 22 for train-
ing, development, and testing, respectively. For
the BLLIP dataset, we followed Cherry and Quirk
(2008): we randomly selected 450K sentences to
train the n-gram language model, and 50K, 3K, and
3K sentences for classifier training, development,
and testing, respectively. All sentences have 100
or fewer words. Table 1 contains statistics of the
datasets used in our experiments.
To build the classifier, we used liblinear (Fan
et al., 2008). A bias of 1 was added to each feature
vector. We varied a cost or regularization parame-
ter between 1e − 5 and 100 in orders of magnitude;
at each step, we built a model, evaluating it on the
development set. The model with the highest score
was then used to produce the result on the test set.
</bodyText>
<subsectionHeader confidence="0.999914">
4.1 Base models and features
</subsectionHeader>
<bodyText confidence="0.999537666666667">
Our experiments compare a number of different fea-
ture sets. Central to these feature sets are features
computed from the output of four language models.
</bodyText>
<listItem confidence="0.994338">
1. Bigram and trigram language models (the same
ones used to generate the negative data)
2. A Treebank grammar (Charniak, 1996)
3. A Bayesian-learned tree substitution grammar
(Post and Gildea, 2009a)2
</listItem>
<bodyText confidence="0.9064236">
2The sampler was run with the default settings for 1,000
iterations, and a grammar of 192,667 fragments was then ex-
tracted from counts taken from every 10th iteration between
iterations 500 and 1,000, inclusive. Code was obtained from
http://github.com/mjpost/dptsg.
</bodyText>
<listItem confidence="0.983622307692308">
• Model scores (5). Model log probabilities.
• Rule features (R). These are counter features
based on the atomic unit of the analysis, i.e., in-
dividual n-grams for the n-gram models, PCFG
rules, and TSG fragments.
• Reranking features (C&amp;J). From the Char-
niak parser output we extract the complete set
of reranking features of Charniak and Johnson
(2005), and just the local ones (C&amp;J local).3
• Frontier size (Fn, Fry,). Instances of this fea-
ture class count the number of TSG fragments
having frontier size n, 1 &lt; n &lt; 9.4 Instances
of F&apos;n count only lexical items for 0 &lt; n &lt; 5.
</listItem>
<subsectionHeader confidence="0.535899">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.9999548">
Table 2 contains the classification results. The first
block of models all perform at chance. We exper-
imented with SVM classifiers instead of maximum
entropy, and the only real change across all the mod-
els was for these first five models, which saw classi-
fication rise to 55 to 60%.
On the BLLIP dataset, the C&amp;J feature sets per-
form the best, even when the set of features is re-
stricted to local ones. However, as shown in Table 3,
this performance comes at a cost of using ten times
as many features. The classifiers with TSG features
outperform all the other models.
The (near)-perfect performance of the TSG mod-
els on the Treebank is a result of the large number
of features relative to the size of the training data:
</bodyText>
<footnote confidence="0.9349728">
3Local features can be computed in a bottom-up manner.
See Huang (2008, §3.2) for more detail.
4A fragment’s frontier is the number of terminals and non-
terminals among its leaves, also known its rank. For example,
the fragment in Figure 1 has a frontier size of 5.
</footnote>
<page confidence="0.996624">
219
</page>
<figure confidence="0.9719435">
feature set
length (l)
3-gram score (53)
PCFG score (5P)
TSG score (5T)
Charniak score (5C)
</figure>
<equation confidence="0.960918363636364">
l + 53
l + 5P
l + 5T
l + 5C
l + R2
l + R3
l + RP
l + RT
l + C&amp;J (local)
l + C&amp;J
l + RT + F. + Fl.
</equation>
<figure confidence="0.985952058823529">
Treebank BLLIP
50.0 46.4
50.0 50.1
49.5 50.0
49.5 49.7
50.0 50.0
61.0 64.3
75.6 70.4
82.4 76.2
76.3 69.1
62.4 70.6
61.3 70.7
60.4 85.0
99.4 89.3
89.1 92.5
88.6 93.0
100.0 89.9
</figure>
<tableCaption confidence="0.865453">
Table 2: Classification accuracy.
</tableCaption>
<bodyText confidence="0.758039">
feature set Treebank BLLIP
</bodyText>
<equation confidence="0.944111833333333">
18K 122K
15K 11K
14K 60K
24K 607K
58K 959K
14K 60K
</equation>
<tableCaption confidence="0.978274">
Table 3: Model size.
</tableCaption>
<bodyText confidence="0.9990485">
the positive and negative data really do evince dif-
ferent fragments, and there are enough such features
relative to the size of the training data that very high
weights can be placed on them. Manual examina-
tion of feature weights bears this out. Despite hav-
ing more features available, the Charniak &amp; John-
son feature set has significantly lower accuracy on
the Treebank data, which suggests that the TSG fea-
tures are more strongly associated with a particular
(positive or negative) outcome.
For comparison, Cherry and Quirk (2008) report
a classification accuracy of 81.42 on BLLIP. We ex-
clude it from the table because a direct comparison is
not possible, since we did not have access to the split
on the BLLIP used in their experiments, but only re-
peated the process they described to generate it.
</bodyText>
<sectionHeader confidence="0.98925" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999954032258064">
Table 4 lists the highest-weighted TSG features as-
sociated with each outcome, taken from the BLLIP
model in the last row of Table 2. The learned
weights accord with the intuitions presented in Sec-
tion 3. Ungrammatical sentences use smaller, ab-
stract (unlexicalized) rules, whereas grammatical
sentences use higher rank rules and are more lexical-
ized. Looking at the fragments themselves, we see
that sensible patterns such as balanced parenthetical
expressions or verb predicate-argument structures
are associated with grammaticality, while many of
the ungrammatical fragments contain unbalanced
quotations and unlikely configurations.
Table 5 contains the most probable depth-one
rules for each outcome. The unary rules associated
with ungrammatical sentences show some interest-
ing patterns. For example, the rule NP -* DT occurs
2,344 times in the training portion of the Treebank.
Most of these occurrences are in subject settings
over articles that aren’t required to modify a noun,
such as that, some, this, and all. However, in the
BLLIP n-gram data, this rule is used over the defi-
nite article the 465 times – the second-most common
use. Yet this rule occurs only nine times in the Tree-
bank where the grammar was learned. The small
fragment size, together with the coarseness of the
nonterminal, permit the fragment to be used in dis-
tributional settings where it should not be licensed.
This suggests some complementarity between frag-
ment learning and work in using nonterminal refine-
ments (Johnson, 1998; Petrov et al., 2006).
</bodyText>
<sectionHeader confidence="0.999957" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999982307692308">
Past approaches using parsers as language models
in discriminative settings have seen varying degrees
of success. Och et al. (2004) found that the score
of a bilexicalized parser was not useful in distin-
guishing machine translation (MT) output from hu-
man reference translations. Cherry and Quirk (2008)
addressed this problem by using a latent SVM to
adjust the CFG rule weights such that the parser
score was a much more useful discriminator be-
tween grammatical text and n-gram samples. Mut-
ton et al. (2007) also addressed this problem by com-
bining scores from different parsers using an SVM
and showed an improved metric of fluency.
</bodyText>
<equation confidence="0.999933166666667">
l + R3
l + RP
l + RT
l + C&amp;J (local)
l + C&amp;J
l + RT + F.
</equation>
<page confidence="0.994666">
220
</page>
<table confidence="0.99969175">
grammatical ungrammatical
(VP VBD (NP CD) Fo
PP)
(S (NP PRP) VP) (NP (NP CD) PP)
(S NP (VP TO VP)) (TOP (NP NP NP .))
F� F5
2
(NP NP (VP VBG (S (NP (NNP UNK-
NP)) CAPS-NUM)))
(SBAR (S (NP PRP) (TOP (S NP VP (. .)))
VP))
(SBAR (IN that) S) (TOP (PP IN NP .))
(TOP (S NP (VP (VBD (TOP (S “ NP VP (. .)))
said) NP SBAR) .))
(NP (NP DT JJ NN) (TOP (S PP NP VP .))
PP)
(NP (NP NNP NNP) , (TOP (NP NP PP .))
NP ,)
(TOP (S NP (ADVP F4
(RB also)) VP .))
(VP (VB be) VP) (NP (DT that) NN)
(NP (NP NNS) PP) (TOP (S NP VP . ”))
(NP NP , (SBAR (TOP (NP NP , NP .))
WHNP (S VP)) ,)
(TOP (S SBAR , NP (QP CD (CD million))
VP .))
(ADJP (QP $ CD (CD (NP NP (CC and) NP)
million)))
(SBAR (IN that) (S NP (PP (IN In) NP)
VP))
F8 (QP $ CD (CD mil-
lion))
</table>
<tableCaption confidence="0.999594">
Table 4: Highest-weighted TSG features.
</tableCaption>
<bodyText confidence="0.991157071428571">
Outside of MT, Foster and Vogel (2004) argued
for parsers that do not assume the grammaticality of
their input. Sun et al. (2007) used a set of templates
to extract labeled sequential part-of-speech patterns
together with some other linguistic features) which
were then used in an SVM setting to classify sen-
tences in Japanese and Chinese learners’ English
corpora. Wagner et al. (2009) and Foster and An-
dersen (2009) attempt finer-grained, more realistic
(and thus more difficult) classifications against un-
grammatical text modeled on the sorts of mistakes
made by language learners using parser probabili-
ties. More recently, some researchers have shown
that using features of parse trees (such as the rules
</bodyText>
<table confidence="0.998986714285714">
grammatical ungrammatical
(WHNP CD) (NN UNK-CAPS)
(NP JJ NNS) (S VP)
(PRT RP) (S NP)
(WHNP WP NN) (TOP FRAG)
(SBAR WHNP S) (NP DT JJ)
(WHNP WDT NN) (NP DT)
</table>
<tableCaption confidence="0.999101">
Table 5: Highest-weighted depth-one rules.
</tableCaption>
<bodyText confidence="0.689398">
used) is fruitful (Wong and Dras, 2010; Post, 2010).
</bodyText>
<sectionHeader confidence="0.965282" genericHeader="conclusions">
7 Summary
</sectionHeader>
<bodyText confidence="0.999987448275862">
Parsers were designed to discriminate among struc-
tures, whereas language models discriminate among
strings. Small fragments, abstract rules, indepen-
dence assumptions, and errors or peculiarities in the
training corpus allow probable structures to be pro-
duced over ungrammatical text when using models
that were optimized for parser accuracy.
The experiments in this paper demonstrate the
utility of tree-substitution grammars in discriminat-
ing between grammatical and ungrammatical sen-
tences. Features are derived from the identities of
the fragments used in the derivations above a se-
quence of words; particular fragments are associated
with each outcome, and simple statistics computed
over those fragments are also useful. The most com-
plicated aspect of using TSGs is grammar learning,
for which there are publicly available tools.
Looking forward, we believe there is significant
potential for TSGs in more subtle discriminative
tasks, for example, in discriminating between finer
grained and more realistic grammatical errors (Fos-
ter and Vogel, 2004; Wagner et al., 2009), or in dis-
criminating among translation candidates in a ma-
chine translation framework. In another line of po-
tential work, it could prove useful to incorporate into
the grammar learning procedure some knowledge of
the sorts of fragments and features shown here to be
helpful for discriminating grammatical and ungram-
matical text.
</bodyText>
<sectionHeader confidence="0.996626" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.875237666666667">
Mohit Bansal and Dan Klein. 2010. Simple, accurate
parsing with an all-fragments grammar. In Proc. ACL,
Uppsala, Sweden, July.
</reference>
<page confidence="0.988492">
221
</page>
<reference confidence="0.999655903846154">
Rens Bod. 1993. Using an annotated corpus as a stochas-
tic grammar. In Proc. ACL, Columbus, Ohio, USA.
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Proc. ACL,
Toulouse, France, July.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. ACL, Ann Arbor, Michigan, USA, June.
Eugene Charniak. 1996. Tree-bank grammars. In Proc.
of the National Conference on Artificial Intelligence.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. NAACL, Seattle, Washington, USA,
April–May.
Colin Cherry and Chris Quirk. 2008. Discriminative,
syntactic language modeling through latent svms. In
Proc. AMTA, Waikiki, Hawaii, USA, October.
Trevor Cohn, Sharon. Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proc. NAACL, Boulder, Colorado, USA,
June.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Jennifer Foster and Øistein E. Andersen. 2009. Gen-
errate: generating errors for use in grammatical error
detection. In Proceedings of the fourth workshop on
innovative use of nlp for building educational appli-
cations, pages 82–90. Association for Computational
Linguistics.
Jennifer Foster and Carl Vogel. 2004. Good reasons
for noting bad grammar: Constructing a corpus of un-
grammatical language. In Pre-Proceedings of the In-
ternational Conference on Linguistic Evidence: Em-
pirical, Theoretical and Computational Perspectives.
Joshua Goodman. 1996. Efficient algorithms for pars-
ing the DOP model. In Proc. EMNLP, Philadelphia,
Pennsylvania, USA, May.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), Columbus, Ohio, June.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613–632.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages: Beyond
Words, volume 3, pages 71–122.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, 19(2):330.
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. Gleu: Automatic evaluation of sentence-
level fluency. In Proc. ACL, volume 45, page 344.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng, et al.
2004. A smorgasbord of features for statistical ma-
chine translation. In Proc. NAACL.
Daisuke Okanohara and Jun’ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In Proc. ACL, Prague, Czech Republic, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. COLING/ACL, Syd-
ney, Australia, July.
Matt Post and Daniel Gildea. 2009a. Bayesian learning
of a tree substitution grammar. In Proc. ACL (short
paper track), Suntec, Singapore, August.
Matt Post and Daniel Gildea. 2009b. Language modeling
with tree substitution grammars. In NIPS workshop on
Grammar Induction, Representation of Language, and
Language Learning, Whistler, British Columbia.
Matt Post. 2010. Syntax-based Language Models for
Statistical Machine Translation. Ph.D. thesis, Univer-
sity of Rochester.
Remko Scha. 1990. Taaltheorie en taaltechnologie; com-
petence en performance. In R. de Kort and G.L.J.
Leerdam, editors, Computertoepassingen in de neer-
landistiek, pages 7–22, Almere, the Netherlands.
Andreas Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proc. International Conference
on Spoken Language Processing.
Ghihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting erroneous sentences using automat-
ically mined sequential patterns. In Proc. ACL, vol-
ume 45.
Joachim Wagner, Jennifer Foster, and Josef van Genabith.
2009. Judging grammaticality: Experiments in sen-
tence classification. CALICO Journal, 26(3):474–490.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
features for sentence grammaticality classification. In
Proc. Australasian Language Technology Association
Workshop, Melbourne, Australia, December.
Andreas Zollmann and Khalil Sima’an. 2005. A consis-
tent and efficient estimator for Data-Oriented Parsing.
Journal of Automata, Languages and Combinatorics,
10(2/3):367–388.
Willem Zuidema. 2007. Parsimonious Data-Oriented
Parsing. In Proc. EMNLP, Prague, Czech Republic,
June.
</reference>
<page confidence="0.998002">
222
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.669660">
<title confidence="0.9999">Judging Grammaticality with Tree Substitution Grammar Derivations</title>
<author confidence="0.969226">Matt</author>
<affiliation confidence="0.8349585">Human Language Technology Center of Johns Hopkins</affiliation>
<address confidence="0.999358">Baltimore, MD 21211</address>
<abstract confidence="0.998254428571429">In this paper, we show that local features computed from the derivations of tree substitution grammars — such as the identify of particular fragments, and a count of large and small fragments — are useful in binary grammatical classification tasks. Such features outperform n-gram features and various model scores by a wide margin. Although they fall short of the performance of the hand-crafted feature set of Charniak and Johnson (2005) developed for parse tree reranking, they do so with an order of magnitude fewer features. Furthermore, since the TSGs employed are learned in a Bayesian setting, the use of their derivations can be viewed as the automatic discovery of tree patterns useful for classification. On the BLLIP dataset, we achieve an accuracy of 89.9% in discriminating between grammatical text and samples from an n-gram language model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Simple, accurate parsing with an all-fragments grammar.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<location>Uppsala, Sweden,</location>
<marker>Bansal, Klein, 2010</marker>
<rawString>Mohit Bansal and Dan Klein. 2010. Simple, accurate parsing with an all-fragments grammar. In Proc. ACL, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Using an annotated corpus as a stochastic grammar.</title>
<date>1993</date>
<booktitle>In Proc. ACL,</booktitle>
<location>Columbus, Ohio, USA.</location>
<marker>Bod, 1993</marker>
<rawString>Rens Bod. 1993. Using an annotated corpus as a stochastic grammar. In Proc. ACL, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>What is the minimal set of fragments that achieves maximal parse accuracy?</title>
<date>2001</date>
<booktitle>In Proc. ACL,</booktitle>
<location>Toulouse, France,</location>
<marker>Bod, 2001</marker>
<rawString>Rens Bod. 2001. What is the minimal set of fragments that achieves maximal parse accuracy? In Proc. ACL, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<location>Ann Arbor, Michigan, USA,</location>
<contexts>
<context position="2608" citStr="Charniak and Johnson (2005)" startWordPosition="411" endWordPosition="414">ver the past two decades in the field of parsing. This paper demonstrates the utility of local features derived from the fragments of tree substitution grammar derivations. Following Cherry and Quirk (2008), we conduct experiments in a classification setting, where the task is to distinguish between real text and “pseudo-negative” text obtained by sampling from a trigram language model (Okanohara and Tsujii, 2007). Our primary points of comparison are the latent SVM training of Cherry and Quirk (2008), mentioned above, and the extensive set of local and nonlocal feature templates developed by Charniak and Johnson (2005) for parse tree reranking. In contrast to this latter set of features, the feature sets from TSG derivations require no engineering; instead, they are obtained directly from the identity of the fragments used in the derivation, plus simple statistics computed over them. Since these fragments are in turn learned automatically from a Treebank with a Bayesian model, their usefulness here suggests a greater potential for adapting to other languages and datasets. 2 Tree substitution grammars Tree substitution grammars (Joshi and Schabes, 1997) generalize context-free grammars by allowing nontermina</context>
<context position="6210" citStr="Charniak and Johnson (2005)" startWordPosition="993" endWordPosition="996">09a)2 2The sampler was run with the default settings for 1,000 iterations, and a grammar of 192,667 fragments was then extracted from counts taken from every 10th iteration between iterations 500 and 1,000, inclusive. Code was obtained from http://github.com/mjpost/dptsg. • Model scores (5). Model log probabilities. • Rule features (R). These are counter features based on the atomic unit of the analysis, i.e., individual n-grams for the n-gram models, PCFG rules, and TSG fragments. • Reranking features (C&amp;J). From the Charniak parser output we extract the complete set of reranking features of Charniak and Johnson (2005), and just the local ones (C&amp;J local).3 • Frontier size (Fn, Fry,). Instances of this feature class count the number of TSG fragments having frontier size n, 1 &lt; n &lt; 9.4 Instances of F&apos;n count only lexical items for 0 &lt; n &lt; 5. 4.2 Results Table 2 contains the classification results. The first block of models all perform at chance. We experimented with SVM classifiers instead of maximum entropy, and the only real change across all the models was for these first five models, which saw classification rise to 55 to 60%. On the BLLIP dataset, the C&amp;J feature sets perform the best, even when the set</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proc. ACL, Ann Arbor, Michigan, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In Proc. of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="5514" citStr="Charniak, 1996" startWordPosition="887" endWordPosition="888">al., 2008). A bias of 1 was added to each feature vector. We varied a cost or regularization parameter between 1e − 5 and 100 in orders of magnitude; at each step, we built a model, evaluating it on the development set. The model with the highest score was then used to produce the result on the test set. 4.1 Base models and features Our experiments compare a number of different feature sets. Central to these feature sets are features computed from the output of four language models. 1. Bigram and trigram language models (the same ones used to generate the negative data) 2. A Treebank grammar (Charniak, 1996) 3. A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a)2 2The sampler was run with the default settings for 1,000 iterations, and a grammar of 192,667 fragments was then extracted from counts taken from every 10th iteration between iterations 500 and 1,000, inclusive. Code was obtained from http://github.com/mjpost/dptsg. • Model scores (5). Model log probabilities. • Rule features (R). These are counter features based on the atomic unit of the analysis, i.e., individual n-grams for the n-gram models, PCFG rules, and TSG fragments. • Reranking features (C&amp;J). From the Charnia</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>Eugene Charniak. 1996. Tree-bank grammars. In Proc. of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. NAACL,</booktitle>
<location>Seattle, Washington, USA, April–May.</location>
<contexts>
<context position="3890" citStr="Charniak, 2000" startWordPosition="606" endWordPosition="607">ly a sequence of one or 217 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 217–222, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics S NPV dataset training devel. test Treebank 3,836 2,690 3,398 91,954 65,474 79,998 BLLIP 100,000 6,000 6,000 2,596,508 155,247 156,353 Table 1: The number of sentences (first line) and words (second line) using for training, development, and testing of the classifier. Each set of sentences is evenly split between positive and negative examples. 4. The Charniak parser (Charniak, 2000), run in language modeling mode The parsing models for both datasets were built from sections 2 - 21 of the WSJ portion of the Treebank. These models were used to score or parse the training, development, and test data for the classifier. From the output, we extract the following feature sets used in the classifier. • Sentence length (l). and 25.6 and 26.2 for the BLLIP data. Each dataset is divided into training, development, and test sets. For the Treebank, we trained the n-gram language model on sections 2 - 21. The classifier then used sections 0, 24, and 22 for training, development, and </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proc. NAACL, Seattle, Washington, USA, April–May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Chris Quirk</author>
</authors>
<title>Discriminative, syntactic language modeling through latent svms.</title>
<date>2008</date>
<booktitle>In Proc. AMTA, Waikiki,</booktitle>
<location>Hawaii, USA,</location>
<contexts>
<context position="2187" citStr="Cherry and Quirk (2008)" startWordPosition="345" endWordPosition="348">e long-studied and well understood, and can be efficiently incorporated into search procedures, such as for machine translation. As a result, the output of such text generation systems is often very poor grammatically, even if it is understandable. Since grammaticality judgments are a matter of the syntax of a language, the obvious approach for modeling grammaticality is to start with the extensive work produced over the past two decades in the field of parsing. This paper demonstrates the utility of local features derived from the fragments of tree substitution grammar derivations. Following Cherry and Quirk (2008), we conduct experiments in a classification setting, where the task is to distinguish between real text and “pseudo-negative” text obtained by sampling from a trigram language model (Okanohara and Tsujii, 2007). Our primary points of comparison are the latent SVM training of Cherry and Quirk (2008), mentioned above, and the extensive set of local and nonlocal feature templates developed by Charniak and Johnson (2005) for parse tree reranking. In contrast to this latter set of features, the feature sets from TSG derivations require no engineering; instead, they are obtained directly from the i</context>
<context position="4571" citStr="Cherry and Quirk (2008)" startWordPosition="723" endWordPosition="726">h datasets were built from sections 2 - 21 of the WSJ portion of the Treebank. These models were used to score or parse the training, development, and test data for the classifier. From the output, we extract the following feature sets used in the classifier. • Sentence length (l). and 25.6 and 26.2 for the BLLIP data. Each dataset is divided into training, development, and test sets. For the Treebank, we trained the n-gram language model on sections 2 - 21. The classifier then used sections 0, 24, and 22 for training, development, and testing, respectively. For the BLLIP dataset, we followed Cherry and Quirk (2008): we randomly selected 450K sentences to train the n-gram language model, and 50K, 3K, and 3K sentences for classifier training, development, and testing, respectively. All sentences have 100 or fewer words. Table 1 contains statistics of the datasets used in our experiments. To build the classifier, we used liblinear (Fan et al., 2008). A bias of 1 was added to each feature vector. We varied a cost or regularization parameter between 1e − 5 and 100 in orders of magnitude; at each step, we built a model, evaluating it on the development set. The model with the highest score was then used to pr</context>
<context position="8470" citStr="Cherry and Quirk (2008)" startWordPosition="1420" endWordPosition="1423"> set Treebank BLLIP 18K 122K 15K 11K 14K 60K 24K 607K 58K 959K 14K 60K Table 3: Model size. the positive and negative data really do evince different fragments, and there are enough such features relative to the size of the training data that very high weights can be placed on them. Manual examination of feature weights bears this out. Despite having more features available, the Charniak &amp; Johnson feature set has significantly lower accuracy on the Treebank data, which suggests that the TSG features are more strongly associated with a particular (positive or negative) outcome. For comparison, Cherry and Quirk (2008) report a classification accuracy of 81.42 on BLLIP. We exclude it from the table because a direct comparison is not possible, since we did not have access to the split on the BLLIP used in their experiments, but only repeated the process they described to generate it. 5 Analysis Table 4 lists the highest-weighted TSG features associated with each outcome, taken from the BLLIP model in the last row of Table 2. The learned weights accord with the intuitions presented in Section 3. Ungrammatical sentences use smaller, abstract (unlexicalized) rules, whereas grammatical sentences use higher rank </context>
<context position="10603" citStr="Cherry and Quirk (2008)" startWordPosition="1763" endWordPosition="1766">The small fragment size, together with the coarseness of the nonterminal, permit the fragment to be used in distributional settings where it should not be licensed. This suggests some complementarity between fragment learning and work in using nonterminal refinements (Johnson, 1998; Petrov et al., 2006). 6 Related work Past approaches using parsers as language models in discriminative settings have seen varying degrees of success. Och et al. (2004) found that the score of a bilexicalized parser was not useful in distinguishing machine translation (MT) output from human reference translations. Cherry and Quirk (2008) addressed this problem by using a latent SVM to adjust the CFG rule weights such that the parser score was a much more useful discriminator between grammatical text and n-gram samples. Mutton et al. (2007) also addressed this problem by combining scores from different parsers using an SVM and showed an improved metric of fluency. l + R3 l + RP l + RT l + C&amp;J (local) l + C&amp;J l + RT + F. 220 grammatical ungrammatical (VP VBD (NP CD) Fo PP) (S (NP PRP) VP) (NP (NP CD) PP) (S NP (VP TO VP)) (TOP (NP NP NP .)) F� F5 2 (NP NP (VP VBG (S (NP (NNP UNKNP)) CAPS-NUM))) (SBAR (S (NP PRP) (TOP (S NP VP (</context>
</contexts>
<marker>Cherry, Quirk, 2008</marker>
<rawString>Colin Cherry and Chris Quirk. 2008. Discriminative, syntactic language modeling through latent svms. In Proc. AMTA, Waikiki, Hawaii, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing compact but accurate tree-substitution grammars.</title>
<date>2009</date>
<booktitle>In Proc. NAACL,</booktitle>
<location>Boulder, Colorado, USA,</location>
<marker>Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon. Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate tree-substitution grammars. In Proc. NAACL, Boulder, Colorado, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="4909" citStr="Fan et al., 2008" startWordPosition="776" endWordPosition="779"> into training, development, and test sets. For the Treebank, we trained the n-gram language model on sections 2 - 21. The classifier then used sections 0, 24, and 22 for training, development, and testing, respectively. For the BLLIP dataset, we followed Cherry and Quirk (2008): we randomly selected 450K sentences to train the n-gram language model, and 50K, 3K, and 3K sentences for classifier training, development, and testing, respectively. All sentences have 100 or fewer words. Table 1 contains statistics of the datasets used in our experiments. To build the classifier, we used liblinear (Fan et al., 2008). A bias of 1 was added to each feature vector. We varied a cost or regularization parameter between 1e − 5 and 100 in orders of magnitude; at each step, we built a model, evaluating it on the development set. The model with the highest score was then used to produce the result on the test set. 4.1 Base models and features Our experiments compare a number of different feature sets. Central to these feature sets are features computed from the output of four language models. 1. Bigram and trigram language models (the same ones used to generate the negative data) 2. A Treebank grammar (Charniak, </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Øistein E Andersen</author>
</authors>
<title>Generrate: generating errors for use in grammatical error detection.</title>
<date>2009</date>
<booktitle>In Proceedings of the</booktitle>
<pages>82--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12186" citStr="Foster and Andersen (2009)" startWordPosition="2095" endWordPosition="2099"> ,) (TOP (S SBAR , NP (QP CD (CD million)) VP .)) (ADJP (QP $ CD (CD (NP NP (CC and) NP) million))) (SBAR (IN that) (S NP (PP (IN In) NP) VP)) F8 (QP $ CD (CD million)) Table 4: Highest-weighted TSG features. Outside of MT, Foster and Vogel (2004) argued for parsers that do not assume the grammaticality of their input. Sun et al. (2007) used a set of templates to extract labeled sequential part-of-speech patterns together with some other linguistic features) which were then used in an SVM setting to classify sentences in Japanese and Chinese learners’ English corpora. Wagner et al. (2009) and Foster and Andersen (2009) attempt finer-grained, more realistic (and thus more difficult) classifications against ungrammatical text modeled on the sorts of mistakes made by language learners using parser probabilities. More recently, some researchers have shown that using features of parse trees (such as the rules grammatical ungrammatical (WHNP CD) (NN UNK-CAPS) (NP JJ NNS) (S VP) (PRT RP) (S NP) (WHNP WP NN) (TOP FRAG) (SBAR WHNP S) (NP DT JJ) (WHNP WDT NN) (NP DT) Table 5: Highest-weighted depth-one rules. used) is fruitful (Wong and Dras, 2010; Post, 2010). 7 Summary Parsers were designed to discriminate among st</context>
</contexts>
<marker>Foster, Andersen, 2009</marker>
<rawString>Jennifer Foster and Øistein E. Andersen. 2009. Generrate: generating errors for use in grammatical error detection. In Proceedings of the fourth workshop on innovative use of nlp for building educational applications, pages 82–90. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Carl Vogel</author>
</authors>
<title>Good reasons for noting bad grammar: Constructing a corpus of ungrammatical language.</title>
<date>2004</date>
<booktitle>In Pre-Proceedings of the International Conference on Linguistic Evidence: Empirical, Theoretical and Computational Perspectives.</booktitle>
<contexts>
<context position="11807" citStr="Foster and Vogel (2004)" startWordPosition="2034" endWordPosition="2037">RP) (TOP (S NP VP (. .))) VP)) (SBAR (IN that) S) (TOP (PP IN NP .)) (TOP (S NP (VP (VBD (TOP (S “ NP VP (. .))) said) NP SBAR) .)) (NP (NP DT JJ NN) (TOP (S PP NP VP .)) PP) (NP (NP NNP NNP) , (TOP (NP NP PP .)) NP ,) (TOP (S NP (ADVP F4 (RB also)) VP .)) (VP (VB be) VP) (NP (DT that) NN) (NP (NP NNS) PP) (TOP (S NP VP . ”)) (NP NP , (SBAR (TOP (NP NP , NP .)) WHNP (S VP)) ,) (TOP (S SBAR , NP (QP CD (CD million)) VP .)) (ADJP (QP $ CD (CD (NP NP (CC and) NP) million))) (SBAR (IN that) (S NP (PP (IN In) NP) VP)) F8 (QP $ CD (CD million)) Table 4: Highest-weighted TSG features. Outside of MT, Foster and Vogel (2004) argued for parsers that do not assume the grammaticality of their input. Sun et al. (2007) used a set of templates to extract labeled sequential part-of-speech patterns together with some other linguistic features) which were then used in an SVM setting to classify sentences in Japanese and Chinese learners’ English corpora. Wagner et al. (2009) and Foster and Andersen (2009) attempt finer-grained, more realistic (and thus more difficult) classifications against ungrammatical text modeled on the sorts of mistakes made by language learners using parser probabilities. More recently, some resear</context>
</contexts>
<marker>Foster, Vogel, 2004</marker>
<rawString>Jennifer Foster and Carl Vogel. 2004. Good reasons for noting bad grammar: Constructing a corpus of ungrammatical language. In Pre-Proceedings of the International Conference on Linguistic Evidence: Empirical, Theoretical and Computational Perspectives.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Efficient algorithms for parsing the DOP model.</title>
<date>1996</date>
<booktitle>In Proc. EMNLP,</booktitle>
<location>Philadelphia, Pennsylvania, USA,</location>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Efficient algorithms for parsing the DOP model. In Proc. EMNLP, Philadelphia, Pennsylvania, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7241" citStr="Huang (2008" startWordPosition="1186" endWordPosition="1187">ge across all the models was for these first five models, which saw classification rise to 55 to 60%. On the BLLIP dataset, the C&amp;J feature sets perform the best, even when the set of features is restricted to local ones. However, as shown in Table 3, this performance comes at a cost of using ten times as many features. The classifiers with TSG features outperform all the other models. The (near)-perfect performance of the TSG models on the Treebank is a result of the large number of features relative to the size of the training data: 3Local features can be computed in a bottom-up manner. See Huang (2008, §3.2) for more detail. 4A fragment’s frontier is the number of terminals and nonterminals among its leaves, also known its rank. For example, the fragment in Figure 1 has a frontier size of 5. 219 feature set length (l) 3-gram score (53) PCFG score (5P) TSG score (5T) Charniak score (5C) l + 53 l + 5P l + 5T l + 5C l + R2 l + R3 l + RP l + RT l + C&amp;J (local) l + C&amp;J l + RT + F. + Fl. Treebank BLLIP 50.0 46.4 50.0 50.1 49.5 50.0 49.5 49.7 50.0 50.0 61.0 64.3 75.6 70.4 82.4 76.2 76.3 69.1 62.4 70.6 61.3 70.7 60.4 85.0 99.4 89.3 89.1 92.5 88.6 93.0 100.0 89.9 Table 2: Classification accuracy. f</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="10262" citStr="Johnson, 1998" startWordPosition="1711" endWordPosition="1712">occurrences are in subject settings over articles that aren’t required to modify a noun, such as that, some, this, and all. However, in the BLLIP n-gram data, this rule is used over the definite article the 465 times – the second-most common use. Yet this rule occurs only nine times in the Treebank where the grammar was learned. The small fragment size, together with the coarseness of the nonterminal, permit the fragment to be used in distributional settings where it should not be licensed. This suggests some complementarity between fragment learning and work in using nonterminal refinements (Johnson, 1998; Petrov et al., 2006). 6 Related work Past approaches using parsers as language models in discriminative settings have seen varying degrees of success. Och et al. (2004) found that the score of a bilexicalized parser was not useful in distinguishing machine translation (MT) output from human reference translations. Cherry and Quirk (2008) addressed this problem by using a latent SVM to adjust the CFG rule weights such that the parser score was a much more useful discriminator between grammatical text and n-gram samples. Mutton et al. (2007) also addressed this problem by combining scores from</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Treeadjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages: Beyond Words,</booktitle>
<volume>3</volume>
<pages>71--122</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<contexts>
<context position="3152" citStr="Joshi and Schabes, 1997" startWordPosition="494" endWordPosition="497">of local and nonlocal feature templates developed by Charniak and Johnson (2005) for parse tree reranking. In contrast to this latter set of features, the feature sets from TSG derivations require no engineering; instead, they are obtained directly from the identity of the fragments used in the derivation, plus simple statistics computed over them. Since these fragments are in turn learned automatically from a Treebank with a Bayesian model, their usefulness here suggests a greater potential for adapting to other languages and datasets. 2 Tree substitution grammars Tree substitution grammars (Joshi and Schabes, 1997) generalize context-free grammars by allowing nonterminals to rewrite as tree fragments of arbitrary size, instead of as only a sequence of one or 217 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 217–222, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics S NPV dataset training devel. test Treebank 3,836 2,690 3,398 91,954 65,474 79,998 BLLIP 100,000 6,000 6,000 2,596,508 155,247 156,353 Table 1: The number of sentences (first line) and words (second line) using for training, development, and testin</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1997. Treeadjoining grammars. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages: Beyond Words, volume 3, pages 71–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational linguistics,</title>
<date>1993</date>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2):330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Mutton</author>
<author>Mark Dras</author>
<author>Stephen Wan</author>
<author>Robert Dale</author>
</authors>
<title>Gleu: Automatic evaluation of sentencelevel fluency.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<volume>45</volume>
<pages>344</pages>
<contexts>
<context position="10809" citStr="Mutton et al. (2007)" startWordPosition="1799" endWordPosition="1803">ragment learning and work in using nonterminal refinements (Johnson, 1998; Petrov et al., 2006). 6 Related work Past approaches using parsers as language models in discriminative settings have seen varying degrees of success. Och et al. (2004) found that the score of a bilexicalized parser was not useful in distinguishing machine translation (MT) output from human reference translations. Cherry and Quirk (2008) addressed this problem by using a latent SVM to adjust the CFG rule weights such that the parser score was a much more useful discriminator between grammatical text and n-gram samples. Mutton et al. (2007) also addressed this problem by combining scores from different parsers using an SVM and showed an improved metric of fluency. l + R3 l + RP l + RT l + C&amp;J (local) l + C&amp;J l + RT + F. 220 grammatical ungrammatical (VP VBD (NP CD) Fo PP) (S (NP PRP) VP) (NP (NP CD) PP) (S NP (VP TO VP)) (TOP (NP NP NP .)) F� F5 2 (NP NP (VP VBG (S (NP (NNP UNKNP)) CAPS-NUM))) (SBAR (S (NP PRP) (TOP (S NP VP (. .))) VP)) (SBAR (IN that) S) (TOP (PP IN NP .)) (TOP (S NP (VP (VBD (TOP (S “ NP VP (. .))) said) NP SBAR) .)) (NP (NP DT JJ NN) (TOP (S PP NP VP .)) PP) (NP (NP NNP NNP) , (TOP (NP NP PP .)) NP ,) (TOP (</context>
</contexts>
<marker>Mutton, Dras, Wan, Dale, 2007</marker>
<rawString>Andrew Mutton, Mark Dras, Stephen Wan, and Robert Dale. 2007. Gleu: Automatic evaluation of sentencelevel fluency. In Proc. ACL, volume 45, page 344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>Proc. NAACL.</booktitle>
<location>Libin Shen, David Smith, Katherine Eng, et</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, et al. 2004. A smorgasbord of features for statistical machine translation. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A discriminative language model with pseudo-negative samples.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2398" citStr="Okanohara and Tsujii, 2007" startWordPosition="377" endWordPosition="380">mmatically, even if it is understandable. Since grammaticality judgments are a matter of the syntax of a language, the obvious approach for modeling grammaticality is to start with the extensive work produced over the past two decades in the field of parsing. This paper demonstrates the utility of local features derived from the fragments of tree substitution grammar derivations. Following Cherry and Quirk (2008), we conduct experiments in a classification setting, where the task is to distinguish between real text and “pseudo-negative” text obtained by sampling from a trigram language model (Okanohara and Tsujii, 2007). Our primary points of comparison are the latent SVM training of Cherry and Quirk (2008), mentioned above, and the extensive set of local and nonlocal feature templates developed by Charniak and Johnson (2005) for parse tree reranking. In contrast to this latter set of features, the feature sets from TSG derivations require no engineering; instead, they are obtained directly from the identity of the fragments used in the derivation, plus simple statistics computed over them. Since these fragments are in turn learned automatically from a Treebank with a Bayesian model, their usefulness here su</context>
</contexts>
<marker>Okanohara, Tsujii, 2007</marker>
<rawString>Daisuke Okanohara and Jun’ichi Tsujii. 2007. A discriminative language model with pseudo-negative samples. In Proc. ACL, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. COLING/ACL,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="10284" citStr="Petrov et al., 2006" startWordPosition="1713" endWordPosition="1716"> in subject settings over articles that aren’t required to modify a noun, such as that, some, this, and all. However, in the BLLIP n-gram data, this rule is used over the definite article the 465 times – the second-most common use. Yet this rule occurs only nine times in the Treebank where the grammar was learned. The small fragment size, together with the coarseness of the nonterminal, permit the fragment to be used in distributional settings where it should not be licensed. This suggests some complementarity between fragment learning and work in using nonterminal refinements (Johnson, 1998; Petrov et al., 2006). 6 Related work Past approaches using parsers as language models in discriminative settings have seen varying degrees of success. Och et al. (2004) found that the score of a bilexicalized parser was not useful in distinguishing machine translation (MT) output from human reference translations. Cherry and Quirk (2008) addressed this problem by using a latent SVM to adjust the CFG rule weights such that the parser score was a much more useful discriminator between grammatical text and n-gram samples. Mutton et al. (2007) also addressed this problem by combining scores from different parsers usi</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. COLING/ACL, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of a tree substitution grammar.</title>
<date>2009</date>
<booktitle>In Proc. ACL (short paper track),</booktitle>
<location>Suntec, Singapore,</location>
<contexts>
<context position="5585" citStr="Post and Gildea, 2009" startWordPosition="895" endWordPosition="898">ed a cost or regularization parameter between 1e − 5 and 100 in orders of magnitude; at each step, we built a model, evaluating it on the development set. The model with the highest score was then used to produce the result on the test set. 4.1 Base models and features Our experiments compare a number of different feature sets. Central to these feature sets are features computed from the output of four language models. 1. Bigram and trigram language models (the same ones used to generate the negative data) 2. A Treebank grammar (Charniak, 1996) 3. A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a)2 2The sampler was run with the default settings for 1,000 iterations, and a grammar of 192,667 fragments was then extracted from counts taken from every 10th iteration between iterations 500 and 1,000, inclusive. Code was obtained from http://github.com/mjpost/dptsg. • Model scores (5). Model log probabilities. • Rule features (R). These are counter features based on the atomic unit of the analysis, i.e., individual n-grams for the n-gram models, PCFG rules, and TSG fragments. • Reranking features (C&amp;J). From the Charniak parser output we extract the complete set of reranking features of Ch</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009a. Bayesian learning of a tree substitution grammar. In Proc. ACL (short paper track), Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Language modeling with tree substitution grammars.</title>
<date>2009</date>
<booktitle>In NIPS workshop on Grammar Induction, Representation of Language, and Language Learning,</booktitle>
<location>Whistler, British Columbia.</location>
<contexts>
<context position="5585" citStr="Post and Gildea, 2009" startWordPosition="895" endWordPosition="898">ed a cost or regularization parameter between 1e − 5 and 100 in orders of magnitude; at each step, we built a model, evaluating it on the development set. The model with the highest score was then used to produce the result on the test set. 4.1 Base models and features Our experiments compare a number of different feature sets. Central to these feature sets are features computed from the output of four language models. 1. Bigram and trigram language models (the same ones used to generate the negative data) 2. A Treebank grammar (Charniak, 1996) 3. A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a)2 2The sampler was run with the default settings for 1,000 iterations, and a grammar of 192,667 fragments was then extracted from counts taken from every 10th iteration between iterations 500 and 1,000, inclusive. Code was obtained from http://github.com/mjpost/dptsg. • Model scores (5). Model log probabilities. • Rule features (R). These are counter features based on the atomic unit of the analysis, i.e., individual n-grams for the n-gram models, PCFG rules, and TSG fragments. • Reranking features (C&amp;J). From the Charniak parser output we extract the complete set of reranking features of Ch</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009b. Language modeling with tree substitution grammars. In NIPS workshop on Grammar Induction, Representation of Language, and Language Learning, Whistler, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
</authors>
<title>Syntax-based Language Models for Statistical Machine Translation.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="12728" citStr="Post, 2010" startWordPosition="2186" endWordPosition="2187"> English corpora. Wagner et al. (2009) and Foster and Andersen (2009) attempt finer-grained, more realistic (and thus more difficult) classifications against ungrammatical text modeled on the sorts of mistakes made by language learners using parser probabilities. More recently, some researchers have shown that using features of parse trees (such as the rules grammatical ungrammatical (WHNP CD) (NN UNK-CAPS) (NP JJ NNS) (S VP) (PRT RP) (S NP) (WHNP WP NN) (TOP FRAG) (SBAR WHNP S) (NP DT JJ) (WHNP WDT NN) (NP DT) Table 5: Highest-weighted depth-one rules. used) is fruitful (Wong and Dras, 2010; Post, 2010). 7 Summary Parsers were designed to discriminate among structures, whereas language models discriminate among strings. Small fragments, abstract rules, independence assumptions, and errors or peculiarities in the training corpus allow probable structures to be produced over ungrammatical text when using models that were optimized for parser accuracy. The experiments in this paper demonstrate the utility of tree-substitution grammars in discriminating between grammatical and ungrammatical sentences. Features are derived from the identities of the fragments used in the derivations above a seque</context>
</contexts>
<marker>Post, 2010</marker>
<rawString>Matt Post. 2010. Syntax-based Language Models for Statistical Machine Translation. Ph.D. thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remko Scha</author>
</authors>
<title>Taaltheorie en taaltechnologie; competence en performance. In</title>
<date>1990</date>
<booktitle>Computertoepassingen in de neerlandistiek,</booktitle>
<pages>7--22</pages>
<editor>R. de Kort and G.L.J. Leerdam, editors,</editor>
<location>Almere, the Netherlands.</location>
<marker>Scha, 1990</marker>
<rawString>Remko Scha. 1990. Taaltheorie en taaltechnologie; competence en performance. In R. de Kort and G.L.J. Leerdam, editors, Computertoepassingen in de neerlandistiek, pages 7–22, Almere, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. International Conference on Spoken Language Processing.</booktitle>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proc. International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ghihua Sun</author>
<author>Xiaohua Liu</author>
<author>Gao Cong</author>
<author>Ming Zhou</author>
<author>Zhongyang Xiong</author>
<author>John Lee</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Detecting erroneous sentences using automatically mined sequential patterns.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<volume>45</volume>
<contexts>
<context position="11898" citStr="Sun et al. (2007)" startWordPosition="2050" endWordPosition="2053">“ NP VP (. .))) said) NP SBAR) .)) (NP (NP DT JJ NN) (TOP (S PP NP VP .)) PP) (NP (NP NNP NNP) , (TOP (NP NP PP .)) NP ,) (TOP (S NP (ADVP F4 (RB also)) VP .)) (VP (VB be) VP) (NP (DT that) NN) (NP (NP NNS) PP) (TOP (S NP VP . ”)) (NP NP , (SBAR (TOP (NP NP , NP .)) WHNP (S VP)) ,) (TOP (S SBAR , NP (QP CD (CD million)) VP .)) (ADJP (QP $ CD (CD (NP NP (CC and) NP) million))) (SBAR (IN that) (S NP (PP (IN In) NP) VP)) F8 (QP $ CD (CD million)) Table 4: Highest-weighted TSG features. Outside of MT, Foster and Vogel (2004) argued for parsers that do not assume the grammaticality of their input. Sun et al. (2007) used a set of templates to extract labeled sequential part-of-speech patterns together with some other linguistic features) which were then used in an SVM setting to classify sentences in Japanese and Chinese learners’ English corpora. Wagner et al. (2009) and Foster and Andersen (2009) attempt finer-grained, more realistic (and thus more difficult) classifications against ungrammatical text modeled on the sorts of mistakes made by language learners using parser probabilities. More recently, some researchers have shown that using features of parse trees (such as the rules grammatical ungramma</context>
</contexts>
<marker>Sun, Liu, Cong, Zhou, Xiong, Lee, Lin, 2007</marker>
<rawString>Ghihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, Zhongyang Xiong, John Lee, and Chin-Yew Lin. 2007. Detecting erroneous sentences using automatically mined sequential patterns. In Proc. ACL, volume 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>Judging grammaticality: Experiments in sentence classification.</title>
<date>2009</date>
<journal>CALICO Journal,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Wagner, Foster, van Genabith, 2009</marker>
<rawString>Joachim Wagner, Jennifer Foster, and Josef van Genabith. 2009. Judging grammaticality: Experiments in sentence classification. CALICO Journal, 26(3):474–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Parser features for sentence grammaticality classification.</title>
<date>2010</date>
<booktitle>In Proc. Australasian Language Technology Association Workshop,</booktitle>
<location>Melbourne, Australia,</location>
<contexts>
<context position="12715" citStr="Wong and Dras, 2010" startWordPosition="2182" endWordPosition="2185">and Chinese learners’ English corpora. Wagner et al. (2009) and Foster and Andersen (2009) attempt finer-grained, more realistic (and thus more difficult) classifications against ungrammatical text modeled on the sorts of mistakes made by language learners using parser probabilities. More recently, some researchers have shown that using features of parse trees (such as the rules grammatical ungrammatical (WHNP CD) (NN UNK-CAPS) (NP JJ NNS) (S VP) (PRT RP) (S NP) (WHNP WP NN) (TOP FRAG) (SBAR WHNP S) (NP DT JJ) (WHNP WDT NN) (NP DT) Table 5: Highest-weighted depth-one rules. used) is fruitful (Wong and Dras, 2010; Post, 2010). 7 Summary Parsers were designed to discriminate among structures, whereas language models discriminate among strings. Small fragments, abstract rules, independence assumptions, and errors or peculiarities in the training corpus allow probable structures to be produced over ungrammatical text when using models that were optimized for parser accuracy. The experiments in this paper demonstrate the utility of tree-substitution grammars in discriminating between grammatical and ungrammatical sentences. Features are derived from the identities of the fragments used in the derivations </context>
</contexts>
<marker>Wong, Dras, 2010</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2010. Parser features for sentence grammaticality classification. In Proc. Australasian Language Technology Association Workshop, Melbourne, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Khalil Sima’an</author>
</authors>
<title>A consistent and efficient estimator for Data-Oriented Parsing.</title>
<date>2005</date>
<journal>Journal of Automata, Languages and Combinatorics,</journal>
<pages>10--2</pages>
<marker>Zollmann, Sima’an, 2005</marker>
<rawString>Andreas Zollmann and Khalil Sima’an. 2005. A consistent and efficient estimator for Data-Oriented Parsing. Journal of Automata, Languages and Combinatorics, 10(2/3):367–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem Zuidema</author>
</authors>
<title>Parsimonious Data-Oriented Parsing.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP,</booktitle>
<location>Prague, Czech Republic,</location>
<marker>Zuidema, 2007</marker>
<rawString>Willem Zuidema. 2007. Parsimonious Data-Oriented Parsing. In Proc. EMNLP, Prague, Czech Republic, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>