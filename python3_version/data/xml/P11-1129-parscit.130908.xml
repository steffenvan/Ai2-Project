<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000400">
<title confidence="0.970147">
Enhancing Language Models in Statistical Machine Translation
with Backward N-grams and Mutual Information Triggers
</title>
<author confidence="0.976087">
Deyi Xiong, Min Zhang, Haizhou Li
</author>
<affiliation confidence="0.876970666666667">
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632
</affiliation>
<email confidence="0.990077">
{dyxiong, mzhang, hli}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.996609" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999858111111111">
In this paper, with a belief that a language
model that embraces a larger context provides
better prediction ability, we present two ex-
tensions to standard n-gram language mod-
els in statistical machine translation: a back-
ward language model that augments the con-
ventional forward language model, and a mu-
tual information trigger model which captures
long-distance dependencies that go beyond
the scope of standard n-gram language mod-
els. We integrate the two proposed models
into phrase-based statistical machine transla-
tion and conduct experiments on large-scale
training data to investigate their effectiveness.
Our experimental results show that both mod-
els are able to significantly improve transla-
tion quality and collectively achieve up to 1
BLEU point over a competitive baseline.
</bodyText>
<sectionHeader confidence="0.998878" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974872340426">
Language model is one of the most important
knowledge sources for statistical machine transla-
tion (SMT) (Brown et al., 1993). The standard
n-gram language model (Goodman, 2001) assigns
probabilities to hypotheses in the target language
conditioning on a context history of the preceding
n − 1 words. Along with the efforts that advance
translation models from word-based paradigm to
syntax-based philosophy, in recent years we have
also witnessed increasing efforts dedicated to ex-
tend standard n-gram language models for SMT. We
roughly categorize these efforts into two directions:
data-volume-oriented and data-depth-oriented.
In the first direction, more data is better. In or-
der to benefit from monolingual corpora (LDC news
data or news data collected from web pages) that
consist of billions or even trillions of English words,
huge language models are built in a distributed man-
ner (Zhang et al., 2006; Brants et al., 2007). Such
language models yield better translation results but
at the cost of huge storage and high computation.
The second direction digs deeply into monolin-
gual data to build linguistically-informed language
models. For example, Charniak et al. (2003) present
a syntax-based language model for machine transla-
tion which is trained on syntactic parse trees. Again,
Shen et al. (2008) explore a dependency language
model to improve translation quality. To some ex-
tent, these syntactically-informed language models
are consistent with syntax-based translation models
in capturing long-distance dependencies.
In this paper, we pursue the second direction with-
out resorting to any linguistic resources such as a
syntactic parser. With a belief that a language model
that embraces a larger context provides better pre-
diction ability, we learn additional information from
training data to enhance conventional n-gram lan-
guage models and extend their ability to capture
richer contexts and long-distance dependencies. In
particular, we integrate backward n-grams and mu-
tual information (MI) triggers into language models
in SMT.
In conventional n-gram language models, we look
at the preceding n − 1 words when calculating the
probability of the current word. We henceforth call
the previous n − 1 words plus the current word
as forward n-grams and a language model built
</bodyText>
<page confidence="0.932957">
1288
</page>
<note confidence="0.979064">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1288–1297,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.997314533333333">
on forward n-grams as forward n-gram language
model. Similarly, backward n-grams refer to the
succeeding n − 1 words plus the current word. We
train a backward n-gram language model on back-
ward n-grams and integrate the forward and back-
ward language models together into the decoder. In
doing so, we attempt to capture both the preceding
and succeeding contexts of the current word.
Different from the backward n-gram language
model, the MI trigger model still looks at previous
contexts, which however go beyond the scope of for-
ward n-grams. If the current word is indexed as wi,
the farthest word that the forward n-gram includes
is wi−n+1. However, the MI triggers are capable of
detecting dependencies between wi and words from
w1 to wi−n. By these triggers ({wk —* wil, 1 &lt;
k &lt; i − n), we can capture long-distance dependen-
cies that are outside the scope of forward n-grams.
We integrate the proposed backward language
model and the MI trigger model into a state-of-
the-art phrase-based SMT system. We evaluate
the effectiveness of both models on Chinese-to-
English translation tasks with large-scale training
data. Compared with the baseline which only uses
the forward language model, our experimental re-
sults show that the additional backward language
model is able to gain about 0.5 BLEU points, while
the MI trigger model gains about 0.4 BLEU points.
When both models are integrated into the decoder,
they collectively improve the performance by up to
</bodyText>
<sectionHeader confidence="0.553966" genericHeader="introduction">
1 BLEU point.
</sectionHeader>
<bodyText confidence="0.999711545454546">
The paper is structured as follows. In Section 2,
we will briefly introduce related work and show how
our models differ from previous work. Section 3 and
4 will elaborate the backward language model and
the MI trigger model respectively in more detail, de-
scribe the training procedures and explain how the
models are integrated into the phrase-based decoder.
Section 5 will empirically evaluate the effectiveness
of these two models. Section 6 will conduct an in-
depth analysis. In the end, we conclude in Section
7.
</bodyText>
<sectionHeader confidence="0.999807" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996000102040816">
Previous work devoted to improving language mod-
els in SMT mostly focus on two categories as we
mentioned beforel: large language models (Zhang
et al., 2006; Emami et al., 2007; Brants et al., 2007;
Talbot and Osborne, 2007) and syntax-based lan-
guage models (Charniak et al., 2003; Shen et al.,
2008; Post and Gildea, 2008). Since our philoso-
phy is fundamentally different from them in that we
build contextually-informed language models by us-
ing backward n-grams and MI triggers, we discuss
previous work that explore these two techniques
(backward n-grams and MI triggers) in this section.
Since the context “history” in the backward lan-
guage model (BLM) is actually the future words
to be generated, BLM is normally used in a post-
processing where all words have already been gener-
ated or in a scenario where sentences are proceeded
from the ending to the beginning. Duchateau et al.
(2002) use the BLM score as a confidence measure
to detect wrongly recognized words in speech recog-
nition. Finch and Sumita (2009) use the BLM in
their reverse translation decoder where source sen-
tences are proceeded from the ending to the begin-
ning. Our BLM is different from theirs in that we ac-
cess the BLM during decoding (rather than after de-
coding) where source sentences are still proceeded
from the beginning to the ending.
Rosenfeld et al. (1994) introduce trigger pairs
into a maximum entropy based language model as
features. The trigger pairs are selected accord-
ing to their mutual information. Zhou (2004) also
propose an enhanced language model (MI-Ngram)
which consists of a standard forward n-gram lan-
guage model and an MI trigger model. The latter
model measures the mutual information of distance-
dependent trigger pairs. Our MI trigger model is
mostly inspired by the work of these two papers, es-
pecially by Zhou’s MI-Ngram model (2004). The
difference is that our model is distance-independent
and, of course, we are interested in an SMT problem
rather than a speech recognition one.
Raybaud et al. (2009) use MI triggers in their con-
fidence measures to assess the quality of translation
results after decoding. Our method is different from
theirs in the MI calculation and trigger pair selec-
tion. Mauser et al. (2009) propose bilingual triggers
where two source words trigger one target word to
&apos;Language model adaptation is not very related to our work
so we ignore it.
</bodyText>
<page confidence="0.991125">
1289
</page>
<bodyText confidence="0.999716">
improve lexical choice of target words. Our analysis
(Section 6) show that our monolingual triggers can
also help in the selection of target words.
</bodyText>
<sectionHeader confidence="0.978141" genericHeader="method">
3 Backward Language Model
</sectionHeader>
<bodyText confidence="0.838192">
Given a sequence of words wm1 = (w1...wm), a
standard forward n-gram language model assigns a
probability Pf(wm1 ) to wm1 as follows.
</bodyText>
<equation confidence="0.999147">
P(wi|wi−1
i−n+1) (1)
</equation>
<bodyText confidence="0.999716210526316">
where the approximation is based on the nth order
Markov assumption. In other words, when we pre-
dict the current word wi, we only consider the pre-
ceding n − 1 words wi−n+1...wi−1 instead of the
whole context history w1...wi−1.
Different from the forward n-gram language
model, the backward n-gram language model as-
signs a probability Pb(wm1 ) to wm1 by looking at the
succeeding context according to
et al., 2006) and 2) a standard phrase-based decoder
(Koehn et al., 2003). Both decoders translate source
sentences from the beginning of a sentence to the
ending. Wu (1996) introduce a dynamic program-
ming algorithm to integrate a forward bigram lan-
guage model with inversion transduction grammar.
His algorithm is then adapted and extended for inte-
grating forward n-gram language models into syn-
chronous CFGs by Chiang (2007). Our algorithms
are different from theirs in two major aspects
</bodyText>
<listItem confidence="0.876492666666667">
1. The string input to the algorithms is in a reverse
order.
2. We adopt a different way to calculate language
</listItem>
<bodyText confidence="0.915421833333333">
model probabilities for partial hypotheses so
that we can utilize incomplete n-grams.
Before we introduce the integration algorithms,
we define three functions P, G, and R on strings (in
a reverse order) over the English terminal alphabet
T. The function P is defined as follows.
</bodyText>
<equation confidence="0.747393">
Pf(wm1 ) = Hm P(wi|wi−1 Hm
i=1 1 ) Pz i=1
Pb(wm1 ) = Hm P(wi|wmi+1) Pz Hm P(wi|wi+n−1 P(wk...w1) =P(wk)...P(wk−n+2|wk...wk−n+3)
i=1 i=1 i+1 ) (2) \ V J
a
</equation>
<subsectionHeader confidence="0.991828">
3.1 Training
</subsectionHeader>
<bodyText confidence="0.999986769230769">
For the convenience of training, we invert the or-
der in each sentence in the training data, i.e., from
the original order (w1...wm) to the reverse order
(wm...w1). In this way, we can use the same toolkit
that we use to train a forward n-gram language
model to train a backward n-gram language model
without any other changes. To be consistent with
training, we also need to reverse the order of trans-
lation hypotheses when we access the trained back-
ward language model2. Note that the Markov con-
text history of Eq. (2) is wi+n−1...wi+1 instead of
wi+1...wi+n−1 after we invert the order. The words
are the same but the order is completely reversed.
</bodyText>
<subsectionHeader confidence="0.998907">
3.2 Decoding
</subsectionHeader>
<bodyText confidence="0.8948674">
In this section, we will present two algorithms
to integrate the backward n-gram language model
into two kinds of phrase-based decoders respec-
tively: 1) a CKY-style decoder that adopts bracket-
ing transduction grammar (BTG) (Wu, 1997; Xiong
2This is different from the reverse decoding in (Finch and
Sumita, 2009) where source sentences are reversed in the order.
This function consists of two parts:
• The first part (a) calculates incomplete n-gram
language model probabilities for word wk to
wk−n+2. That means, we calculate the uni-
gram probability for wk (P(wk)), bigram prob-
ability for wk−1 (P(wk−1|wk)) and so on un-
til we take n − 1-gram probability for wk−n+2
(P(wk−n+2|wk...wk−n+3)). This resembles
the way in which the forward language model
probability in the future cost is computed in
the standard phrase-based SMT (Koehn et al.,
2003).
• The second part (b) calculates complete n-
gram backward language model probabilities
for word wk−n+1 to w1.
The function is different from Chiang’s p func-
tion in that his function p only calculates language
model probabilities for the complete n-grams. Since
</bodyText>
<figure confidence="0.9938726">
HX P(wi|wi+n−1...wi+1)
1≤i≤k−n+1
\ V J
b
(3)
</figure>
<page confidence="0.947413">
1290
</page>
<bodyText confidence="0.999895333333333">
we calculate backward language model probabilities
during a beginning-to-ending (left-to-right) decod-
ing process, the succeeding context for the current
word is either yet to be generated or incomplete in
terms of n-grams. The P function enables us to
utilize incomplete succeeding contexts to approxi-
mately predict words. Once the succeeding con-
texts are complete, we can quickly update language
model probabilities in an efficient way in our algo-
rithms.
The other two functions G and R are defined as
follows
</bodyText>
<equation confidence="0.99252175">
� wk...wk−n+2, if k ≥ n
L(wk...w1) = wk...w1, otherwise (4)
� wn−1...w1, if k ≥ n
R(wk...w1) = wk...w1, otherwise (5)
</equation>
<bodyText confidence="0.99995596">
The G and R function return the leftmost and right-
most n − 1 words from a string in a reverse order
respectively.
Following Chiang (2007), we describe our algo-
rithms in a deductive system. We firstly show the
algorithm3 that integrates the backward language
model into a BTG-style decoder (Xiong et al., 2006)
in Figure 1. The item [A, i, j; l|r] indicates that a
BTG node A has been constructed spanning from i
to j on the source side with the leftmost|rightmost
n −1 words l|r on the target side. As mentioned be-
fore, all target strings assessed by the defined func-
tions (P, G, and R) are in an inverted order (de-
noted by e). We only display the backward lan-
guage model probability for each item, ignoring all
other scores such as phrase translation probabilities.
The Eq. (8) in Figure 1 shows how we calculate
the backward language model probability for the ax-
iom which applies a BTG lexicon rule to translate
a source phrase c into a target phrase e. The Eq.
(9) and (10) show how we update the backward lan-
guage model probabilities for two inference rules
which combine two neighboring blocks in a straight
and inverted order respectively. The fundamental
theories behind this update are
</bodyText>
<equation confidence="0.9980465">
P(e1e2) = P(e1)P(e2) P(R(e2)L(e1)) (6)
P(R(e2))P(L(e1))
</equation>
<footnote confidence="0.961591">
3It can also be easily adapted to integrate the forward n-
gram language model.
</footnote>
<table confidence="0.682556857142857">
Function Value
e1 a1a2a3
e2 b1b2b3
R(e2) b2b1
L(e1) a3a2
P(R(e2)) P(b2)P(b1|b2)
P(L(e1)) P(a3)P(a2|a3)
P(a3)P(a2|a3)P(a1|a3a2)
P(b3)P(b2|b3)P(b1|b3b2)
P(R(e2)L(e1)) P (b2)P(b1|b2)
P(a3|b2b1)P(a2|b1a3)
P (b3)P(b2|b3)P (b1|b3b2)
P(a3|b2b1)P (a2|b1a3)P(a1|a3a2)
P(e1e2)
</table>
<tableCaption confidence="0.915252">
Table 1: Values of P, L, and R in a 3-gram example.
</tableCaption>
<equation confidence="0.9996755">
P(e2e1) = P(e1)P(e2) P(R(e1)L(e2)) (7)
P(R(e1))P(L(e2))
</equation>
<bodyText confidence="0.999814192307692">
Whenever two strings e1 and e2 are concatenated
in a straight or inverted order, we can reuse their
P values (P(e1) and P(e2)) in terms of dynamic
programming. Only the probabilities of boundary
words (e.g., R(e2)G(e1) in Eq. (6)) need to be re-
calculated since they have complete n-grams after
the concatenation. Table 1 shows values of P, G,
and R in a 3-gram example which helps to verify
Eq. (6). These two equations guarantee that our
algorithm can correctly compute the backward lan-
guage model probability of a sentence stepwise in a
dynamic programming framework.4
The theoretical time complexity of this algorithm
is O(m3|T|4(n−1)) because in the update parts in
Eq. (6) and (7) both the numerator and denomina-
tor have up to 2(n−1) terminal symbols. This is the
same as the time complexity of Chiang’s language
model integration (Chiang, 2007).
Figure 2 shows the algorithm that integrates the
backward language model into a standard phrase-
based SMT (Koehn et al., 2003). V denotes a cover-
age vector which records source words translated so
far. The Eq. (11) shows how we update the back-
ward language model probability for a partial hy-
pothesis when it is extended into a longer hypothesis
by a target phrase translating an uncovered source
</bodyText>
<footnote confidence="0.9957845">
4The start-of-sentence symbol (s) and end-of-sentence sym-
bol (/s) can be easily added to update the final language model
probability when a translation hypothesis covering the whole
source sentence is completed.
</footnote>
<page confidence="0.814053">
1291
</page>
<equation confidence="0.9987595">
A → c/e (8)
[A, i, j; L(e)JR(e)] : P(e)
A → [A1, A2] [A1, i, k; L(e1)JR(e1)] : P(e1) [A2, k + 1,j; L(e2)JR(e2)] : P(e2) (9)
[A, i,j;L(e1e2)JR(e1e2)] : P(e1)P(e2) P(R(e2)L(e1))
P(R(e2))P(L(e1))
A → ⟨A1, A2⟩ [A1, i, k; L(e1)JR(e1)] : P(e1) [A2, k + 1,j; L(e2)JR(e2)] : P(e2) (10)
[A, i, j;L(e2e1)JR(e2e1)] : P(e1)P(e2) P(R(e1)L(e2))
P(R(e1))P(L(e2))
</equation>
<figureCaption confidence="0.774003">
Figure 1: Integrating the backward language model into a BTG-style decoder.
</figureCaption>
<equation confidence="0.997349333333333">
[V; L(e1)] : P(e1) c/e2 : P(e2)
P(R(e2)L(e1)) (11)
[V′;L(e1e2)] : P(e1)P(e2)P(R(e2))P(L(e1))
</equation>
<figureCaption confidence="0.998977">
Figure 2: Integrating the backward language model into
a standard phrase-based decoder.
</figureCaption>
<bodyText confidence="0.996403">
segment. This extension on the target side is simi-
lar to the monotone combination of Eq. (9) in that a
newly translated phrase is concatenated to an early
translated sequence.
</bodyText>
<sectionHeader confidence="0.99841" genericHeader="method">
4 MI Trigger Model
</sectionHeader>
<bodyText confidence="0.9999834375">
It is well-known that long-distance dependencies be-
tween words are very important for statistical lan-
guage modeling. However, n-gram language models
can only capture short-distance dependencies within
an n-word window. In order to model long-distance
dependencies, previous work such as (Rosenfeld et
al., 1994) and (Zhou, 2004) exploit trigger pairs. A
trigger pair is defined as an ordered 2-tuple (x, y)
where word x occurs in the preceding context of
word y. It can also be denoted in a more visual man-
ner as x → y with x being the trigger and y the
triggered words.
We use pointwise mutual information (PMI)
(Church and Hanks, 1990) to measure the strength
of the association between x and y, which is defined
as follows
</bodyText>
<equation confidence="0.998635">
PMI(x, y) = log( P (x, y)
P (x)P (y)) (12)
</equation>
<footnote confidence="0.984439">
5In this paper, we require that word x and y occur in the
same sentence.
</footnote>
<bodyText confidence="0.919702666666667">
Zhou (2004) proposes a new language model en-
hanced with MI trigger pairs. In his model, the prob-
ability of a given sentence wi&apos; is approximated as
</bodyText>
<equation confidence="0.999531">
P (wi|wi−1
i−n+1))
exp(PMI(wk, wi, i − k − 1))
(13)
</equation>
<bodyText confidence="0.999966428571429">
There are two components in his model. The first
component is still the standard n-gram language
model. The second one is the MI trigger model
which multiples all exponential PMI values for trig-
ger pairs where the current word is the triggered
word and all preceding words outside the n-gram
window of the current word are triggers. Note that
his MI trigger model is distance-dependent since
trigger pairs (wk, wi) are sensitive to their distance
i − k − 1 (zero distance for adjacent words). There-
fore the distance between word x and word y should
be taken into account when calculating their PMI.
In this paper, for simplicity, we adopt a distance-
independent MI trigger model as follows
</bodyText>
<equation confidence="0.980084">
exp(PMI(wk,wi)) (14)
</equation>
<bodyText confidence="0.999875111111111">
We integrate the MI trigger model into the log-
linear model of machine translation as an additional
knowledge source which complements the standard
n-gram language model in capturing long-distance
dependencies. By MERT (Och, 2003), we are even
able to tune the weight of the MI trigger model
against the weight of the standard n-gram language
model while Zhou (2004) sets equal weights for both
models.
</bodyText>
<equation confidence="0.994634">
P(wm1 ) Pz( Hm
i=1
m
× H
i=n+1
i−nH
k=1
MI(wm1 ) = m i−nH
H k=1
i=n+1
</equation>
<page confidence="0.977868">
1292
</page>
<subsectionHeader confidence="0.980324">
4.1 Training
</subsectionHeader>
<bodyText confidence="0.999937">
We can use the maximum likelihood estimation
method to calculate PMI for each trigger pair by tak-
ing counts from training data. Let C(x, y) be the
co-occurrence count of the trigger pair (x, y) in the
training data. The joint probability of (x, y) is cal-
culated as
phrase-based decoder. But we still can handle it by
dynamic programming as follows
</bodyText>
<equation confidence="0.999819">
MI(e1e2) = MI(e1)MI(e2)MI(e1 → e2) (18)
</equation>
<bodyText confidence="0.999927333333333">
where MI(e1 → e2) represents the PMI values in
which a word in e1 triggers a word in e2. It is defined
as follows
</bodyText>
<equation confidence="0.984762857142857">
C(x,y)
P(x,y) = (15)
Ex,y C(x, y)
�MI(e1 → e2) = �
w;Ee� w�Ee1
i−k&gt;n
exp(PMI(wk, wi))
</equation>
<bodyText confidence="0.998643">
The marginal probabilities of x and y can be de-
duced from the joint probability as follows
</bodyText>
<equation confidence="0.98984125">
�
P(x) = y P(x, y) (16)
�
P(y) = x P(x, y) (17)
</equation>
<bodyText confidence="0.98361325">
Since the number of distinct trigger pairs is
O(|T |2), the question is how to select valuable trig-
ger pairs. We select trigger pairs according to the
following three steps
</bodyText>
<listItem confidence="0.925007090909091">
1. The distance between x and y must not be less
than n − 1. Suppose we use a 5-gram language
model and y = wi , then x E {w1...wi−5}.
2. C(x, y) &gt; c. In all our experiments we set c =
10.
3. Finally, we only keep trigger pairs whose PMI
value is larger than 0. Trigger pairs whose PMI
value is less than 0 often contain stop words,
such as “the”, “a”. These stop words have very
large marginal probabilities due to their high
frequencies.
</listItem>
<subsectionHeader confidence="0.994018">
4.2 Decoding
</subsectionHeader>
<bodyText confidence="0.9998743">
The MI trigger model of Eq. (14) can be directly
integrated into the decoder. For the standard phrase-
based decoder (Koehn et al., 2003), whenever a par-
tial hypothesis is extended by a new target phrase,
we can quickly retrieve the pre-computed PMI value
for each trigger pair where the triggered word lo-
cates in the newly translated target phrase and the
trigger is outside the n-word window of the trig-
gered word. It’s a little more complicated to in-
tegrate the MI trigger model into the CKY-style
</bodyText>
<equation confidence="0.611641">
(19)
</equation>
<sectionHeader confidence="0.992474" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999306">
In this section, we conduct large-scale experiments
on NIST Chinese-to-English translation tasks to
evaluate the effectiveness of the proposed backward
language model and MI trigger model in SMT. Our
experiments focus on the following two issues:
</bodyText>
<listItem confidence="0.997786833333333">
1. How much improvements can we achieve by
separately integrating the backward language
model and the MI trigger model into our
phrase-based SMT system?
2. Can we obtain a further improvement if we
jointly apply both models?
</listItem>
<subsectionHeader confidence="0.994965">
5.1 System Overview
</subsectionHeader>
<bodyText confidence="0.9992688">
Without loss of generality6, we evaluate our models
in a phrase-based SMT system which adapts brack-
eting transduction grammars to phrasal translation
(Xiong et al., 2006). The log-linear model of this
system can be formulated as
</bodyText>
<equation confidence="0.989888">
w(D) =MT(rl1..n�) - MR(rm1..n,,)λx (20)
- PfL(e)λfL - exp(|e|)λ�
</equation>
<bodyText confidence="0.945352636363636">
where D denotes a derivation, ri 1..nl are the BTG
lexicon rules which translate source phrases to tar-
get phrases, and r�1..nm are the merging rules which
combine two neighboring blocks into a larger block
in a straight or inverted order. The translation
model MT consists of widely used phrase and lex-
ical translation probabilities (Koehn et al., 2003).
6We have discussed how to integrate the backward language
model and the MI trigger model into the standard phrase-based
SMT system (Koehn et al., 2003) in Section 3.2 and 4.2 respec-
tively.
</bodyText>
<page confidence="0.968354">
1293
</page>
<bodyText confidence="0.9999455">
The reordering model MR predicts the merging or-
der (straight or inverted) by using discriminative
contextual features (Xiong et al., 2006). PfL is the
standard forward n-gram language model.
If we simultaneously integrate both the backward
language model PbL and the MI trigger model MI
into the system, the new log-linear model will be
formulated as
</bodyText>
<equation confidence="0.9996425">
w(D) =MT(ri..n�) · MR(rn1..nm)aR · PfL(e)afL ·(21)
PbL(e)abL · MI(e)amI · exp( |e|)aw
</equation>
<subsectionHeader confidence="0.998367">
5.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999515115384616">
Our training corpora7 consist of 96.9M Chinese
words and 109.5M English words in 3.8M sentence
pairs. We used all corpora to train our translation
model and smaller corpora without the United Na-
tions corpus to build a maximum entropy based re-
ordering model (Xiong et al., 2006).
To train our language models and MI trigger
model, we used the Xinhua section of the En-
glish Gigaword corpus (306 million words). Firstly,
we built a forward 5-gram language model using
the SRILM toolkit (Stolcke, 2002) with modified
Kneser-Ney smoothing. Then we trained a back-
ward 5-gram language model on the same monolin-
gual corpus in the way described in Section 3.1. Fi-
nally, we trained our MI trigger model still on this
corpus according to the method in Section 4.1. The
trained MI trigger model consists of 2.88M trigger
pairs.
We used the NIST MT03 evaluation test data as
the development set, and the NIST MT04, MT05 as
the test sets. We adopted the case-insensitive BLEU-
4 (Papineni et al., 2002) as the evaluation metric,
which uses the shortest reference sentence length for
the brevity penalty. Statistical significance in BLEU
differences is tested by paired bootstrap re-sampling
(Koehn, 2004).
</bodyText>
<subsectionHeader confidence="0.998736">
5.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999898">
The experimental results on the two NIST test sets
are shown in Table 2. When we combine the back-
ward language model with the forward language
</bodyText>
<footnote confidence="0.774064333333333">
7LDC2004E12, LDC2004T08, LDC2005T10,
LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07
and LDC2004T07.
</footnote>
<table confidence="0.999072">
Model MT-04 MT-05
Forward (Baseline) 35.67 34.41
Forward+Backward 36.16+ 34.97+
Forward+MI 36.00+ 34.85+
Forward+Backward+MI 36.76+ 35.12+
</table>
<tableCaption confidence="0.990009333333333">
Table 2: BLEU-4 scores (%) on the two test sets for dif-
ferent language models and their combinations. +: better
than the baseline (p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.999962333333333">
model, we obtain 0.49 and 0.56 BLEU points over
the baseline on the MT-04 and MT-05 test set respec-
tively. Both improvements are statistically signifi-
cant (p &lt; 0.01). The MI trigger model also achieves
statistically significant improvements of 0.33 and
0.44 BLEU points over the baseline on the MT-04
and MT-05 respectively.
When we integrate both the backward language
model and the MI trigger model into our system,
we obtain improvements of 1.09 and 0.71 BLEU
points over the single forward language model on
the MT-04 and MT-05 respectively. These improve-
ments are larger than those achieved by using only
one model (the backward language model or the MI
trigger model).
</bodyText>
<sectionHeader confidence="0.96836" genericHeader="method">
6 Analysis
</sectionHeader>
<bodyText confidence="0.999927761904762">
In this section, we will study more details of the two
models by looking at the differences that they make
on translation hypotheses. These differences will
help us gain some insights into how the presented
models improve translation quality.
Table 3 shows an example from our test set. The
italic words in the hypothesis generated by using the
backward language model (F+B) exactly match the
reference. However, the italic words in the base-
line hypothesis fail to match the reference due to
the incorrect position of the word “decree” (法令) .
We calculate the forward/backward language model
score (the logarithm of language model probability)
for the italic words in both the baseline and F+B hy-
pothesis according to the trained language models.
The difference in the forward language model score
is only 1.58, which may be offset by differences in
other features in the log-linear translation model. On
the other hand, the difference in the backward lan-
guage model score is 3.52. This larger difference
may guarantee that the hypothesis generated by F+B
</bodyText>
<page confidence="0.969587">
1294
</page>
<table confidence="0.999402466666667">
Source ILA, 4-14K 4K� , ILA, AAkM A
A aLK —it* M WKI 7 A&apos;I M
11
Baseline Beijing Youth Daily reported that
Beijing Agricultural decree recently
issued a series of control and super-
vision
F+B Beijing Youth Daily reported that
Beijing Bureau of Agriculture re-
cently issued a series of prevention
and control laws
Reference Beijing Youth Daily reported that
Beijing Bureau of Agriculture re-
cently issued a series of preventative
and monitoring ordinances
</table>
<tableCaption confidence="0.990979666666667">
Table 3: Translation example from the MT-04 test set,
comparing the baseline with the backward language
model. F+B: forward+backward language model.
</tableCaption>
<bodyText confidence="0.99997008">
is better enough to be selected as the best hypothe-
sis by the decoder. This suggests that the backward
language model is able to provide useful and dis-
criminative information which is complementary to
that given by the forward language model.
In Table 4, we present another example to show
how the MI trigger model improves translation qual-
ity. The major difference in hypotheses of this ex-
ample is the word choice between “is” and “was”.
The new system enhanced with the MI trigger model
(F+M) selects the former while the baseline selects
the latter. The forward language model score for the
baseline hypothesis is -26.41, which is higher than
the score of the F+M hypothesis -26.67. This could
be the reason why the baseline selects the word
“was” instead of “is”. As can be seen, there is an-
other “is” in the preceding context of the word “was”
in the baseline hypothesis. Unfortunately, this word
“is” is located just outside the scope of the preceding
5-gram context of “was”. The forward 5-gram lan-
guage model is hence not able to take it into account
when calculating the probability of “was”. However,
this is not a problem for the MI trigger model. Since
“is” and “was” rarely co-occur in the same sentence,
the PMI value of the trigger pair (is, was)8 is -1.03
</bodyText>
<footnote confidence="0.923274666666667">
8Since we remove all trigger pairs whose PMI value is neg-
ative, the PMI value of this pair (is, was) is set 0 in practice in
the decoder.
</footnote>
<table confidence="0.99311625">
Source AT_P, kIT z.Fl�4 q Aari , A
Q)� L #_4 A —^ AA M ��
0
Baseline Self-Defense Force ’s trip is remark-
able , because it was not an isolated
incident .
F+M Self-Defense Force ’s trip is remark-
able , because it is not an isolated in-
cident .
Reference The Self-Defense Forces’ trip
arouses attention because it is not an
isolated incident.
</table>
<tableCaption confidence="0.903268">
Table 4: Translation example from the MT-04 test set,
comparing the baseline with the MI trigger model. Both
system outputs are not detokenized so that we can see
</tableCaption>
<bodyText confidence="0.959093">
how language model scores are calculated. The un-
derlined words highlight the difference between the en-
hanced models and the baseline. F+M: forward language
model + MI trigger model.
while the PMI value of the trigger pair (is, is) is as
high as 0.32. Therefore our MI trigger model selects
“is” rather than “was”.9 This example illustrates that
the MI trigger model is capable of selecting correct
words by using long-distance trigger pairs.
</bodyText>
<sectionHeader confidence="0.99845" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999945411764706">
We have presented two models to enhance the abil-
ity of standard n-gram language models in captur-
ing richer contexts and long-distance dependencies
that go beyond the scope of forward n-gram win-
dows. The two models have been integrated into
the decoder and have shown to improve a state-of-
the-art phrase-based SMT system. The first model
is the backward language model which uses back-
ward n-grams to predict the current word. We in-
troduced algorithms that directly integrate the back-
ward language model into a CKY-style and a stan-
dard phrase-based decoder respectively. The sec-
ond model is the MI trigger model that incorporates
long-distance trigger pairs into language modeling.
Overall improvements are up to 1 BLEU point on
the NIST Chinese-to-English translation tasks with
large-scale training data. Further study of the two
</bodyText>
<footnote confidence="0.968591333333333">
9The overall MI trigger model scores (the logarithm of Eq.
(14)) of the baseline hypothesis and the F+M hypothesis are
2.09 and 2.25 respectively.
</footnote>
<page confidence="0.990623">
1295
</page>
<bodyText confidence="0.999857363636364">
models indicates that backward n-grams and long-
distance triggers provide useful information to im-
prove translation quality.
In future work, we would like to integrate the
backward language model into a syntax-based sys-
tem in a way that is similar to the proposed algo-
rithm shown in Figure 1. We are also interested in
exploring more morphologically- or syntactically-
informed triggers. For example, a verb in the past
tense triggers another verb also in the past tense
rather than the present tense.
</bodyText>
<sectionHeader confidence="0.997874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999784494505495">
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 858–
867, Prague, Czech Republic, June. Association for
Computational Linguistics.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proceedings of MT Summit IX.
Intl. Assoc. for Machine Translation.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22–29.
Jacques Duchateau, Kris Demuynck, and Patrick
Wambacq. 2002. Confidence scoring based on back-
ward language models. In Proceedings of ICASSP,
pages 221–224, Orlando, FL, April.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of ICASSP, pages 37–40, Honolulu, HI,
April.
Andrew Finch and Eiichiro Sumita. 2009. Bidirectional
phrase-based statistical machine translation. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1124–
1132, Singapore, August. Association for Computa-
tional Linguistics.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling extended version. Technical report,
Microsoft Research.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 58–54, Edmon-
ton, Canada, May-June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388–395, Barcelona, Spain, July.
Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 210–218, Singa-
pore, August. Association for Computational Linguis-
tics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160–167, Sapporo, Japan,
July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July.
Matt Post and Daniel Gildea. 2008. Parsers as language
models for statistical machine translation. In Proceed-
ings of AMTA.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
and Kamel Smaili. 2009. New confidence measures
for statistical machine translation. In Proceedings of
the International Conference on Agents and Artificial
Intelligence, pages 61–68, Porto, Portugal, January.
Roni Rosenfeld, Jaime Carbonell, and Alexander Rud-
nicky. 1994. Adaptive statistical language model-
ing: A maximum entropy approach. Technical report,
Carnegie Mellon University.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings ofACL-08: HLT, pages 577–585, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. Srilm–an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901–904, Denver, Colorado, USA, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine translation.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 512–519,
</reference>
<page confidence="0.787858">
1296
</page>
<reference confidence="0.999627592592593">
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proceedings of the 34th
Annual Meeting of the Association for Computational
Linguistics, pages 152–158, Santa Cruz, California,
USA, June.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 521–528, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list
re-ranking. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 216–223, Sydney, Australia, July. Association
for Computational Linguistics.
GuoDong Zhou. 2004. Modeling of long distance con-
text dependency. In Proceedings of Coling, pages 92–
98, Geneva, Switzerland, Aug 23–Aug 27. COLING.
</reference>
<page confidence="0.992139">
1297
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.522915">
<title confidence="0.9995555">Enhancing Language Models in Statistical Machine with Backward N-grams and Mutual Information Triggers</title>
<author confidence="0.826011">Deyi Xiong</author>
<author confidence="0.826011">Min Zhang</author>
<author confidence="0.826011">Haizhou Human Language</author>
<affiliation confidence="0.950003">Institute for Infocomm</affiliation>
<address confidence="0.789412">1 Fusionopolis Way, #21-01 Connexis, Singapore</address>
<email confidence="0.988637">mzhang,</email>
<abstract confidence="0.998949421052632">In this paper, with a belief that a language model that embraces a larger context provides better prediction ability, we present two exto standard language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond scope of standard language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>858--867</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2045" citStr="Brants et al., 2007" startWordPosition="301" endWordPosition="304">the efforts that advance translation models from word-based paradigm to syntax-based philosophy, in recent years we have also witnessed increasing efforts dedicated to extend standard n-gram language models for SMT. We roughly categorize these efforts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed manner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation. The second direction digs deeply into monolingual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine translation which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing long-distance dependenci</context>
<context position="5805" citStr="Brants et al., 2007" startWordPosition="902" endWordPosition="905">dels differ from previous work. Section 3 and 4 will elaborate the backward language model and the MI trigger model respectively in more detail, describe the training procedures and explain how the models are integrated into the phrase-based decoder. Section 5 will empirically evaluate the effectiveness of these two models. Section 6 will conduct an indepth analysis. In the end, we conclude in Section 7. 2 Related Work Previous work devoted to improving language models in SMT mostly focus on two categories as we mentioned beforel: large language models (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007; Talbot and Osborne, 2007) and syntax-based language models (Charniak et al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philosophy is fundamentally different from them in that we build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a s</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 858– 867, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1238" citStr="Brown et al., 1993" startWordPosition="176" endWordPosition="179"> information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline. 1 Introduction Language model is one of the most important knowledge sources for statistical machine translation (SMT) (Brown et al., 1993). The standard n-gram language model (Goodman, 2001) assigns probabilities to hypotheses in the target language conditioning on a context history of the preceding n − 1 words. Along with the efforts that advance translation models from word-based paradigm to syntax-based philosophy, in recent years we have also witnessed increasing efforts dedicated to extend standard n-gram language models for SMT. We roughly categorize these efforts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Kevin Knight</author>
<author>Kenji Yamada</author>
</authors>
<title>Syntax-based language models for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IX. Intl. Assoc. for Machine Translation.</booktitle>
<contexts>
<context position="2295" citStr="Charniak et al. (2003)" startWordPosition="338" endWordPosition="341">rts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed manner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation. The second direction digs deeply into monolingual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine translation which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing long-distance dependencies. In this paper, we pursue the second direction without resorting to any linguistic resources such as a syntactic parser. With a belief that a language model that embraces a larger context provides better prediction ability, we learn additional inf</context>
<context position="5888" citStr="Charniak et al., 2003" startWordPosition="915" endWordPosition="918">age model and the MI trigger model respectively in more detail, describe the training procedures and explain how the models are integrated into the phrase-based decoder. Section 5 will empirically evaluate the effectiveness of these two models. Section 6 will conduct an indepth analysis. In the end, we conclude in Section 7. 2 Related Work Previous work devoted to improving language models in SMT mostly focus on two categories as we mentioned beforel: large language models (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007; Talbot and Osborne, 2007) and syntax-based language models (Charniak et al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philosophy is fundamentally different from them in that we build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a scenario where sentences are proceeded from the ending to the beginning. Duchateau e</context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for statistical machine translation. In Proceedings of MT Summit IX. Intl. Assoc. for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="9145" citStr="Chiang (2007)" startWordPosition="1458" endWordPosition="1459">ry w1...wi−1. Different from the forward n-gram language model, the backward n-gram language model assigns a probability Pb(wm1 ) to wm1 by looking at the succeeding context according to et al., 2006) and 2) a standard phrase-based decoder (Koehn et al., 2003). Both decoders translate source sentences from the beginning of a sentence to the ending. Wu (1996) introduce a dynamic programming algorithm to integrate a forward bigram language model with inversion transduction grammar. His algorithm is then adapted and extended for integrating forward n-gram language models into synchronous CFGs by Chiang (2007). Our algorithms are different from theirs in two major aspects 1. The string input to the algorithms is in a reverse order. 2. We adopt a different way to calculate language model probabilities for partial hypotheses so that we can utilize incomplete n-grams. Before we introduce the integration algorithms, we define three functions P, G, and R on strings (in a reverse order) over the English terminal alphabet T. The function P is defined as follows. Pf(wm1 ) = Hm P(wi|wi−1 Hm i=1 1 ) Pz i=1 Pb(wm1 ) = Hm P(wi|wmi+1) Pz Hm P(wi|wi+n−1 P(wk...w1) =P(wk)...P(wk−n+2|wk...wk−n+3) i=1 i=1 i+1 ) (2)</context>
<context position="12354" citStr="Chiang (2007)" startWordPosition="1997" endWordPosition="1998">word is either yet to be generated or incomplete in terms of n-grams. The P function enables us to utilize incomplete succeeding contexts to approximately predict words. Once the succeeding contexts are complete, we can quickly update language model probabilities in an efficient way in our algorithms. The other two functions G and R are defined as follows � wk...wk−n+2, if k ≥ n L(wk...w1) = wk...w1, otherwise (4) � wn−1...w1, if k ≥ n R(wk...w1) = wk...w1, otherwise (5) The G and R function return the leftmost and rightmost n − 1 words from a string in a reverse order respectively. Following Chiang (2007), we describe our algorithms in a deductive system. We firstly show the algorithm3 that integrates the backward language model into a BTG-style decoder (Xiong et al., 2006) in Figure 1. The item [A, i, j; l|r] indicates that a BTG node A has been constructed spanning from i to j on the source side with the leftmost|rightmost n −1 words l|r on the target side. As mentioned before, all target strings assessed by the defined functions (P, G, and R) are in an inverted order (denoted by e). We only display the backward language model probability for each item, ignoring all other scores such as phra</context>
<context position="14770" citStr="Chiang, 2007" startWordPosition="2390" endWordPosition="2391">o be recalculated since they have complete n-grams after the concatenation. Table 1 shows values of P, G, and R in a 3-gram example which helps to verify Eq. (6). These two equations guarantee that our algorithm can correctly compute the backward language model probability of a sentence stepwise in a dynamic programming framework.4 The theoretical time complexity of this algorithm is O(m3|T|4(n−1)) because in the update parts in Eq. (6) and (7) both the numerator and denominator have up to 2(n−1) terminal symbols. This is the same as the time complexity of Chiang’s language model integration (Chiang, 2007). Figure 2 shows the algorithm that integrates the backward language model into a standard phrasebased SMT (Koehn et al., 2003). V denotes a coverage vector which records source words translated so far. The Eq. (11) shows how we update the backward language model probability for a partial hypothesis when it is extended into a longer hypothesis by a target phrase translating an uncovered source 4The start-of-sentence symbol (s) and end-of-sentence symbol (/s) can be easily added to update the final language model probability when a translation hypothesis covering the whole source sentence is co</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="16821" citStr="Church and Hanks, 1990" startWordPosition="2723" endWordPosition="2726">wn that long-distance dependencies between words are very important for statistical language modeling. However, n-gram language models can only capture short-distance dependencies within an n-word window. In order to model long-distance dependencies, previous work such as (Rosenfeld et al., 1994) and (Zhou, 2004) exploit trigger pairs. A trigger pair is defined as an ordered 2-tuple (x, y) where word x occurs in the preceding context of word y. It can also be denoted in a more visual manner as x → y with x being the trigger and y the triggered words. We use pointwise mutual information (PMI) (Church and Hanks, 1990) to measure the strength of the association between x and y, which is defined as follows PMI(x, y) = log( P (x, y) P (x)P (y)) (12) 5In this paper, we require that word x and y occur in the same sentence. Zhou (2004) proposes a new language model enhanced with MI trigger pairs. In his model, the probability of a given sentence wi&apos; is approximated as P (wi|wi−1 i−n+1)) exp(PMI(wk, wi, i − k − 1)) (13) There are two components in his model. The first component is still the standard n-gram language model. The second one is the MI trigger model which multiples all exponential PMI values for trigge</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Duchateau</author>
<author>Kris Demuynck</author>
<author>Patrick Wambacq</author>
</authors>
<title>Confidence scoring based on backward language models.</title>
<date>2002</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>221--224</pages>
<location>Orlando, FL,</location>
<contexts>
<context position="6500" citStr="Duchateau et al. (2002)" startWordPosition="1017" endWordPosition="1020">t al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philosophy is fundamentally different from them in that we build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a scenario where sentences are proceeded from the ending to the beginning. Duchateau et al. (2002) use the BLM score as a confidence measure to detect wrongly recognized words in speech recognition. Finch and Sumita (2009) use the BLM in their reverse translation decoder where source sentences are proceeded from the ending to the beginning. Our BLM is different from theirs in that we access the BLM during decoding (rather than after decoding) where source sentences are still proceeded from the beginning to the ending. Rosenfeld et al. (1994) introduce trigger pairs into a maximum entropy based language model as features. The trigger pairs are selected according to their mutual information.</context>
</contexts>
<marker>Duchateau, Demuynck, Wambacq, 2002</marker>
<rawString>Jacques Duchateau, Kris Demuynck, and Patrick Wambacq. 2002. Confidence scoring based on backward language models. In Proceedings of ICASSP, pages 221–224, Orlando, FL, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad Emami</author>
<author>Kishore Papineni</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Large-scale distributed language modeling.</title>
<date>2007</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>37--40</pages>
<location>Honolulu, HI,</location>
<contexts>
<context position="5784" citStr="Emami et al., 2007" startWordPosition="898" endWordPosition="901"> and show how our models differ from previous work. Section 3 and 4 will elaborate the backward language model and the MI trigger model respectively in more detail, describe the training procedures and explain how the models are integrated into the phrase-based decoder. Section 5 will empirically evaluate the effectiveness of these two models. Section 6 will conduct an indepth analysis. In the end, we conclude in Section 7. 2 Related Work Previous work devoted to improving language models in SMT mostly focus on two categories as we mentioned beforel: large language models (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007; Talbot and Osborne, 2007) and syntax-based language models (Charniak et al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philosophy is fundamentally different from them in that we build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already bee</context>
</contexts>
<marker>Emami, Papineni, Sorensen, 2007</marker>
<rawString>Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen. 2007. Large-scale distributed language modeling. In Proceedings of ICASSP, pages 37–40, Honolulu, HI, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Bidirectional phrase-based statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1124--1132</pages>
<institution>Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="6624" citStr="Finch and Sumita (2009)" startWordPosition="1038" endWordPosition="1041"> build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a scenario where sentences are proceeded from the ending to the beginning. Duchateau et al. (2002) use the BLM score as a confidence measure to detect wrongly recognized words in speech recognition. Finch and Sumita (2009) use the BLM in their reverse translation decoder where source sentences are proceeded from the ending to the beginning. Our BLM is different from theirs in that we access the BLM during decoding (rather than after decoding) where source sentences are still proceeded from the beginning to the ending. Rosenfeld et al. (1994) introduce trigger pairs into a maximum entropy based language model as features. The trigger pairs are selected according to their mutual information. Zhou (2004) also propose an enhanced language model (MI-Ngram) which consists of a standard forward n-gram language model a</context>
<context position="10742" citStr="Finch and Sumita, 2009" startWordPosition="1729" endWordPosition="1732">ining, we also need to reverse the order of translation hypotheses when we access the trained backward language model2. Note that the Markov context history of Eq. (2) is wi+n−1...wi+1 instead of wi+1...wi+n−1 after we invert the order. The words are the same but the order is completely reversed. 3.2 Decoding In this section, we will present two algorithms to integrate the backward n-gram language model into two kinds of phrase-based decoders respectively: 1) a CKY-style decoder that adopts bracketing transduction grammar (BTG) (Wu, 1997; Xiong 2This is different from the reverse decoding in (Finch and Sumita, 2009) where source sentences are reversed in the order. This function consists of two parts: • The first part (a) calculates incomplete n-gram language model probabilities for word wk to wk−n+2. That means, we calculate the unigram probability for wk (P(wk)), bigram probability for wk−1 (P(wk−1|wk)) and so on until we take n − 1-gram probability for wk−n+2 (P(wk−n+2|wk...wk−n+3)). This resembles the way in which the forward language model probability in the future cost is computed in the standard phrase-based SMT (Koehn et al., 2003). • The second part (b) calculates complete ngram backward languag</context>
</contexts>
<marker>Finch, Sumita, 2009</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2009. Bidirectional phrase-based statistical machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1124– 1132, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>A bit of progress in language modeling extended version.</title>
<date>2001</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="1290" citStr="Goodman, 2001" startWordPosition="185" endWordPosition="186">ependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline. 1 Introduction Language model is one of the most important knowledge sources for statistical machine translation (SMT) (Brown et al., 1993). The standard n-gram language model (Goodman, 2001) assigns probabilities to hypotheses in the target language conditioning on a context history of the preceding n − 1 words. Along with the efforts that advance translation models from word-based paradigm to syntax-based philosophy, in recent years we have also witnessed increasing efforts dedicated to extend standard n-gram language models for SMT. We roughly categorize these efforts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC news data or news data collected from web pages) th</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua T. Goodman. 2001. A bit of progress in language modeling extended version. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>58--54</pages>
<location>Edmonton, Canada, May-June.</location>
<contexts>
<context position="8792" citStr="Koehn et al., 2003" startWordPosition="1401" endWordPosition="1404">equence of words wm1 = (w1...wm), a standard forward n-gram language model assigns a probability Pf(wm1 ) to wm1 as follows. P(wi|wi−1 i−n+1) (1) where the approximation is based on the nth order Markov assumption. In other words, when we predict the current word wi, we only consider the preceding n − 1 words wi−n+1...wi−1 instead of the whole context history w1...wi−1. Different from the forward n-gram language model, the backward n-gram language model assigns a probability Pb(wm1 ) to wm1 by looking at the succeeding context according to et al., 2006) and 2) a standard phrase-based decoder (Koehn et al., 2003). Both decoders translate source sentences from the beginning of a sentence to the ending. Wu (1996) introduce a dynamic programming algorithm to integrate a forward bigram language model with inversion transduction grammar. His algorithm is then adapted and extended for integrating forward n-gram language models into synchronous CFGs by Chiang (2007). Our algorithms are different from theirs in two major aspects 1. The string input to the algorithms is in a reverse order. 2. We adopt a different way to calculate language model probabilities for partial hypotheses so that we can utilize incomp</context>
<context position="11276" citStr="Koehn et al., 2003" startWordPosition="1816" endWordPosition="1819"> 1997; Xiong 2This is different from the reverse decoding in (Finch and Sumita, 2009) where source sentences are reversed in the order. This function consists of two parts: • The first part (a) calculates incomplete n-gram language model probabilities for word wk to wk−n+2. That means, we calculate the unigram probability for wk (P(wk)), bigram probability for wk−1 (P(wk−1|wk)) and so on until we take n − 1-gram probability for wk−n+2 (P(wk−n+2|wk...wk−n+3)). This resembles the way in which the forward language model probability in the future cost is computed in the standard phrase-based SMT (Koehn et al., 2003). • The second part (b) calculates complete ngram backward language model probabilities for word wk−n+1 to w1. The function is different from Chiang’s p function in that his function p only calculates language model probabilities for the complete n-grams. Since HX P(wi|wi+n−1...wi+1) 1≤i≤k−n+1 \ V J b (3) 1290 we calculate backward language model probabilities during a beginning-to-ending (left-to-right) decoding process, the succeeding context for the current word is either yet to be generated or incomplete in terms of n-grams. The P function enables us to utilize incomplete succeeding contex</context>
<context position="14897" citStr="Koehn et al., 2003" startWordPosition="2409" endWordPosition="2412">am example which helps to verify Eq. (6). These two equations guarantee that our algorithm can correctly compute the backward language model probability of a sentence stepwise in a dynamic programming framework.4 The theoretical time complexity of this algorithm is O(m3|T|4(n−1)) because in the update parts in Eq. (6) and (7) both the numerator and denominator have up to 2(n−1) terminal symbols. This is the same as the time complexity of Chiang’s language model integration (Chiang, 2007). Figure 2 shows the algorithm that integrates the backward language model into a standard phrasebased SMT (Koehn et al., 2003). V denotes a coverage vector which records source words translated so far. The Eq. (11) shows how we update the backward language model probability for a partial hypothesis when it is extended into a longer hypothesis by a target phrase translating an uncovered source 4The start-of-sentence symbol (s) and end-of-sentence symbol (/s) can be easily added to update the final language model probability when a translation hypothesis covering the whole source sentence is completed. 1291 A → c/e (8) [A, i, j; L(e)JR(e)] : P(e) A → [A1, A2] [A1, i, k; L(e1)JR(e1)] : P(e1) [A2, k + 1,j; L(e2)JR(e2)] :</context>
<context position="19913" citStr="Koehn et al., 2003" startWordPosition="3296" endWordPosition="3299">g to the following three steps 1. The distance between x and y must not be less than n − 1. Suppose we use a 5-gram language model and y = wi , then x E {w1...wi−5}. 2. C(x, y) &gt; c. In all our experiments we set c = 10. 3. Finally, we only keep trigger pairs whose PMI value is larger than 0. Trigger pairs whose PMI value is less than 0 often contain stop words, such as “the”, “a”. These stop words have very large marginal probabilities due to their high frequencies. 4.2 Decoding The MI trigger model of Eq. (14) can be directly integrated into the decoder. For the standard phrasebased decoder (Koehn et al., 2003), whenever a partial hypothesis is extended by a new target phrase, we can quickly retrieve the pre-computed PMI value for each trigger pair where the triggered word locates in the newly translated target phrase and the trigger is outside the n-word window of the triggered word. It’s a little more complicated to integrate the MI trigger model into the CKY-style (19) 5 Experiments In this section, we conduct large-scale experiments on NIST Chinese-to-English translation tasks to evaluate the effectiveness of the proposed backward language model and MI trigger model in SMT. Our experiments focus</context>
<context position="21435" citStr="Koehn et al., 2003" startWordPosition="3543" endWordPosition="3546"> we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al., 2006). The log-linear model of this system can be formulated as w(D) =MT(rl1..n�) - MR(rm1..n,,)λx (20) - PfL(e)λfL - exp(|e|)λ� where D denotes a derivation, ri 1..nl are the BTG lexicon rules which translate source phrases to target phrases, and r�1..nm are the merging rules which combine two neighboring blocks into a larger block in a straight or inverted order. The translation model MT consists of widely used phrase and lexical translation probabilities (Koehn et al., 2003). 6We have discussed how to integrate the backward language model and the MI trigger model into the standard phrase-based SMT system (Koehn et al., 2003) in Section 3.2 and 4.2 respectively. 1293 The reordering model MR predicts the merging order (straight or inverted) by using discriminative contextual features (Xiong et al., 2006). PfL is the standard forward n-gram language model. If we simultaneously integrate both the backward language model PbL and the MI trigger model MI into the system, the new log-linear model will be formulated as w(D) =MT(ri..n�) · MR(rn1..nm)aR · PfL(e)afL ·(21) Pb</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 58–54, Edmonton, Canada, May-June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="23279" citStr="Koehn, 2004" startWordPosition="3848" endWordPosition="3849">e model on the same monolingual corpus in the way described in Section 3.1. Finally, we trained our MI trigger model still on this corpus according to the method in Section 4.1. The trained MI trigger model consists of 2.88M trigger pairs. We used the NIST MT03 evaluation test data as the development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU4 (Papineni et al., 2002) as the evaluation metric, which uses the shortest reference sentence length for the brevity penalty. Statistical significance in BLEU differences is tested by paired bootstrap re-sampling (Koehn, 2004). 5.3 Experimental Results The experimental results on the two NIST test sets are shown in Table 2. When we combine the backward language model with the forward language 7LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07 and LDC2004T07. Model MT-04 MT-05 Forward (Baseline) 35.67 34.41 Forward+Backward 36.16+ 34.97+ Forward+MI 36.00+ 34.85+ Forward+Backward+MI 36.76+ 35.12+ Table 2: BLEU-4 scores (%) on the two test sets for different language models and their combinations. +: better than the baseline (p &lt; 0.01). model, we obtain 0.49 and 0.56 BLEU points over t</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Extending statistical machine translation with discriminative and trigger-based lexicon models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>210--218</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7831" citStr="Mauser et al. (2009)" startWordPosition="1239" endWordPosition="1242">guage model and an MI trigger model. The latter model measures the mutual information of distancedependent trigger pairs. Our MI trigger model is mostly inspired by the work of these two papers, especially by Zhou’s MI-Ngram model (2004). The difference is that our model is distance-independent and, of course, we are interested in an SMT problem rather than a speech recognition one. Raybaud et al. (2009) use MI triggers in their confidence measures to assess the quality of translation results after decoding. Our method is different from theirs in the MI calculation and trigger pair selection. Mauser et al. (2009) propose bilingual triggers where two source words trigger one target word to &apos;Language model adaptation is not very related to our work so we ignore it. 1289 improve lexical choice of target words. Our analysis (Section 6) show that our monolingual triggers can also help in the selection of target words. 3 Backward Language Model Given a sequence of words wm1 = (w1...wm), a standard forward n-gram language model assigns a probability Pf(wm1 ) to wm1 as follows. P(wi|wi−1 i−n+1) (1) where the approximation is based on the nth order Markov assumption. In other words, when we predict the current</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009. Extending statistical machine translation with discriminative and trigger-based lexicon models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 210–218, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="18164" citStr="Och, 2003" startWordPosition="2960" endWordPosition="2961">rs. Note that his MI trigger model is distance-dependent since trigger pairs (wk, wi) are sensitive to their distance i − k − 1 (zero distance for adjacent words). Therefore the distance between word x and word y should be taken into account when calculating their PMI. In this paper, for simplicity, we adopt a distanceindependent MI trigger model as follows exp(PMI(wk,wi)) (14) We integrate the MI trigger model into the loglinear model of machine translation as an additional knowledge source which complements the standard n-gram language model in capturing long-distance dependencies. By MERT (Och, 2003), we are even able to tune the weight of the MI trigger model against the weight of the standard n-gram language model while Zhou (2004) sets equal weights for both models. P(wm1 ) Pz( Hm i=1 m × H i=n+1 i−nH k=1 MI(wm1 ) = m i−nH H k=1 i=n+1 1292 4.1 Training We can use the maximum likelihood estimation method to calculate PMI for each trigger pair by taking counts from training data. Let C(x, y) be the co-occurrence count of the trigger pair (x, y) in the training data. The joint probability of (x, y) is calculated as phrase-based decoder. But we still can handle it by dynamic programming as</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="23077" citStr="Papineni et al., 2002" startWordPosition="3818" endWordPosition="3821">English Gigaword corpus (306 million words). Firstly, we built a forward 5-gram language model using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. Then we trained a backward 5-gram language model on the same monolingual corpus in the way described in Section 3.1. Finally, we trained our MI trigger model still on this corpus according to the method in Section 4.1. The trained MI trigger model consists of 2.88M trigger pairs. We used the NIST MT03 evaluation test data as the development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU4 (Papineni et al., 2002) as the evaluation metric, which uses the shortest reference sentence length for the brevity penalty. Statistical significance in BLEU differences is tested by paired bootstrap re-sampling (Koehn, 2004). 5.3 Experimental Results The experimental results on the two NIST test sets are shown in Table 2. When we combine the backward language model with the forward language 7LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07 and LDC2004T07. Model MT-04 MT-05 Forward (Baseline) 35.67 34.41 Forward+Backward 36.16+ 34.97+ Forward+MI 36.00+ 34.85+ Forward+Backward+MI 36.</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Parsers as language models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="5931" citStr="Post and Gildea, 2008" startWordPosition="923" endWordPosition="926">ively in more detail, describe the training procedures and explain how the models are integrated into the phrase-based decoder. Section 5 will empirically evaluate the effectiveness of these two models. Section 6 will conduct an indepth analysis. In the end, we conclude in Section 7. 2 Related Work Previous work devoted to improving language models in SMT mostly focus on two categories as we mentioned beforel: large language models (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007; Talbot and Osborne, 2007) and syntax-based language models (Charniak et al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philosophy is fundamentally different from them in that we build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a scenario where sentences are proceeded from the ending to the beginning. Duchateau et al. (2002) use the BLM score as a confide</context>
</contexts>
<marker>Post, Gildea, 2008</marker>
<rawString>Matt Post and Daniel Gildea. 2008. Parsers as language models for statistical machine translation. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Raybaud</author>
<author>Caroline Lavecchia</author>
<author>David Langlois</author>
<author>Kamel Smaili</author>
</authors>
<title>New confidence measures for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Agents and Artificial Intelligence,</booktitle>
<pages>61--68</pages>
<location>Porto, Portugal,</location>
<contexts>
<context position="7618" citStr="Raybaud et al. (2009)" startWordPosition="1203" endWordPosition="1206">based language model as features. The trigger pairs are selected according to their mutual information. Zhou (2004) also propose an enhanced language model (MI-Ngram) which consists of a standard forward n-gram language model and an MI trigger model. The latter model measures the mutual information of distancedependent trigger pairs. Our MI trigger model is mostly inspired by the work of these two papers, especially by Zhou’s MI-Ngram model (2004). The difference is that our model is distance-independent and, of course, we are interested in an SMT problem rather than a speech recognition one. Raybaud et al. (2009) use MI triggers in their confidence measures to assess the quality of translation results after decoding. Our method is different from theirs in the MI calculation and trigger pair selection. Mauser et al. (2009) propose bilingual triggers where two source words trigger one target word to &apos;Language model adaptation is not very related to our work so we ignore it. 1289 improve lexical choice of target words. Our analysis (Section 6) show that our monolingual triggers can also help in the selection of target words. 3 Backward Language Model Given a sequence of words wm1 = (w1...wm), a standard </context>
</contexts>
<marker>Raybaud, Lavecchia, Langlois, Smaili, 2009</marker>
<rawString>Sylvain Raybaud, Caroline Lavecchia, David Langlois, and Kamel Smaili. 2009. New confidence measures for statistical machine translation. In Proceedings of the International Conference on Agents and Artificial Intelligence, pages 61–68, Porto, Portugal, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roni Rosenfeld</author>
<author>Jaime Carbonell</author>
<author>Alexander Rudnicky</author>
</authors>
<title>Adaptive statistical language modeling: A maximum entropy approach.</title>
<date>1994</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="6949" citStr="Rosenfeld et al. (1994)" startWordPosition="1095" endWordPosition="1098"> used in a postprocessing where all words have already been generated or in a scenario where sentences are proceeded from the ending to the beginning. Duchateau et al. (2002) use the BLM score as a confidence measure to detect wrongly recognized words in speech recognition. Finch and Sumita (2009) use the BLM in their reverse translation decoder where source sentences are proceeded from the ending to the beginning. Our BLM is different from theirs in that we access the BLM during decoding (rather than after decoding) where source sentences are still proceeded from the beginning to the ending. Rosenfeld et al. (1994) introduce trigger pairs into a maximum entropy based language model as features. The trigger pairs are selected according to their mutual information. Zhou (2004) also propose an enhanced language model (MI-Ngram) which consists of a standard forward n-gram language model and an MI trigger model. The latter model measures the mutual information of distancedependent trigger pairs. Our MI trigger model is mostly inspired by the work of these two papers, especially by Zhou’s MI-Ngram model (2004). The difference is that our model is distance-independent and, of course, we are interested in an SM</context>
<context position="16495" citStr="Rosenfeld et al., 1994" startWordPosition="2660" endWordPosition="2663">: P(e1)P(e2)P(R(e2))P(L(e1)) Figure 2: Integrating the backward language model into a standard phrase-based decoder. segment. This extension on the target side is similar to the monotone combination of Eq. (9) in that a newly translated phrase is concatenated to an early translated sequence. 4 MI Trigger Model It is well-known that long-distance dependencies between words are very important for statistical language modeling. However, n-gram language models can only capture short-distance dependencies within an n-word window. In order to model long-distance dependencies, previous work such as (Rosenfeld et al., 1994) and (Zhou, 2004) exploit trigger pairs. A trigger pair is defined as an ordered 2-tuple (x, y) where word x occurs in the preceding context of word y. It can also be denoted in a more visual manner as x → y with x being the trigger and y the triggered words. We use pointwise mutual information (PMI) (Church and Hanks, 1990) to measure the strength of the association between x and y, which is defined as follows PMI(x, y) = log( P (x, y) P (x)P (y)) (12) 5In this paper, we require that word x and y occur in the same sentence. Zhou (2004) proposes a new language model enhanced with MI trigger pa</context>
</contexts>
<marker>Rosenfeld, Carbonell, Rudnicky, 1994</marker>
<rawString>Roni Rosenfeld, Jaime Carbonell, and Alexander Rudnicky. 1994. Adaptive statistical language modeling: A maximum entropy approach. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>577--585</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2426" citStr="Shen et al. (2008)" startWordPosition="359" endWordPosition="362"> from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed manner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation. The second direction digs deeply into monolingual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine translation which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing long-distance dependencies. In this paper, we pursue the second direction without resorting to any linguistic resources such as a syntactic parser. With a belief that a language model that embraces a larger context provides better prediction ability, we learn additional information from training data to enhance conventional n-gram language models and extend their ability to capture richer contexts and </context>
<context position="5907" citStr="Shen et al., 2008" startWordPosition="919" endWordPosition="922">igger model respectively in more detail, describe the training procedures and explain how the models are integrated into the phrase-based decoder. Section 5 will empirically evaluate the effectiveness of these two models. Section 6 will conduct an indepth analysis. In the end, we conclude in Section 7. 2 Related Work Previous work devoted to improving language models in SMT mostly focus on two categories as we mentioned beforel: large language models (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007; Talbot and Osborne, 2007) and syntax-based language models (Charniak et al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philosophy is fundamentally different from them in that we build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a scenario where sentences are proceeded from the ending to the beginning. Duchateau et al. (2002) use th</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings ofACL-08: HLT, pages 577–585, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm–an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="22589" citStr="Stolcke, 2002" startWordPosition="3734" endWordPosition="3735">ted as w(D) =MT(ri..n�) · MR(rn1..nm)aR · PfL(e)afL ·(21) PbL(e)abL · MI(e)amI · exp( |e|)aw 5.2 Experimental Setup Our training corpora7 consist of 96.9M Chinese words and 109.5M English words in 3.8M sentence pairs. We used all corpora to train our translation model and smaller corpora without the United Nations corpus to build a maximum entropy based reordering model (Xiong et al., 2006). To train our language models and MI trigger model, we used the Xinhua section of the English Gigaword corpus (306 million words). Firstly, we built a forward 5-gram language model using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. Then we trained a backward 5-gram language model on the same monolingual corpus in the way described in Section 3.1. Finally, we trained our MI trigger model still on this corpus according to the method in Section 4.1. The trained MI trigger model consists of 2.88M trigger pairs. We used the NIST MT03 evaluation test data as the development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU4 (Papineni et al., 2002) as the evaluation metric, which uses the shortest reference sentence length for the brevity penalty. Statistica</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm–an extensible language modeling toolkit. In Proceedings of the 7th International Conference on Spoken Language Processing, pages 901–904, Denver, Colorado, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>512--519</pages>
<contexts>
<context position="5832" citStr="Talbot and Osborne, 2007" startWordPosition="906" endWordPosition="909">ious work. Section 3 and 4 will elaborate the backward language model and the MI trigger model respectively in more detail, describe the training procedures and explain how the models are integrated into the phrase-based decoder. Section 5 will empirically evaluate the effectiveness of these two models. Section 6 will conduct an indepth analysis. In the end, we conclude in Section 7. 2 Related Work Previous work devoted to improving language models in SMT mostly focus on two categories as we mentioned beforel: large language models (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007; Talbot and Osborne, 2007) and syntax-based language models (Charniak et al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philosophy is fundamentally different from them in that we build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a scenario where sentences are</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 512–519,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Czech Republic Prague</author>
</authors>
<date></date>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Prague, </marker>
<rawString>Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>152--158</pages>
<location>Santa Cruz, California, USA,</location>
<contexts>
<context position="8892" citStr="Wu (1996)" startWordPosition="1419" endWordPosition="1420">m1 as follows. P(wi|wi−1 i−n+1) (1) where the approximation is based on the nth order Markov assumption. In other words, when we predict the current word wi, we only consider the preceding n − 1 words wi−n+1...wi−1 instead of the whole context history w1...wi−1. Different from the forward n-gram language model, the backward n-gram language model assigns a probability Pb(wm1 ) to wm1 by looking at the succeeding context according to et al., 2006) and 2) a standard phrase-based decoder (Koehn et al., 2003). Both decoders translate source sentences from the beginning of a sentence to the ending. Wu (1996) introduce a dynamic programming algorithm to integrate a forward bigram language model with inversion transduction grammar. His algorithm is then adapted and extended for integrating forward n-gram language models into synchronous CFGs by Chiang (2007). Our algorithms are different from theirs in two major aspects 1. The string input to the algorithms is in a reverse order. 2. We adopt a different way to calculate language model probabilities for partial hypotheses so that we can utilize incomplete n-grams. Before we introduce the integration algorithms, we define three functions P, G, and R </context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Dekai Wu. 1996. A polynomial-time algorithm for statistical machine translation. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 152–158, Santa Cruz, California, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="10662" citStr="Wu, 1997" startWordPosition="1718" endWordPosition="1719">nguage model without any other changes. To be consistent with training, we also need to reverse the order of translation hypotheses when we access the trained backward language model2. Note that the Markov context history of Eq. (2) is wi+n−1...wi+1 instead of wi+1...wi+n−1 after we invert the order. The words are the same but the order is completely reversed. 3.2 Decoding In this section, we will present two algorithms to integrate the backward n-gram language model into two kinds of phrase-based decoders respectively: 1) a CKY-style decoder that adopts bracketing transduction grammar (BTG) (Wu, 1997; Xiong 2This is different from the reverse decoding in (Finch and Sumita, 2009) where source sentences are reversed in the order. This function consists of two parts: • The first part (a) calculates incomplete n-gram language model probabilities for word wk to wk−n+2. That means, we calculate the unigram probability for wk (P(wk)), bigram probability for wk−1 (P(wk−1|wk)) and so on until we take n − 1-gram probability for wk−n+2 (P(wk−n+2|wk...wk−n+3)). This resembles the way in which the forward language model probability in the future cost is computed in the standard phrase-based SMT (Koehn</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>521--528</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="12526" citStr="Xiong et al., 2006" startWordPosition="2023" endWordPosition="2026">ds. Once the succeeding contexts are complete, we can quickly update language model probabilities in an efficient way in our algorithms. The other two functions G and R are defined as follows � wk...wk−n+2, if k ≥ n L(wk...w1) = wk...w1, otherwise (4) � wn−1...w1, if k ≥ n R(wk...w1) = wk...w1, otherwise (5) The G and R function return the leftmost and rightmost n − 1 words from a string in a reverse order respectively. Following Chiang (2007), we describe our algorithms in a deductive system. We firstly show the algorithm3 that integrates the backward language model into a BTG-style decoder (Xiong et al., 2006) in Figure 1. The item [A, i, j; l|r] indicates that a BTG node A has been constructed spanning from i to j on the source side with the leftmost|rightmost n −1 words l|r on the target side. As mentioned before, all target strings assessed by the defined functions (P, G, and R) are in an inverted order (denoted by e). We only display the backward language model probability for each item, ignoring all other scores such as phrase translation probabilities. The Eq. (8) in Figure 1 shows how we calculate the backward language model probability for the axiom which applies a BTG lexicon rule to trans</context>
<context position="20958" citStr="Xiong et al., 2006" startWordPosition="3465" endWordPosition="3468">eriments on NIST Chinese-to-English translation tasks to evaluate the effectiveness of the proposed backward language model and MI trigger model in SMT. Our experiments focus on the following two issues: 1. How much improvements can we achieve by separately integrating the backward language model and the MI trigger model into our phrase-based SMT system? 2. Can we obtain a further improvement if we jointly apply both models? 5.1 System Overview Without loss of generality6, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al., 2006). The log-linear model of this system can be formulated as w(D) =MT(rl1..n�) - MR(rm1..n,,)λx (20) - PfL(e)λfL - exp(|e|)λ� where D denotes a derivation, ri 1..nl are the BTG lexicon rules which translate source phrases to target phrases, and r�1..nm are the merging rules which combine two neighboring blocks into a larger block in a straight or inverted order. The translation model MT consists of widely used phrase and lexical translation probabilities (Koehn et al., 2003). 6We have discussed how to integrate the backward language model and the MI trigger model into the standard phrase-based S</context>
<context position="22368" citStr="Xiong et al., 2006" startWordPosition="3695" endWordPosition="3698">(Xiong et al., 2006). PfL is the standard forward n-gram language model. If we simultaneously integrate both the backward language model PbL and the MI trigger model MI into the system, the new log-linear model will be formulated as w(D) =MT(ri..n�) · MR(rn1..nm)aR · PfL(e)afL ·(21) PbL(e)abL · MI(e)amI · exp( |e|)aw 5.2 Experimental Setup Our training corpora7 consist of 96.9M Chinese words and 109.5M English words in 3.8M sentence pairs. We used all corpora to train our translation model and smaller corpora without the United Nations corpus to build a maximum entropy based reordering model (Xiong et al., 2006). To train our language models and MI trigger model, we used the Xinhua section of the English Gigaword corpus (306 million words). Firstly, we built a forward 5-gram language model using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. Then we trained a backward 5-gram language model on the same monolingual corpus in the way described in Section 3.1. Finally, we trained our MI trigger model still on this corpus according to the method in Section 4.1. The trained MI trigger model consists of 2.88M trigger pairs. We used the NIST MT03 evaluation test data as the development</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 521–528, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Distributed language modeling for n-best list re-ranking.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>216--223</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="2023" citStr="Zhang et al., 2006" startWordPosition="297" endWordPosition="300">1 words. Along with the efforts that advance translation models from word-based paradigm to syntax-based philosophy, in recent years we have also witnessed increasing efforts dedicated to extend standard n-gram language models for SMT. We roughly categorize these efforts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed manner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation. The second direction digs deeply into monolingual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine translation which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing lo</context>
<context position="5764" citStr="Zhang et al., 2006" startWordPosition="894" endWordPosition="897">troduce related work and show how our models differ from previous work. Section 3 and 4 will elaborate the backward language model and the MI trigger model respectively in more detail, describe the training procedures and explain how the models are integrated into the phrase-based decoder. Section 5 will empirically evaluate the effectiveness of these two models. Section 6 will conduct an indepth analysis. In the end, we conclude in Section 7. 2 Related Work Previous work devoted to improving language models in SMT mostly focus on two categories as we mentioned beforel: large language models (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007; Talbot and Osborne, 2007) and syntax-based language models (Charniak et al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philosophy is fundamentally different from them in that we build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all wo</context>
</contexts>
<marker>Zhang, Hildebrand, Vogel, 2006</marker>
<rawString>Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel. 2006. Distributed language modeling for n-best list re-ranking. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 216–223, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
</authors>
<title>Modeling of long distance context dependency.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>92--98</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="7112" citStr="Zhou (2004)" startWordPosition="1122" endWordPosition="1123">use the BLM score as a confidence measure to detect wrongly recognized words in speech recognition. Finch and Sumita (2009) use the BLM in their reverse translation decoder where source sentences are proceeded from the ending to the beginning. Our BLM is different from theirs in that we access the BLM during decoding (rather than after decoding) where source sentences are still proceeded from the beginning to the ending. Rosenfeld et al. (1994) introduce trigger pairs into a maximum entropy based language model as features. The trigger pairs are selected according to their mutual information. Zhou (2004) also propose an enhanced language model (MI-Ngram) which consists of a standard forward n-gram language model and an MI trigger model. The latter model measures the mutual information of distancedependent trigger pairs. Our MI trigger model is mostly inspired by the work of these two papers, especially by Zhou’s MI-Ngram model (2004). The difference is that our model is distance-independent and, of course, we are interested in an SMT problem rather than a speech recognition one. Raybaud et al. (2009) use MI triggers in their confidence measures to assess the quality of translation results aft</context>
<context position="16512" citStr="Zhou, 2004" startWordPosition="2665" endWordPosition="2666">Figure 2: Integrating the backward language model into a standard phrase-based decoder. segment. This extension on the target side is similar to the monotone combination of Eq. (9) in that a newly translated phrase is concatenated to an early translated sequence. 4 MI Trigger Model It is well-known that long-distance dependencies between words are very important for statistical language modeling. However, n-gram language models can only capture short-distance dependencies within an n-word window. In order to model long-distance dependencies, previous work such as (Rosenfeld et al., 1994) and (Zhou, 2004) exploit trigger pairs. A trigger pair is defined as an ordered 2-tuple (x, y) where word x occurs in the preceding context of word y. It can also be denoted in a more visual manner as x → y with x being the trigger and y the triggered words. We use pointwise mutual information (PMI) (Church and Hanks, 1990) to measure the strength of the association between x and y, which is defined as follows PMI(x, y) = log( P (x, y) P (x)P (y)) (12) 5In this paper, we require that word x and y occur in the same sentence. Zhou (2004) proposes a new language model enhanced with MI trigger pairs. In his model</context>
<context position="18300" citStr="Zhou (2004)" startWordPosition="2985" endWordPosition="2986">istance for adjacent words). Therefore the distance between word x and word y should be taken into account when calculating their PMI. In this paper, for simplicity, we adopt a distanceindependent MI trigger model as follows exp(PMI(wk,wi)) (14) We integrate the MI trigger model into the loglinear model of machine translation as an additional knowledge source which complements the standard n-gram language model in capturing long-distance dependencies. By MERT (Och, 2003), we are even able to tune the weight of the MI trigger model against the weight of the standard n-gram language model while Zhou (2004) sets equal weights for both models. P(wm1 ) Pz( Hm i=1 m × H i=n+1 i−nH k=1 MI(wm1 ) = m i−nH H k=1 i=n+1 1292 4.1 Training We can use the maximum likelihood estimation method to calculate PMI for each trigger pair by taking counts from training data. Let C(x, y) be the co-occurrence count of the trigger pair (x, y) in the training data. The joint probability of (x, y) is calculated as phrase-based decoder. But we still can handle it by dynamic programming as follows MI(e1e2) = MI(e1)MI(e2)MI(e1 → e2) (18) where MI(e1 → e2) represents the PMI values in which a word in e1 triggers a word in e2</context>
</contexts>
<marker>Zhou, 2004</marker>
<rawString>GuoDong Zhou. 2004. Modeling of long distance context dependency. In Proceedings of Coling, pages 92– 98, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>