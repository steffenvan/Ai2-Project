<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.969869">
Investigating GIS and Smoothing for Maximum Entropy Taggers
</title>
<author confidence="0.985835">
James R. Curran and Stephen Clark
</author>
<affiliation confidence="0.952409333333333">
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh. EH8 9LW
</affiliation>
<email confidence="0.992636">
fjamesc,stephencl@cogsci.ed.ac.uk
</email>
<sectionHeader confidence="0.998571" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9958974">
This paper investigates two elements
of Maximum Entropy tagging: the use
of a correction feature in the Gener-
alised Iterative Scaling (Gis) estimation
algorithm, and techniques for model
smoothing. We show analytically and
empirically that the correction feature,
assumed to be required for the correct-
ness of GIS, is unnecessary. We also ex-
plore the use of a Gaussian prior and a
simple cutoff for smoothing. The exper-
iments are performed with two tagsets:
the standard Penn Treebank POS tagset
and the larger set of lexical types from
Combinatory Categorial Grammar.
</bodyText>
<sectionHeader confidence="0.999494" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954653061224">
The use of maximum entropy (ME) models has
become popular in Statistical NLP; some exam-
ple applications include part-of-speech (Pos) tag-
ging (Ratnaparkhi, 1996), parsing (Ratnaparkhi,
1999; Johnson et al., 1999) and language mod-
elling (Rosenfeld, 1996). Many tagging problems
have been successfully modelled in the ME frame-
work, including POS tagging, with state of the
art performance (van Halteren et al., 2001), &amp;quot;su-
pertagging&amp;quot; (Clark, 2002) and chunking (Koeling,
2000).
Generalised Iterative Scaling (GIS) is a very
simple algorithm for estimating the parameters of
a ME model. The original formulation of GIS (Dar-
roch and Ratcliff, 1972) required the sum of the
feature values for each event to be constant. Since
this is not the case for many applications, the stan-
dard method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;,
feature to each event Improved Iterative Scal-
ing (us) (Berger et al., 1996; Della Pietra et al.,
1997) eliminated the correction feature to improve
the convergence rate of the algorithm. However,
the extra book keeping required for us means that
GIS is often faster in practice (Malouf, 2002). This
paper shows, by a simple adaptation of Berger&apos;s
proof for the convergence of HS (Berger, 1997),
that GIS does not require a correction feature. We
also investigate how the use of a correction feature
affects the performance of ME taggers.
GIS and HS obtain a maximum likelihood es-
timate (mLE) of the parameters, and, like other
MLE methods, are susceptible to overfitting. A
simple technique used to avoid overfitting is a fre-
quency cutoff, in which only frequently occurring
features are included in the model (Ratnaparkhi,
1998). However, more sophisticated smoothing
techniques exist, such as the use of a Gaussian
prior on the parameters of the model (Chen and
Rosenfeld, 1999). This technique has been ap-
plied to language modelling (Chen and Rosenfeld,
1999), text classification (Nigam et al., 1999) and
parsing (Johnson et al., 1999), but to our knowl-
edge it has not been compared with the use of
a feature cutoff. We explore the combination of
Gaussian smoothing and a simple cutoff for two
tagging tasks.
The two taggers used for the experiments are
a POS tagger, trained on the WSJ Penn Treebank,
and a &amp;quot;supertagger&amp;quot;, which assigns tags from the
</bodyText>
<page confidence="0.997516">
91
</page>
<bodyText confidence="0.999881">
much larger set of lexical types from Combinatory
Categorial Grammar (ccG) (Clark, 2002). Elimi-
nation of the correction feature and use of appro-
priate smoothing methods result in state of the art
performance for both tagging tasks.
</bodyText>
<sectionHeader confidence="0.982827" genericHeader="introduction">
2 Maximum Entropy Models
</sectionHeader>
<bodyText confidence="0.99416">
A conditional ME model, also known as a log-
linear model, has the following form:
</bodyText>
<equation confidence="0.996477">
1
P(Y1X) = exp 37)) (1)
</equation>
<bodyText confidence="0.999486333333333">
where the functions fi are the features of the
model, the A, are the parameters, or weights, and
Z(x) is a normalisation constant. This form can be
derived by choosing the model with maximum en-
tropy (i.e. the most uniform model) from a set of
models that satisfy a certain set of constraints. The
constraints are that the expected value of each fea-
ture fi according to the model p is equal to some
value Ki (Rosenfeld, 1996):
</bodyText>
<equation confidence="0.9961955">
E p(x, y)fi(x, y) = Ki (2)
x,)
</equation>
<bodyText confidence="0.99920775">
Calculating the expected value according to p
requires summing over all contexts x, which is not
possible in practice. Therefore we use the now
standard approximation (Rosenfeld, 1996):
</bodyText>
<equation confidence="0.9581885">
E p(x,y).fi(x,y) E 1--.)(x)p(yix)fi(x,y) (3)
x,y
</equation>
<bodyText confidence="0.9999601">
where p(x) is the relative frequency of context x in
the data. This is convenient because p(x) is zero
for all those events not seen in the training data.
Finding the maximum entropy model that satis-
fies these constraints is a constrained optimisation
problem, which can be solved using the method of
Lagrange multipliers, and leads to the form in (1)
where the Ai are the Lagrange multipliers.
A natural choice for Ki is the empirical expected
value of the feature fi:
</bodyText>
<equation confidence="0.95041575">
Ep fi = E y)fi(x, y) (4)
x,y
which leads to the following set of constraints:
E xx)p(ydx)fi(x,y) = Ei3.fi (5)
</equation>
<bodyText confidence="0.933683666666667">
xo,
An alternative motivation for this model is that,
starting with the log-linear form in (1) and deriv-
ing (conditional) MLES, we arrive at the same so-
lution as the ME model which satisfies the con-
straints in (5).
</bodyText>
<sectionHeader confidence="0.995303" genericHeader="method">
3 Generalised Iterative Scaling
</sectionHeader>
<bodyText confidence="0.8697608">
GIS is a very simple algorithm for estimating the
parameters of a ME model. The algorithm is as fol-
lows, where E p f, is the empirical expected value
of J and E p fi is the expected value according to
model p:
</bodyText>
<listItem confidence="0.999421666666667">
• Set Az&amp;quot; equal to some arbitrary value, say:
,(o) = 0
• Repeat until convergence:
</listItem>
<equation confidence="0.817202555555556">
Act+u = (t) 1 + --E p
log
fi
C Ep(ofi
where (t) is the iteration index and the constant C
is defined as follows:
C = max E fi(x, y) (8)
x,y
i= 1
</equation>
<bodyText confidence="0.999896916666667">
In practice C is maximised over the (x, y) pairs
in the training data, although in theory C can be
any constant greater than or equal to the figure in
(8). However, since determines the rate of con-
vergence of the algorithm, it is preferable to keep
C as small as possible.
The original formulation of GIS (Darroch and
Ratcliff, 1972) required the sum of the feature val-
ues for each event to be constant. Since this is
not the case for many applications, the standard
method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;, fea-
ture to each event, defined as follows:
</bodyText>
<equation confidence="0.999149">
fc(x,y) = C — E fi(x, y) (9)
</equation>
<bodyText confidence="0.999825333333333">
For our tagging experiments, the use of a cor-
rection feature did not significantly affect the re-
sults. Moreover, we show in the Appendix, by a
</bodyText>
<page confidence="0.992433">
92
</page>
<bodyText confidence="0.999725578947368">
simple adaptation of Berger&apos;s proof for the con-
vergence of HS (Berger, 1997), that GIS converges
to the maximum likelihood model without a cor-
rection feature.1
The proof works by introducing a correction
feature with fixed weight of 0 into the iis con-
vergence proof. This feature does not contribute
to the model and can be ignored during weight
update. Introducing this null feature still satis-
fies Jensen&apos;s inequality, which is used to provide a
lower bound on the change in likelihood between
iterations, and the existing GIS weight update (7)
can still be derived analytically.
An advantage of GIS is that it is a very simple
algorithm, made even simpler by the removal of
the correction feature. This simplicity means that,
although GIS requires more iterations than 11s to
reach convergence, in practice it is significantly
faster (Malouf, 2002).
</bodyText>
<sectionHeader confidence="0.972646" genericHeader="method">
4 Smoothing Maximum Entropy Models
</sectionHeader>
<bodyText confidence="0.999378285714286">
Several methods have been proposed for smooth-
ing ME models (see Chen and Rosenfeld (1999)).
For taggers, a standard technique is to eliminate
low frequency features, based on the assumption
that they are unreliable or uninformative (Ratna-
parkhi, 1998). Studies of infrequent features in
other domains suggest this assumption may be in-
correct (Daelemans et al., 1999). We test this for
ME taggers by replacing the cutoff with the use of
a Gaussian prior, a technique which works well for
language models (Chen and Rosenfeld, 1999).
When using a Gaussian prior, the objective
function is no longer the likelihood, L(A), but has
the form:
</bodyText>
<equation confidence="0.915882">
L13&apos; (A) = L 1
p
(A) + E log (10)
• .‘121ro l
</equation>
<bodyText confidence="0.985906571428571">
2o-
Maximising this function is a form of maximum a
posteriori estimation, rather than maximum likeli-
hood estimation. The effect of the prior is to pe-
nalise models that have very large positive or neg-
ative weights. This can be thought of as relaxing
the constraints in (5), so that the model fits the data
</bodyText>
<footnote confidence="0.9888455">
1We note that Goodman (2002) suggests that the correc-
tion feature may not be necessary for convergence.
</footnote>
<table confidence="0.999801">
CCG lexical category Description
(S\NP)INP transitive verb
S\NP intransitive verb
NPIN determiner
NIN nominal modifier
(S\NP)\(S\NP) adverbial modifier
</table>
<tableCaption confidence="0.999934">
Table 1: Example CCG lexical categories
</tableCaption>
<bodyText confidence="0.99874775">
less exactly. The parameters o-, are usually col-
lapsed into one parameter which can be set using
heldout data.
The new update rule for GIS with a Gaussian
prior is found by solving the following equation
for the Ai update values (denoted by S), which can
easily be derived from (10) by analogy with the
proof in the Appendix:
</bodyText>
<equation confidence="0.968001333333333">
Epfi= Epfi eCal ± ± 61 (11)
2
cri
</equation>
<bodyText confidence="0.9998784">
This equation does not have an analytic solution
for Si and can be solved using a numerical solver
such as Newton-Raphson. Note that this new up-
date rule is still significantly simpler than that re-
quired for 11s.
</bodyText>
<sectionHeader confidence="0.970554" genericHeader="method">
5 Maximum Entropy Taggers
</sectionHeader>
<bodyText confidence="0.9537235">
We reimplemented Ratnaparkhi&apos;s publicly avail-
able POS tagger MXPOST (Ratnaparkhi, 1996;
Ratnaparkhi, 1998) and Clark&apos;s CCG supertagger
(Clark, 2002) as a starting point for our experi-
ments. CCG supertagging is more difficult than
POS tagging because the set of &amp;quot;tags&amp;quot; assigned by
the supertagger is much larger (398 in this imple-
mentation, compared with 45 POS tags). The su-
pertagger assigns CCG lexical categories (Steed-
man, 2000) which encode subcategorisation infor-
mation. Table 1 gives some examples.
The features used by each tagger are binary val-
ued, and pair a tag with various elements of the
context; for example:
fi(x ) = { 1 if word(x)= the &amp; y = DT
,y
</bodyText>
<sectionHeader confidence="0.300276" genericHeader="method">
0 otherwise
</sectionHeader>
<bodyText confidence="0.9039078">
(12)
word(x) = the is an example of what Ratna-
parkhi calls a contextual predicate. The contex-
tual predicates used by the two taggers are given
in Table 2, where w, is the ith word and ti is the
</bodyText>
<page confidence="0.748926">
/12
93
</page>
<table confidence="0.823072625">
Condition Contextual predicate
freq(w,) 5 wi = X
freq(w,) &lt;5 X is prefix of wi, IXI &lt; 4
(Pos tagger) X is suffix of wi, IX &lt; 4
wi contains a digit
wi contains uppercase char
iv; contains a hyphen
Vwi ti_i = X
</table>
<equation confidence="0.989457">
ti-2ti-1 = XY
wi_i = X
Wi-2 = X
= X
Wi+2 = X
\ 1Wi POSi = X
(supertagger) POSi_i = X
POSi_2 = X
POSi+1 = X
POSi+2 = X
</equation>
<tableCaption confidence="0.907389">
Table 2: Contextual predicates used in the taggers
</tableCaption>
<bodyText confidence="0.994510083333333">
ith tag. We insert a special end of sentence symbol
at sentence boundaries so that the features looking
forwards and backwards are always defined.
The supertagger uses POS tags as additional fea-
tures, which Clark (2002) found improved perfor-
mance significantly, and does not use the morpho-
logical features, since the POS tags provide equiva-
lent information. For the supertagger, t, is the lex-
ical category of the ith word.
The conditional probability of a tag sequence
y ...y, given a sentence w wn is approxi-
mated as follows:
</bodyText>
<equation confidence="0.9978985">
P(Y1 Yn1W1 .Wn) fl p(yiki) (13)
— H7=z(J.„,,exp(z;AjfAxi,y0) (14)
</equation>
<bodyText confidence="0.999275625">
where x; is the context of the ith word. The tag-
ger returns the most probable sequence for the
sentence. Following Ratnaparkhi, beam search is
used to retain only the 20 most probable sequences
during the tagging process;2 we also use a &amp;quot;tag dic-
tionary&amp;quot;, so that words appearing 5 or more times
in the data can only be assigned those tags previ-
ously seen with the word.
</bodyText>
<footnote confidence="0.941038">
2Ratnaparkhi uses a beam width of 5.
</footnote>
<table confidence="0.999939">
Split DATA # SENT. # WORDS
Develop WSJ 00 1921 46451
Train WSJ 02-21 39832 950028
Test WSJ 23 2416 56684
</table>
<tableCaption confidence="0.963839">
Table 3: WSJ training, testing and development
</tableCaption>
<table confidence="0.99972625">
Tagger ACC UWORD UTAG AMB
MXPOST 96.59 85.81 30.04 94.82
BASE 96.58 85.70 29.28 94.82
— CORR 96.60 85.58 31.94 94.85
</table>
<tableCaption confidence="0.99859">
Table 4: Basic tagger performance on WSJ 00
</tableCaption>
<sectionHeader confidence="0.974021" genericHeader="method">
6 POS Tagging Experiments
</sectionHeader>
<bodyText confidence="0.98958">
We develop and test our improved POS tagger
(c &amp;c) using the standard parser development
methodology on the Penn Treebank WSJ corpus.
Table 3 shows the number of sentences and words
in the training, development and test datasets.
As well as evaluating the overall accuracy of
the taggers (Acc), we also calculate the accu-
racy on previously unseen words (UwoRD), previ-
ously unseen word-tag pairs (UTAG) and ambigu-
ous words (AMB), that is, those with more than
one tag over the testing, training and development
datasets. Note that the unseen word-tag pairs do
not include the previously unseen words.
We first replicated the results of the MXPOST
tagger. In doing so, we discovered a number of
minor variations from Ratnaparkhi (1998):
</bodyText>
<listItem confidence="0.989195">
• MXPOST adds a default contextual predicate
which is true for every context;
• MXPOST does not use the cutoff values de-
scribed in Ratnaparkhi (1998).
</listItem>
<bodyText confidence="0.995754666666667">
MXPOST uses a cutoff of 1 for the current word
feature and 5 for other features. However, the cur-
rent word must have appeared at least 5 times with
any tag for the current word feature to be included;
otherwise the word is considered rare and morpho-
logical features are included instead.
</bodyText>
<sectionHeader confidence="0.981342" genericHeader="method">
7 POS Tagging Results
</sectionHeader>
<bodyText confidence="0.963016">
Table 4 shows the performance of MXPOST and
our reimplementation.3 The third row shows a mi-
</bodyText>
<footnote confidence="0.98868">
3By examining the MXPOST model files, we discovered a
minor error in the counts for prefix and suffix features, which
may explain the slight difference in performance.
</footnote>
<page confidence="0.987536">
94
</page>
<table confidence="0.999839333333333">
Tagger Acc UWORD UTAG AMB
BASE a= 2.05 96.75 86.74 33.08 95.06
w&gt; 2,a= 2.06 96.71 86.62 33.46 95.00
vi, &gt; 3, a= 2.05 96.68 86.51 34.22 94.94
pw&gt; 2, a= 1.50 96.76 87.02 32.70 95.06
pw &gt; 3, a= 1.75 96.76 87.14 33.08 95.06
</table>
<tableCaption confidence="0.7857965">
Table 5: WSJ 00 results with varying current and
previous word feature cutoffs
</tableCaption>
<table confidence="0.9994094">
Tagger Acc UWORD UTAG AMB
1,a=1.95 96.82 87.20 30.80 95.07
&gt;2, a = 1.98 96.77 87.02 31.18 95.00
&gt;3,a= 1.73 96.72 86.62 31.94 94.94
&gt;4, a = 1.50 96.72 87.08 34.22 94.96
</table>
<tableCaption confidence="0.997501">
Table 6: WSJ 00 results with varying cutoffs
</tableCaption>
<bodyText confidence="0.999848033333333">
nor improvement in performance when the correc-
tion feature is removed. We also experimented
with the default contextual predicate but found it
had little impact on the performance. For the re-
mainder of the experiments we use neither the cor-
rection nor the default features.
The rest of this section considers various com-
binations of feature cutoffs and Gaussian smooth-
ing. We report optimal results with respect to the
smoothing parameter a, where a = No-2 and N is
the number of training instances. We found that
using a 2 gave the most benefit to our basic
tagger, improving performance by about 0.15% on
the development set. This result is shown in the
first row of Table 5.
The remainder of Table 5 shows a minimal
change in performance when the current word (w)
and previous word (pw) cutoffs are varied. This
led us to reduce the cutoffs for all features simul-
taneously. Table 6 gives results for cutoff values
between 1 and 4. The best performance (in row
1) is obtained when the cutoffs are eliminated en-
tirely.
Gaussian smoothing has allowed us to retain all
of the features extracted from the corpus and re-
duce overfitting. To get more information into the
model, more features must be extracted, and so we
investigated the addition of the current word fea-
ture for all words, including the rare ones. This re-
sulted in a minor improvement, and gave the best
</bodyText>
<table confidence="0.989979333333333">
Tagger Acc UW ORD UTAG AMB
MXP OS T 97.05 83.63 30.20 95.44
C&amp;C 97.27 85.21 28.98 95.69
</table>
<tableCaption confidence="0.9999655">
Table 7: Tagger performance on WSJ 23
Table 8: Model size
</tableCaption>
<bodyText confidence="0.984414214285715">
performance on the development data: 96.83%.
Table 7 shows the final performance on the test
set, using the best configuration on the develop-
ment data (which we call c&amp;c), compared with
MXPOST. The improvement is 0.22% overall (a
reduction in error rate of 7.5%) and 1.58% for un-
known words (a reduction in error rate of 9.7%).
The obvious cost associated with retaining all
the features is the significant increase in model
size, which slows down both the training and tag-
ging and requires more memory. Table 8 shows
the difference in the number of contextual predi-
cates and features between the original and final
taggers.
</bodyText>
<sectionHeader confidence="0.988196" genericHeader="method">
8 POS Tagging Validation
</sectionHeader>
<bodyText confidence="0.999908333333333">
To ensure the robustness of our results, we per-
formed 10-fold cross-validation using the whole of
the WSJ Penn Treebank. The 24 sections were split
into 10 equal components, with 9 used for train-
ing and 1 for testing. The final result is an average
over the 10 different splits, given in Table 9, where
o- is the standard deviation of the overall accuracy.
We also performed 10-fold cross-validation using
MXPOST and TNT, a publicly available Markov
model PO S tagger (Brants, 2000).
The difference between MXPOST and c&amp;c rep-
resents a reduction in error rate of 4.3%, and the
</bodyText>
<table confidence="0.99401175">
Tagger Acc o- UWORD UTAG AMB
MX POST 96.72 0.12 85.50 32.16 95.00
TNT 96.48 0.13 85.31 0.00 94.26
C&amp;C 96.86 0.12 86.43 30.42 95.08
</table>
<tableCaption confidence="0.998809">
Table 9: 10-fold cross-validation results
</tableCaption>
<figure confidence="0.9396465">
BASE
44385 121557
Tagger
# PREDICATES # FEATURES
254038 685682
C&amp;C
</figure>
<page confidence="0.946479">
95
</page>
<table confidence="0.9966676">
Tagger Acc UWORD UTAG AMB
COLLINS 97.07 -
C&amp;C 96.93 87.28 34.44 95.31
T&amp;M 96.86 86.91 -
C&amp;C 97.10 86.43 34.84 95.52
</table>
<tableCaption confidence="0.99981">
Table 10: Comparison with other taggers
</tableCaption>
<bodyText confidence="0.997655157894737">
difference between TNT and C&amp;C a reduction in
error rate of 10.8%.
We also compare our performance against other
published results that use different training and
testing sections. Collins (2002) uses WSJ 00-
18 for training and WSJ 22-24 for testing, and
Toutanova and Manning (2000) use WSJ 00-20 for
training and WSJ 23-24 for testing. Collins uses
a linear perceptron, and Toutanova and Manning
(T&amp;M) use a ME tagger, also based on MXPOST.
Our performance (in Table 10) is slightly worse
than Collins&apos;, but better than T&amp;M (except for un-
known words). We noticed during development
that unknown word performance improves with
larger a values at the expense of overall accuracy
- and so using separate cy&apos;s for different types of
contextual predicates may improve performance.
A similar approach has been shown to be success-
ful for language modelling (Goodman, p.c.).
</bodyText>
<sectionHeader confidence="0.794377" genericHeader="method">
9 Supertagging Experiments
</sectionHeader>
<bodyText confidence="0.999777882352941">
The lexical categories for the supertagging ex-
periments were extracted from CCGbank, a CCG
version of the Penn Treebank (Hockenmaier and
Steedman, 2002). Following Clark (2002), all cat-
egories that occurred at least 10 times in the train-
ing data were used, resulting in a tagset of 398 cat-
egories. Sections 02-21, section 00, and section 23
were used for training, development and testing, as
before.
Our supertagger used the same configuration as
our best performing POS tagger, except that the
a parameter was again optimised on the develop-
ment set. The results on section 00 and section 23
are given in Tables 11 and 12.4 c&amp;c outperforms
Clark&apos;s supertagger by 0.43% on the test set, a re-
duction in error rate of 4.9%.
Supertagging has the potential to benefit more
</bodyText>
<footnote confidence="0.9960595">
4The results in Clark (2002) are slightly lower because
these did not include punctuation.
</footnote>
<table confidence="0.997282">
Tagger Acc UWORD UTAG AMB
CLARK 90.97 90.86 28.48 89.84
C&amp;C a= 1.52 91.45 91.16 28.79 90.38
</table>
<tableCaption confidence="0.944131">
Table 11: S upertagger WSJ 00 results
</tableCaption>
<table confidence="0.999219">
Tagger Acc UWORD UTAG AMB
CLARK 91.27 88.48 32.20 90.32
C&amp;C a= 1.52 91.70 88.92 32.30 90.78
</table>
<tableCaption confidence="0.999023">
Table 12: Supertagger WSJ 23 results
</tableCaption>
<bodyText confidence="0.9988765">
from Gaussian smoothing than POS tagging be-
cause the feature space is sparser by virtue of the
much larger tagset. Gaussian smoothing would
also allow us to incorporate rare longer range de-
pendencies as features, without risk of overfitting.
This may further boost supertagger performance.
</bodyText>
<sectionHeader confidence="0.977146" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999972">
This paper has demonstrated, both analytically
and empirically, that GIS does not require a cor-
rection feature Eliminating the correction feature
simplifies further the already very simple estima-
tion algorithm. Although GIS is not as fast as
some alternatives, such as conjugate gradient and
limited memory variable metric methods (Malouf,
2002), our C&amp;C POS tagger takes less than 10 min-
utes to train, and the space requirements are mod-
est, irrespective of the size of the tagset.
We have also shown that using a Gaussian prior
on the parameters of the ME model improves per-
formance over a simple frequency cutoff. The
Gaussian prior effectively relaxes the constraints
on the ME model, which allows the model to
use low frequency features without overfitting.
Achieving optimal performance with Gaussian
smoothing and without cutoffs demonstrates that
low frequency features can contribute to good per-
formance.
</bodyText>
<sectionHeader confidence="0.972851" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.984370333333333">
We would like to thank Joshua Goodman, Miles
Osborne, Andrew Smith, Hanna Wallach, Tara
Murphy and the anonymous reviewers for their
comments on drafts of this paper. This research
is supported by a Commonwealth scholarship and
a Sydney University Travelling scholarship to the
</bodyText>
<page confidence="0.985894">
96
</page>
<bodyText confidence="0.472906">
first author, and EPSRC grant GR/M96889.
</bodyText>
<sectionHeader confidence="0.617971" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9957946875">
Adam Berger, Stephen Della Pietra, and Vincent Della Pietra.
1996. A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1 ): 39-71.
Adam Berger. 1997. The improved iterative scaling algo-
rithm: A gentle introduction. Unpublished manuscript.
Thorsten Brants. 2000. TnT - a statistical part-of-speech
tagger. In Proceedings of the 6th Conference on Applied
Natural Language Processing.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior
for smoothing maximum entropy models. Technical re-
port, Carnegie Mellon University, Pittsburgh, PA.
Stephen Clark. 2002. A supertagger for Combinatory Cat-
egorial Grammar. In Proceedings of the 6th Interna-
tional Workshop on Tree Adjoining Grammars and Re-
lated Frameworks, pages 19-24, Venice, Italy.
Michael Collins. 2002. Discriminative training methods for
Hidden Markov Models: Theory and experiments with
perceptron algorithms. In Proceedings of the EMNLP
Conference, pages 1-8, Philadelphia, PA.
Walter Daelemans, Antal Van Den Bosch, and Jakub Zavrel.
1999. Forgetting exceptions is harmful in language learn-
ing. Machine Learning, 34(1-3):11-43.
J. N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The Annals of Mathemati-
cal Statistics, 43(5):1470-1480.
Stephen Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE
Transactions Pattern Analysis and Machine Intelligence,
19(4): 380-393.
Joshua Goodman. 2002. Sequential conditional generalized
iterative scaling. In Proceedings of the 40th Meeting of
the ACL, pages 9-16, Philadelphia, PA.
Julia Hockenmaier and Mark Steedman. 2002. Acquiring
compact lexicalized grammars from a cleaner treebank. In
Proceedings of the Third LREC Conference, Las Palmas,
Spain.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
&apos;unification-based&apos; grammars. In Proceedings of the 37th
Meeting of the ACL, pages 535-541, University of Mary-
land, MD.
Rob Koeling. 2000. Chunking with maximum entropy mod-
els. In Proceedings of the CoNLL Workshop 2000, pages
139-141, Lisbon, Portugal.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings of
the Sixth Workshop on Natural Language Learning, pages
49-55, Taipei, Taiwan.
</reference>
<bodyText confidence="0.957414259259259">
Kamal Nigam, John Lafferty, and Andrew McCallum. 1999.
Using maximum entropy for text classification. In Pro-
ceedings of the IJCAI-99 Workshop on Machine Learning
for Information Filtering, pages 61-67, Stockholm, Swe-
den.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the EMNLP Conference,
pages 133-142, Philadelphia, PA.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models for
Natural Language Ambiguity Resolution. Ph.D. thesis,
University of Pennsylvania.
Adwait Ratnaparkhi. 1999. Learning to parse natural lan-
guage with maximum entropy models. Machine Learn-
ing, 34(1-3):151-175.
Ronald Rosenfeld. 1996. A maximum entropy approach to
adaptive statistical language modeling. Computer, Speech
and Language, 10:187-228.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Kristina Toutanova and Christopher D. Manning. 2000. En-
riching the knowledge sources used in a maximum entropy
part-of-speech tagger. In Proceedings of the EMNLP con-
ference, Hong Kong.
Hans van Halteren, Jakub Zavrel, and Walter Daelemans.
2001. Improving accuracy in wordclass tagging through
combination of machine learning systems. Computational
Linguistics, 27(2): 199-229.
</bodyText>
<sectionHeader confidence="0.835392" genericHeader="conclusions">
Appendix A: Correction free GIS
</sectionHeader>
<bodyText confidence="0.9923636">
This proof of GIS convergence without the correc-
tion feature is based on the ITS convergence proof
by Berger (1997).
Start with some initial model with arbitrary pa-
rameters A E , A2, . , Ad. Each iteration of
the GIS algorithm finds a set of new parameters
A&apos; E A+6, 1A1 + 81, A2 + 62, . , An +6} which
increases the log-likelihood of the model.
The change in log-likelihood is as follows:
As in Berger (1997), use the inequality - log a
</bodyText>
<footnote confidence="0.8292465">
1 - a to establish a lower bound on the change in
likelihood:
</footnote>
<equation confidence="0.999089285714286">
Lp(A + A) - L(A)
p(x,y) log p (Mx) -
P(x, y) log PA (Mx)
15(x) log ZA,(x)
ZA (x)
(15)
p(x, y -
</equation>
<page confidence="0.99705">
97
</page>
<bodyText confidence="0.999654181818182">
Call the right hand side of this last equation
g{(A1A). If we can find a A for which R(AA) &gt; 0,
then Lp(A +A) is an improvement over Lp(A). The
obvious approach is to maximise A(AIA) with re-
spect to each Si, but this cannot be performed di-
rectly, since differentiating ANA) with respect to
61 leads to an equation containing all elements of
A.
Let f be a convex function on the interval I. If
xi , x2, x, c I and t1, t2,. t, are non-negative
real numbers such that ri? ti = 1, then
</bodyText>
<equation confidence="0.9678535">
f [X tixi] ti f (xi) (18)
i=1 i=1
</equation>
<bodyText confidence="0.8639065">
Since Z&apos;,1+1 fi(*
= = 1 and the exponential func-
tion is convex, we can apply Jensen&apos;s inequality to
give a new form of A(AIA):
</bodyText>
<equation confidence="0.997918">
R(AIA) 1 + ifi(x,y)
)&amp;quot;) exp (C 6;)
(19)
</equation>
<bodyText confidence="0.965252">
Call this bound B(AIA). Della Pietra et al. (1997)
give extra conditions on the continuity and
derivative of the lower bound, in order to
guarantee convergence. These conditions can
be verified for Y(AA) in a similar way to
Della Pietra et al. (1997).
Differentiating B(AA) with respect to each
weight update di (1 n) gives:
</bodyText>
<figure confidence="0.9777020625">
= 1 + P Y E Offi(x,Y)
1=1
ex (Ai + i)fi(x,y)
i=t
1
PA(Y1x)exp
(5, fi(x, y) (16)
y +
y) + 1 —
LTAA + A) — Lp(A)
E p(x,y)
=1
1 ZA,(X)\
ZA(X)
ZA&apos;(X)
p(x) ZA(x)
</figure>
<bodyText confidence="0.999122">
The trick is to rewrite R(AA) as follows, with
an extra term which will be used to satisfy Jensen&apos;s
inequality:
</bodyText>
<equation confidence="0.998921333333333">
R(AIA) = 1 +
&amp;Mx, )2)
,y).f(x, i!&amp;quot;) (20)
) &gt; PA (Y1x)Mx, y) exp (C fii)
- X 13(x) PA (Yix) exp ( i=1 fi(x,y) C 6)
(17)
</equation>
<bodyText confidence="0.999593666666667">
where C is previously defined in equation 8,
fn-Fi(x, y) = fc(x, y) as in (9), and 6,+1 is defined to
be zero. Note that the correction feature has been
introduced but has been given a constant weight of
zero.
This reformulation of R(AA) is similar to
Berger&apos;s for the ITS proof, but with a crucial dif-
ference: Berger introduces f# = f(x,y) into
the equation rather than C, and does not have the
correction feature.
The next part of the proof introduces another,
less tight, lower bound on the change in likelihood,
by using Jensen&apos;s inequality, which can be stated
as follows:
The effect of introducing C rather than tt is that
solving °BOA)tv, = 0 can be done analytically (at
the cost of a slower convergence rate), giving the
following:
</bodyText>
<equation confidence="0.86333925">
1 E, 13(x, Afi(x, y)
6i = log _
C Ex P(X)Ey P A(YIX)fi(X,Y)
1 E,3 fi
= log
C E p(11) . f i
which leads to the update rule in (7).
(21)
</equation>
<page confidence="0.994582">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.245802">
<title confidence="0.99997">Smoothing for Maximum Entropy Taggers</title>
<author confidence="0.999848">R Curran Clark</author>
<affiliation confidence="0.999917">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.596379">2 Buccleuch Place, Edinburgh. EH8 9LW</address>
<email confidence="0.99812">fjamesc,stephencl@cogsci.ed.ac.uk</email>
<abstract confidence="0.970732733333333">This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (Gis) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctof unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: standard Penn Treebank and the larger set of lexical types from</abstract>
<intro confidence="0.484395">Combinatory Categorial Grammar.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>39--71</pages>
<contexts>
<context position="1700" citStr="Berger et al., 1996" startWordPosition="261" endWordPosition="264">have been successfully modelled in the ME framework, including POS tagging, with state of the art performance (van Halteren et al., 2001), &amp;quot;supertagging&amp;quot; (Clark, 2002) and chunking (Koeling, 2000). Generalised Iterative Scaling (GIS) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;, feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm. However, the extra book keeping required for us means that GIS is often faster in practice (Malouf, 2002). This paper shows, by a simple adaptation of Berger&apos;s proof for the convergence of HS (Berger, 1997), that GIS does not require a correction feature. We also investigate how the use of a correction feature affects the performance of ME taggers. GIS and HS obtain a maximum likelihood estimate (mLE) of the parameters, and, like other MLE methods, are susceptible to overfitting. A </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1 ): 39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
</authors>
<title>The improved iterative scaling algorithm: A gentle introduction.</title>
<date>1997</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="2019" citStr="Berger, 1997" startWordPosition="315" endWordPosition="316">ation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;, feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm. However, the extra book keeping required for us means that GIS is often faster in practice (Malouf, 2002). This paper shows, by a simple adaptation of Berger&apos;s proof for the convergence of HS (Berger, 1997), that GIS does not require a correction feature. We also investigate how the use of a correction feature affects the performance of ME taggers. GIS and HS obtain a maximum likelihood estimate (mLE) of the parameters, and, like other MLE methods, are susceptible to overfitting. A simple technique used to avoid overfitting is a frequency cutoff, in which only frequently occurring features are included in the model (Ratnaparkhi, 1998). However, more sophisticated smoothing techniques exist, such as the use of a Gaussian prior on the parameters of the model (Chen and Rosenfeld, 1999). This techni</context>
<context position="6209" citStr="Berger, 1997" startWordPosition="1065" endWordPosition="1066"> convergence of the algorithm, it is preferable to keep C as small as possible. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;, feature to each event, defined as follows: fc(x,y) = C — E fi(x, y) (9) For our tagging experiments, the use of a correction feature did not significantly affect the results. Moreover, we show in the Appendix, by a 92 simple adaptation of Berger&apos;s proof for the convergence of HS (Berger, 1997), that GIS converges to the maximum likelihood model without a correction feature.1 The proof works by introducing a correction feature with fixed weight of 0 into the iis convergence proof. This feature does not contribute to the model and can be ignored during weight update. Introducing this null feature still satisfies Jensen&apos;s inequality, which is used to provide a lower bound on the change in likelihood between iterations, and the existing GIS weight update (7) can still be derived analytically. An advantage of GIS is that it is a very simple algorithm, made even simpler by the removal of</context>
</contexts>
<marker>Berger, 1997</marker>
<rawString>Adam Berger. 1997. The improved iterative scaling algorithm: A gentle introduction. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT - a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="16111" citStr="Brants, 2000" startWordPosition="2801" endWordPosition="2802">he difference in the number of contextual predicates and features between the original and final taggers. 8 POS Tagging Validation To ensure the robustness of our results, we performed 10-fold cross-validation using the whole of the WSJ Penn Treebank. The 24 sections were split into 10 equal components, with 9 used for training and 1 for testing. The final result is an average over the 10 different splits, given in Table 9, where o- is the standard deviation of the overall accuracy. We also performed 10-fold cross-validation using MXPOST and TNT, a publicly available Markov model PO S tagger (Brants, 2000). The difference between MXPOST and c&amp;c represents a reduction in error rate of 4.3%, and the Tagger Acc o- UWORD UTAG AMB MX POST 96.72 0.12 85.50 32.16 95.00 TNT 96.48 0.13 85.31 0.00 94.26 C&amp;C 96.86 0.12 86.43 30.42 95.08 Table 9: 10-fold cross-validation results BASE 44385 121557 Tagger # PREDICATES # FEATURES 254038 685682 C&amp;C 95 Tagger Acc UWORD UTAG AMB COLLINS 97.07 - C&amp;C 96.93 87.28 34.44 95.31 T&amp;M 96.86 86.91 - C&amp;C 97.10 86.43 34.84 95.52 Table 10: Comparison with other taggers difference between TNT and C&amp;C a reduction in error rate of 10.8%. We also compare our performance against </context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT - a statistical part-of-speech tagger. In Proceedings of the 6th Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2606" citStr="Chen and Rosenfeld, 1999" startWordPosition="408" endWordPosition="411">r the convergence of HS (Berger, 1997), that GIS does not require a correction feature. We also investigate how the use of a correction feature affects the performance of ME taggers. GIS and HS obtain a maximum likelihood estimate (mLE) of the parameters, and, like other MLE methods, are susceptible to overfitting. A simple technique used to avoid overfitting is a frequency cutoff, in which only frequently occurring features are included in the model (Ratnaparkhi, 1998). However, more sophisticated smoothing techniques exist, such as the use of a Gaussian prior on the parameters of the model (Chen and Rosenfeld, 1999). This technique has been applied to language modelling (Chen and Rosenfeld, 1999), text classification (Nigam et al., 1999) and parsing (Johnson et al., 1999), but to our knowledge it has not been compared with the use of a feature cutoff. We explore the combination of Gaussian smoothing and a simple cutoff for two tagging tasks. The two taggers used for the experiments are a POS tagger, trained on the WSJ Penn Treebank, and a &amp;quot;supertagger&amp;quot;, which assigns tags from the 91 much larger set of lexical types from Combinatory Categorial Grammar (ccG) (Clark, 2002). Elimination of the correction fe</context>
<context position="7110" citStr="Chen and Rosenfeld (1999)" startWordPosition="1210" endWordPosition="1213">. Introducing this null feature still satisfies Jensen&apos;s inequality, which is used to provide a lower bound on the change in likelihood between iterations, and the existing GIS weight update (7) can still be derived analytically. An advantage of GIS is that it is a very simple algorithm, made even simpler by the removal of the correction feature. This simplicity means that, although GIS requires more iterations than 11s to reach convergence, in practice it is significantly faster (Malouf, 2002). 4 Smoothing Maximum Entropy Models Several methods have been proposed for smoothing ME models (see Chen and Rosenfeld (1999)). For taggers, a standard technique is to eliminate low frequency features, based on the assumption that they are unreliable or uninformative (Ratnaparkhi, 1998). Studies of infrequent features in other domains suggest this assumption may be incorrect (Daelemans et al., 1999). We test this for ME taggers by replacing the cutoff with the use of a Gaussian prior, a technique which works well for language models (Chen and Rosenfeld, 1999). When using a Gaussian prior, the objective function is no longer the likelihood, L(A), but has the form: L13&apos; (A) = L 1 p (A) + E log (10) • .‘121ro l 2oMaxim</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical report, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>A supertagger for Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks,</booktitle>
<pages>pages</pages>
<location>Venice, Italy.</location>
<contexts>
<context position="1248" citStr="Clark, 2002" startWordPosition="186" endWordPosition="187">xperiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar. 1 Introduction The use of maximum entropy (ME) models has become popular in Statistical NLP; some example applications include part-of-speech (Pos) tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1999; Johnson et al., 1999) and language modelling (Rosenfeld, 1996). Many tagging problems have been successfully modelled in the ME framework, including POS tagging, with state of the art performance (van Halteren et al., 2001), &amp;quot;supertagging&amp;quot; (Clark, 2002) and chunking (Koeling, 2000). Generalised Iterative Scaling (GIS) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;, feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm. However, the extra book keeping req</context>
<context position="3172" citStr="Clark, 2002" startWordPosition="506" endWordPosition="507">meters of the model (Chen and Rosenfeld, 1999). This technique has been applied to language modelling (Chen and Rosenfeld, 1999), text classification (Nigam et al., 1999) and parsing (Johnson et al., 1999), but to our knowledge it has not been compared with the use of a feature cutoff. We explore the combination of Gaussian smoothing and a simple cutoff for two tagging tasks. The two taggers used for the experiments are a POS tagger, trained on the WSJ Penn Treebank, and a &amp;quot;supertagger&amp;quot;, which assigns tags from the 91 much larger set of lexical types from Combinatory Categorial Grammar (ccG) (Clark, 2002). Elimination of the correction feature and use of appropriate smoothing methods result in state of the art performance for both tagging tasks. 2 Maximum Entropy Models A conditional ME model, also known as a loglinear model, has the following form: 1 P(Y1X) = exp 37)) (1) where the functions fi are the features of the model, the A, are the parameters, or weights, and Z(x) is a normalisation constant. This form can be derived by choosing the model with maximum entropy (i.e. the most uniform model) from a set of models that satisfy a certain set of constraints. The constraints are that the expe</context>
<context position="9050" citStr="Clark, 2002" startWordPosition="1540" endWordPosition="1541">GIS with a Gaussian prior is found by solving the following equation for the Ai update values (denoted by S), which can easily be derived from (10) by analogy with the proof in the Appendix: Epfi= Epfi eCal ± ± 61 (11) 2 cri This equation does not have an analytic solution for Si and can be solved using a numerical solver such as Newton-Raphson. Note that this new update rule is still significantly simpler than that required for 11s. 5 Maximum Entropy Taggers We reimplemented Ratnaparkhi&apos;s publicly available POS tagger MXPOST (Ratnaparkhi, 1996; Ratnaparkhi, 1998) and Clark&apos;s CCG supertagger (Clark, 2002) as a starting point for our experiments. CCG supertagging is more difficult than POS tagging because the set of &amp;quot;tags&amp;quot; assigned by the supertagger is much larger (398 in this implementation, compared with 45 POS tags). The supertagger assigns CCG lexical categories (Steedman, 2000) which encode subcategorisation information. Table 1 gives some examples. The features used by each tagger are binary valued, and pair a tag with various elements of the context; for example: fi(x ) = { 1 if word(x)= the &amp; y = DT ,y 0 otherwise (12) word(x) = the is an example of what Ratnaparkhi calls a contextual </context>
<context position="10376" citStr="Clark (2002)" startWordPosition="1796" endWordPosition="1797"> is the /12 93 Condition Contextual predicate freq(w,) 5 wi = X freq(w,) &lt;5 X is prefix of wi, IXI &lt; 4 (Pos tagger) X is suffix of wi, IX &lt; 4 wi contains a digit wi contains uppercase char iv; contains a hyphen Vwi ti_i = X ti-2ti-1 = XY wi_i = X Wi-2 = X = X Wi+2 = X \ 1Wi POSi = X (supertagger) POSi_i = X POSi_2 = X POSi+1 = X POSi+2 = X Table 2: Contextual predicates used in the taggers ith tag. We insert a special end of sentence symbol at sentence boundaries so that the features looking forwards and backwards are always defined. The supertagger uses POS tags as additional features, which Clark (2002) found improved performance significantly, and does not use the morphological features, since the POS tags provide equivalent information. For the supertagger, t, is the lexical category of the ith word. The conditional probability of a tag sequence y ...y, given a sentence w wn is approximated as follows: P(Y1 Yn1W1 .Wn) fl p(yiki) (13) — H7=z(J.„,,exp(z;AjfAxi,y0) (14) where x; is the context of the ith word. The tagger returns the most probable sequence for the sentence. Following Ratnaparkhi, beam search is used to retain only the 20 most probable sequences during the tagging process;2 we </context>
<context position="17677" citStr="Clark (2002)" startWordPosition="3059" endWordPosition="3060">in Table 10) is slightly worse than Collins&apos;, but better than T&amp;M (except for unknown words). We noticed during development that unknown word performance improves with larger a values at the expense of overall accuracy - and so using separate cy&apos;s for different types of contextual predicates may improve performance. A similar approach has been shown to be successful for language modelling (Goodman, p.c.). 9 Supertagging Experiments The lexical categories for the supertagging experiments were extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier and Steedman, 2002). Following Clark (2002), all categories that occurred at least 10 times in the training data were used, resulting in a tagset of 398 categories. Sections 02-21, section 00, and section 23 were used for training, development and testing, as before. Our supertagger used the same configuration as our best performing POS tagger, except that the a parameter was again optimised on the development set. The results on section 00 and section 23 are given in Tables 11 and 12.4 c&amp;c outperforms Clark&apos;s supertagger by 0.43% on the test set, a reduction in error rate of 4.9%. Supertagging has the potential to benefit more 4The re</context>
</contexts>
<marker>Clark, 2002</marker>
<rawString>Stephen Clark. 2002. A supertagger for Combinatory Categorial Grammar. In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks, pages 19-24, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="16799" citStr="Collins (2002)" startWordPosition="2918" endWordPosition="2919">te of 4.3%, and the Tagger Acc o- UWORD UTAG AMB MX POST 96.72 0.12 85.50 32.16 95.00 TNT 96.48 0.13 85.31 0.00 94.26 C&amp;C 96.86 0.12 86.43 30.42 95.08 Table 9: 10-fold cross-validation results BASE 44385 121557 Tagger # PREDICATES # FEATURES 254038 685682 C&amp;C 95 Tagger Acc UWORD UTAG AMB COLLINS 97.07 - C&amp;C 96.93 87.28 34.44 95.31 T&amp;M 96.86 86.91 - C&amp;C 97.10 86.43 34.84 95.52 Table 10: Comparison with other taggers difference between TNT and C&amp;C a reduction in error rate of 10.8%. We also compare our performance against other published results that use different training and testing sections. Collins (2002) uses WSJ 00- 18 for training and WSJ 22-24 for testing, and Toutanova and Manning (2000) use WSJ 00-20 for training and WSJ 23-24 for testing. Collins uses a linear perceptron, and Toutanova and Manning (T&amp;M) use a ME tagger, also based on MXPOST. Our performance (in Table 10) is slightly worse than Collins&apos;, but better than T&amp;M (except for unknown words). We noticed during development that unknown word performance improves with larger a values at the expense of overall accuracy - and so using separate cy&apos;s for different types of contextual predicates may improve performance. A similar approa</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms. In Proceedings of the EMNLP Conference, pages 1-8, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal Van Den Bosch</author>
<author>Jakub Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<marker>Daelemans, Van Den Bosch, Zavrel, 1999</marker>
<rawString>Walter Daelemans, Antal Van Den Bosch, and Jakub Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, 34(1-3):11-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>The Annals of Mathematical Statistics,</journal>
<pages>43--5</pages>
<contexts>
<context position="1447" citStr="Darroch and Ratcliff, 1972" startWordPosition="214" endWordPosition="218">mum entropy (ME) models has become popular in Statistical NLP; some example applications include part-of-speech (Pos) tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1999; Johnson et al., 1999) and language modelling (Rosenfeld, 1996). Many tagging problems have been successfully modelled in the ME framework, including POS tagging, with state of the art performance (van Halteren et al., 2001), &amp;quot;supertagging&amp;quot; (Clark, 2002) and chunking (Koeling, 2000). Generalised Iterative Scaling (GIS) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;, feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm. However, the extra book keeping required for us means that GIS is often faster in practice (Malouf, 2002). This paper shows, by a simple adaptation of Berger&apos;s proof for the convergence of HS (Berger, 1997), that GIS does not require </context>
<context position="5736" citStr="Darroch and Ratcliff, 1972" startWordPosition="974" endWordPosition="977">value of J and E p fi is the expected value according to model p: • Set Az&amp;quot; equal to some arbitrary value, say: ,(o) = 0 • Repeat until convergence: Act+u = (t) 1 + --E p log fi C Ep(ofi where (t) is the iteration index and the constant C is defined as follows: C = max E fi(x, y) (8) x,y i= 1 In practice C is maximised over the (x, y) pairs in the training data, although in theory C can be any constant greater than or equal to the figure in (8). However, since determines the rate of convergence of the algorithm, it is preferable to keep C as small as possible. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;, feature to each event, defined as follows: fc(x,y) = C — E fi(x, y) (9) For our tagging experiments, the use of a correction feature did not significantly affect the results. Moreover, we show in the Appendix, by a 92 simple adaptation of Berger&apos;s proof for the convergence of HS (Berger, 1997), that GIS converges to the maximum likelihood model without a correction feature.1 The proof works by introducing a correction</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. The Annals of Mathematical Statistics, 43(5):1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<pages>380--393</pages>
<contexts>
<context position="1728" citStr="Pietra et al., 1997" startWordPosition="266" endWordPosition="269">lled in the ME framework, including POS tagging, with state of the art performance (van Halteren et al., 2001), &amp;quot;supertagging&amp;quot; (Clark, 2002) and chunking (Koeling, 2000). Generalised Iterative Scaling (GIS) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;, feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm. However, the extra book keeping required for us means that GIS is often faster in practice (Malouf, 2002). This paper shows, by a simple adaptation of Berger&apos;s proof for the convergence of HS (Berger, 1997), that GIS does not require a correction feature. We also investigate how the use of a correction feature affects the performance of ME taggers. GIS and HS obtain a maximum likelihood estimate (mLE) of the parameters, and, like other MLE methods, are susceptible to overfitting. A simple technique used to avo</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions Pattern Analysis and Machine Intelligence, 19(4): 380-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Sequential conditional generalized iterative scaling.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>9--16</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="8035" citStr="Goodman (2002)" startWordPosition="1375" endWordPosition="1376">eplacing the cutoff with the use of a Gaussian prior, a technique which works well for language models (Chen and Rosenfeld, 1999). When using a Gaussian prior, the objective function is no longer the likelihood, L(A), but has the form: L13&apos; (A) = L 1 p (A) + E log (10) • .‘121ro l 2oMaximising this function is a form of maximum a posteriori estimation, rather than maximum likelihood estimation. The effect of the prior is to penalise models that have very large positive or negative weights. This can be thought of as relaxing the constraints in (5), so that the model fits the data 1We note that Goodman (2002) suggests that the correction feature may not be necessary for convergence. CCG lexical category Description (S\NP)INP transitive verb S\NP intransitive verb NPIN determiner NIN nominal modifier (S\NP)\(S\NP) adverbial modifier Table 1: Example CCG lexical categories less exactly. The parameters o-, are usually collapsed into one parameter which can be set using heldout data. The new update rule for GIS with a Gaussian prior is found by solving the following equation for the Ai update values (denoted by S), which can easily be derived from (10) by analogy with the proof in the Appendix: Epfi= </context>
</contexts>
<marker>Goodman, 2002</marker>
<rawString>Joshua Goodman. 2002. Sequential conditional generalized iterative scaling. In Proceedings of the 40th Meeting of the ACL, pages 9-16, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Acquiring compact lexicalized grammars from a cleaner treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third LREC Conference,</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="17653" citStr="Hockenmaier and Steedman, 2002" startWordPosition="3054" endWordPosition="3057">er, also based on MXPOST. Our performance (in Table 10) is slightly worse than Collins&apos;, but better than T&amp;M (except for unknown words). We noticed during development that unknown word performance improves with larger a values at the expense of overall accuracy - and so using separate cy&apos;s for different types of contextual predicates may improve performance. A similar approach has been shown to be successful for language modelling (Goodman, p.c.). 9 Supertagging Experiments The lexical categories for the supertagging experiments were extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier and Steedman, 2002). Following Clark (2002), all categories that occurred at least 10 times in the training data were used, resulting in a tagset of 398 categories. Sections 02-21, section 00, and section 23 were used for training, development and testing, as before. Our supertagger used the same configuration as our best performing POS tagger, except that the a parameter was again optimised on the development set. The results on section 00 and section 23 are given in Tables 11 and 12.4 c&amp;c outperforms Clark&apos;s supertagger by 0.43% on the test set, a reduction in error rate of 4.9%. Supertagging has the potential</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Acquiring compact lexicalized grammars from a cleaner treebank. In Proceedings of the Third LREC Conference, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic &apos;unification-based&apos; grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Meeting of the ACL,</booktitle>
<pages>535--541</pages>
<institution>University of Maryland, MD.</institution>
<contexts>
<context position="1016" citStr="Johnson et al., 1999" startWordPosition="148" endWordPosition="151">es for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar. 1 Introduction The use of maximum entropy (ME) models has become popular in Statistical NLP; some example applications include part-of-speech (Pos) tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1999; Johnson et al., 1999) and language modelling (Rosenfeld, 1996). Many tagging problems have been successfully modelled in the ME framework, including POS tagging, with state of the art performance (van Halteren et al., 2001), &amp;quot;supertagging&amp;quot; (Clark, 2002) and chunking (Koeling, 2000). Generalised Iterative Scaling (GIS) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a &amp;quot;correction&amp;quot;, or</context>
<context position="2765" citStr="Johnson et al., 1999" startWordPosition="433" endWordPosition="436">ance of ME taggers. GIS and HS obtain a maximum likelihood estimate (mLE) of the parameters, and, like other MLE methods, are susceptible to overfitting. A simple technique used to avoid overfitting is a frequency cutoff, in which only frequently occurring features are included in the model (Ratnaparkhi, 1998). However, more sophisticated smoothing techniques exist, such as the use of a Gaussian prior on the parameters of the model (Chen and Rosenfeld, 1999). This technique has been applied to language modelling (Chen and Rosenfeld, 1999), text classification (Nigam et al., 1999) and parsing (Johnson et al., 1999), but to our knowledge it has not been compared with the use of a feature cutoff. We explore the combination of Gaussian smoothing and a simple cutoff for two tagging tasks. The two taggers used for the experiments are a POS tagger, trained on the WSJ Penn Treebank, and a &amp;quot;supertagger&amp;quot;, which assigns tags from the 91 much larger set of lexical types from Combinatory Categorial Grammar (ccG) (Clark, 2002). Elimination of the correction feature and use of appropriate smoothing methods result in state of the art performance for both tagging tasks. 2 Maximum Entropy Models A conditional ME model, </context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic &apos;unification-based&apos; grammars. In Proceedings of the 37th Meeting of the ACL, pages 535-541, University of Maryland, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
</authors>
<title>Chunking with maximum entropy models.</title>
<date>2000</date>
<booktitle>In Proceedings of the CoNLL Workshop</booktitle>
<pages>139--141</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="1277" citStr="Koeling, 2000" startWordPosition="190" endWordPosition="191">th two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar. 1 Introduction The use of maximum entropy (ME) models has become popular in Statistical NLP; some example applications include part-of-speech (Pos) tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1999; Johnson et al., 1999) and language modelling (Rosenfeld, 1996). Many tagging problems have been successfully modelled in the ME framework, including POS tagging, with state of the art performance (van Halteren et al., 2001), &amp;quot;supertagging&amp;quot; (Clark, 2002) and chunking (Koeling, 2000). Generalised Iterative Scaling (GIS) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;, feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm. However, the extra book keeping required for us means that GIS i</context>
</contexts>
<marker>Koeling, 2000</marker>
<rawString>Rob Koeling. 2000. Chunking with maximum entropy models. In Proceedings of the CoNLL Workshop 2000, pages 139-141, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Workshop on Natural Language Learning,</booktitle>
<pages>49--55</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1918" citStr="Malouf, 2002" startWordPosition="298" endWordPosition="299">ing (GIS) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a &amp;quot;correction&amp;quot;, or &amp;quot;slack&amp;quot;, feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm. However, the extra book keeping required for us means that GIS is often faster in practice (Malouf, 2002). This paper shows, by a simple adaptation of Berger&apos;s proof for the convergence of HS (Berger, 1997), that GIS does not require a correction feature. We also investigate how the use of a correction feature affects the performance of ME taggers. GIS and HS obtain a maximum likelihood estimate (mLE) of the parameters, and, like other MLE methods, are susceptible to overfitting. A simple technique used to avoid overfitting is a frequency cutoff, in which only frequently occurring features are included in the model (Ratnaparkhi, 1998). However, more sophisticated smoothing techniques exist, such </context>
<context position="6984" citStr="Malouf, 2002" startWordPosition="1192" endWordPosition="1193">o the iis convergence proof. This feature does not contribute to the model and can be ignored during weight update. Introducing this null feature still satisfies Jensen&apos;s inequality, which is used to provide a lower bound on the change in likelihood between iterations, and the existing GIS weight update (7) can still be derived analytically. An advantage of GIS is that it is a very simple algorithm, made even simpler by the removal of the correction feature. This simplicity means that, although GIS requires more iterations than 11s to reach convergence, in practice it is significantly faster (Malouf, 2002). 4 Smoothing Maximum Entropy Models Several methods have been proposed for smoothing ME models (see Chen and Rosenfeld (1999)). For taggers, a standard technique is to eliminate low frequency features, based on the assumption that they are unreliable or uninformative (Ratnaparkhi, 1998). Studies of infrequent features in other domains suggest this assumption may be incorrect (Daelemans et al., 1999). We test this for ME taggers by replacing the cutoff with the use of a Gaussian prior, a technique which works well for language models (Chen and Rosenfeld, 1999). When using a Gaussian prior, the</context>
<context position="19269" citStr="Malouf, 2002" startWordPosition="3323" endWordPosition="3324">ging because the feature space is sparser by virtue of the much larger tagset. Gaussian smoothing would also allow us to incorporate rare longer range dependencies as features, without risk of overfitting. This may further boost supertagger performance. 10 Conclusion This paper has demonstrated, both analytically and empirically, that GIS does not require a correction feature Eliminating the correction feature simplifies further the already very simple estimation algorithm. Although GIS is not as fast as some alternatives, such as conjugate gradient and limited memory variable metric methods (Malouf, 2002), our C&amp;C POS tagger takes less than 10 minutes to train, and the space requirements are modest, irrespective of the size of the tagset. We have also shown that using a Gaussian prior on the parameters of the ME model improves performance over a simple frequency cutoff. The Gaussian prior effectively relaxes the constraints on the ME model, which allows the model to use low frequency features without overfitting. Achieving optimal performance with Gaussian smoothing and without cutoffs demonstrates that low frequency features can contribute to good performance. Acknowledgements We would like t</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the Sixth Workshop on Natural Language Learning, pages 49-55, Taipei, Taiwan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>