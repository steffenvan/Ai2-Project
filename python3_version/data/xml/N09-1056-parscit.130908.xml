<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005435">
<title confidence="0.839406">
For a few dollars less: Identifying review pages sans human labels
</title>
<author confidence="0.992353">
Luciano Barbosa
</author>
<affiliation confidence="0.868813">
Dept. of Computer Science
University of Utah
Salt Lake City, UT 84112, USA.
</affiliation>
<email confidence="0.997934">
lbarbosa@cs.utah.edu
</email>
<author confidence="0.723978">
Ravi Kumar Bo Pang Andrew Tomkins
</author>
<affiliation confidence="0.613203">
Yahoo! Research
</affiliation>
<address confidence="0.7694625">
701 First Ave
Sunnyvale, CA 94089, USA.
</address>
<email confidence="0.949026">
{ravikumar,bopang,atomkins}@yahoo-inc.com
</email>
<sectionHeader confidence="0.995889" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999575">
We address the problem of large-scale auto-
matic detection of online reviews without us-
ing any human labels. We propose an efficient
method that combines two basic ideas: Build-
ing a classifier from a large number of noisy
examples and using the structure of the web-
site to enhance the performance of this classi-
fier. Experiments suggest that our method is
competitive against supervised learning meth-
ods that mandate expensive human effort.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984781818182">
Shoppers are migrating to the web and online re-
views are playing a critical role in affecting their
shopping decisions, online and offline. According
to two surveys published by comScore (2007) and
Horrigan (2008), 81% of web users have done on-
line research on a product at least once. Among
readers of online reviews, more than 70% reported
that the reviews had a significant influence on their
purchases. Realizing this economic potential, search
engines have been scrambling to cater to such user
needs in innovative ways. For example, in response
to a product-related query, a search engine might
want to surface only review pages, perhaps via a “fil-
ter by” option, to the user. More ambitiously, they
might want to dissect the reviews, segregate them
into novice and expert judgments, distill sentiments,
and present an aggregated “wisdom of the crowds”
opinion to the user. Identifying review pages is the
indispensable enabler to fulfill any such ambition;
nonetheless, this problem does not seem to have
been addressed at web scale before.
Detecting review webpages in a few, review-only
websites is an easy, manually-doable task. A large
fraction of the interesting review content, however,
is present on pages outside such websites. This is
where the task becomes challenging. Review pages
might constitute a minority and can be buried in
a multitude of ways among non-review pages —
for instance, the movie review pages in nytimes.
com, which are scattered among all news articles, or
the product review pages in amazon.com, which
are accessible from the product description page. An
automatic and scalable method to identify reviews
is thus a practical necessity for the next-generation
search engines. The problem is actually more gen-
eral than detecting reviews: it applies to detecting
any “horizontal” category such as buying guides, fo-
rums, discussion boards, FAQs, etc.
Given the nature of these problems, it is tempt-
ing to use supervised classification. A formidable
barrier is the labeling task itself since human la-
bels need time and money. On the other hand, it
is easier to generate an enormous number of low-
quality labeled examples through purely automatic
methods. This prompts the question: Can we do re-
view detection by focusing just on the textual con-
tent of a large number of automatically obtained but
low-quality labeled examples, perhaps also utilizing
the site structure specific to each website? And how
will it compare to the best supervised classification
method? We address these questions in this paper.
Main contributions. We propose the first end-to-
end method that can operate at web scale to effi-
ciently detect review pages. Our method is based
on using simple URL-based clues to automatically
</bodyText>
<page confidence="0.985442">
494
</page>
<note confidence="0.890507">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 494–502,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999558285714286">
partition a large collection of webpages into two
noisy classes: One that consists mostly of review
webpages and another that consists of a mixture
of some review but predominantly non-review web-
pages (more details in Section 4.2).
We analyze the use of a naive Bayes classifier in
this noisy setting and present a simple algorithm for
review page classification. We further enhance the
performance of this classifier by incorporating infor-
mation about the structure of the website that is man-
ifested through the URLs of the webpages. We do
this by partitioning the website into clusters of web-
pages, where the clustering delicately balances the
information in the site-unaware labels provided by
the classifier in the previous step and the site struc-
ture encoded in the URL tokens; a decision tree is
used to accomplish this. Our classification method
for noisily-labeled examples and the use of site-
specific cues to improve upon a site-independent
classifier are general techniques that may be appli-
cable in other large-scale web analyses.
Experiments on 2000 hand-labeled webpages
from 40 websites of varying sizes show that besides
being computationally efficient, our human-label-
free method not only outperforms those based on
off-the-shelf subjectivity detection but also remains
competitive against the state-of-the-art supervised
text classification that relies on editorial labels.
</bodyText>
<sectionHeader confidence="0.999606" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999955846153847">
The related work falls into roughly four categories:
Document- and sentence-level subjectivity detec-
tion, sentiment analysis in the context of reviews,
learning from noisy labeled examples, and exploit-
ing site structure for classification.
Given the subjective nature of reviews, document-
level subjectivity classification is closely related to
our work. There have been a number of approaches
proposed to address document-level subjectivity in
news articles, weblogs, etc. (Yu and Hatzivas-
siloglou, 2003; Wiebe et al., 2004; Finn and Kush-
merick, 2006; Ni et al., 2007; Stepinski and Mit-
tal, 2007). Ng et al. (2006) experiment with review
identification for known domains using datasets with
clean labels (e.g., movie reviews vs. movie-related
non-reviews), a setting different from that of ours.
Pang and Lee (2008b) present a method on re-
ranking documents that are web search results for a
specific query (containing the word review) based
on the subjective/objective distinction. Given the na-
ture of the query, they implicitly detect reviews from
unknown sources. But their re-ranking algorithm
only applies to webpages known to be (roughly) re-
lated to the same narrow subject. Since the web-
pages in our datasets cover not only a diverse range
of websites but also a diverse range of topics, their
approach does not apply. To the best of our knowl-
edge, there has been no work on identifying review
pages at the scale and diversity we consider.
Subjectivity classification of within-document
items, such as terms, has been an active line of re-
search (Wiebe et al. (2004) present a survey). Iden-
tifying subjective sentences in a document via off-
the-shelf packages is an alternative way of detect-
ing reviews without (additional) human annotations.
In particular, the OpinionFinder system (Riloff and
Wiebe, 2003; Wiebe and Riloff, 2005) is a state-of-
the-art knowledge-rich sentiment-analysis system.
We will use it as one of our baselines and compare
its performance with our methods.
There has been a great deal of previous work in
sentiment analysis that worked with reviews, but
they were typically restricted to using reviews ex-
tracted from one or two well-known sources, by-
passing automatic review detection. Examples of
such early work include (Turney, 2002; Pang et al.,
2002; Dave et al., 2003; Hu and Liu, 2004; Popescu
and Etzioni, 2005). See Pang and Lee (2008a) for
a more comprehensive survey. Building a collection
of diverse review webpages, not limited to one or
two hosts, can better facilitate such research.
Learning from noisy examples has been studied
for a long time in the learning theory community
(Angluin and Laird, 1988). Learning naive Bayes
classifiers from noisy data (either features or labels
or both) was studied by Yang et al. (2003). Their
focus, however, is to reconstruct the underlying con-
ditional probability distributions from the observed
noisy dataset. We, on the other hand, rely on the vol-
ume of labels to drown the noise. Along this spirit,
Snow et al. (2008) show that obtaining multiple low-
quality labels (through Mechanical Turk) can ap-
proach high-quality editorial labels. Unlike in their
setting, we do not have multiple low-quality labels
for the same URL. The extensive body of work in
</bodyText>
<page confidence="0.998686">
495
</page>
<bodyText confidence="0.999955727272727">
semi-supervised learning or learning from one class
is also somewhat relevant to our work. A major dif-
ference is that they tend to work with small amount
of clean, labeled data. In addition, many semi-
supervised/transductive learning algorithms are not
efficient for web-scale data.
Using site structure for web analysis tasks has
been addressed in a variety of contexts. For ex-
ample, Kening et al. (2005) exploit the structure
of a website to improve classification. On a re-
lated note, co-training has also been used to utilize
inter-page link information in addition to intra-page
textual content: Blum and Mitchell (1998) use an-
chor texts pointing to a webpage as the alternative
“view” of the page in the context of webpage clas-
sification. Their algorithm is largely site-unaware
in that it does not explicitly exploit site structures.
Utilizing site structures also has remote connections
to wrapper induction, and there is extensive litera-
ture on this topic. Unfortunately, the methods in all
of these work require human labeling, which is pre-
cisely what our work is trying to circumvent.
</bodyText>
<sectionHeader confidence="0.997468" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999971923076923">
In this section we describe our basic methodology
for identifying review pages. Our method consists
of two main steps. The first is to use a large amount
of noisy training examples to learn a basic classifier
for review webpages; we adapt a simple naive Bayes
classifier for this purpose. The second is to improve
the performance of this basic classifier by exploiting
the website structure; we use a decision tree for this.
Let P be the set of all webpages. Let C+ denote
the positive class, i.e., the set of all review pages and
let C− denote the negative class, i.e., the set of all
non-review pages. Each webpage p is exactly in one
of C+ or C−, and is labeled +1 or −1 respectively.
</bodyText>
<subsectionHeader confidence="0.999752">
3.1 Learning from large amounts of noisy data
</subsectionHeader>
<bodyText confidence="0.999858484848485">
Previous work using supervised or semi-supervised
learning approaches for sentiment analysis assumes
relatively high-quality labels that are produced ei-
ther via human annotation or automatically gener-
ated through highly accurate rules (e.g., assigning
positive or negative label to a review according to
automatically extracted star ratings).
We examine a different scenario where we can au-
tomatically generate large amount of relatively low-
quality labels. Section 4.2 describes the process
in more detail, but briefly, in a collection of pages
crawled from sites that are very likely to host re-
views, those with the word review in their URLs
are very elikely to contain reviews (the noisy posi-
tive set C+) and the rest of the pages on those sites
are less likely to contain reviews (the more noisy
negative set eC−). More formally, for a webpage
p, suppose Pr[p ∈ C+  |p ∈ eC+] = α and
Pr[p ∈ C+  |p ∈ eC−] = β, where 1 &gt; αe&gt; β &gt;e0.
Can we still learn something useful from C+ and C−
despite the labels being highly noisy?
The following analysis is based on a naive Bayes
classifier. We chose naive Bayes classifier since the
learning phase can easily be parallelized.
Given a webpage (or a document) p represented
as a bag of features {fi}, we wish to assign a class
arg maxc∈{C+,C−} Pr[c  |p] to this webpage. Naive
Bayes classifiers assume fi’s to be conditionally in-
dependent and we have Pr[p  |c] = QPr[fi  |c].
Let ri = Pr[fi |C+]/ Pr[fi |C−] denote the con-
tribution of each feature towards classification, and
rc = Pr[C+]/Pr[C−] denote the ratio of class pri-
ors. First note that
</bodyText>
<equation confidence="0.998868833333333">
�Pr[C+] �
log Pr[C+|p]
Pr[C−|p] = log Pr[C−] · Pr[p|C+]
Pr[p|C−]
�Pr[C+] = log rc + P log ri.
= log Pr[C−] · Q ri�
</equation>
<bodyText confidence="0.9990895">
A webpage p receives label +1 iff Pr[C+  |p] &gt;
Pr[C−  |p], and by above, if and only if P log ri &gt;
− log rc.
When we do not have a reasonable estimate of
Pr[C+] and Pr[C−], as in our setting, the best we
can do is to assume rc = 1. In this case, p receives
label +1 if and only if P log ri &gt; 0. Thus, a feature
fi with log ri &gt; 0 has a positive contribution to-
wards p being labeled +1; call fi to be a “positive”
feature. Typically we use relative-frequency estima-
tion of Pr[c] and Pr[fi |c] for c ∈ {C+, C−}. Now,
how does the estimation from a dataset with noisy
labels compare with the estimation from a dataset
with clean labels?
</bodyText>
<equation confidence="0.862111384615385">
To examine this, we calculate the following:
Pr[fi  |eC+] = αPr[fi  |C+] + (1 − α) Pr[fi  |C−],
Pr[fi  |eC−] = β Pr[fi  |C+] + (1 − β) Pr[fi  |C−].
αri+(1−α) Clearly ˜ri is mono-
= βri+(1−β).
tonic but not linear in ri. Furthermore, it is bounded:
Let ˜ri = Pr[fi|
Pr[fi|
C+]
496
(1 − α)/(1 − β) &lt; ˜ri &lt; α/β. However,
˜ri &gt; 1 ,#� αri + (1 − α) &gt; βri + (1 − β)
,#=� (α−β)ri &gt; (α−β) ,#=� ri &gt; 1, where
</equation>
<bodyText confidence="0.9992716875">
the last step used α &gt; β. Thus, the sign of log ˜ri is
the same as that of log ri, i.e., a feature contribut-
ing positively to E log ri will continue to contribute
positively to E log ˜ri (although its magnitude is dis-
torted) and vice versa.
The above analysis motivates an alternative model
to naive Bayes. Instead of each feature fi placing
a weighted vote log ˜ri in the final decision, we trust
only the sign of log ˜ri, and let each feature fi place a
vote for the class C+ (respectively, C−) if log ˜ri &gt; 0
(respectively, log ˜ri &lt; 0). Intuitively, this model
just compares the number of “positive” features and
the number of “negative” features, ignoring the mag-
nitude (since it is distorted anyway). This is pre-
cisely our algorithm: For a given threshold γ, the
final label nbuγ(p) of a webpage p is given by
</bodyText>
<equation confidence="0.8777">
nbuγ(p) = sgn (E sgn(log ˜ri) − γ) ,
</equation>
<bodyText confidence="0.996221090909091">
where sgn is the sign function. For comparison
purposes, we also indicate the “weighted” version:
nbwγ(p) = sgn (E log ˜ri − γ) .
If γ = 0, we omit γ and use nb to denote a generic
label assigned by any of the above algorithms.
Note that even though our discussions were for
two-class and in particular, review classification,
they are equally applicable to a wide range of clas-
sification tasks in large-scale web-content analysis.
Our analysis of learning from automatically gener-
ated noisy examples is thus of independent interest.
</bodyText>
<subsectionHeader confidence="0.999732">
3.2 Utilizing site structure
</subsectionHeader>
<bodyText confidence="0.998869215686275">
Can the structure of a website be exploited to im-
prove the classification of webpages given by nb(·)?
While not all websites are well-organized, quite a
number of them exhibit certain structure that makes
it possible to identify large subsites that contain only
review pages. Typically but not always this structure
is manifested through the tokens in the URL corre-
sponding to the webpage. For instance, the pattern
http://www.zagat.com/verticals/
PropertyDetails.aspx?VTD=a&amp;R=b,
where a,b are numbers, is indicative of all
webpages in zagat.com that are reviews of
restaurants. In fact, we can think of this as a
generalization of having the keyword review in
the URL. Now, suppose we have an initial labeling
nb(p) E {+1} for each webpage p produced by a
classifier (as in the previous section, or one that is
trained on a small set of human annotated pages),
can we further improve the labeling using the
pattern in the URL structure?
It is not immediate how to best use the URL
structure to identify the review subsites. First,
URLs contain irrelevant information (e.g., the to-
ken verticals in the above example), thus clus-
tering by simple cosine similarity may not dis-
cover the review subsites. Second, the subsite
may not correspond to a subtree in the URL hi-
erarchy, i.e., it is not reasonable to expect all
the review URLs to share a common prefix.
Third, the URLs contain a mixture of path com-
ponents (e.g., www.zagat.com/verticals/
PropertyDetails.aspx) and key-value pairs
(e.g., VTD=a and R=b) and hence each token (re-
gardless of its position) in the URL could play a
role in determining the review subsite. Furthermore,
conjunction of presence/absence of certain tokens in
the URL may best correspond to subsite member-
ship. In light of these, we represent each URL (and
hence the corresponding webpage) by a bag {gi} of
tokens obtained from the URL. We perform a crude
form of feature selection by dropping tokens that
are either ubiquitous (occurring in more than 99%
of URLs) or infrequent (occurring in fewer than 1%
of URLs) in a website; neither yields useful infor-
mation.
Our overall approach will be to use gi’s to par-
tition P into clusters {Ci} of webpages such that
each cluster Ci is predominantly labeled as either
review or non-review by nb(·). This automati-
cally yields a new label cls(p) for each page p,
which is the majority label of the cluster of p:
</bodyText>
<equation confidence="0.9881185">
(E )
cls(p) = sgn q∈C(p) nb(q) ,
</equation>
<bodyText confidence="0.9999744">
where C(p) is the cluster of p. To this end, we use
a decision tree classifier to build the clusters. This
classifier will use the features {gi} and the target la-
bels nb(·). The classifier is trained on all the web-
pages in the website and in the obtained decision
tree, each leaf, consisting of pages with the same
set of feature values leading down the path, corre-
sponds to a cluster of webpages. Note that the clus-
ters delicately balance the information in the site-
unaware labels nb(·) and the site structure encoded
</bodyText>
<page confidence="0.993449">
497
</page>
<bodyText confidence="0.99906155">
in the URLs (given by gi’s). Thus the label cls(p)
can be thought of as a smoothed version of nb(p).
Even though we can expect most clusters to be ho-
mogeneous (i.e., pure reviews or non-reviews), the
above method can produce clusters that are inher-
ently heterogeneous. This can happen if the web-
site URLs are organized such that many subsites
contain both review and non-review webpages. To
take this into account, we propose the following
hybrid approach that interpolates between the un-
smoothed labels given by nb(·) and the smoothed
labels given by cls(·). For a cluster Ci, the dis-
crepancy disc(Ci) = Ep∈Ci[cls(p) =6 nb(p)]; this
quantity measures the number of disagreements be-
tween the majority label cls(p) and the original label
nb(p) for each page p in the cluster. The decision
tree guarantees disc(Ci) ≤ |Ci|/2. We call a cluster
Ci to be δ-homogeneous if disc(Ci) ≤ δ|Ci|, where
δ ∈ [0,1/2]. For a fixed δ, the hybrid label of a web-
page p is given by
</bodyText>
<equation confidence="0.877525666666667">
� cls(p) if C(p) is δ-homogeneous,
hybδ(p) = nb(p) otherwise.
Note that hyb1/2(p) = cls(p) and hyb0(p) = nb(p).
</equation>
<bodyText confidence="0.9989884">
Note that in the above discussions, any clustering
method that can incorporate the site-unaware labels
nb(·) and the site-specific tokens in gi’s could have
been used; off-the-shelf decision tree was merely a
specific way to realize this.
</bodyText>
<sectionHeader confidence="0.998259" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.9998964">
It is crucial for this study to create a dataset that
is representative of a diverse range of websites that
host reviews over different topics in different styles.
We are not aware of any extensive index of online
review websites and we do not want to restrict our
study to a few well-known review aggregation web-
sites (such as yelp.com or zagat.com) since
this will not represent the less popular and more spe-
cialized ones. Instead, we utilized user-generated
tags for webpages, available on social bookmarking
websites such as del.icio.us.
We obtained (a sample of) a snapshot of URL–tag
pairs from del.icio.us. We took the top one
thousand sites with review* tags; these websites
hopefully represent a broad coverage. We were able
to crawl over nine hundred of these sites and the re-
sulting collection of webpages served as the basis
of the experiments in this paper. We refer to these
websites (or the webpages from these sites, when it
is clear from the context) as Sall.
</bodyText>
<subsectionHeader confidence="0.997267">
4.1 Gold-standard test set
</subsectionHeader>
<bodyText confidence="0.999994975">
When the websites are as diverse as represented in
Sall, there is no perfect automatic way to generate
the ground truth labels. Thus we sampled a number
of pages for human labeling as follows.
First, we set aside 40 sites as the test sites (S40).
In order to represent different types of websites (to
the best we can), we sampled the 40 sites so that S40
covers different size ranges, since large-scale web-
sites and small-scale websites are often quite dif-
ferent in style, topic, and content. We uniformly
sampled 10 sites from each of the four size cate-
gories (roughly, sites with 100–5K, 5K–25K, 25K–
100K, and 100K+ webpages)1. Indeed, S40 (as did
Sall) covered a wide range of topics (e.g., games,
books, restaurants, movies, music, and electronics)
and styles (e.g., dedicated review sites, product sites
that include user reviews, newspapers with movie re-
view sections, religious sites hosting book reviews,
and non-English review sites).
We then sampled 50 pages to be labeled from each
site in S40. Since there are some fairly large sites
that have only a small number of review pages, a
uniform sampling may yield no review webpages
from those sites. To reflect the natural distribu-
tion on a website and to represent pages from both
classes, the webpages were sampled in the follow-
ing way. For each website in S40, 25 pages were
uniformly sampled (representing the natural distri-
bution) and 25 pages were sampled from among
“equivalence classes” based on URLs so that pages
from each major URL pattern were represented.
Here, each webpage in the site is represented by a
URL signature containing the most frequent tokens
that occur in the URLs in that site and all pages with
the same signature form an equivalence class.
For our purposes, a webpage is considered a re-
view if it contains significant amount of textual in-
formation expressing subjective opinions on or per-
sonal experiences with a given product / service.
When in doubt, the guiding principle is whether
</bodyText>
<footnote confidence="0.924839666666667">
1As we do not want to waste human annotation on sites with
no reviews at all, a quick pre-screening process eliminated can-
didate sites that did not seem to host any reviews.
</footnote>
<page confidence="0.9978">
498
</page>
<bodyText confidence="0.999560487804878">
a page can be a satisfactory result page for users
searching for reviews. More specifically, the human
annotation labeled each webpage, after thoroughly
examining the content, with one of the following
seven intuitive labels: “single” (contains exactly one
review), “multiple” (concatenation of more than one
review), “no” (clearly not a review page), “empty”
(looks like a page that could contain reviews but had
none), “login” (a valid user login needed to look at
the content), “hub” (a pointer to one or more review
pages), and “ambiguous” (border-line case, e.g., a
webpage with a one line review). The first two labels
were treated as +1 (i.e., reviews) and the last five la-
bels were treated as −1 (i.e., non-reviews). Out of
the 2000 pages, we obtained 578 pages labeled +1
and the 1422 pages labeled −1. On a pilot study us-
ing two human judges, we obtained 78% inter-judge
agreement for the seven labels and 92% inter-judge
agreement if we collapse the labels to +1. Percent-
ages of reviews in our samples from different sites
range from 14.6% to 93.9%.
Preprocessing for text-based analysis. We pro-
cessed the crawled webpages using lynx to ex-
tract the text content. To discard templated content,
which is an annoying issue in large-scale web pro-
cessing, and HTML artifacts, we used the following
preprocessing. First, the HTML tags &lt;p&gt;, &lt;br&gt;,
&lt;/tr&gt;, and &lt;/td&gt; were interpreted as paragraph
breaks, the ‘.’ inside a paragraph was interpreted as
a sentence break, and whitespace was used to tok-
enize words in a sentence. A sentence is considered
“good” if it has at least seven alphabetic words and
a paragraph is considered “good” if it has at least
two good sentences. After extracting the text us-
ing lynx, only the good paragraphs were retained.
This effectively removes most of the templated con-
tent (e.g., navigational phrases) and retains most of
the “natural language” texts. Because of this pre-
processing, 485 pages out of 2000 turned out to be
empty and these were discarded (human labels on
97% of these empty pages were −1).
</bodyText>
<subsectionHeader confidence="0.999069">
4.2 Dataset with noisy labels
</subsectionHeader>
<bodyText confidence="0.99953384375">
As discussed in Section 3.1, our goal is to obtain a
large noisy set of positive and negative labeled ex-
amples. We obtained these labels for the webpages
in the training sites, Sr,st, which is essentially Sall �
S40. First, the URLs in Sr,st were tokenized using a
unigram model based on an English dictionary; this
is so that strings such as reviewoftheday are
properly interpreted.
eC+: To be labeled +1, the path-component of
the URL of the webpage has to contain the token
review. Our assumption is that such pages are
highly likely to be review pages. On a uniform sam-
ple of 100 such pages in Sall, 90% were found to be
genuine reviews. Thus, we obtained a collection of
webpages with slightly noisy positive labels.
eC_: The rest of the pages in Sr,st were labeled
−1. Clearly this is a noisy negative set since not all
pages containing reviews have review as part of
their URLs (recall the example from zagat.com);
thus many pages in eC_ can still be reviews.
While the negative labels in Sr,st are more noisy
than the positive labels, we believe most of the non-
review pages are in eC_, and as most websites con-
tain a significant number eof non-review pages, the
percentage of reviews in C_ is smaller than that in
eC+ (the assumption α &gt; Q in Section 3.1).
We collected all the paragraphs (as defined ear-
lier) from both eC+ and eC_ separately. We elim-
inated duplicate paragraphs (this further mitigates
the templates issue, especially for sites generated
by content-management software), and trained a un-
igram language model as in Section 3.1.
</bodyText>
<sectionHeader confidence="0.994602" genericHeader="evaluation">
5 Evaluations
</sectionHeader>
<bodyText confidence="0.999982916666667">
The evaluations were conducted on the 1515 labeled
(non-empty) pages in S40 described in Section 4.1.
We report the accuracy (acc.) as well as precision
(prec.), recall (rec.), and f-measure (fmeas.) for C+.
Trivial baselines. Out of the 1515 labeled pages,
565 were labeled +1 and 950 were labeled −1. Ta-
ble 1 summarizes the performance of baselines that
always predict one of the classes and a baseline that
randomly select a class according to the class dis-
tribution S40. As we can see, the best accuracy
is .63, the best f-measure is .54, and they cannot
be achieved by the same baseline. Before present-
</bodyText>
<table confidence="0.8395385">
acc. prec. rec. fmeas.
always C− .63 - 0 -
always C+ .37 .37 1 .54
random .53 .37 .37 .37
</table>
<tableCaption confidence="0.999645">
Table 1: Trivial baseline performances.
</tableCaption>
<page confidence="0.999074">
499
</page>
<bodyText confidence="0.996921">
ing the main results of our methods, we introduce
a much stronger baseline that utilizes a knowledge-
rich subjectivity detection package.
</bodyText>
<subsectionHeader confidence="0.999117">
5.1 Using subjectivity detectors
</subsectionHeader>
<bodyText confidence="0.998097490566038">
This baseline is motivated by the fact that reviews
often contain extensive subjective content. There are
many existing techniques that detect subjectivity in
text. OpinionFinder (http://www.cs.pitt.
edu/mpqa/opinionfinderrelease/) is a
well-known system that processes documents and
automatically identifies subjective sentences in
them. OpinionFinder uses two subjective sentence
classifiers (Riloff and Wiebe, 2003; Wiebe and
Riloff, 2005). The first (denoted opfA) focuses on
yielding the highest accuracy; the second (denoted
opfB) optimizes precision at the expense of recall.
The methods underlying OpinionFinder incorporate
extensive tools from linguistics (including, speech
activity verbs, psychological verbs, FrameNet verbs
and adjectives with frame “experiencer”, among oth-
ers) and machine learning. In terms of performance,
previous work has shown that OpinionFinder is a
challenging system to improve upon for review re-
trieval (Pang and Lee, 2008b). Computationally,
OpinionFinder is very expensive and hence unattrac-
tive for large-scale webpage analysis (running Opin-
ionFinder on 1515 pages took about five hours).
Therefore, we also propose a light-weight subjectiv-
ity detection mechanism called lwd, which counts
the number of opinion words in each sentence in the
text. The opinion words (5403 of them) were ob-
tained from an existing subjectivity lexicon (http:
//www.cs.pitt.edu/mpqa).
We ran both opfA and opfB on the tokenized text
(running them on raw HTML produced worse re-
sults). Each sentence in the text was labeled subjec-
tive or objective. We experimented with two ways
to label a document using sentence-level subjectiv-
ity labels. We labeled a document +1 if it contained
at least k subjective sentences (denoted as opf?(k),
where k &gt; 0 is the absolute threshold), or at least
f fraction of its sentences were labeled subjective
(denoted as opf?(f), where f ∈ (0, 1] is the rela-
tive threshold). We conducted exhaustive parameter
search with both opfA and opfB. For instance, the
performances of opfA as a function of the thresh-
olds, both absolute and relative, is shown in Fig-
ure 1. Table 2 summarizes the best performances
of opf?(k) (first two rows) and opf?(f) (next two
rows), in terms of accuracy and f-measure (bold-
faced). Similarly, for lwd, we labeled a document
+1 if at least k sentences have at least E opin-
ion words (denoted lwd(k, E).) Table 2 once again
shows the best performing parameters for both accu-
racy and f-measure for lwd. Our results indicate that
a simple method such as lwd can come very close to
a sophisticated system such as opf?.
</bodyText>
<tableCaption confidence="0.954523">
Table 2: Best performances of opf* and lwd methods.
</tableCaption>
<figureCaption confidence="0.9988895">
Figure 1: Performance of opfA as a function of thresh-
olds: Absolute and relative.
</figureCaption>
<subsectionHeader confidence="0.998002">
5.2 Main results
</subsectionHeader>
<bodyText confidence="0.926524285714286">
As stated earlier, we do not have any prior knowl-
edge about the value of -y and hence have to work
with -y = 0. To investigate the implications of
this assumption, we study the performance of nbuγ
and nbwγ as a function of -y. The accuracy and f-
measures are plotted in Figure 2. There are three
acc. prec. rec. fmeas.
</bodyText>
<figure confidence="0.99179075">
opfA(2)
opfB(2)
.704 .597 .634 .615
.659 .526 .857 .652
opfB(.36)
.636 .523 .797 .632
lwd(1, 4)
lwd(1,1)
.716 .631 .572 .600
.666 .538 .740 .623
opfA(.17)
.652 .529 .614 .568
</figure>
<page confidence="0.927239">
500
</page>
<table confidence="0.994466">
acc. prec. rec. fmeas.
nbu .753 .652 .726 .687
cls .756 .696 .616 .654
hyb1/3 .777 .712 .674 .693
</table>
<tableCaption confidence="0.999891">
Table 3: Performance of our methods.
</tableCaption>
<bodyText confidence="0.999742125">
conclusions that can be drawn from this study: (i)
The peak values of accuracy and f-measure are com-
parable for both nbu.y and nbw.y, (ii) at -y = 0, nbu is
much better than nbw, in terms of both accuracy and
f-measure, and (iii) the best performance of nbu.y oc-
curs at -y ≈ 0. Given the difficulty of obtaining -y if
one were to use nbw.y, the above conclusions vali-
date our intuition and the algorithm in Section 3.1.
</bodyText>
<figureCaption confidence="0.994447">
Figure 2: Performance as threshold changes: Comparing
nbu.y (marked as (u)) with nbw.y (marked as (w)).
</figureCaption>
<bodyText confidence="0.9999725">
Table 3 shows the performance of the site-specific
method outlined in Section 3.2. The clusters
were generated using the unpruned J48 decision
tree in Weka (www.cs.waikato.ac.nz/ml/
weka). In our experiments, we set δ = 1/3 as a
natural choice for the hybrid method. As we see
the performance of nbu is about 7% better than the
best performance using a subjectivity-based method
(in terms of accuracy). The performance of the
smoothed labels (decision tree-based clustering) is
comparable to that of nbu. However, the hybrid
method hyb1/3 yields an additional 3% relative im-
provement over nbu. Paired t-test over the accura-
cies for these 40 sites shows both hyb1/3 and nbu
to be statistically significantly better than the opf*
with best accuracy (with p &lt; 0.05, p &lt; 0.005,
respectively), and hyb1/3 to be statistically signifi-
cantly better than nbu (with p &lt; 0.05).
</bodyText>
<subsectionHeader confidence="0.924578">
5.3 Cross-validation on S40
</subsectionHeader>
<bodyText confidence="0.999439296296296">
While the main focus of our paper is to study
how to detect reviews without human labels, we
present cross validation results on S40 as a compar-
ison point. The goal of this experiment is to get a
sense of the best possible accuracy and f-measure
numbers using labeled data and the state-of-the-
art method for text classification, namely, SVMs.
In other words, the performance numbers obtained
through SVMs and cross-validation can be thought
of as realistic “upper bounds” on the performance of
content-based review detection. We used SVMlight
(svmlight.joachims.org) for this purpose.
The cross-validation experiment was conducted
as follows. We split the data by site to simulate the
more realistic setting where pages in the test set do
not necessarily come from a known site. Each fold
consisted of one site from each size category; thus,
36 of the 40 sites in S40 were used for training and
the remainder for testing. Over ten folds, the aver-
age performance was: accuracy .795, precision .759,
recall .658, and f-measure .705.
Thus our methods in Section 3 come reason-
ably close to the “upper bound” given by SVMs
and human-labeled data. In fact, while the su-
pervised SVMs statistically significantly outperform
nbu, they are statistically indistinguishable from
hyb1/3 via paired t-test over site-level accuracies.
</bodyText>
<sectionHeader confidence="0.999484" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999643235294118">
In this paper we proposed an automatic method to
perform efficient and large-scale detection of re-
views. Our method is based on two principles:
Building a classifier from a large number of noisy
labeled examples and using the site structure to im-
prove the performance of this classifier. Extensive
experiments suggest that our method is competitive
against supervised learning methods that depend on
expensive human labels. There are several interest-
ing avenues for future research, including improv-
ing the current method for exploiting the site struc-
ture. On a separate note, previous research has ex-
plicitly studied sentiment analysis as an application
of transfer learning (Blitzer et al., 2007). Given the
diverse range of topics present in our dataset, ad-
dressing topic-dependency is also an interesting fu-
ture research direction.
</bodyText>
<page confidence="0.996333">
501
</page>
<sectionHeader confidence="0.993866" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999807493827161">
Dana Angluin and Philip D. Laird. 1988. Learning from
noisy examples. Machine Learning, 2(4):343–370.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of 45th ACL.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of 11th COLT, pages 92–100.
comScore. 2007. Online consumer-generated re-
views have significant impact on offline pur-
chase behavior. Press Release, November.
http://www.comscore.com/press/
release.asp?press=1928.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of 12th WWW, pages 519–528.
Aidan Finn and Nicholas Kushmerick. 2006. Learn-
ing to classify documents according to genre. JASIST,
7(5):1506–1518.
John A. Horrigan. 2008. Online shopping. Pew Internet
&amp; American Life Project Report.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In Proceedings of 19th
AAAI, pages 755–760.
Gao Kening, Yang Leiming, Zhang Bin, Chai Qiaozi, and
Ma Anxiang. 2005. Automatic classification of web
information based on site structure. In Cyberworlds,
pages 552–558.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006.
Examining the role of linguistic knowledge sources in
the automatic identification and classification of re-
views. In Proceedings of 21st COLING/44th ACL
Poster, pages 611–618.
Xiaochuan Ni, Gui-Rong Xue, Xiao Ling, Yong Yu, and
Qiang Yang. 2007. Exploring in the weblog space
by detecting informative and affective articles. In Pro-
ceedings of 16th WWW, pages 281–290.
Bo Pang and Lillian Lee. 2008a. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135.
Bo Pang and Lillian Lee. 2008b. Using very simple
statistics for review search: An exploration. In Pro-
ceedings of 22nd COLING. Poster.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP,
pages 79–86.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT/EMNLP, pages 339–346.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP, pages 105–112.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of EMNLP.
Adam Stepinski and Vibhu Mittal. 2007. A fact/opinion
classifier for news articles. In Proceedings of 30th SI-
GIR, pages 807–808.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of 40th ACL, pages
417–424.
Janyce M. Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unanno-
tated texts. In Proceedings of CICLing, pages 486–
497.
Janyce M. Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277–308.
Yirong Yang, Yi Xia, Yun Chi, and Richard R. Muntz.
2003. Learning naive Bayes classifier from noisy data.
Technical Report 56, UCLA.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP, pages 129–136.
</reference>
<page confidence="0.99762">
502
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.868417">
<title confidence="0.999212">For a few dollars less: Identifying review pages sans human labels</title>
<author confidence="0.998178">Luciano</author>
<affiliation confidence="0.9997135">Dept. of Computer University of</affiliation>
<address confidence="0.992126">Salt Lake City, UT 84112, USA.</address>
<email confidence="0.999904">lbarbosa@cs.utah.edu</email>
<author confidence="0.998387">Ravi Kumar Bo Pang Andrew</author>
<affiliation confidence="0.904755">Yahoo!</affiliation>
<address confidence="0.9921955">701 First Sunnyvale, CA 94089, USA.</address>
<abstract confidence="0.997980636363636">We address the problem of large-scale autodetection of online reviews using any human labels. We propose an efficient method that combines two basic ideas: Building a classifier from a large number of noisy examples and using the structure of the website to enhance the performance of this classifier. Experiments suggest that our method is competitive against supervised learning methods that mandate expensive human effort.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dana Angluin</author>
<author>Philip D Laird</author>
</authors>
<title>Learning from noisy examples.</title>
<date>1988</date>
<booktitle>Machine Learning,</booktitle>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="7758" citStr="Angluin and Laird, 1988" startWordPosition="1220" endWordPosition="1223">s work in sentiment analysis that worked with reviews, but they were typically restricted to using reviews extracted from one or two well-known sources, bypassing automatic review detection. Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005). See Pang and Lee (2008a) for a more comprehensive survey. Building a collection of diverse review webpages, not limited to one or two hosts, can better facilitate such research. Learning from noisy examples has been studied for a long time in the learning theory community (Angluin and Laird, 1988). Learning naive Bayes classifiers from noisy data (either features or labels or both) was studied by Yang et al. (2003). Their focus, however, is to reconstruct the underlying conditional probability distributions from the observed noisy dataset. We, on the other hand, rely on the volume of labels to drown the noise. Along this spirit, Snow et al. (2008) show that obtaining multiple lowquality labels (through Mechanical Turk) can approach high-quality editorial labels. Unlike in their setting, we do not have multiple low-quality labels for the same URL. The extensive body of work in 495 semi-</context>
</contexts>
<marker>Angluin, Laird, 1988</marker>
<rawString>Dana Angluin and Philip D. Laird. 1988. Learning from noisy examples. Machine Learning, 2(4):343–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of 45th ACL.</booktitle>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of 45th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of 11th COLT,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="8976" citStr="Blum and Mitchell (1998)" startWordPosition="1418" endWordPosition="1421"> semi-supervised learning or learning from one class is also somewhat relevant to our work. A major difference is that they tend to work with small amount of clean, labeled data. In addition, many semisupervised/transductive learning algorithms are not efficient for web-scale data. Using site structure for web analysis tasks has been addressed in a variety of contexts. For example, Kening et al. (2005) exploit the structure of a website to improve classification. On a related note, co-training has also been used to utilize inter-page link information in addition to intra-page textual content: Blum and Mitchell (1998) use anchor texts pointing to a webpage as the alternative “view” of the page in the context of webpage classification. Their algorithm is largely site-unaware in that it does not explicitly exploit site structures. Utilizing site structures also has remote connections to wrapper induction, and there is extensive literature on this topic. Unfortunately, the methods in all of these work require human labeling, which is precisely what our work is trying to circumvent. 3 Methodology In this section we describe our basic methodology for identifying review pages. Our method consists of two main ste</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of 11th COLT, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>comScore</author>
</authors>
<title>Online consumer-generated reviews have significant impact on offline purchase behavior. Press Release,</title>
<date>2007</date>
<note>http://www.comscore.com/press/ release.asp?press=1928.</note>
<contexts>
<context position="969" citStr="comScore (2007)" startWordPosition="147" endWordPosition="148">cale automatic detection of online reviews without using any human labels. We propose an efficient method that combines two basic ideas: Building a classifier from a large number of noisy examples and using the structure of the website to enhance the performance of this classifier. Experiments suggest that our method is competitive against supervised learning methods that mandate expensive human effort. 1 Introduction Shoppers are migrating to the web and online reviews are playing a critical role in affecting their shopping decisions, online and offline. According to two surveys published by comScore (2007) and Horrigan (2008), 81% of web users have done online research on a product at least once. Among readers of online reviews, more than 70% reported that the reviews had a significant influence on their purchases. Realizing this economic potential, search engines have been scrambling to cater to such user needs in innovative ways. For example, in response to a product-related query, a search engine might want to surface only review pages, perhaps via a “filter by” option, to the user. More ambitiously, they might want to dissect the reviews, segregate them into novice and expert judgments, dis</context>
</contexts>
<marker>comScore, 2007</marker>
<rawString>comScore. 2007. Online consumer-generated reviews have significant impact on offline purchase behavior. Press Release, November. http://www.comscore.com/press/ release.asp?press=1928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of 12th WWW,</booktitle>
<pages>519--528</pages>
<contexts>
<context position="7412" citStr="Dave et al., 2003" startWordPosition="1163" endWordPosition="1166">ay of detecting reviews without (additional) human annotations. In particular, the OpinionFinder system (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) is a state-ofthe-art knowledge-rich sentiment-analysis system. We will use it as one of our baselines and compare its performance with our methods. There has been a great deal of previous work in sentiment analysis that worked with reviews, but they were typically restricted to using reviews extracted from one or two well-known sources, bypassing automatic review detection. Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005). See Pang and Lee (2008a) for a more comprehensive survey. Building a collection of diverse review webpages, not limited to one or two hosts, can better facilitate such research. Learning from noisy examples has been studied for a long time in the learning theory community (Angluin and Laird, 1988). Learning naive Bayes classifiers from noisy data (either features or labels or both) was studied by Yang et al. (2003). Their focus, however, is to reconstruct the underlying conditional probability distributions from the observed noisy dataset. We, on</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of 12th WWW, pages 519–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aidan Finn</author>
<author>Nicholas Kushmerick</author>
</authors>
<title>Learning to classify documents according to genre.</title>
<date>2006</date>
<journal>JASIST,</journal>
<volume>7</volume>
<issue>5</issue>
<contexts>
<context position="5662" citStr="Finn and Kushmerick, 2006" startWordPosition="877" endWordPosition="881">rvised text classification that relies on editorial labels. 2 Related work The related work falls into roughly four categories: Document- and sentence-level subjectivity detection, sentiment analysis in the context of reviews, learning from noisy labeled examples, and exploiting site structure for classification. Given the subjective nature of reviews, documentlevel subjectivity classification is closely related to our work. There have been a number of approaches proposed to address document-level subjectivity in news articles, weblogs, etc. (Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Finn and Kushmerick, 2006; Ni et al., 2007; Stepinski and Mittal, 2007). Ng et al. (2006) experiment with review identification for known domains using datasets with clean labels (e.g., movie reviews vs. movie-related non-reviews), a setting different from that of ours. Pang and Lee (2008b) present a method on reranking documents that are web search results for a specific query (containing the word review) based on the subjective/objective distinction. Given the nature of the query, they implicitly detect reviews from unknown sources. But their re-ranking algorithm only applies to webpages known to be (roughly) relate</context>
</contexts>
<marker>Finn, Kushmerick, 2006</marker>
<rawString>Aidan Finn and Nicholas Kushmerick. 2006. Learning to classify documents according to genre. JASIST, 7(5):1506–1518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Horrigan</author>
</authors>
<title>Online shopping.</title>
<date>2008</date>
<journal>Pew Internet &amp; American Life Project Report.</journal>
<contexts>
<context position="989" citStr="Horrigan (2008)" startWordPosition="150" endWordPosition="151">tion of online reviews without using any human labels. We propose an efficient method that combines two basic ideas: Building a classifier from a large number of noisy examples and using the structure of the website to enhance the performance of this classifier. Experiments suggest that our method is competitive against supervised learning methods that mandate expensive human effort. 1 Introduction Shoppers are migrating to the web and online reviews are playing a critical role in affecting their shopping decisions, online and offline. According to two surveys published by comScore (2007) and Horrigan (2008), 81% of web users have done online research on a product at least once. Among readers of online reviews, more than 70% reported that the reviews had a significant influence on their purchases. Realizing this economic potential, search engines have been scrambling to cater to such user needs in innovative ways. For example, in response to a product-related query, a search engine might want to surface only review pages, perhaps via a “filter by” option, to the user. More ambitiously, they might want to dissect the reviews, segregate them into novice and expert judgments, distill sentiments, and</context>
</contexts>
<marker>Horrigan, 2008</marker>
<rawString>John A. Horrigan. 2008. Online shopping. Pew Internet &amp; American Life Project Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of 19th AAAI,</booktitle>
<pages>755--760</pages>
<contexts>
<context position="7430" citStr="Hu and Liu, 2004" startWordPosition="1167" endWordPosition="1170">iews without (additional) human annotations. In particular, the OpinionFinder system (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) is a state-ofthe-art knowledge-rich sentiment-analysis system. We will use it as one of our baselines and compare its performance with our methods. There has been a great deal of previous work in sentiment analysis that worked with reviews, but they were typically restricted to using reviews extracted from one or two well-known sources, bypassing automatic review detection. Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005). See Pang and Lee (2008a) for a more comprehensive survey. Building a collection of diverse review webpages, not limited to one or two hosts, can better facilitate such research. Learning from noisy examples has been studied for a long time in the learning theory community (Angluin and Laird, 1988). Learning naive Bayes classifiers from noisy data (either features or labels or both) was studied by Yang et al. (2003). Their focus, however, is to reconstruct the underlying conditional probability distributions from the observed noisy dataset. We, on the other hand, r</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining opinion features in customer reviews. In Proceedings of 19th AAAI, pages 755–760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gao Kening</author>
<author>Yang Leiming</author>
<author>Zhang Bin</author>
<author>Chai Qiaozi</author>
<author>Ma Anxiang</author>
</authors>
<title>Automatic classification of web information based on site structure. In Cyberworlds,</title>
<date>2005</date>
<pages>552--558</pages>
<contexts>
<context position="8757" citStr="Kening et al. (2005)" startWordPosition="1384" endWordPosition="1387">ultiple lowquality labels (through Mechanical Turk) can approach high-quality editorial labels. Unlike in their setting, we do not have multiple low-quality labels for the same URL. The extensive body of work in 495 semi-supervised learning or learning from one class is also somewhat relevant to our work. A major difference is that they tend to work with small amount of clean, labeled data. In addition, many semisupervised/transductive learning algorithms are not efficient for web-scale data. Using site structure for web analysis tasks has been addressed in a variety of contexts. For example, Kening et al. (2005) exploit the structure of a website to improve classification. On a related note, co-training has also been used to utilize inter-page link information in addition to intra-page textual content: Blum and Mitchell (1998) use anchor texts pointing to a webpage as the alternative “view” of the page in the context of webpage classification. Their algorithm is largely site-unaware in that it does not explicitly exploit site structures. Utilizing site structures also has remote connections to wrapper induction, and there is extensive literature on this topic. Unfortunately, the methods in all of the</context>
</contexts>
<marker>Kening, Leiming, Bin, Qiaozi, Anxiang, 2005</marker>
<rawString>Gao Kening, Yang Leiming, Zhang Bin, Chai Qiaozi, and Ma Anxiang. 2005. Automatic classification of web information based on site structure. In Cyberworlds, pages 552–558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Sajib Dasgupta</author>
<author>S M Niaz Arifin</author>
</authors>
<title>Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of 21st COLING/44th ACL Poster,</booktitle>
<pages>611--618</pages>
<contexts>
<context position="5726" citStr="Ng et al. (2006)" startWordPosition="891" endWordPosition="894">k The related work falls into roughly four categories: Document- and sentence-level subjectivity detection, sentiment analysis in the context of reviews, learning from noisy labeled examples, and exploiting site structure for classification. Given the subjective nature of reviews, documentlevel subjectivity classification is closely related to our work. There have been a number of approaches proposed to address document-level subjectivity in news articles, weblogs, etc. (Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Finn and Kushmerick, 2006; Ni et al., 2007; Stepinski and Mittal, 2007). Ng et al. (2006) experiment with review identification for known domains using datasets with clean labels (e.g., movie reviews vs. movie-related non-reviews), a setting different from that of ours. Pang and Lee (2008b) present a method on reranking documents that are web search results for a specific query (containing the word review) based on the subjective/objective distinction. Given the nature of the query, they implicitly detect reviews from unknown sources. But their re-ranking algorithm only applies to webpages known to be (roughly) related to the same narrow subject. Since the webpages in our datasets</context>
</contexts>
<marker>Ng, Dasgupta, Arifin, 2006</marker>
<rawString>Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006. Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews. In Proceedings of 21st COLING/44th ACL Poster, pages 611–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaochuan Ni</author>
<author>Gui-Rong Xue</author>
<author>Xiao Ling</author>
<author>Yong Yu</author>
<author>Qiang Yang</author>
</authors>
<title>Exploring in the weblog space by detecting informative and affective articles.</title>
<date>2007</date>
<booktitle>In Proceedings of 16th WWW,</booktitle>
<pages>281--290</pages>
<contexts>
<context position="5679" citStr="Ni et al., 2007" startWordPosition="882" endWordPosition="885">that relies on editorial labels. 2 Related work The related work falls into roughly four categories: Document- and sentence-level subjectivity detection, sentiment analysis in the context of reviews, learning from noisy labeled examples, and exploiting site structure for classification. Given the subjective nature of reviews, documentlevel subjectivity classification is closely related to our work. There have been a number of approaches proposed to address document-level subjectivity in news articles, weblogs, etc. (Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Finn and Kushmerick, 2006; Ni et al., 2007; Stepinski and Mittal, 2007). Ng et al. (2006) experiment with review identification for known domains using datasets with clean labels (e.g., movie reviews vs. movie-related non-reviews), a setting different from that of ours. Pang and Lee (2008b) present a method on reranking documents that are web search results for a specific query (containing the word review) based on the subjective/objective distinction. Given the nature of the query, they implicitly detect reviews from unknown sources. But their re-ranking algorithm only applies to webpages known to be (roughly) related to the same nar</context>
</contexts>
<marker>Ni, Xue, Ling, Yu, Yang, 2007</marker>
<rawString>Xiaochuan Ni, Gui-Rong Xue, Xiao Ling, Yong Yu, and Qiang Yang. 2007. Exploring in the weblog space by detecting informative and affective articles. In Proceedings of 16th WWW, pages 281–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="5926" citStr="Pang and Lee (2008" startWordPosition="920" endWordPosition="923">ploiting site structure for classification. Given the subjective nature of reviews, documentlevel subjectivity classification is closely related to our work. There have been a number of approaches proposed to address document-level subjectivity in news articles, weblogs, etc. (Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Finn and Kushmerick, 2006; Ni et al., 2007; Stepinski and Mittal, 2007). Ng et al. (2006) experiment with review identification for known domains using datasets with clean labels (e.g., movie reviews vs. movie-related non-reviews), a setting different from that of ours. Pang and Lee (2008b) present a method on reranking documents that are web search results for a specific query (containing the word review) based on the subjective/objective distinction. Given the nature of the query, they implicitly detect reviews from unknown sources. But their re-ranking algorithm only applies to webpages known to be (roughly) related to the same narrow subject. Since the webpages in our datasets cover not only a diverse range of websites but also a diverse range of topics, their approach does not apply. To the best of our knowledge, there has been no work on identifying review pages at the s</context>
<context position="7482" citStr="Pang and Lee (2008" startWordPosition="1176" endWordPosition="1179">rticular, the OpinionFinder system (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) is a state-ofthe-art knowledge-rich sentiment-analysis system. We will use it as one of our baselines and compare its performance with our methods. There has been a great deal of previous work in sentiment analysis that worked with reviews, but they were typically restricted to using reviews extracted from one or two well-known sources, bypassing automatic review detection. Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005). See Pang and Lee (2008a) for a more comprehensive survey. Building a collection of diverse review webpages, not limited to one or two hosts, can better facilitate such research. Learning from noisy examples has been studied for a long time in the learning theory community (Angluin and Laird, 1988). Learning naive Bayes classifiers from noisy data (either features or labels or both) was studied by Yang et al. (2003). Their focus, however, is to reconstruct the underlying conditional probability distributions from the observed noisy dataset. We, on the other hand, rely on the volume of labels to drown the noise. Alon</context>
<context position="27259" citStr="Pang and Lee, 2008" startWordPosition="4588" endWordPosition="4591">ionFinder uses two subjective sentence classifiers (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005). The first (denoted opfA) focuses on yielding the highest accuracy; the second (denoted opfB) optimizes precision at the expense of recall. The methods underlying OpinionFinder incorporate extensive tools from linguistics (including, speech activity verbs, psychological verbs, FrameNet verbs and adjectives with frame “experiencer”, among others) and machine learning. In terms of performance, previous work has shown that OpinionFinder is a challenging system to improve upon for review retrieval (Pang and Lee, 2008b). Computationally, OpinionFinder is very expensive and hence unattractive for large-scale webpage analysis (running OpinionFinder on 1515 pages took about five hours). Therefore, we also propose a light-weight subjectivity detection mechanism called lwd, which counts the number of opinion words in each sentence in the text. The opinion words (5403 of them) were obtained from an existing subjectivity lexicon (http: //www.cs.pitt.edu/mpqa). We ran both opfA and opfB on the tokenized text (running them on raw HTML produced worse results). Each sentence in the text was labeled subjective or obje</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008a. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Using very simple statistics for review search: An exploration.</title>
<date>2008</date>
<booktitle>In Proceedings of 22nd COLING.</booktitle>
<publisher>Poster.</publisher>
<contexts>
<context position="5926" citStr="Pang and Lee (2008" startWordPosition="920" endWordPosition="923">ploiting site structure for classification. Given the subjective nature of reviews, documentlevel subjectivity classification is closely related to our work. There have been a number of approaches proposed to address document-level subjectivity in news articles, weblogs, etc. (Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Finn and Kushmerick, 2006; Ni et al., 2007; Stepinski and Mittal, 2007). Ng et al. (2006) experiment with review identification for known domains using datasets with clean labels (e.g., movie reviews vs. movie-related non-reviews), a setting different from that of ours. Pang and Lee (2008b) present a method on reranking documents that are web search results for a specific query (containing the word review) based on the subjective/objective distinction. Given the nature of the query, they implicitly detect reviews from unknown sources. But their re-ranking algorithm only applies to webpages known to be (roughly) related to the same narrow subject. Since the webpages in our datasets cover not only a diverse range of websites but also a diverse range of topics, their approach does not apply. To the best of our knowledge, there has been no work on identifying review pages at the s</context>
<context position="7482" citStr="Pang and Lee (2008" startWordPosition="1176" endWordPosition="1179">rticular, the OpinionFinder system (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) is a state-ofthe-art knowledge-rich sentiment-analysis system. We will use it as one of our baselines and compare its performance with our methods. There has been a great deal of previous work in sentiment analysis that worked with reviews, but they were typically restricted to using reviews extracted from one or two well-known sources, bypassing automatic review detection. Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005). See Pang and Lee (2008a) for a more comprehensive survey. Building a collection of diverse review webpages, not limited to one or two hosts, can better facilitate such research. Learning from noisy examples has been studied for a long time in the learning theory community (Angluin and Laird, 1988). Learning naive Bayes classifiers from noisy data (either features or labels or both) was studied by Yang et al. (2003). Their focus, however, is to reconstruct the underlying conditional probability distributions from the observed noisy dataset. We, on the other hand, rely on the volume of labels to drown the noise. Alon</context>
<context position="27259" citStr="Pang and Lee, 2008" startWordPosition="4588" endWordPosition="4591">ionFinder uses two subjective sentence classifiers (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005). The first (denoted opfA) focuses on yielding the highest accuracy; the second (denoted opfB) optimizes precision at the expense of recall. The methods underlying OpinionFinder incorporate extensive tools from linguistics (including, speech activity verbs, psychological verbs, FrameNet verbs and adjectives with frame “experiencer”, among others) and machine learning. In terms of performance, previous work has shown that OpinionFinder is a challenging system to improve upon for review retrieval (Pang and Lee, 2008b). Computationally, OpinionFinder is very expensive and hence unattractive for large-scale webpage analysis (running OpinionFinder on 1515 pages took about five hours). Therefore, we also propose a light-weight subjectivity detection mechanism called lwd, which counts the number of opinion words in each sentence in the text. The opinion words (5403 of them) were obtained from an existing subjectivity lexicon (http: //www.cs.pitt.edu/mpqa). We ran both opfA and opfB on the tokenized text (running them on raw HTML produced worse results). Each sentence in the text was labeled subjective or obje</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008b. Using very simple statistics for review search: An exploration. In Proceedings of 22nd COLING. Poster.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="7393" citStr="Pang et al., 2002" startWordPosition="1159" endWordPosition="1162">is an alternative way of detecting reviews without (additional) human annotations. In particular, the OpinionFinder system (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) is a state-ofthe-art knowledge-rich sentiment-analysis system. We will use it as one of our baselines and compare its performance with our methods. There has been a great deal of previous work in sentiment analysis that worked with reviews, but they were typically restricted to using reviews extracted from one or two well-known sources, bypassing automatic review detection. Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005). See Pang and Lee (2008a) for a more comprehensive survey. Building a collection of diverse review webpages, not limited to one or two hosts, can better facilitate such research. Learning from noisy examples has been studied for a long time in the learning theory community (Angluin and Laird, 1988). Learning naive Bayes classifiers from noisy data (either features or labels or both) was studied by Yang et al. (2003). Their focus, however, is to reconstruct the underlying conditional probability distributions from the observed no</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>339--346</pages>
<contexts>
<context position="7458" citStr="Popescu and Etzioni, 2005" startWordPosition="1171" endWordPosition="1174">tional) human annotations. In particular, the OpinionFinder system (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) is a state-ofthe-art knowledge-rich sentiment-analysis system. We will use it as one of our baselines and compare its performance with our methods. There has been a great deal of previous work in sentiment analysis that worked with reviews, but they were typically restricted to using reviews extracted from one or two well-known sources, bypassing automatic review detection. Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005). See Pang and Lee (2008a) for a more comprehensive survey. Building a collection of diverse review webpages, not limited to one or two hosts, can better facilitate such research. Learning from noisy examples has been studied for a long time in the learning theory community (Angluin and Laird, 1988). Learning naive Bayes classifiers from noisy data (either features or labels or both) was studied by Yang et al. (2003). Their focus, however, is to reconstruct the underlying conditional probability distributions from the observed noisy dataset. We, on the other hand, rely on the volume of labels </context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of HLT/EMNLP, pages 339–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="6922" citStr="Riloff and Wiebe, 2003" startWordPosition="1083" endWordPosition="1086">e webpages in our datasets cover not only a diverse range of websites but also a diverse range of topics, their approach does not apply. To the best of our knowledge, there has been no work on identifying review pages at the scale and diversity we consider. Subjectivity classification of within-document items, such as terms, has been an active line of research (Wiebe et al. (2004) present a survey). Identifying subjective sentences in a document via offthe-shelf packages is an alternative way of detecting reviews without (additional) human annotations. In particular, the OpinionFinder system (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) is a state-ofthe-art knowledge-rich sentiment-analysis system. We will use it as one of our baselines and compare its performance with our methods. There has been a great deal of previous work in sentiment analysis that worked with reviews, but they were typically restricted to using reviews extracted from one or two well-known sources, bypassing automatic review detection. Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005). See Pang and Lee (2008a) for a more comprehensive survey. Buil</context>
<context position="26715" citStr="Riloff and Wiebe, 2003" startWordPosition="4510" endWordPosition="4513">Trivial baseline performances. 499 ing the main results of our methods, we introduce a much stronger baseline that utilizes a knowledgerich subjectivity detection package. 5.1 Using subjectivity detectors This baseline is motivated by the fact that reviews often contain extensive subjective content. There are many existing techniques that detect subjectivity in text. OpinionFinder (http://www.cs.pitt. edu/mpqa/opinionfinderrelease/) is a well-known system that processes documents and automatically identifies subjective sentences in them. OpinionFinder uses two subjective sentence classifiers (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005). The first (denoted opfA) focuses on yielding the highest accuracy; the second (denoted opfB) optimizes precision at the expense of recall. The methods underlying OpinionFinder incorporate extensive tools from linguistics (including, speech activity verbs, psychological verbs, FrameNet verbs and adjectives with frame “experiencer”, among others) and machine learning. In terms of performance, previous work has shown that OpinionFinder is a challenging system to improve upon for review retrieval (Pang and Lee, 2008b). Computationally, OpinionFinder is very expensive and</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceedings of EMNLP, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Stepinski</author>
<author>Vibhu Mittal</author>
</authors>
<title>A fact/opinion classifier for news articles.</title>
<date>2007</date>
<booktitle>In Proceedings of 30th SIGIR,</booktitle>
<pages>807--808</pages>
<contexts>
<context position="5708" citStr="Stepinski and Mittal, 2007" startWordPosition="886" endWordPosition="890">itorial labels. 2 Related work The related work falls into roughly four categories: Document- and sentence-level subjectivity detection, sentiment analysis in the context of reviews, learning from noisy labeled examples, and exploiting site structure for classification. Given the subjective nature of reviews, documentlevel subjectivity classification is closely related to our work. There have been a number of approaches proposed to address document-level subjectivity in news articles, weblogs, etc. (Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Finn and Kushmerick, 2006; Ni et al., 2007; Stepinski and Mittal, 2007). Ng et al. (2006) experiment with review identification for known domains using datasets with clean labels (e.g., movie reviews vs. movie-related non-reviews), a setting different from that of ours. Pang and Lee (2008b) present a method on reranking documents that are web search results for a specific query (containing the word review) based on the subjective/objective distinction. Given the nature of the query, they implicitly detect reviews from unknown sources. But their re-ranking algorithm only applies to webpages known to be (roughly) related to the same narrow subject. Since the webpag</context>
</contexts>
<marker>Stepinski, Mittal, 2007</marker>
<rawString>Adam Stepinski and Vibhu Mittal. 2007. A fact/opinion classifier for news articles. In Proceedings of 30th SIGIR, pages 807–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="7374" citStr="Turney, 2002" startWordPosition="1157" endWordPosition="1158">helf packages is an alternative way of detecting reviews without (additional) human annotations. In particular, the OpinionFinder system (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) is a state-ofthe-art knowledge-rich sentiment-analysis system. We will use it as one of our baselines and compare its performance with our methods. There has been a great deal of previous work in sentiment analysis that worked with reviews, but they were typically restricted to using reviews extracted from one or two well-known sources, bypassing automatic review detection. Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005). See Pang and Lee (2008a) for a more comprehensive survey. Building a collection of diverse review webpages, not limited to one or two hosts, can better facilitate such research. Learning from noisy examples has been studied for a long time in the learning theory community (Angluin and Laird, 1988). Learning naive Bayes classifiers from noisy data (either features or labels or both) was studied by Yang et al. (2003). Their focus, however, is to reconstruct the underlying conditional probability distributions f</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of 40th ACL, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
<author>Ellen Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing,</booktitle>
<pages>486--497</pages>
<contexts>
<context position="6947" citStr="Wiebe and Riloff, 2005" startWordPosition="1087" endWordPosition="1090">ts cover not only a diverse range of websites but also a diverse range of topics, their approach does not apply. To the best of our knowledge, there has been no work on identifying review pages at the scale and diversity we consider. Subjectivity classification of within-document items, such as terms, has been an active line of research (Wiebe et al. (2004) present a survey). Identifying subjective sentences in a document via offthe-shelf packages is an alternative way of detecting reviews without (additional) human annotations. In particular, the OpinionFinder system (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) is a state-ofthe-art knowledge-rich sentiment-analysis system. We will use it as one of our baselines and compare its performance with our methods. There has been a great deal of previous work in sentiment analysis that worked with reviews, but they were typically restricted to using reviews extracted from one or two well-known sources, bypassing automatic review detection. Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005). See Pang and Lee (2008a) for a more comprehensive survey. Building a collection of dive</context>
<context position="26740" citStr="Wiebe and Riloff, 2005" startWordPosition="4514" endWordPosition="4517">ances. 499 ing the main results of our methods, we introduce a much stronger baseline that utilizes a knowledgerich subjectivity detection package. 5.1 Using subjectivity detectors This baseline is motivated by the fact that reviews often contain extensive subjective content. There are many existing techniques that detect subjectivity in text. OpinionFinder (http://www.cs.pitt. edu/mpqa/opinionfinderrelease/) is a well-known system that processes documents and automatically identifies subjective sentences in them. OpinionFinder uses two subjective sentence classifiers (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005). The first (denoted opfA) focuses on yielding the highest accuracy; the second (denoted opfB) optimizes precision at the expense of recall. The methods underlying OpinionFinder incorporate extensive tools from linguistics (including, speech activity verbs, psychological verbs, FrameNet verbs and adjectives with frame “experiencer”, among others) and machine learning. In terms of performance, previous work has shown that OpinionFinder is a challenging system to improve upon for review retrieval (Pang and Lee, 2008b). Computationally, OpinionFinder is very expensive and hence unattractive for l</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Janyce M. Wiebe and Ellen Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Proceedings of CICLing, pages 486– 497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="5635" citStr="Wiebe et al., 2004" startWordPosition="873" endWordPosition="876">tate-of-the-art supervised text classification that relies on editorial labels. 2 Related work The related work falls into roughly four categories: Document- and sentence-level subjectivity detection, sentiment analysis in the context of reviews, learning from noisy labeled examples, and exploiting site structure for classification. Given the subjective nature of reviews, documentlevel subjectivity classification is closely related to our work. There have been a number of approaches proposed to address document-level subjectivity in news articles, weblogs, etc. (Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Finn and Kushmerick, 2006; Ni et al., 2007; Stepinski and Mittal, 2007). Ng et al. (2006) experiment with review identification for known domains using datasets with clean labels (e.g., movie reviews vs. movie-related non-reviews), a setting different from that of ours. Pang and Lee (2008b) present a method on reranking documents that are web search results for a specific query (containing the word review) based on the subjective/objective distinction. Given the nature of the query, they implicitly detect reviews from unknown sources. But their re-ranking algorithm only applies to webpages k</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce M. Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Computational Linguistics, 30(3):277–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yirong Yang</author>
<author>Yi Xia</author>
<author>Yun Chi</author>
<author>Richard R Muntz</author>
</authors>
<title>Learning naive Bayes classifier from noisy data.</title>
<date>2003</date>
<tech>Technical Report 56, UCLA.</tech>
<contexts>
<context position="7878" citStr="Yang et al. (2003)" startWordPosition="1240" endWordPosition="1243">or two well-known sources, bypassing automatic review detection. Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005). See Pang and Lee (2008a) for a more comprehensive survey. Building a collection of diverse review webpages, not limited to one or two hosts, can better facilitate such research. Learning from noisy examples has been studied for a long time in the learning theory community (Angluin and Laird, 1988). Learning naive Bayes classifiers from noisy data (either features or labels or both) was studied by Yang et al. (2003). Their focus, however, is to reconstruct the underlying conditional probability distributions from the observed noisy dataset. We, on the other hand, rely on the volume of labels to drown the noise. Along this spirit, Snow et al. (2008) show that obtaining multiple lowquality labels (through Mechanical Turk) can approach high-quality editorial labels. Unlike in their setting, we do not have multiple low-quality labels for the same URL. The extensive body of work in 495 semi-supervised learning or learning from one class is also somewhat relevant to our work. A major difference is that they te</context>
</contexts>
<marker>Yang, Xia, Chi, Muntz, 2003</marker>
<rawString>Yirong Yang, Yi Xia, Yun Chi, and Richard R. Muntz. 2003. Learning naive Bayes classifier from noisy data. Technical Report 56, UCLA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="5615" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="868" endWordPosition="872">mains competitive against the state-of-the-art supervised text classification that relies on editorial labels. 2 Related work The related work falls into roughly four categories: Document- and sentence-level subjectivity detection, sentiment analysis in the context of reviews, learning from noisy labeled examples, and exploiting site structure for classification. Given the subjective nature of reviews, documentlevel subjectivity classification is closely related to our work. There have been a number of approaches proposed to address document-level subjectivity in news articles, weblogs, etc. (Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Finn and Kushmerick, 2006; Ni et al., 2007; Stepinski and Mittal, 2007). Ng et al. (2006) experiment with review identification for known domains using datasets with clean labels (e.g., movie reviews vs. movie-related non-reviews), a setting different from that of ours. Pang and Lee (2008b) present a method on reranking documents that are web search results for a specific query (containing the word review) based on the subjective/objective distinction. Given the nature of the query, they implicitly detect reviews from unknown sources. But their re-ranking algorithm only a</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of EMNLP, pages 129–136.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>