<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002816">
<title confidence="0.9987555">
Issues in the Choice of a Source for
Natural Language Generation
</title>
<author confidence="0.988211">
David D. McDonald*
</author>
<subsectionHeader confidence="0.444788">
Brandeis University
</subsectionHeader>
<bodyText confidence="0.999768756756757">
The most vexing question in natural language generation is &apos;what is the source&apos;—
what do speakers start from when they begin to compose an utterance? Theories of
generation in the literature differ markedly in their assumptions. A few start with an
unanalyzed body of numerical data (e.g. Bourbeau et al. 1990; Kukich 1988). Most start
with the structured objects that are used by a particular reasoning system or simulator
and are cast in that system&apos;s representational formalism (e.g. Hovy 1990; Meteer 1992;
Wisner 1988). A growing number of systems, largely focused on problems in machine
translation or grammatical theory, take their input to be logical formulae based on
lexical predicates (e.g. Wedekind 1988; Shieber et al. 1990).
The lack of a consistent answer to the question of the generator&apos;s source has been
at the heart of the problem of how to make research on generation intelligible and
engaging for the rest of the computational linguistics community, and has complicated
efforts to evaluate alternative treatments even for people in the field. Nevertheless, a
source cannot be imposed by fiat. Differences in what information is assumed to be
available, its relative decomposition when compared to the &amp;quot;packaging&amp;quot; available in
the words or syntactic constructions of the language (linguistic resources), what amount
and kinds of information are contained in the atomic units of the source, and what
sorts of compositions and other larger scale organizations are possible—all these have
an impact on what architectures are plausible for generation and what efficiencies
they can achieve. Advances in the field often come precisely through insights into the
representation of the source.
Language comprehension research does not have this problem—its source is a text.
Differences in methodology govern where this text comes from (e.g., single sentence
vs. discourse, sample sentences vs. corpus study, written vs. spoken), but these aside
there is no question of what the comprehension process starts with.
Where comprehension &amp;quot;ends&amp;quot; is quite another matter. If we go back to some of the
early comprehension systems, the end point of the process was an action, and there
was linguistic processing at every stage (Winograd 1972). Some researchers, this author
included, take the end point to be an elaboration of an already existing semantic model
whereby some new individuals are added and new relations established between
them and other individuals (Martin and Riesbeck 1986; McDonald 1992a). Today&apos;s
dominant paradigm, however, stemming perhaps from the predominance of research
on question-answering and following the lead of theoretical linguistics, is to take the
end point to be a logical form: an expression that codifies the information in the text
at a fairly shallow level, e.g., a first-order formula with content words mapped to
predicates with the same spelling, and with individuals represented by quantified
variables or constants.
</bodyText>
<note confidence="0.746524333333333">
* 14 Brantwood Road, Arlington, MA 02174-8004; mcdonald@cs.brandeis.edu
C) 1993 Association for Computational Linguistics
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.999979294117647">
It is somewhat puzzling that this question of where the comprehension process
ends has apparently never been debated in the literature. Instead it seems largely
taken for granted that the parsing process ends with the assembly of an expression
in a suitable logic that captures the text&apos;s information content, perhaps with some
functional annotations, and that a &amp;quot;reasoning&amp;quot; process then starts with that expression
and draws inferences in order to resolve anaphors and establish the speaker&apos;s intent.
Problems arise when researchers project this default decomposition onto the pro-
cess of producing language. All too often the process is divided into a &amp;quot;reasoning&amp;quot;
and a &amp;quot;generation&amp;quot; component (see, e.g., Shieber, this issue)—an unfortunate choice
of terminology because it reduces the scope of &amp;quot;generation&amp;quot; to triviality as we shall
see. The primary motivation for the division is the desire for a bi-directional natural
language processing system—one where the representation of the linguistic resources
is reversible for use in both the comprehension and production of utterances. But while
a reversible representation is indeed a proper goal for today&apos;s systems, the choice of
logical form as the &amp;quot;pivot point&amp;quot; is problematic, especially a first-order formula.
A truly reversible linguistic mapping between intentional situation and utterance
will have the comprehension process end where the generation process begins. Thus
just as the psycholinguistically correct source for generation is still very much a matter
of research (as it is even when the source is a computational object in a well-designed
AT system), so too is the end-point of comprehension, and by implication the division
of that process into components and representational levels. A declarative, reversible,
form–meaning mapping does not ipso facto have to start/end at the level of logical
form, but can originate at a much deeper level with the class definitions of the object
types and relations of the speaker&apos;s conceptual model (McDonald 1993).
Considered in isolation, the production of text from a logical form is, quite frankly,
trivial. It corresponds to the final &amp;quot;readout&amp;quot; phase of McDonald, Meteer, and Puste-
jovsky (1987), since all that remains to be done is to linearize its elements in keeping
with the constraints of a surface grammar, carry out the trivial mapping to the pho-
netic (orthographic) forms of the words implicit in the predicates, and add the requisite
grammatical function words and morphemes. This capability has been an established
part of the state of the art for well over twenty years (see, e.g., Webber [1971], which
is also the first work on reversible grammars for generation this author is aware of).
Over the years new architectures for this &amp;quot;tactical&amp;quot; part of generation (we prefer the
term surface realization) are introduced only because of new ideas in grammatical the-
ory or in response to shifts in what is given in the immediately prior representational
level.
In current research that focuses just on surface realization, all of the substantial
tasks of generation are invariably subordinated to the &amp;quot;reasoner&amp;quot; or &amp;quot;strategic com-
ponent,&amp;quot; which is treated as a black box whose operations are seldom discussed.
Examples of these tasks include construing the speaker&apos;s situation in realizable terms
given the available vocabulary and syntactic resources (an especially important task
when the source is raw data, e.g., precisely what points of the compass make the wind
&amp;quot;easterly,&amp;quot; [Bourbeau et al. 1990]); selecting the information to include in the utterance
and deciding whether it should be stated explicitly or left for inference; distributing
the information into sentences and giving it an organization that reflects the intended
rhetorical force, coherence, and necessary cohesion given the prior discourse; and find-
ing a mapping of the information to linguistic resources that is collectively expressible
(i.e., has a surface realization; see Meteer 1992). How one chooses to approach these
tasks has substantial implications for the kinds of structures that a surface realization
process can sensibly be given as input and may not be taken for granted. As a conse-
quence, any generation architecture that is proposed without including an articulation
</bodyText>
<page confidence="0.639029">
192
David D. McDonald Source for Natural Language Generation
</page>
<bodyText confidence="0.99995788">
of the early stages of the process is issuing a large promissory note that it may not be
able to redeem.
Often the choice of a two-component process in comprehension (and by extension
in generation) is based on the judgment that linguistic knowledge can and should
be restricted to its own component, the one responsible for the form and content
of grammatical rules, leaving to the other component all matters of general reason-
ing (&amp;quot;reasoners should not have to truck with grammatical issues&amp;quot;). This assumption
has been seriously questioned within the generation community in recent years. The
constraints imposed by the linguistic resources&apos; limitations in what they are able to
express and the delicacy of the conceptual and rhetorical choices state-of-the-art gen-
erators are called on to make combine to force a strong interdependency between early
and late aspects of the process to the point where many generation researchers today
do not recognize any strong division into components, with different aspects of lin-
guistic knowledge appearing at many levels of representation (see Hovy, McDonald,
and Young 1989).
All judgments about &amp;quot;components&amp;quot; are caught up in issues of modularity, infor-
mation encapsulation, and the autonomy of syntax (see, e.g., Fodor 1983), issues that
cannot be settled without substantial empirical experiment and theoretical argument.
That notwithstanding, it already seems evident that if one incorporates within the
purview of a &amp;quot;reasoner&amp;quot; such text planning activities as those listed earlier then it will
be very hard to sustain the argument that knowledge of grammar can be restricted
to just surface realization. Different aspects of this knowledge can still be relatively
segregated, however; in particular it seems likely that early generation decisions only
require tacit knowledge of what lexemes and constructions the language provides,
without yet requiring access to phonetic forms, the assembly of detailed sequential
structures, or the imposition of grammatical relations.
One of the more problematic aspects of taking the source for the generator to be a
logical form is the very fact that it is represented as a single expression in a linear no-
tation. This may seem a small matter of notation, but the computational properties of a
logical form as it is usually represented give it a very low notational efficiency in gen-
eration (see Woods [1986] for a discussion of this notion). These include the simple fact
that expressions must be scanned and parsed before the information they contain can
be deployed, the lack of decompositional locality because of the use of scoping quan-
tifiers and variables to represent individuals (Mellish 1985), and, indeed, as Shieber
(this issue) points out, there is the question of the formula&apos;s intended structural re-
alization, since the logical connectives that link a formula&apos;s terms underspecify their
corresponding syntactic constructions because of the equivalence of other formulas un-
der commutativity, associativity, and other truth-preserving logical transformations.
The force of much of Shieber&apos;s argument in regard to logical-form equivalence rests
on the constraint imposed by bi-directional processing. If the choice of the information
that an utterance is to express is made by a component with no knowledge of what
the syntactic and lexical resources of the language are able to convey, then it is highly
unlikely that its representation of the information will match what the comprehension
process will arrive at as its representation of what the utterance meant—Shieber&apos;s
notion of &amp;quot;canonical logical form.&amp;quot;
Today&apos;s generators confront the problem regularly, as for example when a know-
ledge-based system passes just the symbol &apos;red-porsche&apos; to the generator and its de-
signer wants the phrase &amp;quot;the red porsche,&amp;quot; or &amp;quot;that car,&amp;quot; or &amp;quot;the red one&amp;quot; produced as is
contextually appropriate. Practical generators invariably interpose a special purpose
interface between the raw representation of the application they are speaking for and
</bodyText>
<page confidence="0.992875">
193
</page>
<note confidence="0.553761">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.991513547169811">
their own general linguistic rules so as to compensate for the raw form&apos;s weaknesses
or linguistically inappropriate organization—to &apos;match impedances&apos; as it were.
Seen as a transducer from meaning representations to surface forms, a generator
is driven by the terms and (formal) syntactic structure of its inputs. In the ideal bi-
directional system this mapping would be deterministic and reversible, but in practice
it is nondeterministic, with the generator adding information to a source represen-
tation that severely underspecifies its target utterance. Mismatch with the output of
comprehension is inevitable since the parser in effect picks out a fully specified rep-
resentation, reading into its form a correspondence with syntactico-lexical discrimi-
nations that the knowledge-based system cannot appreciate. In particular, the syntax
of today&apos;s sources&apos; logics provide little useful guidance about the form of the surface
utterance, or, alternatively, if the syntax is carefully attended to, it imposes a straight-
jacket on the space of possible target utterances and limits the possibilities for fluent
phrasing or adapting to the discourse context—a perennial problem with the &apos;direct
production&apos; generators used with expert systems.
While a (very) long-term solution to this problem waits on a fundamental redesign
of meaning representations that would bring them into alignment with the require-
ments of language, we can take steps in this direction now by improving the source
notation: Dispense with connected expressions in favor of dealing independently with
the terms that would have comprised it.1
We know that in any interesting system the logical form that specifies an utter-
ance&apos;s meaning will be composed dynamically as the needs of the situation dictate,
rather than being taken from a preconstructed repository, since if this were not the
case there would be no possibility for the creative use of language to accommodate
new situations. Given this, one has to ask why the components of the representation of
the meaning would ever need to be assembled into an expression rather than entered
directly into an early linguistic level of representation as soon as the need for them is
appreciated. What work in generation does a formula do qua formula that cannot be
done by its elements individually given a suitable representation?
The extension of an abstract linguistic plan through the incremental addition of
elements is in fact a standard technique in generation.&apos; A good example is Jeff Conk-
lin&apos;s GENARO system (Conklin 1983; Arbib, Conklin, and Hill 1987), which produced
paragraph-length descriptions of pictures of houses. GENARO selected the informa-
tion it would include using a procedure known as &amp;quot;iterative proposing,&amp;quot; whereby it
selected successive atomic units of information from its database (a KL-One network)
in a sequence determined by their relative salience given the perspective of the picture.
The units corresponded to individuals (e.g., houses, fences, colors), categorizations
1 Since there would no longer be any logical connectives (the &amp;quot;glue&amp;quot; in the expressions) to be rendered
in different but logically equivalent ways in a text, this technique also has the advantage that it reduces
the possibilities for mismatches between the way the speaker formulates information and a
comprehension system will represent its analysis of the corresponding text to just the more interesting
cases of mismatches in the lexical semantics, e.g., &amp;quot;owns 40% of Ajax Corp.&amp;quot; vs. &amp;quot;has a 40% stake in
Ajax Corp.&amp;quot;
2 Many of the ideas about bi-directional grammars and generation were developed by Shieber and Doug
Appelt at SRI, which makes it interesting to note here that in the original version of Appelt&apos;s KAMP
generator, knowledge of the grammar was distributed throughout the system and acted locally in close
coordination with the system&apos;s planning decisions, making it rather like the approach being described
here (Appelt 1982; p. 112). Appelt later shifted to using Martin Kay&apos;s Functional Unification Grammar
(Kay 1979) to increase modularity, perspicuity, and robustness to revisions in the plan, while at the
same time retaining the temporal interleaving of planning and linguistic realization, i.e., at no one
moment during the processing was there ever a full logical formula corresponding to the eventual
utterance (Appelt 1985; p. 110). The use of a FUG also of course directly facilitates bi-directional
applications (Appelt 1989).
</bodyText>
<page confidence="0.995914">
194
</page>
<note confidence="0.458276">
David D. McDonald Source for Natural Language Generation
</note>
<bodyText confidence="0.9419895">
and properties of individuals, and the relations among them, each unit contributing a
referent or content word(s) to the utterance.
As each unit was selected, it was immediately incorporated into an abstract lin-
guistic level of representation&apos; in the position that best reflected its salience relative to
the units that were there already. Thus the order in which units were selected had a
potentially dramatic impact on the form of the final utterance. Consider, for example,
the NP &amp;quot;a white two-story house,&amp;quot; embedded in the context &amp;quot;This is a picture of &amp;quot; at
the beginning of a description. Following the rough heuristic that the most salient
properties of an object are positioned closest to the head when realized as adjectives,
this NP is the result of GENARO selecting four semantic units in the following order:
</bodyText>
<listItem confidence="0.9987838">
• $house1—the referent, and the source of &amp;quot;a &amp;quot; given that the house is
being newly introduced into the discourse)
• house($house1) &amp;quot;house&amp;quot;
• two-story-building($house1) &amp;quot;two story //
• color($house1, $white) &amp;quot;white
</listItem>
<bodyText confidence="0.999179375">
The order of the units&apos; selection follows their decreasing relative salience: the
numbers in this instance were 2.0, 1.0, .56, and .20 respectively. Had the house or
its appearance in the picture been different, say switching the relative salience of the
two properties, then the order of selection and the resulting NP would reflect this: &amp;quot;a
two-story white house.&amp;quot; In different contexts, these units could have different realization,
e.g., &amp;quot;[it] is two stories high.&amp;quot;
If we were to attempt to rationally reconstruct GENARO&apos;s selections as a standard
logical form, e.g.
</bodyText>
<equation confidence="0.810181">
2(x) house(x) &amp; two-story-building(x) &amp; color(x,white)
</equation>
<bodyText confidence="0.999664647058824">
we would not only have to parse this linear notation and have to introduce some
canonical structural correspondences by which to direct its surface realization, but we
would have lost the salience information that gave GENARO its special sensitivity to
the particulars of the picture it was describing, markedly degrading its fluency.
This example illustrates not only that semantic representations should explicitly
record information about salience, but also that the pivot point for bi-directional pro-
cessing can be moved much deeper than is usually considered. In GENARO and a
goodly number of other generators we have rules for the selection of a set of minimal
semantic units and their organization into a text as just described. On the parsing
side we have the systems cited earlier, whose outputs are comparable units added to
or embellishing an existing semantic model of essentially the same sort as this style
of generator starts from. Given such architectures, the move to properly reversible
rules awaits only a declarative statement of the few remaining parts of these systems
where the mappin: s have been formulated procedurally—a project that is already well
advanced (McDonald 1991, 1992b).
Returning finally to the question of what processes should be given the label
&amp;quot;generation,&amp;quot; we must be very careful to avoid reflexively identifying generation as
</bodyText>
<footnote confidence="0.994116">
3 Today this level would correspond to Meteer&apos;s &amp;quot;Text Structure&amp;quot; (1992). At this level there is a
commitment to constituency, lexical choices for heads, and the structural relations of head-arguments
and matrix-adjuncts. The structure overall is unordered.
</footnote>
<page confidence="0.992938">
195
</page>
<note confidence="0.661249">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.999861652173913">
the obverse of parsing. After all, the determination of where a &amp;quot;parser&amp;quot; leaves off
and some non-text directed process of &amp;quot;general inferencing&amp;quot; takes over is very much a
question of how individual systems are designed. We also have evidence from state-of-
the-art systems that an incommensurate amount of processing is presently being done
in the two directions, and consequently any attempt to make components correspond
is suspect.
Existing comprehension systems as a rule extract considerably less information
from a text than a generator must appreciate in generating one. Examples include the
reasons why a given word or syntactic construction is used rather than an alternative,
what constitutes the style and rhetoric appropriate to a given genre and situation, or
why information is clustered in one pattern of sentences rather than another. There
seems to be no reason in principle why comprehension systems couldn&apos;t notice such
things, though of course their conclusions would have to be indeterminate since they
don&apos;t have access to all the information the speaker used. More likely the present state
of affairs is simply reflective of the fact that the generation of quality text is a harder
task than its comprehension.
My own answer to the question of &apos;how far back does generation go&apos; is that it may
be considered to start at the first point where a speaker must appeal to her knowledge
of language as she begins the process of carrying out some action through the use of
language. This classification is of course principally a mechanism for delimiting a field
of research, but it does also suggest that the way we might best arrive at Shieber&apos;s &amp;quot;AI-
complete&amp;quot; solution to the question of how semantic information should be represented
is through a careful study of the needs of the generation process.
</bodyText>
<sectionHeader confidence="0.863" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.995414435483871">
Appelt, Doug (1982). &amp;quot;Planning
natural-language utterances to satisfy
multiple goals.&amp;quot; SRI Technical Note 259,
Menlo Park, CA.
Appelt, Doug (1985). Planning English
sentences. Cambridge University Press.
Appelt, Doug (1989). &amp;quot;Bidirectional
grammars and the design of natural
language generation systems.&amp;quot; In
Theoretical Issues in Natural Language
Processing, edited by Wilks, 199-205.
Lawrence Erlbaum.
Arbib, Michael; Conklin, Jeffery; and Hill,
Jane (1987). From Schema-Theory to
Language. Oxford University Press.
Bourbeau, L.; Carcagno, D.; Goldberg, E.;
Kittredge, R.; and Polguere, A. (1990).
&amp;quot;Bilingual generation of weather forecasts
in an operations environment.&amp;quot; In
Proceedings, 15th International Conference on
Computational Linguistics (COLING-90).
90-92.
Conklin, E. Jeffery (1983). &amp;quot;Data-driven
indelible planning of discourse generation
using salience.&amp;quot; Doctoral dissertation,
University of Massachusetts, Amherst,
MA. Technical report 83-13.
Hovy, Eduard (1990) &amp;quot;Unresolved issues in
paragraph planning.&amp;quot; In Current Research
in Natural Language Generation, edited by
Dale, Mellish, and Zock. Academic Press.
Hovy, Eduard; McDonald, David; and
Young, Sheryl (1989). &amp;quot;Current issues in
natural language generation: An
overview of the AAAI Workshop on Text
Planning and Realization.&amp;quot; Al Magazine,
10(3), 27-29.
Fodor, Jerry (1983). The Modularity of Mind.
The MIT Press.
Kay, Martin (1979). &amp;quot;Functional grammar.&amp;quot;
In Proceedings, 5th Annual Meeting of the
Berkeley Linguistics Society. University of
California, Berkeley, CA, 142-158.
Kukich, Karen (1988). &amp;quot;Fluency in Natural
Language Reports.&amp;quot; In Natural Language
Generation Systems, edited by McDonald
and Bolc. Springer-Verlag.
Martin, Charles, and Riesbeck, Chris (1986).
&amp;quot;Uniform parsing and inference for
learning.&amp;quot; In Proceedings, AAAI-86.
Philadelphia, PA. Morgan-Kaufmann.
McDonald, David (1993). &amp;quot;Reversible NLP
by deriving the grammars from the
knowledge base.&amp;quot; In Reversible Grammar in
Natural Language Processing. Kluwer
Academic Publishers.
McDonald, David (1992a). &amp;quot;An efficient
chart-based algorithm for partial-parsing
of unrestricted texts.&amp;quot; In Proceedings, 3rd
Conference on Applied Natural Language
Processing (ACL). Trento, Italy, 193-200.
McDonald, David (1992b). &amp;quot;Type-driven
</reference>
<page confidence="0.996758">
196
</page>
<note confidence="0.643283">
David D. McDonald Source for Natural Language Generation
</note>
<reference confidence="0.9980004">
suppression of redundancy in the
generation of inference-rich reports.&amp;quot; In
Aspects of Automated Natural Language
Generation, (Springer Verlag Lecture Notes
in Al, Number 587), edited by Dale, Hovy,
Rosner, and Stock, 73-88. Springer-Verlag.
McDonald, David; Meteer (Vaughan),
Marie; and Pustejovsky, James (1987).
&amp;quot;Factors contributing to efficiency in
natural language generation.&amp;quot; In Natural
Language Generation: Recent Advances in
Artificial Intelligence, Psychology, and
Linguistics, edited by Kempen, 159-181.
Kluwer Academic Publishers.
Meteer, Marie (1992). Expressibility and the
Problem of Efficient Text Planning. Pinter
Publishers.
Mellish, Chris (1985). Computer Interpretation
of Natural Language Descriptions. John
Wiley.
Rosner, Deitmar (1988). &amp;quot;The generation
system of the SEMSYN project: Towards a
task-independent generator for German.&amp;quot;
In Advances in Natural Language Generation,
edited by Zock and Sabah. Pinter
Publishers.
Shieber, Stuart; van Noord, Gertjan; Pereira,
Fernando; and Moore, Robert (1990).
&amp;quot;Semantic-head-driven generation.&amp;quot;
Computational Linguistics, 16(1), 30-42.
Shieber, Stuart (1993). &amp;quot;The problem of
logical-form equivalence.&amp;quot; Computational
Linguistics, 19(1), 179-190.
Webber, Bonnie (1971). &amp;quot;The case for
generation.&amp;quot; In Papers Presented at the
Seminar in Mathematical Linguistics,
Volume XIII, edited by Woods. Aiken
Computer Laboratory, Department of
Linguistics, Harvard University,
Cambridge, MA.
Wedekind, Jurgen (1988). &amp;quot;Generation as
structure driven derivation.&amp;quot; In
Proceedings, 13th International Conference on
Computational Linguistics (COLING-88).
Budapest, Hungary, 732-737.
Winograd, Terry (1972). Understanding
Natural Language. Academic Press.
Woods, William (1986). &amp;quot;Important issues in
knowledge representation.&amp;quot; In Proceedings,
IEEE. 74(10).
</reference>
<page confidence="0.998183">
197
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.998943">Issues in the Choice of a Source for Natural Language Generation</title>
<author confidence="0.999979">David D McDonald</author>
<affiliation confidence="0.999387">Brandeis University</affiliation>
<abstract confidence="0.998682594594595">The most vexing question in natural language generation is &apos;what is the source&apos;— what do speakers start from when they begin to compose an utterance? Theories of generation in the literature differ markedly in their assumptions. A few start with an unanalyzed body of numerical data (e.g. Bourbeau et al. 1990; Kukich 1988). Most start with the structured objects that are used by a particular reasoning system or simulator and are cast in that system&apos;s representational formalism (e.g. Hovy 1990; Meteer 1992; Wisner 1988). A growing number of systems, largely focused on problems in machine translation or grammatical theory, take their input to be logical formulae based on lexical predicates (e.g. Wedekind 1988; Shieber et al. 1990). The lack of a consistent answer to the question of the generator&apos;s source has been at the heart of the problem of how to make research on generation intelligible and engaging for the rest of the computational linguistics community, and has complicated efforts to evaluate alternative treatments even for people in the field. Nevertheless, a source cannot be imposed by fiat. Differences in what information is assumed to be available, its relative decomposition when compared to the &amp;quot;packaging&amp;quot; available in words or syntactic constructions of the language resources), amount and kinds of information are contained in the atomic units of the source, and what sorts of compositions and other larger scale organizations are possible—all these have an impact on what architectures are plausible for generation and what efficiencies they can achieve. Advances in the field often come precisely through insights into the representation of the source. Language comprehension research does not have this problem—its source is a text. Differences in methodology govern where this text comes from (e.g., single sentence vs. discourse, sample sentences vs. corpus study, written vs. spoken), but these aside there is no question of what the comprehension process starts with. Where comprehension &amp;quot;ends&amp;quot; is quite another matter. If we go back to some of the early comprehension systems, the end point of the process was an action, and there was linguistic processing at every stage (Winograd 1972). Some researchers, this author included, take the end point to be an elaboration of an already existing semantic model whereby some new individuals are added and new relations established between them and other individuals (Martin and Riesbeck 1986; McDonald 1992a). Today&apos;s dominant paradigm, however, stemming perhaps from the predominance of research on question-answering and following the lead of theoretical linguistics, is to take the end point to be a logical form: an expression that codifies the information in the text at a fairly shallow level, e.g., a first-order formula with content words mapped to predicates with the same spelling, and with individuals represented by quantified variables or constants.</abstract>
<note confidence="0.850289333333333">14 Brantwood Road, Arlington, MA 02174-8004; mcdonald@cs.brandeis.edu C) 1993 Association for Computational Linguistics Computational Linguistics Volume 19, Number 1</note>
<abstract confidence="0.995510502145923">It is somewhat puzzling that this question of where the comprehension process ends has apparently never been debated in the literature. Instead it seems largely taken for granted that the parsing process ends with the assembly of an expression in a suitable logic that captures the text&apos;s information content, perhaps with some functional annotations, and that a &amp;quot;reasoning&amp;quot; process then starts with that expression and draws inferences in order to resolve anaphors and establish the speaker&apos;s intent. Problems arise when researchers project this default decomposition onto the process of producing language. All too often the process is divided into a &amp;quot;reasoning&amp;quot; and a &amp;quot;generation&amp;quot; component (see, e.g., Shieber, this issue)—an unfortunate choice of terminology because it reduces the scope of &amp;quot;generation&amp;quot; to triviality as we shall see. The primary motivation for the division is the desire for a bi-directional natural language processing system—one where the representation of the linguistic resources use in both the comprehension and production of utterances. But while a reversible representation is indeed a proper goal for today&apos;s systems, the choice of logical form as the &amp;quot;pivot point&amp;quot; is problematic, especially a first-order formula. A truly reversible linguistic mapping between intentional situation and utterance will have the comprehension process end where the generation process begins. Thus just as the psycholinguistically correct source for generation is still very much a matter of research (as it is even when the source is a computational object in a well-designed AT system), so too is the end-point of comprehension, and by implication the division of that process into components and representational levels. A declarative, reversible, form–meaning mapping does not ipso facto have to start/end at the level of logical form, but can originate at a much deeper level with the class definitions of the object types and relations of the speaker&apos;s conceptual model (McDonald 1993). Considered in isolation, the production of text from a logical form is, quite frankly, trivial. It corresponds to the final &amp;quot;readout&amp;quot; phase of McDonald, Meteer, and Pustejovsky (1987), since all that remains to be done is to linearize its elements in keeping with the constraints of a surface grammar, carry out the trivial mapping to the phonetic (orthographic) forms of the words implicit in the predicates, and add the requisite grammatical function words and morphemes. This capability has been an established part of the state of the art for well over twenty years (see, e.g., Webber [1971], which is also the first work on reversible grammars for generation this author is aware of). Over the years new architectures for this &amp;quot;tactical&amp;quot; part of generation (we prefer the realization) introduced only because of new ideas in grammatical theory or in response to shifts in what is given in the immediately prior representational level. In current research that focuses just on surface realization, all of the substantial tasks of generation are invariably subordinated to the &amp;quot;reasoner&amp;quot; or &amp;quot;strategic component,&amp;quot; which is treated as a black box whose operations are seldom discussed. Examples of these tasks include construing the speaker&apos;s situation in realizable terms given the available vocabulary and syntactic resources (an especially important task when the source is raw data, e.g., precisely what points of the compass make the wind et al. 1990]); selecting the information to include in the utterance and deciding whether it should be stated explicitly or left for inference; distributing the information into sentences and giving it an organization that reflects the intended rhetorical force, coherence, and necessary cohesion given the prior discourse; and finda mapping of the information to linguistic resources that is collectively (i.e., has a surface realization; see Meteer 1992). How one chooses to approach these tasks has substantial implications for the kinds of structures that a surface realization process can sensibly be given as input and may not be taken for granted. As a consequence, any generation architecture that is proposed without including an articulation 192 David D. McDonald Source for Natural Language Generation of the early stages of the process is issuing a large promissory note that it may not be able to redeem. Often the choice of a two-component process in comprehension (and by extension in generation) is based on the judgment that linguistic knowledge can and should be restricted to its own component, the one responsible for the form and content of grammatical rules, leaving to the other component all matters of general reasoning (&amp;quot;reasoners should not have to truck with grammatical issues&amp;quot;). This assumption has been seriously questioned within the generation community in recent years. The constraints imposed by the linguistic resources&apos; limitations in what they are able to express and the delicacy of the conceptual and rhetorical choices state-of-the-art generators are called on to make combine to force a strong interdependency between early and late aspects of the process to the point where many generation researchers today do not recognize any strong division into components, with different aspects of linguistic knowledge appearing at many levels of representation (see Hovy, McDonald, and Young 1989). All judgments about &amp;quot;components&amp;quot; are caught up in issues of modularity, information encapsulation, and the autonomy of syntax (see, e.g., Fodor 1983), issues that cannot be settled without substantial empirical experiment and theoretical argument. That notwithstanding, it already seems evident that if one incorporates within the purview of a &amp;quot;reasoner&amp;quot; such text planning activities as those listed earlier then it will be very hard to sustain the argument that knowledge of grammar can be restricted to just surface realization. Different aspects of this knowledge can still be relatively segregated, however; in particular it seems likely that early generation decisions only tacit knowledge of what lexemes and the language without yet requiring access to phonetic forms, the assembly of detailed sequential structures, or the imposition of grammatical relations. One of the more problematic aspects of taking the source for the generator to be a logical form is the very fact that it is represented as a single expression in a linear notation. This may seem a small matter of notation, but the computational properties of a logical form as it is usually represented give it a very low notational efficiency in generation (see Woods [1986] for a discussion of this notion). These include the simple fact that expressions must be scanned and parsed before the information they contain can be deployed, the lack of decompositional locality because of the use of scoping quantifiers and variables to represent individuals (Mellish 1985), and, indeed, as Shieber (this issue) points out, there is the question of the formula&apos;s intended structural realization, since the logical connectives that link a formula&apos;s terms underspecify their corresponding syntactic constructions because of the equivalence of other formulas under commutativity, associativity, and other truth-preserving logical transformations. The force of much of Shieber&apos;s argument in regard to logical-form equivalence rests on the constraint imposed by bi-directional processing. If the choice of the information that an utterance is to express is made by a component with no knowledge of what the syntactic and lexical resources of the language are able to convey, then it is highly unlikely that its representation of the information will match what the comprehension process will arrive at as its representation of what the utterance meant—Shieber&apos;s notion of &amp;quot;canonical logical form.&amp;quot; Today&apos;s generators confront the problem regularly, as for example when a knowledge-based system passes just the symbol &apos;red-porsche&apos; to the generator and its dewants the phrase red porsche,&amp;quot; car,&amp;quot; red one&amp;quot; as is contextually appropriate. Practical generators invariably interpose a special purpose interface between the raw representation of the application they are speaking for and 193 Computational Linguistics Volume 19, Number 1 their own general linguistic rules so as to compensate for the raw form&apos;s weaknesses or linguistically inappropriate organization—to &apos;match impedances&apos; as it were. Seen as a transducer from meaning representations to surface forms, a generator is driven by the terms and (formal) syntactic structure of its inputs. In the ideal bidirectional system this mapping would be deterministic and reversible, but in practice it is nondeterministic, with the generator adding information to a source representation that severely underspecifies its target utterance. Mismatch with the output of comprehension is inevitable since the parser in effect picks out a fully specified representation, reading into its form a correspondence with syntactico-lexical discriminations that the knowledge-based system cannot appreciate. In particular, the syntax of today&apos;s sources&apos; logics provide little useful guidance about the form of the surface utterance, or, alternatively, if the syntax is carefully attended to, it imposes a straightjacket on the space of possible target utterances and limits the possibilities for fluent phrasing or adapting to the discourse context—a perennial problem with the &apos;direct production&apos; generators used with expert systems. While a (very) long-term solution to this problem waits on a fundamental redesign of meaning representations that would bring them into alignment with the requirements of language, we can take steps in this direction now by improving the source notation: Dispense with connected expressions in favor of dealing independently with terms that would have comprised We know that in any interesting system the logical form that specifies an utterance&apos;s meaning will be composed dynamically as the needs of the situation dictate, rather than being taken from a preconstructed repository, since if this were not the case there would be no possibility for the creative use of language to accommodate new situations. Given this, one has to ask why the components of the representation of the meaning would ever need to be assembled into an expression rather than entered directly into an early linguistic level of representation as soon as the need for them is What work in generation does a formula do that cannot be done by its elements individually given a suitable representation? The extension of an abstract linguistic plan through the incremental addition of elements is in fact a standard technique in generation.&apos; A good example is Jeff Conklin&apos;s GENARO system (Conklin 1983; Arbib, Conklin, and Hill 1987), which produced paragraph-length descriptions of pictures of houses. GENARO selected the information it would include using a procedure known as &amp;quot;iterative proposing,&amp;quot; whereby it selected successive atomic units of information from its database (a KL-One network) a sequence determined by their relative the perspective of the picture. The units corresponded to individuals (e.g., houses, fences, colors), categorizations 1 Since there would no longer be any logical connectives (the &amp;quot;glue&amp;quot; in the expressions) to be rendered in different but logically equivalent ways in a text, this technique also has the advantage that it reduces the possibilities for mismatches between the way the speaker formulates information and a comprehension system will represent its analysis of the corresponding text to just the more interesting cases of mismatches in the lexical semantics, e.g., &amp;quot;owns 40% of Ajax Corp.&amp;quot; vs. &amp;quot;has a 40% stake in Ajax Corp.&amp;quot; 2 Many of the ideas about bi-directional grammars and generation were developed by Shieber and Doug Appelt at SRI, which makes it interesting to note here that in the original version of Appelt&apos;s KAMP generator, knowledge of the grammar was distributed throughout the system and acted locally in close coordination with the system&apos;s planning decisions, making it rather like the approach being described here (Appelt 1982; p. 112). Appelt later shifted to using Martin Kay&apos;s Functional Unification Grammar (Kay 1979) to increase modularity, perspicuity, and robustness to revisions in the plan, while at the same time retaining the temporal interleaving of planning and linguistic realization, i.e., at no one moment during the processing was there ever a full logical formula corresponding to the eventual utterance (Appelt 1985; p. 110). The use of a FUG also of course directly facilitates bi-directional applications (Appelt 1989). 194 David D. McDonald Source for Natural Language Generation and properties of individuals, and the relations among them, each unit contributing a referent or content word(s) to the utterance. As each unit was selected, it was immediately incorporated into an abstract linguistic level of representation&apos; in the position that best reflected its salience relative to the units that were there already. Thus the order in which units were selected had a potentially dramatic impact on the form of the final utterance. Consider, for example, NP white two-story house,&amp;quot; in the context is a picture of &amp;quot; the beginning of a description. Following the rough heuristic that the most salient properties of an object are positioned closest to the head when realized as adjectives, this NP is the result of GENARO selecting four semantic units in the following order: $house1—the referent, and the source of &amp;quot; that the house being newly introduced into the discourse) house($house1) two-story-building($house1) story color($house1, $white) The order of the units&apos; selection follows their decreasing relative salience: the numbers in this instance were 2.0, 1.0, .56, and .20 respectively. Had the house or its appearance in the picture been different, say switching the relative salience of the properties, then the order of selection and the resulting NP would reflect this: white house.&amp;quot; different contexts, these units could have different realization, is two stories high.&amp;quot; If we were to attempt to rationally reconstruct GENARO&apos;s selections as a standard logical form, e.g. 2(x) house(x) &amp; two-story-building(x) &amp; color(x,white) we would not only have to parse this linear notation and have to introduce some canonical structural correspondences by which to direct its surface realization, but we would have lost the salience information that gave GENARO its special sensitivity to the particulars of the picture it was describing, markedly degrading its fluency. This example illustrates not only that semantic representations should explicitly record information about salience, but also that the pivot point for bi-directional processing can be moved much deeper than is usually considered. In GENARO and a goodly number of other generators we have rules for the selection of a set of minimal semantic units and their organization into a text as just described. On the parsing side we have the systems cited earlier, whose outputs are comparable units added to or embellishing an existing semantic model of essentially the same sort as this style of generator starts from. Given such architectures, the move to properly reversible rules awaits only a declarative statement of the few remaining parts of these systems where the mappin: s have been formulated procedurally—a project that is already well advanced (McDonald 1991, 1992b). Returning finally to the question of what processes should be given the label &amp;quot;generation,&amp;quot; we must be very careful to avoid reflexively identifying generation as 3 Today this level would correspond to Meteer&apos;s &amp;quot;Text Structure&amp;quot; (1992). At this level there is a commitment to constituency, lexical choices for heads, and the structural relations of head-arguments and matrix-adjuncts. The structure overall is unordered. 195 Computational Linguistics Volume 19, Number 1 the obverse of parsing. After all, the determination of where a &amp;quot;parser&amp;quot; leaves off and some non-text directed process of &amp;quot;general inferencing&amp;quot; takes over is very much a question of how individual systems are designed. We also have evidence from state-ofthe-art systems that an incommensurate amount of processing is presently being done in the two directions, and consequently any attempt to make components correspond is suspect. Existing comprehension systems as a rule extract considerably less information from a text than a generator must appreciate in generating one. Examples include the reasons why a given word or syntactic construction is used rather than an alternative, what constitutes the style and rhetoric appropriate to a given genre and situation, or why information is clustered in one pattern of sentences rather than another. There seems to be no reason in principle why comprehension systems couldn&apos;t notice such things, though of course their conclusions would have to be indeterminate since they don&apos;t have access to all the information the speaker used. More likely the present state of affairs is simply reflective of the fact that the generation of quality text is a harder task than its comprehension. My own answer to the question of &apos;how far back does generation go&apos; is that it may be considered to start at the first point where a speaker must appeal to her knowledge of language as she begins the process of carrying out some action through the use of language. This classification is of course principally a mechanism for delimiting a field of research, but it does also suggest that the way we might best arrive at Shieber&apos;s &amp;quot;AIcomplete&amp;quot; solution to the question of how semantic information should be represented is through a careful study of the needs of the generation process. References Appelt, Doug (1982). &amp;quot;Planning natural-language utterances to satisfy multiple goals.&amp;quot; SRI Technical Note 259,</abstract>
<address confidence="0.735236">Menlo Park, CA.</address>
<author confidence="0.814831">Doug</author>
<affiliation confidence="0.821331">University Press.</affiliation>
<note confidence="0.844749588235294">Appelt, Doug (1989). &amp;quot;Bidirectional grammars and the design of natural language generation systems.&amp;quot; In Theoretical Issues in Natural Language by Wilks, 199-205. Lawrence Erlbaum. Arbib, Michael; Conklin, Jeffery; and Hill, (1987). Schema-Theory to University Press. Bourbeau, L.; Carcagno, D.; Goldberg, E.; Kittredge, R.; and Polguere, A. (1990). &amp;quot;Bilingual generation of weather forecasts in an operations environment.&amp;quot; In Proceedings, 15th International Conference on Computational Linguistics (COLING-90). 90-92. Conklin, E. Jeffery (1983). &amp;quot;Data-driven</note>
<title confidence="0.520341">indelible planning of discourse generation</title>
<author confidence="0.553402">using salience Doctoral dissertation</author>
<affiliation confidence="0.912711">University of Massachusetts, Amherst,</affiliation>
<address confidence="0.63269">MA. Technical report 83-13.</address>
<degree confidence="0.627381857142857">Hovy, Eduard (1990) &amp;quot;Unresolved issues in planning.&amp;quot; In Research Natural Language Generation, by Dale, Mellish, and Zock. Academic Press. Hovy, Eduard; McDonald, David; and Young, Sheryl (1989). &amp;quot;Current issues in natural language generation: An</degree>
<affiliation confidence="0.657076">overview of the AAAI Workshop on Text and Realization.&amp;quot; Magazine,</affiliation>
<address confidence="0.483348">10(3), 27-29.</address>
<note confidence="0.916423461538461">Jerry (1983). Modularity of Mind. The MIT Press. Kay, Martin (1979). &amp;quot;Functional grammar.&amp;quot; 5th Annual Meeting of the Linguistics Society. of California, Berkeley, CA, 142-158. Kukich, Karen (1988). &amp;quot;Fluency in Natural Reports.&amp;quot; In Language Systems, by McDonald and Bolc. Springer-Verlag. Martin, Charles, and Riesbeck, Chris (1986). &amp;quot;Uniform parsing and inference for In AAAI-86. Philadelphia, PA. Morgan-Kaufmann. McDonald, David (1993). &amp;quot;Reversible NLP by deriving the grammars from the base.&amp;quot; In Grammar in Language Processing. Academic Publishers. McDonald, David (1992a). &amp;quot;An efficient chart-based algorithm for partial-parsing unrestricted texts.&amp;quot; In 3rd Conference on Applied Natural Language (ACL). Italy, 193-200. McDonald, David (1992b). &amp;quot;Type-driven 196</note>
<title confidence="0.631208">David D. McDonald Source for Natural Language Generation suppression of redundancy in the generation of inference-rich reports.&amp;quot; In Aspects of Automated Natural Language Verlag Lecture Notes</title>
<note confidence="0.922641352941177">in Al, Number 587), edited by Dale, Hovy, Rosner, and Stock, 73-88. Springer-Verlag. McDonald, David; Meteer (Vaughan), Marie; and Pustejovsky, James (1987). &amp;quot;Factors contributing to efficiency in language generation.&amp;quot; In Language Generation: Recent Advances in Artificial Intelligence, Psychology, and by Kempen, 159-181. Kluwer Academic Publishers. Marie (1992). and the of Efficient Text Planning. Publishers. Chris (1985). Interpretation Natural Language Descriptions. Wiley. Rosner, Deitmar (1988). &amp;quot;The generation</note>
<abstract confidence="0.691777333333333">system of the SEMSYN project: Towards a task-independent generator for German.&amp;quot; in Natural Language Generation,</abstract>
<note confidence="0.813461416666667">edited by Zock and Sabah. Pinter Publishers. Shieber, Stuart; van Noord, Gertjan; Pereira, Fernando; and Moore, Robert (1990). &amp;quot;Semantic-head-driven generation.&amp;quot; Linguistics, 30-42. Shieber, Stuart (1993). &amp;quot;The problem of equivalence.&amp;quot; 179-190. Webber, Bonnie (1971). &amp;quot;The case for In Presented at the Seminar in Mathematical Linguistics,</note>
<author confidence="0.907199">Aiken</author>
<affiliation confidence="0.9792645">Computer Laboratory, Department of Linguistics, Harvard University,</affiliation>
<address confidence="0.988693">Cambridge, MA.</address>
<note confidence="0.887941">Wedekind, Jurgen (1988). &amp;quot;Generation as structure driven derivation.&amp;quot; In Proceedings, 13th International Conference on Computational Linguistics (COLING-88). Budapest, Hungary, 732-737. Terry (1972). Language. Press. Woods, William (1986). &amp;quot;Important issues in representation.&amp;quot; In 197</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Appelt</author>
</authors>
<title>Planning natural-language utterances to satisfy multiple goals.&amp;quot;</title>
<date>1982</date>
<tech>SRI Technical Note 259,</tech>
<location>Menlo Park, CA.</location>
<contexts>
<context position="15622" citStr="Appelt 1982" startWordPosition="2406" endWordPosition="2407">epresent its analysis of the corresponding text to just the more interesting cases of mismatches in the lexical semantics, e.g., &amp;quot;owns 40% of Ajax Corp.&amp;quot; vs. &amp;quot;has a 40% stake in Ajax Corp.&amp;quot; 2 Many of the ideas about bi-directional grammars and generation were developed by Shieber and Doug Appelt at SRI, which makes it interesting to note here that in the original version of Appelt&apos;s KAMP generator, knowledge of the grammar was distributed throughout the system and acted locally in close coordination with the system&apos;s planning decisions, making it rather like the approach being described here (Appelt 1982; p. 112). Appelt later shifted to using Martin Kay&apos;s Functional Unification Grammar (Kay 1979) to increase modularity, perspicuity, and robustness to revisions in the plan, while at the same time retaining the temporal interleaving of planning and linguistic realization, i.e., at no one moment during the processing was there ever a full logical formula corresponding to the eventual utterance (Appelt 1985; p. 110). The use of a FUG also of course directly facilitates bi-directional applications (Appelt 1989). 194 David D. McDonald Source for Natural Language Generation and properties of indivi</context>
</contexts>
<marker>Appelt, 1982</marker>
<rawString>Appelt, Doug (1982). &amp;quot;Planning natural-language utterances to satisfy multiple goals.&amp;quot; SRI Technical Note 259, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Appelt</author>
</authors>
<title>Planning English sentences.</title>
<date>1985</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="16030" citStr="Appelt 1985" startWordPosition="2467" endWordPosition="2468">dge of the grammar was distributed throughout the system and acted locally in close coordination with the system&apos;s planning decisions, making it rather like the approach being described here (Appelt 1982; p. 112). Appelt later shifted to using Martin Kay&apos;s Functional Unification Grammar (Kay 1979) to increase modularity, perspicuity, and robustness to revisions in the plan, while at the same time retaining the temporal interleaving of planning and linguistic realization, i.e., at no one moment during the processing was there ever a full logical formula corresponding to the eventual utterance (Appelt 1985; p. 110). The use of a FUG also of course directly facilitates bi-directional applications (Appelt 1989). 194 David D. McDonald Source for Natural Language Generation and properties of individuals, and the relations among them, each unit contributing a referent or content word(s) to the utterance. As each unit was selected, it was immediately incorporated into an abstract linguistic level of representation&apos; in the position that best reflected its salience relative to the units that were there already. Thus the order in which units were selected had a potentially dramatic impact on the form of</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Appelt, Doug (1985). Planning English sentences. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Appelt</author>
</authors>
<title>Bidirectional grammars and the design of natural language generation systems.&amp;quot;</title>
<date>1989</date>
<booktitle>In Theoretical Issues in Natural Language Processing, edited by Wilks,</booktitle>
<pages>199--205</pages>
<institution>Lawrence Erlbaum.</institution>
<contexts>
<context position="16135" citStr="Appelt 1989" startWordPosition="2483" endWordPosition="2484">e system&apos;s planning decisions, making it rather like the approach being described here (Appelt 1982; p. 112). Appelt later shifted to using Martin Kay&apos;s Functional Unification Grammar (Kay 1979) to increase modularity, perspicuity, and robustness to revisions in the plan, while at the same time retaining the temporal interleaving of planning and linguistic realization, i.e., at no one moment during the processing was there ever a full logical formula corresponding to the eventual utterance (Appelt 1985; p. 110). The use of a FUG also of course directly facilitates bi-directional applications (Appelt 1989). 194 David D. McDonald Source for Natural Language Generation and properties of individuals, and the relations among them, each unit contributing a referent or content word(s) to the utterance. As each unit was selected, it was immediately incorporated into an abstract linguistic level of representation&apos; in the position that best reflected its salience relative to the units that were there already. Thus the order in which units were selected had a potentially dramatic impact on the form of the final utterance. Consider, for example, the NP &amp;quot;a white two-story house,&amp;quot; embedded in the context &amp;quot;T</context>
</contexts>
<marker>Appelt, 1989</marker>
<rawString>Appelt, Doug (1989). &amp;quot;Bidirectional grammars and the design of natural language generation systems.&amp;quot; In Theoretical Issues in Natural Language Processing, edited by Wilks, 199-205. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Arbib</author>
<author>Jeffery Conklin</author>
<author>Hill</author>
</authors>
<title>From Schema-Theory to Language.</title>
<date>1987</date>
<publisher>Oxford University Press.</publisher>
<marker>Arbib, Conklin, Hill, 1987</marker>
<rawString>Arbib, Michael; Conklin, Jeffery; and Hill, Jane (1987). From Schema-Theory to Language. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bourbeau</author>
<author>D Carcagno</author>
<author>E Goldberg</author>
<author>R Kittredge</author>
<author>A Polguere</author>
</authors>
<title>Bilingual generation of weather forecasts in an operations environment.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 15th International Conference on Computational Linguistics (COLING-90).</booktitle>
<pages>90--92</pages>
<contexts>
<context position="6760" citStr="Bourbeau et al. 1990" startWordPosition="1039" endWordPosition="1042">onse to shifts in what is given in the immediately prior representational level. In current research that focuses just on surface realization, all of the substantial tasks of generation are invariably subordinated to the &amp;quot;reasoner&amp;quot; or &amp;quot;strategic component,&amp;quot; which is treated as a black box whose operations are seldom discussed. Examples of these tasks include construing the speaker&apos;s situation in realizable terms given the available vocabulary and syntactic resources (an especially important task when the source is raw data, e.g., precisely what points of the compass make the wind &amp;quot;easterly,&amp;quot; [Bourbeau et al. 1990]); selecting the information to include in the utterance and deciding whether it should be stated explicitly or left for inference; distributing the information into sentences and giving it an organization that reflects the intended rhetorical force, coherence, and necessary cohesion given the prior discourse; and finding a mapping of the information to linguistic resources that is collectively expressible (i.e., has a surface realization; see Meteer 1992). How one chooses to approach these tasks has substantial implications for the kinds of structures that a surface realization process can s</context>
</contexts>
<marker>Bourbeau, Carcagno, Goldberg, Kittredge, Polguere, 1990</marker>
<rawString>Bourbeau, L.; Carcagno, D.; Goldberg, E.; Kittredge, R.; and Polguere, A. (1990). &amp;quot;Bilingual generation of weather forecasts in an operations environment.&amp;quot; In Proceedings, 15th International Conference on Computational Linguistics (COLING-90). 90-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Jeffery Conklin</author>
</authors>
<title>Data-driven indelible planning of discourse generation using salience.&amp;quot; Doctoral dissertation,</title>
<date>1983</date>
<tech>Technical report 83-13.</tech>
<institution>University of Massachusetts,</institution>
<location>Amherst, MA.</location>
<contexts>
<context position="14209" citStr="Conklin 1983" startWordPosition="2189" endWordPosition="2190">date new situations. Given this, one has to ask why the components of the representation of the meaning would ever need to be assembled into an expression rather than entered directly into an early linguistic level of representation as soon as the need for them is appreciated. What work in generation does a formula do qua formula that cannot be done by its elements individually given a suitable representation? The extension of an abstract linguistic plan through the incremental addition of elements is in fact a standard technique in generation.&apos; A good example is Jeff Conklin&apos;s GENARO system (Conklin 1983; Arbib, Conklin, and Hill 1987), which produced paragraph-length descriptions of pictures of houses. GENARO selected the information it would include using a procedure known as &amp;quot;iterative proposing,&amp;quot; whereby it selected successive atomic units of information from its database (a KL-One network) in a sequence determined by their relative salience given the perspective of the picture. The units corresponded to individuals (e.g., houses, fences, colors), categorizations 1 Since there would no longer be any logical connectives (the &amp;quot;glue&amp;quot; in the expressions) to be rendered in different but logica</context>
</contexts>
<marker>Conklin, 1983</marker>
<rawString>Conklin, E. Jeffery (1983). &amp;quot;Data-driven indelible planning of discourse generation using salience.&amp;quot; Doctoral dissertation, University of Massachusetts, Amherst, MA. Technical report 83-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
</authors>
<title>Unresolved issues in paragraph planning.&amp;quot;</title>
<date>1990</date>
<journal>In Current Research in Natural Language Generation, edited</journal>
<publisher>Academic Press.</publisher>
<marker>Hovy, 1990</marker>
<rawString>Hovy, Eduard (1990) &amp;quot;Unresolved issues in paragraph planning.&amp;quot; In Current Research in Natural Language Generation, edited by Dale, Mellish, and Zock. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>David McDonald</author>
<author>Sheryl Young</author>
</authors>
<title>Current issues in natural language generation: An overview of the AAAI Workshop on Text Planning and Realization.&amp;quot;</title>
<date>1989</date>
<journal>Al Magazine,</journal>
<volume>10</volume>
<issue>3</issue>
<pages>27--29</pages>
<marker>Hovy, McDonald, Young, 1989</marker>
<rawString>Hovy, Eduard; McDonald, David; and Young, Sheryl (1989). &amp;quot;Current issues in natural language generation: An overview of the AAAI Workshop on Text Planning and Realization.&amp;quot; Al Magazine, 10(3), 27-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Fodor</author>
</authors>
<title>The Modularity of Mind.</title>
<date>1983</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8847" citStr="Fodor 1983" startWordPosition="1364" endWordPosition="1365"> what they are able to express and the delicacy of the conceptual and rhetorical choices state-of-the-art generators are called on to make combine to force a strong interdependency between early and late aspects of the process to the point where many generation researchers today do not recognize any strong division into components, with different aspects of linguistic knowledge appearing at many levels of representation (see Hovy, McDonald, and Young 1989). All judgments about &amp;quot;components&amp;quot; are caught up in issues of modularity, information encapsulation, and the autonomy of syntax (see, e.g., Fodor 1983), issues that cannot be settled without substantial empirical experiment and theoretical argument. That notwithstanding, it already seems evident that if one incorporates within the purview of a &amp;quot;reasoner&amp;quot; such text planning activities as those listed earlier then it will be very hard to sustain the argument that knowledge of grammar can be restricted to just surface realization. Different aspects of this knowledge can still be relatively segregated, however; in particular it seems likely that early generation decisions only require tacit knowledge of what lexemes and constructions the languag</context>
</contexts>
<marker>Fodor, 1983</marker>
<rawString>Fodor, Jerry (1983). The Modularity of Mind. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Functional grammar.&amp;quot;</title>
<date>1979</date>
<booktitle>In Proceedings, 5th Annual Meeting of the</booktitle>
<pages>142--158</pages>
<institution>Berkeley Linguistics Society. University of California,</institution>
<location>Berkeley, CA,</location>
<contexts>
<context position="15717" citStr="Kay 1979" startWordPosition="2420" endWordPosition="2421">in the lexical semantics, e.g., &amp;quot;owns 40% of Ajax Corp.&amp;quot; vs. &amp;quot;has a 40% stake in Ajax Corp.&amp;quot; 2 Many of the ideas about bi-directional grammars and generation were developed by Shieber and Doug Appelt at SRI, which makes it interesting to note here that in the original version of Appelt&apos;s KAMP generator, knowledge of the grammar was distributed throughout the system and acted locally in close coordination with the system&apos;s planning decisions, making it rather like the approach being described here (Appelt 1982; p. 112). Appelt later shifted to using Martin Kay&apos;s Functional Unification Grammar (Kay 1979) to increase modularity, perspicuity, and robustness to revisions in the plan, while at the same time retaining the temporal interleaving of planning and linguistic realization, i.e., at no one moment during the processing was there ever a full logical formula corresponding to the eventual utterance (Appelt 1985; p. 110). The use of a FUG also of course directly facilitates bi-directional applications (Appelt 1989). 194 David D. McDonald Source for Natural Language Generation and properties of individuals, and the relations among them, each unit contributing a referent or content word(s) to th</context>
</contexts>
<marker>Kay, 1979</marker>
<rawString>Kay, Martin (1979). &amp;quot;Functional grammar.&amp;quot; In Proceedings, 5th Annual Meeting of the Berkeley Linguistics Society. University of California, Berkeley, CA, 142-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Fluency in Natural Language Reports.&amp;quot; In Natural Language Generation Systems, edited by McDonald and Bolc.</title>
<date>1988</date>
<publisher>Springer-Verlag.</publisher>
<marker>Kukich, 1988</marker>
<rawString>Kukich, Karen (1988). &amp;quot;Fluency in Natural Language Reports.&amp;quot; In Natural Language Generation Systems, edited by McDonald and Bolc. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Martin</author>
<author>Chris Riesbeck</author>
</authors>
<title>Uniform parsing and inference for learning.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, AAAI-86.</booktitle>
<publisher>Morgan-Kaufmann.</publisher>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2599" citStr="Martin and Riesbeck 1986" startWordPosition="401" endWordPosition="404">vs. discourse, sample sentences vs. corpus study, written vs. spoken), but these aside there is no question of what the comprehension process starts with. Where comprehension &amp;quot;ends&amp;quot; is quite another matter. If we go back to some of the early comprehension systems, the end point of the process was an action, and there was linguistic processing at every stage (Winograd 1972). Some researchers, this author included, take the end point to be an elaboration of an already existing semantic model whereby some new individuals are added and new relations established between them and other individuals (Martin and Riesbeck 1986; McDonald 1992a). Today&apos;s dominant paradigm, however, stemming perhaps from the predominance of research on question-answering and following the lead of theoretical linguistics, is to take the end point to be a logical form: an expression that codifies the information in the text at a fairly shallow level, e.g., a first-order formula with content words mapped to predicates with the same spelling, and with individuals represented by quantified variables or constants. * 14 Brantwood Road, Arlington, MA 02174-8004; mcdonald@cs.brandeis.edu C) 1993 Association for Computational Linguistics Comput</context>
</contexts>
<marker>Martin, Riesbeck, 1986</marker>
<rawString>Martin, Charles, and Riesbeck, Chris (1986). &amp;quot;Uniform parsing and inference for learning.&amp;quot; In Proceedings, AAAI-86. Philadelphia, PA. Morgan-Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McDonald</author>
</authors>
<title>Reversible NLP by deriving the grammars from the knowledge base.&amp;quot;</title>
<date>1993</date>
<booktitle>In Reversible Grammar in Natural Language Processing.</booktitle>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="5262" citStr="McDonald 1993" startWordPosition="801" endWordPosition="802">process begins. Thus just as the psycholinguistically correct source for generation is still very much a matter of research (as it is even when the source is a computational object in a well-designed AT system), so too is the end-point of comprehension, and by implication the division of that process into components and representational levels. A declarative, reversible, form–meaning mapping does not ipso facto have to start/end at the level of logical form, but can originate at a much deeper level with the class definitions of the object types and relations of the speaker&apos;s conceptual model (McDonald 1993). Considered in isolation, the production of text from a logical form is, quite frankly, trivial. It corresponds to the final &amp;quot;readout&amp;quot; phase of McDonald, Meteer, and Pustejovsky (1987), since all that remains to be done is to linearize its elements in keeping with the constraints of a surface grammar, carry out the trivial mapping to the phonetic (orthographic) forms of the words implicit in the predicates, and add the requisite grammatical function words and morphemes. This capability has been an established part of the state of the art for well over twenty years (see, e.g., Webber [1971], w</context>
</contexts>
<marker>McDonald, 1993</marker>
<rawString>McDonald, David (1993). &amp;quot;Reversible NLP by deriving the grammars from the knowledge base.&amp;quot; In Reversible Grammar in Natural Language Processing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McDonald</author>
</authors>
<title>An efficient chart-based algorithm for partial-parsing of unrestricted texts.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, 3rd Conference on Applied Natural Language Processing (ACL).</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="2614" citStr="McDonald 1992" startWordPosition="405" endWordPosition="406">ences vs. corpus study, written vs. spoken), but these aside there is no question of what the comprehension process starts with. Where comprehension &amp;quot;ends&amp;quot; is quite another matter. If we go back to some of the early comprehension systems, the end point of the process was an action, and there was linguistic processing at every stage (Winograd 1972). Some researchers, this author included, take the end point to be an elaboration of an already existing semantic model whereby some new individuals are added and new relations established between them and other individuals (Martin and Riesbeck 1986; McDonald 1992a). Today&apos;s dominant paradigm, however, stemming perhaps from the predominance of research on question-answering and following the lead of theoretical linguistics, is to take the end point to be a logical form: an expression that codifies the information in the text at a fairly shallow level, e.g., a first-order formula with content words mapped to predicates with the same spelling, and with individuals represented by quantified variables or constants. * 14 Brantwood Road, Arlington, MA 02174-8004; mcdonald@cs.brandeis.edu C) 1993 Association for Computational Linguistics Computational Linguis</context>
</contexts>
<marker>McDonald, 1992</marker>
<rawString>McDonald, David (1992a). &amp;quot;An efficient chart-based algorithm for partial-parsing of unrestricted texts.&amp;quot; In Proceedings, 3rd Conference on Applied Natural Language Processing (ACL). Trento, Italy, 193-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McDonald</author>
</authors>
<title>Type-driven suppression of redundancy in the generation of inference-rich reports.&amp;quot;</title>
<date>1992</date>
<journal>In Aspects of Automated Natural Language Generation, (Springer Verlag Lecture Notes in Al, Number</journal>
<volume>587</volume>
<pages>73--88</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="2614" citStr="McDonald 1992" startWordPosition="405" endWordPosition="406">ences vs. corpus study, written vs. spoken), but these aside there is no question of what the comprehension process starts with. Where comprehension &amp;quot;ends&amp;quot; is quite another matter. If we go back to some of the early comprehension systems, the end point of the process was an action, and there was linguistic processing at every stage (Winograd 1972). Some researchers, this author included, take the end point to be an elaboration of an already existing semantic model whereby some new individuals are added and new relations established between them and other individuals (Martin and Riesbeck 1986; McDonald 1992a). Today&apos;s dominant paradigm, however, stemming perhaps from the predominance of research on question-answering and following the lead of theoretical linguistics, is to take the end point to be a logical form: an expression that codifies the information in the text at a fairly shallow level, e.g., a first-order formula with content words mapped to predicates with the same spelling, and with individuals represented by quantified variables or constants. * 14 Brantwood Road, Arlington, MA 02174-8004; mcdonald@cs.brandeis.edu C) 1993 Association for Computational Linguistics Computational Linguis</context>
</contexts>
<marker>McDonald, 1992</marker>
<rawString>McDonald, David (1992b). &amp;quot;Type-driven suppression of redundancy in the generation of inference-rich reports.&amp;quot; In Aspects of Automated Natural Language Generation, (Springer Verlag Lecture Notes in Al, Number 587), edited by Dale, Hovy, Rosner, and Stock, 73-88. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie</author>
<author>James Pustejovsky</author>
</authors>
<title>Factors contributing to efficiency in natural language generation.&amp;quot;</title>
<date>1987</date>
<booktitle>In Natural Language Generation: Recent Advances in Artificial Intelligence, Psychology, and Linguistics, edited by Kempen,</booktitle>
<pages>159--181</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Marie, Pustejovsky, 1987</marker>
<rawString>McDonald, David; Meteer (Vaughan), Marie; and Pustejovsky, James (1987). &amp;quot;Factors contributing to efficiency in natural language generation.&amp;quot; In Natural Language Generation: Recent Advances in Artificial Intelligence, Psychology, and Linguistics, edited by Kempen, 159-181. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Meteer</author>
</authors>
<title>Expressibility and the Problem of Efficient Text Planning.</title>
<date>1992</date>
<publisher>Pinter Publishers.</publisher>
<contexts>
<context position="613" citStr="Meteer 1992" startWordPosition="96" endWordPosition="97">sues in the Choice of a Source for Natural Language Generation David D. McDonald* Brandeis University The most vexing question in natural language generation is &apos;what is the source&apos;— what do speakers start from when they begin to compose an utterance? Theories of generation in the literature differ markedly in their assumptions. A few start with an unanalyzed body of numerical data (e.g. Bourbeau et al. 1990; Kukich 1988). Most start with the structured objects that are used by a particular reasoning system or simulator and are cast in that system&apos;s representational formalism (e.g. Hovy 1990; Meteer 1992; Wisner 1988). A growing number of systems, largely focused on problems in machine translation or grammatical theory, take their input to be logical formulae based on lexical predicates (e.g. Wedekind 1988; Shieber et al. 1990). The lack of a consistent answer to the question of the generator&apos;s source has been at the heart of the problem of how to make research on generation intelligible and engaging for the rest of the computational linguistics community, and has complicated efforts to evaluate alternative treatments even for people in the field. Nevertheless, a source cannot be imposed by f</context>
<context position="7221" citStr="Meteer 1992" startWordPosition="1108" endWordPosition="1109">s (an especially important task when the source is raw data, e.g., precisely what points of the compass make the wind &amp;quot;easterly,&amp;quot; [Bourbeau et al. 1990]); selecting the information to include in the utterance and deciding whether it should be stated explicitly or left for inference; distributing the information into sentences and giving it an organization that reflects the intended rhetorical force, coherence, and necessary cohesion given the prior discourse; and finding a mapping of the information to linguistic resources that is collectively expressible (i.e., has a surface realization; see Meteer 1992). How one chooses to approach these tasks has substantial implications for the kinds of structures that a surface realization process can sensibly be given as input and may not be taken for granted. As a consequence, any generation architecture that is proposed without including an articulation 192 David D. McDonald Source for Natural Language Generation of the early stages of the process is issuing a large promissory note that it may not be able to redeem. Often the choice of a two-component process in comprehension (and by extension in generation) is based on the judgment that linguistic kno</context>
</contexts>
<marker>Meteer, 1992</marker>
<rawString>Meteer, Marie (1992). Expressibility and the Problem of Efficient Text Planning. Pinter Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Mellish</author>
</authors>
<title>Computer Interpretation of Natural Language Descriptions.</title>
<date>1985</date>
<publisher>John Wiley.</publisher>
<contexts>
<context position="10268" citStr="Mellish 1985" startWordPosition="1587" endWordPosition="1588"> for the generator to be a logical form is the very fact that it is represented as a single expression in a linear notation. This may seem a small matter of notation, but the computational properties of a logical form as it is usually represented give it a very low notational efficiency in generation (see Woods [1986] for a discussion of this notion). These include the simple fact that expressions must be scanned and parsed before the information they contain can be deployed, the lack of decompositional locality because of the use of scoping quantifiers and variables to represent individuals (Mellish 1985), and, indeed, as Shieber (this issue) points out, there is the question of the formula&apos;s intended structural realization, since the logical connectives that link a formula&apos;s terms underspecify their corresponding syntactic constructions because of the equivalence of other formulas under commutativity, associativity, and other truth-preserving logical transformations. The force of much of Shieber&apos;s argument in regard to logical-form equivalence rests on the constraint imposed by bi-directional processing. If the choice of the information that an utterance is to express is made by a component w</context>
</contexts>
<marker>Mellish, 1985</marker>
<rawString>Mellish, Chris (1985). Computer Interpretation of Natural Language Descriptions. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deitmar Rosner</author>
</authors>
<title>The generation system of the SEMSYN project: Towards a task-independent generator for German.&amp;quot; In Advances in Natural Language Generation, edited by Zock and Sabah.</title>
<date>1988</date>
<publisher>Pinter Publishers.</publisher>
<marker>Rosner, 1988</marker>
<rawString>Rosner, Deitmar (1988). &amp;quot;The generation system of the SEMSYN project: Towards a task-independent generator for German.&amp;quot; In Advances in Natural Language Generation, edited by Zock and Sabah. Pinter Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Gertjan van Noord</author>
<author>Fernando Pereira</author>
<author>Robert Moore</author>
</authors>
<title>Semantic-head-driven generation.&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>30--42</pages>
<marker>Shieber, van Noord, Pereira, Moore, 1990</marker>
<rawString>Shieber, Stuart; van Noord, Gertjan; Pereira, Fernando; and Moore, Robert (1990). &amp;quot;Semantic-head-driven generation.&amp;quot; Computational Linguistics, 16(1), 30-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>The problem of logical-form equivalence.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>179--190</pages>
<marker>Shieber, 1993</marker>
<rawString>Shieber, Stuart (1993). &amp;quot;The problem of logical-form equivalence.&amp;quot; Computational Linguistics, 19(1), 179-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
</authors>
<title>The case for generation.&amp;quot; In Papers Presented at the Seminar in Mathematical Linguistics, Volume XIII, edited by Woods.</title>
<date>1971</date>
<institution>Aiken Computer Laboratory, Department of Linguistics, Harvard University,</institution>
<location>Cambridge, MA.</location>
<marker>Webber, 1971</marker>
<rawString>Webber, Bonnie (1971). &amp;quot;The case for generation.&amp;quot; In Papers Presented at the Seminar in Mathematical Linguistics, Volume XIII, edited by Woods. Aiken Computer Laboratory, Department of Linguistics, Harvard University, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Wedekind</author>
</authors>
<title>Generation as structure driven derivation.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 13th International Conference on Computational Linguistics (COLING-88).</booktitle>
<pages>732--737</pages>
<location>Budapest, Hungary,</location>
<contexts>
<context position="819" citStr="Wedekind 1988" startWordPosition="127" endWordPosition="128">t from when they begin to compose an utterance? Theories of generation in the literature differ markedly in their assumptions. A few start with an unanalyzed body of numerical data (e.g. Bourbeau et al. 1990; Kukich 1988). Most start with the structured objects that are used by a particular reasoning system or simulator and are cast in that system&apos;s representational formalism (e.g. Hovy 1990; Meteer 1992; Wisner 1988). A growing number of systems, largely focused on problems in machine translation or grammatical theory, take their input to be logical formulae based on lexical predicates (e.g. Wedekind 1988; Shieber et al. 1990). The lack of a consistent answer to the question of the generator&apos;s source has been at the heart of the problem of how to make research on generation intelligible and engaging for the rest of the computational linguistics community, and has complicated efforts to evaluate alternative treatments even for people in the field. Nevertheless, a source cannot be imposed by fiat. Differences in what information is assumed to be available, its relative decomposition when compared to the &amp;quot;packaging&amp;quot; available in the words or syntactic constructions of the language (linguistic res</context>
</contexts>
<marker>Wedekind, 1988</marker>
<rawString>Wedekind, Jurgen (1988). &amp;quot;Generation as structure driven derivation.&amp;quot; In Proceedings, 13th International Conference on Computational Linguistics (COLING-88). Budapest, Hungary, 732-737.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="2350" citStr="Winograd 1972" startWordPosition="365" endWordPosition="366">en come precisely through insights into the representation of the source. Language comprehension research does not have this problem—its source is a text. Differences in methodology govern where this text comes from (e.g., single sentence vs. discourse, sample sentences vs. corpus study, written vs. spoken), but these aside there is no question of what the comprehension process starts with. Where comprehension &amp;quot;ends&amp;quot; is quite another matter. If we go back to some of the early comprehension systems, the end point of the process was an action, and there was linguistic processing at every stage (Winograd 1972). Some researchers, this author included, take the end point to be an elaboration of an already existing semantic model whereby some new individuals are added and new relations established between them and other individuals (Martin and Riesbeck 1986; McDonald 1992a). Today&apos;s dominant paradigm, however, stemming perhaps from the predominance of research on question-answering and following the lead of theoretical linguistics, is to take the end point to be a logical form: an expression that codifies the information in the text at a fairly shallow level, e.g., a first-order formula with content w</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Winograd, Terry (1972). Understanding Natural Language. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Woods</author>
</authors>
<title>Important issues in knowledge representation.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, IEEE.</booktitle>
<volume>74</volume>
<issue>10</issue>
<marker>Woods, 1986</marker>
<rawString>Woods, William (1986). &amp;quot;Important issues in knowledge representation.&amp;quot; In Proceedings, IEEE. 74(10).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>