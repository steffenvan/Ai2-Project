<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.888515">
A Class of Submodular Functions for Document Summarization
</title>
<author confidence="0.987404">
Hui Lin
</author>
<affiliation confidence="0.997444">
Dept. of Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.951345">
Seattle, WA 98195, USA
</address>
<email confidence="0.999437">
hlin@ee.washington.edu
</email>
<author confidence="0.992774">
Jeff Bilmes
</author>
<affiliation confidence="0.998302">
Dept. of Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.783588">
Seattle, WA 98195, USA
</address>
<email confidence="0.999417">
bilmes@ee.washington.edu
</email>
<sectionHeader confidence="0.994812" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996">
We design a class of submodular functions
meant for document summarization tasks.
These functions each combine two terms,
one which encourages the summary to be
representative of the corpus, and the other
which positively rewards diversity. Critically,
our functions are monotone nondecreasing
and submodular, which means that an efficient
scalable greedy optimization scheme has
a constant factor guarantee of optimality.
When evaluated on DUC 2004-2007 corpora,
we obtain better than existing state-of-art
results in both generic and query-focused
document summarization. Lastly, we show
that several well-established methods for
document summarization correspond, in fact,
to submodular function optimization, adding
further evidence that submodular functions are
a natural fit for document summarization.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999798">
In this paper, we address the problem of generic and
query-based extractive summarization from collec-
tions of related documents, a task commonly known
as multi-document summarization. We treat this task
as monotone submodular function maximization (to
be defined in Section 2). This has a number of criti-
cal benefits. On the one hand, there exists a simple
greedy algorithm for monotone submodular func-
tion maximization where the summary solution ob-
tained (say S) is guaranteed to be almost as good
as the best possible solution (say 5opt) according to
an objective F. More precisely, the greedy algo-
rithm is a constant factor approximation to the car-
dinality constrained version of the problem, so that
</bodyText>
<page confidence="0.559102">
510
</page>
<bodyText confidence="0.992523833333333">
F(S) &gt; (1 − 1/e)F(5opt) Pz� 0.632F(5opt). This
is particularly attractive since the quality of the so-
lution does not depend on the size of the problem,
so even very large size problems do well. It is also
important to note that this is a worst case bound, and
in most cases the quality of the solution obtained will
be much better than this bound suggests.
Of course, none of this is useful if the objective
function F is inappropriate for the summarization
task. In this paper, we argue that monotone nonde-
creasing submodular functions F are an ideal class of
functions to investigate for document summarization.
We show, in fact, that many well-established methods
for summarization (Carbonell and Goldstein, 1998;
Filatova and Hatzivassiloglou, 2004; Takamura and
Okumura, 2009; Riedhammer et al., 2010; Shen and
Li, 2010) correspond to submodular function opti-
mization, a property not explicitly mentioned in these
publications. We take this fact, however, as testament
to the value of submodular functions for summariza-
tion: if summarization algorithms are repeatedly de-
veloped that, by chance, happen to be an instance of a
submodular function optimization, this suggests that
submodular functions are a natural fit. On the other
hand, other authors have started realizing explicitly
the value of submodular functions for summarization
(Lin and Bilmes, 2010; Qazvinian et al., 2010).
Submodular functions share many properties in
common with convex functions, one of which is that
they are closed under a number of common combi-
nation operations (summation, certain compositions,
restrictions, and so on). These operations give us the
tools necessary to design a powerful submodular ob-
jective for submodular document summarization that
extends beyond any previous work. We demonstrate
this by carefully crafting a class of submodular func-
</bodyText>
<note confidence="0.962039">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 510–520,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999973666666667">
tions we feel are ideal for extractive summarization
tasks, both generic and query-focused. In doing so,
we demonstrate better than existing state-of-the-art
performance on a number of standard summarization
evaluation tasks, namely DUC-04 through to DUC-
07. We believe our work, moreover, might act as a
springboard for researchers in summarization to con-
sider the problem of “how to design a submodular
function” for the summarization task.
In Section 2, we provide a brief background on sub-
modular functions and their optimization. Section 3
describes how the task of extractive summarization
can be viewed as a problem of submodular function
maximization. We also in this section show that many
standard methods for summarization are, in fact, al-
ready performing submodular function optimization.
In Section 4, we present our own submodular func-
tions. Section 5 presents results on both generic and
query-focused summarization tasks, showing as far
as we know the best known ROUGE results for DUC-
04 through DUC-06, and the best known precision
results for DUC-07, and the best recall DUC-07 re-
sults among those that do not use a web search engine.
Section 6 discusses implications for future work.
</bodyText>
<sectionHeader confidence="0.8371" genericHeader="method">
2 Background on Submodularity
</sectionHeader>
<bodyText confidence="0.986569677966102">
We are given a set of objects V = {v1, ... , v�} and a
function F : 2V —* R that returns a real value for any
subset S C_ V . We are interested in finding the subset
of bounded size |S |G k that maximizes the function,
e.g., argmaxScV F(S). In general, this operation
is hopelessly intractable, an unfortunate fact since
the optimization coincides with many important ap-
plications. For example, F might correspond to the
value or coverage of a set of sensor locations in an
environment, and the goal is to find the best locations
for a fixed number of sensors (Krause et al., 2008).
If the function F is monotone submodular then the
maximization is still NP complete, but it was shown
in (Nemhauser et al., 1978) that a greedy algorithm
finds an approximate solution guaranteed to be within
e−1
e — 0.63 of the optimal solution, as mentioned
in Section 1. A version of this algorithm (Minoux,
1978), moreover, scales to very large data sets. Sub-
modular functions are those that satisfy the property
of diminishing returns: for any A C_ B C_ V \v, a sub-
modular function F must satisfy F(A+v)—F(A) &gt;
F(B + v) — F(B). That is, the incremental “value”
of v decreases as the context in which v is considered
grows from A to B. An equivalent definition, useful
mathematically, is that for any A, B C_ V , we must
have that F(A) + F(B) &gt; F(A U B) + F(A n B).
If this is satisfied everywhere with equality, then
the function F is called modular, and in such case
F(A) = c + EaEA ~fa for a sized |V  |vector f~ of
real values and constant c. A set function F is mono-
tone nondecreasing if VA C_ B, F(A) G F(B). As
shorthand, in this paper, monotone nondecreasing
submodular functions will simply be referred to as
monotone submodular.
Historically, submodular functions have their roots
in economics, game theory, combinatorial optimiza-
tion, and operations research. More recently, submod-
ular functions have started receiving attention in the
machine learning and computer vision community
(Kempe et al., 2003; Narasimhan and Bilmes, 2005;
Krause and Guestrin, 2005; Narasimhan and Bilmes,
2007; Krause et al., 2008; Kolmogorov and Zabin,
2004) and have recently been introduced to natural
language processing for the tasks of document sum-
marization (Lin and Bilmes, 2010) and word align-
ment (Lin and Bilmes, 2011).
Submodular functions share a number of proper-
ties in common with convex and concave functions
(Lov´asz, 1983), including their wide applicability,
their generality, their multiple options for their repre-
sentation, and their closure under a number of com-
mon operators (including mixtures, truncation, com-
plementation, and certain convolutions). For exam-
ple, if a collection of functions {FZ}Z is submodular,
then so is their weighted sum F = EZ αZFZ where
αZ are nonnegative weights. It is not hard to show
that submodular functions also have the following
composition property with concave functions:
</bodyText>
<construct confidence="0.9256992">
Theorem 1. Given functions F : 2V —* R and
f : R —* R, the composition F&apos; = f o F : 2V —* R
(i.e., F�(S) = f(F(S))) is nondecreasing sub-
modular, if f is non-decreasing concave and F is
nondecreasing submodular.
</construct>
<bodyText confidence="0.8693665">
This property will be quite useful when defining sub-
modular functions for document summarization.
</bodyText>
<page confidence="0.998132">
511
</page>
<sectionHeader confidence="0.987088" genericHeader="method">
3 Submodularity in Summarization
</sectionHeader>
<subsectionHeader confidence="0.999992">
3.1 Summarization with knapsack constraint
</subsectionHeader>
<bodyText confidence="0.996743833333333">
Let the ground set V represents all the sentences
(or other linguistic units) in a document (or docu-
ment collection, in the multi-document summariza-
tion case). The task of extractive document sum-
marization is to select a subset S ⊆ V to represent
the entirety (ground set V ). There are typically con-
straints on S, however. Obviously, we should have
|S |&lt; |V  |= N as it is a summary and should
be small. In standard summarization tasks (e.g.,
DUC evaluations), the summary is usually required
to be length-limited. Therefore, constraints on S
can naturally be modeled as knapsack constraints:
</bodyText>
<equation confidence="0.812418">
E
iES ci ≤ b, where ci is the non-negative cost of
selecting unit i (e.g., the number of words in the sen-
tence) and b is our budget. If we use a set function
F : 2V → R to measure the quality of the summary
</equation>
<bodyText confidence="0.624142">
set S, the summarization problem can then be for-
malized as the following combinatorial optimization
problem:
</bodyText>
<equation confidence="0.9854925">
Problem 1. Find
�
F(S) subject to:
iES
</equation>
<bodyText confidence="0.998006242424243">
Since this is a generalization of the cardinality
constraint (where ci = 1, ∀i), this also constitutes
a (well-known) NP-hard problem. In this case as
well, however, a modified greedy algorithm with par-
tial enumeration can solve Problem 1 near-optimally
with (1−1/e)-approximation factor if F is monotone
submodular (Sviridenko, 2004). The partial enumer-
ation, however, is too computationally expensive for
real world applications. In (Lin and Bilmes, 2010),
we generalize the work by Khuller et al. (1999) on
the budgeted maximum cover problem to the gen-
eral submodular framework, and show a practical
greedy algorithm with a (1 − 1/√e)-approximation
factor, where each greedy step adds the unit with the
largest ratio of objective function gain to scaled cost,
while not violating the budget constraint (see (Lin
and Bilmes, 2010) for details). Note that in all cases,
submodularity and monotonicity are two necessary
ingredients to guarantee that the greedy algorithm
gives near-optimal solutions.
In fact, greedy-like algorithms have been widely
used in summarization. One of the more popular
approaches is maximum marginal relevance (MMR)
(Carbonell and Goldstein, 1998), where a greedy
algorithm selects the most relevant sentences, and
at the same time avoids redundancy by removing
sentences that are too similar to ones already selected.
Interestingly, the gain function defined in the original
MMR paper (Carbonell and Goldstein, 1998) satisfies
diminishing returns, a fact apparently unnoticed until
now. In particular, Carbonell and Goldstein (1998)
define an objective function gain of adding element
k to set S (k ∈/ S) as:
</bodyText>
<equation confidence="0.9726045">
λSim1(sk, q) − (1 − λ) max Sim2(si, sk), (1)
iES
</equation>
<bodyText confidence="0.99268275">
where Sim1(sk, q) measures the similarity between
unit sk to a query q, Sim2(si, sk) measures the simi-
larity between unit si and unit sk, and 0 ≤ λ ≤ 1 is
a trade-off coefficient. We have:
</bodyText>
<equation confidence="0.789151333333333">
Theorem 2. Given an expression for FMMR such that
FMMR(S ∪ {k}) −FMMR(S) is equal to Eq. 1, FMMR
is non-monotone submodular.
Obviously, diminishing-returns hold since
Sim2(si, sk) ≤ max
iER
</equation>
<bodyText confidence="0.999919647058824">
for all S ⊆ R, and therefore FMMR is submodular.
On the other hand, FMMR, would not be monotone, so
the greedy algorithm’s constant-factor approximation
guarantee does not apply in this case.
When scoring a summary at the sub-sentence
level, submodularity naturally arises. Concept-based
summarization (Filatova and Hatzivassiloglou, 2004;
Takamura and Okumura, 2009; Riedhammer et al.,
2010; Qazvinian et al., 2010) usually maximizes the
weighted credit of concepts covered by the summary.
Although the authors may not have noticed, their ob-
jective functions are also submodular, adding more
evidence suggesting that submodularity is natural for
summarization tasks. Indeed, let S be a subset of
sentences in the document and denote F(S) as the
set of concepts contained in S. The total credit of the
concepts covered by S is then
</bodyText>
<equation confidence="0.944029666666667">
o
Fconcept(S) E
iEr(S)
</equation>
<bodyText confidence="0.997195">
where ci is the credit of concept i. This function is
known to be submodular (Narayanan, 1997).
</bodyText>
<figure confidence="0.575077333333333">
S* ∈ argmax
SCV
max
ci ≤ b. iES
Sim2(si, sk)
ci,
</figure>
<page confidence="0.981301">
512
</page>
<bodyText confidence="0.999967142857143">
Similar to the MMR approach, in (Lin and Bilmes,
2010), a submodular graph based objective function
is proposed where a graph cut function, measuring
the similarity of the summary to the rest of document,
is combined with a subtracted redundancy penalty
function. The objective function is submodular but
again, non-monotone. We theoretically justify that
the performance guarantee of the greedy algorithm
holds for this objective function with high probability
(Lin and Bilmes, 2010). Our justification, however,
is shown to be applicable only to certain particular
non-monotone submodular functions, under certain
reasonable assumptions about the probability distri-
bution over weights of the graph.
</bodyText>
<subsectionHeader confidence="0.999963">
3.2 Summarization with covering constraint
</subsectionHeader>
<bodyText confidence="0.9996966">
Another perspective is to treat the summarization
problem as finding a low-cost subset of the document
under the constraint that a summary should cover
all (or a sufficient amount of) the information in the
document. Formally, this can be expressed as
</bodyText>
<equation confidence="0.956113">
Problem 2. Find
1: S� E argmin ci subject to: F(S) &gt; a,
SCV iES
</equation>
<bodyText confidence="0.987216533333333">
where ci are the element costs, and set function F(S)
measure the information covered by S. When F
is submodular, the constraint F(S) &gt; a is called
a submodular cover constraint. When F is mono-
tone submodular, a greedy algorithm that iteratively
selects k with minimum ck/(F(S U {k}) − F(S))
has approximation guarantees (Wolsey, 1982). Re-
cent work (Shen and Li, 2010) proposes to model
document summarization as finding a minimum dom-
inating set and a greedy algorithm is used to solve
the problem. The dominating set constraint is also
a submodular cover constraint. Define S(S) be the
set of elements that is either in S or is adjacent to
some element in S. Then S is a dominating set if
|S(S) |= |V |. Note that
</bodyText>
<equation confidence="0.451275">
Fdom(S) g |S(S)|
</equation>
<bodyText confidence="0.999892538461538">
is monotone submodular. The dominating set
constraint is then also a submodular cover constraint,
and therefore the approaches in (Shen and Li, 2010)
are special cases of Problem 2. The solutions found
in this framework, however, do not necessarily
satisfy a summary’s budget constraint. Consequently,
a subset of the solution found by solving Problem 2
has to be constructed as the final summary, and the
near-optimality is no longer guaranteed. Therefore,
solving Problem 1 for document summarization
appears to be a better framework regarding global
optimality. In the present paper, our framework is
that of Problem 1.
</bodyText>
<subsectionHeader confidence="0.999648">
3.3 Automatic summarization evaluation
</subsectionHeader>
<bodyText confidence="0.982594095238095">
Automatic evaluation of summary quality is impor-
tant for the research of document summarization as
it avoids the labor-intensive and potentially inconsis-
tent human evaluation. ROUGE (Lin, 2004) is widely
used for summarization evaluation and it has been
shown that ROUGE-N scores are highly correlated
with human evaluation (Lin, 2004). Interestingly,
ROUGE-N is monotone submodular, adding further
evidence that monotone submodular functions are
natural for document summarization.
Theorem 3. ROUGE-N is monotone submodular.
Proof. By definition (Lin, 2004), ROUGE-N is the
n-gram recall between a candidate summary and a
set of reference summaries. Precisely, let S be the
candidate summary (a set of sentences extracted from
the ground set V ), ce : 2V —* Z+ be the number of
times n-gram a occurs in summary S, and Ri be the
set of n-grams contained in the reference summary i
(suppose we have K reference summaries, i.e., i =
1, · · · , K). Then ROUGE-N can be written as the
following set function:
</bodyText>
<equation confidence="0.973639333333333">
EK p1 EeERi min(ce(S), re,i)
ROUGE-N(S) = K
Ei=1 EeERi re,i
</equation>
<bodyText confidence="0.968851538461538">
where re,i is the number of times n-gram a occurs
in reference summary i. Since ce(S) is monotone
modular and min(x, a) is a concave non-decreasing
function of x, min(ce(S), re,i) is monotone sub-
modular by Theorem 1. Since summation preserves
submodularity, and the denominator is constant, we
see that FROUGE-N is monotone submodular.
Since the reference summaries are unknown, it is
of course impossible to optimize FROUGE-N directly.
Therefore, some approaches (Filatova and Hatzivas-
siloglou, 2004; Takamura and Okumura, 2009; Ried-
hammer et al., 2010) instead define “concepts”. Alter-
,
</bodyText>
<page confidence="0.69711">
513
</page>
<bodyText confidence="0.99972068">
natively, we herein propose a class of monotone sub- the property of diminishing returns. Indeed, Shan-
modular functions that naturally models the quality of non entropy, as the measurement of information, is
a summary while not depending on an explicit notion another well-known monotone submodular function.
of concepts, as we will see in the following section. There are several ways to define L(S) in our
4 Monotone Submodular Objectives context. For instance, we could use L(S) =
Two properties of a good summary are relevance and P
non-redundancy. Objective functions for extractive iEV,jES wi,j where wi,j represents the similarity
summarization usually measure these two separately between i and j. L(S) could also be facility
and then mix them together trading off encouraging location objective, i.e., L(S) = PiEV maxjES wi,j,
relevance and penalizing redundancy. The redun- as used in (Lin et al., 2009). We could also use
dancy penalty usually violates the monotonicity of L(S) = PiEr(S) ci as used in concept-based
the objective functions (Carbonell and Goldstein, summarization, where the definition of “concept”
1998; Lin and Bilmes, 2010). We therefore propose and the mechanism to extract these concepts become
to positively reward diversity instead of negatively important. All of these are monotone submodular.
penalizing redundancy. In particular, we model the Alternatively, in this paper we propose the follow-
summary quality as ing objective that does not reply on concepts. Let
F(S) = L(S) + AR(S), (2) L(S) = X min {Ci(S), a Ci(V )} , (3)
where L(S) measures the coverage, or “fidelity”, iEV
of summary set S to the document, R(S) rewards where Ci : 2V —* R is a monotone submodular func-
diversity in S, and A &gt; 0 is a trade-off coefficient. tion and 0 &lt; a &lt; 1 is a threshold co-efficient. Firstly,
Note that the above is analogous to the objectives L(S) as defined in Eqn. 3 is a monotone submodular
widely used in machine learning, where a loss function. The monotonicity is immediate. To see that
function that measures the training set error (we L(S) is submodular, consider the fact that f(x) =
measure the coverage of summary to a document), min(x, a) where a &gt; 0 is a concave non-decreasing
is combined with a regularization term encouraging function, and by Theorem 1, each summand in Eqn. 3
certain desirable (e.g., sparsity) properties (in is a submodular function, and as summation pre-
our case, we “regularize” the solution to be more serves submodularity, L(S) is submodular.
diverse). In the following, we discuss how both L(S) Next, we explain the intuition behind Eqn. 3. Basi-
and R(S) are naturally monotone submodular. cally, Ci(S) measures how similar S is to element i,
4.1 Coverage function or how much of i is “covered” by S. Then Ci(V ) is
L(S) can be interpreted either as a set function that just the largest value that Ci(S) can achieve. We call
measures the similarity of summary set S to the docu- i “saturated” by S when min{Ci(S), aCi(V )} =
ment to be summarized, or as a function representing aCi(V ). When i is already saturated in this way,
some form of “coverage” of V by S. Most naturally, any new sentence j can not further improve the
L(S) should be monotone, as coverage improves coverage of i even if it is very similar to i (i.e.,
with a larger summary. L(S) should also be submod- Ci(S U {j}) − Ci(S) is large). This will give other
ular: consider adding a new sentence into two sum- sentences that are not yet saturated a higher chance
mary sets, one a subset of the other. Intuitively, the of being better covered, and therefore the resulting
increment when adding a new sentence to the small summary tends to better cover the entire document.
summary set should be larger than the increment One simple way to define Ci(S) is just to use
when adding it to the larger set, as the information Ci(S) = X wi,j (4)
carried by the new sentence might have already been jES
covered by those sentences that are in the larger sum- where wi,j &gt; 0 measures the similarity between i
mary but not in the smaller summary. This is exactly and j. In this case, when a = 1, Eqn. 3 reduces
514 to the case where L(S) = PiEV,jES wi,j. As we
will see in Section 5, having an a that is less than
1 significantly improves the performance compared
to the case when α = 1, which coincides with our
intuition that using a truncation threshold improves
the final summary’s coverage.
</bodyText>
<subsectionHeader confidence="0.990197">
4.2 Diversity reward function
</subsectionHeader>
<bodyText confidence="0.999964">
Instead of penalizing redundancy by subtracting from
the objective, we propose to reward diversity by
adding the following to the objective:
</bodyText>
<equation confidence="0.951882428571429">
K
1:r3
i=1 j1:S&apos;r3
where
i = 1,
K is a partition of the ground
set V (i.e.,
</equation>
<bodyText confidence="0.8691496">
= V and the Pis are disjoint) into
separate clusters, and
&gt; 0 indicates the singleton
reward of i (i.e., the reward of adding i into the empty
set). The value
estimates the importance of i to
the summary. The function
rewards diversity
in that there is usually more benefit to selecting a
sentence from a cluster not yet having one of its
elements already chosen. As soon as an element
is selected from a cluster, other elements from the
same cluster start having diminishing gain, thanks
to the square root function. For instance, consider
the case where
</bodyText>
<equation confidence="0.9945148">
k2 E
k3 E P2, and
= 4,
rk2 = 9, and rk3 = 4. Assume
is already in the
</equation>
<bodyText confidence="0.929550615384615">
summary set S. Greedily selecting the next element
will choose k3 rather than k2 since
&lt; 2 + 2. In
other words, adding k3 achieves a greater reward as it
increases the diversity of the summary (by choosing
from a different cluster). Note,
is distinct from
in that
might wish to include certain
outlier material that
could ignore.
It is easy to show that
is submodular by
using the composition rule from Theorem 1. The
square root is non-decreasing concave function.
Inside each square root lies a modular function
with non-negative weights (and thus is monotone).
Applying the square root to such a monotone sub-
modular function yields a submodular function, and
summing them all together retains submodularity, as
mentioned in Section 2. The
of
is straightforward. Note, the form of Eqn. 5 is similar
to structured group norms (e.g., (Zhao et al., 2009)),
recently shown to be related to submodularity (Bach,
2010; Jegelka an
</bodyText>
<equation confidence="0.971609142857143">
Pi,
···
UiPi
ri
ri
R(S)
k1,
P1,
rk1
k1
√13
R(S)
G(S)
R(S)
G(S)
R(S)
monotonicity
R(S)
d Bilmes, 2011).
f(x)
R(S),
</equation>
<bodyText confidence="0.968881475">
Hakkani-t¨ur,
d submodularity.
515 Several extensions to Eqn. 5 are discussed
next: First, instead of using a ground set partition,
intersecting clusters can be used. Second, the
square root function in Eqn. 5 can be replaced with
any other non-decreasing concave functions (e.g.,
= log(1 + x)) while preserving the desired
property of
and the curvature of the concave
function then determines the rate that the reward
diminishes. Last, multi-resolution clustering (or
partitions) with different sizes (K) can be used, i.e.,
we can use a mixture of components, each of which
has the structure of Eqn. 5. A mixture can better
represent the core structure of the ground set (e.g.,
the hierarchical structure in the documents (Celiky-
ilmaz and
2010)). All such extensions
preserve both monotonicity an
was the main forum
providing benchmarks for researchers working
on document summarization. The tasks in DUC
evolved from single-document summarization to
multi-document summarization, and from generic
summarization
to query-focused sum-
marization
As ROUGE (Lin, 2004)
has been officially adopted for DUC evaluations
since 2004, we also take it as our main evaluation
criterion. We evaluated our approaches on DUC
data 2003-2007, and demonstrate results on both
generic and query-focused summarization. In all
experiments, the modified greedy algorithm (Lin and
Bilmes, 2010) was used for summary
(http://duc.nist.org)
(2001–2004)
(2005–2007).
generation.
</bodyText>
<subsectionHeader confidence="0.973661">
5.1 Generic summarization
</subsectionHeader>
<bodyText confidence="0.8626926">
our development set, an
d tested on DUC-04 data.
We show ROUGE-1 scores1 as it was the main
evaluation criterion for DUC-03, 04 evaluations.
(5)
</bodyText>
<sectionHeader confidence="0.969916" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.983601">
The document understanding conference (DUC)
Summarization tasks in DUC-03 and DUC-04 are
multi-document summarization on English news
articles. In each task, 50 document clusters are
given, each of which consists of 10 documents.
For each document cluster, the system generated
summary may not be longer than 665 bytes including
spaces and punctuation. We used DUC-03 as
</bodyText>
<equation confidence="0.8649658">
version 1.5.5 with opti
1ROUGE
ons: -a -c 95 -b 665 -m -n 4
-w 1.2
R(S) =
</equation>
<bodyText confidence="0.999935">
Documents were pre-processed by segmenting sen-
tences and stemming words using the Porter Stemmer.
Each sentence was represented using a bag-of-terms
vector, where we used context terms up to bi-grams.
Similarity between sentence i and sentence j, i.e.,
wi,j, was computed using cosine similarity:
</bodyText>
<equation confidence="0.95648325">
Ew∈si tfw,i X tfw,j X idf2w
2 2 2
&apos; 2�
EwEsi �w,sil W&apos; EwEs� tfw,jl w
</equation>
<bodyText confidence="0.932875285714286">
where tfw,i and tfw,j are the numbers of times that
w appears in si and sentence sj respectively, and
idfw is the inverse document frequency (IDF) of
term w (up to bigram), which was calculated as the
logarithm of the ratio of the number of articles that
w appears over the total number of all articles in the
document cluster.
</bodyText>
<tableCaption confidence="0.855376">
Table 1: ROUGE-1 recall (R) and F-measure (F) results
(%) on DUC-04. DUC-03 was used as development set.
</tableCaption>
<table confidence="0.999515444444444">
DUC-04 R F
EiEV EjES wiJ 33.59 32.44
L1(S) 39.03 38.65
R1(S) 38.23 37.81
L1(S) + AR1(S) 39.35 38.90
Takamura and Okumura (2009) 38.50 -
Wang et al. (2009) 39.07 -
Lin and Bilmes (2010) - 38.39
Best system in DUC-04 (peer 65) 38.28 37.94
</table>
<bodyText confidence="0.984876666666667">
We first tested our coverage and diversity re-
ward objectives separately. For coverage, we use a
modular Ci(S) = Ej∈S wi,j for each sentence i, i.e.,
</bodyText>
<equation confidence="0.986926666666667">
�
L1(S) =
i∈V
</equation>
<bodyText confidence="0.998185363636364">
When α = 1, L1(S) reduces to Ei∈V,j∈S wi,j,
which measures the overall similarity of summary
set S to ground set V . As mentioned in Section 4.1,
using such similarity measurement could possibly
over-concentrate on a small portion of the document
and result in a poor coverage of the whole document.
As shown in Table 1, optimizing this objective
function gives a ROUGE-1 F-measure score 32.44%.
On the other hand, when using L1(S) with an α &lt; 1
(the value of α was determined on DUC-03 using
a grid search), a ROUGE-1 F-measure score 38.65%
</bodyText>
<figureCaption confidence="0.764551">
Figure 1: ROUGE-1 F-measure scores on DUC-03 when
α and K vary in objective function L,(S) + λR,(S),
</figureCaption>
<bodyText confidence="0.993128909090909">
where λ = 6 and α = n, .
is achieved, which is already better than the best
performing system in DUC-04.
As for the diversity reward objective, we define
the singleton reward as ri = �, Ej wi j, which is
the average similarity of sentence i to the rest of the
document. It basically states that the more similar to
the whole document a sentence is, the more reward
there will be by adding this sentence to an empty
summary set. By using this singleton reward, we
have the following diversity reward function:
</bodyText>
<equation confidence="0.999491666666667">
K
R1(S) = N wi,j . (7)
k=1 j∈S∩Pk i∈V
</equation>
<bodyText confidence="0.993312666666667">
In order to generate Pk, k = 1, · · · K, we used
CLUTO2 to cluster the sentences, where the IDF-
weighted term vector was used as feature vector, and
a direct K-mean clustering algorithm was used. In
this experiment, we set K = 0.2N. In other words,
there are 5 sentences in each cluster on average.
And as we can see in Table 1, optimizing the
diversity reward function alone achieves comparable
performance to the DUC-04 best system.
Combining L1(S) and R1(S), our system outper-
forms the best system in DUC-04 significantly, and
it also outperforms several recent systems, including
a concept-based summarization approach (Takamura
and Okumura, 2009), a sentence topic model based
system (Wang et al., 2009), and our MMR-styled
submodular system (Lin and Bilmes, 2010). Figure 1
illustrates how ROUGE-1 scores change when α and
K vary on the development set (DUC-03).
</bodyText>
<footnote confidence="0.705326">
2http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview
</footnote>
<figure confidence="0.850042611111111">
ROUGE-1 F-measure (%)
37.6
37.4
37.2
36.8
36.6
36.4
36.2
37
0 5
a
10 15 20
K=0.05N
K=0.1N
K=0.2N
wi,j =
min { � �wi,j, α wi,k } . (6)
j∈S k∈V
</figure>
<page confidence="0.99794">
516
</page>
<tableCaption confidence="0.959157333333333">
Table 2: ROUGE-2 recall (R) and F-measure (F) results
(%) on DUC-05, where DUC-05 was used as training set.
Table 4: ROUGE-2 recall (R) and F-measure (F) results
</tableCaption>
<table confidence="0.692230833333333">
(%) on DUC-06, where DUC-05 was used as training set.
DUC-05 R F
L1(S) + λRQ(S) 8.38 8.31
Daum´e III and Marcu (2006) 7.62 -
Extr, Daum´e et al. (2009) 7.67 -
Vine, Daum´e et al. (2009) 8.24 -
</table>
<tableCaption confidence="0.9085065">
Table 3: ROUGE-2 recall (R) and F-measure (F) results
on DUC-05 (%). We used DUC-06 as training set.
</tableCaption>
<table confidence="0.999787">
DUC-05 R F
L1(S) + λRQ(S) 7.82 7.72
Daum´e III and Marcu (2006) 6.98 -
Best system in DUC-05 (peer 15) 7.44 7.43
</table>
<subsectionHeader confidence="0.997677">
5.2 Query-focused summarization
</subsectionHeader>
<bodyText confidence="0.9999345">
We evaluated our approach on the task of query-
focused summarization using DUC 05-07 data. In
DUC-05 and DUC-06, participants were given 50
document clusters, where each cluster contains 25
news articles related to the same topic. Participants
were asked to generate summaries of at most 250
words for each cluster. For each cluster, a title and
a narrative describing a user’s information need are
provided. The narrative is usually composed of a
set of questions or a multi-sentence task description.
The main task in DUC-07 is the same as in DUC-06.
In DUC 05-07, ROUGE-2 was the primary
criterion for evaluation, and thus we also report
ROUGE-23 (both recall R, and precision F). Docu-
ments were processed as in Section 5.1. We used both
the title and the narrative as query, where stop words,
including some function words (e.g., “describe”) that
appear frequently in the query, were removed. All
queries were then stemmed using the Porter Stemmer.
Note that there are several ways to incorporate
query-focused information into both the coverage
and diversity reward objectives. For instance, Ci(S)
could be query-dependent in how it measures how
much query-dependent information in i is covered
by S. Also, the coefficient a could be query and sen-
tence dependent, where it takes larger value when a
sentence is more relevant to query (i.e., a larger value
of a means later truncation, and therefore more pos-
sible coverage). Similarly, sentence clustering and
singleton rewards in the diversity function can also
</bodyText>
<footnote confidence="0.763461">
3ROUGE version 1.5.5 was used with option -n 2 -x -m -2 4
-u -c 95 -r 1000 -f A -p 0.5 -t 0 -d -l 250
</footnote>
<table confidence="0.9994116">
DUC-06 R F
L1(S) + λRQ(S) 9.75 9.77
Celikyilmaz and Hakkani-t¨ur (2010) 9.10 -
Shen and Li (2010) 9.30 -
Best system in DUC-06 (peer 24) 9.51 9.51
</table>
<tableCaption confidence="0.9766025">
Table 5: ROUGE-2 recall (R) and F-measure (F) re-
sults (%) on DUC-07. DUC-05 was used as training
set for objective L1(S) + ARQ(S). DUC-05 and DUC-
06 were used as training sets for objective L1(S) +
</tableCaption>
<table confidence="0.984168777777778">
P
�A,�RQ,,�(S).
DUC-07 R F
L1(S) + λRQ(S) 12.18 12.13
L1(S) + E3=1 λ�RQ,�(S) 12.38 12.33
Toutanova et al. (2007) 11.89 11.89
Haghighi and Vanderwende (2009) 11.80 -
Celikyilmaz and Hakkani-t¨ur (2010) 11.40 -
Best system in DUC-07 (peer 15) 12.45 12.29
</table>
<bodyText confidence="0.9980928">
be query-dependent. In this experiment, we explore
an objective with a query-independent coverage func-
tion (R1(S)), indicating prior importance, combined
with a query-dependent diversity reward function,
where the latter is defined as:
</bodyText>
<equation confidence="0.988118">
v uX X N
wi,j + (1 − Q)rj,Q
jESnPk iEV
</equation>
<bodyText confidence="0.99999085">
where 0 ≤ Q ≤ 1, and rj,Q represents the rel-
evance between sentence j to query Q. This
query-dependent reward function is derived by
using a singleton reward that is expressed as a
convex combination of the query-independent score
( N PiE V wi ,j) and the query-dependent score (rj,Q)
of a sentence. We simply used the number of
terms (up to a bi-gram) that sentence j overlaps the
query Q as rj,Q, where the IDF weighting is not
used (i.e., every term in the query, after stop word
removal, was treated as equally important). Both
query-independent and query-dependent scores were
then normalized by their largest value respectively
such that they had roughly the same dynamic range.
To better estimate of the relevance between query
and sentences, we further expanded sentences with
synonyms and hypernyms of its constituent words. In
particular, part-of-speech tags were obtained for each
sentence using the maximum entropy part-of-speech
tagger (Ratnaparkhi, 1996), and all nouns were then
</bodyText>
<equation confidence="0.983104">
RQ(S) = XK
k=1
</equation>
<page confidence="0.982202">
517
</page>
<bodyText confidence="0.998636125">
expanded with their synonyms and hypernyms using
WordNet (Fellbaum, 1998). Note that these expanded
documents were only used in the estimation rj,Q, and
we plan to further explore whether there is benefit to
use the expanded documents either in sentence sim-
ilarity estimation or in sentence clustering in our fu-
ture work. We also tried to expand the query with syn-
onyms and observed a performance decrease, presum-
ably due to noisy information in a query expression.
While it is possible to use an approach that is
similar to (Toutanova et al., 2007) to learn the
coefficients in our objective function, we trained all
coefficients to maximize ROUGE-2 F-measure score
using the Nelder-Mead (derivative-free) method.
Using L1(S)+ARQ(S) as the objective and with the
same sentence clustering algorithm as in the generic
summarization experiment (K = 0.2N), our system,
when both trained and tested on DUC-05 (results in
Table 2), outperforms the Bayesian query-focused
summarization approach and the search-based
structured prediction approach, which were also
trained and tested on DUC-05 (Daum´e et al., 2009).
Note that the system in (Daum´e et al., 2009) that
achieves its best performance (8.24% in ROUGE-2
recall) is a so called “vine-growth” system, which
can be seen as an abstractive approach, whereas our
system is purely an extractive system. Comparing
to the extractive system in (Daum´e et al., 2009), our
system performs much better (8.38% v.s. 7.67%).
More importantly, when trained only on DUC-06 and
tested on DUC-05 (results in Table 3), our approach
outperforms the best system in DUC-05 significantly.
We further tested the system trained on DUC-05
on both DUC-06 and DUC-07. The results on
DUC-06 are shown in Table. 4. Our system outper-
forms the best system in DUC-06, as well as two
recent approaches (Shen and Li, 2010; Celikyilmaz
and Hakkani-t¨ur, 2010). On DUC-07, in terms of
ROUGE-2 score, our system outperforms PYTHY
(Toutanova et al., 2007), a state-of-the-art supervised
summarization system, as well as two recent systems
including a generative summarization system based
on topic models (Haghighi and Vanderwende,
2009), and a hybrid hierarchical summarization
system (Celikyilmaz and Hakkani-t¨ur, 2010). It
also achieves comparable performance to the best
DUC-07 system. Note that in the best DUC-07
system (Pingali et al., 2007; Jagarlamudi et al., 2006),
an external web search engine (Yahoo!) was used
to estimate a language model for query relevance. In
our system, no such web search expansion was used.
To further improve the performance of our system,
we used both DUC-05 and DUC-06 as a training
set, and introduced three diversity reward terms
into the objective where three different sentence
clusterings with different resolutions were produced
(with sizes 0.3N, 0.15N and 0.05N). Denoting
a diversity reward corresponding to clustering K
as RQ,κ(S), we model the summary quality as
L1(S) + E3κ_1 AκRQ,κ(S). As shown in Table 5,
using this objective function with multi-resolution
diversity rewards improves our results further, and
outperforms the best system in DUC-07 in terms of
ROUGE-2 F-measure score.
</bodyText>
<sectionHeader confidence="0.993774" genericHeader="conclusions">
6 Conclusion and discussion
</sectionHeader>
<bodyText confidence="0.99993662962963">
In this paper, we show that submodularity naturally
arises in document summarization. Not only do
many existing automatic summarization methods cor-
respond to submodular function optimization, but
also the widely used ROUGE evaluation is closely
related to submodular functions. As the correspond-
ing submodular optimization problem can be solved
efficiently and effectively, the remaining question
is then how to design a submodular objective that
best models the task. To address this problem, we
introduce a powerful class of monotone submodular
functions that are well suited to document summariza-
tion by modeling two important properties of a sum-
mary, fidelity and diversity. While more advanced
NLP techniques could be easily incorporated into our
functions (e.g., language models could define a better
Ci(S), more advanced relevance estimations for the
singleton rewards ri, and better and/or overlapping
clustering algorithms for our diversity reward), we
already show top results on standard benchmark eval-
uations using fairly basic NLP methods (e.g., term
weighting and WordNet expansion), all, we believe,
thanks to the power and generality of submodular
functions. As information retrieval and web search
are closely related to query-focused summarization,
our approach might be beneficial in those areas as
well.
</bodyText>
<page confidence="0.995871">
518
</page>
<sectionHeader confidence="0.982652" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999682480769231">
F. Bach. 2010. Structured sparsity-inducing norms
through submodular functions. Advances in Neural
Information Processing Systems.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents and
producing summaries. In Proc. of SIGIR.
A. Celikyilmaz and D. Hakkani-t¨ur. 2010. A hybrid hier-
archical model for multi-document summarization. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 815–824,
Uppsala, Sweden, July. Association for Computational
Linguistics.
H. Daum´e, J. Langford, and D. Marcu. 2009. Search-
based structured prediction. Machine learning,
75(3):297–325.
H. Daum´e III and D. Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the 21st
International Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, page 312.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. The MIT press.
E. Filatova and V. Hatzivassiloglou. 2004. Event-based
extractive summarization. In Proceedings ofACL Work-
shop on Summarization, volume 111.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362–370, Boulder, Colorado, June. Association
for Computational Linguistics.
J. Jagarlamudi, P. Pingali, and V. Varma. 2006. Query
independent sentence scoring approach to DUC 2006.
In DUC 2006.
S. Jegelka and J. A. Bilmes. 2011. Submodularity beyond
submodular energies: coupling edges in graph cuts.
In Computer Vision and Pattern Recognition (CVPR),
Colorado Springs, CO, June.
D. Kempe, J. Kleinberg, and E. Tardos. 2003. Maximiz-
ing the spread of influence through a social network.
In Proceedings of the 9th Conference on SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD).
S. Khuller, A. Moss, and J. Naor. 1999. The budgeted
maximum coverage problem. Information Processing
Letters, 70(1):39–45.
V. Kolmogorov and R. Zabin. 2004. What energy func-
tions can be minimized via graph cuts? IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
26(2):147–159.
A. Krause and C. Guestrin. 2005. Near-optimal nonmy-
opic value of information in graphical models. In Proc.
of Uncertainty in AI.
A. Krause, H.B. McMahan, C. Guestrin, and A. Gupta.
2008. Robust submodular observation selection. Jour-
nal of Machine Learning Research, 9:2761–2801.
H. Lin and J. Bilmes. 2010. Multi-document summariza-
tion via budgeted maximization of submodular func-
tions. In North American chapter of the Association
for Computational Linguistics/Human Language Tech-
nology Conference (NAACL/HLT-2010), Los Angeles,
CA, June.
H. Lin and J. Bilmes. 2011. Word alignment via submod-
ular maximization over matroids. In The 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT), Port-
land, OR, June.
H. Lin, J. Bilmes, and S. Xie. 2009. Graph-based submod-
ular selection for extractive summarization. In Proc.
IEEE Automatic Speech Recognition and Understand-
ing (ASRU), Merano, Italy, December.
C.-Y. Lin. 2004. ROUGE: A package for automatic eval-
uation of summaries. In Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop.
L. Lov´asz. 1983. Submodular functions and convexity.
Mathematical programming-The state of the art,(eds. A.
Bachem, M. Grotschel and B. Korte) Springer, pages
235–257.
M. Minoux. 1978. Accelerated greedy algorithms for
maximizing submodular set functions. Optimization
Techniques, pages 234–243.
M. Narasimhan and J. Bilmes. 2005. A submodular-
supermodular procedure with applications to discrimi-
native structure learning. In Proc. Conf. Uncertainty in
Artifical Intelligence, Edinburgh, Scotland, July. Mor-
gan Kaufmann Publishers.
M. Narasimhan and J. Bilmes. 2007. Local search for
balanced submodular clusterings. In Twentieth Inter-
national Joint Conference on Artificial Intelligence (IJ-
CAI07), Hyderabad, India, January.
H. Narayanan. 1997. Submodular functions and electrical
networks. North-Holland.
G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. 1978. An
analysis of approximations for maximizing submodular
set functions I. Mathematical Programming, 14(1):265–
294.
P. Pingali, K. Rahul, and V. Varma. 2007. IIIT Hyderabad
at DUC 2007. Proceedings of DUC 2007.
V. Qazvinian, D.R. Radev, and A. Ozg¨ur. 2010. Cita-
tion Summarization Through Keyphrase Extraction. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 895–
903.
</reference>
<page confidence="0.982954">
519
</page>
<reference confidence="0.999719131578947">
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In EMNLP, volume 1, pages
133–142.
K. Riedhammer, B. Favre, and D. Hakkani-T¨ur. 2010.
Long story short-Global unsupervised models for
keyphrase based meeting summarization. Speech Com-
munication.
C. Shen and T. Li. 2010. Multi-document summarization
via the minimum dominating set. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 984–992, Beijing, China,
August. Coling 2010 Organizing Committee.
M. Sviridenko. 2004. A note on maximizing a submodu-
lar set function subject to a knapsack constraint. Oper-
ations Research Letters, 32(1):41–43.
H. Takamura and M. Okumura. 2009. Text summariza-
tion model based on maximum coverage problem and
its variant. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 781–789. Association for
Computational Linguistics.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The PYTHY
summarization system: Microsoft research at DUC
2007. In the proceedings of Document Understanding
Conference.
D. Wang, S. Zhu, T. Li, and Y. Gong. 2009. Multi-
document summarization using sentence-based topic
models. In Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, pages 297–300, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
L.A. Wolsey. 1982. An analysis of the greedy algorithm
for the submodular set covering problem. Combinator-
ica, 2(4):385–393.
P. Zhao, G. Rocha, and B. Yu. 2009. Grouped and hier-
archical model selection through composite absolute
penalties. Annals of Statistics, 37(6A):3468–3497.
</reference>
<page confidence="0.99696">
520
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.962697">
<title confidence="0.999963">A Class of Submodular Functions for Document Summarization</title>
<author confidence="0.999906">Hui Lin</author>
<affiliation confidence="0.9997665">Dept. of Electrical Engineering University of Washington</affiliation>
<address confidence="0.999721">Seattle, WA 98195,</address>
<email confidence="0.999519">hlin@ee.washington.edu</email>
<author confidence="0.980152">Jeff</author>
<affiliation confidence="0.999784">Dept. of Electrical University of</affiliation>
<address confidence="0.998667">Seattle, WA 98195,</address>
<email confidence="0.999769">bilmes@ee.washington.edu</email>
<abstract confidence="0.9992604">We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Bach</author>
</authors>
<title>Structured sparsity-inducing norms through submodular functions.</title>
<date>2010</date>
<booktitle>Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="22722" citStr="Bach, 2010" startWordPosition="3748" endWordPosition="3749">aterial that could ignore. It is easy to show that is submodular by using the composition rule from Theorem 1. The square root is non-decreasing concave function. Inside each square root lies a modular function with non-negative weights (and thus is monotone). Applying the square root to such a monotone submodular function yields a submodular function, and summing them all together retains submodularity, as mentioned in Section 2. The of is straightforward. Note, the form of Eqn. 5 is similar to structured group norms (e.g., (Zhao et al., 2009)), recently shown to be related to submodularity (Bach, 2010; Jegelka an Pi, ··· UiPi ri ri R(S) k1, P1, rk1 k1 √13 R(S) G(S) R(S) G(S) R(S) monotonicity R(S) d Bilmes, 2011). f(x) R(S), Hakkani-t¨ur, d submodularity. 515 Several extensions to Eqn. 5 are discussed next: First, instead of using a ground set partition, intersecting clusters can be used. Second, the square root function in Eqn. 5 can be replaced with any other non-decreasing concave functions (e.g., = log(1 + x)) while preserving the desired property of and the curvature of the concave function then determines the rate that the reward diminishes. Last, multi-resolution clustering (or part</context>
</contexts>
<marker>Bach, 2010</marker>
<rawString>F. Bach. 2010. Structured sparsity-inducing norms through submodular functions. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proc. of SIGIR.</booktitle>
<contexts>
<context position="2543" citStr="Carbonell and Goldstein, 1998" startWordPosition="381" endWordPosition="384">ality of the solution does not depend on the size of the problem, so even very large size problems do well. It is also important to note that this is a worst case bound, and in most cases the quality of the solution obtained will be much better than this bound suggests. Of course, none of this is useful if the objective function F is inappropriate for the summarization task. In this paper, we argue that monotone nondecreasing submodular functions F are an ideal class of functions to investigate for document summarization. We show, in fact, that many well-established methods for summarization (Carbonell and Goldstein, 1998; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009; Riedhammer et al., 2010; Shen and Li, 2010) correspond to submodular function optimization, a property not explicitly mentioned in these publications. We take this fact, however, as testament to the value of submodular functions for summarization: if summarization algorithms are repeatedly developed that, by chance, happen to be an instance of a submodular function optimization, this suggests that submodular functions are a natural fit. On the other hand, other authors have started realizing explicitly the value of submodular f</context>
<context position="10509" citStr="Carbonell and Goldstein, 1998" startWordPosition="1687" endWordPosition="1690">to the general submodular framework, and show a practical greedy algorithm with a (1 − 1/√e)-approximation factor, where each greedy step adds the unit with the largest ratio of objective function gain to scaled cost, while not violating the budget constraint (see (Lin and Bilmes, 2010) for details). Note that in all cases, submodularity and monotonicity are two necessary ingredients to guarantee that the greedy algorithm gives near-optimal solutions. In fact, greedy-like algorithms have been widely used in summarization. One of the more popular approaches is maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998), where a greedy algorithm selects the most relevant sentences, and at the same time avoids redundancy by removing sentences that are too similar to ones already selected. Interestingly, the gain function defined in the original MMR paper (Carbonell and Goldstein, 1998) satisfies diminishing returns, a fact apparently unnoticed until now. In particular, Carbonell and Goldstein (1998) define an objective function gain of adding element k to set S (k ∈/ S) as: λSim1(sk, q) − (1 − λ) max Sim2(si, sk), (1) iES where Sim1(sk, q) measures the similarity between unit sk to a query q, Sim2(si, sk) mea</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Celikyilmaz</author>
<author>D Hakkani-t¨ur</author>
</authors>
<title>A hybrid hierarchical model for multi-document summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>815--824</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Celikyilmaz, Hakkani-t¨ur, 2010</marker>
<rawString>A. Celikyilmaz and D. Hakkani-t¨ur. 2010. A hybrid hierarchical model for multi-document summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 815–824, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>J Langford</author>
<author>D Marcu</author>
</authors>
<title>Searchbased structured prediction.</title>
<date>2009</date>
<booktitle>Machine learning,</booktitle>
<pages>75--3</pages>
<marker>Daum´e, Langford, Marcu, 2009</marker>
<rawString>H. Daum´e, J. Langford, and D. Marcu. 2009. Searchbased structured prediction. Machine learning, 75(3):297–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>D Marcu</author>
</authors>
<title>Bayesian queryfocused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>312</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>H. Daum´e III and D. Marcu. 2006. Bayesian queryfocused summarization. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, page 312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>The MIT press.</publisher>
<contexts>
<context position="32567" citStr="Fellbaum, 1998" startWordPosition="5419" endWordPosition="5420">r stop word removal, was treated as equally important). Both query-independent and query-dependent scores were then normalized by their largest value respectively such that they had roughly the same dynamic range. To better estimate of the relevance between query and sentences, we further expanded sentences with synonyms and hypernyms of its constituent words. In particular, part-of-speech tags were obtained for each sentence using the maximum entropy part-of-speech tagger (Ratnaparkhi, 1996), and all nouns were then RQ(S) = XK k=1 517 expanded with their synonyms and hypernyms using WordNet (Fellbaum, 1998). Note that these expanded documents were only used in the estimation rj,Q, and we plan to further explore whether there is benefit to use the expanded documents either in sentence similarity estimation or in sentence clustering in our future work. We also tried to expand the query with synonyms and observed a performance decrease, presumably due to noisy information in a query expression. While it is possible to use an approach that is similar to (Toutanova et al., 2007) to learn the coefficients in our objective function, we trained all coefficients to maximize ROUGE-2 F-measure score using </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An electronic lexical database. The MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Filatova</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Event-based extractive summarization.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL Workshop on Summarization,</booktitle>
<volume>111</volume>
<contexts>
<context position="2580" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="385" endWordPosition="388">depend on the size of the problem, so even very large size problems do well. It is also important to note that this is a worst case bound, and in most cases the quality of the solution obtained will be much better than this bound suggests. Of course, none of this is useful if the objective function F is inappropriate for the summarization task. In this paper, we argue that monotone nondecreasing submodular functions F are an ideal class of functions to investigate for document summarization. We show, in fact, that many well-established methods for summarization (Carbonell and Goldstein, 1998; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009; Riedhammer et al., 2010; Shen and Li, 2010) correspond to submodular function optimization, a property not explicitly mentioned in these publications. We take this fact, however, as testament to the value of submodular functions for summarization: if summarization algorithms are repeatedly developed that, by chance, happen to be an instance of a submodular function optimization, this suggests that submodular functions are a natural fit. On the other hand, other authors have started realizing explicitly the value of submodular functions for summarization (Lin and B</context>
<context position="11738" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="1887" endWordPosition="1890">k) measures the similarity between unit si and unit sk, and 0 ≤ λ ≤ 1 is a trade-off coefficient. We have: Theorem 2. Given an expression for FMMR such that FMMR(S ∪ {k}) −FMMR(S) is equal to Eq. 1, FMMR is non-monotone submodular. Obviously, diminishing-returns hold since Sim2(si, sk) ≤ max iER for all S ⊆ R, and therefore FMMR is submodular. On the other hand, FMMR, would not be monotone, so the greedy algorithm’s constant-factor approximation guarantee does not apply in this case. When scoring a summary at the sub-sentence level, submodularity naturally arises. Concept-based summarization (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009; Riedhammer et al., 2010; Qazvinian et al., 2010) usually maximizes the weighted credit of concepts covered by the summary. Although the authors may not have noticed, their objective functions are also submodular, adding more evidence suggesting that submodularity is natural for summarization tasks. Indeed, let S be a subset of sentences in the document and denote F(S) as the set of concepts contained in S. The total credit of the concepts covered by S is then o Fconcept(S) E iEr(S) where ci is the credit of concept i. This function is known to be submodular (Naray</context>
<context position="16423" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="2649" endWordPosition="2653"> 1, · · · , K). Then ROUGE-N can be written as the following set function: EK p1 EeERi min(ce(S), re,i) ROUGE-N(S) = K Ei=1 EeERi re,i where re,i is the number of times n-gram a occurs in reference summary i. Since ce(S) is monotone modular and min(x, a) is a concave non-decreasing function of x, min(ce(S), re,i) is monotone submodular by Theorem 1. Since summation preserves submodularity, and the denominator is constant, we see that FROUGE-N is monotone submodular. Since the reference summaries are unknown, it is of course impossible to optimize FROUGE-N directly. Therefore, some approaches (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009; Riedhammer et al., 2010) instead define “concepts”. Alter, 513 natively, we herein propose a class of monotone sub- the property of diminishing returns. Indeed, Shanmodular functions that naturally models the quality of non entropy, as the measurement of information, is a summary while not depending on an explicit notion another well-known monotone submodular function. of concepts, as we will see in the following section. There are several ways to define L(S) in our 4 Monotone Submodular Objectives context. For instance, we could use L(S) = Two properties of a goo</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>E. Filatova and V. Hatzivassiloglou. 2004. Event-based extractive summarization. In Proceedings ofACL Workshop on Summarization, volume 111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>L Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="31109" citStr="Haghighi and Vanderwende (2009)" startWordPosition="5181" endWordPosition="5184"> diversity function can also 3ROUGE version 1.5.5 was used with option -n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d -l 250 DUC-06 R F L1(S) + λRQ(S) 9.75 9.77 Celikyilmaz and Hakkani-t¨ur (2010) 9.10 - Shen and Li (2010) 9.30 - Best system in DUC-06 (peer 24) 9.51 9.51 Table 5: ROUGE-2 recall (R) and F-measure (F) results (%) on DUC-07. DUC-05 was used as training set for objective L1(S) + ARQ(S). DUC-05 and DUC06 were used as training sets for objective L1(S) + P �A,�RQ,,�(S). DUC-07 R F L1(S) + λRQ(S) 12.18 12.13 L1(S) + E3=1 λ�RQ,�(S) 12.38 12.33 Toutanova et al. (2007) 11.89 11.89 Haghighi and Vanderwende (2009) 11.80 - Celikyilmaz and Hakkani-t¨ur (2010) 11.40 - Best system in DUC-07 (peer 15) 12.45 12.29 be query-dependent. In this experiment, we explore an objective with a query-independent coverage function (R1(S)), indicating prior importance, combined with a query-dependent diversity reward function, where the latter is defined as: v uX X N wi,j + (1 − Q)rj,Q jESnPk iEV where 0 ≤ Q ≤ 1, and rj,Q represents the relevance between sentence j to query Q. This query-dependent reward function is derived by using a singleton reward that is expressed as a convex combination of the query-independent sco</context>
<context position="34645" citStr="Haghighi and Vanderwende, 2009" startWordPosition="5746" endWordPosition="5749"> on DUC-05 (results in Table 3), our approach outperforms the best system in DUC-05 significantly. We further tested the system trained on DUC-05 on both DUC-06 and DUC-07. The results on DUC-06 are shown in Table. 4. Our system outperforms the best system in DUC-06, as well as two recent approaches (Shen and Li, 2010; Celikyilmaz and Hakkani-t¨ur, 2010). On DUC-07, in terms of ROUGE-2 score, our system outperforms PYTHY (Toutanova et al., 2007), a state-of-the-art supervised summarization system, as well as two recent systems including a generative summarization system based on topic models (Haghighi and Vanderwende, 2009), and a hybrid hierarchical summarization system (Celikyilmaz and Hakkani-t¨ur, 2010). It also achieves comparable performance to the best DUC-07 system. Note that in the best DUC-07 system (Pingali et al., 2007; Jagarlamudi et al., 2006), an external web search engine (Yahoo!) was used to estimate a language model for query relevance. In our system, no such web search expansion was used. To further improve the performance of our system, we used both DUC-05 and DUC-06 as a training set, and introduced three diversity reward terms into the objective where three different sentence clusterings wi</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>A. Haghighi and L. Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jagarlamudi</author>
<author>P Pingali</author>
<author>V Varma</author>
</authors>
<title>Query independent sentence scoring approach to DUC</title>
<date>2006</date>
<booktitle>In DUC</booktitle>
<contexts>
<context position="34883" citStr="Jagarlamudi et al., 2006" startWordPosition="5781" endWordPosition="5784"> best system in DUC-06, as well as two recent approaches (Shen and Li, 2010; Celikyilmaz and Hakkani-t¨ur, 2010). On DUC-07, in terms of ROUGE-2 score, our system outperforms PYTHY (Toutanova et al., 2007), a state-of-the-art supervised summarization system, as well as two recent systems including a generative summarization system based on topic models (Haghighi and Vanderwende, 2009), and a hybrid hierarchical summarization system (Celikyilmaz and Hakkani-t¨ur, 2010). It also achieves comparable performance to the best DUC-07 system. Note that in the best DUC-07 system (Pingali et al., 2007; Jagarlamudi et al., 2006), an external web search engine (Yahoo!) was used to estimate a language model for query relevance. In our system, no such web search expansion was used. To further improve the performance of our system, we used both DUC-05 and DUC-06 as a training set, and introduced three diversity reward terms into the objective where three different sentence clusterings with different resolutions were produced (with sizes 0.3N, 0.15N and 0.05N). Denoting a diversity reward corresponding to clustering K as RQ,κ(S), we model the summary quality as L1(S) + E3κ_1 AκRQ,κ(S). As shown in Table 5, using this obje</context>
</contexts>
<marker>Jagarlamudi, Pingali, Varma, 2006</marker>
<rawString>J. Jagarlamudi, P. Pingali, and V. Varma. 2006. Query independent sentence scoring approach to DUC 2006. In DUC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jegelka</author>
<author>J A Bilmes</author>
</authors>
<title>Submodularity beyond submodular energies: coupling edges in graph cuts.</title>
<date>2011</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR),</booktitle>
<location>Colorado Springs, CO,</location>
<marker>Jegelka, Bilmes, 2011</marker>
<rawString>S. Jegelka and J. A. Bilmes. 2011. Submodularity beyond submodular energies: coupling edges in graph cuts. In Computer Vision and Pattern Recognition (CVPR), Colorado Springs, CO, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kempe</author>
<author>J Kleinberg</author>
<author>E Tardos</author>
</authors>
<title>Maximizing the spread of influence through a social network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 9th Conference on SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD).</booktitle>
<contexts>
<context position="7089" citStr="Kempe et al., 2003" startWordPosition="1129" endWordPosition="1132">ere with equality, then the function F is called modular, and in such case F(A) = c + EaEA ~fa for a sized |V |vector f~ of real values and constant c. A set function F is monotone nondecreasing if VA C_ B, F(A) G F(B). As shorthand, in this paper, monotone nondecreasing submodular functions will simply be referred to as monotone submodular. Historically, submodular functions have their roots in economics, game theory, combinatorial optimization, and operations research. More recently, submodular functions have started receiving attention in the machine learning and computer vision community (Kempe et al., 2003; Narasimhan and Bilmes, 2005; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2007; Krause et al., 2008; Kolmogorov and Zabin, 2004) and have recently been introduced to natural language processing for the tasks of document summarization (Lin and Bilmes, 2010) and word alignment (Lin and Bilmes, 2011). Submodular functions share a number of properties in common with convex and concave functions (Lov´asz, 1983), including their wide applicability, their generality, their multiple options for their representation, and their closure under a number of common operators (including mixtures, trunc</context>
</contexts>
<marker>Kempe, Kleinberg, Tardos, 2003</marker>
<rawString>D. Kempe, J. Kleinberg, and E. Tardos. 2003. Maximizing the spread of influence through a social network. In Proceedings of the 9th Conference on SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Khuller</author>
<author>A Moss</author>
<author>J Naor</author>
</authors>
<title>The budgeted maximum coverage problem.</title>
<date>1999</date>
<journal>Information Processing Letters,</journal>
<volume>70</volume>
<issue>1</issue>
<contexts>
<context position="9840" citStr="Khuller et al. (1999)" startWordPosition="1586" endWordPosition="1589">can then be formalized as the following combinatorial optimization problem: Problem 1. Find � F(S) subject to: iES Since this is a generalization of the cardinality constraint (where ci = 1, ∀i), this also constitutes a (well-known) NP-hard problem. In this case as well, however, a modified greedy algorithm with partial enumeration can solve Problem 1 near-optimally with (1−1/e)-approximation factor if F is monotone submodular (Sviridenko, 2004). The partial enumeration, however, is too computationally expensive for real world applications. In (Lin and Bilmes, 2010), we generalize the work by Khuller et al. (1999) on the budgeted maximum cover problem to the general submodular framework, and show a practical greedy algorithm with a (1 − 1/√e)-approximation factor, where each greedy step adds the unit with the largest ratio of objective function gain to scaled cost, while not violating the budget constraint (see (Lin and Bilmes, 2010) for details). Note that in all cases, submodularity and monotonicity are two necessary ingredients to guarantee that the greedy algorithm gives near-optimal solutions. In fact, greedy-like algorithms have been widely used in summarization. One of the more popular approache</context>
</contexts>
<marker>Khuller, Moss, Naor, 1999</marker>
<rawString>S. Khuller, A. Moss, and J. Naor. 1999. The budgeted maximum coverage problem. Information Processing Letters, 70(1):39–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Kolmogorov</author>
<author>R Zabin</author>
</authors>
<title>What energy functions can be minimized via graph cuts?</title>
<date>2004</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="7224" citStr="Kolmogorov and Zabin, 2004" startWordPosition="1149" endWordPosition="1152">al values and constant c. A set function F is monotone nondecreasing if VA C_ B, F(A) G F(B). As shorthand, in this paper, monotone nondecreasing submodular functions will simply be referred to as monotone submodular. Historically, submodular functions have their roots in economics, game theory, combinatorial optimization, and operations research. More recently, submodular functions have started receiving attention in the machine learning and computer vision community (Kempe et al., 2003; Narasimhan and Bilmes, 2005; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2007; Krause et al., 2008; Kolmogorov and Zabin, 2004) and have recently been introduced to natural language processing for the tasks of document summarization (Lin and Bilmes, 2010) and word alignment (Lin and Bilmes, 2011). Submodular functions share a number of properties in common with convex and concave functions (Lov´asz, 1983), including their wide applicability, their generality, their multiple options for their representation, and their closure under a number of common operators (including mixtures, truncation, complementation, and certain convolutions). For example, if a collection of functions {FZ}Z is submodular, then so is their weig</context>
</contexts>
<marker>Kolmogorov, Zabin, 2004</marker>
<rawString>V. Kolmogorov and R. Zabin. 2004. What energy functions can be minimized via graph cuts? IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):147–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Krause</author>
<author>C Guestrin</author>
</authors>
<title>Near-optimal nonmyopic value of information in graphical models.</title>
<date>2005</date>
<booktitle>In Proc. of Uncertainty in AI.</booktitle>
<contexts>
<context position="7145" citStr="Krause and Guestrin, 2005" startWordPosition="1137" endWordPosition="1140">modular, and in such case F(A) = c + EaEA ~fa for a sized |V |vector f~ of real values and constant c. A set function F is monotone nondecreasing if VA C_ B, F(A) G F(B). As shorthand, in this paper, monotone nondecreasing submodular functions will simply be referred to as monotone submodular. Historically, submodular functions have their roots in economics, game theory, combinatorial optimization, and operations research. More recently, submodular functions have started receiving attention in the machine learning and computer vision community (Kempe et al., 2003; Narasimhan and Bilmes, 2005; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2007; Krause et al., 2008; Kolmogorov and Zabin, 2004) and have recently been introduced to natural language processing for the tasks of document summarization (Lin and Bilmes, 2010) and word alignment (Lin and Bilmes, 2011). Submodular functions share a number of properties in common with convex and concave functions (Lov´asz, 1983), including their wide applicability, their generality, their multiple options for their representation, and their closure under a number of common operators (including mixtures, truncation, complementation, and certain convolutions). For e</context>
</contexts>
<marker>Krause, Guestrin, 2005</marker>
<rawString>A. Krause and C. Guestrin. 2005. Near-optimal nonmyopic value of information in graphical models. In Proc. of Uncertainty in AI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Krause</author>
<author>H B McMahan</author>
<author>C Guestrin</author>
<author>A Gupta</author>
</authors>
<title>Robust submodular observation selection.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--2761</pages>
<contexts>
<context position="5672" citStr="Krause et al., 2008" startWordPosition="878" endWordPosition="881">re work. 2 Background on Submodularity We are given a set of objects V = {v1, ... , v�} and a function F : 2V —* R that returns a real value for any subset S C_ V . We are interested in finding the subset of bounded size |S |G k that maximizes the function, e.g., argmaxScV F(S). In general, this operation is hopelessly intractable, an unfortunate fact since the optimization coincides with many important applications. For example, F might correspond to the value or coverage of a set of sensor locations in an environment, and the goal is to find the best locations for a fixed number of sensors (Krause et al., 2008). If the function F is monotone submodular then the maximization is still NP complete, but it was shown in (Nemhauser et al., 1978) that a greedy algorithm finds an approximate solution guaranteed to be within e−1 e — 0.63 of the optimal solution, as mentioned in Section 1. A version of this algorithm (Minoux, 1978), moreover, scales to very large data sets. Submodular functions are those that satisfy the property of diminishing returns: for any A C_ B C_ V \v, a submodular function F must satisfy F(A+v)—F(A) &gt; F(B + v) — F(B). That is, the incremental “value” of v decreases as the context in </context>
<context position="7195" citStr="Krause et al., 2008" startWordPosition="1145" endWordPosition="1148">d |V |vector f~ of real values and constant c. A set function F is monotone nondecreasing if VA C_ B, F(A) G F(B). As shorthand, in this paper, monotone nondecreasing submodular functions will simply be referred to as monotone submodular. Historically, submodular functions have their roots in economics, game theory, combinatorial optimization, and operations research. More recently, submodular functions have started receiving attention in the machine learning and computer vision community (Kempe et al., 2003; Narasimhan and Bilmes, 2005; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2007; Krause et al., 2008; Kolmogorov and Zabin, 2004) and have recently been introduced to natural language processing for the tasks of document summarization (Lin and Bilmes, 2010) and word alignment (Lin and Bilmes, 2011). Submodular functions share a number of properties in common with convex and concave functions (Lov´asz, 1983), including their wide applicability, their generality, their multiple options for their representation, and their closure under a number of common operators (including mixtures, truncation, complementation, and certain convolutions). For example, if a collection of functions {FZ}Z is subm</context>
</contexts>
<marker>Krause, McMahan, Guestrin, Gupta, 2008</marker>
<rawString>A. Krause, H.B. McMahan, C. Guestrin, and A. Gupta. 2008. Robust submodular observation selection. Journal of Machine Learning Research, 9:2761–2801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lin</author>
<author>J Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In North American chapter of the Association for Computational Linguistics/Human Language Technology Conference (NAACL/HLT-2010),</booktitle>
<location>Los Angeles, CA,</location>
<contexts>
<context position="3191" citStr="Lin and Bilmes, 2010" startWordPosition="477" endWordPosition="480">lou, 2004; Takamura and Okumura, 2009; Riedhammer et al., 2010; Shen and Li, 2010) correspond to submodular function optimization, a property not explicitly mentioned in these publications. We take this fact, however, as testament to the value of submodular functions for summarization: if summarization algorithms are repeatedly developed that, by chance, happen to be an instance of a submodular function optimization, this suggests that submodular functions are a natural fit. On the other hand, other authors have started realizing explicitly the value of submodular functions for summarization (Lin and Bilmes, 2010; Qazvinian et al., 2010). Submodular functions share many properties in common with convex functions, one of which is that they are closed under a number of common combination operations (summation, certain compositions, restrictions, and so on). These operations give us the tools necessary to design a powerful submodular objective for submodular document summarization that extends beyond any previous work. We demonstrate this by carefully crafting a class of submodular funcProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 510–520, Portland, Oregon</context>
<context position="7352" citStr="Lin and Bilmes, 2010" startWordPosition="1169" endWordPosition="1172">ndecreasing submodular functions will simply be referred to as monotone submodular. Historically, submodular functions have their roots in economics, game theory, combinatorial optimization, and operations research. More recently, submodular functions have started receiving attention in the machine learning and computer vision community (Kempe et al., 2003; Narasimhan and Bilmes, 2005; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2007; Krause et al., 2008; Kolmogorov and Zabin, 2004) and have recently been introduced to natural language processing for the tasks of document summarization (Lin and Bilmes, 2010) and word alignment (Lin and Bilmes, 2011). Submodular functions share a number of properties in common with convex and concave functions (Lov´asz, 1983), including their wide applicability, their generality, their multiple options for their representation, and their closure under a number of common operators (including mixtures, truncation, complementation, and certain convolutions). For example, if a collection of functions {FZ}Z is submodular, then so is their weighted sum F = EZ αZFZ where αZ are nonnegative weights. It is not hard to show that submodular functions also have the following </context>
<context position="9791" citStr="Lin and Bilmes, 2010" startWordPosition="1577" endWordPosition="1580"> of the summary set S, the summarization problem can then be formalized as the following combinatorial optimization problem: Problem 1. Find � F(S) subject to: iES Since this is a generalization of the cardinality constraint (where ci = 1, ∀i), this also constitutes a (well-known) NP-hard problem. In this case as well, however, a modified greedy algorithm with partial enumeration can solve Problem 1 near-optimally with (1−1/e)-approximation factor if F is monotone submodular (Sviridenko, 2004). The partial enumeration, however, is too computationally expensive for real world applications. In (Lin and Bilmes, 2010), we generalize the work by Khuller et al. (1999) on the budgeted maximum cover problem to the general submodular framework, and show a practical greedy algorithm with a (1 − 1/√e)-approximation factor, where each greedy step adds the unit with the largest ratio of objective function gain to scaled cost, while not violating the budget constraint (see (Lin and Bilmes, 2010) for details). Note that in all cases, submodularity and monotonicity are two necessary ingredients to guarantee that the greedy algorithm gives near-optimal solutions. In fact, greedy-like algorithms have been widely used in</context>
<context position="12458" citStr="Lin and Bilmes, 2010" startWordPosition="2012" endWordPosition="2015">e weighted credit of concepts covered by the summary. Although the authors may not have noticed, their objective functions are also submodular, adding more evidence suggesting that submodularity is natural for summarization tasks. Indeed, let S be a subset of sentences in the document and denote F(S) as the set of concepts contained in S. The total credit of the concepts covered by S is then o Fconcept(S) E iEr(S) where ci is the credit of concept i. This function is known to be submodular (Narayanan, 1997). S* ∈ argmax SCV max ci ≤ b. iES Sim2(si, sk) ci, 512 Similar to the MMR approach, in (Lin and Bilmes, 2010), a submodular graph based objective function is proposed where a graph cut function, measuring the similarity of the summary to the rest of document, is combined with a subtracted redundancy penalty function. The objective function is submodular but again, non-monotone. We theoretically justify that the performance guarantee of the greedy algorithm holds for this objective function with high probability (Lin and Bilmes, 2010). Our justification, however, is shown to be applicable only to certain particular non-monotone submodular functions, under certain reasonable assumptions about the proba</context>
<context position="17669" citStr="Lin and Bilmes, 2010" startWordPosition="2844" endWordPosition="2847">nd P non-redundancy. Objective functions for extractive iEV,jES wi,j where wi,j represents the similarity summarization usually measure these two separately between i and j. L(S) could also be facility and then mix them together trading off encouraging location objective, i.e., L(S) = PiEV maxjES wi,j, relevance and penalizing redundancy. The redun- as used in (Lin et al., 2009). We could also use dancy penalty usually violates the monotonicity of L(S) = PiEr(S) ci as used in concept-based the objective functions (Carbonell and Goldstein, summarization, where the definition of “concept” 1998; Lin and Bilmes, 2010). We therefore propose and the mechanism to extract these concepts become to positively reward diversity instead of negatively important. All of these are monotone submodular. penalizing redundancy. In particular, we model the Alternatively, in this paper we propose the followsummary quality as ing objective that does not reply on concepts. Let F(S) = L(S) + AR(S), (2) L(S) = X min {Ci(S), a Ci(V )} , (3) where L(S) measures the coverage, or “fidelity”, iEV of summary set S to the document, R(S) rewards where Ci : 2V —* R is a monotone submodular funcdiversity in S, and A &gt; 0 is a trade-off co</context>
<context position="24220" citStr="Lin and Bilmes, 2010" startWordPosition="3979" endWordPosition="3982">l such extensions preserve both monotonicity an was the main forum providing benchmarks for researchers working on document summarization. The tasks in DUC evolved from single-document summarization to multi-document summarization, and from generic summarization to query-focused summarization As ROUGE (Lin, 2004) has been officially adopted for DUC evaluations since 2004, we also take it as our main evaluation criterion. We evaluated our approaches on DUC data 2003-2007, and demonstrate results on both generic and query-focused summarization. In all experiments, the modified greedy algorithm (Lin and Bilmes, 2010) was used for summary (http://duc.nist.org) (2001–2004) (2005–2007). generation. 5.1 Generic summarization our development set, an d tested on DUC-04 data. We show ROUGE-1 scores1 as it was the main evaluation criterion for DUC-03, 04 evaluations. (5) 5 Experiments The document understanding conference (DUC) Summarization tasks in DUC-03 and DUC-04 are multi-document summarization on English news articles. In each task, 50 document clusters are given, each of which consists of 10 documents. For each document cluster, the system generated summary may not be longer than 665 bytes including space</context>
<context position="25915" citStr="Lin and Bilmes (2010)" startWordPosition="4265" endWordPosition="4268">l w where tfw,i and tfw,j are the numbers of times that w appears in si and sentence sj respectively, and idfw is the inverse document frequency (IDF) of term w (up to bigram), which was calculated as the logarithm of the ratio of the number of articles that w appears over the total number of all articles in the document cluster. Table 1: ROUGE-1 recall (R) and F-measure (F) results (%) on DUC-04. DUC-03 was used as development set. DUC-04 R F EiEV EjES wiJ 33.59 32.44 L1(S) 39.03 38.65 R1(S) 38.23 37.81 L1(S) + AR1(S) 39.35 38.90 Takamura and Okumura (2009) 38.50 - Wang et al. (2009) 39.07 - Lin and Bilmes (2010) - 38.39 Best system in DUC-04 (peer 65) 38.28 37.94 We first tested our coverage and diversity reward objectives separately. For coverage, we use a modular Ci(S) = Ej∈S wi,j for each sentence i, i.e., � L1(S) = i∈V When α = 1, L1(S) reduces to Ei∈V,j∈S wi,j, which measures the overall similarity of summary set S to ground set V . As mentioned in Section 4.1, using such similarity measurement could possibly over-concentrate on a small portion of the document and result in a poor coverage of the whole document. As shown in Table 1, optimizing this objective function gives a ROUGE-1 F-measure sc</context>
<context position="28089" citStr="Lin and Bilmes, 2010" startWordPosition="4653" endWordPosition="4656"> a direct K-mean clustering algorithm was used. In this experiment, we set K = 0.2N. In other words, there are 5 sentences in each cluster on average. And as we can see in Table 1, optimizing the diversity reward function alone achieves comparable performance to the DUC-04 best system. Combining L1(S) and R1(S), our system outperforms the best system in DUC-04 significantly, and it also outperforms several recent systems, including a concept-based summarization approach (Takamura and Okumura, 2009), a sentence topic model based system (Wang et al., 2009), and our MMR-styled submodular system (Lin and Bilmes, 2010). Figure 1 illustrates how ROUGE-1 scores change when α and K vary on the development set (DUC-03). 2http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview ROUGE-1 F-measure (%) 37.6 37.4 37.2 36.8 36.6 36.4 36.2 37 0 5 a 10 15 20 K=0.05N K=0.1N K=0.2N wi,j = min { � �wi,j, α wi,k } . (6) j∈S k∈V 516 Table 2: ROUGE-2 recall (R) and F-measure (F) results (%) on DUC-05, where DUC-05 was used as training set. Table 4: ROUGE-2 recall (R) and F-measure (F) results (%) on DUC-06, where DUC-05 was used as training set. DUC-05 R F L1(S) + λRQ(S) 8.38 8.31 Daum´e III and Marcu (2006) 7.62 - Extr, Daum´e</context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>H. Lin and J. Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In North American chapter of the Association for Computational Linguistics/Human Language Technology Conference (NAACL/HLT-2010), Los Angeles, CA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lin</author>
<author>J Bilmes</author>
</authors>
<title>Word alignment via submodular maximization over matroids.</title>
<date>2011</date>
<booktitle>In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),</booktitle>
<location>Portland, OR,</location>
<contexts>
<context position="7394" citStr="Lin and Bilmes, 2011" startWordPosition="1177" endWordPosition="1180">ly be referred to as monotone submodular. Historically, submodular functions have their roots in economics, game theory, combinatorial optimization, and operations research. More recently, submodular functions have started receiving attention in the machine learning and computer vision community (Kempe et al., 2003; Narasimhan and Bilmes, 2005; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2007; Krause et al., 2008; Kolmogorov and Zabin, 2004) and have recently been introduced to natural language processing for the tasks of document summarization (Lin and Bilmes, 2010) and word alignment (Lin and Bilmes, 2011). Submodular functions share a number of properties in common with convex and concave functions (Lov´asz, 1983), including their wide applicability, their generality, their multiple options for their representation, and their closure under a number of common operators (including mixtures, truncation, complementation, and certain convolutions). For example, if a collection of functions {FZ}Z is submodular, then so is their weighted sum F = EZ αZFZ where αZ are nonnegative weights. It is not hard to show that submodular functions also have the following composition property with concave function</context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>H. Lin and J. Bilmes. 2011. Word alignment via submodular maximization over matroids. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), Portland, OR, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lin</author>
<author>J Bilmes</author>
<author>S Xie</author>
</authors>
<title>Graph-based submodular selection for extractive summarization.</title>
<date>2009</date>
<booktitle>In Proc. IEEE Automatic Speech Recognition and Understanding (ASRU),</booktitle>
<location>Merano, Italy,</location>
<contexts>
<context position="17429" citStr="Lin et al., 2009" startWordPosition="2808" endWordPosition="2811">ar function. of concepts, as we will see in the following section. There are several ways to define L(S) in our 4 Monotone Submodular Objectives context. For instance, we could use L(S) = Two properties of a good summary are relevance and P non-redundancy. Objective functions for extractive iEV,jES wi,j where wi,j represents the similarity summarization usually measure these two separately between i and j. L(S) could also be facility and then mix them together trading off encouraging location objective, i.e., L(S) = PiEV maxjES wi,j, relevance and penalizing redundancy. The redun- as used in (Lin et al., 2009). We could also use dancy penalty usually violates the monotonicity of L(S) = PiEr(S) ci as used in concept-based the objective functions (Carbonell and Goldstein, summarization, where the definition of “concept” 1998; Lin and Bilmes, 2010). We therefore propose and the mechanism to extract these concepts become to positively reward diversity instead of negatively important. All of these are monotone submodular. penalizing redundancy. In particular, we model the Alternatively, in this paper we propose the followsummary quality as ing objective that does not reply on concepts. Let F(S) = L(S) +</context>
</contexts>
<marker>Lin, Bilmes, Xie, 2009</marker>
<rawString>H. Lin, J. Bilmes, and S. Xie. 2009. Graph-based submodular selection for extractive summarization. In Proc. IEEE Automatic Speech Recognition and Understanding (ASRU), Merano, Italy, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop.</booktitle>
<contexts>
<context position="15051" citStr="Lin, 2004" startWordPosition="2427" endWordPosition="2428">ily satisfy a summary’s budget constraint. Consequently, a subset of the solution found by solving Problem 2 has to be constructed as the final summary, and the near-optimality is no longer guaranteed. Therefore, solving Problem 1 for document summarization appears to be a better framework regarding global optimality. In the present paper, our framework is that of Problem 1. 3.3 Automatic summarization evaluation Automatic evaluation of summary quality is important for the research of document summarization as it avoids the labor-intensive and potentially inconsistent human evaluation. ROUGE (Lin, 2004) is widely used for summarization evaluation and it has been shown that ROUGE-N scores are highly correlated with human evaluation (Lin, 2004). Interestingly, ROUGE-N is monotone submodular, adding further evidence that monotone submodular functions are natural for document summarization. Theorem 3. ROUGE-N is monotone submodular. Proof. By definition (Lin, 2004), ROUGE-N is the n-gram recall between a candidate summary and a set of reference summaries. Precisely, let S be the candidate summary (a set of sentences extracted from the ground set V ), ce : 2V —* Z+ be the number of times n-gram a</context>
<context position="23913" citStr="Lin, 2004" startWordPosition="3935" endWordPosition="3936">clustering (or partitions) with different sizes (K) can be used, i.e., we can use a mixture of components, each of which has the structure of Eqn. 5. A mixture can better represent the core structure of the ground set (e.g., the hierarchical structure in the documents (Celikyilmaz and 2010)). All such extensions preserve both monotonicity an was the main forum providing benchmarks for researchers working on document summarization. The tasks in DUC evolved from single-document summarization to multi-document summarization, and from generic summarization to query-focused summarization As ROUGE (Lin, 2004) has been officially adopted for DUC evaluations since 2004, we also take it as our main evaluation criterion. We evaluated our approaches on DUC data 2003-2007, and demonstrate results on both generic and query-focused summarization. In all experiments, the modified greedy algorithm (Lin and Bilmes, 2010) was used for summary (http://duc.nist.org) (2001–2004) (2005–2007). generation. 5.1 Generic summarization our development set, an d tested on DUC-04 data. We show ROUGE-1 scores1 as it was the main evaluation criterion for DUC-03, 04 evaluations. (5) 5 Experiments The document understanding </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C.-Y. Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lov´asz</author>
</authors>
<title>Submodular functions and convexity. Mathematical programming-The state of the art,(eds.</title>
<date>1983</date>
<pages>235--257</pages>
<editor>A. Bachem, M. Grotschel and B. Korte</editor>
<publisher>Springer,</publisher>
<marker>Lov´asz, 1983</marker>
<rawString>L. Lov´asz. 1983. Submodular functions and convexity. Mathematical programming-The state of the art,(eds. A. Bachem, M. Grotschel and B. Korte) Springer, pages 235–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Minoux</author>
</authors>
<title>Accelerated greedy algorithms for maximizing submodular set functions. Optimization Techniques,</title>
<date>1978</date>
<pages>234--243</pages>
<contexts>
<context position="5989" citStr="Minoux, 1978" startWordPosition="935" endWordPosition="936">ractable, an unfortunate fact since the optimization coincides with many important applications. For example, F might correspond to the value or coverage of a set of sensor locations in an environment, and the goal is to find the best locations for a fixed number of sensors (Krause et al., 2008). If the function F is monotone submodular then the maximization is still NP complete, but it was shown in (Nemhauser et al., 1978) that a greedy algorithm finds an approximate solution guaranteed to be within e−1 e — 0.63 of the optimal solution, as mentioned in Section 1. A version of this algorithm (Minoux, 1978), moreover, scales to very large data sets. Submodular functions are those that satisfy the property of diminishing returns: for any A C_ B C_ V \v, a submodular function F must satisfy F(A+v)—F(A) &gt; F(B + v) — F(B). That is, the incremental “value” of v decreases as the context in which v is considered grows from A to B. An equivalent definition, useful mathematically, is that for any A, B C_ V , we must have that F(A) + F(B) &gt; F(A U B) + F(A n B). If this is satisfied everywhere with equality, then the function F is called modular, and in such case F(A) = c + EaEA ~fa for a sized |V |vector </context>
</contexts>
<marker>Minoux, 1978</marker>
<rawString>M. Minoux. 1978. Accelerated greedy algorithms for maximizing submodular set functions. Optimization Techniques, pages 234–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Narasimhan</author>
<author>J Bilmes</author>
</authors>
<title>A submodularsupermodular procedure with applications to discriminative structure learning.</title>
<date>2005</date>
<booktitle>In Proc. Conf. Uncertainty in Artifical Intelligence,</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="7118" citStr="Narasimhan and Bilmes, 2005" startWordPosition="1133" endWordPosition="1136">hen the function F is called modular, and in such case F(A) = c + EaEA ~fa for a sized |V |vector f~ of real values and constant c. A set function F is monotone nondecreasing if VA C_ B, F(A) G F(B). As shorthand, in this paper, monotone nondecreasing submodular functions will simply be referred to as monotone submodular. Historically, submodular functions have their roots in economics, game theory, combinatorial optimization, and operations research. More recently, submodular functions have started receiving attention in the machine learning and computer vision community (Kempe et al., 2003; Narasimhan and Bilmes, 2005; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2007; Krause et al., 2008; Kolmogorov and Zabin, 2004) and have recently been introduced to natural language processing for the tasks of document summarization (Lin and Bilmes, 2010) and word alignment (Lin and Bilmes, 2011). Submodular functions share a number of properties in common with convex and concave functions (Lov´asz, 1983), including their wide applicability, their generality, their multiple options for their representation, and their closure under a number of common operators (including mixtures, truncation, complementation, and c</context>
</contexts>
<marker>Narasimhan, Bilmes, 2005</marker>
<rawString>M. Narasimhan and J. Bilmes. 2005. A submodularsupermodular procedure with applications to discriminative structure learning. In Proc. Conf. Uncertainty in Artifical Intelligence, Edinburgh, Scotland, July. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Narasimhan</author>
<author>J Bilmes</author>
</authors>
<title>Local search for balanced submodular clusterings.</title>
<date>2007</date>
<booktitle>In Twentieth International Joint Conference on Artificial Intelligence (IJCAI07),</booktitle>
<location>Hyderabad, India,</location>
<contexts>
<context position="7174" citStr="Narasimhan and Bilmes, 2007" startWordPosition="1141" endWordPosition="1144">(A) = c + EaEA ~fa for a sized |V |vector f~ of real values and constant c. A set function F is monotone nondecreasing if VA C_ B, F(A) G F(B). As shorthand, in this paper, monotone nondecreasing submodular functions will simply be referred to as monotone submodular. Historically, submodular functions have their roots in economics, game theory, combinatorial optimization, and operations research. More recently, submodular functions have started receiving attention in the machine learning and computer vision community (Kempe et al., 2003; Narasimhan and Bilmes, 2005; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2007; Krause et al., 2008; Kolmogorov and Zabin, 2004) and have recently been introduced to natural language processing for the tasks of document summarization (Lin and Bilmes, 2010) and word alignment (Lin and Bilmes, 2011). Submodular functions share a number of properties in common with convex and concave functions (Lov´asz, 1983), including their wide applicability, their generality, their multiple options for their representation, and their closure under a number of common operators (including mixtures, truncation, complementation, and certain convolutions). For example, if a collection of fu</context>
</contexts>
<marker>Narasimhan, Bilmes, 2007</marker>
<rawString>M. Narasimhan and J. Bilmes. 2007. Local search for balanced submodular clusterings. In Twentieth International Joint Conference on Artificial Intelligence (IJCAI07), Hyderabad, India, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Narayanan</author>
</authors>
<title>Submodular functions and electrical networks.</title>
<date>1997</date>
<publisher>North-Holland.</publisher>
<contexts>
<context position="12349" citStr="Narayanan, 1997" startWordPosition="1991" endWordPosition="1992"> 2004; Takamura and Okumura, 2009; Riedhammer et al., 2010; Qazvinian et al., 2010) usually maximizes the weighted credit of concepts covered by the summary. Although the authors may not have noticed, their objective functions are also submodular, adding more evidence suggesting that submodularity is natural for summarization tasks. Indeed, let S be a subset of sentences in the document and denote F(S) as the set of concepts contained in S. The total credit of the concepts covered by S is then o Fconcept(S) E iEr(S) where ci is the credit of concept i. This function is known to be submodular (Narayanan, 1997). S* ∈ argmax SCV max ci ≤ b. iES Sim2(si, sk) ci, 512 Similar to the MMR approach, in (Lin and Bilmes, 2010), a submodular graph based objective function is proposed where a graph cut function, measuring the similarity of the summary to the rest of document, is combined with a subtracted redundancy penalty function. The objective function is submodular but again, non-monotone. We theoretically justify that the performance guarantee of the greedy algorithm holds for this objective function with high probability (Lin and Bilmes, 2010). Our justification, however, is shown to be applicable only </context>
</contexts>
<marker>Narayanan, 1997</marker>
<rawString>H. Narayanan. 1997. Submodular functions and electrical networks. North-Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G L Nemhauser</author>
<author>L A Wolsey</author>
<author>M L Fisher</author>
</authors>
<title>An analysis of approximations for maximizing submodular set functions I. Mathematical Programming,</title>
<date>1978</date>
<volume>14</volume>
<issue>1</issue>
<pages>294</pages>
<contexts>
<context position="5803" citStr="Nemhauser et al., 1978" startWordPosition="901" endWordPosition="904">a real value for any subset S C_ V . We are interested in finding the subset of bounded size |S |G k that maximizes the function, e.g., argmaxScV F(S). In general, this operation is hopelessly intractable, an unfortunate fact since the optimization coincides with many important applications. For example, F might correspond to the value or coverage of a set of sensor locations in an environment, and the goal is to find the best locations for a fixed number of sensors (Krause et al., 2008). If the function F is monotone submodular then the maximization is still NP complete, but it was shown in (Nemhauser et al., 1978) that a greedy algorithm finds an approximate solution guaranteed to be within e−1 e — 0.63 of the optimal solution, as mentioned in Section 1. A version of this algorithm (Minoux, 1978), moreover, scales to very large data sets. Submodular functions are those that satisfy the property of diminishing returns: for any A C_ B C_ V \v, a submodular function F must satisfy F(A+v)—F(A) &gt; F(B + v) — F(B). That is, the incremental “value” of v decreases as the context in which v is considered grows from A to B. An equivalent definition, useful mathematically, is that for any A, B C_ V , we must have </context>
</contexts>
<marker>Nemhauser, Wolsey, Fisher, 1978</marker>
<rawString>G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. 1978. An analysis of approximations for maximizing submodular set functions I. Mathematical Programming, 14(1):265– 294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pingali</author>
<author>K Rahul</author>
<author>V Varma</author>
</authors>
<date>2007</date>
<booktitle>IIIT Hyderabad at DUC 2007. Proceedings of DUC</booktitle>
<contexts>
<context position="34856" citStr="Pingali et al., 2007" startWordPosition="5777" endWordPosition="5780">system outperforms the best system in DUC-06, as well as two recent approaches (Shen and Li, 2010; Celikyilmaz and Hakkani-t¨ur, 2010). On DUC-07, in terms of ROUGE-2 score, our system outperforms PYTHY (Toutanova et al., 2007), a state-of-the-art supervised summarization system, as well as two recent systems including a generative summarization system based on topic models (Haghighi and Vanderwende, 2009), and a hybrid hierarchical summarization system (Celikyilmaz and Hakkani-t¨ur, 2010). It also achieves comparable performance to the best DUC-07 system. Note that in the best DUC-07 system (Pingali et al., 2007; Jagarlamudi et al., 2006), an external web search engine (Yahoo!) was used to estimate a language model for query relevance. In our system, no such web search expansion was used. To further improve the performance of our system, we used both DUC-05 and DUC-06 as a training set, and introduced three diversity reward terms into the objective where three different sentence clusterings with different resolutions were produced (with sizes 0.3N, 0.15N and 0.05N). Denoting a diversity reward corresponding to clustering K as RQ,κ(S), we model the summary quality as L1(S) + E3κ_1 AκRQ,κ(S). As shown </context>
</contexts>
<marker>Pingali, Rahul, Varma, 2007</marker>
<rawString>P. Pingali, K. Rahul, and V. Varma. 2007. IIIT Hyderabad at DUC 2007. Proceedings of DUC 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Qazvinian</author>
<author>D R Radev</author>
<author>A Ozg¨ur</author>
</authors>
<title>Citation Summarization Through Keyphrase Extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>895--903</pages>
<marker>Qazvinian, Radev, Ozg¨ur, 2010</marker>
<rawString>V. Qazvinian, D.R. Radev, and A. Ozg¨ur. 2010. Citation Summarization Through Keyphrase Extraction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 895– 903.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In EMNLP,</booktitle>
<volume>1</volume>
<pages>133--142</pages>
<contexts>
<context position="32449" citStr="Ratnaparkhi, 1996" startWordPosition="5399" endWordPosition="5400">m) that sentence j overlaps the query Q as rj,Q, where the IDF weighting is not used (i.e., every term in the query, after stop word removal, was treated as equally important). Both query-independent and query-dependent scores were then normalized by their largest value respectively such that they had roughly the same dynamic range. To better estimate of the relevance between query and sentences, we further expanded sentences with synonyms and hypernyms of its constituent words. In particular, part-of-speech tags were obtained for each sentence using the maximum entropy part-of-speech tagger (Ratnaparkhi, 1996), and all nouns were then RQ(S) = XK k=1 517 expanded with their synonyms and hypernyms using WordNet (Fellbaum, 1998). Note that these expanded documents were only used in the estimation rj,Q, and we plan to further explore whether there is benefit to use the expanded documents either in sentence similarity estimation or in sentence clustering in our future work. We also tried to expand the query with synonyms and observed a performance decrease, presumably due to noisy information in a query expression. While it is possible to use an approach that is similar to (Toutanova et al., 2007) to le</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In EMNLP, volume 1, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Riedhammer</author>
<author>B Favre</author>
<author>D Hakkani-T¨ur</author>
</authors>
<title>Long story short-Global unsupervised models for keyphrase based meeting summarization. Speech Communication.</title>
<date>2010</date>
<marker>Riedhammer, Favre, Hakkani-T¨ur, 2010</marker>
<rawString>K. Riedhammer, B. Favre, and D. Hakkani-T¨ur. 2010. Long story short-Global unsupervised models for keyphrase based meeting summarization. Speech Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Shen</author>
<author>T Li</author>
</authors>
<title>Multi-document summarization via the minimum dominating set.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>984--992</pages>
<location>Beijing, China,</location>
<contexts>
<context position="2653" citStr="Shen and Li, 2010" startWordPosition="397" endWordPosition="400">portant to note that this is a worst case bound, and in most cases the quality of the solution obtained will be much better than this bound suggests. Of course, none of this is useful if the objective function F is inappropriate for the summarization task. In this paper, we argue that monotone nondecreasing submodular functions F are an ideal class of functions to investigate for document summarization. We show, in fact, that many well-established methods for summarization (Carbonell and Goldstein, 1998; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009; Riedhammer et al., 2010; Shen and Li, 2010) correspond to submodular function optimization, a property not explicitly mentioned in these publications. We take this fact, however, as testament to the value of submodular functions for summarization: if summarization algorithms are repeatedly developed that, by chance, happen to be an instance of a submodular function optimization, this suggests that submodular functions are a natural fit. On the other hand, other authors have started realizing explicitly the value of submodular functions for summarization (Lin and Bilmes, 2010; Qazvinian et al., 2010). Submodular functions share many pro</context>
<context position="13832" citStr="Shen and Li, 2010" startWordPosition="2228" endWordPosition="2231">ng a low-cost subset of the document under the constraint that a summary should cover all (or a sufficient amount of) the information in the document. Formally, this can be expressed as Problem 2. Find 1: S� E argmin ci subject to: F(S) &gt; a, SCV iES where ci are the element costs, and set function F(S) measure the information covered by S. When F is submodular, the constraint F(S) &gt; a is called a submodular cover constraint. When F is monotone submodular, a greedy algorithm that iteratively selects k with minimum ck/(F(S U {k}) − F(S)) has approximation guarantees (Wolsey, 1982). Recent work (Shen and Li, 2010) proposes to model document summarization as finding a minimum dominating set and a greedy algorithm is used to solve the problem. The dominating set constraint is also a submodular cover constraint. Define S(S) be the set of elements that is either in S or is adjacent to some element in S. Then S is a dominating set if |S(S) |= |V |. Note that Fdom(S) g |S(S)| is monotone submodular. The dominating set constraint is then also a submodular cover constraint, and therefore the approaches in (Shen and Li, 2010) are special cases of Problem 2. The solutions found in this framework, however, do not</context>
<context position="30706" citStr="Shen and Li (2010)" startWordPosition="5108" endWordPosition="5111">i(S) could be query-dependent in how it measures how much query-dependent information in i is covered by S. Also, the coefficient a could be query and sentence dependent, where it takes larger value when a sentence is more relevant to query (i.e., a larger value of a means later truncation, and therefore more possible coverage). Similarly, sentence clustering and singleton rewards in the diversity function can also 3ROUGE version 1.5.5 was used with option -n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d -l 250 DUC-06 R F L1(S) + λRQ(S) 9.75 9.77 Celikyilmaz and Hakkani-t¨ur (2010) 9.10 - Shen and Li (2010) 9.30 - Best system in DUC-06 (peer 24) 9.51 9.51 Table 5: ROUGE-2 recall (R) and F-measure (F) results (%) on DUC-07. DUC-05 was used as training set for objective L1(S) + ARQ(S). DUC-05 and DUC06 were used as training sets for objective L1(S) + P �A,�RQ,,�(S). DUC-07 R F L1(S) + λRQ(S) 12.18 12.13 L1(S) + E3=1 λ�RQ,�(S) 12.38 12.33 Toutanova et al. (2007) 11.89 11.89 Haghighi and Vanderwende (2009) 11.80 - Celikyilmaz and Hakkani-t¨ur (2010) 11.40 - Best system in DUC-07 (peer 15) 12.45 12.29 be query-dependent. In this experiment, we explore an objective with a query-independent coverage fu</context>
<context position="34333" citStr="Shen and Li, 2010" startWordPosition="5703" endWordPosition="5706">so called “vine-growth” system, which can be seen as an abstractive approach, whereas our system is purely an extractive system. Comparing to the extractive system in (Daum´e et al., 2009), our system performs much better (8.38% v.s. 7.67%). More importantly, when trained only on DUC-06 and tested on DUC-05 (results in Table 3), our approach outperforms the best system in DUC-05 significantly. We further tested the system trained on DUC-05 on both DUC-06 and DUC-07. The results on DUC-06 are shown in Table. 4. Our system outperforms the best system in DUC-06, as well as two recent approaches (Shen and Li, 2010; Celikyilmaz and Hakkani-t¨ur, 2010). On DUC-07, in terms of ROUGE-2 score, our system outperforms PYTHY (Toutanova et al., 2007), a state-of-the-art supervised summarization system, as well as two recent systems including a generative summarization system based on topic models (Haghighi and Vanderwende, 2009), and a hybrid hierarchical summarization system (Celikyilmaz and Hakkani-t¨ur, 2010). It also achieves comparable performance to the best DUC-07 system. Note that in the best DUC-07 system (Pingali et al., 2007; Jagarlamudi et al., 2006), an external web search engine (Yahoo!) was used </context>
</contexts>
<marker>Shen, Li, 2010</marker>
<rawString>C. Shen and T. Li. 2010. Multi-document summarization via the minimum dominating set. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 984–992, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sviridenko</author>
</authors>
<title>A note on maximizing a submodular set function subject to a knapsack constraint.</title>
<date>2004</date>
<journal>Operations Research Letters,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="9668" citStr="Sviridenko, 2004" startWordPosition="1561" endWordPosition="1562">., the number of words in the sentence) and b is our budget. If we use a set function F : 2V → R to measure the quality of the summary set S, the summarization problem can then be formalized as the following combinatorial optimization problem: Problem 1. Find � F(S) subject to: iES Since this is a generalization of the cardinality constraint (where ci = 1, ∀i), this also constitutes a (well-known) NP-hard problem. In this case as well, however, a modified greedy algorithm with partial enumeration can solve Problem 1 near-optimally with (1−1/e)-approximation factor if F is monotone submodular (Sviridenko, 2004). The partial enumeration, however, is too computationally expensive for real world applications. In (Lin and Bilmes, 2010), we generalize the work by Khuller et al. (1999) on the budgeted maximum cover problem to the general submodular framework, and show a practical greedy algorithm with a (1 − 1/√e)-approximation factor, where each greedy step adds the unit with the largest ratio of objective function gain to scaled cost, while not violating the budget constraint (see (Lin and Bilmes, 2010) for details). Note that in all cases, submodularity and monotonicity are two necessary ingredients to</context>
</contexts>
<marker>Sviridenko, 2004</marker>
<rawString>M. Sviridenko. 2004. A note on maximizing a submodular set function subject to a knapsack constraint. Operations Research Letters, 32(1):41–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Takamura</author>
<author>M Okumura</author>
</authors>
<title>Text summarization model based on maximum coverage problem and its variant.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>781--789</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2608" citStr="Takamura and Okumura, 2009" startWordPosition="389" endWordPosition="392"> even very large size problems do well. It is also important to note that this is a worst case bound, and in most cases the quality of the solution obtained will be much better than this bound suggests. Of course, none of this is useful if the objective function F is inappropriate for the summarization task. In this paper, we argue that monotone nondecreasing submodular functions F are an ideal class of functions to investigate for document summarization. We show, in fact, that many well-established methods for summarization (Carbonell and Goldstein, 1998; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009; Riedhammer et al., 2010; Shen and Li, 2010) correspond to submodular function optimization, a property not explicitly mentioned in these publications. We take this fact, however, as testament to the value of submodular functions for summarization: if summarization algorithms are repeatedly developed that, by chance, happen to be an instance of a submodular function optimization, this suggests that submodular functions are a natural fit. On the other hand, other authors have started realizing explicitly the value of submodular functions for summarization (Lin and Bilmes, 2010; Qazvinian et al</context>
<context position="11766" citStr="Takamura and Okumura, 2009" startWordPosition="1891" endWordPosition="1894">it si and unit sk, and 0 ≤ λ ≤ 1 is a trade-off coefficient. We have: Theorem 2. Given an expression for FMMR such that FMMR(S ∪ {k}) −FMMR(S) is equal to Eq. 1, FMMR is non-monotone submodular. Obviously, diminishing-returns hold since Sim2(si, sk) ≤ max iER for all S ⊆ R, and therefore FMMR is submodular. On the other hand, FMMR, would not be monotone, so the greedy algorithm’s constant-factor approximation guarantee does not apply in this case. When scoring a summary at the sub-sentence level, submodularity naturally arises. Concept-based summarization (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009; Riedhammer et al., 2010; Qazvinian et al., 2010) usually maximizes the weighted credit of concepts covered by the summary. Although the authors may not have noticed, their objective functions are also submodular, adding more evidence suggesting that submodularity is natural for summarization tasks. Indeed, let S be a subset of sentences in the document and denote F(S) as the set of concepts contained in S. The total credit of the concepts covered by S is then o Fconcept(S) E iEr(S) where ci is the credit of concept i. This function is known to be submodular (Narayanan, 1997). S* ∈ argmax SCV</context>
<context position="16451" citStr="Takamura and Okumura, 2009" startWordPosition="2654" endWordPosition="2657">ritten as the following set function: EK p1 EeERi min(ce(S), re,i) ROUGE-N(S) = K Ei=1 EeERi re,i where re,i is the number of times n-gram a occurs in reference summary i. Since ce(S) is monotone modular and min(x, a) is a concave non-decreasing function of x, min(ce(S), re,i) is monotone submodular by Theorem 1. Since summation preserves submodularity, and the denominator is constant, we see that FROUGE-N is monotone submodular. Since the reference summaries are unknown, it is of course impossible to optimize FROUGE-N directly. Therefore, some approaches (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009; Riedhammer et al., 2010) instead define “concepts”. Alter, 513 natively, we herein propose a class of monotone sub- the property of diminishing returns. Indeed, Shanmodular functions that naturally models the quality of non entropy, as the measurement of information, is a summary while not depending on an explicit notion another well-known monotone submodular function. of concepts, as we will see in the following section. There are several ways to define L(S) in our 4 Monotone Submodular Objectives context. For instance, we could use L(S) = Two properties of a good summary are relevance and </context>
<context position="25858" citStr="Takamura and Okumura (2009)" startWordPosition="4253" endWordPosition="4256">si tfw,i X tfw,j X idf2w 2 2 2 &apos; 2� EwEsi �w,sil W&apos; EwEs� tfw,jl w where tfw,i and tfw,j are the numbers of times that w appears in si and sentence sj respectively, and idfw is the inverse document frequency (IDF) of term w (up to bigram), which was calculated as the logarithm of the ratio of the number of articles that w appears over the total number of all articles in the document cluster. Table 1: ROUGE-1 recall (R) and F-measure (F) results (%) on DUC-04. DUC-03 was used as development set. DUC-04 R F EiEV EjES wiJ 33.59 32.44 L1(S) 39.03 38.65 R1(S) 38.23 37.81 L1(S) + AR1(S) 39.35 38.90 Takamura and Okumura (2009) 38.50 - Wang et al. (2009) 39.07 - Lin and Bilmes (2010) - 38.39 Best system in DUC-04 (peer 65) 38.28 37.94 We first tested our coverage and diversity reward objectives separately. For coverage, we use a modular Ci(S) = Ej∈S wi,j for each sentence i, i.e., � L1(S) = i∈V When α = 1, L1(S) reduces to Ei∈V,j∈S wi,j, which measures the overall similarity of summary set S to ground set V . As mentioned in Section 4.1, using such similarity measurement could possibly over-concentrate on a small portion of the document and result in a poor coverage of the whole document. As shown in Table 1, optimi</context>
<context position="27971" citStr="Takamura and Okumura, 2009" startWordPosition="4634" endWordPosition="4637">, k = 1, · · · K, we used CLUTO2 to cluster the sentences, where the IDFweighted term vector was used as feature vector, and a direct K-mean clustering algorithm was used. In this experiment, we set K = 0.2N. In other words, there are 5 sentences in each cluster on average. And as we can see in Table 1, optimizing the diversity reward function alone achieves comparable performance to the DUC-04 best system. Combining L1(S) and R1(S), our system outperforms the best system in DUC-04 significantly, and it also outperforms several recent systems, including a concept-based summarization approach (Takamura and Okumura, 2009), a sentence topic model based system (Wang et al., 2009), and our MMR-styled submodular system (Lin and Bilmes, 2010). Figure 1 illustrates how ROUGE-1 scores change when α and K vary on the development set (DUC-03). 2http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview ROUGE-1 F-measure (%) 37.6 37.4 37.2 36.8 36.6 36.4 36.2 37 0 5 a 10 15 20 K=0.05N K=0.1N K=0.2N wi,j = min { � �wi,j, α wi,k } . (6) j∈S k∈V 516 Table 2: ROUGE-2 recall (R) and F-measure (F) results (%) on DUC-05, where DUC-05 was used as training set. Table 4: ROUGE-2 recall (R) and F-measure (F) results (%) on DUC-06, wher</context>
</contexts>
<marker>Takamura, Okumura, 2009</marker>
<rawString>H. Takamura and M. Okumura. 2009. Text summarization model based on maximum coverage problem and its variant. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 781–789. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C Brockett</author>
<author>M Gamon</author>
<author>J Jagarlamudi</author>
<author>H Suzuki</author>
<author>L Vanderwende</author>
</authors>
<title>The PYTHY summarization system: Microsoft research at DUC</title>
<date>2007</date>
<booktitle>In the proceedings of Document Understanding Conference.</booktitle>
<contexts>
<context position="31065" citStr="Toutanova et al. (2007)" startWordPosition="5175" endWordPosition="5178">stering and singleton rewards in the diversity function can also 3ROUGE version 1.5.5 was used with option -n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d -l 250 DUC-06 R F L1(S) + λRQ(S) 9.75 9.77 Celikyilmaz and Hakkani-t¨ur (2010) 9.10 - Shen and Li (2010) 9.30 - Best system in DUC-06 (peer 24) 9.51 9.51 Table 5: ROUGE-2 recall (R) and F-measure (F) results (%) on DUC-07. DUC-05 was used as training set for objective L1(S) + ARQ(S). DUC-05 and DUC06 were used as training sets for objective L1(S) + P �A,�RQ,,�(S). DUC-07 R F L1(S) + λRQ(S) 12.18 12.13 L1(S) + E3=1 λ�RQ,�(S) 12.38 12.33 Toutanova et al. (2007) 11.89 11.89 Haghighi and Vanderwende (2009) 11.80 - Celikyilmaz and Hakkani-t¨ur (2010) 11.40 - Best system in DUC-07 (peer 15) 12.45 12.29 be query-dependent. In this experiment, we explore an objective with a query-independent coverage function (R1(S)), indicating prior importance, combined with a query-dependent diversity reward function, where the latter is defined as: v uX X N wi,j + (1 − Q)rj,Q jESnPk iEV where 0 ≤ Q ≤ 1, and rj,Q represents the relevance between sentence j to query Q. This query-dependent reward function is derived by using a singleton reward that is expressed as a con</context>
<context position="33043" citStr="Toutanova et al., 2007" startWordPosition="5500" endWordPosition="5503">ch tagger (Ratnaparkhi, 1996), and all nouns were then RQ(S) = XK k=1 517 expanded with their synonyms and hypernyms using WordNet (Fellbaum, 1998). Note that these expanded documents were only used in the estimation rj,Q, and we plan to further explore whether there is benefit to use the expanded documents either in sentence similarity estimation or in sentence clustering in our future work. We also tried to expand the query with synonyms and observed a performance decrease, presumably due to noisy information in a query expression. While it is possible to use an approach that is similar to (Toutanova et al., 2007) to learn the coefficients in our objective function, we trained all coefficients to maximize ROUGE-2 F-measure score using the Nelder-Mead (derivative-free) method. Using L1(S)+ARQ(S) as the objective and with the same sentence clustering algorithm as in the generic summarization experiment (K = 0.2N), our system, when both trained and tested on DUC-05 (results in Table 2), outperforms the Bayesian query-focused summarization approach and the search-based structured prediction approach, which were also trained and tested on DUC-05 (Daum´e et al., 2009). Note that the system in (Daum´e et al.,</context>
<context position="34463" citStr="Toutanova et al., 2007" startWordPosition="5722" endWordPosition="5725">tem. Comparing to the extractive system in (Daum´e et al., 2009), our system performs much better (8.38% v.s. 7.67%). More importantly, when trained only on DUC-06 and tested on DUC-05 (results in Table 3), our approach outperforms the best system in DUC-05 significantly. We further tested the system trained on DUC-05 on both DUC-06 and DUC-07. The results on DUC-06 are shown in Table. 4. Our system outperforms the best system in DUC-06, as well as two recent approaches (Shen and Li, 2010; Celikyilmaz and Hakkani-t¨ur, 2010). On DUC-07, in terms of ROUGE-2 score, our system outperforms PYTHY (Toutanova et al., 2007), a state-of-the-art supervised summarization system, as well as two recent systems including a generative summarization system based on topic models (Haghighi and Vanderwende, 2009), and a hybrid hierarchical summarization system (Celikyilmaz and Hakkani-t¨ur, 2010). It also achieves comparable performance to the best DUC-07 system. Note that in the best DUC-07 system (Pingali et al., 2007; Jagarlamudi et al., 2006), an external web search engine (Yahoo!) was used to estimate a language model for query relevance. In our system, no such web search expansion was used. To further improve the per</context>
</contexts>
<marker>Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, Vanderwende, 2007</marker>
<rawString>K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi, H. Suzuki, and L. Vanderwende. 2007. The PYTHY summarization system: Microsoft research at DUC 2007. In the proceedings of Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wang</author>
<author>S Zhu</author>
<author>T Li</author>
<author>Y Gong</author>
</authors>
<title>Multidocument summarization using sentence-based topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>297--300</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="25885" citStr="Wang et al. (2009)" startWordPosition="4259" endWordPosition="4262">EwEsi �w,sil W&apos; EwEs� tfw,jl w where tfw,i and tfw,j are the numbers of times that w appears in si and sentence sj respectively, and idfw is the inverse document frequency (IDF) of term w (up to bigram), which was calculated as the logarithm of the ratio of the number of articles that w appears over the total number of all articles in the document cluster. Table 1: ROUGE-1 recall (R) and F-measure (F) results (%) on DUC-04. DUC-03 was used as development set. DUC-04 R F EiEV EjES wiJ 33.59 32.44 L1(S) 39.03 38.65 R1(S) 38.23 37.81 L1(S) + AR1(S) 39.35 38.90 Takamura and Okumura (2009) 38.50 - Wang et al. (2009) 39.07 - Lin and Bilmes (2010) - 38.39 Best system in DUC-04 (peer 65) 38.28 37.94 We first tested our coverage and diversity reward objectives separately. For coverage, we use a modular Ci(S) = Ej∈S wi,j for each sentence i, i.e., � L1(S) = i∈V When α = 1, L1(S) reduces to Ei∈V,j∈S wi,j, which measures the overall similarity of summary set S to ground set V . As mentioned in Section 4.1, using such similarity measurement could possibly over-concentrate on a small portion of the document and result in a poor coverage of the whole document. As shown in Table 1, optimizing this objective functio</context>
<context position="28028" citStr="Wang et al., 2009" startWordPosition="4644" endWordPosition="4647">he IDFweighted term vector was used as feature vector, and a direct K-mean clustering algorithm was used. In this experiment, we set K = 0.2N. In other words, there are 5 sentences in each cluster on average. And as we can see in Table 1, optimizing the diversity reward function alone achieves comparable performance to the DUC-04 best system. Combining L1(S) and R1(S), our system outperforms the best system in DUC-04 significantly, and it also outperforms several recent systems, including a concept-based summarization approach (Takamura and Okumura, 2009), a sentence topic model based system (Wang et al., 2009), and our MMR-styled submodular system (Lin and Bilmes, 2010). Figure 1 illustrates how ROUGE-1 scores change when α and K vary on the development set (DUC-03). 2http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview ROUGE-1 F-measure (%) 37.6 37.4 37.2 36.8 36.6 36.4 36.2 37 0 5 a 10 15 20 K=0.05N K=0.1N K=0.2N wi,j = min { � �wi,j, α wi,k } . (6) j∈S k∈V 516 Table 2: ROUGE-2 recall (R) and F-measure (F) results (%) on DUC-05, where DUC-05 was used as training set. Table 4: ROUGE-2 recall (R) and F-measure (F) results (%) on DUC-06, where DUC-05 was used as training set. DUC-05 R F L1(S) + λRQ</context>
</contexts>
<marker>Wang, Zhu, Li, Gong, 2009</marker>
<rawString>D. Wang, S. Zhu, T. Li, and Y. Gong. 2009. Multidocument summarization using sentence-based topic models. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 297–300, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Wolsey</author>
</authors>
<title>An analysis of the greedy algorithm for the submodular set covering problem.</title>
<date>1982</date>
<journal>Combinatorica,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="13799" citStr="Wolsey, 1982" startWordPosition="2223" endWordPosition="2224">mmarization problem as finding a low-cost subset of the document under the constraint that a summary should cover all (or a sufficient amount of) the information in the document. Formally, this can be expressed as Problem 2. Find 1: S� E argmin ci subject to: F(S) &gt; a, SCV iES where ci are the element costs, and set function F(S) measure the information covered by S. When F is submodular, the constraint F(S) &gt; a is called a submodular cover constraint. When F is monotone submodular, a greedy algorithm that iteratively selects k with minimum ck/(F(S U {k}) − F(S)) has approximation guarantees (Wolsey, 1982). Recent work (Shen and Li, 2010) proposes to model document summarization as finding a minimum dominating set and a greedy algorithm is used to solve the problem. The dominating set constraint is also a submodular cover constraint. Define S(S) be the set of elements that is either in S or is adjacent to some element in S. Then S is a dominating set if |S(S) |= |V |. Note that Fdom(S) g |S(S)| is monotone submodular. The dominating set constraint is then also a submodular cover constraint, and therefore the approaches in (Shen and Li, 2010) are special cases of Problem 2. The solutions found i</context>
</contexts>
<marker>Wolsey, 1982</marker>
<rawString>L.A. Wolsey. 1982. An analysis of the greedy algorithm for the submodular set covering problem. Combinatorica, 2(4):385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Zhao</author>
<author>G Rocha</author>
<author>B Yu</author>
</authors>
<title>Grouped and hierarchical model selection through composite absolute penalties. Annals of Statistics,</title>
<date>2009</date>
<pages>37--6</pages>
<contexts>
<context position="22662" citStr="Zhao et al., 2009" startWordPosition="3737" endWordPosition="3740">te, is distinct from in that might wish to include certain outlier material that could ignore. It is easy to show that is submodular by using the composition rule from Theorem 1. The square root is non-decreasing concave function. Inside each square root lies a modular function with non-negative weights (and thus is monotone). Applying the square root to such a monotone submodular function yields a submodular function, and summing them all together retains submodularity, as mentioned in Section 2. The of is straightforward. Note, the form of Eqn. 5 is similar to structured group norms (e.g., (Zhao et al., 2009)), recently shown to be related to submodularity (Bach, 2010; Jegelka an Pi, ··· UiPi ri ri R(S) k1, P1, rk1 k1 √13 R(S) G(S) R(S) G(S) R(S) monotonicity R(S) d Bilmes, 2011). f(x) R(S), Hakkani-t¨ur, d submodularity. 515 Several extensions to Eqn. 5 are discussed next: First, instead of using a ground set partition, intersecting clusters can be used. Second, the square root function in Eqn. 5 can be replaced with any other non-decreasing concave functions (e.g., = log(1 + x)) while preserving the desired property of and the curvature of the concave function then determines the rate that the r</context>
</contexts>
<marker>Zhao, Rocha, Yu, 2009</marker>
<rawString>P. Zhao, G. Rocha, and B. Yu. 2009. Grouped and hierarchical model selection through composite absolute penalties. Annals of Statistics, 37(6A):3468–3497.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>