<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997379">
A State-Transition Grammar for Data-Oriented Parsing
</title>
<author confidence="0.994475">
David Tugwell*
</author>
<affiliation confidence="0.8705475">
Centre for Cognitive Science, University of Edinburgh
2, Buccleuch Place, Edinburgh EH8 9LW, Scotland
</affiliation>
<email confidence="0.993682">
davidt@cogsci.ed.ac.uk
</email>
<sectionHeader confidence="0.979854" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929166666667">
This paper presents a grammar formalism de-
signed for use in data-oriented approaches to lan-
guage processing. It goes on to investigate ways
in which a corpus pre-parsed with this formalism
may be processed to provide a probabilistic lan-
guage model for use in the parsing of fresh texts.
</bodyText>
<sectionHeader confidence="0.911466" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.98999444">
Recent years have seen a resurgence of interest
in probabilistic techniques for automatic language
analysis. In particular, there has arisen a dis-
tinct paradigm of processing on the basis of pre-
analyzed data which has taken the name Data-
Oriented Parsing.
&amp;quot;Data Oriented Parsing (DOP) is a model
where no abstract rules, but language experi-
ences in the form of an analyzed corpus, con-
stitute the basis for language processing.&amp;quot;1
There is not space here to present full justification
for adopting such an approach or to detail the ad-
vantages that it offers. The main claim it makes is
that effective language processing requires a con-
sideration of both the structural and statistical as-
pects of language, whereas traditional competence
grammars rely only on the former, and standard
statistical techniques such as n-gram models only
on the latter. DOP attempts to combine these two
traditions and produce &amp;quot;performance grammars&amp;quot;,
which:
&amp;quot;... should not only contain information
on the structural possibilities of the general
language system, but also on details of actual
language use in a language community...&amp;quot;2
</bodyText>
<footnote confidence="0.8959395">
*This research was funded by a research studentship
from the ESRC. My thanks also for discussion and com-
ments to Matt Crocker, Chris Brew, David Milward and
Anna Babarczy.
1Bod, 1992.
2ibid.
</footnote>
<bodyText confidence="0.999871615384615">
This approach entails however that a corpus has
first to be pre-analyzed (ie. hand-parsed), and the
question immediately arises as to the formalism
to be used for this. There is no lack of compet-
ing competence grammars available, but also no
reason to expect that such grammars should be
suited to a DOP approach, designed as they were
to characterize the nature of linguistic competence
rather than performance.
The next section sets out some of the properties
that we might require from such a &amp;quot;performance
grammar&amp;quot; and offers a formalism which attempts
to satisfy these requirements.
</bodyText>
<sectionHeader confidence="0.693117" genericHeader="method">
A Formalism for DOP
</sectionHeader>
<bodyText confidence="0.9784735625">
Given that we are attempting to construct a for-
malism that will do justice to both the statistical
and structural aspects of language, the features
that we would wish to maximize will include the
following:
1. The formalism should be easy to use with prob-
abilistic processing techniques, ideally having a
close correspondence to a simple probabilistic
model such as a Markov process.
2. The formalism should&apos; be fine-grained, ie. re-
sponsive to the behaviour of individual words
(as n-gram models are). This suggests a radi-
cally lexicalist approach (cf. Karttunen, 1990)
in which all rules are encoded in the lexicon,
there being no phrase structure rules which do
not introduce lexical items.
</bodyText>
<listItem confidence="0.98740575">
3. It should be capable of capturing fully the lin-
guistic intuitions of language users. In other
words, using the formalism one should be able
to characterize the structural regularities of lan-
guage with at least the sophistication of modern
competence grammars.
4. As it is to be used with real data, the formalism
should be able to characterize the wide range
</listItem>
<page confidence="0.996215">
272
</page>
<bodyText confidence="0.994547642857143">
of syntactic structures found in actual language
use, including those normally excluded by com-
petence grammars as belonging to the &amp;quot;pe-
riphery&amp;quot; of the language or as being &amp;quot;ungram-
matical&amp;quot;. Ideally every interpretable utterance
should have one and only one analysis for any
interpretation of it.
Considering the first of these points, namely a
close relation to a simple probabilistic model, a
good place to start the search might be with a
right-branching finite-state grammar. In this
class of grammars every rule has the form A -* a
B (A,B E {non-terminals}, a E {terminals}) and
all trees have the simple structure :
</bodyText>
<equation confidence="0.77610975">
a A
A— B— C— D-
1 1 1 1 Or:
a b c d
</equation>
<bodyText confidence="0.9969109">
(with an equivalent vertical alignment, henceforth
to be used in this paper, on the right)
In probabilistic terms, a finite-state grammar cor-
responds to a first-order Markov process, where
given a sequence of states Si, Sj,... drawn from
a finite set of possible states {S0,..,S} the prob-
ability of a particular state occurring depends
solely on the identity of the previous state. In
the finite-state grammar each word is associated
with a transition between two categories, in the
tree above &apos;a&apos; with the transition A B and so
on. To calculate the probability that a string of
words x1, x2, x3,... xn, has the parse represented
by the string of category-states S1, S29 S39•••SM
we simply take the product of the probability of
each transition: ie. rr=i P(x, : Si+1).
In addition to satisfying our first criterion, a finite-
state grammar also fulfills the requirement that
the formalism be radically lexicalist, as by defini-
tion every rule introduces a lexical item.
</bodyText>
<subsectionHeader confidence="0.866579">
Accounting for Linguistic Structure
</subsectionHeader>
<bodyText confidence="0.996818235294118">
If a finite-state grammar is chosen however, the
third criterion, that of linguistic adequacy, seems
to present an insurmountable stumbling block.
How can such a simple formalism, in which syntax
is reduced to a string of category-states, hope to
capture even the basic hierarchical structure, the
familiar &amp;quot;tree structure&amp;quot;, of linguistic expressions?
Indeed, if the non-terminals are viewed as atomic
categories then there is no way this can be done. If
however, in line with most current theories, cat-
egories are taken to be bundles of features and
crucially if one of these features has the value of a
stack of categories, then this hierarchical struc-
ture can indeed be represented.
Using the notation A [B] to represent a state of
basic category A carrying a category B on its
stack, the hierarchical structure of the sentence:
</bodyText>
<listItem confidence="0.454316">
(1) The man gave the dog a bone.
can be represented as:
</listItem>
<equation confidence="0.560759285714286">
The S [ ]
man N [VP]
gave VP [ ]
(la) the NP [NP]
dog N [NP]
a NP [ ]
bone N [ ]
</equation>
<bodyText confidence="0.996989466666667">
Intuitively, syntactic links between non-adjacent
words, impossible in a standard finite-state gram-
mar, are here established by passing categories
along on the stack &amp;quot;through&amp;quot; the state of inter-
vening words. That such a formalism can fully
capture basic linguistic structures is confirmed by
the proof in Aho (1968) that an indexed gram-
mar (ie. one where categories are supplemented
with a stack of unbounded length, as above), if
restricted to right linear trees (also as above), is
equivalent to a context-free grammar.
A perusal of the state transitions associated with
individual words in (la) reveals an obvious re-
lationship to the &amp;quot;types&amp;quot; of categorial grammar.
Using a to represent a list of categories (possibly
null), we arrive at the following transitions (with
their corresponding categorial types alongside).
The ditransitive verb &apos;gave&apos; is
VP [a]3 --+ NP [NP,a] (VP/NP)/NP
Determiners in complement position are both:
NP [a] N [a] NP/N
Determiner in subject position is `type-raised&apos;4 to:
S [a] -4 N [VP,a] (S/VP)/N
The common nouns are all:
N [a] -+ a
In fact as no intermediate constituents are formed
in the analysis, an even closer parallel is to a de-
pendency syntax where only rightward pointing
arrows are allowed, of which the formalism as pre-
sented above is a notational variant. This lack of
</bodyText>
<footnote confidence="0.8548082">
3&amp;quot;VP&amp;quot; is used here and henceforth as a shorthand for
an S with a missing (ie. &amp;quot;slashed&amp;quot;) subject.
4The unidirectionality of the formalism results in an
automatic type-raising of all categories appearing before
their heads.
</footnote>
<page confidence="0.997871">
273
</page>
<bodyText confidence="0.986200428571429">
intermediate constituents has the added benefit
that no &amp;quot;spurious ambiguities&amp;quot; can arise.
Knowing now that the addition of a stack-valued
feature suffices to capture the basic hierarchi-
cal structure of language, additional features can
be used to deal with other syntactic relations.
For example, following the example of GPSG,
unbounded dependencies can be captured using
&amp;quot;slashed&amp;quot; categories. If we represent a &amp;quot;slashed&amp;quot;
category X with the lower case x, and use the no-
tation A(b) for a category A carrying a feature
b, then the topicalized sentence:
(2) This bone the man gave the puppy.
will have the analysis:
</bodyText>
<equation confidence="0.997409">
S [I
N [S(np)]
S(np) H
N [VP (np)]
VP (np) [ 11
NP H
N [ ]
</equation>
<bodyText confidence="0.9686885">
Although there is no space in this paper to go
into greater detail, further constructions involving
unbounded dependency and complement control
phenomena can be captured in similar ways.
Coverage
The criterion that remains to be satisfied is that
of width of coverage: can the formalism cope
with the many &amp;quot;peripheral&amp;quot; structures found in
real written and spoken texts? As it stands the
formalism is weakly equivalent to a context-free
grammar and as such will have problems dealing
with phenomena like discontinuous constituents,
non-constituent coordination and gapping. For-
tunately if extensions are made to the formalism,
necessarily taking it outside weak equivalence to
a context-free grammar, natural and general anal-
yses present themselves for such constructions.
Two of these will now be sketched.
</bodyText>
<sectionHeader confidence="0.64674" genericHeader="method">
Discontinuous Constituents
</sectionHeader>
<bodyText confidence="0.997372666666667">
Consider the pair of sentences (3) and (4), iden-
tical in interpretation, but the latter containing a
discontinuous noun phrase and the former not:
</bodyText>
<listItem confidence="0.992431">
(3) I saw a dog which had no nose yesterday.
(4) I saw a dog yesterday which had no nose.
which have the respective analyses:
</listItem>
<table confidence="0.998016722222222">
saw S H =
a VP [I &apos;time adjunct&apos;
dog NP [NP(t)] &apos;rel&apos; =
(3a) which N [NP(t)] &apos;relative&apos;
had S(rel) [NP(t)]
no VP [NP(t)]
nose NP [NP(t)]
yesterday N [NP (t )]
saw NP(t)[ ]
a S [ ]
dog VP [I
(4a) yesterday NP [NP(t)]
which N [NP(t)]
had NP (t) [S(rel)]
no S(rel) [ 1
nose VP H
NP H
N [ ]
</table>
<bodyText confidence="0.99524275">
The only transition in (4a) that differs from that
of the corresponding word in the &apos;core&apos; variant
(3a) is that of &apos;dog&apos; which has the respective tran-
sitions:
</bodyText>
<equation confidence="0.973439">
N [NP(t)] -+ S(rel) [NP(t)] (in 3a)
N [NP(t)] -4 NP(t) [S(rel)] (in 4a)
</equation>
<bodyText confidence="0.992326956521739">
Both nouns introduce a relative clause modifier
S(rel), the difference being that in the discon-
tinuous variant a category has been taken off the
stack at the same time as the modifier has been
placed on the stack. It has been assumed so far
that we are using a right-linear indexed grammar,
but such a rule is expressly disallowed in an in-
dexed grammar and so allowing transitions of this
kind ends the formalism&apos;s weak equivalence to the
context-free grammars.
Of course, having allowed such crossed dependen-
cies, there is nothing in the formalism itself that
will disallow a similar analysis for a discontinuity
unacceptable in English such as:
(5) I saw a yesterday dog.
This does not present a problem, however, as in
DOP it is information in the parsed corpus which
determines the structures that are possible. There
is no need to explicitly rule out (5), as the transi-
tion NP [a] -+ a [N] will be vanishingly rare in
any corpus of even the most garbled speech, while
the transition N [a] -* a [S(rel)] is commonly
met with in both written and spoken English.
</bodyText>
<subsectionHeader confidence="0.818177">
Non-Constituent Coordination
</subsectionHeader>
<bodyText confidence="0.936332">
The analysis of standard coordination is shown in
(6):
</bodyText>
<figure confidence="0.973533166666667">
This
bone
the
(2a) man
gave
the
puppy
274
Fido S [I
gnawed VP [ ]
a NP [VP(+)]
(6) bone N [VP(+)]
and VP(+)[ ]
barked VP [
Instead of a typical transition for &apos;gnawed&apos; of VP
NP, we have a transition introducing a coor-
dinated VP: VP -4 NP [VP(+)]
In general for any transition X Y , where X
</figure>
<figureCaption confidence="0.863992">
is a category and Y a list of categories (possibly
empty), there will be a transition introducing co-
ordination: X -4 Y [X(-1-)]
</figureCaption>
<figure confidence="0.790012">
Non-constituent coordinations such as (7) present
serious problems for phrase-structure approaches:
(7) Fido had a bone yesterday and biscuit today.
</figure>
<figureCaption confidence="0.9018376">
However if we generalize the schema already ob-
tained for standard coordination by allowing X
to be not only a single category, but a list
of categories5, it is found to suffice for non-
constituent coordination as well.
</figureCaption>
<figure confidence="0.937638444444444">
Fido S [I
had VP [ ]
a NP [NP(t)]
(7a) bone N [NP(t)]
yesterday NP (t) [N (+) [NP(t)]]
and N(+) [NP(t)]
biscuit N [NP(t)]
today NP(t)[ ]
In this analysis instead of a regular transition for
</figure>
<figureCaption confidence="0.55648">
&apos;bone&apos; of: N [NP(t)] NP(t) [1
</figureCaption>
<bodyText confidence="0.914141333333333">
there is instead a transition introducing coordina-
tion: N [NP(t)] NP(t) [N(+) [NP(t)]]
Allowing categories on the stack to themselves
have non-empty stacks moves the formalism one
step further from being an indexed grammar. This
is the final incarnation of the formalism, being the
State-Transition Grammar of the title6.
Similar schemas are being investigated to charac-
terize gapping constructions.
Centre-Embedding
It should be noted that an indefinite amount of
centre-embedding can be described, but only
</bodyText>
<footnote confidence="0.95389">
5There is in general no upper limit to the length of this
list, eg. &amp;quot;I gave Fido a biscuit yesterday in the house and
Rover a bone today in his kennel.&amp;quot;
6Milward (1990) introduces a formalism essentially
identical to the one presented here, although viewed from
a very different perspective. Milward (1994) shows how it
handles a wide range of non-constituent co-ordinations.
</footnote>
<bodyText confidence="0.448893">
at the expense of unlimited growth in the length
of states:
</bodyText>
<figure confidence="0.9762022">
The S [ ]
fly N [VP]
the S (np) [VP]
dog N [VP (np) , VP]
(8) the S (np) [VP (np) ,VP]
cat N [VP (np) ,VP (np) ,VP]
scratched VP (np) [VP (np) ,VP]
swallowed VP (np) [VP]
died VP [ ]
This contrasts with unlimited right-recursion
where there is no growth in state length:
S [ ]
saw VP [ ]
the NP [ ]
cat N [ ]
(9) that S(rel)[ ]
scratched VP [ ]
the NP [
dog N [
that S(rel)[ ]
</figure>
<bodyText confidence="0.963068333333333">
As the model is to be trained from real data, tran-
sitions involving long states as in (8) will have an
ever smaller and eventually effectively nil proba-
bility. Therefore, when tuned to any particular
language corpus the resulting grammar will be ef-
fectively finite-state7.
Parsing
Assuming that we now have a corpus parsed with
the state-transition grammar, how can this infor-
mation be used to parse fresh text?
Firstly, for each word type in the corpus we can
collect the transitions with which it occurs and
calculate its probability distribution over all pos-
sible transitions (an infinite number of which will
be zero). To make this concrete, there are five to-
kens of the word &apos;dog&apos; in the examples thus far,
and so &apos;dog&apos; will have the transition probability
distribution:
</bodyText>
<table confidence="0.8484815">
N [NP] -+ NP [ ] 0.2
N [NP(t)] S(rel) [NP(t)] 0.2
N [NP(t)] NP(t) [S(rel)] 0.2
N [VP(np),VP] S(np) [VP(np),VP1 0.2
</table>
<tableCaption confidence="0.728097">
7This may be compared to the claim in Krauwer
Des Tombes (1981) that finite-state automata offer a more
satisfactory characterization of language than context-free
grammars.
</tableCaption>
<page confidence="0.988389">
275
</page>
<equation confidence="0.731428">
N [ ] S(rel) [ 1 0.2
</equation>
<bodyText confidence="0.999991923076923">
To find the most probable parse for a sentence,
we simply find the path from word to word which
maximizes the product of the state transitions (as
we have a first order Markov process).
However this simple-minded approach, although
easy to implement, in other ways leaves much to
be desired. The probability distributions are far
too &amp;quot;gappy&amp;quot; and even if a huge amount of data
were collected, the chances that they would pro-
vide the desired path for a sentence of any reason-
able length are slim. The process of generalizing
or smoothing the transition probabilities is there-
fore seen to be indispensable.
</bodyText>
<subsectionHeader confidence="0.954734">
Smoothing Probability Distributions
</subsectionHeader>
<bodyText confidence="0.950703444444444">
Although far from exhausting the possible meth-
ods for smoothing, the following three are those
used in the implementation described at the end
of the paper.
1. Factor out elements on the stack which are
merely carried over from state to state (which was
done earlier in looking at the correspondence of
state transitions to categorial types). The previ-
ous transitions for &apos;dog&apos; then become:
</bodyText>
<equation confidence="0.8979835">
N [a] --+ a H 0.2
N [a] a [S(rel)] 0.2
N [a] S(np) [a] 0.2
N [a] S(rel) [a] 0.4
</equation>
<listItem confidence="0.934234">
2. Factor out other features which are merely
passed from state to state. For instance in the
example sentences, &apos;the&apos; has the generalized tran-
sitions:
</listItem>
<equation confidence="0.9414866">
S [a] N [VP,a]
S(np) [a] N [VP(np),a]
which can be further generalized to the single
transition:
s($) [a] —+ N [VP(/3),a} p = set of features
</equation>
<bodyText confidence="0.9878350625">
3. Establish word paradigms, ie. classes of words
which occur with similar transitions. The prob-
ability distribution for individual words can then
be smoothed by suitably blending in the paradig-
matic distribution. These paradigms will corre-
spond to a great extent to the word classes of
rule-based grammars. The advantage would be re-
tained however that the system is still fine-grained
enough to reflect the idiosyncratic patterns of in-
dividual words and could override this paradig-
matic information if sufficient data were available.
Words hitherto unknown to the system can be
treated as being extreme examples of words lack-
ing sufficient transition data and they might then
be given a transition distribution blended from the
open class word paradigms.
</bodyText>
<subsectionHeader confidence="0.96489">
Problems Arising from Smoothing
</subsectionHeader>
<bodyText confidence="0.991142538461539">
Although essential for effective processing, the
smoothing operations may give rise to new prob-
lems. For example, factoring out items on the
stack, as in (1), removes from the model the dis-
inclination for long states inherent in the original
corpus. To recapture this discarded aspect of the
language, it would be sufficient to introduce into
the model a probabilistic penalty based on state
length. This penalty may easily be calculated ac-
cording to the lengths of states in the parsed cor-
pus.
Not only would this allow the modelling of the re-
striction on centre-embedding, but it would also
allow many other &amp;quot;processing&amp;quot; phenomena to be
accurately characterized. Taking as an exam-
ple &amp;quot;heavy-NP shift&amp;quot;, suppose that the corpus
contained two distinct transitions for the word
&apos;threw&apos;, with the particle &apos;out&apos; both before and
after the object.
threw VP —&gt; NP, X(out) prob: pl
VP X(out), NP prob: p2
Even if pl were considerably greater than p2, the
cumulative negative effect of the longer states in
(10) would eventually lead to the model giving
the sentence with the shifted NP (11) a higher
probability.
</bodyText>
<figure confidence="0.961145388888889">
S ]
threw VP [ ]
the NP [X(out)]
bacon N [X(out)]
(10) that S(rel) [X(out)]
Fido S(np) [X(out)]
had VP(np) [X(out)]
chewed VP(np) [X(out)]
out X(out) [
S H
threw VP [ ]
out X(out) [NP]
the NP [ ]
(11) bacon N [ ]
that S(rel) H
Fido S(np) [ ]
had VP(np) [ ]
chewed VP(np) H
</figure>
<page confidence="0.989566">
276
</page>
<sectionHeader confidence="0.545059" genericHeader="method">
Capturing Lexical Preferences
</sectionHeader>
<bodyText confidence="0.9995815">
One strength of n-gram models is that they can
capture a certain amount of lexical preference
information. For example, in a bigram model
trained on sufficient data the probability of the
bigram &apos;dog barked&apos; could be expected to be sig-
nificantly higher than &apos;cat barked&apos;, and this slice of
&amp;quot;world knowledge&amp;quot; is something our model lacks.
It would not be difficult to make a small extension
to the present model to capture such information,
namely by introducing an additional feature con-
taining the &amp;quot;lexical value&amp;quot; of the head of a phrase.
Abandoning the shorthand &apos;VP&apos; and representing
a subject explicitly as a &amp;quot;slashed&amp;quot; NP, a sentence
with added lexical head features would appear as:
</bodyText>
<figure confidence="0.899500714285714">
The 1]
dog N (dog) [S(np(dog))]
which S(rel,np (dog)) [S (np(dog))1
(12) chased S (np (dog)) [S (np (dog)))
the NP (cat) [S (np (dog)))
cat N (cat) [S(np(dog))]
barked S(np(dog)) [
</figure>
<bodyText confidence="0.997974">
In contrast to n-grams, where this sentence would
cloud somewhat the &amp;quot;world knowledge&amp;quot;, contain-
ing as it does the bigram &apos;cat barked&apos;, the added
structure of our model allows the lexical prefer-
ence to be captured no matter how far the head
noun is from the head verb. From (12) the world
knowledge of the system would be reinforced by
the two stereotypical transitions:
</bodyText>
<sectionHeader confidence="0.416482" genericHeader="method">
&apos;chased&apos; S(np(dog)) NP(cat)
&apos;barked&apos; S(np(dog)) [
</sectionHeader>
<subsectionHeader confidence="0.846306">
Present Implementation
</subsectionHeader>
<bodyText confidence="0.99792225">
16,000+ running words from section N of the
Brown corpus (texts N01-N08) were hand-parsed
using the state-transition grammar. The actual
formalism used was much fuller than the rather
schematic one given above, including many ad-
ditional features such as case, tense, person and
number. Transition probabilities were generalized
in the ways discussed in the previous section.
</bodyText>
<sectionHeader confidence="0.989238" genericHeader="evaluation">
Results
</sectionHeader>
<bodyText confidence="0.9648475625">
100 sentences of less than 15 words were chosen
randomly from other texts in section N of the
Brown corpus (N09-N14) and fed to the parser
without alteration. Unknown words in the input,
of which there were obviously many, were assigned
to one of seven orthographic classes and given ap-
propriate transitions calculated from the corpus.
• 27 were parsed correctly, ie. exactly the same
as the hand parse or differing in only relatively
insignificant ways which the model could not
hope to know8.
• 23 were parsed wrongly, ie. the analysis differed
from the hand parse in some non-trivial way.
• 50 were not parsed at all, ie. one or more of the
transitions necessary to find a parse path was
lacking, even after generalizing the transitions.
</bodyText>
<subsectionHeader confidence="0.750157">
Future Development
</subsectionHeader>
<bodyText confidence="0.9999619375">
Although the results at present are extremely
modest, it should be borne in mind both that the
amount of data the system has to work on is very
small and that the smoothing of transition prob-
abilities is still far from optimal. The present tar-
get is to achieve such a level of performance that
the corpus can be extended by hand-correction of
the parser output, rather than hand-parsing from
scratch. Not only will this hopefully save a cer-
tain amount of drudgery, it should also help to
minimize errors and maintain consistency.
A more distant goal is to ascertain whether the
performance of the model can improve after pars-
ing new texts and processing the data therein even
without hand-correction of the parses, and if so
what the limits are to such &amp;quot;self-improvement&amp;quot;.
</bodyText>
<sectionHeader confidence="0.998266" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998450066666666">
AHO A.V. 1968. Indexed Grammars. Journal of
the ACM, 15: 647-671.
BOD, RENS 1992. A Computational Model of
Language Performance: Data Oriented Parsing.
COLING-92.
KARTTUNEN L. 1990. Radical Lexicalism. In
Baltin &amp; Kroch (eds), Alternative conceptions of
phrase structure, Univ of Chicago Press, pp 43-65.
KRAUWER, STEVEN &amp; DES TOMBES, LOUIS
1981. Transducers and Grammars as Theories of
Language. Theoretical Linguistics, 8, 173-202.
MILWARD, DAVID 1990. Coordination in an Ax-
iomatic Grammar. COLING-90.
MILWARD, DAVID 1994. Non-constituent Coordi-
nation: Theory and Practice. COLING-94.
</reference>
<footnote confidence="0.859151333333333">
8Such as the system postulating that &amp;quot;Jess&amp;quot; was a sur-
name, as against the hand-parser&apos;s guess of a masculine
first name.
</footnote>
<page confidence="0.990389">
277
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.979042">
<title confidence="0.999988">A State-Transition Grammar for Data-Oriented Parsing</title>
<author confidence="0.999852">David Tugwell</author>
<affiliation confidence="0.999969">Centre for Cognitive Science, University of Edinburgh</affiliation>
<address confidence="0.999648">2, Buccleuch Place, Edinburgh EH8 9LW, Scotland</address>
<email confidence="0.999598">davidt@cogsci.ed.ac.uk</email>
<abstract confidence="0.997125">This paper presents a grammar formalism designed for use in data-oriented approaches to language processing. It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V AHO</author>
</authors>
<title>Indexed Grammars.</title>
<date>1968</date>
<journal>Journal of the ACM,</journal>
<volume>15</volume>
<pages>647--671</pages>
<marker>AHO, 1968</marker>
<rawString>AHO A.V. 1968. Indexed Grammars. Journal of the ACM, 15: 647-671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RENS BOD</author>
</authors>
<title>A Computational Model of Language Performance: Data Oriented Parsing.</title>
<date>1992</date>
<tech>COLING-92.</tech>
<marker>BOD, 1992</marker>
<rawString>BOD, RENS 1992. A Computational Model of Language Performance: Data Oriented Parsing. COLING-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L KARTTUNEN</author>
</authors>
<title>Radical Lexicalism. In Baltin &amp; Kroch (eds), Alternative conceptions of phrase structure, Univ of Chicago Press,</title>
<date>1990</date>
<pages>43--65</pages>
<marker>KARTTUNEN, 1990</marker>
<rawString>KARTTUNEN L. 1990. Radical Lexicalism. In Baltin &amp; Kroch (eds), Alternative conceptions of phrase structure, Univ of Chicago Press, pp 43-65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>STEVEN KRAUWER</author>
<author>DES TOMBES</author>
<author>LOUIS</author>
</authors>
<title>Transducers and Grammars as Theories of Language.</title>
<date>1981</date>
<journal>Theoretical Linguistics,</journal>
<volume>8</volume>
<pages>173--202</pages>
<marker>KRAUWER, TOMBES, LOUIS, 1981</marker>
<rawString>KRAUWER, STEVEN &amp; DES TOMBES, LOUIS 1981. Transducers and Grammars as Theories of Language. Theoretical Linguistics, 8, 173-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DAVID MILWARD</author>
</authors>
<date>1990</date>
<booktitle>Coordination in an Axiomatic Grammar. COLING-90.</booktitle>
<marker>MILWARD, 1990</marker>
<rawString>MILWARD, DAVID 1990. Coordination in an Axiomatic Grammar. COLING-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DAVID MILWARD</author>
</authors>
<title>Non-constituent Coordination: Theory and Practice.</title>
<date>1994</date>
<marker>MILWARD, 1994</marker>
<rawString>MILWARD, DAVID 1994. Non-constituent Coordination: Theory and Practice. COLING-94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>