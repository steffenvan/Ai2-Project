<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.997665">
Word Sense Disambiguation Using Label Propagation Based
Semi-Supervised Learning
</title>
<author confidence="0.997246">
Zheng-Yu Niu, Dong-Hong Ji
</author>
<affiliation confidence="0.983148">
Institute for Infocomm Research
</affiliation>
<address confidence="0.92008">
21 Heng Mui Keng Terrace
119613 Singapore
</address>
<email confidence="0.996501">
{zniu, dhji}@i2r.a-star.edu.sg
</email>
<author confidence="0.995809">
Chew Lim Tan
</author>
<affiliation confidence="0.901279666666667">
Department of Computer Science
National University of Singapore
3 Science Drive 2
</affiliation>
<address confidence="0.70472">
117543 Singapore
</address>
<email confidence="0.997048">
tancl@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.993843" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996365625">
Shortage of manually sense-tagged data is
an obstacle to supervised word sense dis-
ambiguation methods. In this paper we in-
vestigate a label propagation based semi-
supervised learning algorithm for WSD,
which combines labeled and unlabeled
data in learning process to fully realize
a global consistency assumption: simi-
lar examples should have similar labels.
Our experimental results on benchmark
corpora indicate that it consistently out-
performs SVM when only very few la-
beled examples are available, and its per-
formance is also better than monolingual
bootstrapping, and comparable to bilin-
gual bootstrapping.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999893928571429">
In this paper, we address the problem of word sense
disambiguation (WSD), which is to assign an appro-
priate sense to an occurrence of a word in a given
context. Many methods have been proposed to deal
with this problem, including supervised learning al-
gorithms (Leacock et al., 1998), semi-supervised
learning algorithms (Yarowsky, 1995), and unsuper-
vised learning algorithms (Schfitze, 1998).
Supervised sense disambiguation has been very
successful, but it requires a lot of manually sense-
tagged data and can not utilize raw unannotated data
that can be cheaply acquired. Fully unsupervised
methods do not need the definition of senses and
manually sense-tagged data, but their sense cluster-
ing results can not be directly used in many NLP
tasks since there is no sense tag for each instance in
clusters. Considering both the availability of a large
amount of unlabelled data and direct use of word
senses, semi-supervised learning methods have re-
ceived great attention recently.
Semi-supervised methods for WSD are character-
ized in terms of exploiting unlabeled data in learning
procedure with the requirement of predefined sense
inventory for target words. They roughly fall into
three categories according to what is used for su-
pervision in learning process: (1) using external re-
sources, e.g., thesaurus or lexicons, to disambiguate
word senses or automatically generate sense-tagged
corpus, (Lesk, 1986; Lin, 1997; McCarthy et al.,
2004; Seo et al., 2004; Yarowsky, 1992), (2) exploit-
ing the differences between mapping of words to
senses in different languages by the use of bilingual
corpora (e.g. parallel corpora or untagged monolin-
gual corpora in two languages) (Brown et al., 1991;
Dagan and Itai, 1994; Diab and Resnik, 2002; Li and
Li, 2004; Ng et al., 2003), (3) bootstrapping sense-
tagged seed examples to overcome the bottleneck of
acquisition of large sense-tagged data (Hearst, 1991;
Karov and Edelman, 1998; Mihalcea, 2004; Park et
al., 2000; Yarowsky, 1995).
As a commonly used semi-supervised learning
method for WSD, bootstrapping algorithm works
by iteratively classifying unlabeled examples and
adding confidently classified examples into labeled
dataset using a model learned from augmented la-
beled dataset in previous iteration. It can be found
that the affinity information among unlabeled ex-
amples is not fully explored in this bootstrapping
process. Bootstrapping is based on a local consis-
tency assumption: examples close to labeled exam-
ples within same class will have same labels, which
is also the assumption underlying many supervised
learning algorithms, such as kNN.
Recently a promising family of semi-supervised
learning algorithms are introduced, which can ef-
fectively combine unlabeled data with labeled data
</bodyText>
<page confidence="0.98554">
395
</page>
<note confidence="0.991705">
Proceedings of the 43rd Annual Meeting of the ACL, pages 395–402,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.997054290322581">
in learning process by exploiting cluster structure
in data (Belkin and Niyogi, 2002; Blum et al.,
2004; Chapelle et al., 1991; Szummer and Jaakkola,
2001; Zhu and Ghahramani, 2002; Zhu et al., 2003).
Here we investigate a label propagation based semi-
supervised learning algorithm (LP algorithm) (Zhu
and Ghahramani, 2002) for WSD, which works by
representing labeled and unlabeled examples as ver-
tices in a connected graph, then iteratively propagat-
ing label information from any vertex to nearby ver-
tices through weighted edges, finally inferring the
labels of unlabeled examples after this propagation
process converges.
Compared with bootstrapping, LP algorithm is
based on a global consistency assumption. Intu-
itively, if there is at least one labeled example in each
cluster that consists of similar examples, then unla-
beled examples will have the same labels as labeled
examples in the same cluster by propagating the la-
bel information of any example to nearby examples
according to their proximity.
This paper is organized as follows. First, we will
formulate WSD problem in the context of semi-
supervised learning in section 2. Then in section
3 we will describe LP algorithm and discuss the
difference between a supervised learning algorithm
(SVM), bootstrapping algorithm and LP algorithm.
Section 4 will provide experimental results of LP al-
gorithm on widely used benchmark corpora. Finally
we will conclude our work and suggest possible im-
provement in section 5.
</bodyText>
<sectionHeader confidence="0.962762" genericHeader="method">
2 Problem Setup
</sectionHeader>
<bodyText confidence="0.999701375">
Let X = {xi}ni=1 be a set of contexts of occur-
rences of an ambiguous word w, where xi repre-
sents the context of the i-th occurrence, and n is
the total number of this word’s occurrences. Let
5 = {sj}cj=1 denote the sense tag set of w. The first
l examples xg(1 &lt; g &lt; l) are labeled as yg (yg E 5)
and other u (l+u = n) examples xh(l+1 &lt; h &lt; n)
are unlabeled. The goal is to predict the sense of w
in context xh by the use of label information of xg
and similarity information among examples in X.
The cluster structure in X can be represented as a
connected graph, where each vertex corresponds to
an example, and the edge between any two examples
xi and xj is weighted so that the closer the vertices
in some distance measure, the larger the weight as-
sociated with this edge. The weights are defined as
</bodyText>
<page confidence="0.377371">
2
</page>
<bodyText confidence="0.99381125">
follows: Wij = exp( − a2) if i 7� j and Wii = 0
(1 &lt; i, j &lt; n), where dij is the distance (ex. Euclid-
ean distance) between xi and xj, and Q is used to
control the weight Wij.
</bodyText>
<sectionHeader confidence="0.916963" genericHeader="method">
3 Semi-supervised Learning Method
</sectionHeader>
<subsectionHeader confidence="0.998804">
3.1 Label Propagation Algorithm
</subsectionHeader>
<bodyText confidence="0.98948503125">
In LP algorithm (Zhu and Ghahramani, 2002), label
information of any vertex in a graph is propagated
to nearby vertices through weighted edges until a
global stable stage is achieved. Larger edge weights
allow labels to travel through easier. Thus the closer
the examples, more likely they have similar labels
(the global consistency assumption).
In label propagation process, the soft label of each
initial labeled example is clamped in each iteration
to replenish label sources from these labeled data.
Thus the labeled data act like sources to push out la-
bels through unlabeled data. With this push from la-
beled examples, the class boundaries will be pushed
through edges with large weights and settle in gaps
along edges with small weights. If the data structure
fits the classification goal, then LP algorithm can use
these unlabeled data to help learning classification
plane.
Let Y 0 E Nnxc represent initial soft labels at-
tached to vertices, where Y 0 ij= 1 if yi is sj and 0
otherwise. Let YL0 be the top l rows of Y0 and YU0
be the remaining u rows. YL0 is consistent with the
labeling in labeled data, and the initialization of YU0
can be arbitrary.
Optimally we expect that the value of Wij across
different classes is as small as possible and the value
of Wij within same class is as large as possible.
This will make label propagation to stay within same
class. In later experiments, we set Q as the aver-
age distance between labeled examples from differ-
ent classes.
Define n x n probability transition matrix Tij =
</bodyText>
<equation confidence="0.9880025">
P(j , i) = EWz w� , where Tij is the probability
L k=1 7
</equation>
<bodyText confidence="0.82070325">
to jump from example xj to example xi.
Compute the row-normalized matrix T by Tij =
Tij/ Enk=1 Tik. This normalization is to maintain
the class probability interpretation of Y .
</bodyText>
<page confidence="0.997592">
396
</page>
<figure confidence="0.969397263157895">
(a) Dataset with Two−Moon Pattern (b) SVM
(a) Minimum Spanning Tree (b) t=1
2 2
1 C 1
0 B 0
−1 −1
−2 A −2
2
1
0
−1
−2
−2 −1 0 1 2 3 4
(c) Bootstrapping (d) Ideal Classification
−2 −1 0 1 2 3 −2 −1 0 1 2 3
−1
−2
−2 −1 0 1 2 3
2
0
1
labeled +1
unlabeled
labeled −1
B0
A 8
A0
A 9
A10
B10
B9
B8
2
1
0
−1
−2
2
1
0
−1
−2
−2 −1 0 1 2 3 −2 −1 0 1 2 3
(c) t=7 (d) t=10
2
2
1
0
−1
−2
−2 −1 0 1 2 3
1
0
−1
−2
−2 −1 0 1 2 3
(e) t=12 (f) t=100
</figure>
<figureCaption confidence="0.8184025">
Figure 1: Classification result on two-moon pattern dataset.
(a) Two-moon pattern dataset with two labeled points, (b) clas-
sification result by SVM, (c) labeling procedure of bootstrap-
ping algorithm, (d) ideal classification.
</figureCaption>
<figure confidence="0.694821285714286">
2 2
1 1
0 0
−1 −1
−2 −2
−2 −1 0 1 2 3 −2 −1 0 1 2 3
Then LP algorithm is defined as follows:
</figure>
<listItem confidence="0.991964333333333">
1. Initially set t=0, where t is iteration index;
2. Propagate the label by Yt+1 = TYt;
3. Clamp labeled data by replacing the top l row
of Yt+1 with Y0L. Repeat from step 2 until Yt con-
verges;
4. Assign xh(l + 1 &lt; h &lt; n) with a label sˆj,
</listItem>
<bodyText confidence="0.9982431">
where j = argmaxjYhj.
This algorithm has been shown to converge to
�YU = limt→oo YUt =
(I − Tuu)−1TulYL0 (Zhu and Ghahramani, 2002).
We can see that this solution can be obtained with-
out iteration and the initialization of YU0 is not im-
portant, since YU0 does not affect the estimation of
�YU. I is u x u identity matrix. Tuu and Tul are
acquired by splitting matrix T after the l-th row and
the l-th column into 4 sub-matrices.
</bodyText>
<subsectionHeader confidence="0.995238">
3.2 Comparison between SVM, Bootstrapping
and LP
</subsectionHeader>
<bodyText confidence="0.9999105">
For WSD, SVM is one of the state of the art super-
vised learning algorithms (Mihalcea et al., 2004),
while bootstrapping is one of the state of the art
semi-supervised learning algorithms (Li and Li,
2004; Yarowsky, 1995). For comparing LP with
SVM and bootstrapping, let us consider a dataset
with two-moon pattern shown in Figure 1(a). The
upper moon consists of 9 points, while the lower
moon consists of 13 points. There is only one la-
beled point in each moon, and other 20 points are un-
</bodyText>
<figureCaption confidence="0.96702">
Figure 2: Classification result of LP on two-moon pattern
dataset. (a) Minimum spanning tree of this dataset. The conver-
gence process of LP algorithm with t varying from 1 to 100 is
shown from (b) to (f).
</figureCaption>
<bodyText confidence="0.99988704">
labeled. The distance metric is Euclidian distance.
We can see that the points in one moon should be
more similar to each other than the points across the
moons.
Figure 1(b) shows the classification result of
SVM. Vertical line denotes classification hyper-
plane, which has the maximum separating margin
with respect to the labeled points in two classes. We
can see that SVM does not work well when labeled
data can not reveal the structure (two moon pattern)
in each class. The reason is that the classification
hyperplane was learned only from labeled data. In
other words, the coherent structure (two-moon pat-
tern) in unlabeled data was not explored when infer-
ring class boundary.
Figure 1(c) shows bootstrapping procedure using
kNN (k=1) as base classifier with user-specified pa-
rameter b = 1 (the number of added examples from
unlabeled data into classified data for each class in
each iteration). Termination condition is that the dis-
tance between labeled and unlabeled points is more
than inter-class distance (the distance between A0
and B0). Each arrow in Figure 1(c) represents
one classification operation in each iteration for each
class. After eight iterations, A1 ∼ A8 were tagged
</bodyText>
<figure confidence="0.556861">
a unique solution, which is
</figure>
<page confidence="0.9698">
397
</page>
<bodyText confidence="0.997034333333333">
as +1, and B1 - B8 were tagged as -1, while
A9 - A10 and B9 - B10 were still untagged. Then
at the ninth iteration, A9 was tagged as +1 since the
label of A9 was determined only by labeled points in
kNN model: A9 is closer to any point in {A0 - A81
than to any point in {B0 - B81, regardless of the
intrinsic structure in data: A9 - A10 and B9 - B10
are closer to points in lower moon than to points in
upper moon. In other words, bootstrapping method
uses the unlabeled data under a local consistency
based strategy. This is the reason that two points A9
and A10 are misclassified (shown in Figure 1(c)).
From above analysis we see that both SVM and
bootstrapping are based on a local consistency as-
sumption.
Finally we ran LP on a connected graph-minimum
spanning tree generated for this dataset, shown in
Figure 2(a). A, B, C represent three points, and
the edge A - B connects the two moons. Figure
2(b)- 2(f) shows the convergence process of LP with
t increasing from 1 to 100. When t = 1, label in-
formation of labeled data was pushed to only nearby
points. After seven iteration steps (t = 7), point B
in upper moon was misclassified as -1 since it first
received label information from point A through the
edge connecting two moons. After another three it-
eration steps (t=10), this misclassified point was re-
tagged as +1. The reason of this self-correcting be-
havior is that with the push of label information from
nearby points, the value of YB�+1 became higher
than YB�_1. In other words, the weight of edge
B - C is larger than that of edge B - A, which
makes it easier for +1 label of point C to travel to
point B. Finally, when t &gt; 12 LP converged to a
fixed point, which achieved the ideal classification
result.
</bodyText>
<sectionHeader confidence="0.998345" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.925772">
4.1 Experiment Design
</subsectionHeader>
<bodyText confidence="0.9969106">
For empirical comparison with SVM and bootstrap-
ping, we evaluated LP on widely used benchmark
corpora - “interest”, “line” 1 and the data in English
lexical sample task of SENSEVAL-3 (including all
57 English words ) 2.
</bodyText>
<footnote confidence="0.999897">
1Available at http://www.d.umn.edu/∼tpederse/data.html
2Available at http://www.senseval.org/senseval3
</footnote>
<tableCaption confidence="0.98559125">
Table 1: The upper two tables summarize accuracies (aver-
aged over 20 trials) and paired t-test results of SVM and LP on
SENSEVAL-3 corpus with percentage of training set increasing
from 1% to 100%. The lower table lists the official result of
</tableCaption>
<table confidence="0.964481894736842">
baseline (using most frequent sense heuristics) and top 3 sys-
tems in ELS task of SENSEVAL-3.
Percentage SVM LPcosine LPJs
1% 24.9±2.7% 27.5±1.1% 28.1±1.1%
10% 53.4±1.1% 54.4±1.2% 54.9±1.1%
25% 62.3±0.7% 62.3±0.7% 63.3±0.9%
50% 66.6±0.5% 65.7±0.5% 66.9±0.6%
75% 68.7±0.4% 67.3±0.4% 68.7±0.3%
100% 69.7% 68.4% 70.3%
Percentage SVM vs. LPcosine SVM vs. LPJs
p-value Sign. p-value Sign.
1% 8.7e-004 ≪ 8.5e-005 ≪
10% 1.9e-006 ≪ 1.0e-008 ≪
25% 9.2e-001 ∼ 3.0e-006 ≪
50% 1.9e-006 ≫ 6.2e-002 ∼
75% 7.4e-013 ≫ 7.1e-001 ∼
100% - - - -
Systems Baseline htsa3 IRST-Kernels nusels
Accuracy 55.2% 72.9% 72.6% 72.4%
</table>
<bodyText confidence="0.99979928">
We used three types of features to capture con-
textual information: part-of-speech of neighboring
words with position information, unordered sin-
gle words in topical context, and local collocations
(as same as the feature set used in (Lee and Ng,
2002) except that we did not use syntactic relations).
For SVM, we did not perform feature selection on
SENSEVAL-3 data since feature selection deterio-
rates its performance (Lee and Ng, 2002). When
running LP on the three datasets, we removed the
features with occurrence frequency (counted in both
training set and test set) less than 3 times.
We investigated two distance measures for LP: co-
sine similarity and Jensen-Shannon (JS) divergence
(Lin, 1991).
For the three datasets, we constructed connected
graphs following (Zhu et al., 2003): two instances
u, v will be connected by an edge if u is among v’s
k nearest neighbors, or if v is among u’s k nearest
neighbors as measured by cosine or JS distance mea-
sure. For “interest” and “line” corpora, k is 10 (fol-
lowing (Zhu et al., 2003)), while for SENSEVAL-3
data, k is 5 since the size of dataset for each word
in SENSEVAL-3 is much less than that of “interest”
and “line” datasets.
</bodyText>
<page confidence="0.996829">
398
</page>
<subsectionHeader confidence="0.93619">
4.2 Experiment 1: LP vs. SVM
</subsectionHeader>
<bodyText confidence="0.999890707317073">
In this experiment, we evaluated LP and SVM
3 on the data of English lexical sample task in
SENSEVAL-3. We used l examples from training
set as labeled data, and the remaining training ex-
amples and all the test examples as unlabeled data.
For each labeled set size l, we performed 20 trials.
In each trial, we randomly sampled l labeled exam-
ples for each word from training set. If any sense
was absent from the sampled labeled set, we redid
the sampling. We conducted experiments with dif-
ferent values of l, including 1% × Nw,train, 10% ×
Nw,train, 25%×Nw,train, 50%×Nw,train, 75%×
Nw,train, 100% × Nw,train (Nw,train is the number
of examples in training set of word w). SVM and LP
were evaluated using accuracy 4 (fine-grained score)
on test set of SENSEVAL-3.
We conducted paired t-test on the accuracy fig-
ures for each value of l. Paired t-test is not run when
percentage= 100%, since there is only one paired
accuracy figure. Paired t-test is usually used to esti-
mate the difference in means between normal pop-
ulations based on a set of random paired observa-
tions. {≪, ≫}, {&lt;, &gt;}, and ∼ correspond to p-
value ≤ 0.01, (0.01, 0.05], and &gt; 0.05 respectively.
≪ (or ≫) means that the performance of LP is sig-
nificantly better (or significantly worse) than SVM.
&lt; (or &gt;) means that the performance of LP is better
(or worse) than SVM. ∼ means that the performance
of LP is almost as same as SVM.
Table 1 reports the average accuracies and paired
t-test results of SVM and LP with different sizes
of labled data. It also lists the official results of
baseline method and top 3 systems in ELS task of
SENSEVAL-3.
From Table 1, we see that with small labeled
dataset (percentage of labeled data ≤ 10%), LP per-
forms significantly better than SVM. When the per-
centage of labeled data increases from 50% to 75%,
the performance of LPJS and SVM become almost
same, while LP�osine performs significantly worse
than SVM.
</bodyText>
<footnote confidence="0.895649333333333">
3we used linear SV Mlight, available at
http://svmlight.joachims.org/.
4If there are multiple sense tags for an instance in training
set or test set, then only the first tag is considered as correct
answer. Furthermore, if the answer of the instance in test set is
“U”, then this instance will be removed from test set.
</footnote>
<tableCaption confidence="0.8026195">
Table 2: Accuracies from (Li and Li, 2004) and average ac-
curacies of LP with c x b labeled examples on “interest” and
</tableCaption>
<bodyText confidence="0.845152333333333">
“line” corpora. Major is a baseline method in which they al-
ways choose the most frequent sense. MB-D denotes monolin-
gual bootstrapping with decision list as base classifier, MB-B
represents monolingual bootstrapping with ensemble of Naive
Bayes as base classifier, and BB is bilingual bootstrapping with
ensemble of Naive Bayes as base classifier.
</bodyText>
<subsectionHeader confidence="0.985581">
4.3 Experiment 2: LP vs. Bootstrapping
</subsectionHeader>
<bodyText confidence="0.999989541666667">
Li and Li (2004) used “interest” and “line” corpora
as test data. For the word “interest”, they used its
four major senses. For comparison with their re-
sults, we took reduced “interest” corpus (constructed
by retaining four major senses) and complete “line”
corpus as evaluation data. In their algorithm, c is
the number of senses of ambiguous word, and b
(b = 15) is the number of examples added into clas-
sified data for each class in each iteration of boot-
strapping. c × b can be considered as the size of
initial labeled data in their bootstrapping algorithm.
We ran LP with 20 trials on reduced “interest” cor-
pus and complete “line” corpus. In each trial, we
randomly sampled b labeled examples for each sense
of “interest” or “line” as labeled data. The rest
served as both unlabeled data and test data.
Table 2 summarizes the average accuracies of LP
on the two corpora. It also lists the accuracies of
monolingual bootstrapping algorithm (MB), bilin-
gual bootstrapping algorithm (BB) on “interest” and
“line” corpora. We can see that LP performs much
better than MB-D and MB-B on both “interest” and
“line” corpora, while the performance of LP is com-
parable to BB on these two corpora.
</bodyText>
<subsectionHeader confidence="0.997973">
4.4 An Example: Word “use”
</subsectionHeader>
<bodyText confidence="0.999938">
For investigating the reason for LP to outperform
SVM and monolingual bootstrapping, we used the
data of word “use” in English lexical sample task of
SENSEVAL-3 as an example (totally 26 examples
in training set and 14 examples in test set). For data
</bodyText>
<table confidence="0.747106285714286">
Ambiguous
words
interest 54.6% 54.7% 69.3% 75.5%
line 53.5% 55.6% 54.1% 62.7%
Ambiguous
words
interest 4x15=60 80.2±2.0% 79.8±2.0%
line 6x15=90 60.3±4.5% 59.4±3.9%
Accuracies from (Li and Li, 2004)
Major MB-D MB-B BB
Our results
#labeled examples
LPcosine
LPiS
</table>
<page confidence="0.722773">
399
</page>
<figure confidence="0.999660866666666">
(a) Initial Seting (b) Ground−truth
0.5 0.5
0 0
−0.5 −0.5
−0.4 −0.2 0 0.2 0.4 0.6 −0.4 −0.2 0 0.2 0.4 0.6
(c) SVM (d) Bootstrapping
0.5 0.5 C
0 0 B A
−0.5 −0.5
−0.4 −0.2 0 0.2 0.4 0.6 −0.4 −0.2 0 0.2 0.4 0.6
(e) Bootstrapping (f) LP
0.5 0.5
0 0
−0.5 −0.5
−0.4 −0.2 0 0.2 0.4 0.6 −0.4 −0.2 0 0.2 0.4 0.6
</figure>
<figureCaption confidence="0.907138">
Figure 3: Comparison of sense disambiguation results be-
tween SVM, monolingual bootstrapping and LP on word “use”.
</figureCaption>
<bodyText confidence="0.67418">
(a) only one labeled example for each sense of word “use”
as training data before sense disambiguation (◦ and ⊲ denote
the unlabeled examples in SENSEVAL-3 training set and test
set respectively, and other five symbols (+, x, △, ⋄, and ∇)
represent the labeled examples with different sense tags sam-
pled from SENSEVAL-3 training set.), (b) ground-truth re-
sult, (c) classification result on SENSEVAL-3 test set by SVM
(accuracy= 314 = 21.4%), (d) classified data after bootstrap-
ping, (e) classification result on SENSEVAL-3 training set and
test set by 1NN (accuracy= 614 = 42.9% ), (f) classifica-
tion result on SENSEVAL-3 training set and test set by LP
(accuracy= 10
</bodyText>
<equation confidence="0.90482">
14 = 71.4% ).
</equation>
<bodyText confidence="0.990412090909091">
visualization, we conducted unsupervised nonlinear
dimensionality reduction5 on these 40 feature vec-
tors with 210 dimensions. Figure 3 (a) shows the
dimensionality reduced vectors in two-dimensional
space. We randomly sampled only one labeled ex-
ample for each sense of word “use” as labeled data.
The remaining data in training set and test set served
as unlabeled data for bootstrapping and LP. All of
these three algorithms are evaluated using accuracy
on test set.
From Figure 3 (c) we can see that SVM misclassi-
</bodyText>
<footnote confidence="0.7922755">
5We used Isomap to perform dimensionality reduction by
computing two-dimensional, 39-nearest-neighbor-preserving
embedding of 210-dimensional input. Isomap is available at
http://isomap.stanford.edu/.
</footnote>
<bodyText confidence="0.999924527777778">
fied many examples from class + into class × since
using only features occurring in training set can not
reveal the intrinsic structure in full dataset.
For comparison, we implemented monolingual
bootstrapping with kNN (k=1) as base classifier.
The parameter b is set as 1. Only b unlabeled ex-
amples nearest to labeled examples and with the
distance less than dinter−class (the minimum dis-
tance between labeled examples with different sense
tags) will be added into classified data in each itera-
tion till no such unlabeled examples can be found.
Firstly we ran this monolingual bootstrapping on
this dataset to augment initial labeled data. The re-
sulting classified data is shown in Figure 3 (d). Then
a 1NN model was learned on this classified data and
we used this model to perform classification on the
remaining unlabeled data. Figure 3 (e) reports the
final classification result by this 1NN model. We can
see that bootstrapping does not perform well since it
is susceptible to small noise in dataset. For example,
in Figure 3 (d), the unlabeled example B 6 happened
to be closest to labeled example A, then 1NN model
tagged example B with label ⋄. But the correct label
of B should be + as shown in Figure 3 (b). This
error caused misclassification of other unlabeled ex-
amples that should have label +.
In LP, the label information of example C can
travel to B through unlabeled data. Then example A
will compete with C and other unlabeled examples
around B when determining the label of B. In other
words, the labels of unlabeled examples are deter-
mined not only by nearby labeled examples, but also
by nearby unlabeled examples. Using this classifi-
cation strategy achieves better performance than the
local consistency based strategy adopted by SVM
and bootstrapping.
</bodyText>
<subsectionHeader confidence="0.997639">
4.5 Experiment 3: LPcosine vs. LPJS
</subsectionHeader>
<bodyText confidence="0.900515818181818">
Table 3 summarizes the performance comparison
between LPcosine and LPJS on three datasets. We
can see that on SENSEVAL-3 corpus, LPJS per-
6In the two-dimensional space, example B is not the closest
example to A. The reason is that: (1) A is not close to most
of nearby examples around B, and B is not close to most of
nearby examples around A; (2) we used Isomap to maximally
preserve the neighborhood information between any example
and all other examples, which caused the loss of neighborhood
information between a few example pairs for obtaining a glob-
ally optimal solution.
</bodyText>
<page confidence="0.985917">
400
</page>
<table confidence="0.999208583333333">
Data H(D) H(W) H(YU) j
cos. vs. JS cos. vs. JS cos. vs. JS
SENSEVAL-3 (10%) SENSEVAL-3 (1%) &lt; (x) &lt; (x)
SENSEVAL-3 (25%) &gt; (V/) &lt; (x) &lt; (x)
SENSEVAL-3 (50%) &gt; (V/) &gt; (V/) &lt; (x)
SENSEVAL-3 (75%) &gt; (V/) &gt; (V/) &gt; (V/)
SENSEVAL-3 (100%) &lt; (o) &gt; (V/) &gt; (V/)
interest &lt; (V/) &gt; (V/) &lt; (o)
line &gt; (o) &gt; (V/) &lt; (V/)
&gt; (o) &gt; (o)
&gt; (x)
&gt; (o)
</table>
<tableCaption confidence="0.999141">
Table 3: Performance comparison between LPcosine and
</tableCaption>
<bodyText confidence="0.9625525">
LPJS and the results of three model selection criteria are re-
ported in following two tables. In the lower table, &lt; (or &gt;)
means that the average value of function H(Qcosine) is lower
(or higher) than H(QJS), and it will result in selecting cosine
(or JS) as distance measure. Qcosine (or Q JS) represents a ma-
trix using cosine similarity (or JS divergence). V/ and x denote
correct and wrong prediction results respectively, while o means
that any prediction is acceptable.
</bodyText>
<table confidence="0.9984821">
Data LPcosine vs. LPJS
p-value Significance
SENSEVAL-3 (1%) 1.1e-003 �
SENSEVAL-3 (10%) 8.9e-005 �
SENSEVAL-3 (25%) 9.0e-009 �
SENSEVAL-3 (50%) 3.2e-010 �
SENSEVAL-3 (75%) 7.7e-013 �
SENSEVAL-3 (100%) - -
interest 3.3e-002 &gt;
line 8.1e-002 �
</table>
<bodyText confidence="0.999540214285714">
forms significantly better than LPcosine, but their
performance is almost comparable on “interest” and
“line” corpora. This observation motivates us to au-
tomatically select a distance measure that will boost
the performance of LP on a given dataset.
Cross-validation on labeled data is not feasi-
ble due to the setting of semi-supervised learning
(l « u). In (Zhu and Ghahramani, 2002; Zhu et
al., 2003), they suggested a label entropy criterion
H(YU) for model selection, where Y is the label
matrix learned by their semi-supervised algorithms.
The intuition behind their method is that good para-
meters should result in confident labeling. Entropy
on matrix W (H(W)) is a commonly used measure
for unsupervised feature selection (Dash and Liu,
2000), which can be considered here. Another pos-
sible criterion for model selection is to measure the
entropy of c x c inter-class distance matrix D cal-
culated on labeled data (denoted as H(D)), where
Di,j represents the average distance between the i-
th class and the j-th class. We will investigate three
criteria, H(D), H(W) and H(YU), for model se-
lection. The distance measure can be automatically
selected by minimizing the average value of function
H(D), H(W) or H(YU) over 20 trials.
Let Q be the M x N matrix. Function H(Q) can
measure the entropy of matrix Q, which is defined
as (Dash and Liu, 2000):
</bodyText>
<equation confidence="0.9860204">
Si,j = exp (−α * Qi,j), (1)
M N
H(Q) = − E (Si,j log Si,j + (1 − Si,j) log (1 − Si,j)),
i=1 j=1
(2)
</equation>
<bodyText confidence="0.953933">
where α is positive constant. The possible value of α
</bodyText>
<figure confidence="0.3453925">
is − ln 0 .5 , I = M
I where N Ei ,j Qi,j . 5 is introduced
for normalization of matrix Q. For SENSEVAL-
3 data, we calculated an overall average score of
Y(Q) by EwN w,test H(Qw). Nw,test is the
W Nw,test
</figure>
<bodyText confidence="0.999879387096774">
number of examples in test set of word w. H(D),
H(W) and H(YU) can be obtained by replacing Q
with D, W and YU respectively.
Table 3 reports the automatic prediction results
of these three criteria.
From Table 3, we can see that using H(W)
can consistently select the optimal distance measure
when the performance gap between LPcosine and
LPJS is very large (denoted by « or »). But H(D)
and H(YU) fail to find the optimal distance measure
when only very few labeled examples are available
(percentage of labeled data &lt; 10%).
H(W) measures the separability of matrix W.
Higher value of H(W) means that distance mea-
sure decreases the separability of examples in full
dataset. Then the boundary between clusters is ob-
scured, which makes it difficult for LP to locate this
boundary. Therefore higher value of H(W) results
in worse performance of LP.
When labeled dataset is small, the distances be-
tween classes can not be reliably estimated, which
results in unreliable indication of the separability
of examples in full dataset. This is the reason that
H(D) performs poorly on SENSEVAL-3 corpus
when the percentage of labeled data is less than 25%.
For H(YU), small labeled dataset can not reveal
intrinsic structure in data, which may bias the esti-
mation of YU. Then labeling confidence (H(YU))
can not properly indicate the performance of LP.
This may interpret the poor performance of H(YU)
on SENSEVAL-3 data when percentage &lt; 25%.
</bodyText>
<page confidence="0.998355">
401
</page>
<sectionHeader confidence="0.998961" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999902567567568">
In this paper we have investigated a label propaga-
tion based semi-supervised learning algorithm for
WSD, which fully realizes a global consistency as-
sumption: similar examples should have similar la-
bels. In learning process, the labels of unlabeled ex-
amples are determined not only by nearby labeled
examples, but also by nearby unlabeled examples.
Compared with semi-supervised WSD methods in
the first and second categories, our corpus based
method does not need external resources, includ-
ing WordNet, bilingual lexicon, aligned parallel cor-
pora. Our analysis and experimental results demon-
strate the potential of this cluster assumption based
algorithm. It achieves better performance than SVM
when only very few labeled examples are avail-
able, and its performance is also better than mono-
lingual bootstrapping and comparable to bilingual
bootstrapping. Finally we suggest an entropy based
method to automatically identify a distance measure
that can boost the performance of LP algorithm on a
given dataset.
It has been shown that one sense per discourse
property can improve the performance of bootstrap-
ping algorithm (Li and Li, 2004; Yarowsky, 1995).
This heuristics can be integrated into LP algorithm
by setting weight WZj = 1 if the i-th and j-th in-
stances are in the same discourse.
In the future we may extend the evaluation of LP
algorithm and related cluster assumption based al-
gorithms using more benchmark data for WSD. An-
other direction is to use feature clustering technique
to deal with data sparseness and noisy feature prob-
lem.
Acknowledgements We would like to thank
anonymous reviewers for their helpful comments.
Z.Y. Niu is supported by A*STAR Graduate Schol-
arship.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999474">
Belkin, M., &amp; Niyogi, P.. 2002. Using Manifold Structure for Partially Labeled
Classification. NIPS 15.
Blum, A., Lafferty, J., Rwebangira, R., &amp; Reddy, R.. 2004. Semi-Supervised
Learning Using Randomized Mincuts. ICML-2004.
Brown P., Stephen, D.P., Vincent, D.P., &amp; Robert, Mercer.. 1991. Word Sense
Disambiguation Using Statistical Methods. ACL-1991.
Chapelle, O., Weston, J., &amp; Scholkopf, B. 2002. Cluster Kernels for Semi-
supervised Learning. NIPS 15.
Dagan, I. &amp; Itai A.. 1994. Word Sense Disambiguation Using A Second Lan-
guage Monolingual Corpus. Computational Linguistics, Vol. 20(4), pp. 563-
596.
Dash, M., &amp; Liu, H.. 2000. Feature Selection for Clustering. PAKDD(pp. 110–
121).
Diab, M., &amp; Resnik. P.. 2002. An Unsupervised Method for Word Sense Tagging
Using Parallel Corpora. ACL-2002(pp. 255–262).
Hearst, M.. 1991. Noun Homograph Disambiguation using Local Context in
Large Text Corpora. Proceedings of the 7th Annual Conference of the UW
Centre for the New OED and Text Research: Using Corpora, 24:1, 1–41.
Karov, Y. &amp; Edelman, S.. 1998. Similarity-Based Word Sense Disambiguation.
Computational Linguistics, 24(1): 41-59.
Leacock, C., Miller, G.A. &amp; Chodorow, M.. 1998. Using Corpus Statistics and
WordNet Relations for Sense Identification. Computational Linguistics, 24:1,
147–165.
Lee, Y.K. &amp; Ng, H.T.. 2002. An Empirical Evaluation of Knowledge Sources and
Learning Algorithms for Word Sense Disambiguation. EMNLP-2002, (pp.
41-48).
Lesk M.. 1986. Automated Word Sense Disambiguation Using Machine Read-
able Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone. Pro-
ceedings of the ACM SIGDOC Conference.
Li, H. &amp; Li, C.. 2004. Word Translation Disambiguation Using Bilingual Boot-
strapping. Computational Linguistics, 30(1), 1-22.
Lin, D.K.. 1997. Using Syntactic Dependency as Local Context to Resolve Word
Sense Ambiguity. ACL-1997.
Lin, J. 1991. Divergence Measures Based on the Shannon Entropy. IEEE Trans-
actions on Information Theory, 37:1, 145–150.
McCarthy, D., Koeling, R., Weeds, J., &amp; Carroll, J.. 2004. Finding Predominant
Word Senses in Untagged Text. ACL-2004.
Mihalcea R.. 2004. Co-training and Self-training for Word Sense Disambigua-
tion. CoNLL-2004.
Mihalcea R., Chklovski, T., &amp; Kilgariff, A.. 2004. The SENSEVAL-3 English
Lexical Sample Task. SENSEVAL-2004.
Ng, H.T., Wang, B., &amp; Chan, Y.S.. 2003. Exploiting Parallel Texts for Word
Sense Disambiguation: An Empirical Study. ACL-2003, pp. 455-462.
Park, S.B., Zhang, B.T., &amp; Kim, Y.T.. 2000. Word Sense Disambiguation by
Learning from Unlabeled Data. ACL-2000.
Schiitze, H.. 1998. Automatic Word Sense Discrimination. Computational Lin-
guistics, 24:1, 97–123.
Seo, H.C., Chung, H.J., Rim, H.C., Myaeng. S.H., &amp; Kim, S.H.. 2004. Unsu-
pervised Word Sense Disambiguation Using WordNet Relatives. Computer,
Speech and Language, 18:3, 253–273.
Szummer, M., &amp; Jaakkola, T.. 2001. Partially Labeled Classification with Markov
Random Walks. NIPS 14.
Yarowsky, D.. 1995. Unsupervised Word Sense Disambiguation Rivaling Super-
vised Methods. ACL-1995, pp. 189-196.
Yarowsky, D.. 1992. Word Sense Disambiguation Using Statistical Models of
Roget’s Categories Trained on Large Corpora. COLING-1992, pp. 454-460.
Zhu, X. &amp; Ghahramani, Z.. 2002. Learning from Labeled and Unlabeled Data
with Label Propagation. CMU CALD tech report CMU-CALD-02-107.
Zhu, X., Ghahramani, Z., &amp; Lafferty, J.. 2003. Semi-Supervised Learning Using
Gaussian Fields and Harmonic Functions. ICML-2003.
</reference>
<page confidence="0.998615">
402
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.544041">
<title confidence="0.9988885">Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning</title>
<author confidence="0.997358">Zheng-Yu Niu</author>
<author confidence="0.997358">Dong-Hong Ji</author>
<affiliation confidence="0.999976">Institute for Infocomm Research</affiliation>
<address confidence="0.98084">21 Heng Mui Keng Terrace 119613 Singapore</address>
<author confidence="0.997967">Chew Lim Tan</author>
<affiliation confidence="0.898290666666667">Department of Computer Science National University of Singapore 3 Science Drive 2</affiliation>
<address confidence="0.980205">117543 Singapore</address>
<email confidence="0.996331">tancl@comp.nus.edu.sg</email>
<abstract confidence="0.990314529411765">Shortage of manually sense-tagged data is an obstacle to supervised word sense disambiguation methods. In this paper we investigate a label propagation based semisupervised learning algorithm for WSD, which combines labeled and unlabeled data in learning process to fully realize a global consistency assumption: similar examples should have similar labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Belkin</author>
<author>P Niyogi</author>
</authors>
<title>Using Manifold Structure for Partially Labeled Classification.</title>
<date>2002</date>
<journal>NIPS</journal>
<volume>15</volume>
<contexts>
<context position="3957" citStr="Belkin and Niyogi, 2002" startWordPosition="593" endWordPosition="596">this bootstrapping process. Bootstrapping is based on a local consistency assumption: examples close to labeled examples within same class will have same labels, which is also the assumption underlying many supervised learning algorithms, such as kNN. Recently a promising family of semi-supervised learning algorithms are introduced, which can effectively combine unlabeled data with labeled data 395 Proceedings of the 43rd Annual Meeting of the ACL, pages 395–402, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics in learning process by exploiting cluster structure in data (Belkin and Niyogi, 2002; Blum et al., 2004; Chapelle et al., 1991; Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002; Zhu et al., 2003). Here we investigate a label propagation based semisupervised learning algorithm (LP algorithm) (Zhu and Ghahramani, 2002) for WSD, which works by representing labeled and unlabeled examples as vertices in a connected graph, then iteratively propagating label information from any vertex to nearby vertices through weighted edges, finally inferring the labels of unlabeled examples after this propagation process converges. Compared with bootstrapping, LP algorithm is based on a glob</context>
</contexts>
<marker>Belkin, Niyogi, 2002</marker>
<rawString>Belkin, M., &amp; Niyogi, P.. 2002. Using Manifold Structure for Partially Labeled Classification. NIPS 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>J Lafferty</author>
<author>R Rwebangira</author>
<author>R Reddy</author>
</authors>
<title>Semi-Supervised Learning Using Randomized Mincuts.</title>
<date>2004</date>
<contexts>
<context position="3976" citStr="Blum et al., 2004" startWordPosition="597" endWordPosition="600">s. Bootstrapping is based on a local consistency assumption: examples close to labeled examples within same class will have same labels, which is also the assumption underlying many supervised learning algorithms, such as kNN. Recently a promising family of semi-supervised learning algorithms are introduced, which can effectively combine unlabeled data with labeled data 395 Proceedings of the 43rd Annual Meeting of the ACL, pages 395–402, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics in learning process by exploiting cluster structure in data (Belkin and Niyogi, 2002; Blum et al., 2004; Chapelle et al., 1991; Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002; Zhu et al., 2003). Here we investigate a label propagation based semisupervised learning algorithm (LP algorithm) (Zhu and Ghahramani, 2002) for WSD, which works by representing labeled and unlabeled examples as vertices in a connected graph, then iteratively propagating label information from any vertex to nearby vertices through weighted edges, finally inferring the labels of unlabeled examples after this propagation process converges. Compared with bootstrapping, LP algorithm is based on a global consistency assu</context>
</contexts>
<marker>Blum, Lafferty, Rwebangira, Reddy, 2004</marker>
<rawString>Blum, A., Lafferty, J., Rwebangira, R., &amp; Reddy, R.. 2004. Semi-Supervised Learning Using Randomized Mincuts. ICML-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>D P Stephen</author>
<author>D P Vincent</author>
<author>Mercer Robert</author>
</authors>
<title>Word Sense Disambiguation Using Statistical Methods.</title>
<date>1991</date>
<contexts>
<context position="2675" citStr="Brown et al., 1991" startWordPosition="401" endWordPosition="404">arning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity inf</context>
</contexts>
<marker>Brown, Stephen, Vincent, Robert, 1991</marker>
<rawString>Brown P., Stephen, D.P., Vincent, D.P., &amp; Robert, Mercer.. 1991. Word Sense Disambiguation Using Statistical Methods. ACL-1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Chapelle</author>
<author>J Weston</author>
<author>B Scholkopf</author>
</authors>
<title>Cluster Kernels for Semisupervised Learning.</title>
<date>2002</date>
<journal>NIPS</journal>
<volume>15</volume>
<marker>Chapelle, Weston, Scholkopf, 2002</marker>
<rawString>Chapelle, O., Weston, J., &amp; Scholkopf, B. 2002. Cluster Kernels for Semisupervised Learning. NIPS 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>A Itai</author>
</authors>
<title>Word Sense Disambiguation Using A Second Language Monolingual Corpus.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>563--596</pages>
<contexts>
<context position="2697" citStr="Dagan and Itai, 1994" startWordPosition="405" endWordPosition="408">h the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabel</context>
</contexts>
<marker>Dagan, Itai, 1994</marker>
<rawString>Dagan, I. &amp; Itai A.. 1994. Word Sense Disambiguation Using A Second Language Monolingual Corpus. Computational Linguistics, Vol. 20(4), pp. 563-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dash</author>
<author>H Liu</author>
</authors>
<title>Feature Selection for Clustering.</title>
<date>2000</date>
<pages>110--121</pages>
<contexts>
<context position="26253" citStr="Dash and Liu, 2000" startWordPosition="4525" endWordPosition="4528">ation motivates us to automatically select a distance measure that will boost the performance of LP on a given dataset. Cross-validation on labeled data is not feasible due to the setting of semi-supervised learning (l « u). In (Zhu and Ghahramani, 2002; Zhu et al., 2003), they suggested a label entropy criterion H(YU) for model selection, where Y is the label matrix learned by their semi-supervised algorithms. The intuition behind their method is that good parameters should result in confident labeling. Entropy on matrix W (H(W)) is a commonly used measure for unsupervised feature selection (Dash and Liu, 2000), which can be considered here. Another possible criterion for model selection is to measure the entropy of c x c inter-class distance matrix D calculated on labeled data (denoted as H(D)), where Di,j represents the average distance between the ith class and the j-th class. We will investigate three criteria, H(D), H(W) and H(YU), for model selection. The distance measure can be automatically selected by minimizing the average value of function H(D), H(W) or H(YU) over 20 trials. Let Q be the M x N matrix. Function H(Q) can measure the entropy of matrix Q, which is defined as (Dash and Liu, 20</context>
</contexts>
<marker>Dash, Liu, 2000</marker>
<rawString>Dash, M., &amp; Liu, H.. 2000. Feature Selection for Clustering. PAKDD(pp. 110– 121).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P</author>
</authors>
<title>An Unsupervised Method for Word Sense Tagging Using Parallel Corpora.</title>
<date>2002</date>
<pages>2002--255</pages>
<marker>P, 2002</marker>
<rawString>Diab, M., &amp; Resnik. P.. 2002. An Unsupervised Method for Word Sense Tagging Using Parallel Corpora. ACL-2002(pp. 255–262).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Noun Homograph Disambiguation using Local Context in Large Text Corpora.</title>
<date>1991</date>
<booktitle>Proceedings of the 7th Annual Conference of the UW Centre for the New OED and Text Research: Using Corpora,</booktitle>
<volume>24</volume>
<pages>1--41</pages>
<contexts>
<context position="2883" citStr="Hearst, 1991" startWordPosition="437" endWordPosition="438">ces, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process. Bootstrapping is based on a local consistency assumption: examples close to labeled examples within same class will have </context>
</contexts>
<marker>Hearst, 1991</marker>
<rawString>Hearst, M.. 1991. Noun Homograph Disambiguation using Local Context in Large Text Corpora. Proceedings of the 7th Annual Conference of the UW Centre for the New OED and Text Research: Using Corpora, 24:1, 1–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Karov</author>
<author>S Edelman</author>
</authors>
<title>Similarity-Based Word Sense Disambiguation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>41--59</pages>
<contexts>
<context position="2908" citStr="Karov and Edelman, 1998" startWordPosition="439" endWordPosition="442">saurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process. Bootstrapping is based on a local consistency assumption: examples close to labeled examples within same class will have same labels, which is als</context>
</contexts>
<marker>Karov, Edelman, 1998</marker>
<rawString>Karov, Y. &amp; Edelman, S.. 1998. Similarity-Based Word Sense Disambiguation. Computational Linguistics, 24(1): 41-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>G A Miller</author>
<author>M Chodorow</author>
</authors>
<title>Using Corpus Statistics and WordNet Relations for Sense Identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>147--165</pages>
<contexts>
<context position="1265" citStr="Leacock et al., 1998" startWordPosition="184" endWordPosition="187"> consistency assumption: similar examples should have similar labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping. 1 Introduction In this paper, we address the problem of word sense disambiguation (WSD), which is to assign an appropriate sense to an occurrence of a word in a given context. Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al., 1998), semi-supervised learning algorithms (Yarowsky, 1995), and unsupervised learning algorithms (Schfitze, 1998). Supervised sense disambiguation has been very successful, but it requires a lot of manually sensetagged data and can not utilize raw unannotated data that can be cheaply acquired. Fully unsupervised methods do not need the definition of senses and manually sense-tagged data, but their sense clustering results can not be directly used in many NLP tasks since there is no sense tag for each instance in clusters. Considering both the availability of a large amount of unlabelled data and d</context>
</contexts>
<marker>Leacock, Miller, Chodorow, 1998</marker>
<rawString>Leacock, C., Miller, G.A. &amp; Chodorow, M.. 1998. Using Corpus Statistics and WordNet Relations for Sense Identification. Computational Linguistics, 24:1, 147–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y K Lee</author>
<author>H T Ng</author>
</authors>
<title>An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>EMNLP-2002,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="14758" citStr="Lee and Ng, 2002" startWordPosition="2554" endWordPosition="2557">5% 65.7±0.5% 66.9±0.6% 75% 68.7±0.4% 67.3±0.4% 68.7±0.3% 100% 69.7% 68.4% 70.3% Percentage SVM vs. LPcosine SVM vs. LPJs p-value Sign. p-value Sign. 1% 8.7e-004 ≪ 8.5e-005 ≪ 10% 1.9e-006 ≪ 1.0e-008 ≪ 25% 9.2e-001 ∼ 3.0e-006 ≪ 50% 1.9e-006 ≫ 6.2e-002 ∼ 75% 7.4e-013 ≫ 7.1e-001 ∼ 100% - - - - Systems Baseline htsa3 IRST-Kernels nusels Accuracy 55.2% 72.9% 72.6% 72.4% We used three types of features to capture contextual information: part-of-speech of neighboring words with position information, unordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). For SVM, we did not perform feature selection on SENSEVAL-3 data since feature selection deteriorates its performance (Lee and Ng, 2002). When running LP on the three datasets, we removed the features with occurrence frequency (counted in both training set and test set) less than 3 times. We investigated two distance measures for LP: cosine similarity and Jensen-Shannon (JS) divergence (Lin, 1991). For the three datasets, we constructed connected graphs following (Zhu et al., 2003): two instances u, v will be connected by an edge if u is among </context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Lee, Y.K. &amp; Ng, H.T.. 2002. An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation. EMNLP-2002, (pp. 41-48).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automated Word Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone.</title>
<date>1986</date>
<booktitle>Proceedings of the ACM SIGDOC Conference.</booktitle>
<contexts>
<context position="2391" citStr="Lesk, 1986" startWordPosition="356" endWordPosition="357">usters. Considering both the availability of a large amount of unlabelled data and direct use of word senses, semi-supervised learning methods have received great attention recently. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-super</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Lesk M.. 1986. Automated Word Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone. Proceedings of the ACM SIGDOC Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>C Li</author>
</authors>
<title>Word Translation Disambiguation Using Bilingual Bootstrapping.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<pages>1--22</pages>
<contexts>
<context position="2737" citStr="Li and Li, 2004" startWordPosition="413" endWordPosition="416">ry for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in thi</context>
<context position="9850" citStr="Li and Li, 2004" startWordPosition="1709" endWordPosition="1712"> �YU = limt→oo YUt = (I − Tuu)−1TulYL0 (Zhu and Ghahramani, 2002). We can see that this solution can be obtained without iteration and the initialization of YU0 is not important, since YU0 does not affect the estimation of �YU. I is u x u identity matrix. Tuu and Tul are acquired by splitting matrix T after the l-th row and the l-th column into 4 sub-matrices. 3.2 Comparison between SVM, Bootstrapping and LP For WSD, SVM is one of the state of the art supervised learning algorithms (Mihalcea et al., 2004), while bootstrapping is one of the state of the art semi-supervised learning algorithms (Li and Li, 2004; Yarowsky, 1995). For comparing LP with SVM and bootstrapping, let us consider a dataset with two-moon pattern shown in Figure 1(a). The upper moon consists of 9 points, while the lower moon consists of 13 points. There is only one labeled point in each moon, and other 20 points are unFigure 2: Classification result of LP on two-moon pattern dataset. (a) Minimum spanning tree of this dataset. The convergence process of LP algorithm with t varying from 1 to 100 is shown from (b) to (f). labeled. The distance metric is Euclidian distance. We can see that the points in one moon should be more si</context>
<context position="17998" citStr="Li and Li, 2004" startWordPosition="3126" endWordPosition="3129">taset (percentage of labeled data ≤ 10%), LP performs significantly better than SVM. When the percentage of labeled data increases from 50% to 75%, the performance of LPJS and SVM become almost same, while LP�osine performs significantly worse than SVM. 3we used linear SV Mlight, available at http://svmlight.joachims.org/. 4If there are multiple sense tags for an instance in training set or test set, then only the first tag is considered as correct answer. Furthermore, if the answer of the instance in test set is “U”, then this instance will be removed from test set. Table 2: Accuracies from (Li and Li, 2004) and average accuracies of LP with c x b labeled examples on “interest” and “line” corpora. Major is a baseline method in which they always choose the most frequent sense. MB-D denotes monolingual bootstrapping with decision list as base classifier, MB-B represents monolingual bootstrapping with ensemble of Naive Bayes as base classifier, and BB is bilingual bootstrapping with ensemble of Naive Bayes as base classifier. 4.3 Experiment 2: LP vs. Bootstrapping Li and Li (2004) used “interest” and “line” corpora as test data. For the word “interest”, they used its four major senses. For compariso</context>
<context position="20128" citStr="Li and Li, 2004" startWordPosition="3483" endWordPosition="3486"> much better than MB-D and MB-B on both “interest” and “line” corpora, while the performance of LP is comparable to BB on these two corpora. 4.4 An Example: Word “use” For investigating the reason for LP to outperform SVM and monolingual bootstrapping, we used the data of word “use” in English lexical sample task of SENSEVAL-3 as an example (totally 26 examples in training set and 14 examples in test set). For data Ambiguous words interest 54.6% 54.7% 69.3% 75.5% line 53.5% 55.6% 54.1% 62.7% Ambiguous words interest 4x15=60 80.2±2.0% 79.8±2.0% line 6x15=90 60.3±4.5% 59.4±3.9% Accuracies from (Li and Li, 2004) Major MB-D MB-B BB Our results #labeled examples LPcosine LPiS 399 (a) Initial Seting (b) Ground−truth 0.5 0.5 0 0 −0.5 −0.5 −0.4 −0.2 0 0.2 0.4 0.6 −0.4 −0.2 0 0.2 0.4 0.6 (c) SVM (d) Bootstrapping 0.5 0.5 C 0 0 B A −0.5 −0.5 −0.4 −0.2 0 0.2 0.4 0.6 −0.4 −0.2 0 0.2 0.4 0.6 (e) Bootstrapping (f) LP 0.5 0.5 0 0 −0.5 −0.5 −0.4 −0.2 0 0.2 0.4 0.6 −0.4 −0.2 0 0.2 0.4 0.6 Figure 3: Comparison of sense disambiguation results between SVM, monolingual bootstrapping and LP on word “use”. (a) only one labeled example for each sense of word “use” as training data before sense disambiguation (◦ and ⊲ den</context>
</contexts>
<marker>Li, Li, 2004</marker>
<rawString>Li, H. &amp; Li, C.. 2004. Word Translation Disambiguation Using Bilingual Bootstrapping. Computational Linguistics, 30(1), 1-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Lin</author>
</authors>
<title>Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity.</title>
<date>1997</date>
<contexts>
<context position="2402" citStr="Lin, 1997" startWordPosition="358" endWordPosition="359">idering both the availability of a large amount of unlabelled data and direct use of word senses, semi-supervised learning methods have received great attention recently. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learn</context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>Lin, D.K.. 1997. Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity. ACL-1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
</authors>
<title>Divergence Measures Based on the Shannon Entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<pages>145--150</pages>
<contexts>
<context position="15208" citStr="Lin, 1991" startWordPosition="2628" endWordPosition="2629">boring words with position information, unordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). For SVM, we did not perform feature selection on SENSEVAL-3 data since feature selection deteriorates its performance (Lee and Ng, 2002). When running LP on the three datasets, we removed the features with occurrence frequency (counted in both training set and test set) less than 3 times. We investigated two distance measures for LP: cosine similarity and Jensen-Shannon (JS) divergence (Lin, 1991). For the three datasets, we constructed connected graphs following (Zhu et al., 2003): two instances u, v will be connected by an edge if u is among v’s k nearest neighbors, or if v is among u’s k nearest neighbors as measured by cosine or JS distance measure. For “interest” and “line” corpora, k is 10 (following (Zhu et al., 2003)), while for SENSEVAL-3 data, k is 5 since the size of dataset for each word in SENSEVAL-3 is much less than that of “interest” and “line” datasets. 398 4.2 Experiment 1: LP vs. SVM In this experiment, we evaluated LP and SVM 3 on the data of English lexical sample </context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Lin, J. 1991. Divergence Measures Based on the Shannon Entropy. IEEE Transactions on Information Theory, 37:1, 145–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Koeling</author>
<author>J Weeds</author>
<author>J Carroll</author>
</authors>
<title>Finding Predominant Word Senses in Untagged Text.</title>
<date>2004</date>
<contexts>
<context position="2425" citStr="McCarthy et al., 2004" startWordPosition="360" endWordPosition="363">h the availability of a large amount of unlabelled data and direct use of word senses, semi-supervised learning methods have received great attention recently. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, boo</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>McCarthy, D., Koeling, R., Weeds, J., &amp; Carroll, J.. 2004. Finding Predominant Word Senses in Untagged Text. ACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Co-training and Self-training for Word Sense Disambiguation.</title>
<date>2004</date>
<publisher>CoNLL-2004.</publisher>
<contexts>
<context position="2924" citStr="Mihalcea, 2004" startWordPosition="443" endWordPosition="444">sambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process. Bootstrapping is based on a local consistency assumption: examples close to labeled examples within same class will have same labels, which is also the assumption</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>Mihalcea R.. 2004. Co-training and Self-training for Word Sense Disambiguation. CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>T Chklovski</author>
<author>A Kilgariff</author>
</authors>
<date>2004</date>
<booktitle>The SENSEVAL-3 English Lexical Sample Task. SENSEVAL-2004.</booktitle>
<contexts>
<context position="9745" citStr="Mihalcea et al., 2004" startWordPosition="1692" endWordPosition="1695">4. Assign xh(l + 1 &lt; h &lt; n) with a label sˆj, where j = argmaxjYhj. This algorithm has been shown to converge to �YU = limt→oo YUt = (I − Tuu)−1TulYL0 (Zhu and Ghahramani, 2002). We can see that this solution can be obtained without iteration and the initialization of YU0 is not important, since YU0 does not affect the estimation of �YU. I is u x u identity matrix. Tuu and Tul are acquired by splitting matrix T after the l-th row and the l-th column into 4 sub-matrices. 3.2 Comparison between SVM, Bootstrapping and LP For WSD, SVM is one of the state of the art supervised learning algorithms (Mihalcea et al., 2004), while bootstrapping is one of the state of the art semi-supervised learning algorithms (Li and Li, 2004; Yarowsky, 1995). For comparing LP with SVM and bootstrapping, let us consider a dataset with two-moon pattern shown in Figure 1(a). The upper moon consists of 9 points, while the lower moon consists of 13 points. There is only one labeled point in each moon, and other 20 points are unFigure 2: Classification result of LP on two-moon pattern dataset. (a) Minimum spanning tree of this dataset. The convergence process of LP algorithm with t varying from 1 to 100 is shown from (b) to (f). lab</context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgariff, 2004</marker>
<rawString>Mihalcea R., Chklovski, T., &amp; Kilgariff, A.. 2004. The SENSEVAL-3 English Lexical Sample Task. SENSEVAL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>B Wang</author>
<author>Y S Chan</author>
</authors>
<title>Exploiting Parallel Texts for Word Sense Disambiguation: An Empirical Study.</title>
<date>2003</date>
<booktitle>ACL-2003,</booktitle>
<pages>455--462</pages>
<contexts>
<context position="2755" citStr="Ng et al., 2003" startWordPosition="417" endWordPosition="420">ds. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping pr</context>
</contexts>
<marker>Ng, Wang, Chan, 2003</marker>
<rawString>Ng, H.T., Wang, B., &amp; Chan, Y.S.. 2003. Exploiting Parallel Texts for Word Sense Disambiguation: An Empirical Study. ACL-2003, pp. 455-462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Park</author>
<author>B T Zhang</author>
<author>Y T Kim</author>
</authors>
<date>2000</date>
<booktitle>Word Sense Disambiguation by Learning from Unlabeled Data. ACL-2000.</booktitle>
<contexts>
<context position="2943" citStr="Park et al., 2000" startWordPosition="445" endWordPosition="448">senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process. Bootstrapping is based on a local consistency assumption: examples close to labeled examples within same class will have same labels, which is also the assumption underlying many su</context>
</contexts>
<marker>Park, Zhang, Kim, 2000</marker>
<rawString>Park, S.B., Zhang, B.T., &amp; Kim, Y.T.. 2000. Word Sense Disambiguation by Learning from Unlabeled Data. ACL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schiitze</author>
</authors>
<title>Automatic Word Sense Discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>97--123</pages>
<marker>Schiitze, 1998</marker>
<rawString>Schiitze, H.. 1998. Automatic Word Sense Discrimination. Computational Linguistics, 24:1, 97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S H</author>
<author>S H Kim</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Using WordNet Relatives.</title>
<date>2004</date>
<journal>Computer, Speech and Language,</journal>
<volume>18</volume>
<pages>253--273</pages>
<marker>H, Kim, 2004</marker>
<rawString>Seo, H.C., Chung, H.J., Rim, H.C., Myaeng. S.H., &amp; Kim, S.H.. 2004. Unsupervised Word Sense Disambiguation Using WordNet Relatives. Computer, Speech and Language, 18:3, 253–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Szummer</author>
<author>T Jaakkola</author>
</authors>
<title>Partially Labeled Classification with Markov Random Walks.</title>
<date>2001</date>
<journal>NIPS</journal>
<volume>14</volume>
<contexts>
<context position="4027" citStr="Szummer and Jaakkola, 2001" startWordPosition="605" endWordPosition="608">stency assumption: examples close to labeled examples within same class will have same labels, which is also the assumption underlying many supervised learning algorithms, such as kNN. Recently a promising family of semi-supervised learning algorithms are introduced, which can effectively combine unlabeled data with labeled data 395 Proceedings of the 43rd Annual Meeting of the ACL, pages 395–402, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics in learning process by exploiting cluster structure in data (Belkin and Niyogi, 2002; Blum et al., 2004; Chapelle et al., 1991; Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002; Zhu et al., 2003). Here we investigate a label propagation based semisupervised learning algorithm (LP algorithm) (Zhu and Ghahramani, 2002) for WSD, which works by representing labeled and unlabeled examples as vertices in a connected graph, then iteratively propagating label information from any vertex to nearby vertices through weighted edges, finally inferring the labels of unlabeled examples after this propagation process converges. Compared with bootstrapping, LP algorithm is based on a global consistency assumption. Intuitively, if there is at least one label</context>
</contexts>
<marker>Szummer, Jaakkola, 2001</marker>
<rawString>Szummer, M., &amp; Jaakkola, T.. 2001. Partially Labeled Classification with Markov Random Walks. NIPS 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>ACL-1995,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="1319" citStr="Yarowsky, 1995" startWordPosition="191" endWordPosition="192">r labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping. 1 Introduction In this paper, we address the problem of word sense disambiguation (WSD), which is to assign an appropriate sense to an occurrence of a word in a given context. Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al., 1998), semi-supervised learning algorithms (Yarowsky, 1995), and unsupervised learning algorithms (Schfitze, 1998). Supervised sense disambiguation has been very successful, but it requires a lot of manually sensetagged data and can not utilize raw unannotated data that can be cheaply acquired. Fully unsupervised methods do not need the definition of senses and manually sense-tagged data, but their sense clustering results can not be directly used in many NLP tasks since there is no sense tag for each instance in clusters. Considering both the availability of a large amount of unlabelled data and direct use of word senses, semi-supervised learning met</context>
<context position="2960" citStr="Yarowsky, 1995" startWordPosition="449" endWordPosition="450">ally generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process. Bootstrapping is based on a local consistency assumption: examples close to labeled examples within same class will have same labels, which is also the assumption underlying many supervised learning</context>
<context position="9867" citStr="Yarowsky, 1995" startWordPosition="1713" endWordPosition="1714">t = (I − Tuu)−1TulYL0 (Zhu and Ghahramani, 2002). We can see that this solution can be obtained without iteration and the initialization of YU0 is not important, since YU0 does not affect the estimation of �YU. I is u x u identity matrix. Tuu and Tul are acquired by splitting matrix T after the l-th row and the l-th column into 4 sub-matrices. 3.2 Comparison between SVM, Bootstrapping and LP For WSD, SVM is one of the state of the art supervised learning algorithms (Mihalcea et al., 2004), while bootstrapping is one of the state of the art semi-supervised learning algorithms (Li and Li, 2004; Yarowsky, 1995). For comparing LP with SVM and bootstrapping, let us consider a dataset with two-moon pattern shown in Figure 1(a). The upper moon consists of 9 points, while the lower moon consists of 13 points. There is only one labeled point in each moon, and other 20 points are unFigure 2: Classification result of LP on two-moon pattern dataset. (a) Minimum spanning tree of this dataset. The convergence process of LP algorithm with t varying from 1 to 100 is shown from (b) to (f). labeled. The distance metric is Euclidian distance. We can see that the points in one moon should be more similar to each oth</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, D.. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. ACL-1995, pp. 189-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Word Sense Disambiguation Using Statistical Models</title>
<date>1992</date>
<booktitle>of Roget’s Categories Trained on Large Corpora. COLING-1992,</booktitle>
<pages>454--460</pages>
<contexts>
<context position="2460" citStr="Yarowsky, 1992" startWordPosition="368" endWordPosition="369">nlabelled data and direct use of word senses, semi-supervised learning methods have received great attention recently. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by itera</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>Yarowsky, D.. 1992. Word Sense Disambiguation Using Statistical Models of Roget’s Categories Trained on Large Corpora. COLING-1992, pp. 454-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
</authors>
<date>2002</date>
<booktitle>Learning from Labeled and Unlabeled Data with Label Propagation. CMU CALD tech report</booktitle>
<pages>02--107</pages>
<contexts>
<context position="4053" citStr="Zhu and Ghahramani, 2002" startWordPosition="609" endWordPosition="612">close to labeled examples within same class will have same labels, which is also the assumption underlying many supervised learning algorithms, such as kNN. Recently a promising family of semi-supervised learning algorithms are introduced, which can effectively combine unlabeled data with labeled data 395 Proceedings of the 43rd Annual Meeting of the ACL, pages 395–402, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics in learning process by exploiting cluster structure in data (Belkin and Niyogi, 2002; Blum et al., 2004; Chapelle et al., 1991; Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002; Zhu et al., 2003). Here we investigate a label propagation based semisupervised learning algorithm (LP algorithm) (Zhu and Ghahramani, 2002) for WSD, which works by representing labeled and unlabeled examples as vertices in a connected graph, then iteratively propagating label information from any vertex to nearby vertices through weighted edges, finally inferring the labels of unlabeled examples after this propagation process converges. Compared with bootstrapping, LP algorithm is based on a global consistency assumption. Intuitively, if there is at least one labeled example in each cluster</context>
<context position="6453" citStr="Zhu and Ghahramani, 2002" startWordPosition="1033" endWordPosition="1036">information among examples in X. The cluster structure in X can be represented as a connected graph, where each vertex corresponds to an example, and the edge between any two examples xi and xj is weighted so that the closer the vertices in some distance measure, the larger the weight associated with this edge. The weights are defined as 2 follows: Wij = exp( − a2) if i 7� j and Wii = 0 (1 &lt; i, j &lt; n), where dij is the distance (ex. Euclidean distance) between xi and xj, and Q is used to control the weight Wij. 3 Semi-supervised Learning Method 3.1 Label Propagation Algorithm In LP algorithm (Zhu and Ghahramani, 2002), label information of any vertex in a graph is propagated to nearby vertices through weighted edges until a global stable stage is achieved. Larger edge weights allow labels to travel through easier. Thus the closer the examples, more likely they have similar labels (the global consistency assumption). In label propagation process, the soft label of each initial labeled example is clamped in each iteration to replenish label sources from these labeled data. Thus the labeled data act like sources to push out labels through unlabeled data. With this push from labeled examples, the class boundar</context>
<context position="9300" citStr="Zhu and Ghahramani, 2002" startWordPosition="1609" endWordPosition="1612">Two-moon pattern dataset with two labeled points, (b) classification result by SVM, (c) labeling procedure of bootstrapping algorithm, (d) ideal classification. 2 2 1 1 0 0 −1 −1 −2 −2 −2 −1 0 1 2 3 −2 −1 0 1 2 3 Then LP algorithm is defined as follows: 1. Initially set t=0, where t is iteration index; 2. Propagate the label by Yt+1 = TYt; 3. Clamp labeled data by replacing the top l row of Yt+1 with Y0L. Repeat from step 2 until Yt converges; 4. Assign xh(l + 1 &lt; h &lt; n) with a label sˆj, where j = argmaxjYhj. This algorithm has been shown to converge to �YU = limt→oo YUt = (I − Tuu)−1TulYL0 (Zhu and Ghahramani, 2002). We can see that this solution can be obtained without iteration and the initialization of YU0 is not important, since YU0 does not affect the estimation of �YU. I is u x u identity matrix. Tuu and Tul are acquired by splitting matrix T after the l-th row and the l-th column into 4 sub-matrices. 3.2 Comparison between SVM, Bootstrapping and LP For WSD, SVM is one of the state of the art supervised learning algorithms (Mihalcea et al., 2004), while bootstrapping is one of the state of the art semi-supervised learning algorithms (Li and Li, 2004; Yarowsky, 1995). For comparing LP with SVM and b</context>
<context position="25887" citStr="Zhu and Ghahramani, 2002" startWordPosition="4467" endWordPosition="4470">Data LPcosine vs. LPJS p-value Significance SENSEVAL-3 (1%) 1.1e-003 � SENSEVAL-3 (10%) 8.9e-005 � SENSEVAL-3 (25%) 9.0e-009 � SENSEVAL-3 (50%) 3.2e-010 � SENSEVAL-3 (75%) 7.7e-013 � SENSEVAL-3 (100%) - - interest 3.3e-002 &gt; line 8.1e-002 � forms significantly better than LPcosine, but their performance is almost comparable on “interest” and “line” corpora. This observation motivates us to automatically select a distance measure that will boost the performance of LP on a given dataset. Cross-validation on labeled data is not feasible due to the setting of semi-supervised learning (l « u). In (Zhu and Ghahramani, 2002; Zhu et al., 2003), they suggested a label entropy criterion H(YU) for model selection, where Y is the label matrix learned by their semi-supervised algorithms. The intuition behind their method is that good parameters should result in confident labeling. Entropy on matrix W (H(W)) is a commonly used measure for unsupervised feature selection (Dash and Liu, 2000), which can be considered here. Another possible criterion for model selection is to measure the entropy of c x c inter-class distance matrix D calculated on labeled data (denoted as H(D)), where Di,j represents the average distance b</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Zhu, X. &amp; Ghahramani, Z.. 2002. Learning from Labeled and Unlabeled Data with Label Propagation. CMU CALD tech report CMU-CALD-02-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions.</title>
<date>2003</date>
<tech>ICML-2003.</tech>
<contexts>
<context position="4072" citStr="Zhu et al., 2003" startWordPosition="613" endWordPosition="616">within same class will have same labels, which is also the assumption underlying many supervised learning algorithms, such as kNN. Recently a promising family of semi-supervised learning algorithms are introduced, which can effectively combine unlabeled data with labeled data 395 Proceedings of the 43rd Annual Meeting of the ACL, pages 395–402, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics in learning process by exploiting cluster structure in data (Belkin and Niyogi, 2002; Blum et al., 2004; Chapelle et al., 1991; Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002; Zhu et al., 2003). Here we investigate a label propagation based semisupervised learning algorithm (LP algorithm) (Zhu and Ghahramani, 2002) for WSD, which works by representing labeled and unlabeled examples as vertices in a connected graph, then iteratively propagating label information from any vertex to nearby vertices through weighted edges, finally inferring the labels of unlabeled examples after this propagation process converges. Compared with bootstrapping, LP algorithm is based on a global consistency assumption. Intuitively, if there is at least one labeled example in each cluster that consists of s</context>
<context position="15294" citStr="Zhu et al., 2003" startWordPosition="2639" endWordPosition="2642">xt, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). For SVM, we did not perform feature selection on SENSEVAL-3 data since feature selection deteriorates its performance (Lee and Ng, 2002). When running LP on the three datasets, we removed the features with occurrence frequency (counted in both training set and test set) less than 3 times. We investigated two distance measures for LP: cosine similarity and Jensen-Shannon (JS) divergence (Lin, 1991). For the three datasets, we constructed connected graphs following (Zhu et al., 2003): two instances u, v will be connected by an edge if u is among v’s k nearest neighbors, or if v is among u’s k nearest neighbors as measured by cosine or JS distance measure. For “interest” and “line” corpora, k is 10 (following (Zhu et al., 2003)), while for SENSEVAL-3 data, k is 5 since the size of dataset for each word in SENSEVAL-3 is much less than that of “interest” and “line” datasets. 398 4.2 Experiment 1: LP vs. SVM In this experiment, we evaluated LP and SVM 3 on the data of English lexical sample task in SENSEVAL-3. We used l examples from training set as labeled data, and the rema</context>
<context position="25906" citStr="Zhu et al., 2003" startWordPosition="4471" endWordPosition="4474">alue Significance SENSEVAL-3 (1%) 1.1e-003 � SENSEVAL-3 (10%) 8.9e-005 � SENSEVAL-3 (25%) 9.0e-009 � SENSEVAL-3 (50%) 3.2e-010 � SENSEVAL-3 (75%) 7.7e-013 � SENSEVAL-3 (100%) - - interest 3.3e-002 &gt; line 8.1e-002 � forms significantly better than LPcosine, but their performance is almost comparable on “interest” and “line” corpora. This observation motivates us to automatically select a distance measure that will boost the performance of LP on a given dataset. Cross-validation on labeled data is not feasible due to the setting of semi-supervised learning (l « u). In (Zhu and Ghahramani, 2002; Zhu et al., 2003), they suggested a label entropy criterion H(YU) for model selection, where Y is the label matrix learned by their semi-supervised algorithms. The intuition behind their method is that good parameters should result in confident labeling. Entropy on matrix W (H(W)) is a commonly used measure for unsupervised feature selection (Dash and Liu, 2000), which can be considered here. Another possible criterion for model selection is to measure the entropy of c x c inter-class distance matrix D calculated on labeled data (denoted as H(D)), where Di,j represents the average distance between the ith clas</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Zhu, X., Ghahramani, Z., &amp; Lafferty, J.. 2003. Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions. ICML-2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>