<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000117">
<title confidence="0.9984475">
Extracting Opinion Targets in a Single- and Cross-Domain Setting
with Conditional Random Fields
</title>
<author confidence="0.727075">
Niklas Jakob
</author>
<affiliation confidence="0.62461">
Technische Universit¨at Darmstadt
</affiliation>
<address confidence="0.614575833333333">
Hochschulstraf3e 10
64289 Darmstadt, Germany
Iryna Gurevych
Technische Universit¨at Darmstadt
Hochschulstraf3e 10
64289 Darmstadt, Germany
</address>
<email confidence="0.985774">
http://www.ukp.tu-darmstadt.de/people
</email>
<sectionHeader confidence="0.995387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999872363636364">
In this paper, we focus on the opinion tar-
get extraction as part of the opinion min-
ing task. We model the problem as an in-
formation extraction task, which we address
based on Conditional Random Fields (CRF).
As a baseline we employ the supervised al-
gorithm by Zhuang et al. (2006), which rep-
resents the state-of-the-art on the employed
data. We evaluate the algorithms comprehen-
sively on datasets from four different domains
annotated with individual opinion target in-
stances on a sentence level. Furthermore, we
investigate the performance of our CRF-based
approach and the baseline in a single- and
cross-domain opinion target extraction setting.
Our CRF-based approach improves the perfor-
mance by 0.077, 0.126, 0.071 and 0.178 re-
garding F-Measure in the single-domain ex-
traction in the four domains. In the cross-
domain setting our approach improves the per-
formance by 0.409, 0.242, 0.294 and 0.343 re-
garding F-Measure over the baseline.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99986315625">
The automatic extraction and analysis of opinions
has been approached on several levels of granular-
ity throughout the last years. As opinion mining is
typically an enabling technology for another task,
this overlaying system defines requirements regard-
ing the level of granularity. Some tasks only require
an analysis of the opinions on a document or sen-
tence level, while others require an extraction and
analysis on a term or phrase level. Amongst the
tasks which require the finest level of granularity
are: a) Opinion question answering - i.e. with ques-
tions regarding an entity as in “What do the people
like / dislike about X?”. b) Recommender systems
- i.e. if the system shall only recommend entities
which have received good reviews regarding a cer-
tain aspect. c) Opinion summarization - i.e. if one
wants to create an overview of all positive / negative
opinions regarding aspect Y of entity X and cluster
them accordingly. All of these tasks have in com-
mon that in order to fulfill them, the opinion min-
ing system must be capable of identifying what the
opinions in the individual sentences are about, hence
extract the opinion targets.
Our goal in this work is to extract opinion tar-
gets from user-generated discourse, a discourse type
which is quite frequently encountered today, due to
the explosive growth of Web 2.0 community web-
sites. Typical sentences which we encounter in this
discourse type are shown in the following examples.
The opinion targets which we aim to extract are un-
derlined in the sentences, the corresponding opinion
expressions are shown in italics.
</bodyText>
<listItem confidence="0.943051">
(1) While none of the features are
</listItem>
<bodyText confidence="0.9040205">
earth-shattering, eCircles does provide a great
place to keep in touch.
</bodyText>
<listItem confidence="0.923632">
(2) Hyundai’s more-than-modest refresh has
largely addressed all the original car’s
weaknesses while maintaining its price
competitiveness.
</listItem>
<bodyText confidence="0.971082333333333">
The extraction of opinion targets can be consid-
ered as an instance of an information extraction
(IE) task (Cowie and Lehnert, 1996). Conditional
</bodyText>
<page confidence="0.951055">
1035
</page>
<note confidence="0.8235835">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1035–1045,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.997959891891892">
Random Fields (CRF) (Lafferty et al., 2001) have
been successfully applied to several IE tasks in
the past (Peng and McCallum, 2006). A recur-
ring problem, which arises when working with su-
pervised approaches, concerns the domain portabil-
ity. In the opinion mining context this question has
been prominently investigated with respect to opin-
ion polarity analysis (sentiment analysis) in previ-
ous research (Aue and Gamon, 2005; Blitzer et al.,
2007). Terms as “unpredictable” can express a pos-
itive opinion when uttered about the storyline of a
movie but a negative opinion when the handling of
a car is described. Hence the effects of training and
testing a machine learning algorithm for sentiment
analysis on data from different domains have been
analyzed in previous research. However to the best
of our knowledge, these effects have not been inves-
tigated regarding the extraction of opinion targets.
The contribution of this paper is a CRF-based ap-
proach for opinion targets extraction which tackles
the problem of domain portability. We first evalu-
ate our approach in three different domains against
a state-of-the art baseline system and then evaluate
the performance of both systems in a cross-domain
setting. We show that our CRF-based approach out-
performs the baseline in both settings, and how the
diffrerent combinations of features we introduce in-
fluence the results of our CRF-based approach. The
remainder of this paper is structured as follows: In
Section 2 we discuss the related work, and in Sec-
tion 3 we describe our CRF-based approach. Sec-
tion 4 comprises our experimental setup including
the description of the dataset we employ in our ex-
periments in Section 4.1 and the baseline system in
Section 4.2. The results of our experiments and their
discussion follow in Section 5. Finally we draw our
conclusions in Section 6.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999879285714286">
In the following we will discuss the related work re-
garding opinion target extraction and domain adap-
tation in opinion mining. The discussion of the re-
lated work on opinion target extraction is separated
in supervised and unsupervised approaches. We
conclude with a discussion of the related work on
domain adaptation in opinion mining.
</bodyText>
<subsectionHeader confidence="0.954753">
2.1 Unsupervised Opinion Target Extraction
</subsectionHeader>
<bodyText confidence="0.999739413043478">
The first work on opinion target extraction was done
on customer reviews of consumer electronics. Hu
and Liu (2004) introduce the task of feature based
summarization, which aims at creating an overview
of the product features commented on in the re-
views. Their approach relies on a statistical analysis
of the review terms based on association mining. A
dataset of customer reviews from five domains was
annotated by the authors regarding mentioned prod-
uct features with respective opinion polarities. The
association mining based algorithm yields a preci-
sion of 0.72 and a recall of 0.80 in the extraction
of a manually selected subset of product features.
The same dataset of product reviews was used in the
work of Yi et al. (2003). They present and evalu-
ate a complete system for opinion extraction which
is based on a statistical analysis based on the Like-
lihood Ratio Test for opinion target extraction. The
Likelihood Ratio Test yields a precision of 0.97 and
1.00 in the task of opinion target (product feature)
extraction, recall values are not reported.
Popescu and Etzioni (2005) present the OPINE
system for opinion mining on product reviews.
Their algorithm is based on an information extrac-
tion system, which uses the pointwise mutual infor-
mation based on the hitcounts of a web-search en-
gine as an input. They evaluate the opinion target
extraction separately on the dataset by Hu and Liu
(2004). OPINE’s precision is on average 22% higher
than the association mining based approach, while
having an average 3% lower recall.
Bloom et al. (2007) manually create taxonomies
of opinion targets for two datasets. With a hand-
crafted set of dependency tree paths their algorithm
identifies related opinion expressions and targets.
Due to the lack of a dataset annotated with opinion
expressions and targets, they just evaluate the accu-
racy of several aspects of their algorithm by man-
ually assessing an output sample. Their algorithm
yields an accuracy of 0.75 in the identification of
opinion targets.
Kim and Hovy (2006) aim at extracting opinion
holders and opinion targets in newswire with se-
mantic role labeling. They define a mapping of the
semantic roles identified with FrameNet to the re-
spective opinion elements. As a baseline, they im-
</bodyText>
<page confidence="0.990657">
1036
</page>
<bodyText confidence="0.998675888888889">
plement an approach based on a dependency parser,
which identifies the targets following the dependen-
cies of opinion expressions. They measure the over-
lap between two human annotators and their algo-
rithm as well as the baseline system. The algorithm
based on semantic role labeling yields an F-Measure
of 0.315 with annotator1 and 0.127 with annotator2,
while the baseline yields an F-Measure of 0.107 and
0.109 regarding opinion target extraction
</bodyText>
<subsectionHeader confidence="0.99904">
2.2 Supervised Opinion Target Extraction
</subsectionHeader>
<bodyText confidence="0.999989928571429">
Zhuang et al. (2006) present a supervised algorithm
for the extraction of opinion expression - opinion
target pairs. Their algorithm learns the opinion tar-
get candidates and a combination of dependency and
part-of-speech paths connecting such pairs from an
annotated dataset. They evaluate their system in a
cross validation setup on a dataset of user-generated
movie reviews and compare it to the results of the Hu
and Liu (2004) system as a baseline. Thereby, the
system by Zhuang et al. (2006) yields an F-Measure
of 0.529 and outperforms the baseline which yields
an F-Measure of 0.488 in the task of extracting opin-
ion target - opinion expression pairs.
Kessler and Nicolov (2009) solely focus on iden-
tifying which opinion expression is linked to which
opinion target in a sentence. They present a dataset
of car and camera reviews in which opinion expres-
sions and opinion targets are annotated. Starting
with this information, they train a machine learn-
ing classifier for identifying related opinion expres-
sions and targets. Their algorithm receives the opin-
ion expression and opinion target annotations as in-
put during runtime. The classifier is evaluated us-
ing the algorithm by Bloom et al. (2007) as a base-
line. The support vector machine based approach
by Kessler and Nicolov (2009) yields an F-Measure
of 0.698, outperforming the baseline which yields an
F-Measure of 0.445.
</bodyText>
<subsectionHeader confidence="0.999336">
2.3 Domain Adaptation in Opinion Mining
</subsectionHeader>
<bodyText confidence="0.999985034482758">
The task of creating a supervised algorithm, which
when trained on data from domain A, also performs
well on data from another domain B, is a domain
adaptation problem (Daum´e III and Marcu, 2006;
Jiang and Zhai, 2007). Aue and Gamon (2005) have
investigated this challenge very early in the task of
document level sentiment classification (positive /
negative). They observe that increasing the amount
of training data raises the classification accuracy, but
only if the training data is from one source domain.
Increasing the training data by mixing domains does
not yield any consistent improvements. Blitzer et
al. (2007) introduce an extension to a structural cor-
respondence learning algorithm, which was specifi-
cally designed to address the task of domain adap-
tation. Their enhancement aims at identifying pivot
features, which are stable across domains. In a series
of experiments in document level sentiment classi-
fication they show that their extension outperforms
the original structural correspondence learning ap-
proach. In their error analysis, the authors observe
the best results were reached when the training - test-
ing combinations were Books - DVDs or Electronics
- Kitchen appliances. They conclude that the topi-
cal relatedness of the domains is an important factor.
Furthermore they observe that training the algorithm
on a smaller amount of data from a similar domain is
more effective than increasing the amount of train-
ing data by mixing domains.
</bodyText>
<sectionHeader confidence="0.92036" genericHeader="method">
3 CRF-based Approach for Opinion
</sectionHeader>
<subsectionHeader confidence="0.632491">
Target Extraction
</subsectionHeader>
<bodyText confidence="0.971186625">
In the following we will describe the features we
employ as input for our CRF-based approach. As the
development data, we used 29 documents from the
movies dataset, 23 documents from the web-services
dataset and 15 documents from the cars &amp; cameras
datasets.
Token
This feature represents the string of the current token
as a feature. Even though this feature is rather ob-
vious, it can have considerable impact on the target
extraction performance. If the vocabulary of targets
is rather compact for a certain domain (correspond-
ing to a low target type / target ratio), the training
data is likely to contain the majority of the target
types, which should hence be a good indicator. We
will refer to this feature as tk in our result tables.
</bodyText>
<sectionHeader confidence="0.605409" genericHeader="method">
POS
</sectionHeader>
<bodyText confidence="0.984996333333333">
This feature represents the part-of-speech tag of the
current token as identified by the Stanford POS Tag-
ger1. It can provide some means of lexical disam-
</bodyText>
<footnote confidence="0.989288">
1http://nlp.stanford.edu/software/tagger.shtml
</footnote>
<page confidence="0.996901">
1037
</page>
<bodyText confidence="0.999715571428571">
biguation, e.g. indicate that the token “sounds” is
a noun and not a verb in a certain context. At the
same time, the CRF algorithm is provided with ad-
ditional information to extract opinion targets which
are multiword expressions, i.e. noun combinations.
We will refer to this feature as pos in our result ta-
bles.
</bodyText>
<subsectionHeader confidence="0.908701">
Short Dependency Path
</subsectionHeader>
<bodyText confidence="0.999909923076923">
Previous research has successfully employed paths
in the dependency parse tree to link opinion expres-
sions and the corresponding targets (Zhuang et al.,
2006; Kessler and Nicolov, 2009). Both works iden-
tify direct dependency relations such as “amod” and
“nsubj” as the most frequent and at the same time
highly accurate connections between a target and an
opinion expression. We hence label all tokens which
have a direct dependency relation to an opinion ex-
pression in a sentence. The Stanford Parser2 is em-
ployed for the constituent and dependency parsing.
We will refer to this feature as dLn in our result ta-
bles.
</bodyText>
<subsectionHeader confidence="0.907059">
Word Distance
</subsectionHeader>
<bodyText confidence="0.9999585">
From the work of Zhuang et al. (2006) we can infer
that opinion expressions and their target(s) are not
always connected via short paths in the dependency
parse tree. Since we cannot capture such paths with
the abovementioned feature we introduce another
feature which acts as heuristic for identifying the
target to a given opinion expression. Hu and Liu
(2004) and Yi et al. (2003) have shown that (base)
noun phrases are good candidates for opinion targets
in the datasets of product reviews. We therefore la-
bel the token(s) in the closest noun phrase regarding
word distance to each opinion expression in a sen-
tence. We will refer to this feature as wrdDist in our
result tables.
</bodyText>
<subsectionHeader confidence="0.904212">
Opinion Sentence
</subsectionHeader>
<bodyText confidence="0.999991714285714">
With this feature, we simply label all tokens occur-
ring in a sentence containing an opinion expression.
This feature shall enable the CRF algorithm to
distinguish between the occurence of a certain
token in a sentence which contains an opinion vs. a
sentence without an opinion. We will refer to this
feature as sSn in our result tables.
</bodyText>
<footnote confidence="0.501753">
2http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<bodyText confidence="0.99995875">
Our goal is to extract individual instances of opinion
targets from sentences which contain an opinion
expression. This can be modeled as a sequence
segmentation and labeling task. The CRF algorithm
receives a sequence of tokens t1...try,, for which
it has to predict a sequence of labels l1...lry,,. We
represent the possible labels following the IOB
scheme: B-Target, identifying the beginning of an
opinion target, I-Target identifying the continuation
of a target, and O for other (non-target) tokens. We
model the sentences as a linear chain CRF, which
is based on an undirected graph. In the graph, each
node corresponds to a token in the sentence and
edges connect the adjacent tokens as they appear in
the sentence. In our experiments, we use the CRF
implementation from the Mallet toolkit3.
</bodyText>
<sectionHeader confidence="0.998148" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.875047">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999994153846154">
In our experiments, we employ datasets from three
different sources, which span four domains in total
(see Table 1). All of them consist of reviews col-
lected from Web 2.0 sites. The first dataset con-
sists of reviews for 20 different movies collected
from the Internet Movie Database. It was presented
in Zhuang et al. (2006) and annotated regarding
opinion target - opinion expression pairs. The sec-
ond dataset consists of 234 reviews for two different
web-services collected from epinions.com, as de-
scribed in Toprak et al. (2010). The third dataset is
an extended version of the data presented in Kessler
and Nicolov (2009). The authors have provided us
with additional documents, which have been anno-
tated in the meantime. The version of the dataset
used in our experiments consists of 179 blog post-
ings regarding different digital cameras and 336 re-
views of different cars. In the description of their
annotation guidelines, Kessler and Nicolov (2009)
refer to opinion targets as mentions. Mentions are
all aspects of the review topic, which can be targets
of expressed opinions. However, not only mentions
which occur as opinion targets were originally anno-
tated, but also mentions which occur in non-opinion
sentences. In our experiments, we only use the men-
tions which occur as targets of opinion expressions.
</bodyText>
<footnote confidence="0.951337">
3http://mallet.cs.umass.edu/
</footnote>
<page confidence="0.993663">
1038
</page>
<bodyText confidence="0.999973793103448">
All three datasets contain annotations regarding
the antecedents of anaphoric opinion targets. In our
experimental setup, we do not require the algorithms
to also correctly resolve the antecedent of an opin-
ion target representy by a pronoun, as we are solely
interested in evaluating the opinion target extraction
not any anaphora resolution.
As shown in rows 4 and 5 of Table 1, the docu-
ments from the cars and the cameras datasets exhibit
a much higher density of opinions per document.
53.5% of the sentences from the cars dataset contain
an opinion and in the cameras dataset even 56.1%
of the sentences contain an opinion, while in the
movies and the web-services reviews just 22.1% and
22.4% of the sentences contain an opinion. Further-
more in the cars and the cameras datasets the lexical
variability regarding the opinion targets is substan-
tially larger than in the other two datasets: We calcu-
late target types by counting the number of distinct
opinion targets in a dataset. We divide this by the
sum of all opinion target instances in the dataset. For
the cars dataset this ratio is 0.440 and for the cam-
eras dataset it is 0.433, while for the web-services
dataset it is 0.306 and for the movies dataset only
0.122. In terms of reviews this means, that in the
movie reviews the same movie aspects are repeat-
edly commented on, while in the cars and the cam-
eras datasets many different aspects of these entities
are discussed, which in turn each occur infrequently.
</bodyText>
<subsectionHeader confidence="0.9971">
4.2 Baseline System
</subsectionHeader>
<bodyText confidence="0.999689833333333">
In the task of opinion target extraction the super-
vised algorithm by Zhuang et al. (2006) represents
the state-of-the-art on the movies dataset we also
employ in our experiments. We therefore use it as
a baseline. The algorithm learns two aspects from
the labeled training data:
</bodyText>
<listItem confidence="0.9918725">
1. A set of opinion target candidates
2. A set of paths in a dependency tree which iden-
tify valid opinion target - opinion expression
pairs
</listItem>
<bodyText confidence="0.999316117647059">
In our experiments, we learn the full set of opin-
ion targets from the labeled training data in the first
step. This is slightly different from the approach
in (Zhuang et al., 2006), but we expect that this mod-
ification should be beneficial for the overall perfor-
mance in terms of recall, as we do not remove any
learned opinion targets from the candidate list. In
the second step, the annotated sentences are parsed
and a graph containing the words of a sentence is
created, which are connected by the dependency re-
lations between them. For each opinion target -
opinion expression pair from the gold standard, the
shortest path connecting them is extracted from the
dependency graph. A path consists of the part-of-
speech tags of the nodes and the dependency types
of the edges. Example 3 shows a typical dependency
path.
</bodyText>
<tableCaption confidence="0.991603">
Table 1: Dataset Statistics
</tableCaption>
<table confidence="0.999755933333333">
movies web- cars cameras
services
Documents 1829 234 336 179
Sentences 24555 6091 10969 5261
Tokens/ 20.3 17.5 20.3 20.4
sentence
Sentences with 21.4% 22.4% 51.1% 54.0%
target(s)
Sentences with 21.4% 22.4% 53.5% 56.1%
opinion(s)
Targets 7045 1875 8451 4369
Target types 865 574 3722 1893
Tokens / target 1.21 1.35 1.29 1.42
Avg. targets / 1.33 1.37 1.51 1.53
opinion sent.
</table>
<listItem confidence="0.788008">
(3) NN - nsubj - NP - amod - JJ
</listItem>
<bodyText confidence="0.999867071428571">
During runtime, the algorithm identifies opinion tar-
gets from the candidate list in the training data. The
opinion expressions are directly taken from the gold
standard, as we focus on the opinion target extrac-
tion aspect in this work. The sentences are then
parsed and if a valid path between a target and
an opinion expression is found in the list of possi-
ble paths, then the pair is extracted. Since the de-
pendency paths only identify pairs of single word
target and opinion expression candidates, we em-
ploy a merging step. Extracted target candidates are
merged into a multiword target if they are adjacent
in a sentence. Thereby, the baseline system is also
capable of extracting multiword opinion targets.
</bodyText>
<page confidence="0.983619">
1039
</page>
<subsectionHeader confidence="0.988638">
4.3 Metrics
</subsectionHeader>
<bodyText confidence="0.999973529411765">
We employ the following requirements in our eval-
uation of the opinion target extraction: An opin-
ion target must be extracted with exactly the span
boundaries as annotated in the gold standard. This
is especially important regarding multiword tar-
gets. Extracted targets which partially overlap with
the annotated gold standard are counted as errors.
Hence a target extracted by the algorithm which
does not exactly match the boundaries of a target
in the gold standard is counted as a false positive
(FP), e.g. if “battery life” is annotated as the tar-
get in the gold standard, only “battery” or “life”
extracted as targets will be counted as FPs. Exact
matches between the targets extracted by the algo-
rithm and the gold standard are true positives (TP).
We refer to the number of annotated targets in the
gold standard as TGS. Precision is calculated as
</bodyText>
<equation confidence="0.658972">
Precision = TP
TP+FP , and recall is calculated as
Recall = T P
</equation>
<bodyText confidence="0.975665">
T�� . F-Measure is the harmonic mean of
precision and recall.
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9998424">
We investigate the performance of the baseline and
the CRF-based approach for opinion target extrac-
tion in a single- and cross-domain setting. The
single-domain approach assumes that there is a set
of training data available for the same domain as
the domain the algorithm is being tested on. In this
setup, we will both run the baseline and our CRF
based system in a 10-fold cross-validation and report
results macro averaged over all runs. In the cross-
domain approach, we will investigate how the algo-
rithm performs if given training data from domain A
while being tested on another domain B. In this set-
ting, we will train the algorithm on the entire dataset
A, and test it on the entire dataset B, we hence report
one micro averaged result set. In Subsection 5.1 we
present the results of both the baseline system and
our CRF-based approach in the single-domain set-
ting, in Subsection 5.2 we present the results of the
two systems in the cross-domain opinion target ex-
traction.
</bodyText>
<tableCaption confidence="0.982892">
Table 2: Single-Domain Extraction with Zhuang Baseline
</tableCaption>
<table confidence="0.9996882">
Dataset Precision Recall F-Measure
movies 0.663 0.592 0.625
web-services 0.624 0.394 0.483
cars 0.259 0.426 0.322
cameras 0.423 0.431 0.426
</table>
<subsectionHeader confidence="0.9780995">
5.1 Single-Domain Results
5.1.1 Zhuang Baseline
</subsectionHeader>
<bodyText confidence="0.999992647058824">
As shown in Table 2, the state-of-the-art algo-
rithm of Zhuang et al. (2006) performs best on the
movie review dataset and worst on the cars dataset.
The results on the movie dataset are higher than
originally reported in (Zhuang et al., 2006) (Preci-
sion 0.483, Recall 0.585, F-Measure 0.529). We as-
sume that this is due to two reasons: 1. In our task,
the algorithm uses the opinion expression annotation
from the gold standard. 2. We do not remove any
learned opinion target candidates from the training
data (See Section 4.2).
During training we observed that for each dataset
the lists of possible dependency paths (see Exam-
ple 3) contained several hundred entries, many of
them only occurring once. We assume that the re-
call of the algorithm is limited by a large variety
of possible dependency paths between opinion tar-
gets and opinion expressions, since the algorithm
cannot link targets and opinion expressions in the
testing data if there is no valid candidate depen-
dency path. Furthermore, we observe that for the
cars dataset the size of the dependency path candi-
date list (6642 entries) was approximately five times
larger than the dependency graph candidate list for
the web-services dataset (1237 entries), which has a
comparable size regarding documents. At the same
time, the list of target candidates of the cars dataset
was approximately eight times larger than the tar-
get candidate list for the web-services dataset. We
assume that a large number of both the target can-
didates as well as the dependency path candidates
introduces many false positives during the target ex-
traction, hence lowering the precision of the algo-
rithm on the cars dataset considerably.
</bodyText>
<page confidence="0.993741">
1040
</page>
<tableCaption confidence="0.998687">
Table 3: Single-Domain Extraction with our CRF-based Approach
</tableCaption>
<table confidence="0.999383545454545">
movies web-services cars cameras
Features Prec Rec F-Me Prec Rec F-Me Prec Rec F-Me Prec Rec F-Me
tk, pos 0.639 0.133 0.220 0.500 0.051 0.093 0.438 0.110 0.175 0.300 0.085 0.127
tk, pos, wDs 0.542 0.181 0.271 0.451 0.272 0.339 0.570 0.354 0.436 0.549 0.375 0.446
tk, pos, dLn 0.777 0.481 0.595 0.634 0.380 0.475 0.603 0.372 0.460 0.569 0.376 0.453
tk, pos, sSn 0.673 0.637 0.653 0.604 0.397 0.476 0.453 0.180 0.257 0.398 0.172 0.238
tk, pos, dLn, wDs 0.792 0.481 0.598 0.620 0.354 0.450 0.603 0.389 0.473 0.596 0.425 0.496
tk, pos, sSn, wDs 0.662 0.656 0.659 0.664 0.461 0.544 0.564 0.370 0.446 0.544 0.381 0.447
tk, pos, sSn, dLn 0.791 0.477 0.594 0.654 0.501 0.568 0.598 0.384 0.467 0.586 0.391 0.468
tk, pos, sSn, dLn, wDs 0.749 0.661 0.702 0.722 0.526 0.609 0.622 0.414 0.497 0.614 0.423 0.500
pos, sSn, dLn, wDs 0.672 0.441 0.532 0.612 0.322 0.422 0.612 0.369 0.460 0.674 0.398 0.500
</table>
<subsubsectionHeader confidence="0.501094">
5.1.2 Our CRF-based Approach
</subsubsectionHeader>
<bodyText confidence="0.999915913043478">
Table 3 shows the results of the opinion target ex-
traction using the CRF algorithm. Row 8 contains
the results of the feature configuration, which yields
the best performance regarding F-Measure across all
datasets. We observe that our aproach outperforms
the Zhuang baseline on all datasets. The gain in F-
Measure is between 0.077 in the movies domain and
0.175 in the cars domain. Although the CRF-based
approach clearly outperforms the baseline system
on all four datasets, we also observe the same gen-
eral trend regarding the individual results: The CRF
yields the best results on the movies dataset and the
worst results on the cars &amp; cameras dataset.
As shown in the first row, the results when using
just the token string and part-of-speech tags as fea-
tures are very low, especially regarding recall. We
observe that the higher the lexical variability of the
opinion targets is in a dataset, the lower the results
are. If we add the feature based on word distance
(row 2), the recall is improved on all datasets, while
the precision is slightly lowered on the movies and
web-services datasets. The dependency path based
feature performs better compared to the word dis-
tance heuristic as shown in row 3. The precision is
considerably increased on all datasets, on the movies
and cars &amp; cameras datasets even reaching the over-
all highest value. At the same time, we observe
an increase of recall on all datasets. The obser-
vation made in previous research that short paths
in the dependency graph are a high precision indi-
cator of related opinion expressions - opinion tar-
gets (Kessler and Nicolov, 2009) is confirmed on all
datasets. Adding the information regarding opinion
sentences to the basic features of the token string and
the part-of-speech tag (row 4) yields the biggest im-
provements regarding F-Measure on the movies and
web-services dataset (+0.433 / +0.383). On the cars
&amp; cameras dataset the recall is relatively low again.
We assume that this is again due to the high lexical
variability, so that the CRF algorithm will encounter
many actual opinion targets in the testing data which
have not occurred in the training data and will hence
not be extracted.
As shown in row 5, if we combine the dependency
graph based feature with the word distance heuris-
tic, the results regarding F-Measure are consistently
higher than the results of these features in isolation
(rows 2 - 4) on all datasets. We conclude that these
two features are complementary, as they apparently
indicate different kinds of opinion targets which are
then correctly extracted by the CRF. If we combine
each of the opinion expression related features with
the label which identifies opinion sentences in gen-
eral (rows 6 &amp; 7), we observe that this feature is
also complementary to the others. On all datasets the
results regarding F-Measure are consistently higher
compared to the features in isolation (rows 2 - 4).
Row 8 shows the results of all features in combina-
tion. Again, we observe the complementarity of the
features, as the results of this feature combination
are the best regarding F-Measure across all datasets.
In row 9 of the results, we exclude the token
string as a feature. In comparison to the full fea-
ture combination of row 8 we observe a significant
decrease of F-Measure on the movies and the web-
services dataset. On the cars dataset we only observe
a slight decrease of recall. Interestingly on the cam-
eras dataset we even observe a slight increase of pre-
cision which compensates a slight decrease of recall,
</bodyText>
<page confidence="0.984332">
1041
</page>
<bodyText confidence="0.986641886363637">
in turn resulting in stable F-Measure of 0.500 as in
the full feature set of row 8.
We have run some additional experiments in
which we did not rely on the annotated opinion ex-
pressions, but employed a general pupose subjectiv-
ity lexicon4. Already in the single-domain extrac-
tion, we observed that the results declined substan-
tially (e.g. web-services F-Measure: 0.243, movies
F-Measure: 0.309, cars F-Measure: 0.192 and cam-
eras F-Measure: 0.198).
We performed a quantitative error analysis on the
results of the CRF-based approach in the single-
domain setting. In doing so, we focused on misclas-
sifications of B-Target and I-Target instances, as the
recall is consistently lower than the precision across
all datasets. We observe that most of the recall errors
result from one-word opinion targets or the begin-
ning of opinion targets (B-Targets) being missclassi-
fied as non-targets (movies 83%, web-services 73%,
cars 68%, cameras 64%). For the majority of these
missclassifications neither the dLn nor the wDs fea-
tures were present (movies 82%, web-services 56%,
cars 64%, cameras 61%). We assume that our fea-
tures cannot capture the structure of more complex
sentences very well. Our results indicate that the
dLn and wDs features are complementary, but appar-
ently there are quite a few cases in which the opin-
ion target is neither directly related to the opinion
expression in the dependency graph nor close to it
in the sentence. One of these sentences, in this case
from a camera review, in shown in Example 4.
(4) A lens cap and a strap may not sound very
important, but it makes a huge difference in the
speed and usability of the camera.
In this sentence, the dLn and wDs features both la-
beled “speed” which was incorrectly extracted as the
target of the opinion. None of the actual targets “lens
cap”, “strap” and “camera” have a short dependency
path to the opinion expression and “speed” is sim-
ply the closest noun to it. Note that although both
“speed” and “usability” are attributes of a camera,
the opinion in this sentence is about the “lens cap”
and “strap”, hence only these attributes are anno-
tated as targets.
</bodyText>
<footnote confidence="0.936082">
4http://www.cs.pitt.edu/mpqa/
</footnote>
<subsectionHeader confidence="0.9944205">
5.2 Cross-Domain Results
5.2.1 Zhuang Baseline
</subsectionHeader>
<bodyText confidence="0.999964105263158">
Table 4 shows the results of the opinion target ex-
traction with the state-of-the-art system in the cross-
domain setting. We observe that the results on all
domain combinations are very low. A quantitative
error analysis has revealed that there is hardly any
overlap in the opinion target candidates between do-
mains, as reflected by the low recall in all config-
urations. The vocabularies of the opinion targets
are too different, hence the performance of the algo-
rithm by Zhuang et al. (2006) is so low. The overlap
regarding the dependency paths between domains
was however higher. Especially identical short paths
could be found across domains which at the same
time typically occured quite often. For future work
it might be interesting to investigate how the algo-
rithm by Zhuang et al. (2006) performs in the cross-
domain setting if the target candidate learning is per-
formed differently, e.g. with a statistical approach as
outlined in Section 2.1.
</bodyText>
<subsectionHeader confidence="0.642956">
5.2.2 CRF-based Approach
</subsectionHeader>
<bodyText confidence="0.999954375">
The results of the cross-domain target extraction
with the CRF-based algorithm are shown in Table 5.
Due to the increase of system configurations intro-
duced by the training - testing data combinations,
we had to limit results of the feature combinations
reported in the Table. The feature combination pos,
sSn, wDs, dLn yielded the best results regarding F-
Measure. Hence, we report its result as the basic fea-
ture set. When comparing the results of the best per-
forming feature / training data combination of our
CRF-based approach with the baseline, we observe
that our approach outperforms the baseline on all
four domains. The gain in F-Measure is 0.409 in the
movies domain, 0.242 in the web-services domain,
0.294 in the cars domain and 0.343 in the cameras
domain.
</bodyText>
<subsectionHeader confidence="0.991701">
Effects of Features
</subsectionHeader>
<bodyText confidence="0.9997492">
Interestingly with the best performing feature com-
bination from the single-domain extraction, the re-
sults regarding recall in the cross-domain extraction
are very low5. This is due to the fact that the CRF at-
tributed a relatively large weight to the token string
</bodyText>
<footnote confidence="0.783086">
5Not shown in any result table due to limited space.
</footnote>
<page confidence="0.9963">
1042
</page>
<tableCaption confidence="0.998854">
Table 4: Cross-Domain Extraction with Zhuang Baseline
</tableCaption>
<table confidence="0.999700857142857">
Training Testing Precision Recall F-Measure
web-services movies 0.194 0.032 0.055
cars movies 0.032 0.034 0.033
cameras movies 0.155 0.084 0.109
cars + cameras movies 0.071 0.104 0.084
web-services + cars + cameras movies 0.070 0.103 0.083
movies web-services 0.311 0.073 0.118
cars web-services 0.086 0.091 0.089
cameras web-services 0.164 0.081 0.108
cars + cameras web-services 0.086 0.104 0.094
movies + cars + cameras web-services 0.074 0.100 0.080
movies cars 0.182 0.014 0.026
web-services cars 0.218 0.028 0.049
cameras cars 0.250 0.121 0.163
cameras + web-services cars 0.247 0.131 0.171
movies + web-services cars 0.246 0.045 0.076
movies cameras 0.108 0.012 0.022
web-services cameras 0.268 0.048 0.082
cars cameras 0.125 0.160 0.140
cars + web-services cameras 0.119 0.157 0.136
movies + web-services cameras 0.245 0.063 0.100
</table>
<bodyText confidence="0.999908428571429">
feature. As we also observed in the analysis of the
baseline results, the overlap of the opinion target vo-
cabularies between domains is low, which resulted
in a very small number of targets extracted by the
CRF. As shown in Table 5 the results are promising
regarding F-Measure if we just leave the token fea-
ture out of the configuration.
</bodyText>
<subsectionHeader confidence="0.993913">
Effects of Training Data
</subsectionHeader>
<bodyText confidence="0.999992195652174">
When analyzing the results of the different training
- testing domain configurations we observe the fol-
lowing: In isolation training data from the cameras
domain consistently yields the best results regarding
F-Measure when the algorithm is run on the datasets
from the other three domains. This is particularly
interesting since the cameras dataset is the smallest
of the four (see Table 1). We investigated whether
the CRF algorithm was overfitting to the training
datasets by reducing their size to the size of the cam-
eras dataset. However, the reduction of the train-
ing data sizes never improved the extraction results
regarding F-Measure for the movies, web-serviecs
and cars datasets. The good results when training
on the cameras dataset are in line with our obser-
vations from Section 5.1.2. We noticed that on the
cameras dataset the results regarding F-Measure re-
mained stable if the token feature is not used in the
training.
In isolation, training only on the cars data yields
the second highest results on the movies and web-
services datasets and the highest results regarding
F-Measure on the cameras data. However, the re-
sults of the cars + cameras training data combination
indicate that the cameras data does not contribute
any additional information during the learning, since
the results on both the movies and the web-services
datasets are lower than when training only on the
cameras data.
Our results also confirm the insights gained
by Blitzer et al. (2007), who observed that in cross-
domain polarity analysis adding more training data
is not always beneficial. Apparently even the small-
est training dataset (cameras) contain enough feature
instances to learn a model which performs well on
the testing data.
We observe that the results of the cross-domain
extraction regarding F-Measure come relatively
close to the results of the single-domain setting, es-
pecially if the token string feature is removed there
(see Table 3 row 9). On the cars and the cameras
dataset the cross-domain results are even closer to
the single-domain results. The features we employ
seem to scale well across domains and compensate
the difference between training and testing data and
the lack of information regarding the target vocabu-
</bodyText>
<page confidence="0.986799">
1043
</page>
<tableCaption confidence="0.864318">
Table 5: Cross-Domain Extraction with our CRF-based Approach
</tableCaption>
<figure confidence="0.997807137499999">
Testing
training
Pre
-
0.565
0.538
0.529
0.554
0.530
0.562
0.538
-
-
-
web-services
Rec
-
0.219
0.248
0.256
0.249
0.273
0.250
0.254
-
-
-
F-Me
-
0.316
0.340
0.345
0.344
0.360
0.346
0.345
-
-
-
Pre
0.560
-
0.642
0.642
-
-
-
0.641
0.651
0.642
0.639
web-services
movies
cars
cameras
movies + cars
movies + cameras
movies + cars + cameras
cars + cameras
web-services + cars
web-services + cameras
web-services + cars + cameras
web-services
movies
cars
cameras
cameras + movies
cameras + web-services
movies + web-services
movies + cars
web-services + cars
web-services + movies + cars
movies + web-services + cameras
movies F-Me
Rec
0.339 0.422
- -
0.382 0.479
0.408 0.499
- -
- -
- -
0.395 0.489
0.396 0.492
0.435 0.518
0.405 0.496
cameras
Pre
cars
Rec
F-Me
Pre
Rec
F-Me
0.391
0.512
-
0.589
0.567
0.572
0.489
-
-
-
0.549
0.277
0.307
-
0.384
0.394
0.381
0.327
-
-
-
0.381
0.324
0.384
-
0.465
0.465
0.457
0.392
-
-
-
0.450
0.505
0.550
0.665
-
-
-
0.553
0.634
0.678
0.635
-
0.330
0.303
0.369
-
-
-
0.376
0.376
0.378
-
0.399
0.391
0.475
-
-
-
0.472
0.483
0.474
-
0.339 0.421
lary.
</figure>
<sectionHeader confidence="0.988562" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999539">
In this paper, we have shown how a CRF-based
approach for opinion target extraction performs in
a single- and cross-domain setting. We have pre-
sented a comparative evaluation of our approach
on datasets from four different domains. In the
single-domain setting, our CRF-based approach out-
performs a supervised baseline on all four datasets.
Our error analysis indicates that additional features,
which can capture opinions in more complex sen-
tences, are required to improve the performance of
the opinion target extraction. Our CRF-based ap-
proach also yields promising results in the cross-
domain setting. The features we employ scale well
across domains, given that the opinion target vocab-
ularies are substantially different. For future work,
we might investigate how machine learning algo-
rithms, which are specifically designed for the prob-
lem of domain adaptation (Blitzer et al., 2007; Jiang
and Zhai, 2007), perform in comparison to our ap-
proach. Since three of the features we employed in
</bodyText>
<page confidence="0.965545">
1044
</page>
<bodyText confidence="0.999953357142857">
our CRF-based approach are based on the respec-
tive opinion expressions, it is to investigate how to
mitigate the possible negative effects introduced by
errors in the opinion expression identification if they
are not annotated in the gold standard. We observe
similar challenges as Choi et al. (2005) regarding the
analysis of complex sentences. Although our data is
user-generated from Web 2.0 communities, a man-
ual inspection has shown that the documents were
of relatively high textual quality. It is to investigate
to which extent the approaches taken in the analysis
of newswire, such as identifying targets with coref-
erence resolution, can also be applied to our task on
user-generated discourse.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.951032">
The project was funded by means of the German Fed-
eral Ministry of Economy and Technology under the
promotional reference “01MQ07012”. The authors take
the responsibility for the contents. This work has been
supported by the Volkswagen Foundation as part of
the Lichtenberg-Professorship Program under grant No.
I/82806.
</bodyText>
<sectionHeader confidence="0.993341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998849928571429">
Anthony Aue and Michael Gamon. 2005. Customizing
sentiment classifiers to new domains: A case study.
In Proceedings of the 5th International Conference
on Recent Advances in Natural Language Processing,
Borovets, Bulgaria, September.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 440–447,
Prague, Czech Republic, June.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In Proceed-
ings of Human Language Technologies 2007: The
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 308–
315, Rochester, New York, USA, April.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction patterns.
In Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in Nat-
ural Language Processing, pages 355–362, Vancou-
ver, Canada, October.
James R. Cowie and Wendy G. Lehnert. 1996. In-
formation extraction. Communications of the ACM,
39(1):80–91.
Hal Daum´e III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research (JAIR), 26:101–126.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168–177,
Seattle, Washington, USA, August.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 264–271, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Jason Kessler and Nicolas Nicolov. 2009. Targeting sen-
timent expressions through supervised ranking of lin-
guistic configurations. In Proceedings of the Third In-
ternational AAAI Conference on Weblogs and Social
Media, pages 90–97, San Jose, California, USA, May.
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the ACL Workshop
on Sentiment and Subjectivity in Text, pages 1–8, Syd-
ney, Australia, July.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the 18th International Conference
on Machine Learning, pages 282–289, Williamstown,
MA, USA, June.
Fuchun Peng and Andrew McCallum. 2006. Information
extraction from research papers using conditional ran-
dom fields. Information Processing and Management,
42(4):963–979, July.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 339–346, Vancouver,
Canada, October.
Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.
2010. Sentence and expression level annotation of
opinions in user-generated discourse. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 575–584, Uppsala,
Sweden, July.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Extract-
ing sentiments about a given topic using natural lan-
guage processing techniques. In Proceedings of the
3rd IEEE International Conference on Data Mining,
pages 427–434, Melbourne, Florida, USA, December.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie
review mining and summarization. In Proceedings of
the ACM 15th Conference on Information and Knowl-
edge Management, pages 43–50, Arlington, Virginia,
USA, November.
</reference>
<page confidence="0.990627">
1045
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.050029">
<title confidence="0.895589666666667">Extracting Opinion Targets in a Singleand Cross-Domain with Conditional Random Fields Niklas</title>
<author confidence="0.183268">Technische Universit¨at</author>
<affiliation confidence="0.43621">Hochschulstraf3e</affiliation>
<address confidence="0.998537">64289 Darmstadt, Germany</address>
<email confidence="0.846754">Iryna</email>
<affiliation confidence="0.717302">Technische Universit¨at Hochschulstraf3e</affiliation>
<address confidence="0.995911">64289 Darmstadt, Germany</address>
<web confidence="0.857171">http://www.ukp.tu-darmstadt.de/people</web>
<abstract confidence="0.998882826086957">In this paper, we focus on the opinion target extraction as part of the opinion mining task. We model the problem as an information extraction task, which we address based on Conditional Random Fields (CRF). As a baseline we employ the supervised algorithm by Zhuang et al. (2006), which represents the state-of-the-art on the employed data. We evaluate the algorithms comprehensively on datasets from four different domains annotated with individual opinion target instances on a sentence level. Furthermore, we investigate the performance of our CRF-based approach and the baseline in a singleand cross-domain opinion target extraction setting. Our CRF-based approach improves the performance by 0.077, 0.126, 0.071 and 0.178 regarding F-Measure in the single-domain extraction in the four domains. In the crossdomain setting our approach improves the performance by 0.409, 0.242, 0.294 and 0.343 regarding F-Measure over the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anthony Aue</author>
<author>Michael Gamon</author>
</authors>
<title>Customizing sentiment classifiers to new domains: A case study.</title>
<date>2005</date>
<booktitle>In Proceedings of the 5th International Conference on Recent Advances in Natural Language Processing,</booktitle>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="3889" citStr="Aue and Gamon, 2005" startWordPosition="601" endWordPosition="604">ceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1035–1045, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Random Fields (CRF) (Lafferty et al., 2001) have been successfully applied to several IE tasks in the past (Peng and McCallum, 2006). A recurring problem, which arises when working with supervised approaches, concerns the domain portability. In the opinion mining context this question has been prominently investigated with respect to opinion polarity analysis (sentiment analysis) in previous research (Aue and Gamon, 2005; Blitzer et al., 2007). Terms as “unpredictable” can express a positive opinion when uttered about the storyline of a movie but a negative opinion when the handling of a car is described. Hence the effects of training and testing a machine learning algorithm for sentiment analysis on data from different domains have been analyzed in previous research. However to the best of our knowledge, these effects have not been investigated regarding the extraction of opinion targets. The contribution of this paper is a CRF-based approach for opinion targets extraction which tackles the problem of domain</context>
<context position="10115" citStr="Aue and Gamon (2005)" startWordPosition="1615" endWordPosition="1618"> receives the opinion expression and opinion target annotations as input during runtime. The classifier is evaluated using the algorithm by Bloom et al. (2007) as a baseline. The support vector machine based approach by Kessler and Nicolov (2009) yields an F-Measure of 0.698, outperforming the baseline which yields an F-Measure of 0.445. 2.3 Domain Adaptation in Opinion Mining The task of creating a supervised algorithm, which when trained on data from domain A, also performs well on data from another domain B, is a domain adaptation problem (Daum´e III and Marcu, 2006; Jiang and Zhai, 2007). Aue and Gamon (2005) have investigated this challenge very early in the task of document level sentiment classification (positive / negative). They observe that increasing the amount of training data raises the classification accuracy, but only if the training data is from one source domain. Increasing the training data by mixing domains does not yield any consistent improvements. Blitzer et al. (2007) introduce an extension to a structural correspondence learning algorithm, which was specifically designed to address the task of domain adaptation. Their enhancement aims at identifying pivot features, which are st</context>
</contexts>
<marker>Aue, Gamon, 2005</marker>
<rawString>Anthony Aue and Michael Gamon. 2005. Customizing sentiment classifiers to new domains: A case study. In Proceedings of the 5th International Conference on Recent Advances in Natural Language Processing, Borovets, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3912" citStr="Blitzer et al., 2007" startWordPosition="605" endWordPosition="608">Conference on Empirical Methods in Natural Language Processing, pages 1035–1045, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Random Fields (CRF) (Lafferty et al., 2001) have been successfully applied to several IE tasks in the past (Peng and McCallum, 2006). A recurring problem, which arises when working with supervised approaches, concerns the domain portability. In the opinion mining context this question has been prominently investigated with respect to opinion polarity analysis (sentiment analysis) in previous research (Aue and Gamon, 2005; Blitzer et al., 2007). Terms as “unpredictable” can express a positive opinion when uttered about the storyline of a movie but a negative opinion when the handling of a car is described. Hence the effects of training and testing a machine learning algorithm for sentiment analysis on data from different domains have been analyzed in previous research. However to the best of our knowledge, these effects have not been investigated regarding the extraction of opinion targets. The contribution of this paper is a CRF-based approach for opinion targets extraction which tackles the problem of domain portability. We first </context>
<context position="10500" citStr="Blitzer et al. (2007)" startWordPosition="1673" endWordPosition="1676">task of creating a supervised algorithm, which when trained on data from domain A, also performs well on data from another domain B, is a domain adaptation problem (Daum´e III and Marcu, 2006; Jiang and Zhai, 2007). Aue and Gamon (2005) have investigated this challenge very early in the task of document level sentiment classification (positive / negative). They observe that increasing the amount of training data raises the classification accuracy, but only if the training data is from one source domain. Increasing the training data by mixing domains does not yield any consistent improvements. Blitzer et al. (2007) introduce an extension to a structural correspondence learning algorithm, which was specifically designed to address the task of domain adaptation. Their enhancement aims at identifying pivot features, which are stable across domains. In a series of experiments in document level sentiment classification they show that their extension outperforms the original structural correspondence learning approach. In their error analysis, the authors observe the best results were reached when the training - testing combinations were Books - DVDs or Electronics - Kitchen appliances. They conclude that the</context>
<context position="35934" citStr="Blitzer et al. (2007)" startWordPosition="5907" endWordPosition="5910">ng F-Measure remained stable if the token feature is not used in the training. In isolation, training only on the cars data yields the second highest results on the movies and webservices datasets and the highest results regarding F-Measure on the cameras data. However, the results of the cars + cameras training data combination indicate that the cameras data does not contribute any additional information during the learning, since the results on both the movies and the web-services datasets are lower than when training only on the cameras data. Our results also confirm the insights gained by Blitzer et al. (2007), who observed that in crossdomain polarity analysis adding more training data is not always beneficial. Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data. We observe that the results of the cross-domain extraction regarding F-Measure come relatively close to the results of the single-domain setting, especially if the token string feature is removed there (see Table 3 row 9). On the cars and the cameras dataset the cross-domain results are even closer to the single-domain results. The features we em</context>
<context position="38734" citStr="Blitzer et al., 2007" startWordPosition="6384" endWordPosition="6387">F-based approach outperforms a supervised baseline on all four datasets. Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation (Blitzer et al., 2007; Jiang and Zhai, 2007), perform in comparison to our approach. Since three of the features we employed in 1044 our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Bloom</author>
<author>Navendu Garg</author>
<author>Shlomo Argamon</author>
</authors>
<title>Extracting appraisal expressions.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>308--315</pages>
<location>Rochester, New York, USA,</location>
<contexts>
<context position="7260" citStr="Bloom et al. (2007)" startWordPosition="1154" endWordPosition="1157">ds a precision of 0.97 and 1.00 in the task of opinion target (product feature) extraction, recall values are not reported. Popescu and Etzioni (2005) present the OPINE system for opinion mining on product reviews. Their algorithm is based on an information extraction system, which uses the pointwise mutual information based on the hitcounts of a web-search engine as an input. They evaluate the opinion target extraction separately on the dataset by Hu and Liu (2004). OPINE’s precision is on average 22% higher than the association mining based approach, while having an average 3% lower recall. Bloom et al. (2007) manually create taxonomies of opinion targets for two datasets. With a handcrafted set of dependency tree paths their algorithm identifies related opinion expressions and targets. Due to the lack of a dataset annotated with opinion expressions and targets, they just evaluate the accuracy of several aspects of their algorithm by manually assessing an output sample. Their algorithm yields an accuracy of 0.75 in the identification of opinion targets. Kim and Hovy (2006) aim at extracting opinion holders and opinion targets in newswire with semantic role labeling. They define a mapping of the sem</context>
<context position="9654" citStr="Bloom et al. (2007)" startWordPosition="1538" endWordPosition="1541">488 in the task of extracting opinion target - opinion expression pairs. Kessler and Nicolov (2009) solely focus on identifying which opinion expression is linked to which opinion target in a sentence. They present a dataset of car and camera reviews in which opinion expressions and opinion targets are annotated. Starting with this information, they train a machine learning classifier for identifying related opinion expressions and targets. Their algorithm receives the opinion expression and opinion target annotations as input during runtime. The classifier is evaluated using the algorithm by Bloom et al. (2007) as a baseline. The support vector machine based approach by Kessler and Nicolov (2009) yields an F-Measure of 0.698, outperforming the baseline which yields an F-Measure of 0.445. 2.3 Domain Adaptation in Opinion Mining The task of creating a supervised algorithm, which when trained on data from domain A, also performs well on data from another domain B, is a domain adaptation problem (Daum´e III and Marcu, 2006; Jiang and Zhai, 2007). Aue and Gamon (2005) have investigated this challenge very early in the task of document level sentiment classification (positive / negative). They observe tha</context>
</contexts>
<marker>Bloom, Garg, Argamon, 2007</marker>
<rawString>Kenneth Bloom, Navendu Garg, and Shlomo Argamon. 2007. Extracting appraisal expressions. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 308– 315, Rochester, New York, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Identifying sources of opinions with conditional random fields and extraction patterns.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>355--362</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="39146" citStr="Choi et al. (2005)" startWordPosition="6452" endWordPosition="6455">et vocabularies are substantially different. For future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation (Blitzer et al., 2007; Jiang and Zhai, 2007), perform in comparison to our approach. Since three of the features we employed in 1044 our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse. Acknowledgments The project was funded by means of the German Federal Ministry of Economy and Technology under the promotional reference “01MQ07012”. The authors take the responsibility for the con</context>
</contexts>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Identifying sources of opinions with conditional random fields and extraction patterns. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 355–362, Vancouver, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Cowie</author>
<author>Wendy G Lehnert</author>
</authors>
<title>Information extraction.</title>
<date>1996</date>
<journal>Communications of the ACM,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="3248" citStr="Cowie and Lehnert, 1996" startWordPosition="507" endWordPosition="510">y websites. Typical sentences which we encounter in this discourse type are shown in the following examples. The opinion targets which we aim to extract are underlined in the sentences, the corresponding opinion expressions are shown in italics. (1) While none of the features are earth-shattering, eCircles does provide a great place to keep in touch. (2) Hyundai’s more-than-modest refresh has largely addressed all the original car’s weaknesses while maintaining its price competitiveness. The extraction of opinion targets can be considered as an instance of an information extraction (IE) task (Cowie and Lehnert, 1996). Conditional 1035 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1035–1045, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Random Fields (CRF) (Lafferty et al., 2001) have been successfully applied to several IE tasks in the past (Peng and McCallum, 2006). A recurring problem, which arises when working with supervised approaches, concerns the domain portability. In the opinion mining context this question has been prominently investigated with respect to opinion polarity analysis (sentiment analysis) </context>
</contexts>
<marker>Cowie, Lehnert, 1996</marker>
<rawString>James R. Cowie and Wendy G. Lehnert. 1996. Information extraction. Communications of the ACM, 39(1):80–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>26--101</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research (JAIR), 26:101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="5816" citStr="Hu and Liu (2004)" startWordPosition="915" endWordPosition="918">of our experiments and their discussion follow in Section 5. Finally we draw our conclusions in Section 6. 2 Related Work In the following we will discuss the related work regarding opinion target extraction and domain adaptation in opinion mining. The discussion of the related work on opinion target extraction is separated in supervised and unsupervised approaches. We conclude with a discussion of the related work on domain adaptation in opinion mining. 2.1 Unsupervised Opinion Target Extraction The first work on opinion target extraction was done on customer reviews of consumer electronics. Hu and Liu (2004) introduce the task of feature based summarization, which aims at creating an overview of the product features commented on in the reviews. Their approach relies on a statistical analysis of the review terms based on association mining. A dataset of customer reviews from five domains was annotated by the authors regarding mentioned product features with respective opinion polarities. The association mining based algorithm yields a precision of 0.72 and a recall of 0.80 in the extraction of a manually selected subset of product features. The same dataset of product reviews was used in the work </context>
<context position="7111" citStr="Hu and Liu (2004)" startWordPosition="1130" endWordPosition="1133">xtraction which is based on a statistical analysis based on the Likelihood Ratio Test for opinion target extraction. The Likelihood Ratio Test yields a precision of 0.97 and 1.00 in the task of opinion target (product feature) extraction, recall values are not reported. Popescu and Etzioni (2005) present the OPINE system for opinion mining on product reviews. Their algorithm is based on an information extraction system, which uses the pointwise mutual information based on the hitcounts of a web-search engine as an input. They evaluate the opinion target extraction separately on the dataset by Hu and Liu (2004). OPINE’s precision is on average 22% higher than the association mining based approach, while having an average 3% lower recall. Bloom et al. (2007) manually create taxonomies of opinion targets for two datasets. With a handcrafted set of dependency tree paths their algorithm identifies related opinion expressions and targets. Due to the lack of a dataset annotated with opinion expressions and targets, they just evaluate the accuracy of several aspects of their algorithm by manually assessing an output sample. Their algorithm yields an accuracy of 0.75 in the identification of opinion targets</context>
<context position="8879" citStr="Hu and Liu (2004)" startWordPosition="1411" endWordPosition="1414">of 0.315 with annotator1 and 0.127 with annotator2, while the baseline yields an F-Measure of 0.107 and 0.109 regarding opinion target extraction 2.2 Supervised Opinion Target Extraction Zhuang et al. (2006) present a supervised algorithm for the extraction of opinion expression - opinion target pairs. Their algorithm learns the opinion target candidates and a combination of dependency and part-of-speech paths connecting such pairs from an annotated dataset. They evaluate their system in a cross validation setup on a dataset of user-generated movie reviews and compare it to the results of the Hu and Liu (2004) system as a baseline. Thereby, the system by Zhuang et al. (2006) yields an F-Measure of 0.529 and outperforms the baseline which yields an F-Measure of 0.488 in the task of extracting opinion target - opinion expression pairs. Kessler and Nicolov (2009) solely focus on identifying which opinion expression is linked to which opinion target in a sentence. They present a dataset of car and camera reviews in which opinion expressions and opinion targets are annotated. Starting with this information, they train a machine learning classifier for identifying related opinion expressions and targets.</context>
<context position="13676" citStr="Hu and Liu (2004)" startWordPosition="2192" endWordPosition="2195">e hence label all tokens which have a direct dependency relation to an opinion expression in a sentence. The Stanford Parser2 is employed for the constituent and dependency parsing. We will refer to this feature as dLn in our result tables. Word Distance From the work of Zhuang et al. (2006) we can infer that opinion expressions and their target(s) are not always connected via short paths in the dependency parse tree. Since we cannot capture such paths with the abovementioned feature we introduce another feature which acts as heuristic for identifying the target to a given opinion expression. Hu and Liu (2004) and Yi et al. (2003) have shown that (base) noun phrases are good candidates for opinion targets in the datasets of product reviews. We therefore label the token(s) in the closest noun phrase regarding word distance to each opinion expression in a sentence. We will refer to this feature as wrdDist in our result tables. Opinion Sentence With this feature, we simply label all tokens occurring in a sentence containing an opinion expression. This feature shall enable the CRF algorithm to distinguish between the occurence of a certain token in a sentence which contains an opinion vs. a sentence wi</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168–177, Seattle, Washington, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>264--271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10093" citStr="Jiang and Zhai, 2007" startWordPosition="1611" endWordPosition="1614">argets. Their algorithm receives the opinion expression and opinion target annotations as input during runtime. The classifier is evaluated using the algorithm by Bloom et al. (2007) as a baseline. The support vector machine based approach by Kessler and Nicolov (2009) yields an F-Measure of 0.698, outperforming the baseline which yields an F-Measure of 0.445. 2.3 Domain Adaptation in Opinion Mining The task of creating a supervised algorithm, which when trained on data from domain A, also performs well on data from another domain B, is a domain adaptation problem (Daum´e III and Marcu, 2006; Jiang and Zhai, 2007). Aue and Gamon (2005) have investigated this challenge very early in the task of document level sentiment classification (positive / negative). They observe that increasing the amount of training data raises the classification accuracy, but only if the training data is from one source domain. Increasing the training data by mixing domains does not yield any consistent improvements. Blitzer et al. (2007) introduce an extension to a structural correspondence learning algorithm, which was specifically designed to address the task of domain adaptation. Their enhancement aims at identifying pivot </context>
<context position="38757" citStr="Jiang and Zhai, 2007" startWordPosition="6388" endWordPosition="6391">rforms a supervised baseline on all four datasets. Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation (Blitzer et al., 2007; Jiang and Zhai, 2007), perform in comparison to our approach. Since three of the features we employed in 1044 our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to inve</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Kessler</author>
<author>Nicolas Nicolov</author>
</authors>
<title>Targeting sentiment expressions through supervised ranking of linguistic configurations.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>90--97</pages>
<location>San Jose, California, USA,</location>
<contexts>
<context position="9134" citStr="Kessler and Nicolov (2009)" startWordPosition="1454" endWordPosition="1457"> extraction of opinion expression - opinion target pairs. Their algorithm learns the opinion target candidates and a combination of dependency and part-of-speech paths connecting such pairs from an annotated dataset. They evaluate their system in a cross validation setup on a dataset of user-generated movie reviews and compare it to the results of the Hu and Liu (2004) system as a baseline. Thereby, the system by Zhuang et al. (2006) yields an F-Measure of 0.529 and outperforms the baseline which yields an F-Measure of 0.488 in the task of extracting opinion target - opinion expression pairs. Kessler and Nicolov (2009) solely focus on identifying which opinion expression is linked to which opinion target in a sentence. They present a dataset of car and camera reviews in which opinion expressions and opinion targets are annotated. Starting with this information, they train a machine learning classifier for identifying related opinion expressions and targets. Their algorithm receives the opinion expression and opinion target annotations as input during runtime. The classifier is evaluated using the algorithm by Bloom et al. (2007) as a baseline. The support vector machine based approach by Kessler and Nicolov</context>
<context position="12867" citStr="Kessler and Nicolov, 2009" startWordPosition="2055" endWordPosition="2058"> Tagger1. It can provide some means of lexical disam1http://nlp.stanford.edu/software/tagger.shtml 1037 biguation, e.g. indicate that the token “sounds” is a noun and not a verb in a certain context. At the same time, the CRF algorithm is provided with additional information to extract opinion targets which are multiword expressions, i.e. noun combinations. We will refer to this feature as pos in our result tables. Short Dependency Path Previous research has successfully employed paths in the dependency parse tree to link opinion expressions and the corresponding targets (Zhuang et al., 2006; Kessler and Nicolov, 2009). Both works identify direct dependency relations such as “amod” and “nsubj” as the most frequent and at the same time highly accurate connections between a target and an opinion expression. We hence label all tokens which have a direct dependency relation to an opinion expression in a sentence. The Stanford Parser2 is employed for the constituent and dependency parsing. We will refer to this feature as dLn in our result tables. Word Distance From the work of Zhuang et al. (2006) we can infer that opinion expressions and their target(s) are not always connected via short paths in the dependenc</context>
<context position="15863" citStr="Kessler and Nicolov (2009)" startWordPosition="2549" endWordPosition="2552"> In our experiments, we employ datasets from three different sources, which span four domains in total (see Table 1). All of them consist of reviews collected from Web 2.0 sites. The first dataset consists of reviews for 20 different movies collected from the Internet Movie Database. It was presented in Zhuang et al. (2006) and annotated regarding opinion target - opinion expression pairs. The second dataset consists of 234 reviews for two different web-services collected from epinions.com, as described in Toprak et al. (2010). The third dataset is an extended version of the data presented in Kessler and Nicolov (2009). The authors have provided us with additional documents, which have been annotated in the meantime. The version of the dataset used in our experiments consists of 179 blog postings regarding different digital cameras and 336 reviews of different cars. In the description of their annotation guidelines, Kessler and Nicolov (2009) refer to opinion targets as mentions. Mentions are all aspects of the review topic, which can be targets of expressed opinions. However, not only mentions which occur as opinion targets were originally annotated, but also mentions which occur in non-opinion sentences. </context>
<context position="26989" citStr="Kessler and Nicolov, 2009" startWordPosition="4432" endWordPosition="4435">e recall is improved on all datasets, while the precision is slightly lowered on the movies and web-services datasets. The dependency path based feature performs better compared to the word distance heuristic as shown in row 3. The precision is considerably increased on all datasets, on the movies and cars &amp; cameras datasets even reaching the overall highest value. At the same time, we observe an increase of recall on all datasets. The observation made in previous research that short paths in the dependency graph are a high precision indicator of related opinion expressions - opinion targets (Kessler and Nicolov, 2009) is confirmed on all datasets. Adding the information regarding opinion sentences to the basic features of the token string and the part-of-speech tag (row 4) yields the biggest improvements regarding F-Measure on the movies and web-services dataset (+0.433 / +0.383). On the cars &amp; cameras dataset the recall is relatively low again. We assume that this is again due to the high lexical variability, so that the CRF algorithm will encounter many actual opinion targets in the testing data which have not occurred in the training data and will hence not be extracted. As shown in row 5, if we combine</context>
</contexts>
<marker>Kessler, Nicolov, 2009</marker>
<rawString>Jason Kessler and Nicolas Nicolov. 2009. Targeting sentiment expressions through supervised ranking of linguistic configurations. In Proceedings of the Third International AAAI Conference on Weblogs and Social Media, pages 90–97, San Jose, California, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL Workshop on Sentiment and Subjectivity in Text,</booktitle>
<pages>1--8</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="7732" citStr="Kim and Hovy (2006)" startWordPosition="1229" endWordPosition="1232">OPINE’s precision is on average 22% higher than the association mining based approach, while having an average 3% lower recall. Bloom et al. (2007) manually create taxonomies of opinion targets for two datasets. With a handcrafted set of dependency tree paths their algorithm identifies related opinion expressions and targets. Due to the lack of a dataset annotated with opinion expressions and targets, they just evaluate the accuracy of several aspects of their algorithm by manually assessing an output sample. Their algorithm yields an accuracy of 0.75 in the identification of opinion targets. Kim and Hovy (2006) aim at extracting opinion holders and opinion targets in newswire with semantic role labeling. They define a mapping of the semantic roles identified with FrameNet to the respective opinion elements. As a baseline, they im1036 plement an approach based on a dependency parser, which identifies the targets following the dependencies of opinion expressions. They measure the overlap between two human annotators and their algorithm as well as the baseline system. The algorithm based on semantic role labeling yields an F-Measure of 0.315 with annotator1 and 0.127 with annotator2, while the baseline</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Extracting opinions, opinion holders, and topics expressed in online news media text. In Proceedings of the ACL Workshop on Sentiment and Subjectivity in Text, pages 1–8, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<location>Williamstown, MA, USA,</location>
<contexts>
<context position="3508" citStr="Lafferty et al., 2001" startWordPosition="541" endWordPosition="544">f the features are earth-shattering, eCircles does provide a great place to keep in touch. (2) Hyundai’s more-than-modest refresh has largely addressed all the original car’s weaknesses while maintaining its price competitiveness. The extraction of opinion targets can be considered as an instance of an information extraction (IE) task (Cowie and Lehnert, 1996). Conditional 1035 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1035–1045, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Random Fields (CRF) (Lafferty et al., 2001) have been successfully applied to several IE tasks in the past (Peng and McCallum, 2006). A recurring problem, which arises when working with supervised approaches, concerns the domain portability. In the opinion mining context this question has been prominently investigated with respect to opinion polarity analysis (sentiment analysis) in previous research (Aue and Gamon, 2005; Blitzer et al., 2007). Terms as “unpredictable” can express a positive opinion when uttered about the storyline of a movie but a negative opinion when the handling of a car is described. Hence the effects of training </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282–289, Williamstown, MA, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Andrew McCallum</author>
</authors>
<title>Information extraction from research papers using conditional random fields.</title>
<date>2006</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>42</volume>
<issue>4</issue>
<contexts>
<context position="3597" citStr="Peng and McCallum, 2006" startWordPosition="556" endWordPosition="559">ch. (2) Hyundai’s more-than-modest refresh has largely addressed all the original car’s weaknesses while maintaining its price competitiveness. The extraction of opinion targets can be considered as an instance of an information extraction (IE) task (Cowie and Lehnert, 1996). Conditional 1035 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1035–1045, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Random Fields (CRF) (Lafferty et al., 2001) have been successfully applied to several IE tasks in the past (Peng and McCallum, 2006). A recurring problem, which arises when working with supervised approaches, concerns the domain portability. In the opinion mining context this question has been prominently investigated with respect to opinion polarity analysis (sentiment analysis) in previous research (Aue and Gamon, 2005; Blitzer et al., 2007). Terms as “unpredictable” can express a positive opinion when uttered about the storyline of a movie but a negative opinion when the handling of a car is described. Hence the effects of training and testing a machine learning algorithm for sentiment analysis on data from different do</context>
</contexts>
<marker>Peng, McCallum, 2006</marker>
<rawString>Fuchun Peng and Andrew McCallum. 2006. Information extraction from research papers using conditional random fields. Information Processing and Management, 42(4):963–979, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>339--346</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="6791" citStr="Popescu and Etzioni (2005)" startWordPosition="1076" endWordPosition="1079">s with respective opinion polarities. The association mining based algorithm yields a precision of 0.72 and a recall of 0.80 in the extraction of a manually selected subset of product features. The same dataset of product reviews was used in the work of Yi et al. (2003). They present and evaluate a complete system for opinion extraction which is based on a statistical analysis based on the Likelihood Ratio Test for opinion target extraction. The Likelihood Ratio Test yields a precision of 0.97 and 1.00 in the task of opinion target (product feature) extraction, recall values are not reported. Popescu and Etzioni (2005) present the OPINE system for opinion mining on product reviews. Their algorithm is based on an information extraction system, which uses the pointwise mutual information based on the hitcounts of a web-search engine as an input. They evaluate the opinion target extraction separately on the dataset by Hu and Liu (2004). OPINE’s precision is on average 22% higher than the association mining based approach, while having an average 3% lower recall. Bloom et al. (2007) manually create taxonomies of opinion targets for two datasets. With a handcrafted set of dependency tree paths their algorithm id</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 339–346, Vancouver, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cigdem Toprak</author>
<author>Niklas Jakob</author>
<author>Iryna Gurevych</author>
</authors>
<title>Sentence and expression level annotation of opinions in user-generated discourse.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>575--584</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="15769" citStr="Toprak et al. (2010)" startWordPosition="2533" endWordPosition="2536">e use the CRF implementation from the Mallet toolkit3. 4 Experimental Setup 4.1 Datasets In our experiments, we employ datasets from three different sources, which span four domains in total (see Table 1). All of them consist of reviews collected from Web 2.0 sites. The first dataset consists of reviews for 20 different movies collected from the Internet Movie Database. It was presented in Zhuang et al. (2006) and annotated regarding opinion target - opinion expression pairs. The second dataset consists of 234 reviews for two different web-services collected from epinions.com, as described in Toprak et al. (2010). The third dataset is an extended version of the data presented in Kessler and Nicolov (2009). The authors have provided us with additional documents, which have been annotated in the meantime. The version of the dataset used in our experiments consists of 179 blog postings regarding different digital cameras and 336 reviews of different cars. In the description of their annotation guidelines, Kessler and Nicolov (2009) refer to opinion targets as mentions. Mentions are all aspects of the review topic, which can be targets of expressed opinions. However, not only mentions which occur as opini</context>
</contexts>
<marker>Toprak, Jakob, Gurevych, 2010</marker>
<rawString>Cigdem Toprak, Niklas Jakob, and Iryna Gurevych. 2010. Sentence and expression level annotation of opinions in user-generated discourse. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575–584, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeonghee Yi</author>
<author>Tetsuya Nasukawa</author>
<author>Razvan Bunescu</author>
<author>Wayne Niblack</author>
</authors>
<title>Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques.</title>
<date>2003</date>
<booktitle>In Proceedings of the 3rd IEEE International Conference on Data Mining,</booktitle>
<pages>427--434</pages>
<location>Melbourne, Florida, USA,</location>
<contexts>
<context position="6435" citStr="Yi et al. (2003)" startWordPosition="1018" endWordPosition="1021">troduce the task of feature based summarization, which aims at creating an overview of the product features commented on in the reviews. Their approach relies on a statistical analysis of the review terms based on association mining. A dataset of customer reviews from five domains was annotated by the authors regarding mentioned product features with respective opinion polarities. The association mining based algorithm yields a precision of 0.72 and a recall of 0.80 in the extraction of a manually selected subset of product features. The same dataset of product reviews was used in the work of Yi et al. (2003). They present and evaluate a complete system for opinion extraction which is based on a statistical analysis based on the Likelihood Ratio Test for opinion target extraction. The Likelihood Ratio Test yields a precision of 0.97 and 1.00 in the task of opinion target (product feature) extraction, recall values are not reported. Popescu and Etzioni (2005) present the OPINE system for opinion mining on product reviews. Their algorithm is based on an information extraction system, which uses the pointwise mutual information based on the hitcounts of a web-search engine as an input. They evaluate </context>
<context position="13697" citStr="Yi et al. (2003)" startWordPosition="2197" endWordPosition="2200">ns which have a direct dependency relation to an opinion expression in a sentence. The Stanford Parser2 is employed for the constituent and dependency parsing. We will refer to this feature as dLn in our result tables. Word Distance From the work of Zhuang et al. (2006) we can infer that opinion expressions and their target(s) are not always connected via short paths in the dependency parse tree. Since we cannot capture such paths with the abovementioned feature we introduce another feature which acts as heuristic for identifying the target to a given opinion expression. Hu and Liu (2004) and Yi et al. (2003) have shown that (base) noun phrases are good candidates for opinion targets in the datasets of product reviews. We therefore label the token(s) in the closest noun phrase regarding word distance to each opinion expression in a sentence. We will refer to this feature as wrdDist in our result tables. Opinion Sentence With this feature, we simply label all tokens occurring in a sentence containing an opinion expression. This feature shall enable the CRF algorithm to distinguish between the occurence of a certain token in a sentence which contains an opinion vs. a sentence without an opinion. We </context>
</contexts>
<marker>Yi, Nasukawa, Bunescu, Niblack, 2003</marker>
<rawString>Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and Wayne Niblack. 2003. Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques. In Proceedings of the 3rd IEEE International Conference on Data Mining, pages 427–434, Melbourne, Florida, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Zhuang</author>
<author>Feng Jing</author>
<author>Xiao-Yan Zhu</author>
</authors>
<title>Movie review mining and summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACM 15th Conference on Information and Knowledge Management,</booktitle>
<pages>43--50</pages>
<location>Arlington, Virginia, USA,</location>
<contexts>
<context position="8469" citStr="Zhuang et al. (2006)" startWordPosition="1346" endWordPosition="1349"> of the semantic roles identified with FrameNet to the respective opinion elements. As a baseline, they im1036 plement an approach based on a dependency parser, which identifies the targets following the dependencies of opinion expressions. They measure the overlap between two human annotators and their algorithm as well as the baseline system. The algorithm based on semantic role labeling yields an F-Measure of 0.315 with annotator1 and 0.127 with annotator2, while the baseline yields an F-Measure of 0.107 and 0.109 regarding opinion target extraction 2.2 Supervised Opinion Target Extraction Zhuang et al. (2006) present a supervised algorithm for the extraction of opinion expression - opinion target pairs. Their algorithm learns the opinion target candidates and a combination of dependency and part-of-speech paths connecting such pairs from an annotated dataset. They evaluate their system in a cross validation setup on a dataset of user-generated movie reviews and compare it to the results of the Hu and Liu (2004) system as a baseline. Thereby, the system by Zhuang et al. (2006) yields an F-Measure of 0.529 and outperforms the baseline which yields an F-Measure of 0.488 in the task of extracting opin</context>
<context position="12839" citStr="Zhuang et al., 2006" startWordPosition="2051" endWordPosition="2054">d by the Stanford POS Tagger1. It can provide some means of lexical disam1http://nlp.stanford.edu/software/tagger.shtml 1037 biguation, e.g. indicate that the token “sounds” is a noun and not a verb in a certain context. At the same time, the CRF algorithm is provided with additional information to extract opinion targets which are multiword expressions, i.e. noun combinations. We will refer to this feature as pos in our result tables. Short Dependency Path Previous research has successfully employed paths in the dependency parse tree to link opinion expressions and the corresponding targets (Zhuang et al., 2006; Kessler and Nicolov, 2009). Both works identify direct dependency relations such as “amod” and “nsubj” as the most frequent and at the same time highly accurate connections between a target and an opinion expression. We hence label all tokens which have a direct dependency relation to an opinion expression in a sentence. The Stanford Parser2 is employed for the constituent and dependency parsing. We will refer to this feature as dLn in our result tables. Word Distance From the work of Zhuang et al. (2006) we can infer that opinion expressions and their target(s) are not always connected via </context>
<context position="15562" citStr="Zhuang et al. (2006)" startWordPosition="2501" endWordPosition="2504">ar chain CRF, which is based on an undirected graph. In the graph, each node corresponds to a token in the sentence and edges connect the adjacent tokens as they appear in the sentence. In our experiments, we use the CRF implementation from the Mallet toolkit3. 4 Experimental Setup 4.1 Datasets In our experiments, we employ datasets from three different sources, which span four domains in total (see Table 1). All of them consist of reviews collected from Web 2.0 sites. The first dataset consists of reviews for 20 different movies collected from the Internet Movie Database. It was presented in Zhuang et al. (2006) and annotated regarding opinion target - opinion expression pairs. The second dataset consists of 234 reviews for two different web-services collected from epinions.com, as described in Toprak et al. (2010). The third dataset is an extended version of the data presented in Kessler and Nicolov (2009). The authors have provided us with additional documents, which have been annotated in the meantime. The version of the dataset used in our experiments consists of 179 blog postings regarding different digital cameras and 336 reviews of different cars. In the description of their annotation guideli</context>
<context position="18173" citStr="Zhuang et al. (2006)" startWordPosition="2935" endWordPosition="2938">argets in a dataset. We divide this by the sum of all opinion target instances in the dataset. For the cars dataset this ratio is 0.440 and for the cameras dataset it is 0.433, while for the web-services dataset it is 0.306 and for the movies dataset only 0.122. In terms of reviews this means, that in the movie reviews the same movie aspects are repeatedly commented on, while in the cars and the cameras datasets many different aspects of these entities are discussed, which in turn each occur infrequently. 4.2 Baseline System In the task of opinion target extraction the supervised algorithm by Zhuang et al. (2006) represents the state-of-the-art on the movies dataset we also employ in our experiments. We therefore use it as a baseline. The algorithm learns two aspects from the labeled training data: 1. A set of opinion target candidates 2. A set of paths in a dependency tree which identify valid opinion target - opinion expression pairs In our experiments, we learn the full set of opinion targets from the labeled training data in the first step. This is slightly different from the approach in (Zhuang et al., 2006), but we expect that this modification should be beneficial for the overall performance in</context>
<context position="22793" citStr="Zhuang et al. (2006)" startWordPosition="3723" endWordPosition="3726">the entire dataset B, we hence report one micro averaged result set. In Subsection 5.1 we present the results of both the baseline system and our CRF-based approach in the single-domain setting, in Subsection 5.2 we present the results of the two systems in the cross-domain opinion target extraction. Table 2: Single-Domain Extraction with Zhuang Baseline Dataset Precision Recall F-Measure movies 0.663 0.592 0.625 web-services 0.624 0.394 0.483 cars 0.259 0.426 0.322 cameras 0.423 0.431 0.426 5.1 Single-Domain Results 5.1.1 Zhuang Baseline As shown in Table 2, the state-of-the-art algorithm of Zhuang et al. (2006) performs best on the movie review dataset and worst on the cars dataset. The results on the movie dataset are higher than originally reported in (Zhuang et al., 2006) (Precision 0.483, Recall 0.585, F-Measure 0.529). We assume that this is due to two reasons: 1. In our task, the algorithm uses the opinion expression annotation from the gold standard. 2. We do not remove any learned opinion target candidates from the training data (See Section 4.2). During training we observed that for each dataset the lists of possible dependency paths (see Example 3) contained several hundred entries, many o</context>
<context position="31602" citStr="Zhuang et al. (2006)" startWordPosition="5206" endWordPosition="5209">ence only these attributes are annotated as targets. 4http://www.cs.pitt.edu/mpqa/ 5.2 Cross-Domain Results 5.2.1 Zhuang Baseline Table 4 shows the results of the opinion target extraction with the state-of-the-art system in the crossdomain setting. We observe that the results on all domain combinations are very low. A quantitative error analysis has revealed that there is hardly any overlap in the opinion target candidates between domains, as reflected by the low recall in all configurations. The vocabularies of the opinion targets are too different, hence the performance of the algorithm by Zhuang et al. (2006) is so low. The overlap regarding the dependency paths between domains was however higher. Especially identical short paths could be found across domains which at the same time typically occured quite often. For future work it might be interesting to investigate how the algorithm by Zhuang et al. (2006) performs in the crossdomain setting if the target candidate learning is performed differently, e.g. with a statistical approach as outlined in Section 2.1. 5.2.2 CRF-based Approach The results of the cross-domain target extraction with the CRF-based algorithm are shown in Table 5. Due to the in</context>
</contexts>
<marker>Zhuang, Jing, Zhu, 2006</marker>
<rawString>Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie review mining and summarization. In Proceedings of the ACM 15th Conference on Information and Knowledge Management, pages 43–50, Arlington, Virginia, USA, November.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>