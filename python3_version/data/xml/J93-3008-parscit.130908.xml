<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012612">
<title confidence="0.904329">
Connectionist Approaches to Natural Language Processing
</title>
<note confidence="0.902212">
Ronan G. Reilly and Noel E. Sharkey, editors
(University College Dublin and University of Exeter)
</note>
<address confidence="0.3532355">
Hove, UK: Lawrence Erlbaum
Associates, 1992, xiv + 472 pp.
</address>
<figure confidence="0.757137666666667">
Hardbound, ISBN 0-86377-179-3, $89.95
Reviewed by
James Henderson
</figure>
<affiliation confidence="0.474153">
University of Pennsylvania
</affiliation>
<bodyText confidence="0.999969105263158">
With the growing popularity of statistical approaches to natural language processing,
it is natural to consider how connectionist techniques for learning and using soft
constraints might be applicable to this area. However, nagging questions about the
representational and computational adequacy of connectionist networks for processing
language make the fruitfulness of this endeavor uncertain. This book addresses both
these issues. It contains background and overview sections by the editors, plus 13
chapters covering a broad spectrum of recent work involving connectionist models
and natural language processing. Each chapter can be read on its own, or the book
can be read in its entirety. A casual familiarity with connectionist networks is adequate
for understanding most of the book.
The book begins with an introductory chapter that includes some background
about connectionist natural language processing and an overview of the other chapters.
Many references are given to earlier work, and the discussion provides a good context
for the more recent work presented in the book. The remaining chapters are organized
into four sections: semantics, syntax, representational adequacy, and computational
psycholinguistics. Each section has its own introduction, plus chapters written by
researchers in the area.
The section on connectionist approaches to semantics includes four chapters that
range from lexical semantic representations to &amp;quot;story comprehension.&amp;quot; In Chapter 2,
Dyer, Flowers, and Wang propose a connectionist architecture for learning represen-
tations for symbols by training on the relationships between the symbols. They apply
this method to encoding semantic networks using one symbol per semantic node. In
Chapter 3, Sutcliffe argues for representing word meanings using an unstructured set
of semantically interpretable features with associated &amp;quot;centrality&amp;quot; values. This rep-
resentation is uniform across words and provides a simple measure of the distance
between two concepts. Wermter and Lehnert address the issue of disambiguating PP
attachment in noun phrases using semantic information. They train one set of net-
works to produce plausibility values for individual attachments and use a constraint
satisfaction network to combine these preferences with structural constraints to deter-
mine the best set of attachments. St. John and McClelland apply connectionist learn-
ing techniques and a constraint satisfaction network to a variety of tasks involving
semantic preferences. They demonstrate the usefulness of these techniques for some
problems, but their claim that no additional mechanisms are needed for sentence and
story comprehension is not substantiated.
The section on syntax contains two chapters that use hand-structured networks
to implement symbolic grammars of the familiar kind. In Chapter 6, Rager addresses
parsing and correcting extragrammatical input. Individual constraints are represented
in constraint satisfaction (winner-take-all) subnetworks, and in the event of an in-
</bodyText>
<page confidence="0.970033">
557
</page>
<note confidence="0.544768">
Computational Linguistics Volume 19, Number 3
</note>
<bodyText confidence="0.9998855">
consistency the best alternative is chosen. Schnelle and Doust implement an Earley
parser for context-free grammars. This article was hard to follow, but it seemed to
simply illustrate that just about anything can be compiled into units and links given
a sufficiently broad definition of those primitives.
The section on representational adequacy presents three perspectives on the role
of symbolic representations in connectionist models. Bever argues that connection-
ist models can only capture habitual, associative knowledge, but that this ability
makes them an important tool for use within explicit structural symbolic theories.
He argues that connectionist models that claim to circumvent the need for prede-
fined knowledge actually have such knowledge built in, and describes a network
built with explicitly predefined structured knowledge that learns to segment utter-
ances into linguistically appropriate phrases. Berg discusses two ways in which con-
nectionist models of the time were inadequate, namely distinguishing between mul-
tiple instances of the same concept and creating new concepts quickly. He proposes a
hybrid connectionist/marker-passing model to address these problems while staying
massively parallel. Dorffner argues that a model of language need not and should
not depend on explicit representations specified by a researcher. This harkens back
to the debate over whether linguistic behavior can be explained through a general-
purpose learning mechanism that has no prespecified (innate) linguistic knowledge
(cf Chomsky 1966).
The last section is on computational psycholinguistics. The first two chapters pro-
pose alternatives to McClelland and Elman&apos;s (1986) TRACE model of speech percep-
tion, where feedback from the word level facilitates recognition at the phoneme level.
Massaro proposes a feedforward (bottom-up) model of speech perception phenom-
ena based on fuzzy logic. The argument would be more convincing if the network
weren&apos;t trained on the data to be explained. Norris argues for a feedforward network
with a single hidden layer that is shared by two unconnected output layers for word
recognition and phoneme recognition. He shows that the apparent top-down effects
can be the result of the top-down aspects of the backpropagation learning algorithm,
and do not have to result from top-down information during processing. O&apos;Seaghdha,
Dell, Peterson, and Juliano discuss two connectionist models of form-related priming
in lexical access, and opt for the one with articulated representations of the prime and
target words. Mozer and Behrmann model a reading disorder caused by brain damage
as an impairment in the attention component of a previously proposed connectionist
model of word recognition in reading.
Research on connectionist natural language processing is still in its early stages,
and this is evident in this book. Consequently, readers of Computational Linguistics may
be disappointed with the preliminary nature of many of the models compared with
more traditional approaches to language. This also accounts for this book&apos;s emphasis
on connectionist approaches rather than natural language processing. Although most
of the questions and proposals discussed in this book will be with us for a long time
to come, it is inevitable that a book will not include the most recent developments in
such a quickly changing field. For natural language processing, the most important
of these is the development of architectures that support symbolic processing without
violating the basic tenets of connectionism. For a comprehensive bibliography on this
topic, see Sun and Bookman (1993). Despite these inherent problems for any book on
connectionist natural language processing, this book is worthwhile reading for anyone
interested in the lively debate over the adequacy of connectionist models, in the old
debate over learning versus prespecified knowledge, in the application of connectionist
models to problems involving soft constraints, or in connectionist models in general.
</bodyText>
<page confidence="0.995952">
558
</page>
<reference confidence="0.973825">
Book Reviews
References Sun, R., and Bookman, L. (editors) (1993).
Chomsky, Noam (1966). Cartesian Linguistics. Computational Architectures Integrating
Harper and Row. Neural and Symbolic Processes. Kluwer
McClelland, J. L., and Elman, J. L. (1986). Academic Publishers.
&amp;quot;The TRACE model of speech perception.&amp;quot;
Cognitive Psychology, 18, 1-86.
James Henderson is a Ph.D. candidate in Computer and Information Science at the University of
Pennsylvania, majoring in Computational Linguistics. His dissertation is on syntactic parsing in
a recently proposed connectionist architecture that supports symbolic computation. Henderson&apos;s
address is: Institute for Research in Cognitive Science, 3401 Walnut St, Philadelphia, PA 19104;
email: henders@linc.cis.upenn.edu.
</reference>
<page confidence="0.998486">
559
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.279333">
<title confidence="0.999421">Connectionist Approaches to Natural Language Processing</title>
<author confidence="0.976842">Ronan G Reilly</author>
<author confidence="0.976842">Noel E Sharkey</author>
<author confidence="0.976842">editors</author>
<affiliation confidence="0.971637">(University College Dublin and University of Exeter)</affiliation>
<address confidence="0.832754">Hove, UK: Lawrence Erlbaum</address>
<note confidence="0.915223">Associates, 1992, xiv + 472 pp. Hardbound, ISBN 0-86377-179-3, $89.95 Reviewed by</note>
<author confidence="0.999759">James Henderson</author>
<affiliation confidence="0.995058">University of Pennsylvania</affiliation>
<abstract confidence="0.991675571428572">With the growing popularity of statistical approaches to natural language processing, it is natural to consider how connectionist techniques for learning and using soft constraints might be applicable to this area. However, nagging questions about the representational and computational adequacy of connectionist networks for processing language make the fruitfulness of this endeavor uncertain. This book addresses both these issues. It contains background and overview sections by the editors, plus 13 chapters covering a broad spectrum of recent work involving connectionist models and natural language processing. Each chapter can be read on its own, or the book can be read in its entirety. A casual familiarity with connectionist networks is adequate for understanding most of the book. The book begins with an introductory chapter that includes some background about connectionist natural language processing and an overview of the other chapters. Many references are given to earlier work, and the discussion provides a good context for the more recent work presented in the book. The remaining chapters are organized</abstract>
<intro confidence="0.453009">into four sections: semantics, syntax, representational adequacy, and computational</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Book Reviews References Chomsky</author>
<author>Noam</author>
</authors>
<title>Cartesian Linguistics. Harper</title>
<date>1966</date>
<journal>Cognitive Psychology,</journal>
<volume>18</volume>
<pages>1--86</pages>
<marker>Chomsky, Noam, 1966</marker>
<rawString>Book Reviews References Chomsky, Noam (1966). Cartesian Linguistics. Harper and Row. McClelland, J. L., and Elman, J. L. (1986). &amp;quot;The TRACE model of speech perception.&amp;quot; Cognitive Psychology, 18, 1-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sun</author>
<author>L Bookman</author>
</authors>
<date>1993</date>
<booktitle>Computational Architectures Integrating Neural and Symbolic Processes.</booktitle>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Sun, Bookman, 1993</marker>
<rawString>Sun, R., and Bookman, L. (editors) (1993). Computational Architectures Integrating Neural and Symbolic Processes. Kluwer Academic Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>