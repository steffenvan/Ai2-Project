<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.051959">
<title confidence="0.983957">
A Framework for Building Conversational Agents Based on a Multi-Expert
Model
</title>
<author confidence="0.995013">
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, Hiroshi Tsujino
</author>
<affiliation confidence="0.999303">
Honda Research Institute Japan Co., Ltd.
</affiliation>
<address confidence="0.876603">
8-1 Honcho, Wako, Saitama 351-0188, Japan
</address>
<email confidence="0.946876">
{nakano, funakoshi, yuji.hasegawa, tsujino}@jp.honda-ri.com
</email>
<sectionHeader confidence="0.994233" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995165">
This paper presents a novel framework for
building symbol-level control modules of an-
imated agents and robots having a spoken di-
alogue interface. It features distributed mod-
ules called experts each of which is special-
ized to perform certain kinds of tasks. A com-
mon interface that all experts must support is
specified, and any kind of expert can be incor-
porated if it has the interface. Several modules
running in parallel coordinate the experts by
accessing them through the interface, so that
the whole system can achieve flexible control,
such as interruption handling and parallel task
execution.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983611111111">
As much attention is recently paid to autonomous
agents such as robots and animated agents, spoken
dialogue is expected to be a natural interface be-
tween users and such agents. Our objective is to es-
tablish a framework for developing the intelligence
module of such agents.
In establishing such a framework, we focus on
achieving the following features. (1) Multi-domain
dialogue: Since agents are usually expected to per-
form multiple kinds of tasks, they need to work in
multiple domains and switch domains according to
user utterances. (2) Interruption handling: It is cru-
cial for human-agent interaction to be able to handle
users’ interrupting utterances while speaking or per-
forming tasks. (3) Parallel task execution: Agents,
especially robots that perform physical actions, are
expected to be able to execute multiple tasks in par-
allel when possible. For example, robots should be
</bodyText>
<page confidence="0.983212">
88
</page>
<bodyText confidence="0.999954064516129">
able to engage in a dialogue while moving. (4) Ex-
tensibility: Since the agents can be used for a vari-
ety of tasks, various strategies for dialogue and task
planning should be able to be incorporated.
Although a number of models for conversational
agents have been proposed, no model has all of the
above properties. Several multi-domain dialogue
system models have been proposed and they are ex-
tensible, but it is not clear how they handle interrup-
tions to system utterances and actions (e.g., O’Neill
et al. (2004), Lin et al. (1999), and Hartikainen et al.
(2004)). There are several spoken dialogue agents
and robots that can handle interruptions thanks to
their asynchronous control (Asoh et al., 1999; Boye
et al., 2000; Blaylock et al., 2002; Lemon et al.,
2002), they do not focus on making it easy to add
new dialogue domains with a variety of dialogue
strategies.
This paper presents a framework called RIME
(Robot Intelligence based on Multiple Experts),
which employs modules called experts.1 Each ex-
pert is specialized for achieving certain kinds of
tasks by performing physical actions and engaging
in dialogues. It corresponds to the symbol-level con-
trol module of a system that can engage in tasks in
a single small domain, and it employs fixed con-
trol strategies. Only some of the experts take charge
in understanding user utterances and decide actions.
The basic idea behind RIME is to specify a com-
mon interface of experts for coordinating them and
to achieve flexible control. In RIME, several mod-
</bodyText>
<footnote confidence="0.986806333333333">
1RIME is an improved version of our previous model
(Nakano et al., 2005), whose interruption handling was too sim-
ple and which could not achieve parallel task execution.
</footnote>
<note confidence="0.415874">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 88–91,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999770047619048">
ules run in parallel for coordinating experts. They
are understander, which is responsible for speech
understanding, action selector, which is responsible
for selecting actions, and task planner, which is re-
sponsible for deciding which expert should work to
achieve tasks.
RIME achieves the above mentioned features.
Multi-domain dialogues are possible by selecting an
appropriate expert which is specialized to dialogues
in a certain domain. Interruption handling is possi-
ble because each expert must have methods to de-
tect interruptions and decide actions to handle in-
terruptions, and coordinating modules can use these
methods. Parallel task execution is possible because
experts have methods for providing information to
decide which experts can take charge at the same
time, and the task planner utilizes that information.
Extensibility is achieved because any kind of expert
can be incorporated if it supports the common inter-
face. This makes it possible for agent developers to
build a variety of conversational agents.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="method">
2 Multi-Expert Model
</sectionHeader>
<bodyText confidence="0.9985535">
This section explains RIME in detail. Fig. 1 depicts
its module architecture.
</bodyText>
<subsectionHeader confidence="0.913821">
2.1 Experts
</subsectionHeader>
<bodyText confidence="0.999979">
Each expert is a kind of object in the object-oriented
programming framework. In this paper, we call
tasks performed by one expert primitive tasks. Ex-
perts should be prepared for each primitive task type.
For example, if there is an expert for a primitive task
type “telling someone’s extension number”, “telling
person A’s extension number” is a primitive task.
By performing a series of primitive tasks, a com-
plicated task can be performed. For example, a mu-
seum guide robot can perform “explaining object B”
by executing “moving to B” and “giving an explana-
tion on B”. Among the experts, a small number of
experts can perform tasks at one time. Such experts
are called being in charge.
Each expert holds information on the progress of
the primitive task. It includes task-type-independent
information, such as which action in this primitive
task is being performed and whether the previous
robot action finished, and task-type-dependent in-
formation such as the user intention understanding
</bodyText>
<figureCaption confidence="0.998309">
Figure 1: Architecture for RIME-Based Systems
</figureCaption>
<bodyText confidence="0.999651272727272">
results and dialogue history. The contents and the
data structure for the task-type-dependent informa-
tion for each expert can be designed by the system
developer.
Experts are classified into system-initiative task
experts and user-initiative task experts. In this pa-
per, the initiative of a task means who can initiate
the task. For example, the task “understanding a
request for weather information” is a user-initiative
task, and the task “providing weather information”
is a system-initiative task.
In RIME, executing multiple tasks in parallel be-
comes possible by making multiple experts take
charge. To check whether two experts can take
charge simultaneously, we currently use two fea-
tures verbal and physical. Two experts having the
same feature cannot take charge simultaneously.
The interface of experts consists of methods for
accessing its internal state. Below are some of the
task-type-dependent methods, which need to be im-
plemented by system developers.
The understand method updates the internal state
based on the user speech recognition results, us-
ing domain-dependent sentence patterns for utter-
ance understanding. This method returns a score
which indicates the plausibility the user utterance
should be dealt with by the expert. Domain selection
techniques in multi-domain spoken dialogue sys-
tems (Komatani et al., 2006) can be applied to obtain
the score. The select-action method outputs one ac-
tion based on the content of the internal state. Here,
an action is a multimodal command which includes
a text to speak and/or a physical action command.
</bodyText>
<figure confidence="0.99675647368421">
action
action
selector
execution
report
(from experts
in charge)
action
charge
/discharge
task
planner
new task information
across tasks
global
context
expert
selection
information
speech
recognition
result
score
speech
recognition
result
exec. report (to the expert
that selected the action)
action
executor
expert 1
expert 2
expert 3
expert n
understander
microphone etc. agent &amp; speech synthesizer
input
processor
</figure>
<page confidence="0.997037">
89
</page>
<bodyText confidence="0.999972571428572">
The action can be an empty action, which means do-
ing nothing. The detect-interruption method returns
a Boolean value that indicates whether the previous
user utterance is an interruption to the action being
performed when this expert is being in charge. The
handle-interruption method returns the action to be
performed after an interruption is detected. For ex-
ample, an instruction to stop the utterance can be
returned.
In the definition of these methods, experts can
access a common database called global context to
store and utilize information across domains, such
as information on humans, information on the envi-
ronment, and past dialogue topics.
</bodyText>
<subsectionHeader confidence="0.997468">
2.2 Modules Coordinating Experts
</subsectionHeader>
<bodyText confidence="0.99994425">
To exploit experts, three processes, namely the un-
derstander, the action selector, and the task planner,
work in parallel.
The understander receives output of an input pro-
cessor, which typically performs speech recogni-
tion. Each time the understander receives a user
speech recognition result from the input processor,
it performs the following process. First it dispatches
the speech recognition result to the experts in charge
and the user-initiative experts with their understand
methods, which then returns the scores mentioned
above. The expert that returns the highest score is
selected as the expert to take charge. If the selected
expert is not in charge, it tells the task planner that
the expert is selected as the user-initiative expert to
take charge. If the selected expert is in charge, it
calls the detect-interruption method of the expert. If
true is returned, it tells the action selector that an
interruption utterance is detected.
The action selector repeats the following process
for each expert being in charge in a short cycle.
When an interruption for the expert is detected, it
calls the expert’s handle-interruption method, and
it then sends the returned action to the action ex-
ecutor, which is assumed to execute multimodal ac-
tions by controlling agents, speech synthesizers, and
other modules. Otherwise, unless it is not waiting
for a user utterance, it calls the expert’s select-action
methods, and then sends the returned action to the
action executor. The returned action can be an empty
action. Note that it is assumed that the action execu-
tor can perform two or more actions in parallel when
</bodyText>
<table confidence="0.913867375">
ID task type initiative feature
A understanding weather information requests user verbal
B providing weather information agent verbal
C understanding extension number requests user verbal
D providing extension numbers agent verbal
E understanding requests for guiding to places user verbal
F moving to show the way agent physical
G explaining places agent verbal
</table>
<tableCaption confidence="0.999958">
Table 1: Experts in the Example Robotic System
</tableCaption>
<subsubsectionHeader confidence="0.351367">
Utterances and physical actions Experts in charge and tasks
</subsubsectionHeader>
<figureCaption confidence="0.984065">
Figure 2: Expert Selection in a Parallel Task Execution
Example
</figureCaption>
<bodyText confidence="0.998137041666667">
possible.
The task planner is responsible for deciding which
experts take charge and which experts do not. It
sometimes makes an expert take charge by setting
a primitive task, and sometimes it discharges an ex-
pert to cancel the execution of its primitive task. To
make such decisions, it receives several pieces of in-
formation from other modules. First it receives from
the understander information on which expert is se-
lected to understand a new utterance. It also receives
information on the finish of the primitive task from
an expert being in charge. In addition, it receives
new tasks from the experts that understand human
requests. The task planner also consults the global
context to access the information shared by the ex-
perts and the task planner. In this paper we do not
discuss the details of task planning algorithms, but
we have implemented a task planner with a simple
hierarchical planning mechanism.
There can be other processes whose output is
written in the global context. For example, a robot
and human localization process using image pro-
cessing and other sensor information processing can
be used.
</bodyText>
<figure confidence="0.9958617">
Robot: &amp;quot;A&apos;s extension number is
1234.&amp;quot;
Robot: (stop moving)
Robot: &amp;quot;The meeting room is over
there.&amp;quot;
Expert F
move to
show the
way
Human: &amp;quot;Where is the meeting
room?&amp;quot;
Robot: &amp;quot;Would you like to know
where the meeting room is?&amp;quot;
Human: &amp;quot;yes.&amp;quot;
Robot: &amp;quot;Please come this way.&amp;quot;
(start moving)
Human: &amp;quot;Tell me A&apos;s extension
number.&amp;quot;
Expert E
understand request
to show the way
Expert G
show the way
Expert C
understand
request for A&apos;s
ext. number
Expert D
tell A&apos;s ext.
number
</figure>
<page confidence="0.96194">
90
</page>
<sectionHeader confidence="0.919249" genericHeader="method">
3 Implementation as a Toolkit
</sectionHeader>
<bodyText confidence="0.999959805555555">
The flexibility of designing experts increases the
amount of effort for programming in building ex-
perts. We therefore developed RIME-TK (RIME-
ToolKit), which provides libraries that facilitate
building systems based on RIME. It is implemented
in Java, and contains an abstract expert class hier-
archy. The system developers can create new ex-
perts by extending those abstract classes. Those ab-
stract classes have frequently used functions such
as WFST-based language understanding, template-
based language generation, and frame-based dia-
logue management. RIME-TK also contains the im-
plementations of the understander and the action se-
lector. In addition, it specifies the interfaces for the
input processor, the action executor, and the task
planner. Example implementations of these mod-
ules are also included in RIME-TK. Using RIME-
TK, conversational agents can be built by creating
experts, an input processor, an action executor, and
a task planner.
As an example, we have built a robotic system,
which is supposed to work at a reception, and can
perform several small tasks such as providing ex-
tension numbers of office members and guiding to
several places near the reception such as a meeting
room and a restroom. Some experts in the system
are listed in Table 1. Fig. 2 shows an example inter-
action between a human and the robotic system that
includes parallel task execution and how experts are
charged. The detailed explanation is omitted for the
lack of the space.
By developing several other robotic systems and
spoken dialogue systems (e.g., Komatani et al.
(2006), Nakano et al. (2006), and Nishimura et al.
(2007)), we have confirmed that RIME and RIME-
TK are viable.
</bodyText>
<sectionHeader confidence="0.992558" genericHeader="method">
4 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999970571428571">
This paper presented RIME, a framework for build-
ing conversational agents. It is different from pre-
vious frameworks in that it makes it possible to
build agents that can handle interruptions and exe-
cute multiple tasks in parallel by employing experts
which have a common interface. Although the cur-
rent implementation is useful for building various
kinds of systems, we believe that preparing more
kinds of expert templates and improving expert se-
lection for understanding utterances facilitate build-
ing a wider variety of systems.
Acknowledgments We would like to thank all
people who helped us to build RIME-TK and its ap-
plications.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883454545455">
H. Asoh, T. Matsui, J. Fry, F. Asano, and S. Hayamizu.
1999. A spoken dialog system for a mobile office
robot. In Proc. Eurospeech-99, pages 1139–1142.
N. Blaylock, J. Allen, and G. Ferguson. 2002. Synchro-
nization in an asynchronous agent-based architecture
for dialogue systems. In Proc. Third SIGdial Work-
shop, pages 1–10.
J. Boye, B. A. Hockey, and M. Rayner. 2000. Asyn-
chronous dialogue management: Two case-studies. In
Proc. G¨otalog-2000.
M. Hartikainen, M. Turunen, J. Hakulinen, E.-P. Salo-
nen, and J. A. Funk. 2004. Flexible dialogue manage-
ment using distributed and dynamic dialogue control.
In Proc. Interspeech-2004, pages 197–200.
K. Komatani, N. Kanda, M. Nakano, K. Nakadai, H. Tsu-
jino, T. Ogata, and H. G. Okuno. 2006. Multi-domain
spoken dialogue system with extensibility and robust-
ness against speech recognition errors. In Proc. 7th
SIGdial Workshop, pages 9–17.
O. Lemon, A. Gruenstein, A. Battle, and S. Peters. 2002.
Multi-tasking and collaborative activities in dialogue
systems. In Proc. Third SIGdial Workshop, pages
113–124.
B. Lin, H. Wang, and L. Lee. 1999. Consistent dialogue
across concurrent topics based on an expert system
model. In Proc. Eurospeech-99, pages 1427–1430.
M. Nakano, Y. Hasegawa, K. Nakadai, T. Nakamura,
J. Takeuchi, T. Torii, H. Tsujino, N. Kanda, and H. G.
Okuno. 2005. A two-layer model for behavior and
dialogue planning in conversational service robots. In
Proc. 2005 IEEE/RSJ IROS, pages 1542–1547.
M. Nakano, A. Hoshino, J. Takeuchi, Y. Hasegawa,
T. Torii, K. Nakadai, K. Kato, and H. Tsujino. 2006.
A robot that can engage in both task-oriented and non-
task-oriented dialogues. In Proc. 2006 IEEE/RAS Hu-
manoids, pages 404–411.
Y. Nishimura, S. Minotsu, H. Dohi, M. Ishizuka,
M. Nakano, K. Funakoshi, J. Takeuchi, Y. Hasegawa,
and H. Tsujino. 2007. A markup language for describ-
ing interactive humanoid robot presentations. In Proc.
IUI-07.
I. O’Neill, P. Hanna, X. Liu, and M. McTear. 2004.
Cross domain dialogue modelling: an object-based ap-
proach. In Proc. Interspeech-2004, pages 205–208.
</reference>
<page confidence="0.999173">
91
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.822636">
<title confidence="0.988205">A Framework for Building Conversational Agents Based on a Multi-Expert Model</title>
<author confidence="0.962562">Mikio Nakano</author>
<author confidence="0.962562">Kotaro Funakoshi</author>
<author confidence="0.962562">Yuji Hasegawa</author>
<author confidence="0.962562">Hiroshi</author>
<affiliation confidence="0.986871">Honda Research Institute Japan Co.,</affiliation>
<address confidence="0.99051">8-1 Honcho, Wako, Saitama 351-0188,</address>
<email confidence="0.994327">funakoshi,yuji.hasegawa,</email>
<abstract confidence="0.992558333333333">This paper presents a novel framework for building symbol-level control modules of animated agents and robots having a spoken dialogue interface. It features distributed modules called experts each of which is specialized to perform certain kinds of tasks. A common interface that all experts must support is specified, and any kind of expert can be incorporated if it has the interface. Several modules running in parallel coordinate the experts by accessing them through the interface, so that the whole system can achieve flexible control, such as interruption handling and parallel task execution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Asoh</author>
<author>T Matsui</author>
<author>J Fry</author>
<author>F Asano</author>
<author>S Hayamizu</author>
</authors>
<title>A spoken dialog system for a mobile office robot.</title>
<date>1999</date>
<booktitle>In Proc. Eurospeech-99,</booktitle>
<pages>1139--1142</pages>
<contexts>
<context position="2505" citStr="Asoh et al., 1999" startWordPosition="392" endWordPosition="395">ed for a variety of tasks, various strategies for dialogue and task planning should be able to be incorporated. Although a number of models for conversational agents have been proposed, no model has all of the above properties. Several multi-domain dialogue system models have been proposed and they are extensible, but it is not clear how they handle interruptions to system utterances and actions (e.g., O’Neill et al. (2004), Lin et al. (1999), and Hartikainen et al. (2004)). There are several spoken dialogue agents and robots that can handle interruptions thanks to their asynchronous control (Asoh et al., 1999; Boye et al., 2000; Blaylock et al., 2002; Lemon et al., 2002), they do not focus on making it easy to add new dialogue domains with a variety of dialogue strategies. This paper presents a framework called RIME (Robot Intelligence based on Multiple Experts), which employs modules called experts.1 Each expert is specialized for achieving certain kinds of tasks by performing physical actions and engaging in dialogues. It corresponds to the symbol-level control module of a system that can engage in tasks in a single small domain, and it employs fixed control strategies. Only some of the experts </context>
</contexts>
<marker>Asoh, Matsui, Fry, Asano, Hayamizu, 1999</marker>
<rawString>H. Asoh, T. Matsui, J. Fry, F. Asano, and S. Hayamizu. 1999. A spoken dialog system for a mobile office robot. In Proc. Eurospeech-99, pages 1139–1142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Blaylock</author>
<author>J Allen</author>
<author>G Ferguson</author>
</authors>
<title>Synchronization in an asynchronous agent-based architecture for dialogue systems.</title>
<date>2002</date>
<booktitle>In Proc. Third SIGdial Workshop,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="2547" citStr="Blaylock et al., 2002" startWordPosition="400" endWordPosition="403">ategies for dialogue and task planning should be able to be incorporated. Although a number of models for conversational agents have been proposed, no model has all of the above properties. Several multi-domain dialogue system models have been proposed and they are extensible, but it is not clear how they handle interruptions to system utterances and actions (e.g., O’Neill et al. (2004), Lin et al. (1999), and Hartikainen et al. (2004)). There are several spoken dialogue agents and robots that can handle interruptions thanks to their asynchronous control (Asoh et al., 1999; Boye et al., 2000; Blaylock et al., 2002; Lemon et al., 2002), they do not focus on making it easy to add new dialogue domains with a variety of dialogue strategies. This paper presents a framework called RIME (Robot Intelligence based on Multiple Experts), which employs modules called experts.1 Each expert is specialized for achieving certain kinds of tasks by performing physical actions and engaging in dialogues. It corresponds to the symbol-level control module of a system that can engage in tasks in a single small domain, and it employs fixed control strategies. Only some of the experts take charge in understanding user utteranc</context>
</contexts>
<marker>Blaylock, Allen, Ferguson, 2002</marker>
<rawString>N. Blaylock, J. Allen, and G. Ferguson. 2002. Synchronization in an asynchronous agent-based architecture for dialogue systems. In Proc. Third SIGdial Workshop, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boye</author>
<author>B A Hockey</author>
<author>M Rayner</author>
</authors>
<title>Asynchronous dialogue management: Two case-studies.</title>
<date>2000</date>
<booktitle>In Proc. G¨otalog-2000.</booktitle>
<contexts>
<context position="2524" citStr="Boye et al., 2000" startWordPosition="396" endWordPosition="399"> tasks, various strategies for dialogue and task planning should be able to be incorporated. Although a number of models for conversational agents have been proposed, no model has all of the above properties. Several multi-domain dialogue system models have been proposed and they are extensible, but it is not clear how they handle interruptions to system utterances and actions (e.g., O’Neill et al. (2004), Lin et al. (1999), and Hartikainen et al. (2004)). There are several spoken dialogue agents and robots that can handle interruptions thanks to their asynchronous control (Asoh et al., 1999; Boye et al., 2000; Blaylock et al., 2002; Lemon et al., 2002), they do not focus on making it easy to add new dialogue domains with a variety of dialogue strategies. This paper presents a framework called RIME (Robot Intelligence based on Multiple Experts), which employs modules called experts.1 Each expert is specialized for achieving certain kinds of tasks by performing physical actions and engaging in dialogues. It corresponds to the symbol-level control module of a system that can engage in tasks in a single small domain, and it employs fixed control strategies. Only some of the experts take charge in unde</context>
</contexts>
<marker>Boye, Hockey, Rayner, 2000</marker>
<rawString>J. Boye, B. A. Hockey, and M. Rayner. 2000. Asynchronous dialogue management: Two case-studies. In Proc. G¨otalog-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hartikainen</author>
<author>M Turunen</author>
<author>J Hakulinen</author>
<author>E-P Salonen</author>
<author>J A Funk</author>
</authors>
<title>Flexible dialogue management using distributed and dynamic dialogue control.</title>
<date>2004</date>
<booktitle>In Proc. Interspeech-2004,</booktitle>
<pages>197--200</pages>
<contexts>
<context position="2365" citStr="Hartikainen et al. (2004)" startWordPosition="371" endWordPosition="374">in parallel when possible. For example, robots should be 88 able to engage in a dialogue while moving. (4) Extensibility: Since the agents can be used for a variety of tasks, various strategies for dialogue and task planning should be able to be incorporated. Although a number of models for conversational agents have been proposed, no model has all of the above properties. Several multi-domain dialogue system models have been proposed and they are extensible, but it is not clear how they handle interruptions to system utterances and actions (e.g., O’Neill et al. (2004), Lin et al. (1999), and Hartikainen et al. (2004)). There are several spoken dialogue agents and robots that can handle interruptions thanks to their asynchronous control (Asoh et al., 1999; Boye et al., 2000; Blaylock et al., 2002; Lemon et al., 2002), they do not focus on making it easy to add new dialogue domains with a variety of dialogue strategies. This paper presents a framework called RIME (Robot Intelligence based on Multiple Experts), which employs modules called experts.1 Each expert is specialized for achieving certain kinds of tasks by performing physical actions and engaging in dialogues. It corresponds to the symbol-level cont</context>
</contexts>
<marker>Hartikainen, Turunen, Hakulinen, Salonen, Funk, 2004</marker>
<rawString>M. Hartikainen, M. Turunen, J. Hakulinen, E.-P. Salonen, and J. A. Funk. 2004. Flexible dialogue management using distributed and dynamic dialogue control. In Proc. Interspeech-2004, pages 197–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Komatani</author>
<author>N Kanda</author>
<author>M Nakano</author>
<author>K Nakadai</author>
<author>H Tsujino</author>
<author>T Ogata</author>
<author>H G Okuno</author>
</authors>
<title>Multi-domain spoken dialogue system with extensibility and robustness against speech recognition errors.</title>
<date>2006</date>
<booktitle>In Proc. 7th SIGdial Workshop,</booktitle>
<pages>9--17</pages>
<contexts>
<context position="7154" citStr="Komatani et al., 2006" startWordPosition="1117" endWordPosition="1120">o experts having the same feature cannot take charge simultaneously. The interface of experts consists of methods for accessing its internal state. Below are some of the task-type-dependent methods, which need to be implemented by system developers. The understand method updates the internal state based on the user speech recognition results, using domain-dependent sentence patterns for utterance understanding. This method returns a score which indicates the plausibility the user utterance should be dealt with by the expert. Domain selection techniques in multi-domain spoken dialogue systems (Komatani et al., 2006) can be applied to obtain the score. The select-action method outputs one action based on the content of the internal state. Here, an action is a multimodal command which includes a text to speak and/or a physical action command. action action selector execution report (from experts in charge) action charge /discharge task planner new task information across tasks global context expert selection information speech recognition result score speech recognition result exec. report (to the expert that selected the action) action executor expert 1 expert 2 expert 3 expert n understander microphone e</context>
<context position="13847" citStr="Komatani et al. (2006)" startWordPosition="2191" endWordPosition="2194">s an example, we have built a robotic system, which is supposed to work at a reception, and can perform several small tasks such as providing extension numbers of office members and guiding to several places near the reception such as a meeting room and a restroom. Some experts in the system are listed in Table 1. Fig. 2 shows an example interaction between a human and the robotic system that includes parallel task execution and how experts are charged. The detailed explanation is omitted for the lack of the space. By developing several other robotic systems and spoken dialogue systems (e.g., Komatani et al. (2006), Nakano et al. (2006), and Nishimura et al. (2007)), we have confirmed that RIME and RIMETK are viable. 4 Concluding Remarks This paper presented RIME, a framework for building conversational agents. It is different from previous frameworks in that it makes it possible to build agents that can handle interruptions and execute multiple tasks in parallel by employing experts which have a common interface. Although the current implementation is useful for building various kinds of systems, we believe that preparing more kinds of expert templates and improving expert selection for understanding u</context>
</contexts>
<marker>Komatani, Kanda, Nakano, Nakadai, Tsujino, Ogata, Okuno, 2006</marker>
<rawString>K. Komatani, N. Kanda, M. Nakano, K. Nakadai, H. Tsujino, T. Ogata, and H. G. Okuno. 2006. Multi-domain spoken dialogue system with extensibility and robustness against speech recognition errors. In Proc. 7th SIGdial Workshop, pages 9–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>A Gruenstein</author>
<author>A Battle</author>
<author>S Peters</author>
</authors>
<title>Multi-tasking and collaborative activities in dialogue systems.</title>
<date>2002</date>
<booktitle>In Proc. Third SIGdial Workshop,</booktitle>
<pages>113--124</pages>
<contexts>
<context position="2568" citStr="Lemon et al., 2002" startWordPosition="404" endWordPosition="407">d task planning should be able to be incorporated. Although a number of models for conversational agents have been proposed, no model has all of the above properties. Several multi-domain dialogue system models have been proposed and they are extensible, but it is not clear how they handle interruptions to system utterances and actions (e.g., O’Neill et al. (2004), Lin et al. (1999), and Hartikainen et al. (2004)). There are several spoken dialogue agents and robots that can handle interruptions thanks to their asynchronous control (Asoh et al., 1999; Boye et al., 2000; Blaylock et al., 2002; Lemon et al., 2002), they do not focus on making it easy to add new dialogue domains with a variety of dialogue strategies. This paper presents a framework called RIME (Robot Intelligence based on Multiple Experts), which employs modules called experts.1 Each expert is specialized for achieving certain kinds of tasks by performing physical actions and engaging in dialogues. It corresponds to the symbol-level control module of a system that can engage in tasks in a single small domain, and it employs fixed control strategies. Only some of the experts take charge in understanding user utterances and decide actions</context>
</contexts>
<marker>Lemon, Gruenstein, Battle, Peters, 2002</marker>
<rawString>O. Lemon, A. Gruenstein, A. Battle, and S. Peters. 2002. Multi-tasking and collaborative activities in dialogue systems. In Proc. Third SIGdial Workshop, pages 113–124.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Lin</author>
<author>H Wang</author>
<author>L Lee</author>
</authors>
<title>Consistent dialogue across concurrent topics based on an expert system model.</title>
<date>1999</date>
<booktitle>In Proc. Eurospeech-99,</booktitle>
<pages>1427--1430</pages>
<contexts>
<context position="2334" citStr="Lin et al. (1999)" startWordPosition="366" endWordPosition="369">execute multiple tasks in parallel when possible. For example, robots should be 88 able to engage in a dialogue while moving. (4) Extensibility: Since the agents can be used for a variety of tasks, various strategies for dialogue and task planning should be able to be incorporated. Although a number of models for conversational agents have been proposed, no model has all of the above properties. Several multi-domain dialogue system models have been proposed and they are extensible, but it is not clear how they handle interruptions to system utterances and actions (e.g., O’Neill et al. (2004), Lin et al. (1999), and Hartikainen et al. (2004)). There are several spoken dialogue agents and robots that can handle interruptions thanks to their asynchronous control (Asoh et al., 1999; Boye et al., 2000; Blaylock et al., 2002; Lemon et al., 2002), they do not focus on making it easy to add new dialogue domains with a variety of dialogue strategies. This paper presents a framework called RIME (Robot Intelligence based on Multiple Experts), which employs modules called experts.1 Each expert is specialized for achieving certain kinds of tasks by performing physical actions and engaging in dialogues. It corre</context>
</contexts>
<marker>Lin, Wang, Lee, 1999</marker>
<rawString>B. Lin, H. Wang, and L. Lee. 1999. Consistent dialogue across concurrent topics based on an expert system model. In Proc. Eurospeech-99, pages 1427–1430. M. Nakano, Y. Hasegawa, K. Nakadai, T. Nakamura, J. Takeuchi, T. Torii, H. Tsujino, N. Kanda, and H. G. Okuno. 2005. A two-layer model for behavior and dialogue planning in conversational service robots. In Proc. 2005 IEEE/RSJ IROS, pages 1542–1547.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nakano</author>
<author>A Hoshino</author>
<author>J Takeuchi</author>
<author>Y Hasegawa</author>
<author>T Torii</author>
<author>K Nakadai</author>
<author>K Kato</author>
<author>H Tsujino</author>
</authors>
<title>A robot that can engage in both task-oriented and nontask-oriented dialogues.</title>
<date>2006</date>
<booktitle>In Proc. 2006 IEEE/RAS Humanoids,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="13869" citStr="Nakano et al. (2006)" startWordPosition="2195" endWordPosition="2198">ilt a robotic system, which is supposed to work at a reception, and can perform several small tasks such as providing extension numbers of office members and guiding to several places near the reception such as a meeting room and a restroom. Some experts in the system are listed in Table 1. Fig. 2 shows an example interaction between a human and the robotic system that includes parallel task execution and how experts are charged. The detailed explanation is omitted for the lack of the space. By developing several other robotic systems and spoken dialogue systems (e.g., Komatani et al. (2006), Nakano et al. (2006), and Nishimura et al. (2007)), we have confirmed that RIME and RIMETK are viable. 4 Concluding Remarks This paper presented RIME, a framework for building conversational agents. It is different from previous frameworks in that it makes it possible to build agents that can handle interruptions and execute multiple tasks in parallel by employing experts which have a common interface. Although the current implementation is useful for building various kinds of systems, we believe that preparing more kinds of expert templates and improving expert selection for understanding utterances facilitate b</context>
</contexts>
<marker>Nakano, Hoshino, Takeuchi, Hasegawa, Torii, Nakadai, Kato, Tsujino, 2006</marker>
<rawString>M. Nakano, A. Hoshino, J. Takeuchi, Y. Hasegawa, T. Torii, K. Nakadai, K. Kato, and H. Tsujino. 2006. A robot that can engage in both task-oriented and nontask-oriented dialogues. In Proc. 2006 IEEE/RAS Humanoids, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Nishimura</author>
<author>S Minotsu</author>
<author>H Dohi</author>
<author>M Ishizuka</author>
<author>M Nakano</author>
<author>K Funakoshi</author>
<author>J Takeuchi</author>
<author>Y Hasegawa</author>
<author>H Tsujino</author>
</authors>
<title>A markup language for describing interactive humanoid robot presentations.</title>
<date>2007</date>
<booktitle>In Proc. IUI-07.</booktitle>
<contexts>
<context position="13898" citStr="Nishimura et al. (2007)" startWordPosition="2200" endWordPosition="2203">h is supposed to work at a reception, and can perform several small tasks such as providing extension numbers of office members and guiding to several places near the reception such as a meeting room and a restroom. Some experts in the system are listed in Table 1. Fig. 2 shows an example interaction between a human and the robotic system that includes parallel task execution and how experts are charged. The detailed explanation is omitted for the lack of the space. By developing several other robotic systems and spoken dialogue systems (e.g., Komatani et al. (2006), Nakano et al. (2006), and Nishimura et al. (2007)), we have confirmed that RIME and RIMETK are viable. 4 Concluding Remarks This paper presented RIME, a framework for building conversational agents. It is different from previous frameworks in that it makes it possible to build agents that can handle interruptions and execute multiple tasks in parallel by employing experts which have a common interface. Although the current implementation is useful for building various kinds of systems, we believe that preparing more kinds of expert templates and improving expert selection for understanding utterances facilitate building a wider variety of sy</context>
</contexts>
<marker>Nishimura, Minotsu, Dohi, Ishizuka, Nakano, Funakoshi, Takeuchi, Hasegawa, Tsujino, 2007</marker>
<rawString>Y. Nishimura, S. Minotsu, H. Dohi, M. Ishizuka, M. Nakano, K. Funakoshi, J. Takeuchi, Y. Hasegawa, and H. Tsujino. 2007. A markup language for describing interactive humanoid robot presentations. In Proc. IUI-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I O’Neill</author>
<author>P Hanna</author>
<author>X Liu</author>
<author>M McTear</author>
</authors>
<title>Cross domain dialogue modelling: an object-based approach.</title>
<date>2004</date>
<booktitle>In Proc. Interspeech-2004,</booktitle>
<pages>205--208</pages>
<marker>O’Neill, Hanna, Liu, McTear, 2004</marker>
<rawString>I. O’Neill, P. Hanna, X. Liu, and M. McTear. 2004. Cross domain dialogue modelling: an object-based approach. In Proc. Interspeech-2004, pages 205–208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>