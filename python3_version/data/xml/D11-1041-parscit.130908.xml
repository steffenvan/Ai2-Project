<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000289">
<title confidence="0.988603">
Corpus-Guided Sentence Generation of Natural Images
</title>
<author confidence="0.986512">
Yezhou Yang † and Ching Lik Teo † and Hal Daum´e III and Yiannis Aloimonos
</author>
<affiliation confidence="0.998314">
University of Maryland Institute for Advanced Computer Studies
</affiliation>
<address confidence="0.753007">
College Park, Maryland 20742, USA
</address>
<email confidence="0.992651">
{yzyang, cteo, hal, yiannis}@umiacs.umd.edu
</email>
<sectionHeader confidence="0.998561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995925">
We propose a sentence generation strategy
that describes images by predicting the most
likely nouns, verbs, scenes and prepositions
that make up the core sentence structure. The
input are initial noisy estimates of the objects
and scenes detected in the image using state of
the art trained detectors. As predicting actions
from still images directly is unreliable, we use
a language model trained from the English Gi-
gaword corpus to obtain their estimates; to-
gether with probabilities of co-located nouns,
scenes and prepositions. We use these esti-
mates as parameters on a HMM that models
the sentence generation process, with hidden
nodes as sentence components and image de-
tections as the emissions. Experimental re-
sults show that our strategy of combining vi-
sion and language produces readable and de-
scriptive sentences compared to naive strate-
gies that use vision alone.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999660181818182">
What happens when you see a picture? The most
natural thing would be to describe it using words:
using speech or text. This description of an image is
the output of an extremely complex process that in-
volves: 1) perception in the Visual space, 2) ground-
ing to World Knowledge in the Language Space and
3) speech/text production (see Fig. 1). Each of these
components are challenging in their own right and
are still considered open problems in the vision and
linguistics fields. In this paper, we introduce a com-
putational framework that attempts to integrate these
</bodyText>
<note confidence="0.468381">
†indicates equal contribution.
</note>
<figureCaption confidence="0.999915">
Figure 1: The processes involved for describing a scene.
</figureCaption>
<bodyText confidence="0.999876333333333">
components together. Our hypothesis is based on
the assumption that natural images accurately reflect
common everyday scenarios which are captured in
language. For example, knowing that boats usually
occur over water will enable us to constrain the
possible scenes a boat can occur and exclude highly
unlikely ones – street, highway. It also en-
ables us to predict likely actions (Verbs) given the
current object detections in the image: detecting a
dog with a person will likely induce walk rather
than swim, jump, fly. Key to our approach is
the use of a large generic corpus such as the English
Gigaword [Graff, 2003] as the semantic grounding
to predict and correct the initial and often noisy vi-
sual detections of an image to produce a reasonable
sentence that succinctly describes the image.
In order to get an idea of the difficulty of this
task, it is important to first define what makes up
</bodyText>
<page confidence="0.984553">
444
</page>
<note confidence="0.981661">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 444–454,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999518">
Figure 2: Illustration of various perceptual challenges for
sentence generation for images. (a) Different images with
semantically the same content. (b) Pose relates ambigu-
ously to actions in real images. See text for details.
</figureCaption>
<bodyText confidence="0.999609425">
a description of an image. Based on our observa-
tions of annotated image data (see Fig. 4), a de-
scriptive sentence for an image must contain at min-
imum: 1) the important objects (Nouns) that partic-
ipate in the image, 2) Some description of the ac-
tions (Verbs) associated with these objects, 3) the
scene where this image was taken and 4) the prepo-
sition that relates the objects to the scene. That is, a
quadruplet of T = {n, v, s, p} (Noun-Verb-Scene-
Preposition) that represents the core sentence struc-
ture. Generating a sentence from this quadruplet is
obviously a simplification from state of the art gen-
eration work, but as we will show in the experimen-
tal results (sec. 4), it is sufficient to describe im-
ages. The key challenge is that detecting objects, ac-
tions and scenes directly from images is often noisy
and unreliable. We illustrate this using example im-
ages from the Pascal-Visual Object Classes (VOC)
2008 challenge [Everingham et al., 2008]. First,
Fig. 2(a) shows the variability of images in their raw
image representations: pixels, edges and local fea-
tures. This makes it difficult for state of the art ob-
ject detectors [Felzenszwalb et al., 2010; Schwartz
et al., 2009] to reliably detect important objects in
the scene: boat, humans and water – average preci-
sion scores reported in [Felzenszwalb et al., 2010]
manages around 42% for humans and only 11% for
boat over a dataset of almost 5000 images in 20 ob-
ject categories. Yet, these images are semantically
similar in terms of their high level description. Sec-
ond, cognitive studies [Urgesi et al., 2006; Kourtzi,
2004] have proposed that inferring the action from
static images (known as an “implied action”) is of-
ten achieved by detecting the pose of humans in the
image: the position of the limbs with respect to one
another, under the assumption that a unique pose oc-
curs for a unique action. Clearly, this assumption
is weak as 1) similar actions may be represented by
different poses due to the inherent dynamic nature of
the action itself: e.g. walking a dog and 2) different
actions may have the same pose: e.g. walking a dog
versus running (Fig. 2(b)). The missing component
here is whether the key object (dog) under interac-
tion is considered. Recent works [Yao and Fei-Fei,
2010; Yang et al., 2010] that used poses for recog-
nition of actions achieved 70% and 61% accuracy
respectively under extremely limited testing condi-
tions with only 5-6 action classes each. Finally, state
of the art scene detectors [Oliva and Torralba, 2001;
Torralba et al., 2003] need to have enough represen-
tative training examples of scenes from pre-defined
scene classes for a classification to be successful –
with a reported average precision of 83.7% tested
over a dataset of 2600 images.
Addressing all these visual challenges is clearly
a formidable task which is beyond the scope of this
paper. Our focus instead is to show that with the
addition of language to ground the noisy initial vi-
sual detections, we are able to improve the qual-
ity of the generated sentence as a faithful descrip-
tion of the image. In particular, we show that it
is possible to avoid predicting actions directly from
images – which is still unreliable – and to use the
corpus instead to guide our predictions. Our pro-
posed strategy is also generic, that is, we make no
prior assumptions on the image domain considered.
While other works (sec. 2) depend on strong anno-
tations between images and text to ground their pre-
dictions (and to remove wrong sentences), we show
that a large generic corpus is also able to provide
the same grounding over larger domains of images.
It represents a relatively new style of learning: dis-
tant supervision [Liang et al., 2009; Mann and Mc-
callum, 2007]. Here, we do not require “labeled”
data containing images and captions but only sep-
arate data from each side. Another contribution is
a computationally feasible way via dynamic pro-
gramming to determine the most likely quadruplet
T* = {n*, v*, s*, p*} that describes the image for
generating possible sentences.
</bodyText>
<page confidence="0.999492">
445
</page>
<sectionHeader confidence="0.999882" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999983413043478">
Recently, several works from the Computer Vision
domain have attempted to use language to aid im-
age scene understanding. [Kojima et al., 2000] used
predefined production rules to describe actions in
videos. [Berg et al., 2004] processed news captions
to discover names associated with faces in the im-
ages, and [Jie et al., 2009] extended this work to as-
sociate poses detected from images with the verbs
in the captions. Both approaches use annotated ex-
amples from a limited news caption corpus to learn
a joint image-text model so that one can annotate
new unknown images with textual information eas-
ily. Neither of these works have been tested on com-
plex everyday images where the large variations of
objects and poses makes it nearly impossible to learn
a more general model. In addition, no attempt was
made to generate a descriptive sentence from the
learned model. The work of [Farhadi et al., 2010] at-
tempts to “generate” sentences by first learning from
a set of human annotated examples, and produc-
ing the same sentence if both images and sentence
share common properties in terms of their triplets:
(Nouns-Verbs-Scenes). No attempt was made to
generate novel sentences from images beyond what
has been annotated by humans. [Yao et al., 2010]
has recently introduced a framework for parsing im-
ages/videos to textual description that requires sig-
nificant annotated data, a requirement that our pro-
posed approach avoids.
Natural language generation (NLG) is a long-
standing problem. Classic approaches [Traum et al.,
2003] are based on three steps: selection, planning
and realization. A common challenge in generation
problems is the question of: what is the input? Re-
cently, approaches for generation have focused on
formal specification inputs, such as the output of the-
orem provers [McKeown, 2009] or databases [Gol-
land et al., 2010]. Most of the effort in those ap-
proaches has focused on selection and realization.
We address a tangential problem that has not re-
ceived much attention in the generation literature:
how to deal with noisy inputs. In our case, the inputs
themselves are often uncertain (due to misrecogni-
tions by object/scene detectors) and the content se-
lection and realization needs to take this uncertainty
into account.
</bodyText>
<sectionHeader confidence="0.972335" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.996874916666667">
Our approach is summarized in Fig. 3. The input is a
test image where we detect objects and scenes using
trained detection algorithms [Felzenszwalb et al.,
2010; Torralba et al., 2003]. To keep the framework
computationally tractable, we limit the elements of
the quadruplet (Nouns-Verbs-Scenes-Prepositions)
to come from a finite set of objects N, actions V,
scenes S and prepositions P classes that are com-
monly encountered. They are summarized in Ta-
ble. 1. In addition, the sentence that is generated
for each image is limited to at most two objects oc-
curring in a unique scene.
</bodyText>
<figureCaption confidence="0.98717575">
Figure 3: Overview of our approach. (a) Detect objects
and scenes from input image. (b) Estimate optimal sen-
tence structure quadruplet T*. (c) Generating a sentence
from T*.
</figureCaption>
<bodyText confidence="0.959961857142857">
Denoting the current test image as I, the initial
visual processing first detects objects n E N and
scenes s E S using these detectors to compute
Pr(n|I) and Pr(s|I), the probabilities that object
n and scene s exist under I. From the observation
that an action can often be predicted by its key ob-
jects, Nk = {n1, n2, · · · , ni}, ni E N that partici-
pate in the action, we use a trained Language model
Lm to estimate Pr(v|Nk). Lm is also used to com-
pute Pr(s|n, v), the predicted scene using the cor-
pus given the object and verb; and Pr(p|s), the pre-
dicted preposition given the scene. This process is
repeated over all n, v, s, p where we used a modi-
fied HMM inference scheme to determine the most
likely quadruplet: T* = {n*, v*, s*, p*} that makes
up the core sentence structure. Using the contents
and structure of T*, an appropriate sentence is then
generated that describes the image. In the following
sections, we first introduce the image dataset used
for testing followed by details of how these compo-
nents are derived.
</bodyText>
<page confidence="0.992792">
446
</page>
<table confidence="0.877125625">
Objects n E N Actions v E V Scenes s E S Preps p E P
’aeroplane’ ’bicycle’ ’bird’ ’sit’ ’stand’ ’park’ ’airport’ ’in’ ’at’ ’above’
’boat’ ’bottle’ ’bus’ ’car’ ’ride’ ’hold’ ’wear’ ’field’ ’around’ ’behind’
’cat’ ’chair’ ’cow’ ’table’ ’pose’ ’fly’ ’lie’ ’lay’ ’highway’ ’below’ ’beside’
’dog’ ’horse’, ’motorbike’ ’smile’ ’live’ ’walk’ ’lake’ ’room’ ’between’
’person’ ’pottedplant’ ’graze’ ’drive’ ’play’ ’sky’ ’street’ ’before’ ’to’
’sheep’ ’sofa’ ’train’ ’eat’ ’cover’ ’train’ ’track’ ’under’ ’on’
’tvmonitor’ ’close’ ...
</table>
<tableCaption confidence="0.999899">
Table 1: The set of objects, actions (first 20), scenes and preposition classes considered
</tableCaption>
<figure confidence="0.998233">
(a) (b)
</figure>
<figureCaption confidence="0.993764">
Figure 4: Samples of images with corresponding annota-
tions from the UIUC scene description dataset.
</figureCaption>
<subsectionHeader confidence="0.999195">
3.1 Image Dataset
</subsectionHeader>
<bodyText confidence="0.999889071428571">
We use the UIUC Pascal Sentence dataset, first in-
troduced in [Farhadi et al., 2010] and available on-
line1. It contains 1000 images taken from a sub-
set of the Pascal-VOC 2008 challenge image dataset
and are hand annotated with sentences that describe
the image by paid human annotators using Ama-
zon Mechanical Turk. Fig. 4 shows some sample
images with their annotations. There are 5 anno-
tations per image, and each annotation is usually
short – around 10 words long. We randomly selected
900 images (4500 sentences) as the learning corpus
to construct the verb and scene sets, {V, S} as de-
scribed in sec. 3.3, and kept the remaining 100 im-
ages for testing and evaluation.
</bodyText>
<subsectionHeader confidence="0.999884">
3.2 Object and Scene Detections from Images
</subsectionHeader>
<bodyText confidence="0.998120714285714">
We use the Pascal-VOC 2008 trained object detec-
tors [Felzenszwalb et al., 2008] of 20 common ev-
eryday object classes that are defined in N. Each of
the detectors are essentially SVM classifiers trained
on a large number of the objects’ image represen-
tations from a large variety of sources. Although
20 classes may seem small, their existence in many
</bodyText>
<footnote confidence="0.972622">
1http://vision.cs.uiuc.edu/pascal-sentences/
</footnote>
<figureCaption confidence="0.9939542">
Figure 5: (a) [Top] The part based object detector from
[Felzenszwalb et al., 2010]. [Bottom] The graphical
model representation of an object, for e.g. a bike. (b)
Examples of GIST gradients: (left) an outdoor scene vs
(right) an indoor scene [Torralba et al., 2003].
</figureCaption>
<bodyText confidence="0.999962041666667">
natural images (e.g. humans, cars and plants) makes
them particularly important for our task, since hu-
mans tend to describe these common objects as well.
As object representations, the part-based descriptor
of [Felzenszwalb et al., 2010] is used. This repre-
sentation decomposes any object, e.g. a cow, into
its constituent parts: head, torso, legs, which are
shared by other objects in a hierarchical manner.
At each level, image gradient orientations are com-
puted. The relationship between each parts is mod-
eled probabilistically using graphical models where
parts are the nodes and the edges are the conditional
probabilities that relate their spatial compatibility
(Fig. 5(a)). For example, in a cow, the probability
of finding the torso near the head is higher than find-
ing the legs near the head. This model’s intuition lies
in the assumption that objects can be deformed but
the relative position of each constituent parts should
remain the same. We convert the object detec-
tion scores to probabilities using Platt’s method [Lin
et al., 2007] which is numerically more stable to ob-
tain Pr(n|I). The parameters of Platt’s method are
obtained by estimating the number of positives and
negatives from the UIUC annotated dataset, from
</bodyText>
<page confidence="0.993252">
447
</page>
<bodyText confidence="0.999997047619048">
which we determine the appropriate probabilistic
threshold, which gives us approximately 50% recall
and precision.
For detecting scenes defined in S, we use the
GIST-based scene descriptor of [Torralba et al.,
2003]. GIST computes the windowed 2D Gabor fil-
ter responses of an input image. The responses of
Gabor filters (4 scales and 6 orientations) encode the
texture gradients that describe the local properties
of the image. Averaging out these responses over
larger spatial regions gives us a set of global im-
age properties. These high dimensional responses
are then reprojected to a low dimensional space via
PCA, where the number of principal components are
obtained empirically from training scenes. This rep-
resentation forms the GIST descriptor of an image
(Fig. 5(b)) which is used to train a set of SVM clas-
sifiers for each scene class in S. Again, Pr(s|I) is
computed from the SVM scores using [Lin et al.,
2007]. The set of common scenes defined in S is
learned from the UIUC annotated data (sec. 3.3).
</bodyText>
<subsectionHeader confidence="0.931119">
3.3 Corpus-Guided Predictions
</subsectionHeader>
<figureCaption confidence="0.98605675">
Figure 6: (a) Selecting the ROOT verb from the depen-
dency parse ride reveals its subject woman and direct
object bicycle. (b) Selecting the head noun (PMOD)
as the scene street reveals ADV as the preposition on
</figureCaption>
<bodyText confidence="0.999397827586207">
Predicting Verbs: The key component of our ap-
proach is the trained language model Lm that pre-
dicts the most likely verb v, associated with the ob-
jects Nk detected in the image. Since it is possi-
ble that different verbs may be associated with vary-
ing number of object arguments, we limit ourselves
to verbs that take on at most two objects (or more
specifically two noun phrase arguments) as a sim-
plifying assumption: Nk = {n1, n2} where n2 can
be NULL. That is, n1 and n2 are the subject and
direct objects associated with v E V. Using this as-
sumption, we can construct the set of verbs, V. To
do this, we use human labeled descriptions of the
training images from the UIUC Pascal-VOC dataset
(sec. 3.1) as a learning corpus that allows us to deter-
mine the appropriate target verb set that is amenable
to our problem. We first apply the CLEAR parser
[Choi and Palmer, 2010] to obtain a dependency
parse of these annotations, which also performs
stemming of all the verbs and nouns in the sentence.
Next, we process all the parses to select verbs which
are marked as ROOT and check the existence of a
subject (DEP) and direct object (PMOD, OBJ) that
are linked to the ROOT verb (see Fig. 6(a)). Finally,
after removing common “stop” verbs such as {is,
are, be} we rank these verbs in terms of their oc-
currences and select the top 50 verbs which accounts
for 87.5% of the sentences in the UIUC dataset to be
in V.
</bodyText>
<table confidence="0.998511416666667">
Object class n E N Synonyms, (n)
bus autobus charabanc
double-decker jitney
motorbus motorcoach omnibus
passenger-vehicle schoolbus
trolleybus streetcar ...
chair highchair chaise daybed
throne rocker armchair
wheelchair seat ladder-back
lawn-chair fauteuil ...
bicycle bike wheel cycle velocipede
tandem mountain-bike ...
</table>
<tableCaption confidence="0.998927">
Table 2: Samples of synonyms for 3 object classes.
</tableCaption>
<bodyText confidence="0.99997680952381">
Next, we need to explain how n1 and n2 are
selected from the 20 object classes defined previ-
ously in N. Just as the 20 object classes are de-
fined visually over several different kinds of spe-
cific objects, we expand n1 and n2 in their tex-
tual descriptions using synonyms. For example,
the object class n1=aeroplane should include
the synonyms {plane, jet, fighter jet,
aircraft}, denoted as (n1). To do this, we ex-
pand each object class using their corresponding
WordNet synsets up to at most three hyponymns lev-
els. Example synonyms for some of the classes are
summarized in Table 2.
We can now compute from the Gigaword cor-
pus [Graff, 2003] the probability that a verb ex-
ists given the detected nouns, Pr(v|n1, n2). We do
this by computing the log-likelihood ratio [Dunning,
1993] , Anvn, of trigrams ((n1) , v, (n2)), computed
from each sentence in the English Gigaword corpus
[Graff, 2003]. This is done by extracting only the
words in the corpus that are defined in N and V (in-
</bodyText>
<page confidence="0.994568">
448
</page>
<bodyText confidence="0.994040673913044">
cluding their synonyms). This forms a reduced cor-
pus sequence from which we obtain our target tri-
grams. For example, the sentence:
the large brown dog chases a small young cat
around the messy room, forcing the cat to run
away towards its owner.
will be reduced to the stemmed sequence dog
chase cat cat run owner2 from which we ob-
tain the target trigram relationships: {dog chase
cat}, {cat run owner} as these trigrams re-
spect the (n1, v, n2) ordering. The log-likelihood ra-
tios, Anvn, computed for all possible ((n1) , v, (n2))
are then normalized to obtain Pr(v|n1, n2). An ex-
ample of ranked Anvn in Fig. 7(a) shows that Anvn
predicts v that makes sense: with the most likely
predictions near the top of the list.
Predicting Scenes: Just as an action is strongly
related to the objects that participate in it, a
scene can be predicted from the objects and verbs
that occur in the image. For example, detect-
ing Nk={boat, person} with v={row} would
have predicted the scene s={coast}, since boats
usually occur in water regions. To learn this rela-
tionship from the corpus, we use the UIUC dataset
to discover what are the common scenes that should
be included in S. We applied the CLEAR depen-
dency parse [Choi and Palmer, 2010] on the UIUC
data and extracted all the head nouns (PMOD) in
the PP phrases for this purpose and excluded those
nouns with prepositions (marked as ADV) such as
{with, of} which do not co-occur with scenes in
general (see Fig. 6(b)). We then ranked the remain-
ing scenes in terms of their frequency to select the
top 8 scenes used in S.
To improve recall and generalization, we expand
each of the 8 scene classes using their WordNet
synsets (s) (up to a max of three hyponymns levels).
Similar to the procedure of predicting the verbs de-
scribed above, we compute the log-likelihood ratio
of ordered bigrams, {n, (s)} and {v, (s)}: Ans and
Avs, by reducing the corpus sentence to the target
nouns, verbs and scenes defined in N, V and S. The
probabilities Pr(s|n) and Pr(v|n) are then obtained
by normalizing Ans and Avs. Under the assumption
that the priors Pr(n) and Pr(v) are independent and
applying Bayes rule, we can compute the probabil-
</bodyText>
<footnote confidence="0.961594">
2stemming is done using [Choi and Palmer, 2010]
</footnote>
<equation confidence="0.906225125">
ity that a scene co-occurs with the object and action,
Pr(s|n, v) by:
Pr(n,v|s)Pr(s)
Pr(s|n,v) =
Pr(n, v)
Pr(n|s)Pr(v|s)Pr(s)
Pr(n)Pr(v)
a Pr(s|n) x Pr(s|v) (1)
</equation>
<figureCaption confidence="0.517821192307692">
where the constant of proportionality is justified un-
der the assumption that Pr(s) is equiprobable for all
s. (1) is computed for all nouns in Nk. As shown
in Fig. 7(b), we are able to predict scenes that co-
locate with reasonable correctness given the nouns
and verbs.
Predicting Prepositions: It is straightforward to
predict the appropriate prepositions associated with
a given scene. When we construct S from the UIUC
annotated data, we simply collect and rank all the as-
sociated prepositions (ADV) in the PP phrase of the
dependency parses. We then select the top 12 prepo-
sitions used to define P. Using P, we then compute
the log-likelihood ratio of ordered bigrams, {p, (s)}
for prepositions that co-locate with the scene syn-
onyms over the corpus. Normalizing Aps yields
Pr(p|s), the probability that a preposition co-locates
with a scene. Examples of ranked Aps are shown in
Fig. 7(c). Again, we see that reasonable predictions
of p can be found.
Figure 7: Example of how ranked log-likelihood values
(in descending order) suggest a possible T: (a) Anvn for
n1 = person, n2 = bus predicts v = ride. (b) Ans
and Avs for n = bus, v = ride then jointly predicts
s = street and finally (c) Aps with s = street pre-
dicts p = on.
</figureCaption>
<subsectionHeader confidence="0.992473">
3.4 Determining T∗ using HMM inference
</subsectionHeader>
<bodyText confidence="0.926388">
Given the computed conditional probabilities:
Pr(n|I) and Pr(s|I) which are observations
from an input test image with the param-
eters of the trained language model, Lm:
</bodyText>
<page confidence="0.980316">
449
</page>
<equation confidence="0.894485285714286">
Pr(v|n1, n2), Pr(s|n, v), Pr(p|s), we seek to
find the most likely sentence structure T∗ by:
T∗ = arg max Pr(T |n, v, s, p)
n,v,s,p
= arg max {Pr(n1|I)Pr(n2|I)Pr(s|I)x
n,v,s,p
Pr(v|n1,n2)Pr(s|n,v)Pr(p|s)} (2)
</equation>
<bodyText confidence="0.962992857142857">
where the last equality holds by assuming indepen-
dence between the visual detections and corpus pre-
dictions. Obviously a brute force approach to try all
possible combinations to maximize eq. (2) will not
be feasible due to the large number of possible com-
binations: (20*21*8)*(50*20*20)*(8*20*50)*
(12 * 8) ≈ 5 x 1013. A better solution is needed.
</bodyText>
<figureCaption confidence="0.985855">
Figure 8: The HMM used for optimizing T. The relevant
transition and emission probabilities are also shown. See
text for more details.
</figureCaption>
<bodyText confidence="0.999848745454545">
Our proposed strategy is to pose the optimiza-
tion of T as a dynamic programming problem, akin
to a Hidden Markov Model (HMM) where the hid-
den states are related to the (simplified) sentence
structure we seek: T = {n1, n2, s, v, p}, and the
emissions are related to the observed detections:
{n1, n2, s} in the image if they exist. To sim-
plify our notations, as we are concerned with ob-
ject pairs we will write NN as the hidden states for
all n1, n2 pairs and nn as the corresponding emis-
sions (detections); and all object+verb pairs as hid-
den states NV. The hidden states are therefore de-
noted as: {NN, NV, S, P} with values taken from
their respective word classes from Table 1. The
emission states are {nn, s} with binary values: 1
if the detections occur or 0 otherwise. The full
HMM is summarized in Fig. 8. The rationale for
using a HMM is that we can reuse all previous com-
putation of the probabilities at each level to com-
pute the required probabilities at the current level.
From START, we assume all object pair detections
are equiprobable: Pr(NN|START) = N∗(|1
where we have added an additional NULL value for
objects (at most 1). At each NN, the HMM emits
a detection from the image and by independence
we have: Pr(nn|NN) = Pr(n1|I)Pr(n2|I). Af-
ter NN, the HMM transits to the corresponding verb
at state NV with Pr(NV|NN) = Pr(v|n1,n2) ob-
tained from the corpus statistic3. As no action detec-
tions are performed on the image, NV has no emis-
sions. The HMM then transits from NV to S with
Pr(S|NV) = Pr(s|n, v) computed from the corpus
which emits the scene detection score from the im-
age: Pr(s|S) = Pr(s|I). From S, the HMM transits
to P with Pr(P|S) = Pr(p|s) before reaching the
END state.
Comparing the HMM with eq. (2), one can see
that all the corpus and detection probabilities are
accounted for in the transition and emission prob-
abilities respectively. Optimizing T is then equiv-
alent to finding the best (most likely) path through
the HMM given the image observations using the
Viterbi algorithm which can be done in O(105) time
which is significantly faster than the naive approach.
We show in Fig. 9 (right-upper) examples of the top
viterbi paths that produce T∗ for four test images.
Note that the proposed HMM is suitable for gen-
erating sentences that contain the core components
defined in T which produces a sentence of the form
NP-VP-PP, which we will show in sec. 4 is suf-
ficient for the task of generating sentences for de-
scribing images. For more complex sentences with
more components: such as adjectives or adverbs, the
HMM can be easily extended with similar computa-
tions derived from the corpus.
</bodyText>
<subsectionHeader confidence="0.996659">
3.5 Sentence Generation
</subsectionHeader>
<bodyText confidence="0.9121495">
Given the selected sentence structure T =
{n1, n2, v, s, p}, we generate sentences using the
</bodyText>
<footnote confidence="0.984896">
3each verb, v, in NV will have 2 entries with the same value,
one for each noun.
</footnote>
<page confidence="0.993349">
450
</page>
<figureCaption confidence="0.961203">
Figure 9: Four test images (left) and results. (Right-
upper): Sentence structure T∗ predicted using Viterbi
and (Right-lower): Generated sentences. Words marked
in red are considered to be incorrect predictions. Com-
plete results are available at http://www.umiacs.umd.
edu/˜yzyang/sentence_generateOut.html.
</figureCaption>
<bodyText confidence="0.999025">
following strategy for each component:
</bodyText>
<listItem confidence="0.976873555555556">
1) We add in appropriate determiners and cardi-
nals: the, an, a, CARD, based on the content
of n1,n2 and s. For e.g., if n1 = n2, we will use
CARD=two, and modify the nouns to be in the plu-
ral form. When several possible choices are avail-
able, a random choice is made that depends on the
object detection scores: the is preferred when we
are confident of the detections while an, a is pre-
ferred otherwise.
</listItem>
<bodyText confidence="0.8981305">
2) We predict the most likely preposition inserted
between the verbs and nouns learned from the Giga-
word corpus via Pr(p|v, n) during sentence genera-
tion. For example, our method will pick the prepo-
sition at between verb sit and noun table.
3) The verb v is converted to a form that agrees
with in number with the nouns detected. The
present gerund form is preferred such as eating,
drinking, walking as it conveys that an ac-
tion is being performed in the image.
</bodyText>
<listItem confidence="0.994876888888889">
4) The sentence structure is therefore of the form:
NP-VP-PP with variations when only one object
or multiple detections of the same objects are de-
tected. A special case is when no objects are de-
tected (below the predefined threshold). No verbs
can be predicted as well. In this case, we sim-
ply generate a sentence that describes the scene
only: for e.g. This is a coast, This is
a field. Such sentences account for 20% of the
</listItem>
<bodyText confidence="0.999159333333333">
entire UIUC testing dataset which are scored lower
in our evaluation metrics (sec. 4.1) since they do not
fully describe the image content in terms of the ob-
jects and actions.
Some examples of sentences generated using this
strategy are shown in Fig. 9(right-lower).
</bodyText>
<sectionHeader confidence="0.99912" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.998998">
We performed several experiments to evaluate our
proposed approach. The different metrics used for
evaluation and comparison are also presented, fol-
lowed by a discussion of the experimental results.
</bodyText>
<subsectionHeader confidence="0.998531">
4.1 Sentence Generation Results
</subsectionHeader>
<bodyText confidence="0.999771484848485">
Three experiments are performed to evaluate the ef-
fectiveness of our approach. As a baseline, we sim-
ply generated T* directly from images without using
the corpus. There are two variants of this baseline
where we seek to determine if listing all objects in
the image is crucial for scene description. T�1 is a
baseline that uses all possible objects and scene de-
tected: T�1 = {n1, n2, · · · , nm, s} and our sentence
will be of the form: {Object 1, object 2 and
object 3 are IN the scene.} and we simply
selected IN as the only admissible preposition. For
the second baseline, T�2, we limit the number of ob-
jects to just any two: T�2 = {n1, n2, s} and the
sentence generated will be of the form {Object
1 and object 2 are IN the scene}. In the
second experiment, we applied the HMM strategy
described above but made all transition probabilities
equiprobable, removing the effects of the corpus,
and producing a sentence structure which we denote
as T*�Q. The third experiment produces the full T*
with transition probabilities learned from the corpus.
All experiments were performed on the 100 unseen
testing images from the UIUC dataset and we used
only the most likely (top) sentence generated for all
evaluation.
We use two evaluation metrics as a measure of the
accuracy of the generated sentences: 1) ROUGE-1
[Lin and Hovy, 2003] precision scores and 2) Rel-
evance and Readability of the generated sentences.
ROUGE-1 is a recall based metric that is commonly
used to measure the effectiveness of text summariza-
tion. In this work, the short descriptive sentence of
an image can be viewed as summarizing the image
</bodyText>
<page confidence="0.996102">
451
</page>
<table confidence="0.9988891">
Experiment R1,(length) Relevance Readability
Baseline 1, T ∗ 0.35,(8.2) 2.84 ± 1.40 3.64 ± 1.20
b1
Baseline 2, T ∗ 0.39,(6.8) 2.14 ± 1.13 3.94 ± 0.91
b2
HMM no cor- 0.42,(6.5) 2.44 ± 1.25 3.88 ± 1.18
pus, Tey∗
Full HMM, T∗ 0.44,(6.9) 2.51 ± 1.30 4.10 ± 1.03
Human Anno- 0.68,(10.1) 4.91 ± 0.29 4.77 ± 0.42
tation
</table>
<bodyText confidence="0.999592714285714">
content and ROUGE-1 is able to capture how well
this sentence can describe the image by comparing it
with the human annotated ground truth of the UIUC
dataset. Due to the short sentences generated, we
did not consider other ROUGE metrics (ROUGE-2,
ROUGE-SU4) which captures fluency and is not an
issue here.
</bodyText>
<tableCaption confidence="0.890006">
Table 3: Sentence generation evaluation results with hu-
</tableCaption>
<bodyText confidence="0.983499269230769">
man gold standard. Human Rl scores are averaged over
the 5 sentences using a leave one out procedure. Values
in bold are the top scores.
A main shortcoming of using ROUGE-1 is that
the generated sentences are compared only to a fi-
nite set of human labeled ground truth which ob-
viously does not capture all possible sentences that
one can generate. In other words, ROUGE-1 does
not take into account the fact that sentence genera-
tion is innately a creative process, and a better re-
call metric will be to ask humans to judge these
sentences. The second evaluation metric: Rele-
vance and Readability is therefore proposed as an
empirical measure of how much the sentence: 1)
conveys the image content (relevance) in terms of
the objects, actions and scene predicted and 2) is
grammatically correct (readability). We engaged the
services of Amazon Mechanical Turks (AMT) to
judge the generated sentences based on a discrete
scale ranging from 1–5 (low relevance/readability
to high relevance/readability). The averaged results
of ROUGE-1, R1 and mean length of the sentences
with the Relevance+Readability scores for all exper-
iments are summarized in Table 3. For comparison,
we also asked the AMTs to judge the ground truth
sentences as well.
</bodyText>
<subsectionHeader confidence="0.811325">
4.2 Discussion
</subsectionHeader>
<bodyText confidence="0.9999867">
The results reported in Table 3 reveals both the
strengths and some shortcomings of the approach
which we will briefly discuss here. Firstly, the R1
scores indicate that based on a purely summariza-
tion (unigram-overlap) point of view, the proposed
approach of using the HMM to predict T* achieves
the best results compared to all other approaches
with R1 = 0.44. This means that our sentences are
the closest in agreement with the human annotated
ground truth, correctly predicting the sentence struc-
ture components. In addition sentences generated by
T* are also succinct: with an average length of 6.9
words per sentence. However, we are still some way
off the human gold standard since we do not predict
other parts-of-speech such as adjectives and adverbs.
Given this fact, our proposed approach performance
is comparable to other state of the art summarization
work in the literature [Bonnie and Dorr, 2004].
Next, we consider the Relevance+Readability
metrics based on human judges. Interestingly, the
first baseline, Tb1* is considered the most relevant de-
scription of the image and the least readable at the
same time. This is most likely due to the fact that
this recall oriented strategy will almost certainly de-
scribe some objects but the lack of any verb descrip-
tion; and longer sentences that average 8.2 words per
sentence, makes it less readable. It is also possible
that humans tend to penalize less irrelevant objects
compared to missing objects, and further evaluations
are necessary to confirm this. Since Tb2* is limited
to two objects just like the proposed HMM, it is a
more suitable baseline for comparison. Clearly, the
results show that adding the HMM to predict the op-
timal sentence structure increases the relevance of
the produced sentence. Finally, in terms of read-
ability, T* generates the most readable sentences,
and this is achieved by leveraging on the corpus to
guide our predictions of the most reasonable nouns,
verbs, scenes and prepositions that agree with the
detections in the image.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="method">
5 Future Work
</sectionHeader>
<bodyText confidence="0.99997425">
In this work, we have introduced a computationally
feasible framework that integrates visual perception
together with semantic grounding obtained from a
large textual corpus for the purpose of generating a
descriptive sentence of an image. Experimental re-
sults show that our approach produces sentences that
are both relevant and readable. There are, however,
instances where our strategy fails to predict the ap-
</bodyText>
<page confidence="0.995663">
452
</page>
<bodyText confidence="0.984405821428571">
propriate verbs or nouns (see Fig. 9). This is due
to the fact that object/scene detections can be wrong
and noise from the corpus itself remains a problem.
Compared to human gold standards, therefore, much
work still remains in terms of detecting these objects
and scenes with high precision. Currently, at most
two object classes are used to generate simple sen-
tences which was shown in the results to have penal-
ized the relevance score of our approach. This can
be addressed by designing more complex HMMs to
handle larger numbers of object and verb classes.
Another interesting direction of future work would
be to detect salient objects, learned from training
image+corpus or eye-movement data, and to verify
if these objects aid in improving the descriptive sen-
tences we generate. Another potential application
Figure 10: Images retrieved from 3 verbal search terms:
ride,sit,fly.
of representing images using T* is that we can eas-
ily sort and retrieve images that are similar in terms
of their semantic content. This would enable us to
retrieve, for example, more relevant images given a
verbal search query such as {ride,sit,fly}, re-
turning images where these verbs are found in T*.
Some results of retrieved images based on their ver-
bal components are shown in Fig. 10: many images
with dissimilar visual content are correctly classified
based on their semantic meaning.
</bodyText>
<sectionHeader confidence="0.998258" genericHeader="conclusions">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.909177571428571">
This material is based upon work supported by
the National Science Foundation under Grant No.
1035542. In addition, the support of the Eu-
ropean Union under the Cognitive Systems pro-
gram (project POETICON) and the National Sci-
ence Foundation under the Cyberphysical Systems
Program, is gratefully acknowledged.
</bodyText>
<page confidence="0.999198">
453
</page>
<sectionHeader confidence="0.998348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871918367347">
Berg, T. L., Berg, A. C., Edwards, J., and Forsyth, D. A.
(2004). Who’s in the picture? In NIPS.
Bonnie, D. Z. and Dorr, B. (2004). Bbn/umd at duc-2004:
Topiary. In In Proceedings of the 2004 Document Un-
derstanding Conference (DUC 2004) at NLT/NAACL
2004, pages 112–119.
Choi, J. D. and Palmer, M. (2010). Robust constituent-
to-dependency conversion for english. In Proceedings
of the 9th International Workshop on Treebanks and
Linguistic Theories, pages 55–66, Tartu, Estonia.
Dunning, T. (1993). Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61–74.
Everingham, M., Van Gool, L., Williams, C. K. I., Winn,
J., and Zisserman, A. (2008). The PASCAL Visual
Object Classes Challenge 2008 (VOC2008) Results.
Farhadi, A., Hejrati, S. M. M., Sadeghi, M. A., Young, P.,
Rashtchian, C., Hockenmaier, J., and Forsyth, D. A.
(2010). Every picture tells a story: Generating sen-
tences from images. In Daniilidis, K., Maragos, P.,
and Paragios, N., editors, ECCV (4), volume 6314
of Lecture Notes in Computer Science, pages 15–29.
Springer.
Felzenszwalb, P. F., Girshick, R. B., and McAllester, D.
(2008). Discriminatively trained deformable part mod-
els, release 4. http://people.cs.uchicago.edu/ pff/latent-
release4/.
Felzenszwalb, P. F., Girshick, R. B., McAllester, D. A.,
and Ramanan, D. (2010). Object detection with dis-
criminatively trained part-based models. IEEE Trans.
Pattern Anal. Mach. Intell., 32(9):1627–1645.
Golland, D., Liang, P., and Klein, D. (2010). A game-
theoretic approach to generating spatial descriptions.
In Proceedings of EMNLP.
Graff, D. (2003). English gigaword. In Linguistic Data
Consortium, Philadelphia, PA.
Jie, L., Caputo, B., and Ferrari, V. (2009). Who’s do-
ing what: Joint modeling of names and verbs for si-
multaneous face and pose annotation. In NIPS, editor,
Advances in Neural Information Processing Systems,
NIPS. NIPS.
Kojima, A., Izumi, M., Tamura, T., and Fukunaga, K.
(2000). Generating natural language description of hu-
man behavior from video images. In Pattern Recog-
nition, 2000. Proceedings. 15th International Confer-
ence on, volume 4, pages 728 –731 vol.4.
Kourtzi, Z. (2004). But still, it moves. Trends in Cogni-
tive Sciences, 8(2):47 – 49.
Liang, P., Jordan, M. I., and Klein, D. (2009). Learning
from measurements in exponential families. In Inter-
national Conference on Machine Learning (ICML).
Lin, C. and Hovy, E. (2003). Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
NAACLHLT.
Lin, H.-T., Lin, C.-J., and Weng, R. C. (2007). A note
on platt’s probabilistic outputs for support vector ma-
chines. Mach. Learn., 68:267–276.
Mann, G. S. and Mccallum, A. (2007). Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In The 24th International Conference on
Machine Learning.
McKeown, K. (2009). Query-focused summarization us-
ing text-to-text generation: When information comes
from multilingual sources. In Proceedings of the 2009
Workshop on Language Generation and Summarisa-
tion (UCNLG+Sum 2009), page 3, Suntec, Singapore.
Association for Computational Linguistics.
Oliva, A. and Torralba, A. (2001). Modeling the shape
of the scene: A holistic representation of the spatial
envelope. International Journal of Computer Vision,
42(3):145–175.
Schwartz, W., Kembhavi, A., Harwood, D., and Davis,
L. (2009). Human detection using partial least squares
analysis. In International Conference on Computer Vi-
sion.
Torralba, A., Murphy, K. P., Freeman, W. T., and Rubin,
M. A. (2003). Context-based vision system for place
and object recognition. In ICCV, pages 273–280. IEEE
Computer Society.
Traum, D., Fleischman, M., and Hovy, E. (2003). Nl gen-
eration for virtual humans in a complex social environ-
ment. In In Proceedings of he AAAI Spring Symposium
on Natural Language Generation in Spoken and Writ-
ten Dialogue, pages 151–158.
Urgesi, C., Moro, V., Candidi, M., and Aglioti, S. M.
(2006). Mapping implied body actions in the human
motor system. JNeurosci, 26(30):7942–9.
Yang, W., Wang, Y., and Mori, G. (2010). Recognizing
human actions from still images with latent poses. In
CVPR.
Yao, B. and Fei-Fei, L. (2010). Grouplet: a structured
image representation for recognizing human and ob-
ject interactions. In The Twenty-Third IEEE Confer-
ence on Computer Vision and Pattern Recognition, San
Francisco, CA.
Yao, B., Yang, X., Lin, L., Lee, M. W., and Zhu, S.-C.
(2010). I2t: Image parsing to text description. Pro-
ceedings of the IEEE, 98(8):1485 –1508.
</reference>
<page confidence="0.99917">
454
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.661160">
<title confidence="0.994977">Corpus-Guided Sentence Generation of Natural Images</title>
<author confidence="0.981926">Yang Lik Teo Daum´e</author>
<affiliation confidence="0.999623">University of Maryland Institute for Advanced Computer</affiliation>
<address confidence="0.998519">College Park, Maryland 20742,</address>
<email confidence="0.690004">cteo,hal,</email>
<abstract confidence="0.999015809523809">We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions. We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T L Berg</author>
<author>A C Berg</author>
<author>J Edwards</author>
<author>D A Forsyth</author>
</authors>
<title>Who’s in the picture?</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7430" citStr="Berg et al., 2004" startWordPosition="1232" endWordPosition="1235">ant supervision [Liang et al., 2009; Mann and Mccallum, 2007]. Here, we do not require “labeled” data containing images and captions but only separate data from each side. Another contribution is a computationally feasible way via dynamic programming to determine the most likely quadruplet T* = {n*, v*, s*, p*} that describes the image for generating possible sentences. 445 2 Related Work Recently, several works from the Computer Vision domain have attempted to use language to aid image scene understanding. [Kojima et al., 2000] used predefined production rules to describe actions in videos. [Berg et al., 2004] processed news captions to discover names associated with faces in the images, and [Jie et al., 2009] extended this work to associate poses detected from images with the verbs in the captions. Both approaches use annotated examples from a limited news caption corpus to learn a joint image-text model so that one can annotate new unknown images with textual information easily. Neither of these works have been tested on complex everyday images where the large variations of objects and poses makes it nearly impossible to learn a more general model. In addition, no attempt was made to generate a </context>
</contexts>
<marker>Berg, Berg, Edwards, Forsyth, 2004</marker>
<rawString>Berg, T. L., Berg, A. C., Edwards, J., and Forsyth, D. A. (2004). Who’s in the picture? In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Z Bonnie</author>
<author>B Dorr</author>
</authors>
<title>Bbn/umd at duc-2004: Topiary. In</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Document Understanding Conference (DUC</booktitle>
<pages>112--119</pages>
<contexts>
<context position="32603" citStr="Bonnie and Dorr, 2004" startWordPosition="5536" endWordPosition="5539">hieves the best results compared to all other approaches with R1 = 0.44. This means that our sentences are the closest in agreement with the human annotated ground truth, correctly predicting the sentence structure components. In addition sentences generated by T* are also succinct: with an average length of 6.9 words per sentence. However, we are still some way off the human gold standard since we do not predict other parts-of-speech such as adjectives and adverbs. Given this fact, our proposed approach performance is comparable to other state of the art summarization work in the literature [Bonnie and Dorr, 2004]. Next, we consider the Relevance+Readability metrics based on human judges. Interestingly, the first baseline, Tb1* is considered the most relevant description of the image and the least readable at the same time. This is most likely due to the fact that this recall oriented strategy will almost certainly describe some objects but the lack of any verb description; and longer sentences that average 8.2 words per sentence, makes it less readable. It is also possible that humans tend to penalize less irrelevant objects compared to missing objects, and further evaluations are necessary to confir</context>
</contexts>
<marker>Bonnie, Dorr, 2004</marker>
<rawString>Bonnie, D. Z. and Dorr, B. (2004). Bbn/umd at duc-2004: Topiary. In In Proceedings of the 2004 Document Understanding Conference (DUC 2004) at NLT/NAACL 2004, pages 112–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Choi</author>
<author>M Palmer</author>
</authors>
<title>Robust constituentto-dependency conversion for english.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th International Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>55--66</pages>
<location>Tartu, Estonia.</location>
<contexts>
<context position="16748" citStr="Choi and Palmer, 2010" startWordPosition="2795" endWordPosition="2798">umber of object arguments, we limit ourselves to verbs that take on at most two objects (or more specifically two noun phrase arguments) as a simplifying assumption: Nk = {n1, n2} where n2 can be NULL. That is, n1 and n2 are the subject and direct objects associated with v E V. Using this assumption, we can construct the set of verbs, V. To do this, we use human labeled descriptions of the training images from the UIUC Pascal-VOC dataset (sec. 3.1) as a learning corpus that allows us to determine the appropriate target verb set that is amenable to our problem. We first apply the CLEAR parser [Choi and Palmer, 2010] to obtain a dependency parse of these annotations, which also performs stemming of all the verbs and nouns in the sentence. Next, we process all the parses to select verbs which are marked as ROOT and check the existence of a subject (DEP) and direct object (PMOD, OBJ) that are linked to the ROOT verb (see Fig. 6(a)). Finally, after removing common “stop” verbs such as {is, are, be} we rank these verbs in terms of their occurrences and select the top 50 verbs which accounts for 87.5% of the sentences in the UIUC dataset to be in V. Object class n E N Synonyms, (n) bus autobus charabanc doubl</context>
<context position="19875" citStr="Choi and Palmer, 2010" startWordPosition="3340" endWordPosition="3343">. 7(a) shows that Anvn predicts v that makes sense: with the most likely predictions near the top of the list. Predicting Scenes: Just as an action is strongly related to the objects that participate in it, a scene can be predicted from the objects and verbs that occur in the image. For example, detecting Nk={boat, person} with v={row} would have predicted the scene s={coast}, since boats usually occur in water regions. To learn this relationship from the corpus, we use the UIUC dataset to discover what are the common scenes that should be included in S. We applied the CLEAR dependency parse [Choi and Palmer, 2010] on the UIUC data and extracted all the head nouns (PMOD) in the PP phrases for this purpose and excluded those nouns with prepositions (marked as ADV) such as {with, of} which do not co-occur with scenes in general (see Fig. 6(b)). We then ranked the remaining scenes in terms of their frequency to select the top 8 scenes used in S. To improve recall and generalization, we expand each of the 8 scene classes using their WordNet synsets (s) (up to a max of three hyponymns levels). Similar to the procedure of predicting the verbs described above, we compute the log-likelihood ratio of ordered bi</context>
</contexts>
<marker>Choi, Palmer, 2010</marker>
<rawString>Choi, J. D. and Palmer, M. (2010). Robust constituentto-dependency conversion for english. In Proceedings of the 9th International Workshop on Treebanks and Linguistic Theories, pages 55–66, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="18442" citStr="Dunning, 1993" startWordPosition="3084" endWordPosition="3085"> different kinds of specific objects, we expand n1 and n2 in their textual descriptions using synonyms. For example, the object class n1=aeroplane should include the synonyms {plane, jet, fighter jet, aircraft}, denoted as (n1). To do this, we expand each object class using their corresponding WordNet synsets up to at most three hyponymns levels. Example synonyms for some of the classes are summarized in Table 2. We can now compute from the Gigaword corpus [Graff, 2003] the probability that a verb exists given the detected nouns, Pr(v|n1, n2). We do this by computing the log-likelihood ratio [Dunning, 1993] , Anvn, of trigrams ((n1) , v, (n2)), computed from each sentence in the English Gigaword corpus [Graff, 2003]. This is done by extracting only the words in the corpus that are defined in N and V (in448 cluding their synonyms). This forms a reduced corpus sequence from which we obtain our target trigrams. For example, the sentence: the large brown dog chases a small young cat around the messy room, forcing the cat to run away towards its owner. will be reduced to the stemmed sequence dog chase cat cat run owner2 from which we obtain the target trigram relationships: {dog chase cat}, {cat run</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Everingham</author>
<author>L Van Gool</author>
<author>C K I Williams</author>
<author>J Winn</author>
<author>A Zisserman</author>
</authors>
<date>2008</date>
<booktitle>The PASCAL Visual Object Classes Challenge</booktitle>
<note>(VOC2008) Results.</note>
<marker>Everingham, Van Gool, Williams, Winn, Zisserman, 2008</marker>
<rawString>Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., and Zisserman, A. (2008). The PASCAL Visual Object Classes Challenge 2008 (VOC2008) Results.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Farhadi</author>
<author>S M M Hejrati</author>
<author>M A Sadeghi</author>
<author>P Young</author>
<author>C Rashtchian</author>
<author>J Hockenmaier</author>
<author>D A Forsyth</author>
</authors>
<title>Every picture tells a story: Generating sentences from images.</title>
<date>2010</date>
<journal>ECCV</journal>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>4</volume>
<pages>15--29</pages>
<editor>In Daniilidis, K., Maragos, P., and Paragios, N., editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="8108" citStr="Farhadi et al., 2010" startWordPosition="1349" endWordPosition="1352">h faces in the images, and [Jie et al., 2009] extended this work to associate poses detected from images with the verbs in the captions. Both approaches use annotated examples from a limited news caption corpus to learn a joint image-text model so that one can annotate new unknown images with textual information easily. Neither of these works have been tested on complex everyday images where the large variations of objects and poses makes it nearly impossible to learn a more general model. In addition, no attempt was made to generate a descriptive sentence from the learned model. The work of [Farhadi et al., 2010] attempts to “generate” sentences by first learning from a set of human annotated examples, and producing the same sentence if both images and sentence share common properties in terms of their triplets: (Nouns-Verbs-Scenes). No attempt was made to generate novel sentences from images beyond what has been annotated by humans. [Yao et al., 2010] has recently introduced a framework for parsing images/videos to textual description that requires significant annotated data, a requirement that our proposed approach avoids. Natural language generation (NLG) is a longstanding problem. Classic approac</context>
<context position="12081" citStr="Farhadi et al., 2010" startWordPosition="2010" endWordPosition="2013">d’ ’cat’ ’chair’ ’cow’ ’table’ ’pose’ ’fly’ ’lie’ ’lay’ ’highway’ ’below’ ’beside’ ’dog’ ’horse’, ’motorbike’ ’smile’ ’live’ ’walk’ ’lake’ ’room’ ’between’ ’person’ ’pottedplant’ ’graze’ ’drive’ ’play’ ’sky’ ’street’ ’before’ ’to’ ’sheep’ ’sofa’ ’train’ ’eat’ ’cover’ ’train’ ’track’ ’under’ ’on’ ’tvmonitor’ ’close’ ... Table 1: The set of objects, actions (first 20), scenes and preposition classes considered (a) (b) Figure 4: Samples of images with corresponding annotations from the UIUC scene description dataset. 3.1 Image Dataset We use the UIUC Pascal Sentence dataset, first introduced in [Farhadi et al., 2010] and available online1. It contains 1000 images taken from a subset of the Pascal-VOC 2008 challenge image dataset and are hand annotated with sentences that describe the image by paid human annotators using Amazon Mechanical Turk. Fig. 4 shows some sample images with their annotations. There are 5 annotations per image, and each annotation is usually short – around 10 words long. We randomly selected 900 images (4500 sentences) as the learning corpus to construct the verb and scene sets, {V, S} as described in sec. 3.3, and kept the remaining 100 images for testing and evaluation. 3.2 Object</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Farhadi, A., Hejrati, S. M. M., Sadeghi, M. A., Young, P., Rashtchian, C., Hockenmaier, J., and Forsyth, D. A. (2010). Every picture tells a story: Generating sentences from images. In Daniilidis, K., Maragos, P., and Paragios, N., editors, ECCV (4), volume 6314 of Lecture Notes in Computer Science, pages 15–29. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Felzenszwalb</author>
<author>R B Girshick</author>
<author>D McAllester</author>
</authors>
<title>Discriminatively trained deformable part models, release 4.</title>
<date>2008</date>
<note>http://people.cs.uchicago.edu/ pff/latentrelease4/.</note>
<contexts>
<context position="12793" citStr="Felzenszwalb et al., 2008" startWordPosition="2133" endWordPosition="2136">2008 challenge image dataset and are hand annotated with sentences that describe the image by paid human annotators using Amazon Mechanical Turk. Fig. 4 shows some sample images with their annotations. There are 5 annotations per image, and each annotation is usually short – around 10 words long. We randomly selected 900 images (4500 sentences) as the learning corpus to construct the verb and scene sets, {V, S} as described in sec. 3.3, and kept the remaining 100 images for testing and evaluation. 3.2 Object and Scene Detections from Images We use the Pascal-VOC 2008 trained object detectors [Felzenszwalb et al., 2008] of 20 common everyday object classes that are defined in N. Each of the detectors are essentially SVM classifiers trained on a large number of the objects’ image representations from a large variety of sources. Although 20 classes may seem small, their existence in many 1http://vision.cs.uiuc.edu/pascal-sentences/ Figure 5: (a) [Top] The part based object detector from [Felzenszwalb et al., 2010]. [Bottom] The graphical model representation of an object, for e.g. a bike. (b) Examples of GIST gradients: (left) an outdoor scene vs (right) an indoor scene [Torralba et al., 2003]. natural images</context>
</contexts>
<marker>Felzenszwalb, Girshick, McAllester, 2008</marker>
<rawString>Felzenszwalb, P. F., Girshick, R. B., and McAllester, D. (2008). Discriminatively trained deformable part models, release 4. http://people.cs.uchicago.edu/ pff/latentrelease4/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Felzenszwalb</author>
<author>R B Girshick</author>
<author>D A McAllester</author>
<author>D Ramanan</author>
</authors>
<title>Object detection with discriminatively trained part-based models.</title>
<date>2010</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>32</volume>
<issue>9</issue>
<contexts>
<context position="4311" citStr="Felzenszwalb et al., 2010" startWordPosition="701" endWordPosition="704"> quadruplet is obviously a simplification from state of the art generation work, but as we will show in the experimental results (sec. 4), it is sufficient to describe images. The key challenge is that detecting objects, actions and scenes directly from images is often noisy and unreliable. We illustrate this using example images from the Pascal-Visual Object Classes (VOC) 2008 challenge [Everingham et al., 2008]. First, Fig. 2(a) shows the variability of images in their raw image representations: pixels, edges and local features. This makes it difficult for state of the art object detectors [Felzenszwalb et al., 2010; Schwartz et al., 2009] to reliably detect important objects in the scene: boat, humans and water – average precision scores reported in [Felzenszwalb et al., 2010] manages around 42% for humans and only 11% for boat over a dataset of almost 5000 images in 20 object categories. Yet, these images are semantically similar in terms of their high level description. Second, cognitive studies [Urgesi et al., 2006; Kourtzi, 2004] have proposed that inferring the action from static images (known as an “implied action”) is often achieved by detecting the pose of humans in the image: the position of th</context>
<context position="9628" citStr="Felzenszwalb et al., 2010" startWordPosition="1593" endWordPosition="1596">n, 2009] or databases [Golland et al., 2010]. Most of the effort in those approaches has focused on selection and realization. We address a tangential problem that has not received much attention in the generation literature: how to deal with noisy inputs. In our case, the inputs themselves are often uncertain (due to misrecognitions by object/scene detectors) and the content selection and realization needs to take this uncertainty into account. 3 Our Approach Our approach is summarized in Fig. 3. The input is a test image where we detect objects and scenes using trained detection algorithms [Felzenszwalb et al., 2010; Torralba et al., 2003]. To keep the framework computationally tractable, we limit the elements of the quadruplet (Nouns-Verbs-Scenes-Prepositions) to come from a finite set of objects N, actions V, scenes S and prepositions P classes that are commonly encountered. They are summarized in Table. 1. In addition, the sentence that is generated for each image is limited to at most two objects occurring in a unique scene. Figure 3: Overview of our approach. (a) Detect objects and scenes from input image. (b) Estimate optimal sentence structure quadruplet T*. (c) Generating a sentence from T*. Deno</context>
<context position="13193" citStr="Felzenszwalb et al., 2010" startWordPosition="2195" endWordPosition="2198">e sets, {V, S} as described in sec. 3.3, and kept the remaining 100 images for testing and evaluation. 3.2 Object and Scene Detections from Images We use the Pascal-VOC 2008 trained object detectors [Felzenszwalb et al., 2008] of 20 common everyday object classes that are defined in N. Each of the detectors are essentially SVM classifiers trained on a large number of the objects’ image representations from a large variety of sources. Although 20 classes may seem small, their existence in many 1http://vision.cs.uiuc.edu/pascal-sentences/ Figure 5: (a) [Top] The part based object detector from [Felzenszwalb et al., 2010]. [Bottom] The graphical model representation of an object, for e.g. a bike. (b) Examples of GIST gradients: (left) an outdoor scene vs (right) an indoor scene [Torralba et al., 2003]. natural images (e.g. humans, cars and plants) makes them particularly important for our task, since humans tend to describe these common objects as well. As object representations, the part-based descriptor of [Felzenszwalb et al., 2010] is used. This representation decomposes any object, e.g. a cow, into its constituent parts: head, torso, legs, which are shared by other objects in a hierarchical manner. At ea</context>
</contexts>
<marker>Felzenszwalb, Girshick, McAllester, Ramanan, 2010</marker>
<rawString>Felzenszwalb, P. F., Girshick, R. B., McAllester, D. A., and Ramanan, D. (2010). Object detection with discriminatively trained part-based models. IEEE Trans. Pattern Anal. Mach. Intell., 32(9):1627–1645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Golland</author>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>A gametheoretic approach to generating spatial descriptions.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="9046" citStr="Golland et al., 2010" startWordPosition="1496" endWordPosition="1500">by humans. [Yao et al., 2010] has recently introduced a framework for parsing images/videos to textual description that requires significant annotated data, a requirement that our proposed approach avoids. Natural language generation (NLG) is a longstanding problem. Classic approaches [Traum et al., 2003] are based on three steps: selection, planning and realization. A common challenge in generation problems is the question of: what is the input? Recently, approaches for generation have focused on formal specification inputs, such as the output of theorem provers [McKeown, 2009] or databases [Golland et al., 2010]. Most of the effort in those approaches has focused on selection and realization. We address a tangential problem that has not received much attention in the generation literature: how to deal with noisy inputs. In our case, the inputs themselves are often uncertain (due to misrecognitions by object/scene detectors) and the content selection and realization needs to take this uncertainty into account. 3 Our Approach Our approach is summarized in Fig. 3. The input is a test image where we detect objects and scenes using trained detection algorithms [Felzenszwalb et al., 2010; Torralba et al.,</context>
</contexts>
<marker>Golland, Liang, Klein, 2010</marker>
<rawString>Golland, D., Liang, P., and Klein, D. (2010). A gametheoretic approach to generating spatial descriptions. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
</authors>
<title>English gigaword.</title>
<date>2003</date>
<booktitle>In Linguistic Data Consortium,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2440" citStr="Graff, 2003" startWordPosition="391" endWordPosition="392">nts together. Our hypothesis is based on the assumption that natural images accurately reflect common everyday scenarios which are captured in language. For example, knowing that boats usually occur over water will enable us to constrain the possible scenes a boat can occur and exclude highly unlikely ones – street, highway. It also enables us to predict likely actions (Verbs) given the current object detections in the image: detecting a dog with a person will likely induce walk rather than swim, jump, fly. Key to our approach is the use of a large generic corpus such as the English Gigaword [Graff, 2003] as the semantic grounding to predict and correct the initial and often noisy visual detections of an image to produce a reasonable sentence that succinctly describes the image. In order to get an idea of the difficulty of this task, it is important to first define what makes up 444 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 444–454, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Figure 2: Illustration of various perceptual challenges for sentence generation for images. (a) Different images with se</context>
<context position="18302" citStr="Graff, 2003" startWordPosition="3061" endWordPosition="3062">n1 and n2 are selected from the 20 object classes defined previously in N. Just as the 20 object classes are defined visually over several different kinds of specific objects, we expand n1 and n2 in their textual descriptions using synonyms. For example, the object class n1=aeroplane should include the synonyms {plane, jet, fighter jet, aircraft}, denoted as (n1). To do this, we expand each object class using their corresponding WordNet synsets up to at most three hyponymns levels. Example synonyms for some of the classes are summarized in Table 2. We can now compute from the Gigaword corpus [Graff, 2003] the probability that a verb exists given the detected nouns, Pr(v|n1, n2). We do this by computing the log-likelihood ratio [Dunning, 1993] , Anvn, of trigrams ((n1) , v, (n2)), computed from each sentence in the English Gigaword corpus [Graff, 2003]. This is done by extracting only the words in the corpus that are defined in N and V (in448 cluding their synonyms). This forms a reduced corpus sequence from which we obtain our target trigrams. For example, the sentence: the large brown dog chases a small young cat around the messy room, forcing the cat to run away towards its owner. will be r</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>Graff, D. (2003). English gigaword. In Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Jie</author>
<author>B Caputo</author>
<author>V Ferrari</author>
</authors>
<title>Who’s doing what: Joint modeling of names and verbs for simultaneous face and pose annotation.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems, NIPS.</booktitle>
<editor>In NIPS, editor,</editor>
<publisher>NIPS.</publisher>
<contexts>
<context position="7532" citStr="Jie et al., 2009" startWordPosition="1250" endWordPosition="1253">ontaining images and captions but only separate data from each side. Another contribution is a computationally feasible way via dynamic programming to determine the most likely quadruplet T* = {n*, v*, s*, p*} that describes the image for generating possible sentences. 445 2 Related Work Recently, several works from the Computer Vision domain have attempted to use language to aid image scene understanding. [Kojima et al., 2000] used predefined production rules to describe actions in videos. [Berg et al., 2004] processed news captions to discover names associated with faces in the images, and [Jie et al., 2009] extended this work to associate poses detected from images with the verbs in the captions. Both approaches use annotated examples from a limited news caption corpus to learn a joint image-text model so that one can annotate new unknown images with textual information easily. Neither of these works have been tested on complex everyday images where the large variations of objects and poses makes it nearly impossible to learn a more general model. In addition, no attempt was made to generate a descriptive sentence from the learned model. The work of [Farhadi et al., 2010] attempts to “generate”</context>
</contexts>
<marker>Jie, Caputo, Ferrari, 2009</marker>
<rawString>Jie, L., Caputo, B., and Ferrari, V. (2009). Who’s doing what: Joint modeling of names and verbs for simultaneous face and pose annotation. In NIPS, editor, Advances in Neural Information Processing Systems, NIPS. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kojima</author>
<author>M Izumi</author>
<author>T Tamura</author>
<author>K Fukunaga</author>
</authors>
<title>Generating natural language description of human behavior from video images.</title>
<date>2000</date>
<booktitle>In Pattern Recognition,</booktitle>
<volume>4</volume>
<pages>728--731</pages>
<contexts>
<context position="7346" citStr="Kojima et al., 2000" startWordPosition="1219" endWordPosition="1222"> over larger domains of images. It represents a relatively new style of learning: distant supervision [Liang et al., 2009; Mann and Mccallum, 2007]. Here, we do not require “labeled” data containing images and captions but only separate data from each side. Another contribution is a computationally feasible way via dynamic programming to determine the most likely quadruplet T* = {n*, v*, s*, p*} that describes the image for generating possible sentences. 445 2 Related Work Recently, several works from the Computer Vision domain have attempted to use language to aid image scene understanding. [Kojima et al., 2000] used predefined production rules to describe actions in videos. [Berg et al., 2004] processed news captions to discover names associated with faces in the images, and [Jie et al., 2009] extended this work to associate poses detected from images with the verbs in the captions. Both approaches use annotated examples from a limited news caption corpus to learn a joint image-text model so that one can annotate new unknown images with textual information easily. Neither of these works have been tested on complex everyday images where the large variations of objects and poses makes it nearly impos</context>
</contexts>
<marker>Kojima, Izumi, Tamura, Fukunaga, 2000</marker>
<rawString>Kojima, A., Izumi, M., Tamura, T., and Fukunaga, K. (2000). Generating natural language description of human behavior from video images. In Pattern Recognition, 2000. Proceedings. 15th International Conference on, volume 4, pages 728 –731 vol.4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kourtzi</author>
</authors>
<title>But still, it moves.</title>
<date>2004</date>
<journal>Trends in Cognitive Sciences,</journal>
<volume>8</volume>
<issue>2</issue>
<contexts>
<context position="4737" citStr="Kourtzi, 2004" startWordPosition="775" endWordPosition="776">ws the variability of images in their raw image representations: pixels, edges and local features. This makes it difficult for state of the art object detectors [Felzenszwalb et al., 2010; Schwartz et al., 2009] to reliably detect important objects in the scene: boat, humans and water – average precision scores reported in [Felzenszwalb et al., 2010] manages around 42% for humans and only 11% for boat over a dataset of almost 5000 images in 20 object categories. Yet, these images are semantically similar in terms of their high level description. Second, cognitive studies [Urgesi et al., 2006; Kourtzi, 2004] have proposed that inferring the action from static images (known as an “implied action”) is often achieved by detecting the pose of humans in the image: the position of the limbs with respect to one another, under the assumption that a unique pose occurs for a unique action. Clearly, this assumption is weak as 1) similar actions may be represented by different poses due to the inherent dynamic nature of the action itself: e.g. walking a dog and 2) different actions may have the same pose: e.g. walking a dog versus running (Fig. 2(b)). The missing component here is whether the key object (do</context>
</contexts>
<marker>Kourtzi, 2004</marker>
<rawString>Kourtzi, Z. (2004). But still, it moves. Trends in Cognitive Sciences, 8(2):47 – 49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning from measurements in exponential families.</title>
<date>2009</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="6848" citStr="Liang et al., 2009" startWordPosition="1137" endWordPosition="1140">ular, we show that it is possible to avoid predicting actions directly from images – which is still unreliable – and to use the corpus instead to guide our predictions. Our proposed strategy is also generic, that is, we make no prior assumptions on the image domain considered. While other works (sec. 2) depend on strong annotations between images and text to ground their predictions (and to remove wrong sentences), we show that a large generic corpus is also able to provide the same grounding over larger domains of images. It represents a relatively new style of learning: distant supervision [Liang et al., 2009; Mann and Mccallum, 2007]. Here, we do not require “labeled” data containing images and captions but only separate data from each side. Another contribution is a computationally feasible way via dynamic programming to determine the most likely quadruplet T* = {n*, v*, s*, p*} that describes the image for generating possible sentences. 445 2 Related Work Recently, several works from the Computer Vision domain have attempted to use language to aid image scene understanding. [Kojima et al., 2000] used predefined production rules to describe actions in videos. [Berg et al., 2004] processed news c</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Liang, P., Jordan, M. I., and Klein, D. (2009). Learning from measurements in exponential families. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In NAACLHLT.</booktitle>
<contexts>
<context position="29475" citStr="Lin and Hovy, 2003" startWordPosition="5012" endWordPosition="5015"> 2 are IN the scene}. In the second experiment, we applied the HMM strategy described above but made all transition probabilities equiprobable, removing the effects of the corpus, and producing a sentence structure which we denote as T*�Q. The third experiment produces the full T* with transition probabilities learned from the corpus. All experiments were performed on the 100 unseen testing images from the UIUC dataset and we used only the most likely (top) sentence generated for all evaluation. We use two evaluation metrics as a measure of the accuracy of the generated sentences: 1) ROUGE-1 [Lin and Hovy, 2003] precision scores and 2) Relevance and Readability of the generated sentences. ROUGE-1 is a recall based metric that is commonly used to measure the effectiveness of text summarization. In this work, the short descriptive sentence of an image can be viewed as summarizing the image 451 Experiment R1,(length) Relevance Readability Baseline 1, T ∗ 0.35,(8.2) 2.84 ± 1.40 3.64 ± 1.20 b1 Baseline 2, T ∗ 0.39,(6.8) 2.14 ± 1.13 3.94 ± 0.91 b2 HMM no cor- 0.42,(6.5) 2.44 ± 1.25 3.88 ± 1.18 pus, Tey∗ Full HMM, T∗ 0.44,(6.9) 2.51 ± 1.30 4.10 ± 1.03 Human Anno- 0.68,(10.1) 4.91 ± 0.29 4.77 ± 0.42 tation </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin, C. and Hovy, E. (2003). Automatic evaluation of summaries using n-gram co-occurrence statistics. In NAACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-T Lin</author>
<author>C-J Lin</author>
<author>R C Weng</author>
</authors>
<title>A note on platt’s probabilistic outputs for support vector machines.</title>
<date>2007</date>
<pages>68--267</pages>
<location>Mach. Learn.,</location>
<contexts>
<context position="14426" citStr="Lin et al., 2007" startWordPosition="2393" endWordPosition="2396">radient orientations are computed. The relationship between each parts is modeled probabilistically using graphical models where parts are the nodes and the edges are the conditional probabilities that relate their spatial compatibility (Fig. 5(a)). For example, in a cow, the probability of finding the torso near the head is higher than finding the legs near the head. This model’s intuition lies in the assumption that objects can be deformed but the relative position of each constituent parts should remain the same. We convert the object detection scores to probabilities using Platt’s method [Lin et al., 2007] which is numerically more stable to obtain Pr(n|I). The parameters of Platt’s method are obtained by estimating the number of positives and negatives from the UIUC annotated dataset, from 447 which we determine the appropriate probabilistic threshold, which gives us approximately 50% recall and precision. For detecting scenes defined in S, we use the GIST-based scene descriptor of [Torralba et al., 2003]. GIST computes the windowed 2D Gabor filter responses of an input image. The responses of Gabor filters (4 scales and 6 orientations) encode the texture gradients that describe the local pro</context>
</contexts>
<marker>Lin, Lin, Weng, 2007</marker>
<rawString>Lin, H.-T., Lin, C.-J., and Weng, R. C. (2007). A note on platt’s probabilistic outputs for support vector machines. Mach. Learn., 68:267–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>A Mccallum</author>
</authors>
<title>Simple, robust, scalable semi-supervised learning via expectation regularization.</title>
<date>2007</date>
<booktitle>In The 24th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="6873" citStr="Mann and Mccallum, 2007" startWordPosition="1141" endWordPosition="1145">t is possible to avoid predicting actions directly from images – which is still unreliable – and to use the corpus instead to guide our predictions. Our proposed strategy is also generic, that is, we make no prior assumptions on the image domain considered. While other works (sec. 2) depend on strong annotations between images and text to ground their predictions (and to remove wrong sentences), we show that a large generic corpus is also able to provide the same grounding over larger domains of images. It represents a relatively new style of learning: distant supervision [Liang et al., 2009; Mann and Mccallum, 2007]. Here, we do not require “labeled” data containing images and captions but only separate data from each side. Another contribution is a computationally feasible way via dynamic programming to determine the most likely quadruplet T* = {n*, v*, s*, p*} that describes the image for generating possible sentences. 445 2 Related Work Recently, several works from the Computer Vision domain have attempted to use language to aid image scene understanding. [Kojima et al., 2000] used predefined production rules to describe actions in videos. [Berg et al., 2004] processed news captions to discover names</context>
</contexts>
<marker>Mann, Mccallum, 2007</marker>
<rawString>Mann, G. S. and Mccallum, A. (2007). Simple, robust, scalable semi-supervised learning via expectation regularization. In The 24th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
</authors>
<title>Query-focused summarization using text-to-text generation: When information comes from multilingual sources.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Language Generation and Summarisation (UCNLG+Sum 2009),</booktitle>
<pages>page</pages>
<institution>3, Suntec, Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="9010" citStr="McKeown, 2009" startWordPosition="1492" endWordPosition="1493">yond what has been annotated by humans. [Yao et al., 2010] has recently introduced a framework for parsing images/videos to textual description that requires significant annotated data, a requirement that our proposed approach avoids. Natural language generation (NLG) is a longstanding problem. Classic approaches [Traum et al., 2003] are based on three steps: selection, planning and realization. A common challenge in generation problems is the question of: what is the input? Recently, approaches for generation have focused on formal specification inputs, such as the output of theorem provers [McKeown, 2009] or databases [Golland et al., 2010]. Most of the effort in those approaches has focused on selection and realization. We address a tangential problem that has not received much attention in the generation literature: how to deal with noisy inputs. In our case, the inputs themselves are often uncertain (due to misrecognitions by object/scene detectors) and the content selection and realization needs to take this uncertainty into account. 3 Our Approach Our approach is summarized in Fig. 3. The input is a test image where we detect objects and scenes using trained detection algorithms [Felzens</context>
</contexts>
<marker>McKeown, 2009</marker>
<rawString>McKeown, K. (2009). Query-focused summarization using text-to-text generation: When information comes from multilingual sources. In Proceedings of the 2009 Workshop on Language Generation and Summarisation (UCNLG+Sum 2009), page 3, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Oliva</author>
<author>A Torralba</author>
</authors>
<title>Modeling the shape of the scene: A holistic representation of the spatial envelope.</title>
<date>2001</date>
<journal>International Journal of Computer Vision,</journal>
<volume>42</volume>
<issue>3</issue>
<contexts>
<context position="5660" citStr="Oliva and Torralba, 2001" startWordPosition="931" endWordPosition="934">tion is weak as 1) similar actions may be represented by different poses due to the inherent dynamic nature of the action itself: e.g. walking a dog and 2) different actions may have the same pose: e.g. walking a dog versus running (Fig. 2(b)). The missing component here is whether the key object (dog) under interaction is considered. Recent works [Yao and Fei-Fei, 2010; Yang et al., 2010] that used poses for recognition of actions achieved 70% and 61% accuracy respectively under extremely limited testing conditions with only 5-6 action classes each. Finally, state of the art scene detectors [Oliva and Torralba, 2001; Torralba et al., 2003] need to have enough representative training examples of scenes from pre-defined scene classes for a classification to be successful – with a reported average precision of 83.7% tested over a dataset of 2600 images. Addressing all these visual challenges is clearly a formidable task which is beyond the scope of this paper. Our focus instead is to show that with the addition of language to ground the noisy initial visual detections, we are able to improve the quality of the generated sentence as a faithful description of the image. In particular, we show that it is possi</context>
</contexts>
<marker>Oliva, Torralba, 2001</marker>
<rawString>Oliva, A. and Torralba, A. (2001). Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision, 42(3):145–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Schwartz</author>
<author>A Kembhavi</author>
<author>D Harwood</author>
<author>L Davis</author>
</authors>
<title>Human detection using partial least squares analysis.</title>
<date>2009</date>
<booktitle>In International Conference on Computer Vision.</booktitle>
<contexts>
<context position="4334" citStr="Schwartz et al., 2009" startWordPosition="705" endWordPosition="708">simplification from state of the art generation work, but as we will show in the experimental results (sec. 4), it is sufficient to describe images. The key challenge is that detecting objects, actions and scenes directly from images is often noisy and unreliable. We illustrate this using example images from the Pascal-Visual Object Classes (VOC) 2008 challenge [Everingham et al., 2008]. First, Fig. 2(a) shows the variability of images in their raw image representations: pixels, edges and local features. This makes it difficult for state of the art object detectors [Felzenszwalb et al., 2010; Schwartz et al., 2009] to reliably detect important objects in the scene: boat, humans and water – average precision scores reported in [Felzenszwalb et al., 2010] manages around 42% for humans and only 11% for boat over a dataset of almost 5000 images in 20 object categories. Yet, these images are semantically similar in terms of their high level description. Second, cognitive studies [Urgesi et al., 2006; Kourtzi, 2004] have proposed that inferring the action from static images (known as an “implied action”) is often achieved by detecting the pose of humans in the image: the position of the limbs with respect to</context>
</contexts>
<marker>Schwartz, Kembhavi, Harwood, Davis, 2009</marker>
<rawString>Schwartz, W., Kembhavi, A., Harwood, D., and Davis, L. (2009). Human detection using partial least squares analysis. In International Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Torralba</author>
<author>K P Murphy</author>
<author>W T Freeman</author>
<author>M A Rubin</author>
</authors>
<title>Context-based vision system for place and object recognition.</title>
<date>2003</date>
<booktitle>In ICCV,</booktitle>
<pages>273--280</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="5683" citStr="Torralba et al., 2003" startWordPosition="935" endWordPosition="938"> actions may be represented by different poses due to the inherent dynamic nature of the action itself: e.g. walking a dog and 2) different actions may have the same pose: e.g. walking a dog versus running (Fig. 2(b)). The missing component here is whether the key object (dog) under interaction is considered. Recent works [Yao and Fei-Fei, 2010; Yang et al., 2010] that used poses for recognition of actions achieved 70% and 61% accuracy respectively under extremely limited testing conditions with only 5-6 action classes each. Finally, state of the art scene detectors [Oliva and Torralba, 2001; Torralba et al., 2003] need to have enough representative training examples of scenes from pre-defined scene classes for a classification to be successful – with a reported average precision of 83.7% tested over a dataset of 2600 images. Addressing all these visual challenges is clearly a formidable task which is beyond the scope of this paper. Our focus instead is to show that with the addition of language to ground the noisy initial visual detections, we are able to improve the quality of the generated sentence as a faithful description of the image. In particular, we show that it is possible to avoid predicting</context>
<context position="9651" citStr="Torralba et al., 2003" startWordPosition="1597" endWordPosition="1600">and et al., 2010]. Most of the effort in those approaches has focused on selection and realization. We address a tangential problem that has not received much attention in the generation literature: how to deal with noisy inputs. In our case, the inputs themselves are often uncertain (due to misrecognitions by object/scene detectors) and the content selection and realization needs to take this uncertainty into account. 3 Our Approach Our approach is summarized in Fig. 3. The input is a test image where we detect objects and scenes using trained detection algorithms [Felzenszwalb et al., 2010; Torralba et al., 2003]. To keep the framework computationally tractable, we limit the elements of the quadruplet (Nouns-Verbs-Scenes-Prepositions) to come from a finite set of objects N, actions V, scenes S and prepositions P classes that are commonly encountered. They are summarized in Table. 1. In addition, the sentence that is generated for each image is limited to at most two objects occurring in a unique scene. Figure 3: Overview of our approach. (a) Detect objects and scenes from input image. (b) Estimate optimal sentence structure quadruplet T*. (c) Generating a sentence from T*. Denoting the current test i</context>
<context position="13376" citStr="Torralba et al., 2003" startWordPosition="2225" endWordPosition="2228">t detectors [Felzenszwalb et al., 2008] of 20 common everyday object classes that are defined in N. Each of the detectors are essentially SVM classifiers trained on a large number of the objects’ image representations from a large variety of sources. Although 20 classes may seem small, their existence in many 1http://vision.cs.uiuc.edu/pascal-sentences/ Figure 5: (a) [Top] The part based object detector from [Felzenszwalb et al., 2010]. [Bottom] The graphical model representation of an object, for e.g. a bike. (b) Examples of GIST gradients: (left) an outdoor scene vs (right) an indoor scene [Torralba et al., 2003]. natural images (e.g. humans, cars and plants) makes them particularly important for our task, since humans tend to describe these common objects as well. As object representations, the part-based descriptor of [Felzenszwalb et al., 2010] is used. This representation decomposes any object, e.g. a cow, into its constituent parts: head, torso, legs, which are shared by other objects in a hierarchical manner. At each level, image gradient orientations are computed. The relationship between each parts is modeled probabilistically using graphical models where parts are the nodes and the edges are</context>
<context position="14834" citStr="Torralba et al., 2003" startWordPosition="2456" endWordPosition="2459">he assumption that objects can be deformed but the relative position of each constituent parts should remain the same. We convert the object detection scores to probabilities using Platt’s method [Lin et al., 2007] which is numerically more stable to obtain Pr(n|I). The parameters of Platt’s method are obtained by estimating the number of positives and negatives from the UIUC annotated dataset, from 447 which we determine the appropriate probabilistic threshold, which gives us approximately 50% recall and precision. For detecting scenes defined in S, we use the GIST-based scene descriptor of [Torralba et al., 2003]. GIST computes the windowed 2D Gabor filter responses of an input image. The responses of Gabor filters (4 scales and 6 orientations) encode the texture gradients that describe the local properties of the image. Averaging out these responses over larger spatial regions gives us a set of global image properties. These high dimensional responses are then reprojected to a low dimensional space via PCA, where the number of principal components are obtained empirically from training scenes. This representation forms the GIST descriptor of an image (Fig. 5(b)) which is used to train a set of SVM c</context>
</contexts>
<marker>Torralba, Murphy, Freeman, Rubin, 2003</marker>
<rawString>Torralba, A., Murphy, K. P., Freeman, W. T., and Rubin, M. A. (2003). Context-based vision system for place and object recognition. In ICCV, pages 273–280. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
<author>M Fleischman</author>
<author>E Hovy</author>
</authors>
<title>Nl generation for virtual humans in a complex social environment. In</title>
<date>2003</date>
<booktitle>In Proceedings of he AAAI Spring Symposium on Natural Language Generation in Spoken and Written Dialogue,</booktitle>
<pages>151--158</pages>
<contexts>
<context position="8731" citStr="Traum et al., 2003" startWordPosition="1446" endWordPosition="1449">empts to “generate” sentences by first learning from a set of human annotated examples, and producing the same sentence if both images and sentence share common properties in terms of their triplets: (Nouns-Verbs-Scenes). No attempt was made to generate novel sentences from images beyond what has been annotated by humans. [Yao et al., 2010] has recently introduced a framework for parsing images/videos to textual description that requires significant annotated data, a requirement that our proposed approach avoids. Natural language generation (NLG) is a longstanding problem. Classic approaches [Traum et al., 2003] are based on three steps: selection, planning and realization. A common challenge in generation problems is the question of: what is the input? Recently, approaches for generation have focused on formal specification inputs, such as the output of theorem provers [McKeown, 2009] or databases [Golland et al., 2010]. Most of the effort in those approaches has focused on selection and realization. We address a tangential problem that has not received much attention in the generation literature: how to deal with noisy inputs. In our case, the inputs themselves are often uncertain (due to misrecog</context>
</contexts>
<marker>Traum, Fleischman, Hovy, 2003</marker>
<rawString>Traum, D., Fleischman, M., and Hovy, E. (2003). Nl generation for virtual humans in a complex social environment. In In Proceedings of he AAAI Spring Symposium on Natural Language Generation in Spoken and Written Dialogue, pages 151–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Urgesi</author>
<author>V Moro</author>
<author>M Candidi</author>
<author>S M Aglioti</author>
</authors>
<title>Mapping implied body actions in the human motor system.</title>
<date>2006</date>
<pages>26--30</pages>
<contexts>
<context position="4722" citStr="Urgesi et al., 2006" startWordPosition="771" endWordPosition="774"> First, Fig. 2(a) shows the variability of images in their raw image representations: pixels, edges and local features. This makes it difficult for state of the art object detectors [Felzenszwalb et al., 2010; Schwartz et al., 2009] to reliably detect important objects in the scene: boat, humans and water – average precision scores reported in [Felzenszwalb et al., 2010] manages around 42% for humans and only 11% for boat over a dataset of almost 5000 images in 20 object categories. Yet, these images are semantically similar in terms of their high level description. Second, cognitive studies [Urgesi et al., 2006; Kourtzi, 2004] have proposed that inferring the action from static images (known as an “implied action”) is often achieved by detecting the pose of humans in the image: the position of the limbs with respect to one another, under the assumption that a unique pose occurs for a unique action. Clearly, this assumption is weak as 1) similar actions may be represented by different poses due to the inherent dynamic nature of the action itself: e.g. walking a dog and 2) different actions may have the same pose: e.g. walking a dog versus running (Fig. 2(b)). The missing component here is whether the</context>
</contexts>
<marker>Urgesi, Moro, Candidi, Aglioti, 2006</marker>
<rawString>Urgesi, C., Moro, V., Candidi, M., and Aglioti, S. M. (2006). Mapping implied body actions in the human motor system. JNeurosci, 26(30):7942–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Yang</author>
<author>Y Wang</author>
<author>G Mori</author>
</authors>
<title>Recognizing human actions from still images with latent poses.</title>
<date>2010</date>
<booktitle>In CVPR.</booktitle>
<contexts>
<context position="5427" citStr="Yang et al., 2010" startWordPosition="894" endWordPosition="897"> “implied action”) is often achieved by detecting the pose of humans in the image: the position of the limbs with respect to one another, under the assumption that a unique pose occurs for a unique action. Clearly, this assumption is weak as 1) similar actions may be represented by different poses due to the inherent dynamic nature of the action itself: e.g. walking a dog and 2) different actions may have the same pose: e.g. walking a dog versus running (Fig. 2(b)). The missing component here is whether the key object (dog) under interaction is considered. Recent works [Yao and Fei-Fei, 2010; Yang et al., 2010] that used poses for recognition of actions achieved 70% and 61% accuracy respectively under extremely limited testing conditions with only 5-6 action classes each. Finally, state of the art scene detectors [Oliva and Torralba, 2001; Torralba et al., 2003] need to have enough representative training examples of scenes from pre-defined scene classes for a classification to be successful – with a reported average precision of 83.7% tested over a dataset of 2600 images. Addressing all these visual challenges is clearly a formidable task which is beyond the scope of this paper. Our focus instead </context>
</contexts>
<marker>Yang, Wang, Mori, 2010</marker>
<rawString>Yang, W., Wang, Y., and Mori, G. (2010). Recognizing human actions from still images with latent poses. In CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Yao</author>
<author>L Fei-Fei</author>
</authors>
<title>Grouplet: a structured image representation for recognizing human and object interactions.</title>
<date>2010</date>
<booktitle>In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="5408" citStr="Yao and Fei-Fei, 2010" startWordPosition="890" endWordPosition="893">tic images (known as an “implied action”) is often achieved by detecting the pose of humans in the image: the position of the limbs with respect to one another, under the assumption that a unique pose occurs for a unique action. Clearly, this assumption is weak as 1) similar actions may be represented by different poses due to the inherent dynamic nature of the action itself: e.g. walking a dog and 2) different actions may have the same pose: e.g. walking a dog versus running (Fig. 2(b)). The missing component here is whether the key object (dog) under interaction is considered. Recent works [Yao and Fei-Fei, 2010; Yang et al., 2010] that used poses for recognition of actions achieved 70% and 61% accuracy respectively under extremely limited testing conditions with only 5-6 action classes each. Finally, state of the art scene detectors [Oliva and Torralba, 2001; Torralba et al., 2003] need to have enough representative training examples of scenes from pre-defined scene classes for a classification to be successful – with a reported average precision of 83.7% tested over a dataset of 2600 images. Addressing all these visual challenges is clearly a formidable task which is beyond the scope of this paper.</context>
</contexts>
<marker>Yao, Fei-Fei, 2010</marker>
<rawString>Yao, B. and Fei-Fei, L. (2010). Grouplet: a structured image representation for recognizing human and object interactions. In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Yao</author>
<author>X Yang</author>
<author>L Lin</author>
<author>M W Lee</author>
<author>S-C Zhu</author>
</authors>
<title>I2t: Image parsing to text description.</title>
<date>2010</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>98</volume>
<issue>8</issue>
<pages>1508</pages>
<contexts>
<context position="8454" citStr="Yao et al., 2010" startWordPosition="1405" endWordPosition="1408">been tested on complex everyday images where the large variations of objects and poses makes it nearly impossible to learn a more general model. In addition, no attempt was made to generate a descriptive sentence from the learned model. The work of [Farhadi et al., 2010] attempts to “generate” sentences by first learning from a set of human annotated examples, and producing the same sentence if both images and sentence share common properties in terms of their triplets: (Nouns-Verbs-Scenes). No attempt was made to generate novel sentences from images beyond what has been annotated by humans. [Yao et al., 2010] has recently introduced a framework for parsing images/videos to textual description that requires significant annotated data, a requirement that our proposed approach avoids. Natural language generation (NLG) is a longstanding problem. Classic approaches [Traum et al., 2003] are based on three steps: selection, planning and realization. A common challenge in generation problems is the question of: what is the input? Recently, approaches for generation have focused on formal specification inputs, such as the output of theorem provers [McKeown, 2009] or databases [Golland et al., 2010]. Most </context>
</contexts>
<marker>Yao, Yang, Lin, Lee, Zhu, 2010</marker>
<rawString>Yao, B., Yang, X., Lin, L., Lee, M. W., and Zhu, S.-C. (2010). I2t: Image parsing to text description. Proceedings of the IEEE, 98(8):1485 –1508.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>