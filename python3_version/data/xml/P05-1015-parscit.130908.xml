<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.993461">
Seeing stars: Exploiting class relationships for sentiment categorization with
respect to rating scales
</title>
<author confidence="0.848237">
Bo Pang and Lillian Lee
</author>
<affiliation confidence="0.521486">
(1) Department of Computer Science, Cornell University
</affiliation>
<listItem confidence="0.8544145">
(2) Language Technologies Institute, Carnegie Mellon University
(3) Computer Science Department, Carnegie Mellon University
</listItem>
<sectionHeader confidence="0.985277" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998612">
We address the rating-inference problem,
wherein rather than simply decide whether
a review is “thumbs up” or “thumbs
down”, as in previous sentiment analy-
sis work, one must determine an author’s
evaluation with respect to a multi-point
scale (e.g., one to five “stars”). This task
represents an interesting twist on stan-
dard multi-class text categorization be-
cause there are several different degrees
of similarity between class labels; for ex-
ample, “three stars” is intuitively closer to
“four stars” than to “one star”.
We first evaluate human performance at
the task. Then, we apply a meta-
algorithm, based on a metric labeling for-
mulation of the problem, that alters a
given-ary classifier’s output in an ex-
plicit attempt to ensure that similar items
receive similar labels. We show that
the meta-algorithm can provide signifi-
cant improvements over both multi-class
and regression versions of SVMs when we
employ a novel similarity measure appro-
priate to the problem.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999280351351351">
There has recently been a dramatic surge of inter-
est in sentiment analysis, as more and more people
become aware of the scientific challenges posed and
the scope of new applications enabled by the pro-
cessing of subjective language. (The papers col-
lected by Qu, Shanahan, and Wiebe (2004) form a
representative sample of research in the area.) Most
prior work on the specific problem of categorizing
expressly opinionated text has focused on the bi-
nary distinction of positive vs. negative (Turney,
2002; Pang, Lee, and Vaithyanathan, 2002; Dave,
Lawrence, and Pennock, 2003; Yu and Hatzivas-
siloglou, 2003). But it is often helpful to have more
information than this binary distinction provides, es-
pecially if one is ranking items by recommendation
or comparing several reviewers’ opinions: example
applications include collaborative filtering and de-
ciding which conference submissions to accept.
Therefore, in this paper we consider generalizing
to finer-grained scales: rather than just determine
whether a review is “thumbs up” or not, we attempt
to infer the author’s implied numerical rating, such
as “three stars” or “four stars”. Note that this differs
from identifying opinion strength (Wilson, Wiebe,
and Hwa, 2004): rants and raves have the same
strength but represent opposite evaluations, and ref-
eree forms often allow one to indicate that one is
very confident (high strength) that a conference sub-
mission is mediocre (middling rating). Also, our
task differs from ranking not only because one can
be given a single item to classify (as opposed to a
set of items to be ordered relative to one another),
but because there are settings in which classification
is harder than ranking, and vice versa.
One can apply standard-ary classifiers or regres-
sion to this rating-inference problem; independent
work by Koppel and Schler (2005) considers such
</bodyText>
<page confidence="0.982398">
115
</page>
<note confidence="0.991652">
Proceedings of the 43rd Annual Meeting of the ACL, pages 115–124,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.998861121212121">
methods. But an alternative approach that explic-
itly incorporates information about item similarities
together with label similarity information (for in-
stance, “one star” is closer to “two stars” than to
“four stars”) is to think of the task as one of met-
ric labeling (Kleinberg and Tardos, 2002), where
label relations are encoded via a distance metric.
This observation yields a meta-algorithm, applicable
to both semi-supervised (via graph-theoretic tech-
niques) and supervised settings, that alters a given
-ary classifier’s output so that similar items tend to
be assigned similar labels.
In what follows, we first demonstrate that hu-
mans can discern relatively small differences in (hid-
den) evaluation scores, indicating that rating infer-
ence is indeed a meaningful task. We then present
three types of algorithms — one-vs-all, regression,
and metric labeling — that can be distinguished by
how explicitly they attempt to leverage similarity
between items and between labels. Next, we con-
sider what item similarity measure to apply, propos-
ing one based on the positive-sentence percentage.
Incorporating this new measure within the metric-
labeling framework is shown to often provide sig-
nificant improvements over the other algorithms.
We hope that some of the insights derived here
might apply to other scales for text classifcation that
have been considered, such as clause-level opin-
ion strength (Wilson, Wiebe, and Hwa, 2004); af-
fect types like disgust (Subasic and Huettner, 2001;
Liu, Lieberman, and Selker, 2003); reading level
(Collins-Thompson and Callan, 2004); and urgency
or criticality (Horvitz, Jacobs, and Hovel, 1999).
</bodyText>
<sectionHeader confidence="0.752316" genericHeader="introduction">
2 Problem validation and formulation
</sectionHeader>
<bodyText confidence="0.9998546">
We first ran a small pilot study on human subjects
in order to establish a rough idea of what a reason-
able classification granularity is: if even people can-
not accurately infer labels with respect to a five-star
scheme with half stars, say, then we cannot expect a
learning algorithm to do so. Indeed, some potential
obstacles to accurate rating inference include lack
of calibration (e.g., what an understated author in-
tends as high praise may seem lukewarm), author
inconsistency at assigning fine-grained ratings, and
</bodyText>
<table confidence="0.99898">
Rating diff. Pooled Subject 1 Subject 2
or more 100% 100%(35) 100%(15)
2 (e.g., 1 star) 83% 77% (30) 100% (11)
1 (e.g.,star) 69% 65% (57) 90% (10)
0 55% 47% (15) 80% ( 5)
</table>
<tableCaption confidence="0.84803125">
Table 1: Human accuracy at determining relative
positivity. Rating differences are given in “notches”.
Parentheses enclose the number of pairs attempted.
ratings not entirely supported by the text1.
</tableCaption>
<bodyText confidence="0.999958517241379">
For data, we first collected Internet movie reviews
in English from four authors, removing explicit rat-
ing indicators from each document’s text automati-
cally. Now, while the obvious experiment would be
to ask subjects to guess the rating that a review rep-
resents, doing so would force us to specify a fixed
rating-scale granularity in advance. Instead, we ex-
amined people’s ability to discern relative differ-
ences, because by varying the rating differences rep-
resented by the test instances, we can evaluate mul-
tiple granularities in a single experiment. Specifi-
cally, at intervals over a number of weeks, we au-
thors (a non-native and a native speaker of English)
examined pairs of reviews, attemping to determine
whether the first review in each pair was (1) more
positive than, (2) less positive than, or (3) as posi-
tive as the second. The texts in any particular review
pair were taken from the same author to factor out
the effects of cross-author divergence.
As Table 1 shows, both subjects performed per-
fectly when the rating separation was at least 3
“notches” in the original scale (we define a notch
as a half star in a four- or five-star scheme and 10
points in a 100-point scheme). Interestingly, al-
though human performance drops as rating differ-
ence decreases, even at a one-notch separation, both
subjects handily outperformed the random-choice
baseline of 33%. However, there was large variation
in accuracy between subjects.2
</bodyText>
<footnote confidence="0.892392444444444">
1For example, the critic Dennis Schwartz writes that “some-
times the review itself [indicates] the letter grade should have
been higher or lower, as the review might fail to take into con-
sideration my overall impression of the film — which I hope to
capture in the grade” (http://www.sover.net/˜ozus/cinema.htm).
2One contributing factor may be that the subjects viewed
disjoint document sets, since we wanted to maximize experi-
mental coverage of the types of document pairs within each dif-
ference class. We thus cannot report inter-annotator agreement,
</footnote>
<page confidence="0.998743">
116
</page>
<bodyText confidence="0.999777113636364">
Because of this variation, we defined two differ-
ent classification regimes. From the evidence above,
a three-class task (categories 0, 1, and 2 — es-
sentially “negative”, “middling”, and “positive”, re-
spectively) seems like one that most people would
do quite well at (but we should not assume 100%
human accuracy: according to our one-notch re-
sults, people may misclassify borderline cases like
2.5 stars). Our study also suggests that people could
do at least fairly well at distinguishing full stars in
a zero- to four-star scheme. However, when we
began to construct five-category datasets for each
of our four authors (see below), we found that in
each case, either the most negative or the most pos-
itive class (but not both) contained only about 5%
of the documents. To make the classes more bal-
anced, we folded these minority classes into the ad-
jacent class, thus arriving at a four-class problem
(categories 0-3, increasing in positivity). Note that
the four-class problem seems to offer more possi-
bilities for leveraging class relationship information
than the three-class setting, since it involves more
class pairs. Also, even the two-category version of
the rating-inference problem for movie reviews has
proven quite challenging for many automated clas-
sification techniques (Pang, Lee, and Vaithyanathan,
2002; Turney, 2002).
We applied the above two labeling schemes to
a scale dataset3 containing four corpora of movie
reviews. All reviews were automatically pre-
processed to remove both explicit rating indicators
and objective sentences; the motivation for the latter
step is that it has previously aided positive vs. neg-
ative classification (Pang and Lee, 2004). All of the
1770, 902, 1307, or 1027 documents in a given cor-
pus were written by the same author. This decision
facilitates interpretation of the results, since it fac-
tors out the effects of different choices of methods
for calibrating authors’ scales.4 We point out that
but since our goal is to recover a reviewer’s “true” recommen-
dation, reader-author agreement is more relevant.
While another factor might be degree of English fluency, in
an informal experiment (six subjects viewing the same three
pairs), native English speakers made the only two errors.
</bodyText>
<footnote confidence="0.9906755">
3Available at http://www.cs.cornell.edu/People/pabo/movie-
review-data as scale dataset v1.0.
4From the Rotten Tomatoes website’s FAQ: “star systems
are not consistent between critics. For critics like Roger Ebert
and James Berardinelli, 2.5 stars or lower out of 4 stars is al-
ways negative. For other critics, 2.5 stars can either be positive
</footnote>
<bodyText confidence="0.999614666666667">
it is possible to gather author-specific information
in some practical applications: for instance, systems
that use selected authors (e.g., the Rotten Tomatoes
movie-review website — where, we note, not all
authors provide explicit ratings) could require that
someone submit rating-labeled samples of newly-
admitted authors’ work. Moreover, our results at
least partially generalize to mixed-author situations
(see Section 5.2).
</bodyText>
<sectionHeader confidence="0.995391" genericHeader="method">
3 Algorithms
</sectionHeader>
<bodyText confidence="0.999760818181818">
Recall that the problem we are considering is multi-
category classification in which the labels can be
naturally mapped to a metric space (e.g., points on a
line); for simplicity, we assume the distance metric
throughout. In this section, we
present three approaches to this problem in order of
increasingly explicit use of pairwise similarity infor-
mation between items and between labels. In order
to make comparisons between these methods mean-
ingful, we base all three of them on Support Vec-
tor Machines (SVMs) as implemented in Joachims’
</bodyText>
<listItem confidence="0.598579">
(1999) package.
</listItem>
<subsectionHeader confidence="0.991231">
3.1 One-vs-all
</subsectionHeader>
<bodyText confidence="0.999972692307692">
The standard SVM formulation applies only to bi-
nary classification. One-vs-all (OVA) (Rifkin and
Klautau, 2004) is a common extension to the-ary
case. Training consists of building, for each label,
an SVM binary classifier distinguishing labelfrom
“not-”. We consider the final output to be a label
preference function , defined as the signed
distance of (test) item to the side of the vs.
not-decision plane.
Clearly, OVA makes no explicit use of pairwise
label or item relationships. However, it can perform
well if each class exhibits sufficiently distinct lan-
guage; see Section 4 for more discussion.
</bodyText>
<subsectionHeader confidence="0.995011">
3.2 Regression
</subsectionHeader>
<bodyText confidence="0.996429">
Alternatively, we can take a regression perspective
by assuming that the labels come from a discretiza-
tion of a continuous function mapping from the
or negative. Even though Eric Lurio uses a 5 star system, his
grading is very relaxed. So, 2 stars can be positive.” Thus,
calibration may sometimes require strong familiarity with the
authors involved, as anyone who has ever needed to reconcile
conflicting referee reports probably knows.
</bodyText>
<page confidence="0.990993">
117
</page>
<sectionHeader confidence="0.523416" genericHeader="method">
5
</sectionHeader>
<bodyText confidence="0.99995780952381">
feature space to a metric space.If we choose
from a family of sufficiently “gradual” functions,
then similar items necessarily receive similar labels.
In particular, we consider linear,-insensitive SVM
regression (Vapnik, 1995; Smola and Sch¨olkopf,
1998); the idea is to find the hyperplane that best fits
the training data, but where training points whose la-
bels are within distanceof the hyperplane incur no
loss. Then, for (test) instance, the label preference
function is the negative of the distance be-
tweenand the value predicted for by the fitted
hyperplane function.
Wilson, Wiebe, and Hwa (2004) used SVM re-
gression to classify clause-level strength of opinion,
reporting that it provided lower accuracy than other
methods. However, independently of our work,
Koppel and Schler (2005) found that applying lin-
ear regression to classify documents (in a different
corpus than ours) with respect to a three-point rat-
ing scale provided greater accuracy than OVA SVMs
and other algorithms.
</bodyText>
<subsectionHeader confidence="0.999634">
3.3 Metric labeling
</subsectionHeader>
<bodyText confidence="0.99991218">
Regression implicitly encodes the “similar items,
similar labels” heuristic, in that one can restrict
consideration to “gradual” functions. But we can
also think of our task as a metric labeling prob-
lem (Kleinberg and Tardos, 2002), a special case
of the maximum a posteriori estimation problem
for Markov random fields, to explicitly encode our
desideratum. Suppose we have an initial label pref-
erence function , perhaps computed via one
of the two methods described above. Also, let
be a distance metric on labels, and let de-
note the nearest neighbors of item according
to some item-similarity function . Then, it is
quite natural to pose our problem as finding a map-
ping of instances to labels (respecting the orig-
inal labels of the training instances) that minimizes
learning6 (Atkeson, Moore, and Schaal, 1997).) In a
sense, we are using explicit item and label similarity
information to increasingly penalize the initial clas-
sifier as it assigns more divergent labels to similar
items.
In this paper, we only report supervised-learning
experiments in which the nearest neighbors for any
given test item were drawn from the training set
alone. In such a setting, the labeling decisions for
different test items are independent, so that solving
the requisite optimization problem is simple.
Aside: transduction The above formulation also
allows for transductive semi-supervised learning as
well, in that we could allow nearest neighbors to
come from both the training and test sets. We
intend to address this case in future work, since
there are important settings in which one has a
small number of labeled reviews and a large num-
ber of unlabeled reviews, in which case consider-
ing similarities between unlabeled texts could prove
quite helpful. In full generality, the correspond-
ing multi-label optimization problem is intractable,
but for many families of functions (e.g., con-
vex) there exist practical exact or approximation
algorithms based on techniques for finding mini-
mum s-t cuts in graphs (Ishikawa and Geiger, 1998;
Boykov, Veksler, and Zabih, 1999; Ishikawa, 2003).
Interestingly, previous sentiment analysis research
found that a minimum-cut formulation for the binary
subjective/objective distinction yielded good results
(Pang and Lee, 2004). Of course, there are many
other related semi-supervised learning algorithms
that we would like to try as well; see Zhu (2005)
for a survey.
</bodyText>
<sectionHeader confidence="0.9004185" genericHeader="method">
4 Class struggle: finding a label-correlated
item-similarity function
</sectionHeader>
<bodyText confidence="0.999794166666667">
We need to specify an item similarity function
to use the metric-labeling formulation described in
Section 3.3. We could, as is commonly done, em-
ploy a term-overlap-based measure such as the co-
sine between term-frequency-based document vec-
tors (henceforth “TO(cos)”). However, Table 2
</bodyText>
<footnote confidence="0.770367">
6If we ignore the term, different choices ofcor-
</footnote>
<bodyText confidence="0.957076375">
respond to different versions of nearest-neighbor learning, e.g.,
majority-vote, weighted average of labels, or weighted median
of labels.
test
where is monotonically increasing (we chose
unless otherwise specified) and is a
trade-off and/or scaling parameter. (The inner sum-
mation is familiar from work in locally-weighted
</bodyText>
<footnote confidence="0.945249">
5We discuss the ordinal regression variant in Section 6.
</footnote>
<page confidence="0.962178">
118
</page>
<table confidence="0.999787">
Label difference:
1 2 3
Three-class data 37% 33% —
Four-class data 34% 31% 30%
</table>
<tableCaption confidence="0.776244">
Table 2: Average over authors and class pairs of
between-class vocabulary overlap as the class labels
of the pair grow farther apart.
</tableCaption>
<bodyText confidence="0.999724277777778">
shows that in aggregate, the vocabularies of distant
classes overlap to a degree surprisingly similar to
that of the vocabularies of nearby classes. Thus,
item similarity as measured by TO(cos) may not cor-
relate well with similarity of the item’s true labels.
We can potentially develop a more useful similar-
ity metric by asking ourselves what, intuitively, ac-
counts for the label relationships that we seek to ex-
ploit. A simple hypothesis is that ratings can be de-
termined by the positive-sentence percentage (PSP)
of a text, i.e., the number of positive sentences di-
vided by the number of subjective sentences. (Term-
based versions of this premise have motivated much
sentiment-analysis work for over a decade (Das and
Chen, 2001; Tong, 2001; Turney, 2002).) But coun-
terexamples are easy to construct: reviews can con-
tain off-topic opinions, or recount many positive as-
pects before describing a fatal flaw.
We therefore tested the hypothesis as follows.
To avoid the need to hand-label sentences as posi-
tive or negative, we first created a sentence polarity
dataset7 consisting of 10,662 movie-review “snip-
pets” (a striking extract usually one sentence long)
downloaded from www.rottentomatoes.com; each
snippet was labeled with its source review’s label
(positive or negative) as provided by Rotten Toma-
toes. Then, we trained a Naive Bayes classifier on
this data set and applied it to our scale dataset to
identify the positive sentences (recall that objective
sentences were already removed).
Figure 1 shows that all four authors tend to ex-
hibit a higher PSP when they write a more pos-
itive review, and we expect that most typical re-
viewers would follow suit. Hence, PSP appears to
be a promising basis for computing document sim-
ilarity for our rating-inference task. In particular,
</bodyText>
<footnote confidence="0.9881255">
7Available at http://www.cs.cornell.edu/People/pabo/movie-
review-data as sentence polarity dataset v1.0.
</footnote>
<bodyText confidence="0.98987175">
we defined to be the two-dimensional vec-
tor , and then set the item-
similarity function required by the metric-labeling
optimization function (Section 3.3) to
</bodyText>
<figure confidence="0.986894">
s
Positive-sentence percentage (PSP) statistics
0 2 4 6 8 10
rating (in notches)
</figure>
<figureCaption confidence="0.956855">
Figure 1: Average and standard deviation of PSP for
reviews expressing different ratings.
</figureCaption>
<bodyText confidence="0.999877375">
But before proceeding, we note that it is possi-
ble that similarity information might yield no extra
benefit at all. For instance, we don’t need it if we
can reliably identify each class just from some set
of distinguishing terms. If we define such terms
as frequent ones ( ) that appear in a sin-
gle class 50% or more of the time, then we do find
many instances; some examples for one author are:
“meaningless”, “disgusting” (class 0); “pleasant”,
“uneven” (class 1); and “oscar”, “gem” (class 2)
for the three-class case, and, in the four-class case,
“flat”, “tedious” (class 1) versus “straightforward”,
“likeable” (class 2). Some unexpected distinguish-
ing terms for this author are “lion” for class 2 (three-
class case), and for class 2 in the four-class case,
“jennifer”, for a wide variety of Jennifers.
</bodyText>
<sectionHeader confidence="0.999013" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9998695">
This section compares the accuracies of the ap-
proaches outlined in Section 3 on the four corpora
comprising our scale dataset. (Results usinger-
ror were qualitatively similar.) Throughout, when
</bodyText>
<footnote confidence="0.771099">
8While admittedly we initially chose this function because
it was convenient to work with cosines, post hoc analysis re-
vealed that the corresponding metric space “stretched” certain
distances in a useful way.
</footnote>
<figure confidence="0.978486857142857">
mean and standard deviation of PSP
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Author a
Author b
Author c
Author d
</figure>
<page confidence="0.914594">
119
</page>
<bodyText confidence="0.965466647058824">
we refer to something as “significant”, we mean sta-
tistically so with respect to the paired-test, .
The results that follow are based on ’s
default parameter settings for SVM regression and
OVA. Preliminary analysis of the effect of varying
the regression parameterin the four-class case re-
vealed that the default value was often optimal.
The notation “AB” denotes metric labeling
where method A provides the initial label preference
function and B serves as similarity measure. To
train, we first select the meta-parameters and
by running 9-fold cross-validation within the train-
ing set. Fixing and to those values yielding the
best performance, we then re-train A (but with SVM
parameters fixed, as described above) on the whole
training set. At test time, the nearest neighbors of
each item are also taken from the full training set.
</bodyText>
<subsectionHeader confidence="0.987452">
5.1 Main comparison
</subsectionHeader>
<bodyText confidence="0.999952326923077">
Figure 2 summarizes our average 10-fold cross-
validation accuracy results. We first observe from
the plots that all the algorithms described in Section
3 always definitively outperform the simple baseline
of predicting the majority class, although the im-
provements are smaller in the four-class case. In-
cidentally, the data was distributed in such a way
that the absolute performance of the baseline it-
self does not change much between the three- and
four-class case (which implies that the three-class
datasets were relatively more balanced); and Author
c’s datasets seem noticeably easier than the others.
We now examine the effect of implicitly using la-
bel and item similarity. In the four-class case, re-
gression performed better than OVA (significantly
so for two authors, as shown in the righthand ta-
ble); but for the three-category task, OVA signifi-
cantly outperforms regression for all four authors.
One might initially interprete this “flip” as showing
that in the four-class scenario, item and label simi-
larities provide a richer source of information rela-
tive to class-specific characteristics, especially since
for the non-majority classes there is less data avail-
able; whereas in the three-class setting the categories
are better modeled as quite distinct entities.
However, the three-class results for metric label-
ing on top of OVA and regression (shown in Figure 2
by black versions of the corresponding icons) show
that employing explicit similarities always improves
results, often to a significant degree, and yields the
best overall accuracies. Thus, we can in fact effec-
tively exploit similarities in the three-class case. Ad-
ditionally, in both the three- and four- class scenar-
ios, metric labeling often brings the performance of
the weaker base method up to that of the stronger
one (as indicated by the “disappearance” of upward
triangles in corresponding table rows), and never
hurts performance significantly.
In the four-class case, metric labeling and regres-
sion seem roughly equivalent. One possible inter-
pretation is that the relevant structure of the problem
is already captured by linear regression (and per-
haps a different kernel for regression would have
improved its three-class performance). However,
according to additional experiments we ran in the
four-class situation, the test-set-optimal parameter
settings for metric labeling would have produced
significant improvements, indicating there may be
greater potential for our framework. At any rate, we
view the fact that metric labeling performed quite
well for both rating scales as a definitely positive re-
sult.
</bodyText>
<subsectionHeader confidence="0.984675">
5.2 Further discussion
</subsectionHeader>
<bodyText confidence="0.99554247826087">
Q: Metric labeling looks like it’s just combining
SVMs with nearest neighbors, and classifier combi-
nation often improves performance. Couldn’t we get
the same kind of results by combining SVMs with
any other reasonable method?
A: No. For example, if we take the strongest
base SVM method for initial label preferences, but
replace PSP with the term-overlap-based cosine
(TO(cos)), performance often drops significantly.
This result, which is in accordance with Section
4’s data, suggests that choosing an item similarity
function that correlates well with label similarity
is important. (ovaPSP ovaTO(cos) [3c];
regPSPregTO(cos) [4c])
Q: Could you explain that notation, please?
A: Triangles point toward the significantly bet-
ter algorithm for some dataset. For instance,
“M N [3c]” means, “In the 3-class task, method
M is significantly better than N for two author
datasets and significantly worse for one dataset (so
the algorithms were statistically indistinguishable on
the remaining dataset)”. When the algorithms be-
ing compared are statistically indistinguishable on
</bodyText>
<page confidence="0.965294">
120
</page>
<figure confidence="0.985226272727273">
Average accuracies, three-class data Average accuracies, four-class data
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.35
majority
ova
ova+PSP
reg
reg+PSP
majority
ova
ova+PSP
reg
reg+PSP
Author a Author b Author c Author d
Author a Author b Author c Author d
</figure>
<bodyText confidence="0.74758625">
Average ten-fold cross-validation accuracies. Open icons: SVMs in either one-versus-all (square) or re-
gression (circle) mode; dark versions: metric labeling using the corresponding SVM together with the
positive-sentence percentage (PSP). The-axes of the two plots are aligned.
Significant differences, three-class data Significant differences, four-class data
</bodyText>
<figure confidence="0.9878319">
reg
reg+PSP
ova
ova+PSP
a b c d a b c d a b c d a b c d
.. . . ..
ova ova+PSP reg reg+PSP
. .
. .. .
..
ova ova+PSP reg reg+PSP
a b c d a b c d a b c d a b c d
ova
ova+PSP
reg
reg+PSP
. . . . .
. . . . . . .
. . . . . . . . .
. . . . . . . . .
</figure>
<figureCaption confidence="0.9304114">
Triangles point towards significantly better algorithms for the results plotted above. Specifically, if the
difference between a row and a column algorithm for a given author dataset (a, b, c, or d) is significant, a
triangle points to the better one; otherwise, a dot (.) is shown. Dark icons highlight the effect of adding PSP
information via metric labeling.
Figure 2: Results for main experimental comparisons.
</figureCaption>
<bodyText confidence="0.999749375">
all four datasets (the “no triangles” case), we indi-
cate this with an equals sign (“=”).
Q: Thanks. Doesn’t Figure 1 show that the
positive-sentence percentage would be a good
classifier even in isolation, so metric labeling isn’t
necessary?
A: No. Predicting class labels directly from
the PSP value via trained thresholds isn’t as
effective (ovaPSP threshold PSP [3c];
regPSP threshold PSP [4c]).
Alternatively, we could use only the PSP com-
ponent of metric labeling by setting the la-
bel preference function to the constant function
0, but even with test-set-optimal parameter set-
tings, doing so underperforms the trained met-
ric labeling algorithm with access to an ini-
</bodyText>
<subsectionHeader confidence="0.616366">
tial SVM classifier (ovaPSP 0 [3c];
</subsectionHeader>
<bodyText confidence="0.998174428571428">
regPSP 0 [4c]).
Q: What about using PSP as one of the features for
input to a standard classifier?
A: Our focus is on investigating the utility of simi-
larity information. In our particular rating-inference
setting, it so happens that the basis for our pair-
wise similarity measure can be incorporated as an
</bodyText>
<page confidence="0.995874">
121
</page>
<bodyText confidence="0.999741535211268">
item-specific feature, but we view this as a tan-
gential issue. That being said, preliminary experi-
ments show that metric labeling can be better, barely
(for test-set-optimal parameter settings for both al-
gorithms: significantly better results for one author,
four-class case; statistically indistinguishable other-
wise), although one needs to determine an appropri-
ate weight for the PSP feature to get good perfor-
mance.
Q: You defined the “metric transformation” func-
tion as the identity function , imposing
greater loss as the distance between labels assigned
to two similar items increases. Can you do just as
well if you penalize all non-equal label assignments
by the same amount, or does the distance between
labels really matter?
A: You’re asking for a comparison to the Potts
model, which sets to the function
if , otherwise. In the one set-
ting in which there is a significant difference
between the two, the Potts model does worse
(ovaPSPovaPSP [3c]). Also, employing the
Potts model generally leads to fewer significant
improvements over a chosen base method (com-
pare Figure 2’s tables with: regPSPreg [3c];
ovaPSP ova [3c]; ovaPSP ova [4c]; but
note that regPSPreg [4c]). We note that opti-
mizing the Potts model in the multi-label case is NP-
hard, whereas the optimal metric labeling with the
identity metric-transformation function can be effi-
ciently obtained (see Section 3.3).
Q: Your datasets had many labeled reviews and only
one author each. Is your work relevant to settings
with many authors but very little data for each?
A: As discussed in Section 2, it can be quite dif-
ficult to properly calibrate different authors’ scales,
since the same number of “stars” even within what
is ostensibly the same rating system can mean differ-
ent things for different authors. But since you ask:
we temporarily turned a blind eye to this serious is-
sue, creating a collection of 5394 reviews by 496 au-
thors with at most 80 reviews per author, where we
pretended that our rating conversions mapped cor-
rectly into a universal rating scheme. Preliminary
results on this dataset were actually comparable to
the results reported above, although since we are
not confident in the class labels themselves, more
work is needed to derive a clear analysis of this set-
ting. (Abusing notation, since we’re already play-
ing fast and loose: [3c]: baseline 52.4%, reg 61.4%,
regPSP 61.5%, ova (65.4%)ovaPSP (66.3%);
[4c]: baseline 38.8%, reg (51.9%) regPSP
(52.7%), ova (53.8%)ovaPSP (54.6%))
In future work, it would be interesting to deter-
mine author-independent characteristics that can be
used on (or suitably adapted to) data for specific au-
thors.
Q: How about trying —
A: —Yes, there are many alternatives. A few
that we tested are described in the Appendix, and
we propose some others in the next section. We
should mention that we have not yet experimented
with all-vs.-all (AVA), another standard binary-to-
multi-category classifier conversion method, be-
cause we wished to focus on the effect of omit-
ting pairwise information. In independent work on
3-category rating inference for a different corpus,
Koppel and Schler (2005) found that regression out-
performed AVA, and Rifkin and Klautau (2004) ar-
gue that in principle OVA should do just as well as
AVA. But we plan to try it out.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="method">
6 Related work and future directions
</sectionHeader>
<bodyText confidence="0.999895666666667">
In this paper, we addressed the rating-inference
problem, showing the utility of employing label sim-
ilarity and (appropriate choice of) item similarity
— either implicitly, through regression, or explicitly
and often more effectively, through metric labeling.
In the future, we would like to apply our methods
to other scale-based classification problems, and ex-
plore alternative methods. Clearly, varying the ker-
nel in SVM regression might yield better results.
Another choice is ordinal regression (McCullagh,
1980; Herbrich, Graepel, and Obermayer, 2000),
which only considers the ordering on labels, rather
than any explicit distances between them; this ap-
proach could work well if a good metric on labels is
lacking. Also, one could use mixture models (e.g.,
combine “positive” and “negative” language mod-
els) to capture class relationships (McCallum, 1999;
Schapire and Singer, 2000; Takamura, Matsumoto,
and Yamada, 2004).
We are also interested in framing multi-class but
non-scale-based categorization problems as metric
</bodyText>
<page confidence="0.992504">
122
</page>
<bodyText confidence="0.941276625">
labeling tasks. For example, positive vs. negative vs.
neutral sentiment distinctions are sometimes consid-
ered in which neutral means either objective (En-
gstr¨om, 2004) or a conflation of objective with a rat-
ing of mediocre (Das and Chen, 2001). (Koppel and
Schler (2005) in independent work also discuss var-
ious types of neutrality.) In either case, we could
apply a metric in which positive and negative are
closer to objective (or objective+mediocre) than to
each other. As another example, hierarchical label
relationships can be easily encoded in a label met-
ric.
Finally, as mentioned in Section 3.3, we would
like to address the transductive setting, in which one
has a small amount of labeled data and uses rela-
tionships between unlabeled items, since it is par-
ticularly well-suited to the metric-labeling approach
and may be quite important in practice.
Acknowledgments We thank Paul Bennett, Dave Blei,
Claire Cardie, Shimon Edelman, Thorsten Joachims, Jon Klein-
berg, Oren Kurland, John Lafferty, Guy Lebanon, Pradeep
Ravikumar, Jerry Zhu, and the anonymous reviewers for many
very useful comments and discussion. We learned of Moshe
Koppel and Jonathan Schler’s work while preparing the camera-
ready version of this paper; we thank them for so quickly an-
swering our request for a pre-print. Our descriptions of their
work are based on that pre-print; we apologize in advance for
any inaccuracies in our descriptions that result from changes
between their pre-print and their final version. We also thank
CMU for its hospitality during the year. This paper is based
upon work supported in part by the National Science Founda-
tion (NSF) under grant no. IIS-0329064 and CCR-0122581;
SRI International under subcontract no. 03-000211 on their
project funded by the Department of the Interior’s National
Business Center; and by an Alfred P. Sloan Research Fellow-
ship. Any opinions, findings, and conclusions or recommen-
dations expressed are those of the authors and do not neces-
sarily reflect the views or official policies, either expressed or
implied, of any sponsoring institutions, the U.S. government, or
any other entity.
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999399966101695">
Atkeson, Christopher G., Andrew W. Moore, and Stefan Schaal.
1997. Locally weighted learning. Artificial Intelligence Re-
view, 11(1):11–73.
Boykov, Yuri, Olga Veksler, and Ramin Zabih. 1999. Fast ap-
proximate energy minimization via graph cuts. In Proceed-
ings of the International Conference on Computer Vision
(ICCV), pages 377–384. Journal version in IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (PAMI)
23(11):1222–1239, 2001.
Collins-Thompson, Kevyn and Jamie Callan. 2004. A language
modeling approach to predicting reading difficulty. In HLT-
NAACL: Proceedings of the Main Conference, pages 193–
200.
Das, Sanjiv and Mike Chen. 2001. Yahoo! for Amazon: Ex-
tracting market sentiment from stock message boards. In
Proceedings of the Asia Pacific Finance Association Annual
Conference (APFA).
Dave, Kushal, Steve Lawrence, and David M. Pennock. 2003.
Mining the peanut gallery: Opinion extraction and semantic
classification of product reviews. In Proceedings of WWW,
pages 519–528.
Engstr¨om, Charlotta. 2004. Topic dependence in sentiment
classification. Master’s thesis, University of Cambridge.
Herbrich, Ralf, Thore Graepel, and Klaus Obermayer. 2000.
Large margin rank boundaries for ordinal regression. In
Alexander J. Smola, Peter L. Bartlett, Bernhard Sch¨olkopf,
and Dale Schuurmans, editors, Advances in Large Margin
Classifiers, Neural Information Processing Systems. MIT
Press, pages 115–132.
Horvitz, Eric, Andy Jacobs, and David Hovel. 1999. Attention-
sensitive alerting. In Proceedings of the Conference on Un-
certainty and Artificial Intelligence, pages 305–313.
Ishikawa, Hiroshi. 2003. Exact optimization for Markov ran-
dom fields with convex priors. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 25(10).
Ishikawa, Hiroshi and Davi Geiger. 1998. Occlusions, discon-
tinuities, and epipolar lines in stereo. In Proceedings of the
5th European Conference on Computer Vision (ECCV), vol-
ume I, pages 232–248, London, UK. Springer-Verlag.
Joachims, Thorsten. 1999. Making large-scale SVM learning
practical. In Bernhard Sch¨olkopf and Alexander Smola, edi-
tors, Advances in Kernel Methods - Support Vector Learning.
MIT Press, pages 44–56.
Kleinberg, Jon and ´Eva Tardos. 2002. Approximation al-
gorithms for classification problems with pairwise relation-
ships: Metric labeling and Markov random fields. Journal
of the ACM, 49(5):616–639.
Koppel, Moshe and Jonathan Schler. 2005. The importance
of neutral examples for learning sentiment. In Workshop on
the Analysis of Informal and Formal Information Exchange
during Negotiations (FINEXIN).
Liu, Hugo, Henry Lieberman, and Ted Selker. 2003. A model
of textual affect sensing using real-world knowledge. In Pro-
ceedings of Intelligent User Interfaces (IUI), pages 125–132.
McCallum, Andrew. 1999. Multi-label text classification with
a mixture model trained by EM. In AAAI Workshop on Text
Learning.
McCullagh, Peter. 1980. Regression models for ordinal data.
Journal of the Royal Statistical Society, 42(2):109–42.
</reference>
<page confidence="0.986599">
123
</page>
<reference confidence="0.9705636">
Pang, Bo and Lillian Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of the ACL, pages 271–278.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment classification using machine learning
techniques. In Proceedings of EMNLP, pages 79–86.
Qu, Yan, James Shanahan, and Janyce Wiebe, editors. 2004.
Proceedings of the AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text: Theories and Applications.
AAAI Press. AAAI technical report SS-04-07.
Rifkin, Ryan M. and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. Journal of Machine Learning Re-
search, 5:101–141.
Schapire, Robert E. and Yoram Singer. 2000. BoosTexter:
A boosting-based system for text categorization. Machine
Learning, 39(2/3):135–168.
Smola, Alex J. and Bernhard Sch¨olkopf. 1998. A tuto-
rial on support vector regression. Technical Report Neuro-
COLT NC-TR-98-030, Royal Holloway College, University
of London.
Subasic, Pero and Alison Huettner. 2001. Affect analysis of
text using fuzzy semantic typing. IEEE Transactions on
Fuzzy Systems, 9(4):483–496.
Takamura, Hiroya, Yuji Matsumoto, and Hiroyasu Yamada.
2004. Modeling category structures with a kernel function.
In Proceedings of CoNLL, pages 57–64.
Tong, Richard M. 2001. An operational system for detecting
and tracking opinions in on-line discussion. SIGIR Work-
shop on Operational Text Classification.
Turney, Peter. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of reviews.
In Proceedings of the ACL, pages 417–424.
Vapnik, Vladimir. 1995. The Nature of Statistical Learning
Theory. Springer.
Wilson, Theresa, Janyce Wiebe, and Rebecca Hwa. 2004. Just
how mad are you? Finding strong and weak opinion clauses.
In Proceedings of AAAI, pages 761–769.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In Pro-
ceedings of EMNLP.
Zhu, Xiaojin (Jerry). 2005. Semi-Supervised Learning with
Graphs. Ph.D. thesis, Carnegie Mellon University.
A Appendix: other variations attempted
A.1 Discretizing binary classification
</reference>
<bodyText confidence="0.999805647058824">
In our setting, we can also incorporate class relations
by directly altering the output of a binary classifier,
as follows. We first train a standard SVM, treating
ratings greater than 0.5 as positive labels and others
as negative labels. If we then consider the resulting
classifier to output a positivity-preference function
, we can then learn a series of thresholds to
convert this value into the desired label set, under
the assumption that the bigger is, the more
positive the review.9 This algorithm always outper-
forms the majority-class baseline, but not to the de-
gree that the best of SVM OVA and SVM regres-
sion does. Koppel and Schler (2005) independently
found in a three-class study that thresholding a pos-
itive/negative classifier trained only on clearly posi-
tive or clearly negative examples did not yield large
improvements.
</bodyText>
<sectionHeader confidence="0.773543" genericHeader="method">
A.2 Discretizing regression
</sectionHeader>
<bodyText confidence="0.999965357142857">
In our experiments with SVM regression, we dis-
cretized regression output via a set of fixed decision
thresholds to map it into our set of
class labels. Alternatively, we can learn the thresh-
olds instead. Neither option clearly outperforms the
other in the four-class case. In the three-class set-
ting, the learned version provides noticeably better
performance in two of the four datasets. But these
results taken together still mean that in many cases,
the difference is negligible, and if we had started
down this path, we would have needed to consider
similar tweaks for one-vs-all SVM as well. We
therefore stuck with the simpler version in order to
maintain focus on the central issues at hand.
</bodyText>
<footnote confidence="0.6915268">
9This is not necessarily true: if the classifier’s goal is to opti-
mize binary classification error, its major concern is to increase
confidence in the positive/negative distinction, which may not
correspond to higher confidence in separating “five stars” from
“four stars”.
</footnote>
<page confidence="0.993501">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.652540">
<title confidence="0.989791">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
<author confidence="0.999981">Bo Pang</author>
<author confidence="0.999981">Lillian Lee</author>
<affiliation confidence="0.894096">(1) Department of Computer Science, Cornell University (2) Language Technologies Institute, Carnegie Mellon University (3) Computer Science Department, Carnegie Mellon University</affiliation>
<abstract confidence="0.996637846153846">address the wherein rather than simply decide whether a review is “thumbs up” or “thumbs down”, as in previous sentiment analysis work, one must determine an author’s evaluation with respect to a multi-point scale (e.g., one to five “stars”). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, “three stars” is intuitively closer to “four stars” than to “one star”. We first evaluate human performance at task. Then, we apply a based on a labeling formulation of the problem, that alters a given-ary classifier’s output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christopher G Atkeson</author>
<author>Andrew W Moore</author>
<author>Stefan Schaal</author>
</authors>
<title>Locally weighted learning.</title>
<date>1997</date>
<journal>Artificial Intelligence Review,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="14352" citStr="Atkeson, Moore, and Schaal, 1997" startWordPosition="2254" endWordPosition="2258">ric labeling problem (Kleinberg and Tardos, 2002), a special case of the maximum a posteriori estimation problem for Markov random fields, to explicitly encode our desideratum. Suppose we have an initial label preference function , perhaps computed via one of the two methods described above. Also, let be a distance metric on labels, and let denote the nearest neighbors of item according to some item-similarity function . Then, it is quite natural to pose our problem as finding a mapping of instances to labels (respecting the original labels of the training instances) that minimizes learning6 (Atkeson, Moore, and Schaal, 1997).) In a sense, we are using explicit item and label similarity information to increasingly penalize the initial classifier as it assigns more divergent labels to similar items. In this paper, we only report supervised-learning experiments in which the nearest neighbors for any given test item were drawn from the training set alone. In such a setting, the labeling decisions for different test items are independent, so that solving the requisite optimization problem is simple. Aside: transduction The above formulation also allows for transductive semi-supervised learning as well, in that we cou</context>
</contexts>
<marker>Atkeson, Moore, Schaal, 1997</marker>
<rawString>Atkeson, Christopher G., Andrew W. Moore, and Stefan Schaal. 1997. Locally weighted learning. Artificial Intelligence Review, 11(1):11–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri Boykov</author>
<author>Olga Veksler</author>
<author>Ramin Zabih</author>
</authors>
<title>Fast approximate energy minimization via graph cuts.</title>
<date>1999</date>
<journal>Journal version in IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</journal>
<booktitle>In Proceedings of the International Conference on Computer Vision (ICCV),</booktitle>
<volume>23</volume>
<issue>11</issue>
<pages>377--384</pages>
<contexts>
<context position="15601" citStr="Boykov, Veksler, and Zabih, 1999" startWordPosition="2450" endWordPosition="2454">neighbors to come from both the training and test sets. We intend to address this case in future work, since there are important settings in which one has a small number of labeled reviews and a large number of unlabeled reviews, in which case considering similarities between unlabeled texts could prove quite helpful. In full generality, the corresponding multi-label optimization problem is intractable, but for many families of functions (e.g., convex) there exist practical exact or approximation algorithms based on techniques for finding minimum s-t cuts in graphs (Ishikawa and Geiger, 1998; Boykov, Veksler, and Zabih, 1999; Ishikawa, 2003). Interestingly, previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee, 2004). Of course, there are many other related semi-supervised learning algorithms that we would like to try as well; see Zhu (2005) for a survey. 4 Class struggle: finding a label-correlated item-similarity function We need to specify an item similarity function to use the metric-labeling formulation described in Section 3.3. We could, as is commonly done, employ a term-overlap-based measure such as the </context>
</contexts>
<marker>Boykov, Veksler, Zabih, 1999</marker>
<rawString>Boykov, Yuri, Olga Veksler, and Ramin Zabih. 1999. Fast approximate energy minimization via graph cuts. In Proceedings of the International Conference on Computer Vision (ICCV), pages 377–384. Journal version in IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 23(11):1222–1239, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In HLTNAACL: Proceedings of the Main Conference,</booktitle>
<pages>193--200</pages>
<contexts>
<context position="4875" citStr="Collins-Thompson and Callan, 2004" startWordPosition="749" endWordPosition="752">larity between items and between labels. Next, we consider what item similarity measure to apply, proposing one based on the positive-sentence percentage. Incorporating this new measure within the metriclabeling framework is shown to often provide significant improvements over the other algorithms. We hope that some of the insights derived here might apply to other scales for text classifcation that have been considered, such as clause-level opinion strength (Wilson, Wiebe, and Hwa, 2004); affect types like disgust (Subasic and Huettner, 2001; Liu, Lieberman, and Selker, 2003); reading level (Collins-Thompson and Callan, 2004); and urgency or criticality (Horvitz, Jacobs, and Hovel, 1999). 2 Problem validation and formulation We first ran a small pilot study on human subjects in order to establish a rough idea of what a reasonable classification granularity is: if even people cannot accurately infer labels with respect to a five-star scheme with half stars, say, then we cannot expect a learning algorithm to do so. Indeed, some potential obstacles to accurate rating inference include lack of calibration (e.g., what an understated author intends as high praise may seem lukewarm), author inconsistency at assigning fin</context>
</contexts>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>Collins-Thompson, Kevyn and Jamie Callan. 2004. A language modeling approach to predicting reading difficulty. In HLTNAACL: Proceedings of the Main Conference, pages 193– 200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjiv Das</author>
<author>Mike Chen</author>
</authors>
<title>Yahoo! for Amazon: Extracting market sentiment from stock message boards.</title>
<date>2001</date>
<booktitle>In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA).</booktitle>
<contexts>
<context position="17670" citStr="Das and Chen, 2001" startWordPosition="2770" endWordPosition="2773"> the vocabularies of nearby classes. Thus, item similarity as measured by TO(cos) may not correlate well with similarity of the item’s true labels. We can potentially develop a more useful similarity metric by asking ourselves what, intuitively, accounts for the label relationships that we seek to exploit. A simple hypothesis is that ratings can be determined by the positive-sentence percentage (PSP) of a text, i.e., the number of positive sentences divided by the number of subjective sentences. (Termbased versions of this premise have motivated much sentiment-analysis work for over a decade (Das and Chen, 2001; Tong, 2001; Turney, 2002).) But counterexamples are easy to construct: reviews can contain off-topic opinions, or recount many positive aspects before describing a fatal flaw. We therefore tested the hypothesis as follows. To avoid the need to hand-label sentences as positive or negative, we first created a sentence polarity dataset7 consisting of 10,662 movie-review “snippets” (a striking extract usually one sentence long) downloaded from www.rottentomatoes.com; each snippet was labeled with its source review’s label (positive or negative) as provided by Rotten Tomatoes. Then, we trained a </context>
<context position="31980" citStr="Das and Chen, 2001" startWordPosition="5084" endWordPosition="5087">pproach could work well if a good metric on labels is lacking. Also, one could use mixture models (e.g., combine “positive” and “negative” language models) to capture class relationships (McCallum, 1999; Schapire and Singer, 2000; Takamura, Matsumoto, and Yamada, 2004). We are also interested in framing multi-class but non-scale-based categorization problems as metric 122 labeling tasks. For example, positive vs. negative vs. neutral sentiment distinctions are sometimes considered in which neutral means either objective (Engstr¨om, 2004) or a conflation of objective with a rating of mediocre (Das and Chen, 2001). (Koppel and Schler (2005) in independent work also discuss various types of neutrality.) In either case, we could apply a metric in which positive and negative are closer to objective (or objective+mediocre) than to each other. As another example, hierarchical label relationships can be easily encoded in a label metric. Finally, as mentioned in Section 3.3, we would like to address the transductive setting, in which one has a small amount of labeled data and uses relationships between unlabeled items, since it is particularly well-suited to the metric-labeling approach and may be quite impor</context>
</contexts>
<marker>Das, Chen, 2001</marker>
<rawString>Das, Sanjiv and Mike Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>519--528</pages>
<contexts>
<context position="1876" citStr="Dave, Lawrence, and Pennock, 2003" startWordPosition="285" endWordPosition="289">milarity measure appropriate to the problem. 1 Introduction There has recently been a dramatic surge of interest in sentiment analysis, as more and more people become aware of the scientific challenges posed and the scope of new applications enabled by the processing of subjective language. (The papers collected by Qu, Shanahan, and Wiebe (2004) form a representative sample of research in the area.) Most prior work on the specific problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative (Turney, 2002; Pang, Lee, and Vaithyanathan, 2002; Dave, Lawrence, and Pennock, 2003; Yu and Hatzivassiloglou, 2003). But it is often helpful to have more information than this binary distinction provides, especially if one is ranking items by recommendation or comparing several reviewers’ opinions: example applications include collaborative filtering and deciding which conference submissions to accept. Therefore, in this paper we consider generalizing to finer-grained scales: rather than just determine whether a review is “thumbs up” or not, we attempt to infer the author’s implied numerical rating, such as “three stars” or “four stars”. Note that this differs from identifyi</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Dave, Kushal, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of WWW, pages 519–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charlotta Engstr¨om</author>
</authors>
<title>Topic dependence in sentiment classification. Master’s thesis,</title>
<date>2004</date>
<institution>University of Cambridge.</institution>
<marker>Engstr¨om, 2004</marker>
<rawString>Engstr¨om, Charlotta. 2004. Topic dependence in sentiment classification. Master’s thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
<author>Thore Graepel</author>
<author>Klaus Obermayer</author>
</authors>
<title>Large margin rank boundaries for ordinal regression.</title>
<date>2000</date>
<booktitle>Advances in Large Margin Classifiers, Neural Information Processing Systems.</booktitle>
<pages>115--132</pages>
<editor>In Alexander J. Smola, Peter L. Bartlett, Bernhard Sch¨olkopf, and Dale Schuurmans, editors,</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="31258" citStr="Herbrich, Graepel, and Obermayer, 2000" startWordPosition="4973" endWordPosition="4977">t as well as AVA. But we plan to try it out. 6 Related work and future directions In this paper, we addressed the rating-inference problem, showing the utility of employing label similarity and (appropriate choice of) item similarity — either implicitly, through regression, or explicitly and often more effectively, through metric labeling. In the future, we would like to apply our methods to other scale-based classification problems, and explore alternative methods. Clearly, varying the kernel in SVM regression might yield better results. Another choice is ordinal regression (McCullagh, 1980; Herbrich, Graepel, and Obermayer, 2000), which only considers the ordering on labels, rather than any explicit distances between them; this approach could work well if a good metric on labels is lacking. Also, one could use mixture models (e.g., combine “positive” and “negative” language models) to capture class relationships (McCallum, 1999; Schapire and Singer, 2000; Takamura, Matsumoto, and Yamada, 2004). We are also interested in framing multi-class but non-scale-based categorization problems as metric 122 labeling tasks. For example, positive vs. negative vs. neutral sentiment distinctions are sometimes considered in which ne</context>
</contexts>
<marker>Herbrich, Graepel, Obermayer, 2000</marker>
<rawString>Herbrich, Ralf, Thore Graepel, and Klaus Obermayer. 2000. Large margin rank boundaries for ordinal regression. In Alexander J. Smola, Peter L. Bartlett, Bernhard Sch¨olkopf, and Dale Schuurmans, editors, Advances in Large Margin Classifiers, Neural Information Processing Systems. MIT Press, pages 115–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Horvitz</author>
<author>Andy Jacobs</author>
<author>David Hovel</author>
</authors>
<title>Attentionsensitive alerting.</title>
<date>1999</date>
<booktitle>In Proceedings of the Conference on Uncertainty and Artificial Intelligence,</booktitle>
<pages>305--313</pages>
<contexts>
<context position="4937" citStr="Horvitz, Jacobs, and Hovel, 1999" startWordPosition="757" endWordPosition="761">item similarity measure to apply, proposing one based on the positive-sentence percentage. Incorporating this new measure within the metriclabeling framework is shown to often provide significant improvements over the other algorithms. We hope that some of the insights derived here might apply to other scales for text classifcation that have been considered, such as clause-level opinion strength (Wilson, Wiebe, and Hwa, 2004); affect types like disgust (Subasic and Huettner, 2001; Liu, Lieberman, and Selker, 2003); reading level (Collins-Thompson and Callan, 2004); and urgency or criticality (Horvitz, Jacobs, and Hovel, 1999). 2 Problem validation and formulation We first ran a small pilot study on human subjects in order to establish a rough idea of what a reasonable classification granularity is: if even people cannot accurately infer labels with respect to a five-star scheme with half stars, say, then we cannot expect a learning algorithm to do so. Indeed, some potential obstacles to accurate rating inference include lack of calibration (e.g., what an understated author intends as high praise may seem lukewarm), author inconsistency at assigning fine-grained ratings, and Rating diff. Pooled Subject 1 Subject 2</context>
</contexts>
<marker>Horvitz, Jacobs, Hovel, 1999</marker>
<rawString>Horvitz, Eric, Andy Jacobs, and David Hovel. 1999. Attentionsensitive alerting. In Proceedings of the Conference on Uncertainty and Artificial Intelligence, pages 305–313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Ishikawa</author>
</authors>
<title>Exact optimization for Markov random fields with convex priors.</title>
<date>2003</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>25</volume>
<issue>10</issue>
<contexts>
<context position="15618" citStr="Ishikawa, 2003" startWordPosition="2455" endWordPosition="2456">aining and test sets. We intend to address this case in future work, since there are important settings in which one has a small number of labeled reviews and a large number of unlabeled reviews, in which case considering similarities between unlabeled texts could prove quite helpful. In full generality, the corresponding multi-label optimization problem is intractable, but for many families of functions (e.g., convex) there exist practical exact or approximation algorithms based on techniques for finding minimum s-t cuts in graphs (Ishikawa and Geiger, 1998; Boykov, Veksler, and Zabih, 1999; Ishikawa, 2003). Interestingly, previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee, 2004). Of course, there are many other related semi-supervised learning algorithms that we would like to try as well; see Zhu (2005) for a survey. 4 Class struggle: finding a label-correlated item-similarity function We need to specify an item similarity function to use the metric-labeling formulation described in Section 3.3. We could, as is commonly done, employ a term-overlap-based measure such as the cosine between te</context>
</contexts>
<marker>Ishikawa, 2003</marker>
<rawString>Ishikawa, Hiroshi. 2003. Exact optimization for Markov random fields with convex priors. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Ishikawa</author>
<author>Davi Geiger</author>
</authors>
<title>Occlusions, discontinuities, and epipolar lines in stereo.</title>
<date>1998</date>
<booktitle>In Proceedings of the 5th European Conference on Computer Vision (ECCV), volume I,</booktitle>
<pages>232--248</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="15567" citStr="Ishikawa and Geiger, 1998" startWordPosition="2446" endWordPosition="2449">hat we could allow nearest neighbors to come from both the training and test sets. We intend to address this case in future work, since there are important settings in which one has a small number of labeled reviews and a large number of unlabeled reviews, in which case considering similarities between unlabeled texts could prove quite helpful. In full generality, the corresponding multi-label optimization problem is intractable, but for many families of functions (e.g., convex) there exist practical exact or approximation algorithms based on techniques for finding minimum s-t cuts in graphs (Ishikawa and Geiger, 1998; Boykov, Veksler, and Zabih, 1999; Ishikawa, 2003). Interestingly, previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee, 2004). Of course, there are many other related semi-supervised learning algorithms that we would like to try as well; see Zhu (2005) for a survey. 4 Class struggle: finding a label-correlated item-similarity function We need to specify an item similarity function to use the metric-labeling formulation described in Section 3.3. We could, as is commonly done, employ a term-</context>
</contexts>
<marker>Ishikawa, Geiger, 1998</marker>
<rawString>Ishikawa, Hiroshi and Davi Geiger. 1998. Occlusions, discontinuities, and epipolar lines in stereo. In Proceedings of the 5th European Conference on Computer Vision (ECCV), volume I, pages 232–248, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>In Bernhard Sch¨olkopf and Alexander Smola, editors, Advances in Kernel Methods - Support Vector Learning.</booktitle>
<pages>44--56</pages>
<publisher>MIT Press,</publisher>
<marker>Joachims, 1999</marker>
<rawString>Joachims, Thorsten. 1999. Making large-scale SVM learning practical. In Bernhard Sch¨olkopf and Alexander Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press, pages 44–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Kleinberg</author>
<author>´Eva Tardos</author>
</authors>
<title>Approximation algorithms for classification problems with pairwise relationships: Metric labeling and Markov random fields.</title>
<date>2002</date>
<journal>Journal of the ACM,</journal>
<volume>49</volume>
<issue>5</issue>
<contexts>
<context position="3597" citStr="Kleinberg and Tardos, 2002" startWordPosition="556" endWordPosition="559">on is harder than ranking, and vice versa. One can apply standard-ary classifiers or regression to this rating-inference problem; independent work by Koppel and Schler (2005) considers such 115 Proceedings of the 43rd Annual Meeting of the ACL, pages 115–124, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics methods. But an alternative approach that explicitly incorporates information about item similarities together with label similarity information (for instance, “one star” is closer to “two stars” than to “four stars”) is to think of the task as one of metric labeling (Kleinberg and Tardos, 2002), where label relations are encoded via a distance metric. This observation yields a meta-algorithm, applicable to both semi-supervised (via graph-theoretic techniques) and supervised settings, that alters a given -ary classifier’s output so that similar items tend to be assigned similar labels. In what follows, we first demonstrate that humans can discern relatively small differences in (hidden) evaluation scores, indicating that rating inference is indeed a meaningful task. We then present three types of algorithms — one-vs-all, regression, and metric labeling — that can be distinguished by </context>
<context position="13769" citStr="Kleinberg and Tardos, 2002" startWordPosition="2158" endWordPosition="2161">ression to classify clause-level strength of opinion, reporting that it provided lower accuracy than other methods. However, independently of our work, Koppel and Schler (2005) found that applying linear regression to classify documents (in a different corpus than ours) with respect to a three-point rating scale provided greater accuracy than OVA SVMs and other algorithms. 3.3 Metric labeling Regression implicitly encodes the “similar items, similar labels” heuristic, in that one can restrict consideration to “gradual” functions. But we can also think of our task as a metric labeling problem (Kleinberg and Tardos, 2002), a special case of the maximum a posteriori estimation problem for Markov random fields, to explicitly encode our desideratum. Suppose we have an initial label preference function , perhaps computed via one of the two methods described above. Also, let be a distance metric on labels, and let denote the nearest neighbors of item according to some item-similarity function . Then, it is quite natural to pose our problem as finding a mapping of instances to labels (respecting the original labels of the training instances) that minimizes learning6 (Atkeson, Moore, and Schaal, 1997).) In a sense, w</context>
</contexts>
<marker>Kleinberg, Tardos, 2002</marker>
<rawString>Kleinberg, Jon and ´Eva Tardos. 2002. Approximation algorithms for classification problems with pairwise relationships: Metric labeling and Markov random fields. Journal of the ACM, 49(5):616–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
</authors>
<title>The importance of neutral examples for learning sentiment.</title>
<date>2005</date>
<booktitle>In Workshop on the Analysis of Informal and Formal Information Exchange during Negotiations (FINEXIN).</booktitle>
<contexts>
<context position="3144" citStr="Koppel and Schler (2005)" startWordPosition="485" endWordPosition="488"> 2004): rants and raves have the same strength but represent opposite evaluations, and referee forms often allow one to indicate that one is very confident (high strength) that a conference submission is mediocre (middling rating). Also, our task differs from ranking not only because one can be given a single item to classify (as opposed to a set of items to be ordered relative to one another), but because there are settings in which classification is harder than ranking, and vice versa. One can apply standard-ary classifiers or regression to this rating-inference problem; independent work by Koppel and Schler (2005) considers such 115 Proceedings of the 43rd Annual Meeting of the ACL, pages 115–124, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics methods. But an alternative approach that explicitly incorporates information about item similarities together with label similarity information (for instance, “one star” is closer to “two stars” than to “four stars”) is to think of the task as one of metric labeling (Kleinberg and Tardos, 2002), where label relations are encoded via a distance metric. This observation yields a meta-algorithm, applicable to both semi-supervised (via graph-</context>
<context position="13318" citStr="Koppel and Schler (2005)" startWordPosition="2087" endWordPosition="2090">onsider linear,-insensitive SVM regression (Vapnik, 1995; Smola and Sch¨olkopf, 1998); the idea is to find the hyperplane that best fits the training data, but where training points whose labels are within distanceof the hyperplane incur no loss. Then, for (test) instance, the label preference function is the negative of the distance betweenand the value predicted for by the fitted hyperplane function. Wilson, Wiebe, and Hwa (2004) used SVM regression to classify clause-level strength of opinion, reporting that it provided lower accuracy than other methods. However, independently of our work, Koppel and Schler (2005) found that applying linear regression to classify documents (in a different corpus than ours) with respect to a three-point rating scale provided greater accuracy than OVA SVMs and other algorithms. 3.3 Metric labeling Regression implicitly encodes the “similar items, similar labels” heuristic, in that one can restrict consideration to “gradual” functions. But we can also think of our task as a metric labeling problem (Kleinberg and Tardos, 2002), a special case of the maximum a posteriori estimation problem for Markov random fields, to explicitly encode our desideratum. Suppose we have an in</context>
<context position="30508" citStr="Koppel and Schler (2005)" startWordPosition="4857" endWordPosition="4860">t would be interesting to determine author-independent characteristics that can be used on (or suitably adapted to) data for specific authors. Q: How about trying — A: —Yes, there are many alternatives. A few that we tested are described in the Appendix, and we propose some others in the next section. We should mention that we have not yet experimented with all-vs.-all (AVA), another standard binary-tomulti-category classifier conversion method, because we wished to focus on the effect of omitting pairwise information. In independent work on 3-category rating inference for a different corpus, Koppel and Schler (2005) found that regression outperformed AVA, and Rifkin and Klautau (2004) argue that in principle OVA should do just as well as AVA. But we plan to try it out. 6 Related work and future directions In this paper, we addressed the rating-inference problem, showing the utility of employing label similarity and (appropriate choice of) item similarity — either implicitly, through regression, or explicitly and often more effectively, through metric labeling. In the future, we would like to apply our methods to other scale-based classification problems, and explore alternative methods. Clearly, varying </context>
<context position="32007" citStr="Koppel and Schler (2005)" startWordPosition="5088" endWordPosition="5091">l if a good metric on labels is lacking. Also, one could use mixture models (e.g., combine “positive” and “negative” language models) to capture class relationships (McCallum, 1999; Schapire and Singer, 2000; Takamura, Matsumoto, and Yamada, 2004). We are also interested in framing multi-class but non-scale-based categorization problems as metric 122 labeling tasks. For example, positive vs. negative vs. neutral sentiment distinctions are sometimes considered in which neutral means either objective (Engstr¨om, 2004) or a conflation of objective with a rating of mediocre (Das and Chen, 2001). (Koppel and Schler (2005) in independent work also discuss various types of neutrality.) In either case, we could apply a metric in which positive and negative are closer to objective (or objective+mediocre) than to each other. As another example, hierarchical label relationships can be easily encoded in a label metric. Finally, as mentioned in Section 3.3, we would like to address the transductive setting, in which one has a small amount of labeled data and uses relationships between unlabeled items, since it is particularly well-suited to the metric-labeling approach and may be quite important in practice. Acknowled</context>
</contexts>
<marker>Koppel, Schler, 2005</marker>
<rawString>Koppel, Moshe and Jonathan Schler. 2005. The importance of neutral examples for learning sentiment. In Workshop on the Analysis of Informal and Formal Information Exchange during Negotiations (FINEXIN).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Liu</author>
<author>Henry Lieberman</author>
<author>Ted Selker</author>
</authors>
<title>A model of textual affect sensing using real-world knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of Intelligent User Interfaces (IUI),</booktitle>
<pages>125--132</pages>
<contexts>
<context position="4823" citStr="Liu, Lieberman, and Selker, 2003" startWordPosition="742" endWordPosition="746">ed by how explicitly they attempt to leverage similarity between items and between labels. Next, we consider what item similarity measure to apply, proposing one based on the positive-sentence percentage. Incorporating this new measure within the metriclabeling framework is shown to often provide significant improvements over the other algorithms. We hope that some of the insights derived here might apply to other scales for text classifcation that have been considered, such as clause-level opinion strength (Wilson, Wiebe, and Hwa, 2004); affect types like disgust (Subasic and Huettner, 2001; Liu, Lieberman, and Selker, 2003); reading level (Collins-Thompson and Callan, 2004); and urgency or criticality (Horvitz, Jacobs, and Hovel, 1999). 2 Problem validation and formulation We first ran a small pilot study on human subjects in order to establish a rough idea of what a reasonable classification granularity is: if even people cannot accurately infer labels with respect to a five-star scheme with half stars, say, then we cannot expect a learning algorithm to do so. Indeed, some potential obstacles to accurate rating inference include lack of calibration (e.g., what an understated author intends as high praise may s</context>
</contexts>
<marker>Liu, Lieberman, Selker, 2003</marker>
<rawString>Liu, Hugo, Henry Lieberman, and Ted Selker. 2003. A model of textual affect sensing using real-world knowledge. In Proceedings of Intelligent User Interfaces (IUI), pages 125–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Multi-label text classification with a mixture model trained by EM.</title>
<date>1999</date>
<booktitle>In AAAI Workshop on Text Learning.</booktitle>
<contexts>
<context position="31563" citStr="McCallum, 1999" startWordPosition="5024" endWordPosition="5025">ic labeling. In the future, we would like to apply our methods to other scale-based classification problems, and explore alternative methods. Clearly, varying the kernel in SVM regression might yield better results. Another choice is ordinal regression (McCullagh, 1980; Herbrich, Graepel, and Obermayer, 2000), which only considers the ordering on labels, rather than any explicit distances between them; this approach could work well if a good metric on labels is lacking. Also, one could use mixture models (e.g., combine “positive” and “negative” language models) to capture class relationships (McCallum, 1999; Schapire and Singer, 2000; Takamura, Matsumoto, and Yamada, 2004). We are also interested in framing multi-class but non-scale-based categorization problems as metric 122 labeling tasks. For example, positive vs. negative vs. neutral sentiment distinctions are sometimes considered in which neutral means either objective (Engstr¨om, 2004) or a conflation of objective with a rating of mediocre (Das and Chen, 2001). (Koppel and Schler (2005) in independent work also discuss various types of neutrality.) In either case, we could apply a metric in which positive and negative are closer to objecti</context>
</contexts>
<marker>McCallum, 1999</marker>
<rawString>McCallum, Andrew. 1999. Multi-label text classification with a mixture model trained by EM. In AAAI Workshop on Text Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter McCullagh</author>
</authors>
<title>Regression models for ordinal data.</title>
<date>1980</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>42</volume>
<issue>2</issue>
<contexts>
<context position="31218" citStr="McCullagh, 1980" startWordPosition="4971" endWordPosition="4972">OVA should do just as well as AVA. But we plan to try it out. 6 Related work and future directions In this paper, we addressed the rating-inference problem, showing the utility of employing label similarity and (appropriate choice of) item similarity — either implicitly, through regression, or explicitly and often more effectively, through metric labeling. In the future, we would like to apply our methods to other scale-based classification problems, and explore alternative methods. Clearly, varying the kernel in SVM regression might yield better results. Another choice is ordinal regression (McCullagh, 1980; Herbrich, Graepel, and Obermayer, 2000), which only considers the ordering on labels, rather than any explicit distances between them; this approach could work well if a good metric on labels is lacking. Also, one could use mixture models (e.g., combine “positive” and “negative” language models) to capture class relationships (McCallum, 1999; Schapire and Singer, 2000; Takamura, Matsumoto, and Yamada, 2004). We are also interested in framing multi-class but non-scale-based categorization problems as metric 122 labeling tasks. For example, positive vs. negative vs. neutral sentiment distincti</context>
</contexts>
<marker>McCullagh, 1980</marker>
<rawString>McCullagh, Peter. 1980. Regression models for ordinal data. Journal of the Royal Statistical Society, 42(2):109–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="9543" citStr="Pang and Lee, 2004" startWordPosition="1501" endWordPosition="1504">n than the three-class setting, since it involves more class pairs. Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classification techniques (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002). We applied the above two labeling schemes to a scale dataset3 containing four corpora of movie reviews. All reviews were automatically preprocessed to remove both explicit rating indicators and objective sentences; the motivation for the latter step is that it has previously aided positive vs. negative classification (Pang and Lee, 2004). All of the 1770, 902, 1307, or 1027 documents in a given corpus were written by the same author. This decision facilitates interpretation of the results, since it factors out the effects of different choices of methods for calibrating authors’ scales.4 We point out that but since our goal is to recover a reviewer’s “true” recommendation, reader-author agreement is more relevant. While another factor might be degree of English fluency, in an informal experiment (six subjects viewing the same three pairs), native English speakers made the only two errors. 3Available at http://www.cs.cornell.ed</context>
<context position="15798" citStr="Pang and Lee, 2004" startWordPosition="2475" endWordPosition="2478"> unlabeled reviews, in which case considering similarities between unlabeled texts could prove quite helpful. In full generality, the corresponding multi-label optimization problem is intractable, but for many families of functions (e.g., convex) there exist practical exact or approximation algorithms based on techniques for finding minimum s-t cuts in graphs (Ishikawa and Geiger, 1998; Boykov, Veksler, and Zabih, 1999; Ishikawa, 2003). Interestingly, previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee, 2004). Of course, there are many other related semi-supervised learning algorithms that we would like to try as well; see Zhu (2005) for a survey. 4 Class struggle: finding a label-correlated item-similarity function We need to specify an item similarity function to use the metric-labeling formulation described in Section 3.3. We could, as is commonly done, employ a term-overlap-based measure such as the cosine between term-frequency-based document vectors (henceforth “TO(cos)”). However, Table 2 6If we ignore the term, different choices ofcorrespond to different versions of nearest-neighbor learni</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Pang, Bo and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the ACL, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1841" citStr="Pang, Lee, and Vaithyanathan, 2002" startWordPosition="280" endWordPosition="284">ns of SVMs when we employ a novel similarity measure appropriate to the problem. 1 Introduction There has recently been a dramatic surge of interest in sentiment analysis, as more and more people become aware of the scientific challenges posed and the scope of new applications enabled by the processing of subjective language. (The papers collected by Qu, Shanahan, and Wiebe (2004) form a representative sample of research in the area.) Most prior work on the specific problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative (Turney, 2002; Pang, Lee, and Vaithyanathan, 2002; Dave, Lawrence, and Pennock, 2003; Yu and Hatzivassiloglou, 2003). But it is often helpful to have more information than this binary distinction provides, especially if one is ranking items by recommendation or comparing several reviewers’ opinions: example applications include collaborative filtering and deciding which conference submissions to accept. Therefore, in this paper we consider generalizing to finer-grained scales: rather than just determine whether a review is “thumbs up” or not, we attempt to infer the author’s implied numerical rating, such as “three stars” or “four stars”. No</context>
<context position="9187" citStr="Pang, Lee, and Vaithyanathan, 2002" startWordPosition="1445" endWordPosition="1449"> or the most positive class (but not both) contained only about 5% of the documents. To make the classes more balanced, we folded these minority classes into the adjacent class, thus arriving at a four-class problem (categories 0-3, increasing in positivity). Note that the four-class problem seems to offer more possibilities for leveraging class relationship information than the three-class setting, since it involves more class pairs. Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classification techniques (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002). We applied the above two labeling schemes to a scale dataset3 containing four corpora of movie reviews. All reviews were automatically preprocessed to remove both explicit rating indicators and objective sentences; the motivation for the latter step is that it has previously aided positive vs. negative classification (Pang and Lee, 2004). All of the 1770, 902, 1307, or 1027 documents in a given corpus were written by the same author. This decision facilitates interpretation of the results, since it factors out the effects of different choices of methods for calibrating authors</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Qu</author>
<author>James Shanahan</author>
<author>Janyce Wiebe</author>
<author>editors</author>
</authors>
<date>2004</date>
<booktitle>Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications. AAAI Press. AAAI technical report</booktitle>
<pages>04--07</pages>
<marker>Qu, Shanahan, Wiebe, editors, 2004</marker>
<rawString>Qu, Yan, James Shanahan, and Janyce Wiebe, editors. 2004. Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications. AAAI Press. AAAI technical report SS-04-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan M Rifkin</author>
<author>Aldebaro Klautau</author>
</authors>
<title>In defense of one-vs-all classification.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--101</pages>
<contexts>
<context position="11570" citStr="Rifkin and Klautau, 2004" startWordPosition="1812" endWordPosition="1815">ry classification in which the labels can be naturally mapped to a metric space (e.g., points on a line); for simplicity, we assume the distance metric throughout. In this section, we present three approaches to this problem in order of increasingly explicit use of pairwise similarity information between items and between labels. In order to make comparisons between these methods meaningful, we base all three of them on Support Vector Machines (SVMs) as implemented in Joachims’ (1999) package. 3.1 One-vs-all The standard SVM formulation applies only to binary classification. One-vs-all (OVA) (Rifkin and Klautau, 2004) is a common extension to the-ary case. Training consists of building, for each label, an SVM binary classifier distinguishing labelfrom “not-”. We consider the final output to be a label preference function , defined as the signed distance of (test) item to the side of the vs. not-decision plane. Clearly, OVA makes no explicit use of pairwise label or item relationships. However, it can perform well if each class exhibits sufficiently distinct language; see Section 4 for more discussion. 3.2 Regression Alternatively, we can take a regression perspective by assuming that the labels come from a</context>
<context position="30578" citStr="Rifkin and Klautau (2004)" startWordPosition="4868" endWordPosition="4871">s that can be used on (or suitably adapted to) data for specific authors. Q: How about trying — A: —Yes, there are many alternatives. A few that we tested are described in the Appendix, and we propose some others in the next section. We should mention that we have not yet experimented with all-vs.-all (AVA), another standard binary-tomulti-category classifier conversion method, because we wished to focus on the effect of omitting pairwise information. In independent work on 3-category rating inference for a different corpus, Koppel and Schler (2005) found that regression outperformed AVA, and Rifkin and Klautau (2004) argue that in principle OVA should do just as well as AVA. But we plan to try it out. 6 Related work and future directions In this paper, we addressed the rating-inference problem, showing the utility of employing label similarity and (appropriate choice of) item similarity — either implicitly, through regression, or explicitly and often more effectively, through metric labeling. In the future, we would like to apply our methods to other scale-based classification problems, and explore alternative methods. Clearly, varying the kernel in SVM regression might yield better results. Another choic</context>
</contexts>
<marker>Rifkin, Klautau, 2004</marker>
<rawString>Rifkin, Ryan M. and Aldebaro Klautau. 2004. In defense of one-vs-all classification. Journal of Machine Learning Research, 5:101–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>BoosTexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="31590" citStr="Schapire and Singer, 2000" startWordPosition="5026" endWordPosition="5029">the future, we would like to apply our methods to other scale-based classification problems, and explore alternative methods. Clearly, varying the kernel in SVM regression might yield better results. Another choice is ordinal regression (McCullagh, 1980; Herbrich, Graepel, and Obermayer, 2000), which only considers the ordering on labels, rather than any explicit distances between them; this approach could work well if a good metric on labels is lacking. Also, one could use mixture models (e.g., combine “positive” and “negative” language models) to capture class relationships (McCallum, 1999; Schapire and Singer, 2000; Takamura, Matsumoto, and Yamada, 2004). We are also interested in framing multi-class but non-scale-based categorization problems as metric 122 labeling tasks. For example, positive vs. negative vs. neutral sentiment distinctions are sometimes considered in which neutral means either objective (Engstr¨om, 2004) or a conflation of objective with a rating of mediocre (Das and Chen, 2001). (Koppel and Schler (2005) in independent work also discuss various types of neutrality.) In either case, we could apply a metric in which positive and negative are closer to objective (or objective+mediocre) </context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Schapire, Robert E. and Yoram Singer. 2000. BoosTexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex J Smola</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>1998</date>
<tech>Technical Report NeuroCOLT NC-TR-98-030,</tech>
<institution>Royal Holloway College, University of London.</institution>
<marker>Smola, Sch¨olkopf, 1998</marker>
<rawString>Smola, Alex J. and Bernhard Sch¨olkopf. 1998. A tutorial on support vector regression. Technical Report NeuroCOLT NC-TR-98-030, Royal Holloway College, University of London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pero Subasic</author>
<author>Alison Huettner</author>
</authors>
<title>Affect analysis of text using fuzzy semantic typing.</title>
<date>2001</date>
<journal>IEEE Transactions on Fuzzy Systems,</journal>
<volume>9</volume>
<issue>4</issue>
<contexts>
<context position="4789" citStr="Subasic and Huettner, 2001" startWordPosition="738" endWordPosition="741">ng — that can be distinguished by how explicitly they attempt to leverage similarity between items and between labels. Next, we consider what item similarity measure to apply, proposing one based on the positive-sentence percentage. Incorporating this new measure within the metriclabeling framework is shown to often provide significant improvements over the other algorithms. We hope that some of the insights derived here might apply to other scales for text classifcation that have been considered, such as clause-level opinion strength (Wilson, Wiebe, and Hwa, 2004); affect types like disgust (Subasic and Huettner, 2001; Liu, Lieberman, and Selker, 2003); reading level (Collins-Thompson and Callan, 2004); and urgency or criticality (Horvitz, Jacobs, and Hovel, 1999). 2 Problem validation and formulation We first ran a small pilot study on human subjects in order to establish a rough idea of what a reasonable classification granularity is: if even people cannot accurately infer labels with respect to a five-star scheme with half stars, say, then we cannot expect a learning algorithm to do so. Indeed, some potential obstacles to accurate rating inference include lack of calibration (e.g., what an understated a</context>
</contexts>
<marker>Subasic, Huettner, 2001</marker>
<rawString>Subasic, Pero and Alison Huettner. 2001. Affect analysis of text using fuzzy semantic typing. IEEE Transactions on Fuzzy Systems, 9(4):483–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Yuji Matsumoto</author>
<author>Hiroyasu Yamada</author>
</authors>
<title>Modeling category structures with a kernel function.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="31629" citStr="Takamura, Matsumoto, and Yamada, 2004" startWordPosition="5030" endWordPosition="5034">o apply our methods to other scale-based classification problems, and explore alternative methods. Clearly, varying the kernel in SVM regression might yield better results. Another choice is ordinal regression (McCullagh, 1980; Herbrich, Graepel, and Obermayer, 2000), which only considers the ordering on labels, rather than any explicit distances between them; this approach could work well if a good metric on labels is lacking. Also, one could use mixture models (e.g., combine “positive” and “negative” language models) to capture class relationships (McCallum, 1999; Schapire and Singer, 2000; Takamura, Matsumoto, and Yamada, 2004). We are also interested in framing multi-class but non-scale-based categorization problems as metric 122 labeling tasks. For example, positive vs. negative vs. neutral sentiment distinctions are sometimes considered in which neutral means either objective (Engstr¨om, 2004) or a conflation of objective with a rating of mediocre (Das and Chen, 2001). (Koppel and Schler (2005) in independent work also discuss various types of neutrality.) In either case, we could apply a metric in which positive and negative are closer to objective (or objective+mediocre) than to each other. As another example,</context>
</contexts>
<marker>Takamura, Matsumoto, Yamada, 2004</marker>
<rawString>Takamura, Hiroya, Yuji Matsumoto, and Hiroyasu Yamada. 2004. Modeling category structures with a kernel function. In Proceedings of CoNLL, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard M Tong</author>
</authors>
<title>An operational system for detecting and tracking opinions in on-line discussion.</title>
<date>2001</date>
<booktitle>SIGIR Workshop on Operational Text Classification.</booktitle>
<contexts>
<context position="17682" citStr="Tong, 2001" startWordPosition="2774" endWordPosition="2775"> nearby classes. Thus, item similarity as measured by TO(cos) may not correlate well with similarity of the item’s true labels. We can potentially develop a more useful similarity metric by asking ourselves what, intuitively, accounts for the label relationships that we seek to exploit. A simple hypothesis is that ratings can be determined by the positive-sentence percentage (PSP) of a text, i.e., the number of positive sentences divided by the number of subjective sentences. (Termbased versions of this premise have motivated much sentiment-analysis work for over a decade (Das and Chen, 2001; Tong, 2001; Turney, 2002).) But counterexamples are easy to construct: reviews can contain off-topic opinions, or recount many positive aspects before describing a fatal flaw. We therefore tested the hypothesis as follows. To avoid the need to hand-label sentences as positive or negative, we first created a sentence polarity dataset7 consisting of 10,662 movie-review “snippets” (a striking extract usually one sentence long) downloaded from www.rottentomatoes.com; each snippet was labeled with its source review’s label (positive or negative) as provided by Rotten Tomatoes. Then, we trained a Naive Bayes </context>
</contexts>
<marker>Tong, 2001</marker>
<rawString>Tong, Richard M. 2001. An operational system for detecting and tracking opinions in on-line discussion. SIGIR Workshop on Operational Text Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="1805" citStr="Turney, 2002" startWordPosition="278" endWordPosition="279">ression versions of SVMs when we employ a novel similarity measure appropriate to the problem. 1 Introduction There has recently been a dramatic surge of interest in sentiment analysis, as more and more people become aware of the scientific challenges posed and the scope of new applications enabled by the processing of subjective language. (The papers collected by Qu, Shanahan, and Wiebe (2004) form a representative sample of research in the area.) Most prior work on the specific problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative (Turney, 2002; Pang, Lee, and Vaithyanathan, 2002; Dave, Lawrence, and Pennock, 2003; Yu and Hatzivassiloglou, 2003). But it is often helpful to have more information than this binary distinction provides, especially if one is ranking items by recommendation or comparing several reviewers’ opinions: example applications include collaborative filtering and deciding which conference submissions to accept. Therefore, in this paper we consider generalizing to finer-grained scales: rather than just determine whether a review is “thumbs up” or not, we attempt to infer the author’s implied numerical rating, such </context>
<context position="9202" citStr="Turney, 2002" startWordPosition="1450" endWordPosition="1451"> both) contained only about 5% of the documents. To make the classes more balanced, we folded these minority classes into the adjacent class, thus arriving at a four-class problem (categories 0-3, increasing in positivity). Note that the four-class problem seems to offer more possibilities for leveraging class relationship information than the three-class setting, since it involves more class pairs. Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classification techniques (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002). We applied the above two labeling schemes to a scale dataset3 containing four corpora of movie reviews. All reviews were automatically preprocessed to remove both explicit rating indicators and objective sentences; the motivation for the latter step is that it has previously aided positive vs. negative classification (Pang and Lee, 2004). All of the 1770, 902, 1307, or 1027 documents in a given corpus were written by the same author. This decision facilitates interpretation of the results, since it factors out the effects of different choices of methods for calibrating authors’ scales.4 We p</context>
<context position="17697" citStr="Turney, 2002" startWordPosition="2776" endWordPosition="2777">ses. Thus, item similarity as measured by TO(cos) may not correlate well with similarity of the item’s true labels. We can potentially develop a more useful similarity metric by asking ourselves what, intuitively, accounts for the label relationships that we seek to exploit. A simple hypothesis is that ratings can be determined by the positive-sentence percentage (PSP) of a text, i.e., the number of positive sentences divided by the number of subjective sentences. (Termbased versions of this premise have motivated much sentiment-analysis work for over a decade (Das and Chen, 2001; Tong, 2001; Turney, 2002).) But counterexamples are easy to construct: reviews can contain off-topic opinions, or recount many positive aspects before describing a fatal flaw. We therefore tested the hypothesis as follows. To avoid the need to hand-label sentences as positive or negative, we first created a sentence polarity dataset7 consisting of 10,662 movie-review “snippets” (a striking extract usually one sentence long) downloaded from www.rottentomatoes.com; each snippet was labeled with its source review’s label (positive or negative) as provided by Rotten Tomatoes. Then, we trained a Naive Bayes classifier on t</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, Peter. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the ACL, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="12750" citStr="Vapnik, 1995" startWordPosition="1999" endWordPosition="2000">ming that the labels come from a discretization of a continuous function mapping from the or negative. Even though Eric Lurio uses a 5 star system, his grading is very relaxed. So, 2 stars can be positive.” Thus, calibration may sometimes require strong familiarity with the authors involved, as anyone who has ever needed to reconcile conflicting referee reports probably knows. 117 5 feature space to a metric space.If we choose from a family of sufficiently “gradual” functions, then similar items necessarily receive similar labels. In particular, we consider linear,-insensitive SVM regression (Vapnik, 1995; Smola and Sch¨olkopf, 1998); the idea is to find the hyperplane that best fits the training data, but where training points whose labels are within distanceof the hyperplane incur no loss. Then, for (test) instance, the label preference function is the negative of the distance betweenand the value predicted for by the fitted hyperplane function. Wilson, Wiebe, and Hwa (2004) used SVM regression to classify clause-level strength of opinion, reporting that it provided lower accuracy than other methods. However, independently of our work, Koppel and Schler (2005) found that applying linear regr</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vapnik, Vladimir. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Rebecca Hwa</author>
</authors>
<title>Just how mad are you? Finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>761--769</pages>
<contexts>
<context position="2525" citStr="Wilson, Wiebe, and Hwa, 2004" startWordPosition="382" endWordPosition="386">glou, 2003). But it is often helpful to have more information than this binary distinction provides, especially if one is ranking items by recommendation or comparing several reviewers’ opinions: example applications include collaborative filtering and deciding which conference submissions to accept. Therefore, in this paper we consider generalizing to finer-grained scales: rather than just determine whether a review is “thumbs up” or not, we attempt to infer the author’s implied numerical rating, such as “three stars” or “four stars”. Note that this differs from identifying opinion strength (Wilson, Wiebe, and Hwa, 2004): rants and raves have the same strength but represent opposite evaluations, and referee forms often allow one to indicate that one is very confident (high strength) that a conference submission is mediocre (middling rating). Also, our task differs from ranking not only because one can be given a single item to classify (as opposed to a set of items to be ordered relative to one another), but because there are settings in which classification is harder than ranking, and vice versa. One can apply standard-ary classifiers or regression to this rating-inference problem; independent work by Koppe</context>
<context position="4733" citStr="Wilson, Wiebe, and Hwa, 2004" startWordPosition="728" endWordPosition="732"> of algorithms — one-vs-all, regression, and metric labeling — that can be distinguished by how explicitly they attempt to leverage similarity between items and between labels. Next, we consider what item similarity measure to apply, proposing one based on the positive-sentence percentage. Incorporating this new measure within the metriclabeling framework is shown to often provide significant improvements over the other algorithms. We hope that some of the insights derived here might apply to other scales for text classifcation that have been considered, such as clause-level opinion strength (Wilson, Wiebe, and Hwa, 2004); affect types like disgust (Subasic and Huettner, 2001; Liu, Lieberman, and Selker, 2003); reading level (Collins-Thompson and Callan, 2004); and urgency or criticality (Horvitz, Jacobs, and Hovel, 1999). 2 Problem validation and formulation We first ran a small pilot study on human subjects in order to establish a rough idea of what a reasonable classification granularity is: if even people cannot accurately infer labels with respect to a five-star scheme with half stars, say, then we cannot expect a learning algorithm to do so. Indeed, some potential obstacles to accurate rating inference </context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>Wilson, Theresa, Janyce Wiebe, and Rebecca Hwa. 2004. Just how mad are you? Finding strong and weak opinion clauses. In Proceedings of AAAI, pages 761–769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1908" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="290" endWordPosition="294"> problem. 1 Introduction There has recently been a dramatic surge of interest in sentiment analysis, as more and more people become aware of the scientific challenges posed and the scope of new applications enabled by the processing of subjective language. (The papers collected by Qu, Shanahan, and Wiebe (2004) form a representative sample of research in the area.) Most prior work on the specific problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative (Turney, 2002; Pang, Lee, and Vaithyanathan, 2002; Dave, Lawrence, and Pennock, 2003; Yu and Hatzivassiloglou, 2003). But it is often helpful to have more information than this binary distinction provides, especially if one is ranking items by recommendation or comparing several reviewers’ opinions: example applications include collaborative filtering and deciding which conference submissions to accept. Therefore, in this paper we consider generalizing to finer-grained scales: rather than just determine whether a review is “thumbs up” or not, we attempt to infer the author’s implied numerical rating, such as “three stars” or “four stars”. Note that this differs from identifying opinion strength (Wilson, Wie</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Yu, Hong and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
</authors>
<title>Semi-Supervised Learning with Graphs.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="15925" citStr="Zhu (2005)" startWordPosition="2498" endWordPosition="2499">rresponding multi-label optimization problem is intractable, but for many families of functions (e.g., convex) there exist practical exact or approximation algorithms based on techniques for finding minimum s-t cuts in graphs (Ishikawa and Geiger, 1998; Boykov, Veksler, and Zabih, 1999; Ishikawa, 2003). Interestingly, previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee, 2004). Of course, there are many other related semi-supervised learning algorithms that we would like to try as well; see Zhu (2005) for a survey. 4 Class struggle: finding a label-correlated item-similarity function We need to specify an item similarity function to use the metric-labeling formulation described in Section 3.3. We could, as is commonly done, employ a term-overlap-based measure such as the cosine between term-frequency-based document vectors (henceforth “TO(cos)”). However, Table 2 6If we ignore the term, different choices ofcorrespond to different versions of nearest-neighbor learning, e.g., majority-vote, weighted average of labels, or weighted median of labels. test where is monotonically increasing (we c</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>Zhu, Xiaojin (Jerry). 2005. Semi-Supervised Learning with Graphs. Ph.D. thesis, Carnegie Mellon University. A Appendix: other variations attempted A.1 Discretizing binary classification</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>