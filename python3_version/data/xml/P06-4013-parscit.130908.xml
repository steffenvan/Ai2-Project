<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.081658">
<title confidence="0.99257">
Archivus: A multimodal system for multimedia meeting browsing and
retrieval
</title>
<author confidence="0.9529675">
Marita Ailomaa, Miroslav Melichar, Agnes Lisowska,
Martin Rajman Susan Armstrong
</author>
<affiliation confidence="0.892078">
Artificial Intelligence Laboratory ISSCO/TIM/ETI
´Ecole Polytechnique F´ed´erale de Lausanne University of Geneva
</affiliation>
<address confidence="0.859367">
CH-1015 Lausanne, Switzerland CH-1211 Geneva, Switzerland
</address>
<email confidence="0.997485">
marita.ailomaa@epfl.ch agnes.lisowska@issco.unige.ch
</email>
<sectionHeader confidence="0.99383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999065">
This paper presents Archivus, a multi-
modal language-enabled meeting brows-
ing and retrieval system. The prototype
is in an early stage of development, and
we are currently exploring the role of nat-
ural language for interacting in this rela-
tively unfamiliar and complex domain. We
briefly describe the design and implemen-
tation status of the system, and then focus
on how this system is used to elicit useful
data for supporting hypotheses about mul-
timodal interaction in the domain of meet-
ing retrieval and for developing NLP mod-
ules for this specific domain.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99947875">
In the past few years, there has been an increasing
interest in research on developing systems for effi-
cient recording of and access to multimedia meet-
ing data1. This work often results in videos of
meetings, transcripts, electronic copies of docu-
ments referenced, as well as annotations of various
kinds on this data. In order to exploit this work, a
user needs to have an interface that allows them to
retrieve and browse the multimedia meeting data
easily and efficiently.
In our work we have developed a multimodal
(voice, keyboard, mouse/pen) meeting browser,
Archivus, whose purpose is to allow users to ac-
cess multimedia meeting data in a way that is most
natural to them. We believe that since this is a new
domain of interaction, users can be encouraged to
</bodyText>
<footnote confidence="0.99724">
1The IM2 project http://www.im2.ch, the AMI project
www.amiproject.org, The Meeting Room Project at Carnegie
Mellon University, http://www.is.cs.cmu.edu/mie, and rich
transcription of natural and impromptu meetings at ICSI,
Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html
</footnote>
<bodyText confidence="0.9999045">
try out and consistently use novel input modalities
such as voice, including more complex natural lan-
guage, and that in particular in this domain, such
multimodal interaction can help the user find in-
formation more efficiently.
When developing a language interface for an in-
teractive system in a new domain, the Wizard of
Oz (WOz) methodology (Dahlb¨ack et al., 1993;
Salber and Coutaz, 1993) is a very useful tool.
The user interacts with what they believe to be a
fully automated system, when in fact another per-
son, a ‘wizard’ is simulating the missing or incom-
plete NLP modules, typically the speech recogni-
tion, natural language understanding and dialogue
management modules. The recorded experiments
provide valuable information for implementing or
fine-tuning these parts of the system.
However, the methodology is usually applied
to unimodal (voice-only or keyboard-only) sys-
tems, where the elicitation of language data is not
a problem since this is effectively the only type of
data resulting from the experiment. In our case, we
are developing a complex multimodal system. We
found that when the Wizard of Oz methodology
is extended to multimodal systems, the number of
variables that have to be considered and controlled
for in the experiment increases substantially. For
instance, if it is the case that within a single inter-
face any task that can be performed using natural
language can also be performed with other modal-
ities, for example a mouse, the user may prefer
to use the other – more familiar – modality for
a sizeable portion of the experiment. In order to
gather a useful amount of natural language data,
greater care has to be taken to design the system
in a way that encourages language use. But, if
the goal of the experiment is also to study what
modalities users find more useful in some situa-
</bodyText>
<page confidence="0.993681">
49
</page>
<note confidence="0.459439">
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 49–52,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9999">
Figure 1: The Archivus Interface
</figureCaption>
<bodyText confidence="0.999224">
tions compared to others, language use must be
encouraged without being forced, and finding this
balance can be very hard to achieve in practice.
</bodyText>
<sectionHeader confidence="0.799386" genericHeader="introduction">
2 Design and implementation
</sectionHeader>
<bodyText confidence="0.999963352941177">
The Archivus system has been designed to sat-
isfy realistic user needs based on a user require-
ment analysis (Lisowska, 2003), where subjects
were asked to formulate queries that would enable
them to find out “what happened at a meeting”.
The design of the user interface is based on the
metaphor of a person interacting in an archive or
library (Lisowska et al., 2004).
Furthermore, Archivus is flexibly multimodal,
meaning that users can interact unimodally choos-
ing one of the available modalities exclusively,
or multimodally, using any combination of the
modalities. In order to encourage natural lan-
guage interaction, the system gives textual and vo-
cal feedback to the user. The Archivus Interface
is shown in Figure 1. A detailed description of all
of the components can be found in Lisowska et al.
(2004).
Archivus was implemented within a software
framework for designing multimodal applications
with mixed-initiative dialogue models (Cenek et
al., 2005). Systems designed within this frame-
work handle interaction with the user through
a multimodal dialogue manager. The dialogue
manager receives user input from all modalities
(speech, typing and pointing) and provides mul-
timodal responses in the form of graphical, textual
and vocal feedback.
The dialogue manager contains only linguistic
knowledge and interaction algorithms. Domain
knowledge is stored in an SQL database and is ac-
cessed by the dialogue manager based on the con-
straints expressed by the user during interaction.
The above software framework provides sup-
port for remote simulation or supervision of
some of the application functionalities. This fea-
ture makes any application developed under this
methodology well suited for WOz experiments. In
the case of Archivus, pilot experiments strongly
suggested the use of two wizards – one supervising
the user’s input (Input Wizard) and the other con-
trolling the natural language output of the system
(Output Wizard). Both wizards see the user’s in-
put, but their actions are sequential, with the Out-
put Wizard being constrained by the actions of the
Input Wizard.
The role of the Input Wizard is to assure that
the user’s input (in any modality combination)
is correctly conveyed to the system in the form
of sets of semantic pairs. A semantic pair (SP)
is a qualified piece of information that the dia-
</bodyText>
<page confidence="0.98006">
50
</page>
<bodyText confidence="0.999950366666667">
logue system is able to understand. For exam-
ple, a system could understand semantic pairs such
as date:Monday or list:next. A user’s
utterance “What questions did this guy ask in
the meeting yesterday?” combined with point-
ing on the screen at a person called “Raymond”
could translate to dialogact:Question,
speaker:Raymond,day:Monday.
In the current version of Archivus, user clicks
are translated into semantic pairs automatically by
the system. Where written queries are concerned,
the wizard sometimes needs to correct automat-
ically generated pairs due to the currently low
performance of our natural language understand-
ing module. Finally since the speech recognition
engine has not been implemented yet, the user’s
speech is fully processed by a wizard. The Input
Wizard also assures that the fusion of pairs coming
from different modalities is done correctly.
The role of the Output Wizard is to monitor, and
if necessary change the default prompts that are
generated by the system. Changes are made for
example to smooth the dialogue flow, i.e. to bet-
ter explain the dialogue situation to the user or to
make the response more conversational. The wiz-
ard can select a prompt from a predefined list, or
type a new one during interaction.
All wizards’ actions are logged and afterwards
used to help automate the correct behavior of the
system and to increase the overall performance.
</bodyText>
<sectionHeader confidence="0.893904" genericHeader="method">
3 Collecting natural language data
</sectionHeader>
<bodyText confidence="0.999982571428571">
In order to obtain a sufficient amount of language
data from the WOz experiments, several means
have been used to determine what encourages
users to speak to the system. These include giving
users different types of documentation before the
experiment – lists of possible voice commands, a
user manual, and step-by-step tutorials. We found
that the best solution was to give users a tutorial
in which they worked through an example using
voice alone or in combination with other modali-
ties, explaining in each step the consequences of
the user’s actions on the system. The drawback of
this approach is that the user may be biased by the
examples and continue to interact according to the
interaction patterns that are provided, rather than
developing their own patterns. These influences
need to be considered both in the data analysis,
and in how the tutorials are written and structured.
The actual experiment consists of two parts in
which the user gets a mixed set of short-answer
and true-false questions to solve using the system.
First they are only allowed to use a subset of the
available modalities, e.g. voice and pen, and then
the full set of modalities. By giving the users dif-
ferent subsets in the first part, we can compare if
the enforcement of certain modalities has an im-
pact on how they choose to use language when all
modalities are available.
On the backend, the wizards can also to some
extent have an active role in encouraging language
use. The Input Wizard is rather constrained in
terms of what semantic pairs he can produce, be-
cause he is committed to selecting from a set of
pairs that are extracted from the meeting data.
For example if “Monday” is not a meeting date
in the database, the input is interpreted as having
“no match”, which generates the system prompt
“I don’t understand”. Here, the Output Wizard
can intervene by replacing that prompt by one that
more precisely specifies the nature of the problem.
The Output Wizard can also decide to replace
default prompts in situations when they are too
general in a given context. For instance, when
the user is browsing different sections of a meeting
book (cover page, table of contents, transcript and
referenced documents) the default prompt gives
general advice on how to access the different parts
of the book, but can be changed to suggest a spe-
cific section instead.
</bodyText>
<sectionHeader confidence="0.41971" genericHeader="method">
4 Analysis of elicited language data
</sectionHeader>
<bodyText confidence="0.999979210526316">
The data collected with Archivus through WOz
experiments provide useful information in several
ways. One aspect is to see the complexity of the
language used by users – for instance whether they
use more keywords, multi-word expressions or
full-sentence queries. This is important for choos-
ing the appropriate level of language processing,
for instance for the syntactic analysis. Another as-
pect is to see the types of actions performed us-
ing language. On one hand, users can manipulate
elements in the graphical interface by expressing
commands that are semantically equivalent with
pointing, e.g. “next page”. On the other hand,
they can freely formulate queries relating to the
information they are looking for, e.g. “Did they
decide to put a sofa in the lounge?”. Commands
are interface specific rather than domain specific.
From the graphical interface the user can easily
predict what they can say and how the system will
</bodyText>
<page confidence="0.997651">
51
</page>
<table confidence="0.999886272727273">
Part 1 condition Pointing Language
Experiment set 1
voice only 91% 9%
voice+keyboard 88% 12%
keyboard+pointing 66% 34%
voice+keyb.+pointing 79% 21%
Experiment set 2
voice only 68% 32%
voice+pointing 62% 38%
keyboard+pointing 39% 61%
pointing 76% 24%
</table>
<tableCaption confidence="0.999925">
Table 1: Use of each modality in part 2.
</tableCaption>
<bodyText confidence="0.99996685">
respond. Queries depend on the domain and the
data, and are more problematic for the user be-
cause they cannot immediately see what types of
queries they can ask and what the coverage of
the data is. But, using queries can be very use-
ful, because it allows the user to express them-
selves in their own terms. An important goal of the
data analysis is to determine if the language inter-
face enables the user to interact more successfully
than if they are limited to pointing only. In addi-
tion, the way in which the users use language in
these two dimensions has important implications
for the dialogue strategy and for the implementa-
tion of the language processing modules, for in-
stance the speech recognition engine. A speech
recognizer can be very accurate when trained on a
small, fixed set of commands whereas it may per-
form poorly when faced with a wide variety of lan-
guage queries.
Thus far, we have performed 3 sets of pilot
WOz experiments with 40 participants. The pri-
mary aim was to improve and finetune the system
and the WOz environment as a preparation for the
data-collection experiments that we plan to do in
the future. In these experiments we compared how
frequently users used voice and keyboard in rela-
tion to pointing as we progressively changed fea-
tures in the system and the experimental setup to
encourage language use. The results between the
first and the third set of experiments can be seen
in table 1, grouped by the subset of modalities that
the users had in the first part of the experiment.
From the table we can see that changes made
between the different iterations of the system
achieved their goal – by the third experiment set
we were managing to elicit larger amounts of nat-
ural language data. Moreover, we noticed that the
modality conditions that are available to the user
in the first part play a role in the amount of use of
language modalities in the second part.
</bodyText>
<sectionHeader confidence="0.986601" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.99999325">
We believe that the work presented here (both the
system and the WOz environment and experimen-
tal protocol) has now reached a stable stage that
allows for the elicitation of sufficient amounts of
natural language and interaction data. The next
step will be to run a large-scale data collection.
The results from this collection should provide
enough information to allow us to develop and in-
tegrate fairly robust natural language processing
into the system. Ideally, some of the components
used in the software framework will be made pub-
licly available at the end of the project.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99948821875">
Pavel Cenek, Miroslav Melichar, and Martin Rajman.
2005. A Framework for Rapid Multimodal Appli-
cation Design. In V´aclav Matouˇsek, Pavel Mautner,
and Tom´aˇs Pavelka, editors, Proceedings of the 8th
International Conference on Text, Speech and Dia-
logue (TSD 2005), volume 3658 of Lecture Notes
in Computer Science, pages 393–403, Karlovy Vary,
Czech Republic, September 12-15. Springer.
Nils Dahlb¨ack, Arne J¨onsson, and Lars Ahrenberg.
1993. Wizard of Oz Studies – Why and How. In
Dianne Murray Wayne D. Gray, William Hefley, ed-
itor, International Workshop on Intelligent User In-
terfaces 1993, pages 193–200. ACM Press.
Agnes Lisowska, Martin Rajman, and Trung H. Bui.
2004. ARCHIVUS: A System for Accessing the
Content of Recorded Multimodal Meetings. In
In Procedings of the JOINT AMI/PASCAL/IM2/M4
Workshop on Multimodal Interaction and Related
Machine Learning Algorithms, Bourlard H. &amp; Ben-
gio S., eds. (2004), LNCS, Springer-Verlag, Berlin.,
Martigny, Switzerland, June.
Agnes Lisowska. 2003. Multimodal interface design
for the multimodal meeting domain: Preliminary in-
dications from a query analysis study. Project re-
port IM2.MDM-11, University of Geneva, Geneva,
Switzerland, November.
Daniel Salber and Jo¨elle Coutaz. 1993. Applying
the wizard of oz technique to the study of multi-
modal systems. In EWHCI ’93: Selected papers
from the Third International Conference on Human-
Computer Interaction, pages 219–230, London, UK.
Springer-Verlag.
</reference>
<page confidence="0.998848">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.489995">
<title confidence="0.9151655">Archivus: A multimodal system for multimedia meeting browsing and retrieval</title>
<author confidence="0.957889">Marita Ailomaa</author>
<author confidence="0.957889">Miroslav Melichar</author>
<author confidence="0.957889">Agnes Lisowska</author>
<author confidence="0.957889">Martin Rajman Susan Armstrong</author>
<affiliation confidence="0.9982265">Artificial Intelligence Laboratory ISSCO/TIM/ETI ´Ecole Polytechnique F´ed´erale de Lausanne University of Geneva</affiliation>
<address confidence="0.996989">CH-1015 Lausanne, Switzerland CH-1211 Geneva, Switzerland</address>
<email confidence="0.664282">marita.ailomaa@epfl.chagnes.lisowska@issco.unige.ch</email>
<abstract confidence="0.996925466666667">This paper presents Archivus, a multimodal language-enabled meeting browsing and retrieval system. The prototype is in an early stage of development, and we are currently exploring the role of natural language for interacting in this relatively unfamiliar and complex domain. We briefly describe the design and implementation status of the system, and then focus on how this system is used to elicit useful data for supporting hypotheses about multimodal interaction in the domain of meeting retrieval and for developing NLP modules for this specific domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pavel Cenek</author>
<author>Miroslav Melichar</author>
<author>Martin Rajman</author>
</authors>
<title>A Framework for Rapid Multimodal Application Design.</title>
<date>2005</date>
<booktitle>Proceedings of the 8th International Conference on Text, Speech and Dialogue (TSD 2005),</booktitle>
<volume>3658</volume>
<pages>393--403</pages>
<editor>In V´aclav Matouˇsek, Pavel Mautner, and Tom´aˇs Pavelka, editors,</editor>
<publisher>Springer.</publisher>
<location>Karlovy Vary, Czech Republic,</location>
<contexts>
<context position="5159" citStr="Cenek et al., 2005" startWordPosition="802" endWordPosition="805">y (Lisowska et al., 2004). Furthermore, Archivus is flexibly multimodal, meaning that users can interact unimodally choosing one of the available modalities exclusively, or multimodally, using any combination of the modalities. In order to encourage natural language interaction, the system gives textual and vocal feedback to the user. The Archivus Interface is shown in Figure 1. A detailed description of all of the components can be found in Lisowska et al. (2004). Archivus was implemented within a software framework for designing multimodal applications with mixed-initiative dialogue models (Cenek et al., 2005). Systems designed within this framework handle interaction with the user through a multimodal dialogue manager. The dialogue manager receives user input from all modalities (speech, typing and pointing) and provides multimodal responses in the form of graphical, textual and vocal feedback. The dialogue manager contains only linguistic knowledge and interaction algorithms. Domain knowledge is stored in an SQL database and is accessed by the dialogue manager based on the constraints expressed by the user during interaction. The above software framework provides support for remote simulation or </context>
</contexts>
<marker>Cenek, Melichar, Rajman, 2005</marker>
<rawString>Pavel Cenek, Miroslav Melichar, and Martin Rajman. 2005. A Framework for Rapid Multimodal Application Design. In V´aclav Matouˇsek, Pavel Mautner, and Tom´aˇs Pavelka, editors, Proceedings of the 8th International Conference on Text, Speech and Dialogue (TSD 2005), volume 3658 of Lecture Notes in Computer Science, pages 393–403, Karlovy Vary, Czech Republic, September 12-15. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils Dahlb¨ack</author>
<author>Arne J¨onsson</author>
<author>Lars Ahrenberg</author>
</authors>
<date>1993</date>
<booktitle>Wizard of Oz Studies – Why and How. In Dianne Murray</booktitle>
<pages>193--200</pages>
<editor>Wayne D. Gray, William Hefley, editor,</editor>
<publisher>ACM Press.</publisher>
<marker>Dahlb¨ack, J¨onsson, Ahrenberg, 1993</marker>
<rawString>Nils Dahlb¨ack, Arne J¨onsson, and Lars Ahrenberg. 1993. Wizard of Oz Studies – Why and How. In Dianne Murray Wayne D. Gray, William Hefley, editor, International Workshop on Intelligent User Interfaces 1993, pages 193–200. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Agnes Lisowska</author>
<author>Martin Rajman</author>
<author>Trung H Bui</author>
</authors>
<title>ARCHIVUS: A System for Accessing the Content of Recorded Multimodal Meetings. In</title>
<date>2004</date>
<booktitle>In Procedings of the JOINT AMI/PASCAL/IM2/M4 Workshop on Multimodal Interaction and Related Machine Learning Algorithms, Bourlard</booktitle>
<editor>H. &amp; Bengio S., eds. (2004), LNCS,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin., Martigny, Switzerland,</location>
<contexts>
<context position="4565" citStr="Lisowska et al., 2004" startWordPosition="713" endWordPosition="716">�2006 Association for Computational Linguistics Figure 1: The Archivus Interface tions compared to others, language use must be encouraged without being forced, and finding this balance can be very hard to achieve in practice. 2 Design and implementation The Archivus system has been designed to satisfy realistic user needs based on a user requirement analysis (Lisowska, 2003), where subjects were asked to formulate queries that would enable them to find out “what happened at a meeting”. The design of the user interface is based on the metaphor of a person interacting in an archive or library (Lisowska et al., 2004). Furthermore, Archivus is flexibly multimodal, meaning that users can interact unimodally choosing one of the available modalities exclusively, or multimodally, using any combination of the modalities. In order to encourage natural language interaction, the system gives textual and vocal feedback to the user. The Archivus Interface is shown in Figure 1. A detailed description of all of the components can be found in Lisowska et al. (2004). Archivus was implemented within a software framework for designing multimodal applications with mixed-initiative dialogue models (Cenek et al., 2005). Syst</context>
</contexts>
<marker>Lisowska, Rajman, Bui, 2004</marker>
<rawString>Agnes Lisowska, Martin Rajman, and Trung H. Bui. 2004. ARCHIVUS: A System for Accessing the Content of Recorded Multimodal Meetings. In In Procedings of the JOINT AMI/PASCAL/IM2/M4 Workshop on Multimodal Interaction and Related Machine Learning Algorithms, Bourlard H. &amp; Bengio S., eds. (2004), LNCS, Springer-Verlag, Berlin., Martigny, Switzerland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Agnes Lisowska</author>
</authors>
<title>Multimodal interface design for the multimodal meeting domain: Preliminary indications from a query analysis study. Project report IM2.MDM-11,</title>
<date>2003</date>
<institution>University of Geneva,</institution>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="4321" citStr="Lisowska, 2003" startWordPosition="672" endWordPosition="673">t encourages language use. But, if the goal of the experiment is also to study what modalities users find more useful in some situa49 Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 49–52, Sydney, July 2006. c�2006 Association for Computational Linguistics Figure 1: The Archivus Interface tions compared to others, language use must be encouraged without being forced, and finding this balance can be very hard to achieve in practice. 2 Design and implementation The Archivus system has been designed to satisfy realistic user needs based on a user requirement analysis (Lisowska, 2003), where subjects were asked to formulate queries that would enable them to find out “what happened at a meeting”. The design of the user interface is based on the metaphor of a person interacting in an archive or library (Lisowska et al., 2004). Furthermore, Archivus is flexibly multimodal, meaning that users can interact unimodally choosing one of the available modalities exclusively, or multimodally, using any combination of the modalities. In order to encourage natural language interaction, the system gives textual and vocal feedback to the user. The Archivus Interface is shown in Figure 1.</context>
</contexts>
<marker>Lisowska, 2003</marker>
<rawString>Agnes Lisowska. 2003. Multimodal interface design for the multimodal meeting domain: Preliminary indications from a query analysis study. Project report IM2.MDM-11, University of Geneva, Geneva, Switzerland, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Salber</author>
<author>Jo¨elle Coutaz</author>
</authors>
<title>Applying the wizard of oz technique to the study of multimodal systems.</title>
<date>1993</date>
<booktitle>In EWHCI ’93: Selected papers from the Third International Conference on HumanComputer Interaction,</booktitle>
<pages>219--230</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="2406" citStr="Salber and Coutaz, 1993" startWordPosition="356" endWordPosition="359"> www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html try out and consistently use novel input modalities such as voice, including more complex natural language, and that in particular in this domain, such multimodal interaction can help the user find information more efficiently. When developing a language interface for an interactive system in a new domain, the Wizard of Oz (WOz) methodology (Dahlb¨ack et al., 1993; Salber and Coutaz, 1993) is a very useful tool. The user interacts with what they believe to be a fully automated system, when in fact another person, a ‘wizard’ is simulating the missing or incomplete NLP modules, typically the speech recognition, natural language understanding and dialogue management modules. The recorded experiments provide valuable information for implementing or fine-tuning these parts of the system. However, the methodology is usually applied to unimodal (voice-only or keyboard-only) systems, where the elicitation of language data is not a problem since this is effectively the only type of data</context>
</contexts>
<marker>Salber, Coutaz, 1993</marker>
<rawString>Daniel Salber and Jo¨elle Coutaz. 1993. Applying the wizard of oz technique to the study of multimodal systems. In EWHCI ’93: Selected papers from the Third International Conference on HumanComputer Interaction, pages 219–230, London, UK. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>