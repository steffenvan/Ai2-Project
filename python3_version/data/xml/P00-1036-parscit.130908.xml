<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.052754">
<title confidence="0.973158">
Independence and Commitment: Assumptions for Rapid
Training and Execution of Rule-based POS Taggers
</title>
<author confidence="0.998672">
Mark Hepple
</author>
<affiliation confidence="0.999626">
Department of Computer Science, University of Sheffield, Regent Court,
</affiliation>
<address confidence="0.522846">
211 Portobello Street, Sheffield 51 4DP, UK [hepple@dcs .shef .ac .uk]
</address>
<sectionHeader confidence="0.957871" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949428571429">
This paper addresses the rule-based
POS tagging method of Brill, and
questions the importance of rule in-
teractions to its performance. Ad-
opting two assumptions that serve
to exclude rule interactions during
tagging and training, we arrive at
some variants of Brill&apos;s approach
that are instances of decision list
models. These models allow for both
rapid training on large data sets and
rapid tagger execution, giving tag-
ging accuracy that is comparable to,
or better than the Brill method.
</bodyText>
<sectionHeader confidence="0.998518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996230877551021">
Part-of-speech (POS) tagging is the task of
assigning to each word in a sentence a tag
indicating its lexical syntactic category, such
as noun or verb. POS tagging of text is
required for subsequent processes in many
systems, e.g. syntactic parsing. A number
of alternative models and methods for tag-
ging have been explored, most particularly
with a view to improving tagging accuracy,
including: hidden Markov models (Church,
1988; Charniak et al., 1993; Cutting et al.,
1992), rule-based methods (Brill, 1995), max-
imum entropy methods (Ratnaparkhi, 1996),
memory-based methods (Daelemans et al.,
1996), amongst others.
This paper addresses the rule-based POS
tagging approach of Brill (1993; 1995), which
°I am indebted to Rob Gaizauskas for discussions
which helped greatly in the development of the ideas
in this paper.
learns language models that consist of a se-
quence of transformation rules (TRs) which
capture contextual factors in predicting cor-
rect assignments. The approach allows cer-
tain interactions between rules uses, whereby
a change effected by one may affect whether
or not another may subsequently fire. Such
interactions have been assumed important for
the approach in regard to its success on the
POS tagging task.
In this paper, we explore the possibility
that such interactions are not empirically im-
portant for Brill&apos;s approach, at least as ap-
plied to POS tagging. To this end, we in-
troduce two assumptions, called independ-
ence and commitment, which serve to ex-
clude rule interactions, and develop two vari-
ants of Brill&apos;s approach that realise these as-
sumptions. These methods turn out to be
instances of decision list models, a standard
approach within the machine learning field
(Rivest, 1987). These models allow for train-
ing and execution algorithms that give much
improved performance in terms of speed, with
an impact on tagging accuracy that ranges
from slight degradation to small improve-
ment. These results serve to shed light on
the importance of rule interaction to the per-
formance of Brill&apos;s original model.
</bodyText>
<sectionHeader confidence="0.880317" genericHeader="introduction">
2 Brill&apos;s Tagging Approach
</sectionHeader>
<bodyText confidence="0.9964093828125">
Brill&apos;s supervised learning model
&amp;quot;transformation-based error-driven learning&amp;quot;
works as follows. The training data is
correctly annotated text. The corresponding
raw text is fed through an initial-state
annotator, which makes a more or less
well-informed initial guess of how the text
should be annotated. This initial annotation
is compared to the training data as a basis for
learning a sequence of TRs, which are context
dependent &apos;correction&apos; rules, that apply in
sequence to modify the initial annotation to
better approximate the training data. The
trained system consists of the initial-state
annotator together with the ordered sequence
of TRs, which can be applied to unseen text.
For POS tagging, the initial-state annot-
ator assigns each known word its most-
probable tag from amongst those listed in
a lexicon, as determined from some train-
ing corpus. The initial tagging of unknown
words can be handled in a number of ways,
using clues such as affixes and capitalisation.
However, we shall not dwell on the treatment
of unknown words in this paper. The ini-
tial annotation is compared with the training
data to identify TRs that improve accuracy
by making specific context-bound corrections
steps, e.g. replacing tag X with Y, provided
tag Z appears in some nearby position (where
Y is also a lexical tag of the word).
The space of possible TRs is fixed by stat-
ing a set of rule &apos;templates&apos;, which are es-
sentially underspecified TRs. For example,
one template provides the pattern &apos;change tag
A to B where the previous word has tag C&apos;
(where A,B,C are unspecified). Another al-
lows the change if tag C is assigned to either of
the two preceding words, etc. In the TRs that
are learned, these unspecified values (A,B,C)
are instantiated to specific parts of speech.
Rules can also require specific words, rather
than just tags, to appear in context.
TRs are learned as an ordered sequence.
At each stage, the next rule adopted is the
one that gives the best net improvement in
tagging accuracy (since rules can both effect
corrections and introduce errors). This rule
is then applied to the current tag state of
the training set (which is initially the initial-
state assignment), and the next rule sought.
This process terminates when the improve-
ment falls to some prespecified threshold.
An interesting feature of this approach is
that it allows certain interactions between
rules, with possible beneficial effects. A
change effected by one rule may cause the
context requirement of another rule to be sat-
isfied allowing it to fire. For example, given
rules &amp;quot;change b to c if previous tag is a&amp;quot; and
&amp;quot;change d to e if previous tag is c&amp;quot; and a
sequence abd, the first rule may fire to give
acd, allowing the second rule to fire giving
ace. A second form of interaction is where
the tag substituted by one rule is itself sub-
ject to change by another rule. For example,
given rules &amp;quot;change b to c if previous tag is
a&amp;quot; and &amp;quot;change c to d if next tag is e&amp;quot; and
a sequence abe, the first rule can fire giving
ace, allowing the second rule to fire, to again
affect the second position, giving ade. Such
rule interactions have been seen as an im-
portant feature, allowing what Ramshaw and
Marcus (1994) call &apos;leveraging&apos;, as in &amp;quot;lever-
aging of partial solutions between neighboring
instances&amp;quot;, i.e. so that corrections effected by
earlier rules in sequence allow more accurate
operation of later rules (both as part of learn-
ing, and in execution).
An advantage of Brill&apos;s approach is that
its learned model is quite compact, consisting
of a few hundred rules, that can be directly
inspected (c.f. the thousands of contextual
probabilities of a HMM tagger). As discussed
in Ramshaw and Marcus (1994), the approach
is resistant to overtraining effects. In Brill&apos;s
experiments (Brill, 1995), training on 600K
tokens of the PTB tagged Wall Street Journal
corpus under the &apos;closed vocabulary assump-
tion&apos; (where there are no unknown words)
gave tagging accuracy of 97.2%. Without this
assumption, where performance also depends
on the handling unknown words, the score
was 96.6%. These results were state-of-the-
art at the time, but have since been surpassed
by some statistical approaches and &apos;voting&apos;
systems that combine multiple taggers (Rat-
naparkhi, 1996; van Halteren et al., 1998).
Problems of Brill&apos;s approach include,
firstly, the speed of tagging, which was found
to be significantly slower than HMM-based
competitors. However, Roche and Schabes
(1995) show how to compile a rule-based tag-
ger to a finite-state transducer, giving very
fast execution. Secondly, there is the cost
in time of training, which for larger-sized
training sets (e.g. 600K tokens) will take
something around a day or more to complete
(using Brill&apos;s own implementation). Ram-
shaw and Marcus (1994) propose an faster &apos;in-
cremental&apos; training algorithm (which avoids
rescanning the corpus for each rule that is
learned by using lists of pointers to link rules
to the sites where they apply), but note
that its memory requirements are so high
as to limit its applicability. Samuel (1998)
proposes a &apos;lazy&apos; version of transformation-
based learning, a Monte Carlo variant of
the standard method, and applies it to dia-
logue act tagging. The ii-TBL system of La-
ger (1999) is an efficient Prolog implementa-
tion of transformation-based learning, which
trains in shorter time than Brill&apos;s own im-
plementation (by an order of magnitude, for
the tasks reported). ii-TBL also implements
&apos;lazy&apos; learning, and Lager&apos;s results indicate
comparable tagging accuracy for the lazy and
standard methods as applied to POS tagging.
</bodyText>
<sectionHeader confidence="0.954884" genericHeader="method">
3 Is Rule Interaction Important?
</sectionHeader>
<bodyText confidence="0.999970787878788">
The starting point for the work in this pa-
per is questioning the assumption that rule
interaction is important to the performance of
Brill&apos;s method on POS tagging. Consider the
fact that, for PTB Wall Street Journal text,
baseline performance under the closed vocab-
ulary assumption, from assigning the most-
probable tag to each word, is around 94.5%.
A final tagging accuracy of around 97% indic-
ates an improvement of around 1 in 40 tags
being corrected. Given this &apos;spareness&apos; of cor-
rections, we might suspect that the frequency
of one correction being appropriately close to
enable another will be quite low.
The only hard results of which we are aware
that bear on this issue are given by Ram-
shaw and Marcus (1994), who report that
rule learning on a 50K token sample of the
Brown Corpus involved changes at 3395 sites,
of which for only 395 sites did the change de-
pend on the action of more than one rule.
These 395 sites account for about 0.8% of the
training set, but note that this does trans-
late to 0.8% of the resulting tagging accuracy
since applying a rule can have either positive,
negative or neutral consequences.
In the remainder of the paper, we will
develop some rule-based tagging methods
for which rule interaction is explicitly ex-
cluded. The performance of these mod-
els provides empirical evidence concerning
the real importance of rule interaction to
transformation-based POS tagging.
</bodyText>
<sectionHeader confidence="0.977757" genericHeader="method">
4 Independence and Commitment
</sectionHeader>
<bodyText confidence="0.978494720430108">
Our view of rule interaction is embodied by
two assumptions, we call independence and
commitment, which together serve to exclude
rule interactions. The commitment assump-
tion regulates how rules are used, requiring
that where a rule fires to change a tag, that
tag may not subsequently be changed again
(so we are &apos;committed&apos; to the change). In-
dependence is the assumption that the fre-
quency with which an earlier rule will modify
the context relevant to the firing of a later
rule is sufficiently low that it can be ignored.
Individually, these assumptions still allow
a considerable degree of freedom as to how a
method might be specified. During tagging,
for example, commitment leaves it open as to
whether a rule sequence should be used by
applying each rule in turn to the entire ini-
tial tag state or if they should be applied as
a group to tag each token in turn. Either
option allows the possibility that earlier tag
replacement steps might affect later rule fir-
ings. Independence, however, instructs us to
ignore such interactions, in which case the two
options should give essentially equivalent res-
ults. We will pursue the second option, with
rules being applied as a group to each token,
which involves testing each rule in turn until
the list is exhausted or one rule fires, in which
case the remaining rules are ignored.
This &apos;one-shot&apos; style of using rules is famil-
iar in machine learning as an instance of a
decision list model (Rivest, 1987). More spe-
cifically, given the character of the rules in-
volved, we have a propositional decision list
system.&apos; A standard use of decision lists is
1-The more expressive formalism of first-order de-
for classification tasks, i.e. assigning categor-
ies to examples, based on their (static) char-
acteristics. The &apos;dynamic&apos; character tagging
makes it somewhat different from classifica-
tion, a fact which might present difficulties
for decision list learning were it not for the
independence assumption.
In the Brill learning approach, rules are ac-
quired in the same order as they are applied
during tagging. Minimally adapting such an
approach to a decision list use of TRs gives
an instance of a sequential covering algorithm,
i.e. the first rule learned &apos;covers&apos; some part
of the training data (that now &apos;belongs&apos; to
that rule), and then the next rule learned cov-
ers some part of what remains, and so on.
Following Webb and Brkic (1993), we refer
to this way of ordering rules as appending,
i.e. new rules are appended to the end of
the current rule list. Commonly in sequential
rule learning, rules learned earlier are highly
general, applying to many instances, whereas
rules learned later are more specific, applying
to fewer instances. The general rules learned
earlier have a large net positive effect, but
may get specific subsets of cases wrong. The
Brill approach allows later rules to be learned
to correct errors made by earlier rules, via se-
quential rule application. For a decision list
treatment of tagging, this possibility does not
arise.
Webb and Brkic (1993) advocates a vari-
ant of decision list learning in which rules ac-
quired are prepended, i.e. added to the front
of the current list, so that rules are applied
in the reverse of the order in which they are
learned. Such an approach looks promising
for a decision list treatment of tagging as it
will allow for mistakes made by earlier learned
rules to be &apos;overridden&apos; by later more specific
rules that are prepended. In this regard, it
may be useful to allow &apos;identity&apos; TRs, which
fire leaving the default tag unchanged, but
serve to prevent the same position being mod-
ified by any subsequent rule in the list. We
cision lists is used within the field Inductive Logic Pro-
gramming, which in recent work has been applied to
a range of NLP tasks. See, for example, (Cussens,
1997) for an ILP treatment of tagging, which is very
different to the present approach.
will consider training methods for rule-based
tagging without rule interaction in both ap-
pending and prepending variants, which we
will refer to as ICA (&apos;independence and com-
mitment, with appending&apos;) and ICP (`... with
prepending&apos;), respectively.
</bodyText>
<sectionHeader confidence="0.97741" genericHeader="method">
5 The Training Algorithm
</sectionHeader>
<bodyText confidence="0.9999545">
The independence and commitment assump-
tions have a consequence that is crucial in al-
lowing a rapid and memory efficient training
algorithm. This is that we can divide train-
ing into separate phases, each of which sep-
arately addresses only the learning of rules
that modify a given single POS. Thus, in a
phase learning rules that modify tag t, inde-
pendence instructs us to ignore the fact that
rules learned in some earlier (or later) phase
might modify the contexts around positions
tagged t. Also, the possibility that rules of
other phases might modify some other tag to
become t can be ignored, since commitment
prevents this tag from being further modified
by rules learned in the current phase.
Each phase of training can restrict its at-
tention to only a limited portion of the train-
ing corpus, e.g. learning rules to modify tag
t, we need only address relevant &apos;data points&apos;,
which are those positions having initial tag t
and at least one alternative lexical tag. The
TRs that can apply at these positions are a
fraction of those that could apply anywhere
in the corpus. The approach allows for an
&apos;incremental&apos; training algorithm which does
not suffer the space problems of Ramshaw and
Marcus (1994). Initially, we must score for all
transformations that apply at the data points
of the phase. On identifying and adopting
the best rule T, we do not need to recom-
pute all rule scores, but can instead scan the
data points to find those at which T fires, and
update our existing record of rule scores in re-
gard to the possible TRs that can fire at these
locations. We can then immediately determ-
ine the next rule to adopt, and so on.
An informal sketch of the training al-
gorithm is given i n This makes no provi-
sion for unknown words, since we make the
&apos;closed vocabulary assumption&apos; in our exper-
Load training corpus into memory (array), i.e. words + correct tag, and make initial tag
assignment (assign most-probable tag to each word).
Then, for each tag x, learn list of rules that apply for this tag as follows:
</bodyText>
<listItem confidence="0.8793765">
1. Scan corpus, and record &apos;data points&apos; for this phase, i.e. positions where default tag is
x and word has at least one alternative tag (store these array offsets)
2. Compute initial TR correction scores. Firstly, scan for data points where tag x is
incorrect, and at each score +1 for all possible TRs changing x to the correct tag
(scores stored in a hash). Secondly, at each point where tag x is correct, score —1
for all possible TRs changing x to any lexical alternative but only for rules already
present in scores hash (i.e. rules effecting at least one correction).
3. Loop acquiring rules as follows:
</listItem>
<bodyText confidence="0.982887">
Scan scores hash for best rule T (exit loop if score not above threshold) and add to rule
list. Update scores hash as follows: scan for data points where rule T fires, and update
scores for rules that fire at these positions. (Update scoring details given in text.)
</bodyText>
<figureCaption confidence="0.999678">
Figure 1: Training Algorithm
</figureCaption>
<bodyText confidence="0.999979492537314">
iments. Figure 1. omits the specifics of
how rule scores are incrementally updated,
as this differs between appending and pre-
pending approaches (we return to this mat-
ter shortly). An &apos;efficiency feature&apos; of the al-
gorithm is that in computing the initial scores
for TRs, we firstly identify the rules that effect
at least one correction somewhere, and then
subsequently only score negative changes for
these rules. For the appending version, these
initially identified rules are the only ones that
need be considered during the phase. The
situation is similar for the prepending ver-
sion, except that we must additionally allow
for identity rules to be considered as possible
rules during incremental update.
Some further efficiencies that are not de-
scribed in Figure 1. involve using the prespe-
cified stopping threshold for training to prefil-
ter work. When the corpus is first loaded, we
can extract counts for all &apos;error pairs&apos; (x, y)
where the default tag x should have been
y. We need only entertain TRs changing x
to y if the count for the pair (x, y) is above
threshold, i.e. since it is otherwise impossible
that such TRs could score above threshold.
A position with initial tag x is considered a
&apos;data point&apos; in training only if it has an altern-
ative tag y for an above threshold error pair
(x,y). Similarly, when we identify the data
points for a phase of training, we can count
the occurrences of words appearing within the
&apos;context window&apos; around these points. We
need then only consider instantiations of lex-
ical templates using words whose appearance
count in these contexts is above threshold.
As noted above, the details of the incre-
mental update part of the training algorithm
differ for the ICA and ICP variants. For ICA,
the scores hash should be viewed as recording,
at each stage, the scores for each rule if it were
adopted to &apos;cover&apos; some portion of the remain-
ing training instances. When some new rule
T is adopted, we then identify the data points
where T fires. These positions now &apos;belong&apos; to
T, and are discounted for subsequent scoring
(i.e. deleted from the record of &apos;data points&apos;),
and we must also &apos;undo&apos; or cancel any scores
in the rule scores hash that depend on these
positions, i.e. scoring —1 if the rule effects
a correction at one of these positions, or +1
if it introduces an error. For ICP, the scores
hash records, at each stage, the scores for each
rule if it were adopted to override the exist-
ing rule set. Again, when a rule T is adopted,
we can update the scores hash by consider-
ing only the data points where T fires, but in
this case the change made to the score of a
possible rule P that could fire at these posi-
tions depends not on whether P outputs the
correct tag or not, but rather on whether the
position would be correct under the previous
rule set, and whether it will be correct given
T&apos;s adoption. Specifically, if a position was
correct (resp. incorrect) and T makes it cor-
rect (resp. correct), then for rule P that can
fire at this position we score +1 (resp. —1).2
</bodyText>
<sectionHeader confidence="0.921938" genericHeader="method">
6 The Tagging Algorithm
</sectionHeader>
<bodyText confidence="0.999982909090909">
For Brill&apos;s method, the distance at which two
rule uses may directly affect each other is lim-
ited by the width of contexts in rule tem-
plates. By chaining, indirect interactions can
in theory span over much greater distances.
Hence, a tagging algorithm which applies the
rules directly cannot work with a narrow win-
dow over the text, but instead must work with
text units of at least sentence size.
For the present approach, the independence
assumption tells us to ignore rule interactions,
so we can use an algorithm which streams
the text through a buffer that is just wide
for single rules to fire. With contexts ran-
ging upto three elements either side, a win-
dow width of seven elements suffices. At each
step of advancing the window, we examine the
initial tag assigned to the central element, ac-
cess the subclass of rules that can modify this
tag, and check these rules in sequence to see
if any can apply. If any one (or none) of them
fires, we have finished at this position.3
</bodyText>
<sectionHeader confidence="0.8817725" genericHeader="method">
7 Evaluating rule-based tagging
without rule interaction
</sectionHeader>
<bodyText confidence="0.999951">
We compared the performance of Brill&apos;s im-
plementation (for which the code is pub-
</bodyText>
<subsectionHeader confidence="0.876909">
&apos;This can be seen by reasoning through the differ-
</subsectionHeader>
<bodyText confidence="0.919076">
ent cases. For example, if the position was previously
correct, T makes it incorrect, and P assigns the cor-
rect tag, then P&apos;s score is incremented, since before
T&apos;s adoption, P&apos;s effect was neutral at this position,
and now it will effect a correction. If P instead as-
signs an incorrect tag, its score is still incremented,
since P&apos;s effect was previously negative at this posi-
tion and now its effect will be neutral (i.e. T takes the
blame for making the position wrong).
</bodyText>
<footnote confidence="0.987247">
3There is a question as to whether we should actu-
ally overwrite a tag change into the buffer (rather than
just outputting it), so that this is the tag present when
rules applying to the next few positions are checked.
According to the independence assumption, this de-
cision should matter little, and our informal explora-
tions bear this idea out.
</footnote>
<bodyText confidence="0.999943954545455">
licly available) with an implementation in C
of the algorithms sketched above, employ-
ing the same template set (with both lex-
ical and non-lexical templates), and making
the closed vocabulary assumption. A cross-
validation approach was used. We took the
even numbered sections of the tagged Wall
Street Journal component of the PTB (com-
prising 680K tokens) and randomly distrib-
uted the sentences into eight bins, resulting
in bins of size 85±2K tokens. These collec-
tions allow for an eight-fold cross-validation
of results, i.e. each training task is repeated
eight times, with one of the bins selected as
the test data, and the other seven combined
to form the training data, giving training set
sizes of around 595±2K tokens (comparable
to the 600K used by Brill in his larger experi-
ments). The results obtained are averaged to
give a more reliable indication of performance
than could be gained from any single run.4
We trained the Brill system at thresholds
15 (the value suggested for this much training
data in the code&apos;s instructions) and 12. The
results are shown in the Table 1. (where the
scores for accuracy, etc, are averages over the
eight runs). The final column of the table
shows the time taken to tag 1 million tokens
using the rule sets produced (again averaged).
The table also shows corresponding results for
the ICA and ICP training methods, at a range
of different training thresholds.
Let us write &amp;quot;method/threshold&amp;quot; (e.g.
Brill/15) to refer to a method trained at a
given threshold. Observe that the tagging ac-
curacy for ICA/15 is lower, by 0.16%, than
for Brill/15. However, ICA here produces less
rules than Brill (154 vs. 183), so it is interest-
ing instead to compare ICA/12 and Brill/15,
which produce similar rule counts. Here the
accuracy of ICA is lower by only 0.1%. Since
ICA is the &apos;minimal variant&apos; of the original
Brill method, these results can be viewed as
providing direct empirical evidence of the im-
</bodyText>
<footnote confidence="0.982812833333333">
4The Brill training experiments were performed on
a Sun Enterprise 450 Server, having two 400Mhz Ul-
traSPARC CPUs and &gt;1G of memory. The ICA/ICP
training experiments used the same machine, but by
the time they were run, it had been ungraded to have
four 400Mhz UltraSPARC CPUs.
</footnote>
<table confidence="0.998870631578947">
method threshold tagging rule training: memory execution:
accuracy count runtime runtime (secs)
(%) (mins) on 106 tokens
Brill 15 96.99 183 1310 41 180
Brill 12 97.05 214 1625 41 207
ICA 15 96.83 154 1.6 18 10.5
ICA 12 96.89 182 1.6 18 10.9
ICA 9 96.96 231 1.7 18 11.4
ICA 6 97.03 324 1.9 18 12.3
ICA 4 97.07 459 2.2 18 13.9
ICA 3 97.09 605 2.4 18 15.0
ICA 2 97.11 919 2.7 18 18.4
ICP 15 96.96 175 1.9 19 11.0
ICP 12 97.03 211 2.0 19 11.3
ICP 9 97.12 272 2.1 20 12.0
ICP 6 97.21 389 2.5 21 13.1
ICP 4 97.27 566 2.9 22 15.1
ICP 3 97.30 755 3.3 22 17.1
ICP 2 97.35 1183 4.1 23 21.9
</table>
<tableCaption confidence="0.99987">
Table 1: Experimental Results (cross-validation, with training sets of ,--,595K tokens)
</tableCaption>
<bodyText confidence="0.999882756756757">
portance of rule interaction to Brill&apos;s rule-
based tagging method, suggesting that it does
have an impact on performance, but one that
is rather limited. The short training times
for ICA allow that we might seek to recoup
this loss in accuracy, as compared to Brill, by
training at lower thresholds. The accuracy
of Brill/15 is nearly matched by ICA/9 and
slightly surpassed by ICA/6. The results for
ICP bear out the suggestion that prepending
might improve performance. The results for
ICP/15 and ICP/12 are only very slightly be-
low those for Brill/15 and Brill/12. Training
at lower thresholds, ICP produces results that
significantly improve on those for Brill/12,
but still with very short training times.
A striking feature of the results in Table 1.
is the relative insensitivity of ICA/ICP tag-
ging time to rule set size. Comparing ICA/15
and ICA/3, for example, we see a roughly
four-fold increase in rule count, but an in-
crease in tagging time of around 40%. By
contrast, Brill&apos;s method shows an increase in
tagging time that is roughly proportional to
the increase in rule count. This is not surpris-
ing since the tagging algorithm requires a pass
over the loaded input for each rule in the se-
quence. For decision list tagging, only a single
pass over the input is made. At each token
only a subset of the rules are accessed, and if
any rule fires, the remainder are ignored. We
might predict slightly slower tagging for ICP
rules sets than for ICA, i.e. since the highly
general rules that fire most often will appear
later in the decision list for ICP than for ICA,
and there is some evidence for this in the res-
ults (e.g. compare ICP/4 to ICA/3).
</bodyText>
<sectionHeader confidence="0.997958" genericHeader="conclusions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999946025641026">
We have shown how Brill&apos;s rule-based tagging
approach can be reformulated as a decision
list model, allowing for much faster train-
ing, to give results that are comparable to,
or better than, those of Brill&apos;s original sys-
tem. The modified approach allows for tagger
execution that is quite fast, even for an im-
plementation performing direct rule interpret-
ation. This implementation, however, prob-
ably cannot compare with the speed of a tag-
ger for the standard Brill approach produced
using the finite-state compilation method of
Roche and Schabes (1995), being run on cur-
rent hardware. We expect that similar finite-
state compilation is possible for the decision
list approach, and speculate that, since long-
distance interaction effects are excluded, it
should be possible to do a compilation pro-
ducing much smaller finite-state transducers.
A final point to consider is whether ex-
amples for which the Brill approach relies on
interaction can be correctly handled in the
modified approach. We cannot give a general
answer to this question, but the following ex-
ample gives some indication of what may be
happening in many cases. The Brill tagger
assigns the phrase to sign up the initial tags
to/TO sign/NN up/RB, which are (correctly)
changed to be to/TO sign/VB up/RP under
the rules &amp;quot;NN becomes VB if previous tag is
TO&amp;quot; and &amp;quot;RB becomes RP if word is up and
previous tag is VB&amp;quot;, where the second relies
on the first having fired. Both ICA and ICP
yield rules that correctly handle the case also.
For ICP, for example, we have also the first
of the Brill rules, but the second rule involved
is instead &amp;quot;RB becomes RP if word is up and
tag before last is TO&amp;quot;, which correctly fires
without relying on the first rule.
</bodyText>
<sectionHeader confidence="0.99953" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99988196969697">
Eric Brill. 1993. A corpus-based approach to
language learning. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Eric Brill. 1995. Transformation-based error-
driven learning and natural language pro-
cessing: A case study in part of speech tag-
ging. Computational Linguistics, 21(4):543-
565, December.
Eugene Charniak, Curtis Hendrickson, Neil Jac-
obson, and Michael Perkowitz. 1993. Equa-
tions for part of speech tagging. In Proceedings
of the Eleventh National Conference on Artifi-
cial Intelligence, pages 784-789.
Kenneth Church. 1988. A stochastic parts pro-
gram and noun phrase parser for unrestricted
text. In Proceedings of the Second Conference
on Applied Natural Language Processing.
James Cussens. 1997. Part-of-speech tagging us-
ing Progol. In Proceedings of the Seventh In-
ternational Workshop on Inductive Logic Pro-
gramming, pages 93-108.
Doug Cutting, Julian Kupiec, Jan Pedersen, and
Penelope Sibun. 1992. A practical part of
speech tagger. In Proceedings of the Third
Conference on Applied Natural Language Pro-
cessing, pages 133-140.
Walter Daelemans, Jakub Zavrel, Peter Berck,
and Steven Gillis. 1996. Mbt: A memory-based
part of speech tagger-generator. In Proceedings
of the Fourth Workshop on Very Large Corpora,
pages 14-27.
Torbjorn Lager. 1999. The it-TBL system:
Logic programming tools for transformation-
based learning. In Proceedings of the Third In-
ternational Workshop on Computational Nat-
ural Language Learning (CoNLL &apos;99), Bergen.
Lance A. Ramshaw and Mitchell P. Marcus. 1994.
Exploring the statistical derivation of trans-
formation rule sequences for part-of-speech tag-
ging. In Proceedings of the 32nd Annual Meet-
ing of the ACL, pages 86-95.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, pages 133-142.
Ronald L. Rivest. 1987. Learning decision lists.
Machine Learning, 2(3):229-246.
Emmanuel Roche and Yves Schabes. 1995. De-
terministic part-of-speech tagging with finite-
state transducers. Computational Linguistics,
21(2):227-253.
Ken Samuel. 1998. Lazy transformation-based
learning. In Proceedings of the Eleventh Inter-
national Fonda Al Research Symposium Con-
ference, pages 235-239.
Hans van Halteren, Jakub Zavrel, and Walter
Daelemens. 1998. Improving data-driven word-
class tagging by system combination. In Pro-
ceedings of the International Conference on
Computational Linguistics COLING-98, pages
491-497.
Geoff Webb and N. Brkic. 1993. Learning de-
cision lists by prepending inferred rules. In
Proceedings of the AI-93 Workshop on Machine
Learning and Hybrid Systems, pages 6-10, Mel-
bourne.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.804953">
<title confidence="0.9998705">Independence and Commitment: Assumptions for Rapid Training and Execution of Rule-based POS Taggers</title>
<author confidence="0.999989">Mark Hepple</author>
<affiliation confidence="0.999931">Department of Computer Science, University of Sheffield, Regent Court,</affiliation>
<address confidence="0.917492">Portobello Street, Sheffield 51 4DP, UK .shef .ac .uk</address>
<abstract confidence="0.991735066666667">This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill&apos;s approach are instances of list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A corpus-based approach to language learning.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1460" citStr="Brill (1993" startWordPosition="221" endWordPosition="222">a tag indicating its lexical syntactic category, such as noun or verb. POS tagging of text is required for subsequent processes in many systems, e.g. syntactic parsing. A number of alternative models and methods for tagging have been explored, most particularly with a view to improving tagging accuracy, including: hidden Markov models (Church, 1988; Charniak et al., 1993; Cutting et al., 1992), rule-based methods (Brill, 1995), maximum entropy methods (Ratnaparkhi, 1996), memory-based methods (Daelemans et al., 1996), amongst others. This paper addresses the rule-based POS tagging approach of Brill (1993; 1995), which °I am indebted to Rob Gaizauskas for discussions which helped greatly in the development of the ideas in this paper. learns language models that consist of a sequence of transformation rules (TRs) which capture contextual factors in predicting correct assignments. The approach allows certain interactions between rules uses, whereby a change effected by one may affect whether or not another may subsequently fire. Such interactions have been assumed important for the approach in regard to its success on the POS tagging task. In this paper, we explore the possibility that such inte</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Eric Brill. 1993. A corpus-based approach to language learning. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based errordriven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="1279" citStr="Brill, 1995" startWordPosition="196" endWordPosition="197">tion, giving tagging accuracy that is comparable to, or better than the Brill method. 1 Introduction Part-of-speech (POS) tagging is the task of assigning to each word in a sentence a tag indicating its lexical syntactic category, such as noun or verb. POS tagging of text is required for subsequent processes in many systems, e.g. syntactic parsing. A number of alternative models and methods for tagging have been explored, most particularly with a view to improving tagging accuracy, including: hidden Markov models (Church, 1988; Charniak et al., 1993; Cutting et al., 1992), rule-based methods (Brill, 1995), maximum entropy methods (Ratnaparkhi, 1996), memory-based methods (Daelemans et al., 1996), amongst others. This paper addresses the rule-based POS tagging approach of Brill (1993; 1995), which °I am indebted to Rob Gaizauskas for discussions which helped greatly in the development of the ideas in this paper. learns language models that consist of a sequence of transformation rules (TRs) which capture contextual factors in predicting correct assignments. The approach allows certain interactions between rules uses, whereby a change effected by one may affect whether or not another may subsequ</context>
<context position="6623" citStr="Brill, 1995" startWordPosition="1086" endWordPosition="1087">at Ramshaw and Marcus (1994) call &apos;leveraging&apos;, as in &amp;quot;leveraging of partial solutions between neighboring instances&amp;quot;, i.e. so that corrections effected by earlier rules in sequence allow more accurate operation of later rules (both as part of learning, and in execution). An advantage of Brill&apos;s approach is that its learned model is quite compact, consisting of a few hundred rules, that can be directly inspected (c.f. the thousands of contextual probabilities of a HMM tagger). As discussed in Ramshaw and Marcus (1994), the approach is resistant to overtraining effects. In Brill&apos;s experiments (Brill, 1995), training on 600K tokens of the PTB tagged Wall Street Journal corpus under the &apos;closed vocabulary assumption&apos; (where there are no unknown words) gave tagging accuracy of 97.2%. Without this assumption, where performance also depends on the handling unknown words, the score was 96.6%. These results were state-of-theart at the time, but have since been surpassed by some statistical approaches and &apos;voting&apos; systems that combine multiple taggers (Ratnaparkhi, 1996; van Halteren et al., 1998). Problems of Brill&apos;s approach include, firstly, the speed of tagging, which was found to be significantly </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based errordriven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543-565, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Curtis Hendrickson</author>
<author>Neil Jacobson</author>
<author>Michael Perkowitz</author>
</authors>
<title>Equations for part of speech tagging.</title>
<date>1993</date>
<booktitle>In Proceedings of the Eleventh National Conference on Artificial Intelligence,</booktitle>
<pages>784--789</pages>
<contexts>
<context position="1222" citStr="Charniak et al., 1993" startWordPosition="186" endWordPosition="189"> for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method. 1 Introduction Part-of-speech (POS) tagging is the task of assigning to each word in a sentence a tag indicating its lexical syntactic category, such as noun or verb. POS tagging of text is required for subsequent processes in many systems, e.g. syntactic parsing. A number of alternative models and methods for tagging have been explored, most particularly with a view to improving tagging accuracy, including: hidden Markov models (Church, 1988; Charniak et al., 1993; Cutting et al., 1992), rule-based methods (Brill, 1995), maximum entropy methods (Ratnaparkhi, 1996), memory-based methods (Daelemans et al., 1996), amongst others. This paper addresses the rule-based POS tagging approach of Brill (1993; 1995), which °I am indebted to Rob Gaizauskas for discussions which helped greatly in the development of the ideas in this paper. learns language models that consist of a sequence of transformation rules (TRs) which capture contextual factors in predicting correct assignments. The approach allows certain interactions between rules uses, whereby a change effe</context>
</contexts>
<marker>Charniak, Hendrickson, Jacobson, Perkowitz, 1993</marker>
<rawString>Eugene Charniak, Curtis Hendrickson, Neil Jacobson, and Michael Perkowitz. 1993. Equations for part of speech tagging. In Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 784-789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="1199" citStr="Church, 1988" startWordPosition="184" endWordPosition="185">e models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method. 1 Introduction Part-of-speech (POS) tagging is the task of assigning to each word in a sentence a tag indicating its lexical syntactic category, such as noun or verb. POS tagging of text is required for subsequent processes in many systems, e.g. syntactic parsing. A number of alternative models and methods for tagging have been explored, most particularly with a view to improving tagging accuracy, including: hidden Markov models (Church, 1988; Charniak et al., 1993; Cutting et al., 1992), rule-based methods (Brill, 1995), maximum entropy methods (Ratnaparkhi, 1996), memory-based methods (Daelemans et al., 1996), amongst others. This paper addresses the rule-based POS tagging approach of Brill (1993; 1995), which °I am indebted to Rob Gaizauskas for discussions which helped greatly in the development of the ideas in this paper. learns language models that consist of a sequence of transformation rules (TRs) which capture contextual factors in predicting correct assignments. The approach allows certain interactions between rules uses</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Kenneth Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Cussens</author>
</authors>
<title>Part-of-speech tagging using Progol.</title>
<date>1997</date>
<booktitle>In Proceedings of the Seventh International Workshop on Inductive Logic Programming,</booktitle>
<pages>93--108</pages>
<contexts>
<context position="13686" citStr="Cussens, 1997" startWordPosition="2267" endWordPosition="2268">the reverse of the order in which they are learned. Such an approach looks promising for a decision list treatment of tagging as it will allow for mistakes made by earlier learned rules to be &apos;overridden&apos; by later more specific rules that are prepended. In this regard, it may be useful to allow &apos;identity&apos; TRs, which fire leaving the default tag unchanged, but serve to prevent the same position being modified by any subsequent rule in the list. We cision lists is used within the field Inductive Logic Programming, which in recent work has been applied to a range of NLP tasks. See, for example, (Cussens, 1997) for an ILP treatment of tagging, which is very different to the present approach. will consider training methods for rule-based tagging without rule interaction in both appending and prepending variants, which we will refer to as ICA (&apos;independence and commitment, with appending&apos;) and ICP (`... with prepending&apos;), respectively. 5 The Training Algorithm The independence and commitment assumptions have a consequence that is crucial in allowing a rapid and memory efficient training algorithm. This is that we can divide training into separate phases, each of which separately addresses only the lea</context>
</contexts>
<marker>Cussens, 1997</marker>
<rawString>James Cussens. 1997. Part-of-speech tagging using Progol. In Proceedings of the Seventh International Workshop on Inductive Logic Programming, pages 93-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Penelope Sibun</author>
</authors>
<title>A practical part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>133--140</pages>
<contexts>
<context position="1245" citStr="Cutting et al., 1992" startWordPosition="190" endWordPosition="193">g on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method. 1 Introduction Part-of-speech (POS) tagging is the task of assigning to each word in a sentence a tag indicating its lexical syntactic category, such as noun or verb. POS tagging of text is required for subsequent processes in many systems, e.g. syntactic parsing. A number of alternative models and methods for tagging have been explored, most particularly with a view to improving tagging accuracy, including: hidden Markov models (Church, 1988; Charniak et al., 1993; Cutting et al., 1992), rule-based methods (Brill, 1995), maximum entropy methods (Ratnaparkhi, 1996), memory-based methods (Daelemans et al., 1996), amongst others. This paper addresses the rule-based POS tagging approach of Brill (1993; 1995), which °I am indebted to Rob Gaizauskas for discussions which helped greatly in the development of the ideas in this paper. learns language models that consist of a sequence of transformation rules (TRs) which capture contextual factors in predicting correct assignments. The approach allows certain interactions between rules uses, whereby a change effected by one may affect </context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Doug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. A practical part of speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, pages 133-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Peter Berck</author>
<author>Steven Gillis</author>
</authors>
<title>Mbt: A memory-based part of speech tagger-generator.</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<contexts>
<context position="1371" citStr="Daelemans et al., 1996" startWordPosition="206" endWordPosition="209">hod. 1 Introduction Part-of-speech (POS) tagging is the task of assigning to each word in a sentence a tag indicating its lexical syntactic category, such as noun or verb. POS tagging of text is required for subsequent processes in many systems, e.g. syntactic parsing. A number of alternative models and methods for tagging have been explored, most particularly with a view to improving tagging accuracy, including: hidden Markov models (Church, 1988; Charniak et al., 1993; Cutting et al., 1992), rule-based methods (Brill, 1995), maximum entropy methods (Ratnaparkhi, 1996), memory-based methods (Daelemans et al., 1996), amongst others. This paper addresses the rule-based POS tagging approach of Brill (1993; 1995), which °I am indebted to Rob Gaizauskas for discussions which helped greatly in the development of the ideas in this paper. learns language models that consist of a sequence of transformation rules (TRs) which capture contextual factors in predicting correct assignments. The approach allows certain interactions between rules uses, whereby a change effected by one may affect whether or not another may subsequently fire. Such interactions have been assumed important for the approach in regard to its </context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven Gillis. 1996. Mbt: A memory-based part of speech tagger-generator. In Proceedings of the Fourth Workshop on Very Large Corpora, pages 14-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torbjorn Lager</author>
</authors>
<title>The it-TBL system: Logic programming tools for transformationbased learning.</title>
<date>1999</date>
<booktitle>In Proceedings of the Third International Workshop on Computational Natural Language Learning (CoNLL &apos;99),</booktitle>
<location>Bergen.</location>
<contexts>
<context position="8073" citStr="Lager (1999)" startWordPosition="1318" endWordPosition="1320"> training sets (e.g. 600K tokens) will take something around a day or more to complete (using Brill&apos;s own implementation). Ramshaw and Marcus (1994) propose an faster &apos;incremental&apos; training algorithm (which avoids rescanning the corpus for each rule that is learned by using lists of pointers to link rules to the sites where they apply), but note that its memory requirements are so high as to limit its applicability. Samuel (1998) proposes a &apos;lazy&apos; version of transformationbased learning, a Monte Carlo variant of the standard method, and applies it to dialogue act tagging. The ii-TBL system of Lager (1999) is an efficient Prolog implementation of transformation-based learning, which trains in shorter time than Brill&apos;s own implementation (by an order of magnitude, for the tasks reported). ii-TBL also implements &apos;lazy&apos; learning, and Lager&apos;s results indicate comparable tagging accuracy for the lazy and standard methods as applied to POS tagging. 3 Is Rule Interaction Important? The starting point for the work in this paper is questioning the assumption that rule interaction is important to the performance of Brill&apos;s method on POS tagging. Consider the fact that, for PTB Wall Street Journal text, b</context>
</contexts>
<marker>Lager, 1999</marker>
<rawString>Torbjorn Lager. 1999. The it-TBL system: Logic programming tools for transformationbased learning. In Proceedings of the Third International Workshop on Computational Natural Language Learning (CoNLL &apos;99), Bergen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Exploring the statistical derivation of transformation rule sequences for part-of-speech tagging.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the ACL,</booktitle>
<pages>86--95</pages>
<contexts>
<context position="6039" citStr="Ramshaw and Marcus (1994)" startWordPosition="993" endWordPosition="996">to c if previous tag is a&amp;quot; and &amp;quot;change d to e if previous tag is c&amp;quot; and a sequence abd, the first rule may fire to give acd, allowing the second rule to fire giving ace. A second form of interaction is where the tag substituted by one rule is itself subject to change by another rule. For example, given rules &amp;quot;change b to c if previous tag is a&amp;quot; and &amp;quot;change c to d if next tag is e&amp;quot; and a sequence abe, the first rule can fire giving ace, allowing the second rule to fire, to again affect the second position, giving ade. Such rule interactions have been seen as an important feature, allowing what Ramshaw and Marcus (1994) call &apos;leveraging&apos;, as in &amp;quot;leveraging of partial solutions between neighboring instances&amp;quot;, i.e. so that corrections effected by earlier rules in sequence allow more accurate operation of later rules (both as part of learning, and in execution). An advantage of Brill&apos;s approach is that its learned model is quite compact, consisting of a few hundred rules, that can be directly inspected (c.f. the thousands of contextual probabilities of a HMM tagger). As discussed in Ramshaw and Marcus (1994), the approach is resistant to overtraining effects. In Brill&apos;s experiments (Brill, 1995), training on 60</context>
<context position="7609" citStr="Ramshaw and Marcus (1994)" startWordPosition="1237" endWordPosition="1241">sed by some statistical approaches and &apos;voting&apos; systems that combine multiple taggers (Ratnaparkhi, 1996; van Halteren et al., 1998). Problems of Brill&apos;s approach include, firstly, the speed of tagging, which was found to be significantly slower than HMM-based competitors. However, Roche and Schabes (1995) show how to compile a rule-based tagger to a finite-state transducer, giving very fast execution. Secondly, there is the cost in time of training, which for larger-sized training sets (e.g. 600K tokens) will take something around a day or more to complete (using Brill&apos;s own implementation). Ramshaw and Marcus (1994) propose an faster &apos;incremental&apos; training algorithm (which avoids rescanning the corpus for each rule that is learned by using lists of pointers to link rules to the sites where they apply), but note that its memory requirements are so high as to limit its applicability. Samuel (1998) proposes a &apos;lazy&apos; version of transformationbased learning, a Monte Carlo variant of the standard method, and applies it to dialogue act tagging. The ii-TBL system of Lager (1999) is an efficient Prolog implementation of transformation-based learning, which trains in shorter time than Brill&apos;s own implementation (b</context>
<context position="9166" citStr="Ramshaw and Marcus (1994)" startWordPosition="1496" endWordPosition="1500">action is important to the performance of Brill&apos;s method on POS tagging. Consider the fact that, for PTB Wall Street Journal text, baseline performance under the closed vocabulary assumption, from assigning the mostprobable tag to each word, is around 94.5%. A final tagging accuracy of around 97% indicates an improvement of around 1 in 40 tags being corrected. Given this &apos;spareness&apos; of corrections, we might suspect that the frequency of one correction being appropriately close to enable another will be quite low. The only hard results of which we are aware that bear on this issue are given by Ramshaw and Marcus (1994), who report that rule learning on a 50K token sample of the Brown Corpus involved changes at 3395 sites, of which for only 395 sites did the change depend on the action of more than one rule. These 395 sites account for about 0.8% of the training set, but note that this does translate to 0.8% of the resulting tagging accuracy since applying a rule can have either positive, negative or neutral consequences. In the remainder of the paper, we will develop some rule-based tagging methods for which rule interaction is explicitly excluded. The performance of these models provides empirical evidence</context>
<context position="15248" citStr="Ramshaw and Marcus (1994)" startWordPosition="2527" endWordPosition="2530">can be ignored, since commitment prevents this tag from being further modified by rules learned in the current phase. Each phase of training can restrict its attention to only a limited portion of the training corpus, e.g. learning rules to modify tag t, we need only address relevant &apos;data points&apos;, which are those positions having initial tag t and at least one alternative lexical tag. The TRs that can apply at these positions are a fraction of those that could apply anywhere in the corpus. The approach allows for an &apos;incremental&apos; training algorithm which does not suffer the space problems of Ramshaw and Marcus (1994). Initially, we must score for all transformations that apply at the data points of the phase. On identifying and adopting the best rule T, we do not need to recompute all rule scores, but can instead scan the data points to find those at which T fires, and update our existing record of rule scores in regard to the possible TRs that can fire at these locations. We can then immediately determine the next rule to adopt, and so on. An informal sketch of the training algorithm is given i n This makes no provision for unknown words, since we make the &apos;closed vocabulary assumption&apos; in our experLoad </context>
</contexts>
<marker>Ramshaw, Marcus, 1994</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1994. Exploring the statistical derivation of transformation rule sequences for part-of-speech tagging. In Proceedings of the 32nd Annual Meeting of the ACL, pages 86-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="1324" citStr="Ratnaparkhi, 1996" startWordPosition="202" endWordPosition="203">omparable to, or better than the Brill method. 1 Introduction Part-of-speech (POS) tagging is the task of assigning to each word in a sentence a tag indicating its lexical syntactic category, such as noun or verb. POS tagging of text is required for subsequent processes in many systems, e.g. syntactic parsing. A number of alternative models and methods for tagging have been explored, most particularly with a view to improving tagging accuracy, including: hidden Markov models (Church, 1988; Charniak et al., 1993; Cutting et al., 1992), rule-based methods (Brill, 1995), maximum entropy methods (Ratnaparkhi, 1996), memory-based methods (Daelemans et al., 1996), amongst others. This paper addresses the rule-based POS tagging approach of Brill (1993; 1995), which °I am indebted to Rob Gaizauskas for discussions which helped greatly in the development of the ideas in this paper. learns language models that consist of a sequence of transformation rules (TRs) which capture contextual factors in predicting correct assignments. The approach allows certain interactions between rules uses, whereby a change effected by one may affect whether or not another may subsequently fire. Such interactions have been assum</context>
<context position="7088" citStr="Ratnaparkhi, 1996" startWordPosition="1157" endWordPosition="1159">ies of a HMM tagger). As discussed in Ramshaw and Marcus (1994), the approach is resistant to overtraining effects. In Brill&apos;s experiments (Brill, 1995), training on 600K tokens of the PTB tagged Wall Street Journal corpus under the &apos;closed vocabulary assumption&apos; (where there are no unknown words) gave tagging accuracy of 97.2%. Without this assumption, where performance also depends on the handling unknown words, the score was 96.6%. These results were state-of-theart at the time, but have since been surpassed by some statistical approaches and &apos;voting&apos; systems that combine multiple taggers (Ratnaparkhi, 1996; van Halteren et al., 1998). Problems of Brill&apos;s approach include, firstly, the speed of tagging, which was found to be significantly slower than HMM-based competitors. However, Roche and Schabes (1995) show how to compile a rule-based tagger to a finite-state transducer, giving very fast execution. Secondly, there is the cost in time of training, which for larger-sized training sets (e.g. 600K tokens) will take something around a day or more to complete (using Brill&apos;s own implementation). Ramshaw and Marcus (1994) propose an faster &apos;incremental&apos; training algorithm (which avoids rescanning th</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald L Rivest</author>
</authors>
<title>Learning decision lists.</title>
<date>1987</date>
<booktitle>Machine Learning,</booktitle>
<pages>2--3</pages>
<contexts>
<context position="2487" citStr="Rivest, 1987" startWordPosition="388" endWordPosition="389">bsequently fire. Such interactions have been assumed important for the approach in regard to its success on the POS tagging task. In this paper, we explore the possibility that such interactions are not empirically important for Brill&apos;s approach, at least as applied to POS tagging. To this end, we introduce two assumptions, called independence and commitment, which serve to exclude rule interactions, and develop two variants of Brill&apos;s approach that realise these assumptions. These methods turn out to be instances of decision list models, a standard approach within the machine learning field (Rivest, 1987). These models allow for training and execution algorithms that give much improved performance in terms of speed, with an impact on tagging accuracy that ranges from slight degradation to small improvement. These results serve to shed light on the importance of rule interaction to the performance of Brill&apos;s original model. 2 Brill&apos;s Tagging Approach Brill&apos;s supervised learning model &amp;quot;transformation-based error-driven learning&amp;quot; works as follows. The training data is correctly annotated text. The corresponding raw text is fed through an initial-state annotator, which makes a more or less well-in</context>
<context position="11343" citStr="Rivest, 1987" startWordPosition="1870" endWordPosition="1871">o tag each token in turn. Either option allows the possibility that earlier tag replacement steps might affect later rule firings. Independence, however, instructs us to ignore such interactions, in which case the two options should give essentially equivalent results. We will pursue the second option, with rules being applied as a group to each token, which involves testing each rule in turn until the list is exhausted or one rule fires, in which case the remaining rules are ignored. This &apos;one-shot&apos; style of using rules is familiar in machine learning as an instance of a decision list model (Rivest, 1987). More specifically, given the character of the rules involved, we have a propositional decision list system.&apos; A standard use of decision lists is 1-The more expressive formalism of first-order defor classification tasks, i.e. assigning categories to examples, based on their (static) characteristics. The &apos;dynamic&apos; character tagging makes it somewhat different from classification, a fact which might present difficulties for decision list learning were it not for the independence assumption. In the Brill learning approach, rules are acquired in the same order as they are applied during tagging. </context>
</contexts>
<marker>Rivest, 1987</marker>
<rawString>Ronald L. Rivest. 1987. Learning decision lists. Machine Learning, 2(3):229-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Roche</author>
<author>Yves Schabes</author>
</authors>
<title>Deterministic part-of-speech tagging with finitestate transducers.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--2</pages>
<contexts>
<context position="7291" citStr="Roche and Schabes (1995)" startWordPosition="1186" endWordPosition="1189">Wall Street Journal corpus under the &apos;closed vocabulary assumption&apos; (where there are no unknown words) gave tagging accuracy of 97.2%. Without this assumption, where performance also depends on the handling unknown words, the score was 96.6%. These results were state-of-theart at the time, but have since been surpassed by some statistical approaches and &apos;voting&apos; systems that combine multiple taggers (Ratnaparkhi, 1996; van Halteren et al., 1998). Problems of Brill&apos;s approach include, firstly, the speed of tagging, which was found to be significantly slower than HMM-based competitors. However, Roche and Schabes (1995) show how to compile a rule-based tagger to a finite-state transducer, giving very fast execution. Secondly, there is the cost in time of training, which for larger-sized training sets (e.g. 600K tokens) will take something around a day or more to complete (using Brill&apos;s own implementation). Ramshaw and Marcus (1994) propose an faster &apos;incremental&apos; training algorithm (which avoids rescanning the corpus for each rule that is learned by using lists of pointers to link rules to the sites where they apply), but note that its memory requirements are so high as to limit its applicability. Samuel (19</context>
<context position="27110" citStr="Roche and Schabes (1995)" startWordPosition="4625" endWordPosition="4628">nce for this in the results (e.g. compare ICP/4 to ICA/3). 8 Discussion We have shown how Brill&apos;s rule-based tagging approach can be reformulated as a decision list model, allowing for much faster training, to give results that are comparable to, or better than, those of Brill&apos;s original system. The modified approach allows for tagger execution that is quite fast, even for an implementation performing direct rule interpretation. This implementation, however, probably cannot compare with the speed of a tagger for the standard Brill approach produced using the finite-state compilation method of Roche and Schabes (1995), being run on current hardware. We expect that similar finitestate compilation is possible for the decision list approach, and speculate that, since longdistance interaction effects are excluded, it should be possible to do a compilation producing much smaller finite-state transducers. A final point to consider is whether examples for which the Brill approach relies on interaction can be correctly handled in the modified approach. We cannot give a general answer to this question, but the following example gives some indication of what may be happening in many cases. The Brill tagger assigns t</context>
</contexts>
<marker>Roche, Schabes, 1995</marker>
<rawString>Emmanuel Roche and Yves Schabes. 1995. Deterministic part-of-speech tagging with finitestate transducers. Computational Linguistics, 21(2):227-253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Samuel</author>
</authors>
<title>Lazy transformation-based learning.</title>
<date>1998</date>
<booktitle>In Proceedings of the Eleventh International Fonda Al Research Symposium Conference,</booktitle>
<pages>235--239</pages>
<contexts>
<context position="7894" citStr="Samuel (1998)" startWordPosition="1288" endWordPosition="1289">bes (1995) show how to compile a rule-based tagger to a finite-state transducer, giving very fast execution. Secondly, there is the cost in time of training, which for larger-sized training sets (e.g. 600K tokens) will take something around a day or more to complete (using Brill&apos;s own implementation). Ramshaw and Marcus (1994) propose an faster &apos;incremental&apos; training algorithm (which avoids rescanning the corpus for each rule that is learned by using lists of pointers to link rules to the sites where they apply), but note that its memory requirements are so high as to limit its applicability. Samuel (1998) proposes a &apos;lazy&apos; version of transformationbased learning, a Monte Carlo variant of the standard method, and applies it to dialogue act tagging. The ii-TBL system of Lager (1999) is an efficient Prolog implementation of transformation-based learning, which trains in shorter time than Brill&apos;s own implementation (by an order of magnitude, for the tasks reported). ii-TBL also implements &apos;lazy&apos; learning, and Lager&apos;s results indicate comparable tagging accuracy for the lazy and standard methods as applied to POS tagging. 3 Is Rule Interaction Important? The starting point for the work in this pape</context>
</contexts>
<marker>Samuel, 1998</marker>
<rawString>Ken Samuel. 1998. Lazy transformation-based learning. In Proceedings of the Eleventh International Fonda Al Research Symposium Conference, pages 235-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
<author>Jakub Zavrel</author>
<author>Walter Daelemens</author>
</authors>
<title>Improving data-driven wordclass tagging by system combination.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics COLING-98,</booktitle>
<pages>491--497</pages>
<marker>van Halteren, Zavrel, Daelemens, 1998</marker>
<rawString>Hans van Halteren, Jakub Zavrel, and Walter Daelemens. 1998. Improving data-driven wordclass tagging by system combination. In Proceedings of the International Conference on Computational Linguistics COLING-98, pages 491-497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoff Webb</author>
<author>N Brkic</author>
</authors>
<title>Learning decision lists by prepending inferred rules.</title>
<date>1993</date>
<booktitle>In Proceedings of the AI-93 Workshop on Machine Learning and Hybrid Systems,</booktitle>
<pages>6--10</pages>
<location>Melbourne.</location>
<contexts>
<context position="12273" citStr="Webb and Brkic (1993)" startWordPosition="2021" endWordPosition="2024">namic&apos; character tagging makes it somewhat different from classification, a fact which might present difficulties for decision list learning were it not for the independence assumption. In the Brill learning approach, rules are acquired in the same order as they are applied during tagging. Minimally adapting such an approach to a decision list use of TRs gives an instance of a sequential covering algorithm, i.e. the first rule learned &apos;covers&apos; some part of the training data (that now &apos;belongs&apos; to that rule), and then the next rule learned covers some part of what remains, and so on. Following Webb and Brkic (1993), we refer to this way of ordering rules as appending, i.e. new rules are appended to the end of the current rule list. Commonly in sequential rule learning, rules learned earlier are highly general, applying to many instances, whereas rules learned later are more specific, applying to fewer instances. The general rules learned earlier have a large net positive effect, but may get specific subsets of cases wrong. The Brill approach allows later rules to be learned to correct errors made by earlier rules, via sequential rule application. For a decision list treatment of tagging, this possibilit</context>
</contexts>
<marker>Webb, Brkic, 1993</marker>
<rawString>Geoff Webb and N. Brkic. 1993. Learning decision lists by prepending inferred rules. In Proceedings of the AI-93 Workshop on Machine Learning and Hybrid Systems, pages 6-10, Melbourne.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>