<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026824">
<title confidence="0.994865">
Subword Variation in Text Message Classification
</title>
<author confidence="0.999231">
Robert Munro
</author>
<affiliation confidence="0.9845085">
Department of Linguistics
Stanford University
</affiliation>
<address confidence="0.930729">
Stanford, CA 94305
</address>
<email confidence="0.999243">
rmunro@stanford.edu
</email>
<author confidence="0.995769">
Christopher D. Manning
</author>
<affiliation confidence="0.984853">
Department of Computer Science
Stanford University
</affiliation>
<address confidence="0.93096">
Stanford, CA 94305
</address>
<email confidence="0.999563">
manning@stanford.edu
</email>
<sectionHeader confidence="0.995669" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999666833333333">
For millions of people in less resourced re-
gions of the world, text messages (SMS) pro-
vide the only regular contact with their doc-
tor. Classifying messages by medical labels
supports rapid responses to emergencies, the
early identification of epidemics and everyday
administration, but challenges include text-
brevity, rich morphology, phonological vari-
ation, and limited training data. We present
a novel system that addresses these, working
with a clinic in rural Malawi and texts in the
Chichewa language. We show that model-
ing morphological and phonological variation
leads to a substantial average gain of F=0.206
and an error reduction of up to 63.8% for spe-
cific labels, relative to a baseline system opti-
mized over word-sequences. By comparison,
there is no significant gain when applying the
same system to the English translations of the
same texts/labels, emphasizing the need for
subword modeling in many languages. Lan-
guage independent morphological models per-
form as accurately as language specific mod-
els, indicating a broad deployment potential.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999988642857143">
The whole world is texting, but rarely in English.
Africa has seen the greatest recent uptake of cell-
phones, with an 8-fold increase over the last 5 years
and saturation possible in another 5 (Buys et al.,
2009). This is a leapfrog technology – for the ma-
jority of new users cellphones are the only form of
remote communication, surpassing landlines, (non-
mobile) internet access and even grid electricity,
with costs making texts the dominant communica-
tion method. This has led social development orga-
nizations to leverage mobile technologies to support
health (Leach-Lemens, 2009), banking (Peevers et
al., 2008), access to market information (Jagun et al.,
2008), literacy (Isbrandt, 2009) and emergency re-
sponse (Munro, 2010). The possibility to automate
many of these services through text-classification is
huge, as are the potential benefits – those with the
least resources have the most to gain.
However, the data presents many challenges, as
text messages are brief, most languages have rich
morphology, spellings may be overly-phonetic, and
there is often limited training data. We partnered
with a medical clinic in rural Malawi and Front-
lineSMS:Medic, whose text message management
systems serve a patient population of over 2 million
in less developed regions of the world. The system
allows remote community health workers (CHWs)
to communicate directly with more qualified medi-
cal staff at centralized clinics, many for the first time.
We present a short-message classification sys-
tem that incorporates morphological and phono-
logical/orthographic variation, with substantial im-
provements over a system optimized on word-
sequences alone. The average gain is F=0.206 with
an error reduction of up to 63.8% for specific labels.
For 6 of the 9 labels this more than doubles the accu-
racy. By comparison, there is not a significant gain
in accuracy when applying the same system to the
English translations of the same texts/labels, empha-
sizing the need for modeling subword structures, but
also highlighting why morphology has been periph-
eral in text classification until now.
</bodyText>
<page confidence="0.943211">
510
</page>
<note confidence="0.793016">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 510–518,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.635019" genericHeader="introduction">
2 Language and data
</sectionHeader>
<bodyText confidence="0.996540722222222">
Chichewa is a Bantu language with about 13 mil-
lion speakers in Southern Africa including 65%
of Malawians. We limit examples to the nouns:
odwala ‘patient’, mankhwala ‘medicine’; verb: fun
‘want’; and the 1st person pronoun/marker: ndi-
‘I’. Chichewa is closely related to many neighbor-
ing languages – more than 100 million people could
recognize ndifuna as ‘I want’.
The morphological complexity is average with
about 2-3 morpheme boundaries per word, but this is
rich and complex compared to estimates for English,
Spanish and Chinese with average of 0.33, 0.85 and
0.01 morpheme boundaries per word. A typical verb
is ndimakafunabe, ‘I am still wanting’, consisting
of six morphemes, ndi-ma-ka-fun-a-be, expressing:
1st person Subject; present tense; noun-class (gen-
der) agreement with the Object; ‘want’; verb part-
of-speech; and incompletive aspect.
</bodyText>
<subsectionHeader confidence="0.983415">
2.1 Labels
</subsectionHeader>
<bodyText confidence="0.863187666666667">
The text messages are coded for 0-9 labels in 3
groupings (with counts):
Administrative: related to the clinic:
</bodyText>
<listItem confidence="0.999907545454545">
1. Patient-related (394)
2. Clinic-admin: meetings, supplies etc (169)
3. Technological: phone-credit, batteries etc (21)
Requests: from Community Health Workers:
4. Response: any action requested by CHW (124)
5. Request for doctor (62)
6. Medical advice: CHW asking for advice (23)
Illness: changes of interest to monitoring bodies:
7. TB: tuberculosis (44)
8. HIV: HIV, AIDS and/or treatments (45)
9. Death: reported death of a patient (30)
</listItem>
<bodyText confidence="0.999819681818182">
The groupings correspond to the three main stake-
holders of the messages: the clinic itself, interested
in classifying messages according to internal work-
practices; the Community Health Workers and their
patients, acting as the direct care-givers outside the
clinic; and broader bodies like the World Health Or-
ganization who are interested in monitoring diseases
and early identification of epidemics (biosurveil-
lance). The labels are the three most frequent labels
required by each of these user groups.
We analyzed 4 months of texts messages with ap-
proximately 1,500 labels from 600 messages, con-
sisting of 8,000 words and 30,000 morphemes.
While this is small, the final system is being piloted
at a clinic in rural Malawi, where users can define
new labels at any time according to changing work-
practices, new diseases etc. If more than 4 months
of manually labeling were required it could limit the
utility and user acceptance.
All the messages were translated into English by
a medical practitioner, allowing us to make cross-
linguistic comparisons of our system.
</bodyText>
<subsectionHeader confidence="0.998223">
2.2 Variation
</subsectionHeader>
<bodyText confidence="0.999906384615385">
The variation in the data is large. There are &gt;40
forms for ‘patient’ and only 32% are odwala. Of the
rest, &gt;50% occur only once. The variation results
from morphology: ndi-odwala; phonology: odwara,
ndiwodwala, and compounding: ndatindidziwewod-
wala. There are also &gt;10 spellings for the English
borrowing: patient, pachenti etc, and 3 for the syn-
onym matenda.
Similarly, there are &gt;20 forms for ‘medicine’.
For fun ‘want’, there are &gt;30 forms with &gt;80% oc-
curing only once. There are &gt;200 forms containing
ndi and no one form accounts for more than 5% of
the instances.
The co-occurrence of ndi and fun within a word is
a strong non-redundant predictor for several labels,
but &gt;75% of forms occur only once and &gt;85% of
the forms are non-contiguous, as above and in the
most frequent ndi-ma-funa ‘I currently want’.
By contrast, in the English translations ‘needing’
occurs just once but all other forms of ‘patient’,
‘medicine’ and ‘(I) want/need’ are frequent.
This brief introduction to the language and data
should make it clear that specialized methods are re-
quired for modeling variation in text messages, es-
pecially in many languages where text messaging is
the dominant form of digital communication.
</bodyText>
<sectionHeader confidence="0.990201" genericHeader="method">
3 Morphological models
</sectionHeader>
<bodyText confidence="0.998976666666667">
We compared language specific and language inde-
pendent morphological models, comparing 3 meth-
ods (with ndimafuna as an example):
</bodyText>
<footnote confidence="0.841521">
Stemmed: {ndi, fun}
Segmented: {ndi, ma, fun, a}
Morph-config: {ndi-ma, ndi-fun, ndi-a, ma-fun...}
</footnote>
<page confidence="0.99707">
511
</page>
<bodyText confidence="0.99997275">
We also looked at character ngrams, as used by Hi-
dalgo et al. (2006) for morphological variation in
English and Spanish. The results converged with
those of the segmented model, which is not surpris-
ing as the most frequent features would be simi-
lar and increasing data items would overcome the
sparcity. We leave more sophisticated character
ngram modeling for future work.
</bodyText>
<subsectionHeader confidence="0.999811">
3.1 Language specific
</subsectionHeader>
<bodyText confidence="0.999905916666667">
For the language specific morphological models
we implemented a morphological parser as a set
of context-free grammars for all possible prefixes
and suffixes according to the formal definitions of
Chichewa morphology in Mchombo (2004).
We identified stems by parsing potential prefixes
and suffixes, segmenting a word w into n mor-
phemes wm,0, . . . , wm,n−1 leaving a stem ws with
length len(ws) and corpus frequency of f(ws), such
that len(ws) &gt; 0 (ie, there must be a stem). Where
multiple parses could be applied, we minimized
len(ws), then maximized n.
</bodyText>
<subsectionHeader confidence="0.999296">
3.2 Language independent
</subsectionHeader>
<bodyText confidence="0.999995666666667">
For the language independent morphological mod-
els we adapted the word-segmenter of Goldwa-
ter, Griffiths and Johnson (2009), to morphological
parsing (see Related Work for other algorithms we
tested/considered). It was suited to our task because
a) it is largely nonparametric, meaning that it can
be deployed as a black-box before language-specific
properties are known b) it favored recall over preci-
sion (see the Results for discussion) and c) using a
segmentation algorithm, rather than explicitly mod-
eling morphology, also addresses compounds.
This model uses a Hierarchical Dirichlet Process
(HDP) (Teh et al., 2005). Every morpheme in the
corpus mi is drawn from a distribution G which con-
sists of possible morphemes (the affixes and stems)
and probabilities associated with each morpheme. G
is generated from a Dirichlet Process (DP) distri-
bution DP(a0, P0), with morphemes sampled from
P0 and their probabilities determined by a concen-
tration parameter a0. The context-sensitive model
where Hm is the DP for a specific morpheme is:
</bodyText>
<equation confidence="0.978272">
mi|mi−1 = m, Hm—Hm bm
Hm|a1, G —DP(a1, G) bm
G|a0, P —DP(a0, P0)
</equation>
<bodyText confidence="0.993694264705883">
Note that this part of our model is identical to the
bigram HDP in Goldwater et al. (2009), except that
we possess a set of morphemes, not words. Because
word boundaries are already marked in the major-
ity of the messages, we constrain the model to treat
all existing word boundaries in the corpus as mor-
pheme boundaries, thus constraining the model to
morpheme and compound segmentation.
Unlike word-segmentation, not all tokens in the
morpheme lexicon are equal, as we want to model
stems separately from affixes in the stemmed mod-
els. We assume a) the free morphemes (stems and
through compounding) are the least frequent and
therefore have the lowest final probability, P(m), in
the HDP model; and b) each word w must have at
least one free morpheme, the stem ws (ws =� 0).1
The token-optimal process for identifying
stems is straightforward and efficient. The
words are sorted by the argmin probabilities
of P(wm,0), . . . , P(wm,n−1). For each word
w, unless ws can be identified by a previously
observed free morpheme, ws is identified as
argmin(P(wm,0), . . . , P(wm,n−1)) and ws is
added to our lexicon of free morphemes. This algo-
rithm iterates over the words with one extra pass to
mark all free morphemes in each word (assuming
that there might be compounds we missed on the
first pass). The cost, where M is the total number
of morphemes and W the total number of words, is
O(log(W) + M).
This process has the potential to miss free mor-
phemes that only happened to occur in compounds
with less-probable stems, but this did not occur in
our data.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="method">
4 Phonological/Orthographic Models
</sectionHeader>
<bodyText confidence="0.998373285714286">
We compared three models of phonologi-
cal/orthographic variation:
Chichewa: Chichewa specific
Script: Roman script specific
Indep: language independent
We refer to these using the term ‘phonology’ very
broadly. The majority of the variation stems from
</bodyText>
<footnote confidence="0.86224725">
1Note that identifying stems must be a separate step – if we
allowed multiple free morphemes for each word to enter the
lexicon without penalty in the HDP model it would converge on
a zero-penalty distribution where all morphemes were free.
</footnote>
<page confidence="0.993706">
512
</page>
<bodyText confidence="0.9999625">
the phonology, but also from phonetic variation as
expressed in a given writing system, and variation in
the writing system itself arising from fluent speakers
with varying literacy.
</bodyText>
<subsectionHeader confidence="0.998389">
4.1 Chichewa specific
</subsectionHeader>
<bodyText confidence="0.999835210526316">
For the language specific normalization, we applied
a set of heuristics to the data, based on the varia-
tion given in (Paas, 2005) and our own knowledge
of how Bantu languages are expressed in Roman
scripts. The heuristics were used to normalize all
alternates, eg: Jiwo —* iOo} and Jr —* l}, resulting
in ndiwodwara —* ndiodwala.
The heuristics represented forms for phonemes
with the same potential place of articulation (‘c/k’),
forms with an adjacent place-of-articulation that are
common phonological alternates (‘l/r’, ‘e,i’), voic-
ing alternations (‘s/z’), or language-internal phono-
logical processes like the insertion of a glide be-
tween vowels that the morphology has made adja-
cent (like we pronounce but don’t spell in ‘go(w)ing’
in English).
We also implemented hard-coded acronym-
recovery methods for acronyms associated with the
‘Illness’ labels: ‘HIV’, ‘TB’, ‘AIDS’, ‘ARV’.
</bodyText>
<subsectionHeader confidence="0.993763">
4.2 Script specific
</subsectionHeader>
<bodyText confidence="0.99996965">
The script specific techniques used the same sets of
alternates in the language specific model, but nor-
malized such that the heuristic H was applied to
a word w in the corpus C resulting in an alternate
w&apos;, iff w&apos; E C. This method limits the alternates
to those whose existence is supported by the data.
It is therefore more conservative than the previous
method.
For more general acronym identification, we
adapted the method of Schwartz &amp; Hearst (2003).
We created a set of candidate acronyms by iden-
tifying capitalized sequences in non-capitalized
contexts and period-delimited single character se-
quences. All case-insensitive sequences that were
segmented by consistent non-alphabetic characters
were then identified as acronyms, provided that they
ended in a non-alphabetic character. We could not
define a similar acronym-start boundary, as pre-
fixes were often added to acronyms, even when the
acronyms themselves contained spaces, eg: ‘aT. B.’.
</bodyText>
<subsectionHeader confidence="0.99437">
4.3 Language independent
</subsectionHeader>
<bodyText confidence="0.999985764705882">
For complete language independence we applied a
noise-reduction algorithm to the stream of charac-
ters in order to learn the heuristics that represented
potential phonological alternates by identifying all
minimal pairs of characters sequences (sequences
that alternated by one character, include the absence
of a character).
Given all sequences of characters, we identified
all pairs of sequences of length &gt; l that differed
by one character ci, where ci could be null. We
then ranked the pairs of alternating sequences by de-
scending length and applied a threshold t, selecting
the t longest sequences, creating alternating patterns
from all pairs. Regardless of l or t, the resulting
heuristics did not resemble those in 4.1 or 4.2.
We did not implement any acronym identification
methods, for obvious reasons.
</bodyText>
<sectionHeader confidence="0.999939" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999651666666667">
The results are compared to a baseline system op-
timized over word sequences (words and ngrams
but no subword modeling). All results presented
here are from a MaxEnt model using a leave-one-
out cross-validation.
For the English translations of the texts there was
no phonological/orthographic variation beyond that
resulting from morphology, so we only applied the
language independent morphological models.
</bodyText>
<subsectionHeader confidence="0.920454">
5.1 Morphology
</subsectionHeader>
<bodyText confidence="0.999762625">
With the exception of the unsupervised stemming,
all the morphological models led to substantial gains
in accuracy. As Table 1 shows, the most accu-
rate system used the language specific segmenta-
tion, with an average accuracy of F=0.476, a macro-
average gain of 22.4%.
The greatest increase in accuracy occured where
verbs were the best predictors – the words with the
most complex morphology. The ‘Response’ label
showed the greatest relative gain in accuracy for
those with a non-zero baseline, where the accuracy
increased 4-fold from F=0.113 to F=0.442. It is ex-
pected that a label predicated on requests for action
should rely on the isolation of verb stems, but this
is still a very substantial gain. In contrast to this
391.2% gain in accuracy for Chichewa, the gain for
</bodyText>
<page confidence="0.988998">
513
</page>
<table confidence="0.9999725">
Baseline Stemmed Segmented Morph-Config Gain
Label Chich Indep Chich Indep Chich Indep Best Final
Patient-related 0.830 0.842 0.735 0.857 0.832 0.851 0.867 +3.7 +3.7
Clinic-admin 0.358 0.490 0.295 0.612 0.561 0.577 0.580 +25.5 +22.2
Technological 0 0 0 0.320 0.174 0.320 0.091 +32.0 +09.1
Response 0.113 0.397 0.115 0.440 0.477 0.459 0.442 +36.4 +32.9
Request for doctor 0.121 0.312 0.090 0.505 0.395 0.477 0.375 +38.4 +25.4
Medical advice 0 0 0 0.083 0.160 0.083 0.083 +16.0 +08.3
HIV 0.379 0.597 0 0.554 0.357 0.484 0.351 +21.8 (-2.8)
TB 0.235 0.357 0 0.414 0.200 0.386 0.327 +17.8 +09.2
Death 0.235 0.333 0.229 0.500 0.667 0.462 0.723 +48.8 +48.8
Average. 0.252 0.370 0.163 0.476 0.425 0.455 0.427 +22.4 +17.4
</table>
<tableCaption confidence="0.991227333333333">
Table 1: Morphology results: F-values for leave-one-out cross-validation comparing different morphological models.
Indep = language independent, Chich = specific to Chichewa, ( ) = not significant (p &gt; 0.05, x2), Final = Gain of the
‘Morph-Config, Indep’ model over the Baseline.
</tableCaption>
<bodyText confidence="0.999982217391304">
English, while still relying on the isolation of verb
stems, only increased the accuracy by 5.4%.
The unsupervised stemming underperformed the
baseline model by 8.9%, due to over-segmentation.
Compared to the Chichewa stemmer, we estimate
that the unsupervised stemmer had 90-95% recall
and 40-50% precision, resulting in over-stemmed to-
kens. However, this seemed to be favor the seg-
mented and morph-config models, as unnecessary
segmentation can be recovered when the tokens
are sequenced or re-configured, with the supervised
model arriving at the optimal weights for each can-
didate token or sequence. This can be seen by com-
paring the stemmed and morph-config results for
the Chichewa-specific and language independent re-
sults. The difference in stemming is 20.7% but for
the morph-config models it is only 2.8%. A loss in
segmentation recall could not be recovered in the
same way, as adjacent non-segmented morphemes
will remain one token. This leads us to conclude that
recall should be weighted more highly than preci-
sion in unsupervised morphological models applied
to supervised classification tasks.
</bodyText>
<subsectionHeader confidence="0.984038">
5.2 Phonology
</subsectionHeader>
<bodyText confidence="0.999983393939394">
For the phonological models the results in Table 2
show that the script-specific model was the most ac-
curate with an average of F=0.443, a gain of 19.1%
over the baseline.
There are correlations between morphological
variation and phonological variation, with the gains
similar for each label in Table 1 and Table 2. This
is because much phonological variation often arises
from the morphology, as in ndiwodwala where the
glide w is pronounced and variably written be-
tween the vowels made adjacent through morphol-
ogy. It is also because more morphologically com-
plex words are longer and simply have more poten-
tial for phonological and written variation. The were
greater gains in identifying the ‘TB’ and ‘HIV’ la-
bels here than in the morphological models as the
result of acronym identification.
The language independent model did not perform
well. Despite changing the data considerably, there
was little change in the accuracy, indicating that the
changes it made were largely random with respect
to the target concepts. The most frequent alterna-
tions in large contexts were noun-class prefixes dif-
fering by a single character, which has the potential
to change the meaning, and this seemed to negate
any gains from normalization.
While language independent results would have
been ideal, a system with script-specific assump-
tions is realistic. It is likely that text messages are
regularly sent in 1000s of languages but less than
10 scripts, and our definition of ‘script specific’
would be considered ‘language independent’ else-
where. For example, in the Morpho Challenge (see
</bodyText>
<page confidence="0.995749">
514
</page>
<table confidence="0.999813666666667">
Baseline Model Gain
Label Chichewa Script Indep Best Final
Patient-related 0.830 0.842 0.848 0.838 (+1.8) (+1.8)
Clinic-admin 0.358 0.511 0.594 0.358 +23.6 +23.6
Technological 0 0.091 0.091 0 +9.1 +9.1
Response 0.113 0.420 0.473 0.207 +36.0 +36.0
Request for doctor 0.121 0.154 0.354 0 +23.3 +23.3
Medical advice 0 0.375 0.222 0.121 +37.5 +22.2
HIV 0.379 0.508 0.492 0.379 +12.9 +11.3
TB 0.235 0.327 0.492 0.235 +25.7 +25.7
Death 0.235 0.333 0.421 0.235 +18.6 +18.6
Average 0.252 0.396 0.443 0.264 +19.1 +19.1
</table>
<tableCaption confidence="0.985511666666667">
Table 2: Phonological results: F-values for leave-one-out cross-validation comparing different phonological models.
Chichewa = Chichewa specific heuristics, Script = specific to Roman scripts, Indep = language independent, ( ) = not
significant (p &gt; 0.05, x2), Final = Gain of the ‘Script’ model over the Baseline.
</tableCaption>
<bodyText confidence="0.941052666666667">
Related Work) Arabic data was converted to Ro-
man script, and it is likely that the methods could be
adapted with some success to any alphabetic script.
</bodyText>
<subsectionHeader confidence="0.995972">
5.3 Combined results
</subsectionHeader>
<bodyText confidence="0.9999735">
Table 3 gives the final results, comparing the sys-
tems over the original text messages and the English
translations of the same messages. The most accu-
rate results were achieved by applying the phono-
logical normalization before the morphological seg-
mentation, giving a (macro) average of 0.459 which
is an increase of 20.6% over the baseline. The
increase in accuracy was not cumulative – the
combined system outperforms both the standalone
phonological and morphological systems, but with a
comparatively modest gain.
The final English system is 9.2% more accurate
than the final Chichewa system, but the Chichewa
system has closed the gap considerably as the En-
glish baseline system was 25.7% more accurate than
the baseline Chichewa system. Assuming that the
potential accuracy is approximately equal (given
both languages are encoding exactly the same infor-
mation) we conclude that we have made substantial
gains in accuracy but there are further large gains to
be made. Therefore, while we have not solved the
problem of text message classification in morpho-
logically rich languages, we have been able to make
promising gains in an exciting new area of research.
</bodyText>
<subsectionHeader confidence="0.976716">
5.4 Practical effectiveness
</subsectionHeader>
<bodyText confidence="0.992599269230769">
The FrontlineSMS system currently allows users to
filter messages by keywords, similar to many email
clients. Because of the large number of variants per
word this is sub-optimal in many languages. We de-
fined a second baseline to model an idealized version
of the current system that assumes oracle knowledge
of the keyword/label and the optimal order in which
to apply rules created from this knowledge. The only
constraint was that we excluded words that occurred
only once. In essence, it is a MaxEnt model that in-
cludes seen test items and assigns a label according
to the single strongest feature for each test item.
Here, we evaluated the systems according to
Micro-F, recall and precision, as these give a bet-
ter gauge of the frequency of error per incoming
text, and therefore the usability for someone need-
ing to correct mislabeled texts. We also calculated
the Micro-F for each label/non-label decision to give
exact figures per classification decision. The results
are in Table 4. The Micro-F is 0.684 as compared to
0.403 for the keyword system. The higher precision
is also promising, indicating that when we assign a
label we are more often correct. By adjusting the
precision and recall through label confidence thresh-
olds, 90% precision can be achieved with 35.3% re-
call.2 In terms of usability, the Label/no-Label re-
</bodyText>
<footnote confidence="0.941638">
2We confirmed significance relative to confidence by ROC
analysis – results omitted for space.
</footnote>
<page confidence="0.988937">
515
</page>
<table confidence="0.999855538461539">
Chichewa English
Label Baseline Final Sys Gain Baseline Final Sys Gain
Patient-related 0.830 0.847 (+1.7) 0.878 0.878 0
Clinic-admin 0.358 0.624 +26.6 0.682 0.717 (+3.4)
Technological 0 0.174 +17.4 0.174 0.320 +14.6
Response 0.113 0.476 +36.3 0.573 0.555 (-1.8)
Request for doctor 0 0.160 +16.0 0.160 0.357 +19.7
Medical advice 0.121 0.500 +37.9 0.560 0.580 (+2.0)
HIV 0.379 0.357 (-2.2) 0.414 0.576 +16.2
TB 0.235 0.351 +11.6 0.557 0.533 (-2.4)
Death 0.235 0.638 +40.3 0.591 0.439 -15.2
Average 0.252 0.459 +20.6 0.510 0.551 +4.1
Micro F 0.593 0.684 +9.1 0.728 0.737 (+0.9)
</table>
<tableCaption confidence="0.99986">
Table 3: Final Results, comparing the systems in Chichewa and the English translations.
</tableCaption>
<bodyText confidence="0.998937545454545">
sults are very promising, reducing errors from 1 in 4
to 1 in 20.
The learning rates in Figure 1 show that the learn-
ers are converging on accurate models after only see-
ing a handful of text messages. This figure also
makes it clear that subword processing gives rela-
tively little gain to the English translations. The
disparity between the final model and the baseline
widens as more items are seen, indicating that the
failure of the word-optimal baseline model is not just
due to a lack of training items.
</bodyText>
<subsectionHeader confidence="0.985984">
5.5 Other models investigated
</subsectionHeader>
<bodyText confidence="0.999809809523809">
Much recent work in text classification has been in
machine-learning, comparing models over constant
features. We tested SVMs and joint learning strate-
gies. The gains were significant but small and did
not closed the gap between systems with and with-
out subword modeling. We therefore omit these for
space and scope.
However, one interesting result came from ex-
tending the feature space with topics derived from
Latent Dirichlet Allocation (LDA) using similar
methods to Ramage et al. (2009). This produced
significant gains (micro-F=0.029), halving the re-
maining gap with the English system, but only
when the topics were derived from modeling non-
contiguous morpheme sequences, not words-alone
or segmented morphemes. We found that the differ-
ent surface forms of each word cooccurred less often
than chance (0.46 as often as chance for the different
forms of odwala) forming disjunctive distributions.
We suspect that this acts as a bias against robust un-
supervised clustering of the different forms.
</bodyText>
<sectionHeader confidence="0.999882" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999340666666667">
To our best knowledge, no prior researchers have
worked on subword models for text message cate-
gorization, or any NLP task with the Chichewa, but
we build on many recent developments in computa-
tional morphology and NLP for Bantu languages.
Badenhorst et al. (2009) found substantial varia-
tion in a speech recognition corpus for 9 Southern
Bantu languages, where accurate models could also
be built with limited data. Morphological segmenta-
tion improved Swahili-English machine translation
in De Pauw et al. (2009), even in the absense of
gold standard reference segmentations, as was the
case here. The complexity and necessity of model-
ing non-contiguous morphemes in Bantu languages
is discussed by Pretorius et al. (2009).
Computational morphology (Goldsmith, 2001;
Creutz, 2006; Kurimo et al., 2008; Johnson and
Goldwater, 2009; Goldwater et al., 2009) has be-
gun to play a prominent role in machine transla-
tion and speech recognition for morphologically rich
languages (Goldwater and McClosky, 2005; Tach-
belie et al., 2009). In the current-state-of-the-art, a
combination of the ParaMor (Monson et al., 2008)
and Morfessor (Creutz, 2006) algorithms achieved
</bodyText>
<page confidence="0.992174">
516
</page>
<figure confidence="0.803704">
10
</figure>
<figureCaption confidence="0.916911666666667">
Figure 1: The learning rate, comparing micro-F for the
Chichewa and English systems on different training set
sizes. A random stratified sample was used for subsets.
</figureCaption>
<bodyText confidence="0.999725161290323">
the most accurate results in 2008 Morpho Challenge
Workshop (Kurimo et al., 2008). ParaMor assumes
a single affix and is not easily adapted to more com-
plex morphologies, but we were able to test and eval-
uate Morfessor and the earlier Linguistica (Gold-
smith, 2001). Both were more accurate for segmen-
tation than our adaptation of Goldwater et al. (2009),
but with lower recall. For the reasons discussed in
Section 5.3 this meant less accuracy in classification.
Goldwater et al. have also used the Pitman-Yor algo-
rithm for morphological modeling (Goldwater et al.,
2006). In results too recent to test here, Pitman-Yor
has been used for segmentation with accuracy com-
parable to the HDP model but with greater efficiency
(Mochihashi et al., 2009). Biosurveillance systems
currently use simple rule-based pre-processing for
subword models. Dara et al. (2008) found only mod-
est gains, although the data was limited to English.
For text message classification, prior work is lim-
ited to identifying SPAM (Healy et al., 2005; Hi-
dalgo et al., 2006; Cormack et al., 2007), where
specialized algorithms and feature representations
were also found to improve accuracy. For written
variation, Kobus et al. (2008) focussed on SMS-
specific abbreviations in French. Unlike their data,
SMS-specific abbreviations were not present in our
data. This is consistent with the reports on SMS
practices in the related isiXhosa language (Deumert
and Masinyana, 2008), but it may also be because
the data we used contained professional communi-
cations not personal messages.
</bodyText>
<table confidence="0.998477">
Label class Label/No-Label
KWF Final KWF Final
F-val 0.403 0.684 0.713 0.950
Prec. 0.265 0.796 0.570 0.972
Rec. 0.842 0.599 0.953 0.929
</table>
<tableCaption confidence="0.996321">
Table 4: Micro-F, precision and recall, compared with the
oracle keyword system. KWF = Oracle Keyword Filter.
</tableCaption>
<sectionHeader confidence="0.997243" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999970307692308">
We have demonstrated that subword modeling in
Chichewa leads to significant gains in classifying
text messages according to medical labels, reducing
the error from 1 in 4 to 1 in 20 in a system that should
generalize to other languages with similar morpho-
logical complexity.
The rapid expansion of cellphone technologies
has meant that digital data is now being generated
in 100s, if not 1000s, of languages that have not
previously been the focus of language technologies.
The results here therefore represent just one of a
large number of potential new applications for short-
message classification systems.
</bodyText>
<sectionHeader confidence="0.994729" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999352">
Thank you to FrontlineSMS:Medic and the health
care workers they partner with. The first author was
supported by a Stanford Graduate Fellowship.
</bodyText>
<sectionHeader confidence="0.998505" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981573">
Jaco Badenhorst, Charl van Heerden, Marelie Davel, and
Etienne Barnard. 2009. Collecting and evaluating
speech recognition corpora for nine Southern Bantu
languages. In The EACL Workshop on Language Tech-
nologies for African Languages.
Piet Buys, Susmita Dasgupta, Timothy S. Thomas, and
David Wheeler. 2009. Determinants of a digital divide
in Sub-Saharan Africa: A spatial econometric analysis
of cell phone coverage. World Development, 37(9).
Gordon V. Cormack, Jos´e Mara G´omez Hidalgo, and En-
rique Puertas S´anz. 2007. Feature engineering for
mobile (SMS) spam filtering. In The 30th annual in-
ternational ACM SIGIR conference on research and
development in information retrieval.
Mathias Creutz. 2006. Induction of the Morphology of
Natural Language: Unsupervised Morpheme Segmen-
tation with Application to Automatic Speech Recogni-
tion. Ph.D. thesis, University of Technology, Helsinki.
</reference>
<figure confidence="0.99826075">
0.75
0.65
0.55
0.45
</figure>
<page confidence="0.951101">
517
</page>
<reference confidence="0.998697076190476">
Jagan Dara, John N. Dowling, Debbie Travers, Gre-
gory F. Cooper, and Wendy W. Chapman. 2008.
Evaluation of preprocessing techniques for chief com-
plaint classification. Journal of Biomedical Informat-
ics, 41(4):613–23.
Ana Deumert and Sibabalwe Oscar Masinyana. 2008.
Mobile language choices: the use of English and isiX-
hosa in text messages (SMS) evidence from a bilin-
gual South African sample. English World-Wide,
29(2):117–147.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153–198.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens by
estimating power-law generators. Advances in Neural
Information Processing Systems, 18.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21–54.
Matt Healy, Sarah Jane Delany, and Anton Zamolotskikh.
2005. An assessment of case-based reasoning for
Short Text Message Classification. In The 16th Irish
Conference on Artificial Intelligence &amp; Cognitive Sci-
ence.
Jos´e Mara G´omez Hidalgo, Guillermo Cajigas Bringas,
Enrique Puertas S´anz, and Francisco Carrero Garca.
2006. Content based SMS spam filtering. In ACM
symposium on Document engineering.
Scott Isbrandt. 2009. Cell Phones in West Africa: im-
proving literacy and agricultural market information
systems in Niger. White paper: Projet Alphab´etisation
de Base par Cellulaire.
Abi Jagun, Richard Heeks, and Jason Whalley. 2008.
The impact of mobile telephony on developing country
micro-enterprise: A Nigerian case study. Information
Technologies and International Development, 4.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Human Language Technologies.
Catherine Kobus, Franc¸ois Yvon, and Ge´eraldine
Damnati. 2008. Normalizing SMS: are two metaphors
better than one? In The 22nd International Confer-
ence on Computational Linguistics.
Mikko Kurimo, Matti Varjokallio, and Ville Turunen.
2008. Unsupervised morpheme analysis. In Morpho
Challenge Workshop, Finland. Helsinki University of
Technology.
Carole Leach-Lemens. 2009. Using mobile phones in
HIV care and prevention. HIV and AIDS Treatment in
Practice, 137.
Sam Mchombo. 2004. The Syntax of Chichewa. Cam-
bridge University Press, New York, NY.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. In The 47th
Annual Meeting of the Association for Computational
Linguistics.
Christian Monson, Jaime Carbonell, Alon Lavie, and Lori
Levin. 2008. ParaMor: finding paradigms across mor-
phology. Lecture Notes in Computer Science, 5152.
Robert Munro. 2010. Haiti Emergency Response: the
power of crowdsourcing and SMS. In Haiti Crisis Re-
lief 2.0, Stanford, CA.
Steven Paas. 2005. English Chichewa-Chinyanja Dictio-
nary. Mvunguti Books, Zomba, Malawi.
Guy De Pauw, Peter Waiganjo Wagacha, and Gilles-
Maurice de Schryver. 2009. The SAWA Corpus: a
parallel corpus of English - Swahili. In The EACL
Workshop on Language Technologies forAfrican Lan-
guages.
Gareth Peevers, Gary Douglas, and Mervyn A. Jack.
2008. A usability comparison of three alternative mes-
sage formats for an SMS banking service. Interna-
tional Journal of Human-Computer Studies, 66.
Rigardt Pretorius, Ansu Berg, Laurette Pretorius, and
Biffie Viljoen. 2009. Setswana tokenisation and com-
putational verb morphology: Facing the challenge of
a disjunctive orthography. In The EACL Workshop on
Language Technologies for African Languages.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Singapore.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical texts. In The Pacific Symposium on Bio-
computing, University of California, Berkeley.
Martha Yifiru Tachbelie, Solomon Teferra Abate, and
Wolfgang Menzel. 2009. Morpheme-based language
modeling for amharic speech recognition. In The 4th
Language and Technology Conference.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2005. Hierarchical Dirichlet pro-
cesses. In Advances in Neural Information Processing
Systems, 17.
</reference>
<page confidence="0.993843">
518
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.271716">
<title confidence="0.999857">Subword Variation in Text Message Classification</title>
<author confidence="0.99826">Robert</author>
<affiliation confidence="0.8596515">Department of Stanford</affiliation>
<address confidence="0.911203">Stanford, CA</address>
<email confidence="0.999139">rmunro@stanford.edu</email>
<author confidence="0.997984">D Christopher</author>
<affiliation confidence="0.7599995">Department of Computer Stanford</affiliation>
<address confidence="0.787773">Stanford, CA</address>
<email confidence="0.995442">manning@stanford.edu</email>
<abstract confidence="0.99658096">For millions of people in less resourced regions of the world, text messages (SMS) provide the only regular contact with their doctor. Classifying messages by medical labels supports rapid responses to emergencies, the early identification of epidemics and everyday administration, but challenges include textbrevity, rich morphology, phonological variation, and limited training data. We present a novel system that addresses these, working with a clinic in rural Malawi and texts in the Chichewa language. We show that modeling morphological and phonological variation leads to a substantial average gain of F=0.206 and an error reduction of up to 63.8% for specific labels, relative to a baseline system optimized over word-sequences. By comparison, there is no significant gain when applying the same system to the English translations of the same texts/labels, emphasizing the need for subword modeling in many languages. Language independent morphological models perform as accurately as language specific models, indicating a broad deployment potential.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jaco Badenhorst</author>
<author>Charl van Heerden</author>
<author>Marelie Davel</author>
<author>Etienne Barnard</author>
</authors>
<title>Collecting and evaluating speech recognition corpora for nine Southern Bantu languages.</title>
<date>2009</date>
<booktitle>In The EACL Workshop on Language Technologies for African Languages.</booktitle>
<marker>Badenhorst, van Heerden, Davel, Barnard, 2009</marker>
<rawString>Jaco Badenhorst, Charl van Heerden, Marelie Davel, and Etienne Barnard. 2009. Collecting and evaluating speech recognition corpora for nine Southern Bantu languages. In The EACL Workshop on Language Technologies for African Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piet Buys</author>
<author>Susmita Dasgupta</author>
<author>Timothy S Thomas</author>
<author>David Wheeler</author>
</authors>
<title>Determinants of a digital divide in Sub-Saharan Africa: A spatial econometric analysis of cell phone coverage.</title>
<date>2009</date>
<journal>World Development,</journal>
<volume>37</volume>
<issue>9</issue>
<contexts>
<context position="1557" citStr="Buys et al., 2009" startWordPosition="231" endWordPosition="234">labels, relative to a baseline system optimized over word-sequences. By comparison, there is no significant gain when applying the same system to the English translations of the same texts/labels, emphasizing the need for subword modeling in many languages. Language independent morphological models perform as accurately as language specific models, indicating a broad deployment potential. 1 Introduction The whole world is texting, but rarely in English. Africa has seen the greatest recent uptake of cellphones, with an 8-fold increase over the last 5 years and saturation possible in another 5 (Buys et al., 2009). This is a leapfrog technology – for the majority of new users cellphones are the only form of remote communication, surpassing landlines, (nonmobile) internet access and even grid electricity, with costs making texts the dominant communication method. This has led social development organizations to leverage mobile technologies to support health (Leach-Lemens, 2009), banking (Peevers et al., 2008), access to market information (Jagun et al., 2008), literacy (Isbrandt, 2009) and emergency response (Munro, 2010). The possibility to automate many of these services through text-classification is</context>
</contexts>
<marker>Buys, Dasgupta, Thomas, Wheeler, 2009</marker>
<rawString>Piet Buys, Susmita Dasgupta, Timothy S. Thomas, and David Wheeler. 2009. Determinants of a digital divide in Sub-Saharan Africa: A spatial econometric analysis of cell phone coverage. World Development, 37(9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gordon V Cormack</author>
<author>Jos´e Mara G´omez Hidalgo</author>
<author>Enrique Puertas S´anz</author>
</authors>
<title>Feature engineering for mobile (SMS) spam filtering.</title>
<date>2007</date>
<booktitle>In The 30th annual international ACM SIGIR conference on research and development in information retrieval.</booktitle>
<marker>Cormack, Hidalgo, S´anz, 2007</marker>
<rawString>Gordon V. Cormack, Jos´e Mara G´omez Hidalgo, and Enrique Puertas S´anz. 2007. Feature engineering for mobile (SMS) spam filtering. In The 30th annual international ACM SIGIR conference on research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
</authors>
<title>Induction of the Morphology of Natural Language: Unsupervised Morpheme Segmentation with Application to Automatic Speech Recognition.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Technology,</institution>
<location>Helsinki.</location>
<contexts>
<context position="26227" citStr="Creutz, 2006" startWordPosition="4166" endWordPosition="4167">t developments in computational morphology and NLP for Bantu languages. Badenhorst et al. (2009) found substantial variation in a speech recognition corpus for 9 Southern Bantu languages, where accurate models could also be built with limited data. Morphological segmentation improved Swahili-English machine translation in De Pauw et al. (2009), even in the absense of gold standard reference segmentations, as was the case here. The complexity and necessity of modeling non-contiguous morphemes in Bantu languages is discussed by Pretorius et al. (2009). Computational morphology (Goldsmith, 2001; Creutz, 2006; Kurimo et al., 2008; Johnson and Goldwater, 2009; Goldwater et al., 2009) has begun to play a prominent role in machine translation and speech recognition for morphologically rich languages (Goldwater and McClosky, 2005; Tachbelie et al., 2009). In the current-state-of-the-art, a combination of the ParaMor (Monson et al., 2008) and Morfessor (Creutz, 2006) algorithms achieved 516 10 Figure 1: The learning rate, comparing micro-F for the Chichewa and English systems on different training set sizes. A random stratified sample was used for subsets. the most accurate results in 2008 Morpho Chall</context>
</contexts>
<marker>Creutz, 2006</marker>
<rawString>Mathias Creutz. 2006. Induction of the Morphology of Natural Language: Unsupervised Morpheme Segmentation with Application to Automatic Speech Recognition. Ph.D. thesis, University of Technology, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagan Dara</author>
<author>John N Dowling</author>
<author>Debbie Travers</author>
<author>Gregory F Cooper</author>
<author>Wendy W Chapman</author>
</authors>
<title>Evaluation of preprocessing techniques for chief complaint classification.</title>
<date>2008</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>41</volume>
<issue>4</issue>
<contexts>
<context position="27637" citStr="Dara et al. (2008)" startWordPosition="4387" endWordPosition="4390">uistica (Goldsmith, 2001). Both were more accurate for segmentation than our adaptation of Goldwater et al. (2009), but with lower recall. For the reasons discussed in Section 5.3 this meant less accuracy in classification. Goldwater et al. have also used the Pitman-Yor algorithm for morphological modeling (Goldwater et al., 2006). In results too recent to test here, Pitman-Yor has been used for segmentation with accuracy comparable to the HDP model but with greater efficiency (Mochihashi et al., 2009). Biosurveillance systems currently use simple rule-based pre-processing for subword models. Dara et al. (2008) found only modest gains, although the data was limited to English. For text message classification, prior work is limited to identifying SPAM (Healy et al., 2005; Hidalgo et al., 2006; Cormack et al., 2007), where specialized algorithms and feature representations were also found to improve accuracy. For written variation, Kobus et al. (2008) focussed on SMSspecific abbreviations in French. Unlike their data, SMS-specific abbreviations were not present in our data. This is consistent with the reports on SMS practices in the related isiXhosa language (Deumert and Masinyana, 2008), but it may a</context>
</contexts>
<marker>Dara, Dowling, Travers, Cooper, Chapman, 2008</marker>
<rawString>Jagan Dara, John N. Dowling, Debbie Travers, Gregory F. Cooper, and Wendy W. Chapman. 2008. Evaluation of preprocessing techniques for chief complaint classification. Journal of Biomedical Informatics, 41(4):613–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana Deumert</author>
<author>Sibabalwe Oscar Masinyana</author>
</authors>
<title>Mobile language choices: the use of English and isiXhosa in text messages (SMS) evidence from a bilingual South African sample.</title>
<date>2008</date>
<journal>English World-Wide,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="28223" citStr="Deumert and Masinyana, 2008" startWordPosition="4479" endWordPosition="4482">ing for subword models. Dara et al. (2008) found only modest gains, although the data was limited to English. For text message classification, prior work is limited to identifying SPAM (Healy et al., 2005; Hidalgo et al., 2006; Cormack et al., 2007), where specialized algorithms and feature representations were also found to improve accuracy. For written variation, Kobus et al. (2008) focussed on SMSspecific abbreviations in French. Unlike their data, SMS-specific abbreviations were not present in our data. This is consistent with the reports on SMS practices in the related isiXhosa language (Deumert and Masinyana, 2008), but it may also be because the data we used contained professional communications not personal messages. Label class Label/No-Label KWF Final KWF Final F-val 0.403 0.684 0.713 0.950 Prec. 0.265 0.796 0.570 0.972 Rec. 0.842 0.599 0.953 0.929 Table 4: Micro-F, precision and recall, compared with the oracle keyword system. KWF = Oracle Keyword Filter. 7 Conclusions We have demonstrated that subword modeling in Chichewa leads to significant gains in classifying text messages according to medical labels, reducing the error from 1 in 4 to 1 in 20 in a system that should generalize to other languag</context>
</contexts>
<marker>Deumert, Masinyana, 2008</marker>
<rawString>Ana Deumert and Sibabalwe Oscar Masinyana. 2008. Mobile language choices: the use of English and isiXhosa in text messages (SMS) evidence from a bilingual South African sample. English World-Wide, 29(2):117–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="26213" citStr="Goldsmith, 2001" startWordPosition="4164" endWordPosition="4165">ild on many recent developments in computational morphology and NLP for Bantu languages. Badenhorst et al. (2009) found substantial variation in a speech recognition corpus for 9 Southern Bantu languages, where accurate models could also be built with limited data. Morphological segmentation improved Swahili-English machine translation in De Pauw et al. (2009), even in the absense of gold standard reference segmentations, as was the case here. The complexity and necessity of modeling non-contiguous morphemes in Bantu languages is discussed by Pretorius et al. (2009). Computational morphology (Goldsmith, 2001; Creutz, 2006; Kurimo et al., 2008; Johnson and Goldwater, 2009; Goldwater et al., 2009) has begun to play a prominent role in machine translation and speech recognition for morphologically rich languages (Goldwater and McClosky, 2005; Tachbelie et al., 2009). In the current-state-of-the-art, a combination of the ParaMor (Monson et al., 2008) and Morfessor (Creutz, 2006) algorithms achieved 516 10 Figure 1: The learning rate, comparing micro-F for the Chichewa and English systems on different training set sizes. A random stratified sample was used for subsets. the most accurate results in 200</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>David McClosky</author>
</authors>
<title>Improving statistical MT through morphological analysis.</title>
<date>2005</date>
<booktitle>In Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="26448" citStr="Goldwater and McClosky, 2005" startWordPosition="4199" endWordPosition="4202">dels could also be built with limited data. Morphological segmentation improved Swahili-English machine translation in De Pauw et al. (2009), even in the absense of gold standard reference segmentations, as was the case here. The complexity and necessity of modeling non-contiguous morphemes in Bantu languages is discussed by Pretorius et al. (2009). Computational morphology (Goldsmith, 2001; Creutz, 2006; Kurimo et al., 2008; Johnson and Goldwater, 2009; Goldwater et al., 2009) has begun to play a prominent role in machine translation and speech recognition for morphologically rich languages (Goldwater and McClosky, 2005; Tachbelie et al., 2009). In the current-state-of-the-art, a combination of the ParaMor (Monson et al., 2008) and Morfessor (Creutz, 2006) algorithms achieved 516 10 Figure 1: The learning rate, comparing micro-F for the Chichewa and English systems on different training set sizes. A random stratified sample was used for subsets. the most accurate results in 2008 Morpho Challenge Workshop (Kurimo et al., 2008). ParaMor assumes a single affix and is not easily adapted to more complex morphologies, but we were able to test and evaluate Morfessor and the earlier Linguistica (Goldsmith, 2001). Bo</context>
</contexts>
<marker>Goldwater, McClosky, 2005</marker>
<rawString>Sharon Goldwater and David McClosky. 2005. Improving statistical MT through morphological analysis. In Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<volume>18</volume>
<contexts>
<context position="27351" citStr="Goldwater et al., 2006" startWordPosition="4344" endWordPosition="4347">es. A random stratified sample was used for subsets. the most accurate results in 2008 Morpho Challenge Workshop (Kurimo et al., 2008). ParaMor assumes a single affix and is not easily adapted to more complex morphologies, but we were able to test and evaluate Morfessor and the earlier Linguistica (Goldsmith, 2001). Both were more accurate for segmentation than our adaptation of Goldwater et al. (2009), but with lower recall. For the reasons discussed in Section 5.3 this meant less accuracy in classification. Goldwater et al. have also used the Pitman-Yor algorithm for morphological modeling (Goldwater et al., 2006). In results too recent to test here, Pitman-Yor has been used for segmentation with accuracy comparable to the HDP model but with greater efficiency (Mochihashi et al., 2009). Biosurveillance systems currently use simple rule-based pre-processing for subword models. Dara et al. (2008) found only modest gains, although the data was limited to English. For text message classification, prior work is limited to identifying SPAM (Healy et al., 2005; Hidalgo et al., 2006; Cormack et al., 2007), where specialized algorithms and feature representations were also found to improve accuracy. For written</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. Advances in Neural Information Processing Systems, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="9790" citStr="Goldwater et al. (2009)" startWordPosition="1531" endWordPosition="1534">chical Dirichlet Process (HDP) (Teh et al., 2005). Every morpheme in the corpus mi is drawn from a distribution G which consists of possible morphemes (the affixes and stems) and probabilities associated with each morpheme. G is generated from a Dirichlet Process (DP) distribution DP(a0, P0), with morphemes sampled from P0 and their probabilities determined by a concentration parameter a0. The context-sensitive model where Hm is the DP for a specific morpheme is: mi|mi−1 = m, Hm—Hm bm Hm|a1, G —DP(a1, G) bm G|a0, P —DP(a0, P0) Note that this part of our model is identical to the bigram HDP in Goldwater et al. (2009), except that we possess a set of morphemes, not words. Because word boundaries are already marked in the majority of the messages, we constrain the model to treat all existing word boundaries in the corpus as morpheme boundaries, thus constraining the model to morpheme and compound segmentation. Unlike word-segmentation, not all tokens in the morpheme lexicon are equal, as we want to model stems separately from affixes in the stemmed models. We assume a) the free morphemes (stems and through compounding) are the least frequent and therefore have the lowest final probability, P(m), in the HDP </context>
<context position="26302" citStr="Goldwater et al., 2009" startWordPosition="4176" endWordPosition="4179">guages. Badenhorst et al. (2009) found substantial variation in a speech recognition corpus for 9 Southern Bantu languages, where accurate models could also be built with limited data. Morphological segmentation improved Swahili-English machine translation in De Pauw et al. (2009), even in the absense of gold standard reference segmentations, as was the case here. The complexity and necessity of modeling non-contiguous morphemes in Bantu languages is discussed by Pretorius et al. (2009). Computational morphology (Goldsmith, 2001; Creutz, 2006; Kurimo et al., 2008; Johnson and Goldwater, 2009; Goldwater et al., 2009) has begun to play a prominent role in machine translation and speech recognition for morphologically rich languages (Goldwater and McClosky, 2005; Tachbelie et al., 2009). In the current-state-of-the-art, a combination of the ParaMor (Monson et al., 2008) and Morfessor (Creutz, 2006) algorithms achieved 516 10 Figure 1: The learning rate, comparing micro-F for the Chichewa and English systems on different training set sizes. A random stratified sample was used for subsets. the most accurate results in 2008 Morpho Challenge Workshop (Kurimo et al., 2008). ParaMor assumes a single affix and is </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Healy</author>
<author>Sarah Jane Delany</author>
<author>Anton Zamolotskikh</author>
</authors>
<title>An assessment of case-based reasoning for Short Text Message Classification.</title>
<date>2005</date>
<booktitle>In The 16th Irish Conference on Artificial Intelligence &amp; Cognitive Science.</booktitle>
<contexts>
<context position="27799" citStr="Healy et al., 2005" startWordPosition="4415" endWordPosition="4418">ed in Section 5.3 this meant less accuracy in classification. Goldwater et al. have also used the Pitman-Yor algorithm for morphological modeling (Goldwater et al., 2006). In results too recent to test here, Pitman-Yor has been used for segmentation with accuracy comparable to the HDP model but with greater efficiency (Mochihashi et al., 2009). Biosurveillance systems currently use simple rule-based pre-processing for subword models. Dara et al. (2008) found only modest gains, although the data was limited to English. For text message classification, prior work is limited to identifying SPAM (Healy et al., 2005; Hidalgo et al., 2006; Cormack et al., 2007), where specialized algorithms and feature representations were also found to improve accuracy. For written variation, Kobus et al. (2008) focussed on SMSspecific abbreviations in French. Unlike their data, SMS-specific abbreviations were not present in our data. This is consistent with the reports on SMS practices in the related isiXhosa language (Deumert and Masinyana, 2008), but it may also be because the data we used contained professional communications not personal messages. Label class Label/No-Label KWF Final KWF Final F-val 0.403 0.684 0.71</context>
</contexts>
<marker>Healy, Delany, Zamolotskikh, 2005</marker>
<rawString>Matt Healy, Sarah Jane Delany, and Anton Zamolotskikh. 2005. An assessment of case-based reasoning for Short Text Message Classification. In The 16th Irish Conference on Artificial Intelligence &amp; Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e Mara G´omez Hidalgo</author>
<author>Guillermo Cajigas Bringas</author>
<author>Enrique Puertas S´anz</author>
<author>Francisco Carrero Garca</author>
</authors>
<title>Content based SMS spam filtering.</title>
<date>2006</date>
<booktitle>In ACM symposium on Document engineering.</booktitle>
<marker>Hidalgo, Bringas, S´anz, Garca, 2006</marker>
<rawString>Jos´e Mara G´omez Hidalgo, Guillermo Cajigas Bringas, Enrique Puertas S´anz, and Francisco Carrero Garca. 2006. Content based SMS spam filtering. In ACM symposium on Document engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Isbrandt</author>
</authors>
<title>Cell Phones in West Africa: improving literacy and agricultural market information systems in Niger. White paper: Projet Alphab´etisation de Base par Cellulaire.</title>
<date>2009</date>
<contexts>
<context position="2037" citStr="Isbrandt, 2009" startWordPosition="305" endWordPosition="306">est recent uptake of cellphones, with an 8-fold increase over the last 5 years and saturation possible in another 5 (Buys et al., 2009). This is a leapfrog technology – for the majority of new users cellphones are the only form of remote communication, surpassing landlines, (nonmobile) internet access and even grid electricity, with costs making texts the dominant communication method. This has led social development organizations to leverage mobile technologies to support health (Leach-Lemens, 2009), banking (Peevers et al., 2008), access to market information (Jagun et al., 2008), literacy (Isbrandt, 2009) and emergency response (Munro, 2010). The possibility to automate many of these services through text-classification is huge, as are the potential benefits – those with the least resources have the most to gain. However, the data presents many challenges, as text messages are brief, most languages have rich morphology, spellings may be overly-phonetic, and there is often limited training data. We partnered with a medical clinic in rural Malawi and FrontlineSMS:Medic, whose text message management systems serve a patient population of over 2 million in less developed regions of the world. The </context>
</contexts>
<marker>Isbrandt, 2009</marker>
<rawString>Scott Isbrandt. 2009. Cell Phones in West Africa: improving literacy and agricultural market information systems in Niger. White paper: Projet Alphab´etisation de Base par Cellulaire.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abi Jagun</author>
<author>Richard Heeks</author>
<author>Jason Whalley</author>
</authors>
<title>The impact of mobile telephony on developing country micro-enterprise: A Nigerian case study.</title>
<date>2008</date>
<booktitle>Information Technologies and International Development,</booktitle>
<pages>4</pages>
<contexts>
<context position="2010" citStr="Jagun et al., 2008" startWordPosition="300" endWordPosition="303">lish. Africa has seen the greatest recent uptake of cellphones, with an 8-fold increase over the last 5 years and saturation possible in another 5 (Buys et al., 2009). This is a leapfrog technology – for the majority of new users cellphones are the only form of remote communication, surpassing landlines, (nonmobile) internet access and even grid electricity, with costs making texts the dominant communication method. This has led social development organizations to leverage mobile technologies to support health (Leach-Lemens, 2009), banking (Peevers et al., 2008), access to market information (Jagun et al., 2008), literacy (Isbrandt, 2009) and emergency response (Munro, 2010). The possibility to automate many of these services through text-classification is huge, as are the potential benefits – those with the least resources have the most to gain. However, the data presents many challenges, as text messages are brief, most languages have rich morphology, spellings may be overly-phonetic, and there is often limited training data. We partnered with a medical clinic in rural Malawi and FrontlineSMS:Medic, whose text message management systems serve a patient population of over 2 million in less developed</context>
</contexts>
<marker>Jagun, Heeks, Whalley, 2008</marker>
<rawString>Abi Jagun, Richard Heeks, and Jason Whalley. 2008. The impact of mobile telephony on developing country micro-enterprise: A Nigerian case study. Information Technologies and International Development, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Human Language Technologies.</title>
<date>2009</date>
<contexts>
<context position="26277" citStr="Johnson and Goldwater, 2009" startWordPosition="4172" endWordPosition="4175">phology and NLP for Bantu languages. Badenhorst et al. (2009) found substantial variation in a speech recognition corpus for 9 Southern Bantu languages, where accurate models could also be built with limited data. Morphological segmentation improved Swahili-English machine translation in De Pauw et al. (2009), even in the absense of gold standard reference segmentations, as was the case here. The complexity and necessity of modeling non-contiguous morphemes in Bantu languages is discussed by Pretorius et al. (2009). Computational morphology (Goldsmith, 2001; Creutz, 2006; Kurimo et al., 2008; Johnson and Goldwater, 2009; Goldwater et al., 2009) has begun to play a prominent role in machine translation and speech recognition for morphologically rich languages (Goldwater and McClosky, 2005; Tachbelie et al., 2009). In the current-state-of-the-art, a combination of the ParaMor (Monson et al., 2008) and Morfessor (Creutz, 2006) algorithms achieved 516 10 Figure 1: The learning rate, comparing micro-F for the Chichewa and English systems on different training set sizes. A random stratified sample was used for subsets. the most accurate results in 2008 Morpho Challenge Workshop (Kurimo et al., 2008). ParaMor assum</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Kobus</author>
<author>Franc¸ois Yvon</author>
<author>Ge´eraldine Damnati</author>
</authors>
<title>Normalizing SMS: are two metaphors better than one?</title>
<date>2008</date>
<booktitle>In The 22nd International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="27982" citStr="Kobus et al. (2008)" startWordPosition="4443" endWordPosition="4446"> too recent to test here, Pitman-Yor has been used for segmentation with accuracy comparable to the HDP model but with greater efficiency (Mochihashi et al., 2009). Biosurveillance systems currently use simple rule-based pre-processing for subword models. Dara et al. (2008) found only modest gains, although the data was limited to English. For text message classification, prior work is limited to identifying SPAM (Healy et al., 2005; Hidalgo et al., 2006; Cormack et al., 2007), where specialized algorithms and feature representations were also found to improve accuracy. For written variation, Kobus et al. (2008) focussed on SMSspecific abbreviations in French. Unlike their data, SMS-specific abbreviations were not present in our data. This is consistent with the reports on SMS practices in the related isiXhosa language (Deumert and Masinyana, 2008), but it may also be because the data we used contained professional communications not personal messages. Label class Label/No-Label KWF Final KWF Final F-val 0.403 0.684 0.713 0.950 Prec. 0.265 0.796 0.570 0.972 Rec. 0.842 0.599 0.953 0.929 Table 4: Micro-F, precision and recall, compared with the oracle keyword system. KWF = Oracle Keyword Filter. 7 Conc</context>
</contexts>
<marker>Kobus, Yvon, Damnati, 2008</marker>
<rawString>Catherine Kobus, Franc¸ois Yvon, and Ge´eraldine Damnati. 2008. Normalizing SMS: are two metaphors better than one? In The 22nd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Matti Varjokallio</author>
<author>Ville Turunen</author>
</authors>
<title>Unsupervised morpheme analysis.</title>
<date>2008</date>
<booktitle>In Morpho Challenge Workshop,</booktitle>
<institution>Finland. Helsinki University of Technology.</institution>
<contexts>
<context position="26248" citStr="Kurimo et al., 2008" startWordPosition="4168" endWordPosition="4171"> in computational morphology and NLP for Bantu languages. Badenhorst et al. (2009) found substantial variation in a speech recognition corpus for 9 Southern Bantu languages, where accurate models could also be built with limited data. Morphological segmentation improved Swahili-English machine translation in De Pauw et al. (2009), even in the absense of gold standard reference segmentations, as was the case here. The complexity and necessity of modeling non-contiguous morphemes in Bantu languages is discussed by Pretorius et al. (2009). Computational morphology (Goldsmith, 2001; Creutz, 2006; Kurimo et al., 2008; Johnson and Goldwater, 2009; Goldwater et al., 2009) has begun to play a prominent role in machine translation and speech recognition for morphologically rich languages (Goldwater and McClosky, 2005; Tachbelie et al., 2009). In the current-state-of-the-art, a combination of the ParaMor (Monson et al., 2008) and Morfessor (Creutz, 2006) algorithms achieved 516 10 Figure 1: The learning rate, comparing micro-F for the Chichewa and English systems on different training set sizes. A random stratified sample was used for subsets. the most accurate results in 2008 Morpho Challenge Workshop (Kurimo</context>
</contexts>
<marker>Kurimo, Varjokallio, Turunen, 2008</marker>
<rawString>Mikko Kurimo, Matti Varjokallio, and Ville Turunen. 2008. Unsupervised morpheme analysis. In Morpho Challenge Workshop, Finland. Helsinki University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carole Leach-Lemens</author>
</authors>
<title>Using mobile phones in HIV care and prevention.</title>
<date>2009</date>
<booktitle>HIV and AIDS Treatment in Practice,</booktitle>
<pages>137</pages>
<contexts>
<context position="1927" citStr="Leach-Lemens, 2009" startWordPosition="289" endWordPosition="290"> deployment potential. 1 Introduction The whole world is texting, but rarely in English. Africa has seen the greatest recent uptake of cellphones, with an 8-fold increase over the last 5 years and saturation possible in another 5 (Buys et al., 2009). This is a leapfrog technology – for the majority of new users cellphones are the only form of remote communication, surpassing landlines, (nonmobile) internet access and even grid electricity, with costs making texts the dominant communication method. This has led social development organizations to leverage mobile technologies to support health (Leach-Lemens, 2009), banking (Peevers et al., 2008), access to market information (Jagun et al., 2008), literacy (Isbrandt, 2009) and emergency response (Munro, 2010). The possibility to automate many of these services through text-classification is huge, as are the potential benefits – those with the least resources have the most to gain. However, the data presents many challenges, as text messages are brief, most languages have rich morphology, spellings may be overly-phonetic, and there is often limited training data. We partnered with a medical clinic in rural Malawi and FrontlineSMS:Medic, whose text messag</context>
</contexts>
<marker>Leach-Lemens, 2009</marker>
<rawString>Carole Leach-Lemens. 2009. Using mobile phones in HIV care and prevention. HIV and AIDS Treatment in Practice, 137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Mchombo</author>
</authors>
<title>The Syntax of Chichewa.</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="8247" citStr="Mchombo (2004)" startWordPosition="1278" endWordPosition="1279">t character ngrams, as used by Hidalgo et al. (2006) for morphological variation in English and Spanish. The results converged with those of the segmented model, which is not surprising as the most frequent features would be similar and increasing data items would overcome the sparcity. We leave more sophisticated character ngram modeling for future work. 3.1 Language specific For the language specific morphological models we implemented a morphological parser as a set of context-free grammars for all possible prefixes and suffixes according to the formal definitions of Chichewa morphology in Mchombo (2004). We identified stems by parsing potential prefixes and suffixes, segmenting a word w into n morphemes wm,0, . . . , wm,n−1 leaving a stem ws with length len(ws) and corpus frequency of f(ws), such that len(ws) &gt; 0 (ie, there must be a stem). Where multiple parses could be applied, we minimized len(ws), then maximized n. 3.2 Language independent For the language independent morphological models we adapted the word-segmenter of Goldwater, Griffiths and Johnson (2009), to morphological parsing (see Related Work for other algorithms we tested/considered). It was suited to our task because a) it i</context>
</contexts>
<marker>Mchombo, 2004</marker>
<rawString>Sam Mchombo. 2004. The Syntax of Chichewa. Cambridge University Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling.</title>
<date>2009</date>
<booktitle>In The 47th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27526" citStr="Mochihashi et al., 2009" startWordPosition="4373" endWordPosition="4376">not easily adapted to more complex morphologies, but we were able to test and evaluate Morfessor and the earlier Linguistica (Goldsmith, 2001). Both were more accurate for segmentation than our adaptation of Goldwater et al. (2009), but with lower recall. For the reasons discussed in Section 5.3 this meant less accuracy in classification. Goldwater et al. have also used the Pitman-Yor algorithm for morphological modeling (Goldwater et al., 2006). In results too recent to test here, Pitman-Yor has been used for segmentation with accuracy comparable to the HDP model but with greater efficiency (Mochihashi et al., 2009). Biosurveillance systems currently use simple rule-based pre-processing for subword models. Dara et al. (2008) found only modest gains, although the data was limited to English. For text message classification, prior work is limited to identifying SPAM (Healy et al., 2005; Hidalgo et al., 2006; Cormack et al., 2007), where specialized algorithms and feature representations were also found to improve accuracy. For written variation, Kobus et al. (2008) focussed on SMSspecific abbreviations in French. Unlike their data, SMS-specific abbreviations were not present in our data. This is consistent</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. In The 47th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Monson</author>
<author>Jaime Carbonell</author>
<author>Alon Lavie</author>
<author>Lori Levin</author>
</authors>
<title>ParaMor: finding paradigms across morphology.</title>
<date>2008</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>5152</pages>
<contexts>
<context position="26558" citStr="Monson et al., 2008" startWordPosition="4216" endWordPosition="4219"> De Pauw et al. (2009), even in the absense of gold standard reference segmentations, as was the case here. The complexity and necessity of modeling non-contiguous morphemes in Bantu languages is discussed by Pretorius et al. (2009). Computational morphology (Goldsmith, 2001; Creutz, 2006; Kurimo et al., 2008; Johnson and Goldwater, 2009; Goldwater et al., 2009) has begun to play a prominent role in machine translation and speech recognition for morphologically rich languages (Goldwater and McClosky, 2005; Tachbelie et al., 2009). In the current-state-of-the-art, a combination of the ParaMor (Monson et al., 2008) and Morfessor (Creutz, 2006) algorithms achieved 516 10 Figure 1: The learning rate, comparing micro-F for the Chichewa and English systems on different training set sizes. A random stratified sample was used for subsets. the most accurate results in 2008 Morpho Challenge Workshop (Kurimo et al., 2008). ParaMor assumes a single affix and is not easily adapted to more complex morphologies, but we were able to test and evaluate Morfessor and the earlier Linguistica (Goldsmith, 2001). Both were more accurate for segmentation than our adaptation of Goldwater et al. (2009), but with lower recall. </context>
</contexts>
<marker>Monson, Carbonell, Lavie, Levin, 2008</marker>
<rawString>Christian Monson, Jaime Carbonell, Alon Lavie, and Lori Levin. 2008. ParaMor: finding paradigms across morphology. Lecture Notes in Computer Science, 5152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Munro</author>
</authors>
<title>Haiti Emergency Response: the power of crowdsourcing and SMS.</title>
<date>2010</date>
<booktitle>In Haiti Crisis Relief 2.0,</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="2074" citStr="Munro, 2010" startWordPosition="311" endWordPosition="312"> 8-fold increase over the last 5 years and saturation possible in another 5 (Buys et al., 2009). This is a leapfrog technology – for the majority of new users cellphones are the only form of remote communication, surpassing landlines, (nonmobile) internet access and even grid electricity, with costs making texts the dominant communication method. This has led social development organizations to leverage mobile technologies to support health (Leach-Lemens, 2009), banking (Peevers et al., 2008), access to market information (Jagun et al., 2008), literacy (Isbrandt, 2009) and emergency response (Munro, 2010). The possibility to automate many of these services through text-classification is huge, as are the potential benefits – those with the least resources have the most to gain. However, the data presents many challenges, as text messages are brief, most languages have rich morphology, spellings may be overly-phonetic, and there is often limited training data. We partnered with a medical clinic in rural Malawi and FrontlineSMS:Medic, whose text message management systems serve a patient population of over 2 million in less developed regions of the world. The system allows remote community health</context>
</contexts>
<marker>Munro, 2010</marker>
<rawString>Robert Munro. 2010. Haiti Emergency Response: the power of crowdsourcing and SMS. In Haiti Crisis Relief 2.0, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Paas</author>
</authors>
<title>English Chichewa-Chinyanja Dictionary.</title>
<date>2005</date>
<publisher>Mvunguti Books,</publisher>
<location>Zomba, Malawi.</location>
<contexts>
<context position="12123" citStr="Paas, 2005" startWordPosition="1925" endWordPosition="1926">ity of the variation stems from 1Note that identifying stems must be a separate step – if we allowed multiple free morphemes for each word to enter the lexicon without penalty in the HDP model it would converge on a zero-penalty distribution where all morphemes were free. 512 the phonology, but also from phonetic variation as expressed in a given writing system, and variation in the writing system itself arising from fluent speakers with varying literacy. 4.1 Chichewa specific For the language specific normalization, we applied a set of heuristics to the data, based on the variation given in (Paas, 2005) and our own knowledge of how Bantu languages are expressed in Roman scripts. The heuristics were used to normalize all alternates, eg: Jiwo —* iOo} and Jr —* l}, resulting in ndiwodwara —* ndiodwala. The heuristics represented forms for phonemes with the same potential place of articulation (‘c/k’), forms with an adjacent place-of-articulation that are common phonological alternates (‘l/r’, ‘e,i’), voicing alternations (‘s/z’), or language-internal phonological processes like the insertion of a glide between vowels that the morphology has made adjacent (like we pronounce but don’t spell in ‘g</context>
</contexts>
<marker>Paas, 2005</marker>
<rawString>Steven Paas. 2005. English Chichewa-Chinyanja Dictionary. Mvunguti Books, Zomba, Malawi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy De Pauw</author>
<author>Peter Waiganjo Wagacha</author>
<author>GillesMaurice de Schryver</author>
</authors>
<title>The SAWA Corpus: a parallel corpus of English - Swahili.</title>
<date>2009</date>
<booktitle>In The EACL Workshop on Language Technologies forAfrican Languages.</booktitle>
<marker>De Pauw, Wagacha, de Schryver, 2009</marker>
<rawString>Guy De Pauw, Peter Waiganjo Wagacha, and GillesMaurice de Schryver. 2009. The SAWA Corpus: a parallel corpus of English - Swahili. In The EACL Workshop on Language Technologies forAfrican Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gareth Peevers</author>
<author>Gary Douglas</author>
<author>Mervyn A Jack</author>
</authors>
<title>A usability comparison of three alternative message formats for an SMS banking service.</title>
<date>2008</date>
<journal>International Journal of Human-Computer Studies,</journal>
<volume>66</volume>
<contexts>
<context position="1959" citStr="Peevers et al., 2008" startWordPosition="292" endWordPosition="295">duction The whole world is texting, but rarely in English. Africa has seen the greatest recent uptake of cellphones, with an 8-fold increase over the last 5 years and saturation possible in another 5 (Buys et al., 2009). This is a leapfrog technology – for the majority of new users cellphones are the only form of remote communication, surpassing landlines, (nonmobile) internet access and even grid electricity, with costs making texts the dominant communication method. This has led social development organizations to leverage mobile technologies to support health (Leach-Lemens, 2009), banking (Peevers et al., 2008), access to market information (Jagun et al., 2008), literacy (Isbrandt, 2009) and emergency response (Munro, 2010). The possibility to automate many of these services through text-classification is huge, as are the potential benefits – those with the least resources have the most to gain. However, the data presents many challenges, as text messages are brief, most languages have rich morphology, spellings may be overly-phonetic, and there is often limited training data. We partnered with a medical clinic in rural Malawi and FrontlineSMS:Medic, whose text message management systems serve a pat</context>
</contexts>
<marker>Peevers, Douglas, Jack, 2008</marker>
<rawString>Gareth Peevers, Gary Douglas, and Mervyn A. Jack. 2008. A usability comparison of three alternative message formats for an SMS banking service. International Journal of Human-Computer Studies, 66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rigardt Pretorius</author>
<author>Ansu Berg</author>
<author>Laurette Pretorius</author>
<author>Biffie Viljoen</author>
</authors>
<title>Setswana tokenisation and computational verb morphology: Facing the challenge of a disjunctive orthography.</title>
<date>2009</date>
<booktitle>In The EACL Workshop on Language Technologies for African Languages.</booktitle>
<contexts>
<context position="26170" citStr="Pretorius et al. (2009)" startWordPosition="4158" endWordPosition="4161">ation, or any NLP task with the Chichewa, but we build on many recent developments in computational morphology and NLP for Bantu languages. Badenhorst et al. (2009) found substantial variation in a speech recognition corpus for 9 Southern Bantu languages, where accurate models could also be built with limited data. Morphological segmentation improved Swahili-English machine translation in De Pauw et al. (2009), even in the absense of gold standard reference segmentations, as was the case here. The complexity and necessity of modeling non-contiguous morphemes in Bantu languages is discussed by Pretorius et al. (2009). Computational morphology (Goldsmith, 2001; Creutz, 2006; Kurimo et al., 2008; Johnson and Goldwater, 2009; Goldwater et al., 2009) has begun to play a prominent role in machine translation and speech recognition for morphologically rich languages (Goldwater and McClosky, 2005; Tachbelie et al., 2009). In the current-state-of-the-art, a combination of the ParaMor (Monson et al., 2008) and Morfessor (Creutz, 2006) algorithms achieved 516 10 Figure 1: The learning rate, comparing micro-F for the Chichewa and English systems on different training set sizes. A random stratified sample was used fo</context>
</contexts>
<marker>Pretorius, Berg, Pretorius, Viljoen, 2009</marker>
<rawString>Rigardt Pretorius, Ansu Berg, Laurette Pretorius, and Biffie Viljoen. 2009. Setswana tokenisation and computational verb morphology: Facing the challenge of a disjunctive orthography. In The EACL Workshop on Language Technologies for African Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Labeled LDA: A supervised topic model for credit attribution in multilabeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="24921" citStr="Ramage et al. (2009)" startWordPosition="3962" endWordPosition="3965">g that the failure of the word-optimal baseline model is not just due to a lack of training items. 5.5 Other models investigated Much recent work in text classification has been in machine-learning, comparing models over constant features. We tested SVMs and joint learning strategies. The gains were significant but small and did not closed the gap between systems with and without subword modeling. We therefore omit these for space and scope. However, one interesting result came from extending the feature space with topics derived from Latent Dirichlet Allocation (LDA) using similar methods to Ramage et al. (2009). This produced significant gains (micro-F=0.029), halving the remaining gap with the English system, but only when the topics were derived from modeling noncontiguous morpheme sequences, not words-alone or segmented morphemes. We found that the different surface forms of each word cooccurred less often than chance (0.46 as often as chance for the different forms of odwala) forming disjunctive distributions. We suspect that this acts as a bias against robust unsupervised clustering of the different forms. 6 Related Work To our best knowledge, no prior researchers have worked on subword models </context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D. Manning. 2009. Labeled LDA: A supervised topic model for credit attribution in multilabeled corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel S Schwartz</author>
<author>Marti A Hearst</author>
</authors>
<title>A simple algorithm for identifying abbreviation definitions in biomedical texts.</title>
<date>2003</date>
<booktitle>In The Pacific Symposium on Biocomputing,</booktitle>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="13353" citStr="Schwartz &amp; Hearst (2003)" startWordPosition="2118" endWordPosition="2121">g’ in English). We also implemented hard-coded acronymrecovery methods for acronyms associated with the ‘Illness’ labels: ‘HIV’, ‘TB’, ‘AIDS’, ‘ARV’. 4.2 Script specific The script specific techniques used the same sets of alternates in the language specific model, but normalized such that the heuristic H was applied to a word w in the corpus C resulting in an alternate w&apos;, iff w&apos; E C. This method limits the alternates to those whose existence is supported by the data. It is therefore more conservative than the previous method. For more general acronym identification, we adapted the method of Schwartz &amp; Hearst (2003). We created a set of candidate acronyms by identifying capitalized sequences in non-capitalized contexts and period-delimited single character sequences. All case-insensitive sequences that were segmented by consistent non-alphabetic characters were then identified as acronyms, provided that they ended in a non-alphabetic character. We could not define a similar acronym-start boundary, as prefixes were often added to acronyms, even when the acronyms themselves contained spaces, eg: ‘aT. B.’. 4.3 Language independent For complete language independence we applied a noise-reduction algorithm to </context>
</contexts>
<marker>Schwartz, Hearst, 2003</marker>
<rawString>Ariel S. Schwartz and Marti A. Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical texts. In The Pacific Symposium on Biocomputing, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Yifiru Tachbelie</author>
<author>Solomon Teferra Abate</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Morpheme-based language modeling for amharic speech recognition.</title>
<date>2009</date>
<booktitle>In The 4th Language and Technology Conference.</booktitle>
<contexts>
<context position="26473" citStr="Tachbelie et al., 2009" startWordPosition="4203" endWordPosition="4207">limited data. Morphological segmentation improved Swahili-English machine translation in De Pauw et al. (2009), even in the absense of gold standard reference segmentations, as was the case here. The complexity and necessity of modeling non-contiguous morphemes in Bantu languages is discussed by Pretorius et al. (2009). Computational morphology (Goldsmith, 2001; Creutz, 2006; Kurimo et al., 2008; Johnson and Goldwater, 2009; Goldwater et al., 2009) has begun to play a prominent role in machine translation and speech recognition for morphologically rich languages (Goldwater and McClosky, 2005; Tachbelie et al., 2009). In the current-state-of-the-art, a combination of the ParaMor (Monson et al., 2008) and Morfessor (Creutz, 2006) algorithms achieved 516 10 Figure 1: The learning rate, comparing micro-F for the Chichewa and English systems on different training set sizes. A random stratified sample was used for subsets. the most accurate results in 2008 Morpho Challenge Workshop (Kurimo et al., 2008). ParaMor assumes a single affix and is not easily adapted to more complex morphologies, but we were able to test and evaluate Morfessor and the earlier Linguistica (Goldsmith, 2001). Both were more accurate for</context>
</contexts>
<marker>Tachbelie, Abate, Menzel, 2009</marker>
<rawString>Martha Yifiru Tachbelie, Solomon Teferra Abate, and Wolfgang Menzel. 2009. Morpheme-based language modeling for amharic speech recognition. In The 4th Language and Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>17</pages>
<contexts>
<context position="9216" citStr="Teh et al., 2005" startWordPosition="1431" endWordPosition="1434">the language independent morphological models we adapted the word-segmenter of Goldwater, Griffiths and Johnson (2009), to morphological parsing (see Related Work for other algorithms we tested/considered). It was suited to our task because a) it is largely nonparametric, meaning that it can be deployed as a black-box before language-specific properties are known b) it favored recall over precision (see the Results for discussion) and c) using a segmentation algorithm, rather than explicitly modeling morphology, also addresses compounds. This model uses a Hierarchical Dirichlet Process (HDP) (Teh et al., 2005). Every morpheme in the corpus mi is drawn from a distribution G which consists of possible morphemes (the affixes and stems) and probabilities associated with each morpheme. G is generated from a Dirichlet Process (DP) distribution DP(a0, P0), with morphemes sampled from P0 and their probabilities determined by a concentration parameter a0. The context-sensitive model where Hm is the DP for a specific morpheme is: mi|mi−1 = m, Hm—Hm bm Hm|a1, G —DP(a1, G) bm G|a0, P —DP(a0, P0) Note that this part of our model is identical to the bigram HDP in Goldwater et al. (2009), except that we possess a</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2005</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2005. Hierarchical Dirichlet processes. In Advances in Neural Information Processing Systems, 17.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>