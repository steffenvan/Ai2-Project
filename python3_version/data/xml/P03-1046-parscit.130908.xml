<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.991548">
Parsing with generative models of predicate-argument structure
</title>
<author confidence="0.997163">
Julia Hockenmaier
</author>
<affiliation confidence="0.791035333333333">
IRCS, University of Pennsylvania, Philadelphia, USA
and
Informatics, University of Edinburgh, Edinburgh, UK
</affiliation>
<email confidence="0.998289">
juliahr@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999779384615385">
The model used by the CCG parser
of Hockenmaier and Steedman (2002b)
would fail to capture the correct bilexical
dependencies in a language with freer
word order, such as Dutch. This paper
argues that probabilistic parsers should
therefore model the dependencies in the
predicate-argument structure, as in the
model of Clark et al. (2002), and defines
a generative model for CCG derivations
that captures these dependencies, includ-
ing bounded and unbounded long-range
dependencies.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919818181818">
State-of-the-art statistical parsers for Penn
Treebank-style phrase-structure grammars (Collins,
1999), (Charniak, 2000), but also for Categorial
Grammar (Hockenmaier and Steedman, 2002b),
include models of bilexical dependencies defined
in terms of local trees. However, this paper
demonstrates that such models would be inadequate
for languages with freer word order. We use the
example of Dutch ditransitives, but our argument
equally applies to other languages such as Czech
(see Collins et al. (1999)). We argue that this
problem can be avoided if instead the bilexical
dependencies in the predicate-argument structure
are captured, and propose a generative model for
these dependencies.
The focus of this paper is on models for Combina-
tory Categorial Grammar (CCG, Steedman (2000)).
Due to CCG’s transparent syntax-semantics inter-
face, the parser has direct and immediate access
to the predicate-argument structure, which includes
not only local, but also long-range dependencies
arising through coordination, extraction and con-
trol. These dependencies can be captured by our
model in a sound manner, and our experimental re-
sults for English demonstrate that their inclusion im-
proves parsing performance. However, since the
predicate-argument structure itself depends only to
a degree on the grammar formalism, it is likely
that parsers that are based on other grammar for-
malisms could equally benefit from such a model.
The conditional model used by the CCG parser of
Clark et al. (2002) also captures dependencies in the
predicate-argument structure; however, their model
is inconsistent.
First, we review the dependency model proposed
by Hockenmaier and Steedman (2002b). We then
use the example of Dutch ditransitives to demon-
strate its inadequacy for languages with a freer word
order. This leads us to define a new generative model
of CCG derivations, which captures word-word de-
pendencies in the underlying predicate-argument
structure. We show how this model can capture
long-range dependencies, and deal with the pres-
ence of multiple dependencies that arise through the
presence of long-range dependencies. In our current
implementation, the probabilities of derivations are
computed during parsing, and we discuss the dif-
ficulties of integrating the model into a probabilis-
tic chart parsing regime. Since there is no CCG
treebank for other languages available, experimen-
tal results are presented for English, using CCGbank
(Hockenmaier and Steedman, 2002a), a translation
of the Penn Treebank to CCG. These results demon-
strate that this model benefits greatly from the inclu-
sion of long-range dependencies.
</bodyText>
<sectionHeader confidence="0.809782" genericHeader="introduction">
2 A model of surface dependencies
</sectionHeader>
<bodyText confidence="0.999863">
Hockenmaier and Steedman (2002b) define a sur-
face dependency model (henceforth: SD) HWDep
which captures word-word dependencies that are de-
fined in terms of the derivation tree itself. It as-
sumes that binary trees (with parent category P)
have one head child (with category H) and one non-
head child (with category D), and that each node
has one lexical head h=(c, w). In the following tree,
P=S[dcl]\NP, H=(S[dcl]\NP)/NP, D=NP, hH =
((S[dcl]\NP)/NP, opened), and hD=(N, doors).
but also long-range dependencies that are projected
from the lexicon or through some rules such as the
coordination of functor categories. For details, see
Hockenmaier (2003).
</bodyText>
<sectionHeader confidence="0.854014" genericHeader="method">
4 Word-word dependencies in Dutch
</sectionHeader>
<bodyText confidence="0.999842875">
Dutch has a much freer word order than English.
The analyses given in Steedman (2000) assume that
this can be accounted for by an extended use of
composition. As indicated by the indices (which
are only included to improve readability), in the
following examples, hij is the subject (NP3) of
geeft, depolitieman the indirect object (NP2), and
een bloem the direct object (NP1).1
</bodyText>
<equation confidence="0.845213842105263">
Hij
(He
geeft de poliman een bloem
iv
l
a flower)
g
es the po
ictieeman
S/(S/NP3) ((S/NP1)/NP2)/NP3 T\(T/NP2) T\(T/NP1)
T\((T/NP1)/NP2) &lt;B
S[dcl]\NP &lt;Bx
S/NP3
&gt;
NP
S
Een bloem geeft hij de politieman
(S[dcl]\NP)/NP
opened its doors
</equation>
<bodyText confidence="0.9022568">
The model conditions wD on its own lexical cate-
gory cD, on hH = (cH, wH) and on the local tree T
in which the D is generated (represented in terms of
the categories (P, H, D)):
P(wDIcD, T = (P, H, D), hH = (cH, wH))
</bodyText>
<sectionHeader confidence="0.686553" genericHeader="method">
3 Predicate-argument structure in CCG
</sectionHeader>
<bodyText confidence="0.999586894736842">
Like Clark et al. (2002), we define predicate-
argument structure for CCG in terms of the depen-
dencies that hold between words with lexical func-
tor categories and their arguments. We assume that
a lexical head is a pair (c, w), consisting of a word
w and its lexical category c. Each constituent has
at least one lexical head (more if it is a coordinate
construction). The arguments of functor categories
are numbered from 1 to n, starting at the innermost
argument, where n is the arity of the functor, eg.
(S[dcl]\NP1)/NP2, (NP\NP1)/(S[dcl]/NP)2. De-
pendencies hold between lexical heads whose cat-
egory is a functor category and the lexical heads
of their arguments. Such dependencies can be ex-
pressed as 3-tuples ((c, w), i, (c&apos;, w&apos;)), where c is a
functor category with arity &gt; i, and (c&apos;, w&apos;) is a lex-
ical head of the ith argument of c.
The predicate-argument structure that corre-
sponds to a derivation contains not only local,
</bodyText>
<equation confidence="0.9903545">
S/(S/NP1) ((S/NP1)/NP2)/NP3 T\(T/NP3) T\(T/NP2)
&lt;
&lt;
S
De politieman geeft hij een bloem
S/(S/NP2) ((S/NP1)/NP2)/NP3 T\(T/NP3) T\(T/NP1)
&lt;
&lt;Bx
S/NP2
&gt;
</equation>
<bodyText confidence="0.9998607">
A SD model estimated from a corpus containing
these three sentences would not be able to capture
the correct dependencies. Unless we assume that
the above indices are given as a feature on the NP
categories, the model could not distinguish between
the dependency relations of Hij and geeft in the
first sentence, bloem and geeft in the second sen-
tence and politieman and geeft in the third sentence.
Even with the indices, either the dependency be-
tween politieman and geeft or between bloem and
geeft in the first sentence could not be captured by a
model that assumes that each local tree has exactly
one head. Furthermore, if one of these sentences oc-
curred in the training data, all of the dependencies in
the other variants of this sentence would be unseen
to the model. However, in terms of the predicate-
argument structure, all three examples express the
same relations. The model we propose here would
therefore be able to generalize from one example to
the word-word dependencies in the other examples.
</bodyText>
<equation confidence="0.953776166666667">
1The variables T are uninstantiated for reasons of space.
(S/NP1)/NP2
S/NP1
&gt;
(S/NP1)/NP2
S
</equation>
<bodyText confidence="0.997861833333333">
The cross-serial dependencies of Dutch are one
of the syntactic constructions that led people to
believe that more than context-free power is re-
quired for natural language analysis. Here is an
example together with the CCG derivation from
Steedman (2000):
</bodyText>
<equation confidence="0.984124375">
(that I Cecilia the horses saw feed)
dat ik Cecilia depaarden zag voeren
NP1 NP2 NP3 ((S\NP1)\NP2)/VP VP\NP3
gx
((S\NP1)\NP2)\NP3
(S\NP1)\NP2
S\NP1
S
</equation>
<bodyText confidence="0.999887571428571">
Again, a local dependency model would systemat-
ically model the wrong dependencies in this case,
since it would assume that all noun phrases are ar-
guments of the same verb.
However, since there is no Dutch corpus that is
annotated with CCG derivations, we restrict our at-
tention to English in the remainder of this paper.
</bodyText>
<sectionHeader confidence="0.561511" genericHeader="method">
5 A model of predicate-argument
structure
</sectionHeader>
<bodyText confidence="0.9999645">
We first explain how word-word dependencies in the
predicate-argument structure can be captured in a
generative model, and then describe how these prob-
abilities are estimated in the current implementation.
</bodyText>
<subsectionHeader confidence="0.999369">
5.1 Modelling local dependencies
</subsectionHeader>
<bodyText confidence="0.999893714285714">
We first define the probabilities for purely local de-
pendencies without coordination. By excluding non-
local dependencies and coordination, at most one
dependency relation holds for each word. Consider
the following sentence:
words of arguments (such as Smith) are generated
in the following manner:
</bodyText>
<equation confidence="0.69129">
P(waIca, ((ch,wh), i, (ca, wa)))
</equation>
<bodyText confidence="0.99745">
The head word of modifiers (such as yesterday) are
generated differently:
</bodyText>
<equation confidence="0.923589">
P(w.Ic., ((c.,w.), i, (ch,wh))
</equation>
<bodyText confidence="0.999976642857143">
Like Collins (1999) and Charniak (2000), the SD
model assumes that word-word dependencies can be
defined at the maximal projection of a constituent.
However, as the Dutch examples show, the argument
slot i can only be determined if the head constituent
is fully expanded. For instance, if S[dcl] expands
to a non-head S/(S/NP) and to a head S[dcl]/NP,
it is necessary to know how the S[dcl]/NP expands
to determine which argument is filled by the non-
head, even if we already know that the lexical cate-
gory of the head word of S[dcl]/NP is a ditransitive
((S[dcl]/NP)/NP)/NP. Therefore, we assume that
the non-head child of a node is only expanded after
the head child has been fully expanded.
</bodyText>
<subsectionHeader confidence="0.999946">
5.2 Modelling long-range dependencies
</subsectionHeader>
<bodyText confidence="0.999867">
The predicate-argument structure that corresponds
to a derivation contains not only local, but also long-
range dependencies that are projected from the lex-
icon or through some rules such as the coordination
of functor categories. In the following derivation,
Smith is the subject of resigned and of left:
</bodyText>
<figure confidence="0.921597428571429">
S[dcl]
NP S[dcl]\NP
S[dcl]
NP
S[dcl]\NP
Smith
N
S[dcl]\NP
resigned
(S\NP)\(S\NP)
yesterday
S[dcl]\NP[conj]
S[dcl]\NP
left
</figure>
<bodyText confidence="0.92844375">
In order to express both dependencies, Smith has
to be conditioned on resigned and on left:
Smith
N
S[dcl]\NP
resigned
conj
and
This derivation expresses the following depen-
dencies:
((S[dcl]\NP, resigned), 1, (N, Smith))
(((S\NP)\(S\NP),yesterday), 2, (S[dcl]\NP,resigned))
We assume again that heads are generated before
their modifiers or arguments, and that word-word
dependencies are expressed by conditioning modi-
fiers or arguments on heads. Therefore, the head
P(w=SmithI N,((S[dcl]\NP, resigned), 1, (N, w)),
((S[dcl]\NP, left), 1, (N, w)))
In terms of the predicate-argument structure,
resigned and left are both lexical heads of this
sentence. Since neither fills an argument slot of
the other, we assume that they are generated inde-
pendently. This is different from the SD model,
which conditions the head word of the second
and subsequent conjuncts on the head word of
the first conjunct. Similarly, in a sentence such
as Miller and Smith resigned, the current model as-
sumes that the two heads of the subject noun phrase
are conditioned on the verb, but not on each other.
Argument-cluster coordination constructions
such as give a dog a bone and a policeman a flower
are another example where the dependencies in the
predicate-argument structure cannot be expressed
at the level of the local trees that combine the
individual arguments. Instead, these dependencies
are projected down through the category of the
argument cluster:
extraction, words cannot be generated at the max-
imal projection of constituents anymore. Consider
the following examples:
</bodyText>
<figure confidence="0.9961771">
NP
NP
NP
The woman
(NP\NP)/(S[dcl]/NP)
(S[dcl]\NP)/NP
NP\NP
NP
The woman
(NP\NP)/(S[dcl]/NP)
S[dcl]/NP
that S/(S\NP) (S[dcl]\NP)/NP
NP saw
I
S\NP1
((S\NP1)/NP2)/NP3 (S\NP1)\(((S\NP1)/NP2)/NP3)
give
NP\NP
S[dcl]/NP
that
S/(S\NP)
NP
I
NP/NP
(S[dcl]\NP)/NP
saw
NP/PP
a picture
PP/NP
of
</figure>
<bodyText confidence="0.9796745">
Lexical categories that project long-range depen-
dencies include cases such as relative pronouns, con-
trol verbs, auxiliaries, modals and raising verbs.
This can be expressed by co-indexing their argu-
ments, eg. (NP\NP;)/(S[dcl]\NP;) for relative pro-
nouns. Here, Smith is also the subject of resign:
</bodyText>
<equation confidence="0.860653">
S[dcl]
NP
N (S[dcl]\NP)/(S[b]\NP)
</equation>
<bodyText confidence="0.911289">
Smith will
Again, in order to capture this dependency, we as-
sume that the entire verb phrase is generated before
the subject.
In relative clauses, there is a dependency between
the verbs in the relative clause and the head of the
noun phrase that is modified by the relative clause:
</bodyText>
<sectionHeader confidence="0.378053" genericHeader="method">
NP
</sectionHeader>
<bodyText confidence="0.999679964285714">
Since the entire relative clause is an adjunct, it is
generated after the noun phrase Smith. Therefore,
we cannot capture the dependency between Smith
and resigned by conditioning Smith on resigned. In-
stead, resigned needs to be conditioned on the fact
that its subject is Smith. This is similar to the way
in which head words of adjuncts such as yesterday
are generated. In addition to this dependency, we
also assume that there is a dependency between who
and resigned. It follows that if we want to capture
unbounded long-range dependencies such as object
In both cases, there is a S[dcl]/NP with lexical head
(S[dcl]\NP)/NP; however, in the second case, the
NP argument is not the object of the transitive verb.
This problem can be solved by generating
words at the leaf nodes instead of at the maxi-
mal projection of constituents. After expanding
the (S[dcl]\NP)/NP node to (S[dcl]\NP)/NP and
NP/NP, the NP that is co-indexed with woman can-
not be unified with the object of saw anymore.
These examples have shown that two changes to
the generative process are necessary if word-word
dependencies in the predicate-argument structure
are to be captured. First, head constituents have to
be fully expanded before non-head constituents are
generated. Second, words have to be generated at
the leaves of the tree, not at the maximal projection
of constituents.
</bodyText>
<subsectionHeader confidence="0.95623">
5.3 The word probabilities
</subsectionHeader>
<bodyText confidence="0.999682428571429">
Not all words have functor categories or fill argu-
ment slots of other functors. For instance, punctu-
ation marks, conjunctions, and the heads of entire
sentences are not conditioned on any other words.
Therefore, they are only conditioned on their lexical
categories. Therefore, this model contains the fol-
lowing three kinds of word probabilities:
</bodyText>
<listItem confidence="0.6357515">
1. Argument probabilities:
P(wIc,((c&apos;, w&apos;), i, (c, w)))
</listItem>
<bodyText confidence="0.999070333333333">
The probability of generating word w, given
that its lexical category is c and that (c, w) is
head of the ith argument of (c&apos;, w&apos;).
</bodyText>
<figure confidence="0.936510272727273">
NP\NP
(NP\NP)/(S[dcl]\NP)
Smith who
S[dcl]\NP
resigned
NP
N
S[dcl]\NP
S[b]\NP
resign
2. Functor probabilities:
</figure>
<equation confidence="0.485199">
P(wIc,((c, w), i, (c&apos;, w&apos;)))
</equation>
<bodyText confidence="0.9046415">
The probability of generating word w, given
that its lexical category is c and that (c&apos;, w&apos;) is
head of the ith argument of (c, w).
3. Other word probabilities: P (wIc)
If a word does not fill any dependency relation,
it is only conditioned on its lexical category.
</bodyText>
<subsectionHeader confidence="0.992135">
5.4 The structural probabilities
</subsectionHeader>
<bodyText confidence="0.9999848125">
Like the SD model, we assume an underlying pro-
cess which generates CCG derivation trees starting
from the root node. Each node in a derivation tree
has a category, a list of lexical heads and a (possi-
bly empty) list of dependency relations to be filled
by its lexical heads. As discussed in the previous
section, head words cannot in general be generated
at the maximal projection if unbounded long-range
dependencies are to be captured. This is not the case
for lexical categories. We therefore assume that a
node’s lexical head category is generated at its max-
imal projection, whereas head words are generated
at the leaf nodes. Since lexical categories are gen-
erated at the maximal projection, our model has the
same structural probabilities as the LezCat model of
Hockenmaier and Steedman (2002b).
</bodyText>
<subsectionHeader confidence="0.929854">
5.5 Estimating word probabilities
</subsectionHeader>
<bodyText confidence="0.999989111111111">
This model generates words in three different
ways—as arguments of functors that are already
generated, as functors which have already one (or
more) arguments instantiated, or independent of the
surrounding context. The last case is simple, as this
probability can be estimated directly, by counting
the number of times c is the lexical category of w in
the training corpus, and dividing this by the number
of times c occurs as a lexical category in the training
</bodyText>
<equation confidence="0.611911">
E7,, C(hhc0; w0i; i; hc; w00ii)
</equation>
<bodyText confidence="0.975014">
The probability of a functor w, given that its ith ar-
gument is instantiated by a constituent whose lexical
head is (c&apos;, w&apos;) can be estimated in a similar manner:
^P(wjc; hhc; wi; i; hc0; w0ii) =
</bodyText>
<equation confidence="0.9978295">
C(hhc; wi; i; hc0; w0ii)
E7,, C(hhc; w00i; i; hc0; w0ii)
</equation>
<bodyText confidence="0.9942515">
Here we count the number of times the ith argu-
ment of (c, w) is instantiated with (c&apos;, w&apos;), and di-
vide this by the number of times that (c&apos;, w&apos;) is the
ith argument of any lexical head with category c.
For instance, in order to compute the probability
of yesterday modifying resigned as in the previous
section, we count the number of times the transitive
verb resigned was modified by the adverb yesterday
and divide this by the number of times resigned was
modified by any adverb of the same category.
We have seen that functor probabilities are not
only necessary for adjuncts, but also for certain
types of long-range dependencies such as the rela-
tion between the noun modified by a relative clause
and the verb in the relative clause. In the case of zero
or reduced relative clauses, some of these dependen-
cies are also captured by the SD model. However, in
that model, only counts from the same type of con-
struction could be used, whereas in our model, the
functor probability for a verb in a zero or reduced
relative clause can be estimated from all occurrences
of the head noun. In particular, all instances of the
noun and verb occurring together in the training data
(with the same predicate-argument relation between
them, but not necessarily with the same surface con-
figuration) are taken into account by the new model.
To obtain the model probabilities, the relative fre-
quency estimates of the functor and argument prob-
abilities are both interpolated with the word proba-
bilities P�(wIc).
</bodyText>
<subsectionHeader confidence="0.997078">
5.6 Conditioning events on multiple heads
</subsectionHeader>
<bodyText confidence="0.9927072">
In the presence of long-range dependencies and co-
ordination, the new model requires the conditioning
of certain events on multiple heads. Since it is un-
likely that such probabilities can be estimated di-
rectly from data, they have to be approximated in
some manner.
If we assume that all dependencies dept that hold
for a word are equally likely, we can approximate
P (wIc, dept, ..., dep,,,) as the average of the individ-
ual dependency probabilities:
</bodyText>
<equation confidence="0.919034666666667">
corpus:
P^(wjc) = C(w; c)
C(c)
</equation>
<bodyText confidence="0.999953666666667">
In order to estimate the probability of an argument
w, we count the number of times it occurs with lex-
ical category c and is the ith argument of the lexical
functor (c&apos;, w&apos;) in question, divided by the number
of times the ith argument of (c&apos;, w&apos;) is instantiated
with a constituent whose lexical head category is c:
</bodyText>
<equation confidence="0.9591465">
^P(wjc; hhc0; w0i; i; hc; wii) =
C(hhc0; w0i; i; hc; wii)
</equation>
<bodyText confidence="0.999662333333333">
This approximation is has the advantage that it is
easy to compute, but might not give a good estimate,
since it averages over all individual distributions.
</bodyText>
<sectionHeader confidence="0.651829" genericHeader="method">
6 Dynamic programming and beam search
</sectionHeader>
<bodyText confidence="0.999993734375">
This section describes how this model is integrated
into a CKY chart parser. Dynamic programming and
effective beam search strategies are essential to guar-
antee efficient parsing in the face of the high ambi-
guity of wide-coverage grammars. Both use the in-
side probability of constituents. In lexicalized mod-
els where each constituent has exactly one lexical
head, and where this lexical head can only depend
on the lexical head of one other constituent, the in-
side probability of a constituent is the probability
that a node with the label and lexical head of this
constituent expands to the tree below this node. The
probability of generating a node with this label and
lexical head is given by the outside probability of the
constituent.
In the model defined here, the lexical head of
a constituent can depend on more than one other
word. As explained in section 5.2, there are in-
stances where the categorial functor is conditioned
on its arguments – the example given above showed
that verbs in relative clauses are conditioned on the
lexical head of the noun which is modified by the
relative clause. Therefore, the inside probability of
a constituent cannot include the probability of any
lexical head whose argument slots are not all filled.
This means that the equivalence relation defined
by the probability model needs to take into account
not only the head of the constituent itself, but also
all other lexical heads within this constituent which
have at least one unfilled argument slot. As a conse-
quence, dynamic programming becomes less effec-
tive. There is a related problem for the beam search:
in our model, the inside probabilities of constituents
within the same cell cannot be directly compared
anymore. Instead, the number of unfilled lexical
heads needs to be taken into account. If a lexical
head (c, w) is unfilled, the evaluation of the proba-
bility of w is delayed. This creates a problem for the
beam search strategy.
The fact that constituents can have more than one
lexical head causes similar problems for dynamic
programming and the beam search.
In order to be able to parse efficiently with our
model, we use the following approximations for dy-
namic programming and the beam search: Two con-
stituents with the same span and the same category
are considered equivalent if they delay the evalua-
tion of the probabilities of the same words and if
they have the same number of lexical heads, and if
the first two elements of their lists of lexical heads
are identical (the same words and lexical categories).
This is only an approximation to true equivalence,
since we do not check the entire list of lexical heads.
Furthermore, if a cell contains more than 100 con-
stituents, we iteratively narrow the beam (by halv-
ing it in size)2 until the beam search has no further
effect or the cell contains less than 100 constituents.
This is a very aggressive strategy, and it is likely to
adversely affect parsing accuracy. However, more
lenient strategies were found to require too much
space for the chart to be held in memory. A better
way of dealing with the space requirements of our
model would be to implement a packed shared parse
forest, but we leave this to future work.
</bodyText>
<sectionHeader confidence="0.900841" genericHeader="method">
7 An experiment
</sectionHeader>
<bodyText confidence="0.999564263157895">
We use sections 02-21 of CCGbank for training, sec-
tion 00 for development, and section 23 for test-
ing. The input is POS-tagged using the tagger of
Ratnaparkhi (1996). However, since parsing with
the new model is less efficient, only sentences &lt; 40
tokens only are used to test the model. A fre-
quency cutoff of &lt; 20 was used to determine rare
words in the training data, which are replaced with
their POS-tags. Unknown words in the test data
are also replaced by their POS-tags. The models
are evaluated according to their Parseval scores and
to the recovery of dependencies in the predicate-
argument structure. Like Clark et al. (2002), we
do not take the lexical category of the dependent
into account, and evaluate ((c, w), i, ( , w&apos;)) for la-
belled, and (( , w), , ( , w&apos;)) for unlabelled recov-
ery. Undirectional recovery (UdirP/UdirR) evalu-
ates only whether there is a dependency between w
and w&apos;. Unlike unlabelled recovery, this does not pe-
</bodyText>
<footnote confidence="0.862304">
2Beam search is as in Hockenmaier and Steedman (2002b).
</footnote>
<equation confidence="0.9928404">
P(wlc, dep1, ..., depn) N n
1
n
P(wlc, depi)
i=1
</equation>
<bodyText confidence="0.999893854166667">
nalize the parser if it mistakes a complement for an
adjunct or vice versa.
In order to determine the impact of capturing dif-
ferent kinds of long-range dependencies, four differ-
ent models were investigated: The baseline model is
like the LexCat model of (2002b), since the struc-
tural probabilities of our model are like those of
that model. Local only takes local dependencies
into account. LeftArgs only takes long-range de-
pendencies that are projected through left arguments
(\X) into account. This includes for instance long-
range dependencies projected by subjects, subject
and object control verbs, subject extraction and left-
node raising. All takes all long-range dependen-
cies into account, in particular it extends LeftArgs
by capturing also the unbounded dependencies aris-
ing through right-node-raising and object extraction.
Local, LeftArgs and All are all tested with the ag-
gressive beam strategy described above.
In all cases, the CCG derivation includes all long-
range dependencies. However, with the models that
exclude certain kinds of dependencies, it is possible
that a word is conditioned on no dependencies. In
these cases, the word is generated with P(wIc).
Table 1 gives the performance of all four mod-
els on section 23 in terms of the accuracy of lexical
categories, Parseval scores, and in terms of the re-
covery of word-word dependencies in the predicate-
argument structure. Here, results are further bro-
ken up into the recovery of local, all long-range,
bounded long-range and unbounded long-range de-
pendencies.
LexCat does not capture any word-word de-
pendencies. Its performance on the recovery of
predicate-argument structure can be improved by
3% by capturing only local word-word dependen-
cies (Local). This excludes certain kinds of depen-
dencies that were captured by the SD model. For in-
stance, the dependency between the head of a noun
phrase and the head of a reduced relative clause (the
shares bought by John) is captured by the SD model,
since shares and bought are both heads of the local
trees that are combined to form the complex noun
phrase. However, in the SD model the probability of
this dependency can only be estimated from occur-
rences of the same construction, since dependency
relations are defined in terms of local trees and not
in terms of the underlying predicate-argument struc-
</bodyText>
<table confidence="0.999779823529412">
LexCat Local LeftArgs All
Lex. cats: 88.2 89.9 90.1 90.1
Parseval
LP: 76.3 78.4 78.5 78.5
LR: 75.9 78.5 79.0 78.7
UP: 82.0 83.4 83.6 83.2
UR: 81.6 83.6 83.8 83.4
Predicate-argument structure (all)
LP: 77.3 80.8 81.6 81.5
LR: 78.2 80.6 81.5 81.4
UP: 86.4 88.3 88.9 88.7
UR: 87.4 88.1 88.8 88.6
UdirP: 88.0 89.7 90.2 90.0
UdirR: 89.0 89.5 90.1 90.0
Non-long-range dependencies
LP: 78.9 82.5 83.0 82.9
LR: 79.5 82.3 82.7 82.6
UP: 87.5 89.7 89.9 89.8
UR: 88.1 89.4 89.6 89.4
All long-range dependencies
LP: 60.8 62.6 67.1 66.3
LR: 64.4 63.0 68.5 68.8
UP: 75.3 74.2 78.9 78.1
UR: 80.2 74.9 80.5 80.9
Bounded long-range dependencies
LP: 63.9 64.8 69.0 69.2
LR: 65.9 64.1 70.2 70.0
UP: 79.8 77.1 81.4 81.4
UR: 82.4 76.7 82.6 82.6
Unbounded long-range dependencies
LP: 46.0 50.4 55.6 52.4
LR: 54.7 55.8 58.7 61.2
UP: 54.1 58.2 63.8 61.1
UR: 66.5 63.7 66.8 69.9
</table>
<tableCaption confidence="0.999942">
Table 1: Evaluation (sec. 23, &lt; 40 words).
</tableCaption>
<bodyText confidence="0.999952880952381">
ture. By including long-range dependencies on left
arguments (such as subjects) (LeftArgs), a further
improvement of 0.7% on the recovery of predicate-
argument structure is obtained. This model captures
the dependency between shares and bought. In con-
trast to the SD model, it can use all instances of
shares as the subject of a passive verb in the train-
ing data to estimate this probability. Therefore, even
if shares and bought do not co-occur in this partic-
ular construction in the training data, the event that
is modelled by our dependency model might not be
unseen, since it could have occurred in another syn-
tactic context.
Our results indicate that in order to perform well
on long-range dependencies, they have to be in-
cluded in the model, since Local, the model that
captures only local dependencies performs worse on
long-range dependencies than LexCat, the model
that captures no word-word dependencies. How-
ever, with more than 5% difference on labelled pre-
cision and recall on long-range dependencies, the
model which captures long-range dependencies on
left arguments performs significantly better on re-
covering long-range dependencies than Local. The
greatest difference in performance between the mod-
els which do capture long-range dependencies and
the models which do not is on long-range dependen-
cies. This indicates that, at least in the kind of model
considered here, it is very important to model not
just local, but also long-range dependencies. It is not
clear why All, the model that includes all dependen-
cies, performs slightly worse than the model which
includes only long-range dependencies on subjects.
On the Wall Street Journal task, the overall per-
formance of this model is lower than that of the
SD model of Hockenmaier and Steedman (2002b).
In that model, words are generated at the maxi-
mal projection of constituents; therefore, the struc-
tural probabilities can also be conditioned on words,
which improves the scores by about 2%. It is also
very likely that the performance of the new models
is harmed by the very aggressive beam search.
</bodyText>
<sectionHeader confidence="0.910218" genericHeader="conclusions">
8 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999992444444444">
This paper has defined a new generative model for
CCG derivations which captures the word-word de-
pendencies in the corresponding predicate-argument
structure, including bounded and unbounded long-
range dependencies. In contrast to the conditional
model of Clark et al. (2002), our model captures
these dependencies in a sound and consistent man-
ner. The experiments presented here demonstrate
that the performance of a simple baseline model
can be improved significantly if long-range depen-
dencies are also captured. In particular, our re-
sults indicate that it is important not to restrict the
model to local dependencies. Future work will ad-
dress the question whether these models can be
run with a less aggressive beam search strategy, or
whether a different parsing algorithm is more suit-
able. The problems that arise due to the overly
aggressive beam search strategy might be over-
come if we used an n-best parser with a simpler
probability model (eg. of the kind proposed by
Hockenmaier and Steedman (2002b)) and used the
new model as a re-ranker. The current implemen-
tation uses a very simple method of estimating the
probabilities of multiple dependencies, and more so-
phisticated techniques should be investigated.
We have argued that a model of the kind proposed
in this paper is essential for parsing languages with
freer word order, such as Dutch or Czech, where the
model of Hockenmaier and Steedman (2002b) (and
other models of surface dependencies) would sys-
tematically capture the wrong dependencies, even if
only local dependencies are taken into account. For
English, our experimental results demonstrate that
our model benefits greatly from modelling not only
local, but also long-range dependencies, which are
beyond the scope of surface dependency models.
</bodyText>
<sectionHeader confidence="0.997603" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99844075">
I would like to thank Mark Steedman and Stephen Clark for
many helpful discussions, and gratefully acknowledge support
from an EPSRC studentship and grant GR/M96889, the School
of Informatics, and NSF ITR grant 0205 456.
</bodyText>
<sectionHeader confidence="0.998966" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999721321428572">
Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser.
In Proceedings of the First Meeting of the NAACL, Seattle.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building Deep Dependency Structures using a Wide-
Coverage CCG Parser. In Proceedings of the 40th Annual
Meeting of the ACL.
Michael Collins, Jan Hajic, Lance Ramshaw, and Christoph
Tillmann. 1999. A Statistical Parser for Czech. In Pro-
ceedings of the 37th Annual Meeting of the ACL.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Julia Hockenmaier and Mark Steedman. 2002a. Acquiring
Compact Lexicalized Grammars from a Cleaner Treebank.
In Proceedings of the Third LREC, pages 1974–1981, Las
Palmas, May.
Julia Hockenmaier and Mark Steedman. 2002b. Generative
Models for Statistical Parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Annual Meeting of the
ACL.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with CCG. Ph.D. thesis, School of Informatics, Uni-
versity of Edinburgh.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-Of-
Speech Tagger. In Proceedings of the EMNLP Conference,
pages 133–142, Philadelphia, PA.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge Mass.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.752311">
<title confidence="0.998497">Parsing with generative models of predicate-argument structure</title>
<author confidence="0.999956">Julia Hockenmaier</author>
<affiliation confidence="0.933239666666667">IRCS, University of Pennsylvania, Philadelphia, USA and Informatics, University of Edinburgh, Edinburgh, UK</affiliation>
<email confidence="0.999453">juliahr@linc.cis.upenn.edu</email>
<abstract confidence="0.994614857142857">The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al. (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Meeting of the NAACL,</booktitle>
<location>Seattle.</location>
<contexts>
<context position="842" citStr="Charniak, 2000" startWordPosition="109" endWordPosition="110">e model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al. (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. 1 Introduction State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. However, this paper demonstrates that such models would be inadequate for languages with freer word order. We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see Collins et al. (1999)). We argue that this problem can be avoided if instead the bilexical dependencies in the predicate-argument structure are captured, and propose a generative model for these dependencies. The focus of this paper is </context>
<context position="8598" citStr="Charniak (2000)" startWordPosition="1355" endWordPosition="1356"> generative model, and then describe how these probabilities are estimated in the current implementation. 5.1 Modelling local dependencies We first define the probabilities for purely local dependencies without coordination. By excluding nonlocal dependencies and coordination, at most one dependency relation holds for each word. Consider the following sentence: words of arguments (such as Smith) are generated in the following manner: P(waIca, ((ch,wh), i, (ca, wa))) The head word of modifiers (such as yesterday) are generated differently: P(w.Ic., ((c.,w.), i, (ch,wh)) Like Collins (1999) and Charniak (2000), the SD model assumes that word-word dependencies can be defined at the maximal projection of a constituent. However, as the Dutch examples show, the argument slot i can only be determined if the head constituent is fully expanded. For instance, if S[dcl] expands to a non-head S/(S/NP) and to a head S[dcl]/NP, it is necessary to know how the S[dcl]/NP expands to determine which argument is filled by the nonhead, even if we already know that the lexical category of the head word of S[dcl]/NP is a ditransitive ((S[dcl]/NP)/NP)/NP. Therefore, we assume that the non-head child of a node is only e</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of the First Meeting of the NAACL, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Building Deep Dependency Structures using a WideCoverage CCG Parser.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="2216" citStr="Clark et al. (2002)" startWordPosition="318" endWordPosition="321">te access to the predicate-argument structure, which includes not only local, but also long-range dependencies arising through coordination, extraction and control. These dependencies can be captured by our model in a sound manner, and our experimental results for English demonstrate that their inclusion improves parsing performance. However, since the predicate-argument structure itself depends only to a degree on the grammar formalism, it is likely that parsers that are based on other grammar formalisms could equally benefit from such a model. The conditional model used by the CCG parser of Clark et al. (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). We then use the example of Dutch ditransitives to demonstrate its inadequacy for languages with a freer word order. This leads us to define a new generative model of CCG derivations, which captures word-word dependencies in the underlying predicate-argument structure. We show how this model can capture long-range dependencies, and deal with the presence of multiple dependencies that arise through the presence </context>
<context position="4956" citStr="Clark et al. (2002)" startWordPosition="758" endWordPosition="761"> the subject (NP3) of geeft, depolitieman the indirect object (NP2), and een bloem the direct object (NP1).1 Hij (He geeft de poliman een bloem iv l a flower) g es the po ictieeman S/(S/NP3) ((S/NP1)/NP2)/NP3 T\(T/NP2) T\(T/NP1) T\((T/NP1)/NP2) &lt;B S[dcl]\NP &lt;Bx S/NP3 &gt; NP S Een bloem geeft hij de politieman (S[dcl]\NP)/NP opened its doors The model conditions wD on its own lexical category cD, on hH = (cH, wH) and on the local tree T in which the D is generated (represented in terms of the categories (P, H, D)): P(wDIcD, T = (P, H, D), hH = (cH, wH)) 3 Predicate-argument structure in CCG Like Clark et al. (2002), we define predicateargument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments. We assume that a lexical head is a pair (c, w), consisting of a word w and its lexical category c. Each constituent has at least one lexical head (more if it is a coordinate construction). The arguments of functor categories are numbered from 1 to n, starting at the innermost argument, where n is the arity of the functor, eg. (S[dcl]\NP1)/NP2, (NP\NP1)/(S[dcl]/NP)2. Dependencies hold between lexical heads whose category is a functor category </context>
<context position="22564" citStr="Clark et al. (2002)" startWordPosition="3679" endWordPosition="3682"> 02-21 of CCGbank for training, section 00 for development, and section 23 for testing. The input is POS-tagged using the tagger of Ratnaparkhi (1996). However, since parsing with the new model is less efficient, only sentences &lt; 40 tokens only are used to test the model. A frequency cutoff of &lt; 20 was used to determine rare words in the training data, which are replaced with their POS-tags. Unknown words in the test data are also replaced by their POS-tags. The models are evaluated according to their Parseval scores and to the recovery of dependencies in the predicateargument structure. Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate ((c, w), i, ( , w&apos;)) for labelled, and (( , w), , ( , w&apos;)) for unlabelled recovery. Undirectional recovery (UdirP/UdirR) evaluates only whether there is a dependency between w and w&apos;. Unlike unlabelled recovery, this does not pe2Beam search is as in Hockenmaier and Steedman (2002b). P(wlc, dep1, ..., depn) N n 1 n P(wlc, depi) i=1 nalize the parser if it mistakes a complement for an adjunct or vice versa. In order to determine the impact of capturing different kinds of long-range dependencies, four different mode</context>
<context position="28574" citStr="Clark et al. (2002)" startWordPosition="4678" endWordPosition="4681">teedman (2002b). In that model, words are generated at the maximal projection of constituents; therefore, the structural probabilities can also be conditioned on words, which improves the scores by about 2%. It is also very likely that the performance of the new models is harmed by the very aggressive beam search. 8 Conclusion and future work This paper has defined a new generative model for CCG derivations which captures the word-word dependencies in the corresponding predicate-argument structure, including bounded and unbounded longrange dependencies. In contrast to the conditional model of Clark et al. (2002), our model captures these dependencies in a sound and consistent manner. The experiments presented here demonstrate that the performance of a simple baseline model can be improved significantly if long-range dependencies are also captured. In particular, our results indicate that it is important not to restrict the model to local dependencies. Future work will address the question whether these models can be run with a less aggressive beam search strategy, or whether a different parsing algorithm is more suitable. The problems that arise due to the overly aggressive beam search strategy might</context>
</contexts>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>Stephen Clark, Julia Hockenmaier, and Mark Steedman. 2002. Building Deep Dependency Structures using a WideCoverage CCG Parser. In Proceedings of the 40th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Jan Hajic</author>
<author>Lance Ramshaw</author>
<author>Christoph Tillmann</author>
</authors>
<title>A Statistical Parser for Czech.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1227" citStr="Collins et al. (1999)" startWordPosition="166" endWordPosition="169">rivations that captures these dependencies, including bounded and unbounded long-range dependencies. 1 Introduction State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. However, this paper demonstrates that such models would be inadequate for languages with freer word order. We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see Collins et al. (1999)). We argue that this problem can be avoided if instead the bilexical dependencies in the predicate-argument structure are captured, and propose a generative model for these dependencies. The focus of this paper is on models for Combinatory Categorial Grammar (CCG, Steedman (2000)). Due to CCG’s transparent syntax-semantics interface, the parser has direct and immediate access to the predicate-argument structure, which includes not only local, but also long-range dependencies arising through coordination, extraction and control. These dependencies can be captured by our model in a sound manner</context>
</contexts>
<marker>Collins, Hajic, Ramshaw, Tillmann, 1999</marker>
<rawString>Michael Collins, Jan Hajic, Lance Ramshaw, and Christoph Tillmann. 1999. A Statistical Parser for Czech. In Proceedings of the 37th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="824" citStr="Collins, 1999" startWordPosition="107" endWordPosition="108">n.edu Abstract The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al. (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. 1 Introduction State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. However, this paper demonstrates that such models would be inadequate for languages with freer word order. We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see Collins et al. (1999)). We argue that this problem can be avoided if instead the bilexical dependencies in the predicate-argument structure are captured, and propose a generative model for these dependencies. The focus</context>
<context position="8578" citStr="Collins (1999)" startWordPosition="1352" endWordPosition="1353">an be captured in a generative model, and then describe how these probabilities are estimated in the current implementation. 5.1 Modelling local dependencies We first define the probabilities for purely local dependencies without coordination. By excluding nonlocal dependencies and coordination, at most one dependency relation holds for each word. Consider the following sentence: words of arguments (such as Smith) are generated in the following manner: P(waIca, ((ch,wh), i, (ca, wa))) The head word of modifiers (such as yesterday) are generated differently: P(w.Ic., ((c.,w.), i, (ch,wh)) Like Collins (1999) and Charniak (2000), the SD model assumes that word-word dependencies can be defined at the maximal projection of a constituent. However, as the Dutch examples show, the argument slot i can only be determined if the head constituent is fully expanded. For instance, if S[dcl] expands to a non-head S/(S/NP) and to a head S[dcl]/NP, it is necessary to know how the S[dcl]/NP expands to determine which argument is filled by the nonhead, even if we already know that the lexical category of the head word of S[dcl]/NP is a ditransitive ((S[dcl]/NP)/NP)/NP. Therefore, we assume that the non-head child</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Acquiring Compact Lexicalized Grammars from a Cleaner Treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third LREC,</booktitle>
<pages>1974--1981</pages>
<location>Las Palmas,</location>
<contexts>
<context position="907" citStr="Hockenmaier and Steedman, 2002" startWordPosition="116" endWordPosition="119">Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al. (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. 1 Introduction State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. However, this paper demonstrates that such models would be inadequate for languages with freer word order. We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see Collins et al. (1999)). We argue that this problem can be avoided if instead the bilexical dependencies in the predicate-argument structure are captured, and propose a generative model for these dependencies. The focus of this paper is on models for Combinatory Categorial Grammar (CCG, Steedman (2000</context>
<context position="2399" citStr="Hockenmaier and Steedman (2002" startWordPosition="342" endWordPosition="345">ependencies can be captured by our model in a sound manner, and our experimental results for English demonstrate that their inclusion improves parsing performance. However, since the predicate-argument structure itself depends only to a degree on the grammar formalism, it is likely that parsers that are based on other grammar formalisms could equally benefit from such a model. The conditional model used by the CCG parser of Clark et al. (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). We then use the example of Dutch ditransitives to demonstrate its inadequacy for languages with a freer word order. This leads us to define a new generative model of CCG derivations, which captures word-word dependencies in the underlying predicate-argument structure. We show how this model can capture long-range dependencies, and deal with the presence of multiple dependencies that arise through the presence of long-range dependencies. In our current implementation, the probabilities of derivations are computed during parsing, and we discuss the difficulties of integrating the model into </context>
<context position="15339" citStr="Hockenmaier and Steedman (2002" startWordPosition="2430" endWordPosition="2433">st of lexical heads and a (possibly empty) list of dependency relations to be filled by its lexical heads. As discussed in the previous section, head words cannot in general be generated at the maximal projection if unbounded long-range dependencies are to be captured. This is not the case for lexical categories. We therefore assume that a node’s lexical head category is generated at its maximal projection, whereas head words are generated at the leaf nodes. Since lexical categories are generated at the maximal projection, our model has the same structural probabilities as the LezCat model of Hockenmaier and Steedman (2002b). 5.5 Estimating word probabilities This model generates words in three different ways—as arguments of functors that are already generated, as functors which have already one (or more) arguments instantiated, or independent of the surrounding context. The last case is simple, as this probability can be estimated directly, by counting the number of times c is the lexical category of w in the training corpus, and dividing this by the number of times c occurs as a lexical category in the training E7,, C(hhc0; w0i; i; hc; w00ii) The probability of a functor w, given that its ith argument is inst</context>
<context position="22926" citStr="Hockenmaier and Steedman (2002" startWordPosition="3745" endWordPosition="3748">ta, which are replaced with their POS-tags. Unknown words in the test data are also replaced by their POS-tags. The models are evaluated according to their Parseval scores and to the recovery of dependencies in the predicateargument structure. Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate ((c, w), i, ( , w&apos;)) for labelled, and (( , w), , ( , w&apos;)) for unlabelled recovery. Undirectional recovery (UdirP/UdirR) evaluates only whether there is a dependency between w and w&apos;. Unlike unlabelled recovery, this does not pe2Beam search is as in Hockenmaier and Steedman (2002b). P(wlc, dep1, ..., depn) N n 1 n P(wlc, depi) i=1 nalize the parser if it mistakes a complement for an adjunct or vice versa. In order to determine the impact of capturing different kinds of long-range dependencies, four different models were investigated: The baseline model is like the LexCat model of (2002b), since the structural probabilities of our model are like those of that model. Local only takes local dependencies into account. LeftArgs only takes long-range dependencies that are projected through left arguments (\X) into account. This includes for instance longrange dependencies p</context>
<context position="27968" citStr="Hockenmaier and Steedman (2002" startWordPosition="4582" endWordPosition="4585">endencies than Local. The greatest difference in performance between the models which do capture long-range dependencies and the models which do not is on long-range dependencies. This indicates that, at least in the kind of model considered here, it is very important to model not just local, but also long-range dependencies. It is not clear why All, the model that includes all dependencies, performs slightly worse than the model which includes only long-range dependencies on subjects. On the Wall Street Journal task, the overall performance of this model is lower than that of the SD model of Hockenmaier and Steedman (2002b). In that model, words are generated at the maximal projection of constituents; therefore, the structural probabilities can also be conditioned on words, which improves the scores by about 2%. It is also very likely that the performance of the new models is harmed by the very aggressive beam search. 8 Conclusion and future work This paper has defined a new generative model for CCG derivations which captures the word-word dependencies in the corresponding predicate-argument structure, including bounded and unbounded longrange dependencies. In contrast to the conditional model of Clark et al. </context>
<context position="29307" citStr="Hockenmaier and Steedman (2002" startWordPosition="4799" endWordPosition="4802">e demonstrate that the performance of a simple baseline model can be improved significantly if long-range dependencies are also captured. In particular, our results indicate that it is important not to restrict the model to local dependencies. Future work will address the question whether these models can be run with a less aggressive beam search strategy, or whether a different parsing algorithm is more suitable. The problems that arise due to the overly aggressive beam search strategy might be overcome if we used an n-best parser with a simpler probability model (eg. of the kind proposed by Hockenmaier and Steedman (2002b)) and used the new model as a re-ranker. The current implementation uses a very simple method of estimating the probabilities of multiple dependencies, and more sophisticated techniques should be investigated. We have argued that a model of the kind proposed in this paper is essential for parsing languages with freer word order, such as Dutch or Czech, where the model of Hockenmaier and Steedman (2002b) (and other models of surface dependencies) would systematically capture the wrong dependencies, even if only local dependencies are taken into account. For English, our experimental results d</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002a. Acquiring Compact Lexicalized Grammars from a Cleaner Treebank. In Proceedings of the Third LREC, pages 1974–1981, Las Palmas, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="907" citStr="Hockenmaier and Steedman, 2002" startWordPosition="116" endWordPosition="119">Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al. (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. 1 Introduction State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. However, this paper demonstrates that such models would be inadequate for languages with freer word order. We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see Collins et al. (1999)). We argue that this problem can be avoided if instead the bilexical dependencies in the predicate-argument structure are captured, and propose a generative model for these dependencies. The focus of this paper is on models for Combinatory Categorial Grammar (CCG, Steedman (2000</context>
<context position="2399" citStr="Hockenmaier and Steedman (2002" startWordPosition="342" endWordPosition="345">ependencies can be captured by our model in a sound manner, and our experimental results for English demonstrate that their inclusion improves parsing performance. However, since the predicate-argument structure itself depends only to a degree on the grammar formalism, it is likely that parsers that are based on other grammar formalisms could equally benefit from such a model. The conditional model used by the CCG parser of Clark et al. (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). We then use the example of Dutch ditransitives to demonstrate its inadequacy for languages with a freer word order. This leads us to define a new generative model of CCG derivations, which captures word-word dependencies in the underlying predicate-argument structure. We show how this model can capture long-range dependencies, and deal with the presence of multiple dependencies that arise through the presence of long-range dependencies. In our current implementation, the probabilities of derivations are computed during parsing, and we discuss the difficulties of integrating the model into </context>
<context position="15339" citStr="Hockenmaier and Steedman (2002" startWordPosition="2430" endWordPosition="2433">st of lexical heads and a (possibly empty) list of dependency relations to be filled by its lexical heads. As discussed in the previous section, head words cannot in general be generated at the maximal projection if unbounded long-range dependencies are to be captured. This is not the case for lexical categories. We therefore assume that a node’s lexical head category is generated at its maximal projection, whereas head words are generated at the leaf nodes. Since lexical categories are generated at the maximal projection, our model has the same structural probabilities as the LezCat model of Hockenmaier and Steedman (2002b). 5.5 Estimating word probabilities This model generates words in three different ways—as arguments of functors that are already generated, as functors which have already one (or more) arguments instantiated, or independent of the surrounding context. The last case is simple, as this probability can be estimated directly, by counting the number of times c is the lexical category of w in the training corpus, and dividing this by the number of times c occurs as a lexical category in the training E7,, C(hhc0; w0i; i; hc; w00ii) The probability of a functor w, given that its ith argument is inst</context>
<context position="22926" citStr="Hockenmaier and Steedman (2002" startWordPosition="3745" endWordPosition="3748">ta, which are replaced with their POS-tags. Unknown words in the test data are also replaced by their POS-tags. The models are evaluated according to their Parseval scores and to the recovery of dependencies in the predicateargument structure. Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate ((c, w), i, ( , w&apos;)) for labelled, and (( , w), , ( , w&apos;)) for unlabelled recovery. Undirectional recovery (UdirP/UdirR) evaluates only whether there is a dependency between w and w&apos;. Unlike unlabelled recovery, this does not pe2Beam search is as in Hockenmaier and Steedman (2002b). P(wlc, dep1, ..., depn) N n 1 n P(wlc, depi) i=1 nalize the parser if it mistakes a complement for an adjunct or vice versa. In order to determine the impact of capturing different kinds of long-range dependencies, four different models were investigated: The baseline model is like the LexCat model of (2002b), since the structural probabilities of our model are like those of that model. Local only takes local dependencies into account. LeftArgs only takes long-range dependencies that are projected through left arguments (\X) into account. This includes for instance longrange dependencies p</context>
<context position="27968" citStr="Hockenmaier and Steedman (2002" startWordPosition="4582" endWordPosition="4585">endencies than Local. The greatest difference in performance between the models which do capture long-range dependencies and the models which do not is on long-range dependencies. This indicates that, at least in the kind of model considered here, it is very important to model not just local, but also long-range dependencies. It is not clear why All, the model that includes all dependencies, performs slightly worse than the model which includes only long-range dependencies on subjects. On the Wall Street Journal task, the overall performance of this model is lower than that of the SD model of Hockenmaier and Steedman (2002b). In that model, words are generated at the maximal projection of constituents; therefore, the structural probabilities can also be conditioned on words, which improves the scores by about 2%. It is also very likely that the performance of the new models is harmed by the very aggressive beam search. 8 Conclusion and future work This paper has defined a new generative model for CCG derivations which captures the word-word dependencies in the corresponding predicate-argument structure, including bounded and unbounded longrange dependencies. In contrast to the conditional model of Clark et al. </context>
<context position="29307" citStr="Hockenmaier and Steedman (2002" startWordPosition="4799" endWordPosition="4802">e demonstrate that the performance of a simple baseline model can be improved significantly if long-range dependencies are also captured. In particular, our results indicate that it is important not to restrict the model to local dependencies. Future work will address the question whether these models can be run with a less aggressive beam search strategy, or whether a different parsing algorithm is more suitable. The problems that arise due to the overly aggressive beam search strategy might be overcome if we used an n-best parser with a simpler probability model (eg. of the kind proposed by Hockenmaier and Steedman (2002b)) and used the new model as a re-ranker. The current implementation uses a very simple method of estimating the probabilities of multiple dependencies, and more sophisticated techniques should be investigated. We have argued that a model of the kind proposed in this paper is essential for parsing languages with freer word order, such as Dutch or Czech, where the model of Hockenmaier and Steedman (2002b) (and other models of surface dependencies) would systematically capture the wrong dependencies, even if only local dependencies are taken into account. For English, our experimental results d</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002b. Generative Models for Statistical Parsing with Combinatory Categorial Grammar. In Proceedings of the 40th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with CCG.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="4031" citStr="Hockenmaier (2003)" startWordPosition="597" endWordPosition="598">efine a surface dependency model (henceforth: SD) HWDep which captures word-word dependencies that are defined in terms of the derivation tree itself. It assumes that binary trees (with parent category P) have one head child (with category H) and one nonhead child (with category D), and that each node has one lexical head h=(c, w). In the following tree, P=S[dcl]\NP, H=(S[dcl]\NP)/NP, D=NP, hH = ((S[dcl]\NP)/NP, opened), and hD=(N, doors). but also long-range dependencies that are projected from the lexicon or through some rules such as the coordination of functor categories. For details, see Hockenmaier (2003). 4 Word-word dependencies in Dutch Dutch has a much freer word order than English. The analyses given in Steedman (2000) assume that this can be accounted for by an extended use of composition. As indicated by the indices (which are only included to improve readability), in the following examples, hij is the subject (NP3) of geeft, depolitieman the indirect object (NP2), and een bloem the direct object (NP1).1 Hij (He geeft de poliman een bloem iv l a flower) g es the po ictieeman S/(S/NP3) ((S/NP1)/NP2)/NP3 T\(T/NP2) T\(T/NP1) T\((T/NP1)/NP2) &lt;B S[dcl]\NP &lt;Bx S/NP3 &gt; NP S Een bloem geeft hij</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and Models for Statistical Parsing with CCG. Ph.D. thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Part-OfSpeech Tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>133--142</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="22095" citStr="Ratnaparkhi (1996)" startWordPosition="3599" endWordPosition="3600">l the beam search has no further effect or the cell contains less than 100 constituents. This is a very aggressive strategy, and it is likely to adversely affect parsing accuracy. However, more lenient strategies were found to require too much space for the chart to be held in memory. A better way of dealing with the space requirements of our model would be to implement a packed shared parse forest, but we leave this to future work. 7 An experiment We use sections 02-21 of CCGbank for training, section 00 for development, and section 23 for testing. The input is POS-tagged using the tagger of Ratnaparkhi (1996). However, since parsing with the new model is less efficient, only sentences &lt; 40 tokens only are used to test the model. A frequency cutoff of &lt; 20 was used to determine rare words in the training data, which are replaced with their POS-tags. Unknown words in the test data are also replaced by their POS-tags. The models are evaluated according to their Parseval scores and to the recovery of dependencies in the predicateargument structure. Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate ((c, w), i, ( , w&apos;)) for labelled, and (( , w), ,</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-OfSpeech Tagger. In Proceedings of the EMNLP Conference, pages 133–142, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge Mass.</location>
<contexts>
<context position="1508" citStr="Steedman (2000)" startWordPosition="211" endWordPosition="212">Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. However, this paper demonstrates that such models would be inadequate for languages with freer word order. We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see Collins et al. (1999)). We argue that this problem can be avoided if instead the bilexical dependencies in the predicate-argument structure are captured, and propose a generative model for these dependencies. The focus of this paper is on models for Combinatory Categorial Grammar (CCG, Steedman (2000)). Due to CCG’s transparent syntax-semantics interface, the parser has direct and immediate access to the predicate-argument structure, which includes not only local, but also long-range dependencies arising through coordination, extraction and control. These dependencies can be captured by our model in a sound manner, and our experimental results for English demonstrate that their inclusion improves parsing performance. However, since the predicate-argument structure itself depends only to a degree on the grammar formalism, it is likely that parsers that are based on other grammar formalisms </context>
<context position="4152" citStr="Steedman (2000)" startWordPosition="617" endWordPosition="618">he derivation tree itself. It assumes that binary trees (with parent category P) have one head child (with category H) and one nonhead child (with category D), and that each node has one lexical head h=(c, w). In the following tree, P=S[dcl]\NP, H=(S[dcl]\NP)/NP, D=NP, hH = ((S[dcl]\NP)/NP, opened), and hD=(N, doors). but also long-range dependencies that are projected from the lexicon or through some rules such as the coordination of functor categories. For details, see Hockenmaier (2003). 4 Word-word dependencies in Dutch Dutch has a much freer word order than English. The analyses given in Steedman (2000) assume that this can be accounted for by an extended use of composition. As indicated by the indices (which are only included to improve readability), in the following examples, hij is the subject (NP3) of geeft, depolitieman the indirect object (NP2), and een bloem the direct object (NP1).1 Hij (He geeft de poliman een bloem iv l a flower) g es the po ictieeman S/(S/NP3) ((S/NP1)/NP2)/NP3 T\(T/NP2) T\(T/NP1) T\((T/NP1)/NP2) &lt;B S[dcl]\NP &lt;Bx S/NP3 &gt; NP S Een bloem geeft hij de politieman (S[dcl]\NP)/NP opened its doors The model conditions wD on its own lexical category cD, on hH = (cH, wH) a</context>
<context position="7368" citStr="Steedman (2000)" startWordPosition="1166" endWordPosition="1167">nce would be unseen to the model. However, in terms of the predicateargument structure, all three examples express the same relations. The model we propose here would therefore be able to generalize from one example to the word-word dependencies in the other examples. 1The variables T are uninstantiated for reasons of space. (S/NP1)/NP2 S/NP1 &gt; (S/NP1)/NP2 S The cross-serial dependencies of Dutch are one of the syntactic constructions that led people to believe that more than context-free power is required for natural language analysis. Here is an example together with the CCG derivation from Steedman (2000): (that I Cecilia the horses saw feed) dat ik Cecilia depaarden zag voeren NP1 NP2 NP3 ((S\NP1)\NP2)/VP VP\NP3 gx ((S\NP1)\NP2)\NP3 (S\NP1)\NP2 S\NP1 S Again, a local dependency model would systematically model the wrong dependencies in this case, since it would assume that all noun phrases are arguments of the same verb. However, since there is no Dutch corpus that is annotated with CCG derivations, we restrict our attention to English in the remainder of this paper. 5 A model of predicate-argument structure We first explain how word-word dependencies in the predicate-argument structure can b</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge Mass.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>