<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002738">
<note confidence="0.6750358">
Multimodal Database Access on Handheld Devices
Elsa Pecourt and Norbert Reithinger
DFKI GmbH
Stuhlsatzenhausenweg3
D-66123 Saarbr¨ucken, Germany
</note>
<email confidence="0.993216">
{pecourt,reithinger}@dfki.de
</email>
<sectionHeader confidence="0.997299" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912">
We present the final MIAMM system, a multimodal
dialogue system that employs speech, haptic inter-
action and novel techniques of information visual-
ization to allow a natural and fast access to large
multimedia databases on small handheld devices.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996445625">
Navigation in large, complex and multidimensional
information spaces is still a challenging task. The
search is even more difficult in small devices such as
MP3 players, which only have a reduced screen and
lack of a proper keyboard. In the MIAMM project&apos;
we have developed a multimodal dialogue system
that uses speech, haptic interaction and advanced
techniques for information visualization to allow a
natural and fast access to music databases on small
scale devices. The user can pose queries in natural
language, using different dimensions, e.g. release
year, genre, artist, or mood. The retrieved data are
presented along this dimensions using various vi-
sualization metaphors. Haptic feedback allows the
user to feel the size, density and structure of the vi-
sualized data to facilitate the navigation. All modal-
ities are available for the user to access and navi-
gate through the database, and to select titles to be
played.
The envisioned end-user device is a handheld
Personal Digital Assistant (PDA, see figure 1) that
provides an interface to a music database. The
device includes a screen where data and system
messages are visualized, three force-feedback but-
tons on the left side and one combined scroll
wheel/button on the upper right side, that can be
used to navigate on the visualized data, as well as to
perform actions on the data items (e.g. play or se-
lect a song), a microphone to capture spoken input,
and speakers to give audio output. Since we do not
develop the hardware, we simulate the PDA using
a 3D model on a computer screen, and the buttons
</bodyText>
<footnote confidence="0.996255">
1http://www.miamm.org
</footnote>
<figureCaption confidence="0.979897">
Figure 1: The PDA simulator with the terrain visu-
alization of the database
</figureCaption>
<bodyText confidence="0.975294888888889">
by means of Phantom devices that allow the user to
touch and manipulate virtual objects.
In the rest of this paper, we will first give an
overview of the visualization metaphors, the MI-
AMM architecture, and a short description of its
interface language. Then we will demonstrate its
functionality using an example dialogue. For more
details on the MIAMM system and its components
see (Reithinger et al., 2004).
</bodyText>
<sectionHeader confidence="0.980832" genericHeader="method">
2 Visualization metaphors
</sectionHeader>
<bodyText confidence="0.999804125">
The information from the database is presented on
the device using metaphors of real world objects
(cf. conceptual spaces (G¨ardenfors, 2000)) so as to
provide an intuitive handling of abstract concepts.
The lexicon metaphor, shown in figure 2 to the left,
presents the items alphabetically ordered in a rotary
card file. Each card represents one album and con-
tains detailed background information. The time-
</bodyText>
<footnote confidence="0.989367">
2http://www.sensable.com
</footnote>
<figureCaption confidence="0.998418">
Figure 2: Visualizations
</figureCaption>
<bodyText confidence="0.999957842105263">
line visualization shows the items in chronologi-
cal order, on a “rubber” band that can be stretched
to get a more detailed view. The wheel metaphor
presents the items as a list on a conveyor belt, which
can be easily and quickly rotated. Finally, the ter-
rain metaphor (see figure 1) visualizes the entire
database. The rendering is based on a three layer
type hierarchy, with genre, sub-genre and title lay-
ers. Each node of the hierarchy is represented as
a circle containing its daughter nodes. Similarities
between the items are computed from the genre and
mood information in the database and mapped to
interaction forces in a physical model that groups
similar items together on the terrain. Since usually
albums are assigned more than one genre, they can
be contained in different circles and therefore be re-
dundantly represented on the terrain. This redun-
dancy is made clear by lines connecting the different
instances of the same item.
</bodyText>
<sectionHeader confidence="0.996428" genericHeader="method">
3 The MIAMM prototype
</sectionHeader>
<bodyText confidence="0.982724666666667">
The MIAMM system uses the standard architec-
ture for dialogue systems with analysis and gener-
ation layers, interaction management and applica-
tion interface (see figure 3). To minimize the reac-
tion delay of haptic feedback, the visual-haptic in-
teraction component is decoupled from other more
time-consuming reasoning processes. The German
experimental prototype3 incorporates the following
3There are also French and English versions of the system.
The modular architecture facilitates the replacement of the lan-
guage dependent modules.
components, some of which were reused from other
projects (semantic parser and action planning): a
speaker independent, continuous speech recognizer
converts the spoken input in a word lattice; it uses
a 500 word vocabulary, and was trained on a auto-
matically generated corpus. A template based se-
mantic parser for German, see (Engel, 2004), inter-
prets this word lattice semantically. The multimodal
fusion module maintains the dialogue history and
handles anaphoric expressions and quantification.
The action planner, an adapted and enhanced ver-
sion of (L¨ockelt, 2004), uses non-linear regression
planning and the notion of communicative games
to trigger and control system actions. The visual-
haptic interaction manager selects the appropriate
visualization metaphor based on data characteris-
tics, and maintains the visualization history. Finally,
the domain model provides access to the MYSQL
database, which contains 7257 records with 85722
songs by 667 artists. Speech output is done by
speech prompts, both for spoken and for written out-
put. The prototype also includes a MP3 Player to
play the music and speech output files. The demon-
stration system requires a Linux based PC for the
major parts of the modules written in Java and C++,
and a Windows NT computer for visualization and
haptics. The integration environment is based on the
standard Simple Object Access Protocol SOAP4 for
information exchange in a distributed environment.
The communication between the modules uses a
declarative, XML-schema based representation lan-
</bodyText>
<footnote confidence="0.962306">
4http://www.w3.org/TR/SOAP/
</footnote>
<figureCaption confidence="0.999612">
Figure 3: MIAMM architecture
</figureCaption>
<figure confidence="0.998669647058824">
Audio Input
Semantic Representation
Continuous Speech
Recognizer
Semantic Interpretation
Speech Generation
Request
Speech prompts
Audio Output
Player Request
MP3 Player
Music files
Visualization Request
Visualization Status
Domain Model Query Response
Database
Query
Response
Domain Model
Microphone
Speaker
Display
Haptic Device
Database
Visual−Haptic Interaction
Haptic Processor
Visualization
Visual−Haptic Interpretation
Visual−Haptic Generation
DIALOGUE MANAGER
Multimodal Fusion
Goal
Representation
Action Planner
</figure>
<bodyText confidence="0.995638266666667">
guage called MMIL (Romary and Bunt, 2002). This
interface specification accounts for the incremental
integration of multimodal data to achieve a full un-
derstanding of the multimodal acts within the sys-
tem. Therefore, it is flexible enough to handle the
various types of information processed and gener-
ated by the different modules. It is also independent
from any theoretical framework, and extensible so
that further developments can be incorporated. Fur-
thermore it is compatible with existing standardiza-
tion initiatives so that it can be the source of fu-
ture standardizing activities in the fields. Figure 4
shows a sample of MMIL representing the output of
the speech interpretation module for the user’s ut-
terance “Give me rock”.
</bodyText>
<sectionHeader confidence="0.977806" genericHeader="method">
4 An example
</sectionHeader>
<bodyText confidence="0.999994">
To sketch the functionality of the running prototype
we will use a sample interaction, showing the user’s
actions, the system’s textual feedback on the screen
and finally the displayed information. Some of the
dialogue capabilities of the MIAMM system in this
example are, e.g. search history (S2), relaxation
of queries (S3b), and anaphora resolution (S5). At
any moment of the interaction the user is allowed to
navigate on the visualized items, zoom in and out
for details, or change the visualization metaphor.
</bodyText>
<footnote confidence="0.9539179">
U1: Give me rock
S1a: I am looking for rock
S1b: displays a terrain with rock albums
U2: I want something calm
S2a: I am looking for calm rock
S2b: displays list of calm rock albums
U3: I want something from the 30’s
S3a: I am looking for calm rock
5The data categories are expressed in a RDF format com-
patible with ISO 11179-3
</footnote>
<figure confidence="0.4007362">
1930-1939
S3b: I could only find albums of
the adjacent years
displays list of calm rock albums of the 40’s
U4: What about the S0’s
S4a: I am looking for calm rock
1950-1959
S4b: displays a map with rock albums
U5: selects ALBUM with the haptic buttons
Play this one
</figure>
<figureCaption confidence="0.2297885">
S5a: Playing ALBUM
S5b: MP3 player starts
</figureCaption>
<bodyText confidence="0.99999488">
We will show the processing details on the basis
of the first utterance in the sample interaction Give
me rock. The speech recognizer converts the spo-
ken input in a word graph in MPEG7. The semantic
parser analyzes this graph and interprets it semanti-
cally. The semantic representation consists, in this
example, of a speak and a display event, with
two participants, the user and music with con-
straints on its genre (see figure 4).
The multimodal fusion module receives this
representation, updates the dialogue context, and
passes it on to the action planner, which defines the
next goal on the basis of the propositional content
of the top event (in the example event id1) and
its object (in the example participant id3). In this
case the user’s goal cannot be directly achieved be-
cause the object to display is still unresolved. The
action planner has to initiate a database query to ac-
quire the required information. It uses the constraint
on the genre of the requested object to produce a
database query for the domain model and a feed-
back request for the visual-haptic interaction mod-
ule. This feedback message (S1a in the example)
is sent to the user while the database query is being
done, providing thus implicit grounding. The do-
</bodyText>
<figure confidence="0.9994354">
&lt;component&gt;
&lt;event id=&amp;quot;id0&amp;quot;&gt;
&lt;evtType&gt;speak&lt;/evtType&gt;
&lt;speaker&gt;user&lt;/speaker&gt;
&lt;addressee&gt;system&lt;/addressee&gt;
&lt;dialogueAct&gt;request&lt;/dialogueAct&gt;
&lt;/event&gt;
&lt;event id=&amp;quot;id1&amp;quot;&gt;
&lt;evtType&gt;display&lt;/evtType&gt;
&lt;/event&gt;
&lt;participant id=&amp;quot;id2&amp;quot;&gt;
&lt;objType&gt;user&lt;/objType&gt;
&lt;refType&gt;1PPDeixis&lt;/refType&gt;
&lt;refStatus&gt;pending&lt;/refStatus&gt;
&lt;/participant&gt;
&lt;participant id=&amp;quot;id3&amp;quot;&gt;
&lt;objType&gt;music&lt;/objType&gt;
&lt;genre&gt;rock&lt;/genre&gt;
&lt;refType&gt;indefinite&lt;/refType&gt;
&lt;refStatus&gt;pending&lt;/refStatus&gt;
&lt;/participant&gt;
&lt;relation
source=&amp;quot;id3&amp;quot;
target=&amp;quot;id1&amp;quot;
type=&amp;quot;object&amp;quot;/&gt;
&lt;relation
source=&amp;quot;id1&amp;quot;
target=&amp;quot;id0&amp;quot;
type=&amp;quot;propContent&amp;quot;/&gt;
&lt;/component&gt;
</figure>
<figureCaption confidence="0.999988">
Figure 4: MMIL sample
</figureCaption>
<bodyText confidence="0.9999035">
main model sends the result back to the action plan-
ner who inserts the data in a visualization request.
The visual-haptic interaction module computes
the most suitable visualization for this data set, and
sends the request to the visualization module to ren-
der it. This component also reports the actual vi-
sualization status to the multimodal fusion module.
This report is used to update the dialogue context,
that is needed for reference resolution. The user can
now use the haptic buttons to navigate on the search
results, select a title to be played or continue search-
ing.
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999810888888889">
The MIAMM final prototype combines speech with
new techniques for haptic interaction and data visu-
alization to facilitate access to multimedia databases
on small handheld devices. The final evaluation
of the system supports our initial hypothesis that
users prefer language to select information and hap-
tics to navigate in the search space. The visualiza-
tions proved to be intuitive (van Esch and Cremers,
2004).
</bodyText>
<sectionHeader confidence="0.998647" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995823">
This work was sponsored by the European Union
(IST-2000-29487). Thanks are due to our project
partners: Loria (F), Sony Europe (D), Canon (UK)
and TNO (NL) .
</bodyText>
<sectionHeader confidence="0.998184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.967874576923077">
Ralf Engel. 2004. Natural language understanding.
In Wolfgang Wahlster, editor, SmartKom - Foun-
dations of Multi-modal Dialogue Systems, Cog-
nitive Technologies. Springer Verlag (in Press).
Peter G¨ardenfors. 2000. Conceptual Spaces. MIT
Press.
Markus L¨ockelt. 2004. Action planning. In Wolf-
gang Wahlster, editor, SmartKom - Founda-
tions of Multi-modal Dialogue Systems, Cogni-
tive Technologies. Springer Verlag (in Press).
Norbert Reithinger, Dirk Fedeler, Ashwani Kumar,
Christoph Lauer, Elsa Pecourt, and Laurent Ro-
mary. 2004. Miamm - a multimodal dialogue
system using haptics. In Jan van Kuppevelt, Laila
Dybkjaer, and Niels Ole Bersen, editors, Natu-
ral, Intelligent and Effective Interaction in Multi-
modal Dialogue Systems. Kluwer Academic Pub-
lications.
Laurent Romary and Harry Bunt. 2002. Towards
multimodal content representation. In Proceed-
ings of LREC 2002, Workshop on International
Standards of Terminology and Linguistic Re-
sources Management, Las Palmas.
Myra P. van Esch and Anita H. M. Cremers. 2004.
User evaluation. MIAMM Deliverable D1.6.
,
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942034">
<title confidence="0.999887">Multimodal Database Access on Handheld Devices</title>
<author confidence="0.999594">Elsa Pecourt</author>
<author confidence="0.999594">Norbert Reithinger</author>
<affiliation confidence="0.997594">DFKI GmbH</affiliation>
<address confidence="0.9710955">Stuhlsatzenhausenweg3 D-66123 Saarbr¨ucken, Germany</address>
<abstract confidence="0.999656666666667">We present the final MIAMM system, a multimodal dialogue system that employs speech, haptic interaction and novel techniques of information visualization to allow a natural and fast access to large multimedia databases on small handheld devices.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ralf Engel</author>
</authors>
<title>Natural language understanding.</title>
<date>2004</date>
<booktitle>SmartKom - Foundations of Multi-modal Dialogue Systems, Cognitive Technologies.</booktitle>
<editor>In Wolfgang Wahlster, editor,</editor>
<publisher>Springer Verlag (in Press).</publisher>
<contexts>
<context position="4848" citStr="Engel, 2004" startWordPosition="766" endWordPosition="767"> decoupled from other more time-consuming reasoning processes. The German experimental prototype3 incorporates the following 3There are also French and English versions of the system. The modular architecture facilitates the replacement of the language dependent modules. components, some of which were reused from other projects (semantic parser and action planning): a speaker independent, continuous speech recognizer converts the spoken input in a word lattice; it uses a 500 word vocabulary, and was trained on a automatically generated corpus. A template based semantic parser for German, see (Engel, 2004), interprets this word lattice semantically. The multimodal fusion module maintains the dialogue history and handles anaphoric expressions and quantification. The action planner, an adapted and enhanced version of (L¨ockelt, 2004), uses non-linear regression planning and the notion of communicative games to trigger and control system actions. The visualhaptic interaction manager selects the appropriate visualization metaphor based on data characteristics, and maintains the visualization history. Finally, the domain model provides access to the MYSQL database, which contains 7257 records with 8</context>
</contexts>
<marker>Engel, 2004</marker>
<rawString>Ralf Engel. 2004. Natural language understanding. In Wolfgang Wahlster, editor, SmartKom - Foundations of Multi-modal Dialogue Systems, Cognitive Technologies. Springer Verlag (in Press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter G¨ardenfors</author>
</authors>
<title>Conceptual Spaces.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<marker>G¨ardenfors, 2000</marker>
<rawString>Peter G¨ardenfors. 2000. Conceptual Spaces. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus L¨ockelt</author>
</authors>
<title>Action planning.</title>
<date>2004</date>
<booktitle>SmartKom - Foundations of Multi-modal Dialogue Systems, Cognitive Technologies.</booktitle>
<editor>In Wolfgang Wahlster, editor,</editor>
<publisher>Springer Verlag (in Press).</publisher>
<marker>L¨ockelt, 2004</marker>
<rawString>Markus L¨ockelt. 2004. Action planning. In Wolfgang Wahlster, editor, SmartKom - Foundations of Multi-modal Dialogue Systems, Cognitive Technologies. Springer Verlag (in Press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Reithinger</author>
<author>Dirk Fedeler</author>
<author>Ashwani Kumar</author>
<author>Christoph Lauer</author>
<author>Elsa Pecourt</author>
<author>Laurent Romary</author>
</authors>
<title>Miamm - a multimodal dialogue system using haptics.</title>
<date>2004</date>
<booktitle>Natural, Intelligent and Effective Interaction in Multimodal Dialogue Systems.</booktitle>
<editor>In Jan van Kuppevelt, Laila Dybkjaer, and Niels Ole Bersen, editors,</editor>
<publisher>Kluwer Academic Publications.</publisher>
<contexts>
<context position="2522" citStr="Reithinger et al., 2004" startWordPosition="399" endWordPosition="402">put. Since we do not develop the hardware, we simulate the PDA using a 3D model on a computer screen, and the buttons 1http://www.miamm.org Figure 1: The PDA simulator with the terrain visualization of the database by means of Phantom devices that allow the user to touch and manipulate virtual objects. In the rest of this paper, we will first give an overview of the visualization metaphors, the MIAMM architecture, and a short description of its interface language. Then we will demonstrate its functionality using an example dialogue. For more details on the MIAMM system and its components see (Reithinger et al., 2004). 2 Visualization metaphors The information from the database is presented on the device using metaphors of real world objects (cf. conceptual spaces (G¨ardenfors, 2000)) so as to provide an intuitive handling of abstract concepts. The lexicon metaphor, shown in figure 2 to the left, presents the items alphabetically ordered in a rotary card file. Each card represents one album and contains detailed background information. The time2http://www.sensable.com Figure 2: Visualizations line visualization shows the items in chronological order, on a “rubber” band that can be stretched to get a more d</context>
</contexts>
<marker>Reithinger, Fedeler, Kumar, Lauer, Pecourt, Romary, 2004</marker>
<rawString>Norbert Reithinger, Dirk Fedeler, Ashwani Kumar, Christoph Lauer, Elsa Pecourt, and Laurent Romary. 2004. Miamm - a multimodal dialogue system using haptics. In Jan van Kuppevelt, Laila Dybkjaer, and Niels Ole Bersen, editors, Natural, Intelligent and Effective Interaction in Multimodal Dialogue Systems. Kluwer Academic Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Romary</author>
<author>Harry Bunt</author>
</authors>
<title>Towards multimodal content representation.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC 2002, Workshop on International Standards of Terminology and Linguistic Resources Management,</booktitle>
<location>Las Palmas.</location>
<contexts>
<context position="6663" citStr="Romary and Bunt, 2002" startWordPosition="1014" endWordPosition="1017"> lan4http://www.w3.org/TR/SOAP/ Figure 3: MIAMM architecture Audio Input Semantic Representation Continuous Speech Recognizer Semantic Interpretation Speech Generation Request Speech prompts Audio Output Player Request MP3 Player Music files Visualization Request Visualization Status Domain Model Query Response Database Query Response Domain Model Microphone Speaker Display Haptic Device Database Visual−Haptic Interaction Haptic Processor Visualization Visual−Haptic Interpretation Visual−Haptic Generation DIALOGUE MANAGER Multimodal Fusion Goal Representation Action Planner guage called MMIL (Romary and Bunt, 2002). This interface specification accounts for the incremental integration of multimodal data to achieve a full understanding of the multimodal acts within the system. Therefore, it is flexible enough to handle the various types of information processed and generated by the different modules. It is also independent from any theoretical framework, and extensible so that further developments can be incorporated. Furthermore it is compatible with existing standardization initiatives so that it can be the source of future standardizing activities in the fields. Figure 4 shows a sample of MMIL represe</context>
</contexts>
<marker>Romary, Bunt, 2002</marker>
<rawString>Laurent Romary and Harry Bunt. 2002. Towards multimodal content representation. In Proceedings of LREC 2002, Workshop on International Standards of Terminology and Linguistic Resources Management, Las Palmas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myra P van Esch</author>
<author>Anita H M Cremers</author>
</authors>
<date>2004</date>
<booktitle>User evaluation. MIAMM Deliverable D1.6.</booktitle>
<marker>van Esch, Cremers, 2004</marker>
<rawString>Myra P. van Esch and Anita H. M. Cremers. 2004. User evaluation. MIAMM Deliverable D1.6.</rawString>
</citation>
<citation valid="false">
<marker></marker>
<rawString>,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>