<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008966">
<title confidence="0.9978965">
Bridging Morpho-Syntactic Gap between Source and Target Sentences for
English-Korean Statistical Machine Translation
</title>
<author confidence="0.997925">
Gumwon Hong, Seung-Wook Lee and Hae-Chang Rim
</author>
<affiliation confidence="0.9986905">
Department of Computer Science &amp; Engineering
Korea University
</affiliation>
<address confidence="0.92838">
Seoul 136-713, Korea
</address>
<email confidence="0.998941">
{gwhong,swlee,rim}@nlp.korea.ac.kr
</email>
<sectionHeader confidence="0.993592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995879722222223">
Often, Statistical Machine Translation
(SMT) between English and Korean suf-
fers from null alignment. Previous studies
have attempted to resolve this problem by
removing unnecessary function words, or
by reordering source sentences. However,
the removal of function words can cause
a serious loss in information. In this pa-
per, we present a possible method of bridg-
ing the morpho-syntactic gap for English-
Korean SMT. In particular, the proposed
method tries to transform a source sen-
tence by inserting pseudo words, and by
reordering the sentence in such a way
that both sentences have a similar length
and word order. The proposed method
achieves 2.4 increase in BLEU score over
baseline phrase-based system.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990600724137931">
Phrase-based SMT models have performed rea-
sonably well on languages where the syntactic
structures are very similar, including languages
such as French and English. However, Collins et
al. (2005) demonstrated that phrase-based models
have limited potential when applied to languages
that have a relatively different word order; such is
the case between German and English. They pro-
posed a clause restructuring method for reordering
German sentences in order to resemble the order
of English sentences. By modifying the source
sentence structure into the target sentence struc-
ture, they argued that they could solve the de-
coding problem by use of completely monotonic
translation.
The translation from English to Korean can be
more difficult than the translation of other lan-
guage pairs for the following reasons: First, Ko-
rean is language isolate: that is, it has little ge-
nealogical relations with other natural languages.1
Second, the word order in Korean is relatively
free because the functional morphemes, case par-
ticles and word endings, play the role as a gram-
matical information marker. Thus, the functional
morphemes, rather than the word order, determine
whether a word is a subject or an object. Third,
Korean is an agglutinative language, in which a
word is generally composed of at least one con-
tent morpheme and zero or more functional mor-
phemes. Some Korean words are highly synthetic
with complex inflections, and this phenomenon
produces a very large vocabulary and causes data-
sparseness in performing word-based alignment.
To mitigate this problem, many systems tokenize
Korean sentences by the morpheme unit before
training and decoding the sentences.
When analyzing English-Korean translation
with MOSES (Koehn et al., 2007), we found
high ratio of null alignment. In figure 1,
‘o��(eun)’, ‘91(eui)’, ‘a+(ha)’, ‘l--(n)’, ‘-71(ji)’ and
‘Lr+(neunda)’ are not linked to any word in the
English sentence. In many cases, these words are
function words that are attached to preceding con-
tent words. Sometimes they can be linked (in-
correctly) to their head’s corresponding words, or
they can be linked to totally different words with
respect to their meaning.
In the preliminary experiment using GIZA++
(Och and Ney, 2003) with grow-diag-final heuris-
tic, we found that about 25% of words in Ko-
rean sentences and 21% of English sentences fail
to align. This null alignment ratio is relatively
high in comparison to the French-English align-
ment, in which about 9% of French sentences and
6% of English sentences are not aligned. Due to
this null alignment, the estimation of translation
probabilities for Korean function words may be in-
complete; a system would perform mainly based
</bodyText>
<footnote confidence="0.940867">
1Some may consider it an Altaic language family.
</footnote>
<page confidence="0.949097">
233
</page>
<note confidence="0.9809445">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 233–236,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.9999855">
Figure 1: An example of null alignment
Figure 2: An example of ideal alignment
</figureCaption>
<bodyText confidence="0.999836631578947">
on content-words, which can deteriorate the per-
formance of candidate generation during decod-
ing. Also, without generating appropriate function
words, the quality of the translation will undoubt-
edly degrade.
In this paper, we present a preprocessing
method for both training and decoding in English-
Korean SMT. In particular, we transform a source
language sentence by inserting pseudo words and
syntactically reordering it to form a target sen-
tence structure in hopes of reducing the morpho-
syntactic discrepancies between two languages.
Ultimately, we expect an ideal alignment, as
shown in Figure 2. Our results show that the
combined pseudo word insertion and syntactic re-
ordering method reduces null alignment ratio and
makes both sentences have similar length. We re-
port results showing that the proposed method can
improve the translation quality.
</bodyText>
<sectionHeader confidence="0.94451" genericHeader="method">
2 Pseudo Word Insertion
</sectionHeader>
<bodyText confidence="0.999971414634147">
Lee et al. (2006) find that function words in Ko-
rean sentences are not aligned to any English
words, and can simply and easily be removed by
referring to their POS information. The unaligned
words are case particles, final endings, and auxil-
iary particles, and they call these words “untrans-
latable words”.
The method can be effective for Korean-English
SMT where target language does not have corre-
sponding function words, but it has a limitation
in application to the English-Korean SMT because
removing functional morphemes can cause a seri-
ous loss in information. Technically, the function
words they ignored are not ‘untranslatable’ but are
‘unalignable’. Therefore, instead of removing the
function words, we decide to insert some pseudo
words into an English sentence in order to align
them with potential Korean function words and
make the length of both sentences similar.
To insert the pseudo words, we need to decide:
(1) the kinds of words to insert, and (2) the loca-
tion to insert the words. Because we expect that a
pseudo word corresponds to any Korean function
word which decides a syntactic role of its head,
it is reasonable to utilize a dependency relation of
English. Thus, given an English sentence, the can-
didate pseudo words are generated by the follow-
ing methods: First, we parse the English sentence
using Stanford dependency parser (de Marneffe et
al., 2006). Then, we select appropriate typed de-
pendency relations between pairs of words which
are able to generate Korean function words. We
found that 21 out of 48 dependency relations can
be directly used as pseudo words. Among them,
some relations provide very strong cue of case par-
ticles when inserted as pseudo words.
For example, from the following sentence, we
can select as pseudo words a subjective particle
&lt;NS&gt; and an objective particle &lt;DO&gt;, and in-
sert them after the corresponding dependents Eu-
gene and guitar respectively.
</bodyText>
<equation confidence="0.256807">
nominal subject(play, Eugene)
direct object(play, guitar)
Eugene &lt;NS&gt; can ’t play the guitar &lt;DO&gt; well.
</equation>
<bodyText confidence="0.770348">
In a preliminary experiment on word alignment,
</bodyText>
<page confidence="0.94608">
234
</page>
<table confidence="0.998569555555556">
nominal subject =(neun), null, s1(i)
direct object z(eul), null, *(reul)
clausal subject =(neun), null, s1(i)
temporal modifier \•(neun), null, š¸h.(oneul)
adj complement null, o}(ah), -&amp;J(ha)
agent null, \•(e), 71(ga)
numeric modifier null, 91(eui), &gt;h(gae)
adj modifier null, \•(e), 71(ga)
particle modifier null, L(n), ÷�(doe)
</table>
<figureCaption confidence="0.76665525">
Figure 3: Selected dependency relations and their
aligned function words in training data (shown
the top 3 results in descending order of alignment
probability)
</figureCaption>
<bodyText confidence="0.999560857142857">
we observe that inserting too many pseudo words
can, on the contrary, increase null alignment of
English sentence. Thus we filtered some pseudo
words according to their respective null alignment
probabilities. Figure 3 shows the top 9 selected
dependency relations (actually used in the experi-
ment) and the aligned Korean function words.
</bodyText>
<sectionHeader confidence="0.985606" genericHeader="method">
3 Syntactic Reordering
</sectionHeader>
<bodyText confidence="0.999878391304348">
Many approaches use syntactic reordering in the
preprocessing step for SMT systems (Collins et
al., 2005; Xia and McCord, 2004; Zwarts and
Dras, 2007). Some reordering approaches have
given significant improvements in performance for
translation from French to English (Xia and Mc-
Cord, 2004) and from German to English (Collins
et al., 2005). However, on the contrary, Lee et al.
(2006) reported that the reordering of Korean for
Korean-English translation degraded the perfor-
mance. They presumed that the performance de-
crease might come from low parsing performance
for conversational domain.
We believe that it is very important to consider
the structural properties of Korean for reordering
English sentences. Though the word order of a
Korean sentence is relatively free, Korean gener-
ally observes the SOV word order, and it is a head-
final language. Consequently, an object precedes a
predicate, and all dependents precede their heads.
We use both a structured parse tree and de-
pendency relations to extract following reordering
rules.
</bodyText>
<listItem confidence="0.746563">
• Verb final: In any verb phrase, move verbal
head to the end of the phrase. Infinitive verbs or
verb particles are moved together.
</listItem>
<bodyText confidence="0.868047666666667">
He (likes ((to play) (the piano))) (1)
He (likes ((the piano) (to play)))(2)
He (((the piano) (to play)) likes)(3)
</bodyText>
<listItem confidence="0.982064">
• Adjective final: In adjective phrase, move ad-
jective head to the end of the phrase especially if
followed by PP or S/SBAR.
</listItem>
<bodyText confidence="0.969453">
It is ((difficult) to reorder) (1)
It is (to reorder (difficult))(2)
</bodyText>
<listItem confidence="0.977515">
• Antecedent final: In noun phrase containing
relative clause, move preceding NP to the end of a
relative clause.
((rules) that are used for reordering)(1)
(that are used for reordering (rules))(2)
• Negation final: Move negative markers to di-
rectly follow verbal head.
</listItem>
<bodyText confidence="0.378158">
(can ’t) ((play) the guitar) (1)
(can ’t) (the guitar (play)) (2)
(the guitar (play)) (can ’t) (3)
</bodyText>
<sectionHeader confidence="0.998609" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998934">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999956115384615">
The baseline of our approach is a statisti-
cal phrase-based system which is trained using
MOSES (Koehn et al., 2007). We collect bilin-
gual texts from the Web and combine them with
the Sejong parallel corpora 2. About 300K pair of
sentences are collected from the major bilingual
news broadcasting sites. We also collect around
1M monolingual sentences from the sites to train
Korean language models. The best performing
language model is 5-gram order with Kneser-Ney
smoothing.
For sentence level alignment, we modified the
Champollion toolkit for English-Korean pair (Ma,
2006). We randomly selected 5,000 sentence pairs
from Sejong corpora, of which 1,500 were used
for a tuning set for minimum error rate training,
and another 1,500 for development set for analy-
sis experiment. We report testing results on the
remaining 2,000 sentence pairs for the evaluation.
Korean sentences are tokenized by the morpho-
logical analyzer (Lee and Rim, 2004). For English
sentence preprocessing, we use the Stanford parser
with output of typed dependency relations. We
then applied the pseudo word insertion and four
reordering rules described in the previous section
to the parse tree of each sentence.
</bodyText>
<footnote confidence="0.984936333333333">
2The English-Korean parallel corpora open for research
purpose which contain about 60,000 sentence pairs. See
http://www.sejong.or.kr/english.php for more information
</footnote>
<page confidence="0.989392">
235
</page>
<table confidence="0.9995272">
BLEU(gain) Length Ratio
Baseline 18.03(+0.00) 0.78
+PWI only 18.62(+0.59) 0.91
+Reorder only 19.92(+1.89) 0.78
+PWI&amp;Reorder 20.42(+2.39) 0.91
</table>
<tableCaption confidence="0.9955935">
Table 1: BLEU score and sentence length ratio for
each method
</tableCaption>
<table confidence="0.99990225">
Baseline +PWI +Reorder +P&amp;R
src-null 20.5 21.4 19.1 20.9
tgt-null 25.4 22.3 23.4 20.8
all-null 23.3 21.9 21.5 20.8
</table>
<tableCaption confidence="0.8599945">
Table 2: Null alignment ratio (%) for each method
(all-null is calculated on the whole training data)
</tableCaption>
<bodyText confidence="0.998109222222222">
word alignment ratio between two languages can
be a good way to measure the quality of transla-
tion.
When evaluating the proposed approach using
within MOSES, the combined pseudo word inser-
tion and syntactic reordering method outperforms
the other methods. The result proves that the pro-
posed method can be used as a useful technique
for English-Korean machine translation.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="evaluation">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999904">
This work was supported by Microsoft Research
Asia. Any opinions, findings, and conclusions or
recommendations expressed above are those of the
authors and do not necessarily reflect the views of
the sponsor.
</bodyText>
<subsectionHeader confidence="0.921912">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999987263157895">
The BLEU scores are reported in Table 1. Length
ratio indicates the average sentence length ratio
between source sentences and target sentences.
The largest gain (+2.39) is achieved when the
combined pseudo word insertion (PWI) and word
reordering is performed.
There could be reasons why the proposed ap-
proach is effective over baseline approach. Pre-
sumably, transforming to similar length and word
order contributes to lower the distortion and fertil-
ity parameter values. Table 2 analyzes the effect
of individual techniques in terms of the null align-
ment ratio. We discover that the alignment ratio
can be a good way to measure the relation between
the quality of word alignment and the quality of
translation. As shown in Table 2, the BLEU score
tends to increase as the all-null ratio decreases. In-
terestingly, reordering achieves the smallest null
alignment ratio for source language.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999955">
In this paper, we presented a novel approach to
preprocessing English-Korean SMT. The morpho-
syntactic discrepancy between English and Korean
causes a serious null alignment problem.
The main contributions of this paper are the fol-
lowing: 1) we devise a new preprocessing method
for English-Korean SMT by transforming a source
sentence to be much closer to a target sentence in
terms of sentence length and word order. 2) we
discover that the proposed method can reduce the
null alignment problem, and consequently the null
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999860666666667">
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proc. ofACL.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. ofLREC.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. ofACL Demonstration session.
Do-Gil Lee and Hae-Chang Rim. 2004. Part-of-speech
tagging considering surface form for an agglutina-
tive language. In Proc. ofACL.
Jonghoon Lee, Donghyeon Lee, and Gary Geun-
bae Lee. 2006. Improving phrase-based korean-
english statistical machine translation. In Proc. of
Interspeech-ICSLP.
Xiaoyi Ma. 2006. Champollion: A robust parallel text
sentence aligner. In Proc. ofLREC.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proc. of COLING.
Simon Zwarts and Mark Dras. 2007. Syntax-based
word reordering in phrase-based statistical machine
translation: Why does it work? In Proc. of MT-
Summit XI.
</reference>
<page confidence="0.998545">
236
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950363">
<title confidence="0.9986615">Bridging Morpho-Syntactic Gap between Source and Target Sentences for English-Korean Statistical Machine Translation</title>
<author confidence="0.99775">Gumwon Hong</author>
<author confidence="0.99775">Seung-Wook Lee</author>
<author confidence="0.99775">Hae-Chang Rim</author>
<affiliation confidence="0.999681">Department of Computer Science &amp; Engineering Korea University</affiliation>
<address confidence="0.968038">Seoul 136-713, Korea</address>
<abstract confidence="0.999255842105263">Often, Statistical Machine Translation (SMT) between English and Korean suffers from null alignment. Previous studies have attempted to resolve this problem by removing unnecessary function words, or by reordering source sentences. However, the removal of function words can cause a serious loss in information. In this paper, we present a possible method of bridging the morpho-syntactic gap for English- Korean SMT. In particular, the proposed method tries to transform a source sentence by inserting pseudo words, and by reordering the sentence in such a way that both sentences have a similar length and word order. The proposed method achieves 2.4 increase in BLEU score over baseline phrase-based system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofACL.</booktitle>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill Maccartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proc. ofLREC.</booktitle>
<marker>de Marneffe, Maccartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proc. ofLREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. In</title>
<date>2007</date>
<booktitle>Proc. ofACL Demonstration session.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra</location>
<contexts>
<context position="2758" citStr="Koehn et al., 2007" startWordPosition="414" endWordPosition="417">, rather than the word order, determine whether a word is a subject or an object. Third, Korean is an agglutinative language, in which a word is generally composed of at least one content morpheme and zero or more functional morphemes. Some Korean words are highly synthetic with complex inflections, and this phenomenon produces a very large vocabulary and causes datasparseness in performing word-based alignment. To mitigate this problem, many systems tokenize Korean sentences by the morpheme unit before training and decoding the sentences. When analyzing English-Korean translation with MOSES (Koehn et al., 2007), we found high ratio of null alignment. In figure 1, ‘o��(eun)’, ‘91(eui)’, ‘a+(ha)’, ‘l--(n)’, ‘-71(ji)’ and ‘Lr+(neunda)’ are not linked to any word in the English sentence. In many cases, these words are function words that are attached to preceding content words. Sometimes they can be linked (incorrectly) to their head’s corresponding words, or they can be linked to totally different words with respect to their meaning. In the preliminary experiment using GIZA++ (Och and Ney, 2003) with grow-diag-final heuristic, we found that about 25% of words in Korean sentences and 21% of English sent</context>
<context position="9768" citStr="Koehn et al., 2007" startWordPosition="1532" endWordPosition="1535">if followed by PP or S/SBAR. It is ((difficult) to reorder) (1) It is (to reorder (difficult))(2) • Antecedent final: In noun phrase containing relative clause, move preceding NP to the end of a relative clause. ((rules) that are used for reordering)(1) (that are used for reordering (rules))(2) • Negation final: Move negative markers to directly follow verbal head. (can ’t) ((play) the guitar) (1) (can ’t) (the guitar (play)) (2) (the guitar (play)) (can ’t) (3) 4 Experiments 4.1 Experimental Setup The baseline of our approach is a statistical phrase-based system which is trained using MOSES (Koehn et al., 2007). We collect bilingual texts from the Web and combine them with the Sejong parallel corpora 2. About 300K pair of sentences are collected from the major bilingual news broadcasting sites. We also collect around 1M monolingual sentences from the sites to train Korean language models. The best performing language model is 5-gram order with Kneser-Ney smoothing. For sentence level alignment, we modified the Champollion toolkit for English-Korean pair (Ma, 2006). We randomly selected 5,000 sentence pairs from Sejong corpora, of which 1,500 were used for a tuning set for minimum error rate training</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proc. ofACL Demonstration session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Do-Gil Lee</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Part-of-speech tagging considering surface form for an agglutinative language. In</title>
<date>2004</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="10597" citStr="Lee and Rim, 2004" startWordPosition="1662" endWordPosition="1665">1M monolingual sentences from the sites to train Korean language models. The best performing language model is 5-gram order with Kneser-Ney smoothing. For sentence level alignment, we modified the Champollion toolkit for English-Korean pair (Ma, 2006). We randomly selected 5,000 sentence pairs from Sejong corpora, of which 1,500 were used for a tuning set for minimum error rate training, and another 1,500 for development set for analysis experiment. We report testing results on the remaining 2,000 sentence pairs for the evaluation. Korean sentences are tokenized by the morphological analyzer (Lee and Rim, 2004). For English sentence preprocessing, we use the Stanford parser with output of typed dependency relations. We then applied the pseudo word insertion and four reordering rules described in the previous section to the parse tree of each sentence. 2The English-Korean parallel corpora open for research purpose which contain about 60,000 sentence pairs. See http://www.sejong.or.kr/english.php for more information 235 BLEU(gain) Length Ratio Baseline 18.03(+0.00) 0.78 +PWI only 18.62(+0.59) 0.91 +Reorder only 19.92(+1.89) 0.78 +PWI&amp;Reorder 20.42(+2.39) 0.91 Table 1: BLEU score and sentence length r</context>
</contexts>
<marker>Lee, Rim, 2004</marker>
<rawString>Do-Gil Lee and Hae-Chang Rim. 2004. Part-of-speech tagging considering surface form for an agglutinative language. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonghoon Lee</author>
<author>Donghyeon Lee</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Improving phrase-based koreanenglish statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of Interspeech-ICSLP.</booktitle>
<contexts>
<context position="4862" citStr="Lee et al. (2006)" startWordPosition="749" endWordPosition="752">d decoding in EnglishKorean SMT. In particular, we transform a source language sentence by inserting pseudo words and syntactically reordering it to form a target sentence structure in hopes of reducing the morphosyntactic discrepancies between two languages. Ultimately, we expect an ideal alignment, as shown in Figure 2. Our results show that the combined pseudo word insertion and syntactic reordering method reduces null alignment ratio and makes both sentences have similar length. We report results showing that the proposed method can improve the translation quality. 2 Pseudo Word Insertion Lee et al. (2006) find that function words in Korean sentences are not aligned to any English words, and can simply and easily be removed by referring to their POS information. The unaligned words are case particles, final endings, and auxiliary particles, and they call these words “untranslatable words”. The method can be effective for Korean-English SMT where target language does not have corresponding function words, but it has a limitation in application to the English-Korean SMT because removing functional morphemes can cause a serious loss in information. Technically, the function words they ignored are </context>
<context position="8152" citStr="Lee et al. (2006)" startWordPosition="1268" endWordPosition="1271">d some pseudo words according to their respective null alignment probabilities. Figure 3 shows the top 9 selected dependency relations (actually used in the experiment) and the aligned Korean function words. 3 Syntactic Reordering Many approaches use syntactic reordering in the preprocessing step for SMT systems (Collins et al., 2005; Xia and McCord, 2004; Zwarts and Dras, 2007). Some reordering approaches have given significant improvements in performance for translation from French to English (Xia and McCord, 2004) and from German to English (Collins et al., 2005). However, on the contrary, Lee et al. (2006) reported that the reordering of Korean for Korean-English translation degraded the performance. They presumed that the performance decrease might come from low parsing performance for conversational domain. We believe that it is very important to consider the structural properties of Korean for reordering English sentences. Though the word order of a Korean sentence is relatively free, Korean generally observes the SOV word order, and it is a headfinal language. Consequently, an object precedes a predicate, and all dependents precede their heads. We use both a structured parse tree and depend</context>
</contexts>
<marker>Lee, Lee, Lee, 2006</marker>
<rawString>Jonghoon Lee, Donghyeon Lee, and Gary Geunbae Lee. 2006. Improving phrase-based koreanenglish statistical machine translation. In Proc. of Interspeech-ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyi Ma</author>
</authors>
<title>Champollion: A robust parallel text sentence aligner.</title>
<date>2006</date>
<booktitle>In Proc. ofLREC.</booktitle>
<contexts>
<context position="10230" citStr="Ma, 2006" startWordPosition="1605" endWordPosition="1606">eriments 4.1 Experimental Setup The baseline of our approach is a statistical phrase-based system which is trained using MOSES (Koehn et al., 2007). We collect bilingual texts from the Web and combine them with the Sejong parallel corpora 2. About 300K pair of sentences are collected from the major bilingual news broadcasting sites. We also collect around 1M monolingual sentences from the sites to train Korean language models. The best performing language model is 5-gram order with Kneser-Ney smoothing. For sentence level alignment, we modified the Champollion toolkit for English-Korean pair (Ma, 2006). We randomly selected 5,000 sentence pairs from Sejong corpora, of which 1,500 were used for a tuning set for minimum error rate training, and another 1,500 for development set for analysis experiment. We report testing results on the remaining 2,000 sentence pairs for the evaluation. Korean sentences are tokenized by the morphological analyzer (Lee and Rim, 2004). For English sentence preprocessing, we use the Stanford parser with output of typed dependency relations. We then applied the pseudo word insertion and four reordering rules described in the previous section to the parse tree of ea</context>
</contexts>
<marker>Ma, 2006</marker>
<rawString>Xiaoyi Ma. 2006. Champollion: A robust parallel text sentence aligner. In Proc. ofLREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3249" citStr="Och and Ney, 2003" startWordPosition="493" endWordPosition="496">heme unit before training and decoding the sentences. When analyzing English-Korean translation with MOSES (Koehn et al., 2007), we found high ratio of null alignment. In figure 1, ‘o��(eun)’, ‘91(eui)’, ‘a+(ha)’, ‘l--(n)’, ‘-71(ji)’ and ‘Lr+(neunda)’ are not linked to any word in the English sentence. In many cases, these words are function words that are attached to preceding content words. Sometimes they can be linked (incorrectly) to their head’s corresponding words, or they can be linked to totally different words with respect to their meaning. In the preliminary experiment using GIZA++ (Och and Ney, 2003) with grow-diag-final heuristic, we found that about 25% of words in Korean sentences and 21% of English sentences fail to align. This null alignment ratio is relatively high in comparison to the French-English alignment, in which about 9% of French sentences and 6% of English sentences are not aligned. Due to this null alignment, the estimation of translation probabilities for Korean function words may be incomplete; a system would perform mainly based 1Some may consider it an Altaic language family. 233 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 233–236, Suntec, Singap</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical mt system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="7892" citStr="Xia and McCord, 2004" startWordPosition="1227" endWordPosition="1230">y relations and their aligned function words in training data (shown the top 3 results in descending order of alignment probability) we observe that inserting too many pseudo words can, on the contrary, increase null alignment of English sentence. Thus we filtered some pseudo words according to their respective null alignment probabilities. Figure 3 shows the top 9 selected dependency relations (actually used in the experiment) and the aligned Korean function words. 3 Syntactic Reordering Many approaches use syntactic reordering in the preprocessing step for SMT systems (Collins et al., 2005; Xia and McCord, 2004; Zwarts and Dras, 2007). Some reordering approaches have given significant improvements in performance for translation from French to English (Xia and McCord, 2004) and from German to English (Collins et al., 2005). However, on the contrary, Lee et al. (2006) reported that the reordering of Korean for Korean-English translation degraded the performance. They presumed that the performance decrease might come from low parsing performance for conversational domain. We believe that it is very important to consider the structural properties of Korean for reordering English sentences. Though the wo</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical mt system with automatically learned rewrite patterns. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Zwarts</author>
<author>Mark Dras</author>
</authors>
<title>Syntax-based word reordering in phrase-based statistical machine translation: Why does it work?</title>
<date>2007</date>
<booktitle>In Proc. of MTSummit XI.</booktitle>
<contexts>
<context position="7916" citStr="Zwarts and Dras, 2007" startWordPosition="1231" endWordPosition="1234">aligned function words in training data (shown the top 3 results in descending order of alignment probability) we observe that inserting too many pseudo words can, on the contrary, increase null alignment of English sentence. Thus we filtered some pseudo words according to their respective null alignment probabilities. Figure 3 shows the top 9 selected dependency relations (actually used in the experiment) and the aligned Korean function words. 3 Syntactic Reordering Many approaches use syntactic reordering in the preprocessing step for SMT systems (Collins et al., 2005; Xia and McCord, 2004; Zwarts and Dras, 2007). Some reordering approaches have given significant improvements in performance for translation from French to English (Xia and McCord, 2004) and from German to English (Collins et al., 2005). However, on the contrary, Lee et al. (2006) reported that the reordering of Korean for Korean-English translation degraded the performance. They presumed that the performance decrease might come from low parsing performance for conversational domain. We believe that it is very important to consider the structural properties of Korean for reordering English sentences. Though the word order of a Korean sen</context>
</contexts>
<marker>Zwarts, Dras, 2007</marker>
<rawString>Simon Zwarts and Mark Dras. 2007. Syntax-based word reordering in phrase-based statistical machine translation: Why does it work? In Proc. of MTSummit XI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>