<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9972935">
Attacking Decipherment Problems Optimally with Low-Order N-gram
Models
</title>
<author confidence="0.991345">
Sujith Ravi and Kevin Knight
</author>
<affiliation confidence="0.883480333333333">
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
</affiliation>
<email confidence="0.998091">
{sravi,knight}@isi.edu
</email>
<sectionHeader confidence="0.984872" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938181818182">
We introduce a method for solving substi-
tution ciphers using low-order letter n-gram
models. This method enforces global con-
straints using integer programming, and it
guarantees that no decipherment key is over-
looked. We carry out extensive empirical ex-
periments showing how decipherment accu-
racy varies as a function of cipher length and
n-gram order. We also make an empirical in-
vestigation of Shannon’s (1949) theory of un-
certainty in decipherment.
</bodyText>
<sectionHeader confidence="0.992485" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99902440625">
A number of papers have explored algorithms
for automatically solving letter-substitution ciphers.
Some use heuristic methods to search for the best de-
terministic key (Peleg and Rosenfeld, 1979; Gane-
san and Sherman, 1993; Jakobsen, 1995; Olson,
2007), often using word dictionaries to guide that
search. Others use expectation-maximization (EM)
to search for the best probabilistic key using letter
n-gram models (Knight et al., 2006). In this paper,
we introduce an exact decipherment method based
on integer programming. We carry out extensive de-
cipherment experiments using letter n-gram models,
and we find that our accuracy rates far exceed those
of EM-based methods.
We also empirically explore the concepts in Shan-
non’s (1949) paper on information theory as applied
to cipher systems. We provide quantitative plots for
uncertainty in decipherment, including the famous
unicity distance, which estimates how long a cipher
must be to virtually eliminate such uncertainty.
We find the ideas in Shannon’s (1949) paper rel-
evant to problems of statistical machine translation
and transliteration. When first exposed to the idea
of statistical machine translation, many people natu-
rally ask: (1) how much data is needed to get a good
result, and (2) can translation systems be trained
without parallel data? These are tough questions by
any stretch, and it is remarkable that Shannon was
already in the 1940s tackling such questions in the
realm of code-breaking, creating analytic formulas
to estimate answers.
Our novel contributions are as follows:
</bodyText>
<listItem confidence="0.712001461538461">
• We outline an exact letter-substitution deci-
pherment method which:
- guarantees that no key is overlooked, and
- can be executed with standard integer pro-
gramming solvers
• We present empirical results for decipherment
which:
- plot search-error-free decipherment results at
various cipher lengths, and
- demonstrate accuracy rates superior to EM-
based methods
• We carry out empirical testing of Shannon’s
formulas for decipherment uncertainty
</listItem>
<sectionHeader confidence="0.946973" genericHeader="method">
2 Language Models
</sectionHeader>
<bodyText confidence="0.9998425">
We work on letter substitution ciphers with spaces.
We look for the key (among 26! possible ones)
that, when applied to the ciphertext, yields the most
English-like result. We take “English-like” to mean
</bodyText>
<note confidence="0.869931666666667">
812
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 812–819,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997378">
most probable according to some statistical lan-
guage model, whose job is to assign some proba-
bility to any sequence of letters. According to a 1-
gram model of English, the probability of a plaintext
p1...p,,, is given by:
</bodyText>
<equation confidence="0.99927">
P(p1...pn) = P(p1) · P(p2) · ... · P(pn)
</equation>
<bodyText confidence="0.9988062">
That is, we obtain the probability of a sequence
by multiplying together the probabilities of the in-
dividual letters that make it up. This model assigns
a probability to any letter sequence, and the proba-
bilities of all letter sequences sum to one. We col-
lect letter probabilities (including space) from 50
million words of text available from the Linguistic
Data Consortium (Graff and Finch, 1994). We also
estimate 2- and 3-gram models using the same re-
sources:
</bodyText>
<equation confidence="0.9999758">
P(p1...pn) = P(p1  |START) · P(p2  |p1) · P(p3  |p2) ·
... · P(pn  |pn−1) · P(END  |pn)
P(p1...pn) = P(p1  |START) · P(p2  |START p1) ·
P(p3  |p1 p2) · ... · P(pn  |pn−2 pn−1) ·
P(END  |pn−1 pn)
</equation>
<bodyText confidence="0.956749185185185">
Unlike the 1-gram model, the 2-gram model will
assign a low probability to the sequence “ae” be-
cause the probability P(e  |a) is low. Of course, all
these models are fairly weak, as already known by
(Shannon, 1949). When we stochastically generate
text according to these models, we get, for example:
1-gram:... thdo detusar ii c ibt deg irn toihytrsen ...
2-gram:... itariaris s oriorcupunond rke uth ...
3-gram:... ind thnowelf jusision thad inat of ...
4-gram:... rece bence on but ther servier ...
5-gram:... mrs earned age im on d the perious ...
6-gram:... a party to possible upon rest of ...
7-gram:... t our general through approve the ...
We can further estimate the probability of a whole
English sentence or phrase. For example, the prob-
abilities of two plaintext phrases “het oxf” and
“the fox” (which have the same letter frequency
distribution) is shown below. The 1-gram model
which counts only the frequency of occurrence of
each letter in the phrase, estimates the same proba-
bility for both the phrases “het oxf” and “the fox”,
since the same letters occur in both phrases. On the
other hand, the 2-gram and 3-gram models, which
take context into account, are able to distinguish be-
tween the English and non-English phrases better,
and hence assign a higher probability to the English
phrase “the fox”.
</bodyText>
<table confidence="0.99439875">
Model P(het oxf) P(the fox)
1-gram 1.83 x 10−9 1.83 x 10−9
2-gram 3.26 x 10−11 1.18 x 10−7
3-gram 1.89 x 10−13 1.04 x 10−6
</table>
<bodyText confidence="0.623089">
Over a longer sequence X of length N, we can
also calculate −log2(P(X))/N, which (per Shan-
non) gives the compression rate permitted by the
model, in bits per character. In our case, we get:1
1-gram: 4.19
2-gram: 3.51
3-gram: 2.93
</bodyText>
<sectionHeader confidence="0.991866" genericHeader="method">
3 Decipherment
</sectionHeader>
<bodyText confidence="0.999737625">
Given a ciphertext c1...c,,,, we search for the key that
yields the most probable plaintext p1...p,,,. There are
26! possible keys, too many to enumerate. How-
ever, we can still find the best one in a guaranteed
fashion. We do this by taking our most-probable-
plaintext problem and casting it as an integer pro-
gramming problem.2
Here is a sample integer programming problem:
</bodyText>
<equation confidence="0.986338285714286">
variables: x, y
minimize:
2x + y
subject to:
x + y &lt; 6.9
y − x &lt; 2.5
y &gt; 1.1
</equation>
<bodyText confidence="0.99955">
We require that x and y take on integer values. A
solution can be obtained by typing this integer pro-
gram into the publicly available lp solve program,
</bodyText>
<footnote confidence="0.9260836">
1Because spacing is fixed in our letter substitution ciphers,
we normalize P(X) by the sum of probabilities of all English
strings that match the spacing pattern of X.
2For an overview of integer and linear programming, see for
example (Schrijver, 1998).
</footnote>
<page confidence="0.810206">
813
</page>
<figure confidence="0.9463325">
1 2 3 4 5 6 7 8 ...
link-2de link-5ad link-7e_
</figure>
<figureCaption confidence="0.9904118">
Figure 1: A decipherment network. The beginning of the ciphertext is shown at the top of the figure (underscores
represent spaces). Any left-to-right path through the network constitutes a potential decipherment. The bold path
corresponds to the decipherment “decade”. The dotted path corresponds to the decipherment “ababab”. Given a
cipher length of n, the network has 27 · 27 · (n − 1) links and 27n paths. Each link corresponds to a named variable
in our integer program. Three links are shown with their names in the figure.
</figureCaption>
<figure confidence="0.979494388888889">
...
Q W B S Q W
_
_
a a a a a a a a ...
b b b b b b b b ...
c c c c c c c c ...
d d d d d d d d ...
e e e e e
... ... ... ... ... ... ... ... ...
z z z z z z z z ...
...
_ _ _ _ _ _ _ _
ciphertext
network of
possible
plaintexts
e e e ...
</figure>
<bodyText confidence="0.9981758">
or the commercially available CPLEX program,
which yields the result: x = 4, y = 2.
Suppose we want to decipher with a 2-gram lan-
guage model, i.e., we want to find the key that yields
the plaintext of highest 2-gram probability. Given
the ciphertext c1...cn, we create an integer program-
ming problem as follows. First, we set up a net-
work of possible decipherments (Figure 1). Each
of the 27 · 27 · (n − 1) links in the network is a
binary variable in the integer program—it must be
assigned a value of either 0 or 1. We name these
variables linkXY Z, where X indicates the column
of the link’s source, and Y and Z represent the rows
of the link’s source and destination (e.g. variables
link1aa, link1ab, link5qu, ...).
Each distinct left-to-right path through the net-
work corresponds to a different decipherment. For
example, the bold path in Figure 1 corresponds to
the decipherment “decade”. Decipherment amounts
to turning some links “on” (assigning value 1 to the
link variable) and others “off” (assigning value 0).
Not all assignments of 0’s and 1’s to link variables
result in a coherent left-to-right path, so we must
place some “subject to” constraints in our integer
program.
We observe that a set of variables forms a path if,
for every node in columns 2 through n−1 of the net-
work, the following property holds: the sum of the
values of the link variables entering the node equals
the sum of the link variables leaving the node. For
nodes along a chosen decipherment path, this sum
will be 1, and for others, it will be 0.3 Therefore,
we create one “subject to” constraint for each node
(“ ” stands for space). For example, for the node in
column 2, row e we have:
</bodyText>
<equation confidence="0.963415333333333">
subject to:
link1ae + link1be + link1ce + ... + link1 e
= link2ea + link2eb + link2ec + ... + link2e
</equation>
<bodyText confidence="0.9961952">
Now we set up an expression for the “minimize”
part of the integer program. Recall that we want
to select the plaintext p1...pn of highest probability.
For the 2-gram language model, the following are
equivalent:
</bodyText>
<figure confidence="0.98931925">
(a) Maximize P(p1...pn)
(b) Maximize log2 P(p1...pn)
(c) Minimize −log2 P(p1...pn)
(d) Minimize −log2 [ P(p1  |START)
</figure>
<bodyText confidence="0.617990666666667">
3Strictly speaking, this constraint over nodes still allows
multiple decipherment paths to be active, but we can rely on
the rest of our integer program to select only one.
</bodyText>
<equation confidence="0.9814314">
814
·P(p2  |p1)
· ...
·P(pn  |pn−1)
· P(END  |pn) ]
(e) Minimize −log2 P(p1  |START)
−log2 P(p2  |p1)
− ...
−log2 P(pn  |pn−1)
−log2 P(END  |pn)
</equation>
<bodyText confidence="0.9271637">
We can guarantee this last outcome if we con-
struct our minimization function as a sum of 27·27 ·
(n − 1) terms, each of which is a linkXY Z variable
multiplied by −log2P(Z|Y ):
Minimize link1aa · −log2 P(a  |a)
+ link1ab · −log2 P(b  |a)
+ link1ac · −log2 P(c  |a)
+ ...
+ link5qu · −log2 P(u  |q)
+ ...
When we assign value 1 to link variables along
some decipherment path, and 0 to all others, this
function computes the negative log probability of
that path.
We must still add a few more “subject to” con-
straints. We need to ensure that the chosen path im-
itates the repetition pattern of the ciphertext. While
the bold path in Figure 1 represents the fine plain-
text choice “decade”, the dotted path represents the
choice “ababab”, which is not consistent with the
repetition pattern of the cipher “QWBSQW”. To
make sure our substitutions obey a consistent key,
we set up 27 · 27 = 729 new keyxy variables to
represent the choice of key. These new variables
are also binary, taking on values 0 or 1. If variable
keyaQ = 1, that means the key maps plaintext a to
ciphertext Q. Clearly, not all assignments to these
729 variables represent valid keys, so we augment
the “subject to” part of our integer program by re-
quiring that for any letter x,
</bodyText>
<equation confidence="0.97681825">
subject to:
keyxA + keyxB + ... + keyxZ + keyx = 1
keyAx + keyBx + ... + keyZx
+ key x =
</equation>
<bodyText confidence="0.971150342857143">
That is, every plaintext letter must map to exactly
one ciphertext letter, and every ciphertext letter must
map to exactly one plaintext letter. We also add a
constraint to ensure that the ciphertext space charac-
ter maps to the plaintext space character:
subject to:
key = 1
Finally, we ensure that any chosen decipherment
path of linkXY Z variables is consistent with the
chosen key. We know that for every node A along
the decipherment path, exactly one active link has
A as its destination. For all other nodes, zero active
links lead in. Suppose node A represents the de-
cipherment of ciphertext letter ci as plaintext letter
pj—for all such nodes, we stipulate that the sum of
values for link(i−1)xpj (for all x) equals the value of
keypjc,. In other words, whether a node lies along
the chosen decipherment path or not, the chosen key
must support that decision.
Figure 2 summarizes the integer program that we
construct from a given ciphertext c1...cn. The com-
puter code that transforms any given cipher into a
corresponding integer program runs to about one
page. Variations on the decipherment network yield
1-gram and 3-gram decipherment capabilities. Once
an integer program is generated by machine, we
ask the commercially-available CPLEX software
to solve it, and then we note which keyXY variables
are assigned value 1. Because CPLEX computes
the optimal key, the method is not fast—for ciphers
of length 32, the number of variables and constraints
encoded in the integer program (IP) along with aver-
age running times are shown below. It is possible to
obtain less-than-optimal keys faster by interrupting
the solver.
</bodyText>
<note confidence="0.589357">
Model # of IP # of IP Average
</note>
<table confidence="0.82777575">
variables constraints running time
1-gram 1,755 1,083 0.01 seconds
2-gram 27,700 2,054 50 seconds
3-gram 211,600 27,326 450 seconds
</table>
<sectionHeader confidence="0.967001" genericHeader="method">
4 Decipherment Experiments
</sectionHeader>
<bodyText confidence="0.999957833333333">
We create 50 ciphers each of lengths 2, 4, 8,..., 256.
We solve these with 1-gram, 2-gram, and 3-gram
language models. We record the average percentage
of ciphertext tokens decoded incorrectly. 50% error
means half of the ciphertext tokens are deciphered
wrong, while 0% means perfect decipherment. Here
</bodyText>
<figure confidence="0.920754666666666">
815
variables:
linkipr 1 if the ith cipher letter is deciphered as plaintext letter p AND the (i+1)th cipher letter is
deciphered as plaintext letter r
0 otherwise
keypq 1 if decipherment key maps plaintext letter p to ciphertext letter q
0 otherwise
minimize:
Pi= 1 Pp,r linkipr · −log P(r|p) (2-gram probability of chosen plaintext)
</figure>
<subsectionHeader confidence="0.515138">
subject to:
</subsectionHeader>
<bodyText confidence="0.8879086">
for all p: P r keypr = 1 (each plaintext letter maps to exactly one ciphertext letter)
for all p: P r keyrp = 1 (each ciphertext letter maps to exactly one plaintext letter)
key = 1 (cipher space character maps to plain space character)
for (i=1...n-2), for all r: [ Pp linkipr = Pp link(i+1)rp ] (chosen links form a left-to-right path)
for (i=1...n-1), for all p: Pr linkirp = keypc,,+, (chosen links are consistent with chosen key)
</bodyText>
<figureCaption confidence="0.9975235">
Figure 2: Summary of how to build an integer program for any given ciphertext C1...Cr,. Solving the integer program
will yield the decipherment of highest probability.
</figureCaption>
<bodyText confidence="0.9925145">
we illustrate some automatic decipherments with er-
ror rates:
42% error: the avelage ongrichman hal cy wiof a
sevesonme qus antizexty that he buprk lathes we blung
than soment - fotes mmasthes
11%error: the average englishman has so week a
reference for antiality that he would rather be prong than
recent - deter quarteur
2% error: the average englishman has so keep a
reference for antiquity that he would rather be wrong than
recent - peter mcarthur
0% error: the average englishman has so deep a
reverence for antiquity that he would rather be wrong
than recent - peter mcarthur
Figure 3 shows our automatic decipherment re-
sults. We note that the solution method is exact, not
heuristic, so that decipherment error is not due to
search error. Our use of global key constraints also
leads to accuracy that is superior to (Knight et al.,
2006). With a 2-gram model, their EM algorithm
gives 10% error for a 414-letter cipher, while our
method provides a solution with only 0.5% error.
At shorter cipher lengths, we observe much higher
improvements when using our method. For exam-
</bodyText>
<equation confidence="0.309869">
2 4 8 16 32 64 128 256 512 1024
Cipher Length (letters)
</equation>
<figureCaption confidence="0.99916425">
Figure 3: Average decipherment error using integer pro-
gramming vs. cipher length, for 1-gram, 2-gram and 3-
gram models of English. Error bars indicate 95% confi-
dence intervals.
</figureCaption>
<bodyText confidence="0.999783666666667">
ple, on a 52-letter textbook cipher, using a 2-gram
model, the solution from our method resulted in 21%
error as compared to 85% error given by the EM so-
lution.
We see that deciphering with 3-grams works well
on ciphers of length 64 or more. This confirms
</bodyText>
<figure confidence="0.999499733333333">
Decipherment Error (%)
100
90
80
60
50
40
30
20
70
10
0
1-gram
2-gram
3-gram
</figure>
<page confidence="0.592475">
816
</page>
<bodyText confidence="0.995455692307692">
that such ciphers can be attacked with very limited
knowledge of English (no words or grammar) and
little custom programming.
The 1-gram model works badly in this scenario,
which is consistent with Bauer’s (2006) observation
that for short texts, mechanical decryption on the ba-
sis of individual letter frequencies does not work. If
we had infinite amounts of ciphertext and plaintext
drawn from the same stochastic source, we would
expect the plain and cipher frequencies to eventually
line up, allowing us to read off a correct key from the
frequency tables. The upper curve in Figure 3 shows
that convergence to this end is slow.
</bodyText>
<note confidence="0.7937255">
5 Shannon Equivocation and Unicity
Distance
</note>
<bodyText confidence="0.999777333333333">
Very short ciphers are hard to solve accurately.
Shannon (1949) pinpointed an inherent difficulty
with short ciphers, one that is independent of the so-
lution method or language model used; the cipher
itself may not contain enough information for its
proper solution. For example, given a short cipher
like XYYX, we can never be sure if the answer is
peep, noon, anna, etc. Shannon defined a mathemat-
ical measure of our decipherment uncertainty, which
he called equivocation (now called entropy).
Let C be a cipher, M be the plaintext message it
encodes, and K be the key by which the encoding
takes place. Before even seeing C, we can compute
our uncertainty about the key K by noting that there
are 26! equiprobable keys:4
</bodyText>
<equation confidence="0.949087">
H(K) = −(26!) · (1/26!) · log2 (1/26!)
= 88.4 bits
</equation>
<bodyText confidence="0.984753666666667">
That is, any secret key can be revealed in 89 bits.
When we actually receive a cipher C, our uncer-
tainty about the key and the plaintext message is re-
duced. Shannon described our uncertainty about the
plaintext message, letting m range over all decipher-
ments:
</bodyText>
<equation confidence="0.987125">
H(M|C) = equivocation of plaintext message
X= − P(m|C) · log2 P(m|C)
m
</equation>
<bodyText confidence="0.998800608695652">
4(Shannon, 1948) The entropy associated with a set of pos-
sible events whose probabilities of occurrence are p1, p2,..., pn
is given by H = −Pn1 pi · log2(pi).
P(m|C) is probability of plaintext m (according
to the language model) divided by the sum of proba-
bilities of all plaintext messages that obey the repeti-
tion pattern of C. While integer programming gives
us a method to find the most probable decipherment
without enumerating all keys, we do not know of a
similar method to compute a full equivocation with-
out enumerating all keys. Therefore, we sample up
to 100,000 plaintext messages in the neighborhood
of the most probably decipherment5 and compute
H(M|C) over that subset.6
Shannon also described H(K|C), the equivoca-
tion of key. This uncertainty is typically larger than
H(M|C), because a given message M may be de-
rived from C via more than one key, in case C does
not contain all 26 letters of the alphabet.
We compute H(K|C) by letting r(C) be the
number of distinct letters in C, and letting q(C) be
(26 − r(C))!. Letting i range over our sample of
plaintext messages, we get:
</bodyText>
<equation confidence="0.9979853">
H(K|C) = equivocation of key
X= − q(C) · (P(i)/q(C)) · log2 (P(i)/q(C))
i
X= − P(i) · log2 (P(i)/q(C))
i
X= − P(i) · (log2 P(i) − log2 q(C))
i
X= − P(i) · log2 P(i) + X P(i) · log2 q(C)
i i
= H(M|C) + log2 q(C)
</equation>
<bodyText confidence="0.986693444444444">
Shannon (1949) used analytic means to roughly
sketch the curves for H(K|C) and H(M|C), which
we reproduce in Figure 4. Shannon’s curve is drawn
for a human-level language model, and the y-axis is
given in “decimal digits” instead of bits.
5The sampling used to compute H(M|C) starts with the
optimal key and expands out a frontier, by swapping letters in
the key, and recursing to generate new keys (and corresponding
plaintext message decipherments). The plaintext messages are
remembered so that the frontier expands efficiently. The sam-
pling stops if 100,000 different messages are found.
6Interestingly, as we grow our sample out from the most
probable plaintext, we do not guarantee that any intermediate
result is a lower bound on the equivocation. An example is pro-
vided by the growing sample (0.12, 0.01, 0.01, 0.01, 0.01, 0.01,
0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01), whose entropy steadily
increases. However, if we add a 14th item whose P(m) is 0.12,
the entropy suddenly decreases from 2.79 to 2.78.
</bodyText>
<figure confidence="0.9733205">
817
Message Equivocation
Key Equivocation
Unicity Distance
</figure>
<figureCaption confidence="0.925024">
Figure 4: Equivocation for simple substitution on English (Shannon, 1949).
</figureCaption>
<figure confidence="0.999149">
90
1-gram
2-gram
3-gram
80
70
Equivocation of key (bits)
60
50
40
30
20
10
0
Cipher Length
Cipher Length
90
80
70
Equivocation of message (bits)
60
50
40
30
20
10
0
1-gram
2-gram
3-gram
</figure>
<figureCaption confidence="0.9838955">
Figure 5: Average key equivocation observed (bits) vs.
cipher length (letters), for 1-gram, 2-gram and 3-gram
models of English.
Figure 6: Average message equivocation observed (bits)
vs. cipher length (letters), for 1-gram, 2-gram and 3-gram
models of English.
</figureCaption>
<bodyText confidence="0.9999053125">
For comparison, we plot in Figures 5 and 6 the av-
erage equivocations as we empirically observe them
using our 1-, 2-, and 3-gram language models.
The shape of the key equivocation curve follows
Shannon, except that it is curved from the start,
rather than straight.
The message equivocation curve follows Shan-
non’s prediction, rising then falling. Because very
short ciphers have relatively few solutions (for ex-
ample, a one-letter cipher has only 26), the overall
uncertainty is not that great.7 As the cipher gets
longer, message equivocation rises. At some point,
it then decreases, as the cipher begins to reveal its
secret through patterns of repetition.
Shannon’s analytic model also predicts a sharp
decline of message equivocation towards zero. He
</bodyText>
<footnote confidence="0.686944">
7Uncertainty is only loosely related to accuracy—even if we
are quite certain about a solution, it may still be wrong.
</footnote>
<page confidence="0.847776">
818
</page>
<bodyText confidence="0.99900775">
defines the unicity distance (U) as the cipher length
at which we have virtually no more uncertainty
about the plaintext. Using analytic means (and vari-
ous approximations), he gives:
</bodyText>
<equation confidence="0.8435185">
U = H(K)/(A − B)
where:
A = bits per character of a 0-gram model (4.7)
B = bits per character of the model used to decipher
</equation>
<bodyText confidence="0.999832">
For a human-level language model (B — 1.2), he
concludes U — 25, which is confirmed by practice.
For our language models, the formula gives:
</bodyText>
<equation confidence="0.999897666666667">
U = 173 (1-gram)
U = 74 (2-gram)
U = 50 (3-gram)
</equation>
<bodyText confidence="0.999974285714286">
These numbers are in the same ballpark as
Bauer (2006), who gives 167, 74, and 59. We note
that these predicted unicity distances are a bit too
rosy, according to our empirical message equivoca-
tion curves. Our experience confirms this as well, as
1-gram frequency counts over a 173-letter cipher are
generally insufficient to pin down a solution.
</bodyText>
<sectionHeader confidence="0.998973" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99984275">
We provide a method for deciphering letter substi-
tution ciphers with low-order models of English.
This method, based on integer programming, re-
quires very little coding and can perform an opti-
mal search over the key space. We conclude by not-
ing that English language models currently used in
speech recognition (Chelba and Jelinek, 1999) and
automated language translation (Brants et al., 2007)
are much more powerful, employing, for example,
7-gram word models (not letter models) trained on
trillions of words. Obtaining optimal keys accord-
ing to such models will permit the automatic deci-
pherment of shorter ciphers, but this requires more
specialized search than what is provided by gen-
eral integer programming solvers. Methods such
as these should also be useful for natural language
decipherment problems such as character code con-
version, phonetic decipherment, and word substitu-
tion ciphers with applications in machine translation
(Knight et al., 2006).
</bodyText>
<sectionHeader confidence="0.997957" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998576285714286">
The authors wish to gratefully acknowledge
Jonathan Graehl, for providing a proof to support
the argument that taking a larger number of samples
does not necessarily increase the equivocation. This
research was supported by the Defense Advanced
Research Projects Agency under SRI International’s
prime Contract Number NBCHD040058.
</bodyText>
<sectionHeader confidence="0.993527" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999954394736842">
Friedrich L. Bauer. 2006. Decrypted Secrets: Methods
and Maxims of Cryptology. Springer-Verlag.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of EMNLP-
CoNLL.
Ciprian Chelba and Frederick Jelinek. 1999. Structured
language modeling for speech recognition. In Pro-
ceedings of NLDB: 4th International Conference on
Applications of Natural Language to Information Sys-
tems.
Ravi Ganesan and Alan T. Sherman. 1993. Statistical
techniques for language recognition: An introduction
and guide for cryptanalysts. Cryptologia, 17(4):321–
366.
David Graff and Rebecca Finch. 1994. Multilingual text
resources at the linguistic data consortium. In Pro-
ceedings of the HLT Workshop on Human Language
Technology.
Thomas Jakobsen. 1995. A fast method for cryptanalysis
of substitution ciphers. Cryptologia, 19(3):265–274.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of the COLING/ACL.
Edwin Olson. 2007. Robust dictionary attack of short
simple substitution ciphers. Cryptologia, 31(4):332–
342.
Shmuel Peleg and Azriel Rosenfeld. 1979. Break-
ing substitution ciphers using a relaxation algorithm.
Comm. ACM, 22(11):598–605.
Alexander Schrijver. 1998. Theory of Linear and Integer
Programming. John Wiley &amp; Sons.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27:379–423 and 623–656.
Claude E. Shannon. 1949. Communication theory
of secrecy systems. Bell System Technical Journal,
28:656–715.
</reference>
<page confidence="0.950515">
819
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.637148">
<title confidence="0.9987625">Attacking Decipherment Problems Optimally with Low-Order N-gram Models</title>
<author confidence="0.993927">Sujith Ravi</author>
<author confidence="0.993927">Kevin</author>
<affiliation confidence="0.956945">University of Southern Information Sciences</affiliation>
<address confidence="0.697972">Marina del Rey, California</address>
<abstract confidence="0.9931725">We introduce a method for solving substitution ciphers using low-order letter n-gram models. This method enforces global constraints using integer programming, and it guarantees that no decipherment key is overlooked. We carry out extensive empirical experiments showing how decipherment accuracy varies as a function of cipher length and n-gram order. We also make an empirical investigation of Shannon’s (1949) theory of uncertainty in decipherment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Friedrich L Bauer</author>
</authors>
<title>Decrypted Secrets: Methods and Maxims of Cryptology.</title>
<date>2006</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="21965" citStr="Bauer (2006)" startWordPosition="3802" endWordPosition="3803">e quite certain about a solution, it may still be wrong. 818 defines the unicity distance (U) as the cipher length at which we have virtually no more uncertainty about the plaintext. Using analytic means (and various approximations), he gives: U = H(K)/(A − B) where: A = bits per character of a 0-gram model (4.7) B = bits per character of the model used to decipher For a human-level language model (B — 1.2), he concludes U — 25, which is confirmed by practice. For our language models, the formula gives: U = 173 (1-gram) U = 74 (2-gram) U = 50 (3-gram) These numbers are in the same ballpark as Bauer (2006), who gives 167, 74, and 59. We note that these predicted unicity distances are a bit too rosy, according to our empirical message equivocation curves. Our experience confirms this as well, as 1-gram frequency counts over a 173-letter cipher are generally insufficient to pin down a solution. 6 Conclusion We provide a method for deciphering letter substitution ciphers with low-order models of English. This method, based on integer programming, requires very little coding and can perform an optimal search over the key space. We conclude by noting that English language models currently used in sp</context>
</contexts>
<marker>Bauer, 2006</marker>
<rawString>Friedrich L. Bauer. 2006. Decrypted Secrets: Methods and Maxims of Cryptology. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL.</booktitle>
<contexts>
<context position="22665" citStr="Brants et al., 2007" startWordPosition="3913" endWordPosition="3916">a bit too rosy, according to our empirical message equivocation curves. Our experience confirms this as well, as 1-gram frequency counts over a 173-letter cipher are generally insufficient to pin down a solution. 6 Conclusion We provide a method for deciphering letter substitution ciphers with low-order models of English. This method, based on integer programming, requires very little coding and can perform an optimal search over the key space. We conclude by noting that English language models currently used in speech recognition (Chelba and Jelinek, 1999) and automated language translation (Brants et al., 2007) are much more powerful, employing, for example, 7-gram word models (not letter models) trained on trillions of words. Obtaining optimal keys according to such models will permit the automatic decipherment of shorter ciphers, but this requires more specialized search than what is provided by general integer programming solvers. Methods such as these should also be useful for natural language decipherment problems such as character code conversion, phonetic decipherment, and word substitution ciphers with applications in machine translation (Knight et al., 2006). 7 Acknowledgements The authors </context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling for speech recognition.</title>
<date>1999</date>
<booktitle>In Proceedings of NLDB: 4th International Conference on Applications of Natural Language to Information Systems.</booktitle>
<contexts>
<context position="22608" citStr="Chelba and Jelinek, 1999" startWordPosition="3905" endWordPosition="3908">4, and 59. We note that these predicted unicity distances are a bit too rosy, according to our empirical message equivocation curves. Our experience confirms this as well, as 1-gram frequency counts over a 173-letter cipher are generally insufficient to pin down a solution. 6 Conclusion We provide a method for deciphering letter substitution ciphers with low-order models of English. This method, based on integer programming, requires very little coding and can perform an optimal search over the key space. We conclude by noting that English language models currently used in speech recognition (Chelba and Jelinek, 1999) and automated language translation (Brants et al., 2007) are much more powerful, employing, for example, 7-gram word models (not letter models) trained on trillions of words. Obtaining optimal keys according to such models will permit the automatic decipherment of shorter ciphers, but this requires more specialized search than what is provided by general integer programming solvers. Methods such as these should also be useful for natural language decipherment problems such as character code conversion, phonetic decipherment, and word substitution ciphers with applications in machine translati</context>
</contexts>
<marker>Chelba, Jelinek, 1999</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 1999. Structured language modeling for speech recognition. In Proceedings of NLDB: 4th International Conference on Applications of Natural Language to Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Ganesan</author>
<author>Alan T Sherman</author>
</authors>
<title>Statistical techniques for language recognition: An introduction and guide for cryptanalysts.</title>
<date>1993</date>
<journal>Cryptologia,</journal>
<volume>17</volume>
<issue>4</issue>
<pages>366</pages>
<contexts>
<context position="917" citStr="Ganesan and Sherman, 1993" startWordPosition="128" endWordPosition="132">ow-order letter n-gram models. This method enforces global constraints using integer programming, and it guarantees that no decipherment key is overlooked. We carry out extensive empirical experiments showing how decipherment accuracy varies as a function of cipher length and n-gram order. We also make an empirical investigation of Shannon’s (1949) theory of uncertainty in decipherment. 1 Introduction A number of papers have explored algorithms for automatically solving letter-substitution ciphers. Some use heuristic methods to search for the best deterministic key (Peleg and Rosenfeld, 1979; Ganesan and Sherman, 1993; Jakobsen, 1995; Olson, 2007), often using word dictionaries to guide that search. Others use expectation-maximization (EM) to search for the best probabilistic key using letter n-gram models (Knight et al., 2006). In this paper, we introduce an exact decipherment method based on integer programming. We carry out extensive decipherment experiments using letter n-gram models, and we find that our accuracy rates far exceed those of EM-based methods. We also empirically explore the concepts in Shannon’s (1949) paper on information theory as applied to cipher systems. We provide quantitative plot</context>
</contexts>
<marker>Ganesan, Sherman, 1993</marker>
<rawString>Ravi Ganesan and Alan T. Sherman. 1993. Statistical techniques for language recognition: An introduction and guide for cryptanalysts. Cryptologia, 17(4):321– 366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Rebecca Finch</author>
</authors>
<title>Multilingual text resources at the linguistic data consortium.</title>
<date>1994</date>
<booktitle>In Proceedings of the HLT Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="3758" citStr="Graff and Finch, 1994" startWordPosition="573" endWordPosition="576">cal language model, whose job is to assign some probability to any sequence of letters. According to a 1- gram model of English, the probability of a plaintext p1...p,,, is given by: P(p1...pn) = P(p1) · P(p2) · ... · P(pn) That is, we obtain the probability of a sequence by multiplying together the probabilities of the individual letters that make it up. This model assigns a probability to any letter sequence, and the probabilities of all letter sequences sum to one. We collect letter probabilities (including space) from 50 million words of text available from the Linguistic Data Consortium (Graff and Finch, 1994). We also estimate 2- and 3-gram models using the same resources: P(p1...pn) = P(p1 |START) · P(p2 |p1) · P(p3 |p2) · ... · P(pn |pn−1) · P(END |pn) P(p1...pn) = P(p1 |START) · P(p2 |START p1) · P(p3 |p1 p2) · ... · P(pn |pn−2 pn−1) · P(END |pn−1 pn) Unlike the 1-gram model, the 2-gram model will assign a low probability to the sequence “ae” because the probability P(e |a) is low. Of course, all these models are fairly weak, as already known by (Shannon, 1949). When we stochastically generate text according to these models, we get, for example: 1-gram:... thdo detusar ii c ibt deg irn toihytrs</context>
</contexts>
<marker>Graff, Finch, 1994</marker>
<rawString>David Graff and Rebecca Finch. 1994. Multilingual text resources at the linguistic data consortium. In Proceedings of the HLT Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Jakobsen</author>
</authors>
<title>A fast method for cryptanalysis of substitution ciphers.</title>
<date>1995</date>
<journal>Cryptologia,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="933" citStr="Jakobsen, 1995" startWordPosition="133" endWordPosition="134">ls. This method enforces global constraints using integer programming, and it guarantees that no decipherment key is overlooked. We carry out extensive empirical experiments showing how decipherment accuracy varies as a function of cipher length and n-gram order. We also make an empirical investigation of Shannon’s (1949) theory of uncertainty in decipherment. 1 Introduction A number of papers have explored algorithms for automatically solving letter-substitution ciphers. Some use heuristic methods to search for the best deterministic key (Peleg and Rosenfeld, 1979; Ganesan and Sherman, 1993; Jakobsen, 1995; Olson, 2007), often using word dictionaries to guide that search. Others use expectation-maximization (EM) to search for the best probabilistic key using letter n-gram models (Knight et al., 2006). In this paper, we introduce an exact decipherment method based on integer programming. We carry out extensive decipherment experiments using letter n-gram models, and we find that our accuracy rates far exceed those of EM-based methods. We also empirically explore the concepts in Shannon’s (1949) paper on information theory as applied to cipher systems. We provide quantitative plots for uncertaint</context>
</contexts>
<marker>Jakobsen, 1995</marker>
<rawString>Thomas Jakobsen. 1995. A fast method for cryptanalysis of substitution ciphers. Cryptologia, 19(3):265–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Anish Nair</author>
<author>Nishit Rathod</author>
<author>Kenji Yamada</author>
</authors>
<title>Unsupervised analysis for decipherment problems.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL.</booktitle>
<contexts>
<context position="1131" citStr="Knight et al., 2006" startWordPosition="160" endWordPosition="163">erment accuracy varies as a function of cipher length and n-gram order. We also make an empirical investigation of Shannon’s (1949) theory of uncertainty in decipherment. 1 Introduction A number of papers have explored algorithms for automatically solving letter-substitution ciphers. Some use heuristic methods to search for the best deterministic key (Peleg and Rosenfeld, 1979; Ganesan and Sherman, 1993; Jakobsen, 1995; Olson, 2007), often using word dictionaries to guide that search. Others use expectation-maximization (EM) to search for the best probabilistic key using letter n-gram models (Knight et al., 2006). In this paper, we introduce an exact decipherment method based on integer programming. We carry out extensive decipherment experiments using letter n-gram models, and we find that our accuracy rates far exceed those of EM-based methods. We also empirically explore the concepts in Shannon’s (1949) paper on information theory as applied to cipher systems. We provide quantitative plots for uncertainty in decipherment, including the famous unicity distance, which estimates how long a cipher must be to virtually eliminate such uncertainty. We find the ideas in Shannon’s (1949) paper relevant to p</context>
<context position="15043" citStr="Knight et al., 2006" startWordPosition="2604" endWordPosition="2607"> has so week a reference for antiality that he would rather be prong than recent - deter quarteur 2% error: the average englishman has so keep a reference for antiquity that he would rather be wrong than recent - peter mcarthur 0% error: the average englishman has so deep a reverence for antiquity that he would rather be wrong than recent - peter mcarthur Figure 3 shows our automatic decipherment results. We note that the solution method is exact, not heuristic, so that decipherment error is not due to search error. Our use of global key constraints also leads to accuracy that is superior to (Knight et al., 2006). With a 2-gram model, their EM algorithm gives 10% error for a 414-letter cipher, while our method provides a solution with only 0.5% error. At shorter cipher lengths, we observe much higher improvements when using our method. For exam2 4 8 16 32 64 128 256 512 1024 Cipher Length (letters) Figure 3: Average decipherment error using integer programming vs. cipher length, for 1-gram, 2-gram and 3- gram models of English. Error bars indicate 95% confidence intervals. ple, on a 52-letter textbook cipher, using a 2-gram model, the solution from our method resulted in 21% error as compared to 85% e</context>
</contexts>
<marker>Knight, Nair, Rathod, Yamada, 2006</marker>
<rawString>Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada. 2006. Unsupervised analysis for decipherment problems. In Proceedings of the COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin Olson</author>
</authors>
<title>Robust dictionary attack of short simple substitution ciphers.</title>
<date>2007</date>
<journal>Cryptologia,</journal>
<volume>31</volume>
<issue>4</issue>
<pages>342</pages>
<contexts>
<context position="947" citStr="Olson, 2007" startWordPosition="135" endWordPosition="136">enforces global constraints using integer programming, and it guarantees that no decipherment key is overlooked. We carry out extensive empirical experiments showing how decipherment accuracy varies as a function of cipher length and n-gram order. We also make an empirical investigation of Shannon’s (1949) theory of uncertainty in decipherment. 1 Introduction A number of papers have explored algorithms for automatically solving letter-substitution ciphers. Some use heuristic methods to search for the best deterministic key (Peleg and Rosenfeld, 1979; Ganesan and Sherman, 1993; Jakobsen, 1995; Olson, 2007), often using word dictionaries to guide that search. Others use expectation-maximization (EM) to search for the best probabilistic key using letter n-gram models (Knight et al., 2006). In this paper, we introduce an exact decipherment method based on integer programming. We carry out extensive decipherment experiments using letter n-gram models, and we find that our accuracy rates far exceed those of EM-based methods. We also empirically explore the concepts in Shannon’s (1949) paper on information theory as applied to cipher systems. We provide quantitative plots for uncertainty in decipherm</context>
</contexts>
<marker>Olson, 2007</marker>
<rawString>Edwin Olson. 2007. Robust dictionary attack of short simple substitution ciphers. Cryptologia, 31(4):332– 342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shmuel Peleg</author>
<author>Azriel Rosenfeld</author>
</authors>
<title>Breaking substitution ciphers using a relaxation algorithm.</title>
<date>1979</date>
<journal>Comm. ACM,</journal>
<volume>22</volume>
<issue>11</issue>
<contexts>
<context position="890" citStr="Peleg and Rosenfeld, 1979" startWordPosition="124" endWordPosition="127">ubstitution ciphers using low-order letter n-gram models. This method enforces global constraints using integer programming, and it guarantees that no decipherment key is overlooked. We carry out extensive empirical experiments showing how decipherment accuracy varies as a function of cipher length and n-gram order. We also make an empirical investigation of Shannon’s (1949) theory of uncertainty in decipherment. 1 Introduction A number of papers have explored algorithms for automatically solving letter-substitution ciphers. Some use heuristic methods to search for the best deterministic key (Peleg and Rosenfeld, 1979; Ganesan and Sherman, 1993; Jakobsen, 1995; Olson, 2007), often using word dictionaries to guide that search. Others use expectation-maximization (EM) to search for the best probabilistic key using letter n-gram models (Knight et al., 2006). In this paper, we introduce an exact decipherment method based on integer programming. We carry out extensive decipherment experiments using letter n-gram models, and we find that our accuracy rates far exceed those of EM-based methods. We also empirically explore the concepts in Shannon’s (1949) paper on information theory as applied to cipher systems. W</context>
</contexts>
<marker>Peleg, Rosenfeld, 1979</marker>
<rawString>Shmuel Peleg and Azriel Rosenfeld. 1979. Breaking substitution ciphers using a relaxation algorithm. Comm. ACM, 22(11):598–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Schrijver</author>
</authors>
<title>Theory of Linear and Integer Programming.</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="6552" citStr="Schrijver, 1998" startWordPosition="1073" endWordPosition="1074">g our most-probableplaintext problem and casting it as an integer programming problem.2 Here is a sample integer programming problem: variables: x, y minimize: 2x + y subject to: x + y &lt; 6.9 y − x &lt; 2.5 y &gt; 1.1 We require that x and y take on integer values. A solution can be obtained by typing this integer program into the publicly available lp solve program, 1Because spacing is fixed in our letter substitution ciphers, we normalize P(X) by the sum of probabilities of all English strings that match the spacing pattern of X. 2For an overview of integer and linear programming, see for example (Schrijver, 1998). 813 1 2 3 4 5 6 7 8 ... link-2de link-5ad link-7e_ Figure 1: A decipherment network. The beginning of the ciphertext is shown at the top of the figure (underscores represent spaces). Any left-to-right path through the network constitutes a potential decipherment. The bold path corresponds to the decipherment “decade”. The dotted path corresponds to the decipherment “ababab”. Given a cipher length of n, the network has 27 · 27 · (n − 1) links and 27n paths. Each link corresponds to a named variable in our integer program. Three links are shown with their names in the figure. ... Q W B S Q W _</context>
</contexts>
<marker>Schrijver, 1998</marker>
<rawString>Alexander Schrijver. 1998. Theory of Linear and Integer Programming. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude E Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>623--656</pages>
<contexts>
<context position="17649" citStr="Shannon, 1948" startWordPosition="3058" endWordPosition="3059">ipher, M be the plaintext message it encodes, and K be the key by which the encoding takes place. Before even seeing C, we can compute our uncertainty about the key K by noting that there are 26! equiprobable keys:4 H(K) = −(26!) · (1/26!) · log2 (1/26!) = 88.4 bits That is, any secret key can be revealed in 89 bits. When we actually receive a cipher C, our uncertainty about the key and the plaintext message is reduced. Shannon described our uncertainty about the plaintext message, letting m range over all decipherments: H(M|C) = equivocation of plaintext message X= − P(m|C) · log2 P(m|C) m 4(Shannon, 1948) The entropy associated with a set of possible events whose probabilities of occurrence are p1, p2,..., pn is given by H = −Pn1 pi · log2(pi). P(m|C) is probability of plaintext m (according to the language model) divided by the sum of probabilities of all plaintext messages that obey the repetition pattern of C. While integer programming gives us a method to find the most probable decipherment without enumerating all keys, we do not know of a similar method to compute a full equivocation without enumerating all keys. Therefore, we sample up to 100,000 plaintext messages in the neighborhood of</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Claude E. Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal, 27:379–423 and 623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude E Shannon</author>
</authors>
<title>Communication theory of secrecy systems.</title>
<date>1949</date>
<journal>Bell System Technical Journal,</journal>
<pages>28--656</pages>
<contexts>
<context position="4222" citStr="Shannon, 1949" startWordPosition="665" endWordPosition="666"> We collect letter probabilities (including space) from 50 million words of text available from the Linguistic Data Consortium (Graff and Finch, 1994). We also estimate 2- and 3-gram models using the same resources: P(p1...pn) = P(p1 |START) · P(p2 |p1) · P(p3 |p2) · ... · P(pn |pn−1) · P(END |pn) P(p1...pn) = P(p1 |START) · P(p2 |START p1) · P(p3 |p1 p2) · ... · P(pn |pn−2 pn−1) · P(END |pn−1 pn) Unlike the 1-gram model, the 2-gram model will assign a low probability to the sequence “ae” because the probability P(e |a) is low. Of course, all these models are fairly weak, as already known by (Shannon, 1949). When we stochastically generate text according to these models, we get, for example: 1-gram:... thdo detusar ii c ibt deg irn toihytrsen ... 2-gram:... itariaris s oriorcupunond rke uth ... 3-gram:... ind thnowelf jusision thad inat of ... 4-gram:... rece bence on but ther servier ... 5-gram:... mrs earned age im on d the perious ... 6-gram:... a party to possible upon rest of ... 7-gram:... t our general through approve the ... We can further estimate the probability of a whole English sentence or phrase. For example, the probabilities of two plaintext phrases “het oxf” and “the fox” (which</context>
<context position="16590" citStr="Shannon (1949)" startWordPosition="2872" endWordPosition="2873">The 1-gram model works badly in this scenario, which is consistent with Bauer’s (2006) observation that for short texts, mechanical decryption on the basis of individual letter frequencies does not work. If we had infinite amounts of ciphertext and plaintext drawn from the same stochastic source, we would expect the plain and cipher frequencies to eventually line up, allowing us to read off a correct key from the frequency tables. The upper curve in Figure 3 shows that convergence to this end is slow. 5 Shannon Equivocation and Unicity Distance Very short ciphers are hard to solve accurately. Shannon (1949) pinpointed an inherent difficulty with short ciphers, one that is independent of the solution method or language model used; the cipher itself may not contain enough information for its proper solution. For example, given a short cipher like XYYX, we can never be sure if the answer is peep, noon, anna, etc. Shannon defined a mathematical measure of our decipherment uncertainty, which he called equivocation (now called entropy). Let C be a cipher, M be the plaintext message it encodes, and K be the key by which the encoding takes place. Before even seeing C, we can compute our uncertainty abou</context>
<context position="18952" citStr="Shannon (1949)" startWordPosition="3300" endWordPosition="3301">ibed H(K|C), the equivocation of key. This uncertainty is typically larger than H(M|C), because a given message M may be derived from C via more than one key, in case C does not contain all 26 letters of the alphabet. We compute H(K|C) by letting r(C) be the number of distinct letters in C, and letting q(C) be (26 − r(C))!. Letting i range over our sample of plaintext messages, we get: H(K|C) = equivocation of key X= − q(C) · (P(i)/q(C)) · log2 (P(i)/q(C)) i X= − P(i) · log2 (P(i)/q(C)) i X= − P(i) · (log2 P(i) − log2 q(C)) i X= − P(i) · log2 P(i) + X P(i) · log2 q(C) i i = H(M|C) + log2 q(C) Shannon (1949) used analytic means to roughly sketch the curves for H(K|C) and H(M|C), which we reproduce in Figure 4. Shannon’s curve is drawn for a human-level language model, and the y-axis is given in “decimal digits” instead of bits. 5The sampling used to compute H(M|C) starts with the optimal key and expands out a frontier, by swapping letters in the key, and recursing to generate new keys (and corresponding plaintext message decipherments). The plaintext messages are remembered so that the frontier expands efficiently. The sampling stops if 100,000 different messages are found. 6Interestingly, as we </context>
</contexts>
<marker>Shannon, 1949</marker>
<rawString>Claude E. Shannon. 1949. Communication theory of secrecy systems. Bell System Technical Journal, 28:656–715.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>