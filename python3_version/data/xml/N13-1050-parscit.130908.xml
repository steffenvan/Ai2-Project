<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002415">
<title confidence="0.999806">
A Beam-Search Decoder for Normalization of Social Media Text
with Application to Machine Translation
</title>
<author confidence="0.998432">
Pidong Wang
</author>
<affiliation confidence="0.9999405">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.9759085">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.997282">
wangpd@comp.nus.edu.sg
</email>
<author confidence="0.959999">
Hwee Tou Ng
</author>
<affiliation confidence="0.999859">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.975899">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.997742">
nght@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.996632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999113947368421">
Social media texts are written in an infor-
mal style, which hinders other natural lan-
guage processing (NLP) applications such as
machine translation. Text normalization is
thus important for processing of social media
text. Previous work mostly focused on nor-
malizing words by replacing an informal word
with its formal form. In this paper, to fur-
ther improve other downstream NLP applica-
tions, we argue that other normalization oper-
ations should also be performed, e.g., missing
word recovery and punctuation correction. A
novel beam-search decoder is proposed to ef-
fectively integrate various normalization oper-
ations. Empirical results show that our system
obtains statistically significant improvements
over two strong baselines in both normaliza-
tion and translation tasks, for both Chinese
and English.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999720086956522">
Social media texts include SMS (Short Message
Service) messages, Twitter messages, Facebook up-
dates, etc. They are different from formal texts due
to their significant informal characteristics, so they
always pose difficulties for applications such as ma-
chine translation (MT) (Aw et al., 2005) and named
entity recognition (Liu et al., 2011), because of a
lack of training data containing informal texts. Thus,
the applications always suffer from a substantial per-
formance drop when evaluated on social media texts.
For example, Ritter et al. (2011) reported a drop
from 90% to 76% on part-of-speech tagging, and
Foster et al. (2011) found a drop of 20% in depen-
dency parsing.
Creating training data of social media texts specif-
ically for a text processing task is time-consuming.
For example, to create parallel Chinese-English
training texts for translation of social media texts,
it takes three minutes on average to translate an in-
formally written social media text of eleven words
from Chinese into English. On the other hand, it
takes thirty seconds to normalize the same message,
a six-fold increase in speed. After training a text nor-
malization system to normalize social media texts,
we can use an existing statistical machine translation
(SMT) system trained on normal texts (non-social
media texts) to carry out translation. So we argue
that normalization followed by regular translation is
a more practical approach. Thus, text normalization
is important for social media text processing.
Most previous work on normalization of social
media text focused on word substitution (Beaufort
et al., 2010; Gouws et al., 2011; Han and Baldwin,
2011; Liu et al., 2012). However, we argue that
some other normalization operations besides word
substitution are also critical for subsequent natu-
ral language processing (NLP) applications, such
as missing word recovery (e.g., zero pronouns) and
punctuation correction.
In this paper, we propose a novel beam-search
decoder for normalization of social media text for
MT. Our decoder can effectively integrate differ-
ent normalization operations together. In contrast
to previous work, some of our normalization opera-
tions are specifically designed for MT, e.g., missing
word recovery based on conditional random fields
</bodyText>
<page confidence="0.984491">
471
</page>
<note confidence="0.4800935">
Proceedings of NAACL-HLT 2013, pages 471–481,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.994866259259259">
(CRF) (Lafferty et al., 2001) and punctuation cor-
rection based on dynamic conditional random fields
(DCRF) (Sutton et al., 2004).
To the best of our knowledge, our work is the
first to perform missing word recovery and punc-
tuation correction for normalization of social me-
dia text, and also the first to perform message-level
normalization of Chinese social media text. We in-
vestigate the effects on translating social media text
after addressing various characteristics of informal
social media text through normalization. To show
the applicability of our normalization approach for
different languages, we experiment with two lan-
guages, Chinese and English. We achieved statisti-
cally significant improvements over two strong base-
lines: an improvement of 9.98%/7.35% in BLEU
scores for normalization of Chinese/English social
media text, and an improvement of 1.38%/1.35% in
BLEU scores for translation of Chinese/English so-
cial media text. We created two corpora: a Chinese
corpus containing 1,000 Weibo1 messages with their
normalizations and English translations; and another
similar English corpus containing 2,000 SMS mes-
sages from the NUS SMS corpus (How and Kan,
2005). As far as we know, our corpora are the first
publicly available Chinese/English corpora for nor-
malization and translation of social media text2.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999555">
Zhu et al. (2007) performed text normalization of
informally written email messages using CRF (Laf-
ferty et al., 2001). Due to its importance, normaliza-
tion of social media text has been extensively studied
recently. Aw et al. (2005) proposed a noisy chan-
nel model consisting of different operations: sub-
stitution of non-standard acronyms, deletion of fla-
vor words, and insertion of auxiliary verbs and sub-
ject pronouns. Choudhury et al. (2007) used hid-
den Markov model to perform word-level normal-
ization. Kobus et al. (2008) combined MT and auto-
matic speech recognition (ASR) to better normalize
French SMS message. Cook and Stevenson (2009)
used an unsupervised noisy channel model consid-
ering different word formation processes. Han and
Baldwin (2011) normalized informal words using
</bodyText>
<footnote confidence="0.9999685">
1A Chinese version of Twitter at www.weibo.com
2Available at www.comp.nus.edu.sg/∼nlp/corpora.html
</footnote>
<bodyText confidence="0.999724511111111">
morphophonemic similarity. Pennell and Liu (2011)
only dealt with SMS abbreviations. Xue et al. (2011)
normalized social media texts incorporating ortho-
graphic, phonetic, contextual, and acronym factors.
Liu et al. (2012) designed a system combining dif-
ferent human perspectives to perform word-level
normalization. Oliva et al. (2013) normalized Span-
ish SMS messages using a normalization and a pho-
netic dictionary. For normalization of Chinese so-
cial media text, Xia et al. (2005) investigated infor-
mal phrase detection, and Li and Yarowsky (2008)
mined informal-formal phrase pairs from Web cor-
pora.
All the above work focused on normalizing
words. In contrast, our work also performs other
normalization operations such as missing word re-
covery and punctuation correction, to further im-
prove machine translation. Previously, Aw et al.
(2006) adopted phrase-based MT to perform SMS
normalization, and required a relatively large num-
ber of manually normalized SMS messages. In con-
trast, our approach performs beam search at the sen-
tence level, and does not require large training data.
We evaluate the success of social media text nor-
malization in the context of machine translation, so
research on machine translation of social media text
is relevant to our work. However, there is not much
comparative evaluation of social media text transla-
tion other than the Haitian Creole to English SMS
translation task in the 2011 Workshop on Statistical
Machine Translation (WMT 2011) (Callison-Burch
et al., 2011). However, the setup of the WMT 2011
task is different from ours, in that the task provided
parallel training data of SMS texts and their transla-
tions. As such, text normalization is not necessary
in that task. For example, the best reported system
in that task (Costa-juss`a and Banchs, 2011) did not
perform SMS message normalization.
In speech to speech translation (Paul, 2009;
Nakov et al., 2009), the input texts contain wrongly
transcribed words due to errors in automatic speech
recognition, whereas social media texts contain ab-
breviations, new words, etc. Although the input
texts in both cases deviate from normal texts, the ex-
act deviations are different.
</bodyText>
<page confidence="0.997832">
472
</page>
<table confidence="0.993095125">
Category Freq. Example cluding omitted and misused punctuation; (2) redun-
Punctuation 81 t*[hi] —(t* o[hi .]); dant interjections; (3) quotation-related problems,
Pronunciation 47 �k[watch](TTc[don’t]); V (i-_X4-T[this]); e.g., omitted quotation marks; (4) “be” omission;
New word 43 _ffi[bud](7t[cute]); (5) tokenization problems; and (6) informally writ-
Interjection 27 ffAJ[ok] *[oh](ffA1[ok]); ten time expressions.
Pronoun 23 N-Tc[want](R[i] N-Tc[want]); 4 Methods
Segmentation 14 PV (T?c[don’t] iAWT[this]);
Pronunciation 288 4(for); oredi(already);
Abbreviation 98 slp(sleep); whr(where);
Prefix 74 lect(lecture); doin(doing);
Punctuation 69 where r u(where r u ?);
Interjection 68 ok lor.(ok.);
Quotation 24 im sure(i ’m sure); dont go(don ’t go);
Be 24 i coming; you free?;
Tokenization 19 ok.why ?(ok . why ?);
Time 2 end at 730(end at 7:30); 1130 am(11:30 am);
</table>
<tableCaption confidence="0.9942565">
Table 1: Occurrence frequency of various informal char-
acteristics in 200 Chinese/English social media texts.
</tableCaption>
<sectionHeader confidence="0.924527" genericHeader="method">
3 Challenges in Normalization of Social
</sectionHeader>
<subsectionHeader confidence="0.644595">
Media Text
</subsectionHeader>
<bodyText confidence="0.99991946">
To better understand the informal characteristics of
social media texts, we first analyzed a small sample
of such texts in Chinese and English. We crawled
200 Chinese messages from Weibo. The informal
characteristics of these messages are shown in the
first half of Table 1. The manually normalized form
is shown in round brackets, and the English gloss is
shown in square brackets. Omitted, extraneous, and
misused punctuation symbols occur frequently. On
average, each Chinese message contains only less
than one informal word, and many informal words
are either new words or existing words with new
meaning. The messages also contain redundant in-
terjections like “*[oh]”. Pronouns are often omit-
ted in Chinese messages, especially for “R[I]”. Chi-
nese informal words can be wrongly segmented due
to lack of word segmentation training data contain-
ing informal words.
Similarly, 200 English SMS messages were ran-
domly selected from the NUS SMS corpus (How
and Kan, 2005). The informal characteristics of
these messages are shown in the second half of Ta-
ble 1. We found that our English messages contain
more informal words than Chinese messages. En-
glish words are shortened in three ways: (1) using
a shorter word form with similar pronunciation; (2)
abbreviating a formal word; and (3) using only a pre-
fix of a formal word. Other informal characteristics
include: (1) informal punctuation conventions in-
As can be seen in Section 3, social media texts of
different languages exhibit different informal char-
acteristics. For example, English messages have
more informal words than Chinese messages, while
punctuation problems are more prevalent for Chi-
nese messages. Also, fixing different types of infor-
mal characteristics often depends on each other. For
example, to be able to correct punctuation, it helps
that the surrounding words are already correctly nor-
malized. On the other hand, with punctuation al-
ready corrected, it will be easier to normalize the
surrounding words.
In this section, we first present our punctuation
correction method based on a DCRF model, and
then present missing word recovery based on a CRF
model. Next, we present a novel beam-search de-
coder for normalization of social media text, which
can effectively integrate different normalization op-
erations, including statistical and rule-based normal-
ization. Finally, details of text normalization for
Chinese and English are presented.
</bodyText>
<subsectionHeader confidence="0.996448">
4.1 Punctuation Correction
</subsectionHeader>
<bodyText confidence="0.999913705882353">
In normalization of social media text, punctuation
correction is also important besides word normal-
ization, as the subsequent NLP applications are typ-
ically trained on formal texts with correct punctua-
tion. We define punctuation correction as correcting
punctuation in sentences which may have no or un-
reliable punctuation. The task performs three punc-
tuation operations: insertion, deletion, and substitu-
tion.
To our knowledge, no previous work has been
done on punctuation correction for normalization of
social media text. In ASR, punctuation prediction
only inserts punctuation symbols into ASR output
that has no punctuation (Kim and Woodland, 2001;
Huang and Zweig, 2002), but without punctuation
deletion or substitution. Lu and Ng (2010) argued
that punctuation prediction should be jointly per-
</bodyText>
<page confidence="0.99645">
473
</page>
<bodyText confidence="0.999301409090909">
formed with sentence boundary detection, so they
modeled punctuation prediction using a two-layer
DCRF model (Sutton et al., 2004).
We also believe that punctuation correction is
closely related to sentence boundary detection.
Thus, we propose a two-layer DCRF model for
punctuation correction. Layer 1 gives the actual
punctuation tags None, Comma, Period, Question-
Mark, and Exclamatory-Mark. Layer 2 gives
the sentence boundary, including tags Declarative-
Begin, Declarative-In, Question-Begin, Question-In,
Exclamatory-Begin, and Exclamatory-In, indicating
whether the current word is at the beginning of (or
inside) a declarative, question, or exclamatory sen-
tence.
We use word n-grams (n = 1, 2,3) and punctu-
ation symbols within 5 words before and after the
current word as binary features in the DCRF model.
As an example, Table 2 shows the tags and features
for the word “where” in the message “where |.|? i|
can |not |see |you |!|!”, where the punctuation sym-
bols after the vertical bars are the corrected symbols.
</bodyText>
<table confidence="0.992215692307692">
Tags Content
Layer 1 Question-Mark
Layer 2 Question-Begin
Features Content
unigram &lt;s&gt;@-1 where@0 i@1 can@2 not@3
see@4 you@5
bigram &lt;s&gt;+where@-1 where+i@0 i+can@1
can+not@2 not+see@3 see+you@4
you+&lt;/s&gt;@5
trigram &lt;s&gt;+where+i@-1 where+i+can@0
i+can+not@1 can+not+see@2
not+see+you@3 see+you+&lt;/s&gt;@4
punctuation .@0 !@5
</table>
<tableCaption confidence="0.993243">
Table 2: An example of tags and features used in punctu-
ation correction.
</tableCaption>
<bodyText confidence="0.909013">
Due to the lack of informal training texts with cor-
rected punctuation, we train our punctuation correc-
tion model on formal texts with synthetically cre-
ated punctuation errors. We randomly add, delete,
and substitute punctuation symbols in formal texts
with equal probabilities. Specifically, for s E {, .?!},
P(none|s) = P(, |s) = P(.|s) = P(?|s) =
P(!|s) = 0.2 denotes the probability of replacing
a punctuation symbol s (replacing s by none de-
notes deletion); and for a real word (not a punctua-
tion symbol) w, P (none|w) = P(, |w) = P(.|w) =
P(?|w) = P(!|w) = 0.2 denotes the probability
of inserting a punctuation symbol after w (inserting
none after w denotes no insertion).
</bodyText>
<subsectionHeader confidence="0.998331">
4.2 Missing Word Recovery
</subsectionHeader>
<bodyText confidence="0.999985466666667">
As shown in Section 3, some words are often omit-
ted in social media texts, e.g., the pronoun “R[I]”
in Chinese and be in English. To fix this problem,
we propose a CRF model to recover such missing
words. We explain the CRF model using be in En-
glish. The CRF model has five tags: None, BE, IS,
ARE, and AM. In an input sentence, every token (in-
cluding words, punctuation symbols, and a special
start-of-sentence placeholder) will be assigned a tag,
denoting the insertion of the form of be after the to-
ken. We use the same n-gram features as our punc-
tuation correction model, but exclude the punctua-
tion features. The model is trained on synthetically
created training texts in which be has been randomly
deleted with probability 0.5.
</bodyText>
<subsectionHeader confidence="0.999368">
4.3 A Decoder for Text Normalization
</subsectionHeader>
<bodyText confidence="0.999970481481482">
When designing our text normalization system, we
aim for a general framework that can be applied to
text normalization across different languages with
minimal effort. This is a challenging task, since so-
cial media texts in different languages exhibit differ-
ent informal characteristics, as illustrated in Section
3. Motivated by the beam-search decoders for SMT
(Koehn et al., 2007), ASR (Young et al., 2002), and
grammatical error correction (Dahlmeier and Ng,
2012), we propose a novel beam-search decoder for
normalization of social media text.
Given an input message, the normalization de-
coder searches for its best normalization, i.e., the
best hypothesis, by iteratively performing two sub-
tasks: (1) producing new sentence-level hypotheses
from hypotheses in the current stack, carried out by
hypothesis producers; and (2) evaluating the new hy-
potheses to retain good ones, carried out by feature
functions. Each hypothesis is the result of apply-
ing successive normalization operations on the ini-
tial input message, where each normalization oper-
ation is carried out by one hypothesis producer that
deals with one aspect of the informal characteristics
of social media text. The hypotheses are grouped
into stacks, where stack i stores all hypotheses ob-
tained by applying i hypothesis producers on the in-
put message. The beam-search algorithm is shown
</bodyText>
<page confidence="0.994745">
474
</page>
<figure confidence="0.8584625">
whr u
Dictionary: u=&gt;you
Punctuation
where are you ?
</figure>
<figureCaption confidence="0.98386575">
Figure 1: An example search tree when normalizing “whr
u”. The solid (dashed) boxes represent good (bad) hy-
potheses. The hypothesis producers are indicated on the
edges.
</figureCaption>
<bodyText confidence="0.7659408">
in Algorithm 1, and Figure 1 shows an example
search tree for an English message.
Algorithm 1 The beam-search decoder
INPUT: a raw message M whose length is N
RETURN: the best normalization for M
</bodyText>
<listItem confidence="0.984437777777778">
1: initialize hypothesisStacks[N+1] and hypothesisProducers;
2: add the initial hypothesis M to stack hypothesisStacks[0];
3: for i +— 0 to N-1 do
4: for each hypo in hypothesisStacks[i] do
5: for each producer in hypothesisProducers do
6: for each newHypo produced by producer from hypo do
7: add newHypo to hypothesisStacks[i+1];
8: prune hypothesisStacks[i+1];
9: return the best hypothesis in hypothesisStacks[0...N];
</listItem>
<bodyText confidence="0.99997515">
We give the details of the hypothesis producers
for Chinese and English social media texts in the
next two subsections. A number of the hypothesis
producers detect and deal with informal words w
present in a hypothesis by relying on bigram counts
of w in a large corpus of formal texts. Specifically,
a word w in a hypothesis ... w_lwwl ... is consid-
ered an informal word if both bigrams w_lw and
wwl occur infrequently (G 5) in the formal corpus.
Given a hypothesis message h, the feature func-
tions include a language model score (the normal-
ized sentence probability of h), an informal word
count penalty (the number of informal words de-
tected in h), and count feature functions. Each count
feature function gives the count of the modifications
made by a hypothesis producer. The feature func-
tions are used by the decoder to distinguish good
hypotheses from bad ones. All feature functions are
combined using a linear model to obtain the score
for a hypothesis h:
</bodyText>
<equation confidence="0.985403">
�score(h) _ Aifi(h), (1)
i
</equation>
<bodyText confidence="0.9995125">
where fi is the i-th feature function with weight Ai.
The weights of the feature functions are tuned using
the pairwise ranking optimization algorithm (Hop-
kins and May, 2011) on the development set.
</bodyText>
<subsectionHeader confidence="0.990779">
4.4 Text Normalization for Chinese
</subsectionHeader>
<bodyText confidence="0.996154857142857">
Taking into account the informal characteristics of
Chinese social media text in Section 3, we design the
following hypothesis producers for Chinese text
normalization:
Dictionary: We have manually assembled a dic-
tionary of 703 informal-formal word pairs from the
Internet. The word pairs are used to produce new
hypotheses. For example, given a hypothesis “0
[magical horse] R1 -R[time]”, if the dictionary
contains the word pair “(0 �j,À H[what])”, the
Dictionary hypothesis producer generates a new hy-
pothesis “ÀH[what] Rfl-R[time]”.
Punctuation: A punctuation correction model
(Section 4.1) is adopted to correct punctuation in
the current hypothesis, e.g., it may normalize “À
H[what] HJOR[time]” into “ÀH R JOR ?”.
Pronunciation: We use Chinese Pinyin to model
the pronunciation similarity of words. To accom-
plish this, we pair some Pinyin initials that sound
similar into a group. The groups of paired Pinyin
initials are (c, ch), (s, sh), and (z, zh). For exam-
ple, given the hypothesis “I L¬[Beijing] RP[tube]
*†[come]”, the Pinyin of the informal word “Mia
T” is “t ong z i”. The Pinyin of the formal word
“n [comrade]” is “t ong zh i”. Since the sim-
ilar sounding Pinyin initials z and zh are paired
in a group, a new hypothesis “I L ¬[Beijing] n
×[comrade] *†[come]” can be produced.
In practice, this hypothesis producer can propose
many spurious candidates w&apos; for an informal word
w. As such, after we replace w by w&apos; in the hypoth-
esis, we require that some 4-gram containing w&apos; and
its surrounding words in the hypothesis appears in a
formal corpus. We call this filtering process contex-
tual filtering.
</bodyText>
<table confidence="0.695244666666667">
whr you
Abbreviation: whr=&gt;where
be
where you
Punctuation
Be
where are you
whr you are
where you .
</table>
<page confidence="0.993133">
475
</page>
<bodyText confidence="0.999154133333333">
Pronoun: With the method of Section 4.2, a CRF
model is trained to recover the missing pronoun
“R[I]”.
Interjection: If a word w in a pre-defined list
of frequent redundant interjections appears at the
end of a sentence, we produce a new hypothesis by
removing w, e.g., from “ffR`J[ok] *[oh]” to “ff
�[ok]”.
Resegmentation: This hypothesis producer fixes
word segmentation problems. If an informal word is
a concatenation of two constituent informal words
w1 and w2 in our normalization dictionary, the in-
formal word will be segmented into two words w1
and w2. As a result, the Dictionary hypothesis pro-
ducer can subsequently normalize w1 and w2.
</bodyText>
<subsectionHeader confidence="0.933486">
4.5 Text Normalization for English
</subsectionHeader>
<bodyText confidence="0.999917148148148">
Similar to Chinese text normalization, we also cre-
ate the Dictionary, Punctuation, and Interjection hy-
pothesis producers for English text normalization.
We also add the following English-specific hypoth-
esis producers:
Pronunciation: This hypothesis producer uses
pronunciation similarity to find formal candidates
for a given informal word. It considers a word as
a sequence of letters and convert it into a sequence
of phones using phrase-based SMT trained on the
CMU pronouncing dictionary (Weide, 1998). Simi-
lar sounding phones are paired together in a group:
(ah, ao), (ow, uw), and (s, z). To illustrate, in the hy-
pothesis “wat is it”, the informal word “wat” maps
to the phone sequence “w ao t”. Since the formal
word “what” maps to the phone sequence “w ah t”
and the phones ah and ao are paired in a group, the
new hypothesis “what is it” is generated.
Be: We train a CRF model to recover missing
words be, as described in Section 4.2.
Retokenization: This hypothesis producer fixes
tokenization problems. More precisely, given an in-
formal word which is not a URL or email address
and contains a period, it splits the informal word at
the period. For example, “how r u.where r u” is nor-
malized to “how r u . where r u”.
Prefix: This hypothesis producer generates a for-
mal word w0 for an informal word w if w is a prefix
of w0. To avoid spurious candidates, we only gener-
ate w0 if JwJ &gt; 3 and Jw0J − JwJ &lt; 4.
Quotation: If an informal word ends with a letter
in (m, s, t) and if the word produced by inserting a
quotation mark before the letter is a formal word, a
new hypothesis with the quotation mark inserted is
produced. This hypothesis producer thus generates
“i’m” from “im”, “she’s” from “shes”, “isn’t” from
“isnt”, etc.
Abbreviation: Letters denoting the vowels in a
formal word are often deleted to form an infor-
mal word. This hypothesis producer generates a
formal word w0 from an informal word w if w0
can be obtained from w by adding missing vowels.
To avoid spurious candidates, we only consider w
where JwJ &gt; 2.
Time: If a number can be a potential time expres-
sion and appears after “at” or before “am” or “pm”, a
new hypothesis is produced by changing the number
into a time expression, e.g., “1130 am” is normal-
ized to “11 : 30 am”.
Since the Pronunciation, Prefix, and Abbreviation
hypothesis producers can propose spurious candi-
dates for an informal word, we also use contextual
filtering to further filter the candidates for these hy-
pothesis producers.
</bodyText>
<sectionHeader confidence="0.99977" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99747">
5.1 Evaluation Corpora
</subsectionHeader>
<bodyText confidence="0.999926380952381">
As previous work (Choudhury et al., 2007; Han and
Baldwin, 2011; Liu et al., 2012) mostly focused
on word normalization, no data is available with
corrected punctuation and recovered missing words.
We thus create the following two corpora (Table 3):
Chinese-English corpus We crawled 1,000 mes-
sages from Weibo which were first normalized into
formal Chinese and then translated into formal En-
glish. The first half of the corpus serves as our de-
velopment set to tune our text normalization decoder
for Chinese, while the second half serves as the test
set to evaluate text normalization for Chinese and
Chinese-English MT.
English-Chinese corpus From the NUS English
SMS corpus (How and Kan, 2005), we randomly se-
lected 2,000 messages. The messages were first nor-
malized into formal English and then translated into
formal Chinese. Similar to the Chinese-English cor-
pus, the first half of the corpus serves as our devel-
opment set while the second half serves as the test
set.
</bodyText>
<page confidence="0.99809">
476
</page>
<table confidence="0.998859166666667">
Corpus # messages # tokens (EN/CN/NCN)
CN2EN-dev 500 6.95K/5.45K/5.70K
CN2EN-test 500 7.14K/5.64K/5.82K
Corpus # messages # tokens (EN/CN/NEN)
EN2CN-dev 1,000 16.63K/18.14K/18.21K
EN2CN-test 1,000 16.14K/17.69K/17.76K
</table>
<tableCaption confidence="0.998027">
Table 3: Statistics of the corpora. CN2EN-dev/CN2EN-
</tableCaption>
<bodyText confidence="0.960616157894737">
test is the development/test set in our Chinese-
English experiments. EN2CN-dev/EN2CN-test is the
development/test set in our English-Chinese experi-
ments. NEN/NCN denotes manually normalized En-
glish/Chinese texts.
The formal corpus used (as described in Section
4) is the concatenation of two Chinese-English spo-
ken parallel corpora: the IWSLT 2009 corpus (Paul,
2009) and another spoken text corpus collected at
the Harbin Institute of Technology3. The language
model used for Chinese (English) text normalization
is the Chinese (English) side of the formal corpus
and the LDC Chinese (English) Gigaword corpus.
To evaluate the effect of text normalization
on MT, we build phrase-based MT systems
using Moses (Koehn et al., 2007), with word
alignments generated by GIZA++ (Och and
Ney, 2003). The MT training data contains
the above formal corpus and some LDC4 par-
allel corpora (LDC2000T46, LDC2002E18,
LDC2003E14, LDC2004E12, LDC2005T06,
LDC2005T10, LDC2007T23, LDC2008T06,
LDC2008T08, LDC2008T18, LDC2009T02,
LDC2009T06, LDC2009T15, LDC2010T03). In
total, 214M/192M English/Chinese tokens are used
to train our MT systems. The language model
of the Chinese-English (English-Chinese) MT
system is the English (Chinese) side of the FBIS
corpus (LDC2003E14) and the English (Chinese)
Gigaword corpus. Our MT systems are tuned on the
manually normalized messages of our development
sets.
Following (Aw et al., 2006; Oliva et al., 2013),
we use BLEU scores (Papineni et al., 2002) to eval-
uate text normalization. We also use BLEU scores
to evaluate MT quality. We use the sign test to de-
termine statistical significance, for both text normal-
ization and translation.
</bodyText>
<footnote confidence="0.999908">
3http://mitlab.hit.edu.cn/
4http://www.ldc.upenn.edu/Catalog/
</footnote>
<subsectionHeader confidence="0.997802">
5.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999976333333333">
We compare our text normalization decoder against
three baseline methods for performing text normal-
ization. We then send the respective normalized
texts to the same MT system to evaluate the effect
of text normalization on MT.
The simplest baseline for text normalization is
one that does no text normalization. The raw text
(un-normalized) is simply passed on to the MT sys-
tem for translation. We call this baseline ORIGINAL.
The second baseline, LATTICE, is to use a lattice
to normalize text. For each input message, a lattice
is generated in which each informal word is aug-
mented with its formal candidates taken from the
same normalization dictionary (downloaded from
Internet) used in our text normalization decoder. The
lattice is then decoded by the same language model
used in our text normalization decoder to generate
the normalized text (Stolcke, 2002). Another pos-
sible way of using lattice is to directly feed the lat-
tice to the MT system (Eidelman et al., 2011), but
since in this paper, we assume that the MT system
can only translate plain text, we leave this as future
work.
The third baseline, PBMT, is a competitive base-
line that performs text normalization via phrase-
based MT, as proposed in Aw et al. (2006). Moses
(Koehn et al., 2007) is used to perform text normal-
ization, by “translating” un-normalized text to nor-
malized text. The training data used is the same
development set used in our text normalization de-
coder. The normalized text is then sent to our MT
system for translation. This method was also used in
the SMS translation task of WMT 2011 by (Stymne,
2011).
In the tables showing experimental results, nor-
malization and translation BLEU scores that are sig-
nificantly higher than (p &lt; 0.01) the LATTICE or
PBMT baseline are in bold or underlined, respec-
tively.
</bodyText>
<subsectionHeader confidence="0.999534">
5.3 Chinese-English Experimental Results
</subsectionHeader>
<bodyText confidence="0.999185">
The Chinese-English normalization and translation
results are shown in Table 4. The first group of
experiments is the three baselines, and the second
group is an oracle experiment using manually nor-
malized messages as the output of text normaliza-
</bodyText>
<page confidence="0.995187">
477
</page>
<table confidence="0.999958777777778">
System BLEU scores (%)
Normalization MT
ORIGINAL baseline 61.01 9.06
LATTICE baseline 74.52 11.50
PBMT baseline 76.77 12.65
ORACLE 100.00 15.04
Dictionary 77.80 12.35
Punctuation 65.95 9.63
Pronunciation 61.30 9.13
Pronoun 61.11 9.01
Interjection 61.05 9.14
Resegmentation 60.98 9.03
Dictionary 77.80 12.35
+Punctuation 84.69 13.37
+Pronunciation 84.69 13.40
+Pronoun 84.96 13.50
+Interjection 85.33 13.68
+Resegmentation 86.75 14.03
</table>
<tableCaption confidence="0.999727">
Table 4: Chinese-English experimental results.
</tableCaption>
<bodyText confidence="0.99997738">
tion which indicates the theoretical upper bounds of
perfect normalization. In the normalization experi-
ments, the ORIGINAL baseline gets a BLEU score
of 61.01%, and the LATTICE baseline greatly im-
proves the ORIGINAL baseline by 13.51%, which
shows that the dictionary collected from the Inter-
net is highly effective in text normalization. The
PBMT baseline further improves the BLEU score by
2.25%. In the corresponding MT experiments, as the
normalization BLEU scores increase, the MT BLEU
scores also increase.
The third group is the isolated experiments, i.e.,
each experiment only uses one hypothesis producer.
As expected, the individual hypothesis producers
alone do not work well except the Dictionary hy-
pothesis producer. One interesting discovery is that
the Dictionary hypothesis producer outperforms the
LATTICE baseline, which shows that our normaliza-
tion decoder can utilize the dictionary more effec-
tively, probably because of the additional features
used in our normalization decoder such as the infor-
mal word penalty. The Resegmentation hypothesis
producer alone worsens the BLEU scores, since it
can only split informal words, and is designed to
work together with other hypothesis producers to
normalize words.
The last group is the combined experiments. We
add each hypothesis producer in the order of its nor-
malization effectiveness in the isolated experiments.
Adding the Punctuation hypothesis producer greatly
improves the BLEU scores of both normalization
and translation, which confirms the importance of
punctuation correction. The Pronoun and Inter-
jection hypothesis producers also contribute some
improvements. Finally, Resegmentation signifi-
cantly improves the normalization/translation BLEU
scores by 1.42%/0.35%. Compared with the isolated
experiments, the combined experiments show that
our normalization decoder can effectively integrate
different hypothesis producers to achieve better per-
formance for both text normalization and transla-
tion.
Overall, in the Chinese text normalization exper-
iments, our normalization decoder outperforms the
best baseline PBMT by 9.98% in BLEU score. In
the Chinese-English MT experiments, the normal-
ized texts output by our normalization decoder lead
to improved translation quality compared to normal-
ization by the PBMT baseline, by 1.38% in BLEU
score.
</bodyText>
<subsectionHeader confidence="0.996622">
5.4 English-Chinese Experimental Results
</subsectionHeader>
<bodyText confidence="0.999981166666667">
The English-Chinese normalization and translation
results are shown in Table 5, with the same experi-
mental setup as in the Chinese-English experiments.
The text normalization BLEU score of the ORIG-
INAL baseline is much lower in English compared
to Chinese, since the English texts contain more in-
formal words. Again, the individual hypothesis pro-
ducers alone do not work well, except the Dictio-
nary hypothesis producer. The Retokenization hy-
pothesis producer greatly improves the normaliza-
tion/translation BLEU scores by 2.37%/0.86%. The
Punctuation hypothesis producer helps less for En-
glish compared to Chinese, suggesting that our Chi-
nese texts contain noisier punctuation.
Overall, we achieved similar improvements in En-
glish text normalization and English-Chinese trans-
lation, and the improvements in BLEU scores are
7.35% and 1.35% respectively.
</bodyText>
<subsectionHeader confidence="0.934595">
5.5 Further Analysis
</subsectionHeader>
<bodyText confidence="0.9978435">
The effect of contextual filtering. To measure
the effect of contextual filtering proposed in Sec-
tion 4.4, we ran our normalization decoder with-
out contextual filtering. We obtained BLEU scores
of 65.05%/22.38% in the English-Chinese experi-
ments, which were lower than 66.54%/22.81% ob-
</bodyText>
<page confidence="0.995179">
478
</page>
<table confidence="0.999978538461539">
System BLEU scores (%)
Normalization MT
ORIGINAL baseline 37.38 13.63
LATTICE baseline 56.98 20.56
PBMT baseline 59.19 21.46
ORACLE 100.00 28.48
Dictionary 59.90 20.84
Retokenization 38.79 14.06
Prefix 38.68 13.90
Interjection 38.37 13.92
Quotation 38.04 13.65
Abbreviation 37.94 13.74
Time 37.65 13.66
Pronunciation 37.62 13.80
Punctuation 37.62 13.79
Be 37.47 13.59
Dictionary 59.90 20.84
+Retokenization 62.27 21.70
+Prefix 63.22 21.88
+Interjection 64.85 22.30
+Quotation 65.24 22.31
+Abbreviation 65.35 22.34
+Time 65.59 22.38
+Pronunciation 65.64 22.38
+Punctuation 66.38 22.74
+Be 66.54 22.81
</table>
<tableCaption confidence="0.99974">
Table 5: English-Chinese experimental results.
</tableCaption>
<bodyText confidence="0.993413210526316">
tained with contextual filtering. This shows the ben-
eficial effect of contextual filtering.
Decoding speed. The decoding speed of our text
normalization decoder was 0.2 seconds per message
on our test sets, using a 2.27 GHz Intel Xeon CPU
with 32 GB memory.
The effect of text normalization decoder on
MT. We manually analyzed the effect of our text
normalization decoder on MT. For example, given
the un-normalized English test message “yeah must
sign up , im in lt25”, our English-Chinese MT
system translated it into “ù[yeah] ,L&apos;Mf[must] M
X[sign up] , im ;(-E[in] lt25” On the other hand,
our normalization decoder normalized it into “yeah
must sign up , i ’m in lt25 .” which was then trans-
lated into “� MI , R ;(E lt25 -” by our MT
system. This example shows that our text normal-
ization decoder uses word normalization and punc-
tuation correction to improve translation.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999565">
This paper presents a novel beam-search decoder
for normalization of social media text. Our de-
coder for text normalization effectively integrates
multiple normalization operations. In our experi-
ments, we achieved statistically significant improve-
ments over two strong baselines: an improvement of
9.98%/7.35% in BLEU scores for normalization of
Chinese/English social media text, and an improve-
ment of 1.38%/1.35% in BLEU scores for transla-
tion of Chinese/English social media text. Future
work can investigate how to more tightly integrate
our beam-search decoder for text normalization with
a standard MT decoder, e.g., by using a lattice or an
n-best list.
</bodyText>
<sectionHeader confidence="0.996881" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999972666666667">
We thank all the anonymous reviewers for their com-
ments which have helped us improve this paper.
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
</bodyText>
<sectionHeader confidence="0.998283" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999394677419355">
AiTi Aw, Min Zhang, PohKhim Yeo, ZhenZhen Fan, and
Jian Su. 2005. Input normalization for an English-
to-Chinese SMS translation system. In Proceedings of
the Tenth MT Summit.
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of ACL-COLING.
Richard Beaufort, Sophie Roekhaut, Louise-Am´elie
Cougnon, and C´edrick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing SMS messages. In Proceedings of ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of WMT.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10(3-4):157–174.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
Proceedings of the Workshop on Computational Ap-
proaches to Linguistic Creativity.
Marta R. Costa-juss`a and Rafael E. Banchs. 2011. The
BM-I2R Haitian-Cr´eole-to-English translation system
description for the WMT 2011 evaluation campaign.
In Proceedings of WMT.
Daniel Dahlmeier and Hwee Tou Ng. 2012. A beam-
search decoder for grammatical error correction. In
Proceedings of EMNLP-CoNLL.
</reference>
<page confidence="0.989317">
479
</page>
<reference confidence="0.999621161904762">
Vladimir Eidelman, Kristy Hollingshead, and Philip
Resnik. 2011. Noisy SMS machine translation in low-
density languages. In Proceedings of WMT.
Jennifer Foster, ¨Ozlem C¸etinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef Van Genabith. 2011. #
hardtoparse: POS tagging and parsing the twitterverse.
In Proceedings of the AAAI Workshop On Analyzing
Microtext.
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First Workshop on Unsu-
pervised Learning in NLP.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
Proceedings of ACL-HLT.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In Proceedings of Human Computer Inter-
faces International.
Jing Huang and Geoffrey Zweig. 2002. Maximum en-
tropy model for punctuation annotation from speech.
In Proceedings of ICSLP.
Ji Hwan Kim and P. C. Woodland. 2001. The use of
prosody in a combined system for punctuation gener-
ation and speech recognition. In Proceedings of Eu-
rospeech.
Catherine Kobus, Franc¸ois Yvon, and G´eraldine
Damnati. 2008. Normalizing SMS: are two metaphors
better than one? In Proceedings of COLING.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL Demo and Poster Sessions.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Con-
ference on Machine Learning.
Zhifei Li and David Yarowsky. 2008. Mining and mod-
eling relations between formal and informal Chinese
phrases from web corpora. In Proceedings of EMNLP.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In Proceedings of ACL-HLT.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of ACL.
Wei Lu and Hwee Tou Ng. 2010. Better punctuation
prediction with dynamic conditional random fields. In
Proceedings of EMNLP.
Preslav Nakov, Chang Liu, Wei Lu, and Hwee Tou Ng.
2009. The NUS statistical machine translation system
for IWSLT 2009. In Proceedings of the International
Workshop on Spoken Language Translation.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
J. Oliva, J. I. Serrano, M. D. Del Castillo, and ´A. Igesias.
2013. A SMS normalization system integrating mul-
tiple grammatical resources. Natural Language Engi-
neering, 19(1):121–141.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL.
Michael Paul. 2009. Overview of the IWSLT 2009 eval-
uation campaign. In Proceedings of the International
Workshop on Spoken Language Translation.
Deana L. Pennell and Yang Liu. 2011. A character-
level machine translation approach for normalization
of SMS abbreviations. In Proceedings of IJCNLP.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In Proceedings of EMNLP.
Andreas Stolcke. 2002. SRILM – An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing.
Sara Stymne. 2011. Spell checking techniques for re-
placement of unknown words and data cleaning for
Haitian Creole SMS translation. In Proceedings of
WMT.
Charles Sutton, Khashayar Rohanimanesh, and Andrew
McCallum. 2004. Dynamic conditional random
fields: Factorized probabilistic models for labeling and
segmenting sequence data. In Proceedings of the 21st
International Conference on Machine Learning.
Robert L. Weide. 1998. The CMU pronouncing
dictionary. URL: http://www.speech.cs.cmu.edu/cgi-
bin/cmudict.
Yunqing Xia, Kam-Fai Wong, and Wei Gao. 2005. NIL
is not nothing: Recognition of Chinese network infor-
mal language expressions. In 4th SIGHAN Workshop
on Chinese Language Processing.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011.
Normalizing microtext. In Proceedings of the AAAI
Workshop on Analyzing Microtext.
Steve Young, Gunnar Evermann, Dan Kershaw, Gareth
Moore, Julian Odell, Dave Ollason, Valtcho Valtchev,
and Phil Woodland. 2002. The HTK book. Cam-
bridge University Engineering Department.
</reference>
<page confidence="0.976169">
480
</page>
<reference confidence="0.996383333333333">
Conghui Zhu, Jie Tang, Hang Li, Hwee Tou Ng, and Tie-
Jun Zhao. 2007. A unified tagging approach to text
normalization. In Proceedings of ACL.
</reference>
<page confidence="0.998705">
481
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.308623">
<title confidence="0.9929625">A Beam-Search Decoder for Normalization of Social Media with Application to Machine Translation</title>
<author confidence="0.823738">Pidong</author>
<affiliation confidence="0.91113">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.938944">Singapore</address>
<email confidence="0.989805">wangpd@comp.nus.edu.sg</email>
<author confidence="0.979634">Hwee Tou</author>
<affiliation confidence="0.911512333333333">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.939476">Singapore</address>
<email confidence="0.990764">nght@comp.nus.edu.sg</email>
<abstract confidence="0.98880175">Social media texts are written in an informal style, which hinders other natural language processing (NLP) applications such as machine translation. Text normalization is thus important for processing of social media text. Previous work mostly focused on normalizing words by replacing an informal word with its formal form. In this paper, to further improve other downstream NLP applications, we argue that other normalization operations should also be performed, e.g., missing word recovery and punctuation correction. A novel beam-search decoder is proposed to effectively integrate various normalization operations. Empirical results show that our system obtains statistically significant improvements over two strong baselines in both normalization and translation tasks, for both Chinese and English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AiTi Aw</author>
<author>Min Zhang</author>
<author>PohKhim Yeo</author>
<author>ZhenZhen Fan</author>
<author>Jian Su</author>
</authors>
<title>Input normalization for an Englishto-Chinese SMS translation system.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth MT Summit.</booktitle>
<contexts>
<context position="1494" citStr="Aw et al., 2005" startWordPosition="211" endWordPosition="214">ctuation correction. A novel beam-search decoder is proposed to effectively integrate various normalization operations. Empirical results show that our system obtains statistically significant improvements over two strong baselines in both normalization and translation tasks, for both Chinese and English. 1 Introduction Social media texts include SMS (Short Message Service) messages, Twitter messages, Facebook updates, etc. They are different from formal texts due to their significant informal characteristics, so they always pose difficulties for applications such as machine translation (MT) (Aw et al., 2005) and named entity recognition (Liu et al., 2011), because of a lack of training data containing informal texts. Thus, the applications always suffer from a substantial performance drop when evaluated on social media texts. For example, Ritter et al. (2011) reported a drop from 90% to 76% on part-of-speech tagging, and Foster et al. (2011) found a drop of 20% in dependency parsing. Creating training data of social media texts specifically for a text processing task is time-consuming. For example, to create parallel Chinese-English training texts for translation of social media texts, it takes t</context>
<context position="5171" citStr="Aw et al. (2005)" startWordPosition="776" endWordPosition="779">wo corpora: a Chinese corpus containing 1,000 Weibo1 messages with their normalizations and English translations; and another similar English corpus containing 2,000 SMS messages from the NUS SMS corpus (How and Kan, 2005). As far as we know, our corpora are the first publicly available Chinese/English corpora for normalization and translation of social media text2. 2 Related Work Zhu et al. (2007) performed text normalization of informally written email messages using CRF (Lafferty et al., 2001). Due to its importance, normalization of social media text has been extensively studied recently. Aw et al. (2005) proposed a noisy channel model consisting of different operations: substitution of non-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns. Choudhury et al. (2007) used hidden Markov model to perform word-level normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word formation processes. Han and Baldwin (2011) normalized informal words using 1A Chinese version of Twitter at www.weibo.co</context>
</contexts>
<marker>Aw, Zhang, Yeo, Fan, Su, 2005</marker>
<rawString>AiTi Aw, Min Zhang, PohKhim Yeo, ZhenZhen Fan, and Jian Su. 2005. Input normalization for an Englishto-Chinese SMS translation system. In Proceedings of the Tenth MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>AiTi Aw</author>
<author>Min Zhang</author>
<author>Juan Xiao</author>
<author>Jian Su</author>
</authors>
<title>A phrase-based statistical model for SMS text normalization.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-COLING.</booktitle>
<contexts>
<context position="6670" citStr="Aw et al. (2006)" startWordPosition="998" endWordPosition="1001">d a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish SMS messages using a normalization and a phonetic dictionary. For normalization of Chinese social media text, Xia et al. (2005) investigated informal phrase detection, and Li and Yarowsky (2008) mined informal-formal phrase pairs from Web corpora. All the above work focused on normalizing words. In contrast, our work also performs other normalization operations such as missing word recovery and punctuation correction, to further improve machine translation. Previously, Aw et al. (2006) adopted phrase-based MT to perform SMS normalization, and required a relatively large number of manually normalized SMS messages. In contrast, our approach performs beam search at the sentence level, and does not require large training data. We evaluate the success of social media text normalization in the context of machine translation, so research on machine translation of social media text is relevant to our work. However, there is not much comparative evaluation of social media text translation other than the Haitian Creole to English SMS translation task in the 2011 Workshop on Statistic</context>
<context position="26176" citStr="Aw et al., 2006" startWordPosition="4114" endWordPosition="4117">MT training data contains the above formal corpus and some LDC4 parallel corpora (LDC2000T46, LDC2002E18, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2007T23, LDC2008T06, LDC2008T08, LDC2008T18, LDC2009T02, LDC2009T06, LDC2009T15, LDC2010T03). In total, 214M/192M English/Chinese tokens are used to train our MT systems. The language model of the Chinese-English (English-Chinese) MT system is the English (Chinese) side of the FBIS corpus (LDC2003E14) and the English (Chinese) Gigaword corpus. Our MT systems are tuned on the manually normalized messages of our development sets. Following (Aw et al., 2006; Oliva et al., 2013), we use BLEU scores (Papineni et al., 2002) to evaluate text normalization. We also use BLEU scores to evaluate MT quality. We use the sign test to determine statistical significance, for both text normalization and translation. 3http://mitlab.hit.edu.cn/ 4http://www.ldc.upenn.edu/Catalog/ 5.2 Baselines We compare our text normalization decoder against three baseline methods for performing text normalization. We then send the respective normalized texts to the same MT system to evaluate the effect of text normalization on MT. The simplest baseline for text normalization i</context>
<context position="27732" citStr="Aw et al. (2006)" startWordPosition="4368" endWordPosition="4371">ken from the same normalization dictionary (downloaded from Internet) used in our text normalization decoder. The lattice is then decoded by the same language model used in our text normalization decoder to generate the normalized text (Stolcke, 2002). Another possible way of using lattice is to directly feed the lattice to the MT system (Eidelman et al., 2011), but since in this paper, we assume that the MT system can only translate plain text, we leave this as future work. The third baseline, PBMT, is a competitive baseline that performs text normalization via phrasebased MT, as proposed in Aw et al. (2006). Moses (Koehn et al., 2007) is used to perform text normalization, by “translating” un-normalized text to normalized text. The training data used is the same development set used in our text normalization decoder. The normalized text is then sent to our MT system for translation. This method was also used in the SMS translation task of WMT 2011 by (Stymne, 2011). In the tables showing experimental results, normalization and translation BLEU scores that are significantly higher than (p &lt; 0.01) the LATTICE or PBMT baseline are in bold or underlined, respectively. 5.3 Chinese-English Experimenta</context>
</contexts>
<marker>Aw, Zhang, Xiao, Su, 2006</marker>
<rawString>AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for SMS text normalization. In Proceedings of ACL-COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Beaufort</author>
</authors>
<title>Sophie Roekhaut, Louise-Am´elie Cougnon, and C´edrick Fairon.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Beaufort, 2010</marker>
<rawString>Richard Beaufort, Sophie Roekhaut, Louise-Am´elie Cougnon, and C´edrick Fairon. 2010. A hybrid rule/model-based finite-state framework for normalizing SMS messages. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of WMT.</booktitle>
<contexts>
<context position="7333" citStr="Callison-Burch et al., 2011" startWordPosition="1104" endWordPosition="1107"> SMS normalization, and required a relatively large number of manually normalized SMS messages. In contrast, our approach performs beam search at the sentence level, and does not require large training data. We evaluate the success of social media text normalization in the context of machine translation, so research on machine translation of social media text is relevant to our work. However, there is not much comparative evaluation of social media text translation other than the Haitian Creole to English SMS translation task in the 2011 Workshop on Statistical Machine Translation (WMT 2011) (Callison-Burch et al., 2011). However, the setup of the WMT 2011 task is different from ours, in that the task provided parallel training data of SMS texts and their translations. As such, text normalization is not necessary in that task. For example, the best reported system in that task (Costa-juss`a and Banchs, 2011) did not perform SMS message normalization. In speech to speech translation (Paul, 2009; Nakov et al., 2009), the input texts contain wrongly transcribed words due to errors in automatic speech recognition, whereas social media texts contain abbreviations, new words, etc. Although the input texts in both c</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Animesh Mukherjee</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Investigation and modeling of the structure of texting language.</title>
<date>2007</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<pages>10--3</pages>
<contexts>
<context position="5382" citStr="Choudhury et al. (2007)" startWordPosition="809" endWordPosition="812">How and Kan, 2005). As far as we know, our corpora are the first publicly available Chinese/English corpora for normalization and translation of social media text2. 2 Related Work Zhu et al. (2007) performed text normalization of informally written email messages using CRF (Lafferty et al., 2001). Due to its importance, normalization of social media text has been extensively studied recently. Aw et al. (2005) proposed a noisy channel model consisting of different operations: substitution of non-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns. Choudhury et al. (2007) used hidden Markov model to perform word-level normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word formation processes. Han and Baldwin (2011) normalized informal words using 1A Chinese version of Twitter at www.weibo.com 2Available at www.comp.nus.edu.sg/∼nlp/corpora.html morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthograph</context>
<context position="23557" citStr="Choudhury et al., 2007" startWordPosition="3724" endWordPosition="3727">d from w by adding missing vowels. To avoid spurious candidates, we only consider w where JwJ &gt; 2. Time: If a number can be a potential time expression and appears after “at” or before “am” or “pm”, a new hypothesis is produced by changing the number into a time expression, e.g., “1130 am” is normalized to “11 : 30 am”. Since the Pronunciation, Prefix, and Abbreviation hypothesis producers can propose spurious candidates for an informal word, we also use contextual filtering to further filter the candidates for these hypothesis producers. 5 Experiments 5.1 Evaluation Corpora As previous work (Choudhury et al., 2007; Han and Baldwin, 2011; Liu et al., 2012) mostly focused on word normalization, no data is available with corrected punctuation and recovered missing words. We thus create the following two corpora (Table 3): Chinese-English corpus We crawled 1,000 messages from Weibo which were first normalized into formal Chinese and then translated into formal English. The first half of the corpus serves as our development set to tune our text normalization decoder for Chinese, while the second half serves as the test set to evaluate text normalization for Chinese and Chinese-English MT. English-Chinese co</context>
</contexts>
<marker>Choudhury, Saraf, Jain, Mukherjee, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. International Journal on Document Analysis and Recognition, 10(3-4):157–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An unsupervised model for text message normalization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity.</booktitle>
<contexts>
<context position="5581" citStr="Cook and Stevenson (2009)" startWordPosition="841" endWordPosition="844"> performed text normalization of informally written email messages using CRF (Lafferty et al., 2001). Due to its importance, normalization of social media text has been extensively studied recently. Aw et al. (2005) proposed a noisy channel model consisting of different operations: substitution of non-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns. Choudhury et al. (2007) used hidden Markov model to perform word-level normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word formation processes. Han and Baldwin (2011) normalized informal words using 1A Chinese version of Twitter at www.weibo.com 2Available at www.comp.nus.edu.sg/∼nlp/corpora.html morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish S</context>
</contexts>
<marker>Cook, Stevenson, 2009</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta R Costa-juss`a</author>
<author>Rafael E Banchs</author>
</authors>
<title>The BM-I2R Haitian-Cr´eole-to-English translation system description for the WMT 2011 evaluation campaign.</title>
<date>2011</date>
<booktitle>In Proceedings of WMT.</booktitle>
<marker>Costa-juss`a, Banchs, 2011</marker>
<rawString>Marta R. Costa-juss`a and Rafael E. Banchs. 2011. The BM-I2R Haitian-Cr´eole-to-English translation system description for the WMT 2011 evaluation campaign. In Proceedings of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A beamsearch decoder for grammatical error correction.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="15637" citStr="Dahlmeier and Ng, 2012" startWordPosition="2394" endWordPosition="2397">e model is trained on synthetically created training texts in which be has been randomly deleted with probability 0.5. 4.3 A Decoder for Text Normalization When designing our text normalization system, we aim for a general framework that can be applied to text normalization across different languages with minimal effort. This is a challenging task, since social media texts in different languages exhibit different informal characteristics, as illustrated in Section 3. Motivated by the beam-search decoders for SMT (Koehn et al., 2007), ASR (Young et al., 2002), and grammatical error correction (Dahlmeier and Ng, 2012), we propose a novel beam-search decoder for normalization of social media text. Given an input message, the normalization decoder searches for its best normalization, i.e., the best hypothesis, by iteratively performing two subtasks: (1) producing new sentence-level hypotheses from hypotheses in the current stack, carried out by hypothesis producers; and (2) evaluating the new hypotheses to retain good ones, carried out by feature functions. Each hypothesis is the result of applying successive normalization operations on the initial input message, where each normalization operation is carried</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012. A beamsearch decoder for grammatical error correction. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Kristy Hollingshead</author>
<author>Philip Resnik</author>
</authors>
<title>Noisy SMS machine translation in lowdensity languages.</title>
<date>2011</date>
<booktitle>In Proceedings of WMT.</booktitle>
<contexts>
<context position="27479" citStr="Eidelman et al., 2011" startWordPosition="4322" endWordPosition="4325">sed on to the MT system for translation. We call this baseline ORIGINAL. The second baseline, LATTICE, is to use a lattice to normalize text. For each input message, a lattice is generated in which each informal word is augmented with its formal candidates taken from the same normalization dictionary (downloaded from Internet) used in our text normalization decoder. The lattice is then decoded by the same language model used in our text normalization decoder to generate the normalized text (Stolcke, 2002). Another possible way of using lattice is to directly feed the lattice to the MT system (Eidelman et al., 2011), but since in this paper, we assume that the MT system can only translate plain text, we leave this as future work. The third baseline, PBMT, is a competitive baseline that performs text normalization via phrasebased MT, as proposed in Aw et al. (2006). Moses (Koehn et al., 2007) is used to perform text normalization, by “translating” un-normalized text to normalized text. The training data used is the same development set used in our text normalization decoder. The normalized text is then sent to our MT system for translation. This method was also used in the SMS translation task of WMT 2011</context>
</contexts>
<marker>Eidelman, Hollingshead, Resnik, 2011</marker>
<rawString>Vladimir Eidelman, Kristy Hollingshead, and Philip Resnik. 2011. Noisy SMS machine translation in lowdensity languages. In Proceedings of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>¨Ozlem C¸etinoglu</author>
<author>Joachim Wagner</author>
<author>Joseph Le Roux</author>
<author>Stephen Hogan</author>
<author>Joakim Nivre</author>
<author>Deirdre Hogan</author>
<author>Josef Van Genabith</author>
</authors>
<title>hardtoparse: POS tagging and parsing the twitterverse.</title>
<date>2011</date>
<booktitle>In Proceedings of the AAAI Workshop On Analyzing Microtext.</booktitle>
<marker>Foster, C¸etinoglu, Wagner, Le Roux, Hogan, Nivre, Hogan, Van Genabith, 2011</marker>
<rawString>Jennifer Foster, ¨Ozlem C¸etinoglu, Joachim Wagner, Joseph Le Roux, Stephen Hogan, Joakim Nivre, Deirdre Hogan, and Josef Van Genabith. 2011. # hardtoparse: POS tagging and parsing the twitterverse. In Proceedings of the AAAI Workshop On Analyzing Microtext.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Dirk Hovy</author>
<author>Donald Metzler</author>
</authors>
<title>Unsupervised mining of lexical variants from noisy text.</title>
<date>2011</date>
<booktitle>In Proceedings of the First Workshop on Unsupervised Learning in NLP.</booktitle>
<contexts>
<context position="2830" citStr="Gouws et al., 2011" startWordPosition="424" endWordPosition="427">. On the other hand, it takes thirty seconds to normalize the same message, a six-fold increase in speed. After training a text normalization system to normalize social media texts, we can use an existing statistical machine translation (SMT) system trained on normal texts (non-social media texts) to carry out translation. So we argue that normalization followed by regular translation is a more practical approach. Thus, text normalization is important for social media text processing. Most previous work on normalization of social media text focused on word substitution (Beaufort et al., 2010; Gouws et al., 2011; Han and Baldwin, 2011; Liu et al., 2012). However, we argue that some other normalization operations besides word substitution are also critical for subsequent natural language processing (NLP) applications, such as missing word recovery (e.g., zero pronouns) and punctuation correction. In this paper, we propose a novel beam-search decoder for normalization of social media text for MT. Our decoder can effectively integrate different normalization operations together. In contrast to previous work, some of our normalization operations are specifically designed for MT, e.g., missing word recove</context>
</contexts>
<marker>Gouws, Hovy, Metzler, 2011</marker>
<rawString>Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011. Unsupervised mining of lexical variants from noisy text. In Proceedings of the First Workshop on Unsupervised Learning in NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="2853" citStr="Han and Baldwin, 2011" startWordPosition="428" endWordPosition="431"> it takes thirty seconds to normalize the same message, a six-fold increase in speed. After training a text normalization system to normalize social media texts, we can use an existing statistical machine translation (SMT) system trained on normal texts (non-social media texts) to carry out translation. So we argue that normalization followed by regular translation is a more practical approach. Thus, text normalization is important for social media text processing. Most previous work on normalization of social media text focused on word substitution (Beaufort et al., 2010; Gouws et al., 2011; Han and Baldwin, 2011; Liu et al., 2012). However, we argue that some other normalization operations besides word substitution are also critical for subsequent natural language processing (NLP) applications, such as missing word recovery (e.g., zero pronouns) and punctuation correction. In this paper, we propose a novel beam-search decoder for normalization of social media text for MT. Our decoder can effectively integrate different normalization operations together. In contrast to previous work, some of our normalization operations are specifically designed for MT, e.g., missing word recovery based on conditional</context>
<context position="5693" citStr="Han and Baldwin (2011)" startWordPosition="857" endWordPosition="860">portance, normalization of social media text has been extensively studied recently. Aw et al. (2005) proposed a noisy channel model consisting of different operations: substitution of non-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns. Choudhury et al. (2007) used hidden Markov model to perform word-level normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word formation processes. Han and Baldwin (2011) normalized informal words using 1A Chinese version of Twitter at www.weibo.com 2Available at www.comp.nus.edu.sg/∼nlp/corpora.html morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish SMS messages using a normalization and a phonetic dictionary. For normalization of Chinese social media text, Xia</context>
<context position="23580" citStr="Han and Baldwin, 2011" startWordPosition="3728" endWordPosition="3731">ng vowels. To avoid spurious candidates, we only consider w where JwJ &gt; 2. Time: If a number can be a potential time expression and appears after “at” or before “am” or “pm”, a new hypothesis is produced by changing the number into a time expression, e.g., “1130 am” is normalized to “11 : 30 am”. Since the Pronunciation, Prefix, and Abbreviation hypothesis producers can propose spurious candidates for an informal word, we also use contextual filtering to further filter the candidates for these hypothesis producers. 5 Experiments 5.1 Evaluation Corpora As previous work (Choudhury et al., 2007; Han and Baldwin, 2011; Liu et al., 2012) mostly focused on word normalization, no data is available with corrected punctuation and recovered missing words. We thus create the following two corpora (Table 3): Chinese-English corpus We crawled 1,000 messages from Weibo which were first normalized into formal Chinese and then translated into formal English. The first half of the corpus serves as our development set to tune our text normalization decoder for Chinese, while the second half serves as the test set to evaluate text normalization for Chinese and Chinese-English MT. English-Chinese corpus From the NUS Engli</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="18538" citStr="Hopkins and May, 2011" startWordPosition="2866" endWordPosition="2870">ed sentence probability of h), an informal word count penalty (the number of informal words detected in h), and count feature functions. Each count feature function gives the count of the modifications made by a hypothesis producer. The feature functions are used by the decoder to distinguish good hypotheses from bad ones. All feature functions are combined using a linear model to obtain the score for a hypothesis h: �score(h) _ Aifi(h), (1) i where fi is the i-th feature function with weight Ai. The weights of the feature functions are tuned using the pairwise ranking optimization algorithm (Hopkins and May, 2011) on the development set. 4.4 Text Normalization for Chinese Taking into account the informal characteristics of Chinese social media text in Section 3, we design the following hypothesis producers for Chinese text normalization: Dictionary: We have manually assembled a dictionary of 703 informal-formal word pairs from the Internet. The word pairs are used to produce new hypotheses. For example, given a hypothesis “0 [magical horse] R1 -R[time]”, if the dictionary contains the word pair “(0 �j,À H[what])”, the Dictionary hypothesis producer generates a new hypothesis “ÀH[what] Rfl-R[time]”. Pun</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijue How</author>
<author>Min-Yen Kan</author>
</authors>
<title>Optimizing predictive text entry for short message service on mobile phones.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Computer Interfaces International.</booktitle>
<contexts>
<context position="4777" citStr="How and Kan, 2005" startWordPosition="712" endWordPosition="715">ormalization approach for different languages, we experiment with two languages, Chinese and English. We achieved statistically significant improvements over two strong baselines: an improvement of 9.98%/7.35% in BLEU scores for normalization of Chinese/English social media text, and an improvement of 1.38%/1.35% in BLEU scores for translation of Chinese/English social media text. We created two corpora: a Chinese corpus containing 1,000 Weibo1 messages with their normalizations and English translations; and another similar English corpus containing 2,000 SMS messages from the NUS SMS corpus (How and Kan, 2005). As far as we know, our corpora are the first publicly available Chinese/English corpora for normalization and translation of social media text2. 2 Related Work Zhu et al. (2007) performed text normalization of informally written email messages using CRF (Lafferty et al., 2001). Due to its importance, normalization of social media text has been extensively studied recently. Aw et al. (2005) proposed a noisy channel model consisting of different operations: substitution of non-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns. Choudhury et al. (</context>
<context position="10005" citStr="How and Kan, 2005" startWordPosition="1510" endWordPosition="1513">s is shown in square brackets. Omitted, extraneous, and misused punctuation symbols occur frequently. On average, each Chinese message contains only less than one informal word, and many informal words are either new words or existing words with new meaning. The messages also contain redundant interjections like “*[oh]”. Pronouns are often omitted in Chinese messages, especially for “R[I]”. Chinese informal words can be wrongly segmented due to lack of word segmentation training data containing informal words. Similarly, 200 English SMS messages were randomly selected from the NUS SMS corpus (How and Kan, 2005). The informal characteristics of these messages are shown in the second half of Table 1. We found that our English messages contain more informal words than Chinese messages. English words are shortened in three ways: (1) using a shorter word form with similar pronunciation; (2) abbreviating a formal word; and (3) using only a prefix of a formal word. Other informal characteristics include: (1) informal punctuation conventions inAs can be seen in Section 3, social media texts of different languages exhibit different informal characteristics. For example, English messages have more informal wo</context>
<context position="24213" citStr="How and Kan, 2005" startWordPosition="3830" endWordPosition="3833">012) mostly focused on word normalization, no data is available with corrected punctuation and recovered missing words. We thus create the following two corpora (Table 3): Chinese-English corpus We crawled 1,000 messages from Weibo which were first normalized into formal Chinese and then translated into formal English. The first half of the corpus serves as our development set to tune our text normalization decoder for Chinese, while the second half serves as the test set to evaluate text normalization for Chinese and Chinese-English MT. English-Chinese corpus From the NUS English SMS corpus (How and Kan, 2005), we randomly selected 2,000 messages. The messages were first normalized into formal English and then translated into formal Chinese. Similar to the Chinese-English corpus, the first half of the corpus serves as our development set while the second half serves as the test set. 476 Corpus # messages # tokens (EN/CN/NCN) CN2EN-dev 500 6.95K/5.45K/5.70K CN2EN-test 500 7.14K/5.64K/5.82K Corpus # messages # tokens (EN/CN/NEN) EN2CN-dev 1,000 16.63K/18.14K/18.21K EN2CN-test 1,000 16.14K/17.69K/17.76K Table 3: Statistics of the corpora. CN2EN-dev/CN2ENtest is the development/test set in our ChineseE</context>
</contexts>
<marker>How, Kan, 2005</marker>
<rawString>Yijue How and Min-Yen Kan. 2005. Optimizing predictive text entry for short message service on mobile phones. In Proceedings of Human Computer Interfaces International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Huang</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Maximum entropy model for punctuation annotation from speech.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP.</booktitle>
<contexts>
<context position="12152" citStr="Huang and Zweig, 2002" startWordPosition="1839" endWordPosition="1842">ection is also important besides word normalization, as the subsequent NLP applications are typically trained on formal texts with correct punctuation. We define punctuation correction as correcting punctuation in sentences which may have no or unreliable punctuation. The task performs three punctuation operations: insertion, deletion, and substitution. To our knowledge, no previous work has been done on punctuation correction for normalization of social media text. In ASR, punctuation prediction only inserts punctuation symbols into ASR output that has no punctuation (Kim and Woodland, 2001; Huang and Zweig, 2002), but without punctuation deletion or substitution. Lu and Ng (2010) argued that punctuation prediction should be jointly per473 formed with sentence boundary detection, so they modeled punctuation prediction using a two-layer DCRF model (Sutton et al., 2004). We also believe that punctuation correction is closely related to sentence boundary detection. Thus, we propose a two-layer DCRF model for punctuation correction. Layer 1 gives the actual punctuation tags None, Comma, Period, QuestionMark, and Exclamatory-Mark. Layer 2 gives the sentence boundary, including tags DeclarativeBegin, Declara</context>
</contexts>
<marker>Huang, Zweig, 2002</marker>
<rawString>Jing Huang and Geoffrey Zweig. 2002. Maximum entropy model for punctuation annotation from speech. In Proceedings of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji Hwan Kim</author>
<author>P C Woodland</author>
</authors>
<title>The use of prosody in a combined system for punctuation generation and speech recognition.</title>
<date>2001</date>
<booktitle>In Proceedings of Eurospeech.</booktitle>
<contexts>
<context position="12128" citStr="Kim and Woodland, 2001" startWordPosition="1835" endWordPosition="1838">a text, punctuation correction is also important besides word normalization, as the subsequent NLP applications are typically trained on formal texts with correct punctuation. We define punctuation correction as correcting punctuation in sentences which may have no or unreliable punctuation. The task performs three punctuation operations: insertion, deletion, and substitution. To our knowledge, no previous work has been done on punctuation correction for normalization of social media text. In ASR, punctuation prediction only inserts punctuation symbols into ASR output that has no punctuation (Kim and Woodland, 2001; Huang and Zweig, 2002), but without punctuation deletion or substitution. Lu and Ng (2010) argued that punctuation prediction should be jointly per473 formed with sentence boundary detection, so they modeled punctuation prediction using a two-layer DCRF model (Sutton et al., 2004). We also believe that punctuation correction is closely related to sentence boundary detection. Thus, we propose a two-layer DCRF model for punctuation correction. Layer 1 gives the actual punctuation tags None, Comma, Period, QuestionMark, and Exclamatory-Mark. Layer 2 gives the sentence boundary, including tags D</context>
</contexts>
<marker>Kim, Woodland, 2001</marker>
<rawString>Ji Hwan Kim and P. C. Woodland. 2001. The use of prosody in a combined system for punctuation generation and speech recognition. In Proceedings of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Kobus</author>
<author>Franc¸ois Yvon</author>
<author>G´eraldine Damnati</author>
</authors>
<title>Normalizing SMS: are two metaphors better than one?</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="5464" citStr="Kobus et al. (2008)" startWordPosition="823" endWordPosition="826">inese/English corpora for normalization and translation of social media text2. 2 Related Work Zhu et al. (2007) performed text normalization of informally written email messages using CRF (Lafferty et al., 2001). Due to its importance, normalization of social media text has been extensively studied recently. Aw et al. (2005) proposed a noisy channel model consisting of different operations: substitution of non-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns. Choudhury et al. (2007) used hidden Markov model to perform word-level normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word formation processes. Han and Baldwin (2011) normalized informal words using 1A Chinese version of Twitter at www.weibo.com 2Available at www.comp.nus.edu.sg/∼nlp/corpora.html morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system</context>
</contexts>
<marker>Kobus, Yvon, Damnati, 2008</marker>
<rawString>Catherine Kobus, Franc¸ois Yvon, and G´eraldine Damnati. 2008. Normalizing SMS: are two metaphors better than one? In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL Demo</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="15552" citStr="Koehn et al., 2007" startWordPosition="2381" endWordPosition="2384">res as our punctuation correction model, but exclude the punctuation features. The model is trained on synthetically created training texts in which be has been randomly deleted with probability 0.5. 4.3 A Decoder for Text Normalization When designing our text normalization system, we aim for a general framework that can be applied to text normalization across different languages with minimal effort. This is a challenging task, since social media texts in different languages exhibit different informal characteristics, as illustrated in Section 3. Motivated by the beam-search decoders for SMT (Koehn et al., 2007), ASR (Young et al., 2002), and grammatical error correction (Dahlmeier and Ng, 2012), we propose a novel beam-search decoder for normalization of social media text. Given an input message, the normalization decoder searches for its best normalization, i.e., the best hypothesis, by iteratively performing two subtasks: (1) producing new sentence-level hypotheses from hypotheses in the current stack, carried out by hypothesis producers; and (2) evaluating the new hypotheses to retain good ones, carried out by feature functions. Each hypothesis is the result of applying successive normalization o</context>
<context position="25493" citStr="Koehn et al., 2007" startWordPosition="4019" endWordPosition="4022">est set in our English-Chinese experiments. NEN/NCN denotes manually normalized English/Chinese texts. The formal corpus used (as described in Section 4) is the concatenation of two Chinese-English spoken parallel corpora: the IWSLT 2009 corpus (Paul, 2009) and another spoken text corpus collected at the Harbin Institute of Technology3. The language model used for Chinese (English) text normalization is the Chinese (English) side of the formal corpus and the LDC Chinese (English) Gigaword corpus. To evaluate the effect of text normalization on MT, we build phrase-based MT systems using Moses (Koehn et al., 2007), with word alignments generated by GIZA++ (Och and Ney, 2003). The MT training data contains the above formal corpus and some LDC4 parallel corpora (LDC2000T46, LDC2002E18, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2007T23, LDC2008T06, LDC2008T08, LDC2008T18, LDC2009T02, LDC2009T06, LDC2009T15, LDC2010T03). In total, 214M/192M English/Chinese tokens are used to train our MT systems. The language model of the Chinese-English (English-Chinese) MT system is the English (Chinese) side of the FBIS corpus (LDC2003E14) and the English (Chinese) Gigaword corpus. Our MT systems are tuned on t</context>
<context position="27760" citStr="Koehn et al., 2007" startWordPosition="4373" endWordPosition="4376">zation dictionary (downloaded from Internet) used in our text normalization decoder. The lattice is then decoded by the same language model used in our text normalization decoder to generate the normalized text (Stolcke, 2002). Another possible way of using lattice is to directly feed the lattice to the MT system (Eidelman et al., 2011), but since in this paper, we assume that the MT system can only translate plain text, we leave this as future work. The third baseline, PBMT, is a competitive baseline that performs text normalization via phrasebased MT, as proposed in Aw et al. (2006). Moses (Koehn et al., 2007) is used to perform text normalization, by “translating” un-normalized text to normalized text. The training data used is the same development set used in our text normalization decoder. The normalized text is then sent to our MT system for translation. This method was also used in the SMS translation task of WMT 2011 by (Stymne, 2011). In the tables showing experimental results, normalization and translation BLEU scores that are significantly higher than (p &lt; 0.01) the LATTICE or PBMT baseline are in bold or underlined, respectively. 5.3 Chinese-English Experimental Results The Chinese-Englis</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL Demo and Poster Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="3630" citStr="Lafferty et al., 2001" startWordPosition="539" endWordPosition="542"> processing (NLP) applications, such as missing word recovery (e.g., zero pronouns) and punctuation correction. In this paper, we propose a novel beam-search decoder for normalization of social media text for MT. Our decoder can effectively integrate different normalization operations together. In contrast to previous work, some of our normalization operations are specifically designed for MT, e.g., missing word recovery based on conditional random fields 471 Proceedings of NAACL-HLT 2013, pages 471–481, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics (CRF) (Lafferty et al., 2001) and punctuation correction based on dynamic conditional random fields (DCRF) (Sutton et al., 2004). To the best of our knowledge, our work is the first to perform missing word recovery and punctuation correction for normalization of social media text, and also the first to perform message-level normalization of Chinese social media text. We investigate the effects on translating social media text after addressing various characteristics of informal social media text through normalization. To show the applicability of our normalization approach for different languages, we experiment with two l</context>
<context position="5056" citStr="Lafferty et al., 2001" startWordPosition="756" endWordPosition="760">text, and an improvement of 1.38%/1.35% in BLEU scores for translation of Chinese/English social media text. We created two corpora: a Chinese corpus containing 1,000 Weibo1 messages with their normalizations and English translations; and another similar English corpus containing 2,000 SMS messages from the NUS SMS corpus (How and Kan, 2005). As far as we know, our corpora are the first publicly available Chinese/English corpora for normalization and translation of social media text2. 2 Related Work Zhu et al. (2007) performed text normalization of informally written email messages using CRF (Lafferty et al., 2001). Due to its importance, normalization of social media text has been extensively studied recently. Aw et al. (2005) proposed a noisy channel model consisting of different operations: substitution of non-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns. Choudhury et al. (2007) used hidden Markov model to perform word-level normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word format</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>David Yarowsky</author>
</authors>
<title>Mining and modeling relations between formal and informal Chinese phrases from web corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6374" citStr="Li and Yarowsky (2008)" startWordPosition="953" endWordPosition="956">ter at www.weibo.com 2Available at www.comp.nus.edu.sg/∼nlp/corpora.html morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish SMS messages using a normalization and a phonetic dictionary. For normalization of Chinese social media text, Xia et al. (2005) investigated informal phrase detection, and Li and Yarowsky (2008) mined informal-formal phrase pairs from Web corpora. All the above work focused on normalizing words. In contrast, our work also performs other normalization operations such as missing word recovery and punctuation correction, to further improve machine translation. Previously, Aw et al. (2006) adopted phrase-based MT to perform SMS normalization, and required a relatively large number of manually normalized SMS messages. In contrast, our approach performs beam search at the sentence level, and does not require large training data. We evaluate the success of social media text normalization in</context>
</contexts>
<marker>Li, Yarowsky, 2008</marker>
<rawString>Zhifei Li and David Yarowsky. 2008. Mining and modeling relations between formal and informal Chinese phrases from web corpora. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Shaodian Zhang</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
</authors>
<title>Recognizing named entities in tweets.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="1542" citStr="Liu et al., 2011" startWordPosition="219" endWordPosition="222">r is proposed to effectively integrate various normalization operations. Empirical results show that our system obtains statistically significant improvements over two strong baselines in both normalization and translation tasks, for both Chinese and English. 1 Introduction Social media texts include SMS (Short Message Service) messages, Twitter messages, Facebook updates, etc. They are different from formal texts due to their significant informal characteristics, so they always pose difficulties for applications such as machine translation (MT) (Aw et al., 2005) and named entity recognition (Liu et al., 2011), because of a lack of training data containing informal texts. Thus, the applications always suffer from a substantial performance drop when evaluated on social media texts. For example, Ritter et al. (2011) reported a drop from 90% to 76% on part-of-speech tagging, and Foster et al. (2011) found a drop of 20% in dependency parsing. Creating training data of social media texts specifically for a text processing task is time-consuming. For example, to create parallel Chinese-English training texts for translation of social media texts, it takes three minutes on average to translate an informal</context>
</contexts>
<marker>Liu, Zhang, Wei, Zhou, 2011</marker>
<rawString>Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming Zhou. 2011. Recognizing named entities in tweets. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Xiao Jiang</author>
</authors>
<title>A broadcoverage normalization system for social media language.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2872" citStr="Liu et al., 2012" startWordPosition="432" endWordPosition="435">s to normalize the same message, a six-fold increase in speed. After training a text normalization system to normalize social media texts, we can use an existing statistical machine translation (SMT) system trained on normal texts (non-social media texts) to carry out translation. So we argue that normalization followed by regular translation is a more practical approach. Thus, text normalization is important for social media text processing. Most previous work on normalization of social media text focused on word substitution (Beaufort et al., 2010; Gouws et al., 2011; Han and Baldwin, 2011; Liu et al., 2012). However, we argue that some other normalization operations besides word substitution are also critical for subsequent natural language processing (NLP) applications, such as missing word recovery (e.g., zero pronouns) and punctuation correction. In this paper, we propose a novel beam-search decoder for normalization of social media text for MT. Our decoder can effectively integrate different normalization operations together. In contrast to previous work, some of our normalization operations are specifically designed for MT, e.g., missing word recovery based on conditional random fields 471 </context>
<context position="6046" citStr="Liu et al. (2012)" startWordPosition="902" endWordPosition="905"> normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word formation processes. Han and Baldwin (2011) normalized informal words using 1A Chinese version of Twitter at www.weibo.com 2Available at www.comp.nus.edu.sg/∼nlp/corpora.html morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish SMS messages using a normalization and a phonetic dictionary. For normalization of Chinese social media text, Xia et al. (2005) investigated informal phrase detection, and Li and Yarowsky (2008) mined informal-formal phrase pairs from Web corpora. All the above work focused on normalizing words. In contrast, our work also performs other normalization operations such as missing word recovery and punctuation correction, to further improve machine translation. Prev</context>
<context position="23599" citStr="Liu et al., 2012" startWordPosition="3732" endWordPosition="3735">rious candidates, we only consider w where JwJ &gt; 2. Time: If a number can be a potential time expression and appears after “at” or before “am” or “pm”, a new hypothesis is produced by changing the number into a time expression, e.g., “1130 am” is normalized to “11 : 30 am”. Since the Pronunciation, Prefix, and Abbreviation hypothesis producers can propose spurious candidates for an informal word, we also use contextual filtering to further filter the candidates for these hypothesis producers. 5 Experiments 5.1 Evaluation Corpora As previous work (Choudhury et al., 2007; Han and Baldwin, 2011; Liu et al., 2012) mostly focused on word normalization, no data is available with corrected punctuation and recovered missing words. We thus create the following two corpora (Table 3): Chinese-English corpus We crawled 1,000 messages from Weibo which were first normalized into formal Chinese and then translated into formal English. The first half of the corpus serves as our development set to tune our text normalization decoder for Chinese, while the second half serves as the test set to evaluate text normalization for Chinese and Chinese-English MT. English-Chinese corpus From the NUS English SMS corpus (How </context>
</contexts>
<marker>Liu, Weng, Jiang, 2012</marker>
<rawString>Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broadcoverage normalization system for social media language. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better punctuation prediction with dynamic conditional random fields.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="12220" citStr="Lu and Ng (2010)" startWordPosition="1849" endWordPosition="1852"> applications are typically trained on formal texts with correct punctuation. We define punctuation correction as correcting punctuation in sentences which may have no or unreliable punctuation. The task performs three punctuation operations: insertion, deletion, and substitution. To our knowledge, no previous work has been done on punctuation correction for normalization of social media text. In ASR, punctuation prediction only inserts punctuation symbols into ASR output that has no punctuation (Kim and Woodland, 2001; Huang and Zweig, 2002), but without punctuation deletion or substitution. Lu and Ng (2010) argued that punctuation prediction should be jointly per473 formed with sentence boundary detection, so they modeled punctuation prediction using a two-layer DCRF model (Sutton et al., 2004). We also believe that punctuation correction is closely related to sentence boundary detection. Thus, we propose a two-layer DCRF model for punctuation correction. Layer 1 gives the actual punctuation tags None, Comma, Period, QuestionMark, and Exclamatory-Mark. Layer 2 gives the sentence boundary, including tags DeclarativeBegin, Declarative-In, Question-Begin, Question-In, Exclamatory-Begin, and Exclama</context>
</contexts>
<marker>Lu, Ng, 2010</marker>
<rawString>Wei Lu and Hwee Tou Ng. 2010. Better punctuation prediction with dynamic conditional random fields. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Chang Liu</author>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>The NUS statistical machine translation system for IWSLT</title>
<date>2009</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="7734" citStr="Nakov et al., 2009" startWordPosition="1171" endWordPosition="1174">uch comparative evaluation of social media text translation other than the Haitian Creole to English SMS translation task in the 2011 Workshop on Statistical Machine Translation (WMT 2011) (Callison-Burch et al., 2011). However, the setup of the WMT 2011 task is different from ours, in that the task provided parallel training data of SMS texts and their translations. As such, text normalization is not necessary in that task. For example, the best reported system in that task (Costa-juss`a and Banchs, 2011) did not perform SMS message normalization. In speech to speech translation (Paul, 2009; Nakov et al., 2009), the input texts contain wrongly transcribed words due to errors in automatic speech recognition, whereas social media texts contain abbreviations, new words, etc. Although the input texts in both cases deviate from normal texts, the exact deviations are different. 472 Category Freq. Example cluding omitted and misused punctuation; (2) redunPunctuation 81 t*[hi] —(t* o[hi .]); dant interjections; (3) quotation-related problems, Pronunciation 47 �k[watch](TTc[don’t]); V (i-_X4-T[this]); e.g., omitted quotation marks; (4) “be” omission; New word 43 _ffi[bud](7t[cute]); (5) tokenization problems</context>
</contexts>
<marker>Nakov, Liu, Lu, Ng, 2009</marker>
<rawString>Preslav Nakov, Chang Liu, Wei Lu, and Hwee Tou Ng. 2009. The NUS statistical machine translation system for IWSLT 2009. In Proceedings of the International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="25555" citStr="Och and Ney, 2003" startWordPosition="4029" endWordPosition="4032">ually normalized English/Chinese texts. The formal corpus used (as described in Section 4) is the concatenation of two Chinese-English spoken parallel corpora: the IWSLT 2009 corpus (Paul, 2009) and another spoken text corpus collected at the Harbin Institute of Technology3. The language model used for Chinese (English) text normalization is the Chinese (English) side of the formal corpus and the LDC Chinese (English) Gigaword corpus. To evaluate the effect of text normalization on MT, we build phrase-based MT systems using Moses (Koehn et al., 2007), with word alignments generated by GIZA++ (Och and Ney, 2003). The MT training data contains the above formal corpus and some LDC4 parallel corpora (LDC2000T46, LDC2002E18, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2007T23, LDC2008T06, LDC2008T08, LDC2008T18, LDC2009T02, LDC2009T06, LDC2009T15, LDC2010T03). In total, 214M/192M English/Chinese tokens are used to train our MT systems. The language model of the Chinese-English (English-Chinese) MT system is the English (Chinese) side of the FBIS corpus (LDC2003E14) and the English (Chinese) Gigaword corpus. Our MT systems are tuned on the manually normalized messages of our development sets. Follo</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Oliva</author>
<author>J I Serrano</author>
<author>M D Del Castillo</author>
<author>´A Igesias</author>
</authors>
<title>A SMS normalization system integrating multiple grammatical resources.</title>
<date>2013</date>
<journal>Natural Language Engineering,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="6160" citStr="Oliva et al. (2013)" startWordPosition="918" endWordPosition="921">h SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word formation processes. Han and Baldwin (2011) normalized informal words using 1A Chinese version of Twitter at www.weibo.com 2Available at www.comp.nus.edu.sg/∼nlp/corpora.html morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish SMS messages using a normalization and a phonetic dictionary. For normalization of Chinese social media text, Xia et al. (2005) investigated informal phrase detection, and Li and Yarowsky (2008) mined informal-formal phrase pairs from Web corpora. All the above work focused on normalizing words. In contrast, our work also performs other normalization operations such as missing word recovery and punctuation correction, to further improve machine translation. Previously, Aw et al. (2006) adopted phrase-based MT to perform SMS normalization, and required a relatively large num</context>
<context position="26197" citStr="Oliva et al., 2013" startWordPosition="4118" endWordPosition="4121">contains the above formal corpus and some LDC4 parallel corpora (LDC2000T46, LDC2002E18, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2007T23, LDC2008T06, LDC2008T08, LDC2008T18, LDC2009T02, LDC2009T06, LDC2009T15, LDC2010T03). In total, 214M/192M English/Chinese tokens are used to train our MT systems. The language model of the Chinese-English (English-Chinese) MT system is the English (Chinese) side of the FBIS corpus (LDC2003E14) and the English (Chinese) Gigaword corpus. Our MT systems are tuned on the manually normalized messages of our development sets. Following (Aw et al., 2006; Oliva et al., 2013), we use BLEU scores (Papineni et al., 2002) to evaluate text normalization. We also use BLEU scores to evaluate MT quality. We use the sign test to determine statistical significance, for both text normalization and translation. 3http://mitlab.hit.edu.cn/ 4http://www.ldc.upenn.edu/Catalog/ 5.2 Baselines We compare our text normalization decoder against three baseline methods for performing text normalization. We then send the respective normalized texts to the same MT system to evaluate the effect of text normalization on MT. The simplest baseline for text normalization is one that does no te</context>
</contexts>
<marker>Oliva, Serrano, Castillo, Igesias, 2013</marker>
<rawString>J. Oliva, J. I. Serrano, M. D. Del Castillo, and ´A. Igesias. 2013. A SMS normalization system integrating multiple grammatical resources. Natural Language Engineering, 19(1):121–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="26241" citStr="Papineni et al., 2002" startWordPosition="4126" endWordPosition="4129"> LDC4 parallel corpora (LDC2000T46, LDC2002E18, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2007T23, LDC2008T06, LDC2008T08, LDC2008T18, LDC2009T02, LDC2009T06, LDC2009T15, LDC2010T03). In total, 214M/192M English/Chinese tokens are used to train our MT systems. The language model of the Chinese-English (English-Chinese) MT system is the English (Chinese) side of the FBIS corpus (LDC2003E14) and the English (Chinese) Gigaword corpus. Our MT systems are tuned on the manually normalized messages of our development sets. Following (Aw et al., 2006; Oliva et al., 2013), we use BLEU scores (Papineni et al., 2002) to evaluate text normalization. We also use BLEU scores to evaluate MT quality. We use the sign test to determine statistical significance, for both text normalization and translation. 3http://mitlab.hit.edu.cn/ 4http://www.ldc.upenn.edu/Catalog/ 5.2 Baselines We compare our text normalization decoder against three baseline methods for performing text normalization. We then send the respective normalized texts to the same MT system to evaluate the effect of text normalization on MT. The simplest baseline for text normalization is one that does no text normalization. The raw text (un-normalize</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2009</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="7713" citStr="Paul, 2009" startWordPosition="1169" endWordPosition="1170">ere is not much comparative evaluation of social media text translation other than the Haitian Creole to English SMS translation task in the 2011 Workshop on Statistical Machine Translation (WMT 2011) (Callison-Burch et al., 2011). However, the setup of the WMT 2011 task is different from ours, in that the task provided parallel training data of SMS texts and their translations. As such, text normalization is not necessary in that task. For example, the best reported system in that task (Costa-juss`a and Banchs, 2011) did not perform SMS message normalization. In speech to speech translation (Paul, 2009; Nakov et al., 2009), the input texts contain wrongly transcribed words due to errors in automatic speech recognition, whereas social media texts contain abbreviations, new words, etc. Although the input texts in both cases deviate from normal texts, the exact deviations are different. 472 Category Freq. Example cluding omitted and misused punctuation; (2) redunPunctuation 81 t*[hi] —(t* o[hi .]); dant interjections; (3) quotation-related problems, Pronunciation 47 �k[watch](TTc[don’t]); V (i-_X4-T[this]); e.g., omitted quotation marks; (4) “be” omission; New word 43 _ffi[bud](7t[cute]); (5) </context>
<context position="25131" citStr="Paul, 2009" startWordPosition="3964" endWordPosition="3965">ns (EN/CN/NCN) CN2EN-dev 500 6.95K/5.45K/5.70K CN2EN-test 500 7.14K/5.64K/5.82K Corpus # messages # tokens (EN/CN/NEN) EN2CN-dev 1,000 16.63K/18.14K/18.21K EN2CN-test 1,000 16.14K/17.69K/17.76K Table 3: Statistics of the corpora. CN2EN-dev/CN2ENtest is the development/test set in our ChineseEnglish experiments. EN2CN-dev/EN2CN-test is the development/test set in our English-Chinese experiments. NEN/NCN denotes manually normalized English/Chinese texts. The formal corpus used (as described in Section 4) is the concatenation of two Chinese-English spoken parallel corpora: the IWSLT 2009 corpus (Paul, 2009) and another spoken text corpus collected at the Harbin Institute of Technology3. The language model used for Chinese (English) text normalization is the Chinese (English) side of the formal corpus and the LDC Chinese (English) Gigaword corpus. To evaluate the effect of text normalization on MT, we build phrase-based MT systems using Moses (Koehn et al., 2007), with word alignments generated by GIZA++ (Och and Ney, 2003). The MT training data contains the above formal corpus and some LDC4 parallel corpora (LDC2000T46, LDC2002E18, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2007T23, LDC2</context>
</contexts>
<marker>Paul, 2009</marker>
<rawString>Michael Paul. 2009. Overview of the IWSLT 2009 evaluation campaign. In Proceedings of the International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deana L Pennell</author>
<author>Yang Liu</author>
</authors>
<title>A characterlevel machine translation approach for normalization of SMS abbreviations.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="5874" citStr="Pennell and Liu (2011)" startWordPosition="877" endWordPosition="880">of non-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns. Choudhury et al. (2007) used hidden Markov model to perform word-level normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word formation processes. Han and Baldwin (2011) normalized informal words using 1A Chinese version of Twitter at www.weibo.com 2Available at www.comp.nus.edu.sg/∼nlp/corpora.html morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish SMS messages using a normalization and a phonetic dictionary. For normalization of Chinese social media text, Xia et al. (2005) investigated informal phrase detection, and Li and Yarowsky (2008) mined informal-formal phrase pairs from Web corpora. All the above work focused on normalizing word</context>
</contexts>
<marker>Pennell, Liu, 2011</marker>
<rawString>Deana L. Pennell and Yang Liu. 2011. A characterlevel machine translation approach for normalization of SMS abbreviations. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1750" citStr="Ritter et al. (2011)" startWordPosition="252" endWordPosition="255">n and translation tasks, for both Chinese and English. 1 Introduction Social media texts include SMS (Short Message Service) messages, Twitter messages, Facebook updates, etc. They are different from formal texts due to their significant informal characteristics, so they always pose difficulties for applications such as machine translation (MT) (Aw et al., 2005) and named entity recognition (Liu et al., 2011), because of a lack of training data containing informal texts. Thus, the applications always suffer from a substantial performance drop when evaluated on social media texts. For example, Ritter et al. (2011) reported a drop from 90% to 76% on part-of-speech tagging, and Foster et al. (2011) found a drop of 20% in dependency parsing. Creating training data of social media texts specifically for a text processing task is time-consuming. For example, to create parallel Chinese-English training texts for translation of social media texts, it takes three minutes on average to translate an informally written social media text of eleven words from Chinese into English. On the other hand, it takes thirty seconds to normalize the same message, a six-fold increase in speed. After training a text normalizat</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="27367" citStr="Stolcke, 2002" startWordPosition="4302" endWordPosition="4303">or text normalization is one that does no text normalization. The raw text (un-normalized) is simply passed on to the MT system for translation. We call this baseline ORIGINAL. The second baseline, LATTICE, is to use a lattice to normalize text. For each input message, a lattice is generated in which each informal word is augmented with its formal candidates taken from the same normalization dictionary (downloaded from Internet) used in our text normalization decoder. The lattice is then decoded by the same language model used in our text normalization decoder to generate the normalized text (Stolcke, 2002). Another possible way of using lattice is to directly feed the lattice to the MT system (Eidelman et al., 2011), but since in this paper, we assume that the MT system can only translate plain text, we leave this as future work. The third baseline, PBMT, is a competitive baseline that performs text normalization via phrasebased MT, as proposed in Aw et al. (2006). Moses (Koehn et al., 2007) is used to perform text normalization, by “translating” un-normalized text to normalized text. The training data used is the same development set used in our text normalization decoder. The normalized text </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
</authors>
<title>Spell checking techniques for replacement of unknown words and data cleaning for Haitian Creole SMS translation.</title>
<date>2011</date>
<booktitle>In Proceedings of WMT.</booktitle>
<contexts>
<context position="28097" citStr="Stymne, 2011" startWordPosition="4434" endWordPosition="4435"> since in this paper, we assume that the MT system can only translate plain text, we leave this as future work. The third baseline, PBMT, is a competitive baseline that performs text normalization via phrasebased MT, as proposed in Aw et al. (2006). Moses (Koehn et al., 2007) is used to perform text normalization, by “translating” un-normalized text to normalized text. The training data used is the same development set used in our text normalization decoder. The normalized text is then sent to our MT system for translation. This method was also used in the SMS translation task of WMT 2011 by (Stymne, 2011). In the tables showing experimental results, normalization and translation BLEU scores that are significantly higher than (p &lt; 0.01) the LATTICE or PBMT baseline are in bold or underlined, respectively. 5.3 Chinese-English Experimental Results The Chinese-English normalization and translation results are shown in Table 4. The first group of experiments is the three baselines, and the second group is an oracle experiment using manually normalized messages as the output of text normaliza477 System BLEU scores (%) Normalization MT ORIGINAL baseline 61.01 9.06 LATTICE baseline 74.52 11.50 PBMT ba</context>
</contexts>
<marker>Stymne, 2011</marker>
<rawString>Sara Stymne. 2011. Spell checking techniques for replacement of unknown words and data cleaning for Haitian Creole SMS translation. In Proceedings of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Khashayar Rohanimanesh</author>
<author>Andrew McCallum</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning.</booktitle>
<contexts>
<context position="3729" citStr="Sutton et al., 2004" startWordPosition="554" endWordPosition="557">orrection. In this paper, we propose a novel beam-search decoder for normalization of social media text for MT. Our decoder can effectively integrate different normalization operations together. In contrast to previous work, some of our normalization operations are specifically designed for MT, e.g., missing word recovery based on conditional random fields 471 Proceedings of NAACL-HLT 2013, pages 471–481, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics (CRF) (Lafferty et al., 2001) and punctuation correction based on dynamic conditional random fields (DCRF) (Sutton et al., 2004). To the best of our knowledge, our work is the first to perform missing word recovery and punctuation correction for normalization of social media text, and also the first to perform message-level normalization of Chinese social media text. We investigate the effects on translating social media text after addressing various characteristics of informal social media text through normalization. To show the applicability of our normalization approach for different languages, we experiment with two languages, Chinese and English. We achieved statistically significant improvements over two strong b</context>
<context position="12411" citStr="Sutton et al., 2004" startWordPosition="1877" endWordPosition="1880">uation. The task performs three punctuation operations: insertion, deletion, and substitution. To our knowledge, no previous work has been done on punctuation correction for normalization of social media text. In ASR, punctuation prediction only inserts punctuation symbols into ASR output that has no punctuation (Kim and Woodland, 2001; Huang and Zweig, 2002), but without punctuation deletion or substitution. Lu and Ng (2010) argued that punctuation prediction should be jointly per473 formed with sentence boundary detection, so they modeled punctuation prediction using a two-layer DCRF model (Sutton et al., 2004). We also believe that punctuation correction is closely related to sentence boundary detection. Thus, we propose a two-layer DCRF model for punctuation correction. Layer 1 gives the actual punctuation tags None, Comma, Period, QuestionMark, and Exclamatory-Mark. Layer 2 gives the sentence boundary, including tags DeclarativeBegin, Declarative-In, Question-Begin, Question-In, Exclamatory-Begin, and Exclamatory-In, indicating whether the current word is at the beginning of (or inside) a declarative, question, or exclamatory sentence. We use word n-grams (n = 1, 2,3) and punctuation symbols with</context>
</contexts>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum. 2004. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In Proceedings of the 21st International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert L Weide</author>
</authors>
<title>The CMU pronouncing dictionary.</title>
<date>1998</date>
<note>URL: http://www.speech.cs.cmu.edu/cgibin/cmudict.</note>
<contexts>
<context position="21504" citStr="Weide, 1998" startWordPosition="3350" endWordPosition="3351">he Dictionary hypothesis producer can subsequently normalize w1 and w2. 4.5 Text Normalization for English Similar to Chinese text normalization, we also create the Dictionary, Punctuation, and Interjection hypothesis producers for English text normalization. We also add the following English-specific hypothesis producers: Pronunciation: This hypothesis producer uses pronunciation similarity to find formal candidates for a given informal word. It considers a word as a sequence of letters and convert it into a sequence of phones using phrase-based SMT trained on the CMU pronouncing dictionary (Weide, 1998). Similar sounding phones are paired together in a group: (ah, ao), (ow, uw), and (s, z). To illustrate, in the hypothesis “wat is it”, the informal word “wat” maps to the phone sequence “w ao t”. Since the formal word “what” maps to the phone sequence “w ah t” and the phones ah and ao are paired in a group, the new hypothesis “what is it” is generated. Be: We train a CRF model to recover missing words be, as described in Section 4.2. Retokenization: This hypothesis producer fixes tokenization problems. More precisely, given an informal word which is not a URL or email address and contains a p</context>
</contexts>
<marker>Weide, 1998</marker>
<rawString>Robert L. Weide. 1998. The CMU pronouncing dictionary. URL: http://www.speech.cs.cmu.edu/cgibin/cmudict.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunqing Xia</author>
<author>Kam-Fai Wong</author>
<author>Wei Gao</author>
</authors>
<title>NIL is not nothing: Recognition of Chinese network informal language expressions.</title>
<date>2005</date>
<booktitle>In 4th SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="6307" citStr="Xia et al. (2005)" startWordPosition="943" endWordPosition="946">11) normalized informal words using 1A Chinese version of Twitter at www.weibo.com 2Available at www.comp.nus.edu.sg/∼nlp/corpora.html morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish SMS messages using a normalization and a phonetic dictionary. For normalization of Chinese social media text, Xia et al. (2005) investigated informal phrase detection, and Li and Yarowsky (2008) mined informal-formal phrase pairs from Web corpora. All the above work focused on normalizing words. In contrast, our work also performs other normalization operations such as missing word recovery and punctuation correction, to further improve machine translation. Previously, Aw et al. (2006) adopted phrase-based MT to perform SMS normalization, and required a relatively large number of manually normalized SMS messages. In contrast, our approach performs beam search at the sentence level, and does not require large training </context>
</contexts>
<marker>Xia, Wong, Gao, 2005</marker>
<rawString>Yunqing Xia, Kam-Fai Wong, and Wei Gao. 2005. NIL is not nothing: Recognition of Chinese network informal language expressions. In 4th SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenzhen Xue</author>
<author>Dawei Yin</author>
<author>Brian D Davison</author>
</authors>
<title>Normalizing microtext.</title>
<date>2011</date>
<booktitle>In Proceedings of the AAAI Workshop on Analyzing Microtext.</booktitle>
<contexts>
<context position="5927" citStr="Xue et al. (2011)" startWordPosition="886" endWordPosition="889">sertion of auxiliary verbs and subject pronouns. Choudhury et al. (2007) used hidden Markov model to perform word-level normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word formation processes. Han and Baldwin (2011) normalized informal words using 1A Chinese version of Twitter at www.weibo.com 2Available at www.comp.nus.edu.sg/∼nlp/corpora.html morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish SMS messages using a normalization and a phonetic dictionary. For normalization of Chinese social media text, Xia et al. (2005) investigated informal phrase detection, and Li and Yarowsky (2008) mined informal-formal phrase pairs from Web corpora. All the above work focused on normalizing words. In contrast, our work also performs other normaliz</context>
</contexts>
<marker>Xue, Yin, Davison, 2011</marker>
<rawString>Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011. Normalizing microtext. In Proceedings of the AAAI Workshop on Analyzing Microtext.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Gunnar Evermann</author>
<author>Dan Kershaw</author>
<author>Gareth Moore</author>
<author>Julian Odell</author>
<author>Dave Ollason</author>
<author>Valtcho Valtchev</author>
<author>Phil Woodland</author>
</authors>
<title>The HTK book.</title>
<date>2002</date>
<institution>Cambridge University Engineering Department.</institution>
<contexts>
<context position="15578" citStr="Young et al., 2002" startWordPosition="2386" endWordPosition="2389">rection model, but exclude the punctuation features. The model is trained on synthetically created training texts in which be has been randomly deleted with probability 0.5. 4.3 A Decoder for Text Normalization When designing our text normalization system, we aim for a general framework that can be applied to text normalization across different languages with minimal effort. This is a challenging task, since social media texts in different languages exhibit different informal characteristics, as illustrated in Section 3. Motivated by the beam-search decoders for SMT (Koehn et al., 2007), ASR (Young et al., 2002), and grammatical error correction (Dahlmeier and Ng, 2012), we propose a novel beam-search decoder for normalization of social media text. Given an input message, the normalization decoder searches for its best normalization, i.e., the best hypothesis, by iteratively performing two subtasks: (1) producing new sentence-level hypotheses from hypotheses in the current stack, carried out by hypothesis producers; and (2) evaluating the new hypotheses to retain good ones, carried out by feature functions. Each hypothesis is the result of applying successive normalization operations on the initial i</context>
</contexts>
<marker>Young, Evermann, Kershaw, Moore, Odell, Ollason, Valtchev, Woodland, 2002</marker>
<rawString>Steve Young, Gunnar Evermann, Dan Kershaw, Gareth Moore, Julian Odell, Dave Ollason, Valtcho Valtchev, and Phil Woodland. 2002. The HTK book. Cambridge University Engineering Department.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Conghui Zhu</author>
<author>Jie Tang</author>
<author>Hang Li</author>
<author>Hwee Tou Ng</author>
<author>TieJun Zhao</author>
</authors>
<title>A unified tagging approach to text normalization.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4956" citStr="Zhu et al. (2007)" startWordPosition="742" endWordPosition="745">an improvement of 9.98%/7.35% in BLEU scores for normalization of Chinese/English social media text, and an improvement of 1.38%/1.35% in BLEU scores for translation of Chinese/English social media text. We created two corpora: a Chinese corpus containing 1,000 Weibo1 messages with their normalizations and English translations; and another similar English corpus containing 2,000 SMS messages from the NUS SMS corpus (How and Kan, 2005). As far as we know, our corpora are the first publicly available Chinese/English corpora for normalization and translation of social media text2. 2 Related Work Zhu et al. (2007) performed text normalization of informally written email messages using CRF (Lafferty et al., 2001). Due to its importance, normalization of social media text has been extensively studied recently. Aw et al. (2005) proposed a noisy channel model consisting of different operations: substitution of non-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns. Choudhury et al. (2007) used hidden Markov model to perform word-level normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. </context>
</contexts>
<marker>Zhu, Tang, Li, Ng, Zhao, 2007</marker>
<rawString>Conghui Zhu, Jie Tang, Hang Li, Hwee Tou Ng, and TieJun Zhao. 2007. A unified tagging approach to text normalization. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>