<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000404">
<title confidence="0.876187">
Reduced n-gram models for English and Chinese corpora
</title>
<author confidence="0.945512">
Le Q Ha, P Hanna, D W Stewart and F J Smith
</author>
<affiliation confidence="0.971187">
School of Electronics, Electrical Engineering and Computer Science,
Queen’s University Belfast
</affiliation>
<address confidence="0.965772">
Belfast BT7 1NN, Northern Ireland, United Kingdom
</address>
<email confidence="0.996498">
lequanha@lequanha.com
</email>
<sectionHeader confidence="0.96337" genericHeader="abstract">
1 Introduction to the Reduced N-Gram
Approach
</sectionHeader>
<bodyText confidence="0.999733131578948">
Shortly after this laboratory first published a
variable n-gram algorithm (Smith and O’Boyle,
1992), O’Boyle (1993) proposed a statistical
method to improve language models based on the
removal of overlapping phrases.
A distortion in the use of phrase frequencies
had been observed in the small railway timetable
Vodis Corpus when the bigram “RAIL
ENQUIRIES” and its super-phrase “BRITISH
RAIL ENQUIRIES” were examined. Both occur
73 times, which is a large number for such a
small corpus. “ENQUIRIES” follows “RAIL”
with a very high probability when it is preceded
by “BRITISH.” However, when “RAIL” is
preceded by words other than “BRITISH,”
“ENQUIRIES” does not occur, but words like
“TICKET” or “JOURNEY” may. Thus, the
bigram “RAIL ENQUIRIES” gives a misleading
probability that “RAIL” is followed by
“ENQUIRIES” irrespective of what precedes it.
At the time of their research, O’Boyle reduced
the frequencies of “RAIL ENQUIRIES” by
subtracting the frequency of the larger trigram,
which gave a probability of zero for
“ENQUIRIES” following “RAIL” if it was not
preceded by “BRITISH.” The phrase with a new
reduced frequency is called a reduced phrase.
Therefore, a phrase can occur in a corpus as a
reduced n-gram in some places and as part of a
larger reduced n-gram in other places. In a
reduced model, the occurrence of an n-gram is
not counted when it is a part of a larger reduced
n-gram. One algorithm to detect/identify/extract
reduced n-grams from a corpus is the so-called
reduced n-gram algorithm. In 1992, O’Boyle was
able to use it to analyse the Brown corpus of
American English (Francis and Kucera, 1964) (of
one million word tokens, whose longest phrase-
</bodyText>
<sectionHeader confidence="0.66692" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.999794666666666">
Statistical language models should
improve as the size of the n-grams
increases from 3 to 5 or higher. However,
the number of parameters and
calculations, and the storage requirement
increase very rapidly if we attempt to
store all possible combinations of
n-grams. To avoid these problems, the
reduced n-grams’ approach previously
developed by O’Boyle (1993) can be
applied. A reduced n-gram language
model can store an entire corpus’s
phrase-history length within feasible
storage limits. Another theoretical
advantage of reduced n-grams is that they
are closer to being semantically complete
than traditional models, which include all
n-grams. In our experiments, the reduced
n-gram Zipf curves are first presented,
and compared with previously obtained
conventional n-grams for both English
and Chinese. The reduced n-gram model
is then applied to large English and
Chinese corpora. For English, we can
reduce the model sizes, compared to
7-gram traditional model sizes, with
factors of 14.6 for a 40-million-word
corpus and 11.0 for a 500-million-word
corpus while obtaining 5.8% and 4.2%
improvements in perplexities. For
Chinese, we gain a 16.9% perplexity
reductions and we reduce the model size
by a factor larger than 11.2. This paper is
a step towards the modeling of English
and Chinese using semantically complete
phrases in an n-gram model.
</bodyText>
<page confidence="0.98904">
309
</page>
<bodyText confidence="0.9352">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 309–315,
Sydney, July 2006. c�2006 Association for Computational Linguistics
length is 30), which was a considerable
improvement at the time. The results were used
in an n-gram language model by O’Boyle, but
with poor results, due to lack of statistics from
such a small corpus. We have developed and
present here a modification of his method, and
we discuss its usefulness for reducing n-gram
perplexity.
</bodyText>
<sectionHeader confidence="0.762921" genericHeader="method">
2 Similar Approaches and Capability
</sectionHeader>
<bodyText confidence="0.999740672727273">
Recent progress in variable n-gram language
modeling has provided an efficient
representation of n-gram models and made the
training of higher order n-grams possible.
Compared to variable n-grams, class-based
language models are more often used to reduce
the size of a language model, but this typically
leads to recognition performance degradation.
Classes can alternatively be used to smooth a
language model or provide back-off estimates,
which have led to small performance gains. For
the LOB corpus, the varigram model obtained
11.3% higher perplexity in comparison with the
word-trigram model (Niesler and Woodland,
1996.)
Kneser (1996) built up variable-context length
language models based on the North American
Business News (NAB-240 million words) and
the German Verbmobil (300,000 words with a
vocabulary of 5,000 types.) His results show that
the variable-length model outperforms
conventional models of the same size, and if a
moderate loss in performance is acceptable, that
the size of a language model can be reduced
drastically by using his pruning algorithm.
Kneser’s results improve with longer contexts
and a same number of parameters. For example,
reducing the size of the standard NAB trigram
model by a factor of 3 results in a loss of only
7% in perplexity and 3% in the word error rate.
The improvement obtained by Kneser’s method
depended on the length of the fixed context and
on the amount of available training data. In the
case of the NAB corpus, the improvement was
10% in perplexity.
Siu and Ostendorf (2000) developed Kneser‘s
basic ideas further and applied the variable
4-gram, thus improving the perplexity and word
error rate results compared to a fixed trigram
model. They obtained word error reductions of
0.1 and 0.5% (absolute) in development and
evaluation test sets, respectively. However, the
number of parameters was reduced by 60%. By
using the variable 4-gram, they were able to
model a longer history while reducing the size of
the model by more than 50%, compared to a
regular trigram model, and at the same time
improve both the test-set perplexity and
recognition performance. They also reduced the
size of the model by an additional 8%.
Other related work are those of Seymore and
Rosenfeld (1996); Hu, Turin and Brown (1997);
Blasig (1999); and Goodman and Gao (2000.)
In order to obtain an overview of variable
n-grams, Table 1 combines all of their results.
</bodyText>
<table confidence="0.998633421052632">
COMBINATION O F LANGUAGE MODEL TYPES
Basic Variable Category Skipping Classes #params Perplexity Size Source
n-gram n-grams distance
Trigram√ 987k 474 1M LOB
Bigram√ - 603.2
Trigram√ - 544.1
√ √ - 534.1
Trigram√ 743k 81.5 2M Switch
board
Corpus
Trigram√ 379k 78.1
Trigram√ √ 363k 78.0
Trigram√ √ √ 338k 77.7
4-gram√ 580k 108
4-gram√ √ 577k 108
4-gram√ √ √ 536k 107
5-gram√ 383k 77.5
5-gram√ √ 381k 77.4
5-gram√ √ √ 359k 77.2
</table>
<tableCaption confidence="0.999979">
Table 1. Comparison of combinations of variable n-grams and other Language Models
</tableCaption>
<page confidence="0.997595">
310
</page>
<sectionHeader confidence="0.97064" genericHeader="method">
3 Reduced N-Gram Algorithm
</sectionHeader>
<bodyText confidence="0.823848666666667">
The main goal of this algorithm (Ha, 2005) is to
produce three main files from the training text:
• The file that contains all the complete
n-grams appearing at least m times is
called the PHR file (m ≥ 2.)
• The file that contains all the n-grams
appearing as sub-phrases, following the
removal of the first word from any other
complete n-gram in the PHR file, is called
the SUB file.
• The file that contains any overlapping
n-grams that occur at least m times in the
SUB file is called the LOS file.
The final list of reduced phrases is called the FIN
file, where
</bodyText>
<equation confidence="0.9933245">
(1)
FIN:= PHR + LOS −SUB
</equation>
<bodyText confidence="0.9694899">
Before O’Boyle‘s work, a student (Craig) in an
unpublished project used a loop algorithm that
was equivalent to FIN:=PHR–SUB. This yields
negative frequencies for some resulting n-grams
with overlapping, hence the need for the LOS
file.
There are 2 additional files
• To create the PHR file, a SOR file is
needed that contains all the complete
n-grams regardless of m (the SOR file is
the PHR file in the special case where
m = 1.) To create the PHR file, words are
removed from the right-hand side of each
SOR phrase in the SOR file until the
resultant phrase appears at least m times (if
the phrase already occurs more than m
times, no words will be removed.)
• To create the LOS file, O’Boyle applied
a POS file: for any SUB phrase, if one
word can be added back on the right-hand
side (previously removed when the PHR
file was created from the SOR file), then
one POS phrase will exist as the added
phrase. Thus, if any POS phrase appears at
least m times, its original SUB phrase will
be an overlapping n-gram in the LOS file.
The application scope of O’Boyle’s reduced
n-gram algorithm is limited to small corpora,
such as the Brown corpus (American English) of
1 million words (1992), in which the longest
phrase has 30 words. Now their algorithm,
re-checked by us, still works for medium size
and large corpora. In order to work well for very
large corpora, it has been implemented by file
distribution and sort processes.
Ha, Seymour, Hanna and Smith (2005) have
investigated a reduced n-gram model for the
Chinese TREC corpus of the Linguistic Data
Consortium (LDC) (http://www.ldc.upenn.edu/),
catalog no. LDC2000T52.
</bodyText>
<sectionHeader confidence="0.916871" genericHeader="method">
4 Reduced N-Grams and Zipf&apos;s Law
</sectionHeader>
<bodyText confidence="0.999715194444444">
By re-applying O’Boyle and Smith’s algorithm,
we obtained reduced n-grams from two English
large corpora and a Chinese large corpus.
The two English corpora used in our
experiments are the full text of articles appearing
in the Wall Street Journal (WSJ) (Paul and
Baker, 1992) for 1987, 1988, 1989, with sizes
approximately 19 million, 16 million and 6
million tokens respectively; and the North
American News Text (NANT) corpus from the
LDC, sizing 500 million tokens, including Los
Angeles Times &amp; Washington Post for May
1994-August 1997, New York Times News
Syndicate for July 1994-December 1996, Reuters
News Service (General &amp; Financial) for April
1994-December 1996 and Wall Street Journal for
July 1994-December 1996. Therefore, the WSJ
parts from the two English corpora are not
overlapping together.
The Mandarin News corpus from the LDC,
catalog no. LDC95T13 was obtained from the
People’s Daily Newspaper from 1991 to 1996
(125 million syllables); from the Xinhua News
Agency from 1994 to 1996 (25 million
syllables); and from transcripts of China Radio
International broadcast from 1994 to 1996 (100
million syllables), altogether over 250 million
syllables. The number of syllable types (i.e.
unigrams) in the Mandarin News corpus is 6,800.
Ha, Sicilia-Garcia, Ming and Smith (2003)
produced a compound word version of the
Mandarin News corpus with 50,000 types; this
version was employed in our study for reduced
n-grams.
We next present the Zipf curves (Zipf, 1949)
for the English and Chinese reduced n-grams.
</bodyText>
<subsectionHeader confidence="0.990215">
4.1 Wall Street Journal
</subsectionHeader>
<bodyText confidence="0.9993278">
The WSJ reduced n-grams can be created by the
original O’Boyle-Smith algorithm implemented
on a Pentium II 586 of 512MByte RAM for over
40 hours, the disk storage requirement being
only 5GBytes.
</bodyText>
<page confidence="0.995193">
311
</page>
<bodyText confidence="0.97053145">
The conventional 10-highest frequency WSJ
words have been published by Ha et al. (2002)
and the most common WSJ reduced unigrams,
bigrams and trigrams are shown in Table 2. It
illustrates that the most common reduced word is
not THE; even OF is not in the top ten. These
words are now mainly part of longer n-grams
with large n.
The Zipf curves are plotted for reduced
unigrams and n-grams in Figure 1 showing all
the curves have slopes within [-0.6, -0.5]. The
WSJ reduced bigram, trigram, 4-gram and
5-gram curves become almost parallel and
straight, with a small observed noise between the
reduced 4-gram and 5-gram curves when they cut
each other at the beginning. Note that
information theory tells us that an ideal
information channel would be made of symbols
with the same probability. So having a slope of
–0.5 is closer than –1 to this ideal.
</bodyText>
<table confidence="0.9914005">
Rank Unigrams Bigrams Trigrams
Freq Token Freq Token Freq Token
1 4,273 Mr. 2,268 he said 1,231 terms weren’t disclosed
2 2,469 but 2,052 he says 709 the company said
3 2,422 and 1,945 but the 664 as previously reported
4 2,144 the 1,503 but Mr. 538 he said the
5 1,918 says 1,332 and the 524 a spokesman for
6 1,660 or 950 says Mr. 523 the spokesman said
7 1,249 said 856 in addition 488 as a result
8 1,101 however 855 and Mr. 484 earlier this year
9 1,007 while 832 last year 469 in addition to
10 997 meanwhile 754 for example 466 according to Mr.
</table>
<tableCaption confidence="0.99839">
Table 2. Most common WSJ reduced n-grams
</tableCaption>
<figure confidence="0.474635">
log rank
</figure>
<figureCaption confidence="0.995735">
Figure 1. The WSJ reduced n-gram Zipf curves
</figureCaption>
<subsectionHeader confidence="0.9439">
4.2 North American News Text corpus
</subsectionHeader>
<bodyText confidence="0.994086533333333">
The NANT reduced n-grams are created by the
improved algorithm after over 300 hours
processing, needing a storage requirement of
100GBytes on a Pentium II 586 of 512MByte
RAM.
Their Zipf curves are plotted for reduced
unigrams and n-grams in Figure 2 showing all
the curves are just sloped around [-0.54, -0.5].
The reduced unigrams of NANT still show the
2-slope behavior when it starts with slope –0.54
and then drop with slope nearly –2 at the end of
the curve. We have found that the traditional
n-grams also show this behaviour, with an initial
slope of –1 changing to –2 for large ranks (Ha
and Smith, 2004.)
</bodyText>
<figure confidence="0.966141">
0 1 2 3 4 5 6 7 8
log rank
</figure>
<figureCaption confidence="0.999643">
Figure 2. The NANT reduced n-gram Zipf curves
</figureCaption>
<subsectionHeader confidence="0.97612">
4.3 Mandarin News compound words
</subsectionHeader>
<bodyText confidence="0.876743333333333">
The Zipf curves are plotted for the smaller
Chinese TREC reduced unigrams and n-grams
were shown by Ha et al. (2005.)
</bodyText>
<figure confidence="0.984905333333333">
1-gram
2-gram
3-gram
4-gram
5-gram
slope -1
0 1 2 3 4 5 6 7
4
0
3
2
1
log frequency
6
1-gram
2-gram
5
4
3-gram
4-gram
5-gram
slope -1
1
0
3
2
log frequency
312
0 1 2 3 4 5 6 7
log rank
</figure>
<figureCaption confidence="0.999772">
Figure 3. Mandarin reduced n-gram Zipf curves
</figureCaption>
<bodyText confidence="0.999914733333333">
The Mandarin News reduced word n-grams were
created in 120 hours, using 20GB of disk space.
The Zipf curves are plotted in Figure 3 showing
that the unigram curve now has a larger slope
than –1, it is around –1.2. All the n-gram curves
are now straighter and more parallel than the
traditional n-gram curves, have slopes within
[-0.67, -0.5].
Usually, Zipf’s rank-frequency law with a
slope –1 is confirmed by empirical data, but the
reduced n-grams for English and Chinese shown
in Figure 1, Figure 2 and Figure 3 do not confirm
it. In fact, various more sophisticated models for
frequency distributions have been proposed by
Baayen (2001) and Evert (2004.)
</bodyText>
<sectionHeader confidence="0.998691" genericHeader="method">
5 Perplexity for Reduced N-Grams
</sectionHeader>
<bodyText confidence="0.99410475">
The reduced n-gram approach was used to build
a statistical language model based on the
weighted average model of O’Boyle, Owens and
Smith (1994.) We rewrite this model in formulae
</bodyText>
<figure confidence="0.566952">
(2) and (3)
(2)
(3)
1=0
</figure>
<bodyText confidence="0.946219846153846">
This averages the probabilities of a word wi
following the previous one word, two words,
three words, etc. (i.e. making the last word of an
n-gram.) The averaging uses weights that
increase slowly with their frequency and rapidly
with the length of n-gram. This weighted average
model is a variable length model that gives
results comparable to the Katz back-off method
(1987), but is quicker to use.
The probabilities of all of the sentences ~1 in
�
a test text are then calculated by the weighted
average model
</bodyText>
<equation confidence="0.9940112">
� � � � � � � � � �
( ) ( ) ( ) ( 1 )
� �−
= 2 1 ...
1 �� 1 �� �� � 1
</equation>
<bodyText confidence="0.968542">
and an average perplexity of each sentence is
evaluated using Equation (5)
</bodyText>
<equation confidence="0.961813">
�
1
���
( )=  − ∑=
� exp  �� � � � � �
( ( ... ))
1 �� � 1 2 �−1
 � � 1
</equation>
<bodyText confidence="0.99856076">
Ha et al. (2005) already investigated and
analysed the main difficulties arising from
perplexity calculations for our reduced model: a
statistical model problem, an unseen word
problem and an unknown word problem. Their
solutions are applied in this paper also. Similar
problems have been found by other authors, e.g.
Martin, Liermann and Ney (1997); Kneser and
Ney (1995.)
The perplexity calculations for both the
English and Chinese reduced n-grams includes
statistics on phrase lengths starting with
unigrams, bigrams, trigrams...and on up to the
longest phrase which occur in the reduced model.
The perplexities of the WSJ reduced model are
shown in Table 3, North American News Text
corpus in Table 4 and Mandarin News words in
Table 5.
The nature of the reduced model makes the
reporting of results for limited sizes of n-grams
to be inappropriate, although these are valid for a
traditional n-gram model. Therefore we show
results for several n-gram sizes for the traditional
model, but only one perplexity for the reduced
model.
</bodyText>
<figure confidence="0.997335538461538">
1-gram
2-gram
3-gram
4-gram
slope -1
5-gram
log frequency 6
5
4
3
2
1
0
</figure>
<equation confidence="0.971488772727273">
wgt ��
( ) = log( ( � − 1))× 2�−�+1
� � ��
� − 1
( ) ( ) ( ) ( )
� � − 1
��� � � �
× + ∑ ��� � � � �
×
� � � �
− � � �
−
(Wi W,&apos;−N+1)=
PWA
/=1
N−1
∑ ( )
��� ��
� �
−
(4)
(5)
</equation>
<page confidence="0.998226">
313
</page>
<table confidence="0.999906">
Unknowns Tokens 0
Types 0
Unigrams 762.69
Bigrams 144.33
Trigrams 75.36
Traditional Model 4-grams 60.73
5-grams 56.85
6-grams 55.66
7-grams 55.29
Reduced Model 70.98
%Improvement of Reduced 5.81%
Model on baseline Trigrams
Model size reduction 14.56
</table>
<tableCaption confidence="0.992536">
Table 3. Reduced perplexities for English WSJ
</tableCaption>
<table confidence="0.999905538461539">
Unknowns Tokens 24
Types 23
Unigrams 1,442.99
Bigrams 399.61
Trigrams 240.52
Traditional Model 4-grams 202.59
5-grams 194.06
6-grams 191.91
7-grams 191.23
Reduced Model 230.46
%Improvement of Reduced 4.18%
Model on baseline Trigrams
Model size reduction 11.01
</table>
<tableCaption confidence="0.998736">
Table 4. Reduced perplexities for English NANT
</tableCaption>
<table confidence="0.999909">
Unknowns Tokens 84
Types 26
Unigrams 1,620.56
Bigrams 377.43
Trigrams 179.07
Traditional Model 4-grams 135.69
5-grams 121.53
6-grams 114.96
7-grams 111.69
Reduced Model 148.71
%Improvement of Reduced 16.95%
Model on baseline Trigrams
Model size reduction 11.28
</table>
<tableCaption confidence="0.898003">
Table 5. Reduced perplexities for Mandarin
News words
</tableCaption>
<bodyText confidence="0.999930666666667">
In all three cases the reduced model produces a
modest improvement over the traditional
3-gram model, but is not as good as the
traditional 4-gram or higher models. However in
all three cases the result is obtained with a
significant reduction in model size, from a factor
of 11 to almost 15 compared to the traditional
7-gram model size.
We did expect a greater improvement in
perplexity than we obtained and we believe that a
further look at the methods used to solve the
difficult problems listed by Ha et al. (2005)
(mentioned above) and others mentioned by Ha
(2005) might lead to an improvement. Missing
word tests are also needed.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999835">
The conventional n-gram language model is
limited in terms of its ability to represent
extended phrase histories because of the
exponential growth in the number of parameters.
To overcome this limitation, we have
re-investigated the approach of O’Boyle (1993)
and created reduced n-gram models. Our aim
was to try to create an n-gram model that used
semantically more complete n-grams than
traditional n-grams in the expectation that this
might lead to an improvement in language
modeling. The improvement in perplexity is
modest, but there is a large decrease in model
size. So this represents an encouraging step
forward, although still very far from the final
step in language modelling.
</bodyText>
<sectionHeader confidence="0.995135" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999680666666667">
The authors would like to thank Dr Ji Ming for
his support and the reviewers for their valuable
comments.
</bodyText>
<sectionHeader confidence="0.997124" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.966798555555555">
Douglas B. Paul and Janet B. Baker. 1992. The
Design for the Wall Street Journal based CSR
Corpus. In Proc. of the DARPA SLS Workshop,
pages 357-361.
Francis J. Smith and Peter O’Boyle. 1992. The
N-Gram Language Model. The Cognitive Science
of Natural Language Processing Workshop, pages
51-58. Dublin City University.
George K. Zipf. 1949. Human Behaviour and the
Principle of Least Effort. Reading, MA: Addison-
Wesley Publishing Co.
Harald R. Baayen. 2001. Word Frequency
Distributions. Kluwer Academic Publishers.
Jianying Hu, William Turin and Michael K. Brown.
1997. Language Modeling using Stochastic
Automata with Variable Length Contexts.
Computer Speech and Language, volume 11, pages
1-16.
</reference>
<page confidence="0.989672">
314
</page>
<reference confidence="0.999837227848101">
Joshua Goodman and Jianfeng Gao. 2000. Language
Model Size Reduction By Pruning And Clustering.
ICSLP&apos;00. Beijing, China.
Kristie Seymore and Ronald Rosenfeld. 1996.
Scalable Backoff Language Models. ICSLP&apos;96,
pages 232-235.
Le Q. Ha and Francis. J. Smith. 2004. Zipf and Type-
Token rules for the English and Irish languages.
MIDL workshop. Paris.
Le Q. Ha, Elvira I. Sicilia-Garcia, Ji Ming and Francis
J. Smith. 2002. Extension of Zipf’s Law to Words
and Phrases. COLING&apos;02, volume 1, pages 315-
320.
Le Q. Ha, Elvira I. Sicilia-Garcia, Ji Ming and Francis
J. Smith. 2003. Extension of Zipf’s Law to Word
and Character N-Grams for English and Chinese.
CLCLP, 8(1):77-102.
Le Q. Ha, Rowan Seymour, Philip Hanna and Francis
J. Smith. 2005. Reduced N-Grams for Chinese
Evaluation. CLCLP, 10(1):19-34.
Manhung Siu and Mari Ostendorf. 2000. Integrating a
Context-Dependent Phrase Grammar in the
Variable N-Gram framework. ICASSP&apos;00, volume
3, pages 1643-1646.
Manhung Siu and Mari Ostendorf. 2000. Variable
N-Grams and Extensions for Conversational
Speech Language Modelling. IEEE Transactions
on Speech and Audio Processing, 8(1):63-75.
Nelson Francis and Henry Kucera. 1964. Manual of
Information to Accompany A Standard Corpus of
Present-Day Edited American English, for use with
Digital Computers. Department of Linguistics,
Brown University, Providence, Rhode Island.
Peter L. O’Boyle. 1993. A study of an N-Gram
Language Model for Speech Recognition. PhD
thesis. Queen’s University Belfast.
Peter O’Boyle, John McMahon and Francis J. Smith.
1995. Combining a Multi-Level Class Hierarchy
with Weighted-Average Function-Based
Smoothing. IEEE Automatic Speech Recognition
Workshop. Snowbird, Utah.
Peter O’Boyle, Marie Owens and Francis J. Smith.
1994. A weighted average N-Gram model of
natural language. Computer Speech and Language,
volume 8, pages 337-349.
Ramon Ferrer I. Cancho and Ricard V. Solé. 2002.
Two Regimes in the Frequency of Words and the
Origin of Complex Lexicons. Journal of
Quantitative Linguistics, 8(3):165-173.
Reinhard Blasig. 1999. Combination of Words and
Word Categories in Varigram Histories.
ICASSP&apos;99, volume 1, pages 529-532.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-off for M-Gram Language Modeling.
ICASSP&apos;95, volume 1, pages 181-184. Detroit.
Reinhard Kneser. 1996. Statistical Language
Modeling Using a Variable Context Length.
ICSLP&apos;96, volume 1, pages 494-497.
Slava M. Katz. 1987. Estimation of Probabilities from
Sparse Data for the Language Model Component
of a Speech Recognizer. In IEEE Transactions on
Acoustics, Speech and Signal Processing, volume
ASSP-35, pages 400-401.
Stefan Evert. 2004. A Simple LNRE Model for
Random Character Sequences. In Proc. of the
7emes Journees Internationales d&apos;Analyse
Statistique des Donnees Textuelles, pages 411-422.
Sven C. Martin, Jörg Liermann and Hermann Ney.
1997. Adaptive Topic-Dependent Language
Modelling Using Word-Based Varigrams.
EuroSpeech&apos;97, volume 3, pages 1447-1450.
Rhodes.
Thomas R. Niesler and Phil C. Woodland. 1996. A
Variable-Length Category-Based N-Gram
Language Model. ICASSP&apos;96, volume 1, pages
164-167.
Thomas R. Niesler. 1997. Category-based statistical
language models. St. John’s College, University of
Cambridge.
</reference>
<page confidence="0.999305">
315
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.969738">
<title confidence="0.994151">models for English and Chinese corpora</title>
<author confidence="0.999209">Le_Q Ha</author>
<author confidence="0.999209">P Hanna</author>
<author confidence="0.999209">D W Stewart</author>
<author confidence="0.999209">F J Smith</author>
<affiliation confidence="0.999272">School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast</affiliation>
<address confidence="0.99207">Belfast BT7 1NN, Northern Ireland, United Kingdom</address>
<email confidence="0.985225">lequanha@lequanha.com</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas B Paul</author>
<author>Janet B Baker</author>
</authors>
<title>The Design for the Wall Street Journal based CSR Corpus.</title>
<date>1992</date>
<booktitle>In Proc. of the DARPA SLS Workshop,</booktitle>
<pages>357--361</pages>
<contexts>
<context position="9350" citStr="Paul and Baker, 1992" startWordPosition="1533" endWordPosition="1536">a. In order to work well for very large corpora, it has been implemented by file distribution and sort processes. Ha, Seymour, Hanna and Smith (2005) have investigated a reduced n-gram model for the Chinese TREC corpus of the Linguistic Data Consortium (LDC) (http://www.ldc.upenn.edu/), catalog no. LDC2000T52. 4 Reduced N-Grams and Zipf&apos;s Law By re-applying O’Boyle and Smith’s algorithm, we obtained reduced n-grams from two English large corpora and a Chinese large corpus. The two English corpora used in our experiments are the full text of articles appearing in the Wall Street Journal (WSJ) (Paul and Baker, 1992) for 1987, 1988, 1989, with sizes approximately 19 million, 16 million and 6 million tokens respectively; and the North American News Text (NANT) corpus from the LDC, sizing 500 million tokens, including Los Angeles Times &amp; Washington Post for May 1994-August 1997, New York Times News Syndicate for July 1994-December 1996, Reuters News Service (General &amp; Financial) for April 1994-December 1996 and Wall Street Journal for July 1994-December 1996. Therefore, the WSJ parts from the two English corpora are not overlapping together. The Mandarin News corpus from the LDC, catalog no. LDC95T13 was ob</context>
</contexts>
<marker>Paul, Baker, 1992</marker>
<rawString>Douglas B. Paul and Janet B. Baker. 1992. The Design for the Wall Street Journal based CSR Corpus. In Proc. of the DARPA SLS Workshop, pages 357-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis J Smith</author>
<author>Peter O’Boyle</author>
</authors>
<title>The N-Gram Language Model. The Cognitive Science of Natural Language Processing Workshop,</title>
<date>1992</date>
<pages>51--58</pages>
<institution>Dublin City University.</institution>
<marker>Smith, O’Boyle, 1992</marker>
<rawString>Francis J. Smith and Peter O’Boyle. 1992. The N-Gram Language Model. The Cognitive Science of Natural Language Processing Workshop, pages 51-58. Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George K Zipf</author>
</authors>
<title>Human Behaviour and the Principle of Least Effort.</title>
<date>1949</date>
<publisher>AddisonWesley Publishing Co.</publisher>
<location>Reading, MA:</location>
<contexts>
<context position="10555" citStr="Zipf, 1949" startWordPosition="1724" endWordPosition="1725">was obtained from the People’s Daily Newspaper from 1991 to 1996 (125 million syllables); from the Xinhua News Agency from 1994 to 1996 (25 million syllables); and from transcripts of China Radio International broadcast from 1994 to 1996 (100 million syllables), altogether over 250 million syllables. The number of syllable types (i.e. unigrams) in the Mandarin News corpus is 6,800. Ha, Sicilia-Garcia, Ming and Smith (2003) produced a compound word version of the Mandarin News corpus with 50,000 types; this version was employed in our study for reduced n-grams. We next present the Zipf curves (Zipf, 1949) for the English and Chinese reduced n-grams. 4.1 Wall Street Journal The WSJ reduced n-grams can be created by the original O’Boyle-Smith algorithm implemented on a Pentium II 586 of 512MByte RAM for over 40 hours, the disk storage requirement being only 5GBytes. 311 The conventional 10-highest frequency WSJ words have been published by Ha et al. (2002) and the most common WSJ reduced unigrams, bigrams and trigrams are shown in Table 2. It illustrates that the most common reduced word is not THE; even OF is not in the top ten. These words are now mainly part of longer n-grams with large n. Th</context>
</contexts>
<marker>Zipf, 1949</marker>
<rawString>George K. Zipf. 1949. Human Behaviour and the Principle of Least Effort. Reading, MA: AddisonWesley Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald R Baayen</author>
</authors>
<title>Word Frequency Distributions.</title>
<date>2001</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="14069" citStr="Baayen (2001)" startWordPosition="2368" endWordPosition="2369">ord n-grams were created in 120 hours, using 20GB of disk space. The Zipf curves are plotted in Figure 3 showing that the unigram curve now has a larger slope than –1, it is around –1.2. All the n-gram curves are now straighter and more parallel than the traditional n-gram curves, have slopes within [-0.67, -0.5]. Usually, Zipf’s rank-frequency law with a slope –1 is confirmed by empirical data, but the reduced n-grams for English and Chinese shown in Figure 1, Figure 2 and Figure 3 do not confirm it. In fact, various more sophisticated models for frequency distributions have been proposed by Baayen (2001) and Evert (2004.) 5 Perplexity for Reduced N-Grams The reduced n-gram approach was used to build a statistical language model based on the weighted average model of O’Boyle, Owens and Smith (1994.) We rewrite this model in formulae (2) and (3) (2) (3) 1=0 This averages the probabilities of a word wi following the previous one word, two words, three words, etc. (i.e. making the last word of an n-gram.) The averaging uses weights that increase slowly with their frequency and rapidly with the length of n-gram. This weighted average model is a variable length model that gives results comparable t</context>
</contexts>
<marker>Baayen, 2001</marker>
<rawString>Harald R. Baayen. 2001. Word Frequency Distributions. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianying Hu</author>
<author>William Turin</author>
<author>Michael K Brown</author>
</authors>
<title>Language Modeling using Stochastic Automata with Variable Length Contexts. Computer Speech and Language,</title>
<date>1997</date>
<volume>11</volume>
<pages>1--16</pages>
<marker>Hu, Turin, Brown, 1997</marker>
<rawString>Jianying Hu, William Turin and Michael K. Brown. 1997. Language Modeling using Stochastic Automata with Variable Length Contexts. Computer Speech and Language, volume 11, pages 1-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
<author>Jianfeng Gao</author>
</authors>
<title>Language Model Size Reduction By Pruning And Clustering.</title>
<date>2000</date>
<booktitle>ICSLP&apos;00.</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="6188" citStr="Goodman and Gao (2000" startWordPosition="977" endWordPosition="980"> model. They obtained word error reductions of 0.1 and 0.5% (absolute) in development and evaluation test sets, respectively. However, the number of parameters was reduced by 60%. By using the variable 4-gram, they were able to model a longer history while reducing the size of the model by more than 50%, compared to a regular trigram model, and at the same time improve both the test-set perplexity and recognition performance. They also reduced the size of the model by an additional 8%. Other related work are those of Seymore and Rosenfeld (1996); Hu, Turin and Brown (1997); Blasig (1999); and Goodman and Gao (2000.) In order to obtain an overview of variable n-grams, Table 1 combines all of their results. COMBINATION O F LANGUAGE MODEL TYPES Basic Variable Category Skipping Classes #params Perplexity Size Source n-gram n-grams distance Trigram√ 987k 474 1M LOB Bigram√ - 603.2 Trigram√ - 544.1 √ √ - 534.1 Trigram√ 743k 81.5 2M Switch board Corpus Trigram√ 379k 78.1 Trigram√ √ 363k 78.0 Trigram√ √ √ 338k 77.7 4-gram√ 580k 108 4-gram√ √ 577k 108 4-gram√ √ √ 536k 107 5-gram√ 383k 77.5 5-gram√ √ 381k 77.4 5-gram√ √ √ 359k 77.2 Table 1. Comparison of combinations of variable n-grams and other Language Models</context>
</contexts>
<marker>Goodman, Gao, 2000</marker>
<rawString>Joshua Goodman and Jianfeng Gao. 2000. Language Model Size Reduction By Pruning And Clustering. ICSLP&apos;00. Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristie Seymore</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Scalable Backoff Language Models.</title>
<date>1996</date>
<booktitle>ICSLP&apos;96,</booktitle>
<pages>232--235</pages>
<contexts>
<context position="6118" citStr="Seymore and Rosenfeld (1996)" startWordPosition="965" endWordPosition="968">roving the perplexity and word error rate results compared to a fixed trigram model. They obtained word error reductions of 0.1 and 0.5% (absolute) in development and evaluation test sets, respectively. However, the number of parameters was reduced by 60%. By using the variable 4-gram, they were able to model a longer history while reducing the size of the model by more than 50%, compared to a regular trigram model, and at the same time improve both the test-set perplexity and recognition performance. They also reduced the size of the model by an additional 8%. Other related work are those of Seymore and Rosenfeld (1996); Hu, Turin and Brown (1997); Blasig (1999); and Goodman and Gao (2000.) In order to obtain an overview of variable n-grams, Table 1 combines all of their results. COMBINATION O F LANGUAGE MODEL TYPES Basic Variable Category Skipping Classes #params Perplexity Size Source n-gram n-grams distance Trigram√ 987k 474 1M LOB Bigram√ - 603.2 Trigram√ - 544.1 √ √ - 534.1 Trigram√ 743k 81.5 2M Switch board Corpus Trigram√ 379k 78.1 Trigram√ √ 363k 78.0 Trigram√ √ √ 338k 77.7 4-gram√ 580k 108 4-gram√ √ 577k 108 4-gram√ √ √ 536k 107 5-gram√ 383k 77.5 5-gram√ √ 381k 77.4 5-gram√ √ √ 359k 77.2 Table 1. Co</context>
</contexts>
<marker>Seymore, Rosenfeld, 1996</marker>
<rawString>Kristie Seymore and Ronald Rosenfeld. 1996. Scalable Backoff Language Models. ICSLP&apos;96, pages 232-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Smith</author>
</authors>
<title>Zipf and TypeToken rules for the English and Irish languages. MIDL workshop.</title>
<date>2004</date>
<location>Paris.</location>
<contexts>
<context position="12971" citStr="Smith, 2004" startWordPosition="2158" endWordPosition="2159">he NANT reduced n-grams are created by the improved algorithm after over 300 hours processing, needing a storage requirement of 100GBytes on a Pentium II 586 of 512MByte RAM. Their Zipf curves are plotted for reduced unigrams and n-grams in Figure 2 showing all the curves are just sloped around [-0.54, -0.5]. The reduced unigrams of NANT still show the 2-slope behavior when it starts with slope –0.54 and then drop with slope nearly –2 at the end of the curve. We have found that the traditional n-grams also show this behaviour, with an initial slope of –1 changing to –2 for large ranks (Ha and Smith, 2004.) 0 1 2 3 4 5 6 7 8 log rank Figure 2. The NANT reduced n-gram Zipf curves 4.3 Mandarin News compound words The Zipf curves are plotted for the smaller Chinese TREC reduced unigrams and n-grams were shown by Ha et al. (2005.) 1-gram 2-gram 3-gram 4-gram 5-gram slope -1 0 1 2 3 4 5 6 7 4 0 3 2 1 log frequency 6 1-gram 2-gram 5 4 3-gram 4-gram 5-gram slope -1 1 0 3 2 log frequency 312 0 1 2 3 4 5 6 7 log rank Figure 3. Mandarin reduced n-gram Zipf curves The Mandarin News reduced word n-grams were created in 120 hours, using 20GB of disk space. The Zipf curves are plotted in Figure 3 showing th</context>
</contexts>
<marker>Smith, 2004</marker>
<rawString>Le Q. Ha and Francis. J. Smith. 2004. Zipf and TypeToken rules for the English and Irish languages. MIDL workshop. Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Q Ha</author>
<author>Elvira I Sicilia-Garcia</author>
<author>Ji Ming</author>
<author>Francis J Smith</author>
</authors>
<date>2002</date>
<booktitle>Extension of Zipf’s Law to Words and Phrases. COLING&apos;02,</booktitle>
<volume>1</volume>
<pages>315--320</pages>
<contexts>
<context position="10911" citStr="Ha et al. (2002)" startWordPosition="1780" endWordPosition="1783">he Mandarin News corpus is 6,800. Ha, Sicilia-Garcia, Ming and Smith (2003) produced a compound word version of the Mandarin News corpus with 50,000 types; this version was employed in our study for reduced n-grams. We next present the Zipf curves (Zipf, 1949) for the English and Chinese reduced n-grams. 4.1 Wall Street Journal The WSJ reduced n-grams can be created by the original O’Boyle-Smith algorithm implemented on a Pentium II 586 of 512MByte RAM for over 40 hours, the disk storage requirement being only 5GBytes. 311 The conventional 10-highest frequency WSJ words have been published by Ha et al. (2002) and the most common WSJ reduced unigrams, bigrams and trigrams are shown in Table 2. It illustrates that the most common reduced word is not THE; even OF is not in the top ten. These words are now mainly part of longer n-grams with large n. The Zipf curves are plotted for reduced unigrams and n-grams in Figure 1 showing all the curves have slopes within [-0.6, -0.5]. The WSJ reduced bigram, trigram, 4-gram and 5-gram curves become almost parallel and straight, with a small observed noise between the reduced 4-gram and 5-gram curves when they cut each other at the beginning. Note that informat</context>
</contexts>
<marker>Ha, Sicilia-Garcia, Ming, Smith, 2002</marker>
<rawString>Le Q. Ha, Elvira I. Sicilia-Garcia, Ji Ming and Francis J. Smith. 2002. Extension of Zipf’s Law to Words and Phrases. COLING&apos;02, volume 1, pages 315-320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Q Ha</author>
<author>Elvira I Sicilia-Garcia</author>
<author>Ji Ming</author>
<author>Francis J Smith</author>
</authors>
<title>Extension of Zipf’s Law to Word and Character N-Grams for English and Chinese.</title>
<date>2003</date>
<pages>8--1</pages>
<publisher>CLCLP,</publisher>
<marker>Ha, Sicilia-Garcia, Ming, Smith, 2003</marker>
<rawString>Le Q. Ha, Elvira I. Sicilia-Garcia, Ji Ming and Francis J. Smith. 2003. Extension of Zipf’s Law to Word and Character N-Grams for English and Chinese. CLCLP, 8(1):77-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Q Ha</author>
<author>Rowan Seymour</author>
<author>Philip Hanna</author>
<author>Francis J Smith</author>
</authors>
<title>Reduced N-Grams for Chinese Evaluation.</title>
<date>2005</date>
<pages>10--1</pages>
<publisher>CLCLP,</publisher>
<contexts>
<context position="13195" citStr="Ha et al. (2005" startWordPosition="2201" endWordPosition="2204">grams and n-grams in Figure 2 showing all the curves are just sloped around [-0.54, -0.5]. The reduced unigrams of NANT still show the 2-slope behavior when it starts with slope –0.54 and then drop with slope nearly –2 at the end of the curve. We have found that the traditional n-grams also show this behaviour, with an initial slope of –1 changing to –2 for large ranks (Ha and Smith, 2004.) 0 1 2 3 4 5 6 7 8 log rank Figure 2. The NANT reduced n-gram Zipf curves 4.3 Mandarin News compound words The Zipf curves are plotted for the smaller Chinese TREC reduced unigrams and n-grams were shown by Ha et al. (2005.) 1-gram 2-gram 3-gram 4-gram 5-gram slope -1 0 1 2 3 4 5 6 7 4 0 3 2 1 log frequency 6 1-gram 2-gram 5 4 3-gram 4-gram 5-gram slope -1 1 0 3 2 log frequency 312 0 1 2 3 4 5 6 7 log rank Figure 3. Mandarin reduced n-gram Zipf curves The Mandarin News reduced word n-grams were created in 120 hours, using 20GB of disk space. The Zipf curves are plotted in Figure 3 showing that the unigram curve now has a larger slope than –1, it is around –1.2. All the n-gram curves are now straighter and more parallel than the traditional n-gram curves, have slopes within [-0.67, -0.5]. Usually, Zipf’s rank-fr</context>
<context position="15075" citStr="Ha et al. (2005)" startWordPosition="2577" endWordPosition="2580"> of an n-gram.) The averaging uses weights that increase slowly with their frequency and rapidly with the length of n-gram. This weighted average model is a variable length model that gives results comparable to the Katz back-off method (1987), but is quicker to use. The probabilities of all of the sentences ~1 in � a test text are then calculated by the weighted average model � � � � � � � � � � ( ) ( ) ( ) ( 1 ) � �− = 2 1 ... 1 �� 1 �� �� � 1 and an average perplexity of each sentence is evaluated using Equation (5) � 1 ��� ( )=  − ∑= � exp  �� � � � � � ( ( ... )) 1 �� � 1 2 �−1  � � 1 Ha et al. (2005) already investigated and analysed the main difficulties arising from perplexity calculations for our reduced model: a statistical model problem, an unseen word problem and an unknown word problem. Their solutions are applied in this paper also. Similar problems have been found by other authors, e.g. Martin, Liermann and Ney (1997); Kneser and Ney (1995.) The perplexity calculations for both the English and Chinese reduced n-grams includes statistics on phrase lengths starting with unigrams, bigrams, trigrams...and on up to the longest phrase which occur in the reduced model. The perplexities </context>
<context position="17783" citStr="Ha et al. (2005)" startWordPosition="3044" endWordPosition="3047">seline Trigrams Model size reduction 11.28 Table 5. Reduced perplexities for Mandarin News words In all three cases the reduced model produces a modest improvement over the traditional 3-gram model, but is not as good as the traditional 4-gram or higher models. However in all three cases the result is obtained with a significant reduction in model size, from a factor of 11 to almost 15 compared to the traditional 7-gram model size. We did expect a greater improvement in perplexity than we obtained and we believe that a further look at the methods used to solve the difficult problems listed by Ha et al. (2005) (mentioned above) and others mentioned by Ha (2005) might lead to an improvement. Missing word tests are also needed. 6 Conclusions The conventional n-gram language model is limited in terms of its ability to represent extended phrase histories because of the exponential growth in the number of parameters. To overcome this limitation, we have re-investigated the approach of O’Boyle (1993) and created reduced n-gram models. Our aim was to try to create an n-gram model that used semantically more complete n-grams than traditional n-grams in the expectation that this might lead to an improvement</context>
</contexts>
<marker>Ha, Seymour, Hanna, Smith, 2005</marker>
<rawString>Le Q. Ha, Rowan Seymour, Philip Hanna and Francis J. Smith. 2005. Reduced N-Grams for Chinese Evaluation. CLCLP, 10(1):19-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manhung Siu</author>
<author>Mari Ostendorf</author>
</authors>
<title>Integrating a Context-Dependent Phrase Grammar</title>
<date>2000</date>
<booktitle>in the Variable N-Gram framework. ICASSP&apos;00,</booktitle>
<volume>3</volume>
<pages>1643--1646</pages>
<contexts>
<context position="5409" citStr="Siu and Ostendorf (2000)" startWordPosition="849" endWordPosition="852">the same size, and if a moderate loss in performance is acceptable, that the size of a language model can be reduced drastically by using his pruning algorithm. Kneser’s results improve with longer contexts and a same number of parameters. For example, reducing the size of the standard NAB trigram model by a factor of 3 results in a loss of only 7% in perplexity and 3% in the word error rate. The improvement obtained by Kneser’s method depended on the length of the fixed context and on the amount of available training data. In the case of the NAB corpus, the improvement was 10% in perplexity. Siu and Ostendorf (2000) developed Kneser‘s basic ideas further and applied the variable 4-gram, thus improving the perplexity and word error rate results compared to a fixed trigram model. They obtained word error reductions of 0.1 and 0.5% (absolute) in development and evaluation test sets, respectively. However, the number of parameters was reduced by 60%. By using the variable 4-gram, they were able to model a longer history while reducing the size of the model by more than 50%, compared to a regular trigram model, and at the same time improve both the test-set perplexity and recognition performance. They also re</context>
</contexts>
<marker>Siu, Ostendorf, 2000</marker>
<rawString>Manhung Siu and Mari Ostendorf. 2000. Integrating a Context-Dependent Phrase Grammar in the Variable N-Gram framework. ICASSP&apos;00, volume 3, pages 1643-1646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manhung Siu</author>
<author>Mari Ostendorf</author>
</authors>
<title>Variable N-Grams and Extensions for Conversational Speech Language Modelling.</title>
<date>2000</date>
<booktitle>IEEE Transactions on Speech and Audio Processing,</booktitle>
<pages>8--1</pages>
<contexts>
<context position="5409" citStr="Siu and Ostendorf (2000)" startWordPosition="849" endWordPosition="852">the same size, and if a moderate loss in performance is acceptable, that the size of a language model can be reduced drastically by using his pruning algorithm. Kneser’s results improve with longer contexts and a same number of parameters. For example, reducing the size of the standard NAB trigram model by a factor of 3 results in a loss of only 7% in perplexity and 3% in the word error rate. The improvement obtained by Kneser’s method depended on the length of the fixed context and on the amount of available training data. In the case of the NAB corpus, the improvement was 10% in perplexity. Siu and Ostendorf (2000) developed Kneser‘s basic ideas further and applied the variable 4-gram, thus improving the perplexity and word error rate results compared to a fixed trigram model. They obtained word error reductions of 0.1 and 0.5% (absolute) in development and evaluation test sets, respectively. However, the number of parameters was reduced by 60%. By using the variable 4-gram, they were able to model a longer history while reducing the size of the model by more than 50%, compared to a regular trigram model, and at the same time improve both the test-set perplexity and recognition performance. They also re</context>
</contexts>
<marker>Siu, Ostendorf, 2000</marker>
<rawString>Manhung Siu and Mari Ostendorf. 2000. Variable N-Grams and Extensions for Conversational Speech Language Modelling. IEEE Transactions on Speech and Audio Processing, 8(1):63-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nelson Francis</author>
<author>Henry Kucera</author>
</authors>
<title>Manual of Information to Accompany A Standard Corpus of Present-Day Edited American English, for use with Digital Computers.</title>
<date>1964</date>
<institution>Department of Linguistics, Brown University,</institution>
<location>Providence, Rhode Island.</location>
<contexts>
<context position="1935" citStr="Francis and Kucera, 1964" startWordPosition="306" endWordPosition="309"> probability of zero for “ENQUIRIES” following “RAIL” if it was not preceded by “BRITISH.” The phrase with a new reduced frequency is called a reduced phrase. Therefore, a phrase can occur in a corpus as a reduced n-gram in some places and as part of a larger reduced n-gram in other places. In a reduced model, the occurrence of an n-gram is not counted when it is a part of a larger reduced n-gram. One algorithm to detect/identify/extract reduced n-grams from a corpus is the so-called reduced n-gram algorithm. In 1992, O’Boyle was able to use it to analyse the Brown corpus of American English (Francis and Kucera, 1964) (of one million word tokens, whose longest phraseAbstract Statistical language models should improve as the size of the n-grams increases from 3 to 5 or higher. However, the number of parameters and calculations, and the storage requirement increase very rapidly if we attempt to store all possible combinations of n-grams. To avoid these problems, the reduced n-grams’ approach previously developed by O’Boyle (1993) can be applied. A reduced n-gram language model can store an entire corpus’s phrase-history length within feasible storage limits. Another theoretical advantage of reduced n-grams i</context>
</contexts>
<marker>Francis, Kucera, 1964</marker>
<rawString>Nelson Francis and Henry Kucera. 1964. Manual of Information to Accompany A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. Department of Linguistics, Brown University, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter L O’Boyle</author>
</authors>
<title>A study of an N-Gram Language Model for Speech Recognition. PhD thesis.</title>
<date>1993</date>
<institution>Queen’s University Belfast.</institution>
<marker>O’Boyle, 1993</marker>
<rawString>Peter L. O’Boyle. 1993. A study of an N-Gram Language Model for Speech Recognition. PhD thesis. Queen’s University Belfast.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter O’Boyle</author>
<author>John McMahon</author>
<author>Francis J Smith</author>
</authors>
<title>Combining a Multi-Level Class Hierarchy with Weighted-Average Function-Based Smoothing. IEEE Automatic Speech Recognition Workshop.</title>
<date>1995</date>
<location>Snowbird, Utah.</location>
<marker>O’Boyle, McMahon, Smith, 1995</marker>
<rawString>Peter O’Boyle, John McMahon and Francis J. Smith. 1995. Combining a Multi-Level Class Hierarchy with Weighted-Average Function-Based Smoothing. IEEE Automatic Speech Recognition Workshop. Snowbird, Utah.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter O’Boyle</author>
<author>Marie Owens</author>
<author>Francis J Smith</author>
</authors>
<title>A weighted average N-Gram model of natural language.</title>
<date>1994</date>
<journal>Computer Speech and Language,</journal>
<volume>8</volume>
<pages>337--349</pages>
<marker>O’Boyle, Owens, Smith, 1994</marker>
<rawString>Peter O’Boyle, Marie Owens and Francis J. Smith. 1994. A weighted average N-Gram model of natural language. Computer Speech and Language, volume 8, pages 337-349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramon Ferrer I Cancho</author>
<author>Ricard V Solé</author>
</authors>
<title>Two Regimes in the Frequency of Words and the Origin of Complex Lexicons.</title>
<date>2002</date>
<journal>Journal of Quantitative Linguistics,</journal>
<pages>8--3</pages>
<marker>Cancho, Solé, 2002</marker>
<rawString>Ramon Ferrer I. Cancho and Ricard V. Solé. 2002. Two Regimes in the Frequency of Words and the Origin of Complex Lexicons. Journal of Quantitative Linguistics, 8(3):165-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Blasig</author>
</authors>
<date>1999</date>
<booktitle>Combination of Words and Word Categories in Varigram Histories. ICASSP&apos;99,</booktitle>
<volume>1</volume>
<pages>529--532</pages>
<contexts>
<context position="6161" citStr="Blasig (1999)" startWordPosition="974" endWordPosition="975"> to a fixed trigram model. They obtained word error reductions of 0.1 and 0.5% (absolute) in development and evaluation test sets, respectively. However, the number of parameters was reduced by 60%. By using the variable 4-gram, they were able to model a longer history while reducing the size of the model by more than 50%, compared to a regular trigram model, and at the same time improve both the test-set perplexity and recognition performance. They also reduced the size of the model by an additional 8%. Other related work are those of Seymore and Rosenfeld (1996); Hu, Turin and Brown (1997); Blasig (1999); and Goodman and Gao (2000.) In order to obtain an overview of variable n-grams, Table 1 combines all of their results. COMBINATION O F LANGUAGE MODEL TYPES Basic Variable Category Skipping Classes #params Perplexity Size Source n-gram n-grams distance Trigram√ 987k 474 1M LOB Bigram√ - 603.2 Trigram√ - 544.1 √ √ - 534.1 Trigram√ 743k 81.5 2M Switch board Corpus Trigram√ 379k 78.1 Trigram√ √ 363k 78.0 Trigram√ √ √ 338k 77.7 4-gram√ 580k 108 4-gram√ √ 577k 108 4-gram√ √ √ 536k 107 5-gram√ 383k 77.5 5-gram√ √ 381k 77.4 5-gram√ √ √ 359k 77.2 Table 1. Comparison of combinations of variable n-gram</context>
</contexts>
<marker>Blasig, 1999</marker>
<rawString>Reinhard Blasig. 1999. Combination of Words and Word Categories in Varigram Histories. ICASSP&apos;99, volume 1, pages 529-532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Backing-off for M-Gram Language Modeling.</title>
<date>1995</date>
<volume>95</volume>
<pages>181--184</pages>
<location>Detroit.</location>
<contexts>
<context position="15430" citStr="Kneser and Ney (1995" startWordPosition="2631" endWordPosition="2634">y the weighted average model � � � � � � � � � � ( ) ( ) ( ) ( 1 ) � �− = 2 1 ... 1 �� 1 �� �� � 1 and an average perplexity of each sentence is evaluated using Equation (5) � 1 ��� ( )=  − ∑= � exp  �� � � � � � ( ( ... )) 1 �� � 1 2 �−1  � � 1 Ha et al. (2005) already investigated and analysed the main difficulties arising from perplexity calculations for our reduced model: a statistical model problem, an unseen word problem and an unknown word problem. Their solutions are applied in this paper also. Similar problems have been found by other authors, e.g. Martin, Liermann and Ney (1997); Kneser and Ney (1995.) The perplexity calculations for both the English and Chinese reduced n-grams includes statistics on phrase lengths starting with unigrams, bigrams, trigrams...and on up to the longest phrase which occur in the reduced model. The perplexities of the WSJ reduced model are shown in Table 3, North American News Text corpus in Table 4 and Mandarin News words in Table 5. The nature of the reduced model makes the reporting of results for limited sizes of n-grams to be inappropriate, although these are valid for a traditional n-gram model. Therefore we show results for several n-gram sizes for the </context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved Backing-off for M-Gram Language Modeling. ICASSP&apos;95, volume 1, pages 181-184. Detroit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
</authors>
<title>Statistical Language Modeling Using a Variable Context Length.</title>
<date>1996</date>
<volume>96</volume>
<pages>494--497</pages>
<contexts>
<context position="4511" citStr="Kneser (1996)" startWordPosition="701" endWordPosition="702">-gram language modeling has provided an efficient representation of n-gram models and made the training of higher order n-grams possible. Compared to variable n-grams, class-based language models are more often used to reduce the size of a language model, but this typically leads to recognition performance degradation. Classes can alternatively be used to smooth a language model or provide back-off estimates, which have led to small performance gains. For the LOB corpus, the varigram model obtained 11.3% higher perplexity in comparison with the word-trigram model (Niesler and Woodland, 1996.) Kneser (1996) built up variable-context length language models based on the North American Business News (NAB-240 million words) and the German Verbmobil (300,000 words with a vocabulary of 5,000 types.) His results show that the variable-length model outperforms conventional models of the same size, and if a moderate loss in performance is acceptable, that the size of a language model can be reduced drastically by using his pruning algorithm. Kneser’s results improve with longer contexts and a same number of parameters. For example, reducing the size of the standard NAB trigram model by a factor of 3 resu</context>
</contexts>
<marker>Kneser, 1996</marker>
<rawString>Reinhard Kneser. 1996. Statistical Language Modeling Using a Variable Context Length. ICSLP&apos;96, volume 1, pages 494-497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer.</title>
<date>1987</date>
<booktitle>In IEEE Transactions on Acoustics, Speech and Signal Processing,</booktitle>
<volume>35</volume>
<pages>400--401</pages>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. In IEEE Transactions on Acoustics, Speech and Signal Processing, volume ASSP-35, pages 400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>A Simple LNRE Model for Random Character Sequences.</title>
<date>2004</date>
<booktitle>In Proc. of the 7emes Journees Internationales d&apos;Analyse Statistique des Donnees Textuelles,</booktitle>
<pages>411--422</pages>
<contexts>
<context position="14085" citStr="Evert (2004" startWordPosition="2371" endWordPosition="2372">reated in 120 hours, using 20GB of disk space. The Zipf curves are plotted in Figure 3 showing that the unigram curve now has a larger slope than –1, it is around –1.2. All the n-gram curves are now straighter and more parallel than the traditional n-gram curves, have slopes within [-0.67, -0.5]. Usually, Zipf’s rank-frequency law with a slope –1 is confirmed by empirical data, but the reduced n-grams for English and Chinese shown in Figure 1, Figure 2 and Figure 3 do not confirm it. In fact, various more sophisticated models for frequency distributions have been proposed by Baayen (2001) and Evert (2004.) 5 Perplexity for Reduced N-Grams The reduced n-gram approach was used to build a statistical language model based on the weighted average model of O’Boyle, Owens and Smith (1994.) We rewrite this model in formulae (2) and (3) (2) (3) 1=0 This averages the probabilities of a word wi following the previous one word, two words, three words, etc. (i.e. making the last word of an n-gram.) The averaging uses weights that increase slowly with their frequency and rapidly with the length of n-gram. This weighted average model is a variable length model that gives results comparable to the Katz back-</context>
</contexts>
<marker>Evert, 2004</marker>
<rawString>Stefan Evert. 2004. A Simple LNRE Model for Random Character Sequences. In Proc. of the 7emes Journees Internationales d&apos;Analyse Statistique des Donnees Textuelles, pages 411-422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven C Martin</author>
<author>Jörg Liermann</author>
<author>Hermann Ney</author>
</authors>
<title>Adaptive Topic-Dependent Language Modelling Using Word-Based Varigrams.</title>
<date>1997</date>
<booktitle>EuroSpeech&apos;97,</booktitle>
<volume>3</volume>
<pages>1447--1450</pages>
<location>Rhodes.</location>
<marker>Martin, Liermann, Ney, 1997</marker>
<rawString>Sven C. Martin, Jörg Liermann and Hermann Ney. 1997. Adaptive Topic-Dependent Language Modelling Using Word-Based Varigrams. EuroSpeech&apos;97, volume 3, pages 1447-1450. Rhodes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas R Niesler</author>
<author>Phil C Woodland</author>
</authors>
<title>A Variable-Length Category-Based N-Gram Language Model.</title>
<date>1996</date>
<booktitle>ICASSP&apos;96,</booktitle>
<volume>1</volume>
<pages>164--167</pages>
<contexts>
<context position="4495" citStr="Niesler and Woodland, 1996" startWordPosition="697" endWordPosition="700">Recent progress in variable n-gram language modeling has provided an efficient representation of n-gram models and made the training of higher order n-grams possible. Compared to variable n-grams, class-based language models are more often used to reduce the size of a language model, but this typically leads to recognition performance degradation. Classes can alternatively be used to smooth a language model or provide back-off estimates, which have led to small performance gains. For the LOB corpus, the varigram model obtained 11.3% higher perplexity in comparison with the word-trigram model (Niesler and Woodland, 1996.) Kneser (1996) built up variable-context length language models based on the North American Business News (NAB-240 million words) and the German Verbmobil (300,000 words with a vocabulary of 5,000 types.) His results show that the variable-length model outperforms conventional models of the same size, and if a moderate loss in performance is acceptable, that the size of a language model can be reduced drastically by using his pruning algorithm. Kneser’s results improve with longer contexts and a same number of parameters. For example, reducing the size of the standard NAB trigram model by a </context>
</contexts>
<marker>Niesler, Woodland, 1996</marker>
<rawString>Thomas R. Niesler and Phil C. Woodland. 1996. A Variable-Length Category-Based N-Gram Language Model. ICASSP&apos;96, volume 1, pages 164-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas R Niesler</author>
</authors>
<title>Category-based statistical language models.</title>
<date>1997</date>
<institution>St. John’s College, University of Cambridge.</institution>
<marker>Niesler, 1997</marker>
<rawString>Thomas R. Niesler. 1997. Category-based statistical language models. St. John’s College, University of Cambridge.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>