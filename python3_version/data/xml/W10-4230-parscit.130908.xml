<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.067792">
<title confidence="0.962608">
The UMUS system for named entity generation at GREC 2010
</title>
<author confidence="0.941069">
Benoit Favre Bernd Bohnet
</author>
<affiliation confidence="0.6517245">
LIUM, Universit´e du Maine Universit¨at Stuttgart
72000 Le Mans, France Stuttgart, Germany
</affiliation>
<email confidence="0.994048">
benoit.favre@gmail.com bohnet@informatik.uni-stuttgart.de
</email>
<sectionHeader confidence="0.993737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977416666667">
We present the UMUS (Universit´e du
Maine/Universit¨at Stuttgart) submission
for the NEG task at GREC’10. We re-
fined and tuned our 2009 system but we
still rely on predicting generic labels and
then choosing from the list of expressions
that match those labels. We handled recur-
sive expressions with care by generating
specific labels for all the possible embed-
dings. The resulting system performs at a
type accuracy of 0.84 an a string accuracy
of 0.81 on the development set.
</bodyText>
<sectionHeader confidence="0.998793" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997929888888889">
The Named Entity Generation (NEG) task con-
sists in choosing a referential expression (com-
plete name, last name, pronoun, possessive pro-
noun, elision...) for all person entities in a text.
Texts are biographies of chefs, composers and in-
ventors from Wikipedia. For each reference, a list
of expressions is given from which the system has
to choose. This task is challenging because of the
following aspects:
</bodyText>
<listItem confidence="0.985929">
1. The data is imperfect as it is a patchwork of
multiple authors’ writing.
2. The problem is hard to handle with a classi-
fier because text is predicted, not classes.
3. The problem has a complex graph structure.
4. Some decisions are recursive for embedded
references, i.e. “his father”.
5. Syntactic/semantic features cannot be ex-
tracted with a classical parser because the
word sequence is latent.
</listItem>
<bodyText confidence="0.9864688">
We do not deal with all of these challenges but
we try to mitigate their impact. Our system ex-
tends our approach for GREC’09 (Favre and Bon-
het, 2009). We use a sequence classifier to predict
generic labels for the possible expressions.
</bodyText>
<sectionHeader confidence="0.906123" genericHeader="method">
2 Labels for classification
</sectionHeader>
<bodyText confidence="0.718572">
Each referential expression (REFEX) is given a la-
bel consisting of sub-elements:
</bodyText>
<listItem confidence="0.999210727272727">
• The REG08 TYPE as given in the REFEX
(name, common, pronoun, empty...)
• The CASE as given in the REFEX (plain,
genitive, accusative...)
• If the expression is a pronoun, then one of
“he, him, his, who, whom, whose, that”, after
gender and number normalization.
• “self” if the expression contains “self”.
• “short” if the expression is a one-word long
name or common name.
• “nesting” if the expression is recursive.
</listItem>
<bodyText confidence="0.998867444444444">
For recursive expressions, a special handling is ap-
plied: All possible assignments of the embedded
entities are generated with labels corresponding
to the concatenation of the involved entities’ la-
bels. If the embedding is on the right (left) side
of the expression, “right” (“left”) is added to the
label. Non-sensical labels (i.e. “he father”) are not
seen in the training data, and therefore not hypoth-
esized.
</bodyText>
<sectionHeader confidence="0.999821" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.9924895">
Each reference is characterized with the following
features:
</bodyText>
<listItem confidence="0.997781951219512">
• SYNFUNC, SEMCAT, SYNCAT: syntactic
function, semantic category, syntactic cate-
gory, as given in REF node.
• CHANGE, CHANGE+SYNFUNC: previous
reference is for a different entity, possibly
with syntactic function.
• PREV GENDER NUMBER: if the refer-
ence is from a different entity, can be “same”
or “different”. The attribute is being com-
pared is “male”, “female” or “plural”, deter-
mined by looking at the possible expressions.
• FIRST TIME: denotes if it’s the first time
that the entity is seen. For plural entities, the
entity is considered new if at least one of the
involved entities is new.
• BEG PARAGRAPH: the first entity of a
paragraph.
• {PREV,NEXT} PUNCT: the punctuation
immediately before (after) the entity. Can be
“sentence” if the punctuation is one of “.?!”,
“comma” for“,;”, “parenthesis” for “()[]”
and “quote”.
• {PREV,NEXT} SENT: whether or not a sen-
tence boundary occurs after (before) the pre-
vious (next) reference.
• {PREV,NEXT} WORD {1,2}GRAM: cor-
responding word n-gram. Words are ex-
tracted up to the previous/next reference or
the start/end of a sentence, with parenthe-
sized content removed. Words are lower-
cased tokens made of letters and numbers.
• {PREV,NEXT} TAG: most likely part-of-
speech tag for the previous/next word, skip-
ping adverbs.
• {PREV,NEXT} BE: any form of the verb “to
be” is used after (before) the previous (next)
reference.
• EMBEDS PREV: the entity being embedded
was referred to just before.
• EMBEDS ALL KNOWN: all the entities be-
ing embedded have been seen before.
</listItem>
<sectionHeader confidence="0.718479" genericHeader="method">
4 Sequence classifier
</sectionHeader>
<bodyText confidence="0.998660833333334">
We rely on Conditional Random Fields1 (Lafferty
et al., 2001) for predicting one label (as defined
previously) per reference. We lay the problem as
one sequence of decisions per entity to prevent, for
instance, the use of the same name twice in a row.
Last year, we generated one sequence per docu-
ment with all entities, but it was less intuitive. To
the features extracted for each reference, we add
the features of the previous and next reference, ac-
cording to label unigrams and label bigrams. The
c hyperparameter and the frequency cutoff of the
classifier are optimized on the dev set. Note that
</bodyText>
<footnote confidence="0.62975">
1CRF++, http://crfpp.sourceforge.net
</footnote>
<bodyText confidence="0.9990625">
for processing the test set, we added the develop-
ment data to the training set.
</bodyText>
<sectionHeader confidence="0.937194" genericHeader="method">
5 Text generation
</sectionHeader>
<bodyText confidence="0.999917">
For each reference, the given expressions are
ranked by classifier-estimated posterior probabil-
ity and the best one is used for output. In case
multiple expressions have the same labeling (and
the same score), we use the longest one and iter-
ate through the list for each subsequent use (useful
for repeated common names). If an expression is
more than 4 words, it’s flagged for not being used
a second time (only ad-hoc rule in the system).
</bodyText>
<sectionHeader confidence="0.99985" genericHeader="conclusions">
6 Results
</sectionHeader>
<tableCaption confidence="0.85698875">
Evaluation scores for the output are presented in
Table 1. The source code of our systems is made
available to the community at http://code.google
.com/p/icsicrf-grecneg.
</tableCaption>
<table confidence="0.961690333333333">
Sys. T.acc Prec. Rec. S.acc Bleu Nist
Old 0.826 0.830 0.830 0.786 0.811 5.758
New 0.844 0.829 0.816 0.813 0.817 6.021
</table>
<tableCaption confidence="0.991826">
Table 1: Results on the dev set comparing our sys-
</tableCaption>
<bodyText confidence="0.988240875">
tem from last year (old) to the refined one (new),
according to REG08 TYPE accuracy (T.acc), pre-
cision and recall, String accuracy (S.acc), BLEU1
an NIST.
About 50% of the errors are caused by the se-
lection of pronouns instead of a name. The selec-
tion of the pronoun or name seems to depend on
the writing style since a few authors prefer nearly
always the name. The misuse of names instead
of pronouns is second most error with about 15%.
The complex structured named entities are respon-
sible for about 9% of the errors. The selection of
the right name such as given name, family name or
both seems to be more difficult. The next frequent
errors are confusions between pronouns, elisions,
common names, and names.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99965825">
Benoit Favre and Bernd Bonhet. 2009. ICSI-CRF: The
Generation of References to the Main Subject and
Named Entities Using Conditional Random Fields.
In ACL-IJCNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for
segmenting and labeling sequence data. Machine
Learning, pages 282–289.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.847314">
<title confidence="0.89905">The UMUS system for named entity generation at GREC 2010</title>
<author confidence="0.99873">Benoit Favre Bernd Bohnet</author>
<affiliation confidence="0.998438">LIUM, Universit´e du Maine Universit¨at Stuttgart</affiliation>
<address confidence="0.999789">72000 Le Mans, France Stuttgart, Germany</address>
<email confidence="0.993519">benoit.favre@gmail.combohnet@informatik.uni-stuttgart.de</email>
<abstract confidence="0.996140846153846">We present the UMUS (Universit´e du Maine/Universit¨at Stuttgart) submission for the NEG task at GREC’10. We refined and tuned our 2009 system but we still rely on predicting generic labels and then choosing from the list of expressions that match those labels. We handled recursive expressions with care by generating specific labels for all the possible embeddings. The resulting system performs at a type accuracy of 0.84 an a string accuracy of 0.81 on the development set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Benoit Favre</author>
<author>Bernd Bonhet</author>
</authors>
<title>ICSI-CRF: The Generation of References to the Main Subject and Named Entities Using Conditional Random Fields.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="1691" citStr="Favre and Bonhet, 2009" startWordPosition="268" endWordPosition="272">as to choose. This task is challenging because of the following aspects: 1. The data is imperfect as it is a patchwork of multiple authors’ writing. 2. The problem is hard to handle with a classifier because text is predicted, not classes. 3. The problem has a complex graph structure. 4. Some decisions are recursive for embedded references, i.e. “his father”. 5. Syntactic/semantic features cannot be extracted with a classical parser because the word sequence is latent. We do not deal with all of these challenges but we try to mitigate their impact. Our system extends our approach for GREC’09 (Favre and Bonhet, 2009). We use a sequence classifier to predict generic labels for the possible expressions. 2 Labels for classification Each referential expression (REFEX) is given a label consisting of sub-elements: • The REG08 TYPE as given in the REFEX (name, common, pronoun, empty...) • The CASE as given in the REFEX (plain, genitive, accusative...) • If the expression is a pronoun, then one of “he, him, his, who, whom, whose, that”, after gender and number normalization. • “self” if the expression contains “self”. • “short” if the expression is a one-word long name or common name. • “nesting” if the expressio</context>
</contexts>
<marker>Favre, Bonhet, 2009</marker>
<rawString>Benoit Favre and Bernd Bonhet. 2009. ICSI-CRF: The Generation of References to the Main Subject and Named Entities Using Conditional Random Fields. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="4391" citStr="Lafferty et al., 2001" startWordPosition="710" endWordPosition="713">nding word n-gram. Words are extracted up to the previous/next reference or the start/end of a sentence, with parenthesized content removed. Words are lowercased tokens made of letters and numbers. • {PREV,NEXT} TAG: most likely part-ofspeech tag for the previous/next word, skipping adverbs. • {PREV,NEXT} BE: any form of the verb “to be” is used after (before) the previous (next) reference. • EMBEDS PREV: the entity being embedded was referred to just before. • EMBEDS ALL KNOWN: all the entities being embedded have been seen before. 4 Sequence classifier We rely on Conditional Random Fields1 (Lafferty et al., 2001) for predicting one label (as defined previously) per reference. We lay the problem as one sequence of decisions per entity to prevent, for instance, the use of the same name twice in a row. Last year, we generated one sequence per document with all entities, but it was less intuitive. To the features extracted for each reference, we add the features of the previous and next reference, according to label unigrams and label bigrams. The c hyperparameter and the frequency cutoff of the classifier are optimized on the dev set. Note that 1CRF++, http://crfpp.sourceforge.net for processing the test</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. Machine Learning, pages 282–289.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>