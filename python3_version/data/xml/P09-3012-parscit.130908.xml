<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.992631">
Creating a Gold Standard for Sentence Clustering in Multi-Document
Summarization
</title>
<author confidence="0.995055">
Johanna Geiss
</author>
<affiliation confidence="0.9696425">
University of Cambridge
Computer Laboratory
</affiliation>
<address confidence="0.975623">
15 JJ Thomson Avenue
Cambridge, CB3 0FD, UK
</address>
<email confidence="0.999487">
johanna.geiss@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.994815" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99934725">
Sentence Clustering is often used as a first
step in Multi-Document Summarization
(MDS) to find redundant information. All
the same there is no gold standard avail-
able. This paper describes the creation
of a gold standard for sentence cluster-
ing from DUC document sets. The proce-
dure of building the gold standard and the
guidelines which were given to six human
judges are described. The most widely
used and promising evaluation measures
are presented and discussed.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951296296297">
The increasing amount of (online) information and
the growing number of news websites lead to a de-
bilitating amount of redundant information. Dif-
ferent newswires publish different reports about
the same event resulting in information overlap.
Multi-Document Summarization (MDS) can help
to reduce the amount of documents a user has to
read to keep informed. In contrast to single doc-
ument summarization information overlap is one
of the biggest challenges to MDS systems. While
repeated information is a good evidence of im-
portance, this information should be included in
a summary only once in order to avoid a repeti-
tive summary. Sentence clustering has therefore
often been used as an early step in MDS (Hatzi-
vassiloglou et al., 2001; Marcu and Gerber, 2001;
Radev et al., 2000). In sentence clustering se-
mantically similar sentences are grouped together.
Sentences within a cluster overlap in information,
but they do not have to be identical in meaning.
In contrast to paraphrases sentences in a cluster do
not have to cover the same amount of information.
One sentence represents one cluster in the sum-
mary. Either a sentences from the cluster is se-
lected (Aliguliyev, 2006) or a new sentence is
regenerated from all/some sentences in a cluster
(Barzilay and McKeown, 2005). Usually the qual-
ity of the sentence clusters are only evaluated in-
directly by judging the quality of the generated
summary. There is still no standard evaluation
method for summarization and no consensus in the
summarization community how to evaluate a sum-
mary. The methods at hand are either superficial
or time and resource consuming and not easily re-
peatable. Another argument against indirect evalu-
ation of clustering is that troubleshooting becomes
more difficult. If a poor summary was created it is
not clear which component e.g. information ex-
traction through clustering or summary generation
(using for example language regeneration) is re-
sponsible for the lack of quality.
However there is no gold standard for sentence
clustering available to which the output of a clus-
tering systems can be compared. Another chal-
lenge is the evaluation of sentence clusters. There
are a lot of evaluation methods available. Each of
them focus on different properties of a set of clus-
ters. We will discuss and evaluate the most widely
used and most promising measures. In this paper
the main focus is on the development of a gold
standard for sentence clustering using DUC clus-
ters. The guidelines and rules that were given to
the human annotators are described and the inter-
judge agreement is evaluated.
</bodyText>
<sectionHeader confidence="0.999854" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9986449">
Sentence Clustering is used for different applica-
tion in NLP. Radev et al. (2000) use it in their
MDS system MEAD. The centroids of the clusters
are used to create a summary. Only the summary
is evaluated, not the sentence clusters. The same
applies to Wang et al. (2008). They use symmet-
ric matrix factorisation to group similar sentences
together and test their system on DUC2005 and
DUC2006 data set, but do not evaluate the clus-
terings. However Zha (2002) created a gold stan-
</bodyText>
<page confidence="0.944464">
96
</page>
<note confidence="0.9969395">
Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 96–104,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999574444444444">
dard relying on the section structure of web pages
and news articles. In this gold standard the sec-
tion numbers are assumed to give the true cluster
label for a sentence. In this approach only sen-
tences within the same document and even within
the same paragraph are clustered together whereas
our approach is to find similar information be-
tween documents.
A gold standard for event identification was
built by Naughton (2007). Ten annotators tagged
events in a sentence. Each sentence could be as-
signed more than one event number. In our ap-
proach a sentence can only belong to one cluster.
For the evaluation of SIMFINDER Hatzivas-
siloglou et al. (2001) created a set of 10.535 man-
ually marked pairs of paragraphs. Two human an-
notator were asked to judge if the paragraphs con-
tained ’common information’. They were given
the guideline that only paragraphs that described
the same object in the same way or in which the
same object was acting the same are to be consid-
ered similar. They found significant disagreement
between the judges but the annotators were able to
resolve their differences. Here the problem is that
only pairs of paragraphs are annotated whereas we
focus on whole sentences and create not pairs but
clusters of similar sentences.
</bodyText>
<sectionHeader confidence="0.960542" genericHeader="method">
3 Data Set for Clustering
</sectionHeader>
<bodyText confidence="0.99970095">
The data used for the creation of the gold stan-
dard was taken from the Document Understanding
Conference (DUC)1 document sets. These doc-
ument clusters were designed for the DUC tasks
which range from single-/multi-document summa-
rization to update summaries, where it is assumed
that the reader has already read earlier articles
about an event and requires only an update of the
newer development. Since DUC has moved to
TAC in 2008 they focus on the update task. In
this paper only clusters designed for the general
multi-document summarization task are used.
Our clustering data set consists of four sen-
tence sets. They were created from the docu-
ment sets d073b (DUC 2002), D0712C (DUC
2007), D0617H (DUC 2006) and d102a (DUC
2003). Especially the newer document clusters
e.g. from DUC 2006 and 2007 contain a lot of doc-
uments. In order to build good sentence clusters
the judges have to compare each sentence to each
</bodyText>
<footnote confidence="0.7273535">
1DUC has now moved to the Text Analysis Conference
(TAC)
</footnote>
<bodyText confidence="0.9953939">
other sentence and maintain an overview of the
topics within the documents. Because of human
cognitive limitations the number of documents and
sentences have to be reduced. We defined a set of
constraints for a sentence set: (i) from one set, (ii)
a sentence set should consist of 150 – 200 sen-
tences2. To obtain sentence sets that comply with
these requirements we designed an algorithm that
takes the number of documents in a DUC set, the
date of publishing, the number of documents pub-
lished on the same day and the number of sen-
tences in a document into account. If a document
set includes articles published on the same day
they were given preference. Furthermore shorter
documents (in terms of number of sentences) were
favoured. The properties of the resulting sentence
sets are listed in table 1. The documents in a set
were ordered by date and split into sentences us-
ing the sentence boundary detector from RASP
(Briscoe et al., 2006).
</bodyText>
<table confidence="0.995602">
name DUC DUC id docs sen
Volcano 2002 D073b 5 162
Rushdie 2007 D0712C 15 103
EgyptAir 2006 D0617H 9 191
Schulz 2003 d102a 5 248
</table>
<tableCaption confidence="0.999798">
Table 1: Properties of sentence sets
</tableCaption>
<sectionHeader confidence="0.541328" genericHeader="method">
4 Creation of the Gold Standard
</sectionHeader>
<bodyText confidence="0.999677583333333">
Each sentence set was manually clustered by at
least three judges. In total there were six judges
which were all volunteers. They are all second-
language speakers of English and hold at least a
Master’s degree. Three of them (Judge A, Judge J
and Judge O) have a background in computational
linguistics. The judges were given a task descrip-
tion and a list of guidelines. They were only using
the guidelines given and worked independently.
They did not confer with each other or the author.
Table 2 gives details about the set of clusters each
judge created.
</bodyText>
<subsectionHeader confidence="0.989123">
4.1 Guidelines
</subsectionHeader>
<bodyText confidence="0.997583">
The following guidelines were given to the judges:
</bodyText>
<listItem confidence="0.568057">
1. Each cluster should contain only one topic.
2. In an ideal cluster the sentences are very similar.
</listItem>
<footnote confidence="0.9953138">
2If a DUC set contains only 5 documents all of them are
used to create the sentence set, even if that results in more
than 200 sentences. If the DUC set contains more than 15
documents, only 15 documents are used for clustering even if
the number of 150 sentences is not reached.
</footnote>
<page confidence="0.999237">
97
</page>
<table confidence="0.995776625">
judge Rushdie Volcano s EgyptAir s Schulz
s c s/c s c s/c c s/c c s/c
Judge A 70 15 4.6 92 30 3 85 28 3 54 16 3.4
Judge B 41 10 4.1 57 21 2.7 44 15 2.9 38 11 3.5
Judge D 46 16 2.9
Judge H 74 14 5.3 75 19 3.9
Judge J 120 7 17.1
Judge O 53 20 2.6
</table>
<tableCaption confidence="0.9158455">
Table 2: Details of manual clusterings: s number of sentences in a set, c number of clusters, s/c average
number of sentences in a cluster
</tableCaption>
<listItem confidence="0.949836166666666">
3. The information in one cluster should come from
as many different documents as possible. The
more different sources the better. Clusters of sen-
tences from only one document are not allowed.
4. There must be at least two sentences in a cluster,
and more than two if possible.
5. Differences in numbers in the same cluster are
allowed (e.g. vagueness in numbers (300,000 -
350,000), update (two killed - four dead))
6. Break off very similar sentences from one cluster
into their own subcluster, if you feel the cluster is
not homogeneous.
7. Do not use too much inference.
8. Partial overlap – If a sentence has parts that fit in
two clusters, put the sentence in the more impor-
tant cluster.
9. Generalisation is allowed, as long as the sen-
tences are about the same person, fact or event.
</listItem>
<bodyText confidence="0.92856628125">
The guidelines were designed by the author and
her supervisor – Dr Simone Teufel. The starting
point was a single DUC document set which was
clustered by the author and her supervisor with the
task in mind to find clusters of sentences that rep-
resent the main topics in the documents. The mini-
mal constraint was that each cluster is specific and
general enough to be described in one sentence
(see rule 1 and 2). By looking at the differences
between the two manual clustering and reviewing
the reasons for the differences the other rules were
generated and tested on another sentence set.
One rule that emerged early says that a topic can
only be included in the summary of a document
set if it appears in more than one document (rule
3). From our understanding of MDS and our defi-
nition of importance only sentences that depict a
topic which is present in more than one source
document can be summary worthy. From this
it follows that clusters must contain at least two
sentences which come from different documents.
Sentences that are not in any cluster of at least two
are considered irrelevant for the MDS task (rule
4). We defined a spectrum of similarity. In an ideal
cluster the sentences would be very similar, almost
paraphrases. For our task sentences that are not
paraphrases can be in the same cluster (see rule 5,
8, 9). In general there are several constraints that
pull against each other. The judges have to find the
best compromise.
We also gave the judges a recommended proce-
dure:
</bodyText>
<listItem confidence="0.969110739130434">
1. Read all documents. Start clustering from the
first sentence in the list. Put every sentence that
you think will attract other sentences into an initial
cluster. If you feel, that you will not find any similar
sentences to a sentence, put it immediately aside.
Continue clustering and build up the clusters while
you go through the list of sentences.
2. You can rearrange your clusters at any point.
3. When you are finished with clustering check that
all important information from the documents is
covered by your clusters. If you feel that a very
important topic is not expressed in your clusters,
look for evidence for that information in the text,
even in secondary parts of a sentence.
4. Go through your sentences which do not belong
to any cluster and check if you can find a suitable
cluster.
5. Do a quality check and make sure that you wrote
down a sentence for each cluster and that the sen-
tences in a cluster are from more than one docu-
ment.
6. Rank the clusters by importance.
4.2 Differences in manual clusterings
</listItem>
<bodyText confidence="0.999763">
Each judge clustered the sentence sets differently.
No two judges came up with the same separation
into clusters or the same amount of irrelevant sen-
tences. When analysing the differences between
the judges we found three main categories:
Generalisation One judge creates a cluster that
from his point of view is homogeneous:
</bodyText>
<listItem confidence="0.933238666666667">
1. Since then, the Rushdie issue has turned into a
big controversial problem that hinders the rela-
tions between Iran and European countries.
2. The Rushdie affair has been the main hurdle in
Iran’s efforts to improve ties with the European
Union.
</listItem>
<page confidence="0.906923">
98
</page>
<listItem confidence="0.959069">
3. In a statement issued here, the EU said the Iranian
decision opens the way for closer cooperation be-
tween Europe and the Tehran government.
4. “These assurances should make possible a much
more constructive relationship between the United
Kingdom, and I believe the European Union, with
Iran, and the opening of a new chapter in our re-
lations,” Cook said after the meeting.
</listItem>
<bodyText confidence="0.999886230769231">
Another judge however puts these sentences into
two separate cluster (1,2) and (3,4).The first judge
chooses a more general approach and created a
cluster about the relationship between Iran and
the EU, whereas the other judge distinguishes be-
tween the improvement of the relationship and the
reason for the problems in the relationship.
Emphasise Two judges can emphasise on differ-
ent parts of a sentence. For example the sentence
”All 217 people aboard the Boeing 767-300 died when it
plunged into the Atlantic off the Massachusetts coast on
Oct. 31, about 30 minutes out of New York’s Kennedy
Airport on a night flight to Cairo.” was clustered to-
gether with other sentence about the number of ca-
sualties by one judge. Another judge emphasised
on the course of events and put it into a different
cluster.
Inference Humans use different level of inter-
ference. One judge clustered the sentence ”Schulz,
who hated to travel, said he would have been happy liv-
ing his whole life in Minneapolis.” together with other
sentences which said that Schulz is from Min-
nesota although this sentence does not clearly state
this. This judge interfered from ”he would have been
happy living his whole life in Minneapolis” that he actu-
ally is from Minnesota.
</bodyText>
<sectionHeader confidence="0.986847" genericHeader="method">
5 Evaluation measures
</sectionHeader>
<bodyText confidence="0.852723964285714">
The evaluation measures will compare a set of
clusters to a set of classes. An ideal evaluation
measure should reward a set of clusters if the clus-
ters are pure or homogeneous, so that it only con-
tains sentences from one class. On the other hand
it should also reward the set if all/most of the sen-
tences of a class are in one cluster (completeness).
If sentences that in the gold standard make up one
class are grouped into two clusters, the measure
should penalise the clustering less than if a lot of
irrelevant sentences were in the same cluster. Ho-
mogeneity is more important to us.
D is a set of N sentences da so that D = {da|a =
1, ..., N}. A set of clusters L = {lj|j = 1, ..., |L|}
is a partition of a data set D into disjoint subsets
called clusters, so that lj n lm = 0. |L |is the num-
ber of clusters in L. A set of clusters that contains
only one cluster with all the sentences of D will be
called Lone. A cluster that contains only one ob-
ject is called a singleton and a set of clusters that
only consists of singletons is called Lsingle.
A set of classes C = {ci|i = 1, ..., |C|} is a par-
tition of a data set D into disjoint subsets called
classes, so that ci n cm = 0. |C |is the number of
classes in C. C is also called a gold standard of a
clustering of data set D because this set contains
the ”ideal” solution to a clustering task and other
clusterings are compared to it.
</bodyText>
<subsectionHeader confidence="0.975897">
5.1 V-measure and Vbeta
</subsectionHeader>
<bodyText confidence="0.996698">
The V-measure (Rosenberg and Hirschberg, 2007)
is an external evaluation measure based on condi-
tional entropy:
</bodyText>
<equation confidence="0.9993195">
V (L, C) = (1 + β)hc (1)
βh + c
</equation>
<bodyText confidence="0.9974744">
It measures homogeneity (h) and completeness (c)
of a clustering solution (see equation 2 where ni j
is the number of sentences lj and ci share, ni the
number of sentences in ci and nj the number of
sentences in lj)
</bodyText>
<equation confidence="0.9999135">
H(L) = −
H(L|C) = −
</equation>
<bodyText confidence="0.9999387">
A cluster set is homogeneous if only objects from
a single class are assigned to a single cluster. By
calculating the conditional entropy of the class dis-
tribution given the proposed clustering it can be
measured how close the clustering is to complete
homogeneity which would result in zero entropy.
Because conditional entropy is constrained by the
size of the data set and the distribution of the class
sizes it is normalized by H(C) (see equation 2).
Completeness on the other hand is achieved if all
</bodyText>
<equation confidence="0.997690409090909">
H(C|L)
h = 1 − H(C) c= 1 −
H(L|C)
H(L)
H(C|L) = − X |L |X |C |ni N j log nij
j=1 i=1 nj
H(C) = − X |C |ni ni (2)
i=1 N logN
nj nj
N logN
ni
N log
j
|L|
X
j=1
ni j
ni
X |C|
i=1
X |L|
j=1
</equation>
<page confidence="0.955011">
99
</page>
<bodyText confidence="0.997984529411765">
data points from a single class are assigned to a
single cluster which results in H(L|C) = 0.
The V-measure can be weighted. If Q &gt; 1
the completeness is favoured over homogeneity
whereas the weight of homogeneity is increased
if Q &lt; 1.
Vlachos et al. (2009) proposes Vbeta where Q is set
to IC
. This way the shortcoming of the V-measure
to favour cluster sets with many more clusters than
classes can be avoided. If |L |&gt; |C |the weight
of homogeneity is reduced, since clusterings with
large |L |can reach high homogeneity quite eas-
ily, whereas |C |&gt; |L |decreases the weight of
completeness. V-measure and Vbeta can range be-
tween 0 and 1, they reach 1 if the set of clusters is
identical to the set of classes.
</bodyText>
<subsectionHeader confidence="0.992964">
5.2 Normalized Mutual Information
</subsectionHeader>
<bodyText confidence="0.999107666666667">
Mutual Information (I) measures the information
that C and L share and can be expressed by using
entropy and conditional entropy:
</bodyText>
<equation confidence="0.992612">
I = H(C) + H(L) − H(C, L) (3)
</equation>
<bodyText confidence="0.8442565">
There are different ways to normalise I. Manning
et al. (2008) uses
</bodyText>
<equation confidence="0.990209">
= 2 I(L, C)
H(L) + H(C) (4)
</equation>
<bodyText confidence="0.9250874">
which represents the average of the two uncer-
tainty coefficients as described in Press et al.
(1988).
Generalise NMI to NMIβ = (1+β)I
βH(L)+H(C). Then
</bodyText>
<equation confidence="0.98222045">
NMIβ is actually the same as Vβ:
H(C|L)
h = 1 −
H(C)
⇒H(C)h = H(C) − H(C|L)
= H(C) − H(C, L) + H(L) = I
H(L|C)
c = 1 − H(L)
⇒H(L)c = H(L) − H(L|C)
= H(L) − H(L, C) + H(C) = I
V = (1 + β)hc βh + c
(1 + β)H(L)H(C)hc
βH(L)H(C)h + H(L)H(C)c
H(C)h and H(L)c are substituted by I:
(1 + β)I2
βH(L)I + H(C)I
= (1 + β)I
βH(L) + H(C)
V1 = 2 H(L) + H(C)
I = NMI
</equation>
<subsectionHeader confidence="0.9098835">
5.3 Variation of Information (V I) and
Normalized V I
</subsectionHeader>
<bodyText confidence="0.999650666666667">
The VI-measure (Meila, 2007) also measures
completeness and homogeneity using conditional
entropy. It measure the distance between two
clusterings and thereby the amount of information
gained in changing from C to L. For this measure
the conditional entropies are added up:
</bodyText>
<equation confidence="0.998073">
V I(L, C) = H(C|L) + H(L|C) (7)
</equation>
<bodyText confidence="0.552234428571429">
Remember small conditional entropies mean that
the clustering is near to complete homogene-
ity/ completeness, so the smaller V I the better
(V I = 0 if L = C). The maximum of V I is
log N e.g. for V I(Lsingle, Cone). V I can be nor-
malized, then it can range from 0 (identical clus-
ters) to 1.
</bodyText>
<equation confidence="0.9983175">
1
NVI(L, C) = log NV I(L, C) (8)
</equation>
<bodyText confidence="0.999817833333333">
V-measure, Vbeta and V I measure both com-
pleteness and homogeneity, no mapping between
classes and clusters is needed (Rosenberg and
Hirschberg, 2007) and they are only dependent
on the relative size of the clusters (Vlachos et al.,
2009).
</bodyText>
<subsectionHeader confidence="0.975758">
5.4 Rand Index (RI)
</subsectionHeader>
<bodyText confidence="0.999789333333333">
The Rand Index (Rand, 1971) compares two clus-
terings with a combinatorial approach. Each pair
of objects can fall into one of four categories:
</bodyText>
<listItem confidence="0.98584325">
• TP (true positives) = objects belong to one
class and one cluster
• FP (false positives) = objects belong to dif-
ferent classes but to the same cluster
• FN (false negatives) = objects belong to the
same class but to different clusters
• TN (true negatives) = objects belong to dif-
ferent classes and to different cluster
</listItem>
<bodyText confidence="0.992045">
By dividing the total number of correctly clustered
pairs by the number of all pairs, RI gives the per-
centage of correct decisions.
</bodyText>
<equation confidence="0.999452333333333">
TP + TN
RI = (9)
TP + FP + TN + FN
</equation>
<bodyText confidence="0.987338833333333">
RI can range between 0 and 1 where 1 corresponds
to identical clusterings. Meila (2007) mentions
that in practise RI concentrates in a small interval
near 1 (for more detail see section 5.7). Another
shortcoming is that RI gives equal weight to FPs
and FNs.
</bodyText>
<equation confidence="0.959827833333333">
I(L, C)
NMI =
H(L)+H(C)
2
=
= NMIβ
</equation>
<page confidence="0.958434">
100
</page>
<subsectionHeader confidence="0.717207">
5.5 Entropy and Purity
</subsectionHeader>
<bodyText confidence="0.999671272727273">
Entropy and Purity are widely used evaluation
measures (Zhao and Karypis, 2001). They both
can be used to measure homogeneity of a cluster.
Both measures give better values when the num-
ber of clusters increase, with the best result for
Lsingle. Entropy ranges from 0 for identical clus-
terings or Lsingle to log N e.g. for Csingle and
Lone. The values of P can range between 0 and 1,
where a value close to 0 represents a bad cluster-
ing solution and a perfect clustering solution gets
a value of 1.
</bodyText>
<equation confidence="0.8162345">
(10)
5.6 F-measure
</equation>
<bodyText confidence="0.9981588">
The F-measure is a well known metric from IR,
which is based on Recall and Precision. The ver-
sion of the F-score (Hess and Kushmerick, 2003)
described here measures the overall Precision and
Recall. This way a mapping between a cluster and
a class is omitted which may cause problems if |L|
is considerably different to |C |or if a cluster could
be mapped to more than one class. Precision and
Recall here are based on pairs of objects and not
on individual objects.
</bodyText>
<equation confidence="0.95825275">
= TP = TP
P TP + FP R TP + FN (11)
2PR
F(L, C) =
</equation>
<subsectionHeader confidence="0.968013">
5.7 Discussion of the Evaluation measures
</subsectionHeader>
<bodyText confidence="0.9998962">
We used one cluster set to analyse the behaviour
and quality of the evaluation measures. Variations
of that cluster set were created by randomly split-
ting and merging the clusters. These modified sets
were then compared to the original set. This ex-
periment will help to identify the advantages and
disadvantages of the measures, what the values re-
veal about the quality of a set of clusters and how
the measures react to changes in the cluster set.
We used the set of clusters created by Judge A for
the Rushdie sentence set. It contains 70 sentences
in 15 clusters. This cluster set was modified by
splitting and merging the clusters randomly until
we got Lsingle with 70 clusters and Lone with one
cluster. The original set of clusters (CA) was com-
pared to the modified versions of the set (see figure
1). The evaluation measures reach their best val-
ues if CA = 15 clusters is compared to itself.
The F-measure is very sensitive to changes. It
is the only measure which uses its full measure-
ment range. F = 0 if CA is compared to
LA−single, which means that the F-measure con-
siders LA−single to be the opposite of CA. Usually
Lone and LA−single are considered to be observe
and a measure should only reach its worst possible
value if these sets are compared. In other words
the F-measure might be too sensitive for our task.
The RI stays most of the time in an interval be-
tween 0.84 and 1. Even for the comparison be-
tween CA and LA−single the RI is 0.91. This be-
haviour was also described in Meila (2007) who
observed that the RI concentrates in a small inter-
val near 1.
As described in section 5.5 Purity and Entropy
both measure homogeneity. They both react to
changes slowly. Splitting and merging have al-
most the same effect on Purity. It reaches Pz� 0.6
when the clusters of the set were randomly split or
merged four times. As explained above our ideal
evaluation measure should punish a set of clusters
which puts sentences of the same class into two
clusters less than if sentences are merged with ir-
relevant ones. Homogeneity decreases if unrelated
clusters are merged whereas a decline in complete-
ness follows from splitting clusters. In other words
for our task a measure should decrease more if two
clusters are merged than if a cluster is split.
Entropy for example is more sensitive to merg-
ing than splitting. But Entropy only measures ho-
mogeneity and an ideal evaluation measure should
also consider completeness.
The remaining measures Vbeta, V0.5 and NV I/V I
all fulfil our criteria of a good evaluation measure.
All of them are more affected by merging than by
splitting and use their measuring range appropri-
ately. V0.5 favours homogeneity over complete-
ness, but it reacts to changes less than Vbeta. The
V-measure can also be inaccurate if the |L |is con-
siderably different to |C|. Vbeta (Vlachos et al.,
2009) tries to overcome this problem and the ten-
dency of the V-measure to favour clusterings with
a large number of clusters.
Since V I is measured in bits with an upper bound
of log N, values for different sets are difficult to
compare. NVI tries to overcome this problem by
</bodyText>
<equation confidence="0.515089083333333">
Entropy = L |L |⎛ 1 L |C |nij nij ⎞
j=1 nj ⎝N i=1 log ⎠
nj
log |C |nj
1 |L|
Purity = N L
j=1
�ni �
j
Max
i
P + R
</equation>
<page confidence="0.704611">
101
</page>
<figure confidence="0.9987">
1
0.8
evaluation measures 0.6
0.4
0.2
0 0
1 2 4 8 15 30 48 61 70
</figure>
<figureCaption confidence="0.9980415">
Figure 1: Behaviour of evaluation measure when randomly changed sets of clusters are compared to the
original set.
</figureCaption>
<figure confidence="0.992446071428572">
RI
F
VI
NVI
E
Pure
Vbeta
V 0.5
number of clusters
5 VI measure
4
3
2
1
</figure>
<bodyText confidence="0.9782992">
normalising V I by dividing it by log N. As Meila
(2007) pointed out, this is only convenient if the
comparison is limited to one data set.
In this paper Vbeta, V0.5 and NVI will be used for
evaluation purposes.
</bodyText>
<sectionHeader confidence="0.700593" genericHeader="method">
6 Comparability of Clusterings
</sectionHeader>
<bodyText confidence="0.999943615384615">
Following our procedure and guidelines the judges
have to filter out all irrelevant sentences that are
not related to another sentence from a different
document. The number of these irrelevant sen-
tences are different for every sentence set and ev-
ery judge (see table 2). The evaluation measures
require the same number of sentences in each set
of clusters to compare them. The easiest way to
ensure that each cluster set for a sentence set has
the same number of sentences is to add the sen-
tences that were filtered out by the judges to the
corresponding set of clusters. There are different
ways to add these sentences:
</bodyText>
<listItem confidence="0.978993333333333">
1. singletons: Each irrelevant sentence is added
to set of clusters as a cluster of its own
2. bucket cluster: All irrelevant sentences are
</listItem>
<bodyText confidence="0.9678666">
put into one cluster which is added to the set
of clusters.
Adding each irrelevant sentence as a singleton
seems to be the most intuitive way to handle the
problem with the sentences that were filtered out.
However this approach has some disadvantages.
The judges will be rewarded disproportionately
high for any singleton they agreement on. Thereby
the disagreement on the more important clustering
will be less punished. With every singleton the
judges agree on the completeness and homogene-
ity of the whole set of clusters increases.
On the other hand the sentences in a bucket cluster
are not all semantically related to each other and
the cluster is not homogeneous which is contradic-
tory to our definition of a cluster. Since the irrel-
evant sentences are combined to only one cluster,
the judges will not be rewarded disproportionately
high for their agreement. However two bucket
clusters from two different sets of clusters will
never be exactly the same and therefore the judges
will be punished more for the disagreement on the
irrelevant sentences
We have to considers these factors when we in-
terpret the results of the inter-judge agreement.
</bodyText>
<sectionHeader confidence="0.999544" genericHeader="method">
7 Inter-Judge Agreement
</sectionHeader>
<bodyText confidence="0.999992666666667">
We added the irrelevant sentences to each set of
clusters created by the judges as described in sec-
tion 6. These modified sets were then compared to
each other in order to evaluate the agreement be-
tween the judges. The results are shown in table 3.
For each sentence set 100 random sets of clusters
were created and compared to the modified sets (in
total 1300 comparisons for each method of adding
irrelevant sentences). The average values of these
</bodyText>
<page confidence="0.991345">
102
</page>
<table confidence="0.999639611111111">
set judges singleton clusters bucket cluster
Vbeta V0.5 NVI Vbeta V0.5 NVI
Volcano A-B 0.92 0.93 0.13 0.52 0.54 0.39
A-D 0.92 0.93 0.13 0.44 0.49 0.4
B-D 0.95 0.95 0.08 0.48 0.48 0.31
Rushdie A-B 0.87 0.88 0.19 0.3 0.31 0.59
A-H 0.86 0.86 0.2 0.69 0.69 0.32
B-H 0.85 0.87 0.2 0.25 0.27 0.64
EgyptAir A-B 0.94 0.95 0.1 0.41 0.45 0.34
A-H 0.93 0.93 0.12 0.57 0.58 0.31
A-O 0.94 0.94 0.11 0.44 0.46 0.36
B-H 0.93 0.94 0.11 0.44 0.46 0.3
B-O 0.96 0.96 0.08 0.42 0.43 0.28
H-O 0.93 0.94 0.12 0.44 0.44 0.34
Schulz A-B 0.98 0.98 0.04 0.54 0.56 0.15
A-J 0.89 0.9 0.17 0.39 0.4 0.34
B-J 0.89 0.9 0.18 0.28 0.31 0.35
base 0.66 0.75 0.44 0.29 0.28 0.68
</table>
<tableCaption confidence="0.999909">
Table 3: Inter-judge agreement for the four sentence set.
</tableCaption>
<bodyText confidence="0.991194592592593">
comparisons are used as a baseline.
The inter-judge agreement is most of the time
higher than the baseline. Only for the Rushdie
sentence set the agreement between Judge B and
Judge H is lower for Vbeta and V0.5 if the bucket
cluster method is used.
As explained in section 6 the two methods for
adding sentences that were filtered out by the
judges have a notable influence on the values of
the evaluation measures. When adding single-
tons to the set of clusters the inter-judge agree-
ment is considerably higher than with the bucket
cluster method. For example the agreement be-
tween Judge A and Judge B is 0.98 for Vbeta and
V0.5 and 0.04 for NVI when singletons are added.
Here the judges filter out the same 185 sentences
which is equivalent to 74.6% of all sentences in
the set. In other words 185 clusters are already
considered to be homogen and complete, which
gives the comparison a high score. Five of the 15
clusters Judge A created contain only sentences
there were marked as irrelevant by Judge B. In to-
tal 25 sentences are used in clusters by Judge A
which are singletons in Judge B’s set. Judge B in-
cluded nine other sentences that are singletons in
the set of Judge A. Four of the clusters are exactly
the same in both sets, they contain 16 sentences.
To get from Judge A’s set to the set of Judge B
37 sentences would have to be deleted, added or
moved.
With the bucket cluster method Judge A and
Judge H for the Rushdie sentence set have the best
inter-judge agreement. At the same time this com-
bination receives the worst V0.5 and NVI val-
ues with the singleton method. The two judges
agree on 22 irrelevant sentences, which account
for 21.35% of all sentences. Here the singletons
have far less influence on the evaluation measures
then the first example. Judge A includes 7 sen-
tences that are filtered out by Judge H who uses
another 11 sentences. Only one cluster is exactly
the same in both sets. To get from Judge A’s set to
Judge H’s cluster 11 sentences have to be deleted,
7 to be added, one cluster has to be split in two and
11 sentences have to be moved from one cluster to
another.
Although the two methods of adding irrelevant
sentences to the sets of cluster result in differ-
ent values for the inter-judge agreement, we can
conclude that the agreement between the judges
is good and (almost) always exceed the baseline.
Overall Judge B seems to have the highest agree-
ment throughout all sentence sets with all other
judges.
</bodyText>
<sectionHeader confidence="0.989846" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999299181818182">
In this paper we presented a gold standard for sen-
tence clustering for Multi-Document Summariza-
tion. The data set used, the guidelines and pro-
cedure given to the judges were discussed. We
showed that the agreement between the judges in
sentence clustering is good and exceeds the base-
line. This gold standard will be used for further ex-
periments on clustering for Multi-Document Sum-
marization. The next step will be to compared the
output of a standard clustering algorithm to the
gold standard.
</bodyText>
<page confidence="0.999164">
103
</page>
<sectionHeader confidence="0.993884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918741176471">
Ramiz M. Aliguliyev. 2006. A novel partitioning-
based clustering method and generic document sum-
marization. In WI-IATW ’06: Proceedings of the
2006 IEEE/WIC/ACM international conference on
Web Intelligence and Intelligent Agent Technology,
Washington, DC, USA.
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence Fusion for Multidocument News Sum-
mariation. Computational Linguistics, 31(3):297–
327.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The Second Release of the RASP System. In COL-
ING/ACL 2006 Interactive Presentation Sessions,
Sydney, Australien. The Association for Computer
Linguistics.
Vasileios Hatzivassiloglou, Judith L. Klavans,
Melissa L. Holcombe, Regina Barzilay, Min-
Yen Kan, and Kathleen R. McKeown. 2001.
SIMFINDER: A Flexible Clustering Tool for
Summarization. In NAACL Workshop on Automatic
Summarization, pages 41–49. Association for
Computational Linguistics.
Andreas Hess and Nicholas Kushmerick. 2003. Au-
tomatically attaching semantic metadata to web ser-
vices. In Proceedings of the 2nd International Se-
mantic Web Conference (ISWC 2003), Florida, USA.
Christopher D. Manning, Prabhakar Raghavan, and
Heinrich Sch¨utze. 2008. Introduction to Informa-
tion Retrieval. Cambridge University Press.
Daniel Marcu and Laurie Gerber. 2001. An inquiry
into the nature of multidocument abstracts, extracts,
and their evaluation. In Proceedings of the NAACL-
2001 Workshop on Automatic Summarization, Pitts-
burgh, PA.
Marina Meila. 2007. Comparing clusterings–an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873–895.
Martina Naughton. 2007. Exploiting structure for
event discovery using the mdi algorithm. In Pro-
ceedings of the ACL 2007 Student Research Work-
shop, pages 31–36, Prague, Czech Republic, June.
Association for Computational Linguistics.
William H. Press, Brian P. Flannery, Saul A. Teukol-
sky, and William T. Vetterling. 1988. Numerical
Recipies in C: The art of Scientific Programming.
Cambridge University Press, Cambridge, England.
Dragomir R. Radev, Hongyan Jing, and Malgorzata
Budzikowska. 2000. Centroid-based summariza-
tion of multiple documents: sentence extraction,
utility-based evaluation, and user studies. In In
ANLP/NAACL Workshop on Summarization, pages
21–29, Morristown, NJ, USA. Association for Com-
putational Linguistics.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. American Statistical
Association Journal, 66(336):846–850.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 410–
420.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and Constrained
Dirichlet Process Mixture Models for Verb Cluster-
ing. In Proceedings of the EACL workshop on GEo-
metrical Models of Natural Language Semantics.
Dingding Wang, Tao Li, Shenghuo Zhu, and Chris
Ding. 2008. Multi-document summarization via
sentence-level semantic analysis and symmetric ma-
trix factorization. In SIGIR ’08: Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 307–314, New York, NY, USA. ACM.
Hongyuan Zha. 2002. Generic Summarization and
Keyphrase Extraction using Mutual Reinforcement
Principle and Sentence Clustering. In Proceedings
of the 25th Annual ACM SIGIR Conference, pages
113–120, Tampere, Finland.
Ying Zhao and George Karypis. 2001. Criterion
functions for document clustering: Experiments and
analysis. Technical report, Department of Computer
Science, University of Minnesota. (Technical Re-
port #01-40).
</reference>
<page confidence="0.998784">
104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.973599">
<title confidence="0.9991405">Creating a Gold Standard for Sentence Clustering in Multi-Document Summarization</title>
<author confidence="0.999707">Johanna Geiss</author>
<affiliation confidence="0.9999785">University of Cambridge Computer Laboratory</affiliation>
<address confidence="0.992712">15 JJ Thomson Avenue Cambridge, CB3 0FD, UK</address>
<email confidence="0.998361">johanna.geiss@cl.cam.ac.uk</email>
<abstract confidence="0.999338846153846">Sentence Clustering is often used as a first step in Multi-Document Summarization (MDS) to find redundant information. All the same there is no gold standard available. This paper describes the creation of a gold standard for sentence clustering from DUC document sets. The procedure of building the gold standard and the guidelines which were given to six human judges are described. The most widely used and promising evaluation measures are presented and discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ramiz M Aliguliyev</author>
</authors>
<title>A novel partitioningbased clustering method and generic document summarization.</title>
<date>2006</date>
<booktitle>In WI-IATW ’06: Proceedings of the 2006 IEEE/WIC/ACM international conference on Web Intelligence and Intelligent Agent Technology,</booktitle>
<location>Washington, DC, USA.</location>
<contexts>
<context position="1883" citStr="Aliguliyev, 2006" startWordPosition="295" endWordPosition="296">luded in a summary only once in order to avoid a repetitive summary. Sentence clustering has therefore often been used as an early step in MDS (Hatzivassiloglou et al., 2001; Marcu and Gerber, 2001; Radev et al., 2000). In sentence clustering semantically similar sentences are grouped together. Sentences within a cluster overlap in information, but they do not have to be identical in meaning. In contrast to paraphrases sentences in a cluster do not have to cover the same amount of information. One sentence represents one cluster in the summary. Either a sentences from the cluster is selected (Aliguliyev, 2006) or a new sentence is regenerated from all/some sentences in a cluster (Barzilay and McKeown, 2005). Usually the quality of the sentence clusters are only evaluated indirectly by judging the quality of the generated summary. There is still no standard evaluation method for summarization and no consensus in the summarization community how to evaluate a summary. The methods at hand are either superficial or time and resource consuming and not easily repeatable. Another argument against indirect evaluation of clustering is that troubleshooting becomes more difficult. If a poor summary was created</context>
</contexts>
<marker>Aliguliyev, 2006</marker>
<rawString>Ramiz M. Aliguliyev. 2006. A novel partitioningbased clustering method and generic document summarization. In WI-IATW ’06: Proceedings of the 2006 IEEE/WIC/ACM international conference on Web Intelligence and Intelligent Agent Technology, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence Fusion for Multidocument News Summariation.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<pages>327</pages>
<contexts>
<context position="1982" citStr="Barzilay and McKeown, 2005" startWordPosition="309" endWordPosition="312">has therefore often been used as an early step in MDS (Hatzivassiloglou et al., 2001; Marcu and Gerber, 2001; Radev et al., 2000). In sentence clustering semantically similar sentences are grouped together. Sentences within a cluster overlap in information, but they do not have to be identical in meaning. In contrast to paraphrases sentences in a cluster do not have to cover the same amount of information. One sentence represents one cluster in the summary. Either a sentences from the cluster is selected (Aliguliyev, 2006) or a new sentence is regenerated from all/some sentences in a cluster (Barzilay and McKeown, 2005). Usually the quality of the sentence clusters are only evaluated indirectly by judging the quality of the generated summary. There is still no standard evaluation method for summarization and no consensus in the summarization community how to evaluate a summary. The methods at hand are either superficial or time and resource consuming and not easily repeatable. Another argument against indirect evaluation of clustering is that troubleshooting becomes more difficult. If a poor summary was created it is not clear which component e.g. information extraction through clustering or summary generati</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence Fusion for Multidocument News Summariation. Computational Linguistics, 31(3):297– 327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The Second Release of the RASP System. In</title>
<date>2006</date>
<booktitle>COLING/ACL</booktitle>
<contexts>
<context position="7107" citStr="Briscoe et al., 2006" startWordPosition="1178" endWordPosition="1181">sets that comply with these requirements we designed an algorithm that takes the number of documents in a DUC set, the date of publishing, the number of documents published on the same day and the number of sentences in a document into account. If a document set includes articles published on the same day they were given preference. Furthermore shorter documents (in terms of number of sentences) were favoured. The properties of the resulting sentence sets are listed in table 1. The documents in a set were ordered by date and split into sentences using the sentence boundary detector from RASP (Briscoe et al., 2006). name DUC DUC id docs sen Volcano 2002 D073b 5 162 Rushdie 2007 D0712C 15 103 EgyptAir 2006 D0617H 9 191 Schulz 2003 d102a 5 248 Table 1: Properties of sentence sets 4 Creation of the Gold Standard Each sentence set was manually clustered by at least three judges. In total there were six judges which were all volunteers. They are all secondlanguage speakers of English and hold at least a Master’s degree. Three of them (Judge A, Judge J and Judge O) have a background in computational linguistics. The judges were given a task description and a list of guidelines. They were only using the guidel</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The Second Release of the RASP System. In COLING/ACL 2006 Interactive Presentation Sessions, Sydney, Australien. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Melissa L Holcombe</author>
<author>Regina Barzilay</author>
<author>MinYen Kan</author>
<author>Kathleen R McKeown</author>
</authors>
<title>SIMFINDER: A Flexible Clustering Tool for Summarization.</title>
<date>2001</date>
<booktitle>In NAACL Workshop on Automatic Summarization,</booktitle>
<pages>41--49</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1439" citStr="Hatzivassiloglou et al., 2001" startWordPosition="219" endWordPosition="223">g amount of redundant information. Different newswires publish different reports about the same event resulting in information overlap. Multi-Document Summarization (MDS) can help to reduce the amount of documents a user has to read to keep informed. In contrast to single document summarization information overlap is one of the biggest challenges to MDS systems. While repeated information is a good evidence of importance, this information should be included in a summary only once in order to avoid a repetitive summary. Sentence clustering has therefore often been used as an early step in MDS (Hatzivassiloglou et al., 2001; Marcu and Gerber, 2001; Radev et al., 2000). In sentence clustering semantically similar sentences are grouped together. Sentences within a cluster overlap in information, but they do not have to be identical in meaning. In contrast to paraphrases sentences in a cluster do not have to cover the same amount of information. One sentence represents one cluster in the summary. Either a sentences from the cluster is selected (Aliguliyev, 2006) or a new sentence is regenerated from all/some sentences in a cluster (Barzilay and McKeown, 2005). Usually the quality of the sentence clusters are only e</context>
<context position="4564" citStr="Hatzivassiloglou et al. (2001)" startWordPosition="741" endWordPosition="745">tion structure of web pages and news articles. In this gold standard the section numbers are assumed to give the true cluster label for a sentence. In this approach only sentences within the same document and even within the same paragraph are clustered together whereas our approach is to find similar information between documents. A gold standard for event identification was built by Naughton (2007). Ten annotators tagged events in a sentence. Each sentence could be assigned more than one event number. In our approach a sentence can only belong to one cluster. For the evaluation of SIMFINDER Hatzivassiloglou et al. (2001) created a set of 10.535 manually marked pairs of paragraphs. Two human annotator were asked to judge if the paragraphs contained ’common information’. They were given the guideline that only paragraphs that described the same object in the same way or in which the same object was acting the same are to be considered similar. They found significant disagreement between the judges but the annotators were able to resolve their differences. Here the problem is that only pairs of paragraphs are annotated whereas we focus on whole sentences and create not pairs but clusters of similar sentences. 3 </context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Holcombe, Barzilay, Kan, McKeown, 2001</marker>
<rawString>Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa L. Holcombe, Regina Barzilay, MinYen Kan, and Kathleen R. McKeown. 2001. SIMFINDER: A Flexible Clustering Tool for Summarization. In NAACL Workshop on Automatic Summarization, pages 41–49. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Hess</author>
<author>Nicholas Kushmerick</author>
</authors>
<title>Automatically attaching semantic metadata to web services.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd International Semantic Web Conference (ISWC</booktitle>
<location>Florida, USA.</location>
<contexts>
<context position="20789" citStr="Hess and Kushmerick, 2003" startWordPosition="3722" endWordPosition="3725">sed evaluation measures (Zhao and Karypis, 2001). They both can be used to measure homogeneity of a cluster. Both measures give better values when the number of clusters increase, with the best result for Lsingle. Entropy ranges from 0 for identical clusterings or Lsingle to log N e.g. for Csingle and Lone. The values of P can range between 0 and 1, where a value close to 0 represents a bad clustering solution and a perfect clustering solution gets a value of 1. (10) 5.6 F-measure The F-measure is a well known metric from IR, which is based on Recall and Precision. The version of the F-score (Hess and Kushmerick, 2003) described here measures the overall Precision and Recall. This way a mapping between a cluster and a class is omitted which may cause problems if |L| is considerably different to |C |or if a cluster could be mapped to more than one class. Precision and Recall here are based on pairs of objects and not on individual objects. = TP = TP P TP + FP R TP + FN (11) 2PR F(L, C) = 5.7 Discussion of the Evaluation measures We used one cluster set to analyse the behaviour and quality of the evaluation measures. Variations of that cluster set were created by randomly splitting and merging the clusters. T</context>
</contexts>
<marker>Hess, Kushmerick, 2003</marker>
<rawString>Andreas Hess and Nicholas Kushmerick. 2003. Automatically attaching semantic metadata to web services. In Proceedings of the 2nd International Semantic Web Conference (ISWC 2003), Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Heinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Heinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Laurie Gerber</author>
</authors>
<title>An inquiry into the nature of multidocument abstracts, extracts, and their evaluation.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL2001 Workshop on Automatic Summarization,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1463" citStr="Marcu and Gerber, 2001" startWordPosition="224" endWordPosition="227">on. Different newswires publish different reports about the same event resulting in information overlap. Multi-Document Summarization (MDS) can help to reduce the amount of documents a user has to read to keep informed. In contrast to single document summarization information overlap is one of the biggest challenges to MDS systems. While repeated information is a good evidence of importance, this information should be included in a summary only once in order to avoid a repetitive summary. Sentence clustering has therefore often been used as an early step in MDS (Hatzivassiloglou et al., 2001; Marcu and Gerber, 2001; Radev et al., 2000). In sentence clustering semantically similar sentences are grouped together. Sentences within a cluster overlap in information, but they do not have to be identical in meaning. In contrast to paraphrases sentences in a cluster do not have to cover the same amount of information. One sentence represents one cluster in the summary. Either a sentences from the cluster is selected (Aliguliyev, 2006) or a new sentence is regenerated from all/some sentences in a cluster (Barzilay and McKeown, 2005). Usually the quality of the sentence clusters are only evaluated indirectly by j</context>
</contexts>
<marker>Marcu, Gerber, 2001</marker>
<rawString>Daniel Marcu and Laurie Gerber. 2001. An inquiry into the nature of multidocument abstracts, extracts, and their evaluation. In Proceedings of the NAACL2001 Workshop on Automatic Summarization, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meila</author>
</authors>
<title>Comparing clusterings–an information based distance.</title>
<date>2007</date>
<journal>Journal of Multivariate Analysis,</journal>
<volume>98</volume>
<issue>5</issue>
<contexts>
<context position="18318" citStr="Meila, 2007" startWordPosition="3269" endWordPosition="3270">= 2 I(L, C) H(L) + H(C) (4) which represents the average of the two uncertainty coefficients as described in Press et al. (1988). Generalise NMI to NMIβ = (1+β)I βH(L)+H(C). Then NMIβ is actually the same as Vβ: H(C|L) h = 1 − H(C) ⇒H(C)h = H(C) − H(C|L) = H(C) − H(C, L) + H(L) = I H(L|C) c = 1 − H(L) ⇒H(L)c = H(L) − H(L|C) = H(L) − H(L, C) + H(C) = I V = (1 + β)hc βh + c (1 + β)H(L)H(C)hc βH(L)H(C)h + H(L)H(C)c H(C)h and H(L)c are substituted by I: (1 + β)I2 βH(L)I + H(C)I = (1 + β)I βH(L) + H(C) V1 = 2 H(L) + H(C) I = NMI 5.3 Variation of Information (V I) and Normalized V I The VI-measure (Meila, 2007) also measures completeness and homogeneity using conditional entropy. It measure the distance between two clusterings and thereby the amount of information gained in changing from C to L. For this measure the conditional entropies are added up: V I(L, C) = H(C|L) + H(L|C) (7) Remember small conditional entropies mean that the clustering is near to complete homogeneity/ completeness, so the smaller V I the better (V I = 0 if L = C). The maximum of V I is log N e.g. for V I(Lsingle, Cone). V I can be normalized, then it can range from 0 (identical clusters) to 1. 1 NVI(L, C) = log NV I(L, C) (8</context>
<context position="19899" citStr="Meila (2007)" startWordPosition="3560" endWordPosition="3561">n fall into one of four categories: • TP (true positives) = objects belong to one class and one cluster • FP (false positives) = objects belong to different classes but to the same cluster • FN (false negatives) = objects belong to the same class but to different clusters • TN (true negatives) = objects belong to different classes and to different cluster By dividing the total number of correctly clustered pairs by the number of all pairs, RI gives the percentage of correct decisions. TP + TN RI = (9) TP + FP + TN + FN RI can range between 0 and 1 where 1 corresponds to identical clusterings. Meila (2007) mentions that in practise RI concentrates in a small interval near 1 (for more detail see section 5.7). Another shortcoming is that RI gives equal weight to FPs and FNs. I(L, C) NMI = H(L)+H(C) 2 = = NMIβ 100 5.5 Entropy and Purity Entropy and Purity are widely used evaluation measures (Zhao and Karypis, 2001). They both can be used to measure homogeneity of a cluster. Both measures give better values when the number of clusters increase, with the best result for Lsingle. Entropy ranges from 0 for identical clusterings or Lsingle to log N e.g. for Csingle and Lone. The values of P can range b</context>
<context position="22713" citStr="Meila (2007)" startWordPosition="4080" endWordPosition="4081">elf. The F-measure is very sensitive to changes. It is the only measure which uses its full measurement range. F = 0 if CA is compared to LA−single, which means that the F-measure considers LA−single to be the opposite of CA. Usually Lone and LA−single are considered to be observe and a measure should only reach its worst possible value if these sets are compared. In other words the F-measure might be too sensitive for our task. The RI stays most of the time in an interval between 0.84 and 1. Even for the comparison between CA and LA−single the RI is 0.91. This behaviour was also described in Meila (2007) who observed that the RI concentrates in a small interval near 1. As described in section 5.5 Purity and Entropy both measure homogeneity. They both react to changes slowly. Splitting and merging have almost the same effect on Purity. It reaches Pz� 0.6 when the clusters of the set were randomly split or merged four times. As explained above our ideal evaluation measure should punish a set of clusters which puts sentences of the same class into two clusters less than if sentences are merged with irrelevant ones. Homogeneity decreases if unrelated clusters are merged whereas a decline in compl</context>
<context position="24735" citStr="Meila (2007)" startWordPosition="4460" endWordPosition="4461">ings with a large number of clusters. Since V I is measured in bits with an upper bound of log N, values for different sets are difficult to compare. NVI tries to overcome this problem by Entropy = L |L |⎛ 1 L |C |nij nij ⎞ j=1 nj ⎝N i=1 log ⎠ nj log |C |nj 1 |L| Purity = N L j=1 �ni � j Max i P + R 101 1 0.8 evaluation measures 0.6 0.4 0.2 0 0 1 2 4 8 15 30 48 61 70 Figure 1: Behaviour of evaluation measure when randomly changed sets of clusters are compared to the original set. RI F VI NVI E Pure Vbeta V 0.5 number of clusters 5 VI measure 4 3 2 1 normalising V I by dividing it by log N. As Meila (2007) pointed out, this is only convenient if the comparison is limited to one data set. In this paper Vbeta, V0.5 and NVI will be used for evaluation purposes. 6 Comparability of Clusterings Following our procedure and guidelines the judges have to filter out all irrelevant sentences that are not related to another sentence from a different document. The number of these irrelevant sentences are different for every sentence set and every judge (see table 2). The evaluation measures require the same number of sentences in each set of clusters to compare them. The easiest way to ensure that each clus</context>
</contexts>
<marker>Meila, 2007</marker>
<rawString>Marina Meila. 2007. Comparing clusterings–an information based distance. Journal of Multivariate Analysis, 98(5):873–895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martina Naughton</author>
</authors>
<title>Exploiting structure for event discovery using the mdi algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Student Research Workshop,</booktitle>
<pages>31--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4337" citStr="Naughton (2007)" startWordPosition="704" endWordPosition="705">the clusterings. However Zha (2002) created a gold stan96 Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 96–104, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP dard relying on the section structure of web pages and news articles. In this gold standard the section numbers are assumed to give the true cluster label for a sentence. In this approach only sentences within the same document and even within the same paragraph are clustered together whereas our approach is to find similar information between documents. A gold standard for event identification was built by Naughton (2007). Ten annotators tagged events in a sentence. Each sentence could be assigned more than one event number. In our approach a sentence can only belong to one cluster. For the evaluation of SIMFINDER Hatzivassiloglou et al. (2001) created a set of 10.535 manually marked pairs of paragraphs. Two human annotator were asked to judge if the paragraphs contained ’common information’. They were given the guideline that only paragraphs that described the same object in the same way or in which the same object was acting the same are to be considered similar. They found significant disagreement between t</context>
</contexts>
<marker>Naughton, 2007</marker>
<rawString>Martina Naughton. 2007. Exploiting structure for event discovery using the mdi algorithm. In Proceedings of the ACL 2007 Student Research Workshop, pages 31–36, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Brian P Flannery</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
</authors>
<title>Numerical Recipies in C: The art of Scientific Programming.</title>
<date>1988</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="17834" citStr="Press et al. (1988)" startWordPosition="3155" endWordPosition="3158">th large |L |can reach high homogeneity quite easily, whereas |C |&gt; |L |decreases the weight of completeness. V-measure and Vbeta can range between 0 and 1, they reach 1 if the set of clusters is identical to the set of classes. 5.2 Normalized Mutual Information Mutual Information (I) measures the information that C and L share and can be expressed by using entropy and conditional entropy: I = H(C) + H(L) − H(C, L) (3) There are different ways to normalise I. Manning et al. (2008) uses = 2 I(L, C) H(L) + H(C) (4) which represents the average of the two uncertainty coefficients as described in Press et al. (1988). Generalise NMI to NMIβ = (1+β)I βH(L)+H(C). Then NMIβ is actually the same as Vβ: H(C|L) h = 1 − H(C) ⇒H(C)h = H(C) − H(C|L) = H(C) − H(C, L) + H(L) = I H(L|C) c = 1 − H(L) ⇒H(L)c = H(L) − H(L|C) = H(L) − H(L, C) + H(C) = I V = (1 + β)hc βh + c (1 + β)H(L)H(C)hc βH(L)H(C)h + H(L)H(C)c H(C)h and H(L)c are substituted by I: (1 + β)I2 βH(L)I + H(C)I = (1 + β)I βH(L) + H(C) V1 = 2 H(L) + H(C) I = NMI 5.3 Variation of Information (V I) and Normalized V I The VI-measure (Meila, 2007) also measures completeness and homogeneity using conditional entropy. It measure the distance between two clusterin</context>
</contexts>
<marker>Press, Flannery, Teukolsky, Vetterling, 1988</marker>
<rawString>William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. 1988. Numerical Recipies in C: The art of Scientific Programming. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Malgorzata Budzikowska</author>
</authors>
<title>Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies.</title>
<date>2000</date>
<booktitle>In In ANLP/NAACL Workshop on Summarization,</booktitle>
<pages>21--29</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1484" citStr="Radev et al., 2000" startWordPosition="228" endWordPosition="231">publish different reports about the same event resulting in information overlap. Multi-Document Summarization (MDS) can help to reduce the amount of documents a user has to read to keep informed. In contrast to single document summarization information overlap is one of the biggest challenges to MDS systems. While repeated information is a good evidence of importance, this information should be included in a summary only once in order to avoid a repetitive summary. Sentence clustering has therefore often been used as an early step in MDS (Hatzivassiloglou et al., 2001; Marcu and Gerber, 2001; Radev et al., 2000). In sentence clustering semantically similar sentences are grouped together. Sentences within a cluster overlap in information, but they do not have to be identical in meaning. In contrast to paraphrases sentences in a cluster do not have to cover the same amount of information. One sentence represents one cluster in the summary. Either a sentences from the cluster is selected (Aliguliyev, 2006) or a new sentence is regenerated from all/some sentences in a cluster (Barzilay and McKeown, 2005). Usually the quality of the sentence clusters are only evaluated indirectly by judging the quality of</context>
<context position="3379" citStr="Radev et al. (2000)" startWordPosition="540" endWordPosition="543">clustering systems can be compared. Another challenge is the evaluation of sentence clusters. There are a lot of evaluation methods available. Each of them focus on different properties of a set of clusters. We will discuss and evaluate the most widely used and most promising measures. In this paper the main focus is on the development of a gold standard for sentence clustering using DUC clusters. The guidelines and rules that were given to the human annotators are described and the interjudge agreement is evaluated. 2 Related Work Sentence Clustering is used for different application in NLP. Radev et al. (2000) use it in their MDS system MEAD. The centroids of the clusters are used to create a summary. Only the summary is evaluated, not the sentence clusters. The same applies to Wang et al. (2008). They use symmetric matrix factorisation to group similar sentences together and test their system on DUC2005 and DUC2006 data set, but do not evaluate the clusterings. However Zha (2002) created a gold stan96 Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 96–104, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP dard relying on the section structure of web pages and news articles</context>
</contexts>
<marker>Radev, Jing, Budzikowska, 2000</marker>
<rawString>Dragomir R. Radev, Hongyan Jing, and Malgorzata Budzikowska. 2000. Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies. In In ANLP/NAACL Workshop on Summarization, pages 21–29, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>American Statistical Association Journal,</journal>
<volume>66</volume>
<issue>336</issue>
<contexts>
<context position="19207" citStr="Rand, 1971" startWordPosition="3431" endWordPosition="3432">Remember small conditional entropies mean that the clustering is near to complete homogeneity/ completeness, so the smaller V I the better (V I = 0 if L = C). The maximum of V I is log N e.g. for V I(Lsingle, Cone). V I can be normalized, then it can range from 0 (identical clusters) to 1. 1 NVI(L, C) = log NV I(L, C) (8) V-measure, Vbeta and V I measure both completeness and homogeneity, no mapping between classes and clusters is needed (Rosenberg and Hirschberg, 2007) and they are only dependent on the relative size of the clusters (Vlachos et al., 2009). 5.4 Rand Index (RI) The Rand Index (Rand, 1971) compares two clusterings with a combinatorial approach. Each pair of objects can fall into one of four categories: • TP (true positives) = objects belong to one class and one cluster • FP (false positives) = objects belong to different classes but to the same cluster • FN (false negatives) = objects belong to the same class but to different clusters • TN (true negatives) = objects belong to different classes and to different cluster By dividing the total number of correctly clustered pairs by the number of all pairs, RI gives the percentage of correct decisions. TP + TN RI = (9) TP + FP + TN </context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>William M. Rand. 1971. Objective criteria for the evaluation of clustering methods. American Statistical Association Journal, 66(336):846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>410--420</pages>
<contexts>
<context position="15702" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="2739" endWordPosition="2742">ters that contains only one cluster with all the sentences of D will be called Lone. A cluster that contains only one object is called a singleton and a set of clusters that only consists of singletons is called Lsingle. A set of classes C = {ci|i = 1, ..., |C|} is a partition of a data set D into disjoint subsets called classes, so that ci n cm = 0. |C |is the number of classes in C. C is also called a gold standard of a clustering of data set D because this set contains the ”ideal” solution to a clustering task and other clusterings are compared to it. 5.1 V-measure and Vbeta The V-measure (Rosenberg and Hirschberg, 2007) is an external evaluation measure based on conditional entropy: V (L, C) = (1 + β)hc (1) βh + c It measures homogeneity (h) and completeness (c) of a clustering solution (see equation 2 where ni j is the number of sentences lj and ci share, ni the number of sentences in ci and nj the number of sentences in lj) H(L) = − H(L|C) = − A cluster set is homogeneous if only objects from a single class are assigned to a single cluster. By calculating the conditional entropy of the class distribution given the proposed clustering it can be measured how close the clustering is to complete homogeneity wh</context>
<context position="19070" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="3404" endWordPosition="3407">d thereby the amount of information gained in changing from C to L. For this measure the conditional entropies are added up: V I(L, C) = H(C|L) + H(L|C) (7) Remember small conditional entropies mean that the clustering is near to complete homogeneity/ completeness, so the smaller V I the better (V I = 0 if L = C). The maximum of V I is log N e.g. for V I(Lsingle, Cone). V I can be normalized, then it can range from 0 (identical clusters) to 1. 1 NVI(L, C) = log NV I(L, C) (8) V-measure, Vbeta and V I measure both completeness and homogeneity, no mapping between classes and clusters is needed (Rosenberg and Hirschberg, 2007) and they are only dependent on the relative size of the clusters (Vlachos et al., 2009). 5.4 Rand Index (RI) The Rand Index (Rand, 1971) compares two clusterings with a combinatorial approach. Each pair of objects can fall into one of four categories: • TP (true positives) = objects belong to one class and one cluster • FP (false positives) = objects belong to different classes but to the same cluster • FN (false negatives) = objects belong to the same class but to different clusters • TN (true negatives) = objects belong to different classes and to different cluster By dividing the total num</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 410– 420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Anna Korhonen</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Unsupervised and Constrained Dirichlet Process Mixture Models for Verb Clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL workshop on GEometrical Models of Natural Language Semantics.</booktitle>
<contexts>
<context position="16987" citStr="Vlachos et al. (2009)" startWordPosition="2999" endWordPosition="3002"> constrained by the size of the data set and the distribution of the class sizes it is normalized by H(C) (see equation 2). Completeness on the other hand is achieved if all H(C|L) h = 1 − H(C) c= 1 − H(L|C) H(L) H(C|L) = − X |L |X |C |ni N j log nij j=1 i=1 nj H(C) = − X |C |ni ni (2) i=1 N logN nj nj N logN ni N log j |L| X j=1 ni j ni X |C| i=1 X |L| j=1 99 data points from a single class are assigned to a single cluster which results in H(L|C) = 0. The V-measure can be weighted. If Q &gt; 1 the completeness is favoured over homogeneity whereas the weight of homogeneity is increased if Q &lt; 1. Vlachos et al. (2009) proposes Vbeta where Q is set to IC . This way the shortcoming of the V-measure to favour cluster sets with many more clusters than classes can be avoided. If |L |&gt; |C |the weight of homogeneity is reduced, since clusterings with large |L |can reach high homogeneity quite easily, whereas |C |&gt; |L |decreases the weight of completeness. V-measure and Vbeta can range between 0 and 1, they reach 1 if the set of clusters is identical to the set of classes. 5.2 Normalized Mutual Information Mutual Information (I) measures the information that C and L share and can be expressed by using entropy and </context>
<context position="19158" citStr="Vlachos et al., 2009" startWordPosition="3420" endWordPosition="3423">al entropies are added up: V I(L, C) = H(C|L) + H(L|C) (7) Remember small conditional entropies mean that the clustering is near to complete homogeneity/ completeness, so the smaller V I the better (V I = 0 if L = C). The maximum of V I is log N e.g. for V I(Lsingle, Cone). V I can be normalized, then it can range from 0 (identical clusters) to 1. 1 NVI(L, C) = log NV I(L, C) (8) V-measure, Vbeta and V I measure both completeness and homogeneity, no mapping between classes and clusters is needed (Rosenberg and Hirschberg, 2007) and they are only dependent on the relative size of the clusters (Vlachos et al., 2009). 5.4 Rand Index (RI) The Rand Index (Rand, 1971) compares two clusterings with a combinatorial approach. Each pair of objects can fall into one of four categories: • TP (true positives) = objects belong to one class and one cluster • FP (false positives) = objects belong to different classes but to the same cluster • FN (false negatives) = objects belong to the same class but to different clusters • TN (true negatives) = objects belong to different classes and to different cluster By dividing the total number of correctly clustered pairs by the number of all pairs, RI gives the percentage of </context>
<context position="24040" citStr="Vlachos et al., 2009" startWordPosition="4304" endWordPosition="4307"> clusters are merged than if a cluster is split. Entropy for example is more sensitive to merging than splitting. But Entropy only measures homogeneity and an ideal evaluation measure should also consider completeness. The remaining measures Vbeta, V0.5 and NV I/V I all fulfil our criteria of a good evaluation measure. All of them are more affected by merging than by splitting and use their measuring range appropriately. V0.5 favours homogeneity over completeness, but it reacts to changes less than Vbeta. The V-measure can also be inaccurate if the |L |is considerably different to |C|. Vbeta (Vlachos et al., 2009) tries to overcome this problem and the tendency of the V-measure to favour clusterings with a large number of clusters. Since V I is measured in bits with an upper bound of log N, values for different sets are difficult to compare. NVI tries to overcome this problem by Entropy = L |L |⎛ 1 L |C |nij nij ⎞ j=1 nj ⎝N i=1 log ⎠ nj log |C |nj 1 |L| Purity = N L j=1 �ni � j Max i P + R 101 1 0.8 evaluation measures 0.6 0.4 0.2 0 0 1 2 4 8 15 30 48 61 70 Figure 1: Behaviour of evaluation measure when randomly changed sets of clusters are compared to the original set. RI F VI NVI E Pure Vbeta V 0.5 n</context>
</contexts>
<marker>Vlachos, Korhonen, Ghahramani, 2009</marker>
<rawString>Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani. 2009. Unsupervised and Constrained Dirichlet Process Mixture Models for Verb Clustering. In Proceedings of the EACL workshop on GEometrical Models of Natural Language Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Tao Li</author>
<author>Shenghuo Zhu</author>
<author>Chris Ding</author>
</authors>
<title>Multi-document summarization via sentence-level semantic analysis and symmetric matrix factorization.</title>
<date>2008</date>
<booktitle>In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>307--314</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3569" citStr="Wang et al. (2008)" startWordPosition="575" endWordPosition="578">set of clusters. We will discuss and evaluate the most widely used and most promising measures. In this paper the main focus is on the development of a gold standard for sentence clustering using DUC clusters. The guidelines and rules that were given to the human annotators are described and the interjudge agreement is evaluated. 2 Related Work Sentence Clustering is used for different application in NLP. Radev et al. (2000) use it in their MDS system MEAD. The centroids of the clusters are used to create a summary. Only the summary is evaluated, not the sentence clusters. The same applies to Wang et al. (2008). They use symmetric matrix factorisation to group similar sentences together and test their system on DUC2005 and DUC2006 data set, but do not evaluate the clusterings. However Zha (2002) created a gold stan96 Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 96–104, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP dard relying on the section structure of web pages and news articles. In this gold standard the section numbers are assumed to give the true cluster label for a sentence. In this approach only sentences within the same document and even within the same parag</context>
</contexts>
<marker>Wang, Li, Zhu, Ding, 2008</marker>
<rawString>Dingding Wang, Tao Li, Shenghuo Zhu, and Chris Ding. 2008. Multi-document summarization via sentence-level semantic analysis and symmetric matrix factorization. In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 307–314, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyuan Zha</author>
</authors>
<title>Generic Summarization and Keyphrase Extraction using Mutual Reinforcement Principle and Sentence Clustering.</title>
<date>2002</date>
<booktitle>In Proceedings of the 25th Annual ACM SIGIR Conference,</booktitle>
<pages>113--120</pages>
<location>Tampere, Finland.</location>
<contexts>
<context position="3757" citStr="Zha (2002)" startWordPosition="608" endWordPosition="609">DUC clusters. The guidelines and rules that were given to the human annotators are described and the interjudge agreement is evaluated. 2 Related Work Sentence Clustering is used for different application in NLP. Radev et al. (2000) use it in their MDS system MEAD. The centroids of the clusters are used to create a summary. Only the summary is evaluated, not the sentence clusters. The same applies to Wang et al. (2008). They use symmetric matrix factorisation to group similar sentences together and test their system on DUC2005 and DUC2006 data set, but do not evaluate the clusterings. However Zha (2002) created a gold stan96 Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 96–104, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP dard relying on the section structure of web pages and news articles. In this gold standard the section numbers are assumed to give the true cluster label for a sentence. In this approach only sentences within the same document and even within the same paragraph are clustered together whereas our approach is to find similar information between documents. A gold standard for event identification was built by Naughton (2007). Ten annotators tag</context>
</contexts>
<marker>Zha, 2002</marker>
<rawString>Hongyuan Zha. 2002. Generic Summarization and Keyphrase Extraction using Mutual Reinforcement Principle and Sentence Clustering. In Proceedings of the 25th Annual ACM SIGIR Conference, pages 113–120, Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhao</author>
<author>George Karypis</author>
</authors>
<title>Criterion functions for document clustering: Experiments and analysis.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, University of Minnesota.</institution>
<contexts>
<context position="20211" citStr="Zhao and Karypis, 2001" startWordPosition="3614" endWordPosition="3617">cts belong to different classes and to different cluster By dividing the total number of correctly clustered pairs by the number of all pairs, RI gives the percentage of correct decisions. TP + TN RI = (9) TP + FP + TN + FN RI can range between 0 and 1 where 1 corresponds to identical clusterings. Meila (2007) mentions that in practise RI concentrates in a small interval near 1 (for more detail see section 5.7). Another shortcoming is that RI gives equal weight to FPs and FNs. I(L, C) NMI = H(L)+H(C) 2 = = NMIβ 100 5.5 Entropy and Purity Entropy and Purity are widely used evaluation measures (Zhao and Karypis, 2001). They both can be used to measure homogeneity of a cluster. Both measures give better values when the number of clusters increase, with the best result for Lsingle. Entropy ranges from 0 for identical clusterings or Lsingle to log N e.g. for Csingle and Lone. The values of P can range between 0 and 1, where a value close to 0 represents a bad clustering solution and a perfect clustering solution gets a value of 1. (10) 5.6 F-measure The F-measure is a well known metric from IR, which is based on Recall and Precision. The version of the F-score (Hess and Kushmerick, 2003) described here measur</context>
</contexts>
<marker>Zhao, Karypis, 2001</marker>
<rawString>Ying Zhao and George Karypis. 2001. Criterion functions for document clustering: Experiments and analysis. Technical report, Department of Computer Science, University of Minnesota. (Technical Report #01-40).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>