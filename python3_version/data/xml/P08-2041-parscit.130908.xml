<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000848">
<title confidence="0.98928">
Partial Matching Strategy for Phrase-based Statistical Machine Translation
</title>
<author confidence="0.98718">
Zhongjun He1n and Qun Liu1 and Shouxun Lin1
</author>
<affiliation confidence="0.9954596">
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, 100190, China
2Graduate University of Chinese Academy of Sciences
</affiliation>
<address confidence="0.652899">
Beijing, 100049, China
</address>
<email confidence="0.999191">
{zjhe,liuqun,sxlin}@ict.ac.cn
</email>
<sectionHeader confidence="0.996666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999863916666667">
This paper presents a partial matching strat-
egy for phrase-based statistical machine trans-
lation (PBSMT). Source phrases which do not
appear in the training corpus can be trans-
lated by word substitution according to par-
tially matched phrases. The advantage of this
method is that it can alleviate the data sparse-
ness problem if the amount ofbilingual corpus
is limited. We incorporate our approach into
the state-of-the-art PBSMT system Moses and
achieve statistically significant improvements
on both small and large corpora.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975577777778">
Currently, most of the phrase-based statistical ma-
chine translation (PBSMT) models (Marcu and
Wong, 2002; Koehn et al., 2003) adopt full matching
strategy for phrase translation, which means that a
phrase pair ( �f, ee) can be used for translating a source
phrase f, only if f= f. Due to lack of generaliza-
tion ability, the full matching strategy has some lim-
itations. On one hand, the data sparseness problem
is serious, especially when the amount of the bilin-
gual data is limited. On the other hand, for a certain
source text, the phrase table is redundant since most
of the bilingual phrases cannot be fully matched.
In this paper, we address the problem of trans-
lation of unseen phrases, the source phrases that
are not observed in the training corpus. The
alignment template model (Och and Ney, 2004)
enhanced phrasal generalizations by using words
classes rather than the words themselves. But the
phrases are overly generalized. The hierarchical
phrase-based model (Chiang, 2005) used hierar-
chical phrase pairs to strengthen the generalization
ability of phrases and allow long distance reorder-
ings. However, the huge grammar table greatly in-
creases computational complexity. Callison-Burch
et al. (2006) used paraphrases of the trainig corpus
for translating unseen phrases. But they only found
and used the semantically similar phrases. Another
method is to use multi-parallel corpora (Cohn and
Lapata, 2007; Utiyama and Isahara, 2007) to im-
prove phrase coverage and translation quality.
This paper presents a partial matching strategy for
translating unseen phrases. When encountering un-
seen phrases in a source sentence, we search par-
tially matched phrase pairs from the phrase table.
Then we keep the translations of the matched part
and translate the unmatched part by word substitu-
tion. The advantage of our approach is that we alle-
viate the data sparseness problem without increasing
the amount of bilingual corpus. Moreover, the par-
tially matched phrases are not necessarily synony-
mous. We incorporate the partial matching method
into the state-of-the-art PBSMT system, Moses. Ex-
periments show that, our approach achieves statis-
tically significant improvements not only on small
corpus, but also on large corpus.
</bodyText>
<sectionHeader confidence="0.939861" genericHeader="introduction">
2 Partial Matching for PBSMT
</sectionHeader>
<subsectionHeader confidence="0.976267">
2.1 Partial Matching
</subsectionHeader>
<bodyText confidence="0.99864425">
We use matching similarity to measure how well the
source phrases match each other. Given two source
phrases �f1 and �f′i, the matching similarity is com-
puted as:
</bodyText>
<page confidence="0.983491">
161
</page>
<reference confidence="0.2090225">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 161–164,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<figureCaption confidence="0.9723915">
Figure 1: An example of partially matched phrases with
the same POS sequence and word alignment.
</figureCaption>
<figure confidence="0.7796295">
-ji �� ��
arrived in Thailand yesterday
</figure>
<figureCaption confidence="0.854468">
Figure 2: An example of phrase translation.
</figureCaption>
<figure confidence="0.947960714285714">
bring advantage to the Taiwan
*/P o /N
/.C1,/N �,�_/V
)i�k-/N
people
issued warning to the American
�$/P kq/N
/.t1,/N At�/V
u IL/N
people
arrived in Prague last evening
�� �� ���
arrived in
-ji 01%
</figure>
<equation confidence="0.785244">
�J j=1 δ(fj, f′j)
SIM( �fJ 1 , �f′J 1) = (1)
J
where,
δ(f, f′) =� 1 if f = f′
(2)
0 otherwise
</equation>
<bodyText confidence="0.998152736842105">
Therefore, partial matching takes full matching
f) = 1.0) as a special case. Note that in
order to improve search efficiency, we only consider
the partially matched phrases with the same length.
In our experiments, we use a matching threshold
α to tune the precision of partial matching. Low
threshold indicates high coverage of unseen phrases,
but will suffer from much noise. In order to alleviate
this problem, we search partially matched phrases
under the constraint that they must have the same
parts-of-speech (POS) sequence. See Figure 1 for
illustration. Although the matching similarity of the
two phrases is only 0.2, as they have the same POS
sequence, the word alignments are the same. There-
fore, the lower source phrase can be translated ac-
cording to the upper phrase pair with correct word
reordering. Furthermore, this constraint can sharply
decrease the computational complexity since there
is no need to search the whole phrase table.
</bodyText>
<subsectionHeader confidence="0.999792">
2.2 Translating Unseen Phrases
</subsectionHeader>
<bodyText confidence="0.999698">
We translate an unseen phrase fJ1 according to the
partially matched phrase pair (f′J1 , e′I1, a) as follows:
</bodyText>
<listItem confidence="0.94470125">
1. Compare each word between fJ1 and f′J1 to get
the position set of the different words: P =
{j|fj =� f′j, j = 1, 2, ... , J};
2. Remove f′j from f′J1 and e′aj from e′I1, where
j E P;
3. Find the translation e for fj(j E P) from the
phrase table and put it into the position aj in
e′I1 according to the word alignment a.
</listItem>
<bodyText confidence="0.7969515">
Figure 2 shows an example. In fact, we create a
translation template dynamically in step 2:
</bodyText>
<equation confidence="0.840329">
(t X1 X2, arrived in X2 X1) (3)
</equation>
<bodyText confidence="0.999688642857143">
Here, on the source side, each of the non-terminal
X corresponds to a single source word. In addition,
the removed sub-phrase pairs should be consistent
with the word alignment matrix.
Following conventional PBSMT models, we use
4 features to measure phrase translation quality: the
translation weights p(�f|e) _and p(e|�f ), the lexical
weights pw (�f  |e) and pw (e |f ). The new constructed
phrase pairs keep the translation weights of their
“parent” phrase pair. The lexical weights are com-
puted by word substitution. Suppose S{(f′, e′)} is
�e′,�a) which replaced by S{(f, e)}
to create the new phrase pair (�f,e,a), the lexical
weight is computed as:
</bodyText>
<equation confidence="0.99379">
�f′|�e′, a) X H(f,e)∈S{(f,e)} pw(f|e)
(4)
</equation>
<bodyText confidence="0.9999">
Therefore, the newly constructed phrase pairs can be
used for decoding as they have already existed in the
phrase table.
</bodyText>
<subsectionHeader confidence="0.944561">
2.3 Incorporating Partial Matching into the
PBSMT Model
</subsectionHeader>
<bodyText confidence="0.999578666666667">
In this paper, we incorporate the partial matching
strategy into the state-of-the-art PBSMT system,
Moses1. Given a source sentence, Moses firstly
uses the full matching strategy to search all possi-
ble translation options from the phrase table, and
then uses a beam-search algorithm for decoding.
</bodyText>
<footnote confidence="0.727171">
1http://www.statmt.org/moses/
</footnote>
<figure confidence="0.735293125">
�f,
(SIM(
the pair set in (
P,
pw(�f|�e,�a)
pw(
=
H (f′,e′)∈S{(f′,e′)} pw(f′|e′)
</figure>
<page confidence="0.980642">
162
</page>
<bodyText confidence="0.9941632">
Therefore, we do incorporation by performing par-
tial matching for phrase translation before decod-
ing. The advantage is that the main search algorithm
need not be changed.
For a source phrase ef, we search partially
</bodyText>
<table confidence="0.9830585">
α 1.0 0.7 0.5 0.3 0.1
BLEU 24.44 24.43 24.86 25.31 25.13
</table>
<tableCaption confidence="0.999859">
Table 1: Effect of matching threshold on BLEU score.
</tableCaption>
<bodyText confidence="0.98815">
matched phrase pair ( ef′, ee′, ea) from the phrase table. 3.1 Small-scale Task
ef′)=1.0, which means
the training corpus, thus ee′ can be directly stored as a
translation option. However, if α &lt; SIM(
1.0, we construct translations for fe according to Sec-
tion 2.2. Then the newly constructed translations are
stored as translation options.
Moses uses translation weights and lexical
weights to measure the quality of a phrase transla-
tion pair. For partial matching, besides these fea-
tures, we add matching similarity SIM( ef, ef′) as a
new feature. For a source phrase, we select top N
translations for decoding. In Moses, N is set by the
pruning parameter ttable-limit.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="background">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99953768">
We carry out experiments on Chinese-to-English
translation on two tasks: Small-scale task, the train-
ing corpus consists of 30k sentence pairs (840K +
950K words); Large-scale task, the training cor-
pus consists of 2.54M sentence pairs (68M + 74M
words). The 2002 NIST MT evaluation test data is
used as the development set and the 2005 NIST MT
test data is the test set. The baseline system we used
for comparison is the state-of-the-art PBSMT sys-
tem, Moses.
We use the ICTCLAS toolkit2 to perform Chinese
word segmentation and POS tagging. The training
script of Moses is used to train the bilingual corpus.
We set the maximum length of the source phrase
to 7, and record word alignment information in the
phrase table. For the language model, we use the
SRI Language Modeling Toolkit (Stolcke, 2002) to
train a 4-gram model on the Xinhua portion of the
Gigaword corpus.
To run the decoder, we set ttable-limit=20,
distortion-limit=6, stack=100. The translation qual-
ity is evaluated by BLEU-4 (case-sensitive). We per-
form minimum-error-rate training (Och, 2003) to
tune the feature weights of the translation model to
maximize the BLEU score on development set.
</bodyText>
<footnote confidence="0.937205">
2http://www.nlp.org.cn/project/project.php?proj id=6
</footnote>
<bodyText confidence="0.9988005625">
Table 1 shows the effect of matching threshold on
translation quality. The baseline uses full matching
(α=1.0) for phrase translation and achieves a BLEU
score of 24.44. With the decrease of the matching
threshold, the BLEU scores increase. when α=0.3,
the system obtains the highest BLEU score of 25.31,
which achieves an absolute improvement of 0.87
over the baseline. However, if the threshold con-
tinue decreasing, the BLEU score decreases. The
reason is that low threshold increases noise for par-
tial matching.
The effect of matching threshold on the coverage
of n-gram phrases is shown in Figure 3. When us-
ing full matching (α=1.0), long phrases (length&gt;3)
face a serious data sparseness problem. With the de-
crease of the threshold, the coverage increases.
</bodyText>
<figure confidence="0.8722365">
1 2 3 4 5 6 7
phrase length
</figure>
<figureCaption confidence="0.9982875">
Figure 3: Effect of matching threshold on the coverage of
n-gram phrases.
</figureCaption>
<bodyText confidence="0.999945666666667">
Table 2 shows the phrase number of 1-best out-
put under α=1.0 and α=0.3. When α=1.0, the long
phrases (length&gt;3) only account for 2.9% of the to-
tal phrases. When α=0.3, the number increases to
10.7%. Moreover, the total phrase of α=0.3 is less
than that of α=1.0, since source text is segmented
into more long phrases under partial matching, and
most of the long phrases are translated from partially
matched phrases (the row 0.3&lt; SIM &lt;1.0).
</bodyText>
<subsectionHeader confidence="0.997441">
3.2 Large-scale Task
</subsectionHeader>
<bodyText confidence="0.9019055">
For this task, the BLEU score of the baseline is
30.45. However, for partial matching method with
</bodyText>
<figure confidence="0.986948954545455">
coverage ratio on the test set
100
80
60
40
20
90
70
50
30
10
0
α=1.0
α=0.7
α=0.5
α=0.3
α=0.1
If SIM(
ef,
fe is observed in
ef,
ef′) &lt;
</figure>
<page confidence="0.992855">
163
</page>
<table confidence="0.99751025">
Phrase Length 1 2 3 4 5 6 7 total
α=1.0 19485 4416 615 87 12 2 1 24618
α=0.3 SIM=1.0 14750 2977 387 48 10 1 0 21195
0.3≤ SIM &lt;1.0 0 1196 1398 306 93 17 12
</table>
<tableCaption confidence="0.9539405">
Table 2: Phrase number of 1-best output. α=1.0 means full matching. For α=0.3, SIM=1.0 means full matching,
0.3 ≤ SIM &lt; 1.0 means partial matching.
</tableCaption>
<bodyText confidence="0.999270727272727">
α=0.53, the BLEU score is 30.96, achieving an ab-
solute improvement of 0.51. Using Zhang’s signif-
icant tester (Zhang et al., 2004), both the improve-
ments on the two tasks are statistically significant at
p &lt; 0.05.
The improvement on large-scale task is less than
that on small-scale task since larger corpus relieves
data sparseness. However, the partial matching ap-
proach can also improve translation quality by using
long phrases. For example, the segmentation and
translation for the Chinese sentence “ 4 k 1!
</bodyText>
<equation confidence="0.965623833333333">
Ñ &apos;7 •� * 4” are as follows:
Full matching:
•Xq  |i_� jÑ  |��  |61,  |��  |�
long term  |economic output  |, but  |the  |trend  |will
Partial matching:
�a  |i_ 61, •Xq A4-:*  |�
</equation>
<bodyText confidence="0.997869375">
but  |the long-term trend of economic output  |will
Here the source phrase “L;&apos;;)&apos;I- 1&apos;Ñ &apos;7 •Al �
V cannot be fully matched. Thus the decoder
breaks it into 4 short phrases, but performs an in-
correct reordering. Using partial matching, the long
phrase is translated correctly since it can partially
matched the phrase pair “�� �A 611 &amp;quot;;-/-A ALM&amp;quot;
the inevitable trend of economic development”.
</bodyText>
<subsectionHeader confidence="0.985069">
3.3 Conclusion
</subsectionHeader>
<bodyText confidence="0.99983025">
This paper presents a partial matching strategy for
phrase-based statistical machine translation. Phrases
which are not observed in the training corpus can
be translated according to partially matched phrases
by word substitution. Our method can relieve data
sparseness problem without increasing the amount
of the corpus. Experiments show that our approach
achieves statistically significant improvements over
the state-of-the-art PBSMT system Moses.
In future, we will study sophisticated partial
matching methods, since current constraints are ex-
cessively strict. Moreover, we will study the effect
</bodyText>
<footnote confidence="0.9367495">
3Due to time limit, we do not tune the threshold for large-
scale task.
</footnote>
<bodyText confidence="0.839757">
of word alignment on partial matching, which may
affect word substitution and reordering.
</bodyText>
<sectionHeader confidence="0.99807" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999785333333333">
We would like to thank Yajuan Lv and Yang Liu
for their valuable suggestions. This work was sup-
ported by the National Natural Science Foundation
of China (NO. 60573188 and 60736014), and the
High Technology Research and Development Pro-
gram of China (NO.2006AA010108).
</bodyText>
<sectionHeader confidence="0.998725" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995309">
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proc. ofNAACL06, pages 17–24.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ofACL05,
pages 263–270.
T. Cohn and M. Lapata. 2007. Machine translation by
triangulation: Making effective use of multi-parallel
corpora. In Proc. ofACL07, pages 728–735.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL03,
pages 127–133.
D. Marcu and W. Wong. 2002. A phrasebased joint
probabilitymodel for statistical machine translation. In
Proc. of EMNLP02, pages 133–139.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30:417–449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL03, pages 160–
167.
A. Stolcke. 2002. Srilm – an extensible language model-
ing toolkit. In Proc. ofICSLP02, pages 901–904.
M. Utiyama and H. Isahara. 2007. A comparison ofpivot
methods for phrase-based statistical machine transla-
tion. In Proc. ofNAACL-HLT07, pages 484–491.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting
bleu/nist scores: How much improvement do we need
to have a better system? In Proc. of LREC04, pages
2051–2054.
</reference>
<page confidence="0.998516">
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888848">
<title confidence="0.993767">Partial Matching Strategy for Phrase-based Statistical Machine Translation</title>
<affiliation confidence="0.996449333333333">Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<address confidence="0.998119">Beijing, 100190, China</address>
<affiliation confidence="0.999569">University of Chinese Academy of Sciences</affiliation>
<address confidence="0.998697">Beijing, 100049, China</address>
<abstract confidence="0.998482846153846">This paper presents a partial matching strategy for phrase-based statistical machine translation (PBSMT). Source phrases which do not appear in the training corpus can be translated by word substitution according to partially matched phrases. The advantage of this method is that it can alleviate the data sparseness problem if the amount ofbilingual corpus is limited. We incorporate our approach into the state-of-the-art PBSMT system Moses and achieve statistically significant improvements on both small and large corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>161--164</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 161–164,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>M Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proc. ofNAACL06,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="2125" citStr="Callison-Burch et al. (2006)" startWordPosition="319" endWordPosition="322">ual phrases cannot be fully matched. In this paper, we address the problem of translation of unseen phrases, the source phrases that are not observed in the training corpus. The alignment template model (Och and Ney, 2004) enhanced phrasal generalizations by using words classes rather than the words themselves. But the phrases are overly generalized. The hierarchical phrase-based model (Chiang, 2005) used hierarchical phrase pairs to strengthen the generalization ability of phrases and allow long distance reorderings. However, the huge grammar table greatly increases computational complexity. Callison-Burch et al. (2006) used paraphrases of the trainig corpus for translating unseen phrases. But they only found and used the semantically similar phrases. Another method is to use multi-parallel corpora (Cohn and Lapata, 2007; Utiyama and Isahara, 2007) to improve phrase coverage and translation quality. This paper presents a partial matching strategy for translating unseen phrases. When encountering unseen phrases in a source sentence, we search partially matched phrase pairs from the phrase table. Then we keep the translations of the matched part and translate the unmatched part by word substitution. The advant</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>C. Callison-Burch, P. Koehn, and M. Osborne. 2006. Improved statistical machine translation using paraphrases. In Proc. ofNAACL06, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofACL05,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1900" citStr="Chiang, 2005" startWordPosition="289" endWordPosition="290">ne hand, the data sparseness problem is serious, especially when the amount of the bilingual data is limited. On the other hand, for a certain source text, the phrase table is redundant since most of the bilingual phrases cannot be fully matched. In this paper, we address the problem of translation of unseen phrases, the source phrases that are not observed in the training corpus. The alignment template model (Och and Ney, 2004) enhanced phrasal generalizations by using words classes rather than the words themselves. But the phrases are overly generalized. The hierarchical phrase-based model (Chiang, 2005) used hierarchical phrase pairs to strengthen the generalization ability of phrases and allow long distance reorderings. However, the huge grammar table greatly increases computational complexity. Callison-Burch et al. (2006) used paraphrases of the trainig corpus for translating unseen phrases. But they only found and used the semantically similar phrases. Another method is to use multi-parallel corpora (Cohn and Lapata, 2007; Utiyama and Isahara, 2007) to improve phrase coverage and translation quality. This paper presents a partial matching strategy for translating unseen phrases. When enco</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ofACL05, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>M Lapata</author>
</authors>
<title>Machine translation by triangulation: Making effective use of multi-parallel corpora.</title>
<date>2007</date>
<booktitle>In Proc. ofACL07,</booktitle>
<pages>728--735</pages>
<contexts>
<context position="2330" citStr="Cohn and Lapata, 2007" startWordPosition="350" endWordPosition="353">Ney, 2004) enhanced phrasal generalizations by using words classes rather than the words themselves. But the phrases are overly generalized. The hierarchical phrase-based model (Chiang, 2005) used hierarchical phrase pairs to strengthen the generalization ability of phrases and allow long distance reorderings. However, the huge grammar table greatly increases computational complexity. Callison-Burch et al. (2006) used paraphrases of the trainig corpus for translating unseen phrases. But they only found and used the semantically similar phrases. Another method is to use multi-parallel corpora (Cohn and Lapata, 2007; Utiyama and Isahara, 2007) to improve phrase coverage and translation quality. This paper presents a partial matching strategy for translating unseen phrases. When encountering unseen phrases in a source sentence, we search partially matched phrase pairs from the phrase table. Then we keep the translations of the matched part and translate the unmatched part by word substitution. The advantage of our approach is that we alleviate the data sparseness problem without increasing the amount of bilingual corpus. Moreover, the partially matched phrases are not necessarily synonymous. We incorporat</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>T. Cohn and M. Lapata. 2007. Machine translation by triangulation: Making effective use of multi-parallel corpora. In Proc. ofACL07, pages 728–735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL03,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1039" citStr="Koehn et al., 2003" startWordPosition="143" endWordPosition="146">for phrase-based statistical machine translation (PBSMT). Source phrases which do not appear in the training corpus can be translated by word substitution according to partially matched phrases. The advantage of this method is that it can alleviate the data sparseness problem if the amount ofbilingual corpus is limited. We incorporate our approach into the state-of-the-art PBSMT system Moses and achieve statistically significant improvements on both small and large corpora. 1 Introduction Currently, most of the phrase-based statistical machine translation (PBSMT) models (Marcu and Wong, 2002; Koehn et al., 2003) adopt full matching strategy for phrase translation, which means that a phrase pair ( �f, ee) can be used for translating a source phrase f, only if f= f. Due to lack of generalization ability, the full matching strategy has some limitations. On one hand, the data sparseness problem is serious, especially when the amount of the bilingual data is limited. On the other hand, for a certain source text, the phrase table is redundant since most of the bilingual phrases cannot be fully matched. In this paper, we address the problem of translation of unseen phrases, the source phrases that are not o</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL03, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A phrasebased joint probabilitymodel for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP02,</booktitle>
<pages>133--139</pages>
<contexts>
<context position="1018" citStr="Marcu and Wong, 2002" startWordPosition="139" endWordPosition="142">ial matching strategy for phrase-based statistical machine translation (PBSMT). Source phrases which do not appear in the training corpus can be translated by word substitution according to partially matched phrases. The advantage of this method is that it can alleviate the data sparseness problem if the amount ofbilingual corpus is limited. We incorporate our approach into the state-of-the-art PBSMT system Moses and achieve statistically significant improvements on both small and large corpora. 1 Introduction Currently, most of the phrase-based statistical machine translation (PBSMT) models (Marcu and Wong, 2002; Koehn et al., 2003) adopt full matching strategy for phrase translation, which means that a phrase pair ( �f, ee) can be used for translating a source phrase f, only if f= f. Due to lack of generalization ability, the full matching strategy has some limitations. On one hand, the data sparseness problem is serious, especially when the amount of the bilingual data is limited. On the other hand, for a certain source text, the phrase table is redundant since most of the bilingual phrases cannot be fully matched. In this paper, we address the problem of translation of unseen phrases, the source p</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>D. Marcu and W. Wong. 2002. A phrasebased joint probabilitymodel for statistical machine translation. In Proc. of EMNLP02, pages 133–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="1719" citStr="Och and Ney, 2004" startWordPosition="263" endWordPosition="266">ans that a phrase pair ( �f, ee) can be used for translating a source phrase f, only if f= f. Due to lack of generalization ability, the full matching strategy has some limitations. On one hand, the data sparseness problem is serious, especially when the amount of the bilingual data is limited. On the other hand, for a certain source text, the phrase table is redundant since most of the bilingual phrases cannot be fully matched. In this paper, we address the problem of translation of unseen phrases, the source phrases that are not observed in the training corpus. The alignment template model (Och and Ney, 2004) enhanced phrasal generalizations by using words classes rather than the words themselves. But the phrases are overly generalized. The hierarchical phrase-based model (Chiang, 2005) used hierarchical phrase pairs to strengthen the generalization ability of phrases and allow long distance reorderings. However, the huge grammar table greatly increases computational complexity. Callison-Burch et al. (2006) used paraphrases of the trainig corpus for translating unseen phrases. But they only found and used the semantically similar phrases. Another method is to use multi-parallel corpora (Cohn and L</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL03,</booktitle>
<pages>160--167</pages>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL03, pages 160– 167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ofICSLP02,</booktitle>
<pages>901--904</pages>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proc. ofICSLP02, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Utiyama</author>
<author>H Isahara</author>
</authors>
<title>A comparison ofpivot methods for phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ofNAACL-HLT07,</booktitle>
<pages>484--491</pages>
<contexts>
<context position="2358" citStr="Utiyama and Isahara, 2007" startWordPosition="354" endWordPosition="357">asal generalizations by using words classes rather than the words themselves. But the phrases are overly generalized. The hierarchical phrase-based model (Chiang, 2005) used hierarchical phrase pairs to strengthen the generalization ability of phrases and allow long distance reorderings. However, the huge grammar table greatly increases computational complexity. Callison-Burch et al. (2006) used paraphrases of the trainig corpus for translating unseen phrases. But they only found and used the semantically similar phrases. Another method is to use multi-parallel corpora (Cohn and Lapata, 2007; Utiyama and Isahara, 2007) to improve phrase coverage and translation quality. This paper presents a partial matching strategy for translating unseen phrases. When encountering unseen phrases in a source sentence, we search partially matched phrase pairs from the phrase table. Then we keep the translations of the matched part and translate the unmatched part by word substitution. The advantage of our approach is that we alleviate the data sparseness problem without increasing the amount of bilingual corpus. Moreover, the partially matched phrases are not necessarily synonymous. We incorporate the partial matching metho</context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>M. Utiyama and H. Isahara. 2007. A comparison ofpivot methods for phrase-based statistical machine translation. In Proc. ofNAACL-HLT07, pages 484–491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Interpreting bleu/nist scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<booktitle>In Proc. of LREC04,</booktitle>
<pages>2051--2054</pages>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting bleu/nist scores: How much improvement do we need to have a better system? In Proc. of LREC04, pages 2051–2054.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>