<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9995315">
Automatic Stochastic Tagging of
Natural Language Texts
</title>
<author confidence="0.995458">
Evangelos Derma tas* George Kokkinakis*
</author>
<affiliation confidence="0.998133">
University of Patras University of Patras
</affiliation>
<bodyText confidence="0.996843272727273">
Five language and tagset independent stochastic taggers, handling morphological and contextual
information, are presented and tested in corpora of seven European languages (Dutch, English,
French, German, Greek, Italian and Spanish), using two sets of grammatical tags; a small set
containing the eleven main grammatical classes and a large set of grammatical categories common
to all languages. The unknown words are tagged using an experimentally proven stochastic
hypothesis that links the stochastic behavior of the unknown words with that of the less probable
known words. A fully automatic training and tagging program has been implemented on an IBM
PC-compatible 80386-based computer. Measurements of error rate, time response, and memory
requirements have shown that the taggers&apos; performance is satisfactory, even though a small
training text is available. The error rate is improved when new texts are used to update the
stochastic model parameters.
</bodyText>
<sectionHeader confidence="0.990294" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999975227272727">
In the natural language processing community, there has been a growing awareness of
the key importance that lexical and corpora resources, especially annotated corpora,
have to play, both in the advancement of research in this area and in the develop-
ment of relevant products. In order to reduce the huge cost of manually creating such
corpora, the development of automatic taggers is of paramount importance. In this
respect, the ability of a tagger to handle both known and unknown words, to improve
its performance by training, and to achieve a high rate of correctly tagged words, is
the criterion for assessing its usability in practical cases.
Several taggers based on rules, stochastic models, neural networks, and hybrid
systems have already been presented for Part-of-speech (POS) tagging. Rule-based
taggers (Brill 1992; Elenius 1990; Jacobs and Zernik 1988; Karlsson 1990; Karlsson et
al. 1991; Voutilainen, Heikkila, and Antitila 1992; Voutilainen and Tapanainen 1993)
use POS-dependent constraints defined by experienced linguists. A small error rate
has been achieved by such systems when a restricted, application-dependent POS set
is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and
Marcinkiewicz (1993) using the Penn Treebank corpus. Nevertheless, if a large POS set
is specified, the number of rules increases significantly and rule definition becomes
highly costly and cumbersome.
Stochastic taggers use both contextual and morphological information, and the
model parameters are usually defined or updated automatically from tagged texts
(Cerf-Danon and El-Beze 1991; Church 1988; Cutting et al. 1992; Dermatas and Kokki-
nakis 1988, 1990, 1993, 1994; Garside, Leech, and Sampson 1987; Kupiec 1992; Maltese
</bodyText>
<note confidence="0.7098295">
* Department of Electrical Engineering, Wire Communications Laboratory (WCL), University of Patras,
265 00 Patras, Greece. E-mail: dermatas@wdee.upatras.gr.
@ 1995 Association for Computational Linguistics
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.99985331372549">
and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo,
Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are
preferred when tagged texts are available for training, and large tagsets and multilin-
gual applications are involved. In the case where additionally raw untagged text is
available, the Maximum Likelihood training can be used to reestimate the parameters
of HMM taggers (Merialdo 1994).
Connectionist models have been used successfully for lexical acquisition (Eineborg
and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990).
Correct classification rates up to 96.4 percent have been achieved in the latter case by
testing on the Teleman Swedish corpus. On the other hand, a time-consuming training
process has been reported.
Recently, several solutions to the problem of tagging unknown words have been
presented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991). Hypotheses
for unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Maltese
and Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback
1993; Elenius 1990) have been applied to unlimited vocabulary taggers. In taggers that
are based on hidden Markov models (HMM), parameters of the unknown words are
estimated by taking into account morphological information from the last part of the
word (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991). Accurate tagging of
seven European languages has been achieved in the first case (error rates of 3-13 per-
cent for a detailed POS set), but an enormous amount of training text is required
for the estimation of the parameters for unknown words. Similar results have been
reported by Maltese and Mancini (1991) for the Italian language. Weischedel et al.
(1993) have used four categories of word morphology, such as inflectional endings,
derivational endings, hyphenation, and capitalization. For the case in which only a
restricted training text is available, a simple, language- and tagset-independent HMM
tagger has been presented by Dermatas and Kokkinakis (1993), where the HMM pa-
rameters for the unknown words are estimated by assuming that the POS probability
distribution of the unknown words and the POS probability distribution of the less
probable words in the small training text are identical.
In this paper, five natural language stochastic taggers that are able to predict POS
of unknown words are presented and tested following the process of developing anno-
tated corpora (the most recently fully tagged and corrected text is used to update the
model parameters). Three stochastic optimization criteria and seven European lan-
guages (Dutch, English, French, German, Greek, Italian and Spanish) and two POS
sets are used in the tests. The set of main grammatical classes and an extended set
of detailed grammatical categories is the same in all languages. The testing material
consists of newspaper texts with 60,000-180,000 words for each language and an En-
glish EEC-law text with 110,000 words. This material was assembled and annotated
in the framework of the ESPRIT-291 /860 project &amp;quot;Linguistic Analysis of the European
Languages.&amp;quot; In addition, we present transformations of the taggers&apos; calculations to a
fixed-point arithmetic system, which are useful for machines without floating-point
hardware.
The taggers handle both lexical and tag transition information, and without per-
forming morphological analysis can be used to annotate corpora when small training
texts are available. Thus, they are preferred when a new language or a new tagset
is used. When the training text is adequate to estimate the tagger parameters, more
efficient stochastic taggers (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991;
Weischedel et al. 1993) and training methods can be implemented (Merialdo 1994).
The structure of this paper is as follows: in Section 2 the stochastic tagging models
are presented in detail. In Section 3 the influence of the training text errors and the
</bodyText>
<page confidence="0.995989">
138
</page>
<note confidence="0.900614">
Dermatas and Kokkinakis Stochastic Tagging
</note>
<bodyText confidence="0.9991515">
sources of stochastic tagger errors are discussed, followed, in Section 4, by a short pre-
sentation of the implementation. In Section 5, statistical measurements on the corpora
and a short description of the taggers&apos; performance is given. Detailed experimental
results are included in Appendices A and B.
</bodyText>
<sectionHeader confidence="0.863063" genericHeader="method">
2. Stochastic Tagging Models
</sectionHeader>
<bodyText confidence="0.999469333333333">
A stochastic optimal sequence of tags T, to be assigned to the words of a sentence
W, can be expressed as a function of both lexical P(W T) and language model P(T)
probabilities using Bayes&apos; rule:
</bodyText>
<equation confidence="0.9890465">
T, = argmax P(T W) = argmax P(W IT) *P(T) = argmax P(W T) * P(T) (1)
P(W)
</equation>
<bodyText confidence="0.9969325">
Several assumptions and approximations on the probabilities P(W I T) and P(T)
lead to good comprises concerning memory and computational complexity.
</bodyText>
<subsectionHeader confidence="0.661761">
2.1 Hidden Markov Model (HMM) Approach
</subsectionHeader>
<bodyText confidence="0.971009833333333">
The tagging process can be modeled by an HMM by assuming that each hidden tag
state produces a word in the sentence, each word w, is uncorrelated with neighboring
words and their tags, and each tag is probabilistic dependent on the N previous tags
only.
2.1.1 Most probable tag sequence (HMM-TS). The optimal tag sequence for a given
observation sequence of words is given by the following equation:
</bodyText>
<equation confidence="0.891303333333333">
T(HMM-TS)
= argmax P( ) JJ P(ti I P(t, 17_1, ,ti_N) H P(wi ti)
t, ..... tm i=2 i=N+1 i=1
</equation>
<bodyText confidence="0.991013571428572">
(2)
where M is the number of words in the sentence W.
The optimal solution is estimated by the well-known Viterbi algorithm. The first-
(Rabiner 1989) and second- (He 1988) order Viterbi algorithms have been presented
elsewhere. Recently, Tao (1992) described the Viterbi algorithm for generalized HMMs.
2.1.2 Most probable tags (HMM-T). The optimal criterion is to choose the tags that
are most likely to be computed independently at each word event:
</bodyText>
<equation confidence="0.855660833333333">
(HMMT) i 1,M (3)
T, - = {tio, argmax P(ti W)},ti
The optimum tag t,, is estimated using the probabilities of the forward-backward
algorithm (Rabiner 1989):
t,„ = argmax P(t„W) = argmax P(t/, wi, • • • , )P(w,+1,.. .,wm I t,) (4)
t, t,
</equation>
<bodyText confidence="0.998354833333333">
The probabilities in equation 4 are estimated recursively for the first- (Rabiner
1989) and second-order HMM (Watson and Chung 1992).
The main difference between the optimization criteria in 2.1.1 and that in 2.1.2
results from the definition of the expected correct tagging rate; the HMM-TS model
maximizes the correctly tagged sentences, while the HMM-T model maximizes the
correctly tagged words.
</bodyText>
<page confidence="0.996808">
139
</page>
<note confidence="0.885122">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.992364769230769">
2.1.3 Stochastic hypothesis for the unknown words. When a new text is processed,
some words are unknown to the tagger lexicon (i.e. they are not included in the training
text). In this case, in order to use the forward-backward and the Viterbi algorithm we
must estimate the unknown word&apos;s conditional probabilities P(w I t). Methods for the
estimation of these probabilities have already been proposed (e.g. the use of word
endings morphology). Nevertheless, these methods fail if only a small training text is
available because of the huge number of events not occurring in this text, such as pairs
of tags and word endings. To address the above problem we have approximated the
conditional probabilities of the unknown word tags by the conditional probabilities of
the less probable word tags, i.e. tags of the words occurring only once. In the following
we demonstrate experimentally that this approximation is valid and independent of
the training text size.
Figures 1 and 2 show the probability distributions of the tags in the training text
(known words) and that of the words occurring only once in this text for the English
and French language, respectively. Furthermore, the tags&apos; probability distribution of
the words that are not included in the training text and are characterized as unknown
words is shown. This distribution is measured in a different open testing text, i.e.
a text that may include both known and unknown words. The measurements were
carried out on newspaper text and split into two parts of the same size—the training
and the open testing text. Each part contained 90,000 words for the English text and
50,000 words for the French text. In this experiment, a tagset comprising the main
grammatical categories was used: Verb (Ver), Noun (Nou), Adjective (Adj), Adverb
(Adv), Pronoun (Pro), Preposition (Pre), Article/Determiner (A-D), Conjunction (Con),
Particle (Par), Interjection (Int), Miscellaneous (Mis; i.e., tags that cannot be classified
in the previous categories).
This experiment has two significant results:
</bodyText>
<listItem confidence="0.820369888888889">
a. The probability distribution of the tags of unknown words is significantly different
from the distribution for known words, while it is very close to the probability
distribution of the tags of the less probable known words both in the English
and French text.
b. A number of closed and functional grammatical classes has very low probability
for both unknown and words occurring only once, e.g., the tags article,
determiner, conjunction, pronoun, miscellaneous in English text, and
article, determiner, conjunction, pronoun, interjection and miscellaneous
in French text.
</listItem>
<bodyText confidence="0.999152076923077">
In the English text, verbs, adjectives and conjunctions are more frequent than in
the French text. On the other hand, prepositions in the French text have a 0.05 greater
probability, which is also the most significant difference between the distributions of
the two languages. Prepositions in the words occurring only once and in unknown
words are minimal in the English text, while in the French text one out of ten unknown
words is a preposition. The text coverage by prepositions is 11.2 percent for the English
and 16.2 percent for the French corpus. This difference increases significantly in the
lexicon coverage: 0.47 percent for the English and 1.54 percent for the French lexicon.
In Figures 3 and 4, the results of chi-square tests that measure the difference
between the probability distribution of the tags of the less probable words and that
of the unknown words are shown. Various sizes of training text and two sets of
grammatical categories, the main set (11 classes) and an extended set (described in
detail in Section 5) were used.
</bodyText>
<page confidence="0.981267">
140
</page>
<figure confidence="0.992304214285714">
Dermatas and Kokkinakis
Stochastic Tagging
0,7
—A-- Known words
—111— Unknown words
—A—Words occurring only
once
0,4
2 0,3
0,2
0,1
0
Nou Ver Mis Pre A-D Adj Pro Adv Con Par Int
Grammatical class
</figure>
<figureCaption confidence="0.889519">
Figure 1
</figureCaption>
<bodyText confidence="0.5932285">
Distribution of the main grammatical classes of the known and unknown words and the
words occurring only once in English text.
</bodyText>
<subsectionHeader confidence="0.37332575">
—A—Known words
--E— Unknown words
—A—Words occurring only
once
</subsectionHeader>
<figure confidence="0.948291">
0,6
0,5
0,7
0,6
0,5
&gt;.
tz-a. 0,4
:a
.0
2 0,3
0,2
0,1
0
Nou Ver Mis Pre A-D Adj Pro Adv Con Par Int
Grammatical class
</figure>
<figureCaption confidence="0.811749">
Figure 2
</figureCaption>
<bodyText confidence="0.750935">
Distribution of the main grammatical classes of the known and unknown words and the
words occurring only once in French text.
</bodyText>
<page confidence="0.990771">
141
</page>
<note confidence="0.463414">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.890742243902439">
Specifically, the grammatically labeled text of 180,000 word entries of the English
language was separated into two parts: the training text, where the tag probabilities
distribution of the less probable words was estimated, and the open testing text, where
the tag probabilities distribution of the unknown words was measured. Multiple chi-
square experiments were carried out by transferring successively a portion of 30,000
words from the open testing text to the training text and by modifying the word
occurrence threshold from 1 to 15 in order to determine the experimentally optimal
threshold. Words having an occurrence below or equal to this threshold in the training
text are counted as less probable words. The results of the tests shown in Figures 3 and
4 include threshold values up to 15 because the difference between the distributions
for values greater than 15 increases significantly.
As shown in the above figures, the close relation between the tested probabil-
ity distributions is evident for all sizes of training and testing text. Furthermore, we
observe that:
a. The chi-square distance between the tag probability distributions is
minimized for low values of the word occurrence threshold. In the tagset
of main grammatical classes, this distance is minimized for threshold
values less than three, four, or five, depending on the training text size.
In the extended set of grammatical classes the distance is minimized in
all cases for the threshold value one; i.e., when only the words occurring
once in the training text are regarded as less probable words.
b. In the English text the chi-square distance between the tag probability
distributions is minimized for 120,000 words training text for the set of
main grammatical classes and for 60,000 words for the extended set. The
same results are measured in the French text.
c. There is no significant variation in the chi-square test results for
additional training text.
d. The closed and functional grammatical classes can be estimated
automatically as the less probable grammatical classes of the less
probable words in the tagged text. (The manual definition process is
time-consuming when a set of detailed grammatical classes is used).
e. The probability distribution of some grammatical classes of the unknown
words changes significantly when the size of the training text is
increased. These changes can be measured in the training text from the
tags&apos; distribution of the less probable words.
Similar results have been achieved by testing the Dutch, German, Greek, Italian,
and Spanish texts, both with the tagset of the main grammatical categories and with
the common extended set of grammatical categories.
Based on the above we can complete both optimization criteria of the HMM for-
mulation, given in 2.1.1 and 2.1.2, by calculating the conditional probability of the
unknown word tags using Bayes&apos; rule:
</bodyText>
<equation confidence="0.7083635">
P(Unknown word I ) = (5)
P(t, I Unknown word)P(Unknown word)
t,
P(ti)
P(t, I Less probable word)P(Unknown word)
P(t)
</equation>
<page confidence="0.985596">
142
</page>
<figure confidence="0.91362">
Dermatas and Kokkinakis Stochastic Tagging
Tagset of Main Grammatical Classes
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Word Occurrence Threshold
</figure>
<figureCaption confidence="0.840062">
Figure 3
</figureCaption>
<bodyText confidence="0.873117">
Chi-square test for the main grammatical classes&apos; distribution of the unknown and the less
probable words in the English text for various training text sizes.
</bodyText>
<subsectionHeader confidence="0.710364">
Extended tagset of Grammatical classes
</subsectionHeader>
<figure confidence="0.990337482758621">
—•— 30K
—IN— 60K
—A-90K
—X— 120K
—X--- 150K
0,035
0,03
0,025
0,02
0,015
0,01
0,14
0,13
0,12
0,11
0,1
0,09
0,08
0,07
0,06
0,05
0,04
—0-30K
—IN— 60K
—A— 90K
—X— 120K
---)1(--- 150K
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Word Occurrence Threshold
</figure>
<figureCaption confidence="0.911853">
Figure 4
</figureCaption>
<footnote confidence="0.758044333333333">
Chi-square test for the distribution of the grammatical tags of the unknown words and the
less probable words in the English text, for the extended tagset of grammatical classes and
various training text sizes.
</footnote>
<page confidence="0.991866">
143
</page>
<note confidence="0.463576">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.9992864">
The probability P(Unknown word) is approximated in open testing texts by mea-
suring the unknown word frequency. Therefore the model parameters are adapted each time
an open testing text is being tagged. The probability P(t I Less probable word) and the
tags probability P(t) are measured in the training text. Finally, each tag-conditional
probability of the unknown word tags is normalized:
</bodyText>
<equation confidence="0.99392">
E p(.; t) + P(Unknown word I ti) = 1, Vi = 1, T (6)
</equation>
<bodyText confidence="0.961254">
where L is the number of the known words and T is the number of tags.
</bodyText>
<subsectionHeader confidence="0.999929">
2.2 Tagging without Lexical Probabilities
</subsectionHeader>
<bodyText confidence="0.999915333333333">
When the corresponding lexical probabilities p(w I t) are not available in the dictionary
that specifies the possible tags for each word, a simple tagger can be implemented by
assuming that each word w, in a sentence is uncorrelated with the assigned tag t,; e.g.,
</bodyText>
<equation confidence="0.950125833333333">
P(wi I tt) = P(wz).
In this case the most probable tag sequence, according to equation 2, is given by:
10
m(MLM) = argmax P(ti ) H P(t I ti-1, .,t1) P(t i t1-11 ti-N) (7)
ti,•..,t m
i=2 i=N +1
</equation>
<bodyText confidence="0.943871333333333">
which is a Nth-order Markovian chain for the language model (MLM).
Taggers based on MLM require the training process to store each tag assigned to
every lexicon entry and to define the unknown word tagset.
</bodyText>
<subsubsectionHeader confidence="0.867505">
2.2.1 Stochastic hypothesis for the unknown words. The unknown word tagset is
</subsubsectionHeader>
<bodyText confidence="0.999882777777778">
defined by the selection of the most probable tags that have been assigned to the less
probable words of the training text. In this way the unknown words&apos; ambiguity is
decreased significantly. The word occurrence threshold used to define the less prob-
able words and a tag probability threshold used to isolate the less probable tags are
estimated experimentally.
Extensive experiments have shown insignificant differences in the tagging error
rate when alternative word occurrence thresholds have been tested. The best results
are obtained when values less than 10 are used. In this paper the word occurrence
threshold has been set to one in all experiments.
</bodyText>
<sectionHeader confidence="0.966371" genericHeader="method">
3. Tagger Errors
</sectionHeader>
<subsectionHeader confidence="0.999632">
3.1 Errors in the Training Text
</subsectionHeader>
<bodyText confidence="0.99991">
Taggers based on the HMM technique compensate for some serious training problems
inherent in the MLM approach. The most important one is the presence of errors in
the training text. This situation appears when uncorrected tags or analysts&apos; mistakes
remain in the text used to estimate the stochastic model parameters. These errors
generate tag assignments that are not valid. In MLM taggers these tags are equally
weighted to the correct ones. In contrast, in HMM taggers invalid assignments are
biased by the very low value of the corresponding conditional probability of the tags
(the wrong tag rarely appears in the specific word environment), which decreases the
overall probability for incorrect tag assignments.
</bodyText>
<page confidence="0.973924">
144
</page>
<note confidence="0.449129">
Dermatas and Kolddnakis Stochastic Tagging
</note>
<bodyText confidence="0.99984425">
Another important issue concerns the HMM ability to handle lexicon information,
e.g., to find how frequently the tags have been assigned to each lexicon entry. In some
languages, taggers based on HMMs almost reduce the prediction error to the half
compared to the MLM approach.
</bodyText>
<subsectionHeader confidence="0.999877">
3.2 Tagger prediction errors
</subsectionHeader>
<bodyText confidence="0.99828">
Generally, tagger errors can be classified into three categories:
</bodyText>
<listItem confidence="0.993908333333333">
a. Errors due to inadequate training data. When the model parameters are
estimated from a limited amount of training data, tagging errors appear
because of unknown or inaccurately estimated conditional probabilities.
Various interpolation techniques have been proposed for the estimation
of the model parameters for unseen events or to smooth the model
parameters (Church and Gale 1991; Essen and Steinbiss 1992; Jardino
and Adda 1993; Katz 1987; McInnes 1992).
b. Errors due to the syntactical or grammatical style of the testing text. This type of
error appears when the testing text has a style unknown to the model
(i.e., a style used in the open testing text, not included in the training
text). It can be reduced by using multiple models that have been
previously trained in different text styles.
c. Errors due to insufficient model hypotheses. In this case the model hypotheses
are not satisfied; e.g., there are strong intra-tag relations in distances
greater than the model order, idiomatic expressions, language dependent
exceptions, etc. A general solution to the variable length and depth of
dependency for HMM has been already proposed (Tao 1992), but has not
been implemented in taggers.
</listItem>
<sectionHeader confidence="0.689455" genericHeader="method">
4. Implementation
</sectionHeader>
<bodyText confidence="0.999975363636364">
In this section we present techniques to speed up the tagging process and avoid un-
derflow or overflow phenomena during the estimation of the optimum solution. These
techniques do not increase the prediction error rate or have only minimal influence
on it, as proven in the experiments.
Two modules consume the majority of the tagger computational time. The first
module extracts from the model parameters the intra-tag and the word-tag conditional
probabilities requested by the second module, which computes the optimum solution
by multiplying the corresponding conditional probabilities. Binary search maximizes
the searching speed of the first module, while the following three transformation
techniques decrease the computing time of the second module, avoid underflow or
overflow phenomena, and use the faster and low-cost fixed-point arithmetic system.
</bodyText>
<subsectionHeader confidence="0.999465">
4.1 Logarithmic Transformation
</subsectionHeader>
<bodyText confidence="0.999967333333334">
The stochastic solutions described by equations 2 and 7 are computed by multiplying
several conditional probabilities. The floating-point multiplications of these probabili-
ties are transformed into an equal number of floating-point additions, by computing
the logarithm of the optimum criterion probability. This technique solves the under-
flow problem which arises when many small probabilities are multiplied, and accel-
erates the tagger response time.
</bodyText>
<page confidence="0.989341">
145
</page>
<note confidence="0.461537">
Computational Linguistics Volume 21, Number 2
</note>
<subsectionHeader confidence="0.994426">
4.2 Fixed-Point Transformation
</subsectionHeader>
<bodyText confidence="0.969700636363636">
The fixed-point transformation converts the floating-point logarithmic additions into
an equal number of fixed-point additions. It is realized by the following quantization
process:
= Round [My, ln(Pmin) (1n(Prnm) — 111(Px))]
Imax
where: Px is a conditional probability, Prnin is the minimum conditional probability in
the model parameter set, &apos;max is the maximum integer of the fixed-point arithmetic
system, M„ is the maximum number of words in a sentence and Round[.] is a quan-
tization function mapping real numbers into the nearest integer.
After the logarithmic and the fixed-point transformation, equations 2 and 7 be-
come:
</bodyText>
<equation confidence="0.985861285714286">
i(HMM —TS) argmax + E I(t I t1_1,..•, tl)
1=2
+ E /(ti Ei(wi ti)
t=N+1 Jr,
4MLA4) argmax + Ei(ti t,_1,. • • , + E .T4, ti_1,. • • , tt_N)
t, ..... tm
1=2 i=N+1
</equation>
<bodyText confidence="0.999962333333333">
The quantization function approximates the computations, producing theoretically dif-
fering solutions. In practice the prediction error differences measured for all languages,
taggers, and tagsets were less than 0.02 percent.
</bodyText>
<subsectionHeader confidence="0.999915">
4.3 Scaling
</subsectionHeader>
<bodyText confidence="0.999990666666667">
The solution obtained by the forward-backward algorithm cannot be logarithmically
transformed because of the presence of summations. It is well known that for HMMs
the forward and backward probabilities tend exponentially to zero. The scaling process
introduced in this case multiplies the forward and backward probabilities by a scaling
factor at selective word events in order to keep the computations within the floating-
point dynamic range of the computer (Rabiner 1989).
</bodyText>
<subsectionHeader confidence="0.999915">
4.4 Hardware—Software
</subsectionHeader>
<bodyText confidence="0.999965928571429">
The taggers have been realized under MS-DOS using a 32-bit C compiler. The lexicon
size is limited by the available RAM. A mean value of 35 bytes per word is allocated.
The tagger speed exceeds the rate of 500 word/sec in a 80386 (33MHz) for all languages
and tagsets in text with known words. A maximum memory requirement of 930Kb
has been measured in the experiments described in this paper.
A set of symbols and keywords (a sentence separators set) and the maximum
length of a sentence are the only manually defined parameters when the HMM taggers
are applied.
In the MLM taggers, the word occurrence threshold that isolates the less probable
words and the tag probability threshold used to reject the less probable tags from the
unknown words tagset are the manually defined parameters.
The training process has been designed to estimate or update the model param-
eters from fully tagged text without any manual intervention. Therefore, frequency
measurements are defined or updated as model parameters instead of conditional
</bodyText>
<figure confidence="0.460489">
(8)
</figure>
<page confidence="0.954331">
146
</page>
<note confidence="0.717069">
Dermatas and Kokkinakis Stochastic Tagging
</note>
<tableCaption confidence="0.5939315">
Table 1
Size of the corpora.
Text Dutch English French German Greek Italian Spanish
Newspaper 110,000 180,000 100,000 100,000 120,000 160,000 60,000
EEC-Law 110,000
Table 2
</tableCaption>
<table confidence="0.382187666666667">
ESPRIT 291/860: Project partners.
Country Partner
England Acorn Computers Limited
France Centre National de la Recherche Scientifique (CNRS), LIMSI Division
Germany Ruhr - Universitaet Bochum, Lehrstuhl fur Allgemeine Elektrotechnik
und Akustik
</table>
<affiliation confidence="0.9006525">
Greece University of Patras, Wire Communications Laboratory (WCL), Speech and
Language Group
Italy Ing. C. Olivetti &amp; C., S.p.A.
Italy Centro Studi Applicazioni in Tecnologie Avanzate - CSATA
Netherlands Katholieke Universiteit Nijmegen, Dienst A-Faculteiten
Spain Universidad National de Educacion a Distancia (UNED), Madrid
</affiliation>
<bodyText confidence="0.9641445">
probabilities that are computed afterwards by using the corresponding relative fre-
quencies.
</bodyText>
<sectionHeader confidence="0.479498" genericHeader="evaluation">
5. Performance of the Systems
</sectionHeader>
<subsectionHeader confidence="0.999045">
5.1 Taggers
</subsectionHeader>
<bodyText confidence="0.9999898">
Five taggers have been realized and tested using bi-POS and tri-POS transition prob-
abilities. Specifically, the first- and the second-order MLM (MLM1 and MLM2, re-
spectively), the first- and the second-order HMM of the most probable tag sequence
criterion (HMM-TS1 and HMM-TS2, respectively), and the first-order HMM of the
most probable tag criterion (HMM-T1) have been realized.
</bodyText>
<subsectionHeader confidence="0.999737">
5.2 Corpora
</subsectionHeader>
<bodyText confidence="0.9990061">
The tagger performance has been measured in extensive experiments carried out on
corpora of seven languages, English, Dutch, German, French, Greek, Italian and Span-
ish, annotated according to detailed grammatical categories. In Table 1, the type and
the size of these corpora is shown. They are part of corpora selected in the framework
of the ESPRIT-I project 291/860: &amp;quot;Linguistic Analysis of the European Languages&amp;quot;
(1985-1989) by the project partners (Table 2) and annotated by using semi-automatic
taggers. Manual correction was performed by experienced, native analysts for each
language separately. In all languages the entries were tagged as they appeared in the
text. In the German corpus, for example, where multiple words are concatenated, the
words were not separated.
</bodyText>
<subsectionHeader confidence="0.990627">
5.3 Tagsets
</subsectionHeader>
<bodyText confidence="0.9941285">
Two sets of grammatical tags were isolated from a unified set of grammatical categories
defined in the ESPRIT I project 291/860 (ESPRIT-860, Internal report, 1986):
</bodyText>
<page confidence="0.986294">
147
</page>
<note confidence="0.437619">
Computational Linguistics Volume 21, Number 2
</note>
<tableCaption confidence="0.774751">
Table 3
Extended set of grammatical categories.
Main grammatical categories Detailed grammatical information
</tableCaption>
<table confidence="0.9998692">
Adjective, Noun, Pronoun Regular base comparative superlative interrogative person
Adverb number case
Article, Determiner, Preposition Regular base comparative superlative interrogative
Verb Person number case
Tense voice mood person number case
</table>
<tableCaption confidence="0.985897">
Table 4
</tableCaption>
<table confidence="0.80884375">
Number of grammatical tags.
Text Dutch English French German Greek Italian Spanish
Main set 9 News: 10, Law: 10 10 11 11 10 10
Extended set 50 News: 43, Low: 36 14 116 443 121 121
</table>
<tableCaption confidence="0.660295">
Table 5
</tableCaption>
<table confidence="0.9801065">
Word ambiguity in the newspaper corpus.
Tagset English Dutch German French Greek Italian Spanish
Main set 1.336 1.111 1.3 1.69 1.209 1.62 1.197
Extended set 1.417 1.291 1.878 1.705 1.855 1.729 1.25
</table>
<listItem confidence="0.695913">
a. A common tagset of 11 main grammatical categories for each language,
as described in 2.1.3.
b. An extended set including common categorization of the grammatical
</listItem>
<bodyText confidence="0.96149">
information for all languages, as shown in Table 3. In some languages a
number of grammatical categories is not applicable. The depth of
grammatical analysis and the grammatical structure of each language
produce a different number of POS tags. In Table 4 the number of POS
tags used for each language and each set of grammatical categories is
shown.
</bodyText>
<subsectionHeader confidence="0.944384">
5.4 Corpus Ambiguity
</subsectionHeader>
<bodyText confidence="0.999156846153846">
The corpus ambiguity was measured by the mean number of possible tags for each
word of the corpus for both sets of grammatical tags (Table 5). The most ambiguous
texts are the French, Italian, and English in the tagset of main grammatical classes and
the German, Greek, Italian, and French in the extended set of grammatical categories.
In Figure 5 the percent occurrence of unknown words in an open testing text of
10,000 words is shown versus the size of the training text.
The Italian and Greek corpora have the greatest number of unknown words fol-
lowed by the Spanish corpus (for the available results with restricted training text).
Taking into account the word ambiguity in the training text (Table 5), the occur-
rence of unknown words in the open testing text (Figure 5), and the hypothesis that
the unknown word tagset and the application tagset are the same, the ambiguity of
the open testing corpus for both sets of grammatical categories was computed for a
50,000-word training corpus (Table 6).
</bodyText>
<page confidence="0.942888">
148
</page>
<figure confidence="0.998933866666667">
Stochastic Tagging
Dermatas and Kokkinakis
35 —
25
—0—Dutch
—MI—English
French
—X— German
—)k— Greek
Italian
—I—Spanish
15
5 I I I I I I I I I I I I
2 3 4 5 6 7 8 9 10 11 12 13 14 15
Size of training text (*10K words)
</figure>
<figureCaption confidence="0.895661">
Figure 5
</figureCaption>
<tableCaption confidence="0.656187">
Percentage of unknown words in open testing text of 10,000 words for various sizes of the
training text.
Table 6
</tableCaption>
<table confidence="0.9468655">
Corpus ambiguity in newspaper open testing text.
Tagset English Dutch German French Greek Italian Spanish
Main set 8.75 7.83 9.9 9.19 9.32 8.5 8.5
Extended set 37.03 42.78 103.07 12.8 367.25 99.86 100.69
</table>
<bodyText confidence="0.99921975">
For the set of main grammatical classes the ambiguity of the open testing corpus
is more or less the same for all languages, varying from a minimum of 7.83 tags per
word in the Dutch text to a maximum of 9.32 in the Greek corpus. For the extended
set of grammatical categories three types of corpora can be distinguished:
</bodyText>
<listItem confidence="0.710758333333333">
a. The most ambiguous is the corpus of the Greek language, because of the
great number of grammatical tags (443) and the strong presence of
unknown words in the open testing text.
b. In the German, Spanish, and Italian texts the same ambiguity is
measured.
c. The least ambiguous are the Dutch and French texts.
</listItem>
<bodyText confidence="0.999977857142857">
Taking into account the previous results, it is important to note that the great dif-
ferences between languages in text ambiguity, in the presence of unknown words and
in the statistics of the grammatical categories, e.g. the different occurrence of preposi-
tions in English and French corpora, prevent a direct comparison of languages from
the taggers&apos; error rate. Apart from a few obvious observations given in Section 5.7,
such a comparison would require a detailed examination of the corpora and the tag-
gers&apos; errors by experienced linguists. Therefore, the prediction error rates presented in
</bodyText>
<page confidence="0.992713">
149
</page>
<note confidence="0.425394">
Computational Linguistics Volume 21, Number 2
</note>
<tableCaption confidence="0.956102">
Table 7
</tableCaption>
<table confidence="0.812902333333333">
Lexicon size for 100,000-word training text.
Language Dutch English French German Greek Italian
Lexicon size 13,700 12,200 13,500 8,900 17,400 15,300
</table>
<bodyText confidence="0.9806195">
this paper should be regarded only as indication of the probabilistic taggers&apos; efficiency
in each separate language when small training texts are available.
</bodyText>
<subsectionHeader confidence="0.907942">
5.5 Experiments
</subsectionHeader>
<bodyText confidence="0.999993666666667">
The corpora were divided into 10,000-word entries. All parts except the last one were
used to create (initially) and update the model parameters successively. The last part
was tagged each time after the model parameters were updated, giving results of the
tagger performance on open testing text. The influence of the application tagset on the
tagger performance was measured by testing the two totally different tagsets described
in Section 5.3.
The experimental process was repeated for each language, tagset and tagger.
Thus a total number of 2 (tagsets) * 5 (taggers) * [7 (languages) +1 (Test on English
EEC-law text)] = 80 experiments was carried out.
</bodyText>
<subsectionHeader confidence="0.998795">
5.6 Tagger Speed and Memory Requirements
</subsectionHeader>
<bodyText confidence="0.999961">
In Figures 6 and 7 the tagger speed and the memory requirements after the last mem-
ory adaptation process are presented for all taggers and languages, and for the ex-
tended tagset.
The Greek and Italian corpora have a great number of lexical entries (different
word forms) for the same amount of 100,000-word training text, as shown in Table 7.
As a result these taggers require more memory (Figure 7). In contrast, the small size
of the German lexicon decreases the required memory.
Tagger speed is closely related to the corpus ambiguity (Table 6). The ambiguity
of the Greek corpus is more than three times greater than the next one, the German
corpus.
The significant influence of the training text size on tagger speed is proven by
comparing the experimental results in the English corpus (newspaper and EEC-Law).
When the taggers are trained using the 170,000 words of the English newspaper corpus,
a greater number of lexicon entries and a greater number of transition probabilities
(Figure 7) is measured than in the case of the EEC-law corpus (100K words training
text). The model becomes more complex, but tagger speed is slightly higher because of
the greater size of the training text, which reduces the presence of unknown words in
the testing text. Generally, tagger speed increases when the training text is increased.
</bodyText>
<subsectionHeader confidence="0.996857">
5.7 Tagger Error Rate
</subsectionHeader>
<bodyText confidence="0.99984575">
The actual tagger error rates for all experiments are given in Appendices A and B. In
this section we present a discussion of these error rates.
The error rate depends strongly on the test text and language, and the type and
size of the tagset. The worst results have been obtained for the Greek language because
of its significantly greater ambiguity, the number of tags (requiring significantly greater
training text), and its freer syntax.
In the main category of tagset experiments, the model parameters for the MLM
systems are estimated accurately when the training text exceeds 50,000-90,000 words,
</bodyText>
<page confidence="0.947591">
150
</page>
<figure confidence="0.8240984">
Dermatas and Kokkinalcis Stochastic Tagging
Language
Figure 6
Tagger speed after the last adaptation process for the extended set of grammatical categories.
Language
</figure>
<figureCaption confidence="0.681508">
Figure 7
Tagger memory requirements for the extended set of grammatical categories.
</figureCaption>
<figure confidence="0.993076705882353">
Eng
Eng-
Law
Fre
Ger
Gre
Ita
Spa
—I.— MLM1
—11-- M LM 2
—A-- HMM-TSI
—X— HMM-TS2
—X— HMM-T1
Words/sec
--41-- MLM1
—.--- MLM2
—A— HMM-TS1
--X-- HMM-TS2
Ita
Spa
400
300 7
200 - I I II
Dut Eng Eng Fre Ger Gre
Law
\
800 —
700 -:
600
co
S500 7
151
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
Size of training text (*10K words)
</figure>
<figureCaption confidence="0.993002">
Figure 8
</figureCaption>
<bodyText confidence="0.989936142857143">
Unknown word error rate for the HMM-TS2 tagger and the set of main grammatical
categories.
in contrast to the extended tagset experiments, where a greater-size training text for
the German, Greek, and Spanish languages is required. This phenomenon becomes
stronger in taggers based on the HMM where the accuracy of the P(w I t) estimation is
proportional to the word and the tag frequency of occurrence in the training text. Thus,
for all tagsets and languages a larger training text is required in order to minimize the
error rate.
The taggers based on the HMM reduce the prediction error almost to half in
comparison to the same order taggers based on MLM. Strong dependencies on the
language and the estimation accuracy of the model parameters influence this reduction.
The alternative HMM solutions give trivial performance differences, confirming recent
results obtained in the Treebank corpus by using an HMM tagger (Merialdo 1991).
Concerning the performance of the taggers in unknown words, we present in Fig-
ure 8 as an example the HMM-TS2 error rate for the tagset of the main grammatical
categories, which is also the worst case for this set of grammatical categories. Gener-
ally the error rate decreases when the training text is increased. The stochastic model
is successful for only half of the unknown words for the Italian text and for approx-
imately two out of three unknown words for the English text. In all other languages
the HMM-TS2 tagger gives the correct solution for three out of four unknown words.
Similar results are achieved when the extended set of grammatical categories is
tested. In this case the unknown word error rate increases about 10-20 percent for
all the languages except the Greek language. In the Greek text the error rate reaches
approximately 65 percent when 100,000-word text is used to define the parameters of
the HMM.
The unknown words, which initially cover about 25-35 percent of the text, are
reduced to 8-15 percent when all the available text is used as training data. In the ma-
jority of the experiments, the tagger error rate decreases when new text updates the
</bodyText>
<figure confidence="0.9808305">
Computational Linguistics Volume 21, Number 2
- Dutch
-U-English
-A-French
--X- German
-I-Greek
-9- lta lian
- Spanish
I; 35
25
15 15
45
</figure>
<page confidence="0.953305">
152
</page>
<note confidence="0.668889">
Dermatas and Kokkinakis Stochastic Tagging
</note>
<bodyText confidence="0.99978225">
model parameters. Trivial differences of the tagger learning rates between languages
and tagsets show the efficiency of the training method in estimating the model transi-
tion probabilities for the tested languages and the validity of the stochastic hypothesis
for the unknown words.
</bodyText>
<sectionHeader confidence="0.976323" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.999404857142857">
In this paper five automatic, stochastic taggers that are able to tag unknown words
have been presented. The taggers have been tested in newspaper corpora of seven
European languages and an EEC-law text of the English language using two sets of
grammatical categories. When new training text updates the model parameters, the
tagging error rate changes as expected: in text with unknown words a lower error rate
is measured, proving the efficiency of the relative frequencies learning method and
the validity of the hypothesis for the unknown words&apos; stochastic behavior.
</bodyText>
<page confidence="0.99398">
153
</page>
<figure confidence="0.966600066666667">
Computational Linguistics Volume 21, Number 2
Appendix A: Tests in the Main Grammatical Categories Set
Error rate (%) Prediction Error for the Dutch language
Size of the training text (* 10 Kwords)
Tagging system
MLM1
MLM2
HMM-TS1
HMM-TS2
HMM-T1
Main grammatical classes
8 -
7 -
6 -
5 -
4 -
1 2 3 4 5 6 7 8 9 10
3
Error rate (%)
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
Prediction Error for the English language
Main grammatical classes
Tagging system
MLM1
MLM2
HMM-TS1
HMM-TS2
HMM-T1
.e.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
Size of the training text (*10 Kwords)
</figure>
<page confidence="0.971195">
154
</page>
<figure confidence="0.887202943820224">
Dermatas and Kokkinakis Stochastic Tagging
Error rate (%) Prediction Error for the English EEC-law text
11 Main grammatical classes
10 -
9 -
Size of the training text (*10 Kwords)
Error rate (%) Prediction Error for the French language
16- Main grammatical classes
15 -
14 -
13 -
--
---------
12-
11-
10-
9-
8-
-
7-
,
6- ___
5 -
4
3 4 5 7 8 9
Size of the training text (*10 Kwords)
8 -
7 -
6 -
5 -
4-
3 -
a
1 2 3 4 5 6 7 8 9 10
2
Tagging system
MLM1
MLM2
HMM-TS1
HMM-TS2
HMM-T1
Tagging system
MLM1
MLM2
HMM-TS1
HMM-T82
HMM-T1
155
Computational Linguistics Volume 21, Number 2
Error rate (%) Prediction Error for the German language
16 Main grammatical classes
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
-
1 2 3 4 5 6 7 8 9
Tagging system
MLM1
MLM2
HMM-TS1
HMM-TS2
HMM-T1
Size of the training text (*10 Kwords)
Error rate (%) Prediction Error for the Greek language Tagging system
24 - Main grammatical classes
22 - MLM1
20- MLM2
18- • HMM-TS1
16- q HMM-TS2
\ -0-- HMM-T1
14-
12-
-5-
10-
8-
6
4 -
2
Size of the training text (*10 Kwords)
1 2 3 4 5 6 7 8 9 10 11
156
Dermatas and Kokkinakis Stochastic Tagging
Error rate (%) Prediction Error for the Italian language
Tagging system
MLM1
- MLM2
HMM-TS1
-0- HMM-TS2
- HMM-T1
38 - Main grammatical classes
36 -
34 -
32 -
30 -
28 -
26 -
24 -
22 -
20 -
18 -
16 -
14 -
12 -
10
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Size of the training text (*10 Kwords)
Error rate (%) Prediction Error for the Spanish language
18
17
16
15
14
13
12
11
10
9
8
7
6 -
5
Main grammatical classes
Tagging system
MLM1
- MLM2
HMM-TS1
-0- HMM-TS2
HMM-T1
1 5
Size of the training text (*10 Kwords)
157
Computational Linguistics Volume 21, Number 2
Appendix B: Tests in the Extended Grammatical Categories Set
Error rate (%) Prediction Error for the Dutch language
19 - Extended grammatical classes
18 -
17 -
16 -
15 -
14 -
13 -
12 -
11 -
10 -
9 -
-
1 2 3 4 5 7 10
7 -
6
Tagging system
MLM1
MLM2
HMM-TS1
HMM-TS2
HMM-T1
Size of the training text (*10 Kwords)
Size of the training text (*10 Kwords)
Tagging system
MLM1
MLM2
HMM-TS1
HMM-TS2
HMM-T1
Error rate (%) Prediction Error for the English language
Extended grammatical set
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
</figure>
<page confidence="0.892750625">
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
</page>
<equation confidence="0.8656206">
•.&apos;, \ \
. . ■ %.
.k \ - -
.., ..._ ....._-_____
`.C.
</equation>
<page confidence="0.882401">
158
</page>
<figure confidence="0.980226944444444">
Dermatas and Kokkinakis Stochastic Tagging
Error rate (%) Prediction Error for the English EEC-law text
13- Extended grammatical classes
12 -
11 -
cis
10-
&apos;Or
8
7 &amp;quot; • •
6 -
Tagging system
MLM1
MLM2
HMM-TS1
HMM-TS2
HMM-T1
5 -
4 - 77&apos;77,79
3 -
2
1 2 3 4 5 6 7 8 6 10
Size of the training text (*10 Kwords)
Error rate (%) Prediction Error for the French language
16- Extended grammatical classes
15
14 -
13 -
12 -
_
11 -
10-
9 -
8 -
„.„
1 2 3 4 5 6 8
Size of the training text (*10 Kwords)
7 -
6 -
5 -
4
Tagging system
MLM1
MLM2
HMM-TSI
HMM-TS2
HMM-T1
159
Computational Linguistics Volume 21, Number 2
Error rate (%) Prediction Error for the German language Tagging system
28 - Extended grammatical classes MLM1
26- MLM2
b HMM-TS1
HMM-TS2
HMM-T1
6
14 -
_
16 -
2 3 4 5 6 7 8 9
10-
8
Size of the training text (*10 Kwords)
Size of the training text (*10 Kwords)
24 -
„
22 -
20 -
18 -
Tagging system
MLM1
MLM2
HMM-TS1
HMM-TS2
HMM-T1
1 2 3
10 11
4
5
Prediction Error for the Greek language
Extended grammatical classes
-
160
Stochastic Tagging
Dermatas and Kokkinakis
Error rate (%)
45 -
Prediction Error for the Italian language
Extended grammatical set Tagging system
MLM1
- - MLM2
HMM-TS1
- HMM-TS2
- HMM-T1
40 -
35 -
30 -
25 -
20 - • 77
15 -
10 -
5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Size of the training text (.10 Kwords)
Error rate (%) Prediction Error for the Spanish language
Size of the training text (*10 Kwords)
Extended grammatical classes
22 -
20 -
18 -
16 -
14 -
12 -
10 -
8
4 5
a
Tagging system
MLM1
MLM2
HMM-TS1
HMM-TS2
HMM-T1
161
Computational Linguistics Volume 21, Number 2
References
</figure>
<reference confidence="0.998782128205128">
Brill, E. (1992). &amp;quot;A simple rule-based part of
speech tagger.&amp;quot; In Proceedings, Third
Conference on Applied Natural Language
Processing. Trento, Italy, 152-155.
Cerf-Danon, H., and El-Beze, M. (1991).
&amp;quot;Three different probabilistic language
models: Comparison and combination.&amp;quot;
In Proceedings, International Conference on
Acoustics Speech and Signal Processing,
297-300.
Charniak, E.; Hendrickson, C.; Jacobson, N.;
and Perkowitz, M. (1993). &amp;quot;Equations for
part-of-speech tagging.&amp;quot; In Proceedings,
National Conference on Artificial Intelligence.
Church, K. (1988). &amp;quot;A stochastic parts
program and noun phrase parser for
unrestricted text.&amp;quot; In Proceedings, Second
Conference on Applied Natural Language
Processing. Austin, Texas, 136-143.
Church, K., and Gale, W. (1991). &amp;quot;A
comparison of the enhanced Good-Turing
and deleted estimation methods for
estimating probabilities of English
bigrams.&amp;quot; Computer Speech and Language 5,
19-24.
Cutting, D.; Kupiec, J.; Pederson, J.; and
Sibun, P. (1992). &amp;quot;A practical
part-of-speech tagger.&amp;quot; In Proceedings,
Third Conference on Applied Natural
Language Processing. Trento, Italy, 133-140.
Dermatas, E., and Kokkinakis, G. (1988).
&amp;quot;Semi automatic labelling of Greek texts.&amp;quot;
In Proceedings, Seventh FASE Symposium
SPEECH &apos;88. Edinburgh, 239-245.
Dermatas, E., and Kokkinakis, G. (1993). &amp;quot;A
system for automatic text labelling.&amp;quot; In
Proceedings, Eurospeech-90. Paris, 382-385.
Dermatas, E., and Kokkinakis, G. (1993). &amp;quot;A
fast multilingual probabilistic tagger.&amp;quot; In
Proceedings, Eurospeech-93. Berlin,
1323-1326 (presented also in the
Eurospeech-93 exhibition).
Dermatas, E., and Kokkinakis, G. (1994). &amp;quot;A
multilingual unlimited vocabulary
stochastic tagger.&amp;quot; In Advanced Speech
Applications-European Commission ESPRIT
(1), edited by K. Varghese, S. Pfleger, and
J. Lefevre, 98-106. Springer-Verlag.
Eineborg, M., and Gamback, B. (1993).
&amp;quot;Back-propagation based lexical
acquisition experiments.&amp;quot; In Proceedings,
NeuroNimes: Neural Networks and their
Industrial &amp; Cognitive Applications. Nimes,
169-178.
Elenius, K. (1990).&amp;quot;Comparing a
connectionist and rule based model for
assignment parts-of-speech.&amp;quot; In
Proceedings, International Conference on
Acoustics, Speech and Signal Processing,
597-600.
Elenius, K., and Carlson, R. (1989).
&amp;quot;Assigning parts-of-speech of words from
their orthography using a connectionist
model.&amp;quot; In Proceedings, European Conference
on Speech Communication and Technology.
Paris, 534-537.
Partners of ESPRIT-291/860 (1986).
&amp;quot;Unification of the word classes of the
ESPRIT Project 860.&amp;quot; BU-WKL-0376.
Internal Report.
Essen, U., and Steinbiss, V. (1992).
&amp;quot;Cooccurrence smoothing for statistical
language modelling.&amp;quot; In Proceedings,
International Conference on Acoustics, Speech
and Signal Processing, 161-164.
Garside, R.; Leech, G.; and Sampson, G.
(1987). The Computational Analysis of
English: A Corpus-Based Approach.
Longman.
He, Y. (1988). &amp;quot;Extended Viterbi algorithm
for second order hidden Markov
process.&amp;quot; In Proceedings, International
Conference on Acoustics, Speech and Signal
Processing, 718-720.
Jacobs, P., and Zernik, U. (1988). &amp;quot;Acquiring
lexical knowledge from text: A case
study.&amp;quot; In Proceedings, Seventh National
Conference on Artificial Intelligence. Saint
Paul, Minnesota, 739-744.
Jardino, M., and Adda, G. (1993).
&amp;quot;Automatic word classification using
simulated annealing.&amp;quot; In Proceedings,
International Conference on Acoustics, Speech
and Signal Processing, 41-44.
Karlsson, F. (1990). &amp;quot;Constraint grammar as
a framework for parsing running text.&amp;quot; In
Proceedings, Thirteenth International
Conference on Computational Linguistics.
Helsinki, Finland, 168-173.
Karlsson, F.; Voutilainen, A.; Anttila, A.; and
Heikkila, J. (1991). &amp;quot;Constraint grammar:
A language-independent system for
parsing unrestricted text, with an
application to English.&amp;quot; In Workshop Notes
from the Ninth National Conference on
Artificial Intelligence. Anaheim, California.
Katz, S. (1987). &amp;quot;Estimation of probabilities
from sparse data for the language model
component of a speech recognizer.&amp;quot; IEEE
Trans. on Acoustics, Speech, and Language
Processing, 35(3), 400-401.
Kupiec, J. (1992). &amp;quot;Robust part-of-speech
tagging using a Hidden Markov Model.&amp;quot;
Computer Speech &amp; Language, 6(3), 225-242.
Maltese, G., and Mancini, F. (1991). &amp;quot;A
technique to automatically assign
parts-of-speech to words taking into
</reference>
<page confidence="0.9862">
162
</page>
<note confidence="0.681875">
Dermatas and Kokkinakis Stochastic Tagging
</note>
<reference confidence="0.999782786666666">
account word-ending information
through a probabilistic model.&amp;quot; In
Proceedings, Eurospeech-91, 753-756.
Marcus, M.; Santorini, B.; and
Marcinkiewicz, M. (1993). &amp;quot;Building a
large annotated corpus of English: The
Penn Treebank.&amp;quot; Computational Linguistics,
19(2), 315-330.
McInnes, F. (1992). &amp;quot;An enhanced
interpolation technique for
context-specific probability estimation in
speech and language modelling.&amp;quot; In
Proceedings, International Conference on
Spoken Language Processing, 1491-1494.
Merialdo, B. (1991). &amp;quot;Tagging text with a
probabilistic model.&amp;quot; In International
Conference on Acoustics, Speech and Signal
Processing, 809-812.
Merialdo, B. (1994). &amp;quot;Tagging English text
with a probabilistic model.&amp;quot; Computational
Linguistics, 20(2), 155-171.
Meteer, M.; Schwartz, R.; and Weischedel, R.
(1991). &amp;quot;Empirical studies in part of
speech labelling.&amp;quot; In Proceedings, Fourth
DARPA Workshop on Speech and Natural
Language. Morgan Kaufman.
Nakamura, M.; Maruyama, K.; Kawabata,
T.; and Shikano, K. (1990). &amp;quot;Neural
network approach to word category
prediction for English texts.&amp;quot; In
Proceedings, Thirteenth International
Conference on Computational Linguistics.
Helinski, Finland, 213-218.
Pelillo, W.; Moro, F.; and Refice, M. (1992).
&amp;quot;Probabilistic prediction of
parts-of-speech from spelling using
decision trees.&amp;quot; In Proceedings,
International Conference on Spoken Language
Processing, 1343-1346.
Rabiner, L. (1989). &amp;quot;A tutorial on Hidden
Markov Models and selected applications
in speech recognition.&amp;quot; In Proceedings,
IEEE 77(2), 257-285.
Tao, C. (1992). &amp;quot;A generalisation of discrete
Hidden Markov Model and of Viterbi
algorithm.&amp;quot; Pattern Recognition, 25(11),
1381-1397.
Voutilainen, A., and Tapanainen, P. (1993).
&amp;quot;Ambiguity resolution in a reductionistic
parser.&amp;quot; In Proceedings, Sixth Conference of
the European Chapter of the Association for
Computational Linguistics. Utrecht,
Netherlands, 394-403.
Voutilainen, A.; Heikkila, J.; and Antitila, A.
(1992). &amp;quot;Constraint grammar of English.&amp;quot;
Publication 21, Department of General
Linguistics, University of Helinski,
Helinski, Finland.
Watson, B., and Chung Tsoi, A. (1992).
&amp;quot;Second order Hidden Markov Models
for speech recognition.&amp;quot; In Proceedings,
Fourth Australian International Conference on
Speech Science and Technology, 146-151.
Weischedel, R.; Meteer, M.; Schwartz, R.;
Ramshaw, L.; and Palmucci, J. (1993).
&amp;quot;Coping with ambiguity and unknown
words through probabilistic models.&amp;quot;
Computational Linguistics, 19(2), 359-382.
Wothke, K.; Weck-Ulm, I.; Heinecke, J.;
Mertineit, 0.; and Pachunke, T. (1993).
&amp;quot;Statistically based automatic tagging of
German text corpora with
parts-of-speech—some experiments.&amp;quot;
TR75.93.02-IBM. IBM Germany,
Heidelberg Scientific Center.
</reference>
<page confidence="0.999122">
163
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.847329">
<title confidence="0.9998485">Automatic Stochastic Tagging of Natural Language Texts</title>
<author confidence="0.989732">Evangelos Derma tas George Kokkinakis</author>
<affiliation confidence="0.984473">University of Patras University of Patras</affiliation>
<abstract confidence="0.983476272727273">Five language and tagset independent stochastic taggers, handling morphological and contextual information, are presented and tested in corpora of seven European languages (Dutch, English, French, German, Greek, Italian and Spanish), using two sets of grammatical tags; a small set containing the eleven main grammatical classes and a large set of grammatical categories common to all languages. The unknown words are tagged using an experimentally proven stochastic hypothesis that links the stochastic behavior of the unknown words with that of the less probable known words. A fully automatic training and tagging program has been implemented on an IBM PC-compatible 80386-based computer. Measurements of error rate, time response, and memory requirements have shown that the taggers&apos; performance is satisfactory, even though a small training text is available. The error rate is improved when new texts are used to update the stochastic model parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, Third Conference on Applied Natural Language Processing.</booktitle>
<pages>152--155</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="1941" citStr="Brill 1992" startWordPosition="289" endWordPosition="290">in this area and in the development of relevant products. In order to reduce the huge cost of manually creating such corpora, the development of automatic taggers is of paramount importance. In this respect, the ability of a tagger to handle both known and unknown words, to improve its performance by training, and to achieve a high rate of correctly tagged words, is the criterion for assessing its usability in practical cases. Several taggers based on rules, stochastic models, neural networks, and hybrid systems have already been presented for Part-of-speech (POS) tagging. Rule-based taggers (Brill 1992; Elenius 1990; Jacobs and Zernik 1988; Karlsson 1990; Karlsson et al. 1991; Voutilainen, Heikkila, and Antitila 1992; Voutilainen and Tapanainen 1993) use POS-dependent constraints defined by experienced linguists. A small error rate has been achieved by such systems when a restricted, application-dependent POS set is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and Marcinkiewicz (1993) using the Penn Treebank corpus. Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumb</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Brill, E. (1992). &amp;quot;A simple rule-based part of speech tagger.&amp;quot; In Proceedings, Third Conference on Applied Natural Language Processing. Trento, Italy, 152-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cerf-Danon</author>
<author>M El-Beze</author>
</authors>
<title>Three different probabilistic language models: Comparison and combination.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, International Conference on Acoustics Speech and Signal Processing,</booktitle>
<pages>297--300</pages>
<contexts>
<context position="2735" citStr="Cerf-Danon and El-Beze 1991" startWordPosition="401" endWordPosition="404">nt constraints defined by experienced linguists. A small error rate has been achieved by such systems when a restricted, application-dependent POS set is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and Marcinkiewicz (1993) using the Penn Treebank corpus. Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome. Stochastic taggers use both contextual and morphological information, and the model parameters are usually defined or updated automatically from tagged texts (Cerf-Danon and El-Beze 1991; Church 1988; Cutting et al. 1992; Dermatas and Kokkinakis 1988, 1990, 1993, 1994; Garside, Leech, and Sampson 1987; Kupiec 1992; Maltese * Department of Electrical Engineering, Wire Communications Laboratory (WCL), University of Patras, 265 00 Patras, Greece. E-mail: dermatas@wdee.upatras.gr. @ 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available </context>
</contexts>
<marker>Cerf-Danon, El-Beze, 1991</marker>
<rawString>Cerf-Danon, H., and El-Beze, M. (1991). &amp;quot;Three different probabilistic language models: Comparison and combination.&amp;quot; In Proceedings, International Conference on Acoustics Speech and Signal Processing, 297-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>C Hendrickson</author>
<author>N Jacobson</author>
<author>M Perkowitz</author>
</authors>
<title>Equations for part-of-speech tagging.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings, National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="4052" citStr="Charniak et al. 1993" startWordPosition="594" endWordPosition="597">itionally raw untagged text is available, the Maximum Likelihood training can be used to reestimate the parameters of HMM taggers (Merialdo 1994). Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990). Correct classification rates up to 96.4 percent have been achieved in the latter case by testing on the Teleman Swedish corpus. On the other hand, a time-consuming training process has been reported. Recently, several solutions to the problem of tagging unknown words have been presented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991). Hypotheses for unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Maltese and Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback 1993; Elenius 1990) have been applied to unlimited vocabulary taggers. In taggers that are based on hidden Markov models (HMM), parameters of the unknown words are estimated by taking into account morphological information from the last part of the word (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991). Accurate tagging of seven European languages has been achieved in the fi</context>
</contexts>
<marker>Charniak, Hendrickson, Jacobson, Perkowitz, 1993</marker>
<rawString>Charniak, E.; Hendrickson, C.; Jacobson, N.; and Perkowitz, M. (1993). &amp;quot;Equations for part-of-speech tagging.&amp;quot; In Proceedings, National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, Second Conference on Applied Natural Language Processing.</booktitle>
<pages>136--143</pages>
<location>Austin, Texas,</location>
<contexts>
<context position="2748" citStr="Church 1988" startWordPosition="405" endWordPosition="406">erienced linguists. A small error rate has been achieved by such systems when a restricted, application-dependent POS set is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and Marcinkiewicz (1993) using the Penn Treebank corpus. Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome. Stochastic taggers use both contextual and morphological information, and the model parameters are usually defined or updated automatically from tagged texts (Cerf-Danon and El-Beze 1991; Church 1988; Cutting et al. 1992; Dermatas and Kokkinakis 1988, 1990, 1993, 1994; Garside, Leech, and Sampson 1987; Kupiec 1992; Maltese * Department of Electrical Engineering, Wire Communications Laboratory (WCL), University of Patras, 265 00 Patras, Greece. E-mail: dermatas@wdee.upatras.gr. @ 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available for training,</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, K. (1988). &amp;quot;A stochastic parts program and noun phrase parser for unrestricted text.&amp;quot; In Proceedings, Second Conference on Applied Natural Language Processing. Austin, Texas, 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>W Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.&amp;quot;</title>
<date>1991</date>
<journal>Computer Speech and Language</journal>
<volume>5</volume>
<contexts>
<context position="21405" citStr="Church and Gale 1991" startWordPosition="3410" endWordPosition="3413">o each lexicon entry. In some languages, taggers based on HMMs almost reduce the prediction error to the half compared to the MLM approach. 3.2 Tagger prediction errors Generally, tagger errors can be classified into three categories: a. Errors due to inadequate training data. When the model parameters are estimated from a limited amount of training data, tagging errors appear because of unknown or inaccurately estimated conditional probabilities. Various interpolation techniques have been proposed for the estimation of the model parameters for unseen events or to smooth the model parameters (Church and Gale 1991; Essen and Steinbiss 1992; Jardino and Adda 1993; Katz 1987; McInnes 1992). b. Errors due to the syntactical or grammatical style of the testing text. This type of error appears when the testing text has a style unknown to the model (i.e., a style used in the open testing text, not included in the training text). It can be reduced by using multiple models that have been previously trained in different text styles. c. Errors due to insufficient model hypotheses. In this case the model hypotheses are not satisfied; e.g., there are strong intra-tag relations in distances greater than the model o</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Church, K., and Gale, W. (1991). &amp;quot;A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.&amp;quot; Computer Speech and Language 5, 19-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cutting</author>
<author>J Kupiec</author>
<author>J Pederson</author>
<author>P Sibun</author>
</authors>
<title>A practical part-of-speech tagger.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, Third Conference on Applied Natural Language Processing.</booktitle>
<pages>133--140</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="2769" citStr="Cutting et al. 1992" startWordPosition="407" endWordPosition="410">uists. A small error rate has been achieved by such systems when a restricted, application-dependent POS set is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and Marcinkiewicz (1993) using the Penn Treebank corpus. Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome. Stochastic taggers use both contextual and morphological information, and the model parameters are usually defined or updated automatically from tagged texts (Cerf-Danon and El-Beze 1991; Church 1988; Cutting et al. 1992; Dermatas and Kokkinakis 1988, 1990, 1993, 1994; Garside, Leech, and Sampson 1987; Kupiec 1992; Maltese * Department of Electrical Engineering, Wire Communications Laboratory (WCL), University of Patras, 265 00 Patras, Greece. E-mail: dermatas@wdee.upatras.gr. @ 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available for training, and large tagsets an</context>
</contexts>
<marker>Cutting, Kupiec, Pederson, Sibun, 1992</marker>
<rawString>Cutting, D.; Kupiec, J.; Pederson, J.; and Sibun, P. (1992). &amp;quot;A practical part-of-speech tagger.&amp;quot; In Proceedings, Third Conference on Applied Natural Language Processing. Trento, Italy, 133-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Dermatas</author>
<author>G Kokkinakis</author>
</authors>
<title>Semi automatic labelling of Greek texts.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, Seventh FASE Symposium SPEECH &apos;88. Edinburgh,</booktitle>
<pages>239--245</pages>
<contexts>
<context position="2799" citStr="Dermatas and Kokkinakis 1988" startWordPosition="411" endWordPosition="415">rate has been achieved by such systems when a restricted, application-dependent POS set is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and Marcinkiewicz (1993) using the Penn Treebank corpus. Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome. Stochastic taggers use both contextual and morphological information, and the model parameters are usually defined or updated automatically from tagged texts (Cerf-Danon and El-Beze 1991; Church 1988; Cutting et al. 1992; Dermatas and Kokkinakis 1988, 1990, 1993, 1994; Garside, Leech, and Sampson 1987; Kupiec 1992; Maltese * Department of Electrical Engineering, Wire Communications Laboratory (WCL), University of Patras, 265 00 Patras, Greece. E-mail: dermatas@wdee.upatras.gr. @ 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available for training, and large tagsets and multilingual applications ar</context>
</contexts>
<marker>Dermatas, Kokkinakis, 1988</marker>
<rawString>Dermatas, E., and Kokkinakis, G. (1988). &amp;quot;Semi automatic labelling of Greek texts.&amp;quot; In Proceedings, Seventh FASE Symposium SPEECH &apos;88. Edinburgh, 239-245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Dermatas</author>
<author>G Kokkinakis</author>
</authors>
<title>A system for automatic text labelling.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings, Eurospeech-90.</booktitle>
<pages>382--385</pages>
<location>Paris,</location>
<contexts>
<context position="4169" citStr="Dermatas and Kokkinakis 1993" startWordPosition="609" endWordPosition="612">eters of HMM taggers (Merialdo 1994). Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990). Correct classification rates up to 96.4 percent have been achieved in the latter case by testing on the Teleman Swedish corpus. On the other hand, a time-consuming training process has been reported. Recently, several solutions to the problem of tagging unknown words have been presented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991). Hypotheses for unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Maltese and Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback 1993; Elenius 1990) have been applied to unlimited vocabulary taggers. In taggers that are based on hidden Markov models (HMM), parameters of the unknown words are estimated by taking into account morphological information from the last part of the word (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991). Accurate tagging of seven European languages has been achieved in the first case (error rates of 3-13 percent for a detailed POS set), but an enormous amount of training text is required fo</context>
</contexts>
<marker>Dermatas, Kokkinakis, 1993</marker>
<rawString>Dermatas, E., and Kokkinakis, G. (1993). &amp;quot;A system for automatic text labelling.&amp;quot; In Proceedings, Eurospeech-90. Paris, 382-385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Dermatas</author>
<author>G Kokkinakis</author>
</authors>
<title>A fast multilingual probabilistic tagger.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings, Eurospeech-93.</booktitle>
<location>Berlin,</location>
<note>(presented also in the Eurospeech-93 exhibition).</note>
<contexts>
<context position="4169" citStr="Dermatas and Kokkinakis 1993" startWordPosition="609" endWordPosition="612">eters of HMM taggers (Merialdo 1994). Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990). Correct classification rates up to 96.4 percent have been achieved in the latter case by testing on the Teleman Swedish corpus. On the other hand, a time-consuming training process has been reported. Recently, several solutions to the problem of tagging unknown words have been presented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991). Hypotheses for unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Maltese and Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback 1993; Elenius 1990) have been applied to unlimited vocabulary taggers. In taggers that are based on hidden Markov models (HMM), parameters of the unknown words are estimated by taking into account morphological information from the last part of the word (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991). Accurate tagging of seven European languages has been achieved in the first case (error rates of 3-13 percent for a detailed POS set), but an enormous amount of training text is required fo</context>
</contexts>
<marker>Dermatas, Kokkinakis, 1993</marker>
<rawString>Dermatas, E., and Kokkinakis, G. (1993). &amp;quot;A fast multilingual probabilistic tagger.&amp;quot; In Proceedings, Eurospeech-93. Berlin, 1323-1326 (presented also in the Eurospeech-93 exhibition).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Dermatas</author>
<author>G Kokkinakis</author>
</authors>
<title>A multilingual unlimited vocabulary stochastic tagger.&amp;quot;</title>
<date>1994</date>
<booktitle>In Advanced Speech Applications-European Commission ESPRIT</booktitle>
<volume>1</volume>
<pages>98--106</pages>
<publisher>Springer-Verlag.</publisher>
<note>edited by</note>
<contexts>
<context position="4551" citStr="Dermatas and Kokkinakis 1994" startWordPosition="667" endWordPosition="670"> has been reported. Recently, several solutions to the problem of tagging unknown words have been presented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991). Hypotheses for unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Maltese and Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback 1993; Elenius 1990) have been applied to unlimited vocabulary taggers. In taggers that are based on hidden Markov models (HMM), parameters of the unknown words are estimated by taking into account morphological information from the last part of the word (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991). Accurate tagging of seven European languages has been achieved in the first case (error rates of 3-13 percent for a detailed POS set), but an enormous amount of training text is required for the estimation of the parameters for unknown words. Similar results have been reported by Maltese and Mancini (1991) for the Italian language. Weischedel et al. (1993) have used four categories of word morphology, such as inflectional endings, derivational endings, hyphenation, and capitalization. For the case in which only a restricted training text is available, a simple, lan</context>
<context position="6881" citStr="Dermatas and Kokkinakis 1994" startWordPosition="1027" endWordPosition="1030"> ESPRIT-291 /860 project &amp;quot;Linguistic Analysis of the European Languages.&amp;quot; In addition, we present transformations of the taggers&apos; calculations to a fixed-point arithmetic system, which are useful for machines without floating-point hardware. The taggers handle both lexical and tag transition information, and without performing morphological analysis can be used to annotate corpora when small training texts are available. Thus, they are preferred when a new language or a new tagset is used. When the training text is adequate to estimate the tagger parameters, more efficient stochastic taggers (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991; Weischedel et al. 1993) and training methods can be implemented (Merialdo 1994). The structure of this paper is as follows: in Section 2 the stochastic tagging models are presented in detail. In Section 3 the influence of the training text errors and the 138 Dermatas and Kokkinakis Stochastic Tagging sources of stochastic tagger errors are discussed, followed, in Section 4, by a short presentation of the implementation. In Section 5, statistical measurements on the corpora and a short description of the taggers&apos; performance is given. Detailed experimental results ar</context>
</contexts>
<marker>Dermatas, Kokkinakis, 1994</marker>
<rawString>Dermatas, E., and Kokkinakis, G. (1994). &amp;quot;A multilingual unlimited vocabulary stochastic tagger.&amp;quot; In Advanced Speech Applications-European Commission ESPRIT (1), edited by K. Varghese, S. Pfleger, and J. Lefevre, 98-106. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Eineborg</author>
<author>B Gamback</author>
</authors>
<title>Back-propagation based lexical acquisition experiments.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings, NeuroNimes: Neural Networks and their Industrial &amp; Cognitive Applications. Nimes,</booktitle>
<pages>169--178</pages>
<contexts>
<context position="3678" citStr="Eineborg and Gamback 1993" startWordPosition="535" endWordPosition="538"> Computational Linguistics Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available for training, and large tagsets and multilingual applications are involved. In the case where additionally raw untagged text is available, the Maximum Likelihood training can be used to reestimate the parameters of HMM taggers (Merialdo 1994). Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990). Correct classification rates up to 96.4 percent have been achieved in the latter case by testing on the Teleman Swedish corpus. On the other hand, a time-consuming training process has been reported. Recently, several solutions to the problem of tagging unknown words have been presented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991). Hypotheses for unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Maltese and Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback 1993; Elen</context>
</contexts>
<marker>Eineborg, Gamback, 1993</marker>
<rawString>Eineborg, M., and Gamback, B. (1993). &amp;quot;Back-propagation based lexical acquisition experiments.&amp;quot; In Proceedings, NeuroNimes: Neural Networks and their Industrial &amp; Cognitive Applications. Nimes, 169-178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Elenius</author>
</authors>
<title>a connectionist and rule based model for assignment parts-of-speech.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>597--600</pages>
<contexts>
<context position="1955" citStr="Elenius 1990" startWordPosition="291" endWordPosition="292"> and in the development of relevant products. In order to reduce the huge cost of manually creating such corpora, the development of automatic taggers is of paramount importance. In this respect, the ability of a tagger to handle both known and unknown words, to improve its performance by training, and to achieve a high rate of correctly tagged words, is the criterion for assessing its usability in practical cases. Several taggers based on rules, stochastic models, neural networks, and hybrid systems have already been presented for Part-of-speech (POS) tagging. Rule-based taggers (Brill 1992; Elenius 1990; Jacobs and Zernik 1988; Karlsson 1990; Karlsson et al. 1991; Voutilainen, Heikkila, and Antitila 1992; Voutilainen and Tapanainen 1993) use POS-dependent constraints defined by experienced linguists. A small error rate has been achieved by such systems when a restricted, application-dependent POS set is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and Marcinkiewicz (1993) using the Penn Treebank corpus. Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome. Stocha</context>
<context position="3692" citStr="Elenius 1990" startWordPosition="539" endWordPosition="540">Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available for training, and large tagsets and multilingual applications are involved. In the case where additionally raw untagged text is available, the Maximum Likelihood training can be used to reestimate the parameters of HMM taggers (Merialdo 1994). Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990). Correct classification rates up to 96.4 percent have been achieved in the latter case by testing on the Teleman Swedish corpus. On the other hand, a time-consuming training process has been reported. Recently, several solutions to the problem of tagging unknown words have been presented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991). Hypotheses for unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Maltese and Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback 1993; Elenius 1990) have</context>
</contexts>
<marker>Elenius, 1990</marker>
<rawString>Elenius, K. (1990).&amp;quot;Comparing a connectionist and rule based model for assignment parts-of-speech.&amp;quot; In Proceedings, International Conference on Acoustics, Speech and Signal Processing, 597-600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Elenius</author>
<author>R Carlson</author>
</authors>
<title>Assigning parts-of-speech of words from their orthography using a connectionist model.&amp;quot; In</title>
<date>1989</date>
<booktitle>Proceedings, European Conference on Speech Communication and Technology. Paris,</booktitle>
<pages>534--537</pages>
<contexts>
<context position="3718" citStr="Elenius and Carlson 1989" startWordPosition="541" endWordPosition="544">Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available for training, and large tagsets and multilingual applications are involved. In the case where additionally raw untagged text is available, the Maximum Likelihood training can be used to reestimate the parameters of HMM taggers (Merialdo 1994). Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990). Correct classification rates up to 96.4 percent have been achieved in the latter case by testing on the Teleman Swedish corpus. On the other hand, a time-consuming training process has been reported. Recently, several solutions to the problem of tagging unknown words have been presented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991). Hypotheses for unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Maltese and Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback 1993; Elenius 1990) have been applied to unlimited</context>
</contexts>
<marker>Elenius, Carlson, 1989</marker>
<rawString>Elenius, K., and Carlson, R. (1989). &amp;quot;Assigning parts-of-speech of words from their orthography using a connectionist model.&amp;quot; In Proceedings, European Conference on Speech Communication and Technology. Paris, 534-537.</rawString>
</citation>
<citation valid="true">
<title>Partners of ESPRIT-291/860</title>
<date>1986</date>
<tech>Internal Report.</tech>
<marker>1986</marker>
<rawString>Partners of ESPRIT-291/860 (1986). &amp;quot;Unification of the word classes of the ESPRIT Project 860.&amp;quot; BU-WKL-0376. Internal Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Essen</author>
<author>V Steinbiss</author>
</authors>
<title>Cooccurrence smoothing for statistical language modelling.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>161--164</pages>
<contexts>
<context position="21431" citStr="Essen and Steinbiss 1992" startWordPosition="3414" endWordPosition="3417">In some languages, taggers based on HMMs almost reduce the prediction error to the half compared to the MLM approach. 3.2 Tagger prediction errors Generally, tagger errors can be classified into three categories: a. Errors due to inadequate training data. When the model parameters are estimated from a limited amount of training data, tagging errors appear because of unknown or inaccurately estimated conditional probabilities. Various interpolation techniques have been proposed for the estimation of the model parameters for unseen events or to smooth the model parameters (Church and Gale 1991; Essen and Steinbiss 1992; Jardino and Adda 1993; Katz 1987; McInnes 1992). b. Errors due to the syntactical or grammatical style of the testing text. This type of error appears when the testing text has a style unknown to the model (i.e., a style used in the open testing text, not included in the training text). It can be reduced by using multiple models that have been previously trained in different text styles. c. Errors due to insufficient model hypotheses. In this case the model hypotheses are not satisfied; e.g., there are strong intra-tag relations in distances greater than the model order, idiomatic expression</context>
</contexts>
<marker>Essen, Steinbiss, 1992</marker>
<rawString>Essen, U., and Steinbiss, V. (1992). &amp;quot;Cooccurrence smoothing for statistical language modelling.&amp;quot; In Proceedings, International Conference on Acoustics, Speech and Signal Processing, 161-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>G Leech</author>
<author>G Sampson</author>
</authors>
<title>The Computational Analysis of English: A Corpus-Based Approach.</title>
<date>1987</date>
<publisher>Longman.</publisher>
<marker>Garside, Leech, Sampson, 1987</marker>
<rawString>Garside, R.; Leech, G.; and Sampson, G. (1987). The Computational Analysis of English: A Corpus-Based Approach. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
</authors>
<title>Extended Viterbi algorithm for second order hidden Markov process.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>718--720</pages>
<contexts>
<context position="8662" citStr="He 1988" startWordPosition="1332" endWordPosition="1333">uming that each hidden tag state produces a word in the sentence, each word w, is uncorrelated with neighboring words and their tags, and each tag is probabilistic dependent on the N previous tags only. 2.1.1 Most probable tag sequence (HMM-TS). The optimal tag sequence for a given observation sequence of words is given by the following equation: T(HMM-TS) = argmax P( ) JJ P(ti I P(t, 17_1, ,ti_N) H P(wi ti) t, ..... tm i=2 i=N+1 i=1 (2) where M is the number of words in the sentence W. The optimal solution is estimated by the well-known Viterbi algorithm. The first(Rabiner 1989) and second- (He 1988) order Viterbi algorithms have been presented elsewhere. Recently, Tao (1992) described the Viterbi algorithm for generalized HMMs. 2.1.2 Most probable tags (HMM-T). The optimal criterion is to choose the tags that are most likely to be computed independently at each word event: (HMMT) i 1,M (3) T, - = {tio, argmax P(ti W)},ti The optimum tag t,, is estimated using the probabilities of the forward-backward algorithm (Rabiner 1989): t,„ = argmax P(t„W) = argmax P(t/, wi, • • • , )P(w,+1,.. .,wm I t,) (4) t, t, The probabilities in equation 4 are estimated recursively for the first- (Rabiner 198</context>
</contexts>
<marker>He, 1988</marker>
<rawString>He, Y. (1988). &amp;quot;Extended Viterbi algorithm for second order hidden Markov process.&amp;quot; In Proceedings, International Conference on Acoustics, Speech and Signal Processing, 718-720.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jacobs</author>
<author>U Zernik</author>
</authors>
<title>Acquiring lexical knowledge from text: A case study.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, Seventh National Conference on Artificial Intelligence. Saint Paul,</booktitle>
<pages>739--744</pages>
<location>Minnesota,</location>
<contexts>
<context position="1979" citStr="Jacobs and Zernik 1988" startWordPosition="293" endWordPosition="296">velopment of relevant products. In order to reduce the huge cost of manually creating such corpora, the development of automatic taggers is of paramount importance. In this respect, the ability of a tagger to handle both known and unknown words, to improve its performance by training, and to achieve a high rate of correctly tagged words, is the criterion for assessing its usability in practical cases. Several taggers based on rules, stochastic models, neural networks, and hybrid systems have already been presented for Part-of-speech (POS) tagging. Rule-based taggers (Brill 1992; Elenius 1990; Jacobs and Zernik 1988; Karlsson 1990; Karlsson et al. 1991; Voutilainen, Heikkila, and Antitila 1992; Voutilainen and Tapanainen 1993) use POS-dependent constraints defined by experienced linguists. A small error rate has been achieved by such systems when a restricted, application-dependent POS set is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and Marcinkiewicz (1993) using the Penn Treebank corpus. Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome. Stochastic taggers use both co</context>
</contexts>
<marker>Jacobs, Zernik, 1988</marker>
<rawString>Jacobs, P., and Zernik, U. (1988). &amp;quot;Acquiring lexical knowledge from text: A case study.&amp;quot; In Proceedings, Seventh National Conference on Artificial Intelligence. Saint Paul, Minnesota, 739-744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jardino</author>
<author>G Adda</author>
</authors>
<title>Automatic word classification using simulated annealing.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings, International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>41--44</pages>
<contexts>
<context position="21454" citStr="Jardino and Adda 1993" startWordPosition="3418" endWordPosition="3421"> based on HMMs almost reduce the prediction error to the half compared to the MLM approach. 3.2 Tagger prediction errors Generally, tagger errors can be classified into three categories: a. Errors due to inadequate training data. When the model parameters are estimated from a limited amount of training data, tagging errors appear because of unknown or inaccurately estimated conditional probabilities. Various interpolation techniques have been proposed for the estimation of the model parameters for unseen events or to smooth the model parameters (Church and Gale 1991; Essen and Steinbiss 1992; Jardino and Adda 1993; Katz 1987; McInnes 1992). b. Errors due to the syntactical or grammatical style of the testing text. This type of error appears when the testing text has a style unknown to the model (i.e., a style used in the open testing text, not included in the training text). It can be reduced by using multiple models that have been previously trained in different text styles. c. Errors due to insufficient model hypotheses. In this case the model hypotheses are not satisfied; e.g., there are strong intra-tag relations in distances greater than the model order, idiomatic expressions, language dependent e</context>
</contexts>
<marker>Jardino, Adda, 1993</marker>
<rawString>Jardino, M., and Adda, G. (1993). &amp;quot;Automatic word classification using simulated annealing.&amp;quot; In Proceedings, International Conference on Acoustics, Speech and Signal Processing, 41-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Karlsson</author>
</authors>
<title>Constraint grammar as a framework for parsing running text.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Thirteenth International Conference on Computational Linguistics.</booktitle>
<pages>168--173</pages>
<location>Helsinki, Finland,</location>
<contexts>
<context position="1994" citStr="Karlsson 1990" startWordPosition="297" endWordPosition="298">oducts. In order to reduce the huge cost of manually creating such corpora, the development of automatic taggers is of paramount importance. In this respect, the ability of a tagger to handle both known and unknown words, to improve its performance by training, and to achieve a high rate of correctly tagged words, is the criterion for assessing its usability in practical cases. Several taggers based on rules, stochastic models, neural networks, and hybrid systems have already been presented for Part-of-speech (POS) tagging. Rule-based taggers (Brill 1992; Elenius 1990; Jacobs and Zernik 1988; Karlsson 1990; Karlsson et al. 1991; Voutilainen, Heikkila, and Antitila 1992; Voutilainen and Tapanainen 1993) use POS-dependent constraints defined by experienced linguists. A small error rate has been achieved by such systems when a restricted, application-dependent POS set is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and Marcinkiewicz (1993) using the Penn Treebank corpus. Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome. Stochastic taggers use both contextual and mo</context>
</contexts>
<marker>Karlsson, 1990</marker>
<rawString>Karlsson, F. (1990). &amp;quot;Constraint grammar as a framework for parsing running text.&amp;quot; In Proceedings, Thirteenth International Conference on Computational Linguistics. Helsinki, Finland, 168-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Karlsson</author>
<author>A Voutilainen</author>
<author>A Anttila</author>
<author>J Heikkila</author>
</authors>
<title>Constraint grammar: A language-independent system for parsing unrestricted text, with an application to English.&amp;quot;</title>
<date>1991</date>
<booktitle>In Workshop Notes from the Ninth National Conference on Artificial Intelligence.</booktitle>
<location>Anaheim, California.</location>
<contexts>
<context position="2016" citStr="Karlsson et al. 1991" startWordPosition="299" endWordPosition="302">r to reduce the huge cost of manually creating such corpora, the development of automatic taggers is of paramount importance. In this respect, the ability of a tagger to handle both known and unknown words, to improve its performance by training, and to achieve a high rate of correctly tagged words, is the criterion for assessing its usability in practical cases. Several taggers based on rules, stochastic models, neural networks, and hybrid systems have already been presented for Part-of-speech (POS) tagging. Rule-based taggers (Brill 1992; Elenius 1990; Jacobs and Zernik 1988; Karlsson 1990; Karlsson et al. 1991; Voutilainen, Heikkila, and Antitila 1992; Voutilainen and Tapanainen 1993) use POS-dependent constraints defined by experienced linguists. A small error rate has been achieved by such systems when a restricted, application-dependent POS set is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and Marcinkiewicz (1993) using the Penn Treebank corpus. Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome. Stochastic taggers use both contextual and morphological informatio</context>
</contexts>
<marker>Karlsson, Voutilainen, Anttila, Heikkila, 1991</marker>
<rawString>Karlsson, F.; Voutilainen, A.; Anttila, A.; and Heikkila, J. (1991). &amp;quot;Constraint grammar: A language-independent system for parsing unrestricted text, with an application to English.&amp;quot; In Workshop Notes from the Ninth National Conference on Artificial Intelligence. Anaheim, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.&amp;quot;</title>
<date>1987</date>
<journal>IEEE Trans. on Acoustics, Speech, and Language Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<pages>400--401</pages>
<contexts>
<context position="21465" citStr="Katz 1987" startWordPosition="3422" endWordPosition="3423">educe the prediction error to the half compared to the MLM approach. 3.2 Tagger prediction errors Generally, tagger errors can be classified into three categories: a. Errors due to inadequate training data. When the model parameters are estimated from a limited amount of training data, tagging errors appear because of unknown or inaccurately estimated conditional probabilities. Various interpolation techniques have been proposed for the estimation of the model parameters for unseen events or to smooth the model parameters (Church and Gale 1991; Essen and Steinbiss 1992; Jardino and Adda 1993; Katz 1987; McInnes 1992). b. Errors due to the syntactical or grammatical style of the testing text. This type of error appears when the testing text has a style unknown to the model (i.e., a style used in the open testing text, not included in the training text). It can be reduced by using multiple models that have been previously trained in different text styles. c. Errors due to insufficient model hypotheses. In this case the model hypotheses are not satisfied; e.g., there are strong intra-tag relations in distances greater than the model order, idiomatic expressions, language dependent exceptions, </context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, S. (1987). &amp;quot;Estimation of probabilities from sparse data for the language model component of a speech recognizer.&amp;quot; IEEE Trans. on Acoustics, Speech, and Language Processing, 35(3), 400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a Hidden Markov Model.&amp;quot;</title>
<date>1992</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>6</volume>
<issue>3</issue>
<pages>225--242</pages>
<contexts>
<context position="2864" citStr="Kupiec 1992" startWordPosition="424" endWordPosition="425">S set is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and Marcinkiewicz (1993) using the Penn Treebank corpus. Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome. Stochastic taggers use both contextual and morphological information, and the model parameters are usually defined or updated automatically from tagged texts (Cerf-Danon and El-Beze 1991; Church 1988; Cutting et al. 1992; Dermatas and Kokkinakis 1988, 1990, 1993, 1994; Garside, Leech, and Sampson 1987; Kupiec 1992; Maltese * Department of Electrical Engineering, Wire Communications Laboratory (WCL), University of Patras, 265 00 Patras, Greece. E-mail: dermatas@wdee.upatras.gr. @ 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available for training, and large tagsets and multilingual applications are involved. In the case where additionally raw untagged text is a</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>Kupiec, J. (1992). &amp;quot;Robust part-of-speech tagging using a Hidden Markov Model.&amp;quot; Computer Speech &amp; Language, 6(3), 225-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Maltese</author>
<author>F Mancini</author>
</authors>
<title>A technique to automatically assign parts-of-speech to words taking into account word-ending information through a probabilistic model.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, Eurospeech-91,</booktitle>
<pages>753--756</pages>
<contexts>
<context position="4201" citStr="Maltese and Mancini 1991" startWordPosition="614" endWordPosition="617">. Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990). Correct classification rates up to 96.4 percent have been achieved in the latter case by testing on the Teleman Swedish corpus. On the other hand, a time-consuming training process has been reported. Recently, several solutions to the problem of tagging unknown words have been presented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991). Hypotheses for unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Maltese and Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback 1993; Elenius 1990) have been applied to unlimited vocabulary taggers. In taggers that are based on hidden Markov models (HMM), parameters of the unknown words are estimated by taking into account morphological information from the last part of the word (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991). Accurate tagging of seven European languages has been achieved in the first case (error rates of 3-13 percent for a detailed POS set), but an enormous amount of training text is required for the estimation of the paramete</context>
<context position="6907" citStr="Maltese and Mancini 1991" startWordPosition="1031" endWordPosition="1034">uistic Analysis of the European Languages.&amp;quot; In addition, we present transformations of the taggers&apos; calculations to a fixed-point arithmetic system, which are useful for machines without floating-point hardware. The taggers handle both lexical and tag transition information, and without performing morphological analysis can be used to annotate corpora when small training texts are available. Thus, they are preferred when a new language or a new tagset is used. When the training text is adequate to estimate the tagger parameters, more efficient stochastic taggers (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991; Weischedel et al. 1993) and training methods can be implemented (Merialdo 1994). The structure of this paper is as follows: in Section 2 the stochastic tagging models are presented in detail. In Section 3 the influence of the training text errors and the 138 Dermatas and Kokkinakis Stochastic Tagging sources of stochastic tagger errors are discussed, followed, in Section 4, by a short presentation of the implementation. In Section 5, statistical measurements on the corpora and a short description of the taggers&apos; performance is given. Detailed experimental results are included in Appendices A</context>
</contexts>
<marker>Maltese, Mancini, 1991</marker>
<rawString>Maltese, G., and Mancini, F. (1991). &amp;quot;A technique to automatically assign parts-of-speech to words taking into account word-ending information through a probabilistic model.&amp;quot; In Proceedings, Eurospeech-91, 753-756.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>315--330</pages>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M.; Santorini, B.; and Marcinkiewicz, M. (1993). &amp;quot;Building a large annotated corpus of English: The Penn Treebank.&amp;quot; Computational Linguistics, 19(2), 315-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F McInnes</author>
</authors>
<title>An enhanced interpolation technique for context-specific probability estimation in speech and language modelling.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, International Conference on Spoken Language Processing,</booktitle>
<pages>1491--1494</pages>
<contexts>
<context position="21480" citStr="McInnes 1992" startWordPosition="3424" endWordPosition="3425">rediction error to the half compared to the MLM approach. 3.2 Tagger prediction errors Generally, tagger errors can be classified into three categories: a. Errors due to inadequate training data. When the model parameters are estimated from a limited amount of training data, tagging errors appear because of unknown or inaccurately estimated conditional probabilities. Various interpolation techniques have been proposed for the estimation of the model parameters for unseen events or to smooth the model parameters (Church and Gale 1991; Essen and Steinbiss 1992; Jardino and Adda 1993; Katz 1987; McInnes 1992). b. Errors due to the syntactical or grammatical style of the testing text. This type of error appears when the testing text has a style unknown to the model (i.e., a style used in the open testing text, not included in the training text). It can be reduced by using multiple models that have been previously trained in different text styles. c. Errors due to insufficient model hypotheses. In this case the model hypotheses are not satisfied; e.g., there are strong intra-tag relations in distances greater than the model order, idiomatic expressions, language dependent exceptions, etc. A general </context>
</contexts>
<marker>McInnes, 1992</marker>
<rawString>McInnes, F. (1992). &amp;quot;An enhanced interpolation technique for context-specific probability estimation in speech and language modelling.&amp;quot; In Proceedings, International Conference on Spoken Language Processing, 1491-1494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging text with a probabilistic model.&amp;quot;</title>
<date>1991</date>
<booktitle>In International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>809--812</pages>
<contexts>
<context position="3196" citStr="Merialdo 1991" startWordPosition="465" endWordPosition="466">tual and morphological information, and the model parameters are usually defined or updated automatically from tagged texts (Cerf-Danon and El-Beze 1991; Church 1988; Cutting et al. 1992; Dermatas and Kokkinakis 1988, 1990, 1993, 1994; Garside, Leech, and Sampson 1987; Kupiec 1992; Maltese * Department of Electrical Engineering, Wire Communications Laboratory (WCL), University of Patras, 265 00 Patras, Greece. E-mail: dermatas@wdee.upatras.gr. @ 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available for training, and large tagsets and multilingual applications are involved. In the case where additionally raw untagged text is available, the Maximum Likelihood training can be used to reestimate the parameters of HMM taggers (Merialdo 1994). Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990). Correct classification rates up to 96.4 percent have </context>
<context position="37175" citStr="Merialdo 1991" startWordPosition="5943" endWordPosition="5944">I t) estimation is proportional to the word and the tag frequency of occurrence in the training text. Thus, for all tagsets and languages a larger training text is required in order to minimize the error rate. The taggers based on the HMM reduce the prediction error almost to half in comparison to the same order taggers based on MLM. Strong dependencies on the language and the estimation accuracy of the model parameters influence this reduction. The alternative HMM solutions give trivial performance differences, confirming recent results obtained in the Treebank corpus by using an HMM tagger (Merialdo 1991). Concerning the performance of the taggers in unknown words, we present in Figure 8 as an example the HMM-TS2 error rate for the tagset of the main grammatical categories, which is also the worst case for this set of grammatical categories. Generally the error rate decreases when the training text is increased. The stochastic model is successful for only half of the unknown words for the Italian text and for approximately two out of three unknown words for the English text. In all other languages the HMM-TS2 tagger gives the correct solution for three out of four unknown words. Similar result</context>
</contexts>
<marker>Merialdo, 1991</marker>
<rawString>Merialdo, B. (1991). &amp;quot;Tagging text with a probabilistic model.&amp;quot; In International Conference on Acoustics, Speech and Signal Processing, 809-812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.&amp;quot;</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<pages>155--171</pages>
<contexts>
<context position="3577" citStr="Merialdo 1994" startWordPosition="524" endWordPosition="525">of Patras, 265 00 Patras, Greece. E-mail: dermatas@wdee.upatras.gr. @ 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available for training, and large tagsets and multilingual applications are involved. In the case where additionally raw untagged text is available, the Maximum Likelihood training can be used to reestimate the parameters of HMM taggers (Merialdo 1994). Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990). Correct classification rates up to 96.4 percent have been achieved in the latter case by testing on the Teleman Swedish corpus. On the other hand, a time-consuming training process has been reported. Recently, several solutions to the problem of tagging unknown words have been presented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991). Hypotheses for unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; </context>
<context position="6988" citStr="Merialdo 1994" startWordPosition="1045" endWordPosition="1046">taggers&apos; calculations to a fixed-point arithmetic system, which are useful for machines without floating-point hardware. The taggers handle both lexical and tag transition information, and without performing morphological analysis can be used to annotate corpora when small training texts are available. Thus, they are preferred when a new language or a new tagset is used. When the training text is adequate to estimate the tagger parameters, more efficient stochastic taggers (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991; Weischedel et al. 1993) and training methods can be implemented (Merialdo 1994). The structure of this paper is as follows: in Section 2 the stochastic tagging models are presented in detail. In Section 3 the influence of the training text errors and the 138 Dermatas and Kokkinakis Stochastic Tagging sources of stochastic tagger errors are discussed, followed, in Section 4, by a short presentation of the implementation. In Section 5, statistical measurements on the corpora and a short description of the taggers&apos; performance is given. Detailed experimental results are included in Appendices A and B. 2. Stochastic Tagging Models A stochastic optimal sequence of tags T, to </context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Merialdo, B. (1994). &amp;quot;Tagging English text with a probabilistic model.&amp;quot; Computational Linguistics, 20(2), 155-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meteer</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Empirical studies in part of speech labelling.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, Fourth DARPA Workshop on Speech and Natural Language.</booktitle>
<publisher>Morgan Kaufman.</publisher>
<marker>Meteer, Schwartz, Weischedel, 1991</marker>
<rawString>Meteer, M.; Schwartz, R.; and Weischedel, R. (1991). &amp;quot;Empirical studies in part of speech labelling.&amp;quot; In Proceedings, Fourth DARPA Workshop on Speech and Natural Language. Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nakamura</author>
<author>K Maruyama</author>
<author>T Kawabata</author>
<author>K Shikano</author>
</authors>
<title>Neural network approach to word category prediction for English texts.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Thirteenth International Conference on Computational Linguistics. Helinski,</booktitle>
<pages>213--218</pages>
<contexts>
<context position="3741" citStr="Nakamura et al. 1990" startWordPosition="545" endWordPosition="548">ber 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available for training, and large tagsets and multilingual applications are involved. In the case where additionally raw untagged text is available, the Maximum Likelihood training can be used to reestimate the parameters of HMM taggers (Merialdo 1994). Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990). Correct classification rates up to 96.4 percent have been achieved in the latter case by testing on the Teleman Swedish corpus. On the other hand, a time-consuming training process has been reported. Recently, several solutions to the problem of tagging unknown words have been presented (Charniak et al. 1993; Meteer, Schwartz, and Weischedel 1991). Hypotheses for unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Maltese and Mancini 1991; Weischedel et al. 1993), and connectionist (Eineborg and Gamback 1993; Elenius 1990) have been applied to unlimited vocabulary taggers. In</context>
</contexts>
<marker>Nakamura, Maruyama, Kawabata, Shikano, 1990</marker>
<rawString>Nakamura, M.; Maruyama, K.; Kawabata, T.; and Shikano, K. (1990). &amp;quot;Neural network approach to word category prediction for English texts.&amp;quot; In Proceedings, Thirteenth International Conference on Computational Linguistics. Helinski, Finland, 213-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Pelillo</author>
<author>F Moro</author>
<author>M Refice</author>
</authors>
<title>Probabilistic prediction of parts-of-speech from spelling using decision trees.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, International Conference on Spoken Language Processing,</booktitle>
<pages>1343--1346</pages>
<marker>Pelillo, Moro, Refice, 1992</marker>
<rawString>Pelillo, W.; Moro, F.; and Refice, M. (1992). &amp;quot;Probabilistic prediction of parts-of-speech from spelling using decision trees.&amp;quot; In Proceedings, International Conference on Spoken Language Processing, 1343-1346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
</authors>
<title>A tutorial on Hidden Markov Models and selected applications in speech recognition.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, IEEE</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>257--285</pages>
<contexts>
<context position="8640" citStr="Rabiner 1989" startWordPosition="1327" endWordPosition="1329">be modeled by an HMM by assuming that each hidden tag state produces a word in the sentence, each word w, is uncorrelated with neighboring words and their tags, and each tag is probabilistic dependent on the N previous tags only. 2.1.1 Most probable tag sequence (HMM-TS). The optimal tag sequence for a given observation sequence of words is given by the following equation: T(HMM-TS) = argmax P( ) JJ P(ti I P(t, 17_1, ,ti_N) H P(wi ti) t, ..... tm i=2 i=N+1 i=1 (2) where M is the number of words in the sentence W. The optimal solution is estimated by the well-known Viterbi algorithm. The first(Rabiner 1989) and second- (He 1988) order Viterbi algorithms have been presented elsewhere. Recently, Tao (1992) described the Viterbi algorithm for generalized HMMs. 2.1.2 Most probable tags (HMM-T). The optimal criterion is to choose the tags that are most likely to be computed independently at each word event: (HMMT) i 1,M (3) T, - = {tio, argmax P(ti W)},ti The optimum tag t,, is estimated using the probabilities of the forward-backward algorithm (Rabiner 1989): t,„ = argmax P(t„W) = argmax P(t/, wi, • • • , )P(w,+1,.. .,wm I t,) (4) t, t, The probabilities in equation 4 are estimated recursively for t</context>
<context position="25164" citStr="Rabiner 1989" startWordPosition="3985" endWordPosition="3986">ons. In practice the prediction error differences measured for all languages, taggers, and tagsets were less than 0.02 percent. 4.3 Scaling The solution obtained by the forward-backward algorithm cannot be logarithmically transformed because of the presence of summations. It is well known that for HMMs the forward and backward probabilities tend exponentially to zero. The scaling process introduced in this case multiplies the forward and backward probabilities by a scaling factor at selective word events in order to keep the computations within the floatingpoint dynamic range of the computer (Rabiner 1989). 4.4 Hardware—Software The taggers have been realized under MS-DOS using a 32-bit C compiler. The lexicon size is limited by the available RAM. A mean value of 35 bytes per word is allocated. The tagger speed exceeds the rate of 500 word/sec in a 80386 (33MHz) for all languages and tagsets in text with known words. A maximum memory requirement of 930Kb has been measured in the experiments described in this paper. A set of symbols and keywords (a sentence separators set) and the maximum length of a sentence are the only manually defined parameters when the HMM taggers are applied. In the MLM t</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner, L. (1989). &amp;quot;A tutorial on Hidden Markov Models and selected applications in speech recognition.&amp;quot; In Proceedings, IEEE 77(2), 257-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tao</author>
</authors>
<title>A generalisation of discrete Hidden Markov Model and of Viterbi algorithm.&amp;quot;</title>
<date>1992</date>
<journal>Pattern Recognition,</journal>
<volume>25</volume>
<issue>11</issue>
<pages>1381--1397</pages>
<contexts>
<context position="8739" citStr="Tao (1992)" startWordPosition="1342" endWordPosition="1343"> w, is uncorrelated with neighboring words and their tags, and each tag is probabilistic dependent on the N previous tags only. 2.1.1 Most probable tag sequence (HMM-TS). The optimal tag sequence for a given observation sequence of words is given by the following equation: T(HMM-TS) = argmax P( ) JJ P(ti I P(t, 17_1, ,ti_N) H P(wi ti) t, ..... tm i=2 i=N+1 i=1 (2) where M is the number of words in the sentence W. The optimal solution is estimated by the well-known Viterbi algorithm. The first(Rabiner 1989) and second- (He 1988) order Viterbi algorithms have been presented elsewhere. Recently, Tao (1992) described the Viterbi algorithm for generalized HMMs. 2.1.2 Most probable tags (HMM-T). The optimal criterion is to choose the tags that are most likely to be computed independently at each word event: (HMMT) i 1,M (3) T, - = {tio, argmax P(ti W)},ti The optimum tag t,, is estimated using the probabilities of the forward-backward algorithm (Rabiner 1989): t,„ = argmax P(t„W) = argmax P(t/, wi, • • • , )P(w,+1,.. .,wm I t,) (4) t, t, The probabilities in equation 4 are estimated recursively for the first- (Rabiner 1989) and second-order HMM (Watson and Chung 1992). The main difference between </context>
<context position="22180" citStr="Tao 1992" startWordPosition="3539" endWordPosition="3540">rror appears when the testing text has a style unknown to the model (i.e., a style used in the open testing text, not included in the training text). It can be reduced by using multiple models that have been previously trained in different text styles. c. Errors due to insufficient model hypotheses. In this case the model hypotheses are not satisfied; e.g., there are strong intra-tag relations in distances greater than the model order, idiomatic expressions, language dependent exceptions, etc. A general solution to the variable length and depth of dependency for HMM has been already proposed (Tao 1992), but has not been implemented in taggers. 4. Implementation In this section we present techniques to speed up the tagging process and avoid underflow or overflow phenomena during the estimation of the optimum solution. These techniques do not increase the prediction error rate or have only minimal influence on it, as proven in the experiments. Two modules consume the majority of the tagger computational time. The first module extracts from the model parameters the intra-tag and the word-tag conditional probabilities requested by the second module, which computes the optimum solution by multip</context>
</contexts>
<marker>Tao, 1992</marker>
<rawString>Tao, C. (1992). &amp;quot;A generalisation of discrete Hidden Markov Model and of Viterbi algorithm.&amp;quot; Pattern Recognition, 25(11), 1381-1397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilainen</author>
<author>P Tapanainen</author>
</authors>
<title>Ambiguity resolution in a reductionistic parser.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings, Sixth Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<pages>394--403</pages>
<location>Utrecht, Netherlands,</location>
<contexts>
<context position="2092" citStr="Voutilainen and Tapanainen 1993" startWordPosition="308" endWordPosition="311"> development of automatic taggers is of paramount importance. In this respect, the ability of a tagger to handle both known and unknown words, to improve its performance by training, and to achieve a high rate of correctly tagged words, is the criterion for assessing its usability in practical cases. Several taggers based on rules, stochastic models, neural networks, and hybrid systems have already been presented for Part-of-speech (POS) tagging. Rule-based taggers (Brill 1992; Elenius 1990; Jacobs and Zernik 1988; Karlsson 1990; Karlsson et al. 1991; Voutilainen, Heikkila, and Antitila 1992; Voutilainen and Tapanainen 1993) use POS-dependent constraints defined by experienced linguists. A small error rate has been achieved by such systems when a restricted, application-dependent POS set is used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, and Marcinkiewicz (1993) using the Penn Treebank corpus. Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome. Stochastic taggers use both contextual and morphological information, and the model parameters are usually defined or updated automatically fro</context>
</contexts>
<marker>Voutilainen, Tapanainen, 1993</marker>
<rawString>Voutilainen, A., and Tapanainen, P. (1993). &amp;quot;Ambiguity resolution in a reductionistic parser.&amp;quot; In Proceedings, Sixth Conference of the European Chapter of the Association for Computational Linguistics. Utrecht, Netherlands, 394-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilainen</author>
<author>J Heikkila</author>
<author>A Antitila</author>
</authors>
<title>Constraint grammar of English.&amp;quot;</title>
<date>1992</date>
<journal>Publication</journal>
<volume>21</volume>
<institution>Department of General Linguistics, University of Helinski,</institution>
<location>Helinski, Finland.</location>
<marker>Voutilainen, Heikkila, Antitila, 1992</marker>
<rawString>Voutilainen, A.; Heikkila, J.; and Antitila, A. (1992). &amp;quot;Constraint grammar of English.&amp;quot; Publication 21, Department of General Linguistics, University of Helinski, Helinski, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Watson</author>
<author>Chung Tsoi</author>
<author>A</author>
</authors>
<title>Second order Hidden Markov Models for speech recognition.&amp;quot; In</title>
<date>1992</date>
<booktitle>Proceedings, Fourth Australian International Conference on Speech Science and Technology,</booktitle>
<pages>146--151</pages>
<marker>Watson, Tsoi, A, 1992</marker>
<rawString>Watson, B., and Chung Tsoi, A. (1992). &amp;quot;Second order Hidden Markov Models for speech recognition.&amp;quot; In Proceedings, Fourth Australian International Conference on Speech Science and Technology, 146-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>M Meteer</author>
<author>R Schwartz</author>
<author>L Ramshaw</author>
<author>J Palmucci</author>
</authors>
<title>Coping with ambiguity and unknown words through probabilistic models.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>359--382</pages>
<contexts>
<context position="3252" citStr="Weischedel et al. 1993" startWordPosition="472" endWordPosition="475">el parameters are usually defined or updated automatically from tagged texts (Cerf-Danon and El-Beze 1991; Church 1988; Cutting et al. 1992; Dermatas and Kokkinakis 1988, 1990, 1993, 1994; Garside, Leech, and Sampson 1987; Kupiec 1992; Maltese * Department of Electrical Engineering, Wire Communications Laboratory (WCL), University of Patras, 265 00 Patras, Greece. E-mail: dermatas@wdee.upatras.gr. @ 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available for training, and large tagsets and multilingual applications are involved. In the case where additionally raw untagged text is available, the Maximum Likelihood training can be used to reestimate the parameters of HMM taggers (Merialdo 1994). Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990). Correct classification rates up to 96.4 percent have been achieved in the latter case by testing on the Telem</context>
<context position="4938" citStr="Weischedel et al. (1993)" startWordPosition="731" endWordPosition="734">ited vocabulary taggers. In taggers that are based on hidden Markov models (HMM), parameters of the unknown words are estimated by taking into account morphological information from the last part of the word (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991). Accurate tagging of seven European languages has been achieved in the first case (error rates of 3-13 percent for a detailed POS set), but an enormous amount of training text is required for the estimation of the parameters for unknown words. Similar results have been reported by Maltese and Mancini (1991) for the Italian language. Weischedel et al. (1993) have used four categories of word morphology, such as inflectional endings, derivational endings, hyphenation, and capitalization. For the case in which only a restricted training text is available, a simple, language- and tagset-independent HMM tagger has been presented by Dermatas and Kokkinakis (1993), where the HMM parameters for the unknown words are estimated by assuming that the POS probability distribution of the unknown words and the POS probability distribution of the less probable words in the small training text are identical. In this paper, five natural language stochastic tagger</context>
<context position="6932" citStr="Weischedel et al. 1993" startWordPosition="1035" endWordPosition="1038">opean Languages.&amp;quot; In addition, we present transformations of the taggers&apos; calculations to a fixed-point arithmetic system, which are useful for machines without floating-point hardware. The taggers handle both lexical and tag transition information, and without performing morphological analysis can be used to annotate corpora when small training texts are available. Thus, they are preferred when a new language or a new tagset is used. When the training text is adequate to estimate the tagger parameters, more efficient stochastic taggers (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991; Weischedel et al. 1993) and training methods can be implemented (Merialdo 1994). The structure of this paper is as follows: in Section 2 the stochastic tagging models are presented in detail. In Section 3 the influence of the training text errors and the 138 Dermatas and Kokkinakis Stochastic Tagging sources of stochastic tagger errors are discussed, followed, in Section 4, by a short presentation of the implementation. In Section 5, statistical measurements on the corpora and a short description of the taggers&apos; performance is given. Detailed experimental results are included in Appendices A and B. 2. Stochastic Tag</context>
</contexts>
<marker>Weischedel, Meteer, Schwartz, Ramshaw, Palmucci, 1993</marker>
<rawString>Weischedel, R.; Meteer, M.; Schwartz, R.; Ramshaw, L.; and Palmucci, J. (1993). &amp;quot;Coping with ambiguity and unknown words through probabilistic models.&amp;quot; Computational Linguistics, 19(2), 359-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wothke</author>
<author>I Weck-Ulm</author>
<author>J Heinecke</author>
<author>Mertineit</author>
</authors>
<title>Statistically based automatic tagging of German text corpora with parts-of-speech—some experiments.&amp;quot; TR75.93.02-IBM. IBM Germany,</title>
<date>1993</date>
<location>Heidelberg Scientific Center.</location>
<contexts>
<context position="3273" citStr="Wothke et al. 1993" startWordPosition="476" endWordPosition="479">y defined or updated automatically from tagged texts (Cerf-Danon and El-Beze 1991; Church 1988; Cutting et al. 1992; Dermatas and Kokkinakis 1988, 1990, 1993, 1994; Garside, Leech, and Sampson 1987; Kupiec 1992; Maltese * Department of Electrical Engineering, Wire Communications Laboratory (WCL), University of Patras, 265 00 Patras, Greece. E-mail: dermatas@wdee.upatras.gr. @ 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo, Moro, and Refice 1992; Weischedel et al. 1993; Wothke et al. 1993). These taggers are preferred when tagged texts are available for training, and large tagsets and multilingual applications are involved. In the case where additionally raw untagged text is available, the Maximum Likelihood training can be used to reestimate the parameters of HMM taggers (Merialdo 1994). Connectionist models have been used successfully for lexical acquisition (Eineborg and Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al. 1990). Correct classification rates up to 96.4 percent have been achieved in the latter case by testing on the Teleman Swedish corpus. On</context>
</contexts>
<marker>Wothke, Weck-Ulm, Heinecke, Mertineit, 1993</marker>
<rawString>Wothke, K.; Weck-Ulm, I.; Heinecke, J.; Mertineit, 0.; and Pachunke, T. (1993). &amp;quot;Statistically based automatic tagging of German text corpora with parts-of-speech—some experiments.&amp;quot; TR75.93.02-IBM. IBM Germany, Heidelberg Scientific Center.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>