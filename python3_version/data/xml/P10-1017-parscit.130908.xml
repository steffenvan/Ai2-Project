<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993631">
Hierarchical Search for Word Alignment
</title>
<author confidence="0.996624">
Jason Riesa and Daniel Marcu
</author>
<affiliation confidence="0.995889666666667">
Information Sciences Institute
Viterbi School of Engineering
University of Southern California
</affiliation>
<email confidence="0.997856">
{riesa, marcu}@isi.edu
</email>
<sectionHeader confidence="0.993848" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9902143125">
We present a simple yet powerful hier-
archical search algorithm for automatic
word alignment. Our algorithm induces
a forest of alignments from which we
can efficiently extract a ranked k-best list.
We score a given alignment within the
forest with a flexible, linear discrimina-
tive model incorporating hundreds of fea-
tures, and trained on a relatively small
amount of annotated data. We report re-
sults on Arabic-English word alignment
and translation tasks. Our model out-
performs a GIZA++ Model-4 baseline by
6.3 points in F-measure, yielding a 1.1
BLEU score increase over a state-of-the-art
syntax-based machine translation system.
</bodyText>
<sectionHeader confidence="0.998913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941666666667">
Automatic word alignment is generally accepted
as a first step in training any statistical machine
translation system. It is a vital prerequisite for
generating translation tables, phrase tables, or syn-
tactic transformation rules. Generative alignment
models like IBM Model-4 (Brown et al., 1993)
have been in wide use for over 15 years, and while
not perfect (see Figure 1), they are completely un-
supervised, requiring no annotated training data to
learn alignments that have powered many current
state-of-the-art translation system.
Today, there exist human-annotated alignments
and an abundance of other information for many
language pairs potentially useful for inducing ac-
curate alignments. How can we take advantage
of all of this data at our fingertips? Using fea-
ture functions that encode extra information is one
good way. Unfortunately, as Moore (2005) points
out, it is usually difficult to extend a given genera-
tive model with feature functions without chang-
ing the entire generative story. This difficulty
</bodyText>
<figure confidence="0.996565785714286">
!&amp;quot;#$%!&amp;!&apos;()
&amp;quot;* +,-* !&amp;.(
/023 1(
5!67
!*,8.(
9:;
&lt;)+,=.(
&gt;?@!A8BC(
1
DEFG* )
# G(
1
?H()
*
</figure>
<figureCaption confidence="0.6793144">
Figure 1: Model-4 alignment vs. a gold stan-
dard. Circles represent links in a human-annotated
alignment, and black boxes represent links in the
Model-4 alignment. Bold gray boxes show links
gained after fully connecting the alignment.
</figureCaption>
<bodyText confidence="0.999347684210526">
has motivated much recent work in discriminative
modeling for word alignment (Moore, 2005; Itty-
cheriah and Roukos, 2005; Liu et al., 2005; Taskar
et al., 2005; Blunsom and Cohn, 2006; Lacoste-
Julien et al., 2006; Moore et al., 2006).
We present in this paper a discriminative align-
ment model trained on relatively little data, with
a simple, yet powerful hierarchical search proce-
dure. We borrow ideas from both k-best pars-
ing (Klein and Manning, 2001; Huang and Chi-
ang, 2005; Huang, 2008) and forest-based, and
hierarchical phrase-based translation (Huang and
Chiang, 2007; Chiang, 2007), and apply them to
word alignment.
Using a foreign string and an English parse
tree as input, we formulate a bottom-up search
on the parse tree, with the structure of the tree
as a backbone for building a hypergraph of pos-
sible alignments. Our algorithm yields a forest of
</bodyText>
<page confidence="0.967266">
157
</page>
<note confidence="0.970904">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.9991262">
ﺰﺒﳋﺍ
ﻞﺟﺍﺮﻟﺍ
ﻞﻛﺍ
ﺰﺒﳋﺍ
ﻞﻛ
ﻞﺟﺮﻟﺍ
ﺍ
the man ate the
NP
S
VP
NP
bread
ﻞﻛﺍ
ﻞﺟﺮﻟﺍ
ﻞﻛﺍ
ﻞﺟﺮﻟﺍ
ﻞﻛﺍ
ﻞﺟﺮﻟﺍ
ﺰﺒﳋﺍ ﺰﺒﳋﺍ ﺰﺒﳋﺍ
</figure>
<figureCaption confidence="0.999082">
Figure 2: Example of approximate search through a hypergraph with beam size = 5. Each black square
</figureCaption>
<bodyText confidence="0.984386875">
implies a partial alignment. Each partial alignment at each node is ranked according to its model score.
In this figure, we see that the partial alignment implied by the 1-best hypothesis at the leftmost NP
node is constructed by composing the best hypothesis at the terminal node labeled “the” and the 2nd-
best hypothesis at the terminal node labeled “man”. (We ignore terminal nodes in this toy example.)
Hypotheses at the root node imply full alignment structures.
word alignments, from which we can efficiently
extract the k-best. We handle an arbitrary number
of features, compute them efficiently, and score
alignments using a linear model. We train the
parameters of the model using averaged percep-
tron (Collins, 2002) modified for structured out-
puts, but can easily fit into a max-margin or related
framework. Finally, we use relatively little train-
ing data to achieve accurate word alignments. Our
model can generate arbitrary alignments and learn
from arbitrary gold alignments.
</bodyText>
<sectionHeader confidence="0.511357" genericHeader="method">
2 Word Alignment as a Hypergraph
</sectionHeader>
<bodyText confidence="0.995360083333333">
Algorithm input The input to our alignment al-
gorithm is a sentence-pair (en1, f1m) and a parse tree
over one of the input sentences. In this work,
we parse our English data, and for each sentence
E = en1, let T be its syntactic parse. To gener-
ate parse trees, we use the Berkeley parser (Petrov
et al., 2006), and use Collins head rules (Collins,
2003) to head-out binarize each tree.
Overview We present a brief overview here and
delve deeper in Section 2.1. Word alignments are
built bottom-up on the parse tree. Each node v in
the tree holds partial alignments sorted by score.
</bodyText>
<page confidence="0.986513">
158
</page>
<figure confidence="0.966851916666667">
u u12 u13
11 u11 u12 u13
12 13
2.2 1 5
. 2.2 4.1 5.5
4.1 .5
2.4 3.5 2
2.4 .5 7.2
.4 3.5 7.2
3.2 5 14
.2 4.5 11.4
3.2 4.5 1.4
</figure>
<figureCaption confidence="0.85022225">
(a) Score the left corner align-
ment first. Assume it is the 1-
best. Numbers in the rest of the
boxes are hidden at this point.
</figureCaption>
<figure confidence="0.857881222222222">
u1 u12 u13
u1 u12 u13
u1 u1 u1
2. 4.1 5.
2.2 4. 5.5
2.2 4.1 5.5
2.4 3.5 7.2
24 3.5 7.2
2.4 3.5 7.2
3.2 4.5 1.4
3.2 4.5 114
3.2 4.5 .4
(b) Expand the frontier of align-
ments. We are now looking for
the 2nd best.
u1 u12 u13
u1 u1 u1
u u u
2. 4.1 5.
2.2 4 5.5
2. 4. 5
2.4 3.5 7.2
2 3 7.2
24 3.5 7.
3.2 4 14
3. 4.5 11
3.2 4.5 1.4
(c) Expand the frontier further.
After this step we have our top
k alignments.
u21
21 u21
u 22u22
u23
23 u23
u21u21
u2
u2u2u2
u23u23
u2
u21
u2 u
u2u2u
u23
u2 u
</figure>
<figureCaption confidence="0.980192">
Figure 3: Cube pruning with alignment hypotheses to select the top-k alignments at node v with children
(u1, u2). In this example, k = 3. Each box represents the combination of two partial alignments to create
a larger one. The score in each box is the sum of the scores of the child alignments plus a combination
cost.
</figureCaption>
<bodyText confidence="0.976943170212766">
Each partial alignment comprises the columns of
the alignment matrix for the e-words spanned by
v, and each is scored by a linear combination of
feature functions. See Figure 2 for a small exam-
ple.
Initial partial alignments are enumerated and
scored at preterminal nodes, each spanning a sin-
gle column of the word alignment matrix. To
speed up search, we can prune at each node, keep-
ing a beam of size k. In the diagram depicted in
Figure 2, the beam is size k = 5.
From here, we traverse the tree nodes bottom-
up, combining partial alignments from child nodes
until we have constructed a single full alignment at
the root node of the tree. If we are interested in the
k-best, we continue to populate the root node until
we have k alignments.1
We use one set of feature functions for preter-
minal nodes, and another set for nonterminal
nodes. This is analogous to local and nonlo-
cal feature functions for parse-reranking used by
Huang (2008). Using nonlocal features at a non-
terminal node emits a combination cost for com-
posing a set of child partial alignments.
Because combination costs come into play, we
use cube pruning (Chiang, 2007) to approxi-
mate the k-best combinations at some nonterminal
node v. Inference is exact when only local features
are used.
Assumptions There are certain assumptions re-
lated to our search algorithm that we must make:
1We use approximate dynamic programming to store
alignments, keeping only scored lists of pointers to initial
single-column spans. Each item in the list is a derivation that
implies a partial alignment.
(1) that using the structure of 1-best English syn-
tactic parse trees is a reasonable way to frame and
drive our search, and (2) that F-measure approxi-
mately decomposes over hyperedges.
We perform an oracle experiment to validate
these assumptions. We find the oracle for a given
(T,e, f) triple by proceeding through our search al-
gorithm, forcing ourselves to always select correct
links with respect to the gold alignment when pos-
sible, breaking ties arbitrarily. The the F1 score of
our oracle alignment is 98.8%, given this “perfect”
model.
</bodyText>
<subsectionHeader confidence="0.986506">
2.1 Hierarchical search
</subsectionHeader>
<bodyText confidence="0.999959142857143">
Initial alignments We can construct a word
alignment hierarchically, bottom-up, by making
use of the structure inherent in syntactic parse
trees. We can think of building a word alignment
as filling in an MxN matrix (Figure 1), and we be-
gin by visiting each preterminal node in the tree.
Each of these nodes spans a single e word. (Line
2 in Algorithm 1).
From here we can assign links from each e word
to zero or more f words (Lines 6–14). At this
level of the tree the span size is 1, and the par-
tial alignment we have made spans a single col-
umn of the matrix. We can make many such partial
alignments depending on the links selected. Lines
5 through 9 of Algorithm 1 enumerate either the
null alignment, single-link alignments, or two-link
alignments. Each partial alignment is scored and
stored in a sorted heap (Lines 9 and 13).
In practice enumerating all two-link alignments
can be prohibitive for long sentence pairs; we set
a practical limit and score only pairwise combina-
</bodyText>
<page confidence="0.98577">
159
</page>
<figure confidence="0.949281314285714">
Algorithm 1: Hypergraph Alignment
Input:
Source sentence en1
Target sentence f1 m
Parse tree T over en1
Set of feature functions h
Weight vector w
Beam size k
Output:
A k-best list of alignments over en1 and f1m
2 1 function for
3 AraGx(en1, f1m, T)
4 v E T in bottom-up order do
αv (-- 0
if -PᴇᴛᴇᴍᴀNᴏᴅᴇ(v) then
5 i (-- index-of(v)
6 for j=0tomdo
7 links (-- (i, j)
8 score (--w · h(links, v, en1, f1m )
9 Pᴜ(αv, (score, links), k )
10 fork= j + 1 to m do
11 links (-- (i, j), (i, k)
12 score (-- w · h(links, v, en1, f1m )
13 Pᴜ(αv, (score, links), k )
14 end
15 end
16 else
17 αv (-- GᴏᴡSᴘᴀ(children(v), k)
18 end
19 end
20 end
21 function GRowSPAx((u1, u2), k)
22 return CᴜᴇPᴜ((αu1, αu2), k,w,h)
23 end
{|f  |1
</figure>
<bodyText confidence="0.979608620689655">
tions of the top n = max 2 , 10 scoring single-
link alignments.
We limit the number of total partial alignments
αv kept at each node to k. If at any time we wish to
push onto the heap a new partial alignment when
the heap is full, we pop the current worst off the
heap and replace it with our new partial alignment
if its score is better than the current worst.
Building the hypergraph We now visit internal
nodes (Line 16) in the tree in bottom-up order. At
each nonterminal node v we wish to combine the
partial alignments of its children u1, ... , uc. We
use cube pruning (Chiang, 2007; Huang and Chi-
ang, 2007) to select the k-best combinations of the
partial alignments of u1, ... , uc (Line 19). Note
Figure 4: Correct version of Figure 1 after hyper-
graph alignment. Subscripts on the nonterminal
labels denote the branch containing the head word
for that span.
that Algorithm 1 assumes a binary tree2, but is not
necessary. In the general case, cube pruning will
operate on a d-dimensional hypercube, where d is
the branching factor of node v.
We cannot enumerate and score every possibil-
ity; without the cube pruning approximation, we
will have kc possible combinations at each node,
exploding the search space exponentially. Figure 3
depicts how we select the top-k alignments at a
node v from its children ( u1, u2 ).
</bodyText>
<sectionHeader confidence="0.992462" genericHeader="method">
3 Discriminative training
</sectionHeader>
<bodyText confidence="0.9997692">
We incorporate all our new features into a linear
model and learn weights for each using the on-
line averaged perceptron algorithm (Collins, 2002)
with a few modifications for structured outputs in-
spired by Chiang et al. (2008). We define:
</bodyText>
<footnote confidence="0.608526">
2We find empirically that using binarized trees reduces
search errors in cube pruning.
</footnote>
<figure confidence="0.98531356097561">
S-BAR1
S2
VP1
VP-C1
VP-C1
PP1
DT
NP-C1
NPB2
NPB-BAR2
NPB-BAR2
CD JJ NNS
VBP
VBN
VBN
NP-C1
NP-C-BAR1
NP1
NPB2
NPB-BAR2
IN DT NN NN CC
NP1
NPB2
NPB-BAR2
CD JJ NN �
!&amp;quot;#$%!&amp;&apos;()
!
&amp;quot;* +,-* !&amp;.(
/0231(
5!67
!*,8.(
9:;
&lt;)+,=.(
&gt;?@!A8BC(
1
DEFG* )
# G(
1
?H()
*
160
</figure>
<figureCaption confidence="0.6064905">
Figure 5: A common problem with GIZA++
Model 4 alignments is a weak distortion model.
The second English “in” is aligned to the wrong
Arabic token. Circles show the gold alignment.
</figureCaption>
<equation confidence="0.998915">
γ(y) = `(yi,y) + w - (h(yi) − h(y)) (1)
</equation>
<bodyText confidence="0.999864">
where `(yi,y) is a loss function describing how bad
it is to guess y when the correct answer is yi. In our
case, we define `(yi,y) as 1−F1(yi,y). We select the
oracle alignment according to:
</bodyText>
<equation confidence="0.997697">
y+ = arg min γ(y) (2)
yEᴄᴀᴅ(x)
</equation>
<bodyText confidence="0.99994625">
where ᴄᴀᴅ(x) is a set of hypothesis alignments
generated from input x. Instead of the traditional
oracle, which is calculated solely with respect to
the loss `(yi,y), we choose the oracle that jointly
minimizes the loss and the difference in model
score to the true alignment. Note that Equation 2
is equivalent to maximizing the sum of the F-
measure and model score of y:
</bodyText>
<equation confidence="0.9623275">
y+ = arg max (F1(yi, y) + w - h(y)) (3)
yEᴄᴀᴅ(x)
Let yˆ be the 1-best alignment according to our
model:
yˆ = arg max w - h(y) (4)
yEᴄᴀᴅ(x)
Then, at each iteration our weight update is:
w E— w + η(h(y+) − h(ˆy)) (5)
</equation>
<bodyText confidence="0.99960025">
where η is a learning rate parameter.3 We find
that this more conservative update gives rise to a
much more stable search. After each iteration, we
expect y+ to get closer and closer to the true yi.
</bodyText>
<sectionHeader confidence="0.999586" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.936221">
Our simple, flexible linear model makes it easy to
throw in many features, mapping a given complex
</bodyText>
<footnote confidence="0.525566">
3We set η to 0.05 in our experiments.
</footnote>
<bodyText confidence="0.999950210526316">
alignment structure into a single high-dimensional
feature vector. Our hierarchical search framework
allows us to compute these features when needed,
and affords us extra useful syntactic information.
We use two classes of features: local and non-
local. Huang (2008) defines a feature h to be lo-
cal if and only if it can be factored among the lo-
cal productions in a tree, and non-local otherwise.
Analogously for alignments, our class of local fea-
tures are those that can be factored among the local
partial alignments competing to comprise a larger
span of the matrix, and non-local otherwise. These
features score a set of links and the words con-
nected by them.
Feature development Our features are inspired
by analysis of patterns contained among our gold
alignment data and automatically generated parse
trees. We use both local lexical and nonlocal struc-
tural features as described below.
</bodyText>
<subsectionHeader confidence="0.980133">
4.1 Local features
</subsectionHeader>
<bodyText confidence="0.998843">
These features fire on single-column spans.
</bodyText>
<listItem confidence="0.9705965">
• From the output of GIZA++ Model 4, we
compute lexical probabilities p(e  |f) and
p(f  |e), as well as a fertility table φ(e).
From the fertility table, we fire features φ0(e),
φ1(e), and φ2+(e) when a word e is aligned
to zero, one, or two or more words, respec-
tively. Lexical probability features p(e  |f)
and p(f  |e) fire when a word e is aligned to
a word f.
• Based on these features, we include a binary
lexical-zero feature that fires if both p(e  |f)
and p(f  |e) are equal to zero for a given word
pair (e, f). Negative weights essentially pe-
nalize alignments with links never seen be-
fore in the Model 4 alignment, and positive
weights encourage such links. We employ a
separate instance of this feature for each En-
glish part-of-speech tag: p(f  |e, t).
</listItem>
<bodyText confidence="0.999713">
We learn a different feature weight for each.
Critically, this feature tells us how much to
trust alignments involving nouns, verbs, ad-
jectives, function words, punctuation, etc.
from the Model 4 alignments from which our
p(e  |f) and p(f  |e) tables are built. Ta-
ble 1 shows a sample of learned weights. In-
tuitively, alignments involving English parts-
of-speech more likely to be content words
(e.g. NNPS, NNS, NN) are more trustworthy
</bodyText>
<figure confidence="0.952937571428571">
...
...
161
PP NP VP
IN NP DT NP VBD VP
eprep ... ehead edeY ... ehead everb ... ehead
f f f
</figure>
<figureCaption confidence="0.967837333333333">
Figure 6: Features PP-NP-head, NP-DT-head, and VP-VP-head fire on these tree-alignment patterns. For
example, PP-NP-head fires exactly when the head of the PP is aligned to exactly the same f words as the
head of it’s sister NP.
</figureCaption>
<table confidence="0.994795375">
Penalty
NNPS −1.11
NNS −1.03
NN −0.80
NNP −0.62
VB −0.54
VBG −0.52
JJ −0.50
JJS −0.46
VBN −0.45
POS −0.0093
EX −0.0056
RP −0.0037
WP$ −0.0011
TO 0.037
Reward
</table>
<tableCaption confidence="0.998508">
Table 1: A sampling of learned weights for the lex-
</tableCaption>
<bodyText confidence="0.978445571428571">
ical zero feature. Negative weights penalize links
never seen before in a baseline alignment used to
initialize lexical p(e  |f) and p(f  |e) tables. Posi-
tive weights outright reward such links.
than those likely to be function words (e.g.
TO, RP, EX), where the use of such words is
often radically different across languages.
</bodyText>
<listItem confidence="0.98080575">
• We also include a measure of distortion.
This feature returns the distance to the diag-
onal of the matrix for any link in a partial
alignment. If there is more than one link, we
return the distance of the link farthest from
the diagonal.
• As a lexical backoff, we include a tag prob-
ability feature, p(t  |f) that fires for some
</listItem>
<bodyText confidence="0.73471575">
link (e, f) if the part-of-speech tag of e is t.
The conditional probabilities in this table are
computed from our parse trees and the base-
line Model 4 alignments.
</bodyText>
<listItem confidence="0.9873851">
• In cases where the lexical probabilities are
too strong for the distortion feature to
overcome (see Figure 5), we develop the
multiple-distortion feature. Although local
features do not know the partial alignments at
other spans, they do have access to the entire
English sentence at every step because our in-
put is constant. If some e exists more than
once in en1 we fire this feature on all links con-
taining word e, returning again the distance to
the diagonal for that link. We learn a strong
negative weight for this feature.
• We find that binary identity and
punctuation-mismatch features are im-
portant. The binary identity feature fires if
e = f, and proves useful for untranslated
numbers, symbols, names, and punctuation
in the data. Punctuation-mismatch fires on
any link that causes nonpunctuation to be
aligned to punctuation.
</listItem>
<bodyText confidence="0.999291090909091">
Additionally, we include fine-grained versions of
the lexical probability, fertility, and distortion fea-
tures. These fire for for each link (e, f) and part-
of-speech tag. That is, we learn a separate weight
for each feature for each part-of-speech tag in our
data. Given the tag of e, this affords the model the
ability to pay more or less attention to the features
described above depending on the tag given to e.
Arabic-English specific features We describe
here language specific features we implement to
exploit shallow Arabic morphology.
</bodyText>
<figure confidence="0.9402904">
... ...
162
PP
IN NP
from...
</figure>
<figureCaption confidence="0.997971">
Figure 7: This figure depicts the tree/alignment
</figureCaption>
<bodyText confidence="0.919387142857143">
structure for which the feature PP-from-prep
fires. The English preposition “from” is aligned
to Arabic word �e. Any aligned words in the span
of the sister NP are aligned to words following �e.
English preposition structure commonly matches
that of Arabic in our gold data. This family of fea-
tures captures these observations.
</bodyText>
<listItem confidence="0.7366954">
• We observe the Arabic prefix9, transliterated
w- and generally meaning and, to prepend to
most any word in the lexicon, so we define
features p,v(e  |f) and p„v(f  |e). If f be-
gins with w-, we strip off the prefix and return
the values of p(e  |f) and p(f  |e). Otherwise,
these features return 0.
• We also include analogous feature functions
for several functional and pronominal pre-
fixes and suffixes.4
</listItem>
<subsectionHeader confidence="0.977446">
4.2 Nonlocal features
</subsectionHeader>
<bodyText confidence="0.999908428571428">
These features comprise the combination cost
component of a partial alignment score and may
fire when concatenating two partial alignments
to create a larger span. Because these features
can look into any two arbitrary subtrees, they
are considered nonlocal features as defined by
Huang (2008).
</bodyText>
<listItem confidence="0.769418">
• Features PP-NP-head, NP-DT-head, and
VP-VP-head (Figure 6) all exploit head-
</listItem>
<bodyText confidence="0.8941724">
words on the parse tree. We observe English
prepositions and determiners to often align to
the headword of their sister. Likewise, we ob-
serve the head of a VP to align to the head of
an immediate sister VP.
</bodyText>
<subsectionHeader confidence="0.266104">
4Affixes used by our model are currently: ���, J, A JL��,
</subsectionHeader>
<bodyText confidence="0.948874537037037">
�, �_ L,C, �+�, L.+.. Others either we did not experiment
�
with, or seemed to provide no significant benefit, and are not
included.
In Figure 4, when the search arrives at the
left-most NPB node, the NP-DT-head fea-
ture will fire given this structure and links
over the span [the ... tests]. When
search arrives at the second NPB node, it
will fire given the structure and links over the
span [the ... missle], but will not fire at
the right-most NPB node.
• Local lexical preference features compete
with the headword features described above.
However, we also introduce nonlocal lexical-
ized features for the most common types of
English and foreign prepositions to also com-
pete with these general headword features.
PP features PP-of-prep, PP-from-prep, PP-
to-prep, PP-on-prep, and PP-in-prep fire at
any PP whose left child is a preposition and
right child is an NP. The head of the PP is one
of the enumerated English prepositions and is
aligned to any of the three most common for-
eign words to which it has also been observed
aligned in the gold alignments. The last con-
straint on this pattern is that all words un-
der the span of the sister NP, if aligned, must
align to words following the foreign preposi-
tion. Figure 7 illustrates this pattern.
• Finally, we have a tree-distance feature to
avoid making too many many-to-one (from
many English words to a single foreign word)
links. This is a simplified version of and sim-
ilar in spirit to the tree distance metric used
in (DeNero and Klein, 2007). For any pair of
links (ei, f) and (ej, f) in which the e words
differ but the f word is the same token in
each, return the tree height of first common
ancestor of ei and ej.
This feature captures the intuition that it is
much worse to align two English words at
different ends of the tree to the same foreign
word, than it is to align two English words
under the same NP to the same foreign word.
To see why a string distance feature that
counts only the flat horizontal distance from
ei to ej is not the best strategy, consider the
following. We wish to align a determiner
to the same f word as its sister head noun
under the same NP. Now suppose there are
several intermediate adjectives separating the
determiner and noun. A string distance met-
...
</bodyText>
<page confidence="0.971374">
163
</page>
<bodyText confidence="0.82435325">
Training F−measure
ric, with no knowledge of the relationship be-
tween determiner and noun will levy a much
heavier penalty than its tree distance analog.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999985032258064">
Recent work has shown the potential for syntac-
tic information encoded in various ways to sup-
port inference of superior word alignments. Very
recent work in word alignment has also started to
report downstream effects on BLEU score.
Cherry and Lin (2006) introduce soft syntac-
tic ITG (Wu, 1997) constraints into a discrimi-
native model, and use an ITG parser to constrain
the search for a Viterbi alignment. Haghighi et
al. (2009) confirm and extend these results, show-
ing BLEU improvement for a hierarchical phrase-
based MT system on a small Chinese corpus.
As opposed to ITG, we use a linguistically mo-
tivated phrase-structure tree to drive our search
and inform our model. And, unlike ITG-style ap-
proaches, our model can generate arbitrary align-
ments and learn from arbitrary gold alignments.
DeNero and Klein (2007) refine the distor-
tion model of an HMM aligner to reflect tree
distance instead of string distance. Fossum et
al. (2008) start with the output from GIZA++
Model-4 union, and focus on increasing precision
by deleting links based on a linear discriminative
model exposed to syntactic and lexical informa-
tion.
Fraser and Marcu (2007) take a semi-supervised
approach to word alignment, using a small amount
of gold data to further tune parameters of a
headword-aware generative model. They show
a significant improvement over a Model-4 union
baseline on a very large corpus.
</bodyText>
<sectionHeader confidence="0.998807" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999752818181818">
We evaluate our model and and resulting align-
ments on Arabic-English data against those in-
duced by IBM Model-4 using GIZA++ (Och and
Ney, 2003) with both the union and grow-diag-
final heuristics. We use 1,000 sentence pairs and
gold alignments from LDC2006E86 to train model
parameters: 800 sentences for training, 100 for
testing, and 100 as a second held-out development
set to decide when to stop perceptron training. We
also align the test data using GIZA++5 along with
50 million words of English.
</bodyText>
<footnote confidence="0.910028333333333">
5We use a standard training procedure: 5 iterations of
Model-1, 5 iterations of HMM, 3 iterations of Model-3, and 3
iterations of Model-4.
</footnote>
<figure confidence="0.871834916666667">
0.775
0.77
0.765
0.76
0.755
0.75
0.745
0.74
0.735
0.73
0 5 10 15 20 25 30 35 40
Training epoch
</figure>
<figureCaption confidence="0.741335833333333">
Figure 8: Learning curves for 10 random restarts
over time for parallel averaged perceptron train-
ing. These plots show the current F-measure on
the training set as time passes. Perceptron training
here is quite stable, converging to the same general
neighborhood each time.
</figureCaption>
<figure confidence="0.999646692307692">
0.76
0.75
0.74
0.73
0.72
0.71
0.70
0.69
0.68
0.67
F-measure
Model 1 HMM Model 4
Initial alignments
</figure>
<figureCaption confidence="0.999769">
Figure 9: Model robustness to the initial align-
</figureCaption>
<bodyText confidence="0.652763666666667">
ments from which the p(e I f) and p(f I e) features
are derived. The dotted line indicates the baseline
accuracy of GIZA++ Model 4 alone.
</bodyText>
<subsectionHeader confidence="0.999221">
6.1 Alignment Quality
</subsectionHeader>
<bodyText confidence="0.999884923076923">
We empirically choose our beam size k from the
results of a series of experiments, setting k=1, 2,
4, 8, 16, 32, and 64. We find setting k = 16 to yield
the highest accuracy on our held-out test data. Us-
ing wider beams results in higher F-measure on
training data, but those gains do not translate into
higher accuracy on held-out data.
The first three columns of Table 2 show the
balanced F-measure, Precision, and Recall of our
alignments versus the two GIZA++ Model-4 base-
lines. We report an F-measure 8.6 points over
Model-4 union, and 6.3 points over Model-4 grow-
diag-final.
</bodyText>
<page confidence="0.99567">
164
</page>
<table confidence="0.9958184">
F P R Arabic/English # Unknown
BLEU Words
M4 (union) .665 .636 .696 45.1 2,538
M4 (grow-diag-final) .688 .702 .674 46.4 2,262
Hypergraph alignment .751 .780 .724 47.5 1,610
</table>
<tableCaption confidence="0.986579">
Table 2: F-measure, Precision, Recall, the resulting BLEU score, and number of unknown words on a
</tableCaption>
<bodyText confidence="0.9992441875">
held-out test corpus for three types of alignments. BLEU scores are case-insensitive IBM BLEU. We
show a 1.1 BLEU increase over the strongest baseline, Model-4 grow-diag-final. This is statistically
significant at the p &lt; 0.01 level.
Figure 8 shows the stability of the search proce-
dure over ten random restarts of parallel averaged
perceptron training with 40 CPUs. Training ex-
amples are randomized at each epoch, leading to
slight variations in learning curves over time but
all converge into the same general neighborhood.
Figure 9 shows the robustness of the model to
initial alignments used to derive lexical features
p(e I f) and p(f I e). In addition to IBM Model 4,
we experiment with alignments from Model 1 and
the HMM model. In each case, we significantly
outperform the baseline GIZA++ Model 4 align-
ments on a heldout test set.
</bodyText>
<subsectionHeader confidence="0.99642">
6.2 MT Experiments
</subsectionHeader>
<bodyText confidence="0.99997328125">
We align a corpus of 50 million words with
GIZA++ Model-4, and extract translation rules
from a 5.4 million word core subset. We align
the same core subset with our trained hypergraph
alignment model, and extract a second set of trans-
lation rules. For each set of translation rules, we
train a machine translation system and decode a
held-out test corpus for which we report results be-
low.
We use a syntax-based translation system for
these experiments. This system transforms Arabic
strings into target English syntax trees Translation
rules are extracted from (e-tree, f-string, align-
ment) triples as in (Galley et al., 2004; Galley et
al., 2006).
We use a randomized language model (similar
to that of Talbot and Brants (2008)) of 472 mil-
lion English words. We tune the the parameters
of the MT system on a held-out development cor-
pus of 1,172 parallel sentences, and test on a held-
out parallel corpus of 746 parallel sentences. Both
corpora are drawn from the NIST 2004 and 2006
evaluation data, with no overlap at the document
or segment level with our training data.
Columns 4 and 5 in Table 2 show the results
of our MT experiments. Our hypergraph align-
ment algorithm allows us a 1.1 BLEU increase over
the best baseline system, Model-4 grow-diag-final.
This is statistically significant at the p &lt; 0.01
level. We also report a 2.4 BLEU increase over
a system trained with alignments from Model-4
union.
</bodyText>
<sectionHeader confidence="0.998375" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999972">
We have opened up the word alignment task to
advances in hypergraph algorithms currently used
in parsing and machine translation decoding. We
treat word alignment as a parsing problem, and
by taking advantage of English syntax and the hy-
pergraph structure of our search algorithm, we re-
port significant increases in both F-measure and
BLEU score over standard baselines in use by most
state-of-the-art MT systems today.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999142857142857">
We would like to thank our colleagues in the Nat-
ural Language Group at ISI for many meaningful
discussions and the anonymous reviewers for their
thoughtful suggestions. This research was sup-
ported by DARPA contract HR0011-06-C-0022
under subcontract to BBN Technologies, and a
USC CREATE Fellowship to the first author.
</bodyText>
<sectionHeader confidence="0.998955" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998657777777778">
Phil Blunsom and Trevor Cohn. 2006. Discriminative
Word Alignment with Conditional Random Fields.
In Proceedings of the 44th Annual Meeting of the
ACL. Sydney, Australia.
Peter F. Brown, Stephen A. Della Pietra, Vincent Della
J. Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263–
312. MIT Press. Camrbidge, MA. USA.
</reference>
<page confidence="0.983809">
165
</page>
<reference confidence="0.999957575471699">
Colin Cherry and Dekang Lin. 2006. Soft Syntactic
Constraints for Word Alignment through Discrimi-
native Training. In Proceedings of the 44th Annual
Meeting of the ACL. Sydney, Australia.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics. 33(2):201–228.
MIT Press. Cambridge, MA. USA.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online Large-Margin Training of Syntactic and
Structural Translation Features. In Proceedings of
EMNLP. Honolulu, HI. USA.
Michael Collins. 2003. Head-Driven Statistical Mod-
els for Natural Language Parsing. Computational
Linguistics. 29(4):589–637. MIT Press. Cam-
bridge, MA. USA.
Michael Collins 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
John DeNero and Dan Klein. 2007. Tailoring Word
Alignments to Syntactic Machine Translation. In
Proceedings of the 45th Annual Meeting of the ACL.
Prague, Czech Republic.
Alexander Fraser and Daniel Marcu. 2007. Getting
the Structure Right for Word Alignment: LEAF. In
Proceedings of EMNLP-CoNLL. Prague, Czech Re-
public.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using Syntax to Improve Word Alignment
Precision for Syntax-Based Machine Translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation. Columbus, Ohio.
Dan Klein and Christopher D. Manning. 2001. Parsing
and Hypergraphs. In Proceedings of the 7th Interna-
tional Workshop on Parsing Technologies. Beijing,
China.
Aria Haghighi, John Blitzer, and Dan Klein. 2009.
Better Word Alignments with Supervised ITG Mod-
els. In Proceedings of ACL-IJCNLP 2009. Singa-
pore.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of the 9th International
Workshop on Parsing Technologies. Vancouver, BC.
Canada.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the ACL. Prague, Czech Republic.
Liang Huang. 2008. Forest Reranking: Discriminative
Parsing with Non-Local Features. In Proceedings
of the 46th Annual Meeting of the ACL. Columbus,
OH. USA.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a Translation Rule?
In Proceedings of NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training of
Context-Rich Syntactic Models In Proceedings of
the 44th Annual Meeting of the ACL. Sydney, Aus-
tralia.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT-EMNLP.
Vancouver, BC. Canada.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I. Jordan. 2006. Word alignment via
Quadratic Assignment. In Proceedings of HLT-
EMNLP. New York, NY. USA.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear Models for Word Alignment In Proceedings
of the 43rd Annual Meeting of the ACL. Ann Arbor,
Michigan. USA.
Robert C. Moore. 2005. A Discriminative Framework
for Word Alignment. In Proceedings of EMNLP.
Vancouver, BC. Canada.
Robert C. Moore, Wen-tau Yih, and Andreas Bode.
2006. Improved Discriminative Bilingual Word
Alignment In Proceedings of the 44th Annual Meet-
ing of the ACL. Sydney, Australia.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics. 29(1):19–52.
MIT Press. Cambridge, MA. USA.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation In Proceedings of the
44th Annual Meeting of the ACL. Sydney, Australia.
Kishore Papineni, Salim Roukos, T. Ward, and W-J.
Zhu. 2002. BLEU: A Method for Automatic Evalu-
ation of Machine Translation In Proceedings of the
40th Annual Meeting of the ACL. Philadelphia, PA.
USA.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A Discriminative Matching Approach to
Word Alignment. In Proceedings of HLT-EMNLP.
Vancouver, BC. Canada.
David Talbot and Thorsten Brants. 2008. Random-
ized Language Models via Perfect Hash Functions.
In Proceedings of ACL-08: HLT. Columbus, OH.
USA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics. 23(3):377–404. MIT
Press. Cambridge, MA. USA.
</reference>
<page confidence="0.998764">
166
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956565">
<title confidence="0.998346">Hierarchical Search for Word Alignment</title>
<author confidence="0.984828">Riesa Marcu</author>
<affiliation confidence="0.993183666666667">Information Sciences Institute Viterbi School of Engineering University of Southern California</affiliation>
<abstract confidence="0.999463411764706">We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we extract a ranked list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outa baseline by points in F-measure, yielding a 1.1 increase over a state-of-the-art syntax-based machine translation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Discriminative Word Alignment with Conditional Random Fields.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the ACL.</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2371" citStr="Blunsom and Cohn, 2006" startWordPosition="361" endWordPosition="364">end a given generative model with feature functions without changing the entire generative story. This difficulty !&amp;quot;#$%!&amp;!&apos;() &amp;quot;* +,-* !&amp;.( /023 1( 5!67 !*,8.( 9:; &lt;)+,=.( &gt;?@!A8BC( 1 DEFG* ) # G( 1 ?H() * Figure 1: Model-4 alignment vs. a gold standard. Circles represent links in a human-annotated alignment, and black boxes represent links in the Model-4 alignment. Bold gray boxes show links gained after fully connecting the alignment. has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for </context>
</contexts>
<marker>Blunsom, Cohn, 2006</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2006. Discriminative Word Alignment with Conditional Random Fields. In Proceedings of the 44th Annual Meeting of the ACL. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent Della J Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>312</pages>
<publisher>MIT Press. Camrbidge, MA. USA.</publisher>
<contexts>
<context position="1140" citStr="Brown et al., 1993" startWordPosition="163" endWordPosition="166">d trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system. 1 Introduction Automatic word alignment is generally accepted as a first step in training any statistical machine translation system. It is a vital prerequisite for generating translation tables, phrase tables, or syntactic transformation rules. Generative alignment models like IBM Model-4 (Brown et al., 1993) have been in wide use for over 15 years, and while not perfect (see Figure 1), they are completely unsupervised, requiring no annotated training data to learn alignments that have powered many current state-of-the-art translation system. Today, there exist human-annotated alignments and an abundance of other information for many language pairs potentially useful for inducing accurate alignments. How can we take advantage of all of this data at our fingertips? Using feature functions that encode extra information is one good way. Unfortunately, as Moore (2005) points out, it is usually difficu</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent Della J. Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263– 312. MIT Press. Camrbidge, MA. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Soft Syntactic Constraints for Word Alignment through Discriminative Training.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the ACL.</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="22303" citStr="Cherry and Lin (2006)" startWordPosition="3921" endWordPosition="3924">lign a determiner to the same f word as its sister head noun under the same NP. Now suppose there are several intermediate adjectives separating the determiner and noun. A string distance met... 163 Training F−measure ric, with no knowledge of the relationship between determiner and noun will levy a much heavier penalty than its tree distance analog. 5 Related Work Recent work has shown the potential for syntactic information encoded in various ways to support inference of superior word alignments. Very recent work in word alignment has also started to report downstream effects on BLEU score. Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. Haghighi et al. (2009) confirm and extend these results, showing BLEU improvement for a hierarchical phrasebased MT system on a small Chinese corpus. As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model. And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments. DeNero and Klein (2007) refine the distortion model of an HM</context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>Colin Cherry and Dekang Lin. 2006. Soft Syntactic Constraints for Word Alignment through Discriminative Training. In Proceedings of the 44th Annual Meeting of the ACL. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics.</journal>
<volume>33</volume>
<issue>2</issue>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2761" citStr="Chiang, 2007" startWordPosition="425" endWordPosition="426">d after fully connecting the alignment. has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our algorithm yields a forest of 157 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics ﺰﺒﳋﺍ ﻞﺟﺍﺮﻟﺍ ﻞﻛﺍ ﺰﺒﳋﺍ ﻞﻛ ﻞﺟﺮﻟﺍ ﺍ the man ate the NP S VP NP bread ﻞﻛﺍ ﻞﺟﺮﻟﺍ ﻞﻛﺍ ﻞﺟﺮﻟﺍ ﻞﻛﺍ ﻞﺟﺮﻟﺍ ﺰﺒﳋﺍ ﺰﺒﳋﺍ ﺰﺒﳋﺍ Figure 2: </context>
<context position="7252" citStr="Chiang, 2007" startWordPosition="1255" endWordPosition="1256">partial alignments from child nodes until we have constructed a single full alignment at the root node of the tree. If we are interested in the k-best, we continue to populate the root node until we have k alignments.1 We use one set of feature functions for preterminal nodes, and another set for nonterminal nodes. This is analogous to local and nonlocal feature functions for parse-reranking used by Huang (2008). Using nonlocal features at a nonterminal node emits a combination cost for composing a set of child partial alignments. Because combination costs come into play, we use cube pruning (Chiang, 2007) to approximate the k-best combinations at some nonterminal node v. Inference is exact when only local features are used. Assumptions There are certain assumptions related to our search algorithm that we must make: 1We use approximate dynamic programming to store alignments, keeping only scored lists of pointers to initial single-column spans. Each item in the list is a derivation that implies a partial alignment. (1) that using the structure of 1-best English syntactic parse trees is a reasonable way to frame and drive our search, and (2) that F-measure approximately decomposes over hyperedge</context>
<context position="10543" citStr="Chiang, 2007" startWordPosition="1854" endWordPosition="1855">2), k,w,h) 23 end {|f |1 tions of the top n = max 2 , 10 scoring singlelink alignments. We limit the number of total partial alignments αv kept at each node to k. If at any time we wish to push onto the heap a new partial alignment when the heap is full, we pop the current worst off the heap and replace it with our new partial alignment if its score is better than the current worst. Building the hypergraph We now visit internal nodes (Line 16) in the tree in bottom-up order. At each nonterminal node v we wish to combine the partial alignments of its children u1, ... , uc. We use cube pruning (Chiang, 2007; Huang and Chiang, 2007) to select the k-best combinations of the partial alignments of u1, ... , uc (Line 19). Note Figure 4: Correct version of Figure 1 after hypergraph alignment. Subscripts on the nonterminal labels denote the branch containing the head word for that span. that Algorithm 1 assumes a binary tree2, but is not necessary. In the general case, cube pruning will operate on a d-dimensional hypercube, where d is the branching factor of node v. We cannot enumerate and score every possibility; without the cube pruning approximation, we will have kc possible combinations at each nod</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics. 33(2):201–228. MIT Press. Cambridge, MA. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online Large-Margin Training of Syntactic and Structural Translation Features.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<location>Honolulu, HI. USA.</location>
<contexts>
<context position="11534" citStr="Chiang et al. (2008)" startWordPosition="2020" endWordPosition="2023">ning will operate on a d-dimensional hypercube, where d is the branching factor of node v. We cannot enumerate and score every possibility; without the cube pruning approximation, we will have kc possible combinations at each node, exploding the search space exponentially. Figure 3 depicts how we select the top-k alignments at a node v from its children ( u1, u2 ). 3 Discriminative training We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al. (2008). We define: 2We find empirically that using binarized trees reduces search errors in cube pruning. S-BAR1 S2 VP1 VP-C1 VP-C1 PP1 DT NP-C1 NPB2 NPB-BAR2 NPB-BAR2 CD JJ NNS VBP VBN VBN NP-C1 NP-C-BAR1 NP1 NPB2 NPB-BAR2 IN DT NN NN CC NP1 NPB2 NPB-BAR2 CD JJ NN � !&amp;quot;#$%!&amp;&apos;() ! &amp;quot;* +,-* !&amp;.( /0231( 5!67 !*,8.( 9:; &lt;)+,=.( &gt;?@!A8BC( 1 DEFG* ) # G( 1 ?H() * 160 Figure 5: A common problem with GIZA++ Model 4 alignments is a weak distortion model. The second English “in” is aligned to the wrong Arabic token. Circles show the gold alignment. γ(y) = `(yi,y) + w - (h(yi) − h(y)) (1) where `(yi,y) is a los</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online Large-Margin Training of Syntactic and Structural Translation Features. In Proceedings of EMNLP. Honolulu, HI. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>2003</date>
<journal>Computational Linguistics.</journal>
<volume>29</volume>
<issue>4</issue>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4823" citStr="Collins, 2003" startWordPosition="772" endWordPosition="773">ut can easily fit into a max-margin or related framework. Finally, we use relatively little training data to achieve accurate word alignments. Our model can generate arbitrary alignments and learn from arbitrary gold alignments. 2 Word Alignment as a Hypergraph Algorithm input The input to our alignment algorithm is a sentence-pair (en1, f1m) and a parse tree over one of the input sentences. In this work, we parse our English data, and for each sentence E = en1, let T be its syntactic parse. To generate parse trees, we use the Berkeley parser (Petrov et al., 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. Overview We present a brief overview here and delve deeper in Section 2.1. Word alignments are built bottom-up on the parse tree. Each node v in the tree holds partial alignments sorted by score. 158 u u12 u13 11 u11 u12 u13 12 13 2.2 1 5 . 2.2 4.1 5.5 4.1 .5 2.4 3.5 2 2.4 .5 7.2 .4 3.5 7.2 3.2 5 14 .2 4.5 11.4 3.2 4.5 1.4 (a) Score the left corner alignment first. Assume it is the 1- best. Numbers in the rest of the boxes are hidden at this point. u1 u12 u13 u1 u12 u13 u1 u1 u1 2. 4.1 5. 2.2 4. 5.5 2.2 4.1 5.5 2.4 3.5 7.2 24 3.5 7.2 2.4 3.5 7.2 3.2 4.5 1.4 3.2</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-Driven Statistical Models for Natural Language Parsing. Computational Linguistics. 29(4):589–637. MIT Press. Cambridge, MA. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4174" citStr="Collins, 2002" startWordPosition="660" endWordPosition="661">is figure, we see that the partial alignment implied by the 1-best hypothesis at the leftmost NP node is constructed by composing the best hypothesis at the terminal node labeled “the” and the 2ndbest hypothesis at the terminal node labeled “man”. (We ignore terminal nodes in this toy example.) Hypotheses at the root node imply full alignment structures. word alignments, from which we can efficiently extract the k-best. We handle an arbitrary number of features, compute them efficiently, and score alignments using a linear model. We train the parameters of the model using averaged perceptron (Collins, 2002) modified for structured outputs, but can easily fit into a max-margin or related framework. Finally, we use relatively little training data to achieve accurate word alignments. Our model can generate arbitrary alignments and learn from arbitrary gold alignments. 2 Word Alignment as a Hypergraph Algorithm input The input to our alignment algorithm is a sentence-pair (en1, f1m) and a parse tree over one of the input sentences. In this work, we parse our English data, and for each sentence E = en1, let T be its syntactic parse. To generate parse trees, we use the Berkeley parser (Petrov et al., </context>
<context position="11453" citStr="Collins, 2002" startWordPosition="2008" endWordPosition="2009">assumes a binary tree2, but is not necessary. In the general case, cube pruning will operate on a d-dimensional hypercube, where d is the branching factor of node v. We cannot enumerate and score every possibility; without the cube pruning approximation, we will have kc possible combinations at each node, exploding the search space exponentially. Figure 3 depicts how we select the top-k alignments at a node v from its children ( u1, u2 ). 3 Discriminative training We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al. (2008). We define: 2We find empirically that using binarized trees reduces search errors in cube pruning. S-BAR1 S2 VP1 VP-C1 VP-C1 PP1 DT NP-C1 NPB2 NPB-BAR2 NPB-BAR2 CD JJ NNS VBP VBN VBN NP-C1 NP-C-BAR1 NP1 NPB2 NPB-BAR2 IN DT NN NN CC NP1 NPB2 NPB-BAR2 CD JJ NN � !&amp;quot;#$%!&amp;&apos;() ! &amp;quot;* +,-* !&amp;.( /0231( 5!67 !*,8.( 9:; &lt;)+,=.( &gt;?@!A8BC( 1 DEFG* ) # G( 1 ?H() * 160 Figure 5: A common problem with GIZA++ Model 4 alignments is a weak distortion model. The second English “in” is aligned to the wrong Arabic token. Circles show t</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring Word Alignments to Syntactic Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL.</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="21124" citStr="DeNero and Klein, 2007" startWordPosition="3706" endWordPosition="3709"> the PP is one of the enumerated English prepositions and is aligned to any of the three most common foreign words to which it has also been observed aligned in the gold alignments. The last constraint on this pattern is that all words under the span of the sister NP, if aligned, must align to words following the foreign preposition. Figure 7 illustrates this pattern. • Finally, we have a tree-distance feature to avoid making too many many-to-one (from many English words to a single foreign word) links. This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). For any pair of links (ei, f) and (ej, f) in which the e words differ but the f word is the same token in each, return the tree height of first common ancestor of ei and ej. This feature captures the intuition that it is much worse to align two English words at different ends of the tree to the same foreign word, than it is to align two English words under the same NP to the same foreign word. To see why a string distance feature that counts only the flat horizontal distance from ei to ej is not the best strategy, consider the following. We wish to align a determiner to the same f word as it</context>
<context position="22866" citStr="DeNero and Klein (2007)" startWordPosition="4014" endWordPosition="4017">port downstream effects on BLEU score. Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. Haghighi et al. (2009) confirm and extend these results, showing BLEU improvement for a hierarchical phrasebased MT system on a small Chinese corpus. As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model. And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments. DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. Fossum et al. (2008) start with the output from GIZA++ Model-4 union, and focus on increasing precision by deleting links based on a linear discriminative model exposed to syntactic and lexical information. Fraser and Marcu (2007) take a semi-supervised approach to word alignment, using a small amount of gold data to further tune parameters of a headword-aware generative model. They show a significant improvement over a Model-4 union baseline on a very large corpus. 6 Experiments We evaluate our</context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein. 2007. Tailoring Word Alignments to Syntactic Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Getting the Structure Right for Word Alignment: LEAF.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="23196" citStr="Fraser and Marcu (2007)" startWordPosition="4068" endWordPosition="4071">ystem on a small Chinese corpus. As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model. And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments. DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. Fossum et al. (2008) start with the output from GIZA++ Model-4 union, and focus on increasing precision by deleting links based on a linear discriminative model exposed to syntactic and lexical information. Fraser and Marcu (2007) take a semi-supervised approach to word alignment, using a small amount of gold data to further tune parameters of a headword-aware generative model. They show a significant improvement over a Model-4 union baseline on a very large corpus. 6 Experiments We evaluate our model and and resulting alignments on Arabic-English data against those induced by IBM Model-4 using GIZA++ (Och and Ney, 2003) with both the union and grow-diagfinal heuristics. We use 1,000 sentence pairs and gold alignments from LDC2006E86 to train model parameters: 800 sentences for training, 100 for testing, and 100 as a s</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Getting the Structure Right for Word Alignment: LEAF. In Proceedings of EMNLP-CoNLL. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
<author>Steven Abney</author>
</authors>
<title>Using Syntax to Improve Word Alignment Precision for Syntax-Based Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation.</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="22986" citStr="Fossum et al. (2008)" startWordPosition="4035" endWordPosition="4038">criminative model, and use an ITG parser to constrain the search for a Viterbi alignment. Haghighi et al. (2009) confirm and extend these results, showing BLEU improvement for a hierarchical phrasebased MT system on a small Chinese corpus. As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model. And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments. DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. Fossum et al. (2008) start with the output from GIZA++ Model-4 union, and focus on increasing precision by deleting links based on a linear discriminative model exposed to syntactic and lexical information. Fraser and Marcu (2007) take a semi-supervised approach to word alignment, using a small amount of gold data to further tune parameters of a headword-aware generative model. They show a significant improvement over a Model-4 union baseline on a very large corpus. 6 Experiments We evaluate our model and and resulting alignments on Arabic-English data against those induced by IBM Model-4 using GIZA++ (Och and Ne</context>
</contexts>
<marker>Fossum, Knight, Abney, 2008</marker>
<rawString>Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Using Syntax to Improve Word Alignment Precision for Syntax-Based Machine Translation. In Proceedings of the Third Workshop on Statistical Machine Translation. Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and Hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings of the 7th International Workshop on Parsing Technologies.</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="2639" citStr="Klein and Manning, 2001" startWordPosition="406" endWordPosition="409">sent links in a human-annotated alignment, and black boxes represent links in the Model-4 alignment. Bold gray boxes show links gained after fully connecting the alignment. has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our algorithm yields a forest of 157 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistic</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and Hypergraphs. In Proceedings of the 7th International Workshop on Parsing Technologies. Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Better Word Alignments with Supervised ITG Models.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP</booktitle>
<contexts>
<context position="22478" citStr="Haghighi et al. (2009)" startWordPosition="3951" endWordPosition="3954">ring distance met... 163 Training F−measure ric, with no knowledge of the relationship between determiner and noun will levy a much heavier penalty than its tree distance analog. 5 Related Work Recent work has shown the potential for syntactic information encoded in various ways to support inference of superior word alignments. Very recent work in word alignment has also started to report downstream effects on BLEU score. Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. Haghighi et al. (2009) confirm and extend these results, showing BLEU improvement for a hierarchical phrasebased MT system on a small Chinese corpus. As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model. And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments. DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. Fossum et al. (2008) start with the output from GIZA++ Model-4 union, and focus on increasing precision by delet</context>
</contexts>
<marker>Haghighi, Blitzer, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, and Dan Klein. 2009. Better Word Alignments with Supervised ITG Models. In Proceedings of ACL-IJCNLP 2009. Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies.</booktitle>
<publisher>Canada.</publisher>
<location>Vancouver, BC.</location>
<contexts>
<context position="2663" citStr="Huang and Chiang, 2005" startWordPosition="410" endWordPosition="414">otated alignment, and black boxes represent links in the Model-4 alignment. Bold gray boxes show links gained after fully connecting the alignment. has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our algorithm yields a forest of 157 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics ﺰﺒﳋﺍ ﻞﺟﺍﺮﻟﺍ ﻞﻛﺍ ﺰﺒﳋﺍ ﻞ</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best Parsing. In Proceedings of the 9th International Workshop on Parsing Technologies. Vancouver, BC. Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest Rescoring: Faster Decoding with Integrated Language Models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL.</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2761" citStr="Huang and Chiang, 2007" startWordPosition="423" endWordPosition="426">inks gained after fully connecting the alignment. has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our algorithm yields a forest of 157 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics ﺰﺒﳋﺍ ﻞﺟﺍﺮﻟﺍ ﻞﻛﺍ ﺰﺒﳋﺍ ﻞﻛ ﻞﺟﺮﻟﺍ ﺍ the man ate the NP S VP NP bread ﻞﻛﺍ ﻞﺟﺮﻟﺍ ﻞﻛﺍ ﻞﺟﺮﻟﺍ ﻞﻛﺍ ﻞﺟﺮﻟﺍ ﺰﺒﳋﺍ ﺰﺒﳋﺍ ﺰﺒﳋﺍ Figure 2: </context>
<context position="10568" citStr="Huang and Chiang, 2007" startWordPosition="1856" endWordPosition="1860">end {|f |1 tions of the top n = max 2 , 10 scoring singlelink alignments. We limit the number of total partial alignments αv kept at each node to k. If at any time we wish to push onto the heap a new partial alignment when the heap is full, we pop the current worst off the heap and replace it with our new partial alignment if its score is better than the current worst. Building the hypergraph We now visit internal nodes (Line 16) in the tree in bottom-up order. At each nonterminal node v we wish to combine the partial alignments of its children u1, ... , uc. We use cube pruning (Chiang, 2007; Huang and Chiang, 2007) to select the k-best combinations of the partial alignments of u1, ... , uc (Line 19). Note Figure 4: Correct version of Figure 1 after hypergraph alignment. Subscripts on the nonterminal labels denote the branch containing the head word for that span. that Algorithm 1 assumes a binary tree2, but is not necessary. In the general case, cube pruning will operate on a d-dimensional hypercube, where d is the branching factor of node v. We cannot enumerate and score every possibility; without the cube pruning approximation, we will have kc possible combinations at each node, exploding the search s</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. In Proceedings of the 45th Annual Meeting of the ACL. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest Reranking: Discriminative Parsing with Non-Local Features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the ACL.</booktitle>
<location>Columbus, OH. USA.</location>
<contexts>
<context position="2677" citStr="Huang, 2008" startWordPosition="415" endWordPosition="416">ack boxes represent links in the Model-4 alignment. Bold gray boxes show links gained after fully connecting the alignment. has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our algorithm yields a forest of 157 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics ﺰﺒﳋﺍ ﻞﺟﺍﺮﻟﺍ ﻞﻛﺍ ﺰﺒﳋﺍ ﻞﻛ ﻞﺟﺮﻟﺍ ﺍ the </context>
<context position="7054" citStr="Huang (2008)" startWordPosition="1222" endWordPosition="1223">rix. To speed up search, we can prune at each node, keeping a beam of size k. In the diagram depicted in Figure 2, the beam is size k = 5. From here, we traverse the tree nodes bottomup, combining partial alignments from child nodes until we have constructed a single full alignment at the root node of the tree. If we are interested in the k-best, we continue to populate the root node until we have k alignments.1 We use one set of feature functions for preterminal nodes, and another set for nonterminal nodes. This is analogous to local and nonlocal feature functions for parse-reranking used by Huang (2008). Using nonlocal features at a nonterminal node emits a combination cost for composing a set of child partial alignments. Because combination costs come into play, we use cube pruning (Chiang, 2007) to approximate the k-best combinations at some nonterminal node v. Inference is exact when only local features are used. Assumptions There are certain assumptions related to our search algorithm that we must make: 1We use approximate dynamic programming to store alignments, keeping only scored lists of pointers to initial single-column spans. Each item in the list is a derivation that implies a par</context>
<context position="13537" citStr="Huang (2008)" startWordPosition="2389" endWordPosition="2390">where η is a learning rate parameter.3 We find that this more conservative update gives rise to a much more stable search. After each iteration, we expect y+ to get closer and closer to the true yi. 4 Features Our simple, flexible linear model makes it easy to throw in many features, mapping a given complex 3We set η to 0.05 in our experiments. alignment structure into a single high-dimensional feature vector. Our hierarchical search framework allows us to compute these features when needed, and affords us extra useful syntactic information. We use two classes of features: local and nonlocal. Huang (2008) defines a feature h to be local if and only if it can be factored among the local productions in a tree, and non-local otherwise. Analogously for alignments, our class of local features are those that can be factored among the local partial alignments competing to comprise a larger span of the matrix, and non-local otherwise. These features score a set of links and the words connected by them. Feature development Our features are inspired by analysis of patterns contained among our gold alignment data and automatically generated parse trees. We use both local lexical and nonlocal structural f</context>
<context position="19277" citStr="Huang (2008)" startWordPosition="3382" endWordPosition="3383">n the lexicon, so we define features p,v(e |f) and p„v(f |e). If f begins with w-, we strip off the prefix and return the values of p(e |f) and p(f |e). Otherwise, these features return 0. • We also include analogous feature functions for several functional and pronominal prefixes and suffixes.4 4.2 Nonlocal features These features comprise the combination cost component of a partial alignment score and may fire when concatenating two partial alignments to create a larger span. Because these features can look into any two arbitrary subtrees, they are considered nonlocal features as defined by Huang (2008). • Features PP-NP-head, NP-DT-head, and VP-VP-head (Figure 6) all exploit headwords on the parse tree. We observe English prepositions and determiners to often align to the headword of their sister. Likewise, we observe the head of a VP to align to the head of an immediate sister VP. 4Affixes used by our model are currently: ���, J, A JL��, �, �_ L,C, �+�, L.+.. Others either we did not experiment � with, or seemed to provide no significant benefit, and are not included. In Figure 4, when the search arrives at the left-most NPB node, the NP-DT-head feature will fire given this structure and l</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest Reranking: Discriminative Parsing with Non-Local Features. In Proceedings of the 46th Annual Meeting of the ACL. Columbus, OH. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a Translation Rule? In</title>
<date>2004</date>
<booktitle>Proceedings of NAACL.</booktitle>
<contexts>
<context position="27107" citStr="Galley et al., 2004" startWordPosition="4730" endWordPosition="4733">n a corpus of 50 million words with GIZA++ Model-4, and extract translation rules from a 5.4 million word core subset. We align the same core subset with our trained hypergraph alignment model, and extract a second set of translation rules. For each set of translation rules, we train a machine translation system and decode a held-out test corpus for which we report results below. We use a syntax-based translation system for these experiments. This system transforms Arabic strings into target English syntax trees Translation rules are extracted from (e-tree, f-string, alignment) triples as in (Galley et al., 2004; Galley et al., 2006). We use a randomized language model (similar to that of Talbot and Brants (2008)) of 472 million English words. We tune the the parameters of the MT system on a held-out development corpus of 1,172 parallel sentences, and test on a heldout parallel corpus of 746 parallel sentences. Both corpora are drawn from the NIST 2004 and 2006 evaluation data, with no overlap at the document or segment level with our training data. Columns 4 and 5 in Table 2 show the results of our MT experiments. Our hypergraph alignment algorithm allows us a 1.1 BLEU increase over the best baselin</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a Translation Rule? In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Models</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the ACL.</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="27129" citStr="Galley et al., 2006" startWordPosition="4734" endWordPosition="4737">ion words with GIZA++ Model-4, and extract translation rules from a 5.4 million word core subset. We align the same core subset with our trained hypergraph alignment model, and extract a second set of translation rules. For each set of translation rules, we train a machine translation system and decode a held-out test corpus for which we report results below. We use a syntax-based translation system for these experiments. This system transforms Arabic strings into target English syntax trees Translation rules are extracted from (e-tree, f-string, alignment) triples as in (Galley et al., 2004; Galley et al., 2006). We use a randomized language model (similar to that of Talbot and Brants (2008)) of 472 million English words. We tune the the parameters of the MT system on a held-out development corpus of 1,172 parallel sentences, and test on a heldout parallel corpus of 746 parallel sentences. Both corpora are drawn from the NIST 2004 and 2006 evaluation data, with no overlap at the document or segment level with our training data. Columns 4 and 5 in Table 2 show the results of our MT experiments. Our hypergraph alignment algorithm allows us a 1.1 BLEU increase over the best baseline system, Model-4 grow</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Models In Proceedings of the 44th Annual Meeting of the ACL. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for Arabic-English machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<publisher>Canada.</publisher>
<location>Vancouver, BC.</location>
<contexts>
<context position="2308" citStr="Ittycheriah and Roukos, 2005" startWordPosition="348" endWordPosition="352">rtunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty !&amp;quot;#$%!&amp;!&apos;() &amp;quot;* +,-* !&amp;.( /023 1( 5!67 !*,8.( 9:; &lt;)+,=.( &gt;?@!A8BC( 1 DEFG* ) # G( 1 ?H() * Figure 1: Model-4 alignment vs. a gold standard. Circles represent links in a human-annotated alignment, and black boxes represent links in the Model-4 alignment. Bold gray boxes show links gained after fully connecting the alignment. has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on th</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for Arabic-English machine translation. In Proceedings of HLT-EMNLP. Vancouver, BC. Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Lacoste-Julien</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael I Jordan</author>
</authors>
<title>Word alignment via Quadratic Assignment.</title>
<date>2006</date>
<booktitle>In Proceedings of HLTEMNLP.</booktitle>
<location>New York, NY. USA.</location>
<marker>Lacoste-Julien, Taskar, Klein, Jordan, 2006</marker>
<rawString>Simon Lacoste-Julien, Ben Taskar, Dan Klein, and Michael I. Jordan. 2006. Word alignment via Quadratic Assignment. In Proceedings of HLTEMNLP. New York, NY. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Loglinear Models for Word Alignment</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL.</booktitle>
<location>Ann Arbor, Michigan. USA.</location>
<contexts>
<context position="2326" citStr="Liu et al., 2005" startWordPosition="353" endWordPosition="356">nts out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty !&amp;quot;#$%!&amp;!&apos;() &amp;quot;* +,-* !&amp;.( /023 1( 5!67 !*,8.( 9:; &lt;)+,=.( &gt;?@!A8BC( 1 DEFG* ) # G( 1 ?H() * Figure 1: Model-4 alignment vs. a gold standard. Circles represent links in a human-annotated alignment, and black boxes represent links in the Model-4 alignment. Bold gray boxes show links gained after fully connecting the alignment. has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with</context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglinear Models for Word Alignment In Proceedings of the 43rd Annual Meeting of the ACL. Ann Arbor, Michigan. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A Discriminative Framework for Word Alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<publisher>Canada.</publisher>
<location>Vancouver, BC.</location>
<contexts>
<context position="1706" citStr="Moore (2005)" startWordPosition="253" endWordPosition="254">t models like IBM Model-4 (Brown et al., 1993) have been in wide use for over 15 years, and while not perfect (see Figure 1), they are completely unsupervised, requiring no annotated training data to learn alignments that have powered many current state-of-the-art translation system. Today, there exist human-annotated alignments and an abundance of other information for many language pairs potentially useful for inducing accurate alignments. How can we take advantage of all of this data at our fingertips? Using feature functions that encode extra information is one good way. Unfortunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty !&amp;quot;#$%!&amp;!&apos;() &amp;quot;* +,-* !&amp;.( /023 1( 5!67 !*,8.( 9:; &lt;)+,=.( &gt;?@!A8BC( 1 DEFG* ) # G( 1 ?H() * Figure 1: Model-4 alignment vs. a gold standard. Circles represent links in a human-annotated alignment, and black boxes represent links in the Model-4 alignment. Bold gray boxes show links gained after fully connecting the alignment. has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 20</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert C. Moore. 2005. A Discriminative Framework for Word Alignment. In Proceedings of EMNLP. Vancouver, BC. Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Wen-tau Yih</author>
<author>Andreas Bode</author>
</authors>
<title>Improved Discriminative Bilingual Word Alignment</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the ACL.</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2420" citStr="Moore et al., 2006" startWordPosition="370" endWordPosition="373">without changing the entire generative story. This difficulty !&amp;quot;#$%!&amp;!&apos;() &amp;quot;* +,-* !&amp;.( /023 1( 5!67 !*,8.( 9:; &lt;)+,=.( &gt;?@!A8BC( 1 DEFG* ) # G( 1 ?H() * Figure 1: Model-4 alignment vs. a gold standard. Circles represent links in a human-annotated alignment, and black boxes represent links in the Model-4 alignment. Bold gray boxes show links gained after fully connecting the alignment. has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our</context>
</contexts>
<marker>Moore, Yih, Bode, 2006</marker>
<rawString>Robert C. Moore, Wen-tau Yih, and Andreas Bode. 2006. Improved Discriminative Bilingual Word Alignment In Proceedings of the 44th Annual Meeting of the ACL. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics.</title>
<date>2003</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="23594" citStr="Och and Ney, 2003" startWordPosition="4133" endWordPosition="4136">al. (2008) start with the output from GIZA++ Model-4 union, and focus on increasing precision by deleting links based on a linear discriminative model exposed to syntactic and lexical information. Fraser and Marcu (2007) take a semi-supervised approach to word alignment, using a small amount of gold data to further tune parameters of a headword-aware generative model. They show a significant improvement over a Model-4 union baseline on a very large corpus. 6 Experiments We evaluate our model and and resulting alignments on Arabic-English data against those induced by IBM Model-4 using GIZA++ (Och and Ney, 2003) with both the union and grow-diagfinal heuristics. We use 1,000 sentence pairs and gold alignments from LDC2006E86 to train model parameters: 800 sentences for training, 100 for testing, and 100 as a second held-out development set to decide when to stop perceptron training. We also align the test data using GIZA++5 along with 50 million words of English. 5We use a standard training procedure: 5 iterations of Model-1, 5 iterations of HMM, 3 iterations of Model-3, and 3 iterations of Model-4. 0.775 0.77 0.765 0.76 0.755 0.75 0.745 0.74 0.735 0.73 0 5 10 15 20 25 30 35 40 Training epoch Figure </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics. 29(1):19–52. MIT Press. Cambridge, MA. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the ACL.</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="4779" citStr="Petrov et al., 2006" startWordPosition="763" endWordPosition="766">(Collins, 2002) modified for structured outputs, but can easily fit into a max-margin or related framework. Finally, we use relatively little training data to achieve accurate word alignments. Our model can generate arbitrary alignments and learn from arbitrary gold alignments. 2 Word Alignment as a Hypergraph Algorithm input The input to our alignment algorithm is a sentence-pair (en1, f1m) and a parse tree over one of the input sentences. In this work, we parse our English data, and for each sentence E = en1, let T be its syntactic parse. To generate parse trees, we use the Berkeley parser (Petrov et al., 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. Overview We present a brief overview here and delve deeper in Section 2.1. Word alignments are built bottom-up on the parse tree. Each node v in the tree holds partial alignments sorted by score. 158 u u12 u13 11 u11 u12 u13 12 13 2.2 1 5 . 2.2 4.1 5.5 4.1 .5 2.4 3.5 2 2.4 .5 7.2 .4 3.5 7.2 3.2 5 14 .2 4.5 11.4 3.2 4.5 1.4 (a) Score the left corner alignment first. Assume it is the 1- best. Numbers in the rest of the boxes are hidden at this point. u1 u12 u13 u1 u12 u13 u1 u1 u1 2. 4.1 5. 2.2 4. 5.5 2.2 4.1 5.5 2.4 3.</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux and Dan Klein 2006. Learning Accurate, Compact, and Interpretable Tree Annotation In Proceedings of the 44th Annual Meeting of the ACL. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL.</booktitle>
<location>Philadelphia, PA. USA.</location>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, T. Ward, and W-J. Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation In Proceedings of the 40th Annual Meeting of the ACL. Philadelphia, PA. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A Discriminative Matching Approach to Word Alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<publisher>Canada.</publisher>
<location>Vancouver, BC.</location>
<contexts>
<context position="2347" citStr="Taskar et al., 2005" startWordPosition="357" endWordPosition="360">ally difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty !&amp;quot;#$%!&amp;!&apos;() &amp;quot;* +,-* !&amp;.( /023 1( 5!67 !*,8.( 9:; &lt;)+,=.( &gt;?@!A8BC( 1 DEFG* ) # G( 1 ?H() * Figure 1: Model-4 alignment vs. a gold standard. Circles represent links in a human-annotated alignment, and black boxes represent links in the Model-4 alignment. Bold gray boxes show links gained after fully connecting the alignment. has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A Discriminative Matching Approach to Word Alignment. In Proceedings of HLT-EMNLP. Vancouver, BC. Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Thorsten Brants</author>
</authors>
<title>Randomized Language Models via Perfect Hash Functions.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<location>Columbus, OH. USA.</location>
<contexts>
<context position="27210" citStr="Talbot and Brants (2008)" startWordPosition="4748" endWordPosition="4751">on word core subset. We align the same core subset with our trained hypergraph alignment model, and extract a second set of translation rules. For each set of translation rules, we train a machine translation system and decode a held-out test corpus for which we report results below. We use a syntax-based translation system for these experiments. This system transforms Arabic strings into target English syntax trees Translation rules are extracted from (e-tree, f-string, alignment) triples as in (Galley et al., 2004; Galley et al., 2006). We use a randomized language model (similar to that of Talbot and Brants (2008)) of 472 million English words. We tune the the parameters of the MT system on a held-out development corpus of 1,172 parallel sentences, and test on a heldout parallel corpus of 746 parallel sentences. Both corpora are drawn from the NIST 2004 and 2006 evaluation data, with no overlap at the document or segment level with our training data. Columns 4 and 5 in Table 2 show the results of our MT experiments. Our hypergraph alignment algorithm allows us a 1.1 BLEU increase over the best baseline system, Model-4 grow-diag-final. This is statistically significant at the p &lt; 0.01 level. We also rep</context>
</contexts>
<marker>Talbot, Brants, 2008</marker>
<rawString>David Talbot and Thorsten Brants. 2008. Randomized Language Models via Perfect Hash Functions. In Proceedings of ACL-08: HLT. Columbus, OH. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics.</journal>
<volume>23</volume>
<issue>3</issue>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="22343" citStr="Wu, 1997" startWordPosition="3930" endWordPosition="3931">ead noun under the same NP. Now suppose there are several intermediate adjectives separating the determiner and noun. A string distance met... 163 Training F−measure ric, with no knowledge of the relationship between determiner and noun will levy a much heavier penalty than its tree distance analog. 5 Related Work Recent work has shown the potential for syntactic information encoded in various ways to support inference of superior word alignments. Very recent work in word alignment has also started to report downstream effects on BLEU score. Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. Haghighi et al. (2009) confirm and extend these results, showing BLEU improvement for a hierarchical phrasebased MT system on a small Chinese corpus. As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model. And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments. DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance inste</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics. 23(3):377–404. MIT Press. Cambridge, MA. USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>