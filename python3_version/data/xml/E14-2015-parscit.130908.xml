<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.605296">
<title confidence="0.998967">
A Graphical Interface for Automatic Error Mining in Corpora
</title>
<author confidence="0.998303">
Gregor Thiele Wolfgang Seeker Markus G¨artner Anders Bj¨orkelund Jonas Kuhn
</author>
<affiliation confidence="0.99694">
Institute for Natural Language Processing
University of Stuttgart
</affiliation>
<email confidence="0.99157">
{thielegr,seeker,gaertnms,anders,kuhn}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.997288" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999881923076923">
We present an error mining tool that is de-
signed to help human annotators to find
errors and inconsistencies in their anno-
tation. The output of the underlying al-
gorithm is accessible via a graphical user
interface, which provides two aggregate
views: a list of potential errors in con-
text and a distribution over labels. The
user can always directly access the ac-
tual sentence containing the potential er-
ror, thus enabling annotators to quickly
judge whether the found candidate is in-
deed incorrectly labeled.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99982">
Manually annotated corpora and treebanks are the
primary tools that we have for developing and
evaluating models and theories for natural lan-
guage processing. Given their importance for test-
ing our hypotheses, it is imperative that they are
of the best quality possible. However, manual an-
notation is tedious and error-prone, especially if
many annotators are involved. It is therefore desir-
able to have automatic means for detecting errors
and inconsistencies in the annotation.
Automatic methods for error detection in tree-
banks have been developed in the DECCA
project1 for several different annotation types, for
example part-of-speech (Dickinson and Meurers,
2003a), constituency syntax (Dickinson and Meur-
ers, 2003b), and dependency syntax (Boyd et al.,
2008). These algorithms work on the assumption
that two data points that appear in identical con-
texts should be labeled in the same way. While
the data points in question, or nuclei, can be single
tokens, spans of tokens, or edges between two to-
kens, context is usually modeled as n-grams over
the surrounding tokens. A nucleus that occurs
</bodyText>
<footnote confidence="0.913187">
1http://www.decca.osu.edu
</footnote>
<bodyText confidence="0.992954">
multiple times in identical contexts but is labeled
differently shows variation and is considered a po-
tential error.
Natural language is ambiguous and variation
found by an algorithm may be a genuine ambigu-
ity rather than an annotation error. Although we
can support an annotator in finding inconsisten-
cies in a treebank, these inconsistencies still need
to be judged by humans. In this paper, we present
a tool that allows a user to run automatic error de-
tection on a corpus annotated with part-of-speech
or dependency syntax.2 The tool provides the user
with a graphical interface to browse the variation
nuclei found by the algorithm and inspect their la-
bel distribution. The user can always switch be-
tween high-level aggregate views and the actual
sentences containing the potential error in order to
decide if that particular annotation is incorrect or
not. The interface thus brings together the output
of the error detection algorithm with a direct ac-
cess to the corpus data. This speeds up the pro-
cess of tracking down inconsistencies and errors
in the annotation considerably compared to work-
ing with the raw output of the original DECCA
tools. Several options allow the user to fine-tune
the behavior of the algorithm. The tool is part of
ICARUS (G¨artner et al., 2013), a general search
and exploration tool.3
</bodyText>
<sectionHeader confidence="0.954312" genericHeader="method">
2 The Error Detection Algorithm
</sectionHeader>
<bodyText confidence="0.999964">
The algorithm, described in Dickinson and Meur-
ers (2003a) for POS tags, works by starting from
individual tokens (the nuclei) by recording their
assigned part-of-speech over an entire treebank.
From there, it iteratively increases the context for
each instance by extending the string to both sides
to include adjacent tokens. It thus successively
builds larger n-grams by adding tokens to the left
</bodyText>
<footnote confidence="0.995117">
2Generalizing the tool to support any kind of positional
annotation is planned.
3http://www.ims.uni-stuttgart.de/data/icarus.html
</footnote>
<page confidence="0.973983">
57
</page>
<note confidence="0.952414">
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 57–60,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999978">
Figure 1: The variation n-gram view.
</figureCaption>
<bodyText confidence="0.999976826086956">
or to the right. Instances are grouped together if
their context is identical, i. e. if their token n-
grams match. Groups where all instances have
the same label do not show variation and are dis-
carded. The algorithm stops when either no vari-
ation nuclei are left or when none of them can be
further extended. All remaining groups that show
variation are considered potential errors. Erro-
neous annotations that do not show variation in the
data cannot be found by the algorithm. This limits
the usefulness of the method for very small data
sets. Also, given the inherent ambiguity of nat-
ural language, the algorithm is not guaranteed to
exclusively output errors, but it achieves very high
precision in experiments on several languages.
The algorithm has been extended to find errors
in constituency and dependency structures (Dick-
inson and Meurers, 2003b; Boyd et al., 2008),
where the definition of a nucleus is changed to
capture phrases and dependency edges. Context
is always modeled using n-grams over surround-
ing tokens, but see, e. g., Boyd et al. (2007) for
extensions.
</bodyText>
<sectionHeader confidence="0.981501" genericHeader="method">
3 Graphical Error Mining
</sectionHeader>
<bodyText confidence="0.999989555555556">
To start the error mining, a treebank and an error
mining algorithm (part-of-speech or dependency)
must be selected. The algorithm is then executed
on the data to create the variation n-grams. The
user can choose between two views for browsing
the potential errors in the treebank: (1) a view
showing the list of variation n-grams found by the
error detection algorithm and (2) a view showing
label distributions over word forms.
</bodyText>
<subsectionHeader confidence="0.997507">
3.1 The Variation N-Gram View
</subsectionHeader>
<bodyText confidence="0.999936647058823">
Figure 1 shows a screenshot of the view where the
user is presented with the list of variation n-grams
output by the error detection algorithm. The main
window shows the list of n-grams. When the user
selects one of the n-grams, information about the
nucleus is displayed below the main window. The
user can inspect the distribution over labels (here
part-of-speech tags) with their absolute frequen-
cies. Above the main window, the user can adjust
the length of the presented n-grams, sort them, or
search for specific strings.
For example, Figure 1 shows a part of the vari-
ation n-grams found in the German TiGer corpus
(Brants et al., 2002). The minimum and maximum
length was restricted to four, thus the list contains
only 4-grams. The 4-gram so hoch wie in was se-
lected, which contains wie as its nucleus. In the
lower part, the user can see that wie occurs with
four different part-of-speech tags in the treebank,
namely KOKOM, PWAV, KON, and KOUS. Note
that the combination with KOUS occurs only once
in the entire treebank.
Double clicking on the selected 4-gram in the
list will open up a new tab that displays all sen-
tences that contain this n-gram, with the nucleus
being highlighted. The user can then go through
each of the sentences and decide whether the an-
notated part-of-speech tag is correct. Each time
the user clicks on an n-gram, a new tab will be
created, so that the user can jump back to previous
results without having to recreate them.
A double click on one of the lines in the lower
part of the window will bring up all sentences that
contain that particular combination of word form
</bodyText>
<page confidence="0.997531">
58
</page>
<figureCaption confidence="0.999787">
Figure 2: The label distribution view.
</figureCaption>
<bodyText confidence="0.999556166666667">
and part-of-speech tag. The fourth line will, for
example, show the one sentence where wie has
been tagged as KOUS, making it easy to quickly
judge whether the tag is correct. In this case, the
annotation is incorrect (it should have been PWAV)
and should thus be marked for correction.
</bodyText>
<subsectionHeader confidence="0.999225">
3.2 The Label Distribution View
</subsectionHeader>
<bodyText confidence="0.999995441860465">
In addition to the output of the algorithm by Dick-
inson and Meurers (2003a), the tool also provides
a second view, which displays tag distributions of
word forms to the user (see Figure 2). To the left,
a list of unique label combinations is shown. Se-
lecting one of them displays a list of word forms
that occur with exactly these tags in the corpus.
This list is shown below the list of label combina-
tions. To the right, the frequencies of the differ-
ent labels are shown in a bar chart. The leftmost
bar for each label always shows the total frequency
summed over all word forms in the set. Selecting
one or more in the list of word forms adds addi-
tional bars to the chart that show the frequencies
for each selected word form.
As an example, Figure 2 shows the tag combi-
nation [VVINF][VVIZU], which are used to tag in-
finitives with and without incorporated zu in Ger-
man. There are three word forms in the cor-
pus that occur with these two part-of-speech tags:
hinzukommen, aufzul¨osen, and anzun¨ahern. The
chart on the right shows the frequencies for each
word form and part-of-speech tag, revealing that
hinzukommen is mostly tagged as VVINF but once
as VVIZU, whereas for the other two word forms it
is the other way around. This example is interest-
ing if one is looking for annotation errors in the
TiGer treebank, because the two part-of-speech
tags should have a complementary distribution (a
German verb either incorporates zu or it does not).
Double clicking on the word forms in the list in
the lower left corner will again open up a tab that
shows all sentences containing this word form, re-
gardless of their part-of-speech tag. The user may
then inspect the sentences and decide whether the
annotations are erroneous or not. If the user wants
to see a specific combination, which is more use-
ful if the total number of sentences is large, she
can also click on one of the bars in the chart to get
all sentences matching that combination. In the
example, the one instance of hinzukommen being
tagged as VVIZU is incorrect,4 and the instances of
the two other verbs tagged as VVINF are as well.
</bodyText>
<subsectionHeader confidence="0.99827">
3.3 Dependency Annotation Errors
</subsectionHeader>
<bodyText confidence="0.999919363636364">
As mentioned before, the tool also allows the user
to search for errors in dependency structures. The
error mining algorithm for dependency structures
(Boyd et al., 2008) is very similar to the one for
part-of-speech tags, and so is the interface to the
n-gram list or the distribution view. Dependency
edges are therein displayed as triples: the head,
the dependent, and the edge label with the edge’s
direction. As with the part-of-speech tags, the user
can always jump directly to the sentences that con-
tain a particular n-gram or dependency relation.
</bodyText>
<footnote confidence="0.891696666666667">
4Actually, the word form hinzukommen can belong to two
different verbs, hinzu-kommen and hin-kommen. However,
the latter, which incorporates zu, does not occur in TiGer.
</footnote>
<page confidence="0.999023">
59
</page>
<sectionHeader confidence="0.98386" genericHeader="method">
4 Error Detection on TiGer
</sectionHeader>
<bodyText confidence="0.999995173913043">
We ran the error mining algorithm for part-of-
speech on the German TiGer Treebank (the de-
pendency version by Seeker and Kuhn (2012)) and
manually evaluated a small sample of n-grams in
order to get an idea of how useful the output is.
We manually checked 115 out of the 207 vari-
ation 6-grams found by the tool, which amounts
to 119 different nuclei. For 99.16% of these nu-
clei, we found erroneous annotations in the asso-
ciated sentences. 95.6% of these are errors where
we are able to decide what the right tag should
be, the remaining ones are more difficult to disam-
biguate because the annotation guidelines do not
cover them.
These results are in line with findings by Dick-
inson and Meurers (2003a) for the Penn Treebank.
They show that even manually annotated corpora
contain errors and an automatic error mining tool
can be a big help in finding them. Furthermore,
it can help annotators to improve their annotation
guidelines by pointing out phenomena that are not
covered by the guidelines, because these phenom-
ena will be more likely to show variation.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999985578947368">
We are aware of only one other graphical tool that
was developed to help with error detection in tree-
banks: Ambati et al. (2010) and Agarwal et al.
(2012) describe a graphical tool that was used in
the annotation of the Hindi Dependency Treebank.
To find errors, it uses a statistical and a rule-based
component. The statistical component is recall-
oriented and learns a MaxEnt model, which is used
to flag dependency edges as errors if their proba-
bility falls below a predefined threshold. In or-
der to increase the precision, the output is post-
processed by the rule-based component, which is
tailored to the treebank’s annotation guidelines.
Errors are presented to the annotators in tables,
also with the option to go to the sentences di-
rectly from there. Unlike the algorithm we im-
plemented, this approach needs annotated training
data for training the classifier and tuning the re-
spective thresholds.
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998545625">
High-quality annotations for linguistic corpora are
important for testing hypotheses in NLP and lin-
guistic research. Automatically marking potential
annotation errors and inconsistencies are one way
of supporting annotators in their work. We pre-
sented a tool that provides a graphical interface for
annotators to find and evaluate annotation errors
in treebanks. It implements the error detection al-
gorithms by Dickinson and Meurers (2003a) and
Boyd et al. (2008). The user can view errors from
two perspectives that aggregate error information
found by the algorithm, and it is always easy to
go directly to the actual sentences for manual in-
spection. The tool is currently extended such that
annotators can make changes to the data directly
in the interface when they find an error.
</bodyText>
<sectionHeader confidence="0.997963" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999431333333333">
We thank Markus Dickinson for his comments.
Funded by BMBF via project No. 01UG1120F,
CLARIN-D, and by DFG via SFB 732, project D8.
</bodyText>
<sectionHeader confidence="0.999665" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99993521875">
Rahul Agarwal, Bharat Ram Ambati, and Anil Kumar
Singh. 2012. A GUI to Detect and Correct Errors in
Hindi Dependency Treebank. In LREC 2012, pages
1907–1911.
Bharat Ram Ambati, Mridul Gupta, Samar Husain, and
Dipti Misra Sharma. 2010. A High Recall Error
Identification Tool for Hindi Treebank Validation.
In LREC 2010.
Adriane Boyd, Markus Dickinson, and Detmar Meur-
ers. 2007. Increasing the Recall of Corpus Annota-
tion Error Detection. In TLT 2007, pages 19–30.
Adriane Boyd, Markus Dickinson, and Detmar Meur-
ers. 2008. On Detecting Errors in Dependency
Treebanks. Research on Language and Computa-
tion, 6(2):113–137.
Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra,
Wolfgang Lezius, and George Smith. 2002. The
TIGER treebank. In TLT 2002, pages 24–41.
Markus Dickinson and W. Detmar Meurers. 2003a.
Detecting Errors in Part-of-Speech Annotation. In
EACL 2003, pages 107–114.
Markus Dickinson and W. Detmar Meurers. 2003b.
Detecting Inconsistencies in Treebanks. In TLT
2003, pages 45–56.
Markus G¨artner, Gregor Thiele, Wolfgang Seeker, An-
ders Bj¨orkelund, and Jonas Kuhn. 2013. ICARUS
– An Extensible Graphical Search Tool for Depen-
dency Treebanks. In ACL: System Demonstrations,
pages 55–60.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In LREC 2012, pages 3132–3139.
</reference>
<page confidence="0.998414">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.899615">
<title confidence="0.999496">A Graphical Interface for Automatic Error Mining in Corpora</title>
<author confidence="0.995753">Gregor Thiele Wolfgang Seeker Markus G¨artner Anders Bj¨orkelund Jonas</author>
<affiliation confidence="0.9985835">Institute for Natural Language University of</affiliation>
<abstract confidence="0.993165357142857">We present an error mining tool that is designed to help human annotators to find errors and inconsistencies in their annotation. The output of the underlying algorithm is accessible via a graphical user interface, which provides two aggregate views: a list of potential errors in context and a distribution over labels. The user can always directly access the actual sentence containing the potential error, thus enabling annotators to quickly judge whether the found candidate is indeed incorrectly labeled.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rahul Agarwal</author>
<author>Bharat Ram Ambati</author>
<author>Anil Kumar Singh</author>
</authors>
<title>A GUI to Detect and Correct Errors in Hindi Dependency Treebank. In LREC</title>
<date>2012</date>
<pages>1907--1911</pages>
<contexts>
<context position="11731" citStr="Agarwal et al. (2012)" startWordPosition="1950" endWordPosition="1953"> do not cover them. These results are in line with findings by Dickinson and Meurers (2003a) for the Penn Treebank. They show that even manually annotated corpora contain errors and an automatic error mining tool can be a big help in finding them. Furthermore, it can help annotators to improve their annotation guidelines by pointing out phenomena that are not covered by the guidelines, because these phenomena will be more likely to show variation. 5 Related Work We are aware of only one other graphical tool that was developed to help with error detection in treebanks: Ambati et al. (2010) and Agarwal et al. (2012) describe a graphical tool that was used in the annotation of the Hindi Dependency Treebank. To find errors, it uses a statistical and a rule-based component. The statistical component is recalloriented and learns a MaxEnt model, which is used to flag dependency edges as errors if their probability falls below a predefined threshold. In order to increase the precision, the output is postprocessed by the rule-based component, which is tailored to the treebank’s annotation guidelines. Errors are presented to the annotators in tables, also with the option to go to the sentences directly from ther</context>
</contexts>
<marker>Agarwal, Ambati, Singh, 2012</marker>
<rawString>Rahul Agarwal, Bharat Ram Ambati, and Anil Kumar Singh. 2012. A GUI to Detect and Correct Errors in Hindi Dependency Treebank. In LREC 2012, pages 1907–1911.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bharat Ram Ambati</author>
<author>Mridul Gupta</author>
<author>Samar Husain</author>
<author>Dipti Misra Sharma</author>
</authors>
<title>A High Recall Error Identification Tool for Hindi Treebank Validation.</title>
<date>2010</date>
<booktitle>In LREC</booktitle>
<contexts>
<context position="11705" citStr="Ambati et al. (2010)" startWordPosition="1945" endWordPosition="1948">the annotation guidelines do not cover them. These results are in line with findings by Dickinson and Meurers (2003a) for the Penn Treebank. They show that even manually annotated corpora contain errors and an automatic error mining tool can be a big help in finding them. Furthermore, it can help annotators to improve their annotation guidelines by pointing out phenomena that are not covered by the guidelines, because these phenomena will be more likely to show variation. 5 Related Work We are aware of only one other graphical tool that was developed to help with error detection in treebanks: Ambati et al. (2010) and Agarwal et al. (2012) describe a graphical tool that was used in the annotation of the Hindi Dependency Treebank. To find errors, it uses a statistical and a rule-based component. The statistical component is recalloriented and learns a MaxEnt model, which is used to flag dependency edges as errors if their probability falls below a predefined threshold. In order to increase the precision, the output is postprocessed by the rule-based component, which is tailored to the treebank’s annotation guidelines. Errors are presented to the annotators in tables, also with the option to go to the se</context>
</contexts>
<marker>Ambati, Gupta, Husain, Sharma, 2010</marker>
<rawString>Bharat Ram Ambati, Mridul Gupta, Samar Husain, and Dipti Misra Sharma. 2010. A High Recall Error Identification Tool for Hindi Treebank Validation. In LREC 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriane Boyd</author>
<author>Markus Dickinson</author>
<author>Detmar Meurers</author>
</authors>
<title>Increasing the Recall of Corpus Annotation Error Detection. In TLT</title>
<date>2007</date>
<pages>pages</pages>
<contexts>
<context position="5136" citStr="Boyd et al. (2007)" startWordPosition="801" endWordPosition="804"> cannot be found by the algorithm. This limits the usefulness of the method for very small data sets. Also, given the inherent ambiguity of natural language, the algorithm is not guaranteed to exclusively output errors, but it achieves very high precision in experiments on several languages. The algorithm has been extended to find errors in constituency and dependency structures (Dickinson and Meurers, 2003b; Boyd et al., 2008), where the definition of a nucleus is changed to capture phrases and dependency edges. Context is always modeled using n-grams over surrounding tokens, but see, e. g., Boyd et al. (2007) for extensions. 3 Graphical Error Mining To start the error mining, a treebank and an error mining algorithm (part-of-speech or dependency) must be selected. The algorithm is then executed on the data to create the variation n-grams. The user can choose between two views for browsing the potential errors in the treebank: (1) a view showing the list of variation n-grams found by the error detection algorithm and (2) a view showing label distributions over word forms. 3.1 The Variation N-Gram View Figure 1 shows a screenshot of the view where the user is presented with the list of variation n-g</context>
</contexts>
<marker>Boyd, Dickinson, Meurers, 2007</marker>
<rawString>Adriane Boyd, Markus Dickinson, and Detmar Meurers. 2007. Increasing the Recall of Corpus Annotation Error Detection. In TLT 2007, pages 19–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriane Boyd</author>
<author>Markus Dickinson</author>
<author>Detmar Meurers</author>
</authors>
<title>On Detecting Errors in Dependency Treebanks.</title>
<date>2008</date>
<journal>Research on Language and Computation,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="1561" citStr="Boyd et al., 2008" startWordPosition="227" endWordPosition="230">processing. Given their importance for testing our hypotheses, it is imperative that they are of the best quality possible. However, manual annotation is tedious and error-prone, especially if many annotators are involved. It is therefore desirable to have automatic means for detecting errors and inconsistencies in the annotation. Automatic methods for error detection in treebanks have been developed in the DECCA project1 for several different annotation types, for example part-of-speech (Dickinson and Meurers, 2003a), constituency syntax (Dickinson and Meurers, 2003b), and dependency syntax (Boyd et al., 2008). These algorithms work on the assumption that two data points that appear in identical contexts should be labeled in the same way. While the data points in question, or nuclei, can be single tokens, spans of tokens, or edges between two tokens, context is usually modeled as n-grams over the surrounding tokens. A nucleus that occurs 1http://www.decca.osu.edu multiple times in identical contexts but is labeled differently shows variation and is considered a potential error. Natural language is ambiguous and variation found by an algorithm may be a genuine ambiguity rather than an annotation err</context>
<context position="4949" citStr="Boyd et al., 2008" startWordPosition="769" endWordPosition="772">e left or when none of them can be further extended. All remaining groups that show variation are considered potential errors. Erroneous annotations that do not show variation in the data cannot be found by the algorithm. This limits the usefulness of the method for very small data sets. Also, given the inherent ambiguity of natural language, the algorithm is not guaranteed to exclusively output errors, but it achieves very high precision in experiments on several languages. The algorithm has been extended to find errors in constituency and dependency structures (Dickinson and Meurers, 2003b; Boyd et al., 2008), where the definition of a nucleus is changed to capture phrases and dependency edges. Context is always modeled using n-grams over surrounding tokens, but see, e. g., Boyd et al. (2007) for extensions. 3 Graphical Error Mining To start the error mining, a treebank and an error mining algorithm (part-of-speech or dependency) must be selected. The algorithm is then executed on the data to create the variation n-grams. The user can choose between two views for browsing the potential errors in the treebank: (1) a view showing the list of variation n-grams found by the error detection algorithm a</context>
<context position="9917" citStr="Boyd et al., 2008" startWordPosition="1638" endWordPosition="1641">e whether the annotations are erroneous or not. If the user wants to see a specific combination, which is more useful if the total number of sentences is large, she can also click on one of the bars in the chart to get all sentences matching that combination. In the example, the one instance of hinzukommen being tagged as VVIZU is incorrect,4 and the instances of the two other verbs tagged as VVINF are as well. 3.3 Dependency Annotation Errors As mentioned before, the tool also allows the user to search for errors in dependency structures. The error mining algorithm for dependency structures (Boyd et al., 2008) is very similar to the one for part-of-speech tags, and so is the interface to the n-gram list or the distribution view. Dependency edges are therein displayed as triples: the head, the dependent, and the edge label with the edge’s direction. As with the part-of-speech tags, the user can always jump directly to the sentences that contain a particular n-gram or dependency relation. 4Actually, the word form hinzukommen can belong to two different verbs, hinzu-kommen and hin-kommen. However, the latter, which incorporates zu, does not occur in TiGer. 59 4 Error Detection on TiGer We ran the erro</context>
</contexts>
<marker>Boyd, Dickinson, Meurers, 2008</marker>
<rawString>Adriane Boyd, Markus Dickinson, and Detmar Meurers. 2008. On Detecting Errors in Dependency Treebanks. Research on Language and Computation, 6(2):113–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen-Shirra</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In TLT</booktitle>
<pages>24--41</pages>
<contexts>
<context position="6280" citStr="Brants et al., 2002" startWordPosition="993" endWordPosition="996">nshot of the view where the user is presented with the list of variation n-grams output by the error detection algorithm. The main window shows the list of n-grams. When the user selects one of the n-grams, information about the nucleus is displayed below the main window. The user can inspect the distribution over labels (here part-of-speech tags) with their absolute frequencies. Above the main window, the user can adjust the length of the presented n-grams, sort them, or search for specific strings. For example, Figure 1 shows a part of the variation n-grams found in the German TiGer corpus (Brants et al., 2002). The minimum and maximum length was restricted to four, thus the list contains only 4-grams. The 4-gram so hoch wie in was selected, which contains wie as its nucleus. In the lower part, the user can see that wie occurs with four different part-of-speech tags in the treebank, namely KOKOM, PWAV, KON, and KOUS. Note that the combination with KOUS occurs only once in the entire treebank. Double clicking on the selected 4-gram in the list will open up a new tab that displays all sentences that contain this n-gram, with the nucleus being highlighted. The user can then go through each of the sente</context>
</contexts>
<marker>Brants, Dipper, Hansen-Shirra, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank. In TLT 2002, pages 24–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>W Detmar Meurers</author>
</authors>
<title>Detecting Errors in Part-of-Speech Annotation. In EACL</title>
<date>2003</date>
<pages>107--114</pages>
<contexts>
<context position="1464" citStr="Dickinson and Meurers, 2003" startWordPosition="213" endWordPosition="216">are the primary tools that we have for developing and evaluating models and theories for natural language processing. Given their importance for testing our hypotheses, it is imperative that they are of the best quality possible. However, manual annotation is tedious and error-prone, especially if many annotators are involved. It is therefore desirable to have automatic means for detecting errors and inconsistencies in the annotation. Automatic methods for error detection in treebanks have been developed in the DECCA project1 for several different annotation types, for example part-of-speech (Dickinson and Meurers, 2003a), constituency syntax (Dickinson and Meurers, 2003b), and dependency syntax (Boyd et al., 2008). These algorithms work on the assumption that two data points that appear in identical contexts should be labeled in the same way. While the data points in question, or nuclei, can be single tokens, spans of tokens, or edges between two tokens, context is usually modeled as n-grams over the surrounding tokens. A nucleus that occurs 1http://www.decca.osu.edu multiple times in identical contexts but is labeled differently shows variation and is considered a potential error. Natural language is ambig</context>
<context position="3330" citStr="Dickinson and Meurers (2003" startWordPosition="518" endWordPosition="522">ntial error in order to decide if that particular annotation is incorrect or not. The interface thus brings together the output of the error detection algorithm with a direct access to the corpus data. This speeds up the process of tracking down inconsistencies and errors in the annotation considerably compared to working with the raw output of the original DECCA tools. Several options allow the user to fine-tune the behavior of the algorithm. The tool is part of ICARUS (G¨artner et al., 2013), a general search and exploration tool.3 2 The Error Detection Algorithm The algorithm, described in Dickinson and Meurers (2003a) for POS tags, works by starting from individual tokens (the nuclei) by recording their assigned part-of-speech over an entire treebank. From there, it iteratively increases the context for each instance by extending the string to both sides to include adjacent tokens. It thus successively builds larger n-grams by adding tokens to the left 2Generalizing the tool to support any kind of positional annotation is planned. 3http://www.ims.uni-stuttgart.de/data/icarus.html 57 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Lingu</context>
<context position="4928" citStr="Dickinson and Meurers, 2003" startWordPosition="764" endWordPosition="768"> either no variation nuclei are left or when none of them can be further extended. All remaining groups that show variation are considered potential errors. Erroneous annotations that do not show variation in the data cannot be found by the algorithm. This limits the usefulness of the method for very small data sets. Also, given the inherent ambiguity of natural language, the algorithm is not guaranteed to exclusively output errors, but it achieves very high precision in experiments on several languages. The algorithm has been extended to find errors in constituency and dependency structures (Dickinson and Meurers, 2003b; Boyd et al., 2008), where the definition of a nucleus is changed to capture phrases and dependency edges. Context is always modeled using n-grams over surrounding tokens, but see, e. g., Boyd et al. (2007) for extensions. 3 Graphical Error Mining To start the error mining, a treebank and an error mining algorithm (part-of-speech or dependency) must be selected. The algorithm is then executed on the data to create the variation n-grams. The user can choose between two views for browsing the potential errors in the treebank: (1) a view showing the list of variation n-grams found by the error </context>
<context position="7683" citStr="Dickinson and Meurers (2003" startWordPosition="1242" endWordPosition="1246">us results without having to recreate them. A double click on one of the lines in the lower part of the window will bring up all sentences that contain that particular combination of word form 58 Figure 2: The label distribution view. and part-of-speech tag. The fourth line will, for example, show the one sentence where wie has been tagged as KOUS, making it easy to quickly judge whether the tag is correct. In this case, the annotation is incorrect (it should have been PWAV) and should thus be marked for correction. 3.2 The Label Distribution View In addition to the output of the algorithm by Dickinson and Meurers (2003a), the tool also provides a second view, which displays tag distributions of word forms to the user (see Figure 2). To the left, a list of unique label combinations is shown. Selecting one of them displays a list of word forms that occur with exactly these tags in the corpus. This list is shown below the list of label combinations. To the right, the frequencies of the different labels are shown in a bar chart. The leftmost bar for each label always shows the total frequency summed over all word forms in the set. Selecting one or more in the list of word forms adds additional bars to the chart</context>
<context position="11200" citStr="Dickinson and Meurers (2003" startWordPosition="1858" endWordPosition="1862"> Treebank (the dependency version by Seeker and Kuhn (2012)) and manually evaluated a small sample of n-grams in order to get an idea of how useful the output is. We manually checked 115 out of the 207 variation 6-grams found by the tool, which amounts to 119 different nuclei. For 99.16% of these nuclei, we found erroneous annotations in the associated sentences. 95.6% of these are errors where we are able to decide what the right tag should be, the remaining ones are more difficult to disambiguate because the annotation guidelines do not cover them. These results are in line with findings by Dickinson and Meurers (2003a) for the Penn Treebank. They show that even manually annotated corpora contain errors and an automatic error mining tool can be a big help in finding them. Furthermore, it can help annotators to improve their annotation guidelines by pointing out phenomena that are not covered by the guidelines, because these phenomena will be more likely to show variation. 5 Related Work We are aware of only one other graphical tool that was developed to help with error detection in treebanks: Ambati et al. (2010) and Agarwal et al. (2012) describe a graphical tool that was used in the annotation of the Hin</context>
</contexts>
<marker>Dickinson, Meurers, 2003</marker>
<rawString>Markus Dickinson and W. Detmar Meurers. 2003a. Detecting Errors in Part-of-Speech Annotation. In EACL 2003, pages 107–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>W Detmar Meurers</author>
</authors>
<title>Detecting Inconsistencies in Treebanks. In TLT</title>
<date>2003</date>
<pages>45--56</pages>
<contexts>
<context position="1464" citStr="Dickinson and Meurers, 2003" startWordPosition="213" endWordPosition="216">are the primary tools that we have for developing and evaluating models and theories for natural language processing. Given their importance for testing our hypotheses, it is imperative that they are of the best quality possible. However, manual annotation is tedious and error-prone, especially if many annotators are involved. It is therefore desirable to have automatic means for detecting errors and inconsistencies in the annotation. Automatic methods for error detection in treebanks have been developed in the DECCA project1 for several different annotation types, for example part-of-speech (Dickinson and Meurers, 2003a), constituency syntax (Dickinson and Meurers, 2003b), and dependency syntax (Boyd et al., 2008). These algorithms work on the assumption that two data points that appear in identical contexts should be labeled in the same way. While the data points in question, or nuclei, can be single tokens, spans of tokens, or edges between two tokens, context is usually modeled as n-grams over the surrounding tokens. A nucleus that occurs 1http://www.decca.osu.edu multiple times in identical contexts but is labeled differently shows variation and is considered a potential error. Natural language is ambig</context>
<context position="3330" citStr="Dickinson and Meurers (2003" startWordPosition="518" endWordPosition="522">ntial error in order to decide if that particular annotation is incorrect or not. The interface thus brings together the output of the error detection algorithm with a direct access to the corpus data. This speeds up the process of tracking down inconsistencies and errors in the annotation considerably compared to working with the raw output of the original DECCA tools. Several options allow the user to fine-tune the behavior of the algorithm. The tool is part of ICARUS (G¨artner et al., 2013), a general search and exploration tool.3 2 The Error Detection Algorithm The algorithm, described in Dickinson and Meurers (2003a) for POS tags, works by starting from individual tokens (the nuclei) by recording their assigned part-of-speech over an entire treebank. From there, it iteratively increases the context for each instance by extending the string to both sides to include adjacent tokens. It thus successively builds larger n-grams by adding tokens to the left 2Generalizing the tool to support any kind of positional annotation is planned. 3http://www.ims.uni-stuttgart.de/data/icarus.html 57 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Lingu</context>
<context position="4928" citStr="Dickinson and Meurers, 2003" startWordPosition="764" endWordPosition="768"> either no variation nuclei are left or when none of them can be further extended. All remaining groups that show variation are considered potential errors. Erroneous annotations that do not show variation in the data cannot be found by the algorithm. This limits the usefulness of the method for very small data sets. Also, given the inherent ambiguity of natural language, the algorithm is not guaranteed to exclusively output errors, but it achieves very high precision in experiments on several languages. The algorithm has been extended to find errors in constituency and dependency structures (Dickinson and Meurers, 2003b; Boyd et al., 2008), where the definition of a nucleus is changed to capture phrases and dependency edges. Context is always modeled using n-grams over surrounding tokens, but see, e. g., Boyd et al. (2007) for extensions. 3 Graphical Error Mining To start the error mining, a treebank and an error mining algorithm (part-of-speech or dependency) must be selected. The algorithm is then executed on the data to create the variation n-grams. The user can choose between two views for browsing the potential errors in the treebank: (1) a view showing the list of variation n-grams found by the error </context>
<context position="7683" citStr="Dickinson and Meurers (2003" startWordPosition="1242" endWordPosition="1246">us results without having to recreate them. A double click on one of the lines in the lower part of the window will bring up all sentences that contain that particular combination of word form 58 Figure 2: The label distribution view. and part-of-speech tag. The fourth line will, for example, show the one sentence where wie has been tagged as KOUS, making it easy to quickly judge whether the tag is correct. In this case, the annotation is incorrect (it should have been PWAV) and should thus be marked for correction. 3.2 The Label Distribution View In addition to the output of the algorithm by Dickinson and Meurers (2003a), the tool also provides a second view, which displays tag distributions of word forms to the user (see Figure 2). To the left, a list of unique label combinations is shown. Selecting one of them displays a list of word forms that occur with exactly these tags in the corpus. This list is shown below the list of label combinations. To the right, the frequencies of the different labels are shown in a bar chart. The leftmost bar for each label always shows the total frequency summed over all word forms in the set. Selecting one or more in the list of word forms adds additional bars to the chart</context>
<context position="11200" citStr="Dickinson and Meurers (2003" startWordPosition="1858" endWordPosition="1862"> Treebank (the dependency version by Seeker and Kuhn (2012)) and manually evaluated a small sample of n-grams in order to get an idea of how useful the output is. We manually checked 115 out of the 207 variation 6-grams found by the tool, which amounts to 119 different nuclei. For 99.16% of these nuclei, we found erroneous annotations in the associated sentences. 95.6% of these are errors where we are able to decide what the right tag should be, the remaining ones are more difficult to disambiguate because the annotation guidelines do not cover them. These results are in line with findings by Dickinson and Meurers (2003a) for the Penn Treebank. They show that even manually annotated corpora contain errors and an automatic error mining tool can be a big help in finding them. Furthermore, it can help annotators to improve their annotation guidelines by pointing out phenomena that are not covered by the guidelines, because these phenomena will be more likely to show variation. 5 Related Work We are aware of only one other graphical tool that was developed to help with error detection in treebanks: Ambati et al. (2010) and Agarwal et al. (2012) describe a graphical tool that was used in the annotation of the Hin</context>
</contexts>
<marker>Dickinson, Meurers, 2003</marker>
<rawString>Markus Dickinson and W. Detmar Meurers. 2003b. Detecting Inconsistencies in Treebanks. In TLT 2003, pages 45–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus G¨artner</author>
<author>Gregor Thiele</author>
<author>Wolfgang Seeker</author>
<author>Anders Bj¨orkelund</author>
<author>Jonas Kuhn</author>
</authors>
<title>ICARUS – An Extensible Graphical Search Tool for Dependency Treebanks. In ACL: System Demonstrations,</title>
<date>2013</date>
<pages>55--60</pages>
<marker>G¨artner, Thiele, Seeker, Bj¨orkelund, Kuhn, 2013</marker>
<rawString>Markus G¨artner, Gregor Thiele, Wolfgang Seeker, Anders Bj¨orkelund, and Jonas Kuhn. 2013. ICARUS – An Extensible Graphical Search Tool for Dependency Treebanks. In ACL: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Seeker</author>
<author>Jonas Kuhn</author>
</authors>
<title>Making Ellipses Explicit in Dependency Conversion for a German Treebank. In LREC</title>
<date>2012</date>
<pages>3132--3139</pages>
<contexts>
<context position="10632" citStr="Seeker and Kuhn (2012)" startWordPosition="1756" endWordPosition="1759">st or the distribution view. Dependency edges are therein displayed as triples: the head, the dependent, and the edge label with the edge’s direction. As with the part-of-speech tags, the user can always jump directly to the sentences that contain a particular n-gram or dependency relation. 4Actually, the word form hinzukommen can belong to two different verbs, hinzu-kommen and hin-kommen. However, the latter, which incorporates zu, does not occur in TiGer. 59 4 Error Detection on TiGer We ran the error mining algorithm for part-ofspeech on the German TiGer Treebank (the dependency version by Seeker and Kuhn (2012)) and manually evaluated a small sample of n-grams in order to get an idea of how useful the output is. We manually checked 115 out of the 207 variation 6-grams found by the tool, which amounts to 119 different nuclei. For 99.16% of these nuclei, we found erroneous annotations in the associated sentences. 95.6% of these are errors where we are able to decide what the right tag should be, the remaining ones are more difficult to disambiguate because the annotation guidelines do not cover them. These results are in line with findings by Dickinson and Meurers (2003a) for the Penn Treebank. They s</context>
</contexts>
<marker>Seeker, Kuhn, 2012</marker>
<rawString>Wolfgang Seeker and Jonas Kuhn. 2012. Making Ellipses Explicit in Dependency Conversion for a German Treebank. In LREC 2012, pages 3132–3139.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>