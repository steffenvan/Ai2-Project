<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997858">
An Algebra for Semantic Construction in Constraint-based Grammars
</title>
<author confidence="0.985814">
Ann Copestake
</author>
<affiliation confidence="0.991016">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.86298">
New Museums Site
Pembroke St, Cambridge, UK
</address>
<email confidence="0.998223">
aac@cl.cam.ac.uk
</email>
<author confidence="0.986892">
Alex Lascarides
</author>
<affiliation confidence="0.998859">
Division of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.7149535">
2 Buccleuch Place
Edinburgh, Scotland, UK
</address>
<email confidence="0.998135">
alex@cogsci.ed.ac.uk
</email>
<author confidence="0.88107">
Dan Flickinger
</author>
<affiliation confidence="0.724529">
CSLI, Stanford University and
YY Software
</affiliation>
<address confidence="0.82571">
Ventura Hall, 220 Panama St
Stanford, CA 94305, USA
</address>
<email confidence="0.99917">
danf@csli.stanford.edu
</email>
<sectionHeader confidence="0.997395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999770181818182">
We develop a framework for formaliz-
ing semantic construction within gram-
mars expressed in typed feature struc-
ture logics, including HPSG. The ap-
proach provides an alternative to the
lambda calculus; it maintains much of
the desirable flexibility of unification-
based approaches to composition, while
constraining the allowable operations in
order to capture basic generalizations
and improve maintainability.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998833111111111">
Some constraint-based grammar formalisms in-
corporate both syntactic and semantic representa-
tions within the same structure. For instance, Fig-
ure 1 shows representations of typed feature struc-
tures (TFSs) for Kim, sleeps and the phrase Kim
sleeps, in an HPSG-like representation, loosely
based on Sag and Wasow (1999). The semantic
representation expressed is intended to be equiv-
alent to r name(x, Kim) ∧ sleep(e, x).1 Note:
</bodyText>
<listItem confidence="0.952555357142857">
1. Variable equivalence is represented by coin-
dexation within a TFS.
2. The coindexation in Kim sleeps is achieved
as an effect of instantiating the SUBJ slot in
the sign for sleeps.
3. Structures representing individual predicate
applications (henceforth, elementary predi-
cations, or EPs) are accumulated by an ap-
pend operation. Conjunction of EPs is im-
plicit.
1The variables are free, we will discuss scopal relation-
ships and quantifiers below.
4. All signs have an index functioning some-
what like a A-variable.
</listItem>
<bodyText confidence="0.999863861111111">
A similar approach has been used in a large
number of implemented grammars (see Shieber
(1986) for a fairly early example). It is in many
ways easier to work with than A-calculus based
approaches (which we discuss further below) and
has the great advantage of allowing generaliza-
tions about the syntax-semantics interface to be
easily expressed. But there are problems. The
operations are only specified in terms of the TFS
logic: the interpretation relies on an intuitive cor-
respondence with a conventional logical represen-
tation, but this is not spelled out. Furthermore
the operations on the semantics are not tightly
specified or constrained. For instance, although
HPSG has the Semantics Principle (Pollard and
Sag, 1994) this does not stop the composition pro-
cess accessing arbitrary pieces of structure, so it
is often not easy to conceptually disentangle the
syntax and semantics in an HPSG. Nothing guar-
antees that the grammar is monotonic, by which
we mean that in each rule application the seman-
tic content of each daughter subsumes some por-
tion of the semantic content of the mother (i.e.,
no semantic information is dropped during com-
position): this makes it impossible to guarantee
that certain generation algorithms will work ef-
fectively. Finally, from a theoretical perspective,
it seems clear that substantive generalizations are
being missed.
Minimal Recursion Semantics (MRS: Copes-
take et al (1999), see also Egg (1998)) tight-
ens up the specification of composition a little.
It enforces monotonic accumulation of EPs by
making all rules append the EPs of their daugh-
ters (an approach which was followed by Sag
and Wasow (1999)) but it does not fully spec-
</bodyText>
<figure confidence="0.991132941176471">
np
HEAD noun
SUBJ &lt; &gt;
COMPS &lt; &gt;
INDEX 5 ref-ind
&amp;quot; #
RELN R NAME
RESTR &lt;
INSTANCE 5 &gt;
NAME KIM
Kim ⎡ SYN ⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ SEM ⎣
⎡
⎢⎣
⎤
⎦
⎤ sleeps
</figure>
<equation confidence="0.98014575">
⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥
⎡ ⎡ ⎤ ⎤
HEAD verb
⎢ &amp;quot;SYN np #
⎢ ⎢ i ⎥ ⎥
⎢ ⎢ h INDEX 6 ⎥ ⎥
SYN
⎢ ⎣ SUBJ &lt; &gt; ⎦ ⎥
SEM RESTR 7 ⎥
⎢ ⎥
⎢COMPS &lt; &gt; ⎥
⎢ ⎡ ⎤ ⎥
⎢INDEX 15 event ⎥
⎢ &amp;quot; # ⎥
⎢SEM⎢⎥
RELN SLEEP ⎣
RESTR &lt; SIT 15 &gt; ⎦ ⎦
ACT 6
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣
Kim sleeps
⎤
⎦⎥⎥⎥⎥⎥⎥⎥⎥
SYN �HEAD 0 verb �
⎡
INDEX 2 event 1 1
SEM⎢RESTR 10 &lt; &amp;quot; INASTANKIC LN ME 4¢ J &gt; ®11 &lt; � SITevent
SLEEP J &gt; ACT 4
h INDEX 2 i
HEAD-DTR.SEM
RESTR 10
NON-HD-DTR.SEM.RESTR 11
</equation>
<figureCaption confidence="0.898062">
Figure 1: Expressing seman
</figureCaption>
<bodyText confidence="0.465565">
tics in TFSs
</bodyText>
<equation confidence="0.890197333333333">
TFSs
CSLI
http://lingo.stanford.edu).
HPSG
MRS
λ-calculus
</equation>
<bodyText confidence="0.960219857142857">
d to other work
on unification based grammar.
and abstracts away from the specific fea-
ture architecture used in individual grammars, but
the essential features of the algebra can be en-
coded in the hierarchy of lexical and construc-
tional type constraints. Our work actually started
as an attempt at rational reconstruction of se-
mantic composition in the large grammar imple-
mented by the LinGO project at
(available
via
Se-
mantics and the syntax/semantics interface have
accounted for approximately nine-tenths of the
development time of the English Resource Gram-
mar (ERG), largely because the account of seman-
tics within
is so underdetermined.
In this paper, we begin by giving a formal ac-
count of a very simplified form of the algebra and
in §3, we consider its interpretation. In §4 to §6,
we generalize to the full algebra needed to capture
the use of
in the LinGO English Resource
Grammar (ERG). Finally we conclude with some
comparisons to the
an
</bodyText>
<sectionHeader confidence="0.871297" genericHeader="introduction">
2 A simple semantic algebra
</sectionHeader>
<equation confidence="0.908810727272727">
Kim:
[]comp}[r
sleeps:
Kim sleeps:
r
=
The last structure is semantically equivalent to:
r
Kim)].
In the structure for sleeps, the first part,
is
</equation>
<bodyText confidence="0.992848592592593">
a hook and the second part
and
is the holes. The third element (the lzt) is a bag
of elementary predications
Intuitively, the
hook is a record of the value in the semantic en-
tity that can be used to fill a hole in another entity
during composition. The holes record gaps in the
semantic form which occur because it represents
a syntactically unsaturated structure. Some struc-
tures have no holes, such as that for Kim. When
structures are composed, a hole in one structure
(the semantic head) is filled with the hook of the
other (by equating the variables) and their lzts are
appended. It should be intuitively obvious that
there is a straightforward relationship between
this algebra and the
shown in Figure 1, al-
though there are other
architectures which
would share the same encoding.
We now give a formal description of the alge-
bra. In this section, we simplify by assuming that
each entity has only one hole, which is unlabelled,
and only consider two sorts of variables: events
and individuals. The set of semantic entities is
built from the following vocabulary
</bodyText>
<equation confidence="0.983839214285714">
[x2]{[]subj,
name(x2,Kim)]{}
[e1]{[x1]subj,[]comp}[sleep(e1,x1)]{}
[e1]{[]subj,[]comp}[sleep(e1,x1),
name(x2,Kim)]{x1
x2}
[sleep(e1,x1),
name(x1,
[e1],
([x1]subj
[]comp)
(EPs).2
TFSs
TFS
</equation>
<bodyText confidence="0.965008888888889">
:
The following shows the equivalents of the struc-
tures in Figure 1 in our algebra:
ify compositional principles and does not for-
malize composition. We attempt to rectify these
problems, by developing an algebra which gives
a general way of expressing composition. The
semantic algebra lets us specify the allowable
operations in a less cumbersome notation than
</bodyText>
<footnote confidence="0.8821876">
usual in
this is a bag rather than a set because
we do not want to have to check for/disallow repeated
2As
MRS,
</footnote>
<page confidence="0.13539">
EPs;
</page>
<bodyText confidence="0.614429">
e.g., big big car.
</bodyText>
<listItem confidence="0.903743">
1. The absurdity symbol L.
2. indices i1, i2, ..., consisting of two subtypes
of indices: events e1, e2,... and individuals
x1, x2,.. ..
3. n-place predicates, which take indices as ar-
guments
4. =.
Equality can only be used to identify variables of
compatible sorts: e.g., x1 = x2 is well formed,
but e = x is not. Sort compatibility corresponds
to unifiability in the TFS logic.
Definition 1 Simple Elementary Predications
(SEP)
An SEP contains two components:
1. A relation symbol
2. A list of zero or more ordinary variable ar-
guments of the relation (i.e., indices)
</listItem>
<bodyText confidence="0.91344625">
This is written relation(arg1, ... ,argn). For in-
stance, like(e, x, y) is a well-formed SEP.
Equality Conditions: Where i1 and i2 are in-
dices, i1 = i2 is an equality condition.
</bodyText>
<construct confidence="0.7104">
Definition 2 The Set E of Simple semantic Enti-
ties (SSEMENT)
s E E if and only if s = L or s = (s1, s2, s3, s4)
such that:
</construct>
<listItem confidence="0.9953918">
• s1 = {[i]} is a hook;
• s2 = 0 or {[i&apos;]} is a hole;
• s3 is a bag of SEPs(the lzt)
• s4 is a set of equalities between variables
(the eqs).
</listItem>
<bodyText confidence="0.870934555555556">
We write a SSEMENT as: [i1][i2][SEPs]{EQs}.
Note for convenience we omit the set markers {}
from the hook and hole when there is no possible
confusion. The SEPs, and EQs are (partial) de-
scriptions of the fully specified formulae of first
order logic.
Definition 3 The Semantic Algebra
A Semantic Algebra defined on vocabulary V is
the algebra (E, op) where:
</bodyText>
<listItem confidence="0.996395571428571">
• E is the set of SSEMENTs defined on the vo-
cabulary V, as given above;
• op : E x E −→ E is the operation of se-
mantic composition. It satisfies the follow-
ing conditions. If a1 = L or a2 = L or
hole(a2) = 0, then op(a1, a2) = L. Other-
wise:
</listItem>
<equation confidence="0.8245252">
1. hook(op(a1,a2)) = hook(a2)
2. hole(op(a1,a2)) = hole(a1)
3. lzt(op(a1, a2)) = lzt(a1) ⊕ lzt(a2)
4. eq(op(a1, a2)) = Tr(eq(a1)Ueq(a2)U
hook(a1) = hole(a2)})
</equation>
<bodyText confidence="0.7423735">
where Tr stands for transitive closure
(i.e., if S = {x = y, y = z}, then
</bodyText>
<equation confidence="0.808613">
Tr(S) = {x = y,y = z,x = z}).
</equation>
<bodyText confidence="0.9970235">
This definition makes a2 the equivalent of a se-
mantic functor and a1 its argument.
Theorem 1 op is a function
If a1 = a3 and a2 = a4, then a5 = op(a1, a2) =
op(a3, a4) = a6. Thus op is a function. Further-
more, the range of op is within E. So (E, op) is
an algebra.
We can assume that semantic composition al-
ways involves two arguments, since we can de-
fine composition in ternary rules etc as a sequence
of binary operations. Grammar rules (i.e., con-
structions) may contribute semantic information,
but we assume that this information obeys all the
same constraints as the semantics for a sign, so
in effect such a rule is semantically equivalent to
having null elements in the grammar. The corre-
spondence between the order of the arguments to
op and linear order is specified by syntax.
We use variables and equality statements to
achieve the same effect as coindexation in TFSs.
This raises one problem, which is the need to
avoid accidental variable equivalences (e.g., acci-
dentally using x in both the signs for cat and dog
when building the logical form of A dog chased
a cat). We avoid this by adopting a convention
that each instance of a lexical sign comes from
a set of basic sements that have pairwise distinct
variables. The equivalent of coindexation within
a lexical sign is represented by repeating the same
variable but the equivalent of coindexation that
occurs during semantic composition is an equality
condition which identifies two different variables.
Stating this formally is straightforward but a little
long-winded, so we omit it here.
</bodyText>
<sectionHeader confidence="0.904527" genericHeader="method">
3 Interpretation
</sectionHeader>
<bodyText confidence="0.813765">
The SEPs and EQs can be interpreted with respect
to a first order model hE, A, Fi where:
Definition 4 Denotations of SEMENTs
If a =6 ⊥ is a SEMENT, [a]M = h[i], [i0], Gi
where:
</bodyText>
<listItem confidence="0.9136904">
1. E is a set of events
2. A is a set of individuals
3. F is an interpretation function, which as-
signs tuples of appropriate kinds to the pred-
icates of the language.
</listItem>
<bodyText confidence="0.999229333333333">
The truth definition of the SEPs and EQs
(which we group together under the term SMRS,
for simple MRS) is as follows:
</bodyText>
<listItem confidence="0.998694666666667">
1. For all events and individuals v, [v]hM,gi =
g(v).
2. For all n-predicates Pn,
</listItem>
<equation confidence="0.816094166666667">
[Pn]hM,gi = {ht1, . . . , tni : ht1, . . . , tni ∈
F(Pn)}.
3. [Pn(v1, ... , vn)]hM,gi = 1 iff
h[v1]hM,gi, ... , [vn]hM,gii ∈ [Pn]hM,gi.
4. [φ ∧ ψ]hM,gi = 1 iff
[φ]hM,gi = 1 and [ψ]hM,gi = 1.
</equation>
<bodyText confidence="0.99423024">
Thus, with respect to a model M, an SMRS can be
viewed as denoting an element of P(G), where
G is the set of variable assignment functions (i.e.,
elements of G assign the variables e,... and x, .. .
their denotations):
[smrs]M = {g : g is a variable assignment
function and M |=g smrs}
We now consider the semantics of the algebra.
This must define the semantics of the operation op
in terms of a function f which is defined entirely
in terms of the denotations of op’s arguments. In
other words, [op(a1, a2)] = f([a1], [a2]) for
some function f. Intuitively, where the SMRS
of the SEMENT a1 denotes G1 and the SMRS of
the SEMENT a2 denotes G2, we want the seman-
tic value of the SMRS of op(a1, a2) to denote the
following:
G1 ∩ G2 ∩ [hook(a1) = hole(a2)]
But this cannot be constructed purely as a func-
tion of G1 and G2.
The solution is to add hooks and holes to the
denotations of SEMENTS (cf. Zeevat, 1989). We
define the denotation of a SEMENT to be an ele-
ment of I × I × P(G), where I = E ∪ A, as
follows:
</bodyText>
<listItem confidence="0.992536333333333">
1. [i] = hook(a)
2. [i0] = hole(a)
3. G = {g : M |=g smrs(a)}
</listItem>
<equation confidence="0.849126">
[⊥]M = h∅, ∅, ∅i
</equation>
<bodyText confidence="0.919253125">
So, the meanings of SEMENTs are ordered three-
tuples, consisting of the hook and hole elements
(from I) and a set of variable assignment func-
tions that satisfy the SMRS.
We can now define the following operation f
over these denotations to create an algebra:
Definition 5 Semantics of the Semantic Con-
struction Algebra
</bodyText>
<equation confidence="0.990781">
hI × I × P(G), fi is an algebra, where:
f(h∅, ∅, ∅i, h[i2], [i02], G2i) = h∅, ∅, ∅i
f(h[i1], [i01], G1i, h∅, ∅, ∅i) = h∅, ∅, ∅i
f(h[i1], [i01], G1i, h[i2], ∅, G2i = h∅, ∅, ∅i
f(h[i1], [i01], G1i, h[i2], [i02], G2i) =
h[i2], [i01], G1 ∩ G2 ∩ G0i
</equation>
<bodyText confidence="0.979856555555556">
where G0 = {g : g(i1) = g(i02)}
And this operation demonstrates that semantic
construction is compositional:
Theorem 2 Semantics of Semantic Construction
is Compositional
The mapping [] : hΣ, opi −→ hhI, I, Gi, fi
is a homomorphism (so [op(a1, a2)] =
f([a1], [a2])).
This follows from the definitions of [], op and f.
</bodyText>
<sectionHeader confidence="0.987988" genericHeader="method">
4 Labelling holes
</sectionHeader>
<bodyText confidence="0.9998752">
We now start considering the elaborations neces-
sary for real grammars. As we suggested earlier,
it is necessary to have multiple labelled holes.
There will be a fixed inventory of labels for any
grammar framework, although there may be some
differences between variants.3 In HPSG, comple-
ments are represented using a list, but in general
there will be a fixed upper limit for the number
of complements so we can label holes COMP1,
COMP2, etc. The full inventory of labels for
</bodyText>
<footnote confidence="0.928082">
3For instance, Sag and Wasow (1999) omit the distinction
between SPR and SUBJ that is often made in other HPSGs.
</footnote>
<bodyText confidence="0.94661">
the ERG is: SUBJ, SPR, SPEC, COMP1, COMP2,
COMP3 and MOD (see Pollard and Sag, 1994).
To illustrate the way the formalization goes
with multiple slots, consider opsubj:
Definition 6 The definition of opsubj
</bodyText>
<equation confidence="0.99854">
opsubj(a1, a2) is thefollowing: Ifa1 = L or a2 =
L orholesubj(a2) = 0, then opsubj(a1,a2) = L.
And if 1l =� subj such that:
|holel(a1) U holel(a2) |&gt; 1
then opsubj(a1, a2) = L. Otherwise:
1. hook(opsubj(a1,a2)) = hook(a2)
2. For all labels l =� subj:
holel(opsubj(a1,a2)) = holel(a1) U
holel(a2)
3. lzt(opsubj(a1, a2)) = lzt(a1) ® lzt(a2)
4. eq(opsubj(a1, a2)) = Tr(eq(a1) U eq(a2)U
{hook(a1) = holesubj(a2)1)
</equation>
<bodyText confidence="0.99893495">
where Tr stands for transitive closure.
There will be similar operations opcomp1,
opcomp2 etc for each labelled hole. These
operations can be proved to form an algebra
(E, opsubj, opcomp1, . . .) in a similar way to the
unlabelled case shown in Theorem 1. A lit-
tle more work is needed to prove that opl is
closed on E. In particular, with respect to
clause 2 of the above definition, it is necessary
to prove that opl(a1, a2) = L or for all labels l&apos;,
|holel,(opl(a1, a2)) |&lt; 1, but it is straightforward
to see this is the case.
These operations can be extended in a straight-
forward way to handle simple constituent coor-
dination of the kind that is currently dealt with
in the ERG (e.g., Kim sleeps and talks and Kim
and Sandy sleep); such cases involve daughters
with non-empty holes of the same label, and
the semantic operation equates these holes in the
mother SEMENT.
</bodyText>
<sectionHeader confidence="0.993318" genericHeader="method">
5 Scopal relationships
</sectionHeader>
<bodyText confidence="0.999899928571429">
The algebra with labelled holes is sufficient to
deal with simple grammars, such as that in Sag
and Wasow (1999), but to deal with scope, more is
needed. It is now usual in constraint based gram-
mars to allow for underspecification of quantifier
scope by giving labels to pieces of semantic in-
formation and stating constraints between the la-
bels. In MRS, labels called handles are associ-
ated with each EP. Scopal relationships are rep-
resented by EPs with handle-taking arguments.
If all handle arguments are filled by handles la-
belling EPs, the structure is fully scoped, but in
general the relationship is not directly specified
in a logical form but is constrained by the gram-
mar via additional conditions (handle constraints
or hcons).4 A variety of different types of condi-
tion are possible, and the algebra developed here
is neutral between them, so we will simply use
relh to stand for such a constraint, intending it to
be neutral between, for instance, =Q (qeq: equal-
ity modulo quantifiers) relationships used in MRS
and the more usual &lt; relationships from UDRT
(Reyle, 1993). The conditions in hcons are accu-
mulated by append.
To accommodate scoping in the algebra, we
will make hooks and holes pairs of indices and
handles. The handle in the hook corresponds to
the LTOP feature in MRS. The new vocabulary is:
</bodyText>
<listItem confidence="0.999163">
1. The absurdity symbol L.
2. handles h1, h2, .. .
3. indices i1, i2, ..., as before
4. n-predicates which take handles and indices
as arguments
5. relh and =.
</listItem>
<bodyText confidence="0.769624666666667">
The revised definition of an EP is as in MRS:
Definition 7 Elementary Predications (EPs)
An EP contains exactly four components:
</bodyText>
<listItem confidence="0.999292">
1. a handle, which is the label of the EP
2. a relation
3. a list of zero or more ordinary variable ar-
guments of the relation (i.e., indices)
4. a list ofzero or more handles corresponding
to scopal arguments of the relation.
</listItem>
<footnote confidence="0.978715083333333">
4The underspecified scoped forms which correspond to
sentences can be related to first order models of the fully
scoped forms (i.e., to models of WFFs without labels) via
supervaluation (e.g., Reyle, 1993). This corresponds to stip-
ulating that an underspecified logical form u entails a base,
fully specified form 0 only if all possible ways of resolving
the underspecification in u entails 0. For reasons of space,
we do not give details here, but note that this is entirely con-
sistent with treating semantics in terms of a description of
a logical formula. The relationship between the SEMENTS
of non-sentential constituents and a more ‘standard’ formal
language such as A-calculus will be explored in future work.
</footnote>
<bodyText confidence="0.992234">
This is written h:r(a1, ... ,an,sa1, ... ,sam). For
instance, h:every(x, h1, h2) is an EP.5
We revise the definition of semantic entities to
add the hcons conditions and to make hooks and
holes pairs of handles and indices.
</bodyText>
<construct confidence="0.8364988">
H-Cons Conditions: Where h1 and h2 are
handles, h1relhh2 is an H-Cons condition.
Definition 8 The Set E of Semantic Entities
s E E if and only if s = L or s =
(s1, s2, s3, s4, s5) such that:
</construct>
<listItem confidence="0.913523666666667">
• s1 = {[h, i]} is a hook;
• s2 = ∅ or {[h&apos;, i&apos;]} is a hole;
• s3 is a bag of EP conditions
• s4 is a bag of HCONS conditions
• s5 is a set of equalities between variables.
SEMENTs are: [h1,i1]{holes}[eps][hcons]{eqs}.
</listItem>
<bodyText confidence="0.9999626">
We will not repeat the full composition def-
inition, since it is unchanged from that in §2
apart from the addition of the append operation
on hcons and a slight complication of eq to deal
with the handle/index pairs:
</bodyText>
<equation confidence="0.999238333333333">
eq(op(a1, a2)) = Tr(eq(a1) U eq(a2)U
{hdle(hook(a1)) = hdle(hole(a2)),
ind(hook(a1)) = ind(hole(a2))})
</equation>
<bodyText confidence="0.9986193125">
where Tr stands for transitive closure as before
and hdle and ind access the handle and index of
a pair. We can extend this to include (several) la-
belled holes and operations, as before. And these
revised operations still form an algebra.
The truth definition for SEMENTS is analogous
to before. We add to the model a set of la-
bels L (handles denote these via g) and a well-
founded partial order G on L (this helps interpret
the hcons; cf. Fernando (1997)). A SEMENT then
denotes an element of H x ...H x P(G), where
the Hs (= L x I) are the new hook and holes.
Note that the language E is first order, and
we do not use A-abstraction over higher or-
der elements.6 For example, in the standard
Montagovian view, a quantifier such as every
</bodyText>
<footnote confidence="0.902620285714286">
5Note every is a predicate rather than a quantifier in
this language, since MRSs are partial descriptions of logical
forms in a base language.
6Even though we do not use λ-calculus for composition,
we could make use of λ-abstraction as a representation de-
vice, for instance for dealing with adjectives such as former,
cf., Moore (1989).
</footnote>
<bodyText confidence="0.9946495">
is represented by the higher-order expression
APAQbx(P(x), Q(x)). In our framework, how-
ever, every is the following (using qeq conditions,
as in the LinGO ERG):
</bodyText>
<equation confidence="0.913401090909091">
[hf, x]{[]subj, []comp1, [h&apos;, x]spec, ...}
[he : every(x, hr, hs)][hr =q h&apos;]{}
and dog is:
[hd,y]{[]subj, []comp1, []spec,...}[hd : dog(y)][]{}
So these composes via opspec to yield every dog:
[hf, x]{[]subj, []comp1, []spec, . . .}
[he : every(x, hr, hs), hd : dog(y)]
[hr =q h&apos;]{h&apos; = hd, x = y}
This SEMENT is semantically equivalent to:
[hf,x]{[]subj, []comp1, []spec,...}
[he : every(x, hr, hs), hd : dog(x)][hr =q hd]{}
</equation>
<bodyText confidence="0.9999305">
A slight complication is that the determiner is
also syntactically selected by the N&apos; via the SPR
slot (following Pollard and Sag (1994)). How-
ever, from the standpoint of the compositional
semantics, the determiner is the semantic head,
and it is only its SPEC hole which is involved: the
N&apos; must be treated as having an empty SPR hole.
In the ERG, the distinction between intersective
and scopal modification arises because of distinc-
tions in representation at the lexical level. The
repetition of variables in the SEMENT of a lexical
sign (corresponding to TFS coindexation) and the
choice of type on those variables determines the
type of modification.
</bodyText>
<equation confidence="0.921510125">
Intersective modification: white dog:
dog: [hd, y]{[]subj,[]comp1, . . . , []mod}
[hd : dog(y)][]{}
white: [hw, x]{[]subj, []comp1, .., [hw, x]mod}
[hw : white(x)][]{}
white dog: [hw, x]{[]subj, []comp1, . . . , []mod}
(opmod) [hd : dog(y), hw : white(x)][]
{hw = hd, x = y}
</equation>
<construct confidence="0.937096375">
Scopal Modification: probably walks:
walks: [hw, e&apos;]{[h&apos;, x]subj, []comp1, ... , []mod}
[hw : walks(e&apos;,x)][]{}
probably: [hp, e]{[]subj, []comp1, . . . , [h, e]mod}
[hp : probably(hs)][hs =q h]{}
probably [hp, e]{[h&apos;, x]subj, []comp1, . . . , []mod}
walks: [hp:probably(hs), hw:walks(e&apos;, x)]
(opmod) [hs =q h]{hw = h, e = e&apos;}
</construct>
<sectionHeader confidence="0.975139" genericHeader="method">
6 Control and external arguments
</sectionHeader>
<bodyText confidence="0.973629545454545">
We need to make one further extension to allow
for control, which we do by adding an extra slot to
the hooks and holes corresponding to the external
argument (e.g., the external argument of a verb
always corresponds to its subject position). We
illustrate this by showing two uses of expect; note
the third slot in the hooks and holes for the exter-
nal argument of each entity. In both cases, x0e is
both the external argument of expect and its sub-
ject’s index, but in the first structure x0e is also the
external argument of the complement, thus giving
the control effect.
expect 1 (as in Kim expected to sleep)
[he, ee, x0 e]{[hs, x0 e, x0 s]subj, [hc, ec, x0 e]comp1,.. .}
[he : expect(ee, x0e, h0e)][h0e =q hc]{}
expect 2 (Kim expected that Sandy would sleep)
[he, ee, x0e]{[hs, x0e, x0s]subj, [hc, ec, x0c]comp1,.. .}
[h : expect(ee, x0e, h0e)][h0e =q hc]{}
Although these uses require different lexical en-
tries, the semantic predicate expect used in the
two examples is the same, in contrast to Montago-
vian approaches, which either relate two distinct
predicates via meaning postulates, or require an
additional semantic combinator. The HPSG ac-
count does not involve such additional machinery,
but its formal underpinnings have been unclear:
in this algebra, it can be seen that the desired re-
sult arises as a consequence of the restrictions on
variable assignments imposed by the equalities.
This completes our sketch of the algebra neces-
sary to encode semantic composition in the ERG.
We have constrained accessibility by enumerating
the possible labels for holes and by stipulating the
contents of the hooks. We believe that the han-
dle, index, external argument triple constitutes all
the semantic information that a sign should make
accessible to a functor. The fact that only these
pieces of information are visible means, for in-
stance, that it is impossible to define a verb that
controls the object of its complement.7 Although
obviously changes to the syntactic valence fea-
tures would necessitate modification of the hole
labels, we think it unlikely that we will need to in-
crease the inventory further. In combination with
</bodyText>
<footnote confidence="0.980758">
7Readers familiar with MRS will notice that the KEY fea-
ture used for semantic selection violates these accessibility
conditions, but in the current framework, KEY can be re-
placed by KEYPRED which points to the predicate alone.
</footnote>
<bodyText confidence="0.9995516">
the principles defined in Copestake et al (1999)
for qeq conditions, the algebra presented here re-
sults in a much more tightly specified approach
to semantic composition than that in Pollard and
Sag (1994).
</bodyText>
<sectionHeader confidence="0.997681" genericHeader="method">
7 Comparison
</sectionHeader>
<bodyText confidence="0.99991975">
Compared with A-calculus, the approach to com-
position adopted in constraint-based grammars
and formalized here has considerable advantages
in terms of simplicity. The standard Montague
grammar approach requires that arguments be
presented in a fixed order, and that they be strictly
typed, which leads to unnecessary multiplication
of predicates which then have to be interrelated
by meaning postulates (e.g., the two uses of ex-
pect mentioned earlier). Type raising also adds
to the complexity. As standardly presented, A-
calculus does not constrain grammars to be mono-
tonic, and does not control accessibility, since the
variable of the functor that is A-abstracted over
may be arbitrarily deeply embedded inside a A-
expression.
None of the previous work on unification-
based approaches to semantics has considered
constraints on composition in the way we have
presented. In fact, Nerbonne (1995) explicitly
advocates nonmonotonicity. Moore (1989) is
also concerned with formalizing existing prac-
tice in unification grammars (see also Alshawi,
1992), though he assumes Prolog-style unifica-
tion, rather than TFSs. Moore attempts to for-
malize his approach in the logic of unification,
but it is not clear this is entirely successful. He
has to divorce the interpretation of the expres-
sions from the notion of truth with respect to the
model, which is much like treating the semantics
as a description of a logic formula. Our strategy
for formalization is closest to that adopted in Uni-
fication Categorial Grammar (Zeevat et al, 1987),
but rather than composing actual logical forms we
compose partial descriptions to handle semantic
underspecification.
</bodyText>
<sectionHeader confidence="0.991076" genericHeader="conclusions">
8 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999996953488372">
We have developed a framework for formally
specifying semantics within constraint-based rep-
resentations which allows semantic operations in
a grammar to be tightly specified and which al-
lows a representation of semantic content which
is largely independent of the feature structure ar-
chitecture of the syntactic representation. HPSGs
can be written which encode much of the algebra
described here as constraints on types in the gram-
mar, thus ensuring that the grammar is consistent
with the rules on composition. There are some as-
pects which cannot be encoded within currently
implemented TFS formalisms because they in-
volve negative conditions: for instance, we could
not write TFS constraints that absolutely prevent
a grammar writer sneaking in a disallowed coin-
dexation by specifying a path into the lzt. There is
the option of moving to a more general TFS logic
but this would require very considerable research
to develop reasonable tractability. Since the con-
straints need not be checked at runtime, it seems
better to regard them as metalevel conditions on
the description of the grammar, which can any-
way easily be checked by code which converts the
TFS into the algebraic representation.
Because the ERG is large and complex, we have
not yet fully completed the exercise of retrospec-
tively implementing the constraints throughout.
However, much of the work has been done and
the process revealed many bugs in the grammar,
which demonstrates the potential for enhanced
maintainability. We have modified the grammar
to be monotonic, which is important for the chart
generator described in Carroll et al (1999). A
chart generator must determine lexical entries di-
rectly from an input logical form: hence it will
only work if all instances of nonmonotonicity can
be identified in a grammar-specific preparatory
step. We have increased the generator’s reliability
by making the ERG monotonic and we expect fur-
ther improvements in practical performance once
we take full advantage of the restrictions in the
grammar to cut down the search space.
</bodyText>
<sectionHeader confidence="0.999035" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.73009">
This research was partially supported by the Na-
tional Science Foundation, grant number IRI-
9612682. Alex Lascarides was supported by an
ESRC (UK) research fellowship. We are grateful
to Ted Briscoe, Alistair Knott and the anonymous
reviewers for their comments on this paper.
</bodyText>
<sectionHeader confidence="0.969719" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997750361702127">
Alshawi, Hiyan [1992] (ed.) The Core Language
Engine, MIT Press.
Carroll, John, Ann Copestake, Dan Flickinger
and Victor Poznanski [1999] An Efficient Chart
Generator for Lexicalist Grammars, The 7th In-
ternational Workshop on Natural Language Gen-
eration, 86–95.
Copestake, Ann, Dan Flickinger, Ivan Sag
and Carl Pollard [1999] Minimal Recursion Se-
mantics: An Introduction, manuscript at www-
csli.stanford.edu/˜aac/newmrs.ps
Egg, Marcus [1998] Wh-Questions in Under-
specified Minimal Recursion Semantics, Journal
of Semantics, 15.1:37–82.
Fernando, Tim [1997] Ambiguity in Changing
Contexts, Linguistics and Philosophy, 20.6: 575–
606.
Moore, Robert C. [1989] Unification-based Se-
mantic Interpretation, The 27th Annual Meeting
for the Association for Computational Linguistics
(ACL-89), 33–41.
Nerbonne, John [1995] Computational
Semantics—Linguistics and Processing, Shalom
Lappin (ed.) Handbook of Contemporary
Semantic Theory, 461–484, Blackwells.
Pollard, Carl and Ivan Sag [1994] Head-
Driven Phrase Structure Grammar, University of
Chicago Press.
Reyle, Uwe [1993] Dealing with Ambiguities
by Underspecification: Construction, Represen-
tation and Deduction, Journal of Semantics, 10.1:
123–179.
Sag, Ivan, and Tom Wasow [1999] Syntactic
Theory: An Introduction, CSLI Publications.
Shieber, Stuart [1986] An Introduction to
Unification-based Approaches to Grammar,
CSLI Publications.
Zeevat, Henk [1989] A Compositional Ap-
proach to Discourse Representation Theory, Lin-
guistics and Philosophy, 12.1: 95–131.
Zeevat, Henk, Ewan Klein and Jo Calder
[1987] An introduction to unification categorial
grammar, Nick Haddock, Ewan Klein and Glyn
Morrill (eds), Categorial grammar, unification
grammar, and parsing: working papers in cogni-
tive science, Volume 1, 195–222, Centre for Cog-
nitive Science, University of Edinburgh.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.340008">
<title confidence="0.999801">An Algebra for Semantic Construction in Constraint-based Grammars</title>
<author confidence="0.995192">Ann Copestake</author>
<affiliation confidence="0.908047666666667">Computer Laboratory University of Cambridge New Museums Site</affiliation>
<address confidence="0.670656">Pembroke St, Cambridge, UK</address>
<email confidence="0.979276">aac@cl.cam.ac.uk</email>
<author confidence="0.990356">Alex Lascarides</author>
<affiliation confidence="0.904615">Division of Informatics University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.952814">Edinburgh, Scotland, UK</address>
<email confidence="0.988904">alex@cogsci.ed.ac.uk</email>
<author confidence="0.999783">Dan Flickinger</author>
<affiliation confidence="0.9988955">Stanford University and YY Software</affiliation>
<address confidence="0.9973045">Ventura Hall, 220 Panama St Stanford, CA 94305, USA</address>
<email confidence="0.999873">danf@csli.stanford.edu</email>
<abstract confidence="0.999370833333333">We develop a framework for formalizing semantic construction within grammars expressed in typed feature struclogics, including The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unificationbased approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Alshawi</author>
</authors>
<title>Hiyan [1992] (ed.) The Core Language Engine,</title>
<publisher>MIT Press.</publisher>
<marker>Alshawi, </marker>
<rawString>Alshawi, Hiyan [1992] (ed.) The Core Language Engine, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Victor Poznanski</author>
</authors>
<title>An Efficient Chart Generator for Lexicalist Grammars,</title>
<date>1999</date>
<booktitle>The 7th International Workshop on Natural Language Generation,</booktitle>
<pages>86--95</pages>
<contexts>
<context position="27639" citStr="Carroll et al (1999)" startWordPosition="4801" endWordPosition="4804"> runtime, it seems better to regard them as metalevel conditions on the description of the grammar, which can anyway easily be checked by code which converts the TFS into the algebraic representation. Because the ERG is large and complex, we have not yet fully completed the exercise of retrospectively implementing the constraints throughout. However, much of the work has been done and the process revealed many bugs in the grammar, which demonstrates the potential for enhanced maintainability. We have modified the grammar to be monotonic, which is important for the chart generator described in Carroll et al (1999). A chart generator must determine lexical entries directly from an input logical form: hence it will only work if all instances of nonmonotonicity can be identified in a grammar-specific preparatory step. We have increased the generator’s reliability by making the ERG monotonic and we expect further improvements in practical performance once we take full advantage of the restrictions in the grammar to cut down the search space. Acknowledgements This research was partially supported by the National Science Foundation, grant number IRI9612682. Alex Lascarides was supported by an ESRC (UK) resea</context>
</contexts>
<marker>Carroll, Copestake, Flickinger, Poznanski, 1999</marker>
<rawString>Carroll, John, Ann Copestake, Dan Flickinger and Victor Poznanski [1999] An Efficient Chart Generator for Lexicalist Grammars, The 7th International Workshop on Natural Language Generation, 86–95.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Ivan Sag and Carl Pollard [1999] Minimal Recursion Semantics: An Introduction, manuscript at wwwcsli.stanford.edu/˜aac/newmrs.ps</title>
<marker>Copestake, Flickinger, </marker>
<rawString>Copestake, Ann, Dan Flickinger, Ivan Sag and Carl Pollard [1999] Minimal Recursion Semantics: An Introduction, manuscript at wwwcsli.stanford.edu/˜aac/newmrs.ps</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Egg</author>
</authors>
<title>Wh-Questions in Underspecified Minimal Recursion Semantics,</title>
<date>1998</date>
<journal>Journal of Semantics,</journal>
<pages>15--1</pages>
<contexts>
<context position="3253" citStr="Egg (1998)" startWordPosition="492" endWordPosition="493">t easy to conceptually disentangle the syntax and semantics in an HPSG. Nothing guarantees that the grammar is monotonic, by which we mean that in each rule application the semantic content of each daughter subsumes some portion of the semantic content of the mother (i.e., no semantic information is dropped during composition): this makes it impossible to guarantee that certain generation algorithms will work effectively. Finally, from a theoretical perspective, it seems clear that substantive generalizations are being missed. Minimal Recursion Semantics (MRS: Copestake et al (1999), see also Egg (1998)) tightens up the specification of composition a little. It enforces monotonic accumulation of EPs by making all rules append the EPs of their daughters (an approach which was followed by Sag and Wasow (1999)) but it does not fully specnp HEAD noun SUBJ &lt; &gt; COMPS &lt; &gt; INDEX 5 ref-ind &amp;quot; # RELN R NAME RESTR &lt; INSTANCE 5 &gt; NAME KIM Kim ⎡ SYN ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ SEM ⎣ ⎡ ⎢⎣ ⎤ ⎦ ⎤ sleeps ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎡ ⎡ ⎤ ⎤ HEAD verb ⎢ &amp;quot;SYN np # ⎢ ⎢ i ⎥ ⎥ ⎢ ⎢ h INDEX 6 ⎥ ⎥ SYN ⎢ ⎣ SUBJ &lt; &gt; ⎦ ⎥ SEM RESTR 7 ⎥ ⎢ ⎥ ⎢COMPS &lt; &gt; ⎥ ⎢ ⎡ ⎤ ⎥ ⎢INDEX 15 event ⎥ ⎢ &amp;quot; # ⎥ ⎢SEM⎢⎥ RELN SLEEP ⎣ RESTR &lt; SIT 15 &gt; ⎦ ⎦ ACT 6 ⎡ ⎢ ⎢ ⎢ ⎢ ⎢</context>
</contexts>
<marker>Egg, 1998</marker>
<rawString>Egg, Marcus [1998] Wh-Questions in Underspecified Minimal Recursion Semantics, Journal of Semantics, 15.1:37–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Fernando</author>
</authors>
<title>Ambiguity in Changing Contexts,</title>
<date>1997</date>
<journal>Linguistics and Philosophy,</journal>
<volume>20</volume>
<pages>575--606</pages>
<contexts>
<context position="19263" citStr="Fernando (1997)" startWordPosition="3430" endWordPosition="3431">t complication of eq to deal with the handle/index pairs: eq(op(a1, a2)) = Tr(eq(a1) U eq(a2)U {hdle(hook(a1)) = hdle(hole(a2)), ind(hook(a1)) = ind(hole(a2))}) where Tr stands for transitive closure as before and hdle and ind access the handle and index of a pair. We can extend this to include (several) labelled holes and operations, as before. And these revised operations still form an algebra. The truth definition for SEMENTS is analogous to before. We add to the model a set of labels L (handles denote these via g) and a wellfounded partial order G on L (this helps interpret the hcons; cf. Fernando (1997)). A SEMENT then denotes an element of H x ...H x P(G), where the Hs (= L x I) are the new hook and holes. Note that the language E is first order, and we do not use A-abstraction over higher order elements.6 For example, in the standard Montagovian view, a quantifier such as every 5Note every is a predicate rather than a quantifier in this language, since MRSs are partial descriptions of logical forms in a base language. 6Even though we do not use λ-calculus for composition, we could make use of λ-abstraction as a representation device, for instance for dealing with adjectives such as former,</context>
</contexts>
<marker>Fernando, 1997</marker>
<rawString>Fernando, Tim [1997] Ambiguity in Changing Contexts, Linguistics and Philosophy, 20.6: 575– 606.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Unification-based Semantic Interpretation,</title>
<date>1989</date>
<booktitle>The 27th Annual Meeting for the Association for Computational Linguistics (ACL-89),</booktitle>
<pages>33--41</pages>
<contexts>
<context position="19881" citStr="Moore (1989)" startWordPosition="3542" endWordPosition="3543">EMENT then denotes an element of H x ...H x P(G), where the Hs (= L x I) are the new hook and holes. Note that the language E is first order, and we do not use A-abstraction over higher order elements.6 For example, in the standard Montagovian view, a quantifier such as every 5Note every is a predicate rather than a quantifier in this language, since MRSs are partial descriptions of logical forms in a base language. 6Even though we do not use λ-calculus for composition, we could make use of λ-abstraction as a representation device, for instance for dealing with adjectives such as former, cf., Moore (1989). is represented by the higher-order expression APAQbx(P(x), Q(x)). In our framework, however, every is the following (using qeq conditions, as in the LinGO ERG): [hf, x]{[]subj, []comp1, [h&apos;, x]spec, ...} [he : every(x, hr, hs)][hr =q h&apos;]{} and dog is: [hd,y]{[]subj, []comp1, []spec,...}[hd : dog(y)][]{} So these composes via opspec to yield every dog: [hf, x]{[]subj, []comp1, []spec, . . .} [he : every(x, hr, hs), hd : dog(y)] [hr =q h&apos;]{h&apos; = hd, x = y} This SEMENT is semantically equivalent to: [hf,x]{[]subj, []comp1, []spec,...} [he : every(x, hr, hs), hd : dog(x)][hr =q hd]{} A slight com</context>
<context position="25288" citStr="Moore (1989)" startWordPosition="4428" endWordPosition="4429">f predicates which then have to be interrelated by meaning postulates (e.g., the two uses of expect mentioned earlier). Type raising also adds to the complexity. As standardly presented, Acalculus does not constrain grammars to be monotonic, and does not control accessibility, since the variable of the functor that is A-abstracted over may be arbitrarily deeply embedded inside a Aexpression. None of the previous work on unificationbased approaches to semantics has considered constraints on composition in the way we have presented. In fact, Nerbonne (1995) explicitly advocates nonmonotonicity. Moore (1989) is also concerned with formalizing existing practice in unification grammars (see also Alshawi, 1992), though he assumes Prolog-style unification, rather than TFSs. Moore attempts to formalize his approach in the logic of unification, but it is not clear this is entirely successful. He has to divorce the interpretation of the expressions from the notion of truth with respect to the model, which is much like treating the semantics as a description of a logic formula. Our strategy for formalization is closest to that adopted in Unification Categorial Grammar (Zeevat et al, 1987), but rather tha</context>
</contexts>
<marker>Moore, 1989</marker>
<rawString>Moore, Robert C. [1989] Unification-based Semantic Interpretation, The 27th Annual Meeting for the Association for Computational Linguistics (ACL-89), 33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Nerbonne</author>
</authors>
<date>1995</date>
<booktitle>Computational Semantics—Linguistics and Processing, Shalom Lappin (ed.) Handbook of Contemporary Semantic Theory,</booktitle>
<pages>461--484</pages>
<contexts>
<context position="25237" citStr="Nerbonne (1995)" startWordPosition="4423" endWordPosition="4424">tly typed, which leads to unnecessary multiplication of predicates which then have to be interrelated by meaning postulates (e.g., the two uses of expect mentioned earlier). Type raising also adds to the complexity. As standardly presented, Acalculus does not constrain grammars to be monotonic, and does not control accessibility, since the variable of the functor that is A-abstracted over may be arbitrarily deeply embedded inside a Aexpression. None of the previous work on unificationbased approaches to semantics has considered constraints on composition in the way we have presented. In fact, Nerbonne (1995) explicitly advocates nonmonotonicity. Moore (1989) is also concerned with formalizing existing practice in unification grammars (see also Alshawi, 1992), though he assumes Prolog-style unification, rather than TFSs. Moore attempts to formalize his approach in the logic of unification, but it is not clear this is entirely successful. He has to divorce the interpretation of the expressions from the notion of truth with respect to the model, which is much like treating the semantics as a description of a logic formula. Our strategy for formalization is closest to that adopted in Unification Cate</context>
</contexts>
<marker>Nerbonne, 1995</marker>
<rawString>Nerbonne, John [1995] Computational Semantics—Linguistics and Processing, Shalom Lappin (ed.) Handbook of Contemporary Semantic Theory, 461–484, Blackwells.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>HeadDriven Phrase Structure Grammar,</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="2541" citStr="Pollard and Sag, 1994" startWordPosition="376" endWordPosition="379">rly early example). It is in many ways easier to work with than A-calculus based approaches (which we discuss further below) and has the great advantage of allowing generalizations about the syntax-semantics interface to be easily expressed. But there are problems. The operations are only specified in terms of the TFS logic: the interpretation relies on an intuitive correspondence with a conventional logical representation, but this is not spelled out. Furthermore the operations on the semantics are not tightly specified or constrained. For instance, although HPSG has the Semantics Principle (Pollard and Sag, 1994) this does not stop the composition process accessing arbitrary pieces of structure, so it is often not easy to conceptually disentangle the syntax and semantics in an HPSG. Nothing guarantees that the grammar is monotonic, by which we mean that in each rule application the semantic content of each daughter subsumes some portion of the semantic content of the mother (i.e., no semantic information is dropped during composition): this makes it impossible to guarantee that certain generation algorithms will work effectively. Finally, from a theoretical perspective, it seems clear that substantive</context>
<context position="13875" citStr="Pollard and Sag, 1994" startWordPosition="2482" endWordPosition="2485"> for real grammars. As we suggested earlier, it is necessary to have multiple labelled holes. There will be a fixed inventory of labels for any grammar framework, although there may be some differences between variants.3 In HPSG, complements are represented using a list, but in general there will be a fixed upper limit for the number of complements so we can label holes COMP1, COMP2, etc. The full inventory of labels for 3For instance, Sag and Wasow (1999) omit the distinction between SPR and SUBJ that is often made in other HPSGs. the ERG is: SUBJ, SPR, SPEC, COMP1, COMP2, COMP3 and MOD (see Pollard and Sag, 1994). To illustrate the way the formalization goes with multiple slots, consider opsubj: Definition 6 The definition of opsubj opsubj(a1, a2) is thefollowing: Ifa1 = L or a2 = L orholesubj(a2) = 0, then opsubj(a1,a2) = L. And if 1l =� subj such that: |holel(a1) U holel(a2) |&gt; 1 then opsubj(a1, a2) = L. Otherwise: 1. hook(opsubj(a1,a2)) = hook(a2) 2. For all labels l =� subj: holel(opsubj(a1,a2)) = holel(a1) U holel(a2) 3. lzt(opsubj(a1, a2)) = lzt(a1) ® lzt(a2) 4. eq(opsubj(a1, a2)) = Tr(eq(a1) U eq(a2)U {hook(a1) = holesubj(a2)1) where Tr stands for transitive closure. There will be similar opera</context>
<context position="20605" citStr="Pollard and Sag (1994)" startWordPosition="3660" endWordPosition="3663">s the following (using qeq conditions, as in the LinGO ERG): [hf, x]{[]subj, []comp1, [h&apos;, x]spec, ...} [he : every(x, hr, hs)][hr =q h&apos;]{} and dog is: [hd,y]{[]subj, []comp1, []spec,...}[hd : dog(y)][]{} So these composes via opspec to yield every dog: [hf, x]{[]subj, []comp1, []spec, . . .} [he : every(x, hr, hs), hd : dog(y)] [hr =q h&apos;]{h&apos; = hd, x = y} This SEMENT is semantically equivalent to: [hf,x]{[]subj, []comp1, []spec,...} [he : every(x, hr, hs), hd : dog(x)][hr =q hd]{} A slight complication is that the determiner is also syntactically selected by the N&apos; via the SPR slot (following Pollard and Sag (1994)). However, from the standpoint of the compositional semantics, the determiner is the semantic head, and it is only its SPEC hole which is involved: the N&apos; must be treated as having an empty SPR hole. In the ERG, the distinction between intersective and scopal modification arises because of distinctions in representation at the lexical level. The repetition of variables in the SEMENT of a lexical sign (corresponding to TFS coindexation) and the choice of type on those variables determines the type of modification. Intersective modification: white dog: dog: [hd, y]{[]subj,[]comp1, . . . , []mod</context>
<context position="24328" citStr="Pollard and Sag (1994)" startWordPosition="4282" endWordPosition="4285">ly changes to the syntactic valence features would necessitate modification of the hole labels, we think it unlikely that we will need to increase the inventory further. In combination with 7Readers familiar with MRS will notice that the KEY feature used for semantic selection violates these accessibility conditions, but in the current framework, KEY can be replaced by KEYPRED which points to the predicate alone. the principles defined in Copestake et al (1999) for qeq conditions, the algebra presented here results in a much more tightly specified approach to semantic composition than that in Pollard and Sag (1994). 7 Comparison Compared with A-calculus, the approach to composition adopted in constraint-based grammars and formalized here has considerable advantages in terms of simplicity. The standard Montague grammar approach requires that arguments be presented in a fixed order, and that they be strictly typed, which leads to unnecessary multiplication of predicates which then have to be interrelated by meaning postulates (e.g., the two uses of expect mentioned earlier). Type raising also adds to the complexity. As standardly presented, Acalculus does not constrain grammars to be monotonic, and does n</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, Carl and Ivan Sag [1994] HeadDriven Phrase Structure Grammar, University of Chicago Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Reyle</author>
</authors>
<title>Uwe [1993] Dealing with Ambiguities by Underspecification: Construction, Representation and Deduction,</title>
<journal>Journal of Semantics,</journal>
<volume>10</volume>
<pages>123--179</pages>
<marker>Reyle, </marker>
<rawString>Reyle, Uwe [1993] Dealing with Ambiguities by Underspecification: Construction, Representation and Deduction, Journal of Semantics, 10.1: 123–179.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ivan Sag</author>
</authors>
<title>and Tom Wasow [1999] Syntactic Theory: An Introduction,</title>
<publisher>CSLI Publications.</publisher>
<marker>Sag, </marker>
<rawString>Sag, Ivan, and Tom Wasow [1999] Syntactic Theory: An Introduction, CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>An Introduction to Unification-based Approaches to Grammar,</title>
<date>1986</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="1909" citStr="Shieber (1986)" startWordPosition="279" endWordPosition="280">Note: 1. Variable equivalence is represented by coindexation within a TFS. 2. The coindexation in Kim sleeps is achieved as an effect of instantiating the SUBJ slot in the sign for sleeps. 3. Structures representing individual predicate applications (henceforth, elementary predications, or EPs) are accumulated by an append operation. Conjunction of EPs is implicit. 1The variables are free, we will discuss scopal relationships and quantifiers below. 4. All signs have an index functioning somewhat like a A-variable. A similar approach has been used in a large number of implemented grammars (see Shieber (1986) for a fairly early example). It is in many ways easier to work with than A-calculus based approaches (which we discuss further below) and has the great advantage of allowing generalizations about the syntax-semantics interface to be easily expressed. But there are problems. The operations are only specified in terms of the TFS logic: the interpretation relies on an intuitive correspondence with a conventional logical representation, but this is not spelled out. Furthermore the operations on the semantics are not tightly specified or constrained. For instance, although HPSG has the Semantics P</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, Stuart [1986] An Introduction to Unification-based Approaches to Grammar, CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henk Zeevat</author>
</authors>
<title>A Compositional Approach to Discourse Representation Theory,</title>
<date>1989</date>
<journal>Linguistics and Philosophy,</journal>
<volume>12</volume>
<pages>95--131</pages>
<contexts>
<context position="12122" citStr="Zeevat, 1989" startWordPosition="2154" endWordPosition="2155">nsider the semantics of the algebra. This must define the semantics of the operation op in terms of a function f which is defined entirely in terms of the denotations of op’s arguments. In other words, [op(a1, a2)] = f([a1], [a2]) for some function f. Intuitively, where the SMRS of the SEMENT a1 denotes G1 and the SMRS of the SEMENT a2 denotes G2, we want the semantic value of the SMRS of op(a1, a2) to denote the following: G1 ∩ G2 ∩ [hook(a1) = hole(a2)] But this cannot be constructed purely as a function of G1 and G2. The solution is to add hooks and holes to the denotations of SEMENTS (cf. Zeevat, 1989). We define the denotation of a SEMENT to be an element of I × I × P(G), where I = E ∪ A, as follows: 1. [i] = hook(a) 2. [i0] = hole(a) 3. G = {g : M |=g smrs(a)} [⊥]M = h∅, ∅, ∅i So, the meanings of SEMENTs are ordered threetuples, consisting of the hook and hole elements (from I) and a set of variable assignment functions that satisfy the SMRS. We can now define the following operation f over these denotations to create an algebra: Definition 5 Semantics of the Semantic Construction Algebra hI × I × P(G), fi is an algebra, where: f(h∅, ∅, ∅i, h[i2], [i02], G2i) = h∅, ∅, ∅i f(h[i1], [i01], G</context>
</contexts>
<marker>Zeevat, 1989</marker>
<rawString>Zeevat, Henk [1989] A Compositional Approach to Discourse Representation Theory, Linguistics and Philosophy, 12.1: 95–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henk Zeevat</author>
<author>Ewan Klein</author>
<author>Jo Calder</author>
</authors>
<title>An introduction to unification categorial grammar, Nick Haddock, Ewan Klein and Glyn Morrill (eds), Categorial grammar, unification grammar, and parsing: working papers in cognitive science,</title>
<date>1987</date>
<volume>1</volume>
<pages>195--222</pages>
<institution>Centre for Cognitive Science, University of Edinburgh.</institution>
<contexts>
<context position="25872" citStr="Zeevat et al, 1987" startWordPosition="4523" endWordPosition="4526">cates nonmonotonicity. Moore (1989) is also concerned with formalizing existing practice in unification grammars (see also Alshawi, 1992), though he assumes Prolog-style unification, rather than TFSs. Moore attempts to formalize his approach in the logic of unification, but it is not clear this is entirely successful. He has to divorce the interpretation of the expressions from the notion of truth with respect to the model, which is much like treating the semantics as a description of a logic formula. Our strategy for formalization is closest to that adopted in Unification Categorial Grammar (Zeevat et al, 1987), but rather than composing actual logical forms we compose partial descriptions to handle semantic underspecification. 8 Conclusions and future work We have developed a framework for formally specifying semantics within constraint-based representations which allows semantic operations in a grammar to be tightly specified and which allows a representation of semantic content which is largely independent of the feature structure architecture of the syntactic representation. HPSGs can be written which encode much of the algebra described here as constraints on types in the grammar, thus ensuring</context>
</contexts>
<marker>Zeevat, Klein, Calder, 1987</marker>
<rawString>Zeevat, Henk, Ewan Klein and Jo Calder [1987] An introduction to unification categorial grammar, Nick Haddock, Ewan Klein and Glyn Morrill (eds), Categorial grammar, unification grammar, and parsing: working papers in cognitive science, Volume 1, 195–222, Centre for Cognitive Science, University of Edinburgh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>