<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999285333333333">
Is this a wampimuk?
Cross-modal mapping between distributional semantics
and the visual world
</title>
<author confidence="0.98951">
Angeliki Lazaridou and Elia Bruni and Marco Baroni
</author>
<affiliation confidence="0.9953575">
Center for Mind/Brain Sciences
University of Trento
</affiliation>
<email confidence="0.995454">
{angeliki.lazaridou|elia.bruni|marco.baroni}@unitn.it
</email>
<sectionHeader confidence="0.997332" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991164">
Following up on recent work on estab-
lishing a mapping between vector-based
semantic embeddings of words and the
visual representations of the correspond-
ing objects from natural images, we first
present a simple approach to cross-modal
vector-based semantics for the task of
zero-shot learning, in which an image
of a previously unseen object is mapped
to a linguistic representation denoting its
word. We then introduce fast mapping, a
challenging and more cognitively plausi-
ble variant of the zero-shot task, in which
the learner is exposed to new objects and
the corresponding words in very limited
linguistic contexts. By combining prior
linguistic and visual knowledge acquired
about words and their objects, as well as
exploiting the limited new evidence avail-
able, the learner must learn to associate
new objects with words. Our results on
this task pave the way to realistic simula-
tions of how children or robots could use
existing knowledge to bootstrap grounded
semantic knowledge about new concepts.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999793">
Computational models of meaning that rely on
corpus-extracted context vectors, such as LSA
(Landauer and Dumais, 1997), HAL (Lund and
Burgess, 1996), Topic Models (Griffiths et al.,
2007) and more recent neural-network approaches
(Collobert and Weston, 2008; Mikolov et al.,
2013b) have successfully tackled a number of lex-
ical semantics tasks, where context vector sim-
ilarity highly correlates with various indices of
semantic relatedness (Turney and Pantel, 2010).
Given that these models are learned from natu-
rally occurring data using simple associative tech-
niques, various authors have advanced the claim
that they might be also capturing some crucial as-
pects of how humans acquire and use language
(Landauer and Dumais, 1997; Lenci, 2008).
However, the models induce the meaning of
words entirely from their co-occurrence with other
words, without links to the external world. This
constitutes a serious blow to claims of cogni-
tive plausibility in at least two respects. One
is the grounding problem (Harnad, 1990; Searle,
1984). Irrespective of their relatively high per-
formance on various semantic tasks, it is debat-
able whether models that have no access to visual
and perceptual information can capture the holis-
tic, grounded knowledge that humans have about
concepts. However, a possibly even more serious
pitfall of vector models is lack of reference: natu-
ral language is, fundamentally, a means to commu-
nicate, and thus our words must be able to refer to
objects, properties and events in the outside world
(Abbott, 2010). Current vector models are purely
language-internal, solipsistic models of meaning.
Consider the very simple scenario in which visual
information is being provided to an agent about
the current state of the world, and the agent’s task
is to determine the truth of a statement similar to
There is a dog in the room. Although the agent
is equipped with a powerful context vector model,
this will not suffice to successfully complete the
task. The model might suggest that the concepts
of dog and cat are semantically related, but it has
no means to determine the visual appearance of
dogs, and consequently no way to verify the truth
of such a simple statement.
Mapping words to the objects they denote is
such a core function of language that humans are
highly optimized for it, as shown by the so-called
fast mapping phenomenon, whereby children can
learn to associate a word to an object or prop-
erty by a single exposure to it (Bloom, 2000;
Carey, 1978; Carey and Bartlett, 1978; Heibeck
and Markman, 1987). But lack of reference is not
</bodyText>
<page confidence="0.850095">
1403
</page>
<note confidence="0.829853">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1403–1414,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99990288607595">
only a theoretical weakness: Without the ability to
refer to the outside world, context vectors are ar-
guably useless for practical goals such as learning
to execute natural language instructions (Brana-
van et al., 2009; Chen and Mooney, 2011), that
could greatly benefit from the rich network of lex-
ical meaning such vectors encode, in order to scale
up to real-life challenges.
Very recently, a number of papers have ex-
ploited advances in automated feature extraction
form images and videos to enrich context vectors
with visual information (Bruni et al., 2014; Feng
and Lapata, 2010; Leong and Mihalcea, 2011;
Regneri et al., 2013; Silberer et al., 2013). This
line of research tackles the grounding problem:
Word representations are no longer limited to their
linguistic contexts but also encode visual informa-
tion present in images associated with the corre-
sponding objects. In this paper, we rely on the
same image analysis techniques but instead focus
on the reference problem: We do not aim at en-
riching word representations with visual informa-
tion, although this might be a side effect of our
approach, but we address the issue of automati-
cally mapping objects, as depicted in images, to
the context vectors representing the correspond-
ing words. This is achieved by means of a simple
neural network trained to project image-extracted
feature vectors to text-based vectors through a hid-
den layer that can be interpreted as a cross-modal
semantic space.
We first test the effectiveness of our cross-
modal semantic space on the so-called zero-shot
learning task (Palatucci et al., 2009), which has re-
cently been explored in the machine learning com-
munity (Frome et al., 2013; Socher et al., 2013). In
this setting, we assume that our system possesses
linguistic and visual information for a set of con-
cepts in the form of text-based representations of
words and image-based vectors of the correspond-
ing objects, used for vision-to-language-mapping
training. The system is then provided with visual
information for a previously unseen object, and the
task is to associate it with a word by cross-modal
mapping. Our approach is competitive with re-
spect to the recently proposed alternatives, while
being overall simpler.
The aforementioned task is very demanding and
interesting from an engineering point of view.
However, from a cognitive angle, it relies on
strong, unrealistic assumptions: The learner is
asked to establish a link between a new object and
a word for which they possess a full-fledged text-
based vector extracted from a billion-word cor-
pus. On the contrary, the first time a learner is
exposed to a new object, the linguistic informa-
tion available is likely also very limited. Thus, in
order to consider vision-to-language mapping un-
der more plausible conditions, similar to the ones
that children or robots in a new environment are
faced with, we next simulate a scenario akin to fast
mapping. We show that the induced cross-modal
semantic space is powerful enough that sensible
guesses about the correct word denoting an object
can be made, even when the linguistic context vec-
tor representing the word has been created from as
little as 1 sentence containing it.
The contributions of this work are three-fold.
First, we conduct experiments with simple image-
and text-based vector representations and compare
alternative methods to perform cross-modal map-
ping. Then, we complement recent work (Frome
et al., 2013) and show that zero-shot learning
scales to a large and noisy dataset. Finally, we pro-
vide preliminary evidence that cross-modal pro-
jections can be used effectively to simulate a fast
mapping scenario, thus strengthening the claims
of this approach as a full-fledged, fully inductive
theory of meaning acquisition.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999827142857143">
The problem of establishing word reference has
been extensively explored in computational sim-
ulations of cross-situational learning (see Fazly et
al. (2010) for a recent proposal and extended re-
view of previous work). This line of research has
traditionally assumed artificial models of the ex-
ternal world, typically a set of linguistic or logi-
cal labels for objects, actions and possibly other
aspects of a scene (Siskind, 1996). Recently,
Yu and Siskind (2013) presented a system that
induces word-object mappings from features ex-
tracted from short videos paired with sentences.
Our work complements theirs in two ways. First,
unlike Yu and Siskind (2013) who considered a
limited lexicon of 15 items with only 4 nouns, we
conduct experiments in a large search space con-
taining a highly ambiguous set of potential target
words for every object (see Section 4.1). Most im-
portantly, by projecting visual representations of
objects into a shared semantic space, we do not
limit ourselves to establishing a link between ob-
</bodyText>
<page confidence="0.994118">
1404
</page>
<bodyText confidence="0.995143387755102">
jects and words. We induce a rich semantic rep-
resentation of the multimodal concept, that can
lead, among other things, to the discovery of im-
portant properties of an object even when we lack
its linguistic label. Nevertheless, Yu and Siskind’s
system could in principle be used to initialize the
vision-language mapping that we rely upon.
Closer to the spirit of our work are two very
recent studies coming from the machine learning
community. Socher et al. (2013) and Frome et al.
(2013) focus on zero-shot learning in the vision-
language domain by exploiting a shared visual-
linguistic semantic space. Socher et al. (2013)
learn to project unsupervised vector-based image
representations onto a word-based semantic space
using a neural network architecture. Unlike us,
Socher and colleagues train an outlier detector
to decide whether a test image should receive a
known-word label by means of a standard super-
vised object classifier, or be assigned an unseen
label by vision-to-language mapping. In our zero-
shot experiments, we assume no access to an out-
lier detector, and thus, the search for the correct
label is performed in the full concept space. Fur-
thermore, Socher and colleagues present a much
more constrained evaluation setup, where only 10
concepts are considered, compared to our experi-
ments with hundreds or thousands of concepts.
Frome et al. (2013) use linear regression to
transform vector-based image representations onto
vectors representing the same concepts in linguis-
tic semantic space. Unlike Socher et al. (2013) and
the current study that adopt simple unsupervised
techniques for constructing image representations,
Frome et al. (2013) rely on a supervised state-of-
the-art method: They feed low-level features to a
deep neural network trained on a supervised object
recognition task (Krizhevsky et al., 2012). Fur-
thermore, their text-based vectors encode very rich
~
information, such as king − ~man + ~woman =
~queen (Mikolov et al., 2013c). A natural ques-
tion we aim to answer is whether the success of
cross-modal mapping is due to the high-quality
embeddings or to the general algorithmic design.
If the latter is the case, then these results could be
extended to traditional distributional vectors bear-
ing other desirable properties, such as high inter-
pretability of dimensions.
</bodyText>
<figureCaption confidence="0.9342585">
Figure 1: A potential wampimuk (a) together with
its projection onto the linguistic space (b).
</figureCaption>
<sectionHeader confidence="0.820933" genericHeader="method">
3 Zero-shot learning and fast mapping
</sectionHeader>
<bodyText confidence="0.999854921052631">
“We found a cute, hairy wampimuk sleeping be-
hind the tree.” Even though the previous state-
ment is certainly the first time one hears about
wampimuks, the linguistic context already creates
some visual expectations: Wampimuks probably
resemble small animals (Figure 1a). This is the
scenario of zero-shot learning. Moreover, if this is
also the first linguistic encounter of that concept,
then we refer to the task as fast mapping.
Concretely, we assume that concepts, denoted
for convenience by word labels, are represented in
linguistic terms by vectors in a text-based distri-
butional semantic space (see Section 4.3). Objects
corresponding to concepts are represented in vi-
sual terms by vectors in an image-based semantic
space (Section 4.2). For a subset of concepts (e.g.,
a set of animals, a set of vehicles), we possess in-
formation related to both their linguistic and visual
representations. During training, this cross-modal
vocabulary is used to induce a projection func-
tion (Section 4.4), which – intuitively – represents
a mapping between visual and linguistic dimen-
sions. Thus, this function, given a visual vector,
returns its corresponding linguistic representation.
At test time, the system is presented with a previ-
ously unseen object (e.g., wampimuk). This object
is projected onto the linguistic space and associ-
ated with the word label of the nearest neighbor in
that space (degus in Figure 1b).
The fast mapping setting can be seen as a spe-
cial case of the zero-shot task. Whereas for the lat-
ter our system assumes that all concepts have rich
linguistic representations (i.e., representations es-
timated from a large corpus), in the case of the for-
mer, new concepts are assumed to be encounted in
a limited linguistic context and therefore lacking
rich linguistic representations. This is operational-
ized by constructing the text-based vector for these
</bodyText>
<figure confidence="0.7335845">
(a) (b)
1405
</figure>
<figureCaption confidence="0.997576">
Figure 2: Images of chair as extracted from
CIFAR-100 (left) and ESP (right).
</figureCaption>
<bodyText confidence="0.9917745">
concepts from a context of just a few occurrences.
In this way, we simulate the first encounter of a
learner with a concept that is new in both visual
and linguistic terms.
</bodyText>
<sectionHeader confidence="0.998147" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.969183">
4.1 Visual Datasets
</subsectionHeader>
<bodyText confidence="0.997097366666667">
CIFAR-100 The CIFAR-100 dataset
(Krizhevsky, 2009) consists of 60,000 32x32
colour images (note the extremely small size)
representing 100 distinct concepts, with 600
images per concept. The dataset covers a wide
range of concrete domains and is organized into
20 broader categories. Table 1 lists the concepts
used in our experiments organized by category.
ESP Our second dataset consists of 100K im-
ages from the ESP-Game data set, labeled through
a “game with a purpose” (Von Ahn, 2006).1 The
ESP image tags form a vocabulary of 20,515
unique words. Unlike other datasets used for zero-
shot learning, it covers adjectives and verbs in ad-
dition to nouns. On average, an image has 14
tags and a word appears as a tag for 70 images.
Unlike the CIFAR-100 images, which were cho-
sen specifically for image object recognition tasks
(i.e., each image is clearly depicting a single ob-
ject in the foreground), ESP contains a random se-
lection of images from the Web. Consequently,
objects do not appear in most images in their pro-
totypical display, but rather as elements of com-
plex scenes (see Figure 2). Thus, ESP constitutes
a more realistic, and at the same time more chal-
lenging, simulation of how things are encountered
in real life, testing the potentials of cross-modal
mapping in dealing with the complex scenes that
one would encounter in event recognition and cap-
tion generation tasks.
</bodyText>
<footnote confidence="0.9916585">
1http://www.cs.cmu.edu/˜biglou/
resources/
</footnote>
<subsectionHeader confidence="0.996002">
4.2 Visual Semantic Spaces
</subsectionHeader>
<bodyText confidence="0.999980659574469">
Image-based vectors are extracted using the unsu-
pervised bag-of-visual-words (BoVW) represen-
tational architecture (Sivic and Zisserman, 2003;
Csurka et al., 2004), that has been widely and suc-
cessfully applied to computer vision tasks such as
object recognition and image retrieval (Yang et al.,
2007). First, low-level visual features (Szeliski,
2010) are extracted from a large collection of im-
ages and clustered into a set of “visual words”.
The low-level features of a specific image are then
mapped to the corresponding visual words, and the
image is represented by a count vector recording
the number of occurrences of each visual word in
it. We do not attempt any parameter tuning of the
pipeline.
As low-level features, we use Scale Invariant
Feature Transform (SIFT) features (Lowe, 2004).
SIFT features are tailored to capture object parts
and to be invariant to several image transfor-
mations such as rotation, illumination and scale
change. These features are clustered into vocab-
ularies of 5,000 (ESP) and 4,096 (CIFAR-100) vi-
sual words.2 To preserve spatial information in the
BoVW representation, we use the spatial pyramid
technique (Lazebnik et al., 2006), which consists
in dividing the image into several regions, comput-
ing BoVW vectors for each region and concatenat-
ing them. In particular, we divide ESP images into
16 regions and the smaller CIFAR-100 images into
4. The vectors resulting from region concatenation
have dimensionality 5000 × 16 = 80, 000 (ESP)
and 4,096 × 4 = 16,384 (CIFAR-100), respec-
tively. We apply Local Mutual Information (LMI,
(Evert, 2005)) as weighting scheme and reduce the
full co-occurrence space to 300 dimensions using
the Singular Value Decomposition.
For CIFAR-100, we extract distinct visual vec-
tors for single images. For ESP, given the size
and amount of noise in this dataset, we build vec-
tors for visual concepts, by normalizing and sum-
ming the BoVW vectors of all the images that have
the relevant concept as a tag. Note that relevant
literature (Pereira et al., 2010) has emphasized
the importance of learners self-generating multi-
ple views when faced with new objects. Thus, our
multiple-image assumption should not be consid-
ered as problematic in the current setup.
</bodyText>
<footnote confidence="0.990689333333333">
2For selecting the size of the vocabulary size, we relied on
standard settings found in the relevant literature (Bruni et al.,
2014; Chatfield et al., 2011).
</footnote>
<page confidence="0.981368">
1406
</page>
<subsectionHeader confidence="0.947323">
Category Seen Concepts Unseen (Test) Concepts
</subsectionHeader>
<bodyText confidence="0.91978575">
aquatic mammals beaver, otter, seal, whale dolphin
fish ray, trout shark
flowers orchid, poppy, sunflower, tulip rose
food containers bottle, bowl, can ,plate cup
fruit vegetable apple, mushroom, pear orange
household electrical devices keyboard, lamp, telephone, television clock
household furniture chair, couch, table, wardrobe bed
insects bee, beetle, caterpillar, cockroach butterfly
large carnivores bear, leopard, lion, wolf tiger
large man-made outdoor things bridge, castle, house, road skyscraper
large natural outdoor scenes cloud, mountain, plain, sea forest
large omnivores and herbivores camel, cattle, chimpanzee, kangaroo elephant
</bodyText>
<tableCaption confidence="0.94565675">
medium-sized mammals fox, porcupine, possum, skunk raccoon
non-insect invertebrates crab, snail, spider, worm lobster
people baby, girl, man, woman boy
reptiles crocodile, dinosaur, snake, turtle lizard
small mammals hamster, mouse, rabbit, shrew squirrel
vehicles 1 bicycle, motorcycle, train bus
vehicles 2 rocket, tank, tractor streetcar
Table 1: Concepts in our version of the CIFAR-100 data set
</tableCaption>
<bodyText confidence="0.999677666666667">
We implement the entire visual pipeline with
VSEM, an open library for visual seman-
tics (Bruni et al., 2013).3
</bodyText>
<subsectionHeader confidence="0.999737">
4.3 Linguistic Semantic Spaces
</subsectionHeader>
<bodyText confidence="0.99981375">
For constructing the text-based vectors, we fol-
low a standard pipeline in distributional semantics
(Turney and Pantel, 2010) without tuning its pa-
rameters and collect co-occurrence statistics from
the concatenation of ukWaC4 and the Wikipedia,
amounting to 2.7 billion tokens in total. Seman-
tic vectors are constructed for a set of 30K target
words (lemmas), namely the top 20K most fre-
quent nouns, 5K most frequent adjectives and 5K
most frequent verbs, and the same 30K lemmas are
also employed as contextual elements. We collect
co-occurrences in a symmetric context window of
20 elements around a target word. Finally, simi-
larly to the visual semantic space, raw counts are
transformed by applying LMI and then reduced to
300 dimensions with SVD.5
</bodyText>
<subsectionHeader confidence="0.975871">
4.4 Cross-modal Mapping
</subsectionHeader>
<bodyText confidence="0.998824333333333">
The process of learning to map objects to the their
word label is implemented by training a projec-
tion function fprojv→w from the visual onto the lin-
guistic semantic space. For the learning, we use
a set of Ns seen concepts for which we have both
image-based visual representations Vs E RNs×dv
</bodyText>
<footnote confidence="0.9479412">
3http://clic.cimec.unitn.it/vsem/
4http://wacky.sslmit.unibo.it
5We also experimented with the image- and text-based
vectors of Socher et al. (2013), but achieved better perfor-
mance with the reported setup.
</footnote>
<bodyText confidence="0.999918619047619">
and text-based linguistic representations Ws E
RNs×dw. The projection function is subject to
an objective that aims at minimizing some cost
function between the induced text-based represen-
tations ˆWs E RNs×dw and the gold ones Ws.
The induced fprojv→w is then applied to the image-
based representations Vu E RNu×dv of Nu un-
seen objects to transform them into text-based rep-
resentations ˆWu E RNu×dw. We implement 4
alternative learning algorithms for inducing the
cross-modal projection function fprojv→w.
Linear Regression (lin) Our first model is a very
simple linear mapping between the two modali-
ties estimated by solving a least-squares problem.
This method is similar to the one introduced by
Mikolov et al. (2013a) for estimating a translation
matrix, only solved analytically. In our setup, we
can see the two different modalities as if they were
different languages. By using least-squares regres-
sion, the projection function fprojv→w can be de-
rived as
</bodyText>
<equation confidence="0.565021">
fprojv→w = (VTs Vs)−1VTs Ws (1)
</equation>
<subsectionHeader confidence="0.734488">
Canonical Correlation Analysis (CCA)
</subsectionHeader>
<bodyText confidence="0.999403333333333">
CCA (Hardoon et al., 2004; Hotelling, 1936)
and variations thereof have been successfully used
in the past for annotation of regions (Socher and
Fei-Fei, 2010) and complete images (Hardoon et
al., 2006; Hodosh et al., 2013). Given two paired
observation matrices, in our case Vs and Ws,
CCA aims at capturing the linear relationship
that exists between these variables. This is
achieved by finding a pair of matrices, in our
</bodyText>
<page confidence="0.97194">
1407
</page>
<bodyText confidence="0.980929454545455">
case CV E Rdv×d and CW E Rd,,,×d, such that
the correlation between the projections of the
two multidimensional variables into a common,
lower-rank space is maximized. The resulting
multimodal space has been shown to provide a
good approximation to human concept similarity
judgments (Silberer and Lapata, 2012). In our
setup, after applying CCA on the two spaces Vs
and Ws, we obtain the two projection mappings
onto the common space and thus our projection
function can be derived as:
</bodyText>
<equation confidence="0.962226">
fprojv→w = CV CW −1 (2)
</equation>
<bodyText confidence="0.977321529411765">
Singular Value Decomposition (SVD) SVD is
the most widely used dimensionality reduction
technique in distributional semantics (Turney and
Pantel, 2010), and it has recently been exploited
to combine visual and linguistic dimensions in
the multimodal distributional semantic model of
Bruni et al. (2014). SVD smoothing is also a way
to infer values of unseen dimensions in partially
incomplete matrices, a technique that has been ap-
plied to the task of inferring word tags of unanno-
tated images (Hare et al., 2008). Assuming that the
concept-representing rows of Vs and Ws are or-
dered in the same way, we apply the (k-truncated)
SVD to the concatenated matrix [VsWs], such
ˆW$] = UkΣkZTk is a k-rank approxima-
tion of the original matrix.6 The projection func-
tion is then:
</bodyText>
<equation confidence="0.9896995">
fprojv→w = ZkZT (3)
k
</equation>
<bodyText confidence="0.998507">
where the input is appropriately padded with 0s
([Vu0Nu×W]) and we discard the visual block of
the output matrix [ ˆVu ˆWu].
Neural Network (NNet) The last model that we
introduce is a neural network with one hidden
layer. The projection function in this model can
be described as:
</bodyText>
<equation confidence="0.932228">
fprojv→w = Θv→w (4)
</equation>
<bodyText confidence="0.990194307692308">
where Θv→w consists of the model weights 0(1) E
Rdv×h and 0(2) E Rh×d,,, that map the in-
put image-based vectors Vs first to the hid-
den layer and then to the output layer in or-
der to obtain text-based vectors, i.e., ˆWs =
Q(2)(Q(1)(V80(1))0(2)), where Q(1) and Q(2) are
6We denote the right singular vectors matrix by Z instead
of the customary V to avoid confusion with the visual matrix.
the non-linear activation functions. We experi-
mented with sigmoid, hyperbolic tangent and lin-
ear; hyperbolic tangent yielded the highest perfor-
mance. The weights are estimated by minimizing
the objective function
</bodyText>
<equation confidence="0.997344">
1
J(Θv→w) = 2(1 − sim(Ws,ˆWs)) (5)
</equation>
<bodyText confidence="0.9998895">
where sim is some similarity function. In our ex-
periments we used cosine as similarity function,
</bodyText>
<sectionHeader confidence="0.67986" genericHeader="method">
AB
</sectionHeader>
<bodyText confidence="0.995803888888889">
so that sim(A, B) = kAkkBk, thus penalizing pa-
rameter settings leading to a low cosine between
the target linguistic representations Ws and those
produced by the projection function ˆWs. The co-
sine has been widely used in the distributional se-
mantic literature, and it has been shown to out-
perform Euclidean distance (Bullinaria and Levy,
2007).7 Parameters were estimated with standard
backpropagation and L-BFGS.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999980166666667">
Our experiments focus on the tasks of zero-shot
learning (Sections 5.1 and 5.2) and fast mapping
(Section 5.3). In both tasks, the projected vector of
the unseen concept is labeled with the word asso-
ciated to its cosine-based nearest neighbor vector
in the corresponding semantic space.
For the zero-shot task we report the accuracy
of retrieving the correct label among the top k
neighbors from a semantic space populated with
the union of seen and unseen concepts. For fast
mapping, we report the mean rank of the correct
concept among fast mapping candidates.
</bodyText>
<subsectionHeader confidence="0.998916">
5.1 Zero-shot Learning in CIFAR-100
</subsectionHeader>
<bodyText confidence="0.960033823529412">
For this experiment, we use the intersection of
our linguistic space with the concepts present in
CIFAR-100, containing a total of 90 concepts. For
each concept category, we treat all concepts but
one as seen concepts (Table 1). The 71 seen con-
cepts correspond to 42,600 distinct visual vectors
and are used to induce the projection function. Ta-
ble 2 reports results obtained by averaging the per-
formance on the 11,400 distinct vectors of the 19
unseen concepts.
Our 4 models introduced in Section 4.4 are
compared to a theoretically derived baseline
Chance simulating selecting a label at random. For
the neural network NN, we use prior knowledge
7We also experimented with the same objective func-
tion as Socher et al. (2013), however, our objective function
yielded consistently better results in all experimental settings.
</bodyText>
<figure confidence="0.5655125">
ˆVs
that [
</figure>
<page confidence="0.878371">
1408
</page>
<table confidence="0.998000625">
k
M l 1 2 3 5 10 20
Mol_
Chance 1.1 2.2 3.3 5.5 11.0 22.0
SVD 1.9 5.0 8.1 14.5 29.0 48.6
CCA 3.0 6.9 10.7 17.9 31.7 51.7
lin 2.4 6.4 10.5 18.7 33.0 55.0
NN 3.9 6.6 10.6 21.9 37.9 58.2
</table>
<tableCaption confidence="0.981439">
Table 2: Percentage accuracy among top k nearest
neighbors on CIFAR-100.
</tableCaption>
<bodyText confidence="0.9996702">
about the number of concept categories to set the
number of hidden units to 20 in order to avoid
tuning of this parameter. For the SVD model, we
set the number of dimensions to 300, a common
choice in distributional semantics, coherent with
the settings we used for the visual and linguistic
spaces.
First and foremost, all 4 models outperform
Chance by a large margin. Surprisingly, the very
simple lin method outperforms both CCA and SVD.
However, NN, an architecture that can capture
more complex, non-linear relations in features
across modalities, emerges as the best performing
model, confirming on a larger scale the recent find-
ings of Socher et al. (2013).
</bodyText>
<subsubsectionHeader confidence="0.942698">
5.1.1 Concept Categorization
</subsubsectionHeader>
<bodyText confidence="0.9940284">
In order to gain qualitative insights into the perfor-
mance of the projection process of NN, we attempt
to investigate the role and interpretability of the
hidden layer. We achieve this by looking at which
visual concepts result in the highest hidden unit
activation.8 This is inspired by analogous quali-
tative analysis conducted in Topic Models (Grif-
fiths et al., 2007), where “topics” are interpreted
in terms of the words with the highest probability
under each of them.
Table 3 presents both seen and unseen con-
cepts corresponding to visual vectors that trigger
the highest activation for a subset of hidden units.
The table further reports, for each hidden unit, the
“correct” unseen concept for the category of the
top seen concepts, together with its rank in terms
of activation of the unit. The analysis demon-
strates that, although prior knowledge about cat-
egories was not explicitly used to train the net-
work, the latter induced an organization of con-
cepts into superordinate categories in which the
8For this post-hoc analysis, we include a sparsity param-
eter in the objective function of Equation 5 in order to get
more interpretable results; hidden units are therefore maxi-
mally activated by a only few concepts.
</bodyText>
<subsectionHeader confidence="0.938682">
Unseen Concept Nearest Neighbors
</subsectionHeader>
<bodyText confidence="0.96106585">
tiger cat, microchip, kitten, vet, pet
bike spoke, wheel, brake, tyre, motorcycle
blossom bud, leaf, jasmine, petal, dandelion
bakery quiche, bread, pie, bagel, curry
Table 4: Top 5 neighbors in linguistic space after
visual vector projection of 4 unseen concepts.
hidden layer acts as a cross-modal concept cate-
gorization/organization system. When the induced
projection function maps an object onto the lin-
guistic space, the derived text vector will inherit
a mixture of textual features from the concepts
that activated the same hidden unit as the object.
This suggests a bias towards seen concepts. Fur-
thermore, in many cases of miscategorization, the
concepts are still semantically coherent with the
induced category, confirming that the projection
function is indeed capturing a latent, cross-modal
semantic space. A squirrel, although not a “large
omnivore”, is still an animal, while butterflies are
not flowers but often feed on their nectar.
</bodyText>
<subsectionHeader confidence="0.999832">
5.2 Zero-shot Learning in ESP
</subsectionHeader>
<bodyText confidence="0.99996548">
For this experiment, we focus on NN, the best per-
forming model in the previous experiment. We
use a set of approximately 9,500 concepts, the in-
tersection of the ESP-based visual semantic space
with the linguistic space. For tuning the number
of hidden units of NN, we use the MEN-concrete
dataset of Bruni et al. (2014). Finally, we ran-
domly pick 70% of the concepts to induce the pro-
jection function fprojv→w and report results on the
remaining 30%. Note that the search space for the
correct label in this experiment is approximately
95 times larger than the one used for the experi-
ment presented in Section 5.1.
Although our experimental setup differs from
the one of Frome et al. (2013), thus preventing a
direct comparison, the results reported in Table 5
are on a comparable scale to theirs. We note that
previous work on zero-shot learning has used stan-
dard object recognition benchmarks. To the best
of our knowledge, this is the first time this task has
been performed on a dataset as noisy as ESP. Over-
all, the results suggest that cross-modal mapping
could be applied in tasks where images exhibit a
more complex structure, e.g., caption generation
and event recognition.
</bodyText>
<page confidence="0.968929">
1409
</page>
<table confidence="0.557975090909091">
Seen Concepts Unseen Concept Rank of Correct CIFAR-100 Category
Unseen Concept
Unit 1 sunflower, tulip, pear butterfly 2 (rose) flowers
Unit 2 cattle, camel, bear squirrel 2 (elephant) large omnivores and herbivores
Unit 3 castle, bridge, house bus 4 (skyscraper) large man-made outdoor things
Unit 4 man, girl, baby boy 1 people
Unit 5 motorcycle, bicycle, tractor streetcar 2 (bus) vehicles 1
Unit 6 sea, plain, cloud forest 1 large natural outdoor scenes
Unit 7 chair, couch, table bed 1 household furniture
Unit 8 plate, bowl, can clock 3 (cup) food containers
Unit 9 apple, pear, mushroom orange 1 fruit and vegetables
</table>
<tableCaption confidence="0.8662295">
Table 3: Categorization induced by the hidden layer of the NN; concepts belonging in the same CIFAR-
100 categories, reported in the last column, are marked in bold. Example: Unit 1 receives the highest
activation during training by the category flowers and at test time by butterfly, belonging to insects. The
same unit receives the second highest activation by the “correct” test concept, the flower rose.
</tableCaption>
<table confidence="0.99960875">
PPPPPP k 1 2 5 10 50
Model
Chance 0.01 0.02 0.05 0.10 0.5
NN 0.8 1.9 5.6 9.7 30.9
</table>
<tableCaption confidence="0.992152">
Table 5: Percentage accuracy among top k nearest
neighbors on ESP.
</tableCaption>
<subsectionHeader confidence="0.996722">
5.3 Fast Mapping in ESP
</subsectionHeader>
<bodyText confidence="0.999978460000001">
In this section, we aim at simulating a fast map-
ping scenario in which the learner has been just
exposed to a new concept, and thus has limited lin-
guistic evidence for that concept. We operational-
ize this by considering the 34 concrete concepts
introduced by Frassinelli and Keller (2012), and
deriving their text-based representations from just
a few sentences randomly picked from the corpus.
Concretely, we implement 5 models: context 1, con-
text 5, context 10, context 20 and context full, where
the name of the model denotes the number of sen-
tences used to construct the text-based representa-
tions. The derived vectors were reduced with the
same SVD projection induced from the complete
corpus. Cross-modal mapping is done via NN.
The zero-shot framework leads us to frame fast
mapping as the task of projecting visual represen-
tations of new objects onto language space for re-
trieving their word labels (v → w). This mapping
from visual to textual representations is arguably
a more plausible task than vice versa. If we think
about how linguistic reference is acquired, a sce-
nario in which a learner first encounters a new ob-
ject and then seeks its reference in the language of
the surrounding environment (e.g., adults having a
conversation, the text of a book with an illustration
of an unknown object) is very natural. Further-
more, since not all new concepts in the linguistic
environment refer to new objects (they might de-
note abstract concepts or out-of-scene objects), it
seems more reasonable for the learner to be more
alerted to linguistic cues about a recently-spotted
new object than vice versa. Moreover, once the
learner observes a new object, she can easily con-
struct a full visual representation for it (and the
acquisition literature has shown that humans are
wired for good object segmentation and recogni-
tion (Spelke, 1994)) – the more challenging task is
to scan the ongoing and very ambiguous linguistic
communication for contexts that might be relevant
and informative about the new object. However,
fast mapping is often described in the psycholog-
ical literature as the opposite task: The learner
is exposed to a new word in context and has to
search for the right object referring to it. We im-
plement this second setup (w → v) by training the
projection function fproj,→v which maps linguis-
tic vectors to visual ones. The adaptation of NN is
straightforward; the new objective function is de-
rived as
</bodyText>
<equation confidence="0.997725">
1
J(Θw→v) = 2(1 − sim(Vs, ˆVs)) (6)
</equation>
<bodyText confidence="0.999295076923077">
where ˆVs = σ(2)(σ(1)(W,0(1))0(2)), 0(1) E
Rdw×h and 0(2) E Rh×dv.
Table 7 presents the results. Not surprisingly,
performance increases with the number of sen-
tences that are used to construct the textual repre-
sentations. Furthermore, all models perform bet-
ter than Chance, including those that are based on
just 1 or 5 sentences. This suggests that the system
can make reasonable inferences about object-word
connections even when linguistic evidence is very
scarce.
Regarding the sources of error, a qualitative
analysis of predicted word labels and objects as
</bodyText>
<page confidence="0.99027">
1410
</page>
<tableCaption confidence="0.883911333333333">
v→w w→v
Table 6: Top-ranked concepts in cases where the
gold concepts received numerically high ranks.
</tableCaption>
<figure confidence="0.927598068965517">
cooker→potato
clarinet→ drum
gorilla→ elephant
scooter→ car
dishwasher→ corkscrew
potato→ corn
guitar→ violin
scarf→ trouser
❳❳❳❳❳❳❳❳
Mapping
Context
v → ww → v
Chance 17 17
context 1 12.6 14.5
context 5 8.08 13.29
context 10 7.29 13.44
context 20 6.02 12.17
5.52
5.88
and retrieving its linguistic neighbors (v
w) as
well as when mapping atext-based vector and
retrieving its visual neighbors (w
v). Lower
numbers cue better performan
→
→
ce.
or
</figure>
<bodyText confidence="0.999044571428571">
information, are not
enough to single out the properties of the target
concept. As an example, the textual vector of dish-
washer contains kitchen-related dimensions such
as (fridge, oven, gas, hob, ..., sink. After projecting
onto the visual space, its nearest visual neighbours
are the visual ones of the same-domain concepts
corkscrew and kettle. The latter is shown in Figure
3a, with a gas hob well in evidence. As a further
example, the visual vector for cooker is extracted
from pictures such as the one in Figure 3b. Not
surprisingly, when projecting it onto the linguis-
tic space, the nearest neighbours are other kitchen-
related terms, i.e., potato an
</bodyText>
<equation confidence="0.5179955">
“topical”
“domain”
</equation>
<bodyText confidence="0.918096">
d dishwasher.
</bodyText>
<sectionHeader confidence="0.924805" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.732103">
-
</bodyText>
<listItem confidence="0.321663">
(a) A kettle (b) A cooker
om ESP.
</listItem>
<bodyText confidence="0.999938611111111">
butional semantic spaces with objects in the vi-
sual world by means of cross-modal mapping. We
compared recent models for this task both on a
benchmark object recognition dataset and on a
more realistic and noisier dataset covering a wide
range of concepts. The neural network architec-
ture emerged as the best performing approach, and
our qualitative analysis revealed that it induced a
categorical organization of concepts. Most impor-
tantly, our results suggest the viability of cross-
modal mapping for grounded word-meaning ac-
quisition in a simulation of fast mapping.
Given the success of NN, we plan to experi-
ment in the future with more sophisticated neural
network architectures inspired by recent work in
machine translation (Gao et al., 2013) and mul-
timodal deep learning (Srivastava and Salakhut-
dinov, 2012). Furthermore, we intend to adopt
visual attributes (Farhadi et al., 2009; Silberer
et al., 2013) as visual representations, since they
should allow a better understanding of how cross-
modal mapping works, thanks to their linguistic
interpretability. The error analysis in Section 5.3
suggests that automated localization techniques
(van de Sande et al., 2011), distinguishing an ob-
ject from its surroundings, might drastically im-
prove mapping accuracy. Similarly, in the textual
domain, models that extract collocates of a word
that are more likely to denote conceptual proper-
ties (Kelly et al., 2012) might lead to more infor-
mative and discriminative linguistic vectors. Fi-
nally, the lack of large child-directed speech cor-
pora constrained the experimental design of fast
ions of CHILDES) that contain sen-
tences more akin to those a child might effectively
hear or read in her word-learning years.
</bodyText>
<sectionHeader confidence="0.9984" genericHeader="evaluation">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.95212325">
mapping simulations; we plan to run more realis-
tic experiments with true nonce words and using
source corpora (e.g., the Simple Wikipedia, child
stories, port
We thank Adam
for helpful discussions and
the 3 anonymous reviewers for useful comments.
This work was supported by ERC 2011 Starting
Independent Research Gran
Liˇska
t n. 283554 (COM-
POSES).
</bodyText>
<sectionHeader confidence="0.968824" genericHeader="conclusions">
nces
</sectionHeader>
<reference confidence="0.991639">
Barbara Abbott. 2010. Reference. Oxford University
Press, Oxford, UK.
</reference>
<tableCaption confidence="0.697681">
context full
Table 7: Mean rank results averaged across 34
</tableCaption>
<bodyText confidence="0.8598635">
concepts when mapping an image-based vector
presented in Table 6 suggests that both textual
and visual representations, although capturing rel-
evant
At the outset of this work, we considered the
problem of linking purely language-based distri
</bodyText>
<figureCaption confidence="0.957483">
Figure 3: Two images fr
</figureCaption>
<sectionHeader confidence="0.864105" genericHeader="references">
Refere
</sectionHeader>
<page confidence="0.943455">
1411
</page>
<bodyText confidence="0.836818888888889">
Paul Bloom. 2000. How Children Learn the Meanings
of Words. MIT Press, Cambridge, MA.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings
of HLT-NAACL, pages 91–99, Los Angeles, CA.
S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer,
and Regina Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In Proceedings
of ACL/IJCNLP, pages 82–90.
</bodyText>
<reference confidence="0.999209648936171">
Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Ui-
jlings, and Irina Sergienya. 2013. Vsem: An open
library for visual semantics representation. In Pro-
ceedings of ACL, Sofia, Bulgaria.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510–526.
Susan Carey and Elsa Bartlett. 1978. Acquiring a sin-
gle new word. Papers and Reports on Child Lan-
guage Development, 15:17–29.
Susan Carey. 1978. The child as a word learner. In
M. Halle, J. Bresnan, and G. Miller, editors, Linguis-
tics Theory and Psychological Reality. MIT Press,
Cambridge, MA.
Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, and
Andrew Zisserman. 2011. The devil is in the de-
tails: an evaluation of recent feature encoding meth-
ods. In Proceedings of BMVC, Dundee, UK.
David Chen and Raymond Mooney. 2011. Learning
to interpret natural language navigation instructions
from observations. In Proceedings of AAAI, pages
859–865, San Francisco, CA.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160–167, Helsinki, Fin-
land.
Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and C´edric Bray. 2004. Visual cate-
gorization with bags of keypoints. In In Workshop
on Statistical Learning in Computer Vision, ECCV,
pages 1–22, Prague, Czech Republic.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D dissertation, Stuttgart University.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Proceedings of CVPR, pages 1778–
1785, Miami Beach, FL.
Afsaneh Fazly, Afra Alishahi, and Suzanne Steven-
son. 2010. A probabilistic computational model of
cross-situational word learning. Cognitive Science,
34:1017–1063.
Diego Frassinelli and Frank Keller. 2012. The plausi-
bility of semantic properties generated by a distribu-
tional model: Evidence from a visual world experi-
ment. In Proceedings of CogSci, pages 1560–1565.
Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-
gio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas
Mikolov. 2013. DeViSE: A deep visual-semantic
embedding model. In Proceedings of NIPS, pages
2121–2129, Lake Tahoe, Nevada.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning semantic representations
for the phrase translation model. arXiv preprint
arXiv:1312.0482.
Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.
2007. Topics in semantic representation. Psycho-
logical Review, 114:211–244.
David R Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2004. Canonical correlation analysis:
An overview with application to learning methods.
Neural Computation, 16(12):2639–2664.
David R Hardoon, Craig Saunders, Sandor Szedmak,
and John Shawe-Taylor. 2006. A correlation ap-
proach for automatic image annotation. In Ad-
vanced Data Mining and Applications, pages 681–
692. Springer.
Jonathon Hare, Sina Samangooei, Paul Lewis, and
Mark Nixon. 2008. Semantic spaces revisited: In-
vestigating the performance of auto-annotation and
semantic retrieval using semantic spaces. In Pro-
ceedings of CIVR, pages 359–368, Niagara Falls,
Canada.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D: Nonlinear Phenomena, 42(1-3):335–
346.
Tracy Heibeck and Ellen Markman. 1987. Word learn-
ing in children: an examination of fast mapping.
Child Development, 58:1021–1024.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Ar-
tificial Intelligence Research, 47:853–899.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321–377.
Colin Kelly, Barry Devereux, and Anna Korhonen.
2012. Semi-supervised learning for automatic con-
ceptual property extraction. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics, pages 11–20, Montreal, Canada.
</reference>
<page confidence="0.871557">
1412
</page>
<reference confidence="0.999958796116505">
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Proceedings of NIPS,
pages 1106–1114.
Alex Krizhevsky. 2009. Learning multiple layers of
features from tiny images. Master’s thesis.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211–
240.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories.
In Proceedings of CVPR, pages 2169–2178, Wash-
ington, DC.
Alessandro Lenci. 2008. Distributional approaches in
linguistic and cognitive research. Italian Journal of
Linguistics, 20(1):1–31.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403–1407.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203–
208.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013a. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119, Lake
Tahoe, Nevada.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL,
pages 746–751, Atlanta, Georgia.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Proceedings of NIPS,
pages 1410–1418, Vancouver, Canada.
Alfredo F Pereira, Karin H James, Susan S Jones,
and Linda B Smith. 2010. Early biases and de-
velopmental changes in self-generated object views.
Journal of vision, 10(11).
Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred
Pinkal. 2013. Grounding action descriptions in
videos. Transactions of the Association for Com-
putational Linguistics, 1:25–36.
John Searle. 1984. Minds, Brains and Science. Har-
vard University Press, Cambridge, MA.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of EMNLP, pages 1423–1433, Jeju, Korea.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In Proceedings of ACL, pages 572–582,
Sofia, Bulgaria.
Jeffrey Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning
mappings. Cognition, 61:39–91.
Josef Sivic and Andrew Zisserman. 2003. Video
Google: A text retrieval approach to object match-
ing in videos. In Proceedings of ICCV, pages 1470–
1477, Nice, France.
Richard Socher and Li Fei-Fei. 2010. Connecting
modalities: Semi-supervised segmentation and an-
notation of images using unaligned text corpora. In
Proceedings of CVPR, pages 966–973.
Richard Socher, Milind Ganjoo, Christopher Manning,
and Andrew Ng. 2013. Zero-shot learning through
cross-modal transfer. In Proceedings of NIPS, pages
935–943, Lake Tahoe, Nevada.
Elizabeth Spelke. 1994. Initial knowledge: Six sug-
gestions. Cognition, 50:431–445.
Nitish Srivastava and Ruslan Salakhutdinov. 2012.
Multimodal learning with deep boltzmann ma-
chines. In Proceedings of NIPS, pages 2231–2239.
Richard Szeliski. 2010. Computer Vision: Algorithms
and Applications. Springer, Berlin.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Koen van de Sande, Jasper Uijlings, Theo Gevers, and
Arnold Smeulders. 2011. Segmentation as selec-
tive search for object recognition. In Proceedings of
ICCV, pages 1879–1886, Barcelona, Spain.
Luis Von Ahn. 2006. Games with a purpose. Com-
puter, 29(6):92–94.
Jun Yang, Yu-Gang Jiang, Alexander Hauptmann, and
Chong-Wah Ngo. 2007. Evaluating bag-of-visual-
words representations in scene classification. In
James Ze Wang, Nozha Boujemaa, Alberto Del
Bimbo, and Jia Li, editors, Multimedia Information
Retrieval, pages 197–206. ACM.
</reference>
<page confidence="0.624014">
1413
</page>
<reference confidence="0.9173065">
Haonan Yu and Jeffrey Siskind. 2013. Grounded lan-
guage learning from video described with sentences.
In Proceedings of ACL, pages 53–63, Sofia, Bul-
garia.
</reference>
<page confidence="0.99461">
1414
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.364507">
<title confidence="0.499474333333333">Is this a wampimuk? Cross-modal mapping between distributional and the visual world</title>
<author confidence="0.939077">Lazaridou Bruni</author>
<affiliation confidence="0.995828">Center for Mind/Brain University of</affiliation>
<abstract confidence="0.998912307692308">Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images, we first present a simple approach to cross-modal vector-based semantics for the task of in which an image of a previously unseen object is mapped to a linguistic representation denoting its We then introduce a challenging and more cognitively plausible variant of the zero-shot task, in which the learner is exposed to new objects and the corresponding words in very limited linguistic contexts. By combining prior linguistic and visual knowledge acquired about words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Barbara Abbott</author>
</authors>
<title>Reference.</title>
<date>2010</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="2819" citStr="Abbott, 2010" startWordPosition="430" endWordPosition="431">ims of cognitive plausibility in at least two respects. One is the grounding problem (Harnad, 1990; Searle, 1984). Irrespective of their relatively high performance on various semantic tasks, it is debatable whether models that have no access to visual and perceptual information can capture the holistic, grounded knowledge that humans have about concepts. However, a possibly even more serious pitfall of vector models is lack of reference: natural language is, fundamentally, a means to communicate, and thus our words must be able to refer to objects, properties and events in the outside world (Abbott, 2010). Current vector models are purely language-internal, solipsistic models of meaning. Consider the very simple scenario in which visual information is being provided to an agent about the current state of the world, and the agent’s task is to determine the truth of a statement similar to There is a dog in the room. Although the agent is equipped with a powerful context vector model, this will not suffice to successfully complete the task. The model might suggest that the concepts of dog and cat are semantically related, but it has no means to determine the visual appearance of dogs, and consequ</context>
</contexts>
<marker>Abbott, 2010</marker>
<rawString>Barbara Abbott. 2010. Reference. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Ulisse Bordignon</author>
<author>Adam Liska</author>
<author>Jasper Uijlings</author>
<author>Irina Sergienya</author>
</authors>
<title>Vsem: An open library for visual semantics representation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="18519" citStr="Bruni et al., 2013" startWordPosition="2932" endWordPosition="2935">cloud, mountain, plain, sea forest large omnivores and herbivores camel, cattle, chimpanzee, kangaroo elephant medium-sized mammals fox, porcupine, possum, skunk raccoon non-insect invertebrates crab, snail, spider, worm lobster people baby, girl, man, woman boy reptiles crocodile, dinosaur, snake, turtle lizard small mammals hamster, mouse, rabbit, shrew squirrel vehicles 1 bicycle, motorcycle, train bus vehicles 2 rocket, tank, tractor streetcar Table 1: Concepts in our version of the CIFAR-100 data set We implement the entire visual pipeline with VSEM, an open library for visual semantics (Bruni et al., 2013).3 4.3 Linguistic Semantic Spaces For constructing the text-based vectors, we follow a standard pipeline in distributional semantics (Turney and Pantel, 2010) without tuning its parameters and collect co-occurrence statistics from the concatenation of ukWaC4 and the Wikipedia, amounting to 2.7 billion tokens in total. Semantic vectors are constructed for a set of 30K target words (lemmas), namely the top 20K most frequent nouns, 5K most frequent adjectives and 5K most frequent verbs, and the same 30K lemmas are also employed as contextual elements. We collect co-occurrences in a symmetric cont</context>
</contexts>
<marker>Bruni, Bordignon, Liska, Uijlings, Sergienya, 2013</marker>
<rawString>Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Uijlings, and Irina Sergienya. 2013. Vsem: An open library for visual semantics representation. In Proceedings of ACL, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="4621" citStr="Bruni et al., 2014" startWordPosition="724" endWordPosition="727">25 2014. c�2014 Association for Computational Linguistics only a theoretical weakness: Without the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life challenges. Very recently, a number of papers have exploited advances in automated feature extraction form images and videos to enrich context vectors with visual information (Bruni et al., 2014; Feng and Lapata, 2010; Leong and Mihalcea, 2011; Regneri et al., 2013; Silberer et al., 2013). This line of research tackles the grounding problem: Word representations are no longer limited to their linguistic contexts but also encode visual information present in images associated with the corresponding objects. In this paper, we rely on the same image analysis techniques but instead focus on the reference problem: We do not aim at enriching word representations with visual information, although this might be a side effect of our approach, but we address the issue of automatically mapping </context>
<context position="17286" citStr="Bruni et al., 2014" startWordPosition="2761" endWordPosition="2764">sual vectors for single images. For ESP, given the size and amount of noise in this dataset, we build vectors for visual concepts, by normalizing and summing the BoVW vectors of all the images that have the relevant concept as a tag. Note that relevant literature (Pereira et al., 2010) has emphasized the importance of learners self-generating multiple views when faced with new objects. Thus, our multiple-image assumption should not be considered as problematic in the current setup. 2For selecting the size of the vocabulary size, we relied on standard settings found in the relevant literature (Bruni et al., 2014; Chatfield et al., 2011). 1406 Category Seen Concepts Unseen (Test) Concepts aquatic mammals beaver, otter, seal, whale dolphin fish ray, trout shark flowers orchid, poppy, sunflower, tulip rose food containers bottle, bowl, can ,plate cup fruit vegetable apple, mushroom, pear orange household electrical devices keyboard, lamp, telephone, television clock household furniture chair, couch, table, wardrobe bed insects bee, beetle, caterpillar, cockroach butterfly large carnivores bear, leopard, lion, wolf tiger large man-made outdoor things bridge, castle, house, road skyscraper large natural o</context>
<context position="22103" citStr="Bruni et al. (2014)" startWordPosition="3499" endWordPosition="3502">pace has been shown to provide a good approximation to human concept similarity judgments (Silberer and Lapata, 2012). In our setup, after applying CCA on the two spaces Vs and Ws, we obtain the two projection mappings onto the common space and thus our projection function can be derived as: fprojv→w = CV CW −1 (2) Singular Value Decomposition (SVD) SVD is the most widely used dimensionality reduction technique in distributional semantics (Turney and Pantel, 2010), and it has recently been exploited to combine visual and linguistic dimensions in the multimodal distributional semantic model of Bruni et al. (2014). SVD smoothing is also a way to infer values of unseen dimensions in partially incomplete matrices, a technique that has been applied to the task of inferring word tags of unannotated images (Hare et al., 2008). Assuming that the concept-representing rows of Vs and Ws are ordered in the same way, we apply the (k-truncated) SVD to the concatenated matrix [VsWs], such ˆW$] = UkΣkZTk is a k-rank approximation of the original matrix.6 The projection function is then: fprojv→w = ZkZT (3) k where the input is appropriately padded with 0s ([Vu0Nu×W]) and we discard the visual block of the output mat</context>
<context position="29009" citStr="Bruni et al. (2014)" startWordPosition="4651" endWordPosition="4654">ally coherent with the induced category, confirming that the projection function is indeed capturing a latent, cross-modal semantic space. A squirrel, although not a “large omnivore”, is still an animal, while butterflies are not flowers but often feed on their nectar. 5.2 Zero-shot Learning in ESP For this experiment, we focus on NN, the best performing model in the previous experiment. We use a set of approximately 9,500 concepts, the intersection of the ESP-based visual semantic space with the linguistic space. For tuning the number of hidden units of NN, we use the MEN-concrete dataset of Bruni et al. (2014). Finally, we randomly pick 70% of the concepts to induce the projection function fprojv→w and report results on the remaining 30%. Note that the search space for the correct label in this experiment is approximately 95 times larger than the one used for the experiment presented in Section 5.1. Although our experimental setup differs from the one of Frome et al. (2013), thus preventing a direct comparison, the results reported in Table 5 are on a comparable scale to theirs. We note that previous work on zero-shot learning has used standard object recognition benchmarks. To the best of our know</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bullinaria</author>
<author>Joseph Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>39--510</pages>
<contexts>
<context position="23977" citStr="Bullinaria and Levy, 2007" startWordPosition="3822" endWordPosition="3825"> hyperbolic tangent and linear; hyperbolic tangent yielded the highest performance. The weights are estimated by minimizing the objective function 1 J(Θv→w) = 2(1 − sim(Ws,ˆWs)) (5) where sim is some similarity function. In our experiments we used cosine as similarity function, AB so that sim(A, B) = kAkkBk, thus penalizing parameter settings leading to a low cosine between the target linguistic representations Ws and those produced by the projection function ˆWs. The cosine has been widely used in the distributional semantic literature, and it has been shown to outperform Euclidean distance (Bullinaria and Levy, 2007).7 Parameters were estimated with standard backpropagation and L-BFGS. 5 Results Our experiments focus on the tasks of zero-shot learning (Sections 5.1 and 5.2) and fast mapping (Section 5.3). In both tasks, the projected vector of the unseen concept is labeled with the word associated to its cosine-based nearest neighbor vector in the corresponding semantic space. For the zero-shot task we report the accuracy of retrieving the correct label among the top k neighbors from a semantic space populated with the union of seen and unseen concepts. For fast mapping, we report the mean rank of the cor</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John Bullinaria and Joseph Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Carey</author>
<author>Elsa Bartlett</author>
</authors>
<title>Acquiring a single new word.</title>
<date>1978</date>
<booktitle>Papers and Reports on Child Language Development,</booktitle>
<pages>15--17</pages>
<contexts>
<context position="3799" citStr="Carey and Bartlett, 1978" startWordPosition="597" endWordPosition="600">werful context vector model, this will not suffice to successfully complete the task. The model might suggest that the concepts of dog and cat are semantically related, but it has no means to determine the visual appearance of dogs, and consequently no way to verify the truth of such a simple statement. Mapping words to the objects they denote is such a core function of language that humans are highly optimized for it, as shown by the so-called fast mapping phenomenon, whereby children can learn to associate a word to an object or property by a single exposure to it (Bloom, 2000; Carey, 1978; Carey and Bartlett, 1978; Heibeck and Markman, 1987). But lack of reference is not 1403 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1403–1414, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics only a theoretical weakness: Without the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning such vectors encode, in ord</context>
</contexts>
<marker>Carey, Bartlett, 1978</marker>
<rawString>Susan Carey and Elsa Bartlett. 1978. Acquiring a single new word. Papers and Reports on Child Language Development, 15:17–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Carey</author>
</authors>
<title>The child as a word learner.</title>
<date>1978</date>
<booktitle>Linguistics Theory and Psychological Reality.</booktitle>
<editor>In M. Halle, J. Bresnan, and G. Miller, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3773" citStr="Carey, 1978" startWordPosition="595" endWordPosition="596">ped with a powerful context vector model, this will not suffice to successfully complete the task. The model might suggest that the concepts of dog and cat are semantically related, but it has no means to determine the visual appearance of dogs, and consequently no way to verify the truth of such a simple statement. Mapping words to the objects they denote is such a core function of language that humans are highly optimized for it, as shown by the so-called fast mapping phenomenon, whereby children can learn to associate a word to an object or property by a single exposure to it (Bloom, 2000; Carey, 1978; Carey and Bartlett, 1978; Heibeck and Markman, 1987). But lack of reference is not 1403 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1403–1414, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics only a theoretical weakness: Without the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning s</context>
</contexts>
<marker>Carey, 1978</marker>
<rawString>Susan Carey. 1978. The child as a word learner. In M. Halle, J. Bresnan, and G. Miller, editors, Linguistics Theory and Psychological Reality. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Chatfield</author>
<author>Victor Lempitsky</author>
<author>Andrea Vedaldi</author>
<author>Andrew Zisserman</author>
</authors>
<title>The devil is in the details: an evaluation of recent feature encoding methods.</title>
<date>2011</date>
<booktitle>In Proceedings of BMVC,</booktitle>
<location>Dundee, UK.</location>
<contexts>
<context position="17311" citStr="Chatfield et al., 2011" startWordPosition="2765" endWordPosition="2768">gle images. For ESP, given the size and amount of noise in this dataset, we build vectors for visual concepts, by normalizing and summing the BoVW vectors of all the images that have the relevant concept as a tag. Note that relevant literature (Pereira et al., 2010) has emphasized the importance of learners self-generating multiple views when faced with new objects. Thus, our multiple-image assumption should not be considered as problematic in the current setup. 2For selecting the size of the vocabulary size, we relied on standard settings found in the relevant literature (Bruni et al., 2014; Chatfield et al., 2011). 1406 Category Seen Concepts Unseen (Test) Concepts aquatic mammals beaver, otter, seal, whale dolphin fish ray, trout shark flowers orchid, poppy, sunflower, tulip rose food containers bottle, bowl, can ,plate cup fruit vegetable apple, mushroom, pear orange household electrical devices keyboard, lamp, telephone, television clock household furniture chair, couch, table, wardrobe bed insects bee, beetle, caterpillar, cockroach butterfly large carnivores bear, leopard, lion, wolf tiger large man-made outdoor things bridge, castle, house, road skyscraper large natural outdoor scenes cloud, moun</context>
</contexts>
<marker>Chatfield, Lempitsky, Vedaldi, Zisserman, 2011</marker>
<rawString>Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, and Andrew Zisserman. 2011. The devil is in the details: an evaluation of recent feature encoding methods. In Proceedings of BMVC, Dundee, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chen</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to interpret natural language navigation instructions from observations.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>859--865</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="4302" citStr="Chen and Mooney, 2011" startWordPosition="672" endWordPosition="675">ssociate a word to an object or property by a single exposure to it (Bloom, 2000; Carey, 1978; Carey and Bartlett, 1978; Heibeck and Markman, 1987). But lack of reference is not 1403 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1403–1414, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics only a theoretical weakness: Without the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life challenges. Very recently, a number of papers have exploited advances in automated feature extraction form images and videos to enrich context vectors with visual information (Bruni et al., 2014; Feng and Lapata, 2010; Leong and Mihalcea, 2011; Regneri et al., 2013; Silberer et al., 2013). This line of research tackles the grounding problem: Word representations are no longer limited to their linguistic contexts but also encode visual information present in images associ</context>
</contexts>
<marker>Chen, Mooney, 2011</marker>
<rawString>David Chen and Raymond Mooney. 2011. Learning to interpret natural language navigation instructions from observations. In Proceedings of AAAI, pages 859–865, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>160--167</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="1542" citStr="Collobert and Weston, 2008" startWordPosition="224" endWordPosition="227">visual knowledge acquired about words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts. 1 Introduction Computational models of meaning that rely on corpus-extracted context vectors, such as LSA (Landauer and Dumais, 1997), HAL (Lund and Burgess, 1996), Topic Models (Griffiths et al., 2007) and more recent neural-network approaches (Collobert and Weston, 2008; Mikolov et al., 2013b) have successfully tackled a number of lexical semantics tasks, where context vector similarity highly correlates with various indices of semantic relatedness (Turney and Pantel, 2010). Given that these models are learned from naturally occurring data using simple associative techniques, various authors have advanced the claim that they might be also capturing some crucial aspects of how humans acquire and use language (Landauer and Dumais, 1997; Lenci, 2008). However, the models induce the meaning of words entirely from their co-occurrence with other words, without lin</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of ICML, pages 160–167, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriella Csurka</author>
<author>Christopher Dance</author>
<author>Lixin Fan</author>
<author>Jutta Willamowski</author>
<author>C´edric Bray</author>
</authors>
<title>Visual categorization with bags of keypoints.</title>
<date>2004</date>
<booktitle>In In Workshop on Statistical Learning in Computer Vision, ECCV,</booktitle>
<pages>1--22</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="15088" citStr="Csurka et al., 2004" startWordPosition="2404" endWordPosition="2407">t images in their prototypical display, but rather as elements of complex scenes (see Figure 2). Thus, ESP constitutes a more realistic, and at the same time more challenging, simulation of how things are encountered in real life, testing the potentials of cross-modal mapping in dealing with the complex scenes that one would encounter in event recognition and caption generation tasks. 1http://www.cs.cmu.edu/˜biglou/ resources/ 4.2 Visual Semantic Spaces Image-based vectors are extracted using the unsupervised bag-of-visual-words (BoVW) representational architecture (Sivic and Zisserman, 2003; Csurka et al., 2004), that has been widely and successfully applied to computer vision tasks such as object recognition and image retrieval (Yang et al., 2007). First, low-level visual features (Szeliski, 2010) are extracted from a large collection of images and clustered into a set of “visual words”. The low-level features of a specific image are then mapped to the corresponding visual words, and the image is represented by a count vector recording the number of occurrences of each visual word in it. We do not attempt any parameter tuning of the pipeline. As low-level features, we use Scale Invariant Feature Tra</context>
</contexts>
<marker>Csurka, Dance, Fan, Willamowski, Bray, 2004</marker>
<rawString>Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta Willamowski, and C´edric Bray. 2004. Visual categorization with bags of keypoints. In In Workshop on Statistical Learning in Computer Vision, ECCV, pages 1–22, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The Statistics of Word Cooccurrences.</title>
<date>2005</date>
<institution>Stuttgart University.</institution>
<note>Ph.D dissertation,</note>
<contexts>
<context position="16511" citStr="Evert, 2005" startWordPosition="2636" endWordPosition="2637">ustered into vocabularies of 5,000 (ESP) and 4,096 (CIFAR-100) visual words.2 To preserve spatial information in the BoVW representation, we use the spatial pyramid technique (Lazebnik et al., 2006), which consists in dividing the image into several regions, computing BoVW vectors for each region and concatenating them. In particular, we divide ESP images into 16 regions and the smaller CIFAR-100 images into 4. The vectors resulting from region concatenation have dimensionality 5000 × 16 = 80, 000 (ESP) and 4,096 × 4 = 16,384 (CIFAR-100), respectively. We apply Local Mutual Information (LMI, (Evert, 2005)) as weighting scheme and reduce the full co-occurrence space to 300 dimensions using the Singular Value Decomposition. For CIFAR-100, we extract distinct visual vectors for single images. For ESP, given the size and amount of noise in this dataset, we build vectors for visual concepts, by normalizing and summing the BoVW vectors of all the images that have the relevant concept as a tag. Note that relevant literature (Pereira et al., 2010) has emphasized the importance of learners self-generating multiple views when faced with new objects. Thus, our multiple-image assumption should not be cons</context>
</contexts>
<marker>Evert, 2005</marker>
<rawString>Stefan Evert. 2005. The Statistics of Word Cooccurrences. Ph.D dissertation, Stuttgart University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Ian Endres</author>
<author>Derek Hoiem</author>
<author>David Forsyth</author>
</authors>
<title>Describing objects by their attributes.</title>
<date>2009</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>1778--1785</pages>
<location>Miami Beach, FL.</location>
<contexts>
<context position="36291" citStr="Farhadi et al., 2009" startWordPosition="5863" endWordPosition="5866"> network architecture emerged as the best performing approach, and our qualitative analysis revealed that it induced a categorical organization of concepts. Most importantly, our results suggest the viability of crossmodal mapping for grounded word-meaning acquisition in a simulation of fast mapping. Given the success of NN, we plan to experiment in the future with more sophisticated neural network architectures inspired by recent work in machine translation (Gao et al., 2013) and multimodal deep learning (Srivastava and Salakhutdinov, 2012). Furthermore, we intend to adopt visual attributes (Farhadi et al., 2009; Silberer et al., 2013) as visual representations, since they should allow a better understanding of how crossmodal mapping works, thanks to their linguistic interpretability. The error analysis in Section 5.3 suggests that automated localization techniques (van de Sande et al., 2011), distinguishing an object from its surroundings, might drastically improve mapping accuracy. Similarly, in the textual domain, models that extract collocates of a word that are more likely to denote conceptual properties (Kelly et al., 2012) might lead to more informative and discriminative linguistic vectors. F</context>
</contexts>
<marker>Farhadi, Endres, Hoiem, Forsyth, 2009</marker>
<rawString>Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. 2009. Describing objects by their attributes. In Proceedings of CVPR, pages 1778– 1785, Miami Beach, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Afra Alishahi</author>
<author>Suzanne Stevenson</author>
</authors>
<title>A probabilistic computational model of cross-situational word learning.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<pages>34--1017</pages>
<contexts>
<context position="7980" citStr="Fazly et al. (2010)" startWordPosition="1261" endWordPosition="1264"> representations and compare alternative methods to perform cross-modal mapping. Then, we complement recent work (Frome et al., 2013) and show that zero-shot learning scales to a large and noisy dataset. Finally, we provide preliminary evidence that cross-modal projections can be used effectively to simulate a fast mapping scenario, thus strengthening the claims of this approach as a full-fledged, fully inductive theory of meaning acquisition. 2 Related Work The problem of establishing word reference has been extensively explored in computational simulations of cross-situational learning (see Fazly et al. (2010) for a recent proposal and extended review of previous work). This line of research has traditionally assumed artificial models of the external world, typically a set of linguistic or logical labels for objects, actions and possibly other aspects of a scene (Siskind, 1996). Recently, Yu and Siskind (2013) presented a system that induces word-object mappings from features extracted from short videos paired with sentences. Our work complements theirs in two ways. First, unlike Yu and Siskind (2013) who considered a limited lexicon of 15 items with only 4 nouns, we conduct experiments in a large </context>
</contexts>
<marker>Fazly, Alishahi, Stevenson, 2010</marker>
<rawString>Afsaneh Fazly, Afra Alishahi, and Suzanne Stevenson. 2010. A probabilistic computational model of cross-situational word learning. Cognitive Science, 34:1017–1063.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego Frassinelli</author>
<author>Frank Keller</author>
</authors>
<title>The plausibility of semantic properties generated by a distributional model: Evidence from a visual world experiment.</title>
<date>2012</date>
<booktitle>In Proceedings of CogSci,</booktitle>
<pages>1560--1565</pages>
<contexts>
<context position="31369" citStr="Frassinelli and Keller (2012)" startWordPosition="5053" endWordPosition="5056">egory flowers and at test time by butterfly, belonging to insects. The same unit receives the second highest activation by the “correct” test concept, the flower rose. PPPPPP k 1 2 5 10 50 Model Chance 0.01 0.02 0.05 0.10 0.5 NN 0.8 1.9 5.6 9.7 30.9 Table 5: Percentage accuracy among top k nearest neighbors on ESP. 5.3 Fast Mapping in ESP In this section, we aim at simulating a fast mapping scenario in which the learner has been just exposed to a new concept, and thus has limited linguistic evidence for that concept. We operationalize this by considering the 34 concrete concepts introduced by Frassinelli and Keller (2012), and deriving their text-based representations from just a few sentences randomly picked from the corpus. Concretely, we implement 5 models: context 1, context 5, context 10, context 20 and context full, where the name of the model denotes the number of sentences used to construct the text-based representations. The derived vectors were reduced with the same SVD projection induced from the complete corpus. Cross-modal mapping is done via NN. The zero-shot framework leads us to frame fast mapping as the task of projecting visual representations of new objects onto language space for retrieving</context>
</contexts>
<marker>Frassinelli, Keller, 2012</marker>
<rawString>Diego Frassinelli and Frank Keller. 2012. The plausibility of semantic properties generated by a distributional model: Evidence from a visual world experiment. In Proceedings of CogSci, pages 1560–1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Frome</author>
<author>Greg Corrado</author>
<author>Jon Shlens</author>
<author>Samy Bengio</author>
<author>Jeff Dean</author>
<author>Marc’Aurelio Ranzato</author>
<author>Tomas Mikolov</author>
</authors>
<title>DeViSE: A deep visual-semantic embedding model.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2121--2129</pages>
<location>Lake Tahoe, Nevada.</location>
<contexts>
<context position="5738" citStr="Frome et al., 2013" startWordPosition="907" endWordPosition="910">hough this might be a side effect of our approach, but we address the issue of automatically mapping objects, as depicted in images, to the context vectors representing the corresponding words. This is achieved by means of a simple neural network trained to project image-extracted feature vectors to text-based vectors through a hidden layer that can be interpreted as a cross-modal semantic space. We first test the effectiveness of our crossmodal semantic space on the so-called zero-shot learning task (Palatucci et al., 2009), which has recently been explored in the machine learning community (Frome et al., 2013; Socher et al., 2013). In this setting, we assume that our system possesses linguistic and visual information for a set of concepts in the form of text-based representations of words and image-based vectors of the corresponding objects, used for vision-to-language-mapping training. The system is then provided with visual information for a previously unseen object, and the task is to associate it with a word by cross-modal mapping. Our approach is competitive with respect to the recently proposed alternatives, while being overall simpler. The aforementioned task is very demanding and interesti</context>
<context position="7494" citStr="Frome et al., 2013" startWordPosition="1188" endWordPosition="1191">en or robots in a new environment are faced with, we next simulate a scenario akin to fast mapping. We show that the induced cross-modal semantic space is powerful enough that sensible guesses about the correct word denoting an object can be made, even when the linguistic context vector representing the word has been created from as little as 1 sentence containing it. The contributions of this work are three-fold. First, we conduct experiments with simple imageand text-based vector representations and compare alternative methods to perform cross-modal mapping. Then, we complement recent work (Frome et al., 2013) and show that zero-shot learning scales to a large and noisy dataset. Finally, we provide preliminary evidence that cross-modal projections can be used effectively to simulate a fast mapping scenario, thus strengthening the claims of this approach as a full-fledged, fully inductive theory of meaning acquisition. 2 Related Work The problem of establishing word reference has been extensively explored in computational simulations of cross-situational learning (see Fazly et al. (2010) for a recent proposal and extended review of previous work). This line of research has traditionally assumed arti</context>
<context position="9338" citStr="Frome et al. (2013)" startWordPosition="1487" endWordPosition="1490">g visual representations of objects into a shared semantic space, we do not limit ourselves to establishing a link between ob1404 jects and words. We induce a rich semantic representation of the multimodal concept, that can lead, among other things, to the discovery of important properties of an object even when we lack its linguistic label. Nevertheless, Yu and Siskind’s system could in principle be used to initialize the vision-language mapping that we rely upon. Closer to the spirit of our work are two very recent studies coming from the machine learning community. Socher et al. (2013) and Frome et al. (2013) focus on zero-shot learning in the visionlanguage domain by exploiting a shared visuallinguistic semantic space. Socher et al. (2013) learn to project unsupervised vector-based image representations onto a word-based semantic space using a neural network architecture. Unlike us, Socher and colleagues train an outlier detector to decide whether a test image should receive a known-word label by means of a standard supervised object classifier, or be assigned an unseen label by vision-to-language mapping. In our zeroshot experiments, we assume no access to an outlier detector, and thus, the sear</context>
<context position="29380" citStr="Frome et al. (2013)" startWordPosition="4716" endWordPosition="4719">revious experiment. We use a set of approximately 9,500 concepts, the intersection of the ESP-based visual semantic space with the linguistic space. For tuning the number of hidden units of NN, we use the MEN-concrete dataset of Bruni et al. (2014). Finally, we randomly pick 70% of the concepts to induce the projection function fprojv→w and report results on the remaining 30%. Note that the search space for the correct label in this experiment is approximately 95 times larger than the one used for the experiment presented in Section 5.1. Although our experimental setup differs from the one of Frome et al. (2013), thus preventing a direct comparison, the results reported in Table 5 are on a comparable scale to theirs. We note that previous work on zero-shot learning has used standard object recognition benchmarks. To the best of our knowledge, this is the first time this task has been performed on a dataset as noisy as ESP. Overall, the results suggest that cross-modal mapping could be applied in tasks where images exhibit a more complex structure, e.g., caption generation and event recognition. 1409 Seen Concepts Unseen Concept Rank of Correct CIFAR-100 Category Unseen Concept Unit 1 sunflower, tulip</context>
</contexts>
<marker>Frome, Corrado, Shlens, Bengio, Dean, Ranzato, Mikolov, 2013</marker>
<rawString>Andrea Frome, Greg Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. DeViSE: A deep visual-semantic embedding model. In Proceedings of NIPS, pages 2121–2129, Lake Tahoe, Nevada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning semantic representations for the phrase translation model. arXiv preprint arXiv:1312.0482.</title>
<date>2013</date>
<contexts>
<context position="36152" citStr="Gao et al., 2013" startWordPosition="5842" endWordPosition="5845">both on a benchmark object recognition dataset and on a more realistic and noisier dataset covering a wide range of concepts. The neural network architecture emerged as the best performing approach, and our qualitative analysis revealed that it induced a categorical organization of concepts. Most importantly, our results suggest the viability of crossmodal mapping for grounded word-meaning acquisition in a simulation of fast mapping. Given the success of NN, we plan to experiment in the future with more sophisticated neural network architectures inspired by recent work in machine translation (Gao et al., 2013) and multimodal deep learning (Srivastava and Salakhutdinov, 2012). Furthermore, we intend to adopt visual attributes (Farhadi et al., 2009; Silberer et al., 2013) as visual representations, since they should allow a better understanding of how crossmodal mapping works, thanks to their linguistic interpretability. The error analysis in Section 5.3 suggests that automated localization techniques (van de Sande et al., 2011), distinguishing an object from its surroundings, might drastically improve mapping accuracy. Similarly, in the textual domain, models that extract collocates of a word that a</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2013</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2013. Learning semantic representations for the phrase translation model. arXiv preprint arXiv:1312.0482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Griffiths</author>
<author>Mark Steyvers</author>
<author>Josh Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<pages>114--211</pages>
<contexts>
<context position="1472" citStr="Griffiths et al., 2007" startWordPosition="215" endWordPosition="218">ery limited linguistic contexts. By combining prior linguistic and visual knowledge acquired about words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts. 1 Introduction Computational models of meaning that rely on corpus-extracted context vectors, such as LSA (Landauer and Dumais, 1997), HAL (Lund and Burgess, 1996), Topic Models (Griffiths et al., 2007) and more recent neural-network approaches (Collobert and Weston, 2008; Mikolov et al., 2013b) have successfully tackled a number of lexical semantics tasks, where context vector similarity highly correlates with various indices of semantic relatedness (Turney and Pantel, 2010). Given that these models are learned from naturally occurring data using simple associative techniques, various authors have advanced the claim that they might be also capturing some crucial aspects of how humans acquire and use language (Landauer and Dumais, 1997; Lenci, 2008). However, the models induce the meaning of</context>
<context position="26819" citStr="Griffiths et al., 2007" startWordPosition="4298" endWordPosition="4302"> However, NN, an architecture that can capture more complex, non-linear relations in features across modalities, emerges as the best performing model, confirming on a larger scale the recent findings of Socher et al. (2013). 5.1.1 Concept Categorization In order to gain qualitative insights into the performance of the projection process of NN, we attempt to investigate the role and interpretability of the hidden layer. We achieve this by looking at which visual concepts result in the highest hidden unit activation.8 This is inspired by analogous qualitative analysis conducted in Topic Models (Griffiths et al., 2007), where “topics” are interpreted in terms of the words with the highest probability under each of them. Table 3 presents both seen and unseen concepts corresponding to visual vectors that trigger the highest activation for a subset of hidden units. The table further reports, for each hidden unit, the “correct” unseen concept for the category of the top seen concepts, together with its rank in terms of activation of the unit. The analysis demonstrates that, although prior knowledge about categories was not explicitly used to train the network, the latter induced an organization of concepts into</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Tom Griffiths, Mark Steyvers, and Josh Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114:211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Hardoon</author>
<author>Sandor Szedmak</author>
<author>John ShaweTaylor</author>
</authors>
<title>Canonical correlation analysis: An overview with application to learning methods.</title>
<date>2004</date>
<journal>Neural Computation,</journal>
<volume>16</volume>
<issue>12</issue>
<contexts>
<context position="20885" citStr="Hardoon et al., 2004" startWordPosition="3304" endWordPosition="3307">algorithms for inducing the cross-modal projection function fprojv→w. Linear Regression (lin) Our first model is a very simple linear mapping between the two modalities estimated by solving a least-squares problem. This method is similar to the one introduced by Mikolov et al. (2013a) for estimating a translation matrix, only solved analytically. In our setup, we can see the two different modalities as if they were different languages. By using least-squares regression, the projection function fprojv→w can be derived as fprojv→w = (VTs Vs)−1VTs Ws (1) Canonical Correlation Analysis (CCA) CCA (Hardoon et al., 2004; Hotelling, 1936) and variations thereof have been successfully used in the past for annotation of regions (Socher and Fei-Fei, 2010) and complete images (Hardoon et al., 2006; Hodosh et al., 2013). Given two paired observation matrices, in our case Vs and Ws, CCA aims at capturing the linear relationship that exists between these variables. This is achieved by finding a pair of matrices, in our 1407 case CV E Rdv×d and CW E Rd,,,×d, such that the correlation between the projections of the two multidimensional variables into a common, lower-rank space is maximized. The resulting multimodal sp</context>
</contexts>
<marker>Hardoon, Szedmak, ShaweTaylor, 2004</marker>
<rawString>David R Hardoon, Sandor Szedmak, and John ShaweTaylor. 2004. Canonical correlation analysis: An overview with application to learning methods. Neural Computation, 16(12):2639–2664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Hardoon</author>
<author>Craig Saunders</author>
<author>Sandor Szedmak</author>
<author>John Shawe-Taylor</author>
</authors>
<title>A correlation approach for automatic image annotation.</title>
<date>2006</date>
<booktitle>In Advanced Data Mining and Applications,</booktitle>
<pages>681--692</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="21061" citStr="Hardoon et al., 2006" startWordPosition="3331" endWordPosition="3334">ed by solving a least-squares problem. This method is similar to the one introduced by Mikolov et al. (2013a) for estimating a translation matrix, only solved analytically. In our setup, we can see the two different modalities as if they were different languages. By using least-squares regression, the projection function fprojv→w can be derived as fprojv→w = (VTs Vs)−1VTs Ws (1) Canonical Correlation Analysis (CCA) CCA (Hardoon et al., 2004; Hotelling, 1936) and variations thereof have been successfully used in the past for annotation of regions (Socher and Fei-Fei, 2010) and complete images (Hardoon et al., 2006; Hodosh et al., 2013). Given two paired observation matrices, in our case Vs and Ws, CCA aims at capturing the linear relationship that exists between these variables. This is achieved by finding a pair of matrices, in our 1407 case CV E Rdv×d and CW E Rd,,,×d, such that the correlation between the projections of the two multidimensional variables into a common, lower-rank space is maximized. The resulting multimodal space has been shown to provide a good approximation to human concept similarity judgments (Silberer and Lapata, 2012). In our setup, after applying CCA on the two spaces Vs and </context>
</contexts>
<marker>Hardoon, Saunders, Szedmak, Shawe-Taylor, 2006</marker>
<rawString>David R Hardoon, Craig Saunders, Sandor Szedmak, and John Shawe-Taylor. 2006. A correlation approach for automatic image annotation. In Advanced Data Mining and Applications, pages 681– 692. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathon Hare</author>
<author>Sina Samangooei</author>
<author>Paul Lewis</author>
<author>Mark Nixon</author>
</authors>
<title>Semantic spaces revisited: Investigating the performance of auto-annotation and semantic retrieval using semantic spaces.</title>
<date>2008</date>
<booktitle>In Proceedings of CIVR,</booktitle>
<pages>359--368</pages>
<location>Niagara Falls, Canada.</location>
<contexts>
<context position="22314" citStr="Hare et al., 2008" startWordPosition="3537" endWordPosition="3540">ppings onto the common space and thus our projection function can be derived as: fprojv→w = CV CW −1 (2) Singular Value Decomposition (SVD) SVD is the most widely used dimensionality reduction technique in distributional semantics (Turney and Pantel, 2010), and it has recently been exploited to combine visual and linguistic dimensions in the multimodal distributional semantic model of Bruni et al. (2014). SVD smoothing is also a way to infer values of unseen dimensions in partially incomplete matrices, a technique that has been applied to the task of inferring word tags of unannotated images (Hare et al., 2008). Assuming that the concept-representing rows of Vs and Ws are ordered in the same way, we apply the (k-truncated) SVD to the concatenated matrix [VsWs], such ˆW$] = UkΣkZTk is a k-rank approximation of the original matrix.6 The projection function is then: fprojv→w = ZkZT (3) k where the input is appropriately padded with 0s ([Vu0Nu×W]) and we discard the visual block of the output matrix [ ˆVu ˆWu]. Neural Network (NNet) The last model that we introduce is a neural network with one hidden layer. The projection function in this model can be described as: fprojv→w = Θv→w (4) where Θv→w consist</context>
</contexts>
<marker>Hare, Samangooei, Lewis, Nixon, 2008</marker>
<rawString>Jonathon Hare, Sina Samangooei, Paul Lewis, and Mark Nixon. 2008. Semantic spaces revisited: Investigating the performance of auto-annotation and semantic retrieval using semantic spaces. In Proceedings of CIVR, pages 359–368, Niagara Falls, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stevan Harnad</author>
</authors>
<title>The symbol grounding problem. Physica D: Nonlinear Phenomena,</title>
<date>1990</date>
<pages>42--1</pages>
<contexts>
<context position="2304" citStr="Harnad, 1990" startWordPosition="346" endWordPosition="347"> indices of semantic relatedness (Turney and Pantel, 2010). Given that these models are learned from naturally occurring data using simple associative techniques, various authors have advanced the claim that they might be also capturing some crucial aspects of how humans acquire and use language (Landauer and Dumais, 1997; Lenci, 2008). However, the models induce the meaning of words entirely from their co-occurrence with other words, without links to the external world. This constitutes a serious blow to claims of cognitive plausibility in at least two respects. One is the grounding problem (Harnad, 1990; Searle, 1984). Irrespective of their relatively high performance on various semantic tasks, it is debatable whether models that have no access to visual and perceptual information can capture the holistic, grounded knowledge that humans have about concepts. However, a possibly even more serious pitfall of vector models is lack of reference: natural language is, fundamentally, a means to communicate, and thus our words must be able to refer to objects, properties and events in the outside world (Abbott, 2010). Current vector models are purely language-internal, solipsistic models of meaning. </context>
</contexts>
<marker>Harnad, 1990</marker>
<rawString>Stevan Harnad. 1990. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3):335– 346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy Heibeck</author>
<author>Ellen Markman</author>
</authors>
<title>Word learning in children: an examination of fast mapping. Child Development,</title>
<date>1987</date>
<pages>58--1021</pages>
<contexts>
<context position="3827" citStr="Heibeck and Markman, 1987" startWordPosition="601" endWordPosition="604">l, this will not suffice to successfully complete the task. The model might suggest that the concepts of dog and cat are semantically related, but it has no means to determine the visual appearance of dogs, and consequently no way to verify the truth of such a simple statement. Mapping words to the objects they denote is such a core function of language that humans are highly optimized for it, as shown by the so-called fast mapping phenomenon, whereby children can learn to associate a word to an object or property by a single exposure to it (Bloom, 2000; Carey, 1978; Carey and Bartlett, 1978; Heibeck and Markman, 1987). But lack of reference is not 1403 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1403–1414, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics only a theoretical weakness: Without the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life </context>
</contexts>
<marker>Heibeck, Markman, 1987</marker>
<rawString>Tracy Heibeck and Ellen Markman. 1987. Word learning in children: an examination of fast mapping. Child Development, 58:1021–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micah Hodosh</author>
<author>Peter Young</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Framing image description as a ranking task: Data, models and evaluation metrics.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>47--853</pages>
<contexts>
<context position="21083" citStr="Hodosh et al., 2013" startWordPosition="3335" endWordPosition="3338">squares problem. This method is similar to the one introduced by Mikolov et al. (2013a) for estimating a translation matrix, only solved analytically. In our setup, we can see the two different modalities as if they were different languages. By using least-squares regression, the projection function fprojv→w can be derived as fprojv→w = (VTs Vs)−1VTs Ws (1) Canonical Correlation Analysis (CCA) CCA (Hardoon et al., 2004; Hotelling, 1936) and variations thereof have been successfully used in the past for annotation of regions (Socher and Fei-Fei, 2010) and complete images (Hardoon et al., 2006; Hodosh et al., 2013). Given two paired observation matrices, in our case Vs and Ws, CCA aims at capturing the linear relationship that exists between these variables. This is achieved by finding a pair of matrices, in our 1407 case CV E Rdv×d and CW E Rd,,,×d, such that the correlation between the projections of the two multidimensional variables into a common, lower-rank space is maximized. The resulting multimodal space has been shown to provide a good approximation to human concept similarity judgments (Silberer and Lapata, 2012). In our setup, after applying CCA on the two spaces Vs and Ws, we obtain the two </context>
</contexts>
<marker>Hodosh, Young, Hockenmaier, 2013</marker>
<rawString>Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853–899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Hotelling</author>
</authors>
<title>Relations between two sets of variates.</title>
<date>1936</date>
<journal>Biometrika,</journal>
<pages>28--3</pages>
<contexts>
<context position="20903" citStr="Hotelling, 1936" startWordPosition="3308" endWordPosition="3309">g the cross-modal projection function fprojv→w. Linear Regression (lin) Our first model is a very simple linear mapping between the two modalities estimated by solving a least-squares problem. This method is similar to the one introduced by Mikolov et al. (2013a) for estimating a translation matrix, only solved analytically. In our setup, we can see the two different modalities as if they were different languages. By using least-squares regression, the projection function fprojv→w can be derived as fprojv→w = (VTs Vs)−1VTs Ws (1) Canonical Correlation Analysis (CCA) CCA (Hardoon et al., 2004; Hotelling, 1936) and variations thereof have been successfully used in the past for annotation of regions (Socher and Fei-Fei, 2010) and complete images (Hardoon et al., 2006; Hodosh et al., 2013). Given two paired observation matrices, in our case Vs and Ws, CCA aims at capturing the linear relationship that exists between these variables. This is achieved by finding a pair of matrices, in our 1407 case CV E Rdv×d and CW E Rd,,,×d, such that the correlation between the projections of the two multidimensional variables into a common, lower-rank space is maximized. The resulting multimodal space has been shown</context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>Harold Hotelling. 1936. Relations between two sets of variates. Biometrika, 28(3/4):321–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Kelly</author>
<author>Barry Devereux</author>
<author>Anna Korhonen</author>
</authors>
<title>Semi-supervised learning for automatic conceptual property extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics,</booktitle>
<pages>11--20</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="36819" citStr="Kelly et al., 2012" startWordPosition="5943" endWordPosition="5946">lakhutdinov, 2012). Furthermore, we intend to adopt visual attributes (Farhadi et al., 2009; Silberer et al., 2013) as visual representations, since they should allow a better understanding of how crossmodal mapping works, thanks to their linguistic interpretability. The error analysis in Section 5.3 suggests that automated localization techniques (van de Sande et al., 2011), distinguishing an object from its surroundings, might drastically improve mapping accuracy. Similarly, in the textual domain, models that extract collocates of a word that are more likely to denote conceptual properties (Kelly et al., 2012) might lead to more informative and discriminative linguistic vectors. Finally, the lack of large child-directed speech corpora constrained the experimental design of fast ions of CHILDES) that contain sentences more akin to those a child might effectively hear or read in her word-learning years. Acknowledgments mapping simulations; we plan to run more realistic experiments with true nonce words and using source corpora (e.g., the Simple Wikipedia, child stories, port We thank Adam for helpful discussions and the 3 anonymous reviewers for useful comments. This work was supported by ERC 2011 St</context>
</contexts>
<marker>Kelly, Devereux, Korhonen, 2012</marker>
<rawString>Colin Kelly, Barry Devereux, and Anna Korhonen. 2012. Semi-supervised learning for automatic conceptual property extraction. In Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics, pages 11–20, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoff Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1106--1114</pages>
<contexts>
<context position="10683" citStr="Krizhevsky et al., 2012" startWordPosition="1693" endWordPosition="1696">onstrained evaluation setup, where only 10 concepts are considered, compared to our experiments with hundreds or thousands of concepts. Frome et al. (2013) use linear regression to transform vector-based image representations onto vectors representing the same concepts in linguistic semantic space. Unlike Socher et al. (2013) and the current study that adopt simple unsupervised techniques for constructing image representations, Frome et al. (2013) rely on a supervised state-ofthe-art method: They feed low-level features to a deep neural network trained on a supervised object recognition task (Krizhevsky et al., 2012). Furthermore, their text-based vectors encode very rich ~ information, such as king − ~man + ~woman = ~queen (Mikolov et al., 2013c). A natural question we aim to answer is whether the success of cross-modal mapping is due to the high-quality embeddings or to the general algorithmic design. If the latter is the case, then these results could be extended to traditional distributional vectors bearing other desirable properties, such as high interpretability of dimensions. Figure 1: A potential wampimuk (a) together with its projection onto the linguistic space (b). 3 Zero-shot learning and fast</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Proceedings of NIPS, pages 1106–1114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
</authors>
<title>Learning multiple layers of features from tiny images. Master’s thesis.</title>
<date>2009</date>
<contexts>
<context position="13519" citStr="Krizhevsky, 2009" startWordPosition="2152" endWordPosition="2153">ns estimated from a large corpus), in the case of the former, new concepts are assumed to be encounted in a limited linguistic context and therefore lacking rich linguistic representations. This is operationalized by constructing the text-based vector for these (a) (b) 1405 Figure 2: Images of chair as extracted from CIFAR-100 (left) and ESP (right). concepts from a context of just a few occurrences. In this way, we simulate the first encounter of a learner with a concept that is new in both visual and linguistic terms. 4 Experimental Setup 4.1 Visual Datasets CIFAR-100 The CIFAR-100 dataset (Krizhevsky, 2009) consists of 60,000 32x32 colour images (note the extremely small size) representing 100 distinct concepts, with 600 images per concept. The dataset covers a wide range of concrete domains and is organized into 20 broader categories. Table 1 lists the concepts used in our experiments organized by category. ESP Our second dataset consists of 100K images from the ESP-Game data set, labeled through a “game with a purpose” (Von Ahn, 2006).1 The ESP image tags form a vocabulary of 20,515 unique words. Unlike other datasets used for zeroshot learning, it covers adjectives and verbs in addition to no</context>
</contexts>
<marker>Krizhevsky, 2009</marker>
<rawString>Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images. Master’s thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>240</pages>
<contexts>
<context position="1403" citStr="Landauer and Dumais, 1997" startWordPosition="204" endWordPosition="207">h the learner is exposed to new objects and the corresponding words in very limited linguistic contexts. By combining prior linguistic and visual knowledge acquired about words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts. 1 Introduction Computational models of meaning that rely on corpus-extracted context vectors, such as LSA (Landauer and Dumais, 1997), HAL (Lund and Burgess, 1996), Topic Models (Griffiths et al., 2007) and more recent neural-network approaches (Collobert and Weston, 2008; Mikolov et al., 2013b) have successfully tackled a number of lexical semantics tasks, where context vector similarity highly correlates with various indices of semantic relatedness (Turney and Pantel, 2010). Given that these models are learned from naturally occurring data using simple associative techniques, various authors have advanced the claim that they might be also capturing some crucial aspects of how humans acquire and use language (Landauer and </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Lazebnik</author>
<author>Cordelia Schmid</author>
<author>Jean Ponce</author>
</authors>
<title>Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories.</title>
<date>2006</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>2169--2178</pages>
<location>Washington, DC.</location>
<contexts>
<context position="16097" citStr="Lazebnik et al., 2006" startWordPosition="2566" endWordPosition="2569">age is represented by a count vector recording the number of occurrences of each visual word in it. We do not attempt any parameter tuning of the pipeline. As low-level features, we use Scale Invariant Feature Transform (SIFT) features (Lowe, 2004). SIFT features are tailored to capture object parts and to be invariant to several image transformations such as rotation, illumination and scale change. These features are clustered into vocabularies of 5,000 (ESP) and 4,096 (CIFAR-100) visual words.2 To preserve spatial information in the BoVW representation, we use the spatial pyramid technique (Lazebnik et al., 2006), which consists in dividing the image into several regions, computing BoVW vectors for each region and concatenating them. In particular, we divide ESP images into 16 regions and the smaller CIFAR-100 images into 4. The vectors resulting from region concatenation have dimensionality 5000 × 16 = 80, 000 (ESP) and 4,096 × 4 = 16,384 (CIFAR-100), respectively. We apply Local Mutual Information (LMI, (Evert, 2005)) as weighting scheme and reduce the full co-occurrence space to 300 dimensions using the Singular Value Decomposition. For CIFAR-100, we extract distinct visual vectors for single image</context>
</contexts>
<marker>Lazebnik, Schmid, Ponce, 2006</marker>
<rawString>Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In Proceedings of CVPR, pages 2169–2178, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional approaches in linguistic and cognitive research.</title>
<date>2008</date>
<journal>Italian Journal of Linguistics,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="2029" citStr="Lenci, 2008" startWordPosition="302" endWordPosition="303">d Burgess, 1996), Topic Models (Griffiths et al., 2007) and more recent neural-network approaches (Collobert and Weston, 2008; Mikolov et al., 2013b) have successfully tackled a number of lexical semantics tasks, where context vector similarity highly correlates with various indices of semantic relatedness (Turney and Pantel, 2010). Given that these models are learned from naturally occurring data using simple associative techniques, various authors have advanced the claim that they might be also capturing some crucial aspects of how humans acquire and use language (Landauer and Dumais, 1997; Lenci, 2008). However, the models induce the meaning of words entirely from their co-occurrence with other words, without links to the external world. This constitutes a serious blow to claims of cognitive plausibility in at least two respects. One is the grounding problem (Harnad, 1990; Searle, 1984). Irrespective of their relatively high performance on various semantic tasks, it is debatable whether models that have no access to visual and perceptual information can capture the holistic, grounded knowledge that humans have about concepts. However, a possibly even more serious pitfall of vector models is</context>
</contexts>
<marker>Lenci, 2008</marker>
<rawString>Alessandro Lenci. 2008. Distributional approaches in linguistic and cognitive research. Italian Journal of Linguistics, 20(1):1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chee Wee Leong</author>
<author>Rada Mihalcea</author>
</authors>
<title>Going beyond text: A hybrid image-text approach for measuring word relatedness.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>1403--1407</pages>
<contexts>
<context position="4670" citStr="Leong and Mihalcea, 2011" startWordPosition="732" endWordPosition="735">al Linguistics only a theoretical weakness: Without the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life challenges. Very recently, a number of papers have exploited advances in automated feature extraction form images and videos to enrich context vectors with visual information (Bruni et al., 2014; Feng and Lapata, 2010; Leong and Mihalcea, 2011; Regneri et al., 2013; Silberer et al., 2013). This line of research tackles the grounding problem: Word representations are no longer limited to their linguistic contexts but also encode visual information present in images associated with the corresponding objects. In this paper, we rely on the same image analysis techniques but instead focus on the reference problem: We do not aim at enriching word representations with visual information, although this might be a side effect of our approach, but we address the issue of automatically mapping objects, as depicted in images, to the context ve</context>
</contexts>
<marker>Leong, Mihalcea, 2011</marker>
<rawString>Chee Wee Leong and Rada Mihalcea. 2011. Going beyond text: A hybrid image-text approach for measuring word relatedness. In Proceedings of IJCNLP, pages 1403–1407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lowe</author>
</authors>
<title>Distinctive image features from scale-invariant keypoints.</title>
<date>2004</date>
<journal>International Journal of Computer Vision,</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="15723" citStr="Lowe, 2004" startWordPosition="2510" endWordPosition="2511">nd successfully applied to computer vision tasks such as object recognition and image retrieval (Yang et al., 2007). First, low-level visual features (Szeliski, 2010) are extracted from a large collection of images and clustered into a set of “visual words”. The low-level features of a specific image are then mapped to the corresponding visual words, and the image is represented by a count vector recording the number of occurrences of each visual word in it. We do not attempt any parameter tuning of the pipeline. As low-level features, we use Scale Invariant Feature Transform (SIFT) features (Lowe, 2004). SIFT features are tailored to capture object parts and to be invariant to several image transformations such as rotation, illumination and scale change. These features are clustered into vocabularies of 5,000 (ESP) and 4,096 (CIFAR-100) visual words.2 To preserve spatial information in the BoVW representation, we use the spatial pyramid technique (Lazebnik et al., 2006), which consists in dividing the image into several regions, computing BoVW vectors for each region and concatenating them. In particular, we divide ESP images into 16 regions and the smaller CIFAR-100 images into 4. The vecto</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>David Lowe. 2004. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods,</title>
<date>1996</date>
<pages>28--203</pages>
<contexts>
<context position="1433" citStr="Lund and Burgess, 1996" startWordPosition="209" endWordPosition="212">bjects and the corresponding words in very limited linguistic contexts. By combining prior linguistic and visual knowledge acquired about words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts. 1 Introduction Computational models of meaning that rely on corpus-extracted context vectors, such as LSA (Landauer and Dumais, 1997), HAL (Lund and Burgess, 1996), Topic Models (Griffiths et al., 2007) and more recent neural-network approaches (Collobert and Weston, 2008; Mikolov et al., 2013b) have successfully tackled a number of lexical semantics tasks, where context vector similarity highly correlates with various indices of semantic relatedness (Turney and Pantel, 2010). Given that these models are learned from naturally occurring data using simple associative techniques, various authors have advanced the claim that they might be also capturing some crucial aspects of how humans acquire and use language (Landauer and Dumais, 1997; Lenci, 2008). Ho</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, 28:203– 208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</title>
<date>2013</date>
<contexts>
<context position="1564" citStr="Mikolov et al., 2013" startWordPosition="228" endWordPosition="231">out words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts. 1 Introduction Computational models of meaning that rely on corpus-extracted context vectors, such as LSA (Landauer and Dumais, 1997), HAL (Lund and Burgess, 1996), Topic Models (Griffiths et al., 2007) and more recent neural-network approaches (Collobert and Weston, 2008; Mikolov et al., 2013b) have successfully tackled a number of lexical semantics tasks, where context vector similarity highly correlates with various indices of semantic relatedness (Turney and Pantel, 2010). Given that these models are learned from naturally occurring data using simple associative techniques, various authors have advanced the claim that they might be also capturing some crucial aspects of how humans acquire and use language (Landauer and Dumais, 1997; Lenci, 2008). However, the models induce the meaning of words entirely from their co-occurrence with other words, without links to the external wor</context>
<context position="10814" citStr="Mikolov et al., 2013" startWordPosition="1716" endWordPosition="1719"> Frome et al. (2013) use linear regression to transform vector-based image representations onto vectors representing the same concepts in linguistic semantic space. Unlike Socher et al. (2013) and the current study that adopt simple unsupervised techniques for constructing image representations, Frome et al. (2013) rely on a supervised state-ofthe-art method: They feed low-level features to a deep neural network trained on a supervised object recognition task (Krizhevsky et al., 2012). Furthermore, their text-based vectors encode very rich ~ information, such as king − ~man + ~woman = ~queen (Mikolov et al., 2013c). A natural question we aim to answer is whether the success of cross-modal mapping is due to the high-quality embeddings or to the general algorithmic design. If the latter is the case, then these results could be extended to traditional distributional vectors bearing other desirable properties, such as high interpretability of dimensions. Figure 1: A potential wampimuk (a) together with its projection onto the linguistic space (b). 3 Zero-shot learning and fast mapping “We found a cute, hairy wampimuk sleeping behind the tree.” Even though the previous statement is certainly the first time</context>
<context position="20548" citStr="Mikolov et al. (2013" startWordPosition="3251" endWordPosition="3254">jective that aims at minimizing some cost function between the induced text-based representations ˆWs E RNs×dw and the gold ones Ws. The induced fprojv→w is then applied to the imagebased representations Vu E RNu×dv of Nu unseen objects to transform them into text-based representations ˆWu E RNu×dw. We implement 4 alternative learning algorithms for inducing the cross-modal projection function fprojv→w. Linear Regression (lin) Our first model is a very simple linear mapping between the two modalities estimated by solving a least-squares problem. This method is similar to the one introduced by Mikolov et al. (2013a) for estimating a translation matrix, only solved analytically. In our setup, we can see the two different modalities as if they were different languages. By using least-squares regression, the projection function fprojv→w can be derived as fprojv→w = (VTs Vs)−1VTs Ws (1) Canonical Correlation Analysis (CCA) CCA (Hardoon et al., 2004; Hotelling, 1936) and variations thereof have been successfully used in the past for annotation of regions (Socher and Fei-Fei, 2010) and complete images (Hardoon et al., 2006; Hodosh et al., 2013). Given two paired observation matrices, in our case Vs and Ws, C</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>3111--3119</pages>
<location>Lake Tahoe, Nevada.</location>
<contexts>
<context position="1564" citStr="Mikolov et al., 2013" startWordPosition="228" endWordPosition="231">out words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts. 1 Introduction Computational models of meaning that rely on corpus-extracted context vectors, such as LSA (Landauer and Dumais, 1997), HAL (Lund and Burgess, 1996), Topic Models (Griffiths et al., 2007) and more recent neural-network approaches (Collobert and Weston, 2008; Mikolov et al., 2013b) have successfully tackled a number of lexical semantics tasks, where context vector similarity highly correlates with various indices of semantic relatedness (Turney and Pantel, 2010). Given that these models are learned from naturally occurring data using simple associative techniques, various authors have advanced the claim that they might be also capturing some crucial aspects of how humans acquire and use language (Landauer and Dumais, 1997; Lenci, 2008). However, the models induce the meaning of words entirely from their co-occurrence with other words, without links to the external wor</context>
<context position="10814" citStr="Mikolov et al., 2013" startWordPosition="1716" endWordPosition="1719"> Frome et al. (2013) use linear regression to transform vector-based image representations onto vectors representing the same concepts in linguistic semantic space. Unlike Socher et al. (2013) and the current study that adopt simple unsupervised techniques for constructing image representations, Frome et al. (2013) rely on a supervised state-ofthe-art method: They feed low-level features to a deep neural network trained on a supervised object recognition task (Krizhevsky et al., 2012). Furthermore, their text-based vectors encode very rich ~ information, such as king − ~man + ~woman = ~queen (Mikolov et al., 2013c). A natural question we aim to answer is whether the success of cross-modal mapping is due to the high-quality embeddings or to the general algorithmic design. If the latter is the case, then these results could be extended to traditional distributional vectors bearing other desirable properties, such as high interpretability of dimensions. Figure 1: A potential wampimuk (a) together with its projection onto the linguistic space (b). 3 Zero-shot learning and fast mapping “We found a cute, hairy wampimuk sleeping behind the tree.” Even though the previous statement is certainly the first time</context>
<context position="20548" citStr="Mikolov et al. (2013" startWordPosition="3251" endWordPosition="3254">jective that aims at minimizing some cost function between the induced text-based representations ˆWs E RNs×dw and the gold ones Ws. The induced fprojv→w is then applied to the imagebased representations Vu E RNu×dv of Nu unseen objects to transform them into text-based representations ˆWu E RNu×dw. We implement 4 alternative learning algorithms for inducing the cross-modal projection function fprojv→w. Linear Regression (lin) Our first model is a very simple linear mapping between the two modalities estimated by solving a least-squares problem. This method is similar to the one introduced by Mikolov et al. (2013a) for estimating a translation matrix, only solved analytically. In our setup, we can see the two different modalities as if they were different languages. By using least-squares regression, the projection function fprojv→w can be derived as fprojv→w = (VTs Vs)−1VTs Ws (1) Canonical Correlation Analysis (CCA) CCA (Hardoon et al., 2004; Hotelling, 1936) and variations thereof have been successfully used in the past for annotation of regions (Socher and Fei-Fei, 2010) and complete images (Hardoon et al., 2006; Hodosh et al., 2013). Given two paired observation matrices, in our case Vs and Ws, C</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119, Lake Tahoe, Nevada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>746--751</pages>
<location>Atlanta,</location>
<contexts>
<context position="1564" citStr="Mikolov et al., 2013" startWordPosition="228" endWordPosition="231">out words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts. 1 Introduction Computational models of meaning that rely on corpus-extracted context vectors, such as LSA (Landauer and Dumais, 1997), HAL (Lund and Burgess, 1996), Topic Models (Griffiths et al., 2007) and more recent neural-network approaches (Collobert and Weston, 2008; Mikolov et al., 2013b) have successfully tackled a number of lexical semantics tasks, where context vector similarity highly correlates with various indices of semantic relatedness (Turney and Pantel, 2010). Given that these models are learned from naturally occurring data using simple associative techniques, various authors have advanced the claim that they might be also capturing some crucial aspects of how humans acquire and use language (Landauer and Dumais, 1997; Lenci, 2008). However, the models induce the meaning of words entirely from their co-occurrence with other words, without links to the external wor</context>
<context position="10814" citStr="Mikolov et al., 2013" startWordPosition="1716" endWordPosition="1719"> Frome et al. (2013) use linear regression to transform vector-based image representations onto vectors representing the same concepts in linguistic semantic space. Unlike Socher et al. (2013) and the current study that adopt simple unsupervised techniques for constructing image representations, Frome et al. (2013) rely on a supervised state-ofthe-art method: They feed low-level features to a deep neural network trained on a supervised object recognition task (Krizhevsky et al., 2012). Furthermore, their text-based vectors encode very rich ~ information, such as king − ~man + ~woman = ~queen (Mikolov et al., 2013c). A natural question we aim to answer is whether the success of cross-modal mapping is due to the high-quality embeddings or to the general algorithmic design. If the latter is the case, then these results could be extended to traditional distributional vectors bearing other desirable properties, such as high interpretability of dimensions. Figure 1: A potential wampimuk (a) together with its projection onto the linguistic space (b). 3 Zero-shot learning and fast mapping “We found a cute, hairy wampimuk sleeping behind the tree.” Even though the previous statement is certainly the first time</context>
<context position="20548" citStr="Mikolov et al. (2013" startWordPosition="3251" endWordPosition="3254">jective that aims at minimizing some cost function between the induced text-based representations ˆWs E RNs×dw and the gold ones Ws. The induced fprojv→w is then applied to the imagebased representations Vu E RNu×dv of Nu unseen objects to transform them into text-based representations ˆWu E RNu×dw. We implement 4 alternative learning algorithms for inducing the cross-modal projection function fprojv→w. Linear Regression (lin) Our first model is a very simple linear mapping between the two modalities estimated by solving a least-squares problem. This method is similar to the one introduced by Mikolov et al. (2013a) for estimating a translation matrix, only solved analytically. In our setup, we can see the two different modalities as if they were different languages. By using least-squares regression, the projection function fprojv→w can be derived as fprojv→w = (VTs Vs)−1VTs Ws (1) Canonical Correlation Analysis (CCA) CCA (Hardoon et al., 2004; Hotelling, 1936) and variations thereof have been successfully used in the past for annotation of regions (Socher and Fei-Fei, 2010) and complete images (Hardoon et al., 2006; Hodosh et al., 2013). Given two paired observation matrices, in our case Vs and Ws, C</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of NAACL, pages 746–751, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Palatucci</author>
<author>Dean Pomerleau</author>
<author>Geoffrey Hinton</author>
<author>Tom Mitchell</author>
</authors>
<title>Zero-shot learning with semantic output codes.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1410--1418</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5650" citStr="Palatucci et al., 2009" startWordPosition="891" endWordPosition="894">ference problem: We do not aim at enriching word representations with visual information, although this might be a side effect of our approach, but we address the issue of automatically mapping objects, as depicted in images, to the context vectors representing the corresponding words. This is achieved by means of a simple neural network trained to project image-extracted feature vectors to text-based vectors through a hidden layer that can be interpreted as a cross-modal semantic space. We first test the effectiveness of our crossmodal semantic space on the so-called zero-shot learning task (Palatucci et al., 2009), which has recently been explored in the machine learning community (Frome et al., 2013; Socher et al., 2013). In this setting, we assume that our system possesses linguistic and visual information for a set of concepts in the form of text-based representations of words and image-based vectors of the corresponding objects, used for vision-to-language-mapping training. The system is then provided with visual information for a previously unseen object, and the task is to associate it with a word by cross-modal mapping. Our approach is competitive with respect to the recently proposed alternativ</context>
</contexts>
<marker>Palatucci, Pomerleau, Hinton, Mitchell, 2009</marker>
<rawString>Mark Palatucci, Dean Pomerleau, Geoffrey Hinton, and Tom Mitchell. 2009. Zero-shot learning with semantic output codes. In Proceedings of NIPS, pages 1410–1418, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfredo F Pereira</author>
<author>Karin H James</author>
<author>Susan S Jones</author>
<author>Linda B Smith</author>
</authors>
<title>Early biases and developmental changes in self-generated object views.</title>
<date>2010</date>
<journal>Journal of vision,</journal>
<volume>10</volume>
<issue>11</issue>
<contexts>
<context position="16954" citStr="Pereira et al., 2010" startWordPosition="2709" endWordPosition="2712">ng from region concatenation have dimensionality 5000 × 16 = 80, 000 (ESP) and 4,096 × 4 = 16,384 (CIFAR-100), respectively. We apply Local Mutual Information (LMI, (Evert, 2005)) as weighting scheme and reduce the full co-occurrence space to 300 dimensions using the Singular Value Decomposition. For CIFAR-100, we extract distinct visual vectors for single images. For ESP, given the size and amount of noise in this dataset, we build vectors for visual concepts, by normalizing and summing the BoVW vectors of all the images that have the relevant concept as a tag. Note that relevant literature (Pereira et al., 2010) has emphasized the importance of learners self-generating multiple views when faced with new objects. Thus, our multiple-image assumption should not be considered as problematic in the current setup. 2For selecting the size of the vocabulary size, we relied on standard settings found in the relevant literature (Bruni et al., 2014; Chatfield et al., 2011). 1406 Category Seen Concepts Unseen (Test) Concepts aquatic mammals beaver, otter, seal, whale dolphin fish ray, trout shark flowers orchid, poppy, sunflower, tulip rose food containers bottle, bowl, can ,plate cup fruit vegetable apple, mush</context>
</contexts>
<marker>Pereira, James, Jones, Smith, 2010</marker>
<rawString>Alfredo F Pereira, Karin H James, Susan S Jones, and Linda B Smith. 2010. Early biases and developmental changes in self-generated object views. Journal of vision, 10(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Marcus Rohrbach</author>
<author>Dominikus Wetzel</author>
<author>Stefan Thater</author>
<author>Bernt Schiele</author>
<author>Manfred Pinkal</author>
</authors>
<title>Grounding action descriptions in videos.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--25</pages>
<contexts>
<context position="4692" citStr="Regneri et al., 2013" startWordPosition="736" endWordPosition="739">retical weakness: Without the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life challenges. Very recently, a number of papers have exploited advances in automated feature extraction form images and videos to enrich context vectors with visual information (Bruni et al., 2014; Feng and Lapata, 2010; Leong and Mihalcea, 2011; Regneri et al., 2013; Silberer et al., 2013). This line of research tackles the grounding problem: Word representations are no longer limited to their linguistic contexts but also encode visual information present in images associated with the corresponding objects. In this paper, we rely on the same image analysis techniques but instead focus on the reference problem: We do not aim at enriching word representations with visual information, although this might be a side effect of our approach, but we address the issue of automatically mapping objects, as depicted in images, to the context vectors representing the</context>
</contexts>
<marker>Regneri, Rohrbach, Wetzel, Thater, Schiele, Pinkal, 2013</marker>
<rawString>Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. 2013. Grounding action descriptions in videos. Transactions of the Association for Computational Linguistics, 1:25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Searle</author>
</authors>
<title>Minds, Brains and Science.</title>
<date>1984</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2319" citStr="Searle, 1984" startWordPosition="348" endWordPosition="349">mantic relatedness (Turney and Pantel, 2010). Given that these models are learned from naturally occurring data using simple associative techniques, various authors have advanced the claim that they might be also capturing some crucial aspects of how humans acquire and use language (Landauer and Dumais, 1997; Lenci, 2008). However, the models induce the meaning of words entirely from their co-occurrence with other words, without links to the external world. This constitutes a serious blow to claims of cognitive plausibility in at least two respects. One is the grounding problem (Harnad, 1990; Searle, 1984). Irrespective of their relatively high performance on various semantic tasks, it is debatable whether models that have no access to visual and perceptual information can capture the holistic, grounded knowledge that humans have about concepts. However, a possibly even more serious pitfall of vector models is lack of reference: natural language is, fundamentally, a means to communicate, and thus our words must be able to refer to objects, properties and events in the outside world (Abbott, 2010). Current vector models are purely language-internal, solipsistic models of meaning. Consider the ve</context>
</contexts>
<marker>Searle, 1984</marker>
<rawString>John Searle. 1984. Minds, Brains and Science. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Grounded models of semantic representation.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1423--1433</pages>
<location>Jeju,</location>
<contexts>
<context position="21601" citStr="Silberer and Lapata, 2012" startWordPosition="3418" endWordPosition="3421">tation of regions (Socher and Fei-Fei, 2010) and complete images (Hardoon et al., 2006; Hodosh et al., 2013). Given two paired observation matrices, in our case Vs and Ws, CCA aims at capturing the linear relationship that exists between these variables. This is achieved by finding a pair of matrices, in our 1407 case CV E Rdv×d and CW E Rd,,,×d, such that the correlation between the projections of the two multidimensional variables into a common, lower-rank space is maximized. The resulting multimodal space has been shown to provide a good approximation to human concept similarity judgments (Silberer and Lapata, 2012). In our setup, after applying CCA on the two spaces Vs and Ws, we obtain the two projection mappings onto the common space and thus our projection function can be derived as: fprojv→w = CV CW −1 (2) Singular Value Decomposition (SVD) SVD is the most widely used dimensionality reduction technique in distributional semantics (Turney and Pantel, 2010), and it has recently been exploited to combine visual and linguistic dimensions in the multimodal distributional semantic model of Bruni et al. (2014). SVD smoothing is also a way to infer values of unseen dimensions in partially incomplete matrice</context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In Proceedings of EMNLP, pages 1423–1433, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Vittorio Ferrari</author>
<author>Mirella Lapata</author>
</authors>
<title>Models of semantic representation with visual attributes.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>572--582</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="4716" citStr="Silberer et al., 2013" startWordPosition="740" endWordPosition="743">out the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life challenges. Very recently, a number of papers have exploited advances in automated feature extraction form images and videos to enrich context vectors with visual information (Bruni et al., 2014; Feng and Lapata, 2010; Leong and Mihalcea, 2011; Regneri et al., 2013; Silberer et al., 2013). This line of research tackles the grounding problem: Word representations are no longer limited to their linguistic contexts but also encode visual information present in images associated with the corresponding objects. In this paper, we rely on the same image analysis techniques but instead focus on the reference problem: We do not aim at enriching word representations with visual information, although this might be a side effect of our approach, but we address the issue of automatically mapping objects, as depicted in images, to the context vectors representing the corresponding words. Th</context>
<context position="36315" citStr="Silberer et al., 2013" startWordPosition="5867" endWordPosition="5870">emerged as the best performing approach, and our qualitative analysis revealed that it induced a categorical organization of concepts. Most importantly, our results suggest the viability of crossmodal mapping for grounded word-meaning acquisition in a simulation of fast mapping. Given the success of NN, we plan to experiment in the future with more sophisticated neural network architectures inspired by recent work in machine translation (Gao et al., 2013) and multimodal deep learning (Srivastava and Salakhutdinov, 2012). Furthermore, we intend to adopt visual attributes (Farhadi et al., 2009; Silberer et al., 2013) as visual representations, since they should allow a better understanding of how crossmodal mapping works, thanks to their linguistic interpretability. The error analysis in Section 5.3 suggests that automated localization techniques (van de Sande et al., 2011), distinguishing an object from its surroundings, might drastically improve mapping accuracy. Similarly, in the textual domain, models that extract collocates of a word that are more likely to denote conceptual properties (Kelly et al., 2012) might lead to more informative and discriminative linguistic vectors. Finally, the lack of larg</context>
</contexts>
<marker>Silberer, Ferrari, Lapata, 2013</marker>
<rawString>Carina Silberer, Vittorio Ferrari, and Mirella Lapata. 2013. Models of semantic representation with visual attributes. In Proceedings of ACL, pages 572–582, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Siskind</author>
</authors>
<title>A computational study of crosssituational techniques for learning word-to-meaning mappings.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--39</pages>
<contexts>
<context position="8253" citStr="Siskind, 1996" startWordPosition="1309" endWordPosition="1310">be used effectively to simulate a fast mapping scenario, thus strengthening the claims of this approach as a full-fledged, fully inductive theory of meaning acquisition. 2 Related Work The problem of establishing word reference has been extensively explored in computational simulations of cross-situational learning (see Fazly et al. (2010) for a recent proposal and extended review of previous work). This line of research has traditionally assumed artificial models of the external world, typically a set of linguistic or logical labels for objects, actions and possibly other aspects of a scene (Siskind, 1996). Recently, Yu and Siskind (2013) presented a system that induces word-object mappings from features extracted from short videos paired with sentences. Our work complements theirs in two ways. First, unlike Yu and Siskind (2013) who considered a limited lexicon of 15 items with only 4 nouns, we conduct experiments in a large search space containing a highly ambiguous set of potential target words for every object (see Section 4.1). Most importantly, by projecting visual representations of objects into a shared semantic space, we do not limit ourselves to establishing a link between ob1404 ject</context>
</contexts>
<marker>Siskind, 1996</marker>
<rawString>Jeffrey Siskind. 1996. A computational study of crosssituational techniques for learning word-to-meaning mappings. Cognition, 61:39–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Sivic</author>
<author>Andrew Zisserman</author>
</authors>
<title>Video Google: A text retrieval approach to object matching in videos.</title>
<date>2003</date>
<booktitle>In Proceedings of ICCV,</booktitle>
<pages>1470--1477</pages>
<location>Nice, France.</location>
<contexts>
<context position="15066" citStr="Sivic and Zisserman, 2003" startWordPosition="2400" endWordPosition="2403">bjects do not appear in most images in their prototypical display, but rather as elements of complex scenes (see Figure 2). Thus, ESP constitutes a more realistic, and at the same time more challenging, simulation of how things are encountered in real life, testing the potentials of cross-modal mapping in dealing with the complex scenes that one would encounter in event recognition and caption generation tasks. 1http://www.cs.cmu.edu/˜biglou/ resources/ 4.2 Visual Semantic Spaces Image-based vectors are extracted using the unsupervised bag-of-visual-words (BoVW) representational architecture (Sivic and Zisserman, 2003; Csurka et al., 2004), that has been widely and successfully applied to computer vision tasks such as object recognition and image retrieval (Yang et al., 2007). First, low-level visual features (Szeliski, 2010) are extracted from a large collection of images and clustered into a set of “visual words”. The low-level features of a specific image are then mapped to the corresponding visual words, and the image is represented by a count vector recording the number of occurrences of each visual word in it. We do not attempt any parameter tuning of the pipeline. As low-level features, we use Scale</context>
</contexts>
<marker>Sivic, Zisserman, 2003</marker>
<rawString>Josef Sivic and Andrew Zisserman. 2003. Video Google: A text retrieval approach to object matching in videos. In Proceedings of ICCV, pages 1470– 1477, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Li Fei-Fei</author>
</authors>
<title>Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>966--973</pages>
<contexts>
<context position="21019" citStr="Socher and Fei-Fei, 2010" startWordPosition="3324" endWordPosition="3327">near mapping between the two modalities estimated by solving a least-squares problem. This method is similar to the one introduced by Mikolov et al. (2013a) for estimating a translation matrix, only solved analytically. In our setup, we can see the two different modalities as if they were different languages. By using least-squares regression, the projection function fprojv→w can be derived as fprojv→w = (VTs Vs)−1VTs Ws (1) Canonical Correlation Analysis (CCA) CCA (Hardoon et al., 2004; Hotelling, 1936) and variations thereof have been successfully used in the past for annotation of regions (Socher and Fei-Fei, 2010) and complete images (Hardoon et al., 2006; Hodosh et al., 2013). Given two paired observation matrices, in our case Vs and Ws, CCA aims at capturing the linear relationship that exists between these variables. This is achieved by finding a pair of matrices, in our 1407 case CV E Rdv×d and CW E Rd,,,×d, such that the correlation between the projections of the two multidimensional variables into a common, lower-rank space is maximized. The resulting multimodal space has been shown to provide a good approximation to human concept similarity judgments (Silberer and Lapata, 2012). In our setup, af</context>
</contexts>
<marker>Socher, Fei-Fei, 2010</marker>
<rawString>Richard Socher and Li Fei-Fei. 2010. Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora. In Proceedings of CVPR, pages 966–973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Milind Ganjoo</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Zero-shot learning through cross-modal transfer.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>935--943</pages>
<location>Lake Tahoe, Nevada.</location>
<contexts>
<context position="5760" citStr="Socher et al., 2013" startWordPosition="911" endWordPosition="914">a side effect of our approach, but we address the issue of automatically mapping objects, as depicted in images, to the context vectors representing the corresponding words. This is achieved by means of a simple neural network trained to project image-extracted feature vectors to text-based vectors through a hidden layer that can be interpreted as a cross-modal semantic space. We first test the effectiveness of our crossmodal semantic space on the so-called zero-shot learning task (Palatucci et al., 2009), which has recently been explored in the machine learning community (Frome et al., 2013; Socher et al., 2013). In this setting, we assume that our system possesses linguistic and visual information for a set of concepts in the form of text-based representations of words and image-based vectors of the corresponding objects, used for vision-to-language-mapping training. The system is then provided with visual information for a previously unseen object, and the task is to associate it with a word by cross-modal mapping. Our approach is competitive with respect to the recently proposed alternatives, while being overall simpler. The aforementioned task is very demanding and interesting from an engineering</context>
<context position="9314" citStr="Socher et al. (2013)" startWordPosition="1482" endWordPosition="1485">importantly, by projecting visual representations of objects into a shared semantic space, we do not limit ourselves to establishing a link between ob1404 jects and words. We induce a rich semantic representation of the multimodal concept, that can lead, among other things, to the discovery of important properties of an object even when we lack its linguistic label. Nevertheless, Yu and Siskind’s system could in principle be used to initialize the vision-language mapping that we rely upon. Closer to the spirit of our work are two very recent studies coming from the machine learning community. Socher et al. (2013) and Frome et al. (2013) focus on zero-shot learning in the visionlanguage domain by exploiting a shared visuallinguistic semantic space. Socher et al. (2013) learn to project unsupervised vector-based image representations onto a word-based semantic space using a neural network architecture. Unlike us, Socher and colleagues train an outlier detector to decide whether a test image should receive a known-word label by means of a standard supervised object classifier, or be assigned an unseen label by vision-to-language mapping. In our zeroshot experiments, we assume no access to an outlier dete</context>
<context position="19771" citStr="Socher et al. (2013)" startWordPosition="3128" endWordPosition="3131"> a target word. Finally, similarly to the visual semantic space, raw counts are transformed by applying LMI and then reduced to 300 dimensions with SVD.5 4.4 Cross-modal Mapping The process of learning to map objects to the their word label is implemented by training a projection function fprojv→w from the visual onto the linguistic semantic space. For the learning, we use a set of Ns seen concepts for which we have both image-based visual representations Vs E RNs×dv 3http://clic.cimec.unitn.it/vsem/ 4http://wacky.sslmit.unibo.it 5We also experimented with the image- and text-based vectors of Socher et al. (2013), but achieved better performance with the reported setup. and text-based linguistic representations Ws E RNs×dw. The projection function is subject to an objective that aims at minimizing some cost function between the induced text-based representations ˆWs E RNs×dw and the gold ones Ws. The induced fprojv→w is then applied to the imagebased representations Vu E RNu×dv of Nu unseen objects to transform them into text-based representations ˆWu E RNu×dw. We implement 4 alternative learning algorithms for inducing the cross-modal projection function fprojv→w. Linear Regression (lin) Our first mo</context>
<context position="25383" citStr="Socher et al. (2013)" startWordPosition="4052" endWordPosition="4055">0, containing a total of 90 concepts. For each concept category, we treat all concepts but one as seen concepts (Table 1). The 71 seen concepts correspond to 42,600 distinct visual vectors and are used to induce the projection function. Table 2 reports results obtained by averaging the performance on the 11,400 distinct vectors of the 19 unseen concepts. Our 4 models introduced in Section 4.4 are compared to a theoretically derived baseline Chance simulating selecting a label at random. For the neural network NN, we use prior knowledge 7We also experimented with the same objective function as Socher et al. (2013), however, our objective function yielded consistently better results in all experimental settings. ˆVs that [ 1408 k M l 1 2 3 5 10 20 Mol_ Chance 1.1 2.2 3.3 5.5 11.0 22.0 SVD 1.9 5.0 8.1 14.5 29.0 48.6 CCA 3.0 6.9 10.7 17.9 31.7 51.7 lin 2.4 6.4 10.5 18.7 33.0 55.0 NN 3.9 6.6 10.6 21.9 37.9 58.2 Table 2: Percentage accuracy among top k nearest neighbors on CIFAR-100. about the number of concept categories to set the number of hidden units to 20 in order to avoid tuning of this parameter. For the SVD model, we set the number of dimensions to 300, a common choice in distributional semantics, </context>
</contexts>
<marker>Socher, Ganjoo, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Milind Ganjoo, Christopher Manning, and Andrew Ng. 2013. Zero-shot learning through cross-modal transfer. In Proceedings of NIPS, pages 935–943, Lake Tahoe, Nevada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Spelke</author>
</authors>
<title>Initial knowledge: Six suggestions.</title>
<date>1994</date>
<journal>Cognition,</journal>
<pages>50--431</pages>
<contexts>
<context position="32928" citStr="Spelke, 1994" startWordPosition="5313" endWordPosition="5314">conversation, the text of a book with an illustration of an unknown object) is very natural. Furthermore, since not all new concepts in the linguistic environment refer to new objects (they might denote abstract concepts or out-of-scene objects), it seems more reasonable for the learner to be more alerted to linguistic cues about a recently-spotted new object than vice versa. Moreover, once the learner observes a new object, she can easily construct a full visual representation for it (and the acquisition literature has shown that humans are wired for good object segmentation and recognition (Spelke, 1994)) – the more challenging task is to scan the ongoing and very ambiguous linguistic communication for contexts that might be relevant and informative about the new object. However, fast mapping is often described in the psychological literature as the opposite task: The learner is exposed to a new word in context and has to search for the right object referring to it. We implement this second setup (w → v) by training the projection function fproj,→v which maps linguistic vectors to visual ones. The adaptation of NN is straightforward; the new objective function is derived as 1 J(Θw→v) = 2(1 − </context>
</contexts>
<marker>Spelke, 1994</marker>
<rawString>Elizabeth Spelke. 1994. Initial knowledge: Six suggestions. Cognition, 50:431–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Ruslan Salakhutdinov</author>
</authors>
<title>Multimodal learning with deep boltzmann machines.</title>
<date>2012</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2231--2239</pages>
<contexts>
<context position="36218" citStr="Srivastava and Salakhutdinov, 2012" startWordPosition="5851" endWordPosition="5855">nd on a more realistic and noisier dataset covering a wide range of concepts. The neural network architecture emerged as the best performing approach, and our qualitative analysis revealed that it induced a categorical organization of concepts. Most importantly, our results suggest the viability of crossmodal mapping for grounded word-meaning acquisition in a simulation of fast mapping. Given the success of NN, we plan to experiment in the future with more sophisticated neural network architectures inspired by recent work in machine translation (Gao et al., 2013) and multimodal deep learning (Srivastava and Salakhutdinov, 2012). Furthermore, we intend to adopt visual attributes (Farhadi et al., 2009; Silberer et al., 2013) as visual representations, since they should allow a better understanding of how crossmodal mapping works, thanks to their linguistic interpretability. The error analysis in Section 5.3 suggests that automated localization techniques (van de Sande et al., 2011), distinguishing an object from its surroundings, might drastically improve mapping accuracy. Similarly, in the textual domain, models that extract collocates of a word that are more likely to denote conceptual properties (Kelly et al., 2012</context>
</contexts>
<marker>Srivastava, Salakhutdinov, 2012</marker>
<rawString>Nitish Srivastava and Ruslan Salakhutdinov. 2012. Multimodal learning with deep boltzmann machines. In Proceedings of NIPS, pages 2231–2239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Szeliski</author>
</authors>
<title>Computer Vision: Algorithms and Applications.</title>
<date>2010</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="15278" citStr="Szeliski, 2010" startWordPosition="2435" endWordPosition="2436">hings are encountered in real life, testing the potentials of cross-modal mapping in dealing with the complex scenes that one would encounter in event recognition and caption generation tasks. 1http://www.cs.cmu.edu/˜biglou/ resources/ 4.2 Visual Semantic Spaces Image-based vectors are extracted using the unsupervised bag-of-visual-words (BoVW) representational architecture (Sivic and Zisserman, 2003; Csurka et al., 2004), that has been widely and successfully applied to computer vision tasks such as object recognition and image retrieval (Yang et al., 2007). First, low-level visual features (Szeliski, 2010) are extracted from a large collection of images and clustered into a set of “visual words”. The low-level features of a specific image are then mapped to the corresponding visual words, and the image is represented by a count vector recording the number of occurrences of each visual word in it. We do not attempt any parameter tuning of the pipeline. As low-level features, we use Scale Invariant Feature Transform (SIFT) features (Lowe, 2004). SIFT features are tailored to capture object parts and to be invariant to several image transformations such as rotation, illumination and scale change. </context>
</contexts>
<marker>Szeliski, 2010</marker>
<rawString>Richard Szeliski. 2010. Computer Vision: Algorithms and Applications. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1750" citStr="Turney and Pantel, 2010" startWordPosition="255" endWordPosition="258">ay to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts. 1 Introduction Computational models of meaning that rely on corpus-extracted context vectors, such as LSA (Landauer and Dumais, 1997), HAL (Lund and Burgess, 1996), Topic Models (Griffiths et al., 2007) and more recent neural-network approaches (Collobert and Weston, 2008; Mikolov et al., 2013b) have successfully tackled a number of lexical semantics tasks, where context vector similarity highly correlates with various indices of semantic relatedness (Turney and Pantel, 2010). Given that these models are learned from naturally occurring data using simple associative techniques, various authors have advanced the claim that they might be also capturing some crucial aspects of how humans acquire and use language (Landauer and Dumais, 1997; Lenci, 2008). However, the models induce the meaning of words entirely from their co-occurrence with other words, without links to the external world. This constitutes a serious blow to claims of cognitive plausibility in at least two respects. One is the grounding problem (Harnad, 1990; Searle, 1984). Irrespective of their relativ</context>
<context position="18677" citStr="Turney and Pantel, 2010" startWordPosition="2954" endWordPosition="2957">m, skunk raccoon non-insect invertebrates crab, snail, spider, worm lobster people baby, girl, man, woman boy reptiles crocodile, dinosaur, snake, turtle lizard small mammals hamster, mouse, rabbit, shrew squirrel vehicles 1 bicycle, motorcycle, train bus vehicles 2 rocket, tank, tractor streetcar Table 1: Concepts in our version of the CIFAR-100 data set We implement the entire visual pipeline with VSEM, an open library for visual semantics (Bruni et al., 2013).3 4.3 Linguistic Semantic Spaces For constructing the text-based vectors, we follow a standard pipeline in distributional semantics (Turney and Pantel, 2010) without tuning its parameters and collect co-occurrence statistics from the concatenation of ukWaC4 and the Wikipedia, amounting to 2.7 billion tokens in total. Semantic vectors are constructed for a set of 30K target words (lemmas), namely the top 20K most frequent nouns, 5K most frequent adjectives and 5K most frequent verbs, and the same 30K lemmas are also employed as contextual elements. We collect co-occurrences in a symmetric context window of 20 elements around a target word. Finally, similarly to the visual semantic space, raw counts are transformed by applying LMI and then reduced t</context>
<context position="21952" citStr="Turney and Pantel, 2010" startWordPosition="3476" endWordPosition="3479"> that the correlation between the projections of the two multidimensional variables into a common, lower-rank space is maximized. The resulting multimodal space has been shown to provide a good approximation to human concept similarity judgments (Silberer and Lapata, 2012). In our setup, after applying CCA on the two spaces Vs and Ws, we obtain the two projection mappings onto the common space and thus our projection function can be derived as: fprojv→w = CV CW −1 (2) Singular Value Decomposition (SVD) SVD is the most widely used dimensionality reduction technique in distributional semantics (Turney and Pantel, 2010), and it has recently been exploited to combine visual and linguistic dimensions in the multimodal distributional semantic model of Bruni et al. (2014). SVD smoothing is also a way to infer values of unseen dimensions in partially incomplete matrices, a technique that has been applied to the task of inferring word tags of unannotated images (Hare et al., 2008). Assuming that the concept-representing rows of Vs and Ws are ordered in the same way, we apply the (k-truncated) SVD to the concatenated matrix [VsWs], such ˆW$] = UkΣkZTk is a k-rank approximation of the original matrix.6 The projectio</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen van de Sande</author>
<author>Jasper Uijlings</author>
<author>Theo Gevers</author>
<author>Arnold Smeulders</author>
</authors>
<title>Segmentation as selective search for object recognition.</title>
<date>2011</date>
<booktitle>In Proceedings of ICCV,</booktitle>
<pages>1879--1886</pages>
<location>Barcelona,</location>
<marker>van de Sande, Uijlings, Gevers, Smeulders, 2011</marker>
<rawString>Koen van de Sande, Jasper Uijlings, Theo Gevers, and Arnold Smeulders. 2011. Segmentation as selective search for object recognition. In Proceedings of ICCV, pages 1879–1886, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Von Ahn</author>
</authors>
<title>Games with a purpose.</title>
<date>2006</date>
<journal>Computer,</journal>
<volume>29</volume>
<issue>6</issue>
<marker>Von Ahn, 2006</marker>
<rawString>Luis Von Ahn. 2006. Games with a purpose. Computer, 29(6):92–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Yang</author>
<author>Yu-Gang Jiang</author>
<author>Alexander Hauptmann</author>
<author>Chong-Wah Ngo</author>
</authors>
<title>Evaluating bag-of-visualwords representations in scene classification.</title>
<date>2007</date>
<booktitle>Multimedia Information Retrieval,</booktitle>
<pages>197--206</pages>
<editor>In James Ze Wang, Nozha Boujemaa, Alberto Del Bimbo, and Jia Li, editors,</editor>
<publisher>ACM.</publisher>
<contexts>
<context position="15227" citStr="Yang et al., 2007" startWordPosition="2427" endWordPosition="2430">at the same time more challenging, simulation of how things are encountered in real life, testing the potentials of cross-modal mapping in dealing with the complex scenes that one would encounter in event recognition and caption generation tasks. 1http://www.cs.cmu.edu/˜biglou/ resources/ 4.2 Visual Semantic Spaces Image-based vectors are extracted using the unsupervised bag-of-visual-words (BoVW) representational architecture (Sivic and Zisserman, 2003; Csurka et al., 2004), that has been widely and successfully applied to computer vision tasks such as object recognition and image retrieval (Yang et al., 2007). First, low-level visual features (Szeliski, 2010) are extracted from a large collection of images and clustered into a set of “visual words”. The low-level features of a specific image are then mapped to the corresponding visual words, and the image is represented by a count vector recording the number of occurrences of each visual word in it. We do not attempt any parameter tuning of the pipeline. As low-level features, we use Scale Invariant Feature Transform (SIFT) features (Lowe, 2004). SIFT features are tailored to capture object parts and to be invariant to several image transformation</context>
</contexts>
<marker>Yang, Jiang, Hauptmann, Ngo, 2007</marker>
<rawString>Jun Yang, Yu-Gang Jiang, Alexander Hauptmann, and Chong-Wah Ngo. 2007. Evaluating bag-of-visualwords representations in scene classification. In James Ze Wang, Nozha Boujemaa, Alberto Del Bimbo, and Jia Li, editors, Multimedia Information Retrieval, pages 197–206. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haonan Yu</author>
<author>Jeffrey Siskind</author>
</authors>
<title>Grounded language learning from video described with sentences.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>53--63</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="8286" citStr="Yu and Siskind (2013)" startWordPosition="1312" endWordPosition="1315">ulate a fast mapping scenario, thus strengthening the claims of this approach as a full-fledged, fully inductive theory of meaning acquisition. 2 Related Work The problem of establishing word reference has been extensively explored in computational simulations of cross-situational learning (see Fazly et al. (2010) for a recent proposal and extended review of previous work). This line of research has traditionally assumed artificial models of the external world, typically a set of linguistic or logical labels for objects, actions and possibly other aspects of a scene (Siskind, 1996). Recently, Yu and Siskind (2013) presented a system that induces word-object mappings from features extracted from short videos paired with sentences. Our work complements theirs in two ways. First, unlike Yu and Siskind (2013) who considered a limited lexicon of 15 items with only 4 nouns, we conduct experiments in a large search space containing a highly ambiguous set of potential target words for every object (see Section 4.1). Most importantly, by projecting visual representations of objects into a shared semantic space, we do not limit ourselves to establishing a link between ob1404 jects and words. We induce a rich sem</context>
</contexts>
<marker>Yu, Siskind, 2013</marker>
<rawString>Haonan Yu and Jeffrey Siskind. 2013. Grounded language learning from video described with sentences. In Proceedings of ACL, pages 53–63, Sofia, Bulgaria.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>