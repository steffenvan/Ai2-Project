<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.046507">
<title confidence="0.99708">
Semi-Supervised Modeling for Prenominal Modifier Ordering
</title>
<author confidence="0.996256">
Margaret Mitchell
</author>
<affiliation confidence="0.766611">
University of Aberdeen
Aberdeen, Scotland, U.K.
</affiliation>
<email confidence="0.998848">
m.mitchell@abdn.ac.uk
</email>
<author confidence="0.976799">
Aaron Dunlop
</author>
<affiliation confidence="0.931235">
Oregon Health &amp; Science University
</affiliation>
<address confidence="0.861856">
Portland, OR
</address>
<email confidence="0.999514">
dunlopa@cslu.ogi.edu
</email>
<author confidence="0.970669">
Brian Roark
</author>
<affiliation confidence="0.925513">
Oregon Health &amp; Science University
</affiliation>
<address confidence="0.861323">
Portland, OR
</address>
<email confidence="0.999609">
roark@cslu.ogi.edu
</email>
<sectionHeader confidence="0.993923" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999776692307692">
In this paper, we argue that ordering prenom-
inal modifiers – typically pursued as a su-
pervised modeling task – is particularly well-
suited to semi-supervised approaches. By
relying on automatic parses to extract noun
phrases, we can scale up the training data
by orders of magnitude. This minimizes
the predominant issue of data sparsity that
has informed most previous approaches. We
compare several recent approaches, and find
improvements from additional training data
across the board; however, none outperform
a simple n-gram model.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995970703703704">
In any given noun phrase (NP), an arbitrary num-
ber of nominal modifiers may be used. The order of
these modifiers affects how natural or fluent a phrase
sounds. Determining a natural ordering is a key task
in the surface realization stage of a natural language
generation (NLG) system, where the adjectives and
other modifiers chosen to identify a referent must be
ordered before a final string is produced. For ex-
ample, consider the alternation between the phrases
“big red ball” and “red big ball”. The phrase “big
red ball” provides a basic ordering of the words big
and red. The reverse ordering, in “red big ball”,
sounds strange, a phrase that would only occur in
marked situations. There is no consensus on the ex-
act qualities that affect a modifier’s position, but it is
clear that some modifier orderings sound more natu-
ral than others, even if all are strictly speaking gram-
matical.
Determining methods for ordering modifiers
prenominally and investigating the factors underly-
ing modifier ordering have been areas of consider-
able research, including work in natural language236
processing (Shaw and Hatzivassiloglou, 1999; Mal-
ouf, 2000; Mitchell, 2009; Dunlop et al., 2010), lin-
guistics (Whorf, 1945; Vendler, 1968), and psychol-
ogy (Martin, 1969; Danks and Glucksberg, 1971).
A central issue in work on modifier ordering is how
to order modifiers that are unobserved during sys-
tem development. English has upwards of 200,000
words, with over 50,000 words in the vocabulary of
an educated adult (Aitchison, 2003). Up to a quar-
ter of these words may be adjectives, which poses a
significant problem for any system that attempts to
categorize English adjectives in ways that are useful
for an ordering task. Extensive in-context observa-
tion of adjectives and other modifiers is required to
adequately characterize their behavior.
Developers of automatic modifier ordering sys-
tems have thus spent considerable effort attempting
to make reliable predictions despite sparse data, and
have largely limited their systems to order modifier
pairs instead of full modifier strings. Conventional
wisdom has been that direct evidence methods such
as simple n-gram modeling are insufficient for cap-
turing such a complex and productive process.
Recent approaches have therefore utilized in-
creasingly sophisticated data-driven approaches.
Most recently, Dunlop et al. (2010) used both dis-
criminative and generative methods for estimat-
ing class-based language models with multiple-
sequence alignments (MSA). Training on manually
curated syntactic corpora, they showed excellent in-
domain performance relative to prior systems, and
decent cross-domain generalization.
However, following a purely supervised training
approach for this task is unduly limiting and leads
to conventional assumptions that are not borne out
in practice, such as the inapplicability of simple n-
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 236–241,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
gram models. NP segmentation is one of the most
reliable annotations that automatic parsers can now
produce, and may be applied to essentially arbitrary
amounts of unlabeled data. This yields orders-of-
magnitude larger training sets, so that methods that
are sensitive to sparse data and/or are domain spe-
cific can be trained on sufficient data.
In this paper, we compare an n-gram language
model and a hidden Markov model (HMM) con-
structed using expectation maximization (EM) with
several recent ordering approaches, and demonstrate
superior performance of the n-gram model across
different domains, particularly as the training data
size is scaled up. This paper presents two important
results: 1) N-gram modeling performs better than
previously believed for this task, and in fact sur-
passes current class-based systems.1 2) Automatic
parsers can effectively provide essentially unlimited
training data for learning modifier ordering prefer-
ences. Our results point the way to larger scale data-
driven approaches to this and related tasks.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999934478260869">
In one of the earliest automatic prenominal mod-
ifier ordering systems, Shaw and Hatzivassiloglou
(1999) ordered pairs of modifiers, including adjec-
tives, nouns (“baseball field”); gerunds, (“running
man”); and participles (“heated debate”). They
described a direct evidence method, a transitivity
method, and a clustering method for ordering these
different kinds of modifiers, with the transitivity
technique returning the highest accuracy of 90.67%
on a medical text. However, when testing across
domains, their accuracy dropped to 56%, not much
higher than random guessing.
Malouf (2000) continued this work, ordering
prenominal adjective pairs in the BNC. He aban-
doned a bigram model, finding it achieved only
75.57% prediction accuracy, and instead pursued
statistical and machine learning techniques that are
more robust to data sparsity. Malouf achieved an
accuracy of 91.85% by combining three systems.
However, it is not clear whether the proposed or-
dering approaches extend to other kinds of modi-
fiers, such as gerund verbs and nouns, and he did
not present analysis of cross-domain generalization.
</bodyText>
<footnote confidence="0.806501">
1But note that these approaches may still be useful, e.g.,
</footnote>
<bodyText confidence="0.501412">
when the goal is to construct general modifier classes. 237
</bodyText>
<table confidence="0.99666125">
Dataset 2 mods 3 mods 4 mods
WSJ 02-21 auto 10,070 1,333 129
WSJ 02-21 manu 9,976 1,311 129
NYT 1,616,497 191,787 18,183
</table>
<tableCaption confidence="0.997899">
Table 1: Multi-modifier noun phrases in training data
</tableCaption>
<table confidence="0.99944375">
Dataset 2 mods 3 mods 4 mods
WSJ 22-24 1,366 152 20
SWBD 1,376 143 19
Brown 1,428 101 9
</table>
<tableCaption confidence="0.999687">
Table 2: Multi-modifier noun phrases in testing data
</tableCaption>
<bodyText confidence="0.999890066666667">
Later, Mitchell (2009) focused on creating a class-
based model for modifier ordering. Her system
mapped each modifier to a class based on the fre-
quency with which it occurs in different prenominal
positions, and ordered unseen sequences based on
these classes. Dunlop et al. (2010) used a Multiple
Sequence Alignment (MSA) approach to order mod-
ifiers, achieving the highest accuracy to date across
different domains. In contrast to earlier work, both
systems order full modifier strings.
Below, we evaluate these most recent systems,
scaling up the training data by several orders of mag-
nitude. Our results indicate that an n-gram model
outperforms previous systems, and generalizes quite
well across different domains.
</bodyText>
<sectionHeader confidence="0.996973" genericHeader="method">
3 Corpora
</sectionHeader>
<bodyText confidence="0.999883851851852">
Following Dunlop et al. (2010), we use the Wall St.
Journal (WSJ), Switchboard (SWBD) and Brown
corpus sections of the Penn Treebank (Marcus et al.,
1993) as our supervised training and testing base-
lines. For semi-supervised training, we automati-
cally parse sections 02-21 of the WSJ treebank using
cross-validation methods, and scale up the amount
of data used by parsing the New York Times (NYT)
section of the Gigaword (Graff and Cieri, 2003) cor-
pus using the Berkeley Parser (Petrov and Klein,
2007; Petrov, 2010).
Table 1 lists the NP length distributions for each
training corpus. The WSJ training corpus yields just
under 5,100 distinct modifier types (without normal-
izing for capitalization), while the NYT data yields
105,364. Note that the number of NPs extracted
from the manual and automatic parses of the WSJ
are quite close. We find that the overlap between the
two groups is well over 90%, suggesting that extract-
ing NPs from a large, automatically parsed corpus
will provide phrases comparable to manually anno-
tated NPs.
We evaluate across a variety of domains, includ-
ing (1) the WSJ sections 22-24, and sections com-
mensurate in size of (2) the SWBD corpus and (3)
the Brown corpus. Table 2 lists the NP length distri-
butions for each test corpus.
</bodyText>
<sectionHeader confidence="0.999215" genericHeader="method">
4 Methods
</sectionHeader>
<bodyText confidence="0.9999534">
In this section, we present two novel prenominal
modifier ordering approaches: a 5-gram model and
an EM-trained HMM. In both systems, modifiers
that occur only once in the training data are given the
Berkeley parser OOV class labels (Petrov, 2010).
In Section 5, we compare these approaches to the
one-class system described in Mitchell (2010) and
the discriminative MSA described in Dunlop et al.
(2010). We refer the interested reader to those pa-
pers for the details of their learning algorithms.
</bodyText>
<subsectionHeader confidence="0.901386">
4.1 N-Gram Modeling
</subsectionHeader>
<bodyText confidence="0.98838004">
We used the SRILM toolkit (Stolcke, 2002) to build
unpruned 5-gram models using interpolated mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998). In the testing phase, each possible permuta-
tion is assigned a probability by the model, and the
highest probability sequence is chosen.
We explored building n-gram models based on
entire observed sequences (sentences) and on ex-
tracted multiple modifier NPs. As shown in Table
3, we found a very large (12% absolute) accuracy
improvement in a model trained with just NP se-
quences. This is likely due to several factors, in-
cluding the role of the begin string symbol &lt;s&gt;,
which helps to capture word preferences for occur-
ring first in a modifier sequence; also the behav-
ior of modifiers when they occur in NPs may dif-
fer from how they behave in other contexts. Note
that the full-sentence n-gram model performs sim-
ilarly to Malouf’s bigram model; although the re-
sults are not directly comparable, this may explain
the common impression that n-gram modeling is not
effective for modifier ordering. We find that syntac-
tic annotations are critical for this task; all n-gram
results presented in the rest of the paper are trained
on extracted NPs. 238
</bodyText>
<table confidence="0.996766">
Training data for n-gram model Accuracy
Full sentences 75.9
Extracted multi-modifier NPs 88.1
</table>
<tableCaption confidence="0.9897385">
Table 3: Modifier ordering accuracy on WSJ sections 22-
24, trained on sections 2-21
</tableCaption>
<subsectionHeader confidence="0.943289">
4.2 Hidden Markov Model
</subsectionHeader>
<bodyText confidence="0.9998483">
Mitchell’s single-class system and Dunlop et. al’s
MSA approach both group tokens into position clus-
ters. The success of these systems suggests that a
position-specific class-based HMM might perform
well on this task. We use EM (Dempster et al., 1977)
to learn the parameterizations of such an HMM.
The model is defined in terms of state transition
probabilities P(c�  |c), i.e., the probability of transi-
tioning from a state labeled c to a state labeled c�;
and state observation probabilities P(w  |c), i.e.,
the probability of emitting word w from a particu-
lar class c. Since the classes are predicting an or-
dering, we include hard constraints on class tran-
sitions. Specifically, we forbid a transition from a
class closer to the head noun to one farther away.
More formally, if the subscript of a class indicates
its distance from the head, then for any i, j, P(cz |
ci) = 0 if i &gt; j; i.e., cz is stipulated to never occur
closer to the head than ci.
We established 8 classes and an HMM Markov
order of 1 (along with start and end states) based
on performance on a held-out set (section 00 of the
WSJ treebank). We initialize the model with a uni-
form distribution over allowed transition and emis-
sion probabilities, and use add-S regularization in
the M-step of EM at each iteration. We empirically
determined S smoothing values of 0.1 for emissions
and 500 for transitions. Rather than training to full
convergence of the corpus likelihood, we stop train-
ing when there is no improvement in ordering accu-
racy on the held-out dataset for five iterations, and
output the best scoring model.
Because of the constraints on transition probabil-
ities, straightforward application of EM leads to the
transition probabilities strongly skewing the learn-
ing of emission probabilities. We thus followed a
generalized EM procedure (Neal and Hinton, 1998),
updating only emission probabilities until no more
improvement is achieved, and then training both
emission and transition probabilities. Often, we
</bodyText>
<table confidence="0.999597555555555">
Training data Ngr WSJ Accuracy MSA Ngr SWBD Accuracy MSA Ngr Brown Accuracy MSA
1-cl HMM 1-cl HMM 1-cl HMM
WSJ manual 88.1 65.7 87.1 87.1 72.9 44.7 71.3 71.8 67.1 31.9 69.2 71.5
auto 87.8 64.6 86.7 87.2 72.5 41.6 71.5 71.9 67.4 31.3 69.4 70.6
NYT 10% 90.3 75.3 87.4 88.2 84.2 71.1 81.8 83.2 81.7 62.1 79.5 80.4
20% 91.8 77.2 87.9 89.3 85.2 72.2 80.9 83.1 82.2 65.9 78.9 82.1
50% 92.3 78.9 89.7 90.7 86.3 73.5 82.2 83.9 83.1 67.8 80.2 81.6
all 92.4 80.2 89.3 92.1 86.4 74.5 81.4 83.4 82.3 69.3 79.3 82.0
NYT+WSJ auto 93.7 81.1 89.7 92.2 86.3 74.5 81.3 83.4 82.3 69.3 79.3 81.8
</table>
<tableCaption confidence="0.99766">
Table 4: Results on WSJ sections 22-24, Switchboard test set, and Brown test set for n-gram model (Ngr), Mitchell’s
single-class system (1-cl), HMM and MSA systems, under various training conditions.
</tableCaption>
<bodyText confidence="0.956704333333333">
find no improvement with the inclusion of transition
probabilities, and they are left uniform. In this case,
test ordering is determined by the class label alone.
</bodyText>
<sectionHeader confidence="0.995769" genericHeader="method">
5 Empirical results
</sectionHeader>
<bodyText confidence="0.9999521875">
Several measures have been used to evaluate the
accuracy of a system’s modifier ordering, includ-
ing both type/token accuracy, pairwise accuracy, and
full string accuracy. We evaluate full string ordering
accuracy over all tokens in the evaluation set. For
every NP, if the model’s highest-scoring ordering is
identical to the actual observed order, it is correct;
otherwise, it is incorrect. We report the percentage
of orders correctly predicted.
We evaluate under a variety of training conditions,
on WSJ sections 22-24, as well as the testing sec-
tions from the Switchboard and Brown corpus por-
tions of the Penn Treebank. We perform no domain-
specific tuning, so the results on the Switchboard
and Brown corpora demonstrate cross-domain appli-
cability of the approaches.
</bodyText>
<subsectionHeader confidence="0.99947">
5.1 Manual parses versus automatic parses
</subsectionHeader>
<bodyText confidence="0.9995265">
We begin by comparing the NPs extracted from
manual parses to those extracted from automatic
parses. We parsed Wall Street Journal sections 02
through 21 using cross-validation to ensure that the
parses are as errorful as when sentences have never
been observed by training.
Table 4 compares models trained on these two
training corpora, as evaluated on the manually-
annotated test set. No system’s accuracy degrades
greatly when using automatic parses, indicating that
we can likely derive useful training data by automat-
ically parsing a large, unlabeled training corpus. 239
</bodyText>
<subsectionHeader confidence="0.99839">
5.2 Semi-supervised models
</subsectionHeader>
<bodyText confidence="0.999991594594595">
We now evaluate performance of the models on the
scaled up training data. Using the Berkeley parser,
we parsed 169 million words of NYT text from the
English Gigaword corpus (Graff and Cieri, 2003),
extracted the multiple modifier NPs, and trained our
various models on this data. Rows 3-6 of Table
4 show the accuracy on WSJ sections 22-24 after
training on 10%, 20%, 50% and 100% of this data.
Note that this represents approximately 150 times
the amount of training data as the original treebank
training data. Even with just 10% of this data (a
15-fold increase in the training data), we see across
the board improvements. Using all of the NYT data
results in approximately 5% absolute performance
increase for the n-gram and MSA models, yielding
roughly commensurate performance, over 92% ac-
curacy. Although we do not have space to present
the results in this paper, we found further improve-
ments (over 1% absolute, statistically significant) by
combining the four models, indicating a continued
benefit of the other models, even if none of them
best the n-gram individually.
Based on these results, this task is clearly
amenable to semi-supervised learning approaches.
All systems show large accuracy improvements.
Further, contrary to conventional wisdom, n-gram
models are very competitive with recent high-
accuracy frameworks. Additionally, n-gram models
appear to be domain sensitive, as evidenced by the
last row of Table 4, which presents results when the
1.8 million NPs in the NYT corpus are augmented
with just 11 thousand NPs from the WSJ (auto) col-
lection. The n-gram model still outperforms the
other systems, but improves by well over a percent,
while the class-based HMM and MSA approaches
are relatively static. (The single-class system shows
some domain sensitivity, improving nearly a point.)
</bodyText>
<subsectionHeader confidence="0.991251">
5.3 Cross-domain evaluation
</subsectionHeader>
<bodyText confidence="0.999963842105263">
With respect to cross-domain applicability, we see
that, as with the WSJ evaluation, the MSA and n-
gram approaches are roughly commensurate on the
Brown corpus; but the n-gram model shows a greater
advantage on the Switchboard test set when trained
on the NYT data. Perhaps this is due to higher re-
liance on conventionalized collocations in the spo-
ken language of Switchboard. Finally, it is clear
that the addition of the WSJ data to the NYT data
yields improvements only for the specific newswire
domain — none of the results change much for these
two new domains when the WSJ data is included
(last row of the table).
We note that the improvements observed when
scaling the training corpus with in-domain data per-
sist when applied to very diverse domains. Interest-
ingly, n-gram models, which may have been consid-
ered unlikely to generalize well to other domains,
maintain their superior performance in each trial.
</bodyText>
<sectionHeader confidence="0.999753" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999975133333333">
In this paper, we demonstrated the efficacy of scal-
ing up training data for prenominal modifier or-
dering using automatic parses. We presented two
novel systems for ordering prenominal modifiers,
and demonstrated that with sufficient data, a simple
n-gram model outperforms position-specific models,
such as an EM-trained HMM and the MSA approach
of Dunlop et al. (2010). The accuracy achieved by
the n-gram model is particularly interesting, since
such models have previously been considered inef-
fective for this task. This does not obviate the need
for a class based model—modifier classes may in-
form linguistic research, and system combination
still yields large improvements —but points to new
data-rich methods for learning such models.
</bodyText>
<sectionHeader confidence="0.998298" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9989905">
This research was supported in part by NSF Grant
#IIS-0811745 and DARPA grant #HR0011-09-1-
0041. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA.
</bodyText>
<page confidence="0.987188">
240
</page>
<sectionHeader confidence="0.990058" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999536231884058">
Jean Aitchison. 2003. Words in the mind: an intro-
duction to the mental lexicon. Blackwell Publishing,
Cornwall, United Kindgom, third edition. p. 7.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report, TR-10-98, Harvard University.
Joseph H. Danks and Sam Glucksberg. 1971. Psycho-
logical scaling of adjective order. Journal of Verbal
Learning and Verbal Behavior, 10(1):63–67.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society: Se-
ries B, 39(1):1–38.
Aaron Dunlop, Margaret Mitchell, and Brian Roark.
2010. Prenominal modier ordering via multiple se-
quence alignment. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the ACL (HLT-NAACL 2010), pages 600–
608, Los Angeles, CA, USA. Association for Compu-
tational Linguistics.
David Graff and Christopher Cieri. 2003. English Giga-
word. Linguistic Data Consortium, Philadelphia, PA,
USA.
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In Proceedings of
the 38th ACL (ACL 2000), pages 85–92, Hong Kong.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
J. E. Martin. 1969. Semantic determinants of preferred
adjective order. Journal of Verbal Learning and Verbal
Behavior, 8(6):697–704.
Margaret Mitchell. 2009. Class-based ordering of
prenominal modifiers. In Proceedings of the 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG 2009), pages 50–57, Athens, Greece. Associa-
tion for Computational Linguistics.
Margaret Mitchell. 2010. A flexible approach to class-
based ordering of prenominal modifiers. In E. Krah-
mer and M. Theune, editors, Empirical Methods in
Natural Language Generation, volume 5980 of Lec-
ture Notes in Computer Science. Springer, Berlin /
Heidelberg.
Radford M. Neal and Geoffrey E. Hinton. 1998. A view
of the EM algorithm that justifies incremental, sparse,
and other variants. In Michael I. Jordan, editor, Learn-
ing in Graphical Models. Kluwer Academic Publish-
ers, Dordrecht.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the ACL (HLT-NAACL 2007), pages 404–
411, Rochester, NY, USA. Association for Computa-
tional Linguistics.
Slav Petrov. 2010. Berkeley parser. GNU General Pub-
lic License v.2.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In Proceedings of the 37th
ACL (ACL 1999), pages 135–143, College Park, Mary-
land. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP
2002), volume 2, pages 901–904.
Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton, The Netherlands.
Benjamin Lee Whorf. 1945. Grammatical categories.
Language, 21(1):1–11.
</reference>
<page confidence="0.998101">
241
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.511612">
<title confidence="0.998884">Semi-Supervised Modeling for Prenominal Modifier Ordering</title>
<author confidence="0.999745">Margaret Mitchell</author>
<affiliation confidence="0.809468">University of Aberdeen, Scotland, U.K.</affiliation>
<email confidence="0.99219">m.mitchell@abdn.ac.uk</email>
<author confidence="0.999277">Aaron Dunlop</author>
<affiliation confidence="0.999517">Oregon Health &amp; Science</affiliation>
<address confidence="0.933351">Portland, OR</address>
<email confidence="0.998936">dunlopa@cslu.ogi.edu</email>
<author confidence="0.999962">Brian Roark</author>
<affiliation confidence="0.999012">Oregon Health &amp; Science</affiliation>
<address confidence="0.908972">Portland, OR</address>
<email confidence="0.999646">roark@cslu.ogi.edu</email>
<abstract confidence="0.999179928571429">In this paper, we argue that ordering prenominal modifiers – typically pursued as a supervised modeling task – is particularly wellsuited to semi-supervised approaches. By relying on automatic parses to extract noun phrases, we can scale up the training data by orders of magnitude. This minimizes the predominant issue of data sparsity that has informed most previous approaches. We compare several recent approaches, and find improvements from additional training data across the board; however, none outperform a simple n-gram model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jean Aitchison</author>
</authors>
<title>Words in the mind: an introduction to the mental lexicon.</title>
<date>2003</date>
<pages>7</pages>
<publisher>Blackwell Publishing,</publisher>
<location>Cornwall, United</location>
<contexts>
<context position="2389" citStr="Aitchison, 2003" startWordPosition="367" endWordPosition="368">methods for ordering modifiers prenominally and investigating the factors underlying modifier ordering have been areas of considerable research, including work in natural language236 processing (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009; Dunlop et al., 2010), linguistics (Whorf, 1945; Vendler, 1968), and psychology (Martin, 1969; Danks and Glucksberg, 1971). A central issue in work on modifier ordering is how to order modifiers that are unobserved during system development. English has upwards of 200,000 words, with over 50,000 words in the vocabulary of an educated adult (Aitchison, 2003). Up to a quarter of these words may be adjectives, which poses a significant problem for any system that attempts to categorize English adjectives in ways that are useful for an ordering task. Extensive in-context observation of adjectives and other modifiers is required to adequately characterize their behavior. Developers of automatic modifier ordering systems have thus spent considerable effort attempting to make reliable predictions despite sparse data, and have largely limited their systems to order modifier pairs instead of full modifier strings. Conventional wisdom has been that direct</context>
</contexts>
<marker>Aitchison, 2003</marker>
<rawString>Jean Aitchison. 2003. Words in the mind: an introduction to the mental lexicon. Blackwell Publishing, Cornwall, United Kindgom, third edition. p. 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report, TR-10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="9196" citStr="Chen and Goodman, 1998" startWordPosition="1423" endWordPosition="1426">minal modifier ordering approaches: a 5-gram model and an EM-trained HMM. In both systems, modifiers that occur only once in the training data are given the Berkeley parser OOV class labels (Petrov, 2010). In Section 5, we compare these approaches to the one-class system described in Mitchell (2010) and the discriminative MSA described in Dunlop et al. (2010). We refer the interested reader to those papers for the details of their learning algorithms. 4.1 N-Gram Modeling We used the SRILM toolkit (Stolcke, 2002) to build unpruned 5-gram models using interpolated modified Kneser-Ney smoothing (Chen and Goodman, 1998). In the testing phase, each possible permutation is assigned a probability by the model, and the highest probability sequence is chosen. We explored building n-gram models based on entire observed sequences (sentences) and on extracted multiple modifier NPs. As shown in Table 3, we found a very large (12% absolute) accuracy improvement in a model trained with just NP sequences. This is likely due to several factors, including the role of the begin string symbol &lt;s&gt;, which helps to capture word preferences for occurring first in a modifier sequence; also the behavior of modifiers when they occ</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report, TR-10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph H Danks</author>
<author>Sam Glucksberg</author>
</authors>
<title>Psychological scaling of adjective order.</title>
<date>1971</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="2152" citStr="Danks and Glucksberg, 1971" startWordPosition="326" endWordPosition="329">nly occur in marked situations. There is no consensus on the exact qualities that affect a modifier’s position, but it is clear that some modifier orderings sound more natural than others, even if all are strictly speaking grammatical. Determining methods for ordering modifiers prenominally and investigating the factors underlying modifier ordering have been areas of considerable research, including work in natural language236 processing (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009; Dunlop et al., 2010), linguistics (Whorf, 1945; Vendler, 1968), and psychology (Martin, 1969; Danks and Glucksberg, 1971). A central issue in work on modifier ordering is how to order modifiers that are unobserved during system development. English has upwards of 200,000 words, with over 50,000 words in the vocabulary of an educated adult (Aitchison, 2003). Up to a quarter of these words may be adjectives, which poses a significant problem for any system that attempts to categorize English adjectives in ways that are useful for an ordering task. Extensive in-context observation of adjectives and other modifiers is required to adequately characterize their behavior. Developers of automatic modifier ordering syste</context>
</contexts>
<marker>Danks, Glucksberg, 1971</marker>
<rawString>Joseph H. Danks and Sam Glucksberg. 1971. Psychological scaling of adjective order. Journal of Verbal Learning and Verbal Behavior, 10(1):63–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Dempster</author>
<author>Nan Laird</author>
<author>Donald Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society: Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="10698" citStr="Dempster et al., 1977" startWordPosition="1672" endWordPosition="1675">difier ordering. We find that syntactic annotations are critical for this task; all n-gram results presented in the rest of the paper are trained on extracted NPs. 238 Training data for n-gram model Accuracy Full sentences 75.9 Extracted multi-modifier NPs 88.1 Table 3: Modifier ordering accuracy on WSJ sections 22- 24, trained on sections 2-21 4.2 Hidden Markov Model Mitchell’s single-class system and Dunlop et. al’s MSA approach both group tokens into position clusters. The success of these systems suggests that a position-specific class-based HMM might perform well on this task. We use EM (Dempster et al., 1977) to learn the parameterizations of such an HMM. The model is defined in terms of state transition probabilities P(c� |c), i.e., the probability of transitioning from a state labeled c to a state labeled c�; and state observation probabilities P(w |c), i.e., the probability of emitting word w from a particular class c. Since the classes are predicting an ordering, we include hard constraints on class transitions. Specifically, we forbid a transition from a class closer to the head noun to one farther away. More formally, if the subscript of a class indicates its distance from the head, then for</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur Dempster, Nan Laird, and Donald Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Dunlop</author>
<author>Margaret Mitchell</author>
<author>Brian Roark</author>
</authors>
<title>Prenominal modier ordering via multiple sequence alignment.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL (HLT-NAACL 2010),</booktitle>
<pages>600--608</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, CA, USA.</location>
<contexts>
<context position="2051" citStr="Dunlop et al., 2010" startWordPosition="311" endWordPosition="314">ds big and red. The reverse ordering, in “red big ball”, sounds strange, a phrase that would only occur in marked situations. There is no consensus on the exact qualities that affect a modifier’s position, but it is clear that some modifier orderings sound more natural than others, even if all are strictly speaking grammatical. Determining methods for ordering modifiers prenominally and investigating the factors underlying modifier ordering have been areas of considerable research, including work in natural language236 processing (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009; Dunlop et al., 2010), linguistics (Whorf, 1945; Vendler, 1968), and psychology (Martin, 1969; Danks and Glucksberg, 1971). A central issue in work on modifier ordering is how to order modifiers that are unobserved during system development. English has upwards of 200,000 words, with over 50,000 words in the vocabulary of an educated adult (Aitchison, 2003). Up to a quarter of these words may be adjectives, which poses a significant problem for any system that attempts to categorize English adjectives in ways that are useful for an ordering task. Extensive in-context observation of adjectives and other modifiers i</context>
<context position="6808" citStr="Dunlop et al. (2010)" startWordPosition="1035" endWordPosition="1038">l modifier classes. 237 Dataset 2 mods 3 mods 4 mods WSJ 02-21 auto 10,070 1,333 129 WSJ 02-21 manu 9,976 1,311 129 NYT 1,616,497 191,787 18,183 Table 1: Multi-modifier noun phrases in training data Dataset 2 mods 3 mods 4 mods WSJ 22-24 1,366 152 20 SWBD 1,376 143 19 Brown 1,428 101 9 Table 2: Multi-modifier noun phrases in testing data Later, Mitchell (2009) focused on creating a classbased model for modifier ordering. Her system mapped each modifier to a class based on the frequency with which it occurs in different prenominal positions, and ordered unseen sequences based on these classes. Dunlop et al. (2010) used a Multiple Sequence Alignment (MSA) approach to order modifiers, achieving the highest accuracy to date across different domains. In contrast to earlier work, both systems order full modifier strings. Below, we evaluate these most recent systems, scaling up the training data by several orders of magnitude. Our results indicate that an n-gram model outperforms previous systems, and generalizes quite well across different domains. 3 Corpora Following Dunlop et al. (2010), we use the Wall St. Journal (WSJ), Switchboard (SWBD) and Brown corpus sections of the Penn Treebank (Marcus et al., 19</context>
<context position="8934" citStr="Dunlop et al. (2010)" startWordPosition="1382" endWordPosition="1385"> variety of domains, including (1) the WSJ sections 22-24, and sections commensurate in size of (2) the SWBD corpus and (3) the Brown corpus. Table 2 lists the NP length distributions for each test corpus. 4 Methods In this section, we present two novel prenominal modifier ordering approaches: a 5-gram model and an EM-trained HMM. In both systems, modifiers that occur only once in the training data are given the Berkeley parser OOV class labels (Petrov, 2010). In Section 5, we compare these approaches to the one-class system described in Mitchell (2010) and the discriminative MSA described in Dunlop et al. (2010). We refer the interested reader to those papers for the details of their learning algorithms. 4.1 N-Gram Modeling We used the SRILM toolkit (Stolcke, 2002) to build unpruned 5-gram models using interpolated modified Kneser-Ney smoothing (Chen and Goodman, 1998). In the testing phase, each possible permutation is assigned a probability by the model, and the highest probability sequence is chosen. We explored building n-gram models based on entire observed sequences (sentences) and on extracted multiple modifier NPs. As shown in Table 3, we found a very large (12% absolute) accuracy improvement</context>
<context position="17956" citStr="Dunlop et al. (2010)" startWordPosition="2867" endWordPosition="2870">aining corpus with in-domain data persist when applied to very diverse domains. Interestingly, n-gram models, which may have been considered unlikely to generalize well to other domains, maintain their superior performance in each trial. 6 Discussion In this paper, we demonstrated the efficacy of scaling up training data for prenominal modifier ordering using automatic parses. We presented two novel systems for ordering prenominal modifiers, and demonstrated that with sufficient data, a simple n-gram model outperforms position-specific models, such as an EM-trained HMM and the MSA approach of Dunlop et al. (2010). The accuracy achieved by the n-gram model is particularly interesting, since such models have previously been considered ineffective for this task. This does not obviate the need for a class based model—modifier classes may inform linguistic research, and system combination still yields large improvements —but points to new data-rich methods for learning such models. Acknowledgments This research was supported in part by NSF Grant #IIS-0811745 and DARPA grant #HR0011-09-1- 0041. Any opinions, findings, conclusions or recommendations expressed in this publication are those of the authors and </context>
</contexts>
<marker>Dunlop, Mitchell, Roark, 2010</marker>
<rawString>Aaron Dunlop, Margaret Mitchell, and Brian Roark. 2010. Prenominal modier ordering via multiple sequence alignment. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL (HLT-NAACL 2010), pages 600– 608, Los Angeles, CA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Christopher Cieri</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="7702" citStr="Graff and Cieri, 2003" startWordPosition="1176" endWordPosition="1179">ning data by several orders of magnitude. Our results indicate that an n-gram model outperforms previous systems, and generalizes quite well across different domains. 3 Corpora Following Dunlop et al. (2010), we use the Wall St. Journal (WSJ), Switchboard (SWBD) and Brown corpus sections of the Penn Treebank (Marcus et al., 1993) as our supervised training and testing baselines. For semi-supervised training, we automatically parse sections 02-21 of the WSJ treebank using cross-validation methods, and scale up the amount of data used by parsing the New York Times (NYT) section of the Gigaword (Graff and Cieri, 2003) corpus using the Berkeley Parser (Petrov and Klein, 2007; Petrov, 2010). Table 1 lists the NP length distributions for each training corpus. The WSJ training corpus yields just under 5,100 distinct modifier types (without normalizing for capitalization), while the NYT data yields 105,364. Note that the number of NPs extracted from the manual and automatic parses of the WSJ are quite close. We find that the overlap between the two groups is well over 90%, suggesting that extracting NPs from a large, automatically parsed corpus will provide phrases comparable to manually annotated NPs. We evalu</context>
<context position="15012" citStr="Graff and Cieri, 2003" startWordPosition="2392" endWordPosition="2395">o ensure that the parses are as errorful as when sentences have never been observed by training. Table 4 compares models trained on these two training corpora, as evaluated on the manuallyannotated test set. No system’s accuracy degrades greatly when using automatic parses, indicating that we can likely derive useful training data by automatically parsing a large, unlabeled training corpus. 239 5.2 Semi-supervised models We now evaluate performance of the models on the scaled up training data. Using the Berkeley parser, we parsed 169 million words of NYT text from the English Gigaword corpus (Graff and Cieri, 2003), extracted the multiple modifier NPs, and trained our various models on this data. Rows 3-6 of Table 4 show the accuracy on WSJ sections 22-24 after training on 10%, 20%, 50% and 100% of this data. Note that this represents approximately 150 times the amount of training data as the original treebank training data. Even with just 10% of this data (a 15-fold increase in the training data), we see across the board improvements. Using all of the NYT data results in approximately 5% absolute performance increase for the n-gram and MSA models, yielding roughly commensurate performance, over 92% acc</context>
</contexts>
<marker>Graff, Cieri, 2003</marker>
<rawString>David Graff and Christopher Cieri. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>The order of prenominal adjectives in natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th ACL (ACL</booktitle>
<pages>85--92</pages>
<location>Hong Kong.</location>
<contexts>
<context position="2013" citStr="Malouf, 2000" startWordPosition="306" endWordPosition="308">es a basic ordering of the words big and red. The reverse ordering, in “red big ball”, sounds strange, a phrase that would only occur in marked situations. There is no consensus on the exact qualities that affect a modifier’s position, but it is clear that some modifier orderings sound more natural than others, even if all are strictly speaking grammatical. Determining methods for ordering modifiers prenominally and investigating the factors underlying modifier ordering have been areas of considerable research, including work in natural language236 processing (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009; Dunlop et al., 2010), linguistics (Whorf, 1945; Vendler, 1968), and psychology (Martin, 1969; Danks and Glucksberg, 1971). A central issue in work on modifier ordering is how to order modifiers that are unobserved during system development. English has upwards of 200,000 words, with over 50,000 words in the vocabulary of an educated adult (Aitchison, 2003). Up to a quarter of these words may be adjectives, which poses a significant problem for any system that attempts to categorize English adjectives in ways that are useful for an ordering task. Extensive in-context observati</context>
<context position="5574" citStr="Malouf (2000)" startWordPosition="831" endWordPosition="832"> tasks. 2 Related Work In one of the earliest automatic prenominal modifier ordering systems, Shaw and Hatzivassiloglou (1999) ordered pairs of modifiers, including adjectives, nouns (“baseball field”); gerunds, (“running man”); and participles (“heated debate”). They described a direct evidence method, a transitivity method, and a clustering method for ordering these different kinds of modifiers, with the transitivity technique returning the highest accuracy of 90.67% on a medical text. However, when testing across domains, their accuracy dropped to 56%, not much higher than random guessing. Malouf (2000) continued this work, ordering prenominal adjective pairs in the BNC. He abandoned a bigram model, finding it achieved only 75.57% prediction accuracy, and instead pursued statistical and machine learning techniques that are more robust to data sparsity. Malouf achieved an accuracy of 91.85% by combining three systems. However, it is not clear whether the proposed ordering approaches extend to other kinds of modifiers, such as gerund verbs and nouns, and he did not present analysis of cross-domain generalization. 1But note that these approaches may still be useful, e.g., when the goal is to co</context>
</contexts>
<marker>Malouf, 2000</marker>
<rawString>Robert Malouf. 2000. The order of prenominal adjectives in natural language generation. In Proceedings of the 38th ACL (ACL 2000), pages 85–92, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="7411" citStr="Marcus et al., 1993" startWordPosition="1129" endWordPosition="1132">lop et al. (2010) used a Multiple Sequence Alignment (MSA) approach to order modifiers, achieving the highest accuracy to date across different domains. In contrast to earlier work, both systems order full modifier strings. Below, we evaluate these most recent systems, scaling up the training data by several orders of magnitude. Our results indicate that an n-gram model outperforms previous systems, and generalizes quite well across different domains. 3 Corpora Following Dunlop et al. (2010), we use the Wall St. Journal (WSJ), Switchboard (SWBD) and Brown corpus sections of the Penn Treebank (Marcus et al., 1993) as our supervised training and testing baselines. For semi-supervised training, we automatically parse sections 02-21 of the WSJ treebank using cross-validation methods, and scale up the amount of data used by parsing the New York Times (NYT) section of the Gigaword (Graff and Cieri, 2003) corpus using the Berkeley Parser (Petrov and Klein, 2007; Petrov, 2010). Table 1 lists the NP length distributions for each training corpus. The WSJ training corpus yields just under 5,100 distinct modifier types (without normalizing for capitalization), while the NYT data yields 105,364. Note that the numb</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Martin</author>
</authors>
<title>Semantic determinants of preferred adjective order.</title>
<date>1969</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<volume>8</volume>
<issue>6</issue>
<contexts>
<context position="2123" citStr="Martin, 1969" startWordPosition="324" endWordPosition="325">e that would only occur in marked situations. There is no consensus on the exact qualities that affect a modifier’s position, but it is clear that some modifier orderings sound more natural than others, even if all are strictly speaking grammatical. Determining methods for ordering modifiers prenominally and investigating the factors underlying modifier ordering have been areas of considerable research, including work in natural language236 processing (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009; Dunlop et al., 2010), linguistics (Whorf, 1945; Vendler, 1968), and psychology (Martin, 1969; Danks and Glucksberg, 1971). A central issue in work on modifier ordering is how to order modifiers that are unobserved during system development. English has upwards of 200,000 words, with over 50,000 words in the vocabulary of an educated adult (Aitchison, 2003). Up to a quarter of these words may be adjectives, which poses a significant problem for any system that attempts to categorize English adjectives in ways that are useful for an ordering task. Extensive in-context observation of adjectives and other modifiers is required to adequately characterize their behavior. Developers of auto</context>
</contexts>
<marker>Martin, 1969</marker>
<rawString>J. E. Martin. 1969. Semantic determinants of preferred adjective order. Journal of Verbal Learning and Verbal Behavior, 8(6):697–704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
</authors>
<title>Class-based ordering of prenominal modifiers.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG</booktitle>
<pages>50--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece.</location>
<contexts>
<context position="2029" citStr="Mitchell, 2009" startWordPosition="309" endWordPosition="310">ering of the words big and red. The reverse ordering, in “red big ball”, sounds strange, a phrase that would only occur in marked situations. There is no consensus on the exact qualities that affect a modifier’s position, but it is clear that some modifier orderings sound more natural than others, even if all are strictly speaking grammatical. Determining methods for ordering modifiers prenominally and investigating the factors underlying modifier ordering have been areas of considerable research, including work in natural language236 processing (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009; Dunlop et al., 2010), linguistics (Whorf, 1945; Vendler, 1968), and psychology (Martin, 1969; Danks and Glucksberg, 1971). A central issue in work on modifier ordering is how to order modifiers that are unobserved during system development. English has upwards of 200,000 words, with over 50,000 words in the vocabulary of an educated adult (Aitchison, 2003). Up to a quarter of these words may be adjectives, which poses a significant problem for any system that attempts to categorize English adjectives in ways that are useful for an ordering task. Extensive in-context observation of adjectives</context>
<context position="6550" citStr="Mitchell (2009)" startWordPosition="994" endWordPosition="995">posed ordering approaches extend to other kinds of modifiers, such as gerund verbs and nouns, and he did not present analysis of cross-domain generalization. 1But note that these approaches may still be useful, e.g., when the goal is to construct general modifier classes. 237 Dataset 2 mods 3 mods 4 mods WSJ 02-21 auto 10,070 1,333 129 WSJ 02-21 manu 9,976 1,311 129 NYT 1,616,497 191,787 18,183 Table 1: Multi-modifier noun phrases in training data Dataset 2 mods 3 mods 4 mods WSJ 22-24 1,366 152 20 SWBD 1,376 143 19 Brown 1,428 101 9 Table 2: Multi-modifier noun phrases in testing data Later, Mitchell (2009) focused on creating a classbased model for modifier ordering. Her system mapped each modifier to a class based on the frequency with which it occurs in different prenominal positions, and ordered unseen sequences based on these classes. Dunlop et al. (2010) used a Multiple Sequence Alignment (MSA) approach to order modifiers, achieving the highest accuracy to date across different domains. In contrast to earlier work, both systems order full modifier strings. Below, we evaluate these most recent systems, scaling up the training data by several orders of magnitude. Our results indicate that an</context>
</contexts>
<marker>Mitchell, 2009</marker>
<rawString>Margaret Mitchell. 2009. Class-based ordering of prenominal modifiers. In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG 2009), pages 50–57, Athens, Greece. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
</authors>
<title>A flexible approach to classbased ordering of prenominal modifiers.</title>
<date>2010</date>
<booktitle>Empirical Methods in Natural Language Generation,</booktitle>
<volume>5980</volume>
<editor>In E. Krahmer and M. Theune, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="8873" citStr="Mitchell (2010)" startWordPosition="1374" endWordPosition="1375">mparable to manually annotated NPs. We evaluate across a variety of domains, including (1) the WSJ sections 22-24, and sections commensurate in size of (2) the SWBD corpus and (3) the Brown corpus. Table 2 lists the NP length distributions for each test corpus. 4 Methods In this section, we present two novel prenominal modifier ordering approaches: a 5-gram model and an EM-trained HMM. In both systems, modifiers that occur only once in the training data are given the Berkeley parser OOV class labels (Petrov, 2010). In Section 5, we compare these approaches to the one-class system described in Mitchell (2010) and the discriminative MSA described in Dunlop et al. (2010). We refer the interested reader to those papers for the details of their learning algorithms. 4.1 N-Gram Modeling We used the SRILM toolkit (Stolcke, 2002) to build unpruned 5-gram models using interpolated modified Kneser-Ney smoothing (Chen and Goodman, 1998). In the testing phase, each possible permutation is assigned a probability by the model, and the highest probability sequence is chosen. We explored building n-gram models based on entire observed sequences (sentences) and on extracted multiple modifier NPs. As shown in Table</context>
</contexts>
<marker>Mitchell, 2010</marker>
<rawString>Margaret Mitchell. 2010. A flexible approach to classbased ordering of prenominal modifiers. In E. Krahmer and M. Theune, editors, Empirical Methods in Natural Language Generation, volume 5980 of Lecture Notes in Computer Science. Springer, Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1998</date>
<booktitle>Learning in Graphical Models.</booktitle>
<editor>In Michael I. Jordan, editor,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="12290" citStr="Neal and Hinton, 1998" startWordPosition="1945" endWordPosition="1948"> use add-S regularization in the M-step of EM at each iteration. We empirically determined S smoothing values of 0.1 for emissions and 500 for transitions. Rather than training to full convergence of the corpus likelihood, we stop training when there is no improvement in ordering accuracy on the held-out dataset for five iterations, and output the best scoring model. Because of the constraints on transition probabilities, straightforward application of EM leads to the transition probabilities strongly skewing the learning of emission probabilities. We thus followed a generalized EM procedure (Neal and Hinton, 1998), updating only emission probabilities until no more improvement is achieved, and then training both emission and transition probabilities. Often, we Training data Ngr WSJ Accuracy MSA Ngr SWBD Accuracy MSA Ngr Brown Accuracy MSA 1-cl HMM 1-cl HMM 1-cl HMM WSJ manual 88.1 65.7 87.1 87.1 72.9 44.7 71.3 71.8 67.1 31.9 69.2 71.5 auto 87.8 64.6 86.7 87.2 72.5 41.6 71.5 71.9 67.4 31.3 69.4 70.6 NYT 10% 90.3 75.3 87.4 88.2 84.2 71.1 81.8 83.2 81.7 62.1 79.5 80.4 20% 91.8 77.2 87.9 89.3 85.2 72.2 80.9 83.1 82.2 65.9 78.9 82.1 50% 92.3 78.9 89.7 90.7 86.3 73.5 82.2 83.9 83.1 67.8 80.2 81.6 all 92.4 80</context>
</contexts>
<marker>Neal, Hinton, 1998</marker>
<rawString>Radford M. Neal and Geoffrey E. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Michael I. Jordan, editor, Learning in Graphical Models. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the ACL (HLT-NAACL</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, NY, USA.</location>
<contexts>
<context position="7759" citStr="Petrov and Klein, 2007" startWordPosition="1186" endWordPosition="1189">icate that an n-gram model outperforms previous systems, and generalizes quite well across different domains. 3 Corpora Following Dunlop et al. (2010), we use the Wall St. Journal (WSJ), Switchboard (SWBD) and Brown corpus sections of the Penn Treebank (Marcus et al., 1993) as our supervised training and testing baselines. For semi-supervised training, we automatically parse sections 02-21 of the WSJ treebank using cross-validation methods, and scale up the amount of data used by parsing the New York Times (NYT) section of the Gigaword (Graff and Cieri, 2003) corpus using the Berkeley Parser (Petrov and Klein, 2007; Petrov, 2010). Table 1 lists the NP length distributions for each training corpus. The WSJ training corpus yields just under 5,100 distinct modifier types (without normalizing for capitalization), while the NYT data yields 105,364. Note that the number of NPs extracted from the manual and automatic parses of the WSJ are quite close. We find that the overlap between the two groups is well over 90%, suggesting that extracting NPs from a large, automatically parsed corpus will provide phrases comparable to manually annotated NPs. We evaluate across a variety of domains, including (1) the WSJ se</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the ACL (HLT-NAACL 2007), pages 404– 411, Rochester, NY, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<date>2010</date>
<booktitle>Berkeley parser. GNU General Public License v.2.</booktitle>
<contexts>
<context position="7774" citStr="Petrov, 2010" startWordPosition="1190" endWordPosition="1191">el outperforms previous systems, and generalizes quite well across different domains. 3 Corpora Following Dunlop et al. (2010), we use the Wall St. Journal (WSJ), Switchboard (SWBD) and Brown corpus sections of the Penn Treebank (Marcus et al., 1993) as our supervised training and testing baselines. For semi-supervised training, we automatically parse sections 02-21 of the WSJ treebank using cross-validation methods, and scale up the amount of data used by parsing the New York Times (NYT) section of the Gigaword (Graff and Cieri, 2003) corpus using the Berkeley Parser (Petrov and Klein, 2007; Petrov, 2010). Table 1 lists the NP length distributions for each training corpus. The WSJ training corpus yields just under 5,100 distinct modifier types (without normalizing for capitalization), while the NYT data yields 105,364. Note that the number of NPs extracted from the manual and automatic parses of the WSJ are quite close. We find that the overlap between the two groups is well over 90%, suggesting that extracting NPs from a large, automatically parsed corpus will provide phrases comparable to manually annotated NPs. We evaluate across a variety of domains, including (1) the WSJ sections 22-24, a</context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>Slav Petrov. 2010. Berkeley parser. GNU General Public License v.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Shaw</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Ordering among premodifiers.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th ACL (ACL</booktitle>
<pages>135--143</pages>
<institution>College Park, Maryland. Association for Computational Linguistics.</institution>
<contexts>
<context position="1999" citStr="Shaw and Hatzivassiloglou, 1999" startWordPosition="302" endWordPosition="305"> The phrase “big red ball” provides a basic ordering of the words big and red. The reverse ordering, in “red big ball”, sounds strange, a phrase that would only occur in marked situations. There is no consensus on the exact qualities that affect a modifier’s position, but it is clear that some modifier orderings sound more natural than others, even if all are strictly speaking grammatical. Determining methods for ordering modifiers prenominally and investigating the factors underlying modifier ordering have been areas of considerable research, including work in natural language236 processing (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009; Dunlop et al., 2010), linguistics (Whorf, 1945; Vendler, 1968), and psychology (Martin, 1969; Danks and Glucksberg, 1971). A central issue in work on modifier ordering is how to order modifiers that are unobserved during system development. English has upwards of 200,000 words, with over 50,000 words in the vocabulary of an educated adult (Aitchison, 2003). Up to a quarter of these words may be adjectives, which poses a significant problem for any system that attempts to categorize English adjectives in ways that are useful for an ordering task. Extensive in-con</context>
<context position="5087" citStr="Shaw and Hatzivassiloglou (1999)" startWordPosition="760" endWordPosition="763">nstrate superior performance of the n-gram model across different domains, particularly as the training data size is scaled up. This paper presents two important results: 1) N-gram modeling performs better than previously believed for this task, and in fact surpasses current class-based systems.1 2) Automatic parsers can effectively provide essentially unlimited training data for learning modifier ordering preferences. Our results point the way to larger scale datadriven approaches to this and related tasks. 2 Related Work In one of the earliest automatic prenominal modifier ordering systems, Shaw and Hatzivassiloglou (1999) ordered pairs of modifiers, including adjectives, nouns (“baseball field”); gerunds, (“running man”); and participles (“heated debate”). They described a direct evidence method, a transitivity method, and a clustering method for ordering these different kinds of modifiers, with the transitivity technique returning the highest accuracy of 90.67% on a medical text. However, when testing across domains, their accuracy dropped to 56%, not much higher than random guessing. Malouf (2000) continued this work, ordering prenominal adjective pairs in the BNC. He abandoned a bigram model, finding it ach</context>
</contexts>
<marker>Shaw, Hatzivassiloglou, 1999</marker>
<rawString>James Shaw and Vasileios Hatzivassiloglou. 1999. Ordering among premodifiers. In Proceedings of the 37th ACL (ACL 1999), pages 135–143, College Park, Maryland. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="9090" citStr="Stolcke, 2002" startWordPosition="1410" endWordPosition="1411"> length distributions for each test corpus. 4 Methods In this section, we present two novel prenominal modifier ordering approaches: a 5-gram model and an EM-trained HMM. In both systems, modifiers that occur only once in the training data are given the Berkeley parser OOV class labels (Petrov, 2010). In Section 5, we compare these approaches to the one-class system described in Mitchell (2010) and the discriminative MSA described in Dunlop et al. (2010). We refer the interested reader to those papers for the details of their learning algorithms. 4.1 N-Gram Modeling We used the SRILM toolkit (Stolcke, 2002) to build unpruned 5-gram models using interpolated modified Kneser-Ney smoothing (Chen and Goodman, 1998). In the testing phase, each possible permutation is assigned a probability by the model, and the highest probability sequence is chosen. We explored building n-gram models based on entire observed sequences (sentences) and on extracted multiple modifier NPs. As shown in Table 3, we found a very large (12% absolute) accuracy improvement in a model trained with just NP sequences. This is likely due to several factors, including the role of the begin string symbol &lt;s&gt;, which helps to capture</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP 2002), volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeno Vendler</author>
</authors>
<title>Adjectives and Nominalizations.</title>
<date>1968</date>
<location>Mouton, The Netherlands.</location>
<contexts>
<context position="2093" citStr="Vendler, 1968" startWordPosition="319" endWordPosition="320">g ball”, sounds strange, a phrase that would only occur in marked situations. There is no consensus on the exact qualities that affect a modifier’s position, but it is clear that some modifier orderings sound more natural than others, even if all are strictly speaking grammatical. Determining methods for ordering modifiers prenominally and investigating the factors underlying modifier ordering have been areas of considerable research, including work in natural language236 processing (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009; Dunlop et al., 2010), linguistics (Whorf, 1945; Vendler, 1968), and psychology (Martin, 1969; Danks and Glucksberg, 1971). A central issue in work on modifier ordering is how to order modifiers that are unobserved during system development. English has upwards of 200,000 words, with over 50,000 words in the vocabulary of an educated adult (Aitchison, 2003). Up to a quarter of these words may be adjectives, which poses a significant problem for any system that attempts to categorize English adjectives in ways that are useful for an ordering task. Extensive in-context observation of adjectives and other modifiers is required to adequately characterize thei</context>
</contexts>
<marker>Vendler, 1968</marker>
<rawString>Zeno Vendler. 1968. Adjectives and Nominalizations. Mouton, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Lee Whorf</author>
</authors>
<title>Grammatical categories.</title>
<date>1945</date>
<journal>Language,</journal>
<volume>21</volume>
<issue>1</issue>
<contexts>
<context position="2077" citStr="Whorf, 1945" startWordPosition="317" endWordPosition="318">g, in “red big ball”, sounds strange, a phrase that would only occur in marked situations. There is no consensus on the exact qualities that affect a modifier’s position, but it is clear that some modifier orderings sound more natural than others, even if all are strictly speaking grammatical. Determining methods for ordering modifiers prenominally and investigating the factors underlying modifier ordering have been areas of considerable research, including work in natural language236 processing (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009; Dunlop et al., 2010), linguistics (Whorf, 1945; Vendler, 1968), and psychology (Martin, 1969; Danks and Glucksberg, 1971). A central issue in work on modifier ordering is how to order modifiers that are unobserved during system development. English has upwards of 200,000 words, with over 50,000 words in the vocabulary of an educated adult (Aitchison, 2003). Up to a quarter of these words may be adjectives, which poses a significant problem for any system that attempts to categorize English adjectives in ways that are useful for an ordering task. Extensive in-context observation of adjectives and other modifiers is required to adequately c</context>
</contexts>
<marker>Whorf, 1945</marker>
<rawString>Benjamin Lee Whorf. 1945. Grammatical categories. Language, 21(1):1–11.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>