<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000094">
<title confidence="0.980807">
Bayesian Word Sense Induction
</title>
<author confidence="0.994635">
Samuel Brody Mirella Lapata
</author>
<affiliation confidence="0.999775">
Dept. of Biomedical Informatics School of Informatics
Columbia University University of Edinburgh
</affiliation>
<email confidence="0.98556">
samuel.brody@dbmi.columbia.edu mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997229" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920913043478">
Sense induction seeks to automatically
identify word senses directly from a cor-
pus. A key assumption underlying pre-
vious work is that the context surround-
ing an ambiguous word is indicative of
its meaning. Sense induction is thus typ-
ically viewed as an unsupervised cluster-
ing problem where the aim is to partition
a word’s contexts into different classes,
each representing a word sense. Our work
places sense induction in a Bayesian con-
text by modeling the contexts of the am-
biguous word as samples from a multi-
nomial distribution over senses which
are in turn characterized as distributions
over words. The Bayesian framework pro-
vides a principled way to incorporate a
wide range of features beyond lexical co-
occurrences and to systematically assess
their utility on the sense induction task.
The proposed approach yields improve-
ments over state-of-the-art systems on a
benchmark dataset.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921416666666">
Sense induction is the task of discovering automat-
ically all possible senses of an ambiguous word. It
is related to, but distinct from, word sense disam-
biguation (WSD) where the senses are assumed to
be known and the aim is to identify the intended
meaning of the ambiguous word in context.
Although the bulk of previous work has been
devoted to the disambiguation problem1, there are
good reasons to believe that sense induction may
be able to overcome some of the issues associ-
ated with WSD. Since most disambiguation meth-
ods assign senses according to, and with the aid
</bodyText>
<footnote confidence="0.742417666666667">
1Approaches to WSD are too numerous to list; We refer
the interested reader to Agirre et al. (2007) for an overview
of the state of the art.
</footnote>
<bodyText confidence="0.999978738095238">
of, dictionaries or other lexical resources, it is dif-
ficult to adapt them to new domains or to lan-
guages where such resources are scarce. A re-
lated problem concerns the granularity of the sense
distinctions which is fixed, and may not be en-
tirely suitable for different applications. In con-
trast, when sense distinctions are inferred directly
from the data, they are more likely to represent
the task and domain at hand. There is little risk
that an important sense will be left out, or that ir-
relevant senses will influence the results. Further-
more, recent work in machine translation (Vickrey
et al., 2005) and information retrieval (V´eronis,
2004) indicates that induced senses can lead to im-
proved performance in areas where methods based
on a fixed sense inventory have previously failed
(Carpuat and Wu, 2005; Voorhees, 1993).
Sense induction is typically treated as an un-
supervised clustering problem. The input to the
clustering algorithm are instances of the ambigu-
ous word with their accompanying contexts (rep-
resented by co-occurrence vectors) and the output
is a grouping of these instances into classes cor-
responding to the induced senses. In other words,
contexts that are grouped together in the same
class represent a specific word sense. In this paper
we adopt a novel Bayesian approach and formalize
the induction problem in a generative model. For
each ambiguous word we first draw a distribution
over senses, and then generate context words ac-
cording to this distribution. It is thus assumed that
different senses will correspond to distinct lexical
distributions. In this framework, sense distinctions
arise naturally through the generative process: our
model postulates that the observed data (word con-
texts) are explicitly intended to communicate a la-
tent structure (their meaning).
Our work is related to Latent Dirichlet Allo-
cation (LDA, Blei et al. 2003), a probabilistic
model of text generation. LDA models each doc-
ument using a mixture over K topics, which are
in turn characterized as distributions over words.
</bodyText>
<note confidence="0.9229495">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 103–111,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999302">
103
</page>
<bodyText confidence="0.99999046875">
The words in the document are generated by re-
peatedly sampling a topic according to the topic
distribution, and selecting a word given the chosen
topic. Whereas LDA generates words from global
topics corresponding to the whole document, our
model generates words from local topics chosen
based on a context window around the ambiguous
word. Document-level topics resemble general do-
main labels (e.g., finance, education) and cannot
faithfully model more fine-grained meaning dis-
tinctions. In our work, therefore, we create an in-
dividual model for every (ambiguous) word rather
than a global model for an entire document col-
lection. We also show how multiple information
sources can be straightforwardly integrated with-
out changing the underlying probabilistic model.
For instance, besides lexical information we may
want to consider parts of speech or dependen-
cies in our sense induction problem. This is in
marked contrast with previous LDA-based mod-
els which mostly take only word-based informa-
tion into account. We evaluate our model on a
recently released benchmark dataset (Agirre and
Soroa, 2007) and demonstrate improvements over
the state-of-the-art.
The remainder of this paper is structured as fol-
lows. We first present an overview of related work
(Section 2) and then describe our Bayesian model
in more detail (Sections 3 and 4). Section 5 de-
scribes the resources and evaluation methodology
used in our experiments. We discuss our results in
Section 6, and conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999943057142858">
Sense induction is typically treated as a cluster-
ing problem, where instances of a target word
are partitioned into classes by considering their
co-occurring contexts. Considerable latitude is
allowed in selecting and representing the co-
occurring contexts. Previous methods have used
first or second order co-occurrences (Purandare
and Pedersen, 2004; Sch¨utze, 1998), parts of
speech (Purandare and Pedersen, 2004), and gram-
matical relations (Pantel and Lin, 2002; Dorow
and Widdows, 2003). The size of the context win-
dow also varies, it can be a relatively small, such as
two words before and after the target word (Gauch
and Futrelle, 1993), the sentence within which the
target is found (Bordag, 2006), or even larger, such
as the 20 surrounding words on either side of the
target (Purandare and Pedersen, 2004).
In essence, each instance of a target word
is represented as a feature vector which subse-
quently serves as input to the chosen clustering
method. A variety of clustering algorithms have
been employed ranging from k-means (Purandare
and Pedersen, 2004), to agglomerative clustering
(Sch¨utze, 1998), and the Information Bottleneck
(Niu et al., 2007). Graph-based methods have also
been applied to the sense induction task. In this
framework words are represented as nodes in the
graph and vertices are drawn between the tar-
get and its co-occurrences. Senses are induced by
identifying highly dense subgraphs (hubs) in the
co-occurrence graph (V´eronis, 2004; Dorow and
Widdows, 2003).
Although LDA was originally developed as a
generative topic model, it has recently gained
popularity in the WSD literature. The inferred
document-level topics can help determine coarse-
grained sense distinctions. Cai et al. (2007) pro-
pose to use LDA’s word-topic distributions as fea-
tures for training a supervised WSD system. In a
similar vein, Boyd-Graber and Blei (2007) infer
LDA topics from a large corpus, however for un-
supervised WSD. Here, LDA topics are integrated
with McCarthy et al.’s (2004) algorithm. For each
target word, a topic is sampled from the docu-
ment’s topic distribution, and a word is generated
from that topic. Also, a distributional neighbor is
selected based on the topic and distributional sim-
ilarity to the generated word. Then, the word sense
is selected based on the word, neighbor, and topic.
Boyd-Graber et al. (2007) extend the topic mod-
eling framework to include WordNet senses as a
latent variable in the word generation process. In
this case the model discovers both the topics of
the corpus and the senses assigned to each of its
words.
Our own model is also inspired by LDA but cru-
cially performs word sense induction, not disam-
biguation. Unlike the work mentioned above, we
do not rely on a pre-existing list of senses, and do
not assume a correspondence between our auto-
matically derived sense-clusters and those of any
given inventory.2 A key element in these previous
attempts at adapting LDA for WSD is the tendency
to remain at a high level, document-like, setting.
In contrast, we make use of much smaller units
of text (a few sentences, rather than a full doc-
ument), and create an individual model for each
(ambiguous) word type. Our induced senses are
few in number (typically less than ten). This is in
marked contrast to tens, and sometimes hundreds,
</bodyText>
<footnote confidence="0.919392">
2Such a mapping is only performed to enable evaluation
and comparison with other approaches (see Section 5).
</footnote>
<page confidence="0.998587">
104
</page>
<bodyText confidence="0.999909318181818">
of topics commonly used in document-modeling
tasks.
Unlike many conventional clustering meth-
ods (e.g., Purandare and Pedersen 2004; Sch¨utze
1998), our model is probabilistic; it specifies
a probability distribution over possible values,
which makes it easy to integrate and combine with
other systems via mixture or product models. Fur-
thermore, the Bayesian framework allows the in-
corporation of several information sources in a
principled manner. Our model can easily handle an
arbitrary number of feature classes (e.g., parts of
speech, dependencies). This functionality in turn
enables us to evaluate which linguistic informa-
tion matters for the sense induction task. Previous
attempts to handle multiple information sources
in the LDA framework (e.g., Griffiths et al. 2005;
Barnard et al. 2003) have been task-specific and
limited to only two layers of information. Our
model provides this utility in a general framework,
and could be applied to other tasks, besides sense
induction.
</bodyText>
<sectionHeader confidence="0.971089" genericHeader="method">
3 The Sense Induction Model
</sectionHeader>
<bodyText confidence="0.999984166666667">
The core idea behind sense induction is that con-
textual information provides important cues re-
garding a word’s meaning. The idea dates back to
(at least) Firth (1957) (“You shall know a word by
the company it keeps”), and underlies most WSD
and lexicon acquisition work to date. Under this
premise, we should expect different senses to be
signaled by different lexical distributions.
We can place sense induction in a probabilis-
tic setting by modeling the context words around
the ambiguous target as samples from a multino-
mial sense distribution. More formally, we will
write P(s) for the distribution over senses s of
an ambiguous target in a specific context win-
dow and P(w1s) for the probability distribution
over context words w given sense s. Each word wi
in the context window is generated by first sam-
pling a sense from the sense distribution, then
choosing a word from the sense-context distribu-
tion. P(si = j) denotes the probability that the jth
sense was sampled for the ith word token and
P(wi|si = j) the probability of context word wi un-
der sense j. The model thus specifies a distribution
over words within a context window:
</bodyText>
<equation confidence="0.963413">
S
P(wi) = I P(wi|si = j)P(si = j) (1)
j=1
</equation>
<bodyText confidence="0.999636">
where S is the number of senses. We assume that
each target word has C contexts and each context c
</bodyText>
<figureCaption confidence="0.948054666666667">
Figure 1: Bayesian sense induction model; shaded
nodes represent observed variables, unshaded
nodes indicate latent variables. Arrows indi-
</figureCaption>
<bodyText confidence="0.999446717948718">
cate conditional dependencies between variables,
whereas plates (the rectangles in the figure) refer
to repetitions of sampling steps. The variables in
the lower right corner refer to the number of sam-
ples.
consists of Nc word tokens. We shall write �(j) as a
shorthand for P(wi|si = j), the multinomial distri-
bution over words for sense j, and 0(c) as a short-
hand for the distribution of senses in context c.
Following Blei et al. (2003) we will assume that
the mixing proportion over senses 0 is drawn from
a Dirichlet prior with parameters a. The role of
the hyperparameter a is to create a smoothed sense
distribution. We also place a symmetric Dirichlet R
on � (Griffiths and Steyvers, 2002). The hyper-
parmeter R can be interpreted as the prior observa-
tion count on the number of times context words
are sampled from a sense before any word from
the corpus is observed. Our model is represented
in graphical notation in Figure 1.
The model sketched above only takes word in-
formation into account. Methods developed for su-
pervised WSD often use a variety of information
sources based not only on words but also on lem-
mas, parts of speech, collocations and syntactic re-
lationships (Lee and Ng, 2002). The first idea that
comes to mind, is to use the same model while
treating various features as word-like elements. In
other words, we could simply assume that the con-
texts we wish to model are the union of all our
features. Although straightforward, this solution
is undesirable. It merges the distributions of dis-
tinct feature categories into a single one, and is
therefore conceptually incorrect, and can affect the
performance of the model. For instance, parts-of-
speech (which have few values, and therefore high
probability), would share a distribution with words
(which are much sparser). Layers containing more
elements (e.g. 10 word window) would overwhelm
</bodyText>
<figure confidence="0.9976178">
0(R)
a
0
s w Nc
C
</figure>
<page confidence="0.82445">
105
</page>
<figureCaption confidence="0.980416">
Figure 2: Extended sense induction model; inner
</figureCaption>
<bodyText confidence="0.963790137931035">
rectangles represent different sources (layers) of
information. All layers share the same, instance-
specific, sense distribution (0), but each have their
own (multinomial) sense-feature distribution (�).
Shaded nodes represent observed features f; these
can be words, parts of speech, collocations or de-
pendencies.
unconditional joint distribution P(s) of the unob-
served variables (provided certain criteria are ful-
filled).
In our model, each element in each layer is a
variable, and is assigned a sense label (see Fig-
ure 2, where distinct layers correspond to differ-
ent representations of the context around the tar-
get word). From these assignments, we must de-
termine the sense distribution of the instance as a
whole. This is the purpose of the Gibbs sampling
procedure. Specifically, in order to derive the up-
date function used in the Gibbs sampler, we must
provide the conditional probability of the i-th vari-
able being assigned sense si in layer l, given the
feature value fi of the context variable and the cur-
rent sense assignments of all the other variables in
the data (s−i):
p(si|s−i, f) — p(fi|s, f −i,R) · p(si|s−i,a) (2)
The probability of a single sense assignment, si,
is proportional to the product of the likelihood (of
feature fi, given the rest of the data) and the prior
probability of the assignment.
</bodyText>
<figure confidence="0.956837133333333">
(3)
s f Nc1
01(R1)
02(R2)
a
0
s f
Nc2
.
..
0n(Rn)
s f
Ncn
C
p(fi|s, f−i,R) _
</figure>
<bodyText confidence="0.99556975">
smaller ones (e.g. 1 word window).
Our solution is to treat each information source
(or feature type) individually and then combine
all of them together in a unified model. Our un-
derlying assumption is that the context window
around the target word can have multiple represen-
tations, all of which share the same sense distribu-
tion. We illustrate this in Figure 2 where each inner
rectangle (layer) corresponds to a distinct feature
type. We will naively assume independence be-
tween multiple layers, even though this is clearly
not the case in our task. The idea here is to model
each layer as faithfully as possible to the empirical
data while at the same time combining information
from all layers in estimating the sense distribution
of each target instance.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.960725888888889">
Our inference procedure is based on Gibbs sam-
pling (Geman and Geman, 1984). The procedure
begins by randomly initializing all unobserved
random variables. At each iteration, each random
variable si is sampled from the conditional distri-
bution P(si|s−i) where s−i refers to all variables
other than si. Eventually, the distribution over sam-
ples drawn from this process will converge to the
f p(fi |l, S, 0) · p(O |f-i, Rt)dO _ #(fi, si) + Rl
</bodyText>
<note confidence="0.314278">
J #(si) +Vl · Rl
</note>
<bodyText confidence="0.999559333333333">
For the likelihood term p(fi|s, f −i,R), integrating
over all possible values of the multinomial feature-
sense distribution � gives us the rightmost term in
Equation 3, which has an intuitive interpretation.
The term #(fi,si) indicates the number of times
the feature-value fi was assigned sense si in the
rest of the data. Similarly, #(si) indicates the num-
ber of times the sense assignment si was observed
in the data. Rl is the Dirichlet prior for the feature-
sense distribution � in the current layer l, and Vl
is the size of the vocabulary of that layer, i.e., the
number of possible feature values in the layer. In-
tuitively, the probability of a feature-value given
a sense is directly proportional to the number of
times we have seen that value and that sense-
assignment together in the data, taking into ac-
count a pseudo-count prior, expressed through R.
This can also be viewed as a form of smoothing.
A similar approach is taken with regards to the
prior probability p(si|s−i,a). In this case, how-
ever, all layers must be considered:
</bodyText>
<equation confidence="0.936416">
Xl · p(si|l,s−i,al) (4)
p(si|s−i,a) _ Y
l
</equation>
<page confidence="0.98152">
106
</page>
<bodyText confidence="0.9987934375">
Here λl is the weight for the contribution of layer l,
and αl is the portion of the Dirichlet prior for the
sense distribution θ in the current layer. Treating
each layer individually, we integrate over the pos-
sible values of θ, obtaining a similar count-based
term:
where #l(si) indicates the number of elements in
layer l assigned the sense si, #l indicates the num-
ber of elements in layer l, i.e., the size of the layer
and S the number of senses.
To distribute the pseudo counts represented by
α in a reasonable fashion among the layers, we
define αl = #l
#m · α where #m = ∑l #l, i.e., the total
size of the instance. This distributes α according
to the relative size of each layer in the instance.
</bodyText>
<table confidence="0.7297338">
p(si|l,s−i,αl)= #l(si) + #l #m · #l(si) (6)
#m · α #l + α
=
#l +S· #l
#m ·α #m+S·α
</table>
<bodyText confidence="0.776245">
Placing these values in Equation 4 we obtain the
following:
</bodyText>
<equation confidence="0.997363333333333">
#m ·∑l λl · #l(si)
#l + α
p(si|s−i,α) = (7)
</equation>
<bodyText confidence="0.831973666666667">
#m+S·α
Putting it all together, we arrive at the final update
equation for the Gibbs sampling:
</bodyText>
<equation confidence="0.844641666666667">
#(fi,si) + βl #m · ∑l λl · #l(si) #l+ α
p(si|s−i, f)∝ ·(8)
#(si)+Vl ·βl #m+S·α
</equation>
<bodyText confidence="0.997354">
Note that when dealing with a single layer, Equa-
tion 8 collapses to:
</bodyText>
<equation confidence="0.871083666666667">
#(fi,si)+β #m(si)+α
p(si|s−i, f ) ∝ (9)
#(si)+V · β #m+S·α
</equation>
<bodyText confidence="0.999940916666667">
where #m(si) indicates the number of elements
(e.g., words) in the context window assigned to
sense si. This is identical to the update equation
in the original, word-based LDA model.
The sampling algorithm gives direct estimates
of s for every context element. However, in view
of our task, we are more interested in estimating θ,
the sense-context distribution which can be ob-
tained as in Equation 7, but taking into account
all sense assignments, without removing assign-
ment i. Our system labels each instance with the
single, most probable sense.
</bodyText>
<sectionHeader confidence="0.996618" genericHeader="method">
5 Evaluation Setup
</sectionHeader>
<bodyText confidence="0.999889941176471">
In this section we discuss our experimental set-up
for assessing the performance of the model pre-
sented above. We give details on our training pro-
cedure, describe our features, and explain how our
system output was evaluated.
Data In this work, we focus solely on inducing
senses for nouns, since they constitute the largest
portion of content words. For example, nouns rep-
resent 45% of the content words in the British Na-
tional Corpus. Moreover, for many tasks and ap-
plications (e.g., web queries, Jansen et al. 2000)
nouns are the most frequent and most important
part-of-speech.
For evaluation, we used the Semeval-2007
benchmark dataset released as part of the sense
induction and discrimination task (Agirre and
Soroa, 2007). The dataset contains texts from the
Penn Treebank II corpus, a collection of articles
from the first half of the 1989 Wall Street Jour-
nal (WSJ). It is hand-annotated with OntoNotes
senses (Hovy et al., 2006) and has 35 nouns. The
average noun ambiguity is 3.9, with a high (almost
80%) skew towards the predominant sense. This is
not entirely surprising since OntoNotes senses are
less fine-grained than WordNet senses.
We used two corpora for training as we wanted
to evaluate our model’s performance across differ-
ent domains. The British National Corpus (BNC)
is a 100 million word collection of samples of
written and spoken language from a wide range of
sources including newspapers, magazines, books
(both academic and fiction), letters, and school es-
says as well as spontaneous conversations. This
served as our out-of-domain corpus, and con-
tained approximately 730 thousand instances of
the 35 target nouns in the Semeval lexical sample.
The second, in-domain, corpus was built from se-
lected portions of the Wall Street Journal. We used
all articles (excluding the Penn Treebank II por-
tion used in the Semeval dataset) from the years
1987-89 and 1994 to create a corpus of similar size
to the BNC, containing approximately 740 thou-
sand instances of the target words.
Additionally, we used the Senseval 2 and 3 lex-
ical sample data (Preiss and Yarowsky, 2001; Mi-
halcea and Edmonds, 2004) as development sets,
for experimenting with the hyper-parameters of
our model (see Section 6).
Evaluation Methodology Agirre and Soroa
(2007) present two evaluation schemes for as-
sessing sense induction methods. Under the first
</bodyText>
<equation confidence="0.8789935">
(5)
p(si|l,s−i,αl) =
f p(si|l,s−i,θ)· p(θ|f−i,αl)dθ = #l(si)+αl
#l +S·αl
</equation>
<page confidence="0.988897">
107
</page>
<bodyText confidence="0.999819229166667">
scheme, the system output is compared to the
gold standard using standard clustering evalua-
tion metrics (e.g., purity, entropy). Here, no at-
tempt is made to match the induced senses against
the labels of the gold standard. Under the second
scheme, the gold standard is partitioned into a test
and training corpus. The latter is used to derive a
mapping of the induced senses to the gold stan-
dard labels. The mapping is then used to calculate
the system’s F-Score on the test corpus.
Unfortunately, the first scheme failed to dis-
criminate among participating systems. The one-
cluster-per-word baseline outperformed all sys-
tems, except one, which was only marginally bet-
ter. The scheme ignores the actual labeling and
due to the dominance of the first sense in the data,
encourages a single-sense approach which is fur-
ther amplified by the use of a coarse-grained sense
inventory. For the purposes of this work, there-
fore, we focused on the second evaluation scheme.
Here, most of the participating systems outper-
formed the most-frequent-sense baseline, and the
rest obtained only slightly lower scores.
Feature Space Our experiments used a feature
set designed to capture both immediate local con-
text, wider context and syntactic context. Specifi-
cally, we experimented with six feature categories:
±10-word window (10w), ±5-word window (5w),
collocations (1w), word n-grams (ng), part-of-
speech n-grams (pg) and dependency relations
(dp). These features have been widely adopted in
various WSD algorithms (see Lee and Ng 2002 for
a detailed evaluation). In all cases, we use the lem-
matized version of the word(s).
The Semeval workshop organizers provided a
small amount of context for each instance (usu-
ally a sentence or two surrounding the sentence
containing the target word). This context, as well
as the text in the training corpora, was parsed us-
ing RASP (Briscoe and Carroll, 2002), to extract
part-of-speech tags, lemmas, and dependency in-
formation. For instances containing more than one
occurrence of the target word, we disambiguate
the first occurrence. Instances which were not cor-
rectly recognized by the parser (e.g., a target word
labeled with the wrong lemma or part-of-speech),
were automatically assigned to the largest sense-
cluster.3
</bodyText>
<footnote confidence="0.79964">
3This was the case for less than 1% of the instances.
</footnote>
<figure confidence="0.9701985">
3 4 5 6 7 8 9
Number of Senses
</figure>
<figureCaption confidence="0.9785455">
Figure 3: Model performance with varying num-
ber of senses on the WSJ and BNC corpora.
</figureCaption>
<sectionHeader confidence="0.998939" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999870970588235">
Model Selection The framework presented in
Section 3 affords great flexibility in modeling the
empirical data. This however entails that several
parameters must be instantiated. More precisely,
our model is conditioned on the Dirichlet hyper-
parameters α and β and the number of senses S.
Additional parameters include the number of iter-
ations for the Gibbs sampler and whether or not
the layers are assigned different weights.
Our strategy in this paper is to fix α and β
and explore the consequences of varying S. The
value for the α hyperparameter was set to 0.02.
This was optimized in an independent tuning ex-
periment which used the Senseval 2 (Preiss and
Yarowsky, 2001) and Senseval 3 (Mihalcea and
Edmonds, 2004) datasets. We experimented with
α values ranging from 0.005 to 1. The β parame-
ter was set to 0.1 (in all layers). This value is often
considered optimal in LDA-related models (Grif-
fiths and Steyvers, 2002). For simplicity, we used
uniform weights for the layers. The Gibbs sampler
was run for 2,000 iterations. Due to the random-
ized nature of the inference procedure, all reported
results are average scores over ten runs.
Our experiments used the same number of
senses for all the words, since tuning this number
individually for each word would be prohibitive.
We experimented with values ranging from three
to nine senses. Figure 3 shows the results obtained
for different numbers of senses when the model is
trained on the WSJ (in-domain) and BNC (out-of-
domain) corpora, respectively. Here, we are using
the optimal combination of layers for each system
(which we discuss in the following section in de-
</bodyText>
<figure confidence="0.838259625">
In-Domain (WSJ)
Out-of-Domain (BNC)
F-Score (%) 88
87
86
85
84
83
</figure>
<page confidence="0.961475">
108
</page>
<listItem confidence="0.859412857142857">
Senses of drug (WSJ)
1. U.S., administration, federal, against, war, dealer
2. patient, people, problem, doctor, company, abuse
3. company, million, sale, maker, stock, inc.
4. administration, food, company, approval, FDA
Senses of drug (BNC)
1. patient, treatment, effect, anti-inflammatory
2. alcohol, treatment, patient, therapy, addiction
3. patient, new, find, effect, choice, study
4. test, alcohol, patient, abuse, people, crime
5. trafficking, trafficker, charge, use, problem
6. abuse, against, problem, treatment, alcohol
7. people, wonder, find, prescription, drink, addict
8. company, dealer, police, enforcement, patient
</listItem>
<tableCaption confidence="0.7904615">
Table 1: Senses inferred for the word drug from
the WSJ and BNC corpora.
</tableCaption>
<bodyText confidence="0.999265176470588">
tail). For the model trained on WSJ, performance
peaks at four senses, which is similar to the av-
erage ambiguity in the test data. For the model
trained on the BNC, however, the best results are
obtained using twice as many senses. Using fewer
senses with the BNC-trained system can result in
a drop in accuracy of almost 2%. This is due to
the shift in domain. As the sense-divisions of the
learning domain do not match those of the target
domain, finer granularity is required in order to en-
compass all the relevant distinctions.
Table 1 illustrates the senses inferred for the
word drug when using the in-domain and out-of-
domain corpora, respectively. The most probable
words for each sense are also shown. Firstly, note
that the model infers some plausible senses for
drug on the WSJ corpus (top half of Table 1).
Sense 1 corresponds to the “enforcement” sense
of drug, Sense 2 refers to “medication”, Sense 3
to the “drug industry” and Sense 4 to “drugs re-
search”. The inferred senses for drug on the BNC
(bottom half of Table 1) are more fine grained. For
example, the model finds distinct senses for “med-
ication” (Sense 1 and 7) and “illegal substance”
(Senses 2, 4, 6, 7). It also finds a separate sense
for “drug dealing” (Sense 5) and “enforcement”
(Sense 8). Because the BNC has a broader fo-
cus, finer distinctions are needed to cover as many
senses as possible that are relevant to the target do-
main (WSJ).
Layer Analysis We next examine which indi-
vidual feature categories are most informative
in our sense induction task. We also investigate
whether their combination, through our layered
</bodyText>
<table confidence="0.9984965">
1-Layer 5-Layers Combination
10w 86.9 -10w 83.1 10w+5w 87.3%
5w 86.8 -5w 83.0 5w+pg 83.9%
1w 84.6 -1w 83.0 1w+ng 83.2%
ng 83.6 -ng 83.0 10w+pg 83.3%
pg 82.5 -pg 82.7 1w+pg 84.5%
dp 82.2 -dp 84.7 10w+pg+dep 82.2%
MFS 80.9 all 83.3 MFS 80.9%
</table>
<tableCaption confidence="0.922257">
Table 2: Model performance (F-score) on the WSJ
with one layer (left), five layers (middle), and se-
lected combinations of layers (right).
</tableCaption>
<bodyText confidence="0.996978">
model (see Figure 2), yields performance im-
provements. We used 4 senses for the system
trained on WSJ and 8 for the system trained on
the BNC (a was set to 0.02 and b to 0.1)
Table 2 (left side) shows the performance of our
model when using only one layer. The layer com-
posed of words co-occurring within a ±10-word
window (10w), and representing wider, topical, in-
formation gives the highest scores on its own. It
is followed by the ±5 (5w) and ±1 (1w) word
windows, which represent more immediate, local
context. Part-of-speech n-grams (pg) and word n-
grams (ng), on their own, achieve lower scores,
largely due to over-generalization and data sparse-
ness, respectively. The lowest-scoring single layer
is the dependency layer (dp), with performance
only slightly above the most-frequent-sense base-
line (MFS). Dependency information is very infor-
mative when present, but extremely sparse.
Table 2 (middle) also shows the results obtained
when running the layered model with all but one
of the layers as input. We can use this informa-
tion to determine the contribution of each layer by
comparing to the combined model with all layers
(all). Because we are dealing with multiple lay-
ers, there is an element of overlap involved. There-
fore, each of the word-window layers, despite rel-
atively high informativeness on its own, does not
cause as much damage when it is absent, since
the other layers compensate for the topical and lo-
cal information. The absence of the word n-gram
layer, which provides specific local information,
does not make a great impact when the 1w and pg
layers are present. Finally, we can see that the ex-
tremely sparse dependency layer is detrimental to
the multi-layer model as a whole, and its removal
increases performance. The sparsity of the data in
this layer means that there is often little informa-
tion on which to base a decision. In these cases,
the layer contributes a close-to-uniform estimation
</bodyText>
<page confidence="0.993527">
109
</page>
<table confidence="0.998978692307692">
1-Layer 5-Layers Combination
10w 84.6 -10w 83.3 10w+5w 85.5%
5w 84.6 -5w 82.8 5w+pg 83.5%
1w 83.6 -1w 83.5 1w+ng 83.5%
pg 83.1 -pg 83.2 10w+pg 83.4%
ng 82.8 -ng 82.9 1w+pg 84.1%
dp 81.1 -dp 84.7 10w+pg+dep 81.7%
MFS 80.9 all 84.1 MFS 80.9%
System F-Score
10w, 5w (WSJ) 87.3
I2R 86.8
UMND2 84.5
MFS 80.9
</table>
<tableCaption confidence="0.9167132">
Table 4: Comparison of the best-performing
Semeval-07 systems against our model.
Table 3: Model performance (F-score) on the BNC
with one layer (left), five layers (middle), and se-
lected combinations of layers (right).
</tableCaption>
<bodyText confidence="0.996368478260869">
of the sense distribution, which confuses the com-
bined model.
Other layer combinations obtained similar re-
sults. Table 2 (right side) shows the most informa-
tive two and three layer combinations. Again, de-
pendencies tend to decrease performance. On the
other hand, combining features that have similar
performance on their own is beneficial. We obtain
the best performance overall with a two layered
model combining topical (+10w) and local (+5w)
contexts.
Table 3 replicates the same suite of experiments
on the BNC corpus. The general trends are similar.
Some interesting differences are apparent, how-
ever. The sparser layers, notably word n-grams
and dependencies, fare comparatively worse. This
is expected, since the more precise, local, infor-
mation is likely to vary strongly across domains.
Even when both domains refer to the same sense
of a word, it is likely to be used in a different
immediate context, and local contextual informa-
tion learned in one domain will be less effective
in the other. Another observable difference is that
the combined model without the dependency layer
does slightly better than each of the single layers.
The 1w+pg combination improves over its compo-
nents, which have similar individual performance.
Finally, the best performing model on the BNC
also combines two layers capturing wider (10w)
and more local (5w) contextual information (see
Table 3, right side).
Comparison to State-of-the-Art Table 4 com-
pares our model against the two best performing
sense induction systems that participated in the
Semeval-2007 competition. IR2 (Niu et al., 2007)
performed sense induction using the Information
Bottleneck algorithm, whereas UMND2 (Peder-
sen, 2007) used k-means to cluster second order
co-occurrence vectors associated with the target
word. These models and our own model signif-
icantly outperform the most-frequent-sense base-
line (p &lt; 0.01 using a x2 test). Our best sys-
tem (10w+5w on WSJ) is significantly better than
UMND2 (p &lt; 0.01) and quantitatively better than
IR2, although the difference is not statistically sig-
nificant.
</bodyText>
<sectionHeader confidence="0.999831" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999940558823529">
This paper presents a novel Bayesian approach to
sense induction. We formulated sense induction
in a generative framework that describes how the
contexts surrounding an ambiguous word might
be generated on the basis of latent variables. Our
model incorporates features based on lexical in-
formation, parts of speech, and dependencies in a
principled manner, and outperforms state-of-the-
art systems. Crucially, the approach is not specific
to the sense induction task and can be adapted for
other applications where it is desirable to take mul-
tiple levels of information into account. For exam-
ple, in document classification, one could consider
an accompanying image and its caption as possi-
ble additional layers to the main text.
In the future, we hope to explore more rigor-
ous parameter estimation techniques. Goldwater
and Griffiths (2007) describe a method for inte-
grating hyperparameter estimation into the Gibbs
sampling procedure using a prior over possible
values. Such an approach could be adopted in our
framework, as well, and extended to include the
layer weighting parameters, which have strong po-
tential for improving the model’s performance. In
addition, we could allow an infinite number of
senses and use an infinite Dirichlet model (Teh
et al., 2006) to automatically determine how many
senses are optimal. This provides an elegant so-
lution to the model-order problem, and eliminates
the need for external cluster-validation methods.
Acknowledgments The authors acknowledge
the support of EPSRC (grant EP/C538447/1).
We are grateful to Sharon Goldwater for her feed-
back on earlier versions of this work.
</bodyText>
<page confidence="0.997159">
110
</page>
<sectionHeader confidence="0.996407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999945309734514">
Agirre, Eneko, Llu´ıs M`arquez, and Richard Wicentowski, ed-
itors. 2007. Proceedings of the SemEval-2007. Prague,
Czech Republic.
Agirre, Eneko and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimination
systems. In Proceedings of SemEval-2007. Prague, Czech
Republic, pages 7–12.
Barnard, K., P. Duygulu, D. Forsyth, N. De Freitas, D. M.
Blei, and M. I. Jordan. 2003. Matching words and pictures.
J. of Machine Learning Research 3(6):1107–1135.
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent dirichlet allocation. Journal of Machine Learning
Research 3:993–1022.
Bordag, Stefan. 2006. Word sense induction: Triplet-based
clustering and automatic evaluation. In Proceedings of the
11th EACL. Trento, Italy, pages 137–144.
Boyd-Graber, Jordan and David Blei. 2007. Putop: Turning
predominant senses into a topic model for word sense dis-
ambiguation. In Proceedings of SemEval-2007. Prague,
Czech Republic, pages 277–281.
Boyd-Graber, Jordan, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In Pro-
ceedings of the EMNLP-CoNLL. Prague, Czech Republic,
pages 1024–1033.
Briscoe, Ted and John Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499–1504.
Cai, J. F., W. S. Lee, and Y. W. Teh. 2007. Improving word
sense disambiguation using topic features. In Proceedings
of the EMNLP-CoNLL. Prague, Czech Republic, pages
1015–1023.
Carpuat, Marine and Dekai Wu. 2005. Word sense disam-
biguation vs. statistical machine translation. In Proceed-
ings of the 43rd ACL. Ann Arbor, MI, pages 387–394.
Dorow, Beate and Dominic Widdows. 2003. Discovering
corpus-specific word senses. In Proceedings of the 10th
EACL. Budapest, Hungary, pages 79–82.
Firth, J. R. 1957. A Synopsis of Linguistic Theory 1930-1955.
Oxford: Philological Society.
Gauch, Susan and Robert P. Futrelle. 1993. Experiments in
automatic word class and word sense identification for in-
formation retrieval. In Proceedings of the 3rd Annual Sym-
posium on Document Analysis and Information Retrieval.
Las Vegas, NV, pages 425–434.
Geman, S. and D. Geman. 1984. Stochastic relaxation, Gibbs
distribution, and Bayesian restoration of images. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence 6(6):721–741.
Goldwater, Sharon and Tom Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In Pro-
ceedings of the 45th ACL. Prague, Czech Republic, pages
744–751.
Griffiths, Thomas L., Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and syn-
tax. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou,
editors, Advances in Neural Information Processing Sys-
tems 17, MIT Press, Cambridge, MA, pages 537–544.
Griffiths, Tom L. and Mark Steyvers. 2002. A probabilistic
approach to semantic representation. In Proeedings of the
24th Annual Conference of the Cognitive Science Society.
Fairfax, VA, pages 381–386.
Hovy, Eduard, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proceedings of the HLT, Companion Vol-
ume: Short Papers. Association for Computational Lin-
guistics, New York City, USA, pages 57–60.
Jansen, B. J., A. Spink, and A. Pfaff. 2000. Linguistic aspects
of web queries.
Lee, Yoong Keok and Hwee Tou Ng. 2002. An empirical
evaluation of knowledge sources and learning algorithms
for word sense disambiguation. In Proceedings of the
EMNLP. Morristown, NJ, USA, pages 41–48.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Finding predominant senses in untagged text.
In Proceedings of the 42nd ACL. Barcelona, Spain, pages
280–287.
Mihalcea, Rada and Phil Edmonds, editors. 2004. Proceed-
ings of the SENSEVAL-3. Barcelona.
Niu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan. 2007.
I2r: Three systems for word sense discrimination, chinese
word sense disambiguation, and english word sense dis-
ambiguation. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007). As-
sociation for Computational Linguistics, Prague, Czech
Republic, pages 177–182.
Pantel, Patrick and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the 8th KDD. New
York, NY, pages 613–619.
Pedersen, Ted. 2007. Umnd2 : Senseclusters applied to the
sense induction task of senseval-4. In Proceedings of
SemEval-2007. Prague, Czech Republic, pages 394–397.
Preiss, Judita and David Yarowsky, editors. 2001. Proceed-
ings of the 2nd International Workshop on Evaluating
Word Sense Disambiguation Systems. Toulouse, France.
Purandare, Amruta and Ted Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and similarity
spaces. In Proceedings of the CoNLL. Boston, MA, pages
41–48.
Sch¨utze, Hinrich. 1998. Automatic word sense discrimina-
tion. Computational Linguistics 24(1):97–123.
Teh, Y. W., M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association 101(476):1566–1581.
V´eronis, Jean. 2004. Hyperlex: lexical cartography for
information retrieval. Computer Speech &amp; Language
18(3):223–252.
Vickrey, David, Luke Biewald, Marc Teyssier, and Daphne
Koller. 2005. Word-sense disambiguation for machine
translation. In Proceedings of the HLT/EMNLP. Vancou-
ver, pages 771–778.
Voorhees, Ellen M. 1993. Using wordnet to disambiguate
word senses for text retrieval. In Proceedings of the 16th
SIGIR. New York, NY, pages 171–180.
</reference>
<page confidence="0.998799">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974851">
<title confidence="0.99995">Bayesian Word Sense Induction</title>
<author confidence="0.999818">Samuel Brody Mirella Lapata</author>
<affiliation confidence="0.9999315">Dept. of Biomedical Informatics School of Informatics Columbia University University of Edinburgh</affiliation>
<email confidence="0.995878">samuel.brody@dbmi.columbia.edumlap@inf.ed.ac.uk</email>
<abstract confidence="0.999103166666667">Sense induction seeks to automatically identify word senses directly from a corpus. A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning. Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense. Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words. The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical cooccurrences and to systematically assess their utility on the sense induction task. The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2007</date>
<booktitle>Proceedings of the SemEval-2007.</booktitle>
<editor>Agirre, Eneko, Llu´ıs M`arquez, and Richard Wicentowski, editors.</editor>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1793" citStr="(2007)" startWordPosition="285" endWordPosition="285">ble senses of an ambiguous word. It is related to, but distinct from, word sense disambiguation (WSD) where the senses are assumed to be known and the aim is to identify the intended meaning of the ambiguous word in context. Although the bulk of previous work has been devoted to the disambiguation problem1, there are good reasons to believe that sense induction may be able to overcome some of the issues associated with WSD. Since most disambiguation methods assign senses according to, and with the aid 1Approaches to WSD are too numerous to list; We refer the interested reader to Agirre et al. (2007) for an overview of the state of the art. of, dictionaries or other lexical resources, it is difficult to adapt them to new domains or to languages where such resources are scarce. A related problem concerns the granularity of the sense distinctions which is fixed, and may not be entirely suitable for different applications. In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand. There is little risk that an important sense will be left out, or that irrelevant senses will influence the results. Furthermore, recent</context>
<context position="7295" citStr="(2007)" startWordPosition="1158" endWordPosition="1158">Information Bottleneck (Niu et al., 2007). Graph-based methods have also been applied to the sense induction task. In this framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences. Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003). Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature. The inferred document-level topics can help determine coarsegrained sense distinctions. Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system. In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD. Here, LDA topics are integrated with McCarthy et al.’s (2004) algorithm. For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic. Also, a distributional neighbor is selected based on the topic and distributional similarity to the generated word. Then, the word sense is selected based on the word, neighbor, and topic. Boyd</context>
<context position="21100" citStr="(2007)" startWordPosition="3506" endWordPosition="3506"> lexical sample. The second, in-domain, corpus was built from selected portions of the Wall Street Journal. We used all articles (excluding the Penn Treebank II portion used in the Semeval dataset) from the years 1987-89 and 1994 to create a corpus of similar size to the BNC, containing approximately 740 thousand instances of the target words. Additionally, we used the Senseval 2 and 3 lexical sample data (Preiss and Yarowsky, 2001; Mihalcea and Edmonds, 2004) as development sets, for experimenting with the hyper-parameters of our model (see Section 6). Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods. Under the first (5) p(si|l,s−i,αl) = f p(si|l,s−i,θ)· p(θ|f−i,αl)dθ = #l(si)+αl #l +S·αl 107 scheme, the system output is compared to the gold standard using standard clustering evaluation metrics (e.g., purity, entropy). Here, no attempt is made to match the induced senses against the labels of the gold standard. Under the second scheme, the gold standard is partitioned into a test and training corpus. The latter is used to derive a mapping of the induced senses to the gold standard labels. The mapping is then used to calc</context>
<context position="33461" citStr="(2007)" startWordPosition="5533" endWordPosition="5533">s. Our model incorporates features based on lexical information, parts of speech, and dependencies in a principled manner, and outperforms state-of-theart systems. Crucially, the approach is not specific to the sense induction task and can be adapted for other applications where it is desirable to take multiple levels of information into account. For example, in document classification, one could consider an accompanying image and its caption as possible additional layers to the main text. In the future, we hope to explore more rigorous parameter estimation techniques. Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values. Such an approach could be adopted in our framework, as well, and extended to include the layer weighting parameters, which have strong potential for improving the model’s performance. In addition, we could allow an infinite number of senses and use an infinite Dirichlet model (Teh et al., 2006) to automatically determine how many senses are optimal. This provides an elegant solution to the model-order problem, and eliminates the need for external cluster-validation</context>
</contexts>
<marker>2007</marker>
<rawString>Agirre, Eneko, Llu´ıs M`arquez, and Richard Wicentowski, editors. 2007. Proceedings of the SemEval-2007. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007.</booktitle>
<pages>7--12</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5160" citStr="Agirre and Soroa, 2007" startWordPosition="820" endWordPosition="823">inctions. In our work, therefore, we create an individual model for every (ambiguous) word rather than a global model for an entire document collection. We also show how multiple information sources can be straightforwardly integrated without changing the underlying probabilistic model. For instance, besides lexical information we may want to consider parts of speech or dependencies in our sense induction problem. This is in marked contrast with previous LDA-based models which mostly take only word-based information into account. We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art. The remainder of this paper is structured as follows. We first present an overview of related work (Section 2) and then describe our Bayesian model in more detail (Sections 3 and 4). Section 5 describes the resources and evaluation methodology used in our experiments. We discuss our results in Section 6, and conclude in Section 7. 2 Related Work Sense induction is typically treated as a clustering problem, where instances of a target word are partitioned into classes by considering their co-occurring contexts. Considerable latitude is al</context>
<context position="19564" citStr="Agirre and Soroa, 2007" startWordPosition="3252" endWordPosition="3255">ove. We give details on our training procedure, describe our features, and explain how our system output was evaluated. Data In this work, we focus solely on inducing senses for nouns, since they constitute the largest portion of content words. For example, nouns represent 45% of the content words in the British National Corpus. Moreover, for many tasks and applications (e.g., web queries, Jansen et al. 2000) nouns are the most frequent and most important part-of-speech. For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007). The dataset contains texts from the Penn Treebank II corpus, a collection of articles from the first half of the 1989 Wall Street Journal (WSJ). It is hand-annotated with OntoNotes senses (Hovy et al., 2006) and has 35 nouns. The average noun ambiguity is 3.9, with a high (almost 80%) skew towards the predominant sense. This is not entirely surprising since OntoNotes senses are less fine-grained than WordNet senses. We used two corpora for training as we wanted to evaluate our model’s performance across different domains. The British National Corpus (BNC) is a 100 million word collection of </context>
<context position="21100" citStr="Agirre and Soroa (2007)" startWordPosition="3503" endWordPosition="3506">ns in the Semeval lexical sample. The second, in-domain, corpus was built from selected portions of the Wall Street Journal. We used all articles (excluding the Penn Treebank II portion used in the Semeval dataset) from the years 1987-89 and 1994 to create a corpus of similar size to the BNC, containing approximately 740 thousand instances of the target words. Additionally, we used the Senseval 2 and 3 lexical sample data (Preiss and Yarowsky, 2001; Mihalcea and Edmonds, 2004) as development sets, for experimenting with the hyper-parameters of our model (see Section 6). Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods. Under the first (5) p(si|l,s−i,αl) = f p(si|l,s−i,θ)· p(θ|f−i,αl)dθ = #l(si)+αl #l +S·αl 107 scheme, the system output is compared to the gold standard using standard clustering evaluation metrics (e.g., purity, entropy). Here, no attempt is made to match the induced senses against the labels of the gold standard. Under the second scheme, the gold standard is partitioned into a test and training corpus. The latter is used to derive a mapping of the induced senses to the gold standard labels. The mapping is then used to calc</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Agirre, Eneko and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of SemEval-2007. Prague, Czech Republic, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Barnard</author>
<author>P Duygulu</author>
<author>D Forsyth</author>
<author>N De Freitas</author>
<author>D M Blei</author>
<author>M I Jordan</author>
</authors>
<title>Matching words and pictures.</title>
<date>2003</date>
<journal>J. of Machine Learning Research</journal>
<volume>3</volume>
<issue>6</issue>
<marker>Barnard, Duygulu, Forsyth, De Freitas, Blei, Jordan, 2003</marker>
<rawString>Barnard, K., P. Duygulu, D. Forsyth, N. De Freitas, D. M. Blei, and M. I. Jordan. 2003. Matching words and pictures. J. of Machine Learning Research 3(6):1107–1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<pages>3--993</pages>
<contexts>
<context position="3717" citStr="Blei et al. 2003" startWordPosition="595" endWordPosition="598">paper we adopt a novel Bayesian approach and formalize the induction problem in a generative model. For each ambiguous word we first draw a distribution over senses, and then generate context words according to this distribution. It is thus assumed that different senses will correspond to distinct lexical distributions. In this framework, sense distinctions arise naturally through the generative process: our model postulates that the observed data (word contexts) are explicitly intended to communicate a latent structure (their meaning). Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation. LDA models each document using a mixture over K topics, which are in turn characterized as distributions over words. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 103–111, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 103 The words in the document are generated by repeatedly sampling a topic according to the topic distribution, and selecting a word given the chosen topic. Whereas LDA generates words from global topics corresponding to the whole document, our model generates wo</context>
<context position="11856" citStr="Blei et al. (2003)" startWordPosition="1917" endWordPosition="1920">et word has C contexts and each context c Figure 1: Bayesian sense induction model; shaded nodes represent observed variables, unshaded nodes indicate latent variables. Arrows indicate conditional dependencies between variables, whereas plates (the rectangles in the figure) refer to repetitions of sampling steps. The variables in the lower right corner refer to the number of samples. consists of Nc word tokens. We shall write �(j) as a shorthand for P(wi|si = j), the multinomial distribution over words for sense j, and 0(c) as a shorthand for the distribution of senses in context c. Following Blei et al. (2003) we will assume that the mixing proportion over senses 0 is drawn from a Dirichlet prior with parameters a. The role of the hyperparameter a is to create a smoothed sense distribution. We also place a symmetric Dirichlet R on � (Griffiths and Steyvers, 2002). The hyperparmeter R can be interpreted as the prior observation count on the number of times context words are sampled from a sense before any word from the corpus is observed. Our model is represented in graphical notation in Figure 1. The model sketched above only takes word information into account. Methods developed for supervised WSD</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Bordag</author>
</authors>
<title>Word sense induction: Triplet-based clustering and automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th EACL.</booktitle>
<pages>137--144</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="6270" citStr="Bordag, 2006" startWordPosition="999" endWordPosition="1000">rd are partitioned into classes by considering their co-occurring contexts. Considerable latitude is allowed in selecting and representing the cooccurring contexts. Previous methods have used first or second order co-occurrences (Purandare and Pedersen, 2004; Sch¨utze, 1998), parts of speech (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003). The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004). In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen clustering method. A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et al., 2007). Graph-based methods have also been applied to the sense induction task. In this framework words are represented as nodes in the graph and </context>
</contexts>
<marker>Bordag, 2006</marker>
<rawString>Bordag, Stefan. 2006. Word sense induction: Triplet-based clustering and automatic evaluation. In Proceedings of the 11th EACL. Trento, Italy, pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
</authors>
<title>Putop: Turning predominant senses into a topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007.</booktitle>
<pages>277--281</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7438" citStr="Boyd-Graber and Blei (2007)" startWordPosition="1179" endWordPosition="1182"> framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences. Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003). Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature. The inferred document-level topics can help determine coarsegrained sense distinctions. Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system. In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD. Here, LDA topics are integrated with McCarthy et al.’s (2004) algorithm. For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic. Also, a distributional neighbor is selected based on the topic and distributional similarity to the generated word. Then, the word sense is selected based on the word, neighbor, and topic. Boyd-Graber et al. (2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process. In thi</context>
</contexts>
<marker>Boyd-Graber, Blei, 2007</marker>
<rawString>Boyd-Graber, Jordan and David Blei. 2007. Putop: Turning predominant senses into a topic model for word sense disambiguation. In Proceedings of SemEval-2007. Prague, Czech Republic, pages 277–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLP-CoNLL.</booktitle>
<pages>1024--1033</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7916" citStr="Boyd-Graber et al. (2007)" startWordPosition="1259" endWordPosition="1262">007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system. In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD. Here, LDA topics are integrated with McCarthy et al.’s (2004) algorithm. For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic. Also, a distributional neighbor is selected based on the topic and distributional similarity to the generated word. Then, the word sense is selected based on the word, neighbor, and topic. Boyd-Graber et al. (2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process. In this case the model discovers both the topics of the corpus and the senses assigned to each of its words. Our own model is also inspired by LDA but crucially performs word sense induction, not disambiguation. Unlike the work mentioned above, we do not rely on a pre-existing list of senses, and do not assume a correspondence between our automatically derived sense-clusters and those of any given inventory.2 A key element in these previous attempts at adapting LDA for WSD is the</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Boyd-Graber, Jordan, David Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In Proceedings of the EMNLP-CoNLL. Prague, Czech Republic, pages 1024–1033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd LREC. Las Palmas, Gran Canaria,</booktitle>
<pages>1499--1504</pages>
<contexts>
<context position="23149" citStr="Briscoe and Carroll, 2002" startWordPosition="3832" endWordPosition="3835">imented with six feature categories: ±10-word window (10w), ±5-word window (5w), collocations (1w), word n-grams (ng), part-ofspeech n-grams (pg) and dependency relations (dp). These features have been widely adopted in various WSD algorithms (see Lee and Ng 2002 for a detailed evaluation). In all cases, we use the lemmatized version of the word(s). The Semeval workshop organizers provided a small amount of context for each instance (usually a sentence or two surrounding the sentence containing the target word). This context, as well as the text in the training corpora, was parsed using RASP (Briscoe and Carroll, 2002), to extract part-of-speech tags, lemmas, and dependency information. For instances containing more than one occurrence of the target word, we disambiguate the first occurrence. Instances which were not correctly recognized by the parser (e.g., a target word labeled with the wrong lemma or part-of-speech), were automatically assigned to the largest sensecluster.3 3This was the case for less than 1% of the instances. 3 4 5 6 7 8 9 Number of Senses Figure 3: Model performance with varying number of senses on the WSJ and BNC corpora. 6 Experiments Model Selection The framework presented in Sectio</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Briscoe, Ted and John Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd LREC. Las Palmas, Gran Canaria, pages 1499–1504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Cai</author>
<author>W S Lee</author>
<author>Y W Teh</author>
</authors>
<title>Improving word sense disambiguation using topic features.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLP-CoNLL.</booktitle>
<pages>1015--1023</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7295" citStr="Cai et al. (2007)" startWordPosition="1155" endWordPosition="1158">), and the Information Bottleneck (Niu et al., 2007). Graph-based methods have also been applied to the sense induction task. In this framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences. Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003). Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature. The inferred document-level topics can help determine coarsegrained sense distinctions. Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system. In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD. Here, LDA topics are integrated with McCarthy et al.’s (2004) algorithm. For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic. Also, a distributional neighbor is selected based on the topic and distributional similarity to the generated word. Then, the word sense is selected based on the word, neighbor, and topic. Boyd</context>
</contexts>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>Cai, J. F., W. S. Lee, and Y. W. Teh. 2007. Improving word sense disambiguation using topic features. In Proceedings of the EMNLP-CoNLL. Prague, Czech Republic, pages 1015–1023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Word sense disambiguation vs. statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL. Ann Arbor, MI,</booktitle>
<pages>387--394</pages>
<contexts>
<context position="2651" citStr="Carpuat and Wu, 2005" startWordPosition="428" endWordPosition="431">istinctions which is fixed, and may not be entirely suitable for different applications. In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand. There is little risk that an important sense will be left out, or that irrelevant senses will influence the results. Furthermore, recent work in machine translation (Vickrey et al., 2005) and information retrieval (V´eronis, 2004) indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed (Carpuat and Wu, 2005; Voorhees, 1993). Sense induction is typically treated as an unsupervised clustering problem. The input to the clustering algorithm are instances of the ambiguous word with their accompanying contexts (represented by co-occurrence vectors) and the output is a grouping of these instances into classes corresponding to the induced senses. In other words, contexts that are grouped together in the same class represent a specific word sense. In this paper we adopt a novel Bayesian approach and formalize the induction problem in a generative model. For each ambiguous word we first draw a distributio</context>
</contexts>
<marker>Carpuat, Wu, 2005</marker>
<rawString>Carpuat, Marine and Dekai Wu. 2005. Word sense disambiguation vs. statistical machine translation. In Proceedings of the 43rd ACL. Ann Arbor, MI, pages 387–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beate Dorow</author>
<author>Dominic Widdows</author>
</authors>
<title>Discovering corpus-specific word senses.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th EACL.</booktitle>
<pages>79--82</pages>
<location>Budapest, Hungary,</location>
<contexts>
<context position="6055" citStr="Dorow and Widdows, 2003" startWordPosition="958" endWordPosition="961">es and evaluation methodology used in our experiments. We discuss our results in Section 6, and conclude in Section 7. 2 Related Work Sense induction is typically treated as a clustering problem, where instances of a target word are partitioned into classes by considering their co-occurring contexts. Considerable latitude is allowed in selecting and representing the cooccurring contexts. Previous methods have used first or second order co-occurrences (Purandare and Pedersen, 2004; Sch¨utze, 1998), parts of speech (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003). The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004). In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen clustering method. A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clu</context>
</contexts>
<marker>Dorow, Widdows, 2003</marker>
<rawString>Dorow, Beate and Dominic Widdows. 2003. Discovering corpus-specific word senses. In Proceedings of the 10th EACL. Budapest, Hungary, pages 79–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A Synopsis of Linguistic Theory 1930-1955.</title>
<date>1957</date>
<publisher>Philological Society.</publisher>
<location>Oxford:</location>
<contexts>
<context position="10164" citStr="Firth (1957)" startWordPosition="1626" endWordPosition="1627">y in turn enables us to evaluate which linguistic information matters for the sense induction task. Previous attempts to handle multiple information sources in the LDA framework (e.g., Griffiths et al. 2005; Barnard et al. 2003) have been task-specific and limited to only two layers of information. Our model provides this utility in a general framework, and could be applied to other tasks, besides sense induction. 3 The Sense Induction Model The core idea behind sense induction is that contextual information provides important cues regarding a word’s meaning. The idea dates back to (at least) Firth (1957) (“You shall know a word by the company it keeps”), and underlies most WSD and lexicon acquisition work to date. Under this premise, we should expect different senses to be signaled by different lexical distributions. We can place sense induction in a probabilistic setting by modeling the context words around the ambiguous target as samples from a multinomial sense distribution. More formally, we will write P(s) for the distribution over senses s of an ambiguous target in a specific context window and P(w1s) for the probability distribution over context words w given sense s. Each word wi in t</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>Firth, J. R. 1957. A Synopsis of Linguistic Theory 1930-1955. Oxford: Philological Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Gauch</author>
<author>Robert P Futrelle</author>
</authors>
<title>Experiments in automatic word class and word sense identification for information retrieval.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd Annual Symposium on Document Analysis and Information Retrieval. Las Vegas, NV,</booktitle>
<pages>425--434</pages>
<contexts>
<context position="6208" citStr="Gauch and Futrelle, 1993" startWordPosition="987" endWordPosition="990"> typically treated as a clustering problem, where instances of a target word are partitioned into classes by considering their co-occurring contexts. Considerable latitude is allowed in selecting and representing the cooccurring contexts. Previous methods have used first or second order co-occurrences (Purandare and Pedersen, 2004; Sch¨utze, 1998), parts of speech (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003). The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004). In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen clustering method. A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et al., 2007). Graph-based methods have also been applied to the sense induction task. In t</context>
</contexts>
<marker>Gauch, Futrelle, 1993</marker>
<rawString>Gauch, Susan and Robert P. Futrelle. 1993. Experiments in automatic word class and word sense identification for information retrieval. In Proceedings of the 3rd Annual Symposium on Document Analysis and Information Retrieval. Las Vegas, NV, pages 425–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distribution, and Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence</journal>
<volume>6</volume>
<issue>6</issue>
<contexts>
<context position="15601" citStr="Geman and Geman, 1984" startWordPosition="2553" endWordPosition="2556"> around the target word can have multiple representations, all of which share the same sense distribution. We illustrate this in Figure 2 where each inner rectangle (layer) corresponds to a distinct feature type. We will naively assume independence between multiple layers, even though this is clearly not the case in our task. The idea here is to model each layer as faithfully as possible to the empirical data while at the same time combining information from all layers in estimating the sense distribution of each target instance. 4 Inference Our inference procedure is based on Gibbs sampling (Geman and Geman, 1984). The procedure begins by randomly initializing all unobserved random variables. At each iteration, each random variable si is sampled from the conditional distribution P(si|s−i) where s−i refers to all variables other than si. Eventually, the distribution over samples drawn from this process will converge to the f p(fi |l, S, 0) · p(O |f-i, Rt)dO _ #(fi, si) + Rl J #(si) +Vl · Rl For the likelihood term p(fi|s, f −i,R), integrating over all possible values of the multinomial featuresense distribution � gives us the rightmost term in Equation 3, which has an intuitive interpretation. The term </context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Geman, S. and D. Geman. 1984. Stochastic relaxation, Gibbs distribution, and Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence 6(6):721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th ACL.</booktitle>
<pages>744--751</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="33461" citStr="Goldwater and Griffiths (2007)" startWordPosition="5530" endWordPosition="5533">basis of latent variables. Our model incorporates features based on lexical information, parts of speech, and dependencies in a principled manner, and outperforms state-of-theart systems. Crucially, the approach is not specific to the sense induction task and can be adapted for other applications where it is desirable to take multiple levels of information into account. For example, in document classification, one could consider an accompanying image and its caption as possible additional layers to the main text. In the future, we hope to explore more rigorous parameter estimation techniques. Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values. Such an approach could be adopted in our framework, as well, and extended to include the layer weighting parameters, which have strong potential for improving the model’s performance. In addition, we could allow an infinite number of senses and use an infinite Dirichlet model (Teh et al., 2006) to automatically determine how many senses are optimal. This provides an elegant solution to the model-order problem, and eliminates the need for external cluster-validation</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Goldwater, Sharon and Tom Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Proceedings of the 45th ACL. Prague, Czech Republic, pages 744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax. In</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems 17,</booktitle>
<pages>537--544</pages>
<editor>Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="9758" citStr="Griffiths et al. 2005" startWordPosition="1557" endWordPosition="1560"> is probabilistic; it specifies a probability distribution over possible values, which makes it easy to integrate and combine with other systems via mixture or product models. Furthermore, the Bayesian framework allows the incorporation of several information sources in a principled manner. Our model can easily handle an arbitrary number of feature classes (e.g., parts of speech, dependencies). This functionality in turn enables us to evaluate which linguistic information matters for the sense induction task. Previous attempts to handle multiple information sources in the LDA framework (e.g., Griffiths et al. 2005; Barnard et al. 2003) have been task-specific and limited to only two layers of information. Our model provides this utility in a general framework, and could be applied to other tasks, besides sense induction. 3 The Sense Induction Model The core idea behind sense induction is that contextual information provides important cues regarding a word’s meaning. The idea dates back to (at least) Firth (1957) (“You shall know a word by the company it keeps”), and underlies most WSD and lexicon acquisition work to date. Under this premise, we should expect different senses to be signaled by different</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Griffiths, Thomas L., Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2005. Integrating topics and syntax. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, Advances in Neural Information Processing Systems 17, MIT Press, Cambridge, MA, pages 537–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>A probabilistic approach to semantic representation.</title>
<date>2002</date>
<booktitle>In Proeedings of the 24th Annual Conference of the Cognitive Science Society.</booktitle>
<pages>381--386</pages>
<location>Fairfax, VA,</location>
<contexts>
<context position="12114" citStr="Griffiths and Steyvers, 2002" startWordPosition="1962" endWordPosition="1965">e rectangles in the figure) refer to repetitions of sampling steps. The variables in the lower right corner refer to the number of samples. consists of Nc word tokens. We shall write �(j) as a shorthand for P(wi|si = j), the multinomial distribution over words for sense j, and 0(c) as a shorthand for the distribution of senses in context c. Following Blei et al. (2003) we will assume that the mixing proportion over senses 0 is drawn from a Dirichlet prior with parameters a. The role of the hyperparameter a is to create a smoothed sense distribution. We also place a symmetric Dirichlet R on � (Griffiths and Steyvers, 2002). The hyperparmeter R can be interpreted as the prior observation count on the number of times context words are sampled from a sense before any word from the corpus is observed. Our model is represented in graphical notation in Figure 1. The model sketched above only takes word information into account. Methods developed for supervised WSD often use a variety of information sources based not only on words but also on lemmas, parts of speech, collocations and syntactic relationships (Lee and Ng, 2002). The first idea that comes to mind, is to use the same model while treating various features </context>
<context position="24624" citStr="Griffiths and Steyvers, 2002" startWordPosition="4080" endWordPosition="4084">nal parameters include the number of iterations for the Gibbs sampler and whether or not the layers are assigned different weights. Our strategy in this paper is to fix α and β and explore the consequences of varying S. The value for the α hyperparameter was set to 0.02. This was optimized in an independent tuning experiment which used the Senseval 2 (Preiss and Yarowsky, 2001) and Senseval 3 (Mihalcea and Edmonds, 2004) datasets. We experimented with α values ranging from 0.005 to 1. The β parameter was set to 0.1 (in all layers). This value is often considered optimal in LDA-related models (Griffiths and Steyvers, 2002). For simplicity, we used uniform weights for the layers. The Gibbs sampler was run for 2,000 iterations. Due to the randomized nature of the inference procedure, all reported results are average scores over ten runs. Our experiments used the same number of senses for all the words, since tuning this number individually for each word would be prohibitive. We experimented with values ranging from three to nine senses. Figure 3 shows the results obtained for different numbers of senses when the model is trained on the WSJ (in-domain) and BNC (out-ofdomain) corpora, respectively. Here, we are usi</context>
</contexts>
<marker>Griffiths, Steyvers, 2002</marker>
<rawString>Griffiths, Tom L. and Mark Steyvers. 2002. A probabilistic approach to semantic representation. In Proeedings of the 24th Annual Conference of the Cognitive Science Society. Fairfax, VA, pages 381–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT, Companion Volume: Short Papers. Association for Computational Linguistics,</booktitle>
<pages>57--60</pages>
<location>New York City, USA,</location>
<contexts>
<context position="19773" citStr="Hovy et al., 2006" startWordPosition="3288" endWordPosition="3291">gest portion of content words. For example, nouns represent 45% of the content words in the British National Corpus. Moreover, for many tasks and applications (e.g., web queries, Jansen et al. 2000) nouns are the most frequent and most important part-of-speech. For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007). The dataset contains texts from the Penn Treebank II corpus, a collection of articles from the first half of the 1989 Wall Street Journal (WSJ). It is hand-annotated with OntoNotes senses (Hovy et al., 2006) and has 35 nouns. The average noun ambiguity is 3.9, with a high (almost 80%) skew towards the predominant sense. This is not entirely surprising since OntoNotes senses are less fine-grained than WordNet senses. We used two corpora for training as we wanted to evaluate our model’s performance across different domains. The British National Corpus (BNC) is a 100 million word collection of samples of written and spoken language from a wide range of sources including newspapers, magazines, books (both academic and fiction), letters, and school essays as well as spontaneous conversations. This ser</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Hovy, Eduard, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of the HLT, Companion Volume: Short Papers. Association for Computational Linguistics, New York City, USA, pages 57–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Jansen</author>
<author>A Spink</author>
<author>A Pfaff</author>
</authors>
<title>Linguistic aspects of web queries.</title>
<date>2000</date>
<contexts>
<context position="19353" citStr="Jansen et al. 2000" startWordPosition="3221" endWordPosition="3224">signment i. Our system labels each instance with the single, most probable sense. 5 Evaluation Setup In this section we discuss our experimental set-up for assessing the performance of the model presented above. We give details on our training procedure, describe our features, and explain how our system output was evaluated. Data In this work, we focus solely on inducing senses for nouns, since they constitute the largest portion of content words. For example, nouns represent 45% of the content words in the British National Corpus. Moreover, for many tasks and applications (e.g., web queries, Jansen et al. 2000) nouns are the most frequent and most important part-of-speech. For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007). The dataset contains texts from the Penn Treebank II corpus, a collection of articles from the first half of the 1989 Wall Street Journal (WSJ). It is hand-annotated with OntoNotes senses (Hovy et al., 2006) and has 35 nouns. The average noun ambiguity is 3.9, with a high (almost 80%) skew towards the predominant sense. This is not entirely surprising since OntoNotes senses are less f</context>
</contexts>
<marker>Jansen, Spink, Pfaff, 2000</marker>
<rawString>Jansen, B. J., A. Spink, and A. Pfaff. 2000. Linguistic aspects of web queries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<pages>41--48</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="12620" citStr="Lee and Ng, 2002" startWordPosition="2051" endWordPosition="2054">create a smoothed sense distribution. We also place a symmetric Dirichlet R on � (Griffiths and Steyvers, 2002). The hyperparmeter R can be interpreted as the prior observation count on the number of times context words are sampled from a sense before any word from the corpus is observed. Our model is represented in graphical notation in Figure 1. The model sketched above only takes word information into account. Methods developed for supervised WSD often use a variety of information sources based not only on words but also on lemmas, parts of speech, collocations and syntactic relationships (Lee and Ng, 2002). The first idea that comes to mind, is to use the same model while treating various features as word-like elements. In other words, we could simply assume that the contexts we wish to model are the union of all our features. Although straightforward, this solution is undesirable. It merges the distributions of distinct feature categories into a single one, and is therefore conceptually incorrect, and can affect the performance of the model. For instance, parts-ofspeech (which have few values, and therefore high probability), would share a distribution with words (which are much sparser). Laye</context>
<context position="22786" citStr="Lee and Ng 2002" startWordPosition="3770" endWordPosition="3773">, therefore, we focused on the second evaluation scheme. Here, most of the participating systems outperformed the most-frequent-sense baseline, and the rest obtained only slightly lower scores. Feature Space Our experiments used a feature set designed to capture both immediate local context, wider context and syntactic context. Specifically, we experimented with six feature categories: ±10-word window (10w), ±5-word window (5w), collocations (1w), word n-grams (ng), part-ofspeech n-grams (pg) and dependency relations (dp). These features have been widely adopted in various WSD algorithms (see Lee and Ng 2002 for a detailed evaluation). In all cases, we use the lemmatized version of the word(s). The Semeval workshop organizers provided a small amount of context for each instance (usually a sentence or two surrounding the sentence containing the target word). This context, as well as the text in the training corpora, was parsed using RASP (Briscoe and Carroll, 2002), to extract part-of-speech tags, lemmas, and dependency information. For instances containing more than one occurrence of the target word, we disambiguate the first occurrence. Instances which were not correctly recognized by the parser</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Lee, Yoong Keok and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In Proceedings of the EMNLP. Morristown, NJ, USA, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd ACL.</booktitle>
<pages>280--287</pages>
<location>Barcelona,</location>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant senses in untagged text. In Proceedings of the 42nd ACL. Barcelona, Spain, pages 280–287.</rawString>
</citation>
<citation valid="true">
<date>2004</date>
<booktitle>Proceedings of the SENSEVAL-3.</booktitle>
<editor>Mihalcea, Rada and Phil Edmonds, editors.</editor>
<location>Barcelona.</location>
<contexts>
<context position="7568" citStr="(2004)" startWordPosition="1204" endWordPosition="1204">ing highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003). Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature. The inferred document-level topics can help determine coarsegrained sense distinctions. Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system. In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD. Here, LDA topics are integrated with McCarthy et al.’s (2004) algorithm. For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic. Also, a distributional neighbor is selected based on the topic and distributional similarity to the generated word. Then, the word sense is selected based on the word, neighbor, and topic. Boyd-Graber et al. (2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process. In this case the model discovers both the topics of the corpus and the senses assigned to each of its words. Our own model is also inspi</context>
</contexts>
<marker>2004</marker>
<rawString>Mihalcea, Rada and Phil Edmonds, editors. 2004. Proceedings of the SENSEVAL-3. Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng-Yu Niu</author>
<author>Dong-Hong Ji</author>
<author>Chew-Lim Tan</author>
</authors>
<title>I2r: Three systems for word sense discrimination, chinese word sense disambiguation, and english word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007). Association for Computational Linguistics,</booktitle>
<pages>177--182</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6730" citStr="Niu et al., 2007" startWordPosition="1069" endWordPosition="1072">relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004). In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen clustering method. A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et al., 2007). Graph-based methods have also been applied to the sense induction task. In this framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences. Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003). Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature. The inferred document-level topics can help determine coarsegrained sense distinctions. Cai et al. (2007) propose to use LDA’s word-topic di</context>
<context position="32125" citStr="Niu et al., 2007" startWordPosition="5323" endWordPosition="5326"> be less effective in the other. Another observable difference is that the combined model without the dependency layer does slightly better than each of the single layers. The 1w+pg combination improves over its components, which have similar individual performance. Finally, the best performing model on the BNC also combines two layers capturing wider (10w) and more local (5w) contextual information (see Table 3, right side). Comparison to State-of-the-Art Table 4 compares our model against the two best performing sense induction systems that participated in the Semeval-2007 competition. IR2 (Niu et al., 2007) performed sense induction using the Information Bottleneck algorithm, whereas UMND2 (Pedersen, 2007) used k-means to cluster second order co-occurrence vectors associated with the target word. These models and our own model significantly outperform the most-frequent-sense baseline (p &lt; 0.01 using a x2 test). Our best system (10w+5w on WSJ) is significantly better than UMND2 (p &lt; 0.01) and quantitatively better than IR2, although the difference is not statistically significant. 7 Discussion This paper presents a novel Bayesian approach to sense induction. We formulated sense induction in a gen</context>
</contexts>
<marker>Niu, Ji, Tan, 2007</marker>
<rawString>Niu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan. 2007. I2r: Three systems for word sense discrimination, chinese word sense disambiguation, and english word sense disambiguation. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007). Association for Computational Linguistics, Prague, Czech Republic, pages 177–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 8th KDD.</booktitle>
<pages>613--619</pages>
<location>New York, NY,</location>
<contexts>
<context position="6029" citStr="Pantel and Lin, 2002" startWordPosition="954" endWordPosition="957"> describes the resources and evaluation methodology used in our experiments. We discuss our results in Section 6, and conclude in Section 7. 2 Related Work Sense induction is typically treated as a clustering problem, where instances of a target word are partitioned into classes by considering their co-occurring contexts. Considerable latitude is allowed in selecting and representing the cooccurring contexts. Previous methods have used first or second order co-occurrences (Purandare and Pedersen, 2004; Sch¨utze, 1998), parts of speech (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003). The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004). In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen clustering method. A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Pantel, Patrick and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of the 8th KDD. New York, NY, pages 613–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Umnd2 : Senseclusters applied to the sense induction task of senseval-4.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007.</booktitle>
<pages>394--397</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="32226" citStr="Pedersen, 2007" startWordPosition="5337" endWordPosition="5339">ependency layer does slightly better than each of the single layers. The 1w+pg combination improves over its components, which have similar individual performance. Finally, the best performing model on the BNC also combines two layers capturing wider (10w) and more local (5w) contextual information (see Table 3, right side). Comparison to State-of-the-Art Table 4 compares our model against the two best performing sense induction systems that participated in the Semeval-2007 competition. IR2 (Niu et al., 2007) performed sense induction using the Information Bottleneck algorithm, whereas UMND2 (Pedersen, 2007) used k-means to cluster second order co-occurrence vectors associated with the target word. These models and our own model significantly outperform the most-frequent-sense baseline (p &lt; 0.01 using a x2 test). Our best system (10w+5w on WSJ) is significantly better than UMND2 (p &lt; 0.01) and quantitatively better than IR2, although the difference is not statistically significant. 7 Discussion This paper presents a novel Bayesian approach to sense induction. We formulated sense induction in a generative framework that describes how the contexts surrounding an ambiguous word might be generated on</context>
</contexts>
<marker>Pedersen, 2007</marker>
<rawString>Pedersen, Ted. 2007. Umnd2 : Senseclusters applied to the sense induction task of senseval-4. In Proceedings of SemEval-2007. Prague, Czech Republic, pages 394–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judita Preiss</author>
<author>David Yarowsky</author>
<author>editors</author>
</authors>
<date>2001</date>
<booktitle>Proceedings of the 2nd International Workshop on Evaluating Word Sense Disambiguation Systems.</booktitle>
<location>Toulouse, France.</location>
<marker>Preiss, Yarowsky, editors, 2001</marker>
<rawString>Preiss, Judita and David Yarowsky, editors. 2001. Proceedings of the 2nd International Workshop on Evaluating Word Sense Disambiguation Systems. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amruta Purandare</author>
<author>Ted Pedersen</author>
</authors>
<title>Word sense discrimination by clustering contexts in vector and similarity spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the CoNLL.</booktitle>
<pages>41--48</pages>
<location>Boston, MA,</location>
<contexts>
<context position="5915" citStr="Purandare and Pedersen, 2004" startWordPosition="937" endWordPosition="940"> an overview of related work (Section 2) and then describe our Bayesian model in more detail (Sections 3 and 4). Section 5 describes the resources and evaluation methodology used in our experiments. We discuss our results in Section 6, and conclude in Section 7. 2 Related Work Sense induction is typically treated as a clustering problem, where instances of a target word are partitioned into classes by considering their co-occurring contexts. Considerable latitude is allowed in selecting and representing the cooccurring contexts. Previous methods have used first or second order co-occurrences (Purandare and Pedersen, 2004; Sch¨utze, 1998), parts of speech (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003). The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004). In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen cluste</context>
<context position="9110" citStr="Purandare and Pedersen 2004" startWordPosition="1460" endWordPosition="1463">ts at adapting LDA for WSD is the tendency to remain at a high level, document-like, setting. In contrast, we make use of much smaller units of text (a few sentences, rather than a full document), and create an individual model for each (ambiguous) word type. Our induced senses are few in number (typically less than ten). This is in marked contrast to tens, and sometimes hundreds, 2Such a mapping is only performed to enable evaluation and comparison with other approaches (see Section 5). 104 of topics commonly used in document-modeling tasks. Unlike many conventional clustering methods (e.g., Purandare and Pedersen 2004; Sch¨utze 1998), our model is probabilistic; it specifies a probability distribution over possible values, which makes it easy to integrate and combine with other systems via mixture or product models. Furthermore, the Bayesian framework allows the incorporation of several information sources in a principled manner. Our model can easily handle an arbitrary number of feature classes (e.g., parts of speech, dependencies). This functionality in turn enables us to evaluate which linguistic information matters for the sense induction task. Previous attempts to handle multiple information sources i</context>
</contexts>
<marker>Purandare, Pedersen, 2004</marker>
<rawString>Purandare, Amruta and Ted Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In Proceedings of the CoNLL. Boston, MA, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Sch¨utze, Hinrich. 1998. Automatic word sense discrimination. Computational Linguistics 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association</journal>
<volume>101</volume>
<issue>476</issue>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Teh, Y. W., M. I. Jordan, M. J. Beal, and D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>Hyperlex: lexical cartography for information retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language</journal>
<volume>18</volume>
<issue>3</issue>
<marker>V´eronis, 2004</marker>
<rawString>V´eronis, Jean. 2004. Hyperlex: lexical cartography for information retrieval. Computer Speech &amp; Language 18(3):223–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Luke Biewald</author>
<author>Marc Teyssier</author>
<author>Daphne Koller</author>
</authors>
<title>Word-sense disambiguation for machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the HLT/EMNLP. Vancouver,</booktitle>
<pages>771--778</pages>
<contexts>
<context position="2444" citStr="Vickrey et al., 2005" startWordPosition="396" endWordPosition="399">of the art. of, dictionaries or other lexical resources, it is difficult to adapt them to new domains or to languages where such resources are scarce. A related problem concerns the granularity of the sense distinctions which is fixed, and may not be entirely suitable for different applications. In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand. There is little risk that an important sense will be left out, or that irrelevant senses will influence the results. Furthermore, recent work in machine translation (Vickrey et al., 2005) and information retrieval (V´eronis, 2004) indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed (Carpuat and Wu, 2005; Voorhees, 1993). Sense induction is typically treated as an unsupervised clustering problem. The input to the clustering algorithm are instances of the ambiguous word with their accompanying contexts (represented by co-occurrence vectors) and the output is a grouping of these instances into classes corresponding to the induced senses. In other words, contexts that are grouped together in </context>
</contexts>
<marker>Vickrey, Biewald, Teyssier, Koller, 2005</marker>
<rawString>Vickrey, David, Luke Biewald, Marc Teyssier, and Daphne Koller. 2005. Word-sense disambiguation for machine translation. In Proceedings of the HLT/EMNLP. Vancouver, pages 771–778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Using wordnet to disambiguate word senses for text retrieval.</title>
<date>1993</date>
<booktitle>In Proceedings of the 16th SIGIR.</booktitle>
<pages>171--180</pages>
<location>New York, NY,</location>
<contexts>
<context position="2668" citStr="Voorhees, 1993" startWordPosition="432" endWordPosition="433">ixed, and may not be entirely suitable for different applications. In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand. There is little risk that an important sense will be left out, or that irrelevant senses will influence the results. Furthermore, recent work in machine translation (Vickrey et al., 2005) and information retrieval (V´eronis, 2004) indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed (Carpuat and Wu, 2005; Voorhees, 1993). Sense induction is typically treated as an unsupervised clustering problem. The input to the clustering algorithm are instances of the ambiguous word with their accompanying contexts (represented by co-occurrence vectors) and the output is a grouping of these instances into classes corresponding to the induced senses. In other words, contexts that are grouped together in the same class represent a specific word sense. In this paper we adopt a novel Bayesian approach and formalize the induction problem in a generative model. For each ambiguous word we first draw a distribution over senses, an</context>
</contexts>
<marker>Voorhees, 1993</marker>
<rawString>Voorhees, Ellen M. 1993. Using wordnet to disambiguate word senses for text retrieval. In Proceedings of the 16th SIGIR. New York, NY, pages 171–180.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>