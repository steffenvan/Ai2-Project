<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015323">
<title confidence="0.973861">
Efficient Extraction of Oracle-best Translations from Hypergraphs
</title>
<author confidence="0.975955">
Zhifei Li and Sanjeev Khudanpur
</author>
<affiliation confidence="0.9167295">
Center for Language and Speech Processing and Department of Computer Science
The Johns Hopkins University, Baltimore, MD 21218, USA
</affiliation>
<email confidence="0.984312">
zhifei.work@gmail.com and khudanpur@jhu.edu
</email>
<sectionHeader confidence="0.992467" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998982285714286">
Hypergraphs are used in several syntax-
inspired methods of machine translation to
compactly encode exponentially many trans-
lation hypotheses. The hypotheses closest to
given reference translations therefore cannot
be found via brute force, particularly for pop-
ular measures of closeness such as BLEU. We
develop a dynamic program for extracting the
so called oracle-best hypothesis from a hyper-
graph by viewing it as the problem of finding
the most likely hypothesis under an n-gram
language model trained from only the refer-
ence translations. We further identify and re-
move massive redundancies in the dynamic
program state due to the sparsity of n-grams
present in the reference translations, resulting
in a very efficient program. We present run-
time statistics for this program, and demon-
strate successful application of the hypothe-
ses thus found as the targets for discriminative
training of translation system components.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933357142857">
A hypergraph, as demonstrated by Huang and Chi-
ang (2007), is a compact data-structure that can en-
code an exponential number of hypotheses gener-
ated by a regular phrase-based machine translation
(MT) system (e.g., Koehn et al. (2003)) or a syntax-
based MT system (e.g., Chiang (2007)). While the
hypergraph represents a very large set of transla-
tions, it is quite possible that some desired transla-
tions (e.g., the reference translations) are not con-
tained in the hypergraph, due to pruning or inherent
deficiency of the translation model. In this case, one
is often required to find the translation(s) in the hy-
pergraph that are most similar to the desired transla-
tions, with similarity computed via some automatic
</bodyText>
<page confidence="0.957119">
9
</page>
<bodyText confidence="0.999817184210526">
metric such as BLEU (Papineni et al., 2002). Such
maximally similar translations will be called oracle-
best translations, and the process of extracting them
oracle extraction. Oracle extraction is a nontrivial
task because computing the similarity of any one
hypothesis requires information scattered over many
items in the hypergraph, and the exponentially large
number of hypotheses makes a brute-force linear
search intractable. Therefore, efficient algorithms
that can exploit the structure of the hypergraph are
required.
We present an efficient oracle extraction algo-
rithm, which involves two key ideas. Firstly, we
view the oracle extraction as a bottom-up model
scoring process on a hypergraph, where the model is
“trained” on the reference translation(s). This is sim-
ilar to the algorithm proposed for a lattice by Dreyer
et al. (2007). Their algorithm, however, requires
maintaining a separate dynamic programming state
for each distinguished sequence of “state” words and
the number of such sequences can be huge, mak-
ing the search very slow. Secondly, therefore, we
present a novel look-ahead technique, called equiv-
alent oracle-state maintenance, to merge multiple
states that are equivalent for similarity computation.
Our experiments show that the equivalent oracle-
state maintenance technique significantly speeds up
(more than 40 times) the oracle extraction.
Efficient oracle extraction has at least three im-
portant applications in machine translation.
Discriminative Training: In discriminative train-
ing, the objective is to tune the model parameters,
e.g. weights of a perceptron model or conditional
random field, such that the reference translations are
preferred over competitors. However, the reference
translations may not be reachable by the translation
system, in which case the oracle-best hypotheses
should be substituted in training.
</bodyText>
<subsubsectionHeader confidence="0.83237">
Proceedings of NAACL HLT 2009: Short Papers, pages 9–12,
</subsubsectionHeader>
<bodyText confidence="0.97607865">
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
System Combination: In a typical system combi-
nation task, e.g. Rosti et al. (2007), each compo-
nent system produces a set of translations, which
are then grafted to form a confusion network. The
confusion network is then rescored, often employ-
ing additional (language) models, to select the fi-
nal translation. When measuring the goodness of a
hypothesis in the confusion network, one requires
its score under each component system. However,
some translations in the confusion network may not
be reachable by some component systems, in which
case a system’s score for the most similar reachable
translation serves as a good approximation.
Multi-source Translation: In a multi-source
translation task (Och and Ney, 2001) the input is
given in multiple source languages. This leads
to a situation analogous to system combination,
except that each component translation system now
corresponds to a specific source language.
</bodyText>
<sectionHeader confidence="0.599543" genericHeader="method">
2 Oracle Extraction on a Hypergraph
</sectionHeader>
<bodyText confidence="0.999927666666667">
In this section, we present the oracle extraction al-
gorithm: it extracts one or more translations in a hy-
pergraph that have the maximum BLEU score1 with
respect to the corresponding reference translation(s).
The BLEU score of a hypothesis h relative to a
reference r may be expressed in the log domain as,
</bodyText>
<equation confidence="0.99334225">
1 4
log BLEU(r, h) = min 1− i h ||, 0J + 1:4 log pn.
I
n=1
</equation>
<bodyText confidence="0.999937875">
The first component is the brevity penalty when
|h|&lt;|r|, while the second component corresponds to
the geometric mean of n-gram precisions pn (with
clipping). While BLEU is normally defined at the
corpus level, we use the sentence-level BLEU for
the purpose of oracle extraction.
Two key ideas for extracting the oracle-best hy-
pothesis from a hypergraph are presented next.
</bodyText>
<subsectionHeader confidence="0.990924">
2.1 Oracle Extraction as Model Scoring
</subsectionHeader>
<bodyText confidence="0.999034">
Our first key idea is to view the oracle extraction
as a bottom-up model scoring process on the hy-
pergraph. Specifically, we train a 4-gram language
model (LM) on only the reference translation(s),
</bodyText>
<footnote confidence="0.87386">
1We believe our method is general and can be extended to
other metrics capturing only n-gram dependency and other com-
pact data structures, e.g. lattices.
</footnote>
<bodyText confidence="0.999875958333334">
and use this LM as the only model to do a Viterbi
search on the hypergraph to find the hypothesis that
has the maximum (oracle) LM score. Essentially,
the LM is simply a table memorizing the counts of
n-grams found in the reference translation(s), and
the LM score is the log-BLEU value (instead of log-
probability, as in a regular LM). During the search,
the dynamic programming (DP) states maintained
at each item include the left- and right-side LM
context, and the length of the partial translation.
To compute the n-gram precisions pn incrementally
during the search, the algorithm also memorizes at
each item a vector of maximum numbers of n-gram
matches between the partial translation and the ref-
erence(s). Note however that the oracle state of an
item (which decides the uniqueness of an item) de-
pends only on the LM contexts and span lengths, not
on this vector of n-gram match counts.
The computation of BLEU also requires the
brevity penalty, but since there is no explicit align-
ment between the source and the reference(s), we
cannot get the exact reference length |r |at an inter-
mediate item. The exact value of brevity penalty is
thus not computable. We approximate the true refer-
ence length for an item with a product between the
length of the source string spanned by that item and
a ratio (which is between the lengths of the whole
reference and the whole source sentence). Another
approximation is that we do not consider the effect
of clipping, since it is a global feature, making the
strict computation intractable. This does not signifi-
cantly affect the quality of the oracle-best hypothesis
as shown later. Table 1 shows an example how the
BLEU scores are computed in the hypergraph.
The process above may be used either in a first-
stage decoding or a hypergraph-rescoring stage. In
the latter case, if the hypergraph generated by the
first-stage decoding does not have a set of DP states
that is a superset of the DP states required for ora-
cle extraction, we need to split the items of the first-
stage hypergraph and create new items with suffi-
ciently detailed states.
It is worth mentioning that if the hypergraph items
contain the state information necessary for extract-
ing the oracle-best hypothesis, it is straightforward
to further extract the k-best hypotheses in the hyper-
graph (according to BLEU) for any k &gt; 1 using the
algorithm of Huang and Chiang (2005).
</bodyText>
<page confidence="0.944963">
10
</page>
<table confidence="0.99873375">
Item |h ||i |matches log BLEU
Item A 5 6.2 (3, 2, 2, 1) -0.82
Item B 10 9.8 (8, 7, 6, 5) -0.27
Item C 17 18.3 (12, 10, 9, 6) -0.62
</table>
<tableCaption confidence="0.991075">
Table 1: Example computation when items A and B are
combined by a rule to produce item C. |r |is the approxi-
mated reference length as described in the text.
</tableCaption>
<subsectionHeader confidence="0.996742">
2.2 Equivalent Oracle State Maintenance
</subsectionHeader>
<bodyText confidence="0.999935552631579">
The process above, while able to extract the oracle-
best hypothesis from a hypergraph, is very slow due
to the need to maintain a dedicated item for each or-
acle state (i.e., a combination of left-LM state, right-
LM state, and hypothesis length). This is especially
true if the baseline system uses a LM whose order is
smaller than four, since we need to split the items in
the original hypergraph into many sub-items during
the search. To speed up the extraction, our second
key idea is to maintain an equivalent oracle state.
Roughly speaking, instead of maintaining a dif-
ferent state for different language model words, we
collapse them into a single state whenever it does not
affect BLEU. For example, if we have two left-side
LM states a b c and a b d, and we know that
the reference(s) do not have any n-gram ending with
them, then we can reduce them both to a b and ig-
nore the last word. This is because the combination
of neither left-side LM state (a b c or a b d) can
contribute an n-gram match to the BLEU computa-
tion, regardless of which prefix in the hypergraph
they combine with. Similarly, if we have two right-
side LM states a b c and d b c, and if we know
that the reference(s) do not have any n-gram starting
with either, then we can ignore the first word and re-
duce them both to b c. We can continue this reduc-
tion recursively as shown in Figures 1 and 2, where
IS-A-PREFIX(emi ) (or IS-A-SUFFIX(ei1)) checks if
em i (resp. ei1) is a prefix (suffix) of any n-gram in
the reference translation(s). For BLEU, 1 ≤ n ≤ 4.
This equivalent oracle state maintenance tech-
nique, in practice, dramatically reduces the number
of distinct items preserved in the hypergraph for or-
acle extraction. To understand this, observe that if
all hypotheses in the hypergraph together contain m
unique n-grams, for any fixed n, then the total num-
ber of equivalent items takes a multiplicative factor
that is O(m2) due to left- and right-side LM state
</bodyText>
<equation confidence="0.650036">
EQ-L-STATE (em1 )
1 els ← em1
2 for i ← m to 1 D right to left
3 if IS-A-SUFFIX(ei1)
</equation>
<figure confidence="0.9916154">
4 break D stop reducing els
5 else
6 els ← ei−1 D reduce state
1
7 return els
</figure>
<figureCaption confidence="0.996691">
Figure 1: Equivalent Left LM State Computation.
</figureCaption>
<figure confidence="0.932224125">
EQ-R-STATE (em1 )
1 ers ← em1
2 for i ← 1 to m D left to right
3 if IS-A-PREFIX (emi )
4 break D stop reducing ers
5 else
6 ers ← emi+1 D reduce state
7 return ers
</figure>
<figureCaption confidence="0.999806">
Figure 2: Equivalent Right LM State Computation.
</figureCaption>
<bodyText confidence="0.9998995">
maintenance of Section 2.1. This multiplicative fac-
tor under the equivalent state maintenance above is
O( ˜m2), where m˜ is the number of unique n-grams
in the reference translations. Clearly, m˜ « m by
several orders of magnitude, leading to effectively
much fewer items to process in the chart.
One may view this idea of maintaining equivalent
states more generally as an outside look-ahead dur-
ing bottom-up inside parsing. The look-ahead uses
some external information, e.g. IS-A-SUFFIX(·), to
anticipate whether maintaining a detailed state now
will be of consequence later; if not then the in-
side parsing eliminates or collapses the state into
a coarser state. The technique proposed by Li and
Khudanpur (2008a) for decoding with large LMs is
a special case of this general theme.
</bodyText>
<sectionHeader confidence="0.988933" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999977333333333">
We report experimental results on a Chinese to En-
glish task, for a system that is trained using a similar
pipeline and data resource as in Chiang (2007).
</bodyText>
<subsectionHeader confidence="0.999801">
3.1 Goodness of the Oracle-Best Translations
</subsectionHeader>
<bodyText confidence="0.99942525">
Table 2 reports the average speed (seconds/sentence)
for oracle extraction. Hypergraphs were generated
with a trigram LM and expanded on the fly for 4-
gram BLEU computation.
</bodyText>
<page confidence="0.998527">
11
</page>
<table confidence="0.820329">
Basic DP Collapse equiv. states speed-up
25.4 sec/sent 0.6 sec/sent × 42
</table>
<tableCaption confidence="0.997039333333333">
Table 2: Speed of oracle extraction from hypergraphs.
The basic dynamic program (Sec. 2.1) improves signifi-
cantly by collapsing equivalent oracle states (Sec. 2.2).
</tableCaption>
<bodyText confidence="0.9380274">
Table 3 reports the goodness of the oracle-best hy-
potheses on three standard data sets. The highest
achievable BLEU score in a hypergraph is clearly
much higher than in the 500-best unique strings.
This shows that a hypergraph provides a much better
basis, e.g., for reranking than an n-best list.
As mentioned in Section 2.1, we use several ap-
proximations in computing BLEU (e.g., no clipping
and approximate reference length). To justify these
approximations, we first extract 500-best unique or-
acles from the hypergraph, and then rerank the ora-
cles based on the true sentence-level BLEU. The last
row of Table 3 reports the reranked one-best oracle
BLEU scores. Clearly, the approximations do not
hurt the oracle BLEU very much.
</bodyText>
<table confidence="0.9977382">
Hypothesis space MT’04 MT’05 MT’06
1-best (Baseline) 35.7 32.6 28.3
500-unique-best 44.0 41.2 35.1
Hypergraph 52.8 51.8 37.8
500-best oracles 53.2 52.2 38.0
</table>
<tableCaption confidence="0.9150955">
Table 3: Baseline and oracle-best 4-gram BLEU scores
with 4 references for NIST Chinese-English MT datasets.
</tableCaption>
<subsectionHeader confidence="0.988047">
3.2 Discriminative Hypergraph-Reranking
</subsectionHeader>
<bodyText confidence="0.997246428571429">
Oracle extraction is a critical component for
hypergraph-based discriminative reranking, where
millions of model parameters are discriminatively
tuned to prefer the oracle-best hypotheses over oth-
ers. Hypergraph-reranking in MT is similar to the
forest-reranking for monolingual parsing (Huang,
2008). Moreover, once the oracle-best hypothesis
is identified, discriminative models may be trained
on hypergraphs in the same way as on n-best lists
(cf e.g. Li and Khudanpur (2008b)). The results in
Table 4 demonstrate that hypergraph-reranking with
a discriminative LM or TM improves upon the base-
line models on all three test sets. Jointly training
both the LM and TM likely suffers from over-fitting.
</bodyText>
<table confidence="0.9998654">
Test Set MT’04 MT’05 MT’06
Baseline 35.7 32.6 28.3
Discrim. LM 35.9 33.0 28.2
Discrim. TM 36.1 33.2 28.7
Discrim. TM+LM 36.0 33.1 28.6
</table>
<tableCaption confidence="0.840444">
Table 4: BLEU scores after discriminative hypergraph-
reranking. Only the language model (LM) or the transla-
tion model (TM) or both (LM+TM) may be discrimina-
tively trained to prefer the oracle-best hypotheses.
</tableCaption>
<sectionHeader confidence="0.999158" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999968375">
We have presented an efficient algorithm to extract
the oracle-best translation hypothesis from a hyper-
graph. To this end, we introduced a novel technique
for equivalent oracle state maintenance, which sig-
nificantly speeds up the oracle extraction process.
Our algorithm has clear applications in diverse tasks
such as discriminative training, system combination
and multi-source translation.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999888821428571">
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201-228.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing Reordering Constraints for SMT Using Efficient
BLEU Oracle Computation. In Proc. of SSST.
L. Huang. 2008. Forest Reranking: Discriminative Pars-
ing with Non-Local Features. In Proc. of ACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
Proc. of IWPT.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
of ACL.
P. Koehn, F. J. Och, and D. Marcu.2003. Statistical
phrase-based translation. In Proc. of NAACL.
Z. Li and S. Khudanpur. 2008a. A Scalable Decoder for
Parsing-based Machine Translation with Equivalent
Language Model State Maintenance. In Proc. SSST.
Z. Li and S. Khudanpur. 2008b. Large-scale Discrimina-
tive n-gram Language Models for Statistical Machine
Translation. In Proc. of AMTA.
F. Och and H. Ney. 2001. Statistical multisource transla-
tion. In Proc. MT Summit VIII.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
A.I. Rosti, S. Matsoukas, and R. Schwartz. 2007. Im-
proved word-level system combination for machine
translation. In Proc. of ACL.
</reference>
<page confidence="0.99846">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.844961">
<title confidence="0.999955">Efficient Extraction of Oracle-best Translations from Hypergraphs</title>
<author confidence="0.986877">Li</author>
<affiliation confidence="0.998376">Center for Language and Speech Processing and Department of Computer</affiliation>
<address confidence="0.8611">The Johns Hopkins University, Baltimore, MD 21218,</address>
<abstract confidence="0.999714590909091">used in several syntaxinspired methods of machine translation to compactly encode exponentially many translation hypotheses. The hypotheses closest to translations cannot be found via brute force, particularly for popular measures of closeness such as BLEU. We develop a dynamic program for extracting the called hypothesis a hypergraph by viewing it as the problem of finding most likely hypothesis under an model from only the reference translations. We further identify and remove massive redundancies in the dynamic state due to the sparsity of present in the reference translations, resulting in a very efficient program. We present runtime statistics for this program, and demonstrate successful application of the hypotheses thus found as the targets for discriminative training of translation system components.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--2</pages>
<contexts>
<context position="1280" citStr="Chiang (2007)" startWordPosition="186" endWordPosition="188">m a hypergraph by viewing it as the problem of finding the most likely hypothesis under an n-gram language model trained from only the reference translations. We further identify and remove massive redundancies in the dynamic program state due to the sparsity of n-grams present in the reference translations, resulting in a very efficient program. We present runtime statistics for this program, and demonstrate successful application of the hypotheses thus found as the targets for discriminative training of translation system components. 1 Introduction A hypergraph, as demonstrated by Huang and Chiang (2007), is a compact data-structure that can encode an exponential number of hypotheses generated by a regular phrase-based machine translation (MT) system (e.g., Koehn et al. (2003)) or a syntaxbased MT system (e.g., Chiang (2007)). While the hypergraph represents a very large set of translations, it is quite possible that some desired translations (e.g., the reference translations) are not contained in the hypergraph, due to pruning or inherent deficiency of the translation model. In this case, one is often required to find the translation(s) in the hypergraph that are most similar to the desired </context>
<context position="12072" citStr="Chiang (2007)" startWordPosition="2018" endWordPosition="2019">ates more generally as an outside look-ahead during bottom-up inside parsing. The look-ahead uses some external information, e.g. IS-A-SUFFIX(·), to anticipate whether maintaining a detailed state now will be of consequence later; if not then the inside parsing eliminates or collapses the state into a coarser state. The technique proposed by Li and Khudanpur (2008a) for decoding with large LMs is a special case of this general theme. 3 Experimental Results We report experimental results on a Chinese to English task, for a system that is trained using a similar pipeline and data resource as in Chiang (2007). 3.1 Goodness of the Oracle-Best Translations Table 2 reports the average speed (seconds/sentence) for oracle extraction. Hypergraphs were generated with a trigram LM and expanded on the fly for 4- gram BLEU computation. 11 Basic DP Collapse equiv. states speed-up 25.4 sec/sent 0.6 sec/sent × 42 Table 2: Speed of oracle extraction from hypergraphs. The basic dynamic program (Sec. 2.1) improves significantly by collapsing equivalent oracle states (Sec. 2.2). Table 3 reports the goodness of the oracle-best hypotheses on three standard data sets. The highest achievable BLEU score in a hypergraph</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dreyer</author>
<author>K Hall</author>
<author>S Khudanpur</author>
</authors>
<title>Comparing Reordering Constraints for SMT Using Efficient BLEU Oracle Computation.</title>
<date>2007</date>
<booktitle>In Proc. of SSST.</booktitle>
<contexts>
<context position="2783" citStr="Dreyer et al. (2007)" startWordPosition="422" endWordPosition="425">omputing the similarity of any one hypothesis requires information scattered over many items in the hypergraph, and the exponentially large number of hypotheses makes a brute-force linear search intractable. Therefore, efficient algorithms that can exploit the structure of the hypergraph are required. We present an efficient oracle extraction algorithm, which involves two key ideas. Firstly, we view the oracle extraction as a bottom-up model scoring process on a hypergraph, where the model is “trained” on the reference translation(s). This is similar to the algorithm proposed for a lattice by Dreyer et al. (2007). Their algorithm, however, requires maintaining a separate dynamic programming state for each distinguished sequence of “state” words and the number of such sequences can be huge, making the search very slow. Secondly, therefore, we present a novel look-ahead technique, called equivalent oracle-state maintenance, to merge multiple states that are equivalent for similarity computation. Our experiments show that the equivalent oraclestate maintenance technique significantly speeds up (more than 40 times) the oracle extraction. Efficient oracle extraction has at least three important application</context>
</contexts>
<marker>Dreyer, Hall, Khudanpur, 2007</marker>
<rawString>M. Dreyer, K. Hall, and S. Khudanpur. 2007. Comparing Reordering Constraints for SMT Using Efficient BLEU Oracle Computation. In Proc. of SSST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
</authors>
<title>Forest Reranking: Discriminative Parsing with Non-Local Features.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="13873" citStr="Huang, 2008" startWordPosition="2289" endWordPosition="2290">much. Hypothesis space MT’04 MT’05 MT’06 1-best (Baseline) 35.7 32.6 28.3 500-unique-best 44.0 41.2 35.1 Hypergraph 52.8 51.8 37.8 500-best oracles 53.2 52.2 38.0 Table 3: Baseline and oracle-best 4-gram BLEU scores with 4 references for NIST Chinese-English MT datasets. 3.2 Discriminative Hypergraph-Reranking Oracle extraction is a critical component for hypergraph-based discriminative reranking, where millions of model parameters are discriminatively tuned to prefer the oracle-best hypotheses over others. Hypergraph-reranking in MT is similar to the forest-reranking for monolingual parsing (Huang, 2008). Moreover, once the oracle-best hypothesis is identified, discriminative models may be trained on hypergraphs in the same way as on n-best lists (cf e.g. Li and Khudanpur (2008b)). The results in Table 4 demonstrate that hypergraph-reranking with a discriminative LM or TM improves upon the baseline models on all three test sets. Jointly training both the LM and TM likely suffers from over-fitting. Test Set MT’04 MT’05 MT’06 Baseline 35.7 32.6 28.3 Discrim. LM 35.9 33.0 28.2 Discrim. TM 36.1 33.2 28.7 Discrim. TM+LM 36.0 33.1 28.6 Table 4: BLEU scores after discriminative hypergraphreranking. </context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>L. Huang. 2008. Forest Reranking: Discriminative Parsing with Non-Local Features. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="8410" citStr="Huang and Chiang (2005)" startWordPosition="1337" endWordPosition="1340">ding or a hypergraph-rescoring stage. In the latter case, if the hypergraph generated by the first-stage decoding does not have a set of DP states that is a superset of the DP states required for oracle extraction, we need to split the items of the firststage hypergraph and create new items with sufficiently detailed states. It is worth mentioning that if the hypergraph items contain the state information necessary for extracting the oracle-best hypothesis, it is straightforward to further extract the k-best hypotheses in the hypergraph (according to BLEU) for any k &gt; 1 using the algorithm of Huang and Chiang (2005). 10 Item |h ||i |matches log BLEU Item A 5 6.2 (3, 2, 2, 1) -0.82 Item B 10 9.8 (8, 7, 6, 5) -0.27 Item C 17 18.3 (12, 10, 9, 6) -0.62 Table 1: Example computation when items A and B are combined by a rule to produce item C. |r |is the approximated reference length as described in the text. 2.2 Equivalent Oracle State Maintenance The process above, while able to extract the oraclebest hypothesis from a hypergraph, is very slow due to the need to maintain a dedicated item for each oracle state (i.e., a combination of left-LM state, rightLM state, and hypothesis length). This is especially true</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>L. Huang and D. Chiang. 2005. Better k-best parsing. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest Rescoring: Faster Decoding with Integrated Language Models.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1280" citStr="Huang and Chiang (2007)" startWordPosition="184" endWordPosition="188">thesis from a hypergraph by viewing it as the problem of finding the most likely hypothesis under an n-gram language model trained from only the reference translations. We further identify and remove massive redundancies in the dynamic program state due to the sparsity of n-grams present in the reference translations, resulting in a very efficient program. We present runtime statistics for this program, and demonstrate successful application of the hypotheses thus found as the targets for discriminative training of translation system components. 1 Introduction A hypergraph, as demonstrated by Huang and Chiang (2007), is a compact data-structure that can encode an exponential number of hypotheses generated by a regular phrase-based machine translation (MT) system (e.g., Koehn et al. (2003)) or a syntaxbased MT system (e.g., Chiang (2007)). While the hypergraph represents a very large set of translations, it is quite possible that some desired translations (e.g., the reference translations) are not contained in the hypergraph, due to pruning or inherent deficiency of the translation model. In this case, one is often required to find the translation(s) in the hypergraph that are most similar to the desired </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>L. Huang and D. Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. In Proc. of ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu 2003</author>
</authors>
<title>Statistical phrase-based translation.</title>
<booktitle>In Proc. of NAACL.</booktitle>
<marker>Koehn, Och, 2003, </marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu.2003. Statistical phrase-based translation. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>S Khudanpur</author>
</authors>
<title>A Scalable Decoder for Parsing-based Machine Translation with Equivalent Language Model State Maintenance. In</title>
<date>2008</date>
<booktitle>Proc. SSST.</booktitle>
<contexts>
<context position="11825" citStr="Li and Khudanpur (2008" startWordPosition="1972" endWordPosition="1975">e above is O( ˜m2), where m˜ is the number of unique n-grams in the reference translations. Clearly, m˜ « m by several orders of magnitude, leading to effectively much fewer items to process in the chart. One may view this idea of maintaining equivalent states more generally as an outside look-ahead during bottom-up inside parsing. The look-ahead uses some external information, e.g. IS-A-SUFFIX(·), to anticipate whether maintaining a detailed state now will be of consequence later; if not then the inside parsing eliminates or collapses the state into a coarser state. The technique proposed by Li and Khudanpur (2008a) for decoding with large LMs is a special case of this general theme. 3 Experimental Results We report experimental results on a Chinese to English task, for a system that is trained using a similar pipeline and data resource as in Chiang (2007). 3.1 Goodness of the Oracle-Best Translations Table 2 reports the average speed (seconds/sentence) for oracle extraction. Hypergraphs were generated with a trigram LM and expanded on the fly for 4- gram BLEU computation. 11 Basic DP Collapse equiv. states speed-up 25.4 sec/sent 0.6 sec/sent × 42 Table 2: Speed of oracle extraction from hypergraphs. T</context>
<context position="14050" citStr="Li and Khudanpur (2008" startWordPosition="2315" endWordPosition="2318">e 3: Baseline and oracle-best 4-gram BLEU scores with 4 references for NIST Chinese-English MT datasets. 3.2 Discriminative Hypergraph-Reranking Oracle extraction is a critical component for hypergraph-based discriminative reranking, where millions of model parameters are discriminatively tuned to prefer the oracle-best hypotheses over others. Hypergraph-reranking in MT is similar to the forest-reranking for monolingual parsing (Huang, 2008). Moreover, once the oracle-best hypothesis is identified, discriminative models may be trained on hypergraphs in the same way as on n-best lists (cf e.g. Li and Khudanpur (2008b)). The results in Table 4 demonstrate that hypergraph-reranking with a discriminative LM or TM improves upon the baseline models on all three test sets. Jointly training both the LM and TM likely suffers from over-fitting. Test Set MT’04 MT’05 MT’06 Baseline 35.7 32.6 28.3 Discrim. LM 35.9 33.0 28.2 Discrim. TM 36.1 33.2 28.7 Discrim. TM+LM 36.0 33.1 28.6 Table 4: BLEU scores after discriminative hypergraphreranking. Only the language model (LM) or the translation model (TM) or both (LM+TM) may be discriminatively trained to prefer the oracle-best hypotheses. 4 Conclusions We have presented </context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Z. Li and S. Khudanpur. 2008a. A Scalable Decoder for Parsing-based Machine Translation with Equivalent Language Model State Maintenance. In Proc. SSST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>S Khudanpur</author>
</authors>
<title>Large-scale Discriminative n-gram Language Models for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proc. of AMTA.</booktitle>
<contexts>
<context position="11825" citStr="Li and Khudanpur (2008" startWordPosition="1972" endWordPosition="1975">e above is O( ˜m2), where m˜ is the number of unique n-grams in the reference translations. Clearly, m˜ « m by several orders of magnitude, leading to effectively much fewer items to process in the chart. One may view this idea of maintaining equivalent states more generally as an outside look-ahead during bottom-up inside parsing. The look-ahead uses some external information, e.g. IS-A-SUFFIX(·), to anticipate whether maintaining a detailed state now will be of consequence later; if not then the inside parsing eliminates or collapses the state into a coarser state. The technique proposed by Li and Khudanpur (2008a) for decoding with large LMs is a special case of this general theme. 3 Experimental Results We report experimental results on a Chinese to English task, for a system that is trained using a similar pipeline and data resource as in Chiang (2007). 3.1 Goodness of the Oracle-Best Translations Table 2 reports the average speed (seconds/sentence) for oracle extraction. Hypergraphs were generated with a trigram LM and expanded on the fly for 4- gram BLEU computation. 11 Basic DP Collapse equiv. states speed-up 25.4 sec/sent 0.6 sec/sent × 42 Table 2: Speed of oracle extraction from hypergraphs. T</context>
<context position="14050" citStr="Li and Khudanpur (2008" startWordPosition="2315" endWordPosition="2318">e 3: Baseline and oracle-best 4-gram BLEU scores with 4 references for NIST Chinese-English MT datasets. 3.2 Discriminative Hypergraph-Reranking Oracle extraction is a critical component for hypergraph-based discriminative reranking, where millions of model parameters are discriminatively tuned to prefer the oracle-best hypotheses over others. Hypergraph-reranking in MT is similar to the forest-reranking for monolingual parsing (Huang, 2008). Moreover, once the oracle-best hypothesis is identified, discriminative models may be trained on hypergraphs in the same way as on n-best lists (cf e.g. Li and Khudanpur (2008b)). The results in Table 4 demonstrate that hypergraph-reranking with a discriminative LM or TM improves upon the baseline models on all three test sets. Jointly training both the LM and TM likely suffers from over-fitting. Test Set MT’04 MT’05 MT’06 Baseline 35.7 32.6 28.3 Discrim. LM 35.9 33.0 28.2 Discrim. TM 36.1 33.2 28.7 Discrim. TM+LM 36.0 33.1 28.6 Table 4: BLEU scores after discriminative hypergraphreranking. Only the language model (LM) or the translation model (TM) or both (LM+TM) may be discriminatively trained to prefer the oracle-best hypotheses. 4 Conclusions We have presented </context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Z. Li and S. Khudanpur. 2008b. Large-scale Discriminative n-gram Language Models for Statistical Machine Translation. In Proc. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Statistical multisource translation.</title>
<date>2001</date>
<booktitle>In Proc. MT Summit VIII.</booktitle>
<contexts>
<context position="4655" citStr="Och and Ney, 2001" startWordPosition="696" endWordPosition="699">system produces a set of translations, which are then grafted to form a confusion network. The confusion network is then rescored, often employing additional (language) models, to select the final translation. When measuring the goodness of a hypothesis in the confusion network, one requires its score under each component system. However, some translations in the confusion network may not be reachable by some component systems, in which case a system’s score for the most similar reachable translation serves as a good approximation. Multi-source Translation: In a multi-source translation task (Och and Ney, 2001) the input is given in multiple source languages. This leads to a situation analogous to system combination, except that each component translation system now corresponds to a specific source language. 2 Oracle Extraction on a Hypergraph In this section, we present the oracle extraction algorithm: it extracts one or more translations in a hypergraph that have the maximum BLEU score1 with respect to the corresponding reference translation(s). The BLEU score of a hypothesis h relative to a reference r may be expressed in the log domain as, 1 4 log BLEU(r, h) = min 1− i h ||, 0J + 1:4 log pn. I n</context>
</contexts>
<marker>Och, Ney, 2001</marker>
<rawString>F. Och and H. Ney. 2001. Statistical multisource translation. In Proc. MT Summit VIII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1983" citStr="Papineni et al., 2002" startWordPosition="302" endWordPosition="305"> generated by a regular phrase-based machine translation (MT) system (e.g., Koehn et al. (2003)) or a syntaxbased MT system (e.g., Chiang (2007)). While the hypergraph represents a very large set of translations, it is quite possible that some desired translations (e.g., the reference translations) are not contained in the hypergraph, due to pruning or inherent deficiency of the translation model. In this case, one is often required to find the translation(s) in the hypergraph that are most similar to the desired translations, with similarity computed via some automatic 9 metric such as BLEU (Papineni et al., 2002). Such maximally similar translations will be called oraclebest translations, and the process of extracting them oracle extraction. Oracle extraction is a nontrivial task because computing the similarity of any one hypothesis requires information scattered over many items in the hypergraph, and the exponentially large number of hypotheses makes a brute-force linear search intractable. Therefore, efficient algorithms that can exploit the structure of the hypergraph are required. We present an efficient oracle extraction algorithm, which involves two key ideas. Firstly, we view the oracle extrac</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A I Rosti</author>
<author>S Matsoukas</author>
<author>R Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4020" citStr="Rosti et al. (2007)" startWordPosition="598" endWordPosition="601">lation. Discriminative Training: In discriminative training, the objective is to tune the model parameters, e.g. weights of a perceptron model or conditional random field, such that the reference translations are preferred over competitors. However, the reference translations may not be reachable by the translation system, in which case the oracle-best hypotheses should be substituted in training. Proceedings of NAACL HLT 2009: Short Papers, pages 9–12, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al. (2007), each component system produces a set of translations, which are then grafted to form a confusion network. The confusion network is then rescored, often employing additional (language) models, to select the final translation. When measuring the goodness of a hypothesis in the confusion network, one requires its score under each component system. However, some translations in the confusion network may not be reachable by some component systems, in which case a system’s score for the most similar reachable translation serves as a good approximation. Multi-source Translation: In a multi-source t</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>A.I. Rosti, S. Matsoukas, and R. Schwartz. 2007. Improved word-level system combination for machine translation. In Proc. of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>