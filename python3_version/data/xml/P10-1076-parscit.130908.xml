<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000105">
<title confidence="0.996042">
Boosting-based System Combination for Machine Translation
</title>
<author confidence="0.991037">
Tong Xiao, Jingbo Zhu, Muhua Zhu, Huizhen Wang
</author>
<affiliation confidence="0.945482">
Natural Language Processing Lab.
Northeastern University, China
</affiliation>
<email confidence="0.98897">
{xiaotong,zhujingbo,wanghuizhen}@mail.neu.edu.cn
zhumuhua@gmail.com
</email>
<sectionHeader confidence="0.99731" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999629272727273">
In this paper, we present a simple and effective
method to address the issue of how to generate
diversified translation systems from a single
Statistical Machine Translation (SMT) engine
for system combination. Our method is based
on the framework of boosting. First, a se-
quence of weak translation systems is gener-
ated from a baseline system in an iterative
manner. Then, a strong translation system is
built from the ensemble of these weak transla-
tion systems. To adapt boosting to SMT sys-
tem combination, several key components of
the original boosting algorithms are redes-
igned in this work. We evaluate our method on
Chinese-to-English Machine Translation (MT)
tasks in three baseline systems, including a
phrase-based system, a hierarchical phrase-
based system and a syntax-based system. The
experimental results on three NIST evaluation
test sets show that our method leads to signifi-
cant improvements in translation accuracy
over the baseline systems.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99986975862069">
Recent research on Statistical Machine Transla-
tion (SMT) has achieved substantial progress.
Many SMT frameworks have been developed,
including phrase-based SMT (Koehn et al., 2003),
hierarchical phrase-based SMT (Chiang, 2005),
syntax-based SMT (Eisner, 2003; Ding and
Palmer, 2005; Liu et al., 2006; Galley et al., 2006;
Cowan et al., 2006), etc. With the emergence of
various structurally different SMT systems, more
and more studies are focused on combining mul-
tiple SMT systems for achieving higher transla-
tion accuracy rather than using a single transla-
tion system.
The basic idea of system combination is to ex-
tract or generate a translation by voting from an
ensemble of translation outputs. Depending on
how the translation is combined and what voting
strategy is adopted, several methods can be used
for system combination, e.g. sentence-level com-
bination (Hildebrand and Vogel, 2008) simply
selects one from original translations, while
some more sophisticated methods, such as word-
level and phrase-level combination (Matusov et
al., 2006; Rosti et al., 2007), can generate new
translations differing from any of the original
translations.
One of the key factors in SMT system combi-
nation is the diversity in the ensemble of transla-
tion outputs (Macherey and Och, 2007). To ob-
tain diversified translation outputs, most of the
current system combination methods require
multiple translation engines based on different
models. However, this requirement cannot be
met in many cases, since we do not always have
the access to multiple SMT engines due to the
high cost of developing and tuning SMT systems.
To reduce the burden of system development, it
might be a nice way to combine a set of transla-
tion systems built from a single translation en-
gine. A key issue here is how to generate an en-
semble of diversified translation systems from a
single translation engine in a principled way.
Addressing this issue, we propose a boosting-
based system combination method to learn a
combined translation system from a single SMT
engine. In this method, a sequence of weak trans-
lation systems is generated from a baseline sys-
tem in an iterative manner. In each iteration, a
new weak translation system is learned, focusing
more on the sentences that are relatively poorly
translated by the previous weak translation sys-
tem. Finally, a strong translation system is built
from the ensemble of the weak translation sys-
tems.
Our experiments are conducted on Chinese-to-
English translation in three state-of-the-art SMT
systems, including a phrase-based system, a hier-
archical phrase-based system and a syntax-based
</bodyText>
<page confidence="0.977309">
739
</page>
<note confidence="0.947305">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 739–748,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.906995">
Input: a model u, a sequence of (training) samples {(f1, r1), ..., (fm, rm)} where fi is the
i-th source sentence, and ri is the set of reference translations for fi.
Output: a new translation system
</bodyText>
<figureCaption confidence="0.999883">
Figure 1: Boosting-based System Combination
</figureCaption>
<figure confidence="0.993707823529412">
Initialize: D1(i) = 1 / m for all i = 1, ..., m
For t = 1, ..., T
1. Train a translation system u(λ*t) on {(fi, ri)} using distribution Dt
2. Calculate the error rate 6t of u(λ*t) on {(fi, ri)}
3. Set
1 1
=
2
6
6t
α t
�
t
ln( )
(3)
4. Update weights Dt � 1( ) = Dt (i)eαt-li (4)
i Zt
</figure>
<bodyText confidence="0.7376005">
where li is the loss on the i-th training sample, and Zt is the normalization factor.
Output the final system:
</bodyText>
<equation confidence="0.767714">
v(u(λ*1), ..., u (λ*T))
</equation>
<bodyText confidence="0.8939226">
system. All the systems are evaluated on three
NIST MT evaluation test sets. Experimental re-
sults show that our method leads to significant
improvements in translation accuracy over the
baseline systems.
</bodyText>
<sectionHeader confidence="0.994768" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9988385">
Given a source string f, the goal of SMT is to
find a target string e* by the following equation.
</bodyText>
<equation confidence="0.846092333333333">
arg max(Pr(  |))
e f
e
</equation>
<bodyText confidence="0.99983">
where Pr(e  |f ) is the probability that e is the
translation of the given source string f. To model
the posterior probability Pr(e |f) , most of the
state-of-the-art SMT systems utilize the log-
linear model proposed by Och and Ney (2002),
as follows,
</bodyText>
<equation confidence="0.8859035">
exp(Em=1Am- hm(f,e)) (2)
Ee &apos;exp(Em =1Am- hm(f,e&apos;))
</equation>
<bodyText confidence="0.999924642857143">
where {hm( f, e )  |m = 1, ..., M} is a set of fea-
tures, and Am is the feature weight corresponding
to the m-th feature. hm( f, e ) can be regarded as a
function that maps every pair of source string f
and target string e into a non-negative value, and
Am can be viewed as the contribution of hm( f, e )
to the overall score Pr(e |f).
In this paper, u denotes a log-linear model that
has M fixed features {h1( f ,e ), ..., hM( f ,e )}, λ =
{A1, ..., AM} denotes the M parameters of u, and
u(λ) denotes a SMT system based on u with pa-
rameters λ. Generally, λ is trained on a training
data set1 to obtain an optimized weight vector λ*
and consequently an optimized system u(λ*).
</bodyText>
<sectionHeader confidence="0.769911" genericHeader="method">
3 Boosting-based System Combination
</sectionHeader>
<subsectionHeader confidence="0.938853">
for Single Translation Engine
</subsectionHeader>
<bodyText confidence="0.999934916666667">
Suppose that there are T available SMT systems
{u1(λ*1), ..., uT(λ*T)}, the task of system combina-
tion is to build a new translation system
v(u1(λ*1), ..., uT(λ*T)) from {u1(λ*1), ..., uT(λ*T)}.
Here v(u1(λ*1), ..., uT(λ*T)) denotes the combina-
tion system which combines translations from the
ensemble of the output of each ui(λ*i). We call
ui(λ*i) a member system of v(u1(λ*1), ..., uT(λ*T)).
As discussed in Section 1, the diversity among
the outputs of member systems is an important
factor to the success of system combination. To
obtain diversified member systems, traditional
methods concentrate more on using structurally
different member systems, that is u1≠ u2 ≠...≠
uT. However, this constraint condition cannot be
satisfied when multiple translation engines are
not available.
In this paper, we argue that the diversified
member systems can also be generated from a
single engine u(λ*) by adjusting the weight vector
λ* in a principled way. In this work, we assume
that u1 = u2 =...= uT = u. Our goal is to find a se-
ries of λ*i and build a combined system from
{u(λ*i)}. To achieve this goal, we propose a
</bodyText>
<footnote confidence="0.99117675">
1 The data set used for weight training is generally called
development set or tuning set in the SMT field. In this paper,
we use the term training set to emphasize the training of
log-linear model.
</footnote>
<equation confidence="0.9925245">
*
e =
(1)
=
Pr(  |)
e f
</equation>
<page confidence="0.93876">
740
</page>
<bodyText confidence="0.995662733333333">
boosting-based system combination method (Fig-
ure 1).
Like other boosting algorithms, such as
AdaBoost (Freund and Schapire, 1997; Schapire,
2001), the basic idea of this method is to use
weak systems (member systems) to form a strong
system (combined system) by repeatedly calling
weak system trainer on different distributions
over the training samples. However, since most
of the boosting algorithms are designed for the
classification problem that is very different from
the translation problem in natural language proc-
essing, several key components have to be redes-
igned when boosting is adapted to SMT system
combination.
</bodyText>
<subsectionHeader confidence="0.995857">
3.1 Training
</subsectionHeader>
<bodyText confidence="0.999959882352941">
In this work, Minimum Error Rate Training
(MERT) proposed by Och (2003) is used to es-
timate feature weights λ over a series of training
samples. As in other state-of-the-art SMT sys-
tems, BLEU is selected as the accuracy measure
to define the error function used in MERT. Since
the weights of training samples are not taken into
account in BLEU2, we modify the original defi-
nition of BLEU to make it sensitive to the distri-
bution Dt(i) over the training samples. The modi-
fied version of BLEU is called weighted BLEU
(WBLEU) in this paper.
Let E = e1 ... em be the translations produced
by the system, R = r1 ... rm be the reference trans-
lations where ri = {ri1, ..., riN}, and Dt(i) be the
weight of the i-th training sample (fi, ri). The
weighted BLEU metric has the following form:
</bodyText>
<equation confidence="0.995336277777777">
WBLEU(E,R)
⎛ ∑ m
⎧ D i
( ) min  |( ) |
{ }
g r ⎫
ij ⎞
⎪ i 1 t 1
1 j N
exp 1 max 1,
⎜ = ≤ ≤ ⎪ ⎟
= − ⎨ m ⎟ ×
⎜ ⎬
⎜ ⎪ ( )  |( ) |
i
∑ D i g e ⎪ ⎟
i = 1 t 1
⎝ ⎩ ⎭ ⎠
</equation>
<bodyText confidence="0.999902142857143">
where gn (s) is the multi-set of all n-grams in a
string s. In this definition, n-grams in ei and {rij}
are weighted by Dt(i). If the i-th training sample
has a larger weight, the corresponding n-grams
will have more contributions to the overall score
WBLEU(E,R) . As a result, the i-th training
sample gains more importance in MERT. Obvi-
</bodyText>
<footnote confidence="0.784609333333333">
2 In this paper, we use the NIST definition of BLEU where
the effective reference length is the length of the shortest
reference translation.
</footnote>
<bodyText confidence="0.995263333333333">
ously the original BLEU is just a special case of
WBLEU when all the training samples are
equally weighted.
As the weighted BLEU is used to measure the
translation accuracy on the training set, the error
rate is defined to be:
</bodyText>
<equation confidence="0.648375">
εt =1− WBLEU(E,R) (6)
</equation>
<subsectionHeader confidence="0.999741">
3.2 Re-weighting
</subsectionHeader>
<bodyText confidence="0.999796157894737">
Another key point is the maintaining of the dis-
tribution Dt(i) over the training set. Initially all
the weights of training samples are set equally.
On each round, we increase the weights of the
samples that are relatively poorly translated by
the current weak system so that the MERT-based
trainer can focus on the hard samples in next
round. The update rule is given in Equation 4
with two parameters αt and li in it.
αt can be regarded as a measure of the im-
portance that the t-th weak system gains in boost-
ing. The definition of αt guarantees that αt al-
ways has a positive value3. A main effect of αt
is to scale the weight updating (e.g. a larger αt
means a greater update).
li is the loss on the i-th sample. For each i, let
{ei1, ..., ein} be the n-best translation candidates
produced by the system. The loss function is de-
fined to be:
</bodyText>
<equation confidence="0.983401">
1 k
*
li = e
BLEU( , )
i i BLEU( , )
eij i
r − ∑ r (7)
k =
j 1
</equation>
<bodyText confidence="0.999992571428572">
where BLEU(eij, ri) is the smoothed sentence-level
BLEU score (Liang et al., 2006) of the transla-
tion e with respect to the reference translations ri,
and ei* is the oracle translation which is selected
from {ei1, ..., ein} in terms of BLEU(eij, ri). li can
be viewed as a measure of the average cost that
we guess the top-k translation candidates instead
of the oracle translation. The value of li counts
for the magnitude of weight update, that is, a lar-
ger li means a larger weight update on Dt(i). The
definition of the loss function here is similar to
the one used in (Chiang et al., 2008) where only
the top-1 translation candidate (i.e. k = 1) is
taken into account.
</bodyText>
<subsectionHeader confidence="0.998104">
3.3 System Combination Scheme
</subsectionHeader>
<bodyText confidence="0.9995525">
In the last step of our method, a strong transla-
tion system v(u(λ*1), ..., u(λ*T)) is built from the
</bodyText>
<footnote confidence="0.90920175">
3 Note that the definition of αt here is different from that in
the original AdaBoost algorithm (Freund and Schapire,
1997; Schapire, 2001) where αt is a negative number when
εt&gt;0.5.
</footnote>
<equation confidence="0.978615555555556">
1/4
⎛ ∏ ∑m1Dt(i) gn(ei)I(UN1gn(rij)) ⎜
m ⎟
n = 1 ⎜ ∑ D i g e
( ) ( )
i
i = 1 t n ⎟
⎝ ⎠
(5)
</equation>
<page confidence="0.963761">
741
</page>
<bodyText confidence="0.964447181818182">
ensemble of member systems {u(A*1), ..., u(A*T)}.
In this work, a sentence-level combination
method is used to select the best translation from
the pool of the n-best outputs of all the member
systems.
Let H(u(A*t)) (or Ht for short) be the set of the
n-best translation candidates produced by the t-th
member system u(A*t), and H(v) be the union set
of all Ht (i.e. H(v) = UHt ). The final translation
is generated from H(v) based on the following
scoring function:
</bodyText>
<equation confidence="0.997684428571429">
e* = ∑ ⋅
T
arg max β φ e ψ e H v
( ) ( , ( ))
+ (8)
t =1 t t
e∈(H v)
</equation>
<bodyText confidence="0.847724">
where
</bodyText>
<equation confidence="0.44308">
(e) is the log-scaled model score of
in
the t-th member system, and
is the corre-
</equation>
<bodyText confidence="0.926165545454545">
sponding feature weight. It should be noted that
may not exist in any
i. In this case,
we can still calculate the model score of e in any
other member systems, since all the member sys-
tems are based on the same model and share the
same feature space.
is a consensus-
based scoring function which has been success-
fully adopted in SMT system combination (Duan
et al., 2009; Hildebrand an
</bodyText>
<equation confidence="0.967608166666667">
φt
e
βt
e ∈Hi
Hi&apos;≠
ψ(e,H(v))
</equation>
<bodyText confidence="0.96795225">
d Vogel, 2008; Li et
al., 2009). The computation of ψ(e,H(v)) is
based on a linear combination of a set of n-gram
consensuses-based features.
</bodyText>
<equation confidence="0.983815538461538">
ψ e H v
( , ( )) n
= ∑ ⋅
θ + h + e H v
n ( , ( )) +
n
n
hn (e, H(v))(
∑θ
⋅
9)
n
For each order of n-gram,
</equation>
<bodyText confidence="0.953766388888889">
and
are defined to measure the n-gram
agreement and disagreement between
and other
translation candidates in H(v), respectively.
and
are the feature weights corresponding to
(e, H(v)) and
used in our work are exactly the
same as the features used in (Duan et al., 2009)
and similar to the features used in (Hildebrand
and Vogel, 2008; Li et al., 2009), we do not pre-
sent the detailed description of them in this paper.
If p orders of n-gram are used in computing
the total number of features in the
system combination will be T + 2
p (T model-
score-based features defined in Equation 8 an
</bodyText>
<equation confidence="0.957021">
hn (e,H(v))
+
hn (e,H(v))
−
e
θn+
θn−
+
hn (e, H(v))
+ and hn (e, H(v))
− . As hn
hn (e,H(v))
−
ψ(e,H(v)),
×
</equation>
<bodyText confidence="0.999755636363636">
d
2 × p consensus-based features defined in Equa-
tion 9). Since all these features are combined
linearly, we use MERT to optimize them for the
combination model.
model, the acceleration of n-gram lan
guage
model generally leads to substantial speed-up of
SMT system. In our implementation, the n-gram
caching in general brings us over 30% speed im-
provement of the system.
</bodyText>
<sectionHeader confidence="0.999487" genericHeader="method">
4 Optimization
</sectionHeader>
<bodyText confidence="0.999991872340426">
If implemented naively, the translation speed of
the final translation system will be very slow.
For a given input sentence, each member system
has to encode it individually, and the translation
speed is inversely proportional to the number of
member systems generated by our method. For-
tunately, with the thought of computation, there
are a number of optimizations that can make the
system much more efficient in practice.
A simple solution is to run member systems in
parallel when translating a new sentence. Since
all the member systems share the same data re-
sources, such as language model and translation
table, we only need to keep one copy of the re-
quired resources in memory. The translation
speed just depends on the computing power of
parallel computation environment, such as the
number of CPUs.
Furthermore, we can use joint decoding tech-
niques to save the computation of the equivalent
translation hypotheses among member systems.
In joint decoding of member systems, the search
space is structured as a translation hypergraph
where the member systems can share their trans-
lation hypotheses. If more than one member sys-
tems share the same translation hypothesis, we
just need to compute the corresponding feature
values only once, instead of repeating the com-
putation in individual decoders. In our experi-
ments, we find that over 60% translation hy-
potheses can be shared among member systems
when the number of member systems is over 4.
This result indicates that promising speed im-
provement can be achieved by using the joint
decoding and hypothesis sharing techniques.
Another method to speed up the system is to
accelerate n-gram language model with n-gram
caching techniques. In this method, a n-gram
cache is used to store the most frequently and
recently accessed n-grams. When a new n-gram
is accessed during decoding, the cache is
checked first. If the required n-gram hits the
cache, the corresponding n-gram probability is
returned by the cached copy rather than re-
fetching the original data in language model. As
the translation speed of SMT system depends
heavily on the computation of n-gram language
</bodyText>
<page confidence="0.988137">
742
</page>
<sectionHeader confidence="0.999524" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9989695">
Our experiments are conducted on Chinese-to-
English translation in three SMT systems.
</bodyText>
<subsectionHeader confidence="0.988359">
5.1 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.999989794117647">
The first SMT system is a phrase-based system
with two reordering models including the maxi-
mum entropy-based lexicalized reordering model
proposed by Xiong et al. (2006) and the hierar-
chical phrase reordering model proposed by Gal-
ley and Manning (2008). In this system all
phrase pairs are limited to have source length of
at most 3, and the reordering limit is set to 8 by
default4.
The second SMT system is an in-house reim-
plementation of the Hiero system which is based
on the hierarchical phrase-based model proposed
by Chiang (2005).
The third SMT system is a syntax-based sys-
tem based on the string-to-tree model (Galley et
al., 2006; Marcu et al., 2006), where both the
minimal GHKM and SPMT rules are extracted
from the bilingual text, and the composed rules
are generated by combining two or three minimal
GHKM and SPMT rules. Synchronous binariza-
tion (Zhang et al., 2006; Xiao et al., 2009) is per-
formed on each translation rule for the CKY-
style decoding.
In this work, baseline system refers to the sys-
tem produced by the boosting-based system
combination when the number of iterations (i.e.
T ) is set to 1. To obtain satisfactory baseline per-
formance, we train each SMT system for 5 times
using MERT with different initial values of fea-
ture weights to generate a group of baseline can-
didates, and then select the best-performing one
from this group as the final baseline system (i.e.
the starting point in the boosting process) for the
following experiments.
</bodyText>
<subsectionHeader confidence="0.990362">
5.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999988111111111">
Our bilingual data consists of 140K sentence
pairs in the FBIS data set5. GIZA++ is employed
to perform the bi-directional word alignment be-
tween the source and target sentences, and the
final word alignment is generated using the inter-
sect-diag-grow method. All the word-aligned
bilingual sentence pairs are used to extract
phrases and rules for the baseline systems. A 5-
gram language model is trained on the target-side
</bodyText>
<footnote confidence="0.94227275">
4 Our in-house experimental results show that this system
performs slightly better than Moses on Chinese-to-English
translation tasks.
5 LDC catalog number: LDC2003E14
</footnote>
<bodyText confidence="0.99989608">
of the bilingual data and the Xinhua portion of
English Gigaword corpus. Berkeley Parser is
used to generate the English parse trees for the
rule extraction of the syntax-based system. The
data set used for weight training in boosting-
based system combination comes from NIST
MT03 evaluation set. To speed up MERT, all the
sentences with more than 20 Chinese words are
removed. The test sets are the NIST evaluation
sets of MT04, MT05 and MT06. The translation
quality is evaluated in terms of case-insensitive
NIST version BLEU metric. Statistical signifi-
cant test is conducted using the bootstrap re-
sampling method proposed by Koehn (2004).
Beam search and cube pruning (Huang and
Chiang, 2007) are used to prune the search space
in all the three baseline systems. By default, both
of the beam size and the size of n-best list are set
to 20.
In the settings of boosting-based system com-
bination, the maximum number of iterations is
set to 30, and k (in Equation 7) is set to 5. The n-
gram consensuses-based features (in Equation 9)
used in system combination ranges from unigram
to 4-gram.
</bodyText>
<subsectionHeader confidence="0.998">
5.3 Evaluation of Translations
</subsectionHeader>
<bodyText confidence="0.999879035714286">
First we investigate the effectiveness of the
boosting-based system combination on the three
systems.
Figures 2-5 show the BLEU curves on the de-
velopment and test sets, where the X-axis is the
iteration number, and the Y-axis is the BLEU
score of the system generated by the boosting-
based system combination. The points at itera-
tion 1 stand for the performance of the baseline
systems. We see, first of all, that all the three
systems are improved during iterations on the
development set. This trend also holds on the test
sets. After 5, 7 and 8 iterations, relatively stable
improvements are achieved by the phrase-based
system, the Hiero system and the syntax-based
system, respectively. The BLEU scores tend to
converge to the stable values after 20 iterations
for all the systems. Figures 2-5 also show that the
boosting-based system combination seems to be
more helpful to the phrase-based system than to
the Hiero system and the syntax-based system.
For the phrase-based system, it yields over 0.6
BLEU point gains just after the 3rd iteration on
all the data sets.
Table 1 summarizes the evaluation results,
where the BLEU scores at iteration 5, 10, 15, 20
and 30 are reported for the comparison. We see
that the boosting-based system method stably ac-
</bodyText>
<page confidence="0.995068">
743
</page>
<figure confidence="0.995709666666667">
BLEU on MT03 (dev.)
0 5 10 15 20 25 30
iteration number
</figure>
<figureCaption confidence="0.866884">
Figure 2: BLEU scores on the development set
</figureCaption>
<figure confidence="0.991350333333333">
BLEU on MT05 (test)
0 5 10 15 20 25 30
iteration number
</figure>
<figureCaption confidence="0.993618">
Figure 4: BLEU scores on the test set of MT05
</figureCaption>
<figure confidence="0.988837">
BLEU on MT04 (test)
0 5 10 15 20 25 30
iteration number
</figure>
<figureCaption confidence="0.962775">
Figure 3: BLEU scores on the test set of MT04
</figureCaption>
<figure confidence="0.981368">
BLEU on MT06 (test)
0 5 10 15 20 25 30
iteration number
</figure>
<figureCaption confidence="0.988841">
Figure 5: BLEU scores on the test set of MT06
</figureCaption>
<figure confidence="0.987462125">
BLEU4[%]
38
37
36
35
34
33
phrase-based
hiero
syntax-based
BLEU4[%]
37
36
35
34
33
32
phrase-based
hiero
syntax-based
BLEU4[%]
38
37
36
35
34
33
phrase-based
hiero
syntax-based
BLEU4[%]
35
34
33
32
31
30
phrase-based
hiero
syntax-based
</figure>
<table confidence="0.987690555555556">
Phrase-based Hiero Syntax-based
Dev. MT04 MT05 MT06 Dev. MT04 MT05 MT06 Dev. MT04 MT05 MT06
Baseline 33.21 33.68 32.68 30.59 33.42 34.30 33.24 30.62 35.84 35.71 35.11 32.43
Baseline+600best 33.32 33.93 32.84 30.76 33.48 34.46 33.39 30.75 35.95 35.88 35.23 32.58
Boosting-5Iterations 33.95* 34.32* 33.33* 31.33* 33.73 34.48 33.44 30.83 36.03 35.92 35.27 33.09
Boosting-10Iterations 34.14* 34.68* 33.42* 31.35* 33.75 34.65 33.75* 31.02 36.14 36.39* 35.47 33.15*
Boosting-15Iterations 33.99* 34.78* 33.46* 31.45* 34.03* 34.88* 33.98* 31.20* 36.36* 36.46* 35.53* 33.43*
Boosting-20Iterations 34.09* 35.11* 33.56* 31.45* 34.17* 35.00* 34.04* 31.29* 36.44* 36.79* 35.77* 33.36*
Boosting-30Iterations 34.12* 35.16* 33.76* 31.59* 34.05* 34.99* 34.05* 31.30* 36.52* 36.81* 35.71* 33.46*
</table>
<tableCaption confidence="0.9918945">
Table 1: Summary of the results (BLEU4[%]) on the development and test sets. * = significantly better
than baseline (p &lt; 0.05).
</tableCaption>
<bodyText confidence="0.9989966">
hieves significant BLEU improvements after 15
iterations, and the highest BLEU scores are gen-
erally yielded after 20 iterations.
Also as shown in Table 1, over 0.7 BLEU
point gains are obtained on the phrase-based sys-
tem after 10 iterations. The largest BLEU im-
provement on the phrase-based system is over 1
BLEU point in most cases. These results reflect
that our method is relatively more effective for
the phrase-based system than for the other two
systems, and thus confirms the fact we observed
in Figures 2-5.
We also investigate the impact of n-best list
size on the performance of baseline systems. For
the comparison, we show the performance of the
baseline systems with the n-best list size of 600
(Baseline+600best in Table 1) which equals to
the maximum number of translation candidates
accessed in the final combination system (combi-
ne 30 member systems, i.e. Boosing-30Iterations).
</bodyText>
<page confidence="0.992417">
744
</page>
<figure confidence="0.991466666666667">
Diversity on MT03 (dev.)
0 5 10 15 20 25 30
iteration number
</figure>
<figureCaption confidence="0.997392">
Figure 6: Diversity on the development set
</figureCaption>
<figure confidence="0.988806666666667">
Diversity on MT05 (test)
0 5 10 15 20 25 30
iteration number
</figure>
<figureCaption confidence="0.999983">
Figure 8: Diversity on the test set of MT05
</figureCaption>
<bodyText confidence="0.999989545454545">
As shown in Table 1, Baseline+600best obtains
stable improvements over Baseline. It indicates
that the access to larger n-best lists is helpful to
improve the performance of baseline systems.
However, the improvements achieved by Base-
line+600best are modest compared to the im-
provements achieved by Boosting-30Iterations.
These results indicate that the SMT systems can
benefit more from the diversified outputs of
member systems rather than from larger n-best
lists produced by a single system.
</bodyText>
<subsectionHeader confidence="0.996864">
5.4 Diversity among Member Systems
</subsectionHeader>
<bodyText confidence="0.987637857142857">
We also study the change of diversity among the
outputs of member systems during iterations.
The diversity is measured in terms of the Trans-
lation Error Rate (TER) metric proposed in
(Snover et al., 2006). A higher TER score means
that more edit operations are performed if we
transform one translation output into another
</bodyText>
<figure confidence="0.974058666666667">
Diversity on MT04 (test)
0 5 10 15 20 25 30
iteration number
</figure>
<figureCaption confidence="0.998769">
Figure 7: Diversity on the test set of MT04
</figureCaption>
<figure confidence="0.982908">
Diversity on MT06 (test)
0 5 10 15 20 25 30
iteration number
</figure>
<figureCaption confidence="0.999982">
Figure 9: Diversity on the test set of MT06
</figureCaption>
<bodyText confidence="0.9999191">
translation output, and thus reflects a larger di-
versity between the two outputs. In this work, the
TER score for a given group of member systems
is calculated by averaging the TER scores be-
tween the outputs of each pair of member sys-
tems in this group.
Figures 6-9 show the curves of diversity on
the development and test sets, where the X-axis
is the iteration number, and the Y-axis is the di-
versity. The points at iteration 1 stand for the
diversities of baseline systems. In this work, the
baseline’s diversity is the TER score of the group
of baseline candidates that are generated in ad-
vance (Section 5.1).
We see that the diversities of all the systems
increase during iterations in most cases, though a
few drops occur at a few points. It indicates that
our method is very effective to generate diversi-
fied member systems. In addition, the diversities
of baseline systems (iteration 1) are much lower
</bodyText>
<figure confidence="0.998944923076923">
Diversity (TER[%])
40
35
30
25
20
15
phrase-based
hiero
syntax-based
Diversity (TER[%])
35
30
25
20
15
phrase-based
hiero
syntax-based
Diversity (TER[%])
35
30
25
20
15
10
phrase-based
hiero
syntax-based
Diversity (TER[%])
40
35
30
25
20
15
phrase-based
hiero
syntax-based
</figure>
<page confidence="0.995403">
745
</page>
<bodyText confidence="0.999575833333334">
than those of the systems generated by boosting
(iterations 2-30). Together with the results shown
in Figures 2-5, it confirms our motivation that
the diversified translation outputs can lead to
performance improvements over the baseline
systems.
Also as shown in Figures 6-9, the diversity of
the Hiero system is much lower than that of the
phrase-based and syntax-based systems at each
individual setting of iteration number. This inter-
esting finding supports the observation that the
performance of the Hiero system is relatively
more stable than the other two systems as shown
in Figures 2-5. The relative lack of diversity in
the Hiero system might be due to the spurious
ambiguity in Hiero derivations which generally
results in very few different translations in trans-
lation outputs (Chiang, 2007).
</bodyText>
<subsectionHeader confidence="0.997339">
5.5 Evaluation of Oracle Translations
</subsectionHeader>
<bodyText confidence="0.99846665">
In this set of experiments, we evaluate the oracle
performance on the n-best lists of the baseline
systems and the combined systems generated by
boosting-based system combination. Our primary
goal here is to study the impact of our method on
the upper-bound performance.
Table 2 shows the results, where Base-
line+600best stands for the top-600 translation
candidates generated by the baseline systems,
and Boosting-30iterations stands for the ensem-
ble of 30 member systems’ top-20 translation
candidates. As expected, the oracle performance
of Boosting-30Iterations is significantly higher
than that of Baseline+600best. This result indi-
cates that our method can provide much “better”
translation candidates for system combination
than enlarging the size of n-best list naively. It
also gives us a rational explanation for the sig-
nificant improvements achieved by our method
as shown in Section 5.3.
</bodyText>
<table confidence="0.9986807">
Data Method Phrase- Hiero Syntax-
Set based based
Dev. Baseline+600best 46.36 46.51 46.92
Boosting-30Iterations 47.78* 47.44* 48.70*
MT04 Baseline+600best 43.94 44.52 46.88
Boosting-30Iterations 45.97* 45.47* 49.40*
MT05 Baseline+600best 42.32 42.47 45.21
Boosting-30Iterations 44.82* 43.44* 47.02*
MT06 Baseline+600best 39.47 39.39 40.52
Boosting-30Iterations 41.51* 40.10* 41.88*
</table>
<tableCaption confidence="0.9846495">
Table 2: Oracle performance of various systems.
* = significantly better than baseline (p &lt; 0.05).
</tableCaption>
<sectionHeader confidence="0.999858" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999770434782609">
Boosting is a machine learning (ML) method that
has been well studied in the ML community
(Freund, 1995; Freund and Schapire, 1997;
Collins et al., 2002; Rudin et al., 2007), and has
been successfully adopted in natural language
processing (NLP) applications, such as document
classification (Schapire and Singer, 2000) and
named entity classification (Collins and Singer,
1999). However, most of the previous work did
not study the issue of how to improve a single
SMT engine using boosting algorithms. To our
knowledge, the only work addressing this issue is
(Lagarda and Casacuberta, 2008) in which the
boosting algorithm was adopted in phrase-based
SMT. However, Lagarda and Casacuberta
(2008)’s method calculated errors over the
phrases that were chosen by phrase-based sys-
tems, and could not be applied to many other
SMT systems, such as hierarchical phrase-based
systems and syntax-based systems. Differing
from Lagarda and Casacuberta’s work, we are
concerned more with proposing a general
framework which can work with most of the cur-
rent SMT models and empirically demonstrating
its effectiveness on various SMT systems.
There are also some other studies on building
diverse translation systems from a single transla-
tion engine for system combination. The first
attempt is (Macherey and Och, 2007). They em-
pirically showed that diverse translation systems
could be generated by changing parameters at
early-stages of the training procedure. Following
Macherey and Och (2007)’s work, Duan et al.
(2009) proposed a feature subspace method to
build a group of translation systems from various
different sub-models of an existing SMT system.
However, Duan et al. (2009)’s method relied on
the heuristics used in feature sub-space selection.
For example, they used the remove-one-feature
strategy and varied the order of n-gram language
model to obtain a satisfactory group of diverse
systems. Compared to Duan et al. (2009)’s
method, a main advantage of our method is that
it can be applied to most of the SMT systems
without designing any heuristics to adapt it to the
specified systems.
</bodyText>
<sectionHeader confidence="0.998956" genericHeader="discussions">
7 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.9996635">
Actually the method presented in this paper is
doing something rather similar to Minimum
Bayes Risk (MBR) methods. A main difference
lies in that the consensus-based combination
method here does not model the posterior prob-
ability of each hypothesis (i.e. all the hypotheses
are assigned an equal posterior probability when
we calculate the consensus-based features).
</bodyText>
<page confidence="0.994226">
746
</page>
<bodyText confidence="0.999964954545455">
Greater improvements are expected if MBR
methods are used and consensus-based combina-
tion techniques smooth over noise in the MERT
pipeline.
In this work, we use a sentence-level system
combination method to generate final transla-
tions. It is worth studying other more sophisti-
cated alternatives, such as word-level and
phrase-level system combination, to further im-
prove the system performance.
Another issue is how to determine an appro-
priate number of iterations for boosting-based
system combination. It is especially important
when our method is applied in the real-world
applications. Our empirical study shows that the
stable and satisfactory improvements can be
achieved after 6-8 iterations, while the largest
improvements can be achieved after 20 iterations.
In our future work, we will study in-depth prin-
cipled ways to determine the appropriate number
of iterations for boosting-based system combina-
tion.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.9999816">
We have proposed a boosting-based system com-
bination method to address the issue of building
a strong translation system from a group of weak
translation systems generated from a single SMT
engine. We apply our method to three state-of-
the-art SMT systems, and conduct experiments
on three NIST Chinese-to-English MT evalua-
tions test sets. The experimental results show that
our method is very effective to improve the
translation accuracy of the SMT systems.
</bodyText>
<sectionHeader confidence="0.997364" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998727">
This work was supported in part by the National
Science Foundation of China (60873091) and the
Fundamental Research Funds for the Central
Universities (N090604008). The authors would
like to thank the anonymous reviewers for their
pertinent comments, Tongran Liu, Chunliang
Zhang and Shujie Yao for their valuable sugges-
tions for improving this paper, and Tianning Li
and Rushan Chen for developing parts of the
baseline systems.
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999946981818182">
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL 2005, Ann Arbor, Michigan, pages 263-
270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201-228.
David Chiang, Yuval Marton and Philip Resnik. 2008.
Online Large-Margin Training of Syntactic and
Structural Translation Features. In Proc. of
EMNLP 2008, Honolulu, pages 224-233.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In
Proc. of EMNLP/VLC 1999, pages 100-110.
Michael Collins, Robert Schapire and Yoram Singer.
2002. Logistic Regression, AdaBoost and Bregman
Distances. Machine Learning, 48(3): 253-285.
Brooke Cowan, Ivona Kučerová and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proc. of EMNLP 2006, pages 232-241.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proc. of ACL 2005, Ann
Arbor, Michigan, pages 541-548.
Nan Duan, Mu Li, Tong Xiao and Ming Zhou. 2009.
The Feature Subspace Method for SMT System
Combination. In Proc. of EMNLP 2009, pages
1096-1104.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. of ACL
2003, pages 205-208.
Yoav Freund. 1995. Boosting a weak learning algo-
rithm by majority. Information and Computation,
121(2): 256-285.
Yoav Freund and Robert Schapire. 1997. A decision-
theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and
System Sciences, 55(1):119-139.
Michel Galley, Jonathan Graehl, Kevin Knight,
Daniel Marcu, Steve DeNeefe, Wei Wang and
Ignacio Thayer. 2006. Scalable inferences and
training of context-rich syntax translation models.
In Proc. of ACL 2006, Sydney, Australia, pages
961-968.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of EMNLP 2008, Hawaii,
pages 848-856.
Almut Silja Hildebrand and Stephan Vogel. 2008.
Combination of machine translation systems via
hypothesis selection from combined n-best lists. In
Proc. of the 8th AMTA conference, pages 254-261.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language
models. In Proc. of ACL 2007, Prague, Czech Re-
public, pages 144-151.
</reference>
<page confidence="0.972575">
747
</page>
<reference confidence="0.999828466666667">
Philipp Koehn, Franz Och and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. of
HLT-NAACL 2003, Edmonton, USA, pages 48-54.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of
EMNLP 2004, Barcelona, Spain, pages 388-395.
Antonio Lagarda and Francisco Casacuberta. 2008.
Applying Boosting to Statistical Machine Transla-
tion. In Proc. of the 12th EAMT conference, pages
88-96.
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li and
Ming Zhou. 2009. Collaborative Decoding: Partial
Hypothesis Re-Ranking Using Translation Consen-
sus between Decoders. In Proc. of ACL-IJCNLP
2009, Singapore, pages 585-592.
Percy Liang, Alexandre Bouchard-Côté, Dan Klein
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proc. of
COLING/ACL 2006, pages 104-111.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proc. of ACL 2006, pages 609-616.
Wolfgang Macherey and Franz Och. 2007. An Em-
pirical Study on Computing Consensus Transla-
tions from Multiple Machine Translation Systems.
In Proc. of EMNLP 2007, pages 986-995.
Daniel Marcu, Wei Wang, Abdessamad Echihabi and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language
phrases. In Proc. of EMNLP 2006, Sydney, Aus-
tralia, pages 44-52.
Evgeny Matusov, Nicola Ueffing and Hermann Ney.
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced
hypotheses alignment. In Proc. of EACL 2006,
pages 33-40.
Franz Och and Hermann Ney. 2002. Discriminative
Training and Maximum Entropy Models for Statis-
tical Machine Translation. In Proc. of ACL 2002,
Philadelphia, pages 295-302.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of ACL
2003, Japan, pages 160-167.
Antti-Veikko Rosti, Spyros Matsoukas and Richard
Schwartz. 2007. Improved Word-Level System
Combination for Machine Translation. In Proc. of
ACL 2007, pages 312-319.
Cynthia Rudin, Robert Schapire and Ingrid Daube-
chies. 2007. Analysis of boosting algorithms using
the smooth margin function. The Annals of Statis-
tics, 35(6): 2723-2768.
Robert Schapire and Yoram Singer. 2000. BoosTexter:
A boosting-based system for text categorization.
Machine Learning, 39(2/3):135-168.
Robert Schapire. The boosting approach to machine
learning: an overview. 2001. In Proc. of MSRI
Workshop on Nonlinear Estimation and Classifica-
tion, Berkeley, CA, USA, pages 1-23.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla and John Makhoul. 2006. A
Study of Translation Edit Rate with Targeted Hu-
man Annotation. In Proc. of the 7th AMTA confer-
ence, pages 223-231.
Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu and
Ming Zhou. 2009. Better Synchronous Binarization
for Machine Translation. In Proc. of EMNLP 2009,
Singapore, pages 362-370.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for
Statistical Machine Translation. In Proc. of ACL
2006, Sydney, pages 521-528.
Hao Zhang, Liang Huang, Daniel Gildea and Kevin
Knight. 2006. Synchronous Binarization for Ma-
chine Translation. In Proc. of HLT-NAACL 2006,
New York, USA, pages 256- 263.
</reference>
<page confidence="0.996501">
748
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.876065">
<title confidence="0.9992">Boosting-based System Combination for Machine Translation</title>
<author confidence="0.998064">Tong Xiao</author>
<author confidence="0.998064">Jingbo Zhu</author>
<author confidence="0.998064">Muhua Zhu</author>
<author confidence="0.998064">Huizhen Wang</author>
<affiliation confidence="0.9932575">Natural Language Processing Lab. Northeastern University, China</affiliation>
<email confidence="0.946127">xiaotong@mail.neu.edu.cnzhumuhua@gmail.com</email>
<email confidence="0.946127">zhujingbo@mail.neu.edu.cnzhumuhua@gmail.com</email>
<email confidence="0.946127">wanghuizhen@mail.neu.edu.cnzhumuhua@gmail.com</email>
<abstract confidence="0.999871739130435">In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a seof systems is generated from a baseline system in an iterative Then, a system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL 2005,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1446" citStr="Chiang, 2005" startWordPosition="204" endWordPosition="205">is work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combin</context>
<context position="16880" citStr="Chiang (2005)" startWordPosition="2994" endWordPosition="2995">on Chinese-toEnglish translation in three SMT systems. 5.1 Baseline Systems The first SMT system is a phrase-based system with two reordering models including the maximum entropy-based lexicalized reordering model proposed by Xiong et al. (2006) and the hierarchical phrase reordering model proposed by Galley and Manning (2008). In this system all phrase pairs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Synchronous binarization (Zhang et al., 2006; Xiao et al., 2009) is performed on each translation rule for the CKYstyle decoding. In this work, baseline system refers to the system produced by the boosting-based system combination when the number of iterations (i.e. T ) is set to 1. To obtain satisfa</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL 2005, Ann Arbor, Michigan, pages 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--2</pages>
<contexts>
<context position="19125" citStr="Chiang, 2007" startWordPosition="3363" endWordPosition="3364">Parser is used to generate the English parse trees for the rule extraction of the syntax-based system. The data set used for weight training in boostingbased system combination comes from NIST MT03 evaluation set. To speed up MERT, all the sentences with more than 20 Chinese words are removed. The test sets are the NIST evaluation sets of MT04, MT05 and MT06. The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric. Statistical significant test is conducted using the bootstrap resampling method proposed by Koehn (2004). Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems. By default, both of the beam size and the size of n-best list are set to 20. In the settings of boosting-based system combination, the maximum number of iterations is set to 30, and k (in Equation 7) is set to 5. The ngram consensuses-based features (in Equation 9) used in system combination ranges from unigram to 4-gram. 5.3 Evaluation of Translations First we investigate the effectiveness of the boosting-based system combination on the three systems. Figures 2-5 show the BLEU curves on the development and test sets, where</context>
<context position="26527" citStr="Chiang, 2007" startWordPosition="4607" endWordPosition="4608"> to performance improvements over the baseline systems. Also as shown in Figures 6-9, the diversity of the Hiero system is much lower than that of the phrase-based and syntax-based systems at each individual setting of iteration number. This interesting finding supports the observation that the performance of the Hiero system is relatively more stable than the other two systems as shown in Figures 2-5. The relative lack of diversity in the Hiero system might be due to the spurious ambiguity in Hiero derivations which generally results in very few different translations in translation outputs (Chiang, 2007). 5.5 Evaluation of Oracle Translations In this set of experiments, we evaluate the oracle performance on the n-best lists of the baseline systems and the combined systems generated by boosting-based system combination. Our primary goal here is to study the impact of our method on the upper-bound performance. Table 2 shows the results, where Baseline+600best stands for the top-600 translation candidates generated by the baseline systems, and Boosting-30iterations stands for the ensemble of 30 member systems’ top-20 translation candidates. As expected, the oracle performance of Boosting-30Itera</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online Large-Margin Training of Syntactic and Structural Translation Features.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>224--233</pages>
<location>Honolulu,</location>
<contexts>
<context position="11236" citStr="Chiang et al., 2008" startWordPosition="1972" endWordPosition="1975">eij i r − ∑ r (7) k = j 1 where BLEU(eij, ri) is the smoothed sentence-level BLEU score (Liang et al., 2006) of the translation e with respect to the reference translations ri, and ei* is the oracle translation which is selected from {ei1, ..., ein} in terms of BLEU(eij, ri). li can be viewed as a measure of the average cost that we guess the top-k translation candidates instead of the oracle translation. The value of li counts for the magnitude of weight update, that is, a larger li means a larger weight update on Dt(i). The definition of the loss function here is similar to the one used in (Chiang et al., 2008) where only the top-1 translation candidate (i.e. k = 1) is taken into account. 3.3 System Combination Scheme In the last step of our method, a strong translation system v(u(λ*1), ..., u(λ*T)) is built from the 3 Note that the definition of αt here is different from that in the original AdaBoost algorithm (Freund and Schapire, 1997; Schapire, 2001) where αt is a negative number when εt&gt;0.5. 1/4 ⎛ ∏ ∑m1Dt(i) gn(ei)I(UN1gn(rij)) ⎜ m ⎟ n = 1 ⎜ ∑ D i g e ( ) ( ) i i = 1 t n ⎟ ⎝ ⎠ (5) 741 ensemble of member systems {u(A*1), ..., u(A*T)}. In this work, a sentence-level combination method is used to </context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton and Philip Resnik. 2008. Online Large-Margin Training of Syntactic and Structural Translation Features. In Proc. of EMNLP 2008, Honolulu, pages 224-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<booktitle>In Proc. of EMNLP/VLC</booktitle>
<pages>100--110</pages>
<contexts>
<context position="28339" citStr="Collins and Singer, 1999" startWordPosition="4865" endWordPosition="4868"> 45.21 Boosting-30Iterations 44.82* 43.44* 47.02* MT06 Baseline+600best 39.47 39.39 40.52 Boosting-30Iterations 41.51* 40.10* 41.88* Table 2: Oracle performance of various systems. * = significantly better than baseline (p &lt; 0.05). 6 Related Work Boosting is a machine learning (ML) method that has been well studied in the ML community (Freund, 1995; Freund and Schapire, 1997; Collins et al., 2002; Rudin et al., 2007), and has been successfully adopted in natural language processing (NLP) applications, such as document classification (Schapire and Singer, 2000) and named entity classification (Collins and Singer, 1999). However, most of the previous work did not study the issue of how to improve a single SMT engine using boosting algorithms. To our knowledge, the only work addressing this issue is (Lagarda and Casacuberta, 2008) in which the boosting algorithm was adopted in phrase-based SMT. However, Lagarda and Casacuberta (2008)’s method calculated errors over the phrases that were chosen by phrase-based systems, and could not be applied to many other SMT systems, such as hierarchical phrase-based systems and syntax-based systems. Differing from Lagarda and Casacuberta’s work, we are concerned more with </context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised Models for Named Entity Classification. In Proc. of EMNLP/VLC 1999, pages 100-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Robert Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Logistic Regression, AdaBoost and Bregman Distances.</title>
<date>2002</date>
<booktitle>Machine Learning,</booktitle>
<volume>48</volume>
<issue>3</issue>
<pages>253--285</pages>
<contexts>
<context position="28113" citStr="Collins et al., 2002" startWordPosition="4834" endWordPosition="4837">SyntaxSet based based Dev. Baseline+600best 46.36 46.51 46.92 Boosting-30Iterations 47.78* 47.44* 48.70* MT04 Baseline+600best 43.94 44.52 46.88 Boosting-30Iterations 45.97* 45.47* 49.40* MT05 Baseline+600best 42.32 42.47 45.21 Boosting-30Iterations 44.82* 43.44* 47.02* MT06 Baseline+600best 39.47 39.39 40.52 Boosting-30Iterations 41.51* 40.10* 41.88* Table 2: Oracle performance of various systems. * = significantly better than baseline (p &lt; 0.05). 6 Related Work Boosting is a machine learning (ML) method that has been well studied in the ML community (Freund, 1995; Freund and Schapire, 1997; Collins et al., 2002; Rudin et al., 2007), and has been successfully adopted in natural language processing (NLP) applications, such as document classification (Schapire and Singer, 2000) and named entity classification (Collins and Singer, 1999). However, most of the previous work did not study the issue of how to improve a single SMT engine using boosting algorithms. To our knowledge, the only work addressing this issue is (Lagarda and Casacuberta, 2008) in which the boosting algorithm was adopted in phrase-based SMT. However, Lagarda and Casacuberta (2008)’s method calculated errors over the phrases that were </context>
</contexts>
<marker>Collins, Schapire, Singer, 2002</marker>
<rawString>Michael Collins, Robert Schapire and Yoram Singer. 2002. Logistic Regression, AdaBoost and Bregman Distances. Machine Learning, 48(3): 253-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
</authors>
<title>Ivona Kučerová and Michael Collins.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>232--241</pages>
<marker>Cowan, 2006</marker>
<rawString>Brooke Cowan, Ivona Kučerová and Michael Collins. 2006. A discriminative model for tree-to-tree translation. In Proc. of EMNLP 2006, pages 232-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proc. of ACL 2005,</booktitle>
<pages>541--548</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1501" citStr="Ding and Palmer, 2005" startWordPosition="210" endWordPosition="213">nglish Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and </context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proc. of ACL 2005, Ann Arbor, Michigan, pages 541-548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Duan</author>
<author>Mu Li</author>
<author>Tong Xiao</author>
<author>Ming Zhou</author>
</authors>
<title>The Feature Subspace Method for SMT System Combination.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>1096--1104</pages>
<contexts>
<context position="12715" citStr="Duan et al., 2009" startWordPosition="2267" endWordPosition="2270"> UHt ). The final translation is generated from H(v) based on the following scoring function: e* = ∑ ⋅ T arg max β φ e ψ e H v ( ) ( , ( )) + (8) t =1 t t e∈(H v) where (e) is the log-scaled model score of in the t-th member system, and is the corresponding feature weight. It should be noted that may not exist in any i. In this case, we can still calculate the model score of e in any other member systems, since all the member systems are based on the same model and share the same feature space. is a consensusbased scoring function which has been successfully adopted in SMT system combination (Duan et al., 2009; Hildebrand an φt e βt e ∈Hi Hi&apos;≠ ψ(e,H(v)) d Vogel, 2008; Li et al., 2009). The computation of ψ(e,H(v)) is based on a linear combination of a set of n-gram consensuses-based features. ψ e H v ( , ( )) n = ∑ ⋅ θ + h + e H v n ( , ( )) + n n hn (e, H(v))( ∑θ ⋅ 9) n For each order of n-gram, and are defined to measure the n-gram agreement and disagreement between and other translation candidates in H(v), respectively. and are the feature weights corresponding to (e, H(v)) and used in our work are exactly the same as the features used in (Duan et al., 2009) and similar to the features used in (</context>
<context position="29472" citStr="Duan et al. (2009)" startWordPosition="5040" endWordPosition="5043">d systems. Differing from Lagarda and Casacuberta’s work, we are concerned more with proposing a general framework which can work with most of the current SMT models and empirically demonstrating its effectiveness on various SMT systems. There are also some other studies on building diverse translation systems from a single translation engine for system combination. The first attempt is (Macherey and Och, 2007). They empirically showed that diverse translation systems could be generated by changing parameters at early-stages of the training procedure. Following Macherey and Och (2007)’s work, Duan et al. (2009) proposed a feature subspace method to build a group of translation systems from various different sub-models of an existing SMT system. However, Duan et al. (2009)’s method relied on the heuristics used in feature sub-space selection. For example, they used the remove-one-feature strategy and varied the order of n-gram language model to obtain a satisfactory group of diverse systems. Compared to Duan et al. (2009)’s method, a main advantage of our method is that it can be applied to most of the SMT systems without designing any heuristics to adapt it to the specified systems. 7 Discussion and</context>
</contexts>
<marker>Duan, Li, Xiao, Zhou, 2009</marker>
<rawString>Nan Duan, Mu Li, Tong Xiao and Ming Zhou. 2009. The Feature Subspace Method for SMT System Combination. In Proc. of EMNLP 2009, pages 1096-1104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>205--208</pages>
<contexts>
<context position="1478" citStr="Eisner, 2003" startWordPosition="208" endWordPosition="209">n Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combi</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. of ACL 2003, pages 205-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
</authors>
<title>Boosting a weak learning algorithm by majority.</title>
<date>1995</date>
<journal>Information and Computation,</journal>
<volume>121</volume>
<issue>2</issue>
<pages>256--285</pages>
<contexts>
<context position="28064" citStr="Freund, 1995" startWordPosition="4828" endWordPosition="4829">n Section 5.3. Data Method Phrase- Hiero SyntaxSet based based Dev. Baseline+600best 46.36 46.51 46.92 Boosting-30Iterations 47.78* 47.44* 48.70* MT04 Baseline+600best 43.94 44.52 46.88 Boosting-30Iterations 45.97* 45.47* 49.40* MT05 Baseline+600best 42.32 42.47 45.21 Boosting-30Iterations 44.82* 43.44* 47.02* MT06 Baseline+600best 39.47 39.39 40.52 Boosting-30Iterations 41.51* 40.10* 41.88* Table 2: Oracle performance of various systems. * = significantly better than baseline (p &lt; 0.05). 6 Related Work Boosting is a machine learning (ML) method that has been well studied in the ML community (Freund, 1995; Freund and Schapire, 1997; Collins et al., 2002; Rudin et al., 2007), and has been successfully adopted in natural language processing (NLP) applications, such as document classification (Schapire and Singer, 2000) and named entity classification (Collins and Singer, 1999). However, most of the previous work did not study the issue of how to improve a single SMT engine using boosting algorithms. To our knowledge, the only work addressing this issue is (Lagarda and Casacuberta, 2008) in which the boosting algorithm was adopted in phrase-based SMT. However, Lagarda and Casacuberta (2008)’s met</context>
</contexts>
<marker>Freund, 1995</marker>
<rawString>Yoav Freund. 1995. Boosting a weak learning algorithm by majority. Information and Computation, 121(2): 256-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert Schapire</author>
</authors>
<title>A decisiontheoretic generalization of on-line learning and an application to boosting.</title>
<date>1997</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>55--1</pages>
<contexts>
<context position="7535" citStr="Freund and Schapire, 1997" startWordPosition="1255" endWordPosition="1258">s can also be generated from a single engine u(λ*) by adjusting the weight vector λ* in a principled way. In this work, we assume that u1 = u2 =...= uT = u. Our goal is to find a series of λ*i and build a combined system from {u(λ*i)}. To achieve this goal, we propose a 1 The data set used for weight training is generally called development set or tuning set in the SMT field. In this paper, we use the term training set to emphasize the training of log-linear model. * e = (1) = Pr( |) e f 740 boosting-based system combination method (Figure 1). Like other boosting algorithms, such as AdaBoost (Freund and Schapire, 1997; Schapire, 2001), the basic idea of this method is to use weak systems (member systems) to form a strong system (combined system) by repeatedly calling weak system trainer on different distributions over the training samples. However, since most of the boosting algorithms are designed for the classification problem that is very different from the translation problem in natural language processing, several key components have to be redesigned when boosting is adapted to SMT system combination. 3.1 Training In this work, Minimum Error Rate Training (MERT) proposed by Och (2003) is used to estim</context>
<context position="11569" citStr="Freund and Schapire, 1997" startWordPosition="2030" endWordPosition="2033">hat we guess the top-k translation candidates instead of the oracle translation. The value of li counts for the magnitude of weight update, that is, a larger li means a larger weight update on Dt(i). The definition of the loss function here is similar to the one used in (Chiang et al., 2008) where only the top-1 translation candidate (i.e. k = 1) is taken into account. 3.3 System Combination Scheme In the last step of our method, a strong translation system v(u(λ*1), ..., u(λ*T)) is built from the 3 Note that the definition of αt here is different from that in the original AdaBoost algorithm (Freund and Schapire, 1997; Schapire, 2001) where αt is a negative number when εt&gt;0.5. 1/4 ⎛ ∏ ∑m1Dt(i) gn(ei)I(UN1gn(rij)) ⎜ m ⎟ n = 1 ⎜ ∑ D i g e ( ) ( ) i i = 1 t n ⎟ ⎝ ⎠ (5) 741 ensemble of member systems {u(A*1), ..., u(A*T)}. In this work, a sentence-level combination method is used to select the best translation from the pool of the n-best outputs of all the member systems. Let H(u(A*t)) (or Ht for short) be the set of the n-best translation candidates produced by the t-th member system u(A*t), and H(v) be the union set of all Ht (i.e. H(v) = UHt ). The final translation is generated from H(v) based on the follo</context>
<context position="28091" citStr="Freund and Schapire, 1997" startWordPosition="4830" endWordPosition="4833"> Data Method Phrase- Hiero SyntaxSet based based Dev. Baseline+600best 46.36 46.51 46.92 Boosting-30Iterations 47.78* 47.44* 48.70* MT04 Baseline+600best 43.94 44.52 46.88 Boosting-30Iterations 45.97* 45.47* 49.40* MT05 Baseline+600best 42.32 42.47 45.21 Boosting-30Iterations 44.82* 43.44* 47.02* MT06 Baseline+600best 39.47 39.39 40.52 Boosting-30Iterations 41.51* 40.10* 41.88* Table 2: Oracle performance of various systems. * = significantly better than baseline (p &lt; 0.05). 6 Related Work Boosting is a machine learning (ML) method that has been well studied in the ML community (Freund, 1995; Freund and Schapire, 1997; Collins et al., 2002; Rudin et al., 2007), and has been successfully adopted in natural language processing (NLP) applications, such as document classification (Schapire and Singer, 2000) and named entity classification (Collins and Singer, 1999). However, most of the previous work did not study the issue of how to improve a single SMT engine using boosting algorithms. To our knowledge, the only work addressing this issue is (Lagarda and Casacuberta, 2008) in which the boosting algorithm was adopted in phrase-based SMT. However, Lagarda and Casacuberta (2008)’s method calculated errors over </context>
</contexts>
<marker>Freund, Schapire, 1997</marker>
<rawString>Yoav Freund and Robert Schapire. 1997. A decisiontheoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inferences and training of context-rich syntax translation models.</title>
<date>2006</date>
<booktitle>In Proc. of ACL 2006,</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1540" citStr="Galley et al., 2006" startWordPosition="218" endWordPosition="221">three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from or</context>
<context position="16982" citStr="Galley et al., 2006" startWordPosition="3010" endWordPosition="3013"> is a phrase-based system with two reordering models including the maximum entropy-based lexicalized reordering model proposed by Xiong et al. (2006) and the hierarchical phrase reordering model proposed by Galley and Manning (2008). In this system all phrase pairs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Synchronous binarization (Zhang et al., 2006; Xiao et al., 2009) is performed on each translation rule for the CKYstyle decoding. In this work, baseline system refers to the system produced by the boosting-based system combination when the number of iterations (i.e. T ) is set to 1. To obtain satisfactory baseline performance, we train each SMT system for 5 times using MERT with different initial val</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang and Ignacio Thayer. 2006. Scalable inferences and training of context-rich syntax translation models. In Proc. of ACL 2006, Sydney, Australia, pages 961-968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A Simple and Effective Hierarchical Phrase Reordering Model.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>848--856</pages>
<location>Hawaii,</location>
<contexts>
<context position="16595" citStr="Galley and Manning (2008)" startWordPosition="2940" endWordPosition="2944">ired n-gram hits the cache, the corresponding n-gram probability is returned by the cached copy rather than refetching the original data in language model. As the translation speed of SMT system depends heavily on the computation of n-gram language 742 5 Experiments Our experiments are conducted on Chinese-toEnglish translation in three SMT systems. 5.1 Baseline Systems The first SMT system is a phrase-based system with two reordering models including the maximum entropy-based lexicalized reordering model proposed by Xiong et al. (2006) and the hierarchical phrase reordering model proposed by Galley and Manning (2008). In this system all phrase pairs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Synchronous bina</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model. In Proc. of EMNLP 2008, Hawaii, pages 848-856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Combination of machine translation systems via hypothesis selection from combined n-best lists.</title>
<date>2008</date>
<booktitle>In Proc. of the 8th AMTA conference,</booktitle>
<pages>254--261</pages>
<contexts>
<context position="2113" citStr="Hildebrand and Vogel, 2008" startWordPosition="309" endWordPosition="312">nd Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al., 2006; Rosti et al., 2007), can generate new translations differing from any of the original translations. One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. However, this requirement cannot be met in many cases, sin</context>
<context position="13341" citStr="Hildebrand and Vogel, 2008" startWordPosition="2395" endWordPosition="2398">; Hildebrand an φt e βt e ∈Hi Hi&apos;≠ ψ(e,H(v)) d Vogel, 2008; Li et al., 2009). The computation of ψ(e,H(v)) is based on a linear combination of a set of n-gram consensuses-based features. ψ e H v ( , ( )) n = ∑ ⋅ θ + h + e H v n ( , ( )) + n n hn (e, H(v))( ∑θ ⋅ 9) n For each order of n-gram, and are defined to measure the n-gram agreement and disagreement between and other translation candidates in H(v), respectively. and are the feature weights corresponding to (e, H(v)) and used in our work are exactly the same as the features used in (Duan et al., 2009) and similar to the features used in (Hildebrand and Vogel, 2008; Li et al., 2009), we do not present the detailed description of them in this paper. If p orders of n-gram are used in computing the total number of features in the system combination will be T + 2 p (T modelscore-based features defined in Equation 8 an hn (e,H(v)) + hn (e,H(v)) − e θn+ θn− + hn (e, H(v)) + and hn (e, H(v)) − . As hn hn (e,H(v)) − ψ(e,H(v)), × d 2 × p consensus-based features defined in Equation 9). Since all these features are combined linearly, we use MERT to optimize them for the combination model. model, the acceleration of n-gram lan guage model generally leads to substa</context>
</contexts>
<marker>Hildebrand, Vogel, 2008</marker>
<rawString>Almut Silja Hildebrand and Stephan Vogel. 2008. Combination of machine translation systems via hypothesis selection from combined n-best lists. In Proc. of the 8th AMTA conference, pages 254-261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proc. of ACL 2007,</booktitle>
<pages>144--151</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="19125" citStr="Huang and Chiang, 2007" startWordPosition="3361" endWordPosition="3364"> Berkeley Parser is used to generate the English parse trees for the rule extraction of the syntax-based system. The data set used for weight training in boostingbased system combination comes from NIST MT03 evaluation set. To speed up MERT, all the sentences with more than 20 Chinese words are removed. The test sets are the NIST evaluation sets of MT04, MT05 and MT06. The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric. Statistical significant test is conducted using the bootstrap resampling method proposed by Koehn (2004). Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems. By default, both of the beam size and the size of n-best list are set to 20. In the settings of boosting-based system combination, the maximum number of iterations is set to 30, and k (in Equation 7) is set to 5. The ngram consensuses-based features (in Equation 9) used in system combination ranges from unigram to 4-gram. 5.3 Evaluation of Translations First we investigate the effectiveness of the boosting-based system combination on the three systems. Figures 2-5 show the BLEU curves on the development and test sets, where</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proc. of ACL 2007, Prague, Czech Republic, pages 144-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL 2003,</booktitle>
<pages>48--54</pages>
<location>Edmonton, USA,</location>
<contexts>
<context position="1400" citStr="Koehn et al., 2003" startWordPosition="197" endWordPosition="200">he original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted,</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Och and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. of HLT-NAACL 2003, Edmonton, USA, pages 48-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<location>Barcelona,</location>
<contexts>
<context position="19070" citStr="Koehn (2004)" startWordPosition="3354" endWordPosition="3355">e Xinhua portion of English Gigaword corpus. Berkeley Parser is used to generate the English parse trees for the rule extraction of the syntax-based system. The data set used for weight training in boostingbased system combination comes from NIST MT03 evaluation set. To speed up MERT, all the sentences with more than 20 Chinese words are removed. The test sets are the NIST evaluation sets of MT04, MT05 and MT06. The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric. Statistical significant test is conducted using the bootstrap resampling method proposed by Koehn (2004). Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems. By default, both of the beam size and the size of n-best list are set to 20. In the settings of boosting-based system combination, the maximum number of iterations is set to 30, and k (in Equation 7) is set to 5. The ngram consensuses-based features (in Equation 9) used in system combination ranges from unigram to 4-gram. 5.3 Evaluation of Translations First we investigate the effectiveness of the boosting-based system combination on the three systems. Figures 2-5 show </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proc. of EMNLP 2004, Barcelona, Spain, pages 388-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Lagarda</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Applying Boosting to Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proc. of the 12th EAMT conference,</booktitle>
<pages>88--96</pages>
<contexts>
<context position="28553" citStr="Lagarda and Casacuberta, 2008" startWordPosition="4901" endWordPosition="4904">n baseline (p &lt; 0.05). 6 Related Work Boosting is a machine learning (ML) method that has been well studied in the ML community (Freund, 1995; Freund and Schapire, 1997; Collins et al., 2002; Rudin et al., 2007), and has been successfully adopted in natural language processing (NLP) applications, such as document classification (Schapire and Singer, 2000) and named entity classification (Collins and Singer, 1999). However, most of the previous work did not study the issue of how to improve a single SMT engine using boosting algorithms. To our knowledge, the only work addressing this issue is (Lagarda and Casacuberta, 2008) in which the boosting algorithm was adopted in phrase-based SMT. However, Lagarda and Casacuberta (2008)’s method calculated errors over the phrases that were chosen by phrase-based systems, and could not be applied to many other SMT systems, such as hierarchical phrase-based systems and syntax-based systems. Differing from Lagarda and Casacuberta’s work, we are concerned more with proposing a general framework which can work with most of the current SMT models and empirically demonstrating its effectiveness on various SMT systems. There are also some other studies on building diverse transla</context>
</contexts>
<marker>Lagarda, Casacuberta, 2008</marker>
<rawString>Antonio Lagarda and Francisco Casacuberta. 2008. Applying Boosting to Statistical Machine Translation. In Proc. of the 12th EAMT conference, pages 88-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mu Li</author>
<author>Nan Duan</author>
<author>Dongdong Zhang</author>
<author>Chi-Ho Li</author>
<author>Ming Zhou</author>
</authors>
<title>Collaborative Decoding: Partial Hypothesis Re-Ranking Using Translation Consensus between Decoders.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP 2009, Singapore,</booktitle>
<pages>585--592</pages>
<contexts>
<context position="12791" citStr="Li et al., 2009" startWordPosition="2283" endWordPosition="2286">coring function: e* = ∑ ⋅ T arg max β φ e ψ e H v ( ) ( , ( )) + (8) t =1 t t e∈(H v) where (e) is the log-scaled model score of in the t-th member system, and is the corresponding feature weight. It should be noted that may not exist in any i. In this case, we can still calculate the model score of e in any other member systems, since all the member systems are based on the same model and share the same feature space. is a consensusbased scoring function which has been successfully adopted in SMT system combination (Duan et al., 2009; Hildebrand an φt e βt e ∈Hi Hi&apos;≠ ψ(e,H(v)) d Vogel, 2008; Li et al., 2009). The computation of ψ(e,H(v)) is based on a linear combination of a set of n-gram consensuses-based features. ψ e H v ( , ( )) n = ∑ ⋅ θ + h + e H v n ( , ( )) + n n hn (e, H(v))( ∑θ ⋅ 9) n For each order of n-gram, and are defined to measure the n-gram agreement and disagreement between and other translation candidates in H(v), respectively. and are the feature weights corresponding to (e, H(v)) and used in our work are exactly the same as the features used in (Duan et al., 2009) and similar to the features used in (Hildebrand and Vogel, 2008; Li et al., 2009), we do not present the detailed</context>
</contexts>
<marker>Li, Duan, Zhang, Li, Zhou, 2009</marker>
<rawString>Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li and Ming Zhou. 2009. Collaborative Decoding: Partial Hypothesis Re-Ranking Using Translation Consensus between Decoders. In Proc. of ACL-IJCNLP 2009, Singapore, pages 585-592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Côté</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL</booktitle>
<pages>104--111</pages>
<contexts>
<context position="10724" citStr="Liang et al., 2006" startWordPosition="1877" endWordPosition="1880">n Equation 4 with two parameters αt and li in it. αt can be regarded as a measure of the importance that the t-th weak system gains in boosting. The definition of αt guarantees that αt always has a positive value3. A main effect of αt is to scale the weight updating (e.g. a larger αt means a greater update). li is the loss on the i-th sample. For each i, let {ei1, ..., ein} be the n-best translation candidates produced by the system. The loss function is defined to be: 1 k * li = e BLEU( , ) i i BLEU( , ) eij i r − ∑ r (7) k = j 1 where BLEU(eij, ri) is the smoothed sentence-level BLEU score (Liang et al., 2006) of the translation e with respect to the reference translations ri, and ei* is the oracle translation which is selected from {ei1, ..., ein} in terms of BLEU(eij, ri). li can be viewed as a measure of the average cost that we guess the top-k translation candidates instead of the oracle translation. The value of li counts for the magnitude of weight update, that is, a larger li means a larger weight update on Dt(i). The definition of the loss function here is similar to the one used in (Chiang et al., 2008) where only the top-1 translation candidate (i.e. k = 1) is taken into account. 3.3 Syst</context>
</contexts>
<marker>Liang, Bouchard-Côté, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Côté, Dan Klein and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. of COLING/ACL 2006, pages 104-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-toString Alignment Template for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1519" citStr="Liu et al., 2006" startWordPosition="214" endWordPosition="217">ion (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simpl</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-toString Alignment Template for Statistical Machine Translation. In Proc. of ACL 2006, pages 609-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Och</author>
</authors>
<title>An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>986--995</pages>
<contexts>
<context position="2499" citStr="Macherey and Och, 2007" startWordPosition="370" endWordPosition="373">g from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al., 2006; Rosti et al., 2007), can generate new translations differing from any of the original translations. One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. However, this requirement cannot be met in many cases, since we do not always have the access to multiple SMT engines due to the high cost of developing and tuning SMT systems. To reduce the burden of system development, it might be a nice way to combine a set of translation systems built from a single translation engine. A key issue here is how to generate an ensemble of diversified translation systems from a single translation engine in a</context>
<context position="29268" citStr="Macherey and Och, 2007" startWordPosition="5010" endWordPosition="5013">uberta (2008)’s method calculated errors over the phrases that were chosen by phrase-based systems, and could not be applied to many other SMT systems, such as hierarchical phrase-based systems and syntax-based systems. Differing from Lagarda and Casacuberta’s work, we are concerned more with proposing a general framework which can work with most of the current SMT models and empirically demonstrating its effectiveness on various SMT systems. There are also some other studies on building diverse translation systems from a single translation engine for system combination. The first attempt is (Macherey and Och, 2007). They empirically showed that diverse translation systems could be generated by changing parameters at early-stages of the training procedure. Following Macherey and Och (2007)’s work, Duan et al. (2009) proposed a feature subspace method to build a group of translation systems from various different sub-models of an existing SMT system. However, Duan et al. (2009)’s method relied on the heuristics used in feature sub-space selection. For example, they used the remove-one-feature strategy and varied the order of n-gram language model to obtain a satisfactory group of diverse systems. Compared</context>
</contexts>
<marker>Macherey, Och, 2007</marker>
<rawString>Wolfgang Macherey and Franz Och. 2007. An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems. In Proc. of EMNLP 2007, pages 986-995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP 2006,</booktitle>
<pages>44--52</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="17003" citStr="Marcu et al., 2006" startWordPosition="3014" endWordPosition="3017">stem with two reordering models including the maximum entropy-based lexicalized reordering model proposed by Xiong et al. (2006) and the hierarchical phrase reordering model proposed by Galley and Manning (2008). In this system all phrase pairs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Synchronous binarization (Zhang et al., 2006; Xiao et al., 2009) is performed on each translation rule for the CKYstyle decoding. In this work, baseline system refers to the system produced by the boosting-based system combination when the number of iterations (i.e. T ) is set to 1. To obtain satisfactory baseline performance, we train each SMT system for 5 times using MERT with different initial values of feature weight</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proc. of EMNLP 2006, Sydney, Australia, pages 44-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Proc. of EACL</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2268" citStr="Matusov et al., 2006" startWordPosition="331" endWordPosition="334">studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al., 2006; Rosti et al., 2007), can generate new translations differing from any of the original translations. One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. However, this requirement cannot be met in many cases, since we do not always have the access to multiple SMT engines due to the high cost of developing and tuning SMT systems. To reduce the burden of system devel</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing and Hermann Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Proc. of EACL 2006, pages 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL 2002,</booktitle>
<pages>295--302</pages>
<location>Philadelphia,</location>
<contexts>
<context position="5256" citStr="Och and Ney (2002)" startWordPosition="848" endWordPosition="851">ut the final system: v(u(λ*1), ..., u (λ*T)) system. All the systems are evaluated on three NIST MT evaluation test sets. Experimental results show that our method leads to significant improvements in translation accuracy over the baseline systems. 2 Background Given a source string f, the goal of SMT is to find a target string e* by the following equation. arg max(Pr( |)) e f e where Pr(e |f ) is the probability that e is the translation of the given source string f. To model the posterior probability Pr(e |f) , most of the state-of-the-art SMT systems utilize the loglinear model proposed by Och and Ney (2002), as follows, exp(Em=1Am- hm(f,e)) (2) Ee &apos;exp(Em =1Am- hm(f,e&apos;)) where {hm( f, e ) |m = 1, ..., M} is a set of features, and Am is the feature weight corresponding to the m-th feature. hm( f, e ) can be regarded as a function that maps every pair of source string f and target string e into a non-negative value, and Am can be viewed as the contribution of hm( f, e ) to the overall score Pr(e |f). In this paper, u denotes a log-linear model that has M fixed features {h1( f ,e ), ..., hM( f ,e )}, λ = {A1, ..., AM} denotes the M parameters of u, and u(λ) denotes a SMT system based on u with para</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Och and Hermann Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proc. of ACL 2002, Philadelphia, pages 295-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="8118" citStr="Och (2003)" startWordPosition="1348" endWordPosition="1349"> (Freund and Schapire, 1997; Schapire, 2001), the basic idea of this method is to use weak systems (member systems) to form a strong system (combined system) by repeatedly calling weak system trainer on different distributions over the training samples. However, since most of the boosting algorithms are designed for the classification problem that is very different from the translation problem in natural language processing, several key components have to be redesigned when boosting is adapted to SMT system combination. 3.1 Training In this work, Minimum Error Rate Training (MERT) proposed by Och (2003) is used to estimate feature weights λ over a series of training samples. As in other state-of-the-art SMT systems, BLEU is selected as the accuracy measure to define the error function used in MERT. Since the weights of training samples are not taken into account in BLEU2, we modify the original definition of BLEU to make it sensitive to the distribution Dt(i) over the training samples. The modified version of BLEU is called weighted BLEU (WBLEU) in this paper. Let E = e1 ... em be the translations produced by the system, R = r1 ... rm be the reference translations where ri = {ri1, ..., riN},</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of ACL 2003, Japan, pages 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved Word-Level System Combination for Machine Translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>312--319</pages>
<contexts>
<context position="2289" citStr="Rosti et al., 2007" startWordPosition="335" endWordPosition="338"> combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al., 2006; Rosti et al., 2007), can generate new translations differing from any of the original translations. One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. However, this requirement cannot be met in many cases, since we do not always have the access to multiple SMT engines due to the high cost of developing and tuning SMT systems. To reduce the burden of system development, it might be a</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko Rosti, Spyros Matsoukas and Richard Schwartz. 2007. Improved Word-Level System Combination for Machine Translation. In Proc. of ACL 2007, pages 312-319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Rudin</author>
<author>Robert Schapire</author>
<author>Ingrid Daubechies</author>
</authors>
<title>Analysis of boosting algorithms using the smooth margin function.</title>
<date>2007</date>
<journal>The Annals of Statistics,</journal>
<volume>35</volume>
<issue>6</issue>
<pages>2723--2768</pages>
<contexts>
<context position="28134" citStr="Rudin et al., 2007" startWordPosition="4838" endWordPosition="4841">Dev. Baseline+600best 46.36 46.51 46.92 Boosting-30Iterations 47.78* 47.44* 48.70* MT04 Baseline+600best 43.94 44.52 46.88 Boosting-30Iterations 45.97* 45.47* 49.40* MT05 Baseline+600best 42.32 42.47 45.21 Boosting-30Iterations 44.82* 43.44* 47.02* MT06 Baseline+600best 39.47 39.39 40.52 Boosting-30Iterations 41.51* 40.10* 41.88* Table 2: Oracle performance of various systems. * = significantly better than baseline (p &lt; 0.05). 6 Related Work Boosting is a machine learning (ML) method that has been well studied in the ML community (Freund, 1995; Freund and Schapire, 1997; Collins et al., 2002; Rudin et al., 2007), and has been successfully adopted in natural language processing (NLP) applications, such as document classification (Schapire and Singer, 2000) and named entity classification (Collins and Singer, 1999). However, most of the previous work did not study the issue of how to improve a single SMT engine using boosting algorithms. To our knowledge, the only work addressing this issue is (Lagarda and Casacuberta, 2008) in which the boosting algorithm was adopted in phrase-based SMT. However, Lagarda and Casacuberta (2008)’s method calculated errors over the phrases that were chosen by phrase-base</context>
</contexts>
<marker>Rudin, Schapire, Daubechies, 2007</marker>
<rawString>Cynthia Rudin, Robert Schapire and Ingrid Daubechies. 2007. Analysis of boosting algorithms using the smooth margin function. The Annals of Statistics, 35(6): 2723-2768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>BoosTexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="28280" citStr="Schapire and Singer, 2000" startWordPosition="4857" endWordPosition="4860">tions 45.97* 45.47* 49.40* MT05 Baseline+600best 42.32 42.47 45.21 Boosting-30Iterations 44.82* 43.44* 47.02* MT06 Baseline+600best 39.47 39.39 40.52 Boosting-30Iterations 41.51* 40.10* 41.88* Table 2: Oracle performance of various systems. * = significantly better than baseline (p &lt; 0.05). 6 Related Work Boosting is a machine learning (ML) method that has been well studied in the ML community (Freund, 1995; Freund and Schapire, 1997; Collins et al., 2002; Rudin et al., 2007), and has been successfully adopted in natural language processing (NLP) applications, such as document classification (Schapire and Singer, 2000) and named entity classification (Collins and Singer, 1999). However, most of the previous work did not study the issue of how to improve a single SMT engine using boosting algorithms. To our knowledge, the only work addressing this issue is (Lagarda and Casacuberta, 2008) in which the boosting algorithm was adopted in phrase-based SMT. However, Lagarda and Casacuberta (2008)’s method calculated errors over the phrases that were chosen by phrase-based systems, and could not be applied to many other SMT systems, such as hierarchical phrase-based systems and syntax-based systems. Differing from </context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert Schapire and Yoram Singer. 2000. BoosTexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Schapire</author>
</authors>
<title>The boosting approach to machine learning: an overview.</title>
<date>2001</date>
<booktitle>In Proc. of MSRI Workshop on Nonlinear Estimation and Classification,</booktitle>
<pages>1--23</pages>
<location>Berkeley, CA, USA,</location>
<contexts>
<context position="7552" citStr="Schapire, 2001" startWordPosition="1259" endWordPosition="1260">m a single engine u(λ*) by adjusting the weight vector λ* in a principled way. In this work, we assume that u1 = u2 =...= uT = u. Our goal is to find a series of λ*i and build a combined system from {u(λ*i)}. To achieve this goal, we propose a 1 The data set used for weight training is generally called development set or tuning set in the SMT field. In this paper, we use the term training set to emphasize the training of log-linear model. * e = (1) = Pr( |) e f 740 boosting-based system combination method (Figure 1). Like other boosting algorithms, such as AdaBoost (Freund and Schapire, 1997; Schapire, 2001), the basic idea of this method is to use weak systems (member systems) to form a strong system (combined system) by repeatedly calling weak system trainer on different distributions over the training samples. However, since most of the boosting algorithms are designed for the classification problem that is very different from the translation problem in natural language processing, several key components have to be redesigned when boosting is adapted to SMT system combination. 3.1 Training In this work, Minimum Error Rate Training (MERT) proposed by Och (2003) is used to estimate feature weigh</context>
<context position="11586" citStr="Schapire, 2001" startWordPosition="2034" endWordPosition="2035">slation candidates instead of the oracle translation. The value of li counts for the magnitude of weight update, that is, a larger li means a larger weight update on Dt(i). The definition of the loss function here is similar to the one used in (Chiang et al., 2008) where only the top-1 translation candidate (i.e. k = 1) is taken into account. 3.3 System Combination Scheme In the last step of our method, a strong translation system v(u(λ*1), ..., u(λ*T)) is built from the 3 Note that the definition of αt here is different from that in the original AdaBoost algorithm (Freund and Schapire, 1997; Schapire, 2001) where αt is a negative number when εt&gt;0.5. 1/4 ⎛ ∏ ∑m1Dt(i) gn(ei)I(UN1gn(rij)) ⎜ m ⎟ n = 1 ⎜ ∑ D i g e ( ) ( ) i i = 1 t n ⎟ ⎝ ⎠ (5) 741 ensemble of member systems {u(A*1), ..., u(A*T)}. In this work, a sentence-level combination method is used to select the best translation from the pool of the n-best outputs of all the member systems. Let H(u(A*t)) (or Ht for short) be the set of the n-best translation candidates produced by the t-th member system u(A*t), and H(v) be the union set of all Ht (i.e. H(v) = UHt ). The final translation is generated from H(v) based on the following scoring func</context>
</contexts>
<marker>Schapire, 2001</marker>
<rawString>Robert Schapire. The boosting approach to machine learning: an overview. 2001. In Proc. of MSRI Workshop on Nonlinear Estimation and Classification, Berkeley, CA, USA, pages 1-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proc. of the 7th AMTA conference,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="24207" citStr="Snover et al., 2006" startWordPosition="4211" endWordPosition="4214">rger n-best lists is helpful to improve the performance of baseline systems. However, the improvements achieved by Baseline+600best are modest compared to the improvements achieved by Boosting-30Iterations. These results indicate that the SMT systems can benefit more from the diversified outputs of member systems rather than from larger n-best lists produced by a single system. 5.4 Diversity among Member Systems We also study the change of diversity among the outputs of member systems during iterations. The diversity is measured in terms of the Translation Error Rate (TER) metric proposed in (Snover et al., 2006). A higher TER score means that more edit operations are performed if we transform one translation output into another Diversity on MT04 (test) 0 5 10 15 20 25 30 iteration number Figure 7: Diversity on the test set of MT04 Diversity on MT06 (test) 0 5 10 15 20 25 30 iteration number Figure 9: Diversity on the test set of MT06 translation output, and thus reflects a larger diversity between the two outputs. In this work, the TER score for a given group of member systems is calculated by averaging the TER scores between the outputs of each pair of member systems in this group. Figures 6-9 show </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proc. of the 7th AMTA conference, pages 223-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Xiao</author>
<author>Mu Li</author>
<author>Dongdong Zhang</author>
<author>Jingbo Zhu</author>
<author>Ming Zhou</author>
</authors>
<title>Better Synchronous Binarization for Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP 2009, Singapore,</booktitle>
<pages>362--370</pages>
<contexts>
<context position="17243" citStr="Xiao et al., 2009" startWordPosition="3055" endWordPosition="3058">irs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Synchronous binarization (Zhang et al., 2006; Xiao et al., 2009) is performed on each translation rule for the CKYstyle decoding. In this work, baseline system refers to the system produced by the boosting-based system combination when the number of iterations (i.e. T ) is set to 1. To obtain satisfactory baseline performance, we train each SMT system for 5 times using MERT with different initial values of feature weights to generate a group of baseline candidates, and then select the best-performing one from this group as the final baseline system (i.e. the starting point in the boosting process) for the following experiments. 5.2 Experimental Setup Our b</context>
</contexts>
<marker>Xiao, Li, Zhang, Zhu, Zhou, 2009</marker>
<rawString>Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu and Ming Zhou. 2009. Better Synchronous Binarization for Machine Translation. In Proc. of EMNLP 2009, Singapore, pages 362-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL 2006,</booktitle>
<pages>521--528</pages>
<location>Sydney,</location>
<contexts>
<context position="16512" citStr="Xiong et al. (2006)" startWordPosition="2927" endWordPosition="2930">w n-gram is accessed during decoding, the cache is checked first. If the required n-gram hits the cache, the corresponding n-gram probability is returned by the cached copy rather than refetching the original data in language model. As the translation speed of SMT system depends heavily on the computation of n-gram language 742 5 Experiments Our experiments are conducted on Chinese-toEnglish translation in three SMT systems. 5.1 Baseline Systems The first SMT system is a phrase-based system with two reordering models including the maximum entropy-based lexicalized reordering model proposed by Xiong et al. (2006) and the hierarchical phrase reordering model proposed by Galley and Manning (2008). In this system all phrase pairs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules ar</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In Proc. of ACL 2006, Sydney, pages 521-528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous Binarization for Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<pages>256--263</pages>
<location>New York, USA,</location>
<contexts>
<context position="17223" citStr="Zhang et al., 2006" startWordPosition="3051" endWordPosition="3054">system all phrase pairs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Synchronous binarization (Zhang et al., 2006; Xiao et al., 2009) is performed on each translation rule for the CKYstyle decoding. In this work, baseline system refers to the system produced by the boosting-based system combination when the number of iterations (i.e. T ) is set to 1. To obtain satisfactory baseline performance, we train each SMT system for 5 times using MERT with different initial values of feature weights to generate a group of baseline candidates, and then select the best-performing one from this group as the final baseline system (i.e. the starting point in the boosting process) for the following experiments. 5.2 Expe</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea and Kevin Knight. 2006. Synchronous Binarization for Machine Translation. In Proc. of HLT-NAACL 2006, New York, USA, pages 256- 263.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>