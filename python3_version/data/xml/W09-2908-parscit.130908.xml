<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998851">
Bottom-up Named Entity Recognition
using a Two-stage Machine Learning Method
</title>
<author confidence="0.969013">
Hirotaka Funayama Tomohide Shibata Sadao Kurohashi
</author>
<affiliation confidence="0.959572">
Kyoto University, Yoshida-honmachi,
</affiliation>
<address confidence="0.501857">
Sakyo-ku, Kyoto, 606-8501, Japan
</address>
<email confidence="0.992885">
{funayama, shibata, kuro}@nlp.kuee.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.993823" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999563">
This paper proposes Japanese bottom-up
named entity recognition using a two-
stage machine learning method. Most
work has formalized Named Entity Recog-
nition as a sequential labeling problem, in
which only local information is utilized
for the label estimation, and thus a long
named entity consisting of several mor-
phemes tends to be wrongly recognized.
Our proposed method regards a compound
noun (chunk) as a labeling unit, and first
estimates the labels of all the chunks in
a phrasal unit (bunsetsu) using a machine
learning method. Then, the best label as-
signment in the bunsetsu is determined
from bottom up as the CKY parsing al-
gorithm using a machine learning method.
We conducted an experimental on CRL
NE data, and achieved an F measure of
89.79, which is higher than previous work.
</bodyText>
<sectionHeader confidence="0.998707" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990300363636364">
Named Entity Recognition (NER) is a task of rec-
ognizing named entities such as person names,
organization names, and location. It is used for
several NLP applications such as Information Ex-
traction (IE) and Question Answering (QA). Most
work uses machine learning methods such as Sup-
port Vector Machines (SVMs) (Vapnik, 1995) and
Conditional Random Field (CRF) (Lafferty et al.,
2001) using a hand-annotated corpus (Krishnan
and D.Manning, 2006; Kazama and Torisawa,
2008; Sasano and Kurohashi, 2008; Fukushima et
al., 2008; Nakano and Hirai, 2004; Masayuki and
Matsumoto, 2003).
In general, NER is formalized as a sequential
labeling problem. For example, regarding a mor-
pheme as a basic unit, it is first labeled as S-
PERSON, B-PERSON, I-PERSON, E-PERSON,
S-ORGANIZATION, etc. Then, considering the
labeling results of morphemes, the best NE label
sequence is recognized.
When the label of each morpheme is estimated,
only local information around the morpheme (e.g.,
the morpheme, the two preceding morphemes, and
the two following morphemes) is utilized. There-
fore, a long named entity consisting of several
morphemes tends to be wrongly recognized. Let
us consider the example sentences shown in Fig-
ure 1.
In sentence (1), the label of “Kazama” can be
recognized to be S-PERSON (PERSON consist-
ing of one morpheme) by utilizing the surrounding
information such as the suffix “san” (Mr.) and the
verb “kikoku shita” (return home).
On the other hand, in sentence (2), when the
label of “shinyou” (credit) is recognized to be
B-ORGANIZATION (the beginning of ORGA-
NIZATION), only information from “hatsudou”
(invoke) to “kyusai” (relief) can be utilized, and
thus the information of the morpheme “ginkou”
(bank) that is apart from “shinyou” by three mor-
phemes cannot be utilized. To cope with this prob-
lem, Nakano et al. (Nakano and Hirai, 2004) and
Sasano et al. (Sasano and Kurohashi, 2008) uti-
lized information of the head of bunsetsu1. In their
methods, when the label of “shinyou” is recog-
nized, the information of the morpheme “ginkou”
can be utilized.
However, these methods do not work when the
morpheme that we want to refer to is not a head
of bunsetsu as in sentence (3). In this example,
when “gaikoku” (foreign) is recognized to be B-
ARTIFACT (the beginning of ARTIFACT), we
want to refer to “hou” (law), not “ihan” (viola-
tion), which is the head of the bunsetsu.
This paper proposes Japanese bottom-up named
</bodyText>
<footnote confidence="0.995846">
1Bunsetsu is the smallest coherent phrasal unit in
Japanese. It consists of one or more content words followed
by zero or more function words.
</footnote>
<page confidence="0.988717">
55
</page>
<note confidence="0.9989995">
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 55–62,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.928367714285714">
(1) kikoku-shita
return home
Kazama-san-wa . . .
Mr.Kazama TOP
’Mr. Kazama who returned home’
(2) hatsudou-shita shinyou-kumiai-kyusai-ginkou-no setsuritsu-mo.. .
invoke establishment
credit union relief bank GEN
’the establishment of the invoking credit union relief bank’
(3) shibunsyo-gizou-to gaikoku-jin-touroku-hou-ihan-no utagai-de
private document falsification and suspicion INS
foreigner registration law violation GEN
’on suspicion of the private document falsification and the violation of the foreigner registra-
tion law’
</figure>
<figureCaption confidence="0.999965">
Figure 1: Example sentences.
</figureCaption>
<bodyText confidence="0.998232424242424">
entity recognition using a two-stage machine
learning method. Different from previous work,
this method regards a compound noun as a la-
beling unit (we call it chunk, hereafter), and es-
timates the labels of all the chunks in the bun-
setsu using a machine learning method. In sen-
tence (3), all the chunks in the second bunsetsu
(i.e., “gaikoku”, “gaikoku-jin”, · · ·, “gaikoku-jin-
touroku-hou-ihan ”, · · ·, “ihan”) are labeled, and
in the case that the chunk “gaikoku-jin-touroku-
hou” is labeled, the information about “hou” (law)
is utilized in a natural manner. Then, in the bun-
setsu, the best label assignment is determined. For
example, among the combination of “gaikoku-jin-
touroku-hou” (ARTIFACT) and “ihan” (OTHER),
the combination of “gaikoku-jin” (PERSON) and
“touroku-hou-ihan” (OTHER), etc., the best la-
bel assignment, “gaikoku-jin-touroku-hou” (AR-
TIFACT) and “ihan” (OTHER), is chosen based
on a machine learning method. In this determi-
nation of the best label assignment, as the CKY
parsing algorithm, the label assignment is deter-
mined by bottom-up dynamic programming. We
conducted an experimental on CRL NE data, and
achieved an F measure of 89.79, which is higher
than previous work.
This paper is organized as follows. Section 2 re-
views related work of NER, especially focusing on
sequential labeling based method. Section 3 de-
scribes an overview of our proposed method. Sec-
tion 4 presents two machine learning models, and
Section 5 describes an analysis algorithm. Section
6 gives an experimental result.
</bodyText>
<sectionHeader confidence="0.998196" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.4843725">
In Japanese Named Entity Recognition, the defi-
nition of Named Entity in IREX Workshop (IREX
</bodyText>
<tableCaption confidence="0.99209">
Table 1: NE classes and their examples.
</tableCaption>
<bodyText confidence="0.954075259259259">
Committee, 1999) is usually used. In this def-
inition, NEs are classified into eight classes:
PERSON, LOCATION, ORGANIZATION, AR-
TIFACT, DATE, TIME, MONEY, and PERCENT.
Table 1 shows example instances of each class.
NER methods are divided into two approaches:
rule-based approach and machine learning ap-
proach. According to previous work, machine
learning approach achieved better performance
than rule-based approach.
In general, a machine learning method is for-
malized as a sequential labeling problem. This
problem is first assigning each token (character or
morpheme) to several labels. In an SE-algorithm
(Sekine et al., 1998), S is assigned to NE com-
posed of one morpheme, B, I, E is assigned to the
beginning, middle, end of NE, respectively, and O
is assigned to the morpheme that is not an NE2.
The labels S, B, I, and E are prepared for each NE
classes, and thus the total number of labels is 33
(= 8 * 4 + 1).
The model for the label estimation is learned
based on machine learning. The following fea-
tures are generally utilized: characters, type of
2Besides, there are IOB1, IOB2 algorithm using only
I,O,B and IOE1, IOE2 algorithm using only I,O,E (Kim and
Veenstra, 1999).
</bodyText>
<figure confidence="0.92608090625">
class
example
PERCENT
20 percent
PERSON
LOCATION
ORGANIZATION
ARTIFACT
DATE
TIME
MONEY
Kimura Syonosuke
Taiheiyou (Pacific Ocean)
Jimin-tou (Liberal Democratic
Party)
PL-houan (PL bill)
21-seiki (21 century)
gozen-7-ji (7 a.m.)
500-oku-en (50 billions yen)
56
final output
Habu-Yoshiharu +
Meijin
PSN+OTHERe
0.438+0.245
Yoshiharu + Meijin
MNY+OTHERe
0.075+0.245
Meijin
OTHERe
0.245
analysis direction
</figure>
<table confidence="0.767873545454545">
Habu Habu-Yoshiharu Habu-Yoshiharu- Habu Habu-Yoshiharu
PERSON PERSON Meijin PERSON PERSON
0.111 0.438 ORGANIZATION 0.111 0.438
0.083
Yoshiharu Yoshiharu-Meijin Yoshiharu
MONEY OTHERe MONEY
0.075 0.092 0.075
Meijin
OTHERe
0.245
(a):initial state (b):final output
</table>
<figureCaption confidence="0.998596">
Figure 2: An overview of our proposed method. (the bunsetsu “Habu-Yoshiharu-Meijin”)
</figureCaption>
<bodyText confidence="0.99985925">
character, POS, etc. about the morpheme and the
surrounding two morphemes. The methods utiliz-
ing SVM or CRF are proposed.
Most of NER methods based on sequential la-
beling use only local information. Therefore,
methods utilizing global information are pro-
posed. Nakano et al. utilized as a feature the word
sub class of NE on the analyzing direction in the
bunsetsu, the noun in the end of the bunsetsu ad-
jacent to the analyzing direction, and the head of
each bunsetsu (Nakano and Hirai, 2004). Sasano
et al. utilized cache feature, coreference result,
syntactic feature, and caseframe feature as struc-
tural features (Sasano and Kurohashi, 2008).
Some work acquired knowledge from unan-
notated large corpus, and applied it to NER.
Kazama et al. utilized a Named Entity dic-
tionary constructed from Wikipedia and a noun
clustering result obtained using huge amount of
pairs of dependency relations (Kazama and Tori-
sawa, 2008). Fukushima et al. acquired huge
amount of category-instance pairs (e.g., “po-
litical party - New party DAICHI”,“company-
TOYOTA”) by some patterns from a large Web
corpus (Fukushima et al., 2008).
In Japanese NER researches, CRL NE data are
usually utilized for the evaluation. This data in-
cludes approximately 10 thousands sentences in
news paper articles, in which approximately 20
thousands NEs are annotated. Previous work
achieved an F measure of about 0.89 using this
data.
</bodyText>
<sectionHeader confidence="0.932538" genericHeader="method">
3 Overview of Proposed Method
</sectionHeader>
<bodyText confidence="0.999860137931034">
Our proposed method first estimates the label of
all the compound nouns (chunk) in a bunsetsu.
Then, the best label assignment is determined
by bottom-up dynamic programming as the CKY
parsing algorithm. Figure 2 illustrates an overview
of our proposed method. In this example, the
bunsetsu “Habu-Yoshiharu-Meijin” (Grand Mas-
ter Yoshiharu Habu) is analyzed. First, the labels
of all the chunks (“Habu”, “Habu-Yoshiharu”,
“Habu-Yoshiharu-Meijin”, • • •, “Meijin”, etc.) in
the bunsetsu are analyzed using a machine learn-
ing method as shown in Figure 2 (a).
We call the state in Figure 2 (a) initial state,
where the labels of all the chunks have been es-
timated. From this state, the best label assign-
ment in the bunsetsu is determined. This pro-
cedure is performed from the lower left (corre-
sponds to each morpheme) to the upper right like
the CKY parsing algorithm as shown in Figure 2
(b). For example, when the label assignment for
“Habu-Yoshiharu” is determined, the label assign-
ment “Habu-Yoshiharu” (PERSON) and the label
assignment “Habu” (PERSON) and “Yoshiharu”
(OTHER) are compared, and the better one is cho-
sen. While grammatical rules are utilized in a
general CKY algorithm, this method chooses bet-
ter label assignment for each cell using a machine
learning method.
The learned models are the followings:
</bodyText>
<listItem confidence="0.985293">
• the model that estimates the label of a chunk
(label estimation model)
• the model that compares two label assign-
ments (label comparison model)
</listItem>
<bodyText confidence="0.9934215">
The two models are described in detail in the
next section.
</bodyText>
<page confidence="0.985316">
57
</page>
<figure confidence="0.9988648">
Habu Yoshiharu Meijin ga
PERSON OTHERe
invalid invalid
invalid
invalid
</figure>
<figureCaption confidence="0.977818">
Figure 3: Label assignment for all the chunks in
the bunsetsu “Habu-Yoshiharu-Meijin.”
</figureCaption>
<sectionHeader confidence="0.981741" genericHeader="method">
4 Model Learning
</sectionHeader>
<subsectionHeader confidence="0.9987">
4.1 Label Estimation Model
</subsectionHeader>
<bodyText confidence="0.9850855">
This model estimates the label for each chunk. An
analysis unit is basically bunsetsu. This is because
93.5% of named entities is located in a bunsetsu
in CRL NE data. Exceptionally, the following ex-
pressions located in multiple bunsetsus tend to be
an NE:
</bodyText>
<listItem confidence="0.998658833333333">
• expressions enclosed in parentheses (e.g., “
‘Himeyuri-no tou’ ” (The tower of Himeyuri)
(ARTIFACT))
• expressions that have an entry in Wikipedia
(e.g., “Nihon-yatyou-no kai” (Wild Bird So-
ciety of Japan) (ORGANIZATION))
</listItem>
<bodyText confidence="0.9999065">
Hereafter, bunsetsu is expanded when one of the
above conditions meet. By this expansion, 98.6%
of named entities is located in a bunsetsu3.
For each bunsetsu, the head or tail function
words are deleted. For example, in the bun-
setsu “Habu-Yoshiharu-Meijin-wa”, the tail func-
tion word “wa” (TOP) is deleted. In the bunsetsu
“yaku-san-bai” (about three times), the head func-
tion word “yaku” (about) is deleted.
Next, for learning the label estimation model,
all the chunks in a bunsetsu are attached to the cor-
rect label from a hand-annotated corpus. The la-
bel set is 13 classes, which includes eight NE class
(as shown in Table 1), and five classes: OTHERs,
OTHERb, OTHERi, OTHERe, and invalid.
The chunk that corresponds to a whole bun-
setsu and does not contain any NEs is labeled
as OTHERs, and the head, middle, tail chunk
that does not correspond to an NE is labeled as
OTHERb, OTHERi, OTHERe, respectively4.
</bodyText>
<footnote confidence="0.997695166666667">
3As an example in which an NE is not included by an
expanded bunsetsu, there are “Toru-no Kimi” (PERSON)
and “Osaka-fu midori-no kankyo-seibi-shitsu” (ORGANI-
ZATION).
4Each OTHER is assigned to the longest chunk that satis-
fies its condition in a chunk.
</footnote>
<listItem confidence="0.966053583333333">
1. # of morphemes in the chunk
2. the position of the chunk in its bunsetsu
3. character type5
4. the combination of the character type of adjoining
morphemes
- For the chunk “Russian Army”, this feature is
“Katakana,Kanji”
5. word class, word sub class, and several features pro-
vided by a morphological analyzer JUMAN
6. several features6 provided by a parser KNP
7. string of the morpheme in the chunk
8. IPADIC7 feature
</listItem>
<bodyText confidence="0.8777885">
- If the string of the chunk are registered in the fol-
lowing categories of IPADIC: “person”, “lo-
cation”, “organization”, and “general”, this
feature fires.
</bodyText>
<sectionHeader confidence="0.632958" genericHeader="method">
9. Wikipedia feature
</sectionHeader>
<bodyText confidence="0.9971955">
- If the string of the chunk has an entry in
Wikipedia, this feature fires.
- the hypernym extracted from its definition sen-
tence using some patterns (e.g., The hyper-
nym of “the Liberal Democratic Party” is a
political party.)
</bodyText>
<subsectionHeader confidence="0.445937">
10. cache feature
</subsectionHeader>
<bodyText confidence="0.810813388888889">
- When the same string of the chunk appears in the
preceding context, the label of the preceding
chunk is used for the feature.
11. particles that the bunsetsu includes
12. the morphemes, particles, and head morpheme in the
parent bunsetsu
13. the NE/category ratio in a case slot of predicate/noun
case frame(Sasano and Kurohashi, 2008)
- For example, in the case ga (NOM) of the pred-
icate case frame “kaiken” (interview), the NE
ratio “PERSON:0.245” is assigned to the case
slot. Hence, in the sentence “Habu-ga kaiken-
shita” (Mr. Habu interviewed), the feature
“PERSON:0.245” is utilized for the chunk
“Habu.”
14. parenthesis feature
- When the chunk in a parenthesis, this feature
fires.
</bodyText>
<tableCaption confidence="0.805489">
Table 2: Features for the label estimation model.
</tableCaption>
<bodyText confidence="0.998624875">
The chunk that is neither any eight NE class nor
the above four OTHER is labeled as invalid.
In an example as shown in Figure 3, “Habu-
Yoshiharu” is labeled as PERSON, “Meijin” is la-
beled as OTHERe, and the other chunks are la-
beled as invalid.
Next, the label estimation model is learned from
the data in which the above label set is assigned
</bodyText>
<footnote confidence="0.9977776">
5The following five character types are considered: Kanji,
Hiragana, Katakana, Number, and Alphabet.
6When a morpheme has an ambiguity, all the correspond-
ing features fire.
7http://chasen.aist-nara.ac.jp/chasen/distribution.html.ja
</footnote>
<page confidence="0.998161">
58
</page>
<bodyText confidence="0.998007625">
to all the chunks. The features for the label esti-
mation model are shown in Table 2. Among the
features, as for feature (3), (5)−(8), three cate-
gories according to the position of a morpheme
in the chunk are prepared: “head”, “tail”, and
“anywhere.” For example, in the chunk “Habu-
Yoshiharu-Meijin,” as for the morpheme “Habu”,
feature (7) is set to be “Habu” in “head” and as for
the morpheme “Yoshiharu”, feature (7) is set to be
“Yoshiharu” in “anywhere.”
The label estimation model is learned from pairs
of label and feature in each chunk. To classify the
multi classes, the one-vs-rest method is adopted
(consequently, 13 models are learned). The SVM
output is transformed by using the sigmoid func-
tion 1 , and the transformed value is nor-
</bodyText>
<equation confidence="0.803701">
1+exp(−,C3x)
</equation>
<bodyText confidence="0.996796916666667">
malized so that the sum of the value of 13 labels
in a chunk is one.
The purpose for setting up the label “invalid” is
as follows. In the chunk “Habu” and “Yoshiharu”
in Figure 3, since the label “invalid” has a rela-
tively higher score, the score of the label PERSON
is relatively low. Therefore, when the label com-
parison described in Section 4.2 is performed, the
label assignment “Habu-Yoshiharu” (PERSON) is
likely to be chosen. In the chunk where the score
of the label invalid has the highest score, the label
that has the second highest score is adopted.
</bodyText>
<subsectionHeader confidence="0.958307">
4.2 Label Comparison Model
</subsectionHeader>
<bodyText confidence="0.999848">
This model compares the two label assignments
for a certain string. For example, in the string
“Habu-Yoshiharu”, the model compares the fol-
lowing two label assignments:
</bodyText>
<listItem confidence="0.988073333333333">
• “Habu-Yoshiharu” is labeled as PERSON
• “Habu” is labeled as PERSON and “Yoshi-
haru” is labeled as MONEY
</listItem>
<bodyText confidence="0.999886083333333">
First, as shown in Figure 4, the two compared
sets of chunks are lined up by sandwiching “vs.”
(The left one, right one is called the first set, the
second set, respectively.) When the first set is cor-
rect, this example is positive: otherwise, this ex-
ample is negative. The max number of chunks for
each set is five, and thus examples in which the
first or second set has more than five chunks are
not utilized for the model learning.
Then, the feature is assigned to each example.
The feature (13 dimensions) for each chunk is de-
fined as follows: the first 12 dimensions are used
</bodyText>
<figure confidence="0.9871356">
positive:
+1 Habu-Yoshiharu vs Habu + Yoshiharu
PSN PSN + MNY
+1 Habu-Yoshiharu + Meijin vs Habu + Yoshiharu + Meijin
PSN + OTHERe PSN + MONEY + OTHERe
...
negative:
- 1 Habu-Yoshiharu-Meijin vs Habu-Yoshiharu + Meijin
ORG PSN + OTHERe
...
</figure>
<figureCaption confidence="0.9852445">
Figure 4: Assignment of positive/negative exam-
ples.
</figureCaption>
<bodyText confidence="0.9955538125">
for each label, which is estimated by the label esti-
mation model, and the last 13th dimension is used
for the score of an SVM output. Then, for the first
and second set, the features for each chunk are ar-
ranged from the left, and zero vectors are placed
in the remainder part.
Figure 5 illustrates the feature for “Habu-
Yoshiharu” vs “Habu + Yoshiharu.” The label
comparison model is learned from such data us-
ing SVM. Note that only the fact that “Habu-
Yoshiharu” is PERSON can be found from the
hand-annotated corpus, and thus in the example
“Habu-Yoshiharu-Meijin” vs “Habu + Yoshiharu-
Meijin”, we cannot determine which one is cor-
rect. Therefore, such example cannot be used for
the model learning.
</bodyText>
<sectionHeader confidence="0.992203" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999950428571429">
First, the label of all the chunks in a bunsetsu is
estimated by using the label estimation model de-
scribed in Section 4.1. Then, the best label assign-
ment in the bunsetsu is determined by applying the
label comparison model described in Section 4.2
iteratively as shown in Figure 2 (b). In this step,
the better label assignment is determined from bot-
tom up as the CKY parsing algorithm.
For example, the initial state shown in Figure
2(a) is obtained using the label estimation model.
Then, the label assignment is determined using the
label comparison model from the lower left (cor-
responds to each morpheme) to the upper right.
In determining the label assignment for the cell
of “Habu-Yoshiharu” as shown in 6(a), the model
compares the label assignment “B” with the la-
bel assignment “A+D.” In this case, the model
chooses the label assignment “B”, that is, “Habu
- Yoshiharu” is labeled as PERSON. Similarly,
in determining the label assignment for the cell
of “Yoshiharu-Meijin”, the model compares the
</bodyText>
<page confidence="0.998933">
59
</page>
<figureCaption confidence="0.995267">
Figure 5: An example of the feature for the label comparison model. (The example is “Habu-Yoshiharu
vs Habu + Yoshiharu”, and V11, V21, V22, and 0 is a vector whose dimension is 13.)
Figure 6: The label comparison model.
</figureCaption>
<figure confidence="0.999669852941176">
vector
0
0
0
0
0
0
0
V11
V22
V21
chunk
label
Habu-Yoshiharu
PERSON
Habu
PERSON
Yoshiharu
MONEY
label assignment
“Habu-Yoshiharu-Meijin”
label assighment “Habu-Yoshiharu&apos;
(a): label assignment for the cell “Habu-Yoshiharu”.
(b): label assignment for the cell “Habu-Yoshiharu-Meijin”.
Habu
PERSON
0.111
A Habu-Yoshiharu B
PERSON
0.438
Habu-Yoshiharu-
Meijin
ORGANIZATION
0.083
C
Habu
PERSON
0.111
A Habu-YoshiharuB Habu-Yoshiharu- C
Meijin
PERSON
ORGANIZATION
0.438 0.083
label assignment
“Habu&apos; + “Yoshiharu&apos;
Yoshiharu
MONEY
0.075
D Yoshiharu-Meijin E
OTHERe
0.092
Meijin
OTHERe
0.245
F
Yoshiharu D Yoshiharu + MeijinE
label assignment
“Habu” + “Yoshiharu” + “Meijin”
label assignment
“Habu-Yoshiharu” + “Meijin”
MONEY
0.075
MNY+OTHERe
0.075+0.245
Meijin
OTHERe
0.245
F
</figure>
<bodyText confidence="0.999874458333333">
label assignment “E” with the label assignment
“D+F.” In this case, the model chooses the label
assignment “D+F”, that is, “Yoshiharu” is labeled
as MONEY and “Meijin” is labeled as OTHERe.
When the label assignment consists of multiple
chunks, the content of the cell is updated. In
this case, the cell “E” is changed from “Yoshi-
haru-Meijin” (OTHERe) to “Yoshiharu + Meijin”
(MONEY + OTHERe).
As shown in Figure 6(b), in determining the best
label assignment for the upper right cell, that is,
the final output is determined, the model compares
the label assignment “A+D+F”, “B+F”, and “C”.
When there are more than two candidates of label
assignments for a cell, all the label assignments are
compared in a pairwise, and the label assignment
that obtains the highest score is adopted.
In the label comparing step, the label as-
signment in which OTHER* follows OTHER*
(OTHER* - OTHER*) is not allowed since each
OTHER is assigned to the longest chunk as de-
scribed in Section 4.1. When the first combina-
tion of chunks equals to the second combination
of chunks, the comparison is not performed.
</bodyText>
<sectionHeader confidence="0.997544" genericHeader="evaluation">
6 Experiment
</sectionHeader>
<bodyText confidence="0.99995">
To demonstrate the effectiveness of our proposed
method, we conducted an experiment on CRL NE
data. In this data, 10,718 sentences in 1,174 news
articles are annotated with eight NEs. The expres-
sion to which it is difficult to annotate manually is
labeled as OPTIONAL, and was not used for both
</bodyText>
<figure confidence="0.390135333333333">
Corpus: CRL NE data
learn the label estimation model 1
learn the label comparison model
</figure>
<figureCaption confidence="0.99944">
Figure 7: 5-fold cross validation.
</figureCaption>
<bodyText confidence="0.998176357142857">
the model learning8 and the evaluation.
We performed 5-fold cross validation following
previous work. Different from previous work, our
work has to learn the SVM models twice. There-
fore, the corpus was divided as shown in Figure 7.
Let us consider the analysis in the part (a). First,
the label estimation model 1 is learned from the
part (b)-(e). Then, the label estimation model 2b
is learned from the part (c)-(e), and applying the
learned model to the part (b), features for learning
the label comparison model are obtained. Simi-
larly, the label estimation model 2c is learned from
the part (b),(d),(e), and applying it to the part (c),
features are obtained. It is the same with the part
</bodyText>
<footnote confidence="0.870771">
8Exceptionally, “OPTIONAL” is used when the label es-
timation model for OTHER* and invalid is learned.
</footnote>
<figure confidence="0.97995275">
learn the label estimation model 2c
a
b
c d e
learn the label estimation model 2b
obtain features for the label apply
comparison model
b c d e
b c d e
learn the label estimation model 2d
b c d e
learn the label estimation model 2e
</figure>
<page confidence="0.959277">
60
</page>
<table confidence="0.999123">
Recall Precision
ORGANIZATION 81.83 (3008/3676) 88.37 (3008/3404)
PERSON 90.05 (3458/3840) 93.87 (3458/3684)
LOCATION 91.38 (4992/5463) 92.44 (4992/5400)
ARTIFACT 46.72 ( 349/ 747) 74.89 ( 349/ 466)
DATE 93.27 (3327/3567) 93.12 (3327/3573)
TIME 88.25 ( 443/ 502) 90.59 ( 443/ 489)
MONEY 93.85 ( 366/ 390) 97.60 ( 366/ 375)
PERCENT 95.33 ( 469/ 492) 95.91 ( 469/ 489)
ALL-SLOT 87.87 91.79
F-measure 89.79
</table>
<tableCaption confidence="0.999896">
Table 3: Experimental result.
</tableCaption>
<bodyText confidence="0.99960725">
(d) and (e). Then, the label comparison model is
learned from the obtained features. After that, the
analysis in the part (a) is performed by using both
the label estimation model 1 and the label compar-
ison model.
In this experiment, a Japanese morphological
analyzer, JUMAN9, and a Japanese parser, KNP10
were adopted. The two SVM models were learned
with polynomial kernel of degree 2, and Q in the
sigmoid function was set to be 1.
Table 6 shows an experimental result. An F-
measure in all NE classes is 89.79.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="discussions">
7 Discussion
</sectionHeader>
<subsectionHeader confidence="0.99983">
7.1 Comparison with Previous Work
</subsectionHeader>
<bodyText confidence="0.999805130434782">
Table 7 presents the comparison with previ-
ous work, and our method outperformed previ-
ous work. Among previous work, Fukushima
et al. acquired huge amount of category-
instance pairs (e.g., “political party - New party
DAICHI”,“company-TOYOTA”) by some patterns
from a large Web corpus, and Sasano et al. uti-
lized the analysis result of corefer resolution as a
feature for the model learning. Therefore, in our
method, by incorporating these knowledge and/or
such analysis result, the performance would be im-
proved.
Compared with Sasano et al., our method
achieved the better performance in analyzing
a long compound noun. For example, in the
bunsetsu “Oushu-tsuujyou-senryoku-sakugen-
jyouyaku” (Treaty on Conventional Armed
Forces in Europe), while Sasano et al. labeled
“Oushu” (Europe) as LOCATION, our method
correctly labeled “Oushu-tsuujyou-senryoku-
sakugen-jyouyaku” as ARTIFACT. Sasano et
al. incorrectly labeled “Oushu” as LOCATION
although they utilized the information about
</bodyText>
<footnote confidence="0.988771">
9http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
10http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
</footnote>
<bodyText confidence="0.999966166666667">
the head of bunsetsu “jyouyaku” (treaty). In
our method, for the cell “Oushu”, invalid has
the highest score, and thus the score of LOCA-
TION relatively drops. Similarly, for the cell
“senryoku-sakugen-jyouyaku”, invalid has the
highest score. Consequently, “Oushu-tsuujyou-
senryoku-sakugen-jyouyaku” is correctly labeled
as ARTIFACT.
In the bunsetsu “gaikoku-jin-touroku-hou-ihan”
(the violation of the foreigner registration law),
while Sasano et al. labeled “touroku-hou” as AR-
TIFACT, our method correctly labeled “gaikoku-
jin-touroku-hou” as ARTIFACT. Sasano et al. can-
not utilize the information about “hou” that is use-
ful for the label estimation since the head of this
bunsetsu is “ihan.” In contrast, in estimating the
label of the chunk “gaikoku-jin-touroku-hou”, the
information of “hou” can be utilized.
</bodyText>
<subsectionHeader confidence="0.823126">
7.2 Error Analysis
</subsectionHeader>
<bodyText confidence="0.99990525">
There were some errors in analyzing a Katakana
alphabet word. In the following example, although
the correct is that “Batistuta” is labeled as PER-
SON, the system labeled it as OTHERs.
</bodyText>
<figure confidence="0.5718975">
(4) Italy-de katsuyaku-suru Batistuta-wo
Italy LOC active Batistuta ACC
kuwaeta Argentine
call Argentine
</figure>
<bodyText confidence="0.993724090909091">
’Argentine called Batistuta who was active in
Italy.’
There is not an entry of “Batistuta” in the dictio-
nary of JUMAN nor Wikipedia, and thus only the
surrounding information is utilized. However, the
case analysis of “katsuyaku” (active) is incorrect,
which leads to the error of “Batistuta”.
There were some errors in applying the la-
bel comparison model although the analysis of
each chunk is correct. For example, in the
bunsetsu “HongKong-seityou” (Government of
HongKong), the correct is that “HongKong-
seityou” is labeled as ORGANIZATION. As
shown in Figure 8 (b), the system incorrectly
labeled “HongKong” as LOCATION. As shown
in Figure 8(a), although in the initial state,
“HongKong-seityou” was correctly labeled as OR-
GANIZATION, the label assignment “HongKong
+ seityou” was incorrectly chosen by the label
comparison model. To cope with this problem,
we are planning to the adjustment of the value Q
in the sigmoid function and the refinement of the
</bodyText>
<page confidence="0.998822">
61
</page>
<table confidence="0.9850965">
F1 analysis unit distinctive features
(Fukushima et al., 2008) 89.29 character Web
(Kazama and Torisawa, 2008) 88.93 character Wikipedia,Web
(Sasano and Kurohashi, 2008) 89.40 morpheme structural information
(Nakano and Hirai, 2004) 89.03 character bunsetsu feature
(Masayuki and Matsumoto, 2003) 87.21 character
(Isozaki and Kazawa, 2003) 86.77 morpheme
proposed method 89.79 compound noun Wikipedia,structural information
</table>
<tableCaption confidence="0.8929065">
Table 4: Comparison with previous work. (All work was evaluated on CRL NE data using cross valida-
tion.)
</tableCaption>
<table confidence="0.8098932">
HongKong HongKong-seityou HongKong HongKong + seityou
LOCATION ORGANIZATION LOCATION LOC+OTHERe
0.266 0.205 0.266 0.266+0.184
seityou seityou
OTHERe OTHERe
</table>
<reference confidence="0.5462325">
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL2007),
pages 304–311.
0.184 0.184
(a):initial state (b):the final output
</reference>
<figureCaption confidence="0.9957145">
Figure 8: An example of the error in the label com-
parison model.
</figureCaption>
<bodyText confidence="0.985257">
features for the label comparison model.
</bodyText>
<sectionHeader confidence="0.986466" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999978384615385">
This paper proposed bottom-up Named Entity
Recognition using a two-stage machine learning
method. This method first estimates the label of
all the chunks in a bunsetsu using a machine learn-
ing, and then the best label assignment is deter-
mined by bottom-up dynamic programming. We
conducted an experiment on CRL NE data, and
achieved an F-measure of 89.79.
We are planning to integrate this method with
the syntactic and case analysis method (Kawa-
hara and Kurohashi, 2007), and perform syntactic,
case, and Named Entity analysis simultaneously to
improve the overall accuracy.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989354">
Ken’ichi Fukushima, Nobuhiro Kaji, and Masaru
Kitsuregawa. 2008. Use of massive amounts
of web text in Japanese named entity recogni-
tion. In Proceedings of Data Engineering Workshop
(DEWS2008). A3-3 (in Japanese).
IREX Committee, editor. 1999. Proceedings of the
IREX Workshop.
Hideki Isozaki and Hideto Kazawa. 2003. Speeding up
support vector machines for named entity recogni-
tion. Transaction of Information Processing Society
of Japan, 44(3):970–979. (in Japanese).
Daisuke Kawahara and Sadao Kurohashi. 2007. Prob-
abilistic coordination disambiguation in a fully-
lexicalized Japanese parser. In Proceedings of the
Jun’ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings ofACL-08: HLT, pages 407–415.
Erik F. Tjong Kim and Jorn Veenstra. 1999. Repre-
senting text chunks. In Proceedings of EACL ’99,
pages 173–179.
Vajay Krishnan and Christopher D.Manning. 2006.
An effective two-stage model for exploiting non-
local dependencies in named entity recognition.
pages 1121–1128.
John Lafferty, Andrew McCallun, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference (ICML’01), pages 282–289.
Asahara Masayuki and Yuji Matsumoto. 2003.
Japanese named entity extraction with redundant
morphological analysis. In Proceeding of HLT-
NAACL 2003, pages 8–15.
Keigo Nakano and Yuzo Hirai. 2004. Japanese named
entity extraction with bunsetsu features. Transac-
tion of Information Processing Society of Japan,
45(3):934–941. (in Japanese).
Ryohei Sasano and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural
language processing. In Proceeding of Third In-
ternational Joint Conference on Natural Language
Processing, pages 607–612.
Satoshi Sekine, Ralph Grishman, and Hiroyuki Shin-
nou. 1998. A decision tree method for finding and
classifying names in japanese texts. In Proceed-
ings of the Sixth Workshop on Very Large Corpora
(WVLC-6), pages 171–178.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
</reference>
<page confidence="0.999188">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.472784">
<title confidence="0.9938435">Bottom-up Named Entity using a Two-stage Machine Learning Method</title>
<author confidence="0.930456">Hirotaka Funayama Tomohide Shibata Sadao</author>
<affiliation confidence="0.999935">Kyoto University,</affiliation>
<address confidence="0.77713">Sakyo-ku, Kyoto, 606-8501, shibata,</address>
<abstract confidence="0.992864571428572">This paper proposes Japanese bottom-up named entity recognition using a twostage machine learning method. Most work has formalized Named Entity Recognition as a sequential labeling problem, in which only local information is utilized for the label estimation, and thus a long named entity consisting of several morphemes tends to be wrongly recognized. Our proposed method regards a compound as a labeling unit, and first estimates the labels of all the chunks in phrasal unit using a machine learning method. Then, the best label asin the determined from bottom up as the CKY parsing algorithm using a machine learning method. We conducted an experimental on CRL NE data, and achieved an F measure of 89.79, which is higher than previous work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2007</date>
<booktitle>Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL2007),</booktitle>
<pages>304--311</pages>
<marker>2007</marker>
<rawString>2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL2007), pages 304–311.</rawString>
</citation>
<citation valid="false">
<note>0.184 0.184 (a):initial state (b):the final output</note>
<marker></marker>
<rawString>0.184 0.184 (a):initial state (b):the final output</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken’ichi Fukushima</author>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Use of massive amounts of web text in Japanese named entity recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of Data Engineering Workshop (DEWS2008).</booktitle>
<pages>3--3</pages>
<note>(in Japanese).</note>
<contexts>
<context position="1585" citStr="Fukushima et al., 2008" startWordPosition="238" endWordPosition="241">RL NE data, and achieved an F measure of 89.79, which is higher than previous work. 1 Introduction Named Entity Recognition (NER) is a task of recognizing named entities such as person names, organization names, and location. It is used for several NLP applications such as Information Extraction (IE) and Question Answering (QA). Most work uses machine learning methods such as Support Vector Machines (SVMs) (Vapnik, 1995) and Conditional Random Field (CRF) (Lafferty et al., 2001) using a hand-annotated corpus (Krishnan and D.Manning, 2006; Kazama and Torisawa, 2008; Sasano and Kurohashi, 2008; Fukushima et al., 2008; Nakano and Hirai, 2004; Masayuki and Matsumoto, 2003). In general, NER is formalized as a sequential labeling problem. For example, regarding a morpheme as a basic unit, it is first labeled as SPERSON, B-PERSON, I-PERSON, E-PERSON, S-ORGANIZATION, etc. Then, considering the labeling results of morphemes, the best NE label sequence is recognized. When the label of each morpheme is estimated, only local information around the morpheme (e.g., the morpheme, the two preceding morphemes, and the two following morphemes) is utilized. Therefore, a long named entity consisting of several morphemes te</context>
<context position="9051" citStr="Fukushima et al., 2008" startWordPosition="1411" endWordPosition="1414">). Sasano et al. utilized cache feature, coreference result, syntactic feature, and caseframe feature as structural features (Sasano and Kurohashi, 2008). Some work acquired knowledge from unannotated large corpus, and applied it to NER. Kazama et al. utilized a Named Entity dictionary constructed from Wikipedia and a noun clustering result obtained using huge amount of pairs of dependency relations (Kazama and Torisawa, 2008). Fukushima et al. acquired huge amount of category-instance pairs (e.g., “political party - New party DAICHI”,“companyTOYOTA”) by some patterns from a large Web corpus (Fukushima et al., 2008). In Japanese NER researches, CRL NE data are usually utilized for the evaluation. This data includes approximately 10 thousands sentences in news paper articles, in which approximately 20 thousands NEs are annotated. Previous work achieved an F measure of about 0.89 using this data. 3 Overview of Proposed Method Our proposed method first estimates the label of all the compound nouns (chunk) in a bunsetsu. Then, the best label assignment is determined by bottom-up dynamic programming as the CKY parsing algorithm. Figure 2 illustrates an overview of our proposed method. In this example, the bun</context>
<context position="26892" citStr="Fukushima et al., 2008" startWordPosition="4330" endWordPosition="4333"> For example, in the bunsetsu “HongKong-seityou” (Government of HongKong), the correct is that “HongKongseityou” is labeled as ORGANIZATION. As shown in Figure 8 (b), the system incorrectly labeled “HongKong” as LOCATION. As shown in Figure 8(a), although in the initial state, “HongKong-seityou” was correctly labeled as ORGANIZATION, the label assignment “HongKong + seityou” was incorrectly chosen by the label comparison model. To cope with this problem, we are planning to the adjustment of the value Q in the sigmoid function and the refinement of the 61 F1 analysis unit distinctive features (Fukushima et al., 2008) 89.29 character Web (Kazama and Torisawa, 2008) 88.93 character Wikipedia,Web (Sasano and Kurohashi, 2008) 89.40 morpheme structural information (Nakano and Hirai, 2004) 89.03 character bunsetsu feature (Masayuki and Matsumoto, 2003) 87.21 character (Isozaki and Kazawa, 2003) 86.77 morpheme proposed method 89.79 compound noun Wikipedia,structural information Table 4: Comparison with previous work. (All work was evaluated on CRL NE data using cross validation.) HongKong HongKong-seityou HongKong HongKong + seityou LOCATION ORGANIZATION LOCATION LOC+OTHERe 0.266 0.205 0.266 0.266+0.184 seityou </context>
</contexts>
<marker>Fukushima, Kaji, Kitsuregawa, 2008</marker>
<rawString>Ken’ichi Fukushima, Nobuhiro Kaji, and Masaru Kitsuregawa. 2008. Use of massive amounts of web text in Japanese named entity recognition. In Proceedings of Data Engineering Workshop (DEWS2008). A3-3 (in Japanese).</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>Proceedings of the IREX Workshop.</booktitle>
<editor>IREX Committee, editor.</editor>
<marker>1999</marker>
<rawString>IREX Committee, editor. 1999. Proceedings of the IREX Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Hideto Kazawa</author>
</authors>
<title>Speeding up support vector machines for named entity recognition.</title>
<date>2003</date>
<journal>Transaction of Information Processing Society of Japan,</journal>
<volume>44</volume>
<issue>3</issue>
<note>(in Japanese).</note>
<marker>Isozaki, Kazawa, 2003</marker>
<rawString>Hideki Isozaki and Hideto Kazawa. 2003. Speeding up support vector machines for named entity recognition. Transaction of Information Processing Society of Japan, 44(3):970–979. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Probabilistic coordination disambiguation in a fullylexicalized Japanese parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the</booktitle>
<marker>Kawahara, Kurohashi, 2007</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2007. Probabilistic coordination disambiguation in a fullylexicalized Japanese parser. In Proceedings of the</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>407--415</pages>
<contexts>
<context position="1533" citStr="Kazama and Torisawa, 2008" startWordPosition="230" endWordPosition="233">hine learning method. We conducted an experimental on CRL NE data, and achieved an F measure of 89.79, which is higher than previous work. 1 Introduction Named Entity Recognition (NER) is a task of recognizing named entities such as person names, organization names, and location. It is used for several NLP applications such as Information Extraction (IE) and Question Answering (QA). Most work uses machine learning methods such as Support Vector Machines (SVMs) (Vapnik, 1995) and Conditional Random Field (CRF) (Lafferty et al., 2001) using a hand-annotated corpus (Krishnan and D.Manning, 2006; Kazama and Torisawa, 2008; Sasano and Kurohashi, 2008; Fukushima et al., 2008; Nakano and Hirai, 2004; Masayuki and Matsumoto, 2003). In general, NER is formalized as a sequential labeling problem. For example, regarding a morpheme as a basic unit, it is first labeled as SPERSON, B-PERSON, I-PERSON, E-PERSON, S-ORGANIZATION, etc. Then, considering the labeling results of morphemes, the best NE label sequence is recognized. When the label of each morpheme is estimated, only local information around the morpheme (e.g., the morpheme, the two preceding morphemes, and the two following morphemes) is utilized. Therefore, a </context>
<context position="8858" citStr="Kazama and Torisawa, 2008" startWordPosition="1380" endWordPosition="1384"> the word sub class of NE on the analyzing direction in the bunsetsu, the noun in the end of the bunsetsu adjacent to the analyzing direction, and the head of each bunsetsu (Nakano and Hirai, 2004). Sasano et al. utilized cache feature, coreference result, syntactic feature, and caseframe feature as structural features (Sasano and Kurohashi, 2008). Some work acquired knowledge from unannotated large corpus, and applied it to NER. Kazama et al. utilized a Named Entity dictionary constructed from Wikipedia and a noun clustering result obtained using huge amount of pairs of dependency relations (Kazama and Torisawa, 2008). Fukushima et al. acquired huge amount of category-instance pairs (e.g., “political party - New party DAICHI”,“companyTOYOTA”) by some patterns from a large Web corpus (Fukushima et al., 2008). In Japanese NER researches, CRL NE data are usually utilized for the evaluation. This data includes approximately 10 thousands sentences in news paper articles, in which approximately 20 thousands NEs are annotated. Previous work achieved an F measure of about 0.89 using this data. 3 Overview of Proposed Method Our proposed method first estimates the label of all the compound nouns (chunk) in a bunsets</context>
</contexts>
<marker>Kazama, Torisawa, 2008</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations. In Proceedings ofACL-08: HLT, pages 407–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim</author>
<author>Jorn Veenstra</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL ’99,</booktitle>
<pages>173--179</pages>
<contexts>
<context position="7185" citStr="Kim and Veenstra, 1999" startWordPosition="1135" endWordPosition="1138">morpheme) to several labels. In an SE-algorithm (Sekine et al., 1998), S is assigned to NE composed of one morpheme, B, I, E is assigned to the beginning, middle, end of NE, respectively, and O is assigned to the morpheme that is not an NE2. The labels S, B, I, and E are prepared for each NE classes, and thus the total number of labels is 33 (= 8 * 4 + 1). The model for the label estimation is learned based on machine learning. The following features are generally utilized: characters, type of 2Besides, there are IOB1, IOB2 algorithm using only I,O,B and IOE1, IOE2 algorithm using only I,O,E (Kim and Veenstra, 1999). class example PERCENT 20 percent PERSON LOCATION ORGANIZATION ARTIFACT DATE TIME MONEY Kimura Syonosuke Taiheiyou (Pacific Ocean) Jimin-tou (Liberal Democratic Party) PL-houan (PL bill) 21-seiki (21 century) gozen-7-ji (7 a.m.) 500-oku-en (50 billions yen) 56 final output Habu-Yoshiharu + Meijin PSN+OTHERe 0.438+0.245 Yoshiharu + Meijin MNY+OTHERe 0.075+0.245 Meijin OTHERe 0.245 analysis direction Habu Habu-Yoshiharu Habu-Yoshiharu- Habu Habu-Yoshiharu PERSON PERSON Meijin PERSON PERSON 0.111 0.438 ORGANIZATION 0.111 0.438 0.083 Yoshiharu Yoshiharu-Meijin Yoshiharu MONEY OTHERe MONEY 0.075 0</context>
</contexts>
<marker>Kim, Veenstra, 1999</marker>
<rawString>Erik F. Tjong Kim and Jorn Veenstra. 1999. Representing text chunks. In Proceedings of EACL ’99, pages 173–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vajay Krishnan</author>
<author>Christopher D Manning</author>
</authors>
<title>An effective two-stage model for exploiting nonlocal dependencies in named entity recognition.</title>
<date>2006</date>
<pages>1121--1128</pages>
<marker>Krishnan, Manning, 2006</marker>
<rawString>Vajay Krishnan and Christopher D.Manning. 2006. An effective two-stage model for exploiting nonlocal dependencies in named entity recognition. pages 1121–1128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallun</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference (ICML’01),</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1446" citStr="Lafferty et al., 2001" startWordPosition="218" endWordPosition="221">n the bunsetsu is determined from bottom up as the CKY parsing algorithm using a machine learning method. We conducted an experimental on CRL NE data, and achieved an F measure of 89.79, which is higher than previous work. 1 Introduction Named Entity Recognition (NER) is a task of recognizing named entities such as person names, organization names, and location. It is used for several NLP applications such as Information Extraction (IE) and Question Answering (QA). Most work uses machine learning methods such as Support Vector Machines (SVMs) (Vapnik, 1995) and Conditional Random Field (CRF) (Lafferty et al., 2001) using a hand-annotated corpus (Krishnan and D.Manning, 2006; Kazama and Torisawa, 2008; Sasano and Kurohashi, 2008; Fukushima et al., 2008; Nakano and Hirai, 2004; Masayuki and Matsumoto, 2003). In general, NER is formalized as a sequential labeling problem. For example, regarding a morpheme as a basic unit, it is first labeled as SPERSON, B-PERSON, I-PERSON, E-PERSON, S-ORGANIZATION, etc. Then, considering the labeling results of morphemes, the best NE label sequence is recognized. When the label of each morpheme is estimated, only local information around the morpheme (e.g., the morpheme, t</context>
</contexts>
<marker>Lafferty, McCallun, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallun, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference (ICML’01), pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asahara Masayuki</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese named entity extraction with redundant morphological analysis.</title>
<date>2003</date>
<booktitle>In Proceeding of HLTNAACL</booktitle>
<pages>8--15</pages>
<contexts>
<context position="1640" citStr="Masayuki and Matsumoto, 2003" startWordPosition="246" endWordPosition="249">which is higher than previous work. 1 Introduction Named Entity Recognition (NER) is a task of recognizing named entities such as person names, organization names, and location. It is used for several NLP applications such as Information Extraction (IE) and Question Answering (QA). Most work uses machine learning methods such as Support Vector Machines (SVMs) (Vapnik, 1995) and Conditional Random Field (CRF) (Lafferty et al., 2001) using a hand-annotated corpus (Krishnan and D.Manning, 2006; Kazama and Torisawa, 2008; Sasano and Kurohashi, 2008; Fukushima et al., 2008; Nakano and Hirai, 2004; Masayuki and Matsumoto, 2003). In general, NER is formalized as a sequential labeling problem. For example, regarding a morpheme as a basic unit, it is first labeled as SPERSON, B-PERSON, I-PERSON, E-PERSON, S-ORGANIZATION, etc. Then, considering the labeling results of morphemes, the best NE label sequence is recognized. When the label of each morpheme is estimated, only local information around the morpheme (e.g., the morpheme, the two preceding morphemes, and the two following morphemes) is utilized. Therefore, a long named entity consisting of several morphemes tends to be wrongly recognized. Let us consider the examp</context>
</contexts>
<marker>Masayuki, Matsumoto, 2003</marker>
<rawString>Asahara Masayuki and Yuji Matsumoto. 2003. Japanese named entity extraction with redundant morphological analysis. In Proceeding of HLTNAACL 2003, pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keigo Nakano</author>
<author>Yuzo Hirai</author>
</authors>
<title>Japanese named entity extraction with bunsetsu features.</title>
<date>2004</date>
<journal>Transaction of Information Processing Society of Japan,</journal>
<volume>45</volume>
<issue>3</issue>
<note>(in Japanese).</note>
<contexts>
<context position="1609" citStr="Nakano and Hirai, 2004" startWordPosition="242" endWordPosition="245"> an F measure of 89.79, which is higher than previous work. 1 Introduction Named Entity Recognition (NER) is a task of recognizing named entities such as person names, organization names, and location. It is used for several NLP applications such as Information Extraction (IE) and Question Answering (QA). Most work uses machine learning methods such as Support Vector Machines (SVMs) (Vapnik, 1995) and Conditional Random Field (CRF) (Lafferty et al., 2001) using a hand-annotated corpus (Krishnan and D.Manning, 2006; Kazama and Torisawa, 2008; Sasano and Kurohashi, 2008; Fukushima et al., 2008; Nakano and Hirai, 2004; Masayuki and Matsumoto, 2003). In general, NER is formalized as a sequential labeling problem. For example, regarding a morpheme as a basic unit, it is first labeled as SPERSON, B-PERSON, I-PERSON, E-PERSON, S-ORGANIZATION, etc. Then, considering the labeling results of morphemes, the best NE label sequence is recognized. When the label of each morpheme is estimated, only local information around the morpheme (e.g., the morpheme, the two preceding morphemes, and the two following morphemes) is utilized. Therefore, a long named entity consisting of several morphemes tends to be wrongly recogn</context>
<context position="2906" citStr="Nakano and Hirai, 2004" startWordPosition="451" endWordPosition="454"> (1), the label of “Kazama” can be recognized to be S-PERSON (PERSON consisting of one morpheme) by utilizing the surrounding information such as the suffix “san” (Mr.) and the verb “kikoku shita” (return home). On the other hand, in sentence (2), when the label of “shinyou” (credit) is recognized to be B-ORGANIZATION (the beginning of ORGANIZATION), only information from “hatsudou” (invoke) to “kyusai” (relief) can be utilized, and thus the information of the morpheme “ginkou” (bank) that is apart from “shinyou” by three morphemes cannot be utilized. To cope with this problem, Nakano et al. (Nakano and Hirai, 2004) and Sasano et al. (Sasano and Kurohashi, 2008) utilized information of the head of bunsetsu1. In their methods, when the label of “shinyou” is recognized, the information of the morpheme “ginkou” can be utilized. However, these methods do not work when the morpheme that we want to refer to is not a head of bunsetsu as in sentence (3). In this example, when “gaikoku” (foreign) is recognized to be BARTIFACT (the beginning of ARTIFACT), we want to refer to “hou” (law), not “ihan” (violation), which is the head of the bunsetsu. This paper proposes Japanese bottom-up named 1Bunsetsu is the smalles</context>
<context position="8429" citStr="Nakano and Hirai, 2004" startWordPosition="1315" endWordPosition="1318">ERe 0.245 (a):initial state (b):final output Figure 2: An overview of our proposed method. (the bunsetsu “Habu-Yoshiharu-Meijin”) character, POS, etc. about the morpheme and the surrounding two morphemes. The methods utilizing SVM or CRF are proposed. Most of NER methods based on sequential labeling use only local information. Therefore, methods utilizing global information are proposed. Nakano et al. utilized as a feature the word sub class of NE on the analyzing direction in the bunsetsu, the noun in the end of the bunsetsu adjacent to the analyzing direction, and the head of each bunsetsu (Nakano and Hirai, 2004). Sasano et al. utilized cache feature, coreference result, syntactic feature, and caseframe feature as structural features (Sasano and Kurohashi, 2008). Some work acquired knowledge from unannotated large corpus, and applied it to NER. Kazama et al. utilized a Named Entity dictionary constructed from Wikipedia and a noun clustering result obtained using huge amount of pairs of dependency relations (Kazama and Torisawa, 2008). Fukushima et al. acquired huge amount of category-instance pairs (e.g., “political party - New party DAICHI”,“companyTOYOTA”) by some patterns from a large Web corpus (F</context>
</contexts>
<marker>Nakano, Hirai, 2004</marker>
<rawString>Keigo Nakano and Yuzo Hirai. 2004. Japanese named entity extraction with bunsetsu features. Transaction of Information Processing Society of Japan, 45(3):934–941. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Japanese named entity recognition using structural natural language processing.</title>
<date>2008</date>
<booktitle>In Proceeding of Third International Joint Conference on Natural Language Processing,</booktitle>
<pages>607--612</pages>
<contexts>
<context position="1561" citStr="Sasano and Kurohashi, 2008" startWordPosition="234" endWordPosition="237">nducted an experimental on CRL NE data, and achieved an F measure of 89.79, which is higher than previous work. 1 Introduction Named Entity Recognition (NER) is a task of recognizing named entities such as person names, organization names, and location. It is used for several NLP applications such as Information Extraction (IE) and Question Answering (QA). Most work uses machine learning methods such as Support Vector Machines (SVMs) (Vapnik, 1995) and Conditional Random Field (CRF) (Lafferty et al., 2001) using a hand-annotated corpus (Krishnan and D.Manning, 2006; Kazama and Torisawa, 2008; Sasano and Kurohashi, 2008; Fukushima et al., 2008; Nakano and Hirai, 2004; Masayuki and Matsumoto, 2003). In general, NER is formalized as a sequential labeling problem. For example, regarding a morpheme as a basic unit, it is first labeled as SPERSON, B-PERSON, I-PERSON, E-PERSON, S-ORGANIZATION, etc. Then, considering the labeling results of morphemes, the best NE label sequence is recognized. When the label of each morpheme is estimated, only local information around the morpheme (e.g., the morpheme, the two preceding morphemes, and the two following morphemes) is utilized. Therefore, a long named entity consisting</context>
<context position="2953" citStr="Sasano and Kurohashi, 2008" startWordPosition="459" endWordPosition="462">ed to be S-PERSON (PERSON consisting of one morpheme) by utilizing the surrounding information such as the suffix “san” (Mr.) and the verb “kikoku shita” (return home). On the other hand, in sentence (2), when the label of “shinyou” (credit) is recognized to be B-ORGANIZATION (the beginning of ORGANIZATION), only information from “hatsudou” (invoke) to “kyusai” (relief) can be utilized, and thus the information of the morpheme “ginkou” (bank) that is apart from “shinyou” by three morphemes cannot be utilized. To cope with this problem, Nakano et al. (Nakano and Hirai, 2004) and Sasano et al. (Sasano and Kurohashi, 2008) utilized information of the head of bunsetsu1. In their methods, when the label of “shinyou” is recognized, the information of the morpheme “ginkou” can be utilized. However, these methods do not work when the morpheme that we want to refer to is not a head of bunsetsu as in sentence (3). In this example, when “gaikoku” (foreign) is recognized to be BARTIFACT (the beginning of ARTIFACT), we want to refer to “hou” (law), not “ihan” (violation), which is the head of the bunsetsu. This paper proposes Japanese bottom-up named 1Bunsetsu is the smallest coherent phrasal unit in Japanese. It consist</context>
<context position="8581" citStr="Sasano and Kurohashi, 2008" startWordPosition="1336" endWordPosition="1339">c. about the morpheme and the surrounding two morphemes. The methods utilizing SVM or CRF are proposed. Most of NER methods based on sequential labeling use only local information. Therefore, methods utilizing global information are proposed. Nakano et al. utilized as a feature the word sub class of NE on the analyzing direction in the bunsetsu, the noun in the end of the bunsetsu adjacent to the analyzing direction, and the head of each bunsetsu (Nakano and Hirai, 2004). Sasano et al. utilized cache feature, coreference result, syntactic feature, and caseframe feature as structural features (Sasano and Kurohashi, 2008). Some work acquired knowledge from unannotated large corpus, and applied it to NER. Kazama et al. utilized a Named Entity dictionary constructed from Wikipedia and a noun clustering result obtained using huge amount of pairs of dependency relations (Kazama and Torisawa, 2008). Fukushima et al. acquired huge amount of category-instance pairs (e.g., “political party - New party DAICHI”,“companyTOYOTA”) by some patterns from a large Web corpus (Fukushima et al., 2008). In Japanese NER researches, CRL NE data are usually utilized for the evaluation. This data includes approximately 10 thousands s</context>
<context position="13920" citStr="Sasano and Kurohashi, 2008" startWordPosition="2215" endWordPosition="2218">l”, this feature fires. 9. Wikipedia feature - If the string of the chunk has an entry in Wikipedia, this feature fires. - the hypernym extracted from its definition sentence using some patterns (e.g., The hypernym of “the Liberal Democratic Party” is a political party.) 10. cache feature - When the same string of the chunk appears in the preceding context, the label of the preceding chunk is used for the feature. 11. particles that the bunsetsu includes 12. the morphemes, particles, and head morpheme in the parent bunsetsu 13. the NE/category ratio in a case slot of predicate/noun case frame(Sasano and Kurohashi, 2008) - For example, in the case ga (NOM) of the predicate case frame “kaiken” (interview), the NE ratio “PERSON:0.245” is assigned to the case slot. Hence, in the sentence “Habu-ga kaikenshita” (Mr. Habu interviewed), the feature “PERSON:0.245” is utilized for the chunk “Habu.” 14. parenthesis feature - When the chunk in a parenthesis, this feature fires. Table 2: Features for the label estimation model. The chunk that is neither any eight NE class nor the above four OTHER is labeled as invalid. In an example as shown in Figure 3, “HabuYoshiharu” is labeled as PERSON, “Meijin” is labeled as OTHERe</context>
</contexts>
<marker>Sasano, Kurohashi, 2008</marker>
<rawString>Ryohei Sasano and Sadao Kurohashi. 2008. Japanese named entity recognition using structural natural language processing. In Proceeding of Third International Joint Conference on Natural Language Processing, pages 607–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
<author>Hiroyuki Shinnou</author>
</authors>
<title>A decision tree method for finding and classifying names in japanese texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora (WVLC-6),</booktitle>
<pages>171--178</pages>
<contexts>
<context position="6631" citStr="Sekine et al., 1998" startWordPosition="1030" endWordPosition="1033"> 1999) is usually used. In this definition, NEs are classified into eight classes: PERSON, LOCATION, ORGANIZATION, ARTIFACT, DATE, TIME, MONEY, and PERCENT. Table 1 shows example instances of each class. NER methods are divided into two approaches: rule-based approach and machine learning approach. According to previous work, machine learning approach achieved better performance than rule-based approach. In general, a machine learning method is formalized as a sequential labeling problem. This problem is first assigning each token (character or morpheme) to several labels. In an SE-algorithm (Sekine et al., 1998), S is assigned to NE composed of one morpheme, B, I, E is assigned to the beginning, middle, end of NE, respectively, and O is assigned to the morpheme that is not an NE2. The labels S, B, I, and E are prepared for each NE classes, and thus the total number of labels is 33 (= 8 * 4 + 1). The model for the label estimation is learned based on machine learning. The following features are generally utilized: characters, type of 2Besides, there are IOB1, IOB2 algorithm using only I,O,B and IOE1, IOE2 algorithm using only I,O,E (Kim and Veenstra, 1999). class example PERCENT 20 percent PERSON LOCA</context>
</contexts>
<marker>Sekine, Grishman, Shinnou, 1998</marker>
<rawString>Satoshi Sekine, Ralph Grishman, and Hiroyuki Shinnou. 1998. A decision tree method for finding and classifying names in japanese texts. In Proceedings of the Sixth Workshop on Very Large Corpora (WVLC-6), pages 171–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="1387" citStr="Vapnik, 1995" startWordPosition="211" endWordPosition="212">learning method. Then, the best label assignment in the bunsetsu is determined from bottom up as the CKY parsing algorithm using a machine learning method. We conducted an experimental on CRL NE data, and achieved an F measure of 89.79, which is higher than previous work. 1 Introduction Named Entity Recognition (NER) is a task of recognizing named entities such as person names, organization names, and location. It is used for several NLP applications such as Information Extraction (IE) and Question Answering (QA). Most work uses machine learning methods such as Support Vector Machines (SVMs) (Vapnik, 1995) and Conditional Random Field (CRF) (Lafferty et al., 2001) using a hand-annotated corpus (Krishnan and D.Manning, 2006; Kazama and Torisawa, 2008; Sasano and Kurohashi, 2008; Fukushima et al., 2008; Nakano and Hirai, 2004; Masayuki and Matsumoto, 2003). In general, NER is formalized as a sequential labeling problem. For example, regarding a morpheme as a basic unit, it is first labeled as SPERSON, B-PERSON, I-PERSON, E-PERSON, S-ORGANIZATION, etc. Then, considering the labeling results of morphemes, the best NE label sequence is recognized. When the label of each morpheme is estimated, only l</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>