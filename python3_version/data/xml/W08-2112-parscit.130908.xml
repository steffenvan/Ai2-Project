<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003129">
<title confidence="0.993858">
An Incremental Bayesian Model for Learning Syntactic Categories
</title>
<author confidence="0.997913">
Christopher Parisien, Afsaneh Fazly and Suzanne Stevenson
</author>
<affiliation confidence="0.854617666666667">
Department of Computer Science
University of Toronto
Toronto, ON, Canada
</affiliation>
<email confidence="0.998799">
[chris,afsaneh,suzanne]@cs.toronto.edu
</email>
<sectionHeader confidence="0.993901" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986166666667">
We present an incremental Bayesian model for
the unsupervised learning of syntactic cate-
gories from raw text. The model draws infor-
mation from the distributional cues of words
within an utterance, while explicitly bootstrap-
ping its development on its own partially-
learned knowledge of syntactic categories.
Testing our model on actual child-directed
data, we demonstrate that it is robust to noise,
learns reasonable categories, manages lexical
ambiguity, and in general shows learning be-
haviours similar to those observed in children.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999795">
An important open problem in cognitive science and
artificial intelligence is how children successfully
learn their native language despite the lack of explicit
training. A key challenge in the early stages of lan-
guage acquisition is to learn the notion of abstract
syntactic categories (e.g., nouns, verbs, or determin-
ers), which is necessary for acquiring the syntactic
structure of language. Indeed, children as young as
two years old show evidence of having acquired a
good knowledge of some of these abstract categories
(Olguin and Tomasello, 1993); by around six years of
age, they have learned almost all syntactic categories
(Kemp et al., 2005). Computational models help to
elucidate the kinds of learning mechanisms that may
be capable of achieving this feat. Such studies shed
light on the possible cognitive mechanisms at work
in human language acquisition, and also on potential
means for unsupervised learning of complex linguis-
tic knowledge in a computational system.
Learning the syntactic categories of words has
been suggested to be based on the morphological and
phonological properties of individual words, as well
</bodyText>
<note confidence="0.724212">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.909152">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999986853658537">
as on the distributional information about the con-
texts in which they appear. Several computational
models have been proposed that draw on one or more
of the above-mentioned properties in order to group
words into discrete unlabeled categories. Most ex-
isting models only intend to show the relevance of
such properties to the acquisition of adult-like syn-
tactic categories such as nouns and verbs; hence, they
do not necessarily incorporate the types of learning
mechanisms used by children (Sch¨utze, 1993; Red-
ington et al., 1998; Clark, 2000; Mintz, 2003; Onnis
and Christiansen, 2005). For example, in contrast to
the above models, children acquire their knowledge
of syntactic categories incrementally, processing the
utterances they hear one at a time. Moreover, chil-
dren appear to be sensitive to the fact that syntactic
categories are partially defined in terms of other cat-
egories, e.g., nouns tend to follow determiners, and
can be modified by adjectives.
We thus argue that a computational model should
be incremental, and should use more abstract cate-
gory knowledge to help better identify syntactic cat-
egories. Incremental processing also allows a model
to incorporate its partially-learned knowledge of cat-
egories, letting the model bootstrap its development.
To our knowledge, the only incremental model of
category acquisition that also incorporates bootstrap-
ping is that of Cartwright and Brent (1997). Their
template-based model, however, draws on very spe-
cific linguistic constraints and rules to learn cate-
gories. Moreover, their model has difficulty with the
variability of natural language data.
We address these shortcomings by developing an
incremental probabilistic model of syntactic category
acquisition that uses a domain-general learning algo-
rithm. The model also incorporates a bootstrapping
mechanism, and learns syntactic categories by look-
ing only at the general patterns of distributional sim-
ilarity in the input. Experiments performed on actual
(noisy) child-directed data show that an explicit boot-
strapping component improves the model’s ability to
</bodyText>
<page confidence="0.998779">
89
</page>
<note confidence="0.880934">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 89–96
Manchester, August 2008
</note>
<bodyText confidence="0.98593535">
learn adult-like categories. The model’s learning tra-
jectory resembles some relevant behaviours seen in
children, and we also show that the categories that
our model learns can be successfully used in a lexical
disambiguation task.
2 Overview of the Computational Model
We adapt a probabilistic incremental model of un-
supervised categorization (i.e., clustering) proposed
by Anderson (1991). The original model has been
used to simulate human categorization in a variety
of domains, including the acquisition of verb argu-
ment structure (Alishahi and Stevenson, 2008). Our
adaptation of the model incorporates an explicit boot-
strapping mechanism and a periodic merge of clus-
ters, both facilitating generalization over input data.
Here, we explain the input to our model (Section 2.1),
the categorization model itself (Section 2.2), how we
estimate probabilities to facilitate bootstrapping (Sec-
tion 2.3), and our approach for merging similar clus-
ters (Section 2.4).
</bodyText>
<subsectionHeader confidence="0.943256">
2.1 Input Frames
</subsectionHeader>
<bodyText confidence="0.99648915">
We aim to learn categories of words, and we do this
by looking for groups of similar word usages. Thus,
rather than categorizing a word alone, we categorize a
word token with its context from that usage. The ini-
tial input to our model is a sequence of unannotated
utterances, that is, words separated by spaces. Before
being categorized by the model, each word usage in
the input is processed to produce a frame that con-
tains the word itself (the head word of the frame) and
its distributional context (the two words before and
after it). For example, in the utterance ‘I gave Josie
a present,’ when processing the head word Josie, we
create the following frame for input to the categoriza-
tion system:
feature w−2 w−1 w0 w+1 w+2
I gave Josie a present
where w0 denotes the head word feature, and w−2,
w−1, w+1, w+2 are the context word features. A con-
text word may be ‘null’ if there are fewer than two
preceding or following words in the utterance.
</bodyText>
<subsectionHeader confidence="0.997313">
2.2 Categorization
</subsectionHeader>
<bodyText confidence="0.999883777777778">
Using Anderson’s (1991) incremental Bayesian cat-
egorization algorithm, we learn clusters of word us-
ages (i.e., the input frames) by drawing on the overall
similarity of their features (here, the head word and
the context words). The clusters themselves are not
predefined, but emerge from similarities in the input.
More formally, for each successive frame F in the
input, processed in the order of the input words, we
place F into the most likely cluster, either from the
</bodyText>
<equation confidence="0.997750333333333">
K existing clusters, or a new one:
BestCluster(F) = argmax P(k|F) (1)
k
</equation>
<bodyText confidence="0.998898333333333">
where k = 0, 1, .., K, including the new cluster
k = 0. Using Bayes’ rule, and dropping P(F) from
the denominator, which is constant for all k, we find:
</bodyText>
<equation confidence="0.995816333333333">
P(k)P(F |k)
P(k|F) = P (F ) a P(k)P(F|k) (2)
The prior probability of k, P(k), is given by:
cnk
P(k) = 1 &lt; k &lt; K (3)
(1 − c) + cn
1 − c
P(0) = (4)
(1 − c) + cn
</equation>
<bodyText confidence="0.9999190625">
where nk is the number of frames in k, and n is
the total number of frames observed at the time of
processing frame F. Intuitively, a well-entrenched
(large) cluster should be a more likely candidate for
categorization than a small one. We reserve a small
probability for creating a new cluster (Eq. 4). As the
model processes more input overall, it should become
less necessary to create new clusters to fit the data, so
P(0) decreases with large n. In our experiments, we
set c to a large value, 0.95, to further increase the
likelihood of using existing clusters.1
The probability of a frame F given a cluster k,
P(F|k), depends on the probabilities of the features
in F given k. We assume that the individual fea-
tures in a frame are conditionally independent given
k, hence:
</bodyText>
<equation confidence="0.9869395">
P(F|k) = PH(w0|k) � P(wi|k) (5)
i∈{−2,−1,+1,+2}
</equation>
<bodyText confidence="0.999926166666667">
where PH is the head word probability, i.e., the like-
lihood of seeing w0 as a head word among the frames
in cluster k. The context word probability P(wi|k) is
the likelihood of seeing wi in the ith context position
of the frames in cluster k. Next, we explain how we
estimate each of these probabilities from the input.
</bodyText>
<subsectionHeader confidence="0.998503">
2.3 Probabilities and Bootstrapping
</subsectionHeader>
<bodyText confidence="0.99978925">
For the head word probability PH(w0|k), we use a
smoothed maximum likelihood estimate (i.e., the pro-
portion of frames in cluster k with head word w0).
For the context word probability P(wi|k), we can
form two estimates. The first is a simple maximum
likelihood estimate, which enforces a preference for
creating clusters of frames with the same context
words. That is, head words in the same cluster will
</bodyText>
<footnote confidence="0.979545666666667">
1The prior P(k) is equivalent to the prior in a Dirichlet pro-
cess mixture model (Sanborn et al., 2006), commonly used for
sampling clusters of objects.
</footnote>
<page confidence="0.998803">
90
</page>
<bodyText confidence="0.997496357142857">
tend to share the same adjacent words. We call this
word-based estimate Pword.
Alternatively, we may consider the likelihood of
seeing not just the context word wi, but similar words
in that position. For example, if wi can be used as a
noun or a verb, then we want the likelihood of seeing
other nouns or verbs in position i of frames in cluster
k. Here, we use the partial knowledge of the learned
clusters. That is, we look over all existing clusters
k0, estimate the probability that wi is the head word
of frames in k0, then estimate the probability of using
the head words from those other clusters in position i
in cluster k. We refer to this category-based estimate
as Pcat:
</bodyText>
<equation confidence="0.981767">
�Pcat(wi|k) = PH(wi|k0)Pi(k0|k) (6)
k&apos;
</equation>
<bodyText confidence="0.999941090909091">
where Pi(k0|k) is the probability of finding usages
from cluster k0 in position i given cluster k. To sup-
port this we record the categorization decisions the
model has made. When we categorize the frames of
an utterance, we get a sequence of clusters for that
utterance, which gives additional information to sup-
plement the frame. We use this information to esti-
mate Pi(k0|k) for future categorizations, again using
a smoothed maximum likelihood formula.
In contrast to the Pword estimate, the estimate in
Eq. (6) prefers clusters of frames that use the same
categories as context. While some of the results of
these preferences will be the same, the latter approach
lets the model make second-order inferences about
categories. There may be no context words in com-
mon between the current frame and a potential clus-
ter, but if the context words in the cluster have been
found to be distributionally similar to those in the
frame, it may be a good cluster for that frame.
We equally weight the word-based and the
category-based estimates for P(wi|k) to get the like-
lihood of a context word; that is:
</bodyText>
<equation confidence="0.999139">
1 1
P(wi|k) &amp;quot;&apos; 2Pword(wi|k) + 2Pcat(wi|k) (7)
</equation>
<bodyText confidence="0.999988">
This way, the model sees an input utterance simulta-
neously as a sequence of words and as a sequence of
categories. It is the Pcat component, by using devel-
oping category knowledge, that yields the bootstrap-
ping abilities of our model.
</bodyText>
<subsectionHeader confidence="0.971716">
2.4 Generalization
</subsectionHeader>
<bodyText confidence="0.999857885245902">
Our model relies heavily on the similarity of word
contexts in order to find category structure. In nat-
ural language, these context features are highly vari-
able, so it is difficult to draw consistent structure from
the input in the early stages of an incremental model.
When little information is available, there is a risk of
incorrectly generalizing, leading to clustering errors
which may be difficult to overcome. Children face
a similar problem in early learning, but there is ev-
idence that they may manage the problem by using
conservative strategies (see, e.g., Tomasello, 2000).
Children may form specific hypotheses about each
word type, only later generalizing their knowledge to
similar words. Drawing on this observation, we form
early small clusters specific to the head word type,
then later aid generalization by merging these smaller
clusters. By doing this, we ensure that the model only
groups words of different types when there is suffi-
cient evidence for their contextual similarity.
Thus, when a cluster has been newly created, we
require that all frames put into the cluster share the
same head word type.2 When clusters are small, this
prevents the model from making potentially incorrect
generalizations to different words. Periodically, we
evaluate a set of reasonably-sized clusters, and merge
pairs of clusters that have highly similar contexts (see
below for details). If the model decides to merge two
clusters with different head word types—e.g., one
cluster with all instances of dog, and another with
cat—it has in effect made a decision to generalize.
Intuitively, the model has learned that the contexts
in the newly merged cluster apply to more than one
word type. We now say that any word type could be
a member of this cluster, if its context is sufficiently
similar to that of the cluster. Thus, when categoriz-
ing a new word token (represented as a frame F),
our model can choose from among the clusters with
a matching head word, and any of these ‘generalized’
clusters that contain mixed head words.
Periodically, we look through a subset of the clus-
ters to find similar pairs to merge. In order to limit
the number of potential merges to consider, we only
examine pairs of clusters in which at least one cluster
has changed since the last check. Thus, after pro-
cessing every 100 frames of input, we consider the
clusters used to hold those recent 100 frames as can-
didates to be merged with another cluster. We only
consider clusters of reasonable size (here, at least 10
frames) as candidates for merging. For each candi-
date pair of clusters, k1 and k2, we first evaluate a
heuristic merge score that determines if the pair is
appropriate to be merged, according to some local
criteria, i.e., the size and the contents of the candi-
date clusters. For each suggested merge (a pair whose
merge score exceeds a pre-determined threshold), we
then look at the set of all clusters, the global evidence,
to decide whether to accept the merge.
The merge score combines two factors: the en-
trenchment of the two clusters, and the similarity of
2However, a word type may exist in several clusters (e.g., for
distinct noun and verb usages), thus handling lexical ambiguity.
</bodyText>
<page confidence="0.99375">
91
</page>
<bodyText confidence="0.999740041666667">
their context features. The entrenchment measure
identifies clusters that contain enough frames to show
a significant trend. We take a sigmoid function over
the number of frames in the clusters, giving a soft
threshold approaching 0 for small clusters and 1 for
large clusters. The similarity measure identifies pairs
of clusters with similar distributions of word and cat-
egory contexts. Given two clusters, we measure the
symmetric Kullback-Leibler divergence for each cor-
responding pair of context feature probabilities (in-
cluding the category contexts Pi(k�|k), 8 pairs in to-
tal), then place the sum of those measures on another
sigmoid function. The merge score is the sum of the
entrenchment and similarity measures.
Since it is only a local measure, the merge score is
not sufficient on its own for determining if a merge
is appropriate. For each suggested merge, we thus
examine the likelihood of a sample of input frames
(here, the last 100 frames) under two states: the set
of clusters before the merge, and the set of clusters if
the merge is accepted. We only accept a merge if it
results in an increase in the likelihood of the sample
data. The likelihood of a sample set of frames, S,
over a set of clusters, K, is calculated as in:
</bodyText>
<equation confidence="0.992194">
P (S) = � E P(F|k)P(k) (8)
FcS kcK
</equation>
<sectionHeader confidence="0.995934" genericHeader="introduction">
3 Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.999934517857143">
To test our proposed model, we train it on a sample of
language representative of what children would hear,
and evaluate its categorization abilities. We have
multiple goals in this evaluation. First, we determine
the model’s ability to discover adult-level syntactic
categories from the input. Since this is intended to be
a cognitively plausible learning model, we also com-
pare the model’s qualitative learning behaviours with
those of children. In the first experiment (Section 4),
we compare the model’s categorization with a gold
standard of adult-level syntactic categories and exam-
ine the effect of the bootstrapping component. The
second experiment (Section 5) examines the model’s
development of three specific parts of speech. De-
velopmental evidence suggests that children acquire
different syntactic categories at different ages, so we
compare the model’s learning rates of nouns, verbs,
and adjectives. Lastly, we examine our model’s abil-
ity to handle lexically ambiguous words (Section 6).
English word forms commonly belong to more than
one syntactic category, so we show how our model
uses context to disambiguate a word’s category.
In all experiments, we train and test the model us-
ing the Manchester corpus (Theakston et al., 2001)
from the CHILDES database (MacWhinney, 2000).
The corpus contains transcripts of mothers’ conver-
sations with 12 British children between the ages of
1;8 (years;months) and 3;0. There are 34 one-hour
sessions per child over the course of a year. The age
range of the children roughly corresponds with the
ages at which children show the first evidence of syn-
tactic categories.
We extract the mothers’ speech from each of the
transcripts, then concatenate the input of all 12 chil-
dren (all of Anne’s sessions, followed by all of Aran’s
sessions, and so on). We remove all punctuation. We
spell out contractions, so that each token in the input
corresponds to only one part-of-speech (PoS) label
(noun, verb, etc.). We also remove single-word ut-
terances and utterances with a single repeated word
type, since they contain no distributional informa-
tion. We randomly split the data into development
and evaluation sets, each containing approximately
683,000 tokens. We use the development set to fine-
tune the model parameters and develop the experi-
ments, then use the evaluation set as a final test of
the model. We further split the development set into
about 672,000 tokens (about 8,000 types) for training
and 11,000 tokens (1,300 types) for validation. We
split the evaluation set comparably, into training and
test subsets. All reported results are for the evaluation
set. A conservative estimate suggests that children
are exposed to at least 1.5 million words of child-
directed speech annually (Redington et al., 1998), so
this corpus represents only a small portion of a child’s
available input.
</bodyText>
<sectionHeader confidence="0.992365" genericHeader="method">
4 Experiment 1: Adult Categories
</sectionHeader>
<subsectionHeader confidence="0.863789">
4.1 Methods
</subsectionHeader>
<bodyText confidence="0.999980684210526">
We use three separate versions of the categorization
model, in which we change the components used to
estimate the context word probability, P(wi|k) (as
used in Eq. (5), Section 2.2). In the word-based
model, we estimate the context probabilities using
only the words in the context window, by directly
using the maximum-likelihood Pword estimate. The
bootstrap model uses only the existing clusters to es-
timate the probability, directly using the Pcat esti-
mate from Eq. (6). The combination model uses an
equally-weighted combination of the two probabili-
ties, as presented in Eq. (7).
We run the model on the training set, categoriz-
ing each of the resulting frames in order. After every
10,000 words of input, we evaluate the model’s cate-
gorization performance on the test set. We categorize
each of the frames of the test set as usual, treating the
text as regular input. So that the test set remains un-
seen, the model does not record these categorizations.
</bodyText>
<subsectionHeader confidence="0.794605">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.9998485">
The PoS tags in the Manchester corpus are too fine-
grained for our evaluation, so for our gold standard
</bodyText>
<page confidence="0.980505">
92
</page>
<bodyText confidence="0.99998275">
we map them to the following 11 tags: noun, verb,
auxiliary, adjective, adverb, determiner, conjunction,
negation, preposition, infinitive to, and ‘other.’ When
we evaluate the model’s categorization performance,
we have two different sets of clusters of the words in
the test set: one set resulting from the gold standard,
and another as a result of the model’s categorization.
We compare these two clusterings, using the adjusted
Rand index (Hubert and Arabie, 1985), which mea-
sures the overall agreement between two clusterings
of a set of data points. The measure is ‘corrected for
chance,’ so that a random grouping has an expected
score of zero. This measure tends to be very con-
servative, giving values much lower than an intuitive
percentage score. However, it offers a useful relative
comparison of overall cluster similarity.
</bodyText>
<subsectionHeader confidence="0.473738">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.888255617647059">
Figure 1 gives the adjusted Rand scores of the three
model variants, word-based, bootstrap, and combi-
nation. Higher values indicate a better fit with the
gold-standard categorization scheme. The adjusted
Rand score is corrected for chance, thus providing a
built-in baseline measure. Since the expected score
for a random clustering is zero, all three model vari-
ants operate at above-baseline performance.
As seen in Figure 1, the word-based model gains
an early advantage in the comparison, but its per-
formance approaches a plateau at around 200,000
words of input. This suggests that while simple
word distributions provide a reliable source of infor-
mation early in the model’s development, the infor-
mation is not sufficient to sustain long-term learn-
ing. The bootstrap model learns much more slowly,
which is unsurprising, given that it depends on hav-
ing some reasonable category knowledge in order to
develop its clusters—leading to a chicken-and-egg
problem. However, once started, its performance im-
proves well beyond the word-based model’s plateau.
These results suggest that on its own, each compo-
nent of the model may be effectively throwing away
useful information. By combining the two models,
the combination model appears to gain complemen-
tary benefits from each component, outperforming
both. The word-based component helps to create a
base of reliable clusters, which the bootstrap compo-
nent uses to continue development.
After all of the training text, the combination
model uses 411 clusters to categorize the test tokens
(compared to over 2,000 at the first test point). While
this seems excessive, we note that 92.5% of the test
tokens are placed in the 25 most populated clusters.3
</bodyText>
<footnote confidence="0.995068">
3See www.cs.toronto.edu/˜chris/syncat for examples.
</footnote>
<figureCaption confidence="0.997149666666667">
Figure 1: Adjusted Rand Index of each of three mod-
els’ clusterings of the test set, as compared with the
PoS tags of the test data.
</figureCaption>
<sectionHeader confidence="0.9709" genericHeader="method">
5 Experiment 2: Learning Trends
</sectionHeader>
<bodyText confidence="0.999992764705882">
A common trend observed in children is that differ-
ent syntactic categories are learned at different rates.
Children appear to have learned the category of nouns
by 23 months of age, verbs shortly thereafter, and
adjectives relatively late (Kemp et al., 2005). Our
goal in this experiment is to look for these specific
trends in the behaviour of our model. We thus simu-
late an experiment where a child uses a novel word’s
linguistic context to infer its syntactic category (e.g.,
Tomasello et al., 1997). For our experiment, we ran-
domly generate input frames with novel head words
using contexts associated with nouns, verbs, and ad-
jectives, then examine the model’s categorization in
each case. We expect that our model should approxi-
mate the developmental trends of children, who tend
to learn the category of ‘noun’ before ‘verb,’ and both
of these before ‘adjective.’
</bodyText>
<subsectionHeader confidence="0.933167">
5.1 Methods
</subsectionHeader>
<bodyText confidence="0.999978555555556">
We generate new input frames using the most com-
mon syntactic patterns in the training data. For each
of the noun, verb, and adjective categories (from the
gold standard), we collect the five most frequent PoS
sequences in which these are used, bounded by the
usual four-word context window. For example, the
Adjective set includes the sequence ‘V Det Adj N
null’, where the sentence ends after the N. For each
of the three categories, we generate each of 500 input
frames by sampling one of the five PoS sequences,
weighted by frequency, then sampling words of the
right PoS from the lexicon, also weighted by fre-
quency. We replace the head word with a novel word,
forcing the model to use only the context for cluster-
ing. Since the context words are chosen at random,
most of the word sequences generated will be novel.
This makes the task more difficult, rather than sim-
ply sampling utterances from the corpus, where rep-
</bodyText>
<figure confidence="0.999695454545455">
0 1 2 3 4 5 6
Training set size (words) x 105
Radj
0.15
0.05
0.2
0.1
0
Combination
Word−based
Bootstrap
</figure>
<page confidence="0.99636">
93
</page>
<bodyText confidence="0.999974931034483">
etitions are common. While a few of the sequences
may exist in the training data, we expect the model
to mostly use the underlying category information to
cluster the frames.
We intend to show that the model uses context to
find the right category for a novel word. To evaluate
the model’s behaviour, we let it categorize each of
the randomly generated frames. We score each frame
as follows: if the frame gets put into a new cluster,
it earns score zero. Otherwise, its score is the pro-
portion of frames in the chosen cluster matching the
correct part of speech (we use a PoS-tagged version
of the training corpus; for example, a noun frame put
into a cluster with 60% nouns would get 0.6). We re-
port the mean score for each of the noun, verb, and
adjective sets. Intuitively, the matching score indi-
cates how well the model recognizes that the given
contexts are similar to input it has seen before. If the
model clusters the novel word frame with others of
the right type, then it has formed a category for the
contextual information in that frame.
We use the full combination model (Eq. (7)) to
evaluate the learning rates of individual parts of
speech. We run the model on the training subset of
the evaluation corpus. After every 10,000 words of
input, we use the model to categorize the 1,500 con-
text frames with novel words (500 frames each for
noun, verb, and adjective). As in experiment 1, the
model does not record these categorizations.
</bodyText>
<subsectionHeader confidence="0.9108">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.99954132">
Figure 2 shows the mean matching scores for each
of the tested parts of speech. Recall that since the
frames each use a novel head word, a higher match-
ing score indicates that the model has learned to cor-
rectly recognize the contexts in the frames. This does
not necessarily mean that the model has learned sin-
gle, complete categories of ‘noun,’ ‘verb,’ and ‘ad-
jective,’ but it does show that when the head word
gives no information, the model can generalize based
on the contextual patterns alone. The model learns
to categorize novel nouns better than verbs until late
in training, which matches the trends seen in children.
Adjectives progress slowly, and show nearly no learn-
ing ability by the end of the trial. Again, this appears
to reflect natural behaviour in children, although the
effect we see here may simply be a result of the over-
all frequency of the PoS types. Over the entire corpus
(development and evaluation), 35.4% of the word to-
kens are nouns and 24.3% are verbs, but only 2.9%
are tagged as adjectives. The model, and similarly a
child, may need much more data to learn adjectives
than is available at this stage.
The scores in Figure 2 tend to fluctuate, partic-
ularly for the noun contexts. This fluctuation cor-
responds to periods of overgeneralization, followed
</bodyText>
<figureCaption confidence="0.9274115">
Figure 2: Comparative learning trends of noun, verb,
and adjective patterns.
</figureCaption>
<bodyText confidence="0.999935285714286">
by recovery (also observed in children; see, e.g.,
Tomasello, 2000). When the model merges two clus-
ters, the contents of the resulting cluster can initially
be quite heterogeneous. Furthermore, the new cluster
is much larger, so it becomes a magnet for new cate-
gorizations. This results in overgeneralization errors,
giving the periodic drops seen in Figure 2. While our
formulation in Section 2.4 aims to prevent such er-
rors, they are likely to occur on occasion. Eventually,
the model recovers from these errors, and it is worth
noting that the fluctuations diminish over time. As the
model gradually improves with more input, the dom-
inant clusters become heavily entrenched, and incon-
sistent merges are less likely to occur.
</bodyText>
<sectionHeader confidence="0.967741" genericHeader="method">
6 Experiment 3: Disambiguation
</sectionHeader>
<bodyText confidence="0.9999145">
The category structure of our model allows a single
word type to be a member of multiple categories. For
example, kiss could belong to a category of predom-
inantly noun usages (Can I have a kiss?) and also
to a category of verb usages (Kiss me!). As a result,
the model easily represents lexical ambiguity. In this
experiment, inspired by disambiguation work in psy-
cholinguistics (see, e.g., MacDonald, 1993), we ex-
amine the model’s ability to correctly disambiguate
category memberships.
</bodyText>
<subsectionHeader confidence="0.995426">
6.1 Methods
</subsectionHeader>
<bodyText confidence="0.999976833333334">
Given a word that the model has previously seen as
various different parts of speech, we examine how
well the model can use that ambiguous word’s con-
text to determine its category in the current usage.
For example, by presenting the word kiss in sepa-
rate noun and verb contexts, we expect that the model
should categorize kiss as a noun, then as a verb, re-
spectively. We also wish to examine the effect of the
target word’s lexical bias, that is, the predominance of
a word type to be used as one category over another.
As with adults, if kiss is mainly used as a noun, we
expect the model to more accurately categorize the
</bodyText>
<figure confidence="0.9723682">
0 1 2 3 4 5 6
Training set size (words) x 105
Matching score
0.25
0.15
0.05
0.2
0.1
0
Nouns
Verbs
Adjectives
94
P
0
Context:
PoS proportion in chosen clusters
0.4
Nouns
Verbs
N V N V N V N V N V N V
0.3
0.2
0.1
Word bias: Noun only Noun biased Equibiased Verb biased Verb only Novel word
</figure>
<figureCaption confidence="0.991278">
Figure 3: Syntactic category disambiguation. Shown are the proportions of nouns and verbs in the chosen
clusters for ambiguous words used in either noun (N) or verb (V) contexts.
</figureCaption>
<bodyText confidence="0.989912914285714">
word in a noun context than in a verb context.
We focus on noun/verb ambiguities. We artificially
generate input frames for noun and verb contexts as
in experiment 2, with the following exceptions. To
make the most use of the context information, we al-
low no null words in the input frames. We also ensure
that the contexts are distinctive enough to guide dis-
ambiguation. For each PoS sequence surrounding a
noun (e.g., ‘V Det head Prep Det’), we ensure that
over 80% of the instances of that pattern in the cor-
pus are for nouns, and likewise for verbs.
We test the model’s disambiguation in six con-
ditions, with varying degrees of lexical bias. Un-
ambiguous (‘noun/verb only’) conditions test words
seen in the corpus only as nouns or verbs (10 words
each). ‘Biased’ conditions test words with a clear
bias (15 with average 93% noun bias; 15 with aver-
age 84% verb bias). An ‘equibiased’ condition uses 4
words of approximately equal bias, and a novel word
condition provides an unbiased case.
For the six sets of test words, we measure the ef-
fect of placing each of these words in both noun and
verb contexts. That is, each word in each condition
was used as the head word in each of the 500 noun
and 500 verb disambiguating frames. For example,
we create 500 frames where book is used as a noun,
and 500 frames where it is used as a verb. We then
use the fully-trained ‘combination’ model (Eq. (7)) to
categorize each frame. Unlike in the previous experi-
ment, we do not let the model create new clusters. For
each frame, we choose the best-fitting existing clus-
ter, then examine that cluster’s contents. As in ex-
periment 2, we measure the proportions of each PoS
of the frames in this cluster. We then average these
measures over all tested frames in each condition.
</bodyText>
<sectionHeader confidence="0.525207" genericHeader="evaluation">
6.2 Results
</sectionHeader>
<bodyText confidence="0.999817045454546">
Figure 3 presents the measured PoS proportions for
each of the six conditions. For both the equibias and
novel word conditions, we see that the clusters cho-
sen for the noun context frames (labeled N) contain
more nouns than verbs, and the clusters chosen for
the verb context frames (V) contain more verbs than
nouns. This suggests that although the model’s past
experience with the head word is not sufficiently in-
formative, the model can use the word’s context to
disambiguate its category. In the ‘unambiguous’ and
the ‘biased’ conditions, the head words’ lexical biases
are too strong for the model to overcome.
However, the results show a realistic effect of the
lexical bias. Note the contrasts from the ‘noun only’
condition, to the ‘noun biased’ condition, to ‘equibi-
ased’ (and likewise for the verb biases). As the lex-
ical bias weakens, the counter-bias contexts (e.g., a
noun bias with a verb context) show a stronger ef-
fect on the chosen clusters. This is a realistic effect
of disambiguation seen in adults (MacDonald, 1993).
Strongly biased words are more difficult to categorize
in conflict with their bias than weakly biased words.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999576647058823">
Several existing computational models use distribu-
tional cues to find syntactic categories. Sch¨utze
(1993) employs co-occurrence statistics for common
words, while Redington et al. (1998) build word dis-
tributional profiles using corpus bigram counts. Clark
(2000) also builds distributional profiles, introducing
an iterative clustering method to better handle am-
biguity and rare words. Mintz (2003) shows that
even very simple three-word templates can effec-
tively define syntactic categories. Each of these mod-
els demonstrates that by using the kinds of simple in-
formation to which children are known to be sensi-
tive, syntactic categories are learnable. However, the
specific learning mechanisms they use, such as the
hierarchical clustering methods of Redington et al.
(1998), are not intended to be cognitively plausible.
In contrast, Cartwright and Brent (1997) propose
</bodyText>
<page confidence="0.99614">
95
</page>
<bodyText confidence="0.999972">
an incremental model of syntactic category acquisi-
tion that uses a series of linguistic preferences to find
common patterns across sentence-length templates.
Their model presents an important incremental al-
gorithm which is very effective for discovering cat-
egories in artificial languages. However, the model’s
reliance on templates limits its applicability to tran-
scripts of actual spoken language data, which contain
high variability and noise.
Recent models that apply Bayesian approaches
to PoS tagging are not incremental and assume a
fixed number of tags (Goldwater and Griffiths, 2007;
Toutanova and Johnson, 2008). In syntactic cate-
gory acquisition, the true number of categories is un-
known, and must be inferred from the input.
</bodyText>
<sectionHeader confidence="0.997898" genericHeader="conclusions">
8 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999994609756098">
We have developed a computational model of syn-
tactic category acquisition in children, and demon-
strated its behaviour on a corpus of naturalistic child-
directed data. The model is based on domain-general
properties of feature similarity, in contrast to earlier,
more linguistically-specific methods. The incremen-
tal nature of the algorithm contributes to a substantial
improvement in psychological plausibility over pre-
vious models of syntactic category learning. Further-
more, due to its probabilistic framework, our model
is robust to noise and variability in natural language.
Our model successfully uses a syntactic bootstrap-
ping mechanism to build on the distributional proper-
ties of words. Using its existing partial knowledge
of categories, the model applies a second level of
analysis to learn patterns in the input. By making
few assumptions about prior linguistic knowledge,
the model develops realistic syntactic categories from
the input data alone. The explicit bootstrapping com-
ponent improves the model’s ability to learn adult cat-
egories, and its learning trajectory resembles relevant
behaviours seen in children. Using the contextual
patterns of individual parts of speech, we show dif-
ferential learning rates across nouns, verbs, and ad-
jectives that mimic child development. We also show
an effect of a lexical bias in category disambiguation.
The algorithm is currently only implemented as an
incremental process. However, comparison with a
batch version of the algorithm, such as by using a
Gibbs sampler (Sanborn et al., 2006), would help us
further understand the effect of incrementality on lan-
guage fidelity.
While we have only examined the effects of learn-
ing categories from simple distributional information,
the feature-based framework of our model could eas-
ily be extended to include other sources of informa-
tion, such as morphological and phonological cues.
Furthermore, it would also be possible to include se-
mantic features, thereby allowing the model to draw
on correlations between semantic and syntactic cate-
gories in learning.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997015">
We thank Afra Alishahi for valuable discussions,
and the anonymous reviewers for their comments.
We gratefully acknowledge the financial support of
NSERC of Canada and the University of Toronto.
</bodyText>
<sectionHeader confidence="0.999185" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999929843137255">
Alishahi, A. and S. Stevenson 2008. A computational
model for early argument structure acquisition. Cog-
nitive Science, 32(5).
Anderson, J. R. 1991. The adaptive nature of human cate-
gorization. Psychological Review, 98(3):409–429.
Cartwright, T. A. and M. R. Brent 1997. Syntactic catego-
rization in early language acquisition: formalizing the
role of distributional analysis. Cognition, 63:121–170.
Clark, A. 2000. Inducing syntactic categories by context
distribution clustering. In CoNLL2000, pp. 91–94.
Goldwater, S. and T. L. Griffiths 2007. A fully bayesian
approach to unsupervised part-of-speech tagging. In
Proc. ofACL2007, pp. 744–751.
Hubert, L. and P. Arabie 1985. Comparing partitions.
Journal of Classification, 2:193–218.
Kemp, N., E. Lieven, and M. Tomasello 2005. Young chil-
dren’s knowledge of the “determiner” and “adjective”
categories. J. Speech Lang. Hear. R., 48:592–609.
MacDonald, M. C. 1993. The interaction of lexical and
syntactic ambiguity. J. Mem. Lang., 32:692–715.
MacWhinney, B. 2000. The CHILDES Project: Tools for
analyzing talk, volume 2: The Database. Lawrence Erl-
baum, Mahwah, NJ, 3 edition.
Mintz, T. H. 2003. Frequent frames as a cue for gram-
matical categories in child directed speech. Cognition,
90:91–117.
Olguin, R. and M. Tomasello 1993. Twenty-five-month-
old children do not have a grammatical category of verb.
Cognitive Development, 8:245–272.
Onnis, L. and M. H. Christiansen 2005. New beginnings
and happy endings: psychological plausibility in com-
putational models of language acquisition. CogSci2005.
Redington, M., N. Chater, and S. Finch 1998. Distribu-
tional information: A powerful cue for acquiring syn-
tactic categories. Cognitive Science, 22(4):425–469.
Sanborn, A. N., T. L. Griffiths, and D. J. Navarro 2006. A
more rational model of categorization. CogSci2006.
Sch¨utze, H. 1993. Part of speech induction from scratch.
In Proc. ofACL1993, pp. 251–258.
Theakston, A. L., E. V. Lieven, J. M. Pine, and C. F. Row-
land 2001. The role of performance limitations in the
acquisition of verb-argument structure: an alternative
account. J. Child Lang., 28:127–152.
Tomasello, M. 2000. Do young children have adult syn-
tactic competence? Cognition, 74:209–253.
Tomasello, M., N. Akhtar, K. Dodson, and L. Rekau 1997.
Differential productivity in young children’s use of
nouns and verbs. J. Child Lang., 24:373–387.
Toutanova, K. and M. Johnson 2008. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In NIPS2008.
</reference>
<page confidence="0.998461">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.813509">
<title confidence="0.999953">An Incremental Bayesian Model for Learning Syntactic Categories</title>
<author confidence="0.998255">Christopher Parisien</author>
<author confidence="0.998255">Afsaneh Fazly</author>
<author confidence="0.998255">Suzanne</author>
<affiliation confidence="0.999882">Department of Computer University of</affiliation>
<address confidence="0.841165">Toronto, ON,</address>
<email confidence="0.99972">[chris,afsaneh,suzanne]@cs.toronto.edu</email>
<abstract confidence="0.997626461538462">We present an incremental Bayesian model for the unsupervised learning of syntactic categories from raw text. The model draws information from the distributional cues of words within an utterance, while explicitly bootstrapping its development on its own partiallylearned knowledge of syntactic categories. Testing our model on actual child-directed data, we demonstrate that it is robust to noise, learns reasonable categories, manages lexical ambiguity, and in general shows learning behaviours similar to those observed in children.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Alishahi</author>
<author>S Stevenson</author>
</authors>
<title>A computational model for early argument structure acquisition.</title>
<date>2008</date>
<journal>Cognitive Science,</journal>
<volume>32</volume>
<issue>5</issue>
<contexts>
<context position="4888" citStr="Alishahi and Stevenson, 2008" startWordPosition="714" endWordPosition="717">Natural Language Learning, pages 89–96 Manchester, August 2008 learn adult-like categories. The model’s learning trajectory resembles some relevant behaviours seen in children, and we also show that the categories that our model learns can be successfully used in a lexical disambiguation task. 2 Overview of the Computational Model We adapt a probabilistic incremental model of unsupervised categorization (i.e., clustering) proposed by Anderson (1991). The original model has been used to simulate human categorization in a variety of domains, including the acquisition of verb argument structure (Alishahi and Stevenson, 2008). Our adaptation of the model incorporates an explicit bootstrapping mechanism and a periodic merge of clusters, both facilitating generalization over input data. Here, we explain the input to our model (Section 2.1), the categorization model itself (Section 2.2), how we estimate probabilities to facilitate bootstrapping (Section 2.3), and our approach for merging similar clusters (Section 2.4). 2.1 Input Frames We aim to learn categories of words, and we do this by looking for groups of similar word usages. Thus, rather than categorizing a word alone, we categorize a word token with its conte</context>
</contexts>
<marker>Alishahi, Stevenson, 2008</marker>
<rawString>Alishahi, A. and S. Stevenson 2008. A computational model for early argument structure acquisition. Cognitive Science, 32(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Anderson</author>
</authors>
<title>The adaptive nature of human categorization.</title>
<date>1991</date>
<journal>Psychological Review,</journal>
<volume>98</volume>
<issue>3</issue>
<contexts>
<context position="4712" citStr="Anderson (1991)" startWordPosition="689" endWordPosition="690">ld-directed data show that an explicit bootstrapping component improves the model’s ability to 89 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 89–96 Manchester, August 2008 learn adult-like categories. The model’s learning trajectory resembles some relevant behaviours seen in children, and we also show that the categories that our model learns can be successfully used in a lexical disambiguation task. 2 Overview of the Computational Model We adapt a probabilistic incremental model of unsupervised categorization (i.e., clustering) proposed by Anderson (1991). The original model has been used to simulate human categorization in a variety of domains, including the acquisition of verb argument structure (Alishahi and Stevenson, 2008). Our adaptation of the model incorporates an explicit bootstrapping mechanism and a periodic merge of clusters, both facilitating generalization over input data. Here, we explain the input to our model (Section 2.1), the categorization model itself (Section 2.2), how we estimate probabilities to facilitate bootstrapping (Section 2.3), and our approach for merging similar clusters (Section 2.4). 2.1 Input Frames We aim t</context>
</contexts>
<marker>Anderson, 1991</marker>
<rawString>Anderson, J. R. 1991. The adaptive nature of human categorization. Psychological Review, 98(3):409–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A Cartwright</author>
<author>M R Brent</author>
</authors>
<title>Syntactic categorization in early language acquisition: formalizing the role of distributional analysis.</title>
<date>1997</date>
<journal>Cognition,</journal>
<pages>63--121</pages>
<contexts>
<context position="3524" citStr="Cartwright and Brent (1997)" startWordPosition="517" endWordPosition="520">sensitive to the fact that syntactic categories are partially defined in terms of other categories, e.g., nouns tend to follow determiners, and can be modified by adjectives. We thus argue that a computational model should be incremental, and should use more abstract category knowledge to help better identify syntactic categories. Incremental processing also allows a model to incorporate its partially-learned knowledge of categories, letting the model bootstrap its development. To our knowledge, the only incremental model of category acquisition that also incorporates bootstrapping is that of Cartwright and Brent (1997). Their template-based model, however, draws on very specific linguistic constraints and rules to learn categories. Moreover, their model has difficulty with the variability of natural language data. We address these shortcomings by developing an incremental probabilistic model of syntactic category acquisition that uses a domain-general learning algorithm. The model also incorporates a bootstrapping mechanism, and learns syntactic categories by looking only at the general patterns of distributional similarity in the input. Experiments performed on actual (noisy) child-directed data show that </context>
<context position="33140" citStr="Cartwright and Brent (1997)" startWordPosition="5509" endWordPosition="5512">gram counts. Clark (2000) also builds distributional profiles, introducing an iterative clustering method to better handle ambiguity and rare words. Mintz (2003) shows that even very simple three-word templates can effectively define syntactic categories. Each of these models demonstrates that by using the kinds of simple information to which children are known to be sensitive, syntactic categories are learnable. However, the specific learning mechanisms they use, such as the hierarchical clustering methods of Redington et al. (1998), are not intended to be cognitively plausible. In contrast, Cartwright and Brent (1997) propose 95 an incremental model of syntactic category acquisition that uses a series of linguistic preferences to find common patterns across sentence-length templates. Their model presents an important incremental algorithm which is very effective for discovering categories in artificial languages. However, the model’s reliance on templates limits its applicability to transcripts of actual spoken language data, which contain high variability and noise. Recent models that apply Bayesian approaches to PoS tagging are not incremental and assume a fixed number of tags (Goldwater and Griffiths, 2</context>
</contexts>
<marker>Cartwright, Brent, 1997</marker>
<rawString>Cartwright, T. A. and M. R. Brent 1997. Syntactic categorization in early language acquisition: formalizing the role of distributional analysis. Cognition, 63:121–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Inducing syntactic categories by context distribution clustering.</title>
<date>2000</date>
<booktitle>In CoNLL2000,</booktitle>
<pages>91--94</pages>
<contexts>
<context position="2650" citStr="Clark, 2000" startWordPosition="387" endWordPosition="388">/creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. as on the distributional information about the contexts in which they appear. Several computational models have been proposed that draw on one or more of the above-mentioned properties in order to group words into discrete unlabeled categories. Most existing models only intend to show the relevance of such properties to the acquisition of adult-like syntactic categories such as nouns and verbs; hence, they do not necessarily incorporate the types of learning mechanisms used by children (Sch¨utze, 1993; Redington et al., 1998; Clark, 2000; Mintz, 2003; Onnis and Christiansen, 2005). For example, in contrast to the above models, children acquire their knowledge of syntactic categories incrementally, processing the utterances they hear one at a time. Moreover, children appear to be sensitive to the fact that syntactic categories are partially defined in terms of other categories, e.g., nouns tend to follow determiners, and can be modified by adjectives. We thus argue that a computational model should be incremental, and should use more abstract category knowledge to help better identify syntactic categories. Incremental processi</context>
<context position="32538" citStr="Clark (2000)" startWordPosition="5420" endWordPosition="5421">). As the lexical bias weakens, the counter-bias contexts (e.g., a noun bias with a verb context) show a stronger effect on the chosen clusters. This is a realistic effect of disambiguation seen in adults (MacDonald, 1993). Strongly biased words are more difficult to categorize in conflict with their bias than weakly biased words. 7 Related Work Several existing computational models use distributional cues to find syntactic categories. Sch¨utze (1993) employs co-occurrence statistics for common words, while Redington et al. (1998) build word distributional profiles using corpus bigram counts. Clark (2000) also builds distributional profiles, introducing an iterative clustering method to better handle ambiguity and rare words. Mintz (2003) shows that even very simple three-word templates can effectively define syntactic categories. Each of these models demonstrates that by using the kinds of simple information to which children are known to be sensitive, syntactic categories are learnable. However, the specific learning mechanisms they use, such as the hierarchical clustering methods of Redington et al. (1998), are not intended to be cognitively plausible. In contrast, Cartwright and Brent (199</context>
</contexts>
<marker>Clark, 2000</marker>
<rawString>Clark, A. 2000. Inducing syntactic categories by context distribution clustering. In CoNLL2000, pp. 91–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T L Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proc. ofACL2007,</booktitle>
<pages>744--751</pages>
<contexts>
<context position="33743" citStr="Goldwater and Griffiths, 2007" startWordPosition="5598" endWordPosition="5601">artwright and Brent (1997) propose 95 an incremental model of syntactic category acquisition that uses a series of linguistic preferences to find common patterns across sentence-length templates. Their model presents an important incremental algorithm which is very effective for discovering categories in artificial languages. However, the model’s reliance on templates limits its applicability to transcripts of actual spoken language data, which contain high variability and noise. Recent models that apply Bayesian approaches to PoS tagging are not incremental and assume a fixed number of tags (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008). In syntactic category acquisition, the true number of categories is unknown, and must be inferred from the input. 8 Conclusions and Future Directions We have developed a computational model of syntactic category acquisition in children, and demonstrated its behaviour on a corpus of naturalistic childdirected data. The model is based on domain-general properties of feature similarity, in contrast to earlier, more linguistically-specific methods. The incremental nature of the algorithm contributes to a substantial improvement in psychological plausibility over pre</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Goldwater, S. and T. L. Griffiths 2007. A fully bayesian approach to unsupervised part-of-speech tagging. In Proc. ofACL2007, pp. 744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hubert</author>
<author>P Arabie</author>
</authors>
<title>Comparing partitions.</title>
<date>1985</date>
<journal>Journal of Classification,</journal>
<pages>2--193</pages>
<contexts>
<context position="19947" citStr="Hubert and Arabie, 1985" startWordPosition="3264" endWordPosition="3267"> not record these categorizations. 4.2 Evaluation The PoS tags in the Manchester corpus are too finegrained for our evaluation, so for our gold standard 92 we map them to the following 11 tags: noun, verb, auxiliary, adjective, adverb, determiner, conjunction, negation, preposition, infinitive to, and ‘other.’ When we evaluate the model’s categorization performance, we have two different sets of clusters of the words in the test set: one set resulting from the gold standard, and another as a result of the model’s categorization. We compare these two clusterings, using the adjusted Rand index (Hubert and Arabie, 1985), which measures the overall agreement between two clusterings of a set of data points. The measure is ‘corrected for chance,’ so that a random grouping has an expected score of zero. This measure tends to be very conservative, giving values much lower than an intuitive percentage score. However, it offers a useful relative comparison of overall cluster similarity. 4.3 Results Figure 1 gives the adjusted Rand scores of the three model variants, word-based, bootstrap, and combination. Higher values indicate a better fit with the gold-standard categorization scheme. The adjusted Rand score is co</context>
</contexts>
<marker>Hubert, Arabie, 1985</marker>
<rawString>Hubert, L. and P. Arabie 1985. Comparing partitions. Journal of Classification, 2:193–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kemp</author>
<author>E Lieven</author>
<author>M Tomasello</author>
</authors>
<title>Young children’s knowledge of the “determiner” and “adjective”</title>
<date>2005</date>
<pages>48--592</pages>
<contexts>
<context position="1446" citStr="Kemp et al., 2005" startWordPosition="208" endWordPosition="211">tive science and artificial intelligence is how children successfully learn their native language despite the lack of explicit training. A key challenge in the early stages of language acquisition is to learn the notion of abstract syntactic categories (e.g., nouns, verbs, or determiners), which is necessary for acquiring the syntactic structure of language. Indeed, children as young as two years old show evidence of having acquired a good knowledge of some of these abstract categories (Olguin and Tomasello, 1993); by around six years of age, they have learned almost all syntactic categories (Kemp et al., 2005). Computational models help to elucidate the kinds of learning mechanisms that may be capable of achieving this feat. Such studies shed light on the possible cognitive mechanisms at work in human language acquisition, and also on potential means for unsupervised learning of complex linguistic knowledge in a computational system. Learning the syntactic categories of words has been suggested to be based on the morphological and phonological properties of individual words, as well © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creati</context>
<context position="22503" citStr="Kemp et al., 2005" startWordPosition="3671" endWordPosition="3674">,000 at the first test point). While this seems excessive, we note that 92.5% of the test tokens are placed in the 25 most populated clusters.3 3See www.cs.toronto.edu/˜chris/syncat for examples. Figure 1: Adjusted Rand Index of each of three models’ clusterings of the test set, as compared with the PoS tags of the test data. 5 Experiment 2: Learning Trends A common trend observed in children is that different syntactic categories are learned at different rates. Children appear to have learned the category of nouns by 23 months of age, verbs shortly thereafter, and adjectives relatively late (Kemp et al., 2005). Our goal in this experiment is to look for these specific trends in the behaviour of our model. We thus simulate an experiment where a child uses a novel word’s linguistic context to infer its syntactic category (e.g., Tomasello et al., 1997). For our experiment, we randomly generate input frames with novel head words using contexts associated with nouns, verbs, and adjectives, then examine the model’s categorization in each case. We expect that our model should approximate the developmental trends of children, who tend to learn the category of ‘noun’ before ‘verb,’ and both of these before </context>
</contexts>
<marker>Kemp, Lieven, Tomasello, 2005</marker>
<rawString>Kemp, N., E. Lieven, and M. Tomasello 2005. Young children’s knowledge of the “determiner” and “adjective” categories. J. Speech Lang. Hear. R., 48:592–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C MacDonald</author>
</authors>
<title>The interaction of lexical and syntactic ambiguity.</title>
<date>1993</date>
<journal>J. Mem. Lang.,</journal>
<pages>32--692</pages>
<contexts>
<context position="28148" citStr="MacDonald, 1993" startWordPosition="4648" endWordPosition="4649">ctuations diminish over time. As the model gradually improves with more input, the dominant clusters become heavily entrenched, and inconsistent merges are less likely to occur. 6 Experiment 3: Disambiguation The category structure of our model allows a single word type to be a member of multiple categories. For example, kiss could belong to a category of predominantly noun usages (Can I have a kiss?) and also to a category of verb usages (Kiss me!). As a result, the model easily represents lexical ambiguity. In this experiment, inspired by disambiguation work in psycholinguistics (see, e.g., MacDonald, 1993), we examine the model’s ability to correctly disambiguate category memberships. 6.1 Methods Given a word that the model has previously seen as various different parts of speech, we examine how well the model can use that ambiguous word’s context to determine its category in the current usage. For example, by presenting the word kiss in separate noun and verb contexts, we expect that the model should categorize kiss as a noun, then as a verb, respectively. We also wish to examine the effect of the target word’s lexical bias, that is, the predominance of a word type to be used as one category o</context>
<context position="32148" citStr="MacDonald, 1993" startWordPosition="5364" endWordPosition="5365">tive, the model can use the word’s context to disambiguate its category. In the ‘unambiguous’ and the ‘biased’ conditions, the head words’ lexical biases are too strong for the model to overcome. However, the results show a realistic effect of the lexical bias. Note the contrasts from the ‘noun only’ condition, to the ‘noun biased’ condition, to ‘equibiased’ (and likewise for the verb biases). As the lexical bias weakens, the counter-bias contexts (e.g., a noun bias with a verb context) show a stronger effect on the chosen clusters. This is a realistic effect of disambiguation seen in adults (MacDonald, 1993). Strongly biased words are more difficult to categorize in conflict with their bias than weakly biased words. 7 Related Work Several existing computational models use distributional cues to find syntactic categories. Sch¨utze (1993) employs co-occurrence statistics for common words, while Redington et al. (1998) build word distributional profiles using corpus bigram counts. Clark (2000) also builds distributional profiles, introducing an iterative clustering method to better handle ambiguity and rare words. Mintz (2003) shows that even very simple three-word templates can effectively define s</context>
</contexts>
<marker>MacDonald, 1993</marker>
<rawString>MacDonald, M. C. 1993. The interaction of lexical and syntactic ambiguity. J. Mem. Lang., 32:692–715.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for analyzing talk, volume 2: The Database. Lawrence Erlbaum,</title>
<date>2000</date>
<volume>3</volume>
<pages>edition.</pages>
<location>Mahwah, NJ,</location>
<contexts>
<context position="16799" citStr="MacWhinney, 2000" startWordPosition="2752" endWordPosition="2753">s the model’s development of three specific parts of speech. Developmental evidence suggests that children acquire different syntactic categories at different ages, so we compare the model’s learning rates of nouns, verbs, and adjectives. Lastly, we examine our model’s ability to handle lexically ambiguous words (Section 6). English word forms commonly belong to more than one syntactic category, so we show how our model uses context to disambiguate a word’s category. In all experiments, we train and test the model using the Manchester corpus (Theakston et al., 2001) from the CHILDES database (MacWhinney, 2000). The corpus contains transcripts of mothers’ conversations with 12 British children between the ages of 1;8 (years;months) and 3;0. There are 34 one-hour sessions per child over the course of a year. The age range of the children roughly corresponds with the ages at which children show the first evidence of syntactic categories. We extract the mothers’ speech from each of the transcripts, then concatenate the input of all 12 children (all of Anne’s sessions, followed by all of Aran’s sessions, and so on). We remove all punctuation. We spell out contractions, so that each token in the input co</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>MacWhinney, B. 2000. The CHILDES Project: Tools for analyzing talk, volume 2: The Database. Lawrence Erlbaum, Mahwah, NJ, 3 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Mintz</author>
</authors>
<title>Frequent frames as a cue for grammatical categories in child directed speech.</title>
<date>2003</date>
<journal>Cognition,</journal>
<pages>90--91</pages>
<contexts>
<context position="2663" citStr="Mintz, 2003" startWordPosition="389" endWordPosition="390">ons.org/licenses/by-nc-sa/3.0/). Some rights reserved. as on the distributional information about the contexts in which they appear. Several computational models have been proposed that draw on one or more of the above-mentioned properties in order to group words into discrete unlabeled categories. Most existing models only intend to show the relevance of such properties to the acquisition of adult-like syntactic categories such as nouns and verbs; hence, they do not necessarily incorporate the types of learning mechanisms used by children (Sch¨utze, 1993; Redington et al., 1998; Clark, 2000; Mintz, 2003; Onnis and Christiansen, 2005). For example, in contrast to the above models, children acquire their knowledge of syntactic categories incrementally, processing the utterances they hear one at a time. Moreover, children appear to be sensitive to the fact that syntactic categories are partially defined in terms of other categories, e.g., nouns tend to follow determiners, and can be modified by adjectives. We thus argue that a computational model should be incremental, and should use more abstract category knowledge to help better identify syntactic categories. Incremental processing also allow</context>
<context position="32674" citStr="Mintz (2003)" startWordPosition="5439" endWordPosition="5440">lusters. This is a realistic effect of disambiguation seen in adults (MacDonald, 1993). Strongly biased words are more difficult to categorize in conflict with their bias than weakly biased words. 7 Related Work Several existing computational models use distributional cues to find syntactic categories. Sch¨utze (1993) employs co-occurrence statistics for common words, while Redington et al. (1998) build word distributional profiles using corpus bigram counts. Clark (2000) also builds distributional profiles, introducing an iterative clustering method to better handle ambiguity and rare words. Mintz (2003) shows that even very simple three-word templates can effectively define syntactic categories. Each of these models demonstrates that by using the kinds of simple information to which children are known to be sensitive, syntactic categories are learnable. However, the specific learning mechanisms they use, such as the hierarchical clustering methods of Redington et al. (1998), are not intended to be cognitively plausible. In contrast, Cartwright and Brent (1997) propose 95 an incremental model of syntactic category acquisition that uses a series of linguistic preferences to find common pattern</context>
</contexts>
<marker>Mintz, 2003</marker>
<rawString>Mintz, T. H. 2003. Frequent frames as a cue for grammatical categories in child directed speech. Cognition, 90:91–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Olguin</author>
<author>M Tomasello</author>
</authors>
<title>Twenty-five-monthold children do not have a grammatical category of verb. Cognitive Development,</title>
<date>1993</date>
<pages>8--245</pages>
<contexts>
<context position="1347" citStr="Olguin and Tomasello, 1993" startWordPosition="191" endWordPosition="194">learning behaviours similar to those observed in children. 1 Introduction An important open problem in cognitive science and artificial intelligence is how children successfully learn their native language despite the lack of explicit training. A key challenge in the early stages of language acquisition is to learn the notion of abstract syntactic categories (e.g., nouns, verbs, or determiners), which is necessary for acquiring the syntactic structure of language. Indeed, children as young as two years old show evidence of having acquired a good knowledge of some of these abstract categories (Olguin and Tomasello, 1993); by around six years of age, they have learned almost all syntactic categories (Kemp et al., 2005). Computational models help to elucidate the kinds of learning mechanisms that may be capable of achieving this feat. Such studies shed light on the possible cognitive mechanisms at work in human language acquisition, and also on potential means for unsupervised learning of complex linguistic knowledge in a computational system. Learning the syntactic categories of words has been suggested to be based on the morphological and phonological properties of individual words, as well © 2008. Licensed u</context>
</contexts>
<marker>Olguin, Tomasello, 1993</marker>
<rawString>Olguin, R. and M. Tomasello 1993. Twenty-five-monthold children do not have a grammatical category of verb. Cognitive Development, 8:245–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Onnis</author>
<author>M H Christiansen</author>
</authors>
<title>New beginnings and happy endings: psychological plausibility in computational models of language acquisition.</title>
<date>2005</date>
<journal>CogSci2005.</journal>
<contexts>
<context position="2694" citStr="Onnis and Christiansen, 2005" startWordPosition="391" endWordPosition="394">ses/by-nc-sa/3.0/). Some rights reserved. as on the distributional information about the contexts in which they appear. Several computational models have been proposed that draw on one or more of the above-mentioned properties in order to group words into discrete unlabeled categories. Most existing models only intend to show the relevance of such properties to the acquisition of adult-like syntactic categories such as nouns and verbs; hence, they do not necessarily incorporate the types of learning mechanisms used by children (Sch¨utze, 1993; Redington et al., 1998; Clark, 2000; Mintz, 2003; Onnis and Christiansen, 2005). For example, in contrast to the above models, children acquire their knowledge of syntactic categories incrementally, processing the utterances they hear one at a time. Moreover, children appear to be sensitive to the fact that syntactic categories are partially defined in terms of other categories, e.g., nouns tend to follow determiners, and can be modified by adjectives. We thus argue that a computational model should be incremental, and should use more abstract category knowledge to help better identify syntactic categories. Incremental processing also allows a model to incorporate its pa</context>
</contexts>
<marker>Onnis, Christiansen, 2005</marker>
<rawString>Onnis, L. and M. H. Christiansen 2005. New beginnings and happy endings: psychological plausibility in computational models of language acquisition. CogSci2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Redington</author>
<author>N Chater</author>
<author>S Finch</author>
</authors>
<title>Distributional information: A powerful cue for acquiring syntactic categories.</title>
<date>1998</date>
<journal>Cognitive Science,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="2637" citStr="Redington et al., 1998" startWordPosition="382" endWordPosition="386">Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. as on the distributional information about the contexts in which they appear. Several computational models have been proposed that draw on one or more of the above-mentioned properties in order to group words into discrete unlabeled categories. Most existing models only intend to show the relevance of such properties to the acquisition of adult-like syntactic categories such as nouns and verbs; hence, they do not necessarily incorporate the types of learning mechanisms used by children (Sch¨utze, 1993; Redington et al., 1998; Clark, 2000; Mintz, 2003; Onnis and Christiansen, 2005). For example, in contrast to the above models, children acquire their knowledge of syntactic categories incrementally, processing the utterances they hear one at a time. Moreover, children appear to be sensitive to the fact that syntactic categories are partially defined in terms of other categories, e.g., nouns tend to follow determiners, and can be modified by adjectives. We thus argue that a computational model should be incremental, and should use more abstract category knowledge to help better identify syntactic categories. Increme</context>
<context position="18274" citStr="Redington et al., 1998" startWordPosition="2990" endWordPosition="2993">d evaluation sets, each containing approximately 683,000 tokens. We use the development set to finetune the model parameters and develop the experiments, then use the evaluation set as a final test of the model. We further split the development set into about 672,000 tokens (about 8,000 types) for training and 11,000 tokens (1,300 types) for validation. We split the evaluation set comparably, into training and test subsets. All reported results are for the evaluation set. A conservative estimate suggests that children are exposed to at least 1.5 million words of childdirected speech annually (Redington et al., 1998), so this corpus represents only a small portion of a child’s available input. 4 Experiment 1: Adult Categories 4.1 Methods We use three separate versions of the categorization model, in which we change the components used to estimate the context word probability, P(wi|k) (as used in Eq. (5), Section 2.2). In the word-based model, we estimate the context probabilities using only the words in the context window, by directly using the maximum-likelihood Pword estimate. The bootstrap model uses only the existing clusters to estimate the probability, directly using the Pcat estimate from Eq. (6). </context>
<context position="32462" citStr="Redington et al. (1998)" startWordPosition="5407" endWordPosition="5410">tion, to the ‘noun biased’ condition, to ‘equibiased’ (and likewise for the verb biases). As the lexical bias weakens, the counter-bias contexts (e.g., a noun bias with a verb context) show a stronger effect on the chosen clusters. This is a realistic effect of disambiguation seen in adults (MacDonald, 1993). Strongly biased words are more difficult to categorize in conflict with their bias than weakly biased words. 7 Related Work Several existing computational models use distributional cues to find syntactic categories. Sch¨utze (1993) employs co-occurrence statistics for common words, while Redington et al. (1998) build word distributional profiles using corpus bigram counts. Clark (2000) also builds distributional profiles, introducing an iterative clustering method to better handle ambiguity and rare words. Mintz (2003) shows that even very simple three-word templates can effectively define syntactic categories. Each of these models demonstrates that by using the kinds of simple information to which children are known to be sensitive, syntactic categories are learnable. However, the specific learning mechanisms they use, such as the hierarchical clustering methods of Redington et al. (1998), are not </context>
</contexts>
<marker>Redington, Chater, Finch, 1998</marker>
<rawString>Redington, M., N. Chater, and S. Finch 1998. Distributional information: A powerful cue for acquiring syntactic categories. Cognitive Science, 22(4):425–469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A N Sanborn</author>
<author>T L Griffiths</author>
<author>D J Navarro</author>
</authors>
<title>A more rational model of categorization.</title>
<date>2006</date>
<journal>CogSci2006.</journal>
<contexts>
<context position="8821" citStr="Sanborn et al., 2006" startWordPosition="1413" endWordPosition="1416"> Next, we explain how we estimate each of these probabilities from the input. 2.3 Probabilities and Bootstrapping For the head word probability PH(w0|k), we use a smoothed maximum likelihood estimate (i.e., the proportion of frames in cluster k with head word w0). For the context word probability P(wi|k), we can form two estimates. The first is a simple maximum likelihood estimate, which enforces a preference for creating clusters of frames with the same context words. That is, head words in the same cluster will 1The prior P(k) is equivalent to the prior in a Dirichlet process mixture model (Sanborn et al., 2006), commonly used for sampling clusters of objects. 90 tend to share the same adjacent words. We call this word-based estimate Pword. Alternatively, we may consider the likelihood of seeing not just the context word wi, but similar words in that position. For example, if wi can be used as a noun or a verb, then we want the likelihood of seeing other nouns or verbs in position i of frames in cluster k. Here, we use the partial knowledge of the learned clusters. That is, we look over all existing clusters k0, estimate the probability that wi is the head word of frames in k0, then estimate the prob</context>
<context position="35471" citStr="Sanborn et al., 2006" startWordPosition="5860" endWordPosition="5863">egories from the input data alone. The explicit bootstrapping component improves the model’s ability to learn adult categories, and its learning trajectory resembles relevant behaviours seen in children. Using the contextual patterns of individual parts of speech, we show differential learning rates across nouns, verbs, and adjectives that mimic child development. We also show an effect of a lexical bias in category disambiguation. The algorithm is currently only implemented as an incremental process. However, comparison with a batch version of the algorithm, such as by using a Gibbs sampler (Sanborn et al., 2006), would help us further understand the effect of incrementality on language fidelity. While we have only examined the effects of learning categories from simple distributional information, the feature-based framework of our model could easily be extended to include other sources of information, such as morphological and phonological cues. Furthermore, it would also be possible to include semantic features, thereby allowing the model to draw on correlations between semantic and syntactic categories in learning. Acknowledgments We thank Afra Alishahi for valuable discussions, and the anonymous r</context>
</contexts>
<marker>Sanborn, Griffiths, Navarro, 2006</marker>
<rawString>Sanborn, A. N., T. L. Griffiths, and D. J. Navarro 2006. A more rational model of categorization. CogSci2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Part of speech induction from scratch.</title>
<date>1993</date>
<booktitle>In Proc. ofACL1993,</booktitle>
<pages>251--258</pages>
<marker>Sch¨utze, 1993</marker>
<rawString>Sch¨utze, H. 1993. Part of speech induction from scratch. In Proc. ofACL1993, pp. 251–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Theakston</author>
<author>E V Lieven</author>
<author>J M Pine</author>
<author>C F Rowland</author>
</authors>
<title>The role of performance limitations in the acquisition of verb-argument structure: an alternative account.</title>
<date>2001</date>
<journal>J. Child Lang.,</journal>
<pages>28--127</pages>
<contexts>
<context position="16754" citStr="Theakston et al., 2001" startWordPosition="2744" endWordPosition="2747">omponent. The second experiment (Section 5) examines the model’s development of three specific parts of speech. Developmental evidence suggests that children acquire different syntactic categories at different ages, so we compare the model’s learning rates of nouns, verbs, and adjectives. Lastly, we examine our model’s ability to handle lexically ambiguous words (Section 6). English word forms commonly belong to more than one syntactic category, so we show how our model uses context to disambiguate a word’s category. In all experiments, we train and test the model using the Manchester corpus (Theakston et al., 2001) from the CHILDES database (MacWhinney, 2000). The corpus contains transcripts of mothers’ conversations with 12 British children between the ages of 1;8 (years;months) and 3;0. There are 34 one-hour sessions per child over the course of a year. The age range of the children roughly corresponds with the ages at which children show the first evidence of syntactic categories. We extract the mothers’ speech from each of the transcripts, then concatenate the input of all 12 children (all of Anne’s sessions, followed by all of Aran’s sessions, and so on). We remove all punctuation. We spell out con</context>
</contexts>
<marker>Theakston, Lieven, Pine, Rowland, 2001</marker>
<rawString>Theakston, A. L., E. V. Lieven, J. M. Pine, and C. F. Rowland 2001. The role of performance limitations in the acquisition of verb-argument structure: an alternative account. J. Child Lang., 28:127–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomasello</author>
</authors>
<title>Do young children have adult syntactic competence?</title>
<date>2000</date>
<journal>Cognition,</journal>
<pages>74--209</pages>
<contexts>
<context position="11580" citStr="Tomasello, 2000" startWordPosition="1886" endWordPosition="1887"> our model. 2.4 Generalization Our model relies heavily on the similarity of word contexts in order to find category structure. In natural language, these context features are highly variable, so it is difficult to draw consistent structure from the input in the early stages of an incremental model. When little information is available, there is a risk of incorrectly generalizing, leading to clustering errors which may be difficult to overcome. Children face a similar problem in early learning, but there is evidence that they may manage the problem by using conservative strategies (see, e.g., Tomasello, 2000). Children may form specific hypotheses about each word type, only later generalizing their knowledge to similar words. Drawing on this observation, we form early small clusters specific to the head word type, then later aid generalization by merging these smaller clusters. By doing this, we ensure that the model only groups words of different types when there is sufficient evidence for their contextual similarity. Thus, when a cluster has been newly created, we require that all frames put into the cluster share the same head word type.2 When clusters are small, this prevents the model from ma</context>
<context position="27049" citStr="Tomasello, 2000" startWordPosition="4467" endWordPosition="4468">simply be a result of the overall frequency of the PoS types. Over the entire corpus (development and evaluation), 35.4% of the word tokens are nouns and 24.3% are verbs, but only 2.9% are tagged as adjectives. The model, and similarly a child, may need much more data to learn adjectives than is available at this stage. The scores in Figure 2 tend to fluctuate, particularly for the noun contexts. This fluctuation corresponds to periods of overgeneralization, followed Figure 2: Comparative learning trends of noun, verb, and adjective patterns. by recovery (also observed in children; see, e.g., Tomasello, 2000). When the model merges two clusters, the contents of the resulting cluster can initially be quite heterogeneous. Furthermore, the new cluster is much larger, so it becomes a magnet for new categorizations. This results in overgeneralization errors, giving the periodic drops seen in Figure 2. While our formulation in Section 2.4 aims to prevent such errors, they are likely to occur on occasion. Eventually, the model recovers from these errors, and it is worth noting that the fluctuations diminish over time. As the model gradually improves with more input, the dominant clusters become heavily e</context>
</contexts>
<marker>Tomasello, 2000</marker>
<rawString>Tomasello, M. 2000. Do young children have adult syntactic competence? Cognition, 74:209–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomasello</author>
<author>N Akhtar</author>
<author>K Dodson</author>
<author>L Rekau</author>
</authors>
<title>Differential productivity in young children’s use of nouns and verbs.</title>
<date>1997</date>
<journal>J. Child Lang.,</journal>
<pages>24--373</pages>
<contexts>
<context position="22747" citStr="Tomasello et al., 1997" startWordPosition="3714" endWordPosition="3717">ree models’ clusterings of the test set, as compared with the PoS tags of the test data. 5 Experiment 2: Learning Trends A common trend observed in children is that different syntactic categories are learned at different rates. Children appear to have learned the category of nouns by 23 months of age, verbs shortly thereafter, and adjectives relatively late (Kemp et al., 2005). Our goal in this experiment is to look for these specific trends in the behaviour of our model. We thus simulate an experiment where a child uses a novel word’s linguistic context to infer its syntactic category (e.g., Tomasello et al., 1997). For our experiment, we randomly generate input frames with novel head words using contexts associated with nouns, verbs, and adjectives, then examine the model’s categorization in each case. We expect that our model should approximate the developmental trends of children, who tend to learn the category of ‘noun’ before ‘verb,’ and both of these before ‘adjective.’ 5.1 Methods We generate new input frames using the most common syntactic patterns in the training data. For each of the noun, verb, and adjective categories (from the gold standard), we collect the five most frequent PoS sequences </context>
</contexts>
<marker>Tomasello, Akhtar, Dodson, Rekau, 1997</marker>
<rawString>Tomasello, M., N. Akhtar, K. Dodson, and L. Rekau 1997. Differential productivity in young children’s use of nouns and verbs. J. Child Lang., 24:373–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>M Johnson</author>
</authors>
<title>A Bayesian LDAbased model for semi-supervised part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In NIPS2008.</booktitle>
<contexts>
<context position="33773" citStr="Toutanova and Johnson, 2008" startWordPosition="5602" endWordPosition="5605">ose 95 an incremental model of syntactic category acquisition that uses a series of linguistic preferences to find common patterns across sentence-length templates. Their model presents an important incremental algorithm which is very effective for discovering categories in artificial languages. However, the model’s reliance on templates limits its applicability to transcripts of actual spoken language data, which contain high variability and noise. Recent models that apply Bayesian approaches to PoS tagging are not incremental and assume a fixed number of tags (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008). In syntactic category acquisition, the true number of categories is unknown, and must be inferred from the input. 8 Conclusions and Future Directions We have developed a computational model of syntactic category acquisition in children, and demonstrated its behaviour on a corpus of naturalistic childdirected data. The model is based on domain-general properties of feature similarity, in contrast to earlier, more linguistically-specific methods. The incremental nature of the algorithm contributes to a substantial improvement in psychological plausibility over previous models of syntactic cate</context>
</contexts>
<marker>Toutanova, Johnson, 2008</marker>
<rawString>Toutanova, K. and M. Johnson 2008. A Bayesian LDAbased model for semi-supervised part-of-speech tagging. In NIPS2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>