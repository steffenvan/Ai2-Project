<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002083">
<title confidence="0.852585333333333">
An Ensemble Model that Combines Syntactic and Semantic Clustering
for Discriminative Dependency Parsing
is:
</title>
<author confidence="0.972298">
Gholamreza Haffari
</author>
<affiliation confidence="0.995379">
Faculty of Information Technology
Monash University
</affiliation>
<address confidence="0.688329">
Melbourne, Australia
</address>
<email confidence="0.99925">
reza@monash.edu
</email>
<sectionHeader confidence="0.997391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997412916666667">
We combine multiple word representations
based on semantic clusters extracted from the
(Brown et al., 1992) algorithm and syntac-
tic clusters obtained from the Berkeley parser
(Petrov et al., 2006) in order to improve dis-
criminative dependency parsing in the MST-
Parser framework (McDonald et al., 2005).
We also provide an ensemble method for com-
bining diverse cluster-based models. The two
contributions together significantly improves
unlabeled dependency accuracy from 90.82%
to 92.13%.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941782608696">
A simple method for using unlabeled data in
discriminative dependency parsing was provided
in (Koo et al., 2008) which involved clustering the
labeled and unlabeled data and then each word in the
dependency treebank was assigned a cluster identi-
fier. These identifiers were used to augment the fea-
ture representation of the edge-factored or second-
order features, and this extended feature set was
used to discriminatively train a dependency parser.
The use of clusters leads to the question of
how to integrate various types of clusters (possibly
from different clustering algorithms) in discrimina-
tive dependency parsing. Clusters obtained from the
(Brown et al., 1992) clustering algorithm are typi-
cally viewed as “semantic”, e.g. one cluster might
contain plan, letter, request, memo, ... while an-
other may contain people, customers, employees,
students, .... Another clustering view that is more
“syntactic” in nature comes from the use of state-
splitting in PCFGs. For instance, we could ex-
tract a syntactic cluster loss, time, profit, earnings,
performance, rating, ...: all head words of noun
phrases corresponding to cluster of direct objects of
</bodyText>
<author confidence="0.427007">
Marzieh Razavi and Anoop Sarkar
</author>
<affiliation confidence="0.897239666666667">
School of Computing Science
Simon Fraser University
Vancouver, Canada
</affiliation>
<email confidence="0.989619">
{mrazavi,anoop}@cs.sfu.ca
</email>
<bodyText confidence="0.999914933333333">
verbs like improve. In this paper, we obtain syn-
tactic clusters from the Berkeley parser (Petrov et
al., 2006). This paper makes two contributions: 1)
We combine together multiple word representations
based on semantic and syntactic clusters in order to
improve discriminative dependency parsing in the
MSTParser framework (McDonald et al., 2005), and
2) We provide an ensemble method for combining
diverse clustering algorithms that is the discrimina-
tive parsing analog to the generative product of ex-
perts model for parsing described in (Petrov, 2010).
These two contributions combined significantly im-
proves unlabeled dependency accuracy: 90.82% to
92.13% on Sec. 23 of the Penn Treebank, and we
see consistent improvements across all our test sets.
</bodyText>
<sectionHeader confidence="0.985878" genericHeader="method">
2 Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999958384615385">
A dependency tree represents the syntactic structure
of a sentence with a directed graph (Figure 1), where
nodes correspond to the words, and arcs indicate
head-modifier pairs (Mel’ˇcuk, 1987). Graph-based
dependency parsing searches for the highest-scoring
tree according to apart-factored scoring function. In
the first-order parsing models, the parts are individ-
ual head-modifier arcs in the dependency tree (Mc-
Donald et al., 2005). In the higher-order models, the
parts consist of arcs together with some context, e.g.
the parent or the sister arcs (McDonald and Pereira,
2006; Carreras, 2007; Koo and Collins, 2010). With
a linear scoring function, the parse for a sentence s
</bodyText>
<equation confidence="0.982952333333333">
1:
PARSE(s) = arg max w · f(s, r) (1)
tET(s) rEt
</equation>
<bodyText confidence="0.9999574">
where T (s) is the space of dependency trees for s,
and f(s, r) is the feature vector for the part r which
is linearly combined using the model parameter w
to give the part score. The above arg max search
for non-projective dependency parsing is accom-
</bodyText>
<page confidence="0.952187">
710
</page>
<note confidence="0.892101">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710–714,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figureCaption confidence="0.889444333333333">
Figure 1: Dependency tree with cluster identifiers obtained from the split non-terminals from the Berkeley parser output. The first
row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is
the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters.
</figureCaption>
<figure confidence="0.997808804878048">
For
root
IN-1
PP-2
0111
Japan
NNP-19
NP-10
0110
,
,-0
,-0
0010
improves
VBZ-1
S-14
0101
American
JJ-31
JJ-31
0110
markets
NNS-25
NP-9
1011
the
DT-15
DT-15
1101
trend
NN-23
NP-18
1010
to
TO-0
TO-0
0011
access
NN-13
NP-24
0011
</figure>
<bodyText confidence="0.998524086956522">
plished using minimum spanning tree algorithms
(West, 2001) or approximate inference algorithms
(Smith and Eisner, 2008; Koo et al., 2010). The
(Eisner, 1996) algorithm is typically used for pro-
jective parsing. The model parameters are trained
using a discriminative learning algorithm, e.g. av-
eraged perceptron (Collins, 2002) or MIRA (Cram-
mer and Singer, 2003). In this paper, we work with
both first-order and second-order models, we train
the models using MIRA, and we use the (Eisner,
1996) algorithm for inference.
The baseline features capture information about
the lexical items and their part of speech (POS) tags
(as defined in (McDonald et al., 2005)). In this work,
following (Koo et al., 2008), we use word cluster
identifiers as the source of an additional set of fea-
tures. The reader is directed to (Koo et al., 2008)
for the list of cluster-based feature templates. The
clusters inject long distance syntactic or semantic in-
formation into the model (in contrast with the use
of POS tags in the baseline) and help alleviate the
sparse data problem for complex features that in-
clude n-grams.
</bodyText>
<sectionHeader confidence="0.996023" genericHeader="method">
3 The Ensemble Model
</sectionHeader>
<bodyText confidence="0.999956666666666">
A word can have different syntactic or semantic
cluster representations, each of which may lead to a
different parsing model. We use ensemble learning
(Dietterich, 2002) in order to combine a collection
of diverse and accurate models into a more powerful
model. In this paper, we construct the base models
based on different syntactic/semantic clusters used
in the features in each model. Our ensemble parsing
model is a linear combination of the base models:
</bodyText>
<equation confidence="0.988745">
� �
PARSE(s) = arg max αk wk · fk(s, r) (2)
tET(s)
k rEt
</equation>
<bodyText confidence="0.999959571428572">
where αk is the weight of the kth base model, and
each base model has its own feature mapping fk(.)
based on its cluster annotation. Each expert pars-
ing model in the ensemble contains all of the base-
line and the cluster-based feature templates; there-
fore, the experts have in common (at least) the base-
line features. The only difference between individ-
ual parsing models is the assigned cluster labels, and
hence some of the cluster-based features. In a fu-
ture work, we plan to take the union of all of the
feature sets and train a joint discriminative parsing
model. The ensemble approach seems more scal-
able though, since we can incrementally add a large
number of clustering algorithms into the ensemble.
</bodyText>
<sectionHeader confidence="0.990239" genericHeader="method">
4 Syntactic and Semantic Clustering
</sectionHeader>
<bodyText confidence="0.999749384615384">
In our ensemble model we use three different clus-
tering methods to obtain three types of word rep-
resentations that can help alleviate sparse data in a
dependency parser. Our first word representation is
exactly the same as the one used in (Koo et al., 2008)
where words are clustered using the Brown algo-
rithm (Brown et al., 1992). Our two other clusterings
are extracted from the split non-terminals obtained
from the PCFG-based Berkeley parser (Petrov et al.,
2006). Split non-terminals from the Berkeley parser
output are converted into cluster identifiers in two
different ways: 1) the split POS tags for each word
are used as an alternate word representation. We
call this representation Syn-Low, and 2) head per-
colation rules are used to label each non-terminal in
the parse such that each non-terminal has a unique
daughter labeled as head. Each word is assigned a
cluster identifier which is defined as the parent split
non-terminal of that word if it is not marked as head,
else if the parent is marked as head we recursively
check its parent until we reach the unique split non-
terminal that is not marked as head. This recursion
terminates at the start symbol TOP. We call this rep-
resentation Syn-High. We only use cluster identi-
fiers from the Berkeley parser, rather than dependen-
cies, or any other information.
</bodyText>
<page confidence="0.98949">
711
</page>
<table confidence="0.9990456">
First order features
Sec Baseline BrownSyn- LowSyn -High Ensemble
00 89.61 90.39 90.01 89.97 90.82
34.68 36.97 34.42 34.94 37.96
01 90.44 91.48 90.89 90.76 91.84
36.36 38.62 35.66 36.56 39.67
23 90.02 91.13 90.46 90.35 91.30
34.13 39.64 36.95 35.00 39.43
24 88.84 90.06 89.44 89.40 90.33
30.85 34.49 32.49 31.22 34.05
Second order features
Sec Baseline BrownSyn- LowSyn -High Ensemble
00 90.34 90.98 90.89 90.59 91.41
38.02 41.04 38.80 39.16 40.93
01 91.48 92.13 91.95 91.72 92.51
41.48 43.84 42.24 41.28 45.05
23 90.82 91.84 91.31 91.21 92.13
39.18 43.66 40.84 39.97 44.28
24 89.87 90.61 90.28 90.31 91.18
35.53 37.99 37.32 35.61 39.55
</table>
<tableCaption confidence="0.782618666666667">
Table 1: For each test section and model, the number in the
first/second row is the unlabeled-accuracy/unlabeled-complete-
correct. See the text for more explanation.
</tableCaption>
<equation confidence="0.924194692307692">
(TOP
(S-14
(PP-2 (IN-1 For)
(NP-10 (NNP-19 Japan)))
(,-0 ,)
(NP-18 (DT-15 the) (NN-23 trend))
(VP-6 (VBZ-1 improves)
(NP-24 (NN-13 access))
(PP-14 (TO-0 to)
(NP-9 (JJ-31 American)
(NNS-25 markets))))))
Verb Noun Pronoun Adverb Adjective Adpos. Conjunc.
ende
</equation>
<figureCaption confidence="0.983325666666667">
Figure 2: (a) Error rate of the head attachment for different
types of modifier categories. (b) F-score for each dependency
length.
</figureCaption>
<figure confidence="0.999274415094339">
0.04 0.06 0.08 0.10 0.12 0.14
Baseline
Brown
Syn−Low
Syn−High
Ensemble
1 3 5 7 9 11
13 +15
0.80 0.85 0.90 0.95
!
!
!
!!
!
!
!
!
!
!
!
!
!
! ! ! ! !
! !
!
!
!
!
!
!
!
!
!
!
!
! ! !
!
!
!
!
!
!
Baseline
Brown
Syn−Low
Syn−High
Ensemble
!
!
!
!
!
!
</figure>
<bodyText confidence="0.998728">
For the Berkeley parser output shown above, the
resulting word representations and dependency tree
is shown in Fig. 1. If we group all the head-words in
the training data that project up to split non-terminal
NP-24 then we get a cluster: loss, time, profit, earn-
ings, performance, rating, ... which are head words
of the noun phrases that appear as direct object of
verbs like improve.
</bodyText>
<sectionHeader confidence="0.992636" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.9997939375">
The experiments were done on the English Penn
Treebank, using standard head-percolation rules
(Yamada and Matsumoto, 2003) to convert the
phrase structure into dependency trees. We split the
Treebank into a training set (Sections 2-21), a devel-
opment set (Section 22), and test sets (Sections 0,
1, 23, and 24). All our experimental settings match
previous work (Yamada and Matsumoto, 2003; Mc-
Donald et al., 2005; Koo et al., 2008). POS tags for
the development and test data were assigned by MX-
POST (Ratnaparkhi, 1996), where the tagger was
trained on the entire training corpus. To generate
part of speech tags for the training data, we used 20-
way jackknifing, i.e. we tagged each fold with the
tagger trained on the other 19 folds. We set model
weights αk in Eqn (2) to one for all experiments.
</bodyText>
<subsectionHeader confidence="0.798365">
Syntactic State-Splitting The sentence-specific
</subsectionHeader>
<bodyText confidence="0.675147">
word clusters are derived from the parse trees using
</bodyText>
<page confidence="0.95861">
712
</page>
<bodyText confidence="0.986117755102041">
Berkeley parser1, which generates phrase-structure dency, Fig. 2(a) shows the error rate for each depen-
parse trees with split syntactic categories. To gen- dent grouped by a coarse POS tag (c.f. (McDonald
erate parse trees for development and test data, the and Nivre, 2007)). For most POS categories, the
parser is trained on the entire training data to learn Brown cluster model is the best individual model,
a PCFG with latent annotations using split-merge but for Adjectives it is Syn-High, and for Pronouns
operations for 5 iterations. To generate parse trees it is Syn-Low that is the best. But the ensemble al-
for the training data, we used 20-way jackknifing as ways does the best in every grammatical category.
with the tagger. Fig. 2(b) shows the F-score of the different models
Word Clusterings from Brown Algorithm The for various dependency lengths, where the length of
word clusters were derived using Percy Liang’s im- a dependency from word wi to word wj is equal to
plementation of the (Brown et al., 1992) algorithm |i — j|. We see that different models are experts on
on the BLLIP corpus (Charniak et al., 2000) which different lengths (Syn-Low on 8, Syn-High on 9),
contains —43M words of Wall Street Journal text.2 while the ensemble model can always combine their
This produces a hierarchical clustering over the expertise and do better at each length.
words which is then sliced at a certain height to ob- 6 Comparison to Related Work
tain the clusters. In our experiments we use the clus- Several ensemble models have been proposed for
ters obtained in (Koo et al., 2008)3, but were unable dependency parsing (Sagae and Lavie, 2006; Hall et
to match the accuracy reported there, perhaps due to al., 2007; Nivre and McDonald, 2008; Attardi and
additional features used in their implementation not Dell’Orletta, 2009; Surdeanu and Manning, 2010).
described in the paper.4 Essentially, all of these approaches combine dif-
Results Table 1 presents our results for each ferent dependency parsing systems, i.e. transition-
model on each test set. In this table, the baseline based and graph-based. Although graph-based mod-
(first column) does not use any cluster-based fea- els are globally trained and can use exact inference
tures, the next three models use cluster-based fea- algorithms, their features are defined over a lim-
tures using different clustering algorithms, and the ited history of parsing decisions. Since transition-
last column is our ensemble model which is the lin- based parsing models have the opposite character-
ear combination of the three cluster-based models. istics, the idea is to combine these two types of
As Table 1 shows, the ensemble model has out- models to exploit their complementary strengths.
performed the baseline and individual models in al- The base parsing models are either independently
most all cases. Among the individual models, the trained (Sagae and Lavie, 2006; Hall et al., 2007;
model with Brown semantic clusters clearly outper- Attardi and Dell’Orletta, 2009; Surdeanu and Man-
forms the baseline, but the two models with syntac- ning, 2010), or their training is integrated, e.g. using
tic clusters perform almost the same as the baseline. stacking (Nivre and McDonald, 2008; Attardi and
The ensemble model outperforms all of the individ- Dell’Orletta, 2009; Surdeanu and Manning, 2010).
ual models and does so very consistently across both Our work is distinguished from the aforemen-
first-order and second-order dependency models. tioned works in two dimensions. Firstly, we com-
Error Analysis To better understand the contri- bine various graph-based models, constructed using
bution of each model to the ensemble, we take a different syntactic/semantic clusters. Secondly, we
closer look at the parsing errors for each model and do exact inference on the shared hypothesis space of
the ensemble. For each dependent to head depen- the base models. This is in contrast to previous work
which combine the best parse trees suggested by the
individual base-models to generate a final parse tree,
i.e. a two-phase inference scheme.
7 Conclusion
We presented an ensemble of different dependency
parsing models, each model corresponding to a dif-
1code.google.com/p/berkeleyparser
2Sentences of the Penn Treebank were excluded from the
text used for the clustering.
</bodyText>
<table confidence="0.684465">
3people.csail.mit.edu/maestro/papers/bllip-clusters.gz
4Terry Koo was kind enough to share the source code for the
(Koo et al., 2008) paper with us, and we plan to incorporate all
the features in our future work.
</table>
<page confidence="0.456712">
713
</page>
<bodyText confidence="0.999316615384616">
ferent syntactic/semantic word clustering annota-
tion. The ensemble obtains consistent improve-
ments in unlabeled dependency parsing, e.g. from
90.82% to 92.13% for Sec. 23 of the Penn Tree-
bank. Our error analysis has revealed that each syn-
tactic/semantic parsing model is an expert in cap-
turing different dependency lengths, and the ensem-
ble model can always combine their expertise and
do better at each dependency length. We can in-
crementally add a large number models using dif-
ferent clustering algorithms, and our preliminary re-
sults show increased improvement in accuracy when
more models are added into the ensemble.
</bodyText>
<sectionHeader confidence="0.99838" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999880166666667">
This research was partially supported by NSERC,
Canada (RGPIN: 264905). We would like to thank
Terry Koo for his help with the cluster-based fea-
tures for dependency parsing and Ryan McDonald
for the MSTParser source code which we modified
and used for the experiments in this paper.
</bodyText>
<sectionHeader confidence="0.997616" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997385455882353">
G. Attardi and F. Dell’Orletta. 2009. Reverse revision
and linear tree combination for dependency parsing.
In Proc. of NAACL-HLT.
P. F. Brown, P. V. deSouza, R. L. Mercer, T. J. Watson,
V. J. Della Pietra, and J. C. Lai. 1992. Class-based
n-gram models of natural language. Computational
Linguistics, 18(4).
X. Carreras. 2007. Experiments with a higher-order pro-
jective dependency parser. In Proc. of EMNLP-CoNLL
Shared Task.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.
2000. BLLIP 1987-89 WSJ Corpus Release 1, LDC
No. LDC2000T43, Linguistic Data Consortium.
M. Collins. 2002. Discriminative training methods for
hidden markov models: theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. J. Mach.
Learn. Res., 3:951–991.
T. Dietterich. 2002. Ensemble learning. In The Hand-
book of Brain Theory and Neural Networks, Second
Edition.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In COLING.
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,
M. Nilsson, and M. Saers. 2007. Single malt or
blended? a study in multilingual parser optimization.
In Proc. of CoNLL Shared Task.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. ofACL/HLT.
T. Koo, A. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of EMNLP-CONLL.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
I. Mel’ˇcuk. 1987. Dependency syntax: theory and prac-
tice. State University of New York Press.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proc. of ACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. COLING-ACL.
S. Petrov. 2010. Products of random latent variable
grammars. In Proc. of NAACL-HLT.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of NAACL-HLT.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
M. Surdeanu and C. Manning. 2010. Ensemble models
for dependency parsing: Cheap and good? In Proc. of
NAACL.
D. West. 2001. Introduction to Graph Theory. Prentice
Hall, 2nd editoin.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of IWPT.
</reference>
<page confidence="0.998163">
714
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.097295">
<title confidence="0.9675205">An Ensemble Model that Combines Syntactic and Semantic for Discriminative Dependency Parsing</title>
<address confidence="0.388217">is:</address>
<email confidence="0.356667">Gholamreza</email>
<affiliation confidence="0.674536333333333">Faculty of Information Monash Melbourne,</affiliation>
<email confidence="0.999923">reza@monash.edu</email>
<abstract confidence="0.976426076923077">We combine multiple word representations based on semantic clusters extracted from the (Brown et al., 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al., 2006) in order to improve discriminative dependency parsing in the MST- Parser framework (McDonald et al., 2005). We also provide an ensemble method for combining diverse cluster-based models. The two contributions together significantly improves unlabeled dependency accuracy from 90.82% to 92.13%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Attardi</author>
<author>F Dell’Orletta</author>
</authors>
<title>Reverse revision and linear tree combination for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<marker>Attardi, Dell’Orletta, 2009</marker>
<rawString>G. Attardi and F. Dell’Orletta. 2009. Reverse revision and linear tree combination for dependency parsing. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V deSouza</author>
<author>R L Mercer</author>
<author>T J Watson</author>
<author>V J Della Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="1399" citStr="Brown et al., 1992" startWordPosition="200" endWordPosition="203">inative dependency parsing was provided in (Koo et al., 2008) which involved clustering the labeled and unlabeled data and then each word in the dependency treebank was assigned a cluster identifier. These identifiers were used to augment the feature representation of the edge-factored or secondorder features, and this extended feature set was used to discriminatively train a dependency parser. The use of clusters leads to the question of how to integrate various types of clusters (possibly from different clustering algorithms) in discriminative dependency parsing. Clusters obtained from the (Brown et al., 1992) clustering algorithm are typically viewed as “semantic”, e.g. one cluster might contain plan, letter, request, memo, ... while another may contain people, customers, employees, students, .... Another clustering view that is more “syntactic” in nature comes from the use of statesplitting in PCFGs. For instance, we could extract a syntactic cluster loss, time, profit, earnings, performance, rating, ...: all head words of noun phrases corresponding to cluster of direct objects of Marzieh Razavi and Anoop Sarkar School of Computing Science Simon Fraser University Vancouver, Canada {mrazavi,anoop}</context>
<context position="4309" citStr="Brown et al., 1992" startWordPosition="655" endWordPosition="658">re. The above arg max search for non-projective dependency parsing is accom710 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710–714, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Figure 1: Dependency tree with cluster identifiers obtained from the split non-terminals from the Berkeley parser output. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. For root IN-1 PP-2 0111 Japan NNP-19 NP-10 0110 , ,-0 ,-0 0010 improves VBZ-1 S-14 0101 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 the DT-15 DT-15 1101 trend NN-23 NP-18 1010 to TO-0 TO-0 0011 access NN-13 NP-24 0011 plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). </context>
<context position="7263" citStr="Brown et al., 1992" startWordPosition="1157" endWordPosition="1160">uture work, we plan to take the union of all of the feature sets and train a joint discriminative parsing model. The ensemble approach seems more scalable though, since we can incrementally add a large number of clustering algorithms into the ensemble. 4 Syntactic and Semantic Clustering In our ensemble model we use three different clustering methods to obtain three types of word representations that can help alleviate sparse data in a dependency parser. Our first word representation is exactly the same as the one used in (Koo et al., 2008) where words are clustered using the Brown algorithm (Brown et al., 1992). Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al., 2006). Split non-terminals from the Berkeley parser output are converted into cluster identifiers in two different ways: 1) the split POS tags for each word are used as an alternate word representation. We call this representation Syn-Low, and 2) head percolation rules are used to label each non-terminal in the parse such that each non-terminal has a unique daughter labeled as head. Each word is assigned a cluster identifier which is defined as the parent split no</context>
<context position="12033" citStr="Brown et al., 1992" startWordPosition="1977" endWordPosition="1980"> model, a PCFG with latent annotations using split-merge but for Adjectives it is Syn-High, and for Pronouns operations for 5 iterations. To generate parse trees it is Syn-Low that is the best. But the ensemble alfor the training data, we used 20-way jackknifing as ways does the best in every grammatical category. with the tagger. Fig. 2(b) shows the F-score of the different models Word Clusterings from Brown Algorithm The for various dependency lengths, where the length of word clusters were derived using Percy Liang’s im- a dependency from word wi to word wj is equal to plementation of the (Brown et al., 1992) algorithm |i — j|. We see that different models are experts on on the BLLIP corpus (Charniak et al., 2000) which different lengths (Syn-Low on 8, Syn-High on 9), contains —43M words of Wall Street Journal text.2 while the ensemble model can always combine their This produces a hierarchical clustering over the expertise and do better at each length. words which is then sliced at a certain height to ob- 6 Comparison to Related Work tain the clusters. In our experiments we use the clus- Several ensemble models have been proposed for ters obtained in (Koo et al., 2008)3, but were unable dependenc</context>
</contexts>
<marker>Brown, deSouza, Mercer, Watson, Pietra, Lai, 1992</marker>
<rawString>P. F. Brown, P. V. deSouza, R. L. Mercer, T. J. Watson, V. J. Della Pietra, and J. C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL Shared Task.</booktitle>
<contexts>
<context position="3380" citStr="Carreras, 2007" startWordPosition="499" endWordPosition="500">ependency Parsing A dependency tree represents the syntactic structure of a sentence with a directed graph (Figure 1), where nodes correspond to the words, and arcs indicate head-modifier pairs (Mel’ˇcuk, 1987). Graph-based dependency parsing searches for the highest-scoring tree according to apart-factored scoring function. In the first-order parsing models, the parts are individual head-modifier arcs in the dependency tree (McDonald et al., 2005). In the higher-order models, the parts consist of arcs together with some context, e.g. the parent or the sister arcs (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). With a linear scoring function, the parse for a sentence s 1: PARSE(s) = arg max w · f(s, r) (1) tET(s) rEt where T (s) is the space of dependency trees for s, and f(s, r) is the feature vector for the part r which is linearly combined using the model parameter w to give the part score. The above arg max search for non-projective dependency parsing is accom710 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710–714, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Figure 1: </context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proc. of EMNLP-CoNLL Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>D Blaheta</author>
<author>N Ge</author>
<author>K Hall</author>
<author>M Johnson</author>
</authors>
<date>2000</date>
<booktitle>BLLIP 1987-89 WSJ Corpus Release 1, LDC No. LDC2000T43, Linguistic Data Consortium.</booktitle>
<contexts>
<context position="12140" citStr="Charniak et al., 2000" startWordPosition="1997" endWordPosition="2000">ouns operations for 5 iterations. To generate parse trees it is Syn-Low that is the best. But the ensemble alfor the training data, we used 20-way jackknifing as ways does the best in every grammatical category. with the tagger. Fig. 2(b) shows the F-score of the different models Word Clusterings from Brown Algorithm The for various dependency lengths, where the length of word clusters were derived using Percy Liang’s im- a dependency from word wi to word wj is equal to plementation of the (Brown et al., 1992) algorithm |i — j|. We see that different models are experts on on the BLLIP corpus (Charniak et al., 2000) which different lengths (Syn-Low on 8, Syn-High on 9), contains —43M words of Wall Street Journal text.2 while the ensemble model can always combine their This produces a hierarchical clustering over the expertise and do better at each length. words which is then sliced at a certain height to ob- 6 Comparison to Related Work tain the clusters. In our experiments we use the clus- Several ensemble models have been proposed for ters obtained in (Koo et al., 2008)3, but were unable dependency parsing (Sagae and Lavie, 2006; Hall et to match the accuracy reported there, perhaps due to al., 2007; N</context>
</contexts>
<marker>Charniak, Blaheta, Ge, Hall, Johnson, 2000</marker>
<rawString>E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson. 2000. BLLIP 1987-89 WSJ Corpus Release 1, LDC No. LDC2000T43, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="4872" citStr="Collins, 2002" startWordPosition="746" endWordPosition="747">e space in this figure) of the (Brown et al., 1992) clusters. For root IN-1 PP-2 0111 Japan NNP-19 NP-10 0110 , ,-0 ,-0 0010 improves VBZ-1 S-14 0101 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 the DT-15 DT-15 1101 trend NN-23 NP-18 1010 to TO-0 TO-0 0011 access NN-13 NP-24 0011 plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al., 2008) for the list of cluster-based feature templates. The clusters inject long distance syntactic o</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--951</pages>
<contexts>
<context position="4907" citStr="Crammer and Singer, 2003" startWordPosition="750" endWordPosition="754">of the (Brown et al., 1992) clusters. For root IN-1 PP-2 0111 Japan NNP-19 NP-10 0110 , ,-0 ,-0 0010 improves VBZ-1 S-14 0101 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 the DT-15 DT-15 1101 trend NN-23 NP-18 1010 to TO-0 TO-0 0011 access NN-13 NP-24 0011 plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al., 2008) for the list of cluster-based feature templates. The clusters inject long distance syntactic or semantic information into the mod</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. J. Mach. Learn. Res., 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dietterich</author>
</authors>
<title>Ensemble learning.</title>
<date>2002</date>
<booktitle>In The Handbook of Brain Theory and Neural Networks, Second Edition.</booktitle>
<contexts>
<context position="5841" citStr="Dietterich, 2002" startWordPosition="909" endWordPosition="910">ollowing (Koo et al., 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al., 2008) for the list of cluster-based feature templates. The clusters inject long distance syntactic or semantic information into the model (in contrast with the use of POS tags in the baseline) and help alleviate the sparse data problem for complex features that include n-grams. 3 The Ensemble Model A word can have different syntactic or semantic cluster representations, each of which may lead to a different parsing model. We use ensemble learning (Dietterich, 2002) in order to combine a collection of diverse and accurate models into a more powerful model. In this paper, we construct the base models based on different syntactic/semantic clusters used in the features in each model. Our ensemble parsing model is a linear combination of the base models: � � PARSE(s) = arg max αk wk · fk(s, r) (2) tET(s) k rEt where αk is the weight of the kth base model, and each base model has its own feature mapping fk(.) based on its cluster annotation. Each expert parsing model in the ensemble contains all of the baseline and the cluster-based feature templates; therefo</context>
</contexts>
<marker>Dietterich, 2002</marker>
<rawString>T. Dietterich. 2002. Ensemble learning. In The Handbook of Brain Theory and Neural Networks, Second Edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: an exploration. In</title>
<date>1996</date>
<booktitle>In Proc. of CoNLL Shared Task.</booktitle>
<contexts>
<context position="4703" citStr="Eisner, 1996" startWordPosition="722" endWordPosition="723">. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. For root IN-1 PP-2 0111 Japan NNP-19 NP-10 0110 , ,-0 ,-0 0010 improves VBZ-1 S-14 0101 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 the DT-15 DT-15 1101 trend NN-23 NP-18 1010 to TO-0 TO-0 0011 access NN-13 NP-24 0011 plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of a</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: an exploration. In COLING. J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. Nilsson, and M. Saers. 2007. Single malt or blended? a study in multilingual parser optimization. In Proc. of CoNLL Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3404" citStr="Koo and Collins, 2010" startWordPosition="501" endWordPosition="504">g A dependency tree represents the syntactic structure of a sentence with a directed graph (Figure 1), where nodes correspond to the words, and arcs indicate head-modifier pairs (Mel’ˇcuk, 1987). Graph-based dependency parsing searches for the highest-scoring tree according to apart-factored scoring function. In the first-order parsing models, the parts are individual head-modifier arcs in the dependency tree (McDonald et al., 2005). In the higher-order models, the parts consist of arcs together with some context, e.g. the parent or the sister arcs (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). With a linear scoring function, the parse for a sentence s 1: PARSE(s) = arg max w · f(s, r) (1) tET(s) rEt where T (s) is the space of dependency trees for s, and f(s, r) is the feature vector for the part r which is linearly combined using the model parameter w to give the part score. The above arg max search for non-projective dependency parsing is accom710 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710–714, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Figure 1: Dependency tree with clu</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>T. Koo and M. Collins. 2010. Efficient third-order dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semisupervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. ofACL/HLT.</booktitle>
<contexts>
<context position="841" citStr="Koo et al., 2008" startWordPosition="114" endWordPosition="117"> We combine multiple word representations based on semantic clusters extracted from the (Brown et al., 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al., 2006) in order to improve discriminative dependency parsing in the MSTParser framework (McDonald et al., 2005). We also provide an ensemble method for combining diverse cluster-based models. The two contributions together significantly improves unlabeled dependency accuracy from 90.82% to 92.13%. 1 Introduction A simple method for using unlabeled data in discriminative dependency parsing was provided in (Koo et al., 2008) which involved clustering the labeled and unlabeled data and then each word in the dependency treebank was assigned a cluster identifier. These identifiers were used to augment the feature representation of the edge-factored or secondorder features, and this extended feature set was used to discriminatively train a dependency parser. The use of clusters leads to the question of how to integrate various types of clusters (possibly from different clustering algorithms) in discriminative dependency parsing. Clusters obtained from the (Brown et al., 1992) clustering algorithm are typically viewed</context>
<context position="5251" citStr="Koo et al., 2008" startWordPosition="808" endWordPosition="811">rithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al., 2008) for the list of cluster-based feature templates. The clusters inject long distance syntactic or semantic information into the model (in contrast with the use of POS tags in the baseline) and help alleviate the sparse data problem for complex features that include n-grams. 3 The Ensemble Model A word can have different syntactic or semantic cluster representations, each of which may lead to a different parsing model. We use ensemble learning (Dietterich, 2002) in order </context>
<context position="7190" citStr="Koo et al., 2008" startWordPosition="1144" endWordPosition="1147">ed cluster labels, and hence some of the cluster-based features. In a future work, we plan to take the union of all of the feature sets and train a joint discriminative parsing model. The ensemble approach seems more scalable though, since we can incrementally add a large number of clustering algorithms into the ensemble. 4 Syntactic and Semantic Clustering In our ensemble model we use three different clustering methods to obtain three types of word representations that can help alleviate sparse data in a dependency parser. Our first word representation is exactly the same as the one used in (Koo et al., 2008) where words are clustered using the Brown algorithm (Brown et al., 1992). Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al., 2006). Split non-terminals from the Berkeley parser output are converted into cluster identifiers in two different ways: 1) the split POS tags for each word are used as an alternate word representation. We call this representation Syn-Low, and 2) head percolation rules are used to label each non-terminal in the parse such that each non-terminal has a unique daughter labeled as head. Each word</context>
<context position="10537" citStr="Koo et al., 2008" startWordPosition="1725" endWordPosition="1728">get a cluster: loss, time, profit, earnings, performance, rating, ... which are head words of the noun phrases that appear as direct object of verbs like improve. 5 Experimental Results The experiments were done on the English Penn Treebank, using standard head-percolation rules (Yamada and Matsumoto, 2003) to convert the phrase structure into dependency trees. We split the Treebank into a training set (Sections 2-21), a development set (Section 22), and test sets (Sections 0, 1, 23, and 24). All our experimental settings match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005; Koo et al., 2008). POS tags for the development and test data were assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus. To generate part of speech tags for the training data, we used 20- way jackknifing, i.e. we tagged each fold with the tagger trained on the other 19 folds. We set model weights αk in Eqn (2) to one for all experiments. Syntactic State-Splitting The sentence-specific word clusters are derived from the parse trees using 712 Berkeley parser1, which generates phrase-structure dency, Fig. 2(a) shows the error rate for each depenparse trees with split </context>
<context position="12605" citStr="Koo et al., 2008" startWordPosition="2076" endWordPosition="2079">l to plementation of the (Brown et al., 1992) algorithm |i — j|. We see that different models are experts on on the BLLIP corpus (Charniak et al., 2000) which different lengths (Syn-Low on 8, Syn-High on 9), contains —43M words of Wall Street Journal text.2 while the ensemble model can always combine their This produces a hierarchical clustering over the expertise and do better at each length. words which is then sliced at a certain height to ob- 6 Comparison to Related Work tain the clusters. In our experiments we use the clus- Several ensemble models have been proposed for ters obtained in (Koo et al., 2008)3, but were unable dependency parsing (Sagae and Lavie, 2006; Hall et to match the accuracy reported there, perhaps due to al., 2007; Nivre and McDonald, 2008; Attardi and additional features used in their implementation not Dell’Orletta, 2009; Surdeanu and Manning, 2010). described in the paper.4 Essentially, all of these approaches combine difResults Table 1 presents our results for each ferent dependency parsing systems, i.e. transitionmodel on each test set. In this table, the baseline based and graph-based. Although graph-based mod(first column) does not use any cluster-based fea- els are</context>
<context position="15466" citStr="Koo et al., 2008" startWordPosition="2513" endWordPosition="2516"> space of the ensemble. For each dependent to head depen- the base models. This is in contrast to previous work which combine the best parse trees suggested by the individual base-models to generate a final parse tree, i.e. a two-phase inference scheme. 7 Conclusion We presented an ensemble of different dependency parsing models, each model corresponding to a dif1code.google.com/p/berkeleyparser 2Sentences of the Penn Treebank were excluded from the text used for the clustering. 3people.csail.mit.edu/maestro/papers/bllip-clusters.gz 4Terry Koo was kind enough to share the source code for the (Koo et al., 2008) paper with us, and we plan to incorporate all the features in our future work. 713 ferent syntactic/semantic word clustering annotation. The ensemble obtains consistent improvements in unlabeled dependency parsing, e.g. from 90.82% to 92.13% for Sec. 23 of the Penn Treebank. Our error analysis has revealed that each syntactic/semantic parsing model is an expert in capturing different dependency lengths, and the ensemble model can always combine their expertise and do better at each dependency length. We can incrementally add a large number models using different clustering algorithms, and our</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semisupervised dependency parsing. In Proc. ofACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A Rush</author>
<author>M Collins</author>
<author>T Jaakkola</author>
<author>D Sontag</author>
</authors>
<title>Dual decomposition for parsing with nonprojective head automata.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="4683" citStr="Koo et al., 2010" startWordPosition="717" endWordPosition="720">e Berkeley parser output. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. For root IN-1 PP-2 0111 Japan NNP-19 NP-10 0110 , ,-0 ,-0 0010 improves VBZ-1 S-14 0101 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 the DT-15 DT-15 1101 trend NN-23 NP-18 1010 to TO-0 TO-0 0011 access NN-13 NP-24 0011 plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifier</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>T. Koo, A. Rush, M. Collins, T. Jaakkola, and D. Sontag. 2010. Dual decomposition for parsing with nonprojective head automata. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>J Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CONLL.</booktitle>
<marker>McDonald, Nivre, 2007</marker>
<rawString>R. McDonald and J. Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proc. of EMNLP-CONLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="3364" citStr="McDonald and Pereira, 2006" startWordPosition="495" endWordPosition="498">cross all our test sets. 2 Dependency Parsing A dependency tree represents the syntactic structure of a sentence with a directed graph (Figure 1), where nodes correspond to the words, and arcs indicate head-modifier pairs (Mel’ˇcuk, 1987). Graph-based dependency parsing searches for the highest-scoring tree according to apart-factored scoring function. In the first-order parsing models, the parts are individual head-modifier arcs in the dependency tree (McDonald et al., 2005). In the higher-order models, the parts consist of arcs together with some context, e.g. the parent or the sister arcs (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). With a linear scoring function, the parse for a sentence s 1: PARSE(s) = arg max w · f(s, r) (1) tET(s) rEt where T (s) is the space of dependency trees for s, and f(s, r) is the feature vector for the part r which is linearly combined using the model parameter w to give the part score. The above arg max search for non-projective dependency parsing is accom710 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710–714, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Lingui</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2356" citStr="McDonald et al., 2005" startWordPosition="342" endWordPosition="345">tactic cluster loss, time, profit, earnings, performance, rating, ...: all head words of noun phrases corresponding to cluster of direct objects of Marzieh Razavi and Anoop Sarkar School of Computing Science Simon Fraser University Vancouver, Canada {mrazavi,anoop}@cs.sfu.ca verbs like improve. In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). This paper makes two contributions: 1) We combine together multiple word representations based on semantic and syntactic clusters in order to improve discriminative dependency parsing in the MSTParser framework (McDonald et al., 2005), and 2) We provide an ensemble method for combining diverse clustering algorithms that is the discriminative parsing analog to the generative product of experts model for parsing described in (Petrov, 2010). These two contributions combined significantly improves unlabeled dependency accuracy: 90.82% to 92.13% on Sec. 23 of the Penn Treebank, and we see consistent improvements across all our test sets. 2 Dependency Parsing A dependency tree represents the syntactic structure of a sentence with a directed graph (Figure 1), where nodes correspond to the words, and arcs indicate head-modifier pa</context>
<context position="5206" citStr="McDonald et al., 2005" startWordPosition="800" endWordPosition="803">orithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al., 2008) for the list of cluster-based feature templates. The clusters inject long distance syntactic or semantic information into the model (in contrast with the use of POS tags in the baseline) and help alleviate the sparse data problem for complex features that include n-grams. 3 The Ensemble Model A word can have different syntactic or semantic cluster representations, each of which may lead to a different parsing model. We use e</context>
<context position="10518" citStr="McDonald et al., 2005" startWordPosition="1720" endWordPosition="1724">terminal NP-24 then we get a cluster: loss, time, profit, earnings, performance, rating, ... which are head words of the noun phrases that appear as direct object of verbs like improve. 5 Experimental Results The experiments were done on the English Penn Treebank, using standard head-percolation rules (Yamada and Matsumoto, 2003) to convert the phrase structure into dependency trees. We split the Treebank into a training set (Sections 2-21), a development set (Section 22), and test sets (Sections 0, 1, 23, and 24). All our experimental settings match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005; Koo et al., 2008). POS tags for the development and test data were assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus. To generate part of speech tags for the training data, we used 20- way jackknifing, i.e. we tagged each fold with the tagger trained on the other 19 folds. We set model weights αk in Eqn (2) to one for all experiments. Syntactic State-Splitting The sentence-specific word clusters are derived from the parse trees using 712 Berkeley parser1, which generates phrase-structure dency, Fig. 2(a) shows the error rate for each depenpars</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mel’ˇcuk</author>
</authors>
<title>Dependency syntax: theory and practice.</title>
<date>1987</date>
<publisher>Press.</publisher>
<institution>State University of New York</institution>
<marker>Mel’ˇcuk, 1987</marker>
<rawString>I. Mel’ˇcuk. 1987. Dependency syntax: theory and practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>R McDonald</author>
</authors>
<title>Integrating graphbased and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="12763" citStr="Nivre and McDonald, 2008" startWordPosition="2102" endWordPosition="2105">) which different lengths (Syn-Low on 8, Syn-High on 9), contains —43M words of Wall Street Journal text.2 while the ensemble model can always combine their This produces a hierarchical clustering over the expertise and do better at each length. words which is then sliced at a certain height to ob- 6 Comparison to Related Work tain the clusters. In our experiments we use the clus- Several ensemble models have been proposed for ters obtained in (Koo et al., 2008)3, but were unable dependency parsing (Sagae and Lavie, 2006; Hall et to match the accuracy reported there, perhaps due to al., 2007; Nivre and McDonald, 2008; Attardi and additional features used in their implementation not Dell’Orletta, 2009; Surdeanu and Manning, 2010). described in the paper.4 Essentially, all of these approaches combine difResults Table 1 presents our results for each ferent dependency parsing systems, i.e. transitionmodel on each test set. In this table, the baseline based and graph-based. Although graph-based mod(first column) does not use any cluster-based fea- els are globally trained and can use exact inference tures, the next three models use cluster-based fea- algorithms, their features are defined over a limtures using</context>
<context position="14249" citStr="Nivre and McDonald, 2008" startWordPosition="2332" endWordPosition="2335">s to combine these two types of As Table 1 shows, the ensemble model has out- models to exploit their complementary strengths. performed the baseline and individual models in al- The base parsing models are either independently most all cases. Among the individual models, the trained (Sagae and Lavie, 2006; Hall et al., 2007; model with Brown semantic clusters clearly outper- Attardi and Dell’Orletta, 2009; Surdeanu and Manforms the baseline, but the two models with syntac- ning, 2010), or their training is integrated, e.g. using tic clusters perform almost the same as the baseline. stacking (Nivre and McDonald, 2008; Attardi and The ensemble model outperforms all of the individ- Dell’Orletta, 2009; Surdeanu and Manning, 2010). ual models and does so very consistently across both Our work is distinguished from the aforemenfirst-order and second-order dependency models. tioned works in two dimensions. Firstly, we comError Analysis To better understand the contri- bine various graph-based models, constructed using bution of each model to the ensemble, we take a different syntactic/semantic clusters. Secondly, we closer look at the parsing errors for each model and do exact inference on the shared hypothesis</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>J. Nivre and R. McDonald. 2008. Integrating graphbased and transition-based dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL.</booktitle>
<contexts>
<context position="2120" citStr="Petrov et al., 2006" startWordPosition="309" endWordPosition="312">, request, memo, ... while another may contain people, customers, employees, students, .... Another clustering view that is more “syntactic” in nature comes from the use of statesplitting in PCFGs. For instance, we could extract a syntactic cluster loss, time, profit, earnings, performance, rating, ...: all head words of noun phrases corresponding to cluster of direct objects of Marzieh Razavi and Anoop Sarkar School of Computing Science Simon Fraser University Vancouver, Canada {mrazavi,anoop}@cs.sfu.ca verbs like improve. In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). This paper makes two contributions: 1) We combine together multiple word representations based on semantic and syntactic clusters in order to improve discriminative dependency parsing in the MSTParser framework (McDonald et al., 2005), and 2) We provide an ensemble method for combining diverse clustering algorithms that is the discriminative parsing analog to the generative product of experts model for parsing described in (Petrov, 2010). These two contributions combined significantly improves unlabeled dependency accuracy: 90.82% to 92.13% on Sec. 23 of the Penn Treebank, and we see consist</context>
<context position="7400" citStr="Petrov et al., 2006" startWordPosition="1177" endWordPosition="1180">eems more scalable though, since we can incrementally add a large number of clustering algorithms into the ensemble. 4 Syntactic and Semantic Clustering In our ensemble model we use three different clustering methods to obtain three types of word representations that can help alleviate sparse data in a dependency parser. Our first word representation is exactly the same as the one used in (Koo et al., 2008) where words are clustered using the Brown algorithm (Brown et al., 1992). Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al., 2006). Split non-terminals from the Berkeley parser output are converted into cluster identifiers in two different ways: 1) the split POS tags for each word are used as an alternate word representation. We call this representation Syn-Low, and 2) head percolation rules are used to label each non-terminal in the parse such that each non-terminal has a unique daughter labeled as head. Each word is assigned a cluster identifier which is defined as the parent split non-terminal of that word if it is not marked as head, else if the parent is marked as head we recursively check its parent until we reach </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
</authors>
<title>Products of random latent variable grammars.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="2563" citStr="Petrov, 2010" startWordPosition="377" endWordPosition="378">aser University Vancouver, Canada {mrazavi,anoop}@cs.sfu.ca verbs like improve. In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). This paper makes two contributions: 1) We combine together multiple word representations based on semantic and syntactic clusters in order to improve discriminative dependency parsing in the MSTParser framework (McDonald et al., 2005), and 2) We provide an ensemble method for combining diverse clustering algorithms that is the discriminative parsing analog to the generative product of experts model for parsing described in (Petrov, 2010). These two contributions combined significantly improves unlabeled dependency accuracy: 90.82% to 92.13% on Sec. 23 of the Penn Treebank, and we see consistent improvements across all our test sets. 2 Dependency Parsing A dependency tree represents the syntactic structure of a sentence with a directed graph (Figure 1), where nodes correspond to the words, and arcs indicate head-modifier pairs (Mel’ˇcuk, 1987). Graph-based dependency parsing searches for the highest-scoring tree according to apart-factored scoring function. In the first-order parsing models, the parts are individual head-modif</context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>S. Petrov. 2010. Products of random latent variable grammars. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proc. of EMNLP. K. Sagae</booktitle>
<contexts>
<context position="10625" citStr="Ratnaparkhi, 1996" startWordPosition="1742" endWordPosition="1743">rds of the noun phrases that appear as direct object of verbs like improve. 5 Experimental Results The experiments were done on the English Penn Treebank, using standard head-percolation rules (Yamada and Matsumoto, 2003) to convert the phrase structure into dependency trees. We split the Treebank into a training set (Sections 2-21), a development set (Section 22), and test sets (Sections 0, 1, 23, and 24). All our experimental settings match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005; Koo et al., 2008). POS tags for the development and test data were assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus. To generate part of speech tags for the training data, we used 20- way jackknifing, i.e. we tagged each fold with the tagger trained on the other 19 folds. We set model weights αk in Eqn (2) to one for all experiments. Syntactic State-Splitting The sentence-specific word clusters are derived from the parse trees using 712 Berkeley parser1, which generates phrase-structure dency, Fig. 2(a) shows the error rate for each depenparse trees with split syntactic categories. To gen- dent grouped by a coarse POS tag (c.f. (McDonald erate par</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proc. of EMNLP. K. Sagae and A. Lavie. 2006. Parser combination by reparsing. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="4664" citStr="Smith and Eisner, 2008" startWordPosition="713" endWordPosition="716">it non-terminals from the Berkeley parser output. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. For root IN-1 PP-2 0111 Japan NNP-19 NP-10 0110 , ,-0 ,-0 0010 improves VBZ-1 S-14 0101 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 the DT-15 DT-15 1101 trend NN-23 NP-18 1010 to TO-0 TO-0 0011 access NN-13 NP-24 0011 plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>D. A. Smith and J. Eisner. 2008. Dependency parsing by belief propagation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>C Manning</author>
</authors>
<title>Ensemble models for dependency parsing: Cheap and good?</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="12877" citStr="Surdeanu and Manning, 2010" startWordPosition="2117" endWordPosition="2120">e the ensemble model can always combine their This produces a hierarchical clustering over the expertise and do better at each length. words which is then sliced at a certain height to ob- 6 Comparison to Related Work tain the clusters. In our experiments we use the clus- Several ensemble models have been proposed for ters obtained in (Koo et al., 2008)3, but were unable dependency parsing (Sagae and Lavie, 2006; Hall et to match the accuracy reported there, perhaps due to al., 2007; Nivre and McDonald, 2008; Attardi and additional features used in their implementation not Dell’Orletta, 2009; Surdeanu and Manning, 2010). described in the paper.4 Essentially, all of these approaches combine difResults Table 1 presents our results for each ferent dependency parsing systems, i.e. transitionmodel on each test set. In this table, the baseline based and graph-based. Although graph-based mod(first column) does not use any cluster-based fea- els are globally trained and can use exact inference tures, the next three models use cluster-based fea- algorithms, their features are defined over a limtures using different clustering algorithms, and the ited history of parsing decisions. Since transitionlast column is our en</context>
<context position="14361" citStr="Surdeanu and Manning, 2010" startWordPosition="2348" endWordPosition="2351">entary strengths. performed the baseline and individual models in al- The base parsing models are either independently most all cases. Among the individual models, the trained (Sagae and Lavie, 2006; Hall et al., 2007; model with Brown semantic clusters clearly outper- Attardi and Dell’Orletta, 2009; Surdeanu and Manforms the baseline, but the two models with syntac- ning, 2010), or their training is integrated, e.g. using tic clusters perform almost the same as the baseline. stacking (Nivre and McDonald, 2008; Attardi and The ensemble model outperforms all of the individ- Dell’Orletta, 2009; Surdeanu and Manning, 2010). ual models and does so very consistently across both Our work is distinguished from the aforemenfirst-order and second-order dependency models. tioned works in two dimensions. Firstly, we comError Analysis To better understand the contri- bine various graph-based models, constructed using bution of each model to the ensemble, we take a different syntactic/semantic clusters. Secondly, we closer look at the parsing errors for each model and do exact inference on the shared hypothesis space of the ensemble. For each dependent to head depen- the base models. This is in contrast to previous work </context>
</contexts>
<marker>Surdeanu, Manning, 2010</marker>
<rawString>M. Surdeanu and C. Manning. 2010. Ensemble models for dependency parsing: Cheap and good? In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D West</author>
</authors>
<title>Introduction to Graph Theory.</title>
<date>2001</date>
<publisher>Prentice Hall,</publisher>
<note>2nd editoin.</note>
<contexts>
<context position="4604" citStr="West, 2001" startWordPosition="707" endWordPosition="708">ee with cluster identifiers obtained from the split non-terminals from the Berkeley parser output. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. For root IN-1 PP-2 0111 Japan NNP-19 NP-10 0110 , ,-0 ,-0 0010 improves VBZ-1 S-14 0101 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 the DT-15 DT-15 1101 trend NN-23 NP-18 1010 to TO-0 TO-0 0011 access NN-13 NP-24 0011 plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 200</context>
</contexts>
<marker>West, 2001</marker>
<rawString>D. West. 2001. Introduction to Graph Theory. Prentice Hall, 2nd editoin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="10228" citStr="Yamada and Matsumoto, 2003" startWordPosition="1672" endWordPosition="1675"> ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! Baseline Brown Syn−Low Syn−High Ensemble ! ! ! ! ! ! For the Berkeley parser output shown above, the resulting word representations and dependency tree is shown in Fig. 1. If we group all the head-words in the training data that project up to split non-terminal NP-24 then we get a cluster: loss, time, profit, earnings, performance, rating, ... which are head words of the noun phrases that appear as direct object of verbs like improve. 5 Experimental Results The experiments were done on the English Penn Treebank, using standard head-percolation rules (Yamada and Matsumoto, 2003) to convert the phrase structure into dependency trees. We split the Treebank into a training set (Sections 2-21), a development set (Section 22), and test sets (Sections 0, 1, 23, and 24). All our experimental settings match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005; Koo et al., 2008). POS tags for the development and test data were assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus. To generate part of speech tags for the training data, we used 20- way jackknifing, i.e. we tagged each fold with the tagger trained on the o</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. of IWPT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>