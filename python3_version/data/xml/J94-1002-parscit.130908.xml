<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990778666666667">
A Hierarchical Stochastic Model for
Automatic Prediction of Prosodic
Boundary Location
</title>
<author confidence="0.989553">
M. Ostendorf.
</author>
<affiliation confidence="0.970868">
Boston University
</affiliation>
<author confidence="0.986973">
N. Veilleux&apos;r
</author>
<affiliation confidence="0.934198">
Boston University
</affiliation>
<bodyText confidence="0.990606583333333">
Prosodic phrase structure provides important information for the understanding and naturalness
of synthetic speech, and a good model of prosodic phrases has applications in both speech synthesis
and speech understanding. This work describes a statistical model of an embedded hierarchy of
prosodic phrase structure, motivated by results in linguistic theory. Each level of the hierarchy is
modeled as a sequence of subunits at the next level, with the lowest level of the hierarchy repre-
senting factors such as syntactic branching and prosodic constituent length using a binary tree
classification. A maximum likelihood solution for parameter estimation is presented, allowing
automatic training of different speaking styles. For predicting prosodic phrase breaks from text,
a dynamic programming algorithm is given for finding the maximum probability prosodic parse.
Experimental results on a corpus of radio news demonstrate a high rate of success for predict-
ing major and minor phrase boundaries from text without syntactic information (81% correct
prediction with 4% false prediction).
</bodyText>
<sectionHeader confidence="0.992086" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.998879294117647">
Prosodic phrase structure plays a role in both naturalness and intelligibility of speech.
For example, prosodic phrase boundaries break the flow of a sentence, dividing it into
smaller units for easier processing. In addition, researchers have shown that prosodic
phrase break placement is important in syntactic disambiguation (Lehiste 1973; Price,
Ostendorf, Shattuck-Hufnagel, and Fong 1991). For these reasons, computational mod-
eling of prosodic phrases is important both for text-to-speech synthesis and speech
understanding applications. In this work, we present a computational model that rep-
resents a hierarchy of prosodic constituents using a stochastic formalism to capture the
natural variability allowable in prosodic phrasing. The model is useful for both anal-
ysis and synthesis applications; we focus on synthesis here, and present experimental
results for predicting prosodic phrase structure from text.
Prosodic phrase structure, or groupings of words in a sentence, can be equiva-
lently represented by different phrase break markers. The location and relative size
of these breaks define the prosodic phrase structure, which we will refer to here as
a prosodic parse. Prosodic phrase breaks are discrete events that are associated with
acoustic cues such as duration lengthening, pause insertion, and intonation markers.
In this work, we are concerned only with the relationship between the abstract events
</bodyText>
<footnote confidence="0.595819">
ECS Department, 44 Cummington Street, Boston, MA 02215
</footnote>
<note confidence="0.870074">
C) 1994 Association for Computational Linguistics
Computational Linguistics Volume 20, Number]
</note>
<bodyText confidence="0.93403693877551">
(different levels of phrase breaks) and text. To be useful in synthesis or understanding
applications, the results presented here need to be integrated with a component that
models the acoustics associated with these abstract events (see, for example, Hirose
and Fujisaki 1982).
Several observations about prosodic phrase breaks raise issues to be considered
in designing an algorithm to predict such breaks from text. First, there is a signifi-
cant body of literature in linguistics concerning various hierarchies that specify the
relationship among prosodic constituents, and the model should reflect this structure.
Second, several different prosodic parses may all be acceptable for one sentence. This
variability is particularly important to represent if the model is to be useful for anal-
ysis as well as synthesis. Third, prosodic phrase breaks do not always coincide with
syntactic phrase boundaries, and the relationship between prosody and syntax is not
well understood. This means that prosodic phrases cannot simply be predicted from
syntactic structure. Finally, since most text-to-speech synthesis applications require a
low cost implementation, there is the concern of computational complexity. We shall
expand on these points separately below, to motivate the work described here.
The various linguistic theories of prosodic phrase structure (e.g., Liberman and
Prince 1977; Selkirk 1980, 1984; Beckman and Pierrehumbert 1986; Nespor and Vogel
1983; Ladd 1986) differ in the specific levels that they represent, but all have a similar
hierarchical structure. Two levels of prosodic phrases are common to most propos-
als: the intonational phrase and the intermediate phrase, using the terminology of
Beckman and Pierrehumbert. A sentence is composed of a sequence of intonational
phrases, which in turn are composed of sequences of intermediate phrases. An in-
tonational phrase break is therefore perceived as stronger or more salient than an
intermediate phrase break. Intonational phrases are delimited by boundary tones, and
intermediate phrases are theoretically marked with a phrase accent, where the pitch
markers can be either high or low (Beckman and Pierrehumbert 1986). (In other the-
ories of intonation, for example, t&apos;Hart, Collier, and Cohen [1990], pitch markers also
occur at phrase boundaries, but are identified with movement and referred to as either
rising or falling.) Both types of constituents are also cued by segmental lengthening in
the phrase final syllable (Wightman, Shattuck-Hufnagel, Ostendorf, and Price 1992).
Since intonational and intermediate phrases are generally accepted, the experiments
here will only address these two levels, referring to them as major and minor phrases,
respectively. However, other types of prosodic constituents may be useful and, in fact,
there is durational evidence for at least four levels (Wightman, Shattuck-Hufnagel, Os-
tendorf, and Price 1991; Ladd and Campbell 1991). We therefore propose a more gen-
eral hierarchical model that can be extended to an arbitrary, but fixed, number of levels.
In the examples given here, we will represent intonational phrases (I) using &amp;quot;II&amp;quot; to
mark a major break and intermediate phrases (i) using &amp;quot;I&amp;quot; to mark a minor break. The
example below illustrates how phrase breaks are used to represent prosodic phrase
structure:
Those on early release I must check in with correction officials II
fifty times a week 11 according to Ash, hi
who says about half I the contacts for a select group
will now be made I by the computerized phone calls. I
((Those on early release)i (must check in with correction officials),)i
((fifty times a week),), ((according to Ash,)1)1
((who says about halpi (the contacts for a select group),),
((will now be made); (by the computerized phone calls.);),
</bodyText>
<page confidence="0.991421">
28
</page>
<note confidence="0.910695">
M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction
</note>
<bodyText confidence="0.998872777777778">
Another important consideration in modeling prosody (and evaluating the model)
is that prosodic phrase structure is not deterministic. Speakers can produce a sentence
in several ways without altering the naturalness or the meaning. Prosodic breaks can
differ in size and/or placement because of differences in style, competence, or simply
natural speaking variations. For example, the following sentence was said three ways
by five speakers:
They&apos;re in jail for such things II as bad checks or stealing.
They&apos;re in jail for such things as bad checks or stealing.
They&apos;re in jail I for such things as bad checks or stealing.
Although deterministic rules can be used to predict phrase breaks for speech syn-
thesis applications, such a model will be limited in its usefulness in speech analysis.
In addition, speech synthesis might be more natural if variability is included in the
model. Here, a stochastic model is used to represent the natural variability in prosodic
structure by deriving probabilities of phrase breaks, rather than predicting locations
of phrase breaks by rule.
The relationship between prosody and syntax is not fully understood, though it
is generally accepted that there is such a relationship. For example, relatively higher
syntactic attachment usually corresponds to relatively larger prosodic breaks, but there
are many exceptions, as in:
(iMaryinp [was amazed [Ann Dewey was angrylnipis
which was produced by four speakers as
Mary was amazed H Ann Dewey was angry.
In an analysis of the Londonâ€”Lund corpus, Altenberg (1987) finds relative frequencies
that describe the correspondence between prosodic constituents (tone units) and dif-
ferent syntactic units. This data supports the use of a probabilistic model, which also
has an advantage in that it can be trained automatically, facilitating representation of
a wide variety of speaking styles and allowing a means of discovering syntax-prosody
relationships from a large corpus. One reason that the mapping between syntax and
prosody is not simple is because, in speech, the constraints of syntactic structure and
phrase length are balanced to produce a regular, roughly equal, sequence of prosodic
phrases (Gee and Grosjean 1983). Consequently, we include constituent length as a
factor in the model.
The cost of obtaining a full and accurate syntactic parse can be high, which presents
difficulties for text-to-speech synthesis systems. In addition, a full syntactic parse may
not be necessary for predicting prosodic phrases, since prosody is not directly related to
syntax. Consequently, we investigate computation/performance trade-offs associated
with using a skeletal syntactic parse vs. simple part-of-speech (POS) assignments.
To summarize, the model proposed here addresses several issues in modeling
prosodic phrase structure. The model is a general formalism for an embedded hier-
archy, which we specifically apply to represent sentences, major phrases, and minor
phrases. In order to account for the allowable variability in prosodic parsing, the model
is probabilistic. The structure of the model allows use of grammatical information such
as part-of-speech labels, syntactic structure and constituent length, but the specific pa-
rameters are trained automatically. Finally, computational complexity trade-offs are
investigated by evaluating the algorithm with and without syntactic cues.
</bodyText>
<page confidence="0.997713">
29
</page>
<note confidence="0.92547">
Computational Linguistics Volume 20, Number 1
</note>
<bodyText confidence="0.999304888888889">
The remainder of the paper is organized as follows. We begin, in Section 2, by
discussing past work in predicting prosodic phrase breaks from text for speech syn-
thesis. In Section 3, we introduce the probabilistic formalism of the hierarchical model
and outline the implementation: text pre-processing, parameter estimation, and phrase
break prediction using a dynamic programming algorithm to obtain the most likely
prosodic parse. In Section 4, we present experimental results for prediction of major
and minor prosodic phrase breaks based on a corpus of FM radio news stories. Finally,
we conclude in Section 5 by discussing possible implications and extensions of these
results.
</bodyText>
<sectionHeader confidence="0.982742" genericHeader="related work">
2. Previous Work
</sectionHeader>
<bodyText confidence="0.999916205128205">
Initial attempts to incorporate prosody in speech synthesis involved determining into-
nation and duration patterns as a function of syntactic phrase structure (Allen, Hunni-
cutt, Carlson, and Granstrom 1979; Allen, Hunnicutt, and Klatt 1987), which requires
syntactic parsing. More recently, researchers have attempted to address the fact that
prosody and syntax are not directly related by explicitly predicting prosodic phrase
boundaries rather than using syntactic clause boundaries. An important difference be-
tween these subsequent approaches is in the amount of syntactic information used to
predict prosodic boundaries. The algorithms reflect different assumptions about the
relationship between prosody and syntax, as well as different levels of computational
complexity. Clearly, a greater use of syntactic information will require more computa-
tion for finding a more detailed syntactic parse.
One approach is based on the idea that a prosodic parse may not require a full
syntactic parse and that detailed part-of-speech information (e.g., noun, verb, deter-
miner) may not be necessary for generating a prosodic parse. Sorin, Larreur, and Llorca
(1987) proposed a simple prosodic parser for French based on content/function word
classification to determine prosodic constituents referred to as prosodic groups. The
length and relative location of these prosodic groups is then used to determine phrase
break locations that are marked with a pause. Our earlier work drew on this scheme
for predicting phrase boundaries in English: a Markov model was developed to pre-
dict phrase breaks by representing the sequence of prosodic groups and breaks as a
Markov chain (Veilleux, Ostendorf, Price, and Shattuck-Hufnagel 1990). An advantage
of these approaches is that they only require a small dictionary of function words to as-
sign part-of-speech labels. Motivated by similar principles and using only a 300-word
dictionary, O&apos;Shaughnessy (1989) proposes a somewhat more sophisticated parser for
English based on function word identification, number agreement, and suffix identi-
fication. O&apos;Shaughnessy&apos;s work differs from the other approaches in that his goal is a
syntactic parse, though not complete, and he does not address the issue of differences
between prosody and syntax.
At the other end of the spectrum are approaches based on the hypothesis that
prosodic phrase boundaries can be predicted by rule from a full syntactic parse. Gee
and Grosjean (1983) developed a rule-based system, called the Phi Algorithm, to pre-
dict psycholinguistic &amp;quot;performance structures&amp;quot; that are represented by assigning an
integer number corresponding to boundary salience between each pair of words. Con-
stituent length information is incorporated primarily through the application of their
verb balancing rule, which splits the verb phrase and groups the verb with either the
previous or subsequent material, subject to syntactic constraints. Gee and Grosjean
developed their Phi Algorithm only to predict performance structures. However, their
work has been extended to prosodic phrase prediction for speech synthesis applica-
tions by Bachenko and Fitzpatrick (1990), who explicitly find prosodic phrase breaks
</bodyText>
<page confidence="0.99268">
30
</page>
<note confidence="0.577339">
M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction
</note>
<bodyText confidence="0.999580192307693">
from derived boundary salience indices. They relax many of the constraints on the
use of the verb rule and propose a Verb Adjacency Rule, so their algorithm requires
a fairly detailed parse, although not a complete one. One of the relaxed constraints
obviates the need for clause information. Altenberg (1987) has also proposed an algo-
rithm for prediction of phrase boundary locations (specifically, tone unit boundaries
for British English) by rule from syntactic structure and semantic information. How-
ever, the detailed information required for the algorithm cannot currently be acquired
automatically from text.
Departing from these approaches, Wang and Hirschberg (1992) have recently used
binary decision trees to predict the presence or absence of a prosodic break at each
word boundary in a sentence. They consider a range of input variables, including text-
derived information such as detailed POS labels and syntactic constituent structure,
and in some experiments, acoustic information. POS labels were given by Church&apos;s
tagger (Church 1988) and syntactic constituents by Hindle&apos;s parser (Hindle 1987). The
acoustic information (previous boundary location, pitch accent location, and phrase
duration), which was based on hand-labeled prosodic markers, did not improve per-
formance but resulted in a much smaller tree for prediction.
All of these approaches have influenced the model proposed here. For example, we
investigate simple content/function word POS assignment, as in Sorin, Larreur, and
Llorca (1987). Like Wang and Hirschberg (1992), we use decision trees to automatically
determine the important factors influencing phrase break location. In addition, all of
the above works have influenced the choice of factors and questions incorporated
in the decision tree. Two important differences in our approach include a stochastic
model to capture variability and an explicit representation of a linguistically motivated
hierarchy. Of course, whether it is effective and/or efficient for a computational model
to reflect a linguistic hierarchy is an empirical question.
</bodyText>
<sectionHeader confidence="0.952179" genericHeader="method">
3. Hierarchical Model of Prosodic Phrases
</sectionHeader>
<bodyText confidence="0.999985333333333">
A prosodic parse of a sentence can be represented by a sequence of break indices, one
index following each word, which code the level of bracketing or attachment in a tree.
A prosodic parse .5 is therefore given by
</bodyText>
<equation confidence="0.449603">
S=
</equation>
<bodyText confidence="0.999761333333333">
where bi is the break index after the ith word and L is the number of words in the
sentence. A break is a random variable that can take on one of a finite number of
values from no break&amp;quot; (orthographic word boundary, but not a prosodic constituent
boundary) to &amp;quot;sentence boundary,&amp;quot; where the values form an ordered set that corre-
spond to the different levels of the hierarchy. Below we consider a stochastic model
for first a general hierarchical prosodic parse (any specified number of levels), and
then specifically for the three-level case that models a sentence as a sequence of major
phrases, which are in turn modeled as a sequence of minor phrases. Although most
phonological theories do not recognize the &amp;quot;sentence&amp;quot; as a unit, it is useful for both
synthesis and recognition applications to model sentences separately, as sentence-final
boundaries tend to be acoustically different from sentence-internal boundaries (e.g., a
low boundary tone is much more likely).1
</bodyText>
<footnote confidence="0.999487">
1 We have chosen to use the term &amp;quot;sentence&amp;quot; rather than the more general term &amp;quot;utterance,&amp;quot; since the
algorithm is designed to predict boundaries from text that in our data and in many applications
</footnote>
<page confidence="0.999669">
31
</page>
<note confidence="0.661905">
Computational Linguistics Volume 20, Number I
</note>
<bodyText confidence="0.99991975">
We will begin by presenting the mathematical structure, first generally and then
specifically as a three-level embedded hierarchy. Next, some pragmatic details of text
processing are discussed, followed by a description of the parameter estimation and
phrase break prediction algorithms.
</bodyText>
<subsectionHeader confidence="0.999153">
3.1 Stochastic Model
</subsectionHeader>
<bodyText confidence="0.998070428571428">
We assume the relationship between units and subunits will hold at any level of the
hierarchy. Therefore, in describing the general case, we need only consider one level of
embedding and will use U, and u,i when referring to units and subunits, respectively,
at some unspecified level of the hierarchy. Using this notation, the probability of a unit
111 is parameterized in terms of the probability of the sequence of subunits u,i (on the
next lower level) and the length ni in subunits of that sequence given the orthographic
transcription of the sentence W:
</bodyText>
<equation confidence="0.99649">
p(1-1; IN) = p(ui , â€¢
P(Uil â€¢ â€¢ )1â€˜), ni)P(nil)&apos;V)
ni
p(nilvv)p(uiliw)Hpoli;
j=2
</equation>
<bodyText confidence="0.827699428571429">
The specific hierarchy considered here involves representing the prosodic parse of
a sentence S as an N-length sequence of major phrases /A:
5= (A/11, ,MN),
A major phrase Mi is composed of a n,-length sequence of minor phrases
Mi = , , â€¢ , min, )â€¢
Finally, a minor phrase rn,1 is composed of a vg-length sequence of breaks bt starting
at time t(i,j) and ending at time 1(1, + 1) â€” 1,
</bodyText>
<equation confidence="0.98217">
Illij = (bt(1,1), â€¢ -
</equation>
<bodyText confidence="0.9477869375">
where t(i,j) is the time index of the first word of mu and t(i,j+ 1) â€”1 is the time index
of the final word of in, v = f(i,j + 1) â€” t(i,j) is simply the number of words in the
minor phrase, and the breaks b! take on values from the set {no break, minor break,
major break, sentence break}.
It might be useful to consider phonological words rather than orthographic words
as possible sites for break indices. This could be accomplished, without using de-
terministic rules, by specifying the bottom of the hierarchy (e.g., break level 0) to
represent locations internal to a phonological word and the next level of the hierar-
chy (e.g., break level I) to represent phonological word boundaries. However, it is
controversial as to whether phonological words can be larger or smaller than ortho-
graphic (lexical) words (Booij 1983; Nespor and Vogel 1983), so it is not clear how the
lowest level should be defined relative to the orthographic words. In this work, we
have chosen not to distinguish between these two levels, to reduce the complexity
of implementation and performance evaluation. For similar reasons, we have limited
comprises syntactically well-formed sentences. The phrase prediction model may also be useful in
speech recognition applications, in which case the term &amp;quot;utterance&amp;quot; would clearly be more appropriate.
</bodyText>
<page confidence="0.99869">
32
</page>
<note confidence="0.800871">
M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction
</note>
<tableCaption confidence="0.995617">
Table 1
</tableCaption>
<table confidence="0.694051111111111">
Histograms of number of minor phrases in a major phrase and number of major phrases in a
sentence, as a function of quantized length of the unit. The quantizer regions are indicated by
the length ranges.
Number of minor phrases in major phrase Number of major phrases in sentence
t(A/1) 1 2 3 4 5 â‚¬(5) 1 2 3 4 5 6
1-5 522 69 2 1-14 31 39 11 2 1
6-7 106 78 9 15-21 4 23 28 17 9 5
8-14 61 107 43 1 22-31 6 6 20 24 11
15-19 2 2 1 1 32-37 2 6 4
</table>
<bodyText confidence="0.998649857142857">
this study to the more universally agreed-upon levels of major and minor prosodic
phrases, although there is durational evidence that a more detailed hierarchy would
be useful (Ladd and Campbell 1991; Wightman, Shattuck-Flufnagel, Ostendorf, and
Price 1992).
In the current work, we make several simplifying assumptions due to training
limitations. First, the probability of the number of subunits in a unit p(n,ILI,) is assumed
to depend only on the number of words in the unit i(U). As is not surprising, our data
indicate that units that span a larger number of words tend to comprise more subunits.
Altenberg has noticed similar tendencies in the Londonâ€”Lund corpus (Altenberg 1987,
p. 81). (Alternatively, it has been suggested that either phonological word count or
stressed syllable count rather than orthographic word count may be a useful measure
of phrase length on the lowest level [Bachenko and Fitzpatrick 19901.) In addition,
the probability distribution is approximated by conditioning on quantized lengths
Qu(k(U,)). The quantizer varies as a function of the specific unit and is designed
using a regression tree (Breiman, Friedman, Olshen, and Stone 1984). A regression
tree partitions the data along intervals of a continuous variable, in this case length of
the unit, to decrease variance of the response variable, the number of subunits in the
unit. The resulting quantizer regions and the corresponding distribution of subunits
in a unit are given in Table 1 for major phrase and sentence units.
Using these simplifying assumptions, the constituent length probability distribu-
tions are then:
</bodyText>
<equation confidence="0.999867666666667">
p(NW) == qs (NIL) (1)
p(rzi IN) gm(nilti) (2)
p(villW) rAy) (3)
</equation>
<bodyText confidence="0.989202">
where L = Q5 (?(S)), I; = Qm(i(Mi)) and = (2/1/ )) â€¢
Next, major phrases in a sequence are assumed to be Markov given the number
in the sequence:
</bodyText>
<equation confidence="0.932031">
, â€¢ â€¢ â€¢ mi- P(MTI )4&apos;, ) â€¢
</equation>
<bodyText confidence="0.999122">
Minor phrases are also assumed to be Markov, depending only on the previous minor
phrase and the features of the major phrase it is contained in:
</bodyText>
<equation confidence="0.9859445">
p(mO/Pi, m11, ..â€¢ m1-1) = p(miif m1(,-1))
P(tnii m) â€” m(i-i)n,_,),
</equation>
<bodyText confidence="0.883571">
where Wi is the sequence of feature vectors spanning the ith major phrase. For sim-
</bodyText>
<page confidence="0.987673">
33
</page>
<note confidence="0.275859">
Computational Linguistics Volume 20, Number 1
</note>
<bodyText confidence="0.996247">
plicity, we will abbreviate the notation to:
mil , â€¢ - â€¢ , , MI-1) = illprev)â€¢
The conditional probability of a sequence of words within a minor phrase is assumed
to depend on a state determined by the (variable-length) sequence of past words and
the time of the last break, where the state is given by a decision tree as in Bahl, Brown,
deSouza, and Mercer (1989):
</bodyText>
<equation confidence="0.965314">
P(bkiWi, bt(i.j), â€¢ â€¢ â€¢ bk-1 nirrev) p(bklf (vvi,mprev)).
</equation>
<bodyText confidence="0.9915725">
Note that within a minor phrase, probabilities for only two cases, that of no break or
that of any higher level break index, are used.
Incorporating all of the above simplifying assumptions, the probability of a specific
prosodic parse is given by
</bodyText>
<equation confidence="0.985205">
P(SIW) = qS(N1L) ft0/111W,Mi-i), (4)
n,
P(Mr1W,Mi-1) (5)
t(i,j)-1-vo-1
= tim(Vii1A0 p(bk f(VVI,mpraMâ€¢
P(mii (6)
k-=t(i,j)
W used in this model is not simply the orthographic word sequence. Rather, it is a
</equation>
<bodyText confidence="0.99954665">
sequence of feature vectors, one per word extracted from the word sequence. Examples
of possible features include part-of-speech labels and syntactic information such as
bracketing labels or labels of an associated node in a syntactic tree. The decision tree
f (wi,mpreâ€ž) used in determining the probabilities p(bklf inpreu)) includes questions
based on these features, attributes of the previous minor phrase and the current major
phrase, and length in words of the sentence. Details on our specific choice of features
and questions is given in Section 4.
Our use of decision trees is different from the phrase break detection algorithm
of Wang and Hirschberg (1992), although the tree design algorithm and choice of
features is similar. The tree is not used to classify phrase breaks directly; instead it is
used to determine the probability of the occurrence of a minor break at some location,
conditioned on the decision tree structure. This probability is used to represent the
lowest level in the hierarchical model.
Previously, we mentioned two important factors affecting the placement of phrase
breaks: (1) grammatical structure and (2) length constraints on the prosodic con-
stituents such as overall length and length relative to neighboring phrases. Gram-
matical information is incorporated in the tree f(WhInp,) through questions about
the feature sequence Wi. Prosodic constituent length is modeled in two ways, through
the constituent length probability distributions and through questions about the length
of the previous phrase used in the tree f (1Vi, Mprev).
</bodyText>
<subsectionHeader confidence="0.983321">
3.2 Text Processing
</subsectionHeader>
<bodyText confidence="0.9995674">
In the experiments reported here, the feature vectors include part-of-speech labels,
punctuation and, optionally, information from a skeletal syntactic parse. The feature
extraction is described in more detail below, and an example is given in Figure 1.
Two levels of detail are considered for part-of-speech (POS) labeling. At the sim-
plest level, a function word table look-up is used to categorize words either as one of
</bodyText>
<page confidence="0.982065">
34
</page>
<bodyText confidence="0.8560141">
M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction
[VP is [AP free [PP on [NP bail]] [PP after [NP [VP facing ...
word # in sent. 5 6 7 8 9 10
word class v c P c p c
part-of-speech v adj P noun P verb
punctuation none none none none none none
left dominated same same same PP same
right dominated AP PP NP PP NP â€”
both dominated VP AP PP AP PP â€”
# init. constit. 1 1 1 1 2
# term. constit. 0 0 0 2 0 â€”
Figure
Seven features are extracted for each word in a sentence to describe the boundary between
that word and the following word. Syntactic information is based on a skeletal parse, as
shown. Part-of-speech assignment is based on a table look-up from lists of function words.
six types of function words, as a proper name (P) if capitalized, or otherwise as a con-
tent word (c). Function words are divided into several classes: conjunctions (j) (such as
and, but, if, because), auxiliary verbs and modals (v), determiners (d), prepositions (p),
pronouns (n), and a general category (g), which includes the quantifiers and function-
like adverbs such as not, no, ever, now. The POS labels given by the simple table look-up
are referred to here as &amp;quot;word classes.&amp;quot; A more detailed part-of-speech classification is
given by Penn Treebank POS tags (Marcus and Santorini 1993), which were obtained
automatically using the BBN tagger (Meteer, Schwartz, and Weischedel 1991). What
we refer to here as POS labels is actually a grouping of these classes that includes
the above function word categories, the proper name category (now determined by
the tagger rather than from capitalization), plus categories for particles (pa), nouns
(noun), verbs (verb), adjectives (adj), adverbs (adv), and all other content words (def).
Contractions are not decomposed into separate words, since it is not possible that
a phrase break will occur within the word contraction. The contraction is treated as a
single word in constituent length measures and feature extraction, and it is assigned
the POS label of its base word (left component).
Punctuation following a word is incorporated as a feature for that word. In our
data, the only punctuation that appears are commas and periods. Periods and other
sentence-final punctuation deterministically assign a sentence break. This implies some
text preprocessing that distinguishes periods used for abbreviation from sentence-final
periods. While commas often correspond to major breaks, there is a systematic excep-
tion: a series of the same syntactic units such as a series of nouns (an apple, an orange,
and a pear) or a series of adjectives (safe, cost-effective alternative ...) may or may not be
associated with a major prosodic break. Therefore, we have chosen to use commas as a
feature to determine the likelihood of a phrase break rather than as a deterministic cue
to a prosodic break. Although using commas deterministically to assign a major phrase
boundary yields better performance on our test set than using commas as a feature,
we felt that using commas as a feature was a more extensible approach, and have used
this strategy in the results reported here. Including commas as a feature does improve
performance relative to not using commas, as will be discussed in Section 4. Commas
(and other punctuation) can be very useful for prosodic boundary prediction when
they are available, and they are used in other algorithms (e.g., Allen, Hunnicutt, and
Klatt 1987; O&apos;Shaughnessy 1989; I3achenko and Fitzpatrick 1990). However, commas
are not reliably transcribed from spoken language and not consistently used in written
text, so it is important that the algorithm not depend too heavily on commas.
</bodyText>
<page confidence="0.99216">
35
</page>
<note confidence="0.630653">
Computational Linguistics Volume 20, Number I
</note>
<bodyText confidence="0.999985826086957">
Syntactic features were extracted from skeletal parses provided through a prelim-
inary version of the Penn Treebank Corpus. Since these are hand-corrected parses, the
results are indicative of the performance possible using syntactic information, but do
not reflect performance achievable with an existing parser. Several researchers have
investigated the relationship between prosody and syntax (e.g., Selkirk 1984; Gee and
Grosjean 1983; Cooper and Paccia-Cooper 1980; Altenberg 1987). Our features have
been motivated by some of these results, which suggest that some syntactic con-
stituents are more likely to be separated by a phrase break than others. However,
we have chosen to let the important constituents be determined automatically, similar
to Wang and Hirschberg (1992), rather than by rule. One feature is the highest syn-
tactic constituent dominating the left word but not dominating the right word, which
describes potential locations for phrase breaks after a specific syntactic constituent. We
also consider the similar case, the highest syntactic constituent dominating the right
word but not the left, to allow for prosodic phrase breaks that may be associated with
the beginning of a syntactic constituent. The lowest syntactic constituent that domi-
nates both words is a feature that will provide information about which constituents
are not likely to be divided by a phrase break. In addition, the number of terminat-
ing constituents and the number of initiating constituents between the two words
were included as features to investigate the influence of relative strength of syntac-
tic attachment. Eight categories of syntactic constituent were used: sentence (5), noun
phrase (NP), verb phrase (VP), prepositional phrase (PP), wh-noun phrase (VVIINP),
adjective or adverbial phrase (AP), any other constituent (0), and both words in the
same lowest level constituent (same).
</bodyText>
<subsectionHeader confidence="0.999717">
3.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999901">
An advantage of a stochastic model is that the parameters can be estimated automat-
ically from a large corpus of data, which means that it is relatively straightforward to
redesign the model to reflect a different speaking style. Here we describe a maximum
likelihood approach to parameter estimation, where model parameters are chosen to
maximize the likelihood of the training data.
We will assume that sentences are independent and identically distributed to sim-
plify parameter estimation and prediction, although the independence assumption
precludes capturing any speaker-dependent or discourse effects. In this case, the like-
lihood of the prosodic parse of a corpus of sentences (51, , ST) given parameters 0
is, from Equations (4)â€”(6),
</bodyText>
<equation confidence="0.990029333333333">
E(0) = E log p(StIW)
log P(01W,M;-1)]
log qs(Ntlf!) +
= E loggs(Ntlo+ [E logqm(10)+Elogp(n4;
t i 1
t [log qs(Ntle) +[ [log qm (fil&apos;ii0+
i
[E[logq.(,,,p;])--1-
k lipg P(bfk If 0/41Mtprw))1 lil.
</equation>
<page confidence="0.879914">
36
</page>
<bodyText confidence="0.661543">
M. Ostendort and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction
Arranging terms we have
</bodyText>
<equation confidence="0.999415285714286">
Â£(0) = log qs(NtiLt)
+ EElogqm(o)
I
EEElOgqm(141,g)
t I I
EEEE log p(b; mtpr,)). (7)
t I jk
</equation>
<bodyText confidence="0.99989652">
Since there are no cross dependencies between parameters, the four terms in Equa-
tion (7) can be maximized separately. The resulting parameter estimates for qs, qm, and
are then simply relative frequency estimates. The last term is maximized jointly with
the design of the state function f(.) using standard classification tree design techniques,
as described below.
The tree is grown using a greedy algorithm, which iteratively extends branches
by choosing the parameters of a question, the question at a node and the node in
a tree that together maximize some criterion for reducing the impurity of the class
distributions at the leaves of the tree. The tree is used to find the probability of a
minor phrase boundary, so there are only two classes: &amp;quot;break&amp;quot; and &amp;quot;no break.&amp;quot; In
this work, we have used the Gini criterion, i.e., the node distribution impurity is
given by i(t) = t)p(ilt) (Breirnan, Friedman, Olshen, and Stone 1984). Since
the relative frequency of the &amp;quot;break&amp;quot; class is so low (8% of all breaks), we include
different error costs in the design criterion. Generally, the cost of classifying a &amp;quot;break&amp;quot;
as &amp;quot;no break&amp;quot; is chosen to be three to four times higher than the opposite error,
and the specific costs for each tree are chosen to control the false prediction rate
on the training set. Initially a tree is grown using two-thirds of the training data,
and the remaining one-third of the data is used to determine a good complexity-
performance trade-off point. The complexity criterion determined at this point is then
used in pruning a second tree grown with the entire training set, in order to make
better use of the available data. Each leaf t of the tree is associated with a conditional
probability distribution of &amp;quot;break&amp;quot; vs. &amp;quot;no break&amp;quot; (actually, the relative frequency es-
timate). This probability distribution p(b t) is used in the hierarchical model for com-
puting the probability of a minor phrase, Equation (6), by running test data through
the tree and using the probability distribution associated with the final leaf node
</bodyText>
<equation confidence="0.899298">
t â€” f mpruc,) â€¢
</equation>
<subsectionHeader confidence="0.999385">
3.4 Phrase Break Prediction Algorithm
</subsectionHeader>
<bodyText confidence="0.999437636363636">
The stochastic model can be used to predict a prosodic parse for a sentence simply
by finding the most probable prosodic parse for that sequence of words, where the
probability of any given parse is determined by Equations (4)â€”(6). In other words,
we hypothesize all possible prosodic parses, compute the probability of each, and
choose the most probable. The most likely prosodic parse can be found efficiently
using a dynamic programming (DP) algorithm that is similar to algorithms used in
speech recognition, in particular that for the Stochastic Segment Model (Ostendorf
and Roukos 1989), except that the dynamic programming routine is called recursively
for successive levels in the hierarchy. Defining pr(un ...uthIVPI,U1) as the probability
of the most likely sequence of n subunits in but not necessarily spanning U, and
ending at location t, and u,i(s, t) as a subunit that spans boundaries {bs,...,bt}, the
</bodyText>
<page confidence="0.994798">
37
</page>
<note confidence="0.559308">
Computational Linguistics Volume 20, Number 1
</note>
<bodyText confidence="0.945863857142857">
dynamic programming algorithm can be expressed generally in the subroutine that
follows. This subroutine is called recursively for each level of the hierarchy, with the
lowest level constituent probability being computed using probabilities given by the
tree.
Dynamic Programming Routine for Prosodic Parse Prediction
For each word tin unit Li, (t = 1,...
Compute
</bodyText>
<equation confidence="0.924653">
Compute log Pt (1411 (1, t)
For each n-length sequence of subunits spanning [1, t] (t/ = 2, , t):
L11) = maxs&lt;t log ps(u,i - , 14;,(1-1)114),, -1)
+ log p(u(s + 1, t)111,ui(n)
</equation>
<bodyText confidence="0.954560666666667">
(Computing log p(u(s + 1t) Wi,u,(â€ž_i)) with a recursive call to this routine.)
Save pointers to best previous break location s.
To find the most likely sequence,
</bodyText>
<equation confidence="0.718175">
max, log pi, ( Ai, â€¢ â€¢ L-1;-1) logq(n Ii)
</equation>
<bodyText confidence="0.9999673">
The final step is to decode the sequence of breaks once the value n* that maximizes
the above equation is determined. Using the n* associated with any level unit, we can
trace back to find the optimal segmentation of subunits that comprise that unit. The
complete parse is found by tracing back at the highest level units and successively
tracing back in each lower level.
For the specific case of a three-level hierarchy, the most likely major phrase se-
quence in a sentence p(51W) and the most likely minor phrase sequence in a major
phrase p(M, W) are found by a dynamic programming algorithm, called recursively.
The lowest level unit considered here is the minor phrase, and the probability of the
minor phrase is computed as given in Equation (6) using the decision tree.
</bodyText>
<sectionHeader confidence="0.989266" genericHeader="method">
4. Experiments
</sectionHeader>
<subsectionHeader confidence="0.977835">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9991215">
For our investigation of prosodic phrase structure, an FM radio news story corpus
was used. The training data included ten stories from one announcer and another ten
stories from a second announcer, both female, for a total of 312 sentences (6,157 words,
or potential boundary locations). The stories were studio recordings of actual radio
broadcasts, which were transcribed by a listener who did not have access to the original
scripts. It is likely that transcription of punctuation did not exactly match the original
written text and may have been biased by the prosody of the utterance. However,
the radio announcers tended to annotate the transcribed text before reading the test
stories, so we conjecture that commas were more often omitted than inserted in our
transcriptions. All of the training stories were used to estimate the probabilities of the
</bodyText>
<page confidence="0.992621">
38
</page>
<note confidence="0.757644">
M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction
</note>
<bodyText confidence="0.999909696969697">
number of subconstituents (Equations 1-3). In the first pass of tree design, two-thirds
of the training data was used to grow the tree and one-third was used to determine
the performance complexity trade-off, but the final tree used was redesigned on the
entire training set.
For testing, we used five versions of a different story spoken by two female and
two male announcers (one radio broadcast version and four radio-news-style lab
recordings). One of the female announcers (two spoken versions) was the same as
the speaker who provided roughly three-quarters of the training data. Multiple test
versions are used in order to allow for some acceptable differences in phrasing in
the context of the FM radio news style, and to investigate the possibility of speaker-
dependent effects. On average, there were 3.3 different prosodic parses among the five
versions. The test story contained 23 sentences (385 words) ranging in length from 3
to 36 words. For reference, the test sentences are included in an appendix with the
phrase predictions of our best system.
Prosodic phrase breaks were hand-labeled in the entire corpus; the training set
labels were used for estimating the parameters of the model and test set labels were
used for evaluating the performance of the model. The prosodic phrase labeling system
used break indices marked between each pair of words, based on auditory perceptual
judgments (that is, the labelers did not have access to spectrogram or pitch displays).
The break indices ranged on a scale of 0 to 6, chosen to map to a superset of the
prosodic hierarchies proposed in the literature. The labeling scheme is described in
more detail in Price, Ostendorf, Shattuck-Hufnagel, and Fong (1991). Six of the stories
were labeled by polling two listeners who discussed any discrepancies. The remaining
stories were labeled by a third listener working independently. Comparing the labels
of one story using both schemes showed that there was a high degree of consistency
across labelers. For the full seven-level labeling system, the correlation between the
two sets of labels was 0.93, where correlation is computed as the maximum likelihood
estimate of the correlation coefficient based on the two sets of labels. Only 1% of the
labels differed by 2, and these were at locations where the disagreement was actually
over the location of the boundary rather than the relative strength of the boundary.
In this work we considered only a three-level hierarchy and therefore mapped breaks
0-2 to &amp;quot;no break,&amp;quot; 3 to a &amp;quot;minor break&amp;quot; (I), 4 and 5 to a &amp;quot;major break&amp;quot; (II) and 6 to a
&amp;quot;sentence break.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.91582">
4.2 Evaluation Methods
</subsectionHeader>
<bodyText confidence="0.999928666666667">
The goal of this algorithm is to predict placement of phrase breaks that sound natural
to listeners and that communicate the intended meaning of the sentence. As men-
tioned above, many renditions of a sentence can fulfill this criterion. Therefore, we
have attempted to estimate system performance by comparing the predicted breaks to
parses observed in five spoken versions of the sentence. Although the ultimate test of
the algorithm is in a speech synthesis system, a quantitative measure of system per-
formance is useful in algorithm development and comparison. We have considered
four performance measures in this work.
Since one incorrectly assigned break could make a whole sentence or clause un-
acceptable, one measure of system performance is the number of sentences with a
predicted parse that matches entirely a parse observed in any of the five spoken ver-
sions. When such a match occurs, we call the predicted parse &amp;quot;correct.&amp;quot; The five spoken
versions do not represent an exhaustive set of acceptable parses, however. Therefore
in a separate evaluation, the sentence is also judged subjectively to determine whether
it is an &amp;quot;acceptable&amp;quot; parse. The number of sentences that fall into these two categories
</bodyText>
<page confidence="0.996708">
39
</page>
<note confidence="0.816967">
Computational Linguistics Volume 20, Number I
</note>
<bodyText confidence="0.9998518">
are reported separately, and for the best case system are marked separately in the
results in the appendix.
In order to better understand the system performance, we have chosen to com-
pute additional error measures based on the prediction accuracy at individual break
locations. A predicted sentence is compared to each of the five spoken versions, and
the closest spoken version is used as the reference for that sentence. (The closeness
of parses is measured using a Euclidean distance with 0 for no break, 1 for minor
break and 2 for major break.) Then the correspondence between predicted and ob-
served breaks is tabulated in a confusion matrix. Sentence breaks are deterministically
assigned at periods, but these are included in the performance results reported here
(as major breaks) to be consistent with results reported elsewhere. Also, note that con-
fusion tables for different systems sometimes reflect different numbers of observed
minor and major breaks because the predicted sentences may best match different
versions of the test sentence.
It is also useful to have a simple measure for comparing systems. One possible
performance figure is the overall percent correct, but we have found this measure to
be difficult to interpret because the overall figure is dominated by the performance
on the much more frequent &amp;quot;no break&amp;quot; locations. Instead, we compute the correct
prediction and false prediction rate for breaks as a combined class (merging minor
and major breaks). Using terminology from detection theory, these are also referred to
as correct detection (CD) and false detection (FD) in the following sections. CD/FD
results must be interpreted with some caution, because there is a trade-off between the
two error rates: higher break detection rates are associated with a higher rate of false
break insertion. If the insertion rate is too high, there will be few good parses at the
sentence level. We have therefore tried to control the insertion rate as much as possible
for the different systems evaluated. Two types of CD/FD results are reported. One
figure is computed based on comparison to the nearest sentence of the five versions.
In addition, since other research results have been reported based on comparison to
only one spoken version, we include correct prediction and false prediction rates that
correspond to the average rates over the five separate test versions. In general, the
correct prediction rates using the single version comparison are roughly 10% lower
than using the comparison to five versions, so comparison to one version significantly
underestimates performance of the algorithms. The variation in error rate over the
five versions is relatively small, as shown later in the discussion of speaker-dependent
effects.
</bodyText>
<subsectionHeader confidence="0.927659">
4.3 Tree Questions and Designs
</subsectionHeader>
<bodyText confidence="0.999915769230769">
Several experiments using different sets of questions to train the embedded decision
trees were performed in order to compare the relative merits of different information in
the hierarchical model, as well as trade-offs associated with computational complexity.
The entire set of questions is listed below. All experiments included questions 1-8,
which were based on features that were relatively straightforward to extract from text,
using a table look-up to assign part-of-speech labels. Experiments that made use of
syntactic features also allowed questions 9-13. The syntax experiments were based
on trees that were trained using only 14 of the 20 stories, since skeletal parses were
only available for these stories. Another set of experiments included question 14, which
tested the ratio of the current minor phrase length to the previous minor phrase length.
Finally, experiments that made use of the more detailed POS classifications included
question 15, and used the additional particle category in question 3. All questions
were based on features derived from text information only.
</bodyText>
<page confidence="0.987661">
40
</page>
<note confidence="0.754175">
M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction
</note>
<bodyText confidence="0.999021">
Below we enumerate the questions used in the different tree design experiments,
together with the motivation for each question.
</bodyText>
<listItem confidence="0.973831951219512">
1. Is this a sentence or major phrase boundary? Assuming major breaks occur at
qualitatively different locations than minor breaks, we effectively remove
the major breaks and sentences from our training corpus with this
question.
2. Is the left word a content word and the right word a function word? In the
training data, 65% of the minor and major breaks combined occur at
content word/function word (CW/FW) boundaries, and about half of the
CW/FW boundaries are marked with breaks. The CW/FW boundaries
also correspond to the prosodic group boundaries used deterministically
in Sorin, Larreur, and Llorca (1987) and in Veilleux et al. (1990).
3. What is the function word type of the word to the right? Previous work in
prosodic parsing with a small dictionary (Sorin, Larreur, and Llorca
1987) suggested that different types of function words may be more or
less likely to signal a prosodic phrase break.
4. Is either adjacent word a proper name (capitalized)? Preliminary examination
of our data suggested there was some relationship between proper
nouns and phrase boundaries, probably related to the phrasing of
complex nominals.
5. How many content words have occurred since the previous function word?
Speakers seemed to insert phrase breaks when a string of content words
became long, e.g., exceeded four or five words.
6. Is there a comma at this location? Usually, but not always, a major phrase
break occurs at locations orthographically transcribed with commas.
7. What is the relative location in the sentence (in eighths)? Previous work (Gee
and Grosjeari 1983) has suggested that prosodic phrase boundaries tend
to bisect a longer unit. Therefore, one of the questions used to partition
the training data is the ratio of the word number over the sentence
length, quantized to the nearest eighth.
8. What is the relative location in the proposed major phrase (in eighths)? This
question is included following the same reasoning as the previous
question.
9. VVhat is the largest syntactic unit that dominates the word preceding the potential
boundary location and not dominating the succeeding word? Phrase breaks are
known to co-occur with certain syntactic configurations. For example,
phrase breaks often occur before subordinate clauses.
10. What is the largest syntactic unit that dominates the word succeeding the
potential boundary location and not dominating the preceding word? The
rationale behind this question is similar to that of the previous question.
H. What is the smallest syntactic unit that dominates both? Some syntactic units
may be less likely to be broken up by a phrase break.
12. How many syntactic units end between the two words? This question provides
</listItem>
<bodyText confidence="0.979477">
information on the relative level of syntactic attachment between the two
words, capturing the effect of constituent endings.
</bodyText>
<page confidence="0.99674">
41
</page>
<note confidence="0.182918">
Computational Linguistics Volume 20, Number 1
</note>
<listItem confidence="0.903709214285714">
13. How many syntactic units begin between the two words? This question is
similar to the previous one, except that it captures effects associated with
the start of new constituents.
14. How large is the ratio of the current minor phrase length over the previous minor
phrase length? This question incorporates the concept of balancing minor
phrase lengths noted by other researchers (Gee and Grosjean 1983;
Bachenko and Fitzpatrick 1990), and was found to be useful in phrase
prediction trees investigated by Wang and Hirschberg (1992). In the
beginning of a sentence where there is no previous minor phrase, the
ratio is treated as missing data and handled using a surrogate variable
(Breiman, Friedman, Olshen, and Stone 1984).
15. What is the label of the content word to the right? to the left? Wang and
Hirschberg found that part-of-speech information is useful in phrase
break prediction (Wang and Hirschberg 1992).
</listItem>
<bodyText confidence="0.999461885714286">
Questions 5, 12, 13, and 14 are based on numerical features, so the binary ques-
tion asks whether the feature is greater than some threshold, where the threshold is
determined automatically in tree design. All other questions are based on categorical
variables, and the best binary groupings of the possible values are determined auto-
matically (Breimart, Friedman, Olshen, and Stone 1984). Two of the questions (8, 14)
require knowledge of major or minor phrase boundaries. This information is available
in the training data or from a spoken utterance, but hypothesized locations of minor
and major phrase breaks must be used in phrase prediction from text. Therefore, these
features are calculated dynamically in the prediction algorithm for each hypothesized
prosodic parse.
The first tree designed used only the very simple information represented by
questions 1-8. The resulting tree is shown in Figure 2, with the relative frequency of a
break in the training data included at each node. The first split trivially locates the sen-
tence and major break boundaries. The second split utilized the content word/function
word boundary question that we had used deterministically in previous work (Veilleux
et al. 1990). The content/function word boundaries seem to be important in other al-
gorithms as well: they correspond closely to the phi-phrase boundaries that would be
predicted by the Bachenkoâ€”Fitzpatrick algorithm, and they seem to be captured in the
Wangâ€”Hirschberg text-only tree by a succession of questions about the part-of-speech
labels of the words adjacent to the break. Of the boundaries that were preceded by
a content word and followed by a function word, 30% were hand-labeled as minor
breaks, whereas only 4% of other locations were labeled as minor breaks and these
were identified by the next question as coinciding with a comma. The complete tree
was relatively small (9 nodes), and used almost all questions provided. On the training
data, the resulting tree classified 89% of the nonbreaks correctly and 59% of the minor
breaks correctly. All sentence and major breaks were given in the tree design.
The next stage was to incorporate syntactic information (questions 9-13) into the
tree design algorithm to determine minor phrase probabilities. Syntactic parses were
available for only 14 of the 20 training stories (2.17 sentences, 4,230 words), and the tree
was designed using this subset. A very simple five-node tree was designed, as shown
in Figure 3. Again the first nontrivial question was concerning the content/function
word boundaries, and the presence of a comma was again used to predict minor
breaks at other locations. The two other questions in the tree were based on which
syntactic unit dominated one or the other words at the boundary site. The tree design
algorithm chose syntactic units that were less likely to contain a boundary as: words
</bodyText>
<page confidence="0.999047">
42
</page>
<figureCaption confidence="0.978164">
Figure 2
</figureCaption>
<bodyText confidence="0.9871769375">
Tree designed using only simple part-of-speech information, questions 1-8. Relative frequency
of a &amp;quot;break&amp;quot; (in the training data) is indicated in each node for the subset of data associated
with that node, and the left branch in a split is more likely to have a break.
in the same constituent, words separated by a wh-noun phrase boundary, and words
separated by a verb phrase initial boundary. (In their work on spontaneous speech,
Wang and Hirschberg found that noun phrases in general tended to be less likely
to contain boundaries.) The tree with syntactic information seemed to classify minor
breaks with slightly higher accuracy than the previous tree: 90% correct classification
of nonbreaks and 62% correct classification of minor breaks in the training data.
A third tree, illustrated in Figure 4, was grown using the first 8 baseline questions
and question 14, which examines the ratio of current minor phrase length to previous
minor phrase length. The motivation behind this question is constituent balancing, as
mentioned earlier. The main difference between this tree and the first one is that the
minor phrase length ratio test is chosen instead of the question about the position in a
major phrase. These two questions served similar roles, as evidenced by the fact that
the surrogate variable for the ratio test was the location of the current word within
</bodyText>
<figure confidence="0.99261824">
no
yes
Hierarchical Stochastic Model for Automatic Prediction
sentence / major word
boundary ?
M. Ostendorf and N. Veilleux
1422/5581
yes
yes
279/912
151/3677
comma?
no
38/3664
comma ?
no
261/894
place in
major phrase?
content / function word
boundary ?
yes no
992/992 430/4589
13/13
18/18
24/152
17/48
180/571
place in
sentence ?,
first or third
eighth
first or last
quarter?
other
function word
type ?
d or p
other
other
163/452
other
function word
type?
d, p or g
yes no
43/161
capital?
28/162
27/119
</figure>
<page confidence="0.995508">
43
</page>
<figureCaption confidence="0.894603">
Figure 3
Tree designed using syntactic information but not the minor phrase length ratio test, questions
1-13. Relative frequency of a &amp;quot;break&amp;quot; (in the training data) is indicated in each node for the
subset of data associated with that node, and the left branch in a split is more likely to have a
break.
</figureCaption>
<bodyText confidence="0.9999599">
the major phrase, in terms of the ratio of the number of words up to the current
position over the total length of the major phrase. Classification rates on the training
data for this tree were 87% for nonbreaks and 66% for minor breaks. A fourth tree
was designed using the first fourteen questions, and performance was similar to that
for the third tree.
The decision tree design algorithm&apos;s performance was not significantly changed
by the introduction of additional features. New features can supplant previously used
ones, as also found by Wang and Hirschberg (1992), because of the redundancy in
information between features. For example, in the syntax trees, many of the baseline
questions were no longer chosen, but the overall classification performance was similar.
</bodyText>
<subsectionHeader confidence="0.796857">
4.4 Phrase Prediction Results
</subsectionHeader>
<bodyText confidence="0.9999436">
The trees were used in the hierarchical model, and the phrase break prediction algo-
rithm was evaluated on the independent test set described in Section 4.1. A summary
of the results is given in Table 2, and the corresponding confusion matrices are in
Tables 3 and 4. The baseline system (questions 1-8) gave the best performance, with
a correct prediction rate of 81% and a false prediction rate of 4%. The results indicate
that syntactic information did not improve the performance of the algorithm, and in
fact gave poorer phrase predictions by every measure of performance on the test data.
The difference in performance cannot be attributed to the smaller amount of training
data used in the experiments with syntax, because designing the model without syn-
tax on this subset actually yielded slightly better performance on the test set than that
</bodyText>
<figure confidence="0.9491381875">
comma?
left word
dominated by ...
sanie
WHNP
othe
Computational Linguistics
Volume 20, Number 1
sentence / major phrase
boundary?
no
content /function word
boundary?
yes
yes
yes
no
right word
dominated by ...
104/2760
same,
WHNP, VP
othe
44
Hierarchical Stochastic Model for Automatic Prediction
M. Ostendorf and N. Veilleux
442/5581 sentence /major phrase boundary?
yes no
992/992 430/4589 content /function word
boundary?
yes no
comma? comma?
</figure>
<figureCaption confidence="0.6452122">
Figure 4
Tree designed using the minor phrase length ratio test but no syntax questions, questions 1-8
and 14. Relative frequency of a &amp;quot;break&amp;quot; (in the training data) is indicated in each node for the
subset of data associated with that node, and the left branch in a split is more likely to have a
break.
</figureCaption>
<bodyText confidence="0.999321583333333">
designed on the full training set. We conjecture that the poorer performance associ-
ated with using syntax in our model may be due to the fact that syntax plays more
of a role in location of major breaks as opposed to the minor breaks predicted in the
tree. As we shall see later, syntactic cues were useful in our implementations of the
Bachenko-Fitzpatrick and Wang-Hirschberg algorithms. We also found that the minor
phrase length ratio test hurt performance, which is likely due to the fact that the ra-
tios are based on hypothesized boundary locations in phrase prediction, as opposed
to the known locations used in training. In examining the confusion matrices, we see
the main effect of the additional syntactic and minor phrase length ratio questions is
more errors at minor phrase boundary locations.
Examining the sentence level performance of the algorithms, we find that a phrase
break was inserted between the verb and the particle in three of the six unacceptable
</bodyText>
<figure confidence="0.98338025">
234/726
function word
t.e?
d or p
27/168
279/912
79/3677
yes no yes no
minor
phrase ratio?
&lt;34.5 %
18/18
261/894
13/13
&gt; 34.5 %
138/3664
&gt; 19 % 25/106 minor phrase
ratio?
&lt; 19 %
place in
25/88 sentence?
other first, fifth or sixth
eighth
20/49
</figure>
<page confidence="0.972268">
45
</page>
<note confidence="0.676196">
Computational Linguistics Volume 20, Number 1
</note>
<tableCaption confidence="0.979206222222222">
Table 2
Performance of different break prediction algorithms, including variations of our hierarchical
model, variations of a tree-based classifier, and the Bachenkoâ€”Fitzpatrick (Bâ€”F) algorithm
based on a test set of 23 sentences (386 words). &amp;quot;Questions Used&amp;quot; refers to those questions
listed in Section 4.3 for the two tree-based algorithms. Although the Bâ€”F algorithm does not
use these specific questions, it does utilize syntactic information as well as relative constituent
length. Correct Detection/False Detection (CD/FD) rates are for the merged category of minor
and major breaks computing (a) error according to the closest utterance of five versions, and
(b) the average error in comparing to a single utterance.
</tableCaption>
<table confidence="0.999613888888889">
Phrase model Questions used Sentences CD/FD of merged breaks
Syntax Minor ratio Correct Accept. 5 versions 1 version
No No 7 11 .81 / .04 .70/.05
Hierarchical model Yes No 4 6 .77/.04 .68/.06
No Yes 5 10 .79/.03 .67/.05
Yes Yes 3 11 .77/.04 .70/.05
Classification trees No No 7 6 .68 / .03 .62/.03
Yes No 7 5 .72/.01 .61 / .01
Bâ€”F Yes Yes 6 10 .88/.07 .84/.09
</table>
<tableCaption confidence="0.913294666666667">
Table 3
Confusion matrices for predicted breaks using the hierarchical system with and without
syntax, and without the minor phrase length ratio test.
</tableCaption>
<table confidence="0.977541">
No Syntax Syntax
Actual Actual
Predicted major minor no-break Predicted major minor no-break
major 49 6 5 major 46 14 11
minor 5 12 6 minor 7 6 1
no-break 7 10 286 no-break 12 10 279
</table>
<tableCaption confidence="0.99903">
Table 4
</tableCaption>
<table confidence="0.848659">
Confusion matrices for predicted breaks using the hierarchical system with and without
syntax, in both cases with the minor phrase length ratio test.
No Syntax Syntax
Actual Actual
Predicted major minor no-break Predicted major minor no-break
major 51 15 9 major 49 13 9
minor 5 1 1 minor 5 5 2
no-break 9 9 286 no-break 9 12 282
</table>
<bodyText confidence="0.9951664">
parses (e.g., tried I out, plugs I in, and check I in). This is not surprising since the simple
POS labeling scheme labels the particle as a preposition. The trees using syntactic
information were not able to overcome this effect because of the relative sparsity
of particles in the training data (only 5% of the words labeled as prepositions are
particles). Other mistakes included a misplaced minor phrase and a deleted major
phrase where a comma occurs in the original text. Most of the sentences that were
correct (had an exact match with one of the spoken versions) were shorter in length.
However, there were several long sentences judged to have acceptable parses. Since
many more variations in prosodic phrasing are allowable for longer sentences, it is
not surprising that the predicted version was not one of the five spoken versions. The
</bodyText>
<page confidence="0.999325">
46
</page>
<note confidence="0.920086">
M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction
</note>
<tableCaption confidence="0.999091666666667">
Table 5
Confusion matrices for predicted breaks using the simple classification trees (no hierarchical
model) with and without syntax, in both cases without the minor phrase length ratio test.
</tableCaption>
<table confidence="0.9712305">
No Syntax Syntax
Actual Actual
Predicted major minor no-break Predicted major minor no-break
major 57 4 8 major 57 3 4
minor 0 0 0 minor 0 0 0
no-break 13 15 288 no-break 5 18 298
</table>
<bodyText confidence="0.998994333333333">
predicted breaks for the best system, the hierarchical model based on questions 1-8,
are shown in the Appendix together with the closest spoken prosodic parse.
In tree design, we chose to represent major breaks as a separate category that the
tree was not explicitly designed to detect. A consequence of this choice is that there
are fewer &amp;quot;break&amp;quot; data points for training the tree, since there are less than half as
many minor breaks as major breaks in the training data. This choice is reasonable if
the two breaks occur at qualitatively different locations, which we suspect. In fact,
results using trees that were trained by merging major and minor breaks into a single
category and then embedded in the hierarchical model had either lower prediction
accuracy or a higher false prediction rate. Another consequence of using only minor
breaks to train the tree is that features that are associated with major breaks are not
represented in the model, which may explain the poor performance of the model with
syntax. However, this problem could be addressed with an extension of the current
model.
In order to see if explicit modeling of a prosodic hierarchy was a useful aspect
of the model, we conducted similar experiments using trees designed specifically for
classification. A binary decision tree was trained using the baseline questions (1-8),
and another tree was trained using syntax as well (1-13). In both cases, the trees were
trained to predict three classes: major break, minor break, and no break. Including the
minor phrase length ratio test using known break locations (from hand labels) did not
improve prediction performance, so we did not implement a dynamic version based
on hypothesized minor breaks. Results for these two trees are included in Table 2, with
confusion matrices given in Table 5. The costs of different errors were chosen to obtain
a false detection rate similar to that for the hierarchical model. Choosing good costs
proved difficult, so the correct detection rate is lower than that for the other models
primarily because the false detection rate was so low. The difference in false detection
rates makes comparison to the hierarchical model difficult. However, experience with
performance of the models at different false detection rates suggests that the baseline
hierarchical model outperforms the classification tree that does not use syntax, but that
the classification tree that uses syntax is at least as good as the hierarchical model.
Since the complexity associated with obtaining a syntactic parse is significantly greater
than that associated with the simple three-level hierarchy that we have proposed, we
conclude that explicit modeling of a hierarchy is a useful feature of the model. In
addition, the fact that syntactic information was useful for the classification tree but
not for the hierarchical model suggests that syntactic features are more important for
predicting major breaks than minor breaks, since major breaks are not represented as
a class in tree design for the hierarchical model.
For both the hierarchical model and the simple classification trees, we also inves-
tigated the use of more detailed part-of-speech information both with and without
</bodyText>
<page confidence="0.998835">
47
</page>
<note confidence="0.694749">
Computational Linguistics Volume 20, Number 1
</note>
<tableCaption confidence="0.967205333333333">
Table 6
Confusion matrix for the Bachenkoâ€”Fitzpatrick algorithm, which uses syntax and rules to
account for balancing constituent lengths.
</tableCaption>
<table confidence="0.9753348">
Predicted Actual
major minor no-break
major 49 9 7
minor 10 21 12
no-break 4 8 265
</table>
<bodyText confidence="0.99776158974359">
the syntactic features. The more detailed part-of-speech did not improve performance
under any of these conditions. For the classification trees, correct detection improved
slightly, but there was a corresponding increase in false detection. For the hierarchi-
cal model, performance actually degraded. These results suggest that the complexity
associated with more detailed part-of-speech tagging may not be necessary; however,
further research is needed to answer this question. It may be that other POS questions,
such as testing a larger window of words around the break as in Wang and Hirschberg
(1992), would yield better results.
Finally, we thought it would be interesting to compare the results of our predic-
tion algorithm to that of Bachenko and Fitzpatrick&apos;s algorithm on our corpus. Since
the test set was relatively small, we were able to implement the algorithm by predict-
ing the phrase boundaries from the rules by hand. To assign node indices to prosodic
breaks (Bachenko and Fitzpatrick 1990), a critical value for separating major and mi-
nor phrase breaks is calculated based on an average of the indices associated with the
prosodic phrase nodes, where the prosodic phrase nodes are all those created by the
Bachenkoâ€”Fitzpatrick primary salience rules. Boundaries with an index greater than
the critical value are assigned a major break, indices below 5 have no prosodic break,
and intermediate indices map to a minor prosodic break. (Bachenko and Fitzpatrick
include index 4 in the minor break category, but 5 was used here to obtain a lower in-
sertion rate.) For multiple verb phrases in sequence, the verb balancing rule is applied
left-to-right until all verb phrases are grouped before applying the verb adjacency rule
or other processing. The confusion matrix for these results is shown in Table 6, and the
performance summary is also included in Table 2. Although the correct break detection
rate is significantly higher than that for the other algorithms, the false detection rate is
also higher, and so the sentence accuracy is similar to that for the baseline hierarchical
model. Unlike the other algorithms, the Bachenkoâ€”Fitzpatrick algorithm did not make
the mistake of assigning a minor phrase break before a particle, but this relies on
having a parser that can make that distinction. An advantage of both the classification
tree and the hierarchical model over the Bachenkoâ€”Fitzpatrick model is that they can
be automatically trained, and thus can be tuned to handle particular tasks.
Table 7 gives the correct detection and false detection rates calculated by compar-
ing the predicted prosodic parses to each of the different spoken versions. The perfor-
mance of speaker f2b, whose speech made up roughly three-quarters of the training
data, had performance similar to the average for the five versions with slightly lower
correct detection rates but also slightly lower false detection rates. These results sug-
gest that the automatic algorithms are not particularly speaker-dependent, though we
expect that it is important to have similar styles for both training and test data. There
was no consistent difference in performance between male and female speakers, and
the difference in error rates for different speakers was relatively small.
</bodyText>
<page confidence="0.998661">
48
</page>
<note confidence="0.919607">
M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction
</note>
<tableCaption confidence="0.995478">
Table 7
</tableCaption>
<bodyText confidence="0.5192082">
Correct detection/false detection rates for predicted phrase breaks to each of the five different
test versions. With the exception of the Bachenkoâ€”Fitzpatrick algorithm, the systems included
here did not use the minor ratio question. Speaker codes begin with &amp;quot;f&amp;quot; or &amp;quot;m&amp;quot; for female and
male speakers, respectively. Speaker code f2b is annotated with &amp;quot;r&amp;quot; for the original radio
recording and &amp;quot;1&amp;quot; for the subsequent lab recording.
</bodyText>
<table confidence="0.999720142857143">
Phrase model Uses syntax? CD/FD of merged breaks
f2b(r) f2b(1) f3a m1b m3b average
Hierarchical model No .69/.04 .66/.05 .73/.06 .72/.06 .70/.06 .70/.05
Yes .667.06 .69/.05 .68/.08 .717.07 .69/.07 .68/.06
Classification trees No .60/.02 .59/.02 .64/.04 .65/.04 .62/.04 .62/.03
Yes .58/.01 .57/.01 .61/.02 .67/.01 .63 / .01 .617.01
B-F Yes .85/.06 .82/.07 .81/.11 .85/.10 .85/.10 .84/.09
</table>
<tableCaption confidence="0.992054">
Table 8
</tableCaption>
<table confidence="0.914863166666667">
Comparison of correct detection/false detection rates computed from five test versions for
different break prediction algorithms that do and do not use comma information.
Hierarchical Model Classification Trees
baseline all features baseline w/syntax
Commas .817.04 .77/.04 .68/.03 .72/.01
No commas .66/.05 .71/.04 .59/.04 .68/.02
</table>
<bodyText confidence="0.998846565217391">
In all of the previous experiments, the presence of a comma was an important fea-
ture for predicting phrase boundaries for all algorithms implemented. While this is a
valid feature in text-to-speech synthesis applications, it is not available in applications
involving spoken speech. (Although presence of a pause might be a useful alternative
feature.) In addition, as we have mentioned earlier, commas are not reliably used even
in written text. Therefore, it is interesting to determine the performance of the algo-
rithm without the comma feature. As expected, performance degrades significantly,
both for the hierarchical model and for the classification trees. In addition, foy the
hierarchical model, syntactic information and the minor phrase length ratio test now
provide information that improves performance over the baseline system. To illustrate
performance differences, some correct prediction/false prediction rates are given in
Table 8.
It is difficult to compare our performance figures with other reported results
because of differences in corpora and speaking styles. However, the average single
speaker correct detection and false detection rates reported here for our implementa-
tions of the Bachenko-Fitzpatrick and Wang-Hirschberg algorithms indicate the ro-
bustness of these algorithms to different types of data. Our results for the Bachenko-
Fitzpatrick algorithm are somewhat higher than those that they report, .84/.09 vs.
.78/.08.2 Using only the features inferable from text, Wang and Hirschberg use classi-
fication trees to predict prosodic boundaries in spontaneous speech, achieving phrase
break prediction results of .66/.02.3 (Again, note that these results are not directly
comparable because of the differences in false detection rates, and results for other
trees in Wang and Hirschberg [1992] suggest that these two algorithms have similar
</bodyText>
<footnote confidence="0.9047716">
2 This figure is calculated from the examples in the appendix in Bachenko and Fitzpatrick (1990),
ignoring tertiary boundaries and including sentence-final boundaries as correct. The sentence that did
not parse was not included in the calculation.
3 This result is computed from Figure 6 of Wang and Hirschberg (1992), which illustrates classification
on training data. Cross validation results may vary slightly.
</footnote>
<page confidence="0.99778">
49
</page>
<note confidence="0.800416">
Computational Linguistics Volume 20, Number 1
</note>
<bodyText confidence="0.998614833333333">
performance.) Our classification trees used somewhat different features, though also
based on POS and syntactic information, and achieved results on radio news speech
that are surprisingly lower, i.e., .59/.02 for the tree that used syntax but did not use
commas. Of course, the POS and syntactic information used here may not have been
as detailed and/or reliable as that used by Wang and Hirschberg. (The comparisons
here are based on the average error rates for single version comparisons.)
</bodyText>
<sectionHeader confidence="0.994968" genericHeader="method">
5. Discussion
</sectionHeader>
<bodyText confidence="0.999993142857143">
In summary, the model proposed here addresses several issues in modeling prosodic
phrase structure. The model is a general formalism for an embedded hierarchy, which
represents a unit in terms of the probability of the sequence of subunits comprising
it. The model is specifically applied to represent a hierarchy containing sentences,
major phrases, and minor phrases. The model captures grammatical and constituent
length factors through the use of a decision tree, as in Wang and Hirschberg (1992),
but embedding the tree within a hierarchical structure yields better performance than
that achieved by a decision tree alone. The model is stochastic, which accounts for
the natural variability in prosodic parsing, and can be automatically trained to reflect
different speaking styles. The automatic training algorithm described here, based on
maximum likelihood estimation, involves simple relative frequency estimates and de-
cision tree design. Automatic training on a large corpus to design the best predictors of
phrase breaks can also provide new insight into the relationship between prosody and
syntax. Using the stochastic model, prosodic phrase break prediction involves choos-
ing the most likely prosodic parse, which can be achieved using a recursive dynamic
programming algorithm. We have found that good phrase break prediction results can
be achieved without the use of syntactic information or detailed part-of-speech labels,
resulting in a very low complexity prediction algorithm. Without syntax, our algo-
rithm predicted a good prosodic parse for 18 out of 23 sentences, which corresponds
to a correct break prediction of 81% and a false prediction rate of 4%.
There are many ways in which this work could be extended. As we have pointed
out, it would be useful to use features directly in determining the probability of a unit,
rather than simply representing a unit in terms of the probabilities of the subunits. For
example, we conjecture that commas and certain syntactic structures might be good
predictors of major phrase breaks, but not minor phrase breaks. In addition, other
features could be used in the model, including different syntactic features and differ-
ent questions about the more detailed part-of-speech labels. Automatically predicted
prominence (or pitch accent) locations might also be useful in phrase boundary pre-
diction, although it is arguable whether prominence prediction should come before or
after boundary placement. Of course, it would also be interesting to consider a higher
order hierarchy, though we anticipate that a successful implementation would require
representation of features at the different levels. The results reported here were limited
to some extent by the amount of available data. A larger training set would enable
the study of more factors, including possibly paragraph-level phenomena. A larger
test set would better establish the significance of the results. Finally, the best test of a
phrase break prediction algorithm is in perceptual judgments of synthesized speech,
and we would like to evaluate our algorithm in this context.
In this work we have focused on the synthesis application of prosodic phrase break
prediction. However, one of the advantages of a stochastic model is that it may be use-
ful for analysis of spoken speech. Because there is some relationship between prosody
and syntax, prosodic phrase structure can be used to improve the performance and/or
speed of speech understanding systems. For example, a score of the consistency be-
</bodyText>
<page confidence="0.979086">
50
</page>
<note confidence="0.754613">
M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction
</note>
<bodyText confidence="0.9999262">
tween a syntactic parse and a prosodic parse has been used to resolve ambiguities
in sentence interpretation, where the score is computed by comparing automatically
detected prosodic phrase breaks to phrase breaks predicted from the text of the dif-
ferent interpretations (Ostendorf, Wightman, and Veilleux 1993). Another approach to
syntactic disambiguation using prosodic information is described in Bear and Price
(1990) and Ostendorf, Price, Bear, and Wightman (1990), where prosodic breaks are
used to constrain grammar rules in a parser. A stochastic phrase model could also be
used to improve performance of this system simply by serving as a &amp;quot;language model&amp;quot;
(as in speech recognition) to improve the performance of a detection algorithm using
acoustic information.
</bodyText>
<sectionHeader confidence="0.982855" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996729769230769">
We would like to thank Patti Price and
Stefanie Shattuck-Hufnagel for their useful
comments and insights, an anonymous
reviewer for many valuable comments, and
Gay Baldwin and Liz Shriberg for
hand-labeling part of the corpus with
prosodic labels. The radio news corpus was
provided by WBUR, a public radio station.
The skeletal parses for the corpus are part of
a preliminary version of the Penn Treebank
Corpus, copyright of the University of
Pennsylvania. This work was supported by
NSF under grant number IRI-8805680.
</bodyText>
<sectionHeader confidence="0.989849" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999297675324675">
Allen, J.; Hunnicutt, S.; Carlson, R.; and
Granstrom, B. (1979). &amp;quot;MITalk-79: The
1979 MIT text-to-speech system.&amp;quot; In
Speech Communications Papers Presented at
the 97th Meeting of the ASA, edited by Wolf
and Klatt, 507-510.
Allen, J.; Hunnicutt, M. S.; and Klatt, D.
(1987). From Text to Speech: The MITalk
System. Cambridge University Press.
Altenberg, B. (1987). Prosodic Patterns in
Spoken English. Lund University Press.
Bachenko, J., and Fitzpatrick, E. (1990). &amp;quot;A
computational grammar of
discourse-neutral prosodic phrasing in
English.&amp;quot; Computational Linguistics 16(3),
155-170.
Bahl, L. R.; Brown, P. E; deSouza, P. V.; and
Mercer, R. L. (1989). &amp;quot;A tree-based
statistical language model for natural
language speech recognition.&amp;quot; IEEE
Transactions on Acoustics, Speech, and Signal
Processing 37(7), 1001-1008.
Bear, J., and Price, P. J. (1990). &amp;quot;Prosody,
syntax and parsing.&amp;quot; In Proceedings, ACL
Conference, 17-22.
Beckman, M., and Pierrehumbert, J. (1986).
&amp;quot;Intonational structure in Japanese and
English.&amp;quot; In Phonology Yearbook 3, edited
by J. Ohala, 255-309.
Booij, G. (1983). &amp;quot;Principles and parameters
in prosodic phonology.&amp;quot; Linguistics 21,
249-280.
Breiman, L.; Friedman, J.; Olshen R.; and
Stone, C. (1984). Classification and
Regression Trees, The Wadsworth
Statistics/Probability Series, Wadsworth
and Brooks.
Church, K. W. (1988). &amp;quot;A stochastic parts
program and noun phrase parser for
unrestricted text.&amp;quot; In Proceedings, Second
Conference on Applied Natural Language
Processing, 136-143.
Cooper, W., and Paccia-Cooper, J. (1980).
Syntax and Speech. Harvard University
Press.
Gee, J., and Grosjean, E (1983).
&amp;quot;Performance structures: A
psycholinguistic and linguistic appraisal.&amp;quot;
Cognitive Psychology 15,411-458.
Hindle, D. M. (1987). &amp;quot;Acquiring
disambiguation rules from text.&amp;quot; In
Proceedings, Association for Computational
Linguistics Meeting, 118-125.
Hirose, K., and Fujisaki, H. (1982).
&amp;quot;Analysis and synthesis of voice
fundamental frequency contours of
spoken sentences.&amp;quot; In Proceedings,
International Conference on Acoustics, Speech,
and Signal Processing, 950-953.
Ladd, D. R. (1986). &amp;quot;Intonational phrasing:
The case for recursive prosodic structure.&amp;quot;
In Phonology Yearbook 3, edited by J. -Ohala,
311-340.
Ladd, D. R., and Campbell, N. (1991).
&amp;quot;Theories of prosodic structure: Evidence
from syllable duration.&amp;quot; In Proceedings,
XII International Congress of Phonetic
Sciences, 2,290-293.
Lehiste, I. (1973). &amp;quot;Phonetic disambiguation
of syntactic ambiguity.&amp;quot; Glossa 7(2),
107-121.
Liberman, M. Y, and Prince, A. S. (1977).
&amp;quot;On stress and linguistic rhythm.&amp;quot;
Linguistic Inquiry 8,249-336.
Marcus, M. P.; Santorini, B.; and
Marcinkiewicz, M. (1993). &amp;quot;Building a
very large annotated corpus of English:
</reference>
<page confidence="0.967206">
51
</page>
<note confidence="0.464362">
Computational Linguistics Volume 20, Number 1
</note>
<reference confidence="0.9992789">
The Penn Treebank.&amp;quot; Computational
Linguistics 19(2), 313-330.
Meteer, M.; Schwartz, R.; and Weischedel, R.
(1991). &amp;quot;POST: Using probabilities in
language processing.&amp;quot; In Proceedings,
International Joint Conference on Artificial
Intelligence, 960-965.
Nespor, M., and Vogel, I. (1983). &amp;quot;Prosodic
structure above the word.&amp;quot; In Prosody:
Models and Measurements, edited by
A. Cutler and a R. Ladd, 123-140.
Springer-Verlag.
Nespor, M., and Vogel, I. (1986). Prosodic
Phonology. Foris.
O&apos;Shaughnessy, D. (1989). &amp;quot;Parsing with a
small dictionary for applications such as
text-to-speech.&amp;quot; Computational Linguistics
15(2), 97-108.
Ostendorf, M.; Price, P.; Bear, J.; and
Wightman, C. W. (1990). &amp;quot;The use of
relative duration in syntactic
disambiguation.&amp;quot; In Proceedings, Third
DARPA Workshop on Speech and Natural
Language, 26-31.
Ostendorf, M., and Roukos, S. (1989). &amp;quot;A
stochastic segment model for
phoneme-based continuous speech
recognition.&amp;quot; IEEE Transactions on
Acoustics, Speech, and Signal Processing
37(12), 1857-1869.
Ostendorf, M.; Wightman, C. W.; and
Veilleux, N. M. (1993). &amp;quot;Parse scoring
with prosodic information: An
analysis/synthesis approach.&amp;quot; Computer
Speech and Language, 193-210.
Price, P.; Ostendorf, M.; Shattuck-Hufnagel,
S.; and Fong, C. (1991). &amp;quot;The use of
prosody in syntactic disambiguation.&amp;quot;
Journal of the Acoustical Society of America
90(6), 2956-2970.
Selkirk, E. (1980). &amp;quot;The role of prosodic
categories in English word stress.&amp;quot;
Linguistic Inquiry 11,563-605.
Selkirk, E. (1984). Phonology and Syntax: The
Relation between Sound and Structure. MIT
Press.
Sorin, C.; Larreur, D.; and Llorca, R. (1987).
&amp;quot;A rhythm-based prosodic parser for
text-to-speech systems in French.&amp;quot; In
Proceedings, International Congress of
Phonetic Sciences 1,125-128.
t&apos;Hart, J.; Collier, R.; and Cohen, A. (1990).
A Perceptual Study of Intonation. Cambridge
University Press.
Veilleux, N.; Ostendorf, M.; Price, P.; and
Shattuck-Hufnagel, S. (1990). &amp;quot;Markov
modeling of prosodic phrase structure.&amp;quot;
In Proceedings, International Conference on
Acoustics, Speech, and Signal Processing,
777-780.
Wang, M., and Hirschberg, J. (1992).
&amp;quot;Automatic classification of intonational
phrase boundaries.&amp;quot; Computer Speech and
Language 6(2), 175-196.
Wightman, C.; Shattuck-Hufnagel, S.;
Ostendorf, M.; and Price, P. (1992).
&amp;quot;Segmental durations in the vicinity of
prosodic phrase boundaries.&amp;quot; Journal of the
Acoustical Society of America 91(3),
1707-1717.
</reference>
<page confidence="0.994406">
52
</page>
<note confidence="0.567326">
M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction
</note>
<sectionHeader confidence="0.67222" genericHeader="method">
Appendix: Predicted Breaks in Test Corpus
</sectionHeader>
<bodyText confidence="0.999347285714286">
The sentences below illustrate the predicted minor (1) and major (II) phrase boundaries
for our best case algorithm, that which uses the basic questions 1-8 in the hierarchical
model. The evaluations &amp;quot;correct&amp;quot; (C), &amp;quot;acceptable&amp;quot; (A), and &amp;quot;incorrect&amp;quot; (I) are indicated
after each sentence. For the cases where the predicted sentences were either acceptable
or incorrect, we have included the spoken version that was closest to the predicted
sentence in the sense of minimizing the Euclidean distance based on representing no
break with 0, a minor break with 1, and a major break with 2.
</bodyText>
<reference confidence="0.989927972222222">
1. Computerized phone calls, I I which do everything from selling magazine
subscriptions I to reminding people about meetings, II have become the
telephone equivalent I I of junk mail. I I (A)
Computerized phone calls, I I which do everything I from selling
magazine subscriptions I Ito reminding people about meetings I I have
become the telephone equivalent I of junk mail. I I
2. But a new application of the technology II is about to be tried I out in
Massachusetts II to ease crowded jail conditions. I I (I)
But a new application of the technology I I is about to be tried out in
Massachusetts I Ito ease crowded jail conditions. I I
3. Next week some inmates released early I from the Hampton County jail
in Springfield I I will be wearing a wristband that hooks up with a special
jack 1 on their home phones. I I (A)
Next week II some inmates released early I from the Hampton County
jail in Springfield I I will be wearing a wristband I I that hooks up with a
special jack I on their home phones. I I
4. Whenever a computer randomly calls I I them from jail, I I the former
prisoner plugs I in to let corrections officials know II they&apos;re in the right
place I at the right time. 11 (1)
Whenever a computer randomly calls them from jail, I I the former
prisoner plugs in I to let corrections officials know II they&apos;re in the right
place I at the right time. I I
5. Margo Melnicove reports. H (C)
6. The device is attached Ito a plastic wristband. I 1(C)
7. It looks like a watch. 11(C)
8. It functions like an electronic probation officer. 1 I (A)
It functions I like an electronic probation officer. II
9. When a computerized call is made I to a former prisoner&apos;s home phone,
II that person answers I by plugging in the device. 11(C)
10. The wristband can be removed I I only by breaking its clasp, I I and if
that&apos;s done the inmate I is immediately returned to jail. I 1(A)
The wristband I can be removed II only by breaking its clasp, II and if
that&apos;s done I I the inmate I is immediately returned to jail. I I
11. The description conjures up images I of big brother watching. 11(C)
12. But Jay Ash, I I deputy superintendent of the Hampton County jail I in
Springfield, I says the surveillance system I I is not that sinister. 11(1)
</reference>
<page confidence="0.977932">
53
</page>
<note confidence="0.479466">
Computational Linguistics Volume 20, Number 1
</note>
<reference confidence="0.99623775">
But Jay Ash, I I deputy superintendent of the Hampton County jail in
Springfield, I I says the surveillance system I is not that sinister. I I
13. Such supervision, I I according to Ash, I is a sensible, I I cost effective
alternative to incarceration I that should not alarm civil libertarians. 1 I (A)
Such supervision, I I according to Ash, I I is a sensible, I I cost effective
alternative to incarceration I I that should not alarm I 1 civil libertarians. I
14. Doctor Norman Rosenblatt, I dean of the college I of criminal justice at
Northeastern University, I I agrees. 1 I (A)
Doctor Norman Rosenblatt, I I dean of the college I of criminal justice at
Northeastern University, I I agrees. I I
15. Rosenblatt expects electronic surveillance I in parole situations to become
more widespread, I I and he thinks eventually people I I will get used to
the idea. I l(I)
Rosenblatt expects electronic surveillance in parole situations to become
more widespread, I I and he thinks eventually I I people will get used to
the idea. I I
16. Springfield jail deputy superintendent Ash says I I although it will allow
I I some prisoners to be released I I a few months I I before their sentences
are up, I I concerns that may raise about public safety I I are not well
founded. 1 I (A)
Springfield jail deputy superintendent Ash I says I I although it will allow
I some prisoners to be released I a few months before their sentences are
up, I I concerns that may raise I about public safety II are not well
founded. I I
17. Most county jail inmates I I did not commit violent crimes. 11(C)
18. They&apos;re in jail for such things I as bad checks or stealing. 1 I (A)
They&apos;re in jail for such things I as bad checks I or stealing. I I
19. Those on early release must check I in with corrections officials fifty times
I I a week according to Ash,I I who says about half I the contacts for a
select group I I will now be made I I by the computerized phone calls. 1 I (I)
Those on early release I must check in with corrections officials I I fifty
times a week I I according to Ash,I I who says about half I the contacts for
a select group I I will now be made I by the computerized phone calls. I I
20. Initially the program will involve I I only a handful of inmates. I I (A)
Initially I the program will involve I only a handful of inmates. I I
21. Ash says the ultimate goal I I is to use it to get I about forty out of jail
early. I I (A)
Ash says I the ultimate goal I I is to use it to get about forty I I out of jail
early. I I
22. The Springfield jail, I I built for 270 people, I now houses more than 500.
11(A)
The Springfield jail, I I built for 270 people, I I now houses more than 500.
11
23. For WBUR, I I I&apos;m Margo Melnicove. (C)
</reference>
<page confidence="0.998981">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.542968">
<title confidence="0.997528">A Hierarchical Stochastic Model for Automatic Prediction of Prosodic Boundary Location</title>
<author confidence="0.780056">M</author>
<affiliation confidence="0.989362">Boston University</affiliation>
<author confidence="0.979891">N Veilleux&apos;r</author>
<affiliation confidence="0.999867">Boston University</affiliation>
<abstract confidence="0.969510916666667">Prosodic phrase structure provides important information for the understanding and naturalness of synthetic speech, and a good model of prosodic phrases has applications in both speech synthesis and speech understanding. This work describes a statistical model of an embedded hierarchy of prosodic phrase structure, motivated by results in linguistic theory. Each level of the hierarchy is modeled as a sequence of subunits at the next level, with the lowest level of the hierarchy representing factors such as syntactic branching and prosodic constituent length using a binary tree classification. A maximum likelihood solution for parameter estimation is presented, allowing automatic training of different speaking styles. For predicting prosodic phrase breaks from text, a dynamic programming algorithm is given for finding the maximum probability prosodic parse. Experimental results on a corpus of radio news demonstrate a high rate of success for predicting major and minor phrase boundaries from text without syntactic information (81% correct prediction with 4% false prediction).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>S Hunnicutt</author>
<author>R Carlson</author>
<author>B Granstrom</author>
</authors>
<title>MITalk-79: The</title>
<date>1979</date>
<booktitle>In Speech Communications Papers Presented at the 97th Meeting of the ASA, edited by Wolf and Klatt,</booktitle>
<pages>507--510</pages>
<marker>Allen, Hunnicutt, Carlson, Granstrom, 1979</marker>
<rawString>Allen, J.; Hunnicutt, S.; Carlson, R.; and Granstrom, B. (1979). &amp;quot;MITalk-79: The 1979 MIT text-to-speech system.&amp;quot; In Speech Communications Papers Presented at the 97th Meeting of the ASA, edited by Wolf and Klatt, 507-510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>M S Hunnicutt</author>
<author>D Klatt</author>
</authors>
<title>From Text to Speech: The MITalk System.</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<marker>Allen, Hunnicutt, Klatt, 1987</marker>
<rawString>Allen, J.; Hunnicutt, M. S.; and Klatt, D. (1987). From Text to Speech: The MITalk System. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Altenberg</author>
</authors>
<title>Prosodic Patterns in Spoken English.</title>
<date>1987</date>
<publisher>Lund University Press.</publisher>
<contexts>
<context position="8257" citStr="Altenberg (1987)" startWordPosition="1240" endWordPosition="1241">e natural variability in prosodic structure by deriving probabilities of phrase breaks, rather than predicting locations of phrase breaks by rule. The relationship between prosody and syntax is not fully understood, though it is generally accepted that there is such a relationship. For example, relatively higher syntactic attachment usually corresponds to relatively larger prosodic breaks, but there are many exceptions, as in: (iMaryinp [was amazed [Ann Dewey was angrylnipis which was produced by four speakers as Mary was amazed H Ann Dewey was angry. In an analysis of the Londonâ€”Lund corpus, Altenberg (1987) finds relative frequencies that describe the correspondence between prosodic constituents (tone units) and different syntactic units. This data supports the use of a probabilistic model, which also has an advantage in that it can be trained automatically, facilitating representation of a wide variety of speaking styles and allowing a means of discovering syntax-prosody relationships from a large corpus. One reason that the mapping between syntax and prosody is not simple is because, in speech, the constraints of syntactic structure and phrase length are balanced to produce a regular, roughly </context>
<context position="14429" citStr="Altenberg (1987)" startWordPosition="2152" endWordPosition="2153">nly to predict performance structures. However, their work has been extended to prosodic phrase prediction for speech synthesis applications by Bachenko and Fitzpatrick (1990), who explicitly find prosodic phrase breaks 30 M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction from derived boundary salience indices. They relax many of the constraints on the use of the verb rule and propose a Verb Adjacency Rule, so their algorithm requires a fairly detailed parse, although not a complete one. One of the relaxed constraints obviates the need for clause information. Altenberg (1987) has also proposed an algorithm for prediction of phrase boundary locations (specifically, tone unit boundaries for British English) by rule from syntactic structure and semantic information. However, the detailed information required for the algorithm cannot currently be acquired automatically from text. Departing from these approaches, Wang and Hirschberg (1992) have recently used binary decision trees to predict the presence or absence of a prosodic break at each word boundary in a sentence. They consider a range of input variables, including textderived information such as detailed POS lab</context>
<context position="21587" citStr="Altenberg 1987" startWordPosition="3332" endWordPosition="3333">r prosodic phrases, although there is durational evidence that a more detailed hierarchy would be useful (Ladd and Campbell 1991; Wightman, Shattuck-Flufnagel, Ostendorf, and Price 1992). In the current work, we make several simplifying assumptions due to training limitations. First, the probability of the number of subunits in a unit p(n,ILI,) is assumed to depend only on the number of words in the unit i(U). As is not surprising, our data indicate that units that span a larger number of words tend to comprise more subunits. Altenberg has noticed similar tendencies in the Londonâ€”Lund corpus (Altenberg 1987, p. 81). (Alternatively, it has been suggested that either phonological word count or stressed syllable count rather than orthographic word count may be a useful measure of phrase length on the lowest level [Bachenko and Fitzpatrick 19901.) In addition, the probability distribution is approximated by conditioning on quantized lengths Qu(k(U,)). The quantizer varies as a function of the specific unit and is designed using a regression tree (Breiman, Friedman, Olshen, and Stone 1984). A regression tree partitions the data along intervals of a continuous variable, in this case length of the unit</context>
<context position="30202" citStr="Altenberg 1987" startWordPosition="4737" endWordPosition="4738">en text, so it is important that the algorithm not depend too heavily on commas. 35 Computational Linguistics Volume 20, Number I Syntactic features were extracted from skeletal parses provided through a preliminary version of the Penn Treebank Corpus. Since these are hand-corrected parses, the results are indicative of the performance possible using syntactic information, but do not reflect performance achievable with an existing parser. Several researchers have investigated the relationship between prosody and syntax (e.g., Selkirk 1984; Gee and Grosjean 1983; Cooper and Paccia-Cooper 1980; Altenberg 1987). Our features have been motivated by some of these results, which suggest that some syntactic constituents are more likely to be separated by a phrase break than others. However, we have chosen to let the important constituents be determined automatically, similar to Wang and Hirschberg (1992), rather than by rule. One feature is the highest syntactic constituent dominating the left word but not dominating the right word, which describes potential locations for phrase breaks after a specific syntactic constituent. We also consider the similar case, the highest syntactic constituent dominating</context>
</contexts>
<marker>Altenberg, 1987</marker>
<rawString>Altenberg, B. (1987). Prosodic Patterns in Spoken English. Lund University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bachenko</author>
<author>E Fitzpatrick</author>
</authors>
<title>A computational grammar of discourse-neutral prosodic phrasing in English.&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics</journal>
<volume>16</volume>
<issue>3</issue>
<pages>155--170</pages>
<contexts>
<context position="13988" citStr="Bachenko and Fitzpatrick (1990)" startWordPosition="2082" endWordPosition="2085">predict psycholinguistic &amp;quot;performance structures&amp;quot; that are represented by assigning an integer number corresponding to boundary salience between each pair of words. Constituent length information is incorporated primarily through the application of their verb balancing rule, which splits the verb phrase and groups the verb with either the previous or subsequent material, subject to syntactic constraints. Gee and Grosjean developed their Phi Algorithm only to predict performance structures. However, their work has been extended to prosodic phrase prediction for speech synthesis applications by Bachenko and Fitzpatrick (1990), who explicitly find prosodic phrase breaks 30 M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction from derived boundary salience indices. They relax many of the constraints on the use of the verb rule and propose a Verb Adjacency Rule, so their algorithm requires a fairly detailed parse, although not a complete one. One of the relaxed constraints obviates the need for clause information. Altenberg (1987) has also proposed an algorithm for prediction of phrase boundary locations (specifically, tone unit boundaries for British English) by rule from syntactic str</context>
<context position="21825" citStr="Bachenko and Fitzpatrick 1990" startWordPosition="3367" endWordPosition="3370">l simplifying assumptions due to training limitations. First, the probability of the number of subunits in a unit p(n,ILI,) is assumed to depend only on the number of words in the unit i(U). As is not surprising, our data indicate that units that span a larger number of words tend to comprise more subunits. Altenberg has noticed similar tendencies in the Londonâ€”Lund corpus (Altenberg 1987, p. 81). (Alternatively, it has been suggested that either phonological word count or stressed syllable count rather than orthographic word count may be a useful measure of phrase length on the lowest level [Bachenko and Fitzpatrick 19901.) In addition, the probability distribution is approximated by conditioning on quantized lengths Qu(k(U,)). The quantizer varies as a function of the specific unit and is designed using a regression tree (Breiman, Friedman, Olshen, and Stone 1984). A regression tree partitions the data along intervals of a continuous variable, in this case length of the unit, to decrease variance of the response variable, the number of subunits in the unit. The resulting quantizer regions and the corresponding distribution of subunits in a unit are given in Table 1 for major phrase and sentence units. Using </context>
<context position="49656" citStr="Bachenko and Fitzpatrick 1990" startWordPosition="7859" endWordPosition="7862">uestion provides information on the relative level of syntactic attachment between the two words, capturing the effect of constituent endings. 41 Computational Linguistics Volume 20, Number 1 13. How many syntactic units begin between the two words? This question is similar to the previous one, except that it captures effects associated with the start of new constituents. 14. How large is the ratio of the current minor phrase length over the previous minor phrase length? This question incorporates the concept of balancing minor phrase lengths noted by other researchers (Gee and Grosjean 1983; Bachenko and Fitzpatrick 1990), and was found to be useful in phrase prediction trees investigated by Wang and Hirschberg (1992). In the beginning of a sentence where there is no previous minor phrase, the ratio is treated as missing data and handled using a surrogate variable (Breiman, Friedman, Olshen, and Stone 1984). 15. What is the label of the content word to the right? to the left? Wang and Hirschberg found that part-of-speech information is useful in phrase break prediction (Wang and Hirschberg 1992). Questions 5, 12, 13, and 14 are based on numerical features, so the binary question asks whether the feature is gre</context>
<context position="66462" citStr="Bachenko and Fitzpatrick 1990" startWordPosition="10615" endWordPosition="10618">-of-speech tagging may not be necessary; however, further research is needed to answer this question. It may be that other POS questions, such as testing a larger window of words around the break as in Wang and Hirschberg (1992), would yield better results. Finally, we thought it would be interesting to compare the results of our prediction algorithm to that of Bachenko and Fitzpatrick&apos;s algorithm on our corpus. Since the test set was relatively small, we were able to implement the algorithm by predicting the phrase boundaries from the rules by hand. To assign node indices to prosodic breaks (Bachenko and Fitzpatrick 1990), a critical value for separating major and minor phrase breaks is calculated based on an average of the indices associated with the prosodic phrase nodes, where the prosodic phrase nodes are all those created by the Bachenkoâ€”Fitzpatrick primary salience rules. Boundaries with an index greater than the critical value are assigned a major break, indices below 5 have no prosodic break, and intermediate indices map to a minor prosodic break. (Bachenko and Fitzpatrick include index 4 in the minor break category, but 5 was used here to obtain a lower insertion rate.) For multiple verb phrases in se</context>
<context position="71945" citStr="Bachenko and Fitzpatrick (1990)" startWordPosition="11448" endWordPosition="11451"> results for the BachenkoFitzpatrick algorithm are somewhat higher than those that they report, .84/.09 vs. .78/.08.2 Using only the features inferable from text, Wang and Hirschberg use classification trees to predict prosodic boundaries in spontaneous speech, achieving phrase break prediction results of .66/.02.3 (Again, note that these results are not directly comparable because of the differences in false detection rates, and results for other trees in Wang and Hirschberg [1992] suggest that these two algorithms have similar 2 This figure is calculated from the examples in the appendix in Bachenko and Fitzpatrick (1990), ignoring tertiary boundaries and including sentence-final boundaries as correct. The sentence that did not parse was not included in the calculation. 3 This result is computed from Figure 6 of Wang and Hirschberg (1992), which illustrates classification on training data. Cross validation results may vary slightly. 49 Computational Linguistics Volume 20, Number 1 performance.) Our classification trees used somewhat different features, though also based on POS and syntactic information, and achieved results on radio news speech that are surprisingly lower, i.e., .59/.02 for the tree that used </context>
</contexts>
<marker>Bachenko, Fitzpatrick, 1990</marker>
<rawString>Bachenko, J., and Fitzpatrick, E. (1990). &amp;quot;A computational grammar of discourse-neutral prosodic phrasing in English.&amp;quot; Computational Linguistics 16(3), 155-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>P E Brown</author>
<author>P V deSouza</author>
<author>R L Mercer</author>
</authors>
<title>A tree-based statistical language model for natural language speech recognition.&amp;quot;</title>
<date>1989</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing</journal>
<volume>37</volume>
<issue>7</issue>
<pages>1001--1008</pages>
<marker>Bahl, Brown, deSouza, Mercer, 1989</marker>
<rawString>Bahl, L. R.; Brown, P. E; deSouza, P. V.; and Mercer, R. L. (1989). &amp;quot;A tree-based statistical language model for natural language speech recognition.&amp;quot; IEEE Transactions on Acoustics, Speech, and Signal Processing 37(7), 1001-1008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bear</author>
<author>P J Price</author>
</authors>
<title>Prosody, syntax and parsing.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, ACL Conference,</booktitle>
<pages>17--22</pages>
<contexts>
<context position="76854" citStr="Bear and Price (1990)" startWordPosition="12198" endWordPosition="12201">improve the performance and/or speed of speech understanding systems. For example, a score of the consistency be50 M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction tween a syntactic parse and a prosodic parse has been used to resolve ambiguities in sentence interpretation, where the score is computed by comparing automatically detected prosodic phrase breaks to phrase breaks predicted from the text of the different interpretations (Ostendorf, Wightman, and Veilleux 1993). Another approach to syntactic disambiguation using prosodic information is described in Bear and Price (1990) and Ostendorf, Price, Bear, and Wightman (1990), where prosodic breaks are used to constrain grammar rules in a parser. A stochastic phrase model could also be used to improve performance of this system simply by serving as a &amp;quot;language model&amp;quot; (as in speech recognition) to improve the performance of a detection algorithm using acoustic information. Acknowledgments We would like to thank Patti Price and Stefanie Shattuck-Hufnagel for their useful comments and insights, an anonymous reviewer for many valuable comments, and Gay Baldwin and Liz Shriberg for hand-labeling part of the corpus with pr</context>
</contexts>
<marker>Bear, Price, 1990</marker>
<rawString>Bear, J., and Price, P. J. (1990). &amp;quot;Prosody, syntax and parsing.&amp;quot; In Proceedings, ACL Conference, 17-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Beckman</author>
<author>J Pierrehumbert</author>
</authors>
<title>Intonational structure in Japanese and English.&amp;quot;</title>
<date>1986</date>
<booktitle>In Phonology Yearbook 3,</booktitle>
<pages>255--309</pages>
<note>edited by</note>
<contexts>
<context position="4269" citStr="Beckman and Pierrehumbert 1986" startWordPosition="614" endWordPosition="617">synthesis. Third, prosodic phrase breaks do not always coincide with syntactic phrase boundaries, and the relationship between prosody and syntax is not well understood. This means that prosodic phrases cannot simply be predicted from syntactic structure. Finally, since most text-to-speech synthesis applications require a low cost implementation, there is the concern of computational complexity. We shall expand on these points separately below, to motivate the work described here. The various linguistic theories of prosodic phrase structure (e.g., Liberman and Prince 1977; Selkirk 1980, 1984; Beckman and Pierrehumbert 1986; Nespor and Vogel 1983; Ladd 1986) differ in the specific levels that they represent, but all have a similar hierarchical structure. Two levels of prosodic phrases are common to most proposals: the intonational phrase and the intermediate phrase, using the terminology of Beckman and Pierrehumbert. A sentence is composed of a sequence of intonational phrases, which in turn are composed of sequences of intermediate phrases. An intonational phrase break is therefore perceived as stronger or more salient than an intermediate phrase break. Intonational phrases are delimited by boundary tones, and </context>
</contexts>
<marker>Beckman, Pierrehumbert, 1986</marker>
<rawString>Beckman, M., and Pierrehumbert, J. (1986). &amp;quot;Intonational structure in Japanese and English.&amp;quot; In Phonology Yearbook 3, edited by J. Ohala, 255-309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Booij</author>
</authors>
<title>Principles and parameters in prosodic phonology.&amp;quot;</title>
<date>1983</date>
<journal>Linguistics</journal>
<volume>21</volume>
<pages>249--280</pages>
<contexts>
<context position="19864" citStr="Booij 1983" startWordPosition="3031" endWordPosition="3032">on values from the set {no break, minor break, major break, sentence break}. It might be useful to consider phonological words rather than orthographic words as possible sites for break indices. This could be accomplished, without using deterministic rules, by specifying the bottom of the hierarchy (e.g., break level 0) to represent locations internal to a phonological word and the next level of the hierarchy (e.g., break level I) to represent phonological word boundaries. However, it is controversial as to whether phonological words can be larger or smaller than orthographic (lexical) words (Booij 1983; Nespor and Vogel 1983), so it is not clear how the lowest level should be defined relative to the orthographic words. In this work, we have chosen not to distinguish between these two levels, to reduce the complexity of implementation and performance evaluation. For similar reasons, we have limited comprises syntactically well-formed sentences. The phrase prediction model may also be useful in speech recognition applications, in which case the term &amp;quot;utterance&amp;quot; would clearly be more appropriate. 32 M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction Table 1 His</context>
</contexts>
<marker>Booij, 1983</marker>
<rawString>Booij, G. (1983). &amp;quot;Principles and parameters in prosodic phonology.&amp;quot; Linguistics 21, 249-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
<author>J Friedman</author>
<author>R Olshen</author>
<author>C Stone</author>
</authors>
<title>Classification and Regression Trees, The Wadsworth Statistics/Probability Series,</title>
<date>1984</date>
<location>Wadsworth and Brooks.</location>
<marker>Breiman, Friedman, Olshen, Stone, 1984</marker>
<rawString>Breiman, L.; Friedman, J.; Olshen R.; and Stone, C. (1984). Classification and Regression Trees, The Wadsworth Statistics/Probability Series, Wadsworth and Brooks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="15171" citStr="Church 1988" startWordPosition="2261" endWordPosition="2262">h) by rule from syntactic structure and semantic information. However, the detailed information required for the algorithm cannot currently be acquired automatically from text. Departing from these approaches, Wang and Hirschberg (1992) have recently used binary decision trees to predict the presence or absence of a prosodic break at each word boundary in a sentence. They consider a range of input variables, including textderived information such as detailed POS labels and syntactic constituent structure, and in some experiments, acoustic information. POS labels were given by Church&apos;s tagger (Church 1988) and syntactic constituents by Hindle&apos;s parser (Hindle 1987). The acoustic information (previous boundary location, pitch accent location, and phrase duration), which was based on hand-labeled prosodic markers, did not improve performance but resulted in a much smaller tree for prediction. All of these approaches have influenced the model proposed here. For example, we investigate simple content/function word POS assignment, as in Sorin, Larreur, and Llorca (1987). Like Wang and Hirschberg (1992), we use decision trees to automatically determine the important factors influencing phrase break l</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, K. W. (1988). &amp;quot;A stochastic parts program and noun phrase parser for unrestricted text.&amp;quot; In Proceedings, Second Conference on Applied Natural Language Processing, 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cooper</author>
<author>J Paccia-Cooper</author>
</authors>
<title>Syntax and Speech.</title>
<date>1980</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="30185" citStr="Cooper and Paccia-Cooper 1980" startWordPosition="4733" endWordPosition="4736"> not consistently used in written text, so it is important that the algorithm not depend too heavily on commas. 35 Computational Linguistics Volume 20, Number I Syntactic features were extracted from skeletal parses provided through a preliminary version of the Penn Treebank Corpus. Since these are hand-corrected parses, the results are indicative of the performance possible using syntactic information, but do not reflect performance achievable with an existing parser. Several researchers have investigated the relationship between prosody and syntax (e.g., Selkirk 1984; Gee and Grosjean 1983; Cooper and Paccia-Cooper 1980; Altenberg 1987). Our features have been motivated by some of these results, which suggest that some syntactic constituents are more likely to be separated by a phrase break than others. However, we have chosen to let the important constituents be determined automatically, similar to Wang and Hirschberg (1992), rather than by rule. One feature is the highest syntactic constituent dominating the left word but not dominating the right word, which describes potential locations for phrase breaks after a specific syntactic constituent. We also consider the similar case, the highest syntactic const</context>
</contexts>
<marker>Cooper, Paccia-Cooper, 1980</marker>
<rawString>Cooper, W., and Paccia-Cooper, J. (1980). Syntax and Speech. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gee</author>
<author>E Grosjean</author>
</authors>
<title>Performance structures: A psycholinguistic and linguistic appraisal.&amp;quot;</title>
<date>1983</date>
<journal>Cognitive Psychology</journal>
<pages>15--411</pages>
<contexts>
<context position="8916" citStr="Gee and Grosjean 1983" startWordPosition="1337" endWordPosition="1340">ribe the correspondence between prosodic constituents (tone units) and different syntactic units. This data supports the use of a probabilistic model, which also has an advantage in that it can be trained automatically, facilitating representation of a wide variety of speaking styles and allowing a means of discovering syntax-prosody relationships from a large corpus. One reason that the mapping between syntax and prosody is not simple is because, in speech, the constraints of syntactic structure and phrase length are balanced to produce a regular, roughly equal, sequence of prosodic phrases (Gee and Grosjean 1983). Consequently, we include constituent length as a factor in the model. The cost of obtaining a full and accurate syntactic parse can be high, which presents difficulties for text-to-speech synthesis systems. In addition, a full syntactic parse may not be necessary for predicting prosodic phrases, since prosody is not directly related to syntax. Consequently, we investigate computation/performance trade-offs associated with using a skeletal syntactic parse vs. simple part-of-speech (POS) assignments. To summarize, the model proposed here addresses several issues in modeling prosodic phrase str</context>
<context position="13296" citStr="Gee and Grosjean (1983)" startWordPosition="1984" endWordPosition="1987">f-speech labels. Motivated by similar principles and using only a 300-word dictionary, O&apos;Shaughnessy (1989) proposes a somewhat more sophisticated parser for English based on function word identification, number agreement, and suffix identification. O&apos;Shaughnessy&apos;s work differs from the other approaches in that his goal is a syntactic parse, though not complete, and he does not address the issue of differences between prosody and syntax. At the other end of the spectrum are approaches based on the hypothesis that prosodic phrase boundaries can be predicted by rule from a full syntactic parse. Gee and Grosjean (1983) developed a rule-based system, called the Phi Algorithm, to predict psycholinguistic &amp;quot;performance structures&amp;quot; that are represented by assigning an integer number corresponding to boundary salience between each pair of words. Constituent length information is incorporated primarily through the application of their verb balancing rule, which splits the verb phrase and groups the verb with either the previous or subsequent material, subject to syntactic constraints. Gee and Grosjean developed their Phi Algorithm only to predict performance structures. However, their work has been extended to pro</context>
<context position="30154" citStr="Gee and Grosjean 1983" startWordPosition="4729" endWordPosition="4732">rom spoken language and not consistently used in written text, so it is important that the algorithm not depend too heavily on commas. 35 Computational Linguistics Volume 20, Number I Syntactic features were extracted from skeletal parses provided through a preliminary version of the Penn Treebank Corpus. Since these are hand-corrected parses, the results are indicative of the performance possible using syntactic information, but do not reflect performance achievable with an existing parser. Several researchers have investigated the relationship between prosody and syntax (e.g., Selkirk 1984; Gee and Grosjean 1983; Cooper and Paccia-Cooper 1980; Altenberg 1987). Our features have been motivated by some of these results, which suggest that some syntactic constituents are more likely to be separated by a phrase break than others. However, we have chosen to let the important constituents be determined automatically, similar to Wang and Hirschberg (1992), rather than by rule. One feature is the highest syntactic constituent dominating the left word but not dominating the right word, which describes potential locations for phrase breaks after a specific syntactic constituent. We also consider the similar ca</context>
<context position="49624" citStr="Gee and Grosjean 1983" startWordPosition="7855" endWordPosition="7858">n the two words? This question provides information on the relative level of syntactic attachment between the two words, capturing the effect of constituent endings. 41 Computational Linguistics Volume 20, Number 1 13. How many syntactic units begin between the two words? This question is similar to the previous one, except that it captures effects associated with the start of new constituents. 14. How large is the ratio of the current minor phrase length over the previous minor phrase length? This question incorporates the concept of balancing minor phrase lengths noted by other researchers (Gee and Grosjean 1983; Bachenko and Fitzpatrick 1990), and was found to be useful in phrase prediction trees investigated by Wang and Hirschberg (1992). In the beginning of a sentence where there is no previous minor phrase, the ratio is treated as missing data and handled using a surrogate variable (Breiman, Friedman, Olshen, and Stone 1984). 15. What is the label of the content word to the right? to the left? Wang and Hirschberg found that part-of-speech information is useful in phrase break prediction (Wang and Hirschberg 1992). Questions 5, 12, 13, and 14 are based on numerical features, so the binary question</context>
</contexts>
<marker>Gee, Grosjean, 1983</marker>
<rawString>Gee, J., and Grosjean, E (1983). &amp;quot;Performance structures: A psycholinguistic and linguistic appraisal.&amp;quot; Cognitive Psychology 15,411-458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Hindle</author>
</authors>
<title>Acquiring disambiguation rules from text.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, Association for Computational Linguistics Meeting,</booktitle>
<pages>118--125</pages>
<contexts>
<context position="15231" citStr="Hindle 1987" startWordPosition="2269" endWordPosition="2270">. However, the detailed information required for the algorithm cannot currently be acquired automatically from text. Departing from these approaches, Wang and Hirschberg (1992) have recently used binary decision trees to predict the presence or absence of a prosodic break at each word boundary in a sentence. They consider a range of input variables, including textderived information such as detailed POS labels and syntactic constituent structure, and in some experiments, acoustic information. POS labels were given by Church&apos;s tagger (Church 1988) and syntactic constituents by Hindle&apos;s parser (Hindle 1987). The acoustic information (previous boundary location, pitch accent location, and phrase duration), which was based on hand-labeled prosodic markers, did not improve performance but resulted in a much smaller tree for prediction. All of these approaches have influenced the model proposed here. For example, we investigate simple content/function word POS assignment, as in Sorin, Larreur, and Llorca (1987). Like Wang and Hirschberg (1992), we use decision trees to automatically determine the important factors influencing phrase break location. In addition, all of the above works have influenced</context>
</contexts>
<marker>Hindle, 1987</marker>
<rawString>Hindle, D. M. (1987). &amp;quot;Acquiring disambiguation rules from text.&amp;quot; In Proceedings, Association for Computational Linguistics Meeting, 118-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hirose</author>
<author>H Fujisaki</author>
</authors>
<title>Analysis and synthesis of voice fundamental frequency contours of spoken sentences.&amp;quot;</title>
<date>1982</date>
<booktitle>In Proceedings, International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>950--953</pages>
<contexts>
<context position="3107" citStr="Hirose and Fujisaki 1982" startWordPosition="445" endWordPosition="448">s that are associated with acoustic cues such as duration lengthening, pause insertion, and intonation markers. In this work, we are concerned only with the relationship between the abstract events ECS Department, 44 Cummington Street, Boston, MA 02215 C) 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number] (different levels of phrase breaks) and text. To be useful in synthesis or understanding applications, the results presented here need to be integrated with a component that models the acoustics associated with these abstract events (see, for example, Hirose and Fujisaki 1982). Several observations about prosodic phrase breaks raise issues to be considered in designing an algorithm to predict such breaks from text. First, there is a significant body of literature in linguistics concerning various hierarchies that specify the relationship among prosodic constituents, and the model should reflect this structure. Second, several different prosodic parses may all be acceptable for one sentence. This variability is particularly important to represent if the model is to be useful for analysis as well as synthesis. Third, prosodic phrase breaks do not always coincide with</context>
</contexts>
<marker>Hirose, Fujisaki, 1982</marker>
<rawString>Hirose, K., and Fujisaki, H. (1982). &amp;quot;Analysis and synthesis of voice fundamental frequency contours of spoken sentences.&amp;quot; In Proceedings, International Conference on Acoustics, Speech, and Signal Processing, 950-953.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Ladd</author>
</authors>
<title>Intonational phrasing: The case for recursive prosodic structure.&amp;quot;</title>
<date>1986</date>
<booktitle>In Phonology Yearbook 3,</booktitle>
<pages>311--340</pages>
<note>edited by</note>
<contexts>
<context position="4304" citStr="Ladd 1986" startWordPosition="622" endWordPosition="623">coincide with syntactic phrase boundaries, and the relationship between prosody and syntax is not well understood. This means that prosodic phrases cannot simply be predicted from syntactic structure. Finally, since most text-to-speech synthesis applications require a low cost implementation, there is the concern of computational complexity. We shall expand on these points separately below, to motivate the work described here. The various linguistic theories of prosodic phrase structure (e.g., Liberman and Prince 1977; Selkirk 1980, 1984; Beckman and Pierrehumbert 1986; Nespor and Vogel 1983; Ladd 1986) differ in the specific levels that they represent, but all have a similar hierarchical structure. Two levels of prosodic phrases are common to most proposals: the intonational phrase and the intermediate phrase, using the terminology of Beckman and Pierrehumbert. A sentence is composed of a sequence of intonational phrases, which in turn are composed of sequences of intermediate phrases. An intonational phrase break is therefore perceived as stronger or more salient than an intermediate phrase break. Intonational phrases are delimited by boundary tones, and intermediate phrases are theoretica</context>
</contexts>
<marker>Ladd, 1986</marker>
<rawString>Ladd, D. R. (1986). &amp;quot;Intonational phrasing: The case for recursive prosodic structure.&amp;quot; In Phonology Yearbook 3, edited by J. -Ohala, 311-340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Ladd</author>
<author>N Campbell</author>
</authors>
<title>Theories of prosodic structure: Evidence from syllable duration.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, XII International Congress of Phonetic Sciences,</booktitle>
<pages>2--290</pages>
<contexts>
<context position="5775" citStr="Ladd and Campbell 1991" startWordPosition="841" endWordPosition="844">t are identified with movement and referred to as either rising or falling.) Both types of constituents are also cued by segmental lengthening in the phrase final syllable (Wightman, Shattuck-Hufnagel, Ostendorf, and Price 1992). Since intonational and intermediate phrases are generally accepted, the experiments here will only address these two levels, referring to them as major and minor phrases, respectively. However, other types of prosodic constituents may be useful and, in fact, there is durational evidence for at least four levels (Wightman, Shattuck-Hufnagel, Ostendorf, and Price 1991; Ladd and Campbell 1991). We therefore propose a more general hierarchical model that can be extended to an arbitrary, but fixed, number of levels. In the examples given here, we will represent intonational phrases (I) using &amp;quot;II&amp;quot; to mark a major break and intermediate phrases (i) using &amp;quot;I&amp;quot; to mark a minor break. The example below illustrates how phrase breaks are used to represent prosodic phrase structure: Those on early release I must check in with correction officials II fifty times a week 11 according to Ash, hi who says about half I the contacts for a select group will now be made I by the computerized phone cal</context>
<context position="21101" citStr="Ladd and Campbell 1991" startWordPosition="3253" endWordPosition="3256">er of minor phrases in a major phrase and number of major phrases in a sentence, as a function of quantized length of the unit. The quantizer regions are indicated by the length ranges. Number of minor phrases in major phrase Number of major phrases in sentence t(A/1) 1 2 3 4 5 â‚¬(5) 1 2 3 4 5 6 1-5 522 69 2 1-14 31 39 11 2 1 6-7 106 78 9 15-21 4 23 28 17 9 5 8-14 61 107 43 1 22-31 6 6 20 24 11 15-19 2 2 1 1 32-37 2 6 4 this study to the more universally agreed-upon levels of major and minor prosodic phrases, although there is durational evidence that a more detailed hierarchy would be useful (Ladd and Campbell 1991; Wightman, Shattuck-Flufnagel, Ostendorf, and Price 1992). In the current work, we make several simplifying assumptions due to training limitations. First, the probability of the number of subunits in a unit p(n,ILI,) is assumed to depend only on the number of words in the unit i(U). As is not surprising, our data indicate that units that span a larger number of words tend to comprise more subunits. Altenberg has noticed similar tendencies in the Londonâ€”Lund corpus (Altenberg 1987, p. 81). (Alternatively, it has been suggested that either phonological word count or stressed syllable count rat</context>
</contexts>
<marker>Ladd, Campbell, 1991</marker>
<rawString>Ladd, D. R., and Campbell, N. (1991). &amp;quot;Theories of prosodic structure: Evidence from syllable duration.&amp;quot; In Proceedings, XII International Congress of Phonetic Sciences, 2,290-293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Lehiste</author>
</authors>
<title>Phonetic disambiguation of syntactic ambiguity.&amp;quot;</title>
<date>1973</date>
<journal>Glossa</journal>
<volume>7</volume>
<issue>2</issue>
<pages>107--121</pages>
<contexts>
<context position="1598" citStr="Lehiste 1973" startWordPosition="227" endWordPosition="228">imum probability prosodic parse. Experimental results on a corpus of radio news demonstrate a high rate of success for predicting major and minor phrase boundaries from text without syntactic information (81% correct prediction with 4% false prediction). 1. Introduction Prosodic phrase structure plays a role in both naturalness and intelligibility of speech. For example, prosodic phrase boundaries break the flow of a sentence, dividing it into smaller units for easier processing. In addition, researchers have shown that prosodic phrase break placement is important in syntactic disambiguation (Lehiste 1973; Price, Ostendorf, Shattuck-Hufnagel, and Fong 1991). For these reasons, computational modeling of prosodic phrases is important both for text-to-speech synthesis and speech understanding applications. In this work, we present a computational model that represents a hierarchy of prosodic constituents using a stochastic formalism to capture the natural variability allowable in prosodic phrasing. The model is useful for both analysis and synthesis applications; we focus on synthesis here, and present experimental results for predicting prosodic phrase structure from text. Prosodic phrase struct</context>
</contexts>
<marker>Lehiste, 1973</marker>
<rawString>Lehiste, I. (1973). &amp;quot;Phonetic disambiguation of syntactic ambiguity.&amp;quot; Glossa 7(2), 107-121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Y Liberman</author>
<author>A S Prince</author>
</authors>
<title>On stress and linguistic rhythm.&amp;quot;</title>
<date>1977</date>
<journal>Linguistic Inquiry</journal>
<pages>8--249</pages>
<contexts>
<context position="4217" citStr="Liberman and Prince 1977" startWordPosition="607" endWordPosition="610">model is to be useful for analysis as well as synthesis. Third, prosodic phrase breaks do not always coincide with syntactic phrase boundaries, and the relationship between prosody and syntax is not well understood. This means that prosodic phrases cannot simply be predicted from syntactic structure. Finally, since most text-to-speech synthesis applications require a low cost implementation, there is the concern of computational complexity. We shall expand on these points separately below, to motivate the work described here. The various linguistic theories of prosodic phrase structure (e.g., Liberman and Prince 1977; Selkirk 1980, 1984; Beckman and Pierrehumbert 1986; Nespor and Vogel 1983; Ladd 1986) differ in the specific levels that they represent, but all have a similar hierarchical structure. Two levels of prosodic phrases are common to most proposals: the intonational phrase and the intermediate phrase, using the terminology of Beckman and Pierrehumbert. A sentence is composed of a sequence of intonational phrases, which in turn are composed of sequences of intermediate phrases. An intonational phrase break is therefore perceived as stronger or more salient than an intermediate phrase break. Intona</context>
</contexts>
<marker>Liberman, Prince, 1977</marker>
<rawString>Liberman, M. Y, and Prince, A. S. (1977). &amp;quot;On stress and linguistic rhythm.&amp;quot; Linguistic Inquiry 8,249-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a very large annotated corpus of English:</title>
<date>1993</date>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M. P.; Santorini, B.; and Marcinkiewicz, M. (1993). &amp;quot;Building a very large annotated corpus of English:</rawString>
</citation>
<citation valid="false">
<title>The Penn Treebank.&amp;quot;</title>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<marker></marker>
<rawString>The Penn Treebank.&amp;quot; Computational Linguistics 19(2), 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meteer</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>POST: Using probabilities in language processing.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, International Joint Conference on Artificial Intelligence,</booktitle>
<pages>960--965</pages>
<marker>Meteer, Schwartz, Weischedel, 1991</marker>
<rawString>Meteer, M.; Schwartz, R.; and Weischedel, R. (1991). &amp;quot;POST: Using probabilities in language processing.&amp;quot; In Proceedings, International Joint Conference on Artificial Intelligence, 960-965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nespor</author>
<author>I Vogel</author>
</authors>
<title>Prosodic structure above the word.&amp;quot; In Prosody: Models and Measurements, edited by A. Cutler and a</title>
<date>1983</date>
<pages>123--140</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="4292" citStr="Nespor and Vogel 1983" startWordPosition="618" endWordPosition="621">e breaks do not always coincide with syntactic phrase boundaries, and the relationship between prosody and syntax is not well understood. This means that prosodic phrases cannot simply be predicted from syntactic structure. Finally, since most text-to-speech synthesis applications require a low cost implementation, there is the concern of computational complexity. We shall expand on these points separately below, to motivate the work described here. The various linguistic theories of prosodic phrase structure (e.g., Liberman and Prince 1977; Selkirk 1980, 1984; Beckman and Pierrehumbert 1986; Nespor and Vogel 1983; Ladd 1986) differ in the specific levels that they represent, but all have a similar hierarchical structure. Two levels of prosodic phrases are common to most proposals: the intonational phrase and the intermediate phrase, using the terminology of Beckman and Pierrehumbert. A sentence is composed of a sequence of intonational phrases, which in turn are composed of sequences of intermediate phrases. An intonational phrase break is therefore perceived as stronger or more salient than an intermediate phrase break. Intonational phrases are delimited by boundary tones, and intermediate phrases ar</context>
<context position="19888" citStr="Nespor and Vogel 1983" startWordPosition="3033" endWordPosition="3036">om the set {no break, minor break, major break, sentence break}. It might be useful to consider phonological words rather than orthographic words as possible sites for break indices. This could be accomplished, without using deterministic rules, by specifying the bottom of the hierarchy (e.g., break level 0) to represent locations internal to a phonological word and the next level of the hierarchy (e.g., break level I) to represent phonological word boundaries. However, it is controversial as to whether phonological words can be larger or smaller than orthographic (lexical) words (Booij 1983; Nespor and Vogel 1983), so it is not clear how the lowest level should be defined relative to the orthographic words. In this work, we have chosen not to distinguish between these two levels, to reduce the complexity of implementation and performance evaluation. For similar reasons, we have limited comprises syntactically well-formed sentences. The phrase prediction model may also be useful in speech recognition applications, in which case the term &amp;quot;utterance&amp;quot; would clearly be more appropriate. 32 M. Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Prediction Table 1 Histograms of number of min</context>
</contexts>
<marker>Nespor, Vogel, 1983</marker>
<rawString>Nespor, M., and Vogel, I. (1983). &amp;quot;Prosodic structure above the word.&amp;quot; In Prosody: Models and Measurements, edited by A. Cutler and a R. Ladd, 123-140. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nespor</author>
<author>I Vogel</author>
</authors>
<date>1986</date>
<note>Prosodic Phonology. Foris.</note>
<marker>Nespor, Vogel, 1986</marker>
<rawString>Nespor, M., and Vogel, I. (1986). Prosodic Phonology. Foris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D O&apos;Shaughnessy</author>
</authors>
<title>Parsing with a small dictionary for applications such as text-to-speech.&amp;quot;</title>
<date>1989</date>
<journal>Computational Linguistics</journal>
<volume>15</volume>
<issue>2</issue>
<pages>97--108</pages>
<contexts>
<context position="12780" citStr="O&apos;Shaughnessy (1989)" startWordPosition="1905" endWordPosition="1906">gth and relative location of these prosodic groups is then used to determine phrase break locations that are marked with a pause. Our earlier work drew on this scheme for predicting phrase boundaries in English: a Markov model was developed to predict phrase breaks by representing the sequence of prosodic groups and breaks as a Markov chain (Veilleux, Ostendorf, Price, and Shattuck-Hufnagel 1990). An advantage of these approaches is that they only require a small dictionary of function words to assign part-of-speech labels. Motivated by similar principles and using only a 300-word dictionary, O&apos;Shaughnessy (1989) proposes a somewhat more sophisticated parser for English based on function word identification, number agreement, and suffix identification. O&apos;Shaughnessy&apos;s work differs from the other approaches in that his goal is a syntactic parse, though not complete, and he does not address the issue of differences between prosody and syntax. At the other end of the spectrum are approaches based on the hypothesis that prosodic phrase boundaries can be predicted by rule from a full syntactic parse. Gee and Grosjean (1983) developed a rule-based system, called the Phi Algorithm, to predict psycholinguisti</context>
<context position="29452" citStr="O&apos;Shaughnessy 1989" startWordPosition="4630" endWordPosition="4631">prosodic break. Although using commas deterministically to assign a major phrase boundary yields better performance on our test set than using commas as a feature, we felt that using commas as a feature was a more extensible approach, and have used this strategy in the results reported here. Including commas as a feature does improve performance relative to not using commas, as will be discussed in Section 4. Commas (and other punctuation) can be very useful for prosodic boundary prediction when they are available, and they are used in other algorithms (e.g., Allen, Hunnicutt, and Klatt 1987; O&apos;Shaughnessy 1989; I3achenko and Fitzpatrick 1990). However, commas are not reliably transcribed from spoken language and not consistently used in written text, so it is important that the algorithm not depend too heavily on commas. 35 Computational Linguistics Volume 20, Number I Syntactic features were extracted from skeletal parses provided through a preliminary version of the Penn Treebank Corpus. Since these are hand-corrected parses, the results are indicative of the performance possible using syntactic information, but do not reflect performance achievable with an existing parser. Several researchers ha</context>
</contexts>
<marker>O&apos;Shaughnessy, 1989</marker>
<rawString>O&apos;Shaughnessy, D. (1989). &amp;quot;Parsing with a small dictionary for applications such as text-to-speech.&amp;quot; Computational Linguistics 15(2), 97-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>P Price</author>
<author>J Bear</author>
<author>C W Wightman</author>
</authors>
<title>The use of relative duration in syntactic disambiguation.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Third DARPA Workshop on Speech and Natural Language,</booktitle>
<pages>26--31</pages>
<marker>Ostendorf, Price, Bear, Wightman, 1990</marker>
<rawString>Ostendorf, M.; Price, P.; Bear, J.; and Wightman, C. W. (1990). &amp;quot;The use of relative duration in syntactic disambiguation.&amp;quot; In Proceedings, Third DARPA Workshop on Speech and Natural Language, 26-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>S Roukos</author>
</authors>
<title>A stochastic segment model for phoneme-based continuous speech recognition.&amp;quot;</title>
<date>1989</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing</journal>
<volume>37</volume>
<issue>12</issue>
<pages>1857--1869</pages>
<contexts>
<context position="35472" citStr="Ostendorf and Roukos 1989" startWordPosition="5585" endWordPosition="5588">Phrase Break Prediction Algorithm The stochastic model can be used to predict a prosodic parse for a sentence simply by finding the most probable prosodic parse for that sequence of words, where the probability of any given parse is determined by Equations (4)â€”(6). In other words, we hypothesize all possible prosodic parses, compute the probability of each, and choose the most probable. The most likely prosodic parse can be found efficiently using a dynamic programming (DP) algorithm that is similar to algorithms used in speech recognition, in particular that for the Stochastic Segment Model (Ostendorf and Roukos 1989), except that the dynamic programming routine is called recursively for successive levels in the hierarchy. Defining pr(un ...uthIVPI,U1) as the probability of the most likely sequence of n subunits in but not necessarily spanning U, and ending at location t, and u,i(s, t) as a subunit that spans boundaries {bs,...,bt}, the 37 Computational Linguistics Volume 20, Number 1 dynamic programming algorithm can be expressed generally in the subroutine that follows. This subroutine is called recursively for each level of the hierarchy, with the lowest level constituent probability being computed usin</context>
</contexts>
<marker>Ostendorf, Roukos, 1989</marker>
<rawString>Ostendorf, M., and Roukos, S. (1989). &amp;quot;A stochastic segment model for phoneme-based continuous speech recognition.&amp;quot; IEEE Transactions on Acoustics, Speech, and Signal Processing 37(12), 1857-1869.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>C W Wightman</author>
<author>N M Veilleux</author>
</authors>
<title>Parse scoring with prosodic information: An analysis/synthesis approach.&amp;quot;</title>
<date>1993</date>
<journal>Computer Speech and Language,</journal>
<pages>193--210</pages>
<marker>Ostendorf, Wightman, Veilleux, 1993</marker>
<rawString>Ostendorf, M.; Wightman, C. W.; and Veilleux, N. M. (1993). &amp;quot;Parse scoring with prosodic information: An analysis/synthesis approach.&amp;quot; Computer Speech and Language, 193-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Price</author>
<author>M Ostendorf</author>
<author>S Shattuck-Hufnagel</author>
<author>C Fong</author>
</authors>
<title>The use of prosody in syntactic disambiguation.&amp;quot;</title>
<date>1991</date>
<journal>Journal of the Acoustical Society of America</journal>
<volume>90</volume>
<issue>6</issue>
<pages>2956--2970</pages>
<marker>Price, Ostendorf, Shattuck-Hufnagel, Fong, 1991</marker>
<rawString>Price, P.; Ostendorf, M.; Shattuck-Hufnagel, S.; and Fong, C. (1991). &amp;quot;The use of prosody in syntactic disambiguation.&amp;quot; Journal of the Acoustical Society of America 90(6), 2956-2970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Selkirk</author>
</authors>
<title>The role of prosodic categories in English word stress.&amp;quot;</title>
<date>1980</date>
<journal>Linguistic Inquiry</journal>
<pages>11--563</pages>
<contexts>
<context position="4231" citStr="Selkirk 1980" startWordPosition="611" endWordPosition="612">analysis as well as synthesis. Third, prosodic phrase breaks do not always coincide with syntactic phrase boundaries, and the relationship between prosody and syntax is not well understood. This means that prosodic phrases cannot simply be predicted from syntactic structure. Finally, since most text-to-speech synthesis applications require a low cost implementation, there is the concern of computational complexity. We shall expand on these points separately below, to motivate the work described here. The various linguistic theories of prosodic phrase structure (e.g., Liberman and Prince 1977; Selkirk 1980, 1984; Beckman and Pierrehumbert 1986; Nespor and Vogel 1983; Ladd 1986) differ in the specific levels that they represent, but all have a similar hierarchical structure. Two levels of prosodic phrases are common to most proposals: the intonational phrase and the intermediate phrase, using the terminology of Beckman and Pierrehumbert. A sentence is composed of a sequence of intonational phrases, which in turn are composed of sequences of intermediate phrases. An intonational phrase break is therefore perceived as stronger or more salient than an intermediate phrase break. Intonational phrases</context>
</contexts>
<marker>Selkirk, 1980</marker>
<rawString>Selkirk, E. (1980). &amp;quot;The role of prosodic categories in English word stress.&amp;quot; Linguistic Inquiry 11,563-605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Selkirk</author>
</authors>
<title>Phonology and Syntax: The Relation between Sound and Structure.</title>
<date>1984</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="30131" citStr="Selkirk 1984" startWordPosition="4727" endWordPosition="4728"> transcribed from spoken language and not consistently used in written text, so it is important that the algorithm not depend too heavily on commas. 35 Computational Linguistics Volume 20, Number I Syntactic features were extracted from skeletal parses provided through a preliminary version of the Penn Treebank Corpus. Since these are hand-corrected parses, the results are indicative of the performance possible using syntactic information, but do not reflect performance achievable with an existing parser. Several researchers have investigated the relationship between prosody and syntax (e.g., Selkirk 1984; Gee and Grosjean 1983; Cooper and Paccia-Cooper 1980; Altenberg 1987). Our features have been motivated by some of these results, which suggest that some syntactic constituents are more likely to be separated by a phrase break than others. However, we have chosen to let the important constituents be determined automatically, similar to Wang and Hirschberg (1992), rather than by rule. One feature is the highest syntactic constituent dominating the left word but not dominating the right word, which describes potential locations for phrase breaks after a specific syntactic constituent. We also </context>
</contexts>
<marker>Selkirk, 1984</marker>
<rawString>Selkirk, E. (1984). Phonology and Syntax: The Relation between Sound and Structure. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sorin</author>
<author>D Larreur</author>
<author>R Llorca</author>
</authors>
<title>A rhythm-based prosodic parser for text-to-speech systems in French.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, International Congress of Phonetic Sciences</booktitle>
<pages>1--125</pages>
<marker>Sorin, Larreur, Llorca, 1987</marker>
<rawString>Sorin, C.; Larreur, D.; and Llorca, R. (1987). &amp;quot;A rhythm-based prosodic parser for text-to-speech systems in French.&amp;quot; In Proceedings, International Congress of Phonetic Sciences 1,125-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J t&apos;Hart</author>
<author>R Collier</author>
<author>A Cohen</author>
</authors>
<title>A Perceptual Study of Intonation.</title>
<date>1990</date>
<publisher>Cambridge University Press.</publisher>
<marker>t&apos;Hart, Collier, Cohen, 1990</marker>
<rawString>t&apos;Hart, J.; Collier, R.; and Cohen, A. (1990). A Perceptual Study of Intonation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Veilleux</author>
<author>M Ostendorf</author>
<author>P Price</author>
<author>S Shattuck-Hufnagel</author>
</authors>
<title>Markov modeling of prosodic phrase structure.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>777--780</pages>
<contexts>
<context position="46991" citStr="Veilleux et al. (1990)" startWordPosition="7435" endWordPosition="7438">or major phrase boundary? Assuming major breaks occur at qualitatively different locations than minor breaks, we effectively remove the major breaks and sentences from our training corpus with this question. 2. Is the left word a content word and the right word a function word? In the training data, 65% of the minor and major breaks combined occur at content word/function word (CW/FW) boundaries, and about half of the CW/FW boundaries are marked with breaks. The CW/FW boundaries also correspond to the prosodic group boundaries used deterministically in Sorin, Larreur, and Llorca (1987) and in Veilleux et al. (1990). 3. What is the function word type of the word to the right? Previous work in prosodic parsing with a small dictionary (Sorin, Larreur, and Llorca 1987) suggested that different types of function words may be more or less likely to signal a prosodic phrase break. 4. Is either adjacent word a proper name (capitalized)? Preliminary examination of our data suggested there was some relationship between proper nouns and phrase boundaries, probably related to the phrasing of complex nominals. 5. How many content words have occurred since the previous function word? Speakers seemed to insert phrase </context>
<context position="51357" citStr="Veilleux et al. 1990" startWordPosition="8130" endWordPosition="8133">or phrase breaks must be used in phrase prediction from text. Therefore, these features are calculated dynamically in the prediction algorithm for each hypothesized prosodic parse. The first tree designed used only the very simple information represented by questions 1-8. The resulting tree is shown in Figure 2, with the relative frequency of a break in the training data included at each node. The first split trivially locates the sentence and major break boundaries. The second split utilized the content word/function word boundary question that we had used deterministically in previous work (Veilleux et al. 1990). The content/function word boundaries seem to be important in other algorithms as well: they correspond closely to the phi-phrase boundaries that would be predicted by the Bachenkoâ€”Fitzpatrick algorithm, and they seem to be captured in the Wangâ€”Hirschberg text-only tree by a succession of questions about the part-of-speech labels of the words adjacent to the break. Of the boundaries that were preceded by a content word and followed by a function word, 30% were hand-labeled as minor breaks, whereas only 4% of other locations were labeled as minor breaks and these were identified by the next qu</context>
</contexts>
<marker>Veilleux, Ostendorf, Price, Shattuck-Hufnagel, 1990</marker>
<rawString>Veilleux, N.; Ostendorf, M.; Price, P.; and Shattuck-Hufnagel, S. (1990). &amp;quot;Markov modeling of prosodic phrase structure.&amp;quot; In Proceedings, International Conference on Acoustics, Speech, and Signal Processing, 777-780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>J Hirschberg</author>
</authors>
<title>Automatic classification of intonational phrase boundaries.&amp;quot;</title>
<date>1992</date>
<journal>Computer Speech and Language</journal>
<volume>6</volume>
<issue>2</issue>
<pages>175--196</pages>
<contexts>
<context position="14795" citStr="Wang and Hirschberg (1992)" startWordPosition="2201" endWordPosition="2204">x many of the constraints on the use of the verb rule and propose a Verb Adjacency Rule, so their algorithm requires a fairly detailed parse, although not a complete one. One of the relaxed constraints obviates the need for clause information. Altenberg (1987) has also proposed an algorithm for prediction of phrase boundary locations (specifically, tone unit boundaries for British English) by rule from syntactic structure and semantic information. However, the detailed information required for the algorithm cannot currently be acquired automatically from text. Departing from these approaches, Wang and Hirschberg (1992) have recently used binary decision trees to predict the presence or absence of a prosodic break at each word boundary in a sentence. They consider a range of input variables, including textderived information such as detailed POS labels and syntactic constituent structure, and in some experiments, acoustic information. POS labels were given by Church&apos;s tagger (Church 1988) and syntactic constituents by Hindle&apos;s parser (Hindle 1987). The acoustic information (previous boundary location, pitch accent location, and phrase duration), which was based on hand-labeled prosodic markers, did not impro</context>
<context position="24640" citStr="Wang and Hirschberg (1992)" startWordPosition="3837" endWordPosition="3840"> extracted from the word sequence. Examples of possible features include part-of-speech labels and syntactic information such as bracketing labels or labels of an associated node in a syntactic tree. The decision tree f (wi,mpreâ€ž) used in determining the probabilities p(bklf inpreu)) includes questions based on these features, attributes of the previous minor phrase and the current major phrase, and length in words of the sentence. Details on our specific choice of features and questions is given in Section 4. Our use of decision trees is different from the phrase break detection algorithm of Wang and Hirschberg (1992), although the tree design algorithm and choice of features is similar. The tree is not used to classify phrase breaks directly; instead it is used to determine the probability of the occurrence of a minor break at some location, conditioned on the decision tree structure. This probability is used to represent the lowest level in the hierarchical model. Previously, we mentioned two important factors affecting the placement of phrase breaks: (1) grammatical structure and (2) length constraints on the prosodic constituents such as overall length and length relative to neighboring phrases. Gramma</context>
<context position="30497" citStr="Wang and Hirschberg (1992)" startWordPosition="4782" endWordPosition="4785">parses, the results are indicative of the performance possible using syntactic information, but do not reflect performance achievable with an existing parser. Several researchers have investigated the relationship between prosody and syntax (e.g., Selkirk 1984; Gee and Grosjean 1983; Cooper and Paccia-Cooper 1980; Altenberg 1987). Our features have been motivated by some of these results, which suggest that some syntactic constituents are more likely to be separated by a phrase break than others. However, we have chosen to let the important constituents be determined automatically, similar to Wang and Hirschberg (1992), rather than by rule. One feature is the highest syntactic constituent dominating the left word but not dominating the right word, which describes potential locations for phrase breaks after a specific syntactic constituent. We also consider the similar case, the highest syntactic constituent dominating the right word but not the left, to allow for prosodic phrase breaks that may be associated with the beginning of a syntactic constituent. The lowest syntactic constituent that dominates both words is a feature that will provide information about which constituents are not likely to be divided</context>
<context position="49754" citStr="Wang and Hirschberg (1992)" startWordPosition="7875" endWordPosition="7878">uring the effect of constituent endings. 41 Computational Linguistics Volume 20, Number 1 13. How many syntactic units begin between the two words? This question is similar to the previous one, except that it captures effects associated with the start of new constituents. 14. How large is the ratio of the current minor phrase length over the previous minor phrase length? This question incorporates the concept of balancing minor phrase lengths noted by other researchers (Gee and Grosjean 1983; Bachenko and Fitzpatrick 1990), and was found to be useful in phrase prediction trees investigated by Wang and Hirschberg (1992). In the beginning of a sentence where there is no previous minor phrase, the ratio is treated as missing data and handled using a surrogate variable (Breiman, Friedman, Olshen, and Stone 1984). 15. What is the label of the content word to the right? to the left? Wang and Hirschberg found that part-of-speech information is useful in phrase break prediction (Wang and Hirschberg 1992). Questions 5, 12, 13, and 14 are based on numerical features, so the binary question asks whether the feature is greater than some threshold, where the threshold is determined automatically in tree design. All othe</context>
<context position="55794" citStr="Wang and Hirschberg (1992)" startWordPosition="8864" endWordPosition="8867">left branch in a split is more likely to have a break. the major phrase, in terms of the ratio of the number of words up to the current position over the total length of the major phrase. Classification rates on the training data for this tree were 87% for nonbreaks and 66% for minor breaks. A fourth tree was designed using the first fourteen questions, and performance was similar to that for the third tree. The decision tree design algorithm&apos;s performance was not significantly changed by the introduction of additional features. New features can supplant previously used ones, as also found by Wang and Hirschberg (1992), because of the redundancy in information between features. For example, in the syntax trees, many of the baseline questions were no longer chosen, but the overall classification performance was similar. 4.4 Phrase Prediction Results The trees were used in the hierarchical model, and the phrase break prediction algorithm was evaluated on the independent test set described in Section 4.1. A summary of the results is given in Table 2, and the corresponding confusion matrices are in Tables 3 and 4. The baseline system (questions 1-8) gave the best performance, with a correct prediction rate of 8</context>
<context position="66060" citStr="Wang and Hirschberg (1992)" startWordPosition="10549" endWordPosition="10552"> 12 no-break 4 8 265 the syntactic features. The more detailed part-of-speech did not improve performance under any of these conditions. For the classification trees, correct detection improved slightly, but there was a corresponding increase in false detection. For the hierarchical model, performance actually degraded. These results suggest that the complexity associated with more detailed part-of-speech tagging may not be necessary; however, further research is needed to answer this question. It may be that other POS questions, such as testing a larger window of words around the break as in Wang and Hirschberg (1992), would yield better results. Finally, we thought it would be interesting to compare the results of our prediction algorithm to that of Bachenko and Fitzpatrick&apos;s algorithm on our corpus. Since the test set was relatively small, we were able to implement the algorithm by predicting the phrase boundaries from the rules by hand. To assign node indices to prosodic breaks (Bachenko and Fitzpatrick 1990), a critical value for separating major and minor phrase breaks is calculated based on an average of the indices associated with the prosodic phrase nodes, where the prosodic phrase nodes are all th</context>
<context position="72166" citStr="Wang and Hirschberg (1992)" startWordPosition="11482" endWordPosition="11485">c boundaries in spontaneous speech, achieving phrase break prediction results of .66/.02.3 (Again, note that these results are not directly comparable because of the differences in false detection rates, and results for other trees in Wang and Hirschberg [1992] suggest that these two algorithms have similar 2 This figure is calculated from the examples in the appendix in Bachenko and Fitzpatrick (1990), ignoring tertiary boundaries and including sentence-final boundaries as correct. The sentence that did not parse was not included in the calculation. 3 This result is computed from Figure 6 of Wang and Hirschberg (1992), which illustrates classification on training data. Cross validation results may vary slightly. 49 Computational Linguistics Volume 20, Number 1 performance.) Our classification trees used somewhat different features, though also based on POS and syntactic information, and achieved results on radio news speech that are surprisingly lower, i.e., .59/.02 for the tree that used syntax but did not use commas. Of course, the POS and syntactic information used here may not have been as detailed and/or reliable as that used by Wang and Hirschberg. (The comparisons here are based on the average error</context>
</contexts>
<marker>Wang, Hirschberg, 1992</marker>
<rawString>Wang, M., and Hirschberg, J. (1992). &amp;quot;Automatic classification of intonational phrase boundaries.&amp;quot; Computer Speech and Language 6(2), 175-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wightman</author>
<author>S Shattuck-Hufnagel</author>
<author>M Ostendorf</author>
<author>P Price</author>
</authors>
<title>Segmental durations in the vicinity of prosodic phrase boundaries.&amp;quot;</title>
<date>1992</date>
<journal>Journal of the Acoustical Society of America</journal>
<volume>91</volume>
<issue>3</issue>
<pages>1707--1717</pages>
<marker>Wightman, Shattuck-Hufnagel, Ostendorf, Price, 1992</marker>
<rawString>Wightman, C.; Shattuck-Hufnagel, S.; Ostendorf, M.; and Price, P. (1992). &amp;quot;Segmental durations in the vicinity of prosodic phrase boundaries.&amp;quot; Journal of the Acoustical Society of America 91(3), 1707-1717.</rawString>
</citation>
<citation valid="false">
<title>Computerized phone calls, I I which do everything from selling magazine subscriptions I to reminding people about meetings, II have become the telephone equivalent I I of junk mail. I I (A) Computerized phone calls, I I which do everything I from selling magazine subscriptions I Ito reminding people about meetings I I have become the telephone equivalent I of junk mail.</title>
<journal>I I</journal>
<marker></marker>
<rawString>1. Computerized phone calls, I I which do everything from selling magazine subscriptions I to reminding people about meetings, II have become the telephone equivalent I I of junk mail. I I (A) Computerized phone calls, I I which do everything I from selling magazine subscriptions I Ito reminding people about meetings I I have become the telephone equivalent I of junk mail. I I</rawString>
</citation>
<citation valid="false">
<title>But a new application of the technology II is about to be tried I out in Massachusetts II to ease crowded jail conditions. I I (I) But a new application of the technology I I is about to be tried out in Massachusetts I Ito ease crowded jail conditions.</title>
<journal>I I</journal>
<marker></marker>
<rawString>2. But a new application of the technology II is about to be tried I out in Massachusetts II to ease crowded jail conditions. I I (I) But a new application of the technology I I is about to be tried out in Massachusetts I Ito ease crowded jail conditions. I I</rawString>
</citation>
<citation valid="false">
<title>Next week some inmates released early I from the Hampton County jail in Springfield I I will be wearing a wristband that hooks up with a special jack 1 on their home phones. I I (A) Next week II some inmates released early I from the Hampton County jail in Springfield I I will be wearing a wristband I I that hooks up with a special jack I on their home phones.</title>
<journal>I I</journal>
<marker></marker>
<rawString>3. Next week some inmates released early I from the Hampton County jail in Springfield I I will be wearing a wristband that hooks up with a special jack 1 on their home phones. I I (A) Next week II some inmates released early I from the Hampton County jail in Springfield I I will be wearing a wristband I I that hooks up with a special jack I on their home phones. I I</rawString>
</citation>
<citation valid="false">
<title>Whenever a computer randomly calls I I them from jail, I I the former prisoner plugs I in to let corrections officials know II they&apos;re in the right place I at the right time.</title>
<volume>11</volume>
<issue>1</issue>
<marker></marker>
<rawString>4. Whenever a computer randomly calls I I them from jail, I I the former prisoner plugs I in to let corrections officials know II they&apos;re in the right place I at the right time. 11 (1)</rawString>
</citation>
<citation valid="false">
<title>Whenever a computer randomly calls them from jail, I I the former prisoner plugs in I to let corrections officials know II they&apos;re in the right place I at the right time. I I 5. Margo Melnicove reports. H (C) 6. The device is attached Ito a plastic wristband. I 1(C) 7. It looks like a watch. 11(C)</title>
<marker></marker>
<rawString>Whenever a computer randomly calls them from jail, I I the former prisoner plugs in I to let corrections officials know II they&apos;re in the right place I at the right time. I I 5. Margo Melnicove reports. H (C) 6. The device is attached Ito a plastic wristband. I 1(C) 7. It looks like a watch. 11(C)</rawString>
</citation>
<citation valid="false">
<title>It functions like an electronic probation officer. 1 I (A) It functions I like an electronic probation officer. II 9. When a computerized call is made I to a former prisoner&apos;s home phone, II that person answers I by plugging in the device.</title>
<pages>11</pages>
<marker></marker>
<rawString>8. It functions like an electronic probation officer. 1 I (A) It functions I like an electronic probation officer. II 9. When a computerized call is made I to a former prisoner&apos;s home phone, II that person answers I by plugging in the device. 11(C)</rawString>
</citation>
<citation valid="false">
<title>The wristband can be removed I I only by breaking its clasp, I I and if that&apos;s done the inmate I is immediately returned to jail. I 1(A) The wristband I can be removed II only by breaking its clasp, II and if that&apos;s done I I the inmate I is immediately returned to jail. I I 11. The description conjures up images I of big brother watching.</title>
<pages>11</pages>
<marker></marker>
<rawString>10. The wristband can be removed I I only by breaking its clasp, I I and if that&apos;s done the inmate I is immediately returned to jail. I 1(A) The wristband I can be removed II only by breaking its clasp, II and if that&apos;s done I I the inmate I is immediately returned to jail. I I 11. The description conjures up images I of big brother watching. 11(C)</rawString>
</citation>
<citation valid="false">
<authors>
<author>But Jay Ash</author>
</authors>
<title>I I deputy superintendent of the Hampton County jail I in Springfield, I says the surveillance system I I is not that sinister. 11(1)</title>
<marker>Ash, </marker>
<rawString>12. But Jay Ash, I I deputy superintendent of the Hampton County jail I in Springfield, I says the surveillance system I I is not that sinister. 11(1)</rawString>
</citation>
<citation valid="false">
<authors>
<author>But Jay Ash</author>
</authors>
<title>I I deputy superintendent of the Hampton County jail in Springfield, I I says the surveillance system I is not that sinister.</title>
<journal>I I</journal>
<marker>Ash, </marker>
<rawString>But Jay Ash, I I deputy superintendent of the Hampton County jail in Springfield, I I says the surveillance system I is not that sinister. I I</rawString>
</citation>
<citation valid="false">
<title>Such supervision, I I according to Ash, I is a sensible, I I cost effective alternative to incarceration I that should not alarm civil libertarians. 1 I (A) Such supervision, I I according to Ash, I I is a sensible, I I cost effective alternative to incarceration I I that should not alarm I 1 civil libertarians. I</title>
<marker></marker>
<rawString>13. Such supervision, I I according to Ash, I is a sensible, I I cost effective alternative to incarceration I that should not alarm civil libertarians. 1 I (A) Such supervision, I I according to Ash, I I is a sensible, I I cost effective alternative to incarceration I I that should not alarm I 1 civil libertarians. I</rawString>
</citation>
<citation valid="false">
<authors>
<author>Doctor Norman Rosenblatt</author>
</authors>
<title>I dean of the college I of criminal justice at Northeastern University, I I agrees. 1 I (A) Doctor Norman Rosenblatt, I I dean of the college I of criminal justice at Northeastern University, I I agrees.</title>
<journal>I I</journal>
<marker>Rosenblatt, </marker>
<rawString>14. Doctor Norman Rosenblatt, I dean of the college I of criminal justice at Northeastern University, I I agrees. 1 I (A) Doctor Norman Rosenblatt, I I dean of the college I of criminal justice at Northeastern University, I I agrees. I I</rawString>
</citation>
<citation valid="false">
<title>Rosenblatt expects electronic surveillance I in parole situations to become more widespread, I I and he thinks eventually people I I will get used to the idea. I l(I) Rosenblatt expects electronic surveillance in parole situations to become more widespread, I I and he thinks eventually I I people will get used to the idea.</title>
<journal>I I</journal>
<marker></marker>
<rawString>15. Rosenblatt expects electronic surveillance I in parole situations to become more widespread, I I and he thinks eventually people I I will get used to the idea. I l(I) Rosenblatt expects electronic surveillance in parole situations to become more widespread, I I and he thinks eventually I I people will get used to the idea. I I</rawString>
</citation>
<citation valid="false">
<authors>
<author>Springfield</author>
</authors>
<title>jail deputy superintendent Ash says I I although it will allow I I some prisoners to be released I I a few months I I before their sentences are up, I I concerns that may raise about public safety I I are not well founded. 1 I (A) Springfield jail deputy superintendent Ash I says I I although it will allow I some prisoners to be released I a few months before their sentences are up, I I concerns that may raise I about public safety II are not well founded. I I 17. Most county jail inmates I I did not commit violent crimes.</title>
<pages>11</pages>
<marker>Springfield, </marker>
<rawString>16. Springfield jail deputy superintendent Ash says I I although it will allow I I some prisoners to be released I I a few months I I before their sentences are up, I I concerns that may raise about public safety I I are not well founded. 1 I (A) Springfield jail deputy superintendent Ash I says I I although it will allow I some prisoners to be released I a few months before their sentences are up, I I concerns that may raise I about public safety II are not well founded. I I 17. Most county jail inmates I I did not commit violent crimes. 11(C)</rawString>
</citation>
<citation valid="false">
<title>They&apos;re in jail for such things I as bad checks or stealing. 1 I (A) They&apos;re in jail for such things I as bad checks I or stealing. I I 19. Those on early release must check I in with corrections officials fifty times I I a week according to Ash,I I who says about half I the contacts for a select group I I will now be made I I by the computerized phone calls. 1 I (I) Those on early release I must check in with corrections officials I I fifty times a week I I according to Ash,I I who says about half I the contacts for a select group I I will now be made I by the computerized phone calls.</title>
<journal>I I</journal>
<marker></marker>
<rawString>18. They&apos;re in jail for such things I as bad checks or stealing. 1 I (A) They&apos;re in jail for such things I as bad checks I or stealing. I I 19. Those on early release must check I in with corrections officials fifty times I I a week according to Ash,I I who says about half I the contacts for a select group I I will now be made I I by the computerized phone calls. 1 I (I) Those on early release I must check in with corrections officials I I fifty times a week I I according to Ash,I I who says about half I the contacts for a select group I I will now be made I by the computerized phone calls. I I</rawString>
</citation>
<citation valid="false">
<title>Initially the program will involve I I only a handful of inmates. I I (A) Initially I the program will involve I only a handful of inmates.</title>
<journal>I I</journal>
<marker></marker>
<rawString>20. Initially the program will involve I I only a handful of inmates. I I (A) Initially I the program will involve I only a handful of inmates. I I</rawString>
</citation>
<citation valid="false">
<title>Ash says the ultimate goal I I is to use it to get I about forty out of jail early. I I (A) Ash says I the ultimate goal I I is to use it to get about forty I I out of jail early. I I 22. The Springfield jail, I I built for 270 people, I now houses more than 500.</title>
<marker></marker>
<rawString>21. Ash says the ultimate goal I I is to use it to get I about forty out of jail early. I I (A) Ash says I the ultimate goal I I is to use it to get about forty I I out of jail early. I I 22. The Springfield jail, I I built for 270 people, I now houses more than 500.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>