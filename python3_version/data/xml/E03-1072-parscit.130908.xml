<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012377">
<title confidence="0.967475">
The Role of Initiative in Tutorial Dialogue
</title>
<author confidence="0.947761">
Mark G. Core and Johanna D. Moore and Claus Zinn
</author>
<affiliation confidence="0.974327">
School of Informatics
University of Edinburgh, 2 Buccleuch Place
</affiliation>
<address confidence="0.561102">
Edinburgh EH8 9LW, UK
</address>
<bodyText confidence="0.736018">
[markcl jmoorelzinn] @inf . ed. ac .uk
</bodyText>
<sectionHeader confidence="0.981692" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972291666667">
This work is the first systematic inves-
tigation of initiative in human-human
tutorial dialogue. We studied initia-
tive management in two dialogue strate-
gies: didactic tutoring and Socratic tu-
toring. We hypothesized that didactic
tutoring would be mostly tutor-initiative
while Socratic tutoring would be mixed-
initiative, and that more student initia-
tive would lead to more learning (i.e.,
task success for the tutor). Surpris-
ingly, students had initiative more of
the time in the didactic dialogues (21%
of the turns) than in the Socratic dia-
logues (10% of the turns), and there was
no direct relationship between student
initiative and learning. However, So-
cratic dialogues were more interactive
than didactic dialogues as measured by
percentage of tutor utterances that were
questions and percentage of words in
the dialogue uttered by the student, and
interactivity had a positive correlation
with learning.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952444444444">
Tutorial dialogue systems face the unique problem
that users (students) often do not know the answers
to questions asked by the system and may produce
wrong answers that are not in the system&apos;s domain
model. Because of these difficulties, current tuto-
rial dialogue systems are largely system-initiative;
only the system asks questions, and for each ques-
tion, system designers build a database of poten-
tial correct and incorrect answers, and a set of re-
sponses to deal with the incorrect answers.
There has been a similar trend in the spoken di-
alogue systems community. The problem in this
case is poor speech recognition performance and
the solution is for the system to ask questions with
a limited set of answers. However, Chu-Carroll
and Nickerson (2000) showed that a suitably intel-
ligent mixed-initiative dialogue system (MIMIC)
outperformed a comparable system-initiative di-
alogue system in terms of user satisfaction and
task efficiency. MIMIC could back off to system-
initiative mode when necessary but otherwise op-
erate in mixed-initiative mode.
The cognitive science literature indicates that
such a breakthrough is also needed in the tutor-
ing community. The current system-initiative ap-
proaches conflict with arguments that it is the
highly collaborative nature of human-human tutor-
ing dialogue that leads to learning (Merrill et al.,
1992a; Fox, 1993; Graesser et al., 1995). Through
this dialogue, tutors can intervene to ensure that
errors are detected and repaired and that students
can work around impasses (Merrill et al., 1992b).
Previous research has also shown that students
must be allowed to construct knowledge them-
selves to learn most effectively (Chi et al., 1989;
Chi et al., 1994; VanLehn et al., 1998). The con-
sensus from these studies is that experienced tutors
maintain a delicate balance allowing students to do
as much of the work as possible and to maintain a
feeling of control, while providing students with
enough guidance to keep them from becoming too
frustrated or confused. We refer to this style of tu-
toring as &amp;quot;Socratic&amp;quot; because it is characterized by
the use of questions and other hints to draw out
answers from students having difficulty.
</bodyText>
<page confidence="0.99895">
67
</page>
<bodyText confidence="0.997229678571428">
(Rosé et al., 2000) gives an overview of the ev-
idence in favor of Socratic tutoring as well as de-
scribing an opposing viewpoint supporting a tutor-
ing style referred to as didactic. Here, rather than
drawing out the answer from the student, the tutor
points out the student&apos;s error and explains how to
derive the correct answer.
We hypothesized that (1) didactic tutoring cor-
responds to the system-initiative dialogue man-
agement currently implemented in tutorial dia-
logue systems, (2) Socratic tutoring is mixed-
initiative, and (3) furthermore that initiative is di-
rectly related to &amp;quot;Socraticness&amp;quot; — more student
initiative would mean more student learning al-
though a minimum amount of tutor initiative is
likely to be necessary.
To investigate these hypotheses, we undertook
a systematic investigation of initiative, tutoring
strategy (Socratic vs. didactic), and learning (task
success) using a series of human-human tutor-
ing dialogues from an earlier project (Rosé et al.,
2000). In one set of dialogues, the tutor used a
Socratic tutoring style while in the other she used
a didactic tutoring style. We annotated these dia-
logues for initiative, measured the distribution of
initiative in the Socratic and didactic dialogues,
and measured the relationship between initiative
and student learning.
</bodyText>
<sectionHeader confidence="0.988749" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<subsectionHeader confidence="0.986537">
2.1 Defining Initiative
</subsectionHeader>
<bodyText confidence="0.999374338709678">
Shah (1997) defines student initiative as &amp;quot;any con-
tribution by the student that attempts to change the
course of the [tutoring] session&amp;quot; (p. 13). Shah&apos;s
corpus analysis dealt with remediation dialogues
where a tutor quizzed students about the answers
they gave during problem solving. In this cor-
pus, student initiatives are student utterances that
are not answers to questions. Shah assumes that
these initiatives are dealt with exclusively by the
tutor&apos;s next speech act, and that initiative then re-
verts back to the tutor. This definition was too lim-
ited for our more free-form tutoring dialogues.
Sinclair and Coulthard (1975) developed a dia-
logue grammar for classroom discussions. Their
minimal unit of dialogue is the exchange which is
composed of an initiating move, an optional re-
sponding move, and an optional feedback move.
Whoever makes the initiating move is said to have
initiative for the exchange. Although questions
can be reasked in cases of incorrect student an-
swers, this framework does not capture other ways
an exchange can be disrupted (e.g., the student
asks a question rather than answering the current
question), and again this definition was too limited
for our dialogues.
Line11 et al. (1988) discuss how a responder can
ask for clarification, challenge the speaker, and
change topics as well as respond directly to an
initiating move. Line11 et al. do not assign initia-
tive directly to speakers but instead rank speaker
moves based on how much &amp;quot;they can be regarded
as governing or steering the ensuing dialogue and
as being governed or commanded by the preced-
ing dialogue&amp;quot; (p. 419). For example, an utterance
which is not a response in any way but requires a
response from the listener is ranked highest with
a value of six. Minimal responses are at the other
end of the scale (with a rank of two); they invite
no response and give no more information than re-
quired.
Line11 et al.&apos;s approach was to sample a wide
variety of dialogue genres in developing their
definition; in contrast, Chu-Carroll and Brown
(1998) focussed specifically on problem-solving
dialogues. They found that it was important to dif-
ferentiate initiative (which they call dialogue ini-
tiative) from task initiative. They define dialogue
initiative by stating that it &amp;quot;tracks the lead in de-
termining the current discourse focus&amp;quot; (p. 6),1 and
that task initiative &amp;quot;tracks the lead in the devel-
opment of the agents&apos; plan&amp;quot; (p. 6). Presumably,
determining the discourse focus means setting the
discourse segment purpose as defined in Grosz
and Sidner&apos;s (1986) theory of discourse. What it
means to take the lead in developing the agents&apos;
plan depends on the plan representation but infor-
mally can refer to adding or taking away actions
from the plan, rearranging actions, or setting pa-
rameters.
Whittaker and Stenton (1988) do not define ini-
tiative beyond calling it control of the dialogue
by its participants. Their work is notable in that
</bodyText>
<footnote confidence="0.9986655">
1The page numbers come from the digital version:
http://citeseer.nj.nec.com/244268.htm1
</footnote>
<page confidence="0.999553">
68
</page>
<bodyText confidence="0.998338125">
they define a set of rules (see Figure 1) speci-
fying who has initiative for each turn in a dia-
logue. These rules approximate the more complex
definition given by Chu-Carroll and Brown and
have been used in several projects because they
facilitate reliable annotation (Strayer and Heeman,
2001; Jordan and Di Eugenio, 1997; Doran et al.,
2001; Walker and Whittaker, 1990).
</bodyText>
<subsectionHeader confidence="0.995058">
2.2 Initiative in human-human corpora
</subsectionHeader>
<bodyText confidence="0.999989930555556">
Previous work has shown a pattern to how
initiative shifts among dialogue participants in
problem-solving dialogues. Guinn (1996) used
simulated conversational agents to argue that the
most efficient problem-solving dialogues are those
where the participant who knows the most about
the current subtask takes initiative. The corpus
analysis of Walker and Whittaker (1990) gives
evidence that in natural dialogue, knowledgeable
speakers do take initiative. Walker and Whittaker
studied task-oriented dialogues (TODs) involving
an expert guiding a novice through assembling a
water pump, and advisory dialogues (ADs) involv-
ing an expert giving advice about financial and
software problems. In the TODs, as we would
expect, the expert had initiative most of the time
(91% of the turns). However, ADs have closer to
an equal sharing of initiative — the expert had ini-
tiative for 60% of the turns in finance ADs and
51% of the turns in software ADs. This is because
in the ADs, the novice must communicate the de-
tails of his problem to the expert as well as the
expert telling the novice what to do.
Shah (1997) investigated initiative in tutorial
dialogue, typed human-human tutoring dialogues
dealing with the circulatory system. Her corpus
consisted of students&apos; initial tutoring session and a
subsequent session with each of the same students.
She categorized student initiatives based on their
communicative goal (e.g., challenge, support, re-
pair, request information). Shah found that the ini-
tial sessions had twice the number of student ini-
tiatives as the subsequent sessions. The nature of
student initiatives also changed over time: the pro-
portion of student initiatives associated with con-
fusion (long pauses and self repairs) decreased in
subsequent sessions and the proportion of chal-
lenges increased. Shah also looked at tutor reac-
tions to student initiatives; she found that tutors
sometimes rejected student initiatives, but she did
not investigate what triggered such actions.
Graesser and Person (1994) labeled student
questions (a subset of the initiatives studied by
Shah) in a corpus of tutoring sessions for a re-
search methods course. Graesser and Person de-
veloped a taxonomy of different question types.
Of specific interest are deep-reasoning and knowl-
edge deficit questions. Deep-reasoning questions
involve causal reasoning and hypothetical situa-
tions. Knowledge deficit questions are triggered
when a student realizes an inconsistency or gap
in his understanding or gets stuck on a prob-
lem. Graesser and Person found that in the first
half of the course there was a negative corre-
lation between overall number of student ques-
tions and exam scores. In the second half of the
course, there were positive correlations between
exam scores and the proportion of student ques-
tions that were deep-reasoning questions and the
proportion of student questions that were knowl-
edge deficit questions.
Our study focused solely on initiative and did
not address the difficult problem of categorizing
question semantics. Initiative is a noisy measure
of student participation. Shallow questions such
as &amp;quot;What do I do next?&amp;quot; were treated the same
as insightful questions such as &amp;quot;Is a load basically
the opposite of a source?&amp;quot;. Despite this interfer-
ence, we hypothesized that high levels of initia-
tive would characterize students who took control
of their learning and as a result scored well in the
post experiment test.
</bodyText>
<sectionHeader confidence="0.990644" genericHeader="method">
3 Our Initiative Study
</sectionHeader>
<bodyText confidence="0.999947">
This section is a summary of our methodology and
results. For more details or to download the cor-
pus or annotation manual, consult the web page
http ://www.cog s ci. ed. ac .ukr jmoore/tutoring/
BEE_corpus.html.
</bodyText>
<subsectionHeader confidence="0.999223">
3.1 Method
</subsectionHeader>
<bodyText confidence="0.9999518">
The setting for this study is a course on basic elec-
tricity and electronics (BEE) developed with the
VIVIDS authoring tool (Munro, 1994). Students
read four textbook-style lessons and performed six
labs using a circuit simulator with a graphical in-
</bodyText>
<page confidence="0.996204">
69
</page>
<bodyText confidence="0.9999528125">
terface. (Rosé et al., 2000) describes an experi-
ment where students went through these lessons
and labs with the guidance of a human tutor (the
same one for the entire study). Before the lessons,
students were given pretests to gauge their initial
knowledge. After being tutored, students took the
same tests again. We refer to the difference in their
scores as learning gain. There were three sets of
tutoring sessions (a session means all the dialogue
between the tutor and one particular student): (1)
the trial sessions where the tutor was not given any
instructions on how to tutor 113 students], (2) the
Socratic sessions where the tutor was instructed
not to give explanations and to ask questions in-
stead 1110 students], and (3) the didactic sessions
where the tutor was encouraged to give explana-
tions and then probe student understanding with
questions [10 students]. During these sessions, the
student and tutor communicated through a chat in-
terface. We will refer to the logs of this chat inter-
face as the BEE dialogues.
In previous work (Core et al., 2002), we ad-
dressed the question of whether these Socratic
and didactic dialogues were really Socratic and
didactic. We used interactivity to approximate
&amp;quot;Socraticness&amp;quot;, and showed that the Socratic di-
alogues were more interactive than the didactic di-
alogues. On average in the Socratic dialogues: a
greater proportion of tutor utterances were ques-
tions (42% vs. 29%); the students produced a
higher percentage of words in the dialogues (33%
vs. 26%); and tutor turns and utterances were
shorter. It is debatable whether this means the dia-
logues are really Socratic and didactic but it proves
they reflect different tutoring styles which is suffi-
cient for the purposes of this study.
Rosé et al. (2000) addressed the issue of
whether the Socratic dialogues in this corpus were
more effective than the didactic ones. They found
a trend for Socratically tutored students to learn
more, but additional data is needed to verify this
trend. Chi et al. (2001) performed a similar study;
in this case, no difference was found between the
two tutoring strategies. However, Chi et al. noted
that the didactic tutors sometimes inadvertently re-
vealed answers to questions on the post-test (the
test given after tutoring to measure how much was
learned). So we cannot say anything conclusive
</bodyText>
<construct confidence="0.971690769230769">
if turn = command then
speaker has initiative
if turn = question then
if (last_turn = question or
last_turn = command) then
listener has initiative
else speaker has initiative
if turn - statement then
if last_turn = question then
listener has initiative
else speaker has initiative
if turn = prompt then
listener has initiative
</construct>
<figureCaption confidence="0.999306">
Figure 1: Rules for Assigning Initiative
</figureCaption>
<bodyText confidence="0.929857">
about the effectiveness of Socratic tutoring in the
BEE domain or Socratic tutoring in general.
</bodyText>
<subsectionHeader confidence="0.97464">
3.2 Initiative Annotation Method
</subsectionHeader>
<bodyText confidence="0.999960090909091">
The two definitions of initiative we considered
were that of Chu-Carroll and Brown (1998) and
Line11 et al. (1988). We felt that the extra gran-
ularity provided by Line11 et al.&apos;s initiative ranks
would not be necessary and adopted Chu-Carroll
and Brown&apos;s definition. However, this definition
makes reference to discourse focus without giv-
ing guidelines as to how discourse focus is to be
recognized during annotation. For this reason, we
used Whittaker and Stenton&apos;s initiative assignment
rules (1988) as an approximation to Chu-Carroll
and Brown&apos;s definition of (dialogue) initiative. We
did not attempt to annotate task initiative, but men-
tion this issue again in the conclusions.
We first give details of the initiative assignment
rules and then come back to the issue of whether
this was a valid choice. Before the rules can be
applied, each turn in the dialogue must be classi-
fied into one of the following types based on its
main purpose: assertions - declarative turns used
to state facts, commands - turns intended to in-
stigate action, questions - turns intended to elicit
information, and prompts - turns not expressing
propositional content (e.g., &amp;quot;yeah&amp;quot;, &amp;quot;okay&amp;quot;).
We used the rules in Figure 1 to assign initiative.
These are the same as the rules given by Whittaker
and Stenton except that we make the assumption
that a statement following a question responds to
that question.
A benefit of this annotation scheme is that in our
corpus the majority of turns can be automatically
labeled: questions often ended in question marks;
commands often started with verbs; a list of com-
</bodyText>
<page confidence="0.990149">
70
</page>
<bodyText confidence="0.999439553571428">
mon prompts (&amp;quot;okay&amp;quot;, &amp;quot;yeah&amp;quot;) allowed most of
these to be labeled, and statement could be used
to label everything else.
We needed human annotators to correct the au-
tomatic labeling. One of the authors of the pa-
per and another human annotator (not a project
member) corrected the automatic annotations. The
annotators had a reference manual and trained
on trial sessions of the dialogues. To test inter-
annotator reliability, the author and external anno-
tator labeled the same 757 examples taken from
non-training data; the resulting inter-annotator re-
liability as measured with the kappa statistic was
0.92. Generally, kappa values above 0.8 are con-
sidered acceptable.2
Although these initiative assignment rules al-
low reliable annotation and are easy to implement,
the question remains whether they actually cap-
ture initiative. It is clear that commands and ques-
tions not following questions (i.e., not clarification
questions) set the discourse segment purpose (i.e.,
take initiative). The contentious aspects of these
rules are assuming that answers never take initia-
tive and that questions following questions never
take initiative. It is simple to construct counter-
examples to these assumptions; however, the rules
work well in practice. Walker and Whittaker
(1990) showed that third person and one anaphora
rarely crossed segment boundaries marked by ini-
tiative changes annotated with these guidelines.3
It may be the case that these annotation assump-
tions fail on selected examples. However, in elim-
inating the assumptions it is likely that we will in-
troduce more errors than we correct. For example,
it is clear that some answers take initiative; if a
speaker asks &amp;quot;what time is it?&amp;quot; and the listener
gives more information than the current time, then
the listener has taken initiative. However, if the
speaker asks &amp;quot;what causes current to flow?&amp;quot;, it is
much more difficult to say which answers take ini-
tiative. Similarly, it is difficult to say when a ques-
`These guidelines are based on comments by Krippen-
dorff (1980) as summarized in Carletta (1996). Krippendorff
considered the case of two annotated variables. He said that
comparisons were reliable when the kappas for those vari-
ables were above 0.8.
31n this study, hierarchical discourse segments were an-
notated using changes in initiative as a starting point; these
changes were taken as marking either a segment endpoint or
the beginning of a nested segment.
tion following a question takes initiative. Some
factors are the content of the second question, how
many times the first speaker has been interrupted,
and the reaction of the first speaker. But it seems
very difficult to define these factors more precisely
and to define how they interact.
</bodyText>
<subsectionHeader confidence="0.996671">
3.3 Initiative Analysis
</subsectionHeader>
<bodyText confidence="0.9682526">
Our first analysis was to measure the average per-
centage of turns for which students had initiative
in the Socratic and didactic dialogues. The So-
cratic dialogues had 1547 turns, 2853 utterances,
and 23,451 words while the didactic dialogues had
1378 turns, 2993 utterances, and 26,195 words.
Surprisingly, students had initiative for fewer turns
on average (10%) in the Socratic dialogues than in
the didactic dialogues (21%).4 These results show
that students did not take advantage of the fact that
the Socratic dialogues were more interactive, and
did not ask more questions; in fact students asked
fewer questions in the Socratic condition. We no-
ticed that many student questions in the didactic
dialogues followed explanations, perhaps because
the long explanations confused students.
We next tested the relationship between initia-
tive and learning gain. Since Socratic and didactic
dialogues also differ in interactivity, we tested the
relationship between learning gain and the inter-
activity measures of average percentage of words
and utterances produced by the student and aver-
age percentage of tutor utterances that were ques-
tions. Figure 2 shows this data; the top graph
shows that initiative varies erratically as learn-
ing gain increases; there is no relationship (Pear-
son&apos;s r=-.0689, n=23, NS) between these vari-
ables. The same graph also shows average per-
centage of words produced by the student; this
does have a relationship with learning gain (Pear-
son&apos;s r = 0.6, n = 23, p &lt; 0.005). The bottom
graph shows the relationship between percentage
of utterances produced by the student and learn-
ing gain (Pearson&apos;s r = 0.56, n = 23, p &lt; 0.005),
and the relationship between average percentage
4To analyze significance, we looked at average percentage
of expert initiative per session rather than per corpus. For the
didactic dialogues, this average is 82% and for the Socratic
dialogues it is 90%, a significant difference (t = 2.26, df=18,
p &lt; 0.05 two-tailed).
</bodyText>
<page confidence="0.994799">
71
</page>
<figure confidence="0.864698444444444">
50
45
40
35
30
25
20
5 10 15 20 25 30 35 40
learning gain
</figure>
<figureCaption confidence="0.999507">
Figure 2: Learning Gain Comparisons
</figureCaption>
<bodyText confidence="0.999596177777778">
of tutor utterances that were questions and learn-
ing gain (Pearson&apos;s r = 0.46, n = 23, p &lt; 0.05).
In section 1, we discussed the work of Walker
and Whittaker (1990) on investigating initiative in
the genres of advisory dialogues (ADs) and task
oriented dialogues (TODs). Walker and Whittaker
also investigated the difference between TODs in
a spoken (telephone) modality and a typed (com-
puter chat) modality. The results of their study are
shown in columns 3-6 of Table 1 and the corre-
sponding measures from our study are in columns
1 and 2. The Socratic dialogues have almost the
same average expert initiative as TODs. In the
TODs, the expert would issue a series of com-
mands in order to get the novice to perform a pro-
cedure. In the Socratic dialogues, the tutor was
issuing a series of questions in order to get the stu-
dent to work through a line of reasoning to a cor-
rect answer.
The second row of the table shows average per-
centage of initiative changes that were abdica-
tions. Abdications are the use of prompts to give
away initiative; these often occur after interrup-
tions5 to signal the original speaker to continue.
Walker and Whittaker noted that spoken TODs
had the most abdications but typed TODs had the
least; modality has an impact on how initiative is
managed.
In the didactic and Socratic dialogues (both of
which are typed) shown in columns 1 and 2, we
see that abdications are rarely used. A number of
reasons are possible. In the typed TODs, com-
munication consisted of two simultaneously up-
dated channels. In the tutoring dialogues, par-
ticipants would send each other short messages.
This modality, typed text and restricted turn tak-
ing might have reduced the number of abdica-
tions. Another possible factor is that students in
this study were relatively passive; the tutor could
not rely on them to take initiative if she uttered
a prompt. The tutor&apos;s initiative management also
played a role. In our dialogues, after the student
took initiative, the tutor would address the stu-
dent&apos;s turn and then often take back initiative not
giving the student a chance to utter a prompt.
</bodyText>
<sectionHeader confidence="0.996834" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999959238095238">
One interpretation of this data is that the defini-
tion of initiative was too crude and with a more
precise definition, the results would show that stu-
dents had more initiative in the Socratic dialogues
than in the didactic dialogues. However, it would
not involve simply changing three or four border-
line examples. A large number of examples would
have to change such that there was no longer sig-
nificantly more student initiative in the didactic di-
alogues and instead significantly more student ini-
tiative in the Socratic dialogues.
A more likely interpretation is that when the tu-
tor was employing the Socratic tutoring strategy,
she did often take initiative (control of the dia-
logue) through constant questioning of the student.
However, as shown by the interactivity statistics,
students produced a higher percentage of words
in the Socratic dialogues than in the didactic di-
alogues, and the percentage of words in the di-
alogue uttered by the student roughly correlated
with learning. Given this correlation, we hypothe-
</bodyText>
<footnote confidence="0.966087">
5Walker and Whittaker define interruptions as taking the
initiative without invitation. It does not refer to interrupting
the utterance of the other speaker.
</footnote>
<page confidence="0.71342325">
50
45
40
35
</page>
<figure confidence="0.944836727272727">
30
25
20
15
10
5
0
0 5
10 15 20 25 30 35 40
learning gain
percentages
</figure>
<page confidence="0.980421">
72
</page>
<table confidence="0.9979018">
Didactic Socratic AD Finance AD Software TOD Phone TOD Key
Expert-Initiative 79% 90% 60% 51% 91% 91%
Abdication 2.32% 0.43% 38% 38% 45% 28%
Expert-Initiative - % of total turns with expert initiative
Abdication - % of initiative shifts that are abdications
</table>
<tableCaption confidence="0.999747">
Table 1: Initiative Measures for six Corpora
</tableCaption>
<bodyText confidence="0.999978">
size that student language production is an indica-
tion of student knowledge construction.
In future work, we see two ways of more closely
measuring knowledge construction. The first is to
use a question taxonomy such as (Graesser and
Person, 1994) to identify deep tutor and student
questions. (Jordan and Siler, 2002) suggests going
further and classifying student answers. Although
a tutor may ask a shallow question, the student
may give more information than requested acting
as if a deep question had been asked.
We plan to explore a second route based on
discourse structure, in particular when a question
has been dropped (i.e., it has been answered cor-
rectly or abandoned). Our hypothesis is that in
successful dialogues (ones where students learned
the most), tutors do not drop questions until stu-
dents correctly answer them meaning that the av-
erage discourse segment for a question is longer
and may contain more nested segments.
</bodyText>
<sectionHeader confidence="0.999327" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999992025641026">
In our corpus analysis, we found that initiative did
not correlate with student learning and thus may
not reflect activities such as problem solving and
deep reasoning that lead to learning. Chu-Carroll
and Brown (1998) identified the possibility that a
speaker might have (dialogue) initiative but not be
advancing the problem solving process. They cre-
ated a measure called task initiative to track who is
currently taking the lead in problem solving. For
this measure to be useful in the tutoring domain,
it will have to reflect student knowledge construc-
tion as well as problem solving participation. Our
corpus analysis suggests that students may have
such &amp;quot;learning&amp;quot; initiative without having dialogue
initiative. We must further investigate this hypoth-
esis in order to predict better the success of tutor-
ing dialogues.
Our current results suggest that tutoring sys-
tems that encourage students&apos; language production
will be most successful, and that a Socratic tutor-
ing style is better at promoting student language
production than didactic tutoring. These results
may be good news for system builders; one pos-
sible Socratic teaching strategy would be to ask
sequences of targeted questions where strong ex-
pectations about plausible answers make it easier
to interpret student input.
However, we must be mindful of the fact that,
even in Socratic interaction, students sometimes
do take initiative rather than simply answering the
sequence of questions posed by the tutor. It is not
the case that human tutors simply brush off all stu-
dent initiatives. And (Chi et al., 2001) shows that
it is crucial that tutors do not plough ahead with
their own plans, ignoring students&apos; signs of confu-
sion. In future work, we will investigate the factors
influencing the tutor&apos;s decision about whether to
entertain a student initiative, and investigate how
these actions are signaled linguistically.
</bodyText>
<sectionHeader confidence="0.994147" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9977296">
The research presented in this paper is supported
by Grant # N00014-914-1694 from the Office of
Naval Research, Cognitive and Neural Sciences
Division. Thanks to Jean Carletta and our review-
ers for their comments on this work.
</bodyText>
<sectionHeader confidence="0.99751" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9757762">
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the Kappa statistic. Computational
Linguistics, 22(2)249-254.
Michelene T. H. Chi, Miriam Bassok, Matthew W.
Lewis, Peter Reimann, and Robert Glaser. 1989.
Self-explanations: How students study and use ex-
amples in learning to solve problems. Cognitive Sci-
ence, 13(2):145-182.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian Lavancher. 1994. Eliciting
</reference>
<page confidence="0.993359">
73
</page>
<reference confidence="0.99978264">
self-explanations improves understanding. Cogni-
tive Science, 18(3):439-477.
Michelene T. H. Chi, Stephanie A. Siler, Heisawn
Jeong, Takashi Yamauchi, and Robert G. Hausmann.
2001. Learning from human tutoring. Cognitive
Science, 25:471-533.
Jennifer Chu-Carroll and Michael K. Brown. 1998. An
evidential model for tracking initiative in collabora-
tive dialogue interactions. User Modeling and User-
Adapted Interaction, 8:215-253.
Jennifer Chu-Carroll and Jill S. Nickerson. 2000.
Evaluating automatic dialogue strategy adaptation
for a spoken dialogue system. In Proc. of the Is&apos;
Annual Meeting of the North American Chapter of
the ACL, Seatle, pages 202-209.
Mark a Core, Johanna D. Moore, and Claus Zinn.
2002. Initiative in tutorial dialogue. In Proc. of the
ITS &apos;02 Workshop on Empirical Methods for Tutorial
Dialogue Systems.
Christine Doran, John Aberdeen, Laurie Damianos,
and Lynette Hirschman. 2001. Comparing several
aspects of human-computer and human-human dia-
logues. In 2nd SIGdial Workshop on Discourse and
Dialogue, Aalborg, Denmark.
Barbara A. Fox. 1993. The Human Tutorial Dialogue
Project: Issues in the design of instructional sys-
tems. Lawrence Erlbaum Associates, Hillsdale, NJ.
Arthur C. Graesser and Natalie K. Person. 1994. Ques-
tion asking during tutoring. American Educational
Research Journal, 31(1):104-137.
Arthur C. Graesser, Natalie K. Person, and Joseph P.
Magliano. 1995. Collaborative dialogue patterns in
naturalistic one-to-one tutoring. Applied Cognitive
Psychology, 9:495-522.
Barbara J. Grosz and Candace L. Sidner. 1986. At-
tention, intensions, and the structure of discourse.
Computational Linguistics, 12(3):175-204.
Curry I. Guinn. 1996. An analysis of initiative se-
lection in collaborative task-oriented discourse. In
Proc. of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 278-285.
Pamela W. Jordan and Barbara Di Eugenio. 1997.
Control and initiative in collaborative problem solv-
ing dialogues. In AAAI 1997 Spring Symposium on
Computational Models fOr Mixed Initiative Interac-
tions, Stanford, CA.
Pamela W. Jordan and Stephanie Siler. 2002. Control
and initiative in computer-mediated human tutoring
dialogues. In Proc. of the ITS&apos;02 Workshop on Em-
pirical Methods for Tutorial Dialogue Systems.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications,
Beverly Hills, CA.
Per Linell, Lennart Gustavsson, and Paivi Juvonen.
1988. Interactional dominance in dyadic communi-
cation: a presentation of initiative-response analysis.
Linguistics, 26:415-442.
Douglas C. Merrill, Brian J. Reiser, and S. Landes.
1992a. Human tutoring: Pedagogical strategies and
learning outcomes. Paper presented at the annual
meeting of the American Educational Research As-
sociation.
Douglas C. Merrill, Brian J. Reiser, Michael Ranney,
and J. Gregory Trafton. 1992b. Effective tutoring
techniques: Comparison of human tutors and intelli-
gent tutoring systems. Journal of the Learning Sci-
ences, 2(3):277-305.
Allen Munro. 1994. Authoring interactive graphical
models. In T. de Jong, D. M. Towne, and H. Spada,
editors, The Use of Computer Models for Explica-
tion, Analysis and Experimental Learning. Springer.
Carolyn P. Rosé, Johanna D. Moore, Kurt VanLehn,
and David Allbritton. 2000. A comparative evalu-
ation of socratic versus didactic tutoring. Technical
Report LRDC-BEE-1, University of Pittsburgh.
Farhana Shah. 1997. Recognizing and Responding
to Student Plans in an Intelligent Tutoring System:
CIRCSIM-Tutor. Ph.D. thesis, Illinois Institute of
Technology.
John M. Sinclair and R. Malcolm Coulthard. 1975. To-
wards an Analysis of Discourse: The English used
by teachers and pupils. Oxford University Press.
Susan E. Strayer and Peter A. Heeman. 2001. Recon-
ciling initiative and discourse structure. In 2nd SIG-
dial Workshop on Discourse and Dialogue, Aalborg,
Denmark, September.
Kurt VanLehn, Stephanie Siler, Charles Murray, and
William B. Baggett. 1998. What makes a tutorial
event effective? In M. A. Gernsbacher and S. Derry,
editors, Proc. of the Twentieth Annual Conference of
the Cognitive Science Society. Erlbaum.
Marilyn A. Walker and Steve Whittaker. 1990. Mixed
initiative in dialogue: An investigation into dis-
course segmentation. In Proc. of the 28th Annual
Meeting of the Association for Computational Lin-
guistics, pages 70-78.
Steve Whittaker and Phil Stenton. 1988. Cues and
control in expert-client dialogues. In Proc. of the
26th Annual Meeting of the Association for Compu-
tational Linguistics, pages 123-130.
</reference>
<page confidence="0.999133">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839692">
<title confidence="0.999924">The Role of Initiative in Tutorial Dialogue</title>
<author confidence="0.999992">Mark G Core</author>
<author confidence="0.999992">Johanna D Moore</author>
<author confidence="0.999992">Claus Zinn</author>
<affiliation confidence="0.995938">School of Informatics University of Edinburgh, 2 Buccleuch Place</affiliation>
<address confidence="0.994179">Edinburgh EH8 9LW, UK</address>
<email confidence="0.869247">[markcljmoorelzinn]@inf.ed.ac.uk</email>
<abstract confidence="0.99903992">This work is the first systematic investigation of initiative in human-human tutorial dialogue. We studied initiative management in two dialogue strategies: didactic tutoring and Socratic tutoring. We hypothesized that didactic tutoring would be mostly tutor-initiative while Socratic tutoring would be mixedinitiative, and that more student initiawould lead to more learning task success for the tutor). Surprisingly, students had initiative more of the time in the didactic dialogues (21% of the turns) than in the Socratic dialogues (10% of the turns), and there was no direct relationship between student initiative and learning. However, Socratic dialogues were more interactive than didactic dialogues as measured by percentage of tutor utterances that were questions and percentage of words in the dialogue uttered by the student, and interactivity had a positive correlation with learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the Kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--2</pages>
<contexts>
<context position="18533" citStr="Carletta (1996)" startWordPosition="2976" endWordPosition="2977">ation assumptions fail on selected examples. However, in eliminating the assumptions it is likely that we will introduce more errors than we correct. For example, it is clear that some answers take initiative; if a speaker asks &amp;quot;what time is it?&amp;quot; and the listener gives more information than the current time, then the listener has taken initiative. However, if the speaker asks &amp;quot;what causes current to flow?&amp;quot;, it is much more difficult to say which answers take initiative. Similarly, it is difficult to say when a ques`These guidelines are based on comments by Krippendorff (1980) as summarized in Carletta (1996). Krippendorff considered the case of two annotated variables. He said that comparisons were reliable when the kappas for those variables were above 0.8. 31n this study, hierarchical discourse segments were annotated using changes in initiative as a starting point; these changes were taken as marking either a segment endpoint or the beginning of a nested segment. tion following a question takes initiative. Some factors are the content of the second question, how many times the first speaker has been interrupted, and the reaction of the first speaker. But it seems very difficult to define these</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the Kappa statistic. Computational Linguistics, 22(2)249-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelene T H Chi</author>
<author>Miriam Bassok</author>
<author>Matthew W Lewis</author>
<author>Peter Reimann</author>
<author>Robert Glaser</author>
</authors>
<title>Self-explanations: How students study and use examples in learning to solve problems.</title>
<date>1989</date>
<journal>Cognitive Science,</journal>
<pages>13--2</pages>
<contexts>
<context position="2840" citStr="Chi et al., 1989" startWordPosition="442" endWordPosition="445">ence literature indicates that such a breakthrough is also needed in the tutoring community. The current system-initiative approaches conflict with arguments that it is the highly collaborative nature of human-human tutoring dialogue that leads to learning (Merrill et al., 1992a; Fox, 1993; Graesser et al., 1995). Through this dialogue, tutors can intervene to ensure that errors are detected and repaired and that students can work around impasses (Merrill et al., 1992b). Previous research has also shown that students must be allowed to construct knowledge themselves to learn most effectively (Chi et al., 1989; Chi et al., 1994; VanLehn et al., 1998). The consensus from these studies is that experienced tutors maintain a delicate balance allowing students to do as much of the work as possible and to maintain a feeling of control, while providing students with enough guidance to keep them from becoming too frustrated or confused. We refer to this style of tutoring as &amp;quot;Socratic&amp;quot; because it is characterized by the use of questions and other hints to draw out answers from students having difficulty. 67 (Rosé et al., 2000) gives an overview of the evidence in favor of Socratic tutoring as well as descri</context>
</contexts>
<marker>Chi, Bassok, Lewis, Reimann, Glaser, 1989</marker>
<rawString>Michelene T. H. Chi, Miriam Bassok, Matthew W. Lewis, Peter Reimann, and Robert Glaser. 1989. Self-explanations: How students study and use examples in learning to solve problems. Cognitive Science, 13(2):145-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelene T H Chi</author>
<author>Nicholas de Leeuw</author>
<author>Mei-Hung Chiu</author>
<author>Christian Lavancher</author>
</authors>
<title>Eliciting self-explanations improves understanding.</title>
<date>1994</date>
<journal>Cognitive Science,</journal>
<pages>18--3</pages>
<marker>Chi, de Leeuw, Chiu, Lavancher, 1994</marker>
<rawString>Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung Chiu, and Christian Lavancher. 1994. Eliciting self-explanations improves understanding. Cognitive Science, 18(3):439-477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelene T H Chi</author>
<author>Stephanie A Siler</author>
<author>Heisawn Jeong</author>
<author>Takashi Yamauchi</author>
<author>Robert G Hausmann</author>
</authors>
<title>Learning from human tutoring.</title>
<date>2001</date>
<journal>Cognitive Science,</journal>
<pages>25--471</pages>
<contexts>
<context position="14057" citStr="Chi et al. (2001)" startWordPosition="2253" endWordPosition="2256">es were questions (42% vs. 29%); the students produced a higher percentage of words in the dialogues (33% vs. 26%); and tutor turns and utterances were shorter. It is debatable whether this means the dialogues are really Socratic and didactic but it proves they reflect different tutoring styles which is sufficient for the purposes of this study. Rosé et al. (2000) addressed the issue of whether the Socratic dialogues in this corpus were more effective than the didactic ones. They found a trend for Socratically tutored students to learn more, but additional data is needed to verify this trend. Chi et al. (2001) performed a similar study; in this case, no difference was found between the two tutoring strategies. However, Chi et al. noted that the didactic tutors sometimes inadvertently revealed answers to questions on the post-test (the test given after tutoring to measure how much was learned). So we cannot say anything conclusive if turn = command then speaker has initiative if turn = question then if (last_turn = question or last_turn = command) then listener has initiative else speaker has initiative if turn - statement then if last_turn = question then listener has initiative else speaker has in</context>
</contexts>
<marker>Chi, Siler, Jeong, Yamauchi, Hausmann, 2001</marker>
<rawString>Michelene T. H. Chi, Stephanie A. Siler, Heisawn Jeong, Takashi Yamauchi, and Robert G. Hausmann. 2001. Learning from human tutoring. Cognitive Science, 25:471-533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Michael K Brown</author>
</authors>
<title>An evidential model for tracking initiative in collaborative dialogue interactions. User Modeling and UserAdapted Interaction,</title>
<date>1998</date>
<pages>8--215</pages>
<contexts>
<context position="6720" citStr="Chu-Carroll and Brown (1998)" startWordPosition="1077" endWordPosition="1080">speakers but instead rank speaker moves based on how much &amp;quot;they can be regarded as governing or steering the ensuing dialogue and as being governed or commanded by the preceding dialogue&amp;quot; (p. 419). For example, an utterance which is not a response in any way but requires a response from the listener is ranked highest with a value of six. Minimal responses are at the other end of the scale (with a rank of two); they invite no response and give no more information than required. Line11 et al.&apos;s approach was to sample a wide variety of dialogue genres in developing their definition; in contrast, Chu-Carroll and Brown (1998) focussed specifically on problem-solving dialogues. They found that it was important to differentiate initiative (which they call dialogue initiative) from task initiative. They define dialogue initiative by stating that it &amp;quot;tracks the lead in determining the current discourse focus&amp;quot; (p. 6),1 and that task initiative &amp;quot;tracks the lead in the development of the agents&apos; plan&amp;quot; (p. 6). Presumably, determining the discourse focus means setting the discourse segment purpose as defined in Grosz and Sidner&apos;s (1986) theory of discourse. What it means to take the lead in developing the agents&apos; plan depe</context>
<context position="14971" citStr="Chu-Carroll and Brown (1998)" startWordPosition="2398" endWordPosition="2401">d). So we cannot say anything conclusive if turn = command then speaker has initiative if turn = question then if (last_turn = question or last_turn = command) then listener has initiative else speaker has initiative if turn - statement then if last_turn = question then listener has initiative else speaker has initiative if turn = prompt then listener has initiative Figure 1: Rules for Assigning Initiative about the effectiveness of Socratic tutoring in the BEE domain or Socratic tutoring in general. 3.2 Initiative Annotation Method The two definitions of initiative we considered were that of Chu-Carroll and Brown (1998) and Line11 et al. (1988). We felt that the extra granularity provided by Line11 et al.&apos;s initiative ranks would not be necessary and adopted Chu-Carroll and Brown&apos;s definition. However, this definition makes reference to discourse focus without giving guidelines as to how discourse focus is to be recognized during annotation. For this reason, we used Whittaker and Stenton&apos;s initiative assignment rules (1988) as an approximation to Chu-Carroll and Brown&apos;s definition of (dialogue) initiative. We did not attempt to annotate task initiative, but mention this issue again in the conclusions. We fir</context>
<context position="26140" citStr="Chu-Carroll and Brown (1998)" startWordPosition="4245" endWordPosition="4248">route based on discourse structure, in particular when a question has been dropped (i.e., it has been answered correctly or abandoned). Our hypothesis is that in successful dialogues (ones where students learned the most), tutors do not drop questions until students correctly answer them meaning that the average discourse segment for a question is longer and may contain more nested segments. 5 Conclusions In our corpus analysis, we found that initiative did not correlate with student learning and thus may not reflect activities such as problem solving and deep reasoning that lead to learning. Chu-Carroll and Brown (1998) identified the possibility that a speaker might have (dialogue) initiative but not be advancing the problem solving process. They created a measure called task initiative to track who is currently taking the lead in problem solving. For this measure to be useful in the tutoring domain, it will have to reflect student knowledge construction as well as problem solving participation. Our corpus analysis suggests that students may have such &amp;quot;learning&amp;quot; initiative without having dialogue initiative. We must further investigate this hypothesis in order to predict better the success of tutoring dialo</context>
</contexts>
<marker>Chu-Carroll, Brown, 1998</marker>
<rawString>Jennifer Chu-Carroll and Michael K. Brown. 1998. An evidential model for tracking initiative in collaborative dialogue interactions. User Modeling and UserAdapted Interaction, 8:215-253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Jill S Nickerson</author>
</authors>
<title>Evaluating automatic dialogue strategy adaptation for a spoken dialogue system.</title>
<date>2000</date>
<booktitle>In Proc. of the Is&apos; Annual Meeting of the North American Chapter of the ACL, Seatle,</booktitle>
<pages>202--209</pages>
<contexts>
<context position="1910" citStr="Chu-Carroll and Nickerson (2000)" startWordPosition="301" endWordPosition="304">he system and may produce wrong answers that are not in the system&apos;s domain model. Because of these difficulties, current tutorial dialogue systems are largely system-initiative; only the system asks questions, and for each question, system designers build a database of potential correct and incorrect answers, and a set of responses to deal with the incorrect answers. There has been a similar trend in the spoken dialogue systems community. The problem in this case is poor speech recognition performance and the solution is for the system to ask questions with a limited set of answers. However, Chu-Carroll and Nickerson (2000) showed that a suitably intelligent mixed-initiative dialogue system (MIMIC) outperformed a comparable system-initiative dialogue system in terms of user satisfaction and task efficiency. MIMIC could back off to systeminitiative mode when necessary but otherwise operate in mixed-initiative mode. The cognitive science literature indicates that such a breakthrough is also needed in the tutoring community. The current system-initiative approaches conflict with arguments that it is the highly collaborative nature of human-human tutoring dialogue that leads to learning (Merrill et al., 1992a; Fox, </context>
</contexts>
<marker>Chu-Carroll, Nickerson, 2000</marker>
<rawString>Jennifer Chu-Carroll and Jill S. Nickerson. 2000. Evaluating automatic dialogue strategy adaptation for a spoken dialogue system. In Proc. of the Is&apos; Annual Meeting of the North American Chapter of the ACL, Seatle, pages 202-209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark a Core</author>
<author>Johanna D Moore</author>
<author>Claus Zinn</author>
</authors>
<title>Initiative in tutorial dialogue.</title>
<date>2002</date>
<booktitle>In Proc. of the ITS &apos;02 Workshop on Empirical Methods for Tutorial Dialogue Systems.</booktitle>
<contexts>
<context position="13109" citStr="Core et al., 2002" startWordPosition="2098" endWordPosition="2101">ue between the tutor and one particular student): (1) the trial sessions where the tutor was not given any instructions on how to tutor 113 students], (2) the Socratic sessions where the tutor was instructed not to give explanations and to ask questions instead 1110 students], and (3) the didactic sessions where the tutor was encouraged to give explanations and then probe student understanding with questions [10 students]. During these sessions, the student and tutor communicated through a chat interface. We will refer to the logs of this chat interface as the BEE dialogues. In previous work (Core et al., 2002), we addressed the question of whether these Socratic and didactic dialogues were really Socratic and didactic. We used interactivity to approximate &amp;quot;Socraticness&amp;quot;, and showed that the Socratic dialogues were more interactive than the didactic dialogues. On average in the Socratic dialogues: a greater proportion of tutor utterances were questions (42% vs. 29%); the students produced a higher percentage of words in the dialogues (33% vs. 26%); and tutor turns and utterances were shorter. It is debatable whether this means the dialogues are really Socratic and didactic but it proves they reflect</context>
</contexts>
<marker>Core, Moore, Zinn, 2002</marker>
<rawString>Mark a Core, Johanna D. Moore, and Claus Zinn. 2002. Initiative in tutorial dialogue. In Proc. of the ITS &apos;02 Workshop on Empirical Methods for Tutorial Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Doran</author>
<author>John Aberdeen</author>
<author>Laurie Damianos</author>
<author>Lynette Hirschman</author>
</authors>
<title>Comparing several aspects of human-computer and human-human dialogues.</title>
<date>2001</date>
<booktitle>In 2nd SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="8047" citStr="Doran et al., 2001" startWordPosition="1289" endWordPosition="1292">anging actions, or setting parameters. Whittaker and Stenton (1988) do not define initiative beyond calling it control of the dialogue by its participants. Their work is notable in that 1The page numbers come from the digital version: http://citeseer.nj.nec.com/244268.htm1 68 they define a set of rules (see Figure 1) specifying who has initiative for each turn in a dialogue. These rules approximate the more complex definition given by Chu-Carroll and Brown and have been used in several projects because they facilitate reliable annotation (Strayer and Heeman, 2001; Jordan and Di Eugenio, 1997; Doran et al., 2001; Walker and Whittaker, 1990). 2.2 Initiative in human-human corpora Previous work has shown a pattern to how initiative shifts among dialogue participants in problem-solving dialogues. Guinn (1996) used simulated conversational agents to argue that the most efficient problem-solving dialogues are those where the participant who knows the most about the current subtask takes initiative. The corpus analysis of Walker and Whittaker (1990) gives evidence that in natural dialogue, knowledgeable speakers do take initiative. Walker and Whittaker studied task-oriented dialogues (TODs) involving an ex</context>
</contexts>
<marker>Doran, Aberdeen, Damianos, Hirschman, 2001</marker>
<rawString>Christine Doran, John Aberdeen, Laurie Damianos, and Lynette Hirschman. 2001. Comparing several aspects of human-computer and human-human dialogues. In 2nd SIGdial Workshop on Discourse and Dialogue, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara A Fox</author>
</authors>
<title>The Human Tutorial Dialogue Project: Issues in the design of instructional systems. Lawrence Erlbaum Associates,</title>
<date>1993</date>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="2514" citStr="Fox, 1993" startWordPosition="392" endWordPosition="393">2000) showed that a suitably intelligent mixed-initiative dialogue system (MIMIC) outperformed a comparable system-initiative dialogue system in terms of user satisfaction and task efficiency. MIMIC could back off to systeminitiative mode when necessary but otherwise operate in mixed-initiative mode. The cognitive science literature indicates that such a breakthrough is also needed in the tutoring community. The current system-initiative approaches conflict with arguments that it is the highly collaborative nature of human-human tutoring dialogue that leads to learning (Merrill et al., 1992a; Fox, 1993; Graesser et al., 1995). Through this dialogue, tutors can intervene to ensure that errors are detected and repaired and that students can work around impasses (Merrill et al., 1992b). Previous research has also shown that students must be allowed to construct knowledge themselves to learn most effectively (Chi et al., 1989; Chi et al., 1994; VanLehn et al., 1998). The consensus from these studies is that experienced tutors maintain a delicate balance allowing students to do as much of the work as possible and to maintain a feeling of control, while providing students with enough guidance to </context>
</contexts>
<marker>Fox, 1993</marker>
<rawString>Barbara A. Fox. 1993. The Human Tutorial Dialogue Project: Issues in the design of instructional systems. Lawrence Erlbaum Associates, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur C Graesser</author>
<author>Natalie K Person</author>
</authors>
<title>Question asking during tutoring.</title>
<date>1994</date>
<journal>American Educational Research Journal,</journal>
<pages>31--1</pages>
<contexts>
<context position="10130" citStr="Graesser and Person (1994)" startWordPosition="1611" endWordPosition="1614">their communicative goal (e.g., challenge, support, repair, request information). Shah found that the initial sessions had twice the number of student initiatives as the subsequent sessions. The nature of student initiatives also changed over time: the proportion of student initiatives associated with confusion (long pauses and self repairs) decreased in subsequent sessions and the proportion of challenges increased. Shah also looked at tutor reactions to student initiatives; she found that tutors sometimes rejected student initiatives, but she did not investigate what triggered such actions. Graesser and Person (1994) labeled student questions (a subset of the initiatives studied by Shah) in a corpus of tutoring sessions for a research methods course. Graesser and Person developed a taxonomy of different question types. Of specific interest are deep-reasoning and knowledge deficit questions. Deep-reasoning questions involve causal reasoning and hypothetical situations. Knowledge deficit questions are triggered when a student realizes an inconsistency or gap in his understanding or gets stuck on a problem. Graesser and Person found that in the first half of the course there was a negative correlation betwee</context>
<context position="25213" citStr="Graesser and Person, 1994" startWordPosition="4095" endWordPosition="4098"> 35 30 25 20 15 10 5 0 0 5 10 15 20 25 30 35 40 learning gain percentages 72 Didactic Socratic AD Finance AD Software TOD Phone TOD Key Expert-Initiative 79% 90% 60% 51% 91% 91% Abdication 2.32% 0.43% 38% 38% 45% 28% Expert-Initiative - % of total turns with expert initiative Abdication - % of initiative shifts that are abdications Table 1: Initiative Measures for six Corpora size that student language production is an indication of student knowledge construction. In future work, we see two ways of more closely measuring knowledge construction. The first is to use a question taxonomy such as (Graesser and Person, 1994) to identify deep tutor and student questions. (Jordan and Siler, 2002) suggests going further and classifying student answers. Although a tutor may ask a shallow question, the student may give more information than requested acting as if a deep question had been asked. We plan to explore a second route based on discourse structure, in particular when a question has been dropped (i.e., it has been answered correctly or abandoned). Our hypothesis is that in successful dialogues (ones where students learned the most), tutors do not drop questions until students correctly answer them meaning that</context>
</contexts>
<marker>Graesser, Person, 1994</marker>
<rawString>Arthur C. Graesser and Natalie K. Person. 1994. Question asking during tutoring. American Educational Research Journal, 31(1):104-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur C Graesser</author>
<author>Natalie K Person</author>
<author>Joseph P Magliano</author>
</authors>
<title>Collaborative dialogue patterns in naturalistic one-to-one tutoring.</title>
<date>1995</date>
<journal>Applied Cognitive Psychology,</journal>
<pages>9--495</pages>
<contexts>
<context position="2538" citStr="Graesser et al., 1995" startWordPosition="394" endWordPosition="397">d that a suitably intelligent mixed-initiative dialogue system (MIMIC) outperformed a comparable system-initiative dialogue system in terms of user satisfaction and task efficiency. MIMIC could back off to systeminitiative mode when necessary but otherwise operate in mixed-initiative mode. The cognitive science literature indicates that such a breakthrough is also needed in the tutoring community. The current system-initiative approaches conflict with arguments that it is the highly collaborative nature of human-human tutoring dialogue that leads to learning (Merrill et al., 1992a; Fox, 1993; Graesser et al., 1995). Through this dialogue, tutors can intervene to ensure that errors are detected and repaired and that students can work around impasses (Merrill et al., 1992b). Previous research has also shown that students must be allowed to construct knowledge themselves to learn most effectively (Chi et al., 1989; Chi et al., 1994; VanLehn et al., 1998). The consensus from these studies is that experienced tutors maintain a delicate balance allowing students to do as much of the work as possible and to maintain a feeling of control, while providing students with enough guidance to keep them from becoming </context>
</contexts>
<marker>Graesser, Person, Magliano, 1995</marker>
<rawString>Arthur C. Graesser, Natalie K. Person, and Joseph P. Magliano. 1995. Collaborative dialogue patterns in naturalistic one-to-one tutoring. Applied Cognitive Psychology, 9:495-522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intensions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--3</pages>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intensions, and the structure of discourse. Computational Linguistics, 12(3):175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Curry I Guinn</author>
</authors>
<title>An analysis of initiative selection in collaborative task-oriented discourse.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>278--285</pages>
<contexts>
<context position="8245" citStr="Guinn (1996)" startWordPosition="1318" endWordPosition="1319">come from the digital version: http://citeseer.nj.nec.com/244268.htm1 68 they define a set of rules (see Figure 1) specifying who has initiative for each turn in a dialogue. These rules approximate the more complex definition given by Chu-Carroll and Brown and have been used in several projects because they facilitate reliable annotation (Strayer and Heeman, 2001; Jordan and Di Eugenio, 1997; Doran et al., 2001; Walker and Whittaker, 1990). 2.2 Initiative in human-human corpora Previous work has shown a pattern to how initiative shifts among dialogue participants in problem-solving dialogues. Guinn (1996) used simulated conversational agents to argue that the most efficient problem-solving dialogues are those where the participant who knows the most about the current subtask takes initiative. The corpus analysis of Walker and Whittaker (1990) gives evidence that in natural dialogue, knowledgeable speakers do take initiative. Walker and Whittaker studied task-oriented dialogues (TODs) involving an expert guiding a novice through assembling a water pump, and advisory dialogues (ADs) involving an expert giving advice about financial and software problems. In the TODs, as we would expect, the expe</context>
</contexts>
<marker>Guinn, 1996</marker>
<rawString>Curry I. Guinn. 1996. An analysis of initiative selection in collaborative task-oriented discourse. In Proc. of the 34th Annual Meeting of the Association for Computational Linguistics, pages 278-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela W Jordan</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>Control and initiative in collaborative problem solving dialogues.</title>
<date>1997</date>
<booktitle>In AAAI 1997 Spring Symposium on Computational Models fOr Mixed Initiative Interactions,</booktitle>
<location>Stanford, CA.</location>
<marker>Jordan, Di Eugenio, 1997</marker>
<rawString>Pamela W. Jordan and Barbara Di Eugenio. 1997. Control and initiative in collaborative problem solving dialogues. In AAAI 1997 Spring Symposium on Computational Models fOr Mixed Initiative Interactions, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela W Jordan</author>
<author>Stephanie Siler</author>
</authors>
<title>Control and initiative in computer-mediated human tutoring dialogues.</title>
<date>2002</date>
<booktitle>In Proc. of the ITS&apos;02 Workshop on Empirical Methods for Tutorial Dialogue Systems.</booktitle>
<contexts>
<context position="25284" citStr="Jordan and Siler, 2002" startWordPosition="4106" endWordPosition="4109">72 Didactic Socratic AD Finance AD Software TOD Phone TOD Key Expert-Initiative 79% 90% 60% 51% 91% 91% Abdication 2.32% 0.43% 38% 38% 45% 28% Expert-Initiative - % of total turns with expert initiative Abdication - % of initiative shifts that are abdications Table 1: Initiative Measures for six Corpora size that student language production is an indication of student knowledge construction. In future work, we see two ways of more closely measuring knowledge construction. The first is to use a question taxonomy such as (Graesser and Person, 1994) to identify deep tutor and student questions. (Jordan and Siler, 2002) suggests going further and classifying student answers. Although a tutor may ask a shallow question, the student may give more information than requested acting as if a deep question had been asked. We plan to explore a second route based on discourse structure, in particular when a question has been dropped (i.e., it has been answered correctly or abandoned). Our hypothesis is that in successful dialogues (ones where students learned the most), tutors do not drop questions until students correctly answer them meaning that the average discourse segment for a question is longer and may contain</context>
</contexts>
<marker>Jordan, Siler, 2002</marker>
<rawString>Pamela W. Jordan and Stephanie Siler. 2002. Control and initiative in computer-mediated human tutoring dialogues. In Proc. of the ITS&apos;02 Workshop on Empirical Methods for Tutorial Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage Publications,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="18500" citStr="Krippendorff (1980)" startWordPosition="2970" endWordPosition="2972">3 It may be the case that these annotation assumptions fail on selected examples. However, in eliminating the assumptions it is likely that we will introduce more errors than we correct. For example, it is clear that some answers take initiative; if a speaker asks &amp;quot;what time is it?&amp;quot; and the listener gives more information than the current time, then the listener has taken initiative. However, if the speaker asks &amp;quot;what causes current to flow?&amp;quot;, it is much more difficult to say which answers take initiative. Similarly, it is difficult to say when a ques`These guidelines are based on comments by Krippendorff (1980) as summarized in Carletta (1996). Krippendorff considered the case of two annotated variables. He said that comparisons were reliable when the kappas for those variables were above 0.8. 31n this study, hierarchical discourse segments were annotated using changes in initiative as a starting point; these changes were taken as marking either a segment endpoint or the beginning of a nested segment. tion following a question takes initiative. Some factors are the content of the second question, how many times the first speaker has been interrupted, and the reaction of the first speaker. But it see</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Sage Publications, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Per Linell</author>
<author>Lennart Gustavsson</author>
<author>Paivi Juvonen</author>
</authors>
<title>Interactional dominance in dyadic communication: a presentation of initiative-response analysis.</title>
<date>1988</date>
<journal>Linguistics,</journal>
<pages>26--415</pages>
<marker>Linell, Gustavsson, Juvonen, 1988</marker>
<rawString>Per Linell, Lennart Gustavsson, and Paivi Juvonen. 1988. Interactional dominance in dyadic communication: a presentation of initiative-response analysis. Linguistics, 26:415-442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas C Merrill</author>
<author>Brian J Reiser</author>
<author>S Landes</author>
</authors>
<title>Human tutoring: Pedagogical strategies and learning outcomes. Paper presented at the annual meeting of the American Educational Research Association.</title>
<date>1992</date>
<contexts>
<context position="2502" citStr="Merrill et al., 1992" startWordPosition="388" endWordPosition="391">Carroll and Nickerson (2000) showed that a suitably intelligent mixed-initiative dialogue system (MIMIC) outperformed a comparable system-initiative dialogue system in terms of user satisfaction and task efficiency. MIMIC could back off to systeminitiative mode when necessary but otherwise operate in mixed-initiative mode. The cognitive science literature indicates that such a breakthrough is also needed in the tutoring community. The current system-initiative approaches conflict with arguments that it is the highly collaborative nature of human-human tutoring dialogue that leads to learning (Merrill et al., 1992a; Fox, 1993; Graesser et al., 1995). Through this dialogue, tutors can intervene to ensure that errors are detected and repaired and that students can work around impasses (Merrill et al., 1992b). Previous research has also shown that students must be allowed to construct knowledge themselves to learn most effectively (Chi et al., 1989; Chi et al., 1994; VanLehn et al., 1998). The consensus from these studies is that experienced tutors maintain a delicate balance allowing students to do as much of the work as possible and to maintain a feeling of control, while providing students with enough </context>
</contexts>
<marker>Merrill, Reiser, Landes, 1992</marker>
<rawString>Douglas C. Merrill, Brian J. Reiser, and S. Landes. 1992a. Human tutoring: Pedagogical strategies and learning outcomes. Paper presented at the annual meeting of the American Educational Research Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas C Merrill</author>
<author>Brian J Reiser</author>
<author>Michael Ranney</author>
<author>J Gregory Trafton</author>
</authors>
<title>Effective tutoring techniques: Comparison of human tutors and intelligent tutoring systems.</title>
<date>1992</date>
<journal>Journal of the Learning Sciences,</journal>
<pages>2--3</pages>
<contexts>
<context position="2502" citStr="Merrill et al., 1992" startWordPosition="388" endWordPosition="391">Carroll and Nickerson (2000) showed that a suitably intelligent mixed-initiative dialogue system (MIMIC) outperformed a comparable system-initiative dialogue system in terms of user satisfaction and task efficiency. MIMIC could back off to systeminitiative mode when necessary but otherwise operate in mixed-initiative mode. The cognitive science literature indicates that such a breakthrough is also needed in the tutoring community. The current system-initiative approaches conflict with arguments that it is the highly collaborative nature of human-human tutoring dialogue that leads to learning (Merrill et al., 1992a; Fox, 1993; Graesser et al., 1995). Through this dialogue, tutors can intervene to ensure that errors are detected and repaired and that students can work around impasses (Merrill et al., 1992b). Previous research has also shown that students must be allowed to construct knowledge themselves to learn most effectively (Chi et al., 1989; Chi et al., 1994; VanLehn et al., 1998). The consensus from these studies is that experienced tutors maintain a delicate balance allowing students to do as much of the work as possible and to maintain a feeling of control, while providing students with enough </context>
</contexts>
<marker>Merrill, Reiser, Ranney, Trafton, 1992</marker>
<rawString>Douglas C. Merrill, Brian J. Reiser, Michael Ranney, and J. Gregory Trafton. 1992b. Effective tutoring techniques: Comparison of human tutors and intelligent tutoring systems. Journal of the Learning Sciences, 2(3):277-305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allen Munro</author>
</authors>
<title>Authoring interactive graphical models. In</title>
<date>1994</date>
<booktitle>The Use of Computer Models for Explication, Analysis and Experimental Learning.</booktitle>
<editor>T. de Jong, D. M. Towne, and H. Spada, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="11925" citStr="Munro, 1994" startWordPosition="1902" endWordPosition="1903">sically the opposite of a source?&amp;quot;. Despite this interference, we hypothesized that high levels of initiative would characterize students who took control of their learning and as a result scored well in the post experiment test. 3 Our Initiative Study This section is a summary of our methodology and results. For more details or to download the corpus or annotation manual, consult the web page http ://www.cog s ci. ed. ac .ukr jmoore/tutoring/ BEE_corpus.html. 3.1 Method The setting for this study is a course on basic electricity and electronics (BEE) developed with the VIVIDS authoring tool (Munro, 1994). Students read four textbook-style lessons and performed six labs using a circuit simulator with a graphical in69 terface. (Rosé et al., 2000) describes an experiment where students went through these lessons and labs with the guidance of a human tutor (the same one for the entire study). Before the lessons, students were given pretests to gauge their initial knowledge. After being tutored, students took the same tests again. We refer to the difference in their scores as learning gain. There were three sets of tutoring sessions (a session means all the dialogue between the tutor and one parti</context>
</contexts>
<marker>Munro, 1994</marker>
<rawString>Allen Munro. 1994. Authoring interactive graphical models. In T. de Jong, D. M. Towne, and H. Spada, editors, The Use of Computer Models for Explication, Analysis and Experimental Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn P Rosé</author>
<author>Johanna D Moore</author>
<author>Kurt VanLehn</author>
<author>David Allbritton</author>
</authors>
<title>A comparative evaluation of socratic versus didactic tutoring.</title>
<date>2000</date>
<tech>Technical Report LRDC-BEE-1,</tech>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="3358" citStr="Rosé et al., 2000" startWordPosition="532" endWordPosition="535">udents must be allowed to construct knowledge themselves to learn most effectively (Chi et al., 1989; Chi et al., 1994; VanLehn et al., 1998). The consensus from these studies is that experienced tutors maintain a delicate balance allowing students to do as much of the work as possible and to maintain a feeling of control, while providing students with enough guidance to keep them from becoming too frustrated or confused. We refer to this style of tutoring as &amp;quot;Socratic&amp;quot; because it is characterized by the use of questions and other hints to draw out answers from students having difficulty. 67 (Rosé et al., 2000) gives an overview of the evidence in favor of Socratic tutoring as well as describing an opposing viewpoint supporting a tutoring style referred to as didactic. Here, rather than drawing out the answer from the student, the tutor points out the student&apos;s error and explains how to derive the correct answer. We hypothesized that (1) didactic tutoring corresponds to the system-initiative dialogue management currently implemented in tutorial dialogue systems, (2) Socratic tutoring is mixedinitiative, and (3) furthermore that initiative is directly related to &amp;quot;Socraticness&amp;quot; — more student initiati</context>
<context position="12068" citStr="Rosé et al., 2000" startWordPosition="1923" endWordPosition="1926"> who took control of their learning and as a result scored well in the post experiment test. 3 Our Initiative Study This section is a summary of our methodology and results. For more details or to download the corpus or annotation manual, consult the web page http ://www.cog s ci. ed. ac .ukr jmoore/tutoring/ BEE_corpus.html. 3.1 Method The setting for this study is a course on basic electricity and electronics (BEE) developed with the VIVIDS authoring tool (Munro, 1994). Students read four textbook-style lessons and performed six labs using a circuit simulator with a graphical in69 terface. (Rosé et al., 2000) describes an experiment where students went through these lessons and labs with the guidance of a human tutor (the same one for the entire study). Before the lessons, students were given pretests to gauge their initial knowledge. After being tutored, students took the same tests again. We refer to the difference in their scores as learning gain. There were three sets of tutoring sessions (a session means all the dialogue between the tutor and one particular student): (1) the trial sessions where the tutor was not given any instructions on how to tutor 113 students], (2) the Socratic sessions </context>
<context position="13806" citStr="Rosé et al. (2000)" startWordPosition="2211" endWordPosition="2214">re really Socratic and didactic. We used interactivity to approximate &amp;quot;Socraticness&amp;quot;, and showed that the Socratic dialogues were more interactive than the didactic dialogues. On average in the Socratic dialogues: a greater proportion of tutor utterances were questions (42% vs. 29%); the students produced a higher percentage of words in the dialogues (33% vs. 26%); and tutor turns and utterances were shorter. It is debatable whether this means the dialogues are really Socratic and didactic but it proves they reflect different tutoring styles which is sufficient for the purposes of this study. Rosé et al. (2000) addressed the issue of whether the Socratic dialogues in this corpus were more effective than the didactic ones. They found a trend for Socratically tutored students to learn more, but additional data is needed to verify this trend. Chi et al. (2001) performed a similar study; in this case, no difference was found between the two tutoring strategies. However, Chi et al. noted that the didactic tutors sometimes inadvertently revealed answers to questions on the post-test (the test given after tutoring to measure how much was learned). So we cannot say anything conclusive if turn = command then</context>
</contexts>
<marker>Rosé, Moore, VanLehn, Allbritton, 2000</marker>
<rawString>Carolyn P. Rosé, Johanna D. Moore, Kurt VanLehn, and David Allbritton. 2000. A comparative evaluation of socratic versus didactic tutoring. Technical Report LRDC-BEE-1, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Farhana Shah</author>
</authors>
<title>Recognizing and Responding to Student Plans in an Intelligent Tutoring System:</title>
<date>1997</date>
<tech>CIRCSIM-Tutor. Ph.D. thesis,</tech>
<institution>Illinois Institute of Technology.</institution>
<contexts>
<context position="4688" citStr="Shah (1997)" startWordPosition="741" endWordPosition="742">gate these hypotheses, we undertook a systematic investigation of initiative, tutoring strategy (Socratic vs. didactic), and learning (task success) using a series of human-human tutoring dialogues from an earlier project (Rosé et al., 2000). In one set of dialogues, the tutor used a Socratic tutoring style while in the other she used a didactic tutoring style. We annotated these dialogues for initiative, measured the distribution of initiative in the Socratic and didactic dialogues, and measured the relationship between initiative and student learning. 2 Previous Work 2.1 Defining Initiative Shah (1997) defines student initiative as &amp;quot;any contribution by the student that attempts to change the course of the [tutoring] session&amp;quot; (p. 13). Shah&apos;s corpus analysis dealt with remediation dialogues where a tutor quizzed students about the answers they gave during problem solving. In this corpus, student initiatives are student utterances that are not answers to questions. Shah assumes that these initiatives are dealt with exclusively by the tutor&apos;s next speech act, and that initiative then reverts back to the tutor. This definition was too limited for our more free-form tutoring dialogues. Sinclair a</context>
<context position="9222" citStr="Shah (1997)" startWordPosition="1480" endWordPosition="1481">ted dialogues (TODs) involving an expert guiding a novice through assembling a water pump, and advisory dialogues (ADs) involving an expert giving advice about financial and software problems. In the TODs, as we would expect, the expert had initiative most of the time (91% of the turns). However, ADs have closer to an equal sharing of initiative — the expert had initiative for 60% of the turns in finance ADs and 51% of the turns in software ADs. This is because in the ADs, the novice must communicate the details of his problem to the expert as well as the expert telling the novice what to do. Shah (1997) investigated initiative in tutorial dialogue, typed human-human tutoring dialogues dealing with the circulatory system. Her corpus consisted of students&apos; initial tutoring session and a subsequent session with each of the same students. She categorized student initiatives based on their communicative goal (e.g., challenge, support, repair, request information). Shah found that the initial sessions had twice the number of student initiatives as the subsequent sessions. The nature of student initiatives also changed over time: the proportion of student initiatives associated with confusion (long</context>
</contexts>
<marker>Shah, 1997</marker>
<rawString>Farhana Shah. 1997. Recognizing and Responding to Student Plans in an Intelligent Tutoring System: CIRCSIM-Tutor. Ph.D. thesis, Illinois Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Sinclair</author>
<author>R Malcolm Coulthard</author>
</authors>
<title>Towards an Analysis of Discourse: The English used by teachers and pupils.</title>
<date>1975</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="5307" citStr="Sinclair and Coulthard (1975)" startWordPosition="837" endWordPosition="840">hah (1997) defines student initiative as &amp;quot;any contribution by the student that attempts to change the course of the [tutoring] session&amp;quot; (p. 13). Shah&apos;s corpus analysis dealt with remediation dialogues where a tutor quizzed students about the answers they gave during problem solving. In this corpus, student initiatives are student utterances that are not answers to questions. Shah assumes that these initiatives are dealt with exclusively by the tutor&apos;s next speech act, and that initiative then reverts back to the tutor. This definition was too limited for our more free-form tutoring dialogues. Sinclair and Coulthard (1975) developed a dialogue grammar for classroom discussions. Their minimal unit of dialogue is the exchange which is composed of an initiating move, an optional responding move, and an optional feedback move. Whoever makes the initiating move is said to have initiative for the exchange. Although questions can be reasked in cases of incorrect student answers, this framework does not capture other ways an exchange can be disrupted (e.g., the student asks a question rather than answering the current question), and again this definition was too limited for our dialogues. Line11 et al. (1988) discuss h</context>
</contexts>
<marker>Sinclair, Coulthard, 1975</marker>
<rawString>John M. Sinclair and R. Malcolm Coulthard. 1975. Towards an Analysis of Discourse: The English used by teachers and pupils. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan E Strayer</author>
<author>Peter A Heeman</author>
</authors>
<title>Reconciling initiative and discourse structure.</title>
<date>2001</date>
<booktitle>In 2nd SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Aalborg, Denmark,</location>
<contexts>
<context position="7998" citStr="Strayer and Heeman, 2001" startWordPosition="1280" endWordPosition="1283">r to adding or taking away actions from the plan, rearranging actions, or setting parameters. Whittaker and Stenton (1988) do not define initiative beyond calling it control of the dialogue by its participants. Their work is notable in that 1The page numbers come from the digital version: http://citeseer.nj.nec.com/244268.htm1 68 they define a set of rules (see Figure 1) specifying who has initiative for each turn in a dialogue. These rules approximate the more complex definition given by Chu-Carroll and Brown and have been used in several projects because they facilitate reliable annotation (Strayer and Heeman, 2001; Jordan and Di Eugenio, 1997; Doran et al., 2001; Walker and Whittaker, 1990). 2.2 Initiative in human-human corpora Previous work has shown a pattern to how initiative shifts among dialogue participants in problem-solving dialogues. Guinn (1996) used simulated conversational agents to argue that the most efficient problem-solving dialogues are those where the participant who knows the most about the current subtask takes initiative. The corpus analysis of Walker and Whittaker (1990) gives evidence that in natural dialogue, knowledgeable speakers do take initiative. Walker and Whittaker studi</context>
</contexts>
<marker>Strayer, Heeman, 2001</marker>
<rawString>Susan E. Strayer and Peter A. Heeman. 2001. Reconciling initiative and discourse structure. In 2nd SIGdial Workshop on Discourse and Dialogue, Aalborg, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt VanLehn</author>
<author>Stephanie Siler</author>
<author>Charles Murray</author>
<author>William B Baggett</author>
</authors>
<title>What makes a tutorial event effective? In</title>
<date>1998</date>
<booktitle>Proc. of the Twentieth Annual Conference of the Cognitive</booktitle>
<editor>M. A. Gernsbacher and S. Derry, editors,</editor>
<publisher>Science Society. Erlbaum.</publisher>
<contexts>
<context position="2881" citStr="VanLehn et al., 1998" startWordPosition="450" endWordPosition="453">a breakthrough is also needed in the tutoring community. The current system-initiative approaches conflict with arguments that it is the highly collaborative nature of human-human tutoring dialogue that leads to learning (Merrill et al., 1992a; Fox, 1993; Graesser et al., 1995). Through this dialogue, tutors can intervene to ensure that errors are detected and repaired and that students can work around impasses (Merrill et al., 1992b). Previous research has also shown that students must be allowed to construct knowledge themselves to learn most effectively (Chi et al., 1989; Chi et al., 1994; VanLehn et al., 1998). The consensus from these studies is that experienced tutors maintain a delicate balance allowing students to do as much of the work as possible and to maintain a feeling of control, while providing students with enough guidance to keep them from becoming too frustrated or confused. We refer to this style of tutoring as &amp;quot;Socratic&amp;quot; because it is characterized by the use of questions and other hints to draw out answers from students having difficulty. 67 (Rosé et al., 2000) gives an overview of the evidence in favor of Socratic tutoring as well as describing an opposing viewpoint supporting a t</context>
</contexts>
<marker>VanLehn, Siler, Murray, Baggett, 1998</marker>
<rawString>Kurt VanLehn, Stephanie Siler, Charles Murray, and William B. Baggett. 1998. What makes a tutorial event effective? In M. A. Gernsbacher and S. Derry, editors, Proc. of the Twentieth Annual Conference of the Cognitive Science Society. Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Steve Whittaker</author>
</authors>
<title>Mixed initiative in dialogue: An investigation into discourse segmentation.</title>
<date>1990</date>
<booktitle>In Proc. of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>70--78</pages>
<contexts>
<context position="8076" citStr="Walker and Whittaker, 1990" startWordPosition="1293" endWordPosition="1296">etting parameters. Whittaker and Stenton (1988) do not define initiative beyond calling it control of the dialogue by its participants. Their work is notable in that 1The page numbers come from the digital version: http://citeseer.nj.nec.com/244268.htm1 68 they define a set of rules (see Figure 1) specifying who has initiative for each turn in a dialogue. These rules approximate the more complex definition given by Chu-Carroll and Brown and have been used in several projects because they facilitate reliable annotation (Strayer and Heeman, 2001; Jordan and Di Eugenio, 1997; Doran et al., 2001; Walker and Whittaker, 1990). 2.2 Initiative in human-human corpora Previous work has shown a pattern to how initiative shifts among dialogue participants in problem-solving dialogues. Guinn (1996) used simulated conversational agents to argue that the most efficient problem-solving dialogues are those where the participant who knows the most about the current subtask takes initiative. The corpus analysis of Walker and Whittaker (1990) gives evidence that in natural dialogue, knowledgeable speakers do take initiative. Walker and Whittaker studied task-oriented dialogues (TODs) involving an expert guiding a novice through</context>
<context position="17743" citStr="Walker and Whittaker (1990)" startWordPosition="2841" endWordPosition="2844">re considered acceptable.2 Although these initiative assignment rules allow reliable annotation and are easy to implement, the question remains whether they actually capture initiative. It is clear that commands and questions not following questions (i.e., not clarification questions) set the discourse segment purpose (i.e., take initiative). The contentious aspects of these rules are assuming that answers never take initiative and that questions following questions never take initiative. It is simple to construct counterexamples to these assumptions; however, the rules work well in practice. Walker and Whittaker (1990) showed that third person and one anaphora rarely crossed segment boundaries marked by initiative changes annotated with these guidelines.3 It may be the case that these annotation assumptions fail on selected examples. However, in eliminating the assumptions it is likely that we will introduce more errors than we correct. For example, it is clear that some answers take initiative; if a speaker asks &amp;quot;what time is it?&amp;quot; and the listener gives more information than the current time, then the listener has taken initiative. However, if the speaker asks &amp;quot;what causes current to flow?&amp;quot;, it is much mor</context>
<context position="21450" citStr="Walker and Whittaker (1990)" startWordPosition="3456" endWordPosition="3459">g gain (Pearson&apos;s r = 0.56, n = 23, p &lt; 0.005), and the relationship between average percentage 4To analyze significance, we looked at average percentage of expert initiative per session rather than per corpus. For the didactic dialogues, this average is 82% and for the Socratic dialogues it is 90%, a significant difference (t = 2.26, df=18, p &lt; 0.05 two-tailed). 71 50 45 40 35 30 25 20 5 10 15 20 25 30 35 40 learning gain Figure 2: Learning Gain Comparisons of tutor utterances that were questions and learning gain (Pearson&apos;s r = 0.46, n = 23, p &lt; 0.05). In section 1, we discussed the work of Walker and Whittaker (1990) on investigating initiative in the genres of advisory dialogues (ADs) and task oriented dialogues (TODs). Walker and Whittaker also investigated the difference between TODs in a spoken (telephone) modality and a typed (computer chat) modality. The results of their study are shown in columns 3-6 of Table 1 and the corresponding measures from our study are in columns 1 and 2. The Socratic dialogues have almost the same average expert initiative as TODs. In the TODs, the expert would issue a series of commands in order to get the novice to perform a procedure. In the Socratic dialogues, the tuto</context>
</contexts>
<marker>Walker, Whittaker, 1990</marker>
<rawString>Marilyn A. Walker and Steve Whittaker. 1990. Mixed initiative in dialogue: An investigation into discourse segmentation. In Proc. of the 28th Annual Meeting of the Association for Computational Linguistics, pages 70-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Whittaker</author>
<author>Phil Stenton</author>
</authors>
<title>Cues and control in expert-client dialogues.</title>
<date>1988</date>
<booktitle>In Proc. of the 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>123--130</pages>
<contexts>
<context position="7496" citStr="Whittaker and Stenton (1988)" startWordPosition="1200" endWordPosition="1203">ve) from task initiative. They define dialogue initiative by stating that it &amp;quot;tracks the lead in determining the current discourse focus&amp;quot; (p. 6),1 and that task initiative &amp;quot;tracks the lead in the development of the agents&apos; plan&amp;quot; (p. 6). Presumably, determining the discourse focus means setting the discourse segment purpose as defined in Grosz and Sidner&apos;s (1986) theory of discourse. What it means to take the lead in developing the agents&apos; plan depends on the plan representation but informally can refer to adding or taking away actions from the plan, rearranging actions, or setting parameters. Whittaker and Stenton (1988) do not define initiative beyond calling it control of the dialogue by its participants. Their work is notable in that 1The page numbers come from the digital version: http://citeseer.nj.nec.com/244268.htm1 68 they define a set of rules (see Figure 1) specifying who has initiative for each turn in a dialogue. These rules approximate the more complex definition given by Chu-Carroll and Brown and have been used in several projects because they facilitate reliable annotation (Strayer and Heeman, 2001; Jordan and Di Eugenio, 1997; Doran et al., 2001; Walker and Whittaker, 1990). 2.2 Initiative in </context>
</contexts>
<marker>Whittaker, Stenton, 1988</marker>
<rawString>Steve Whittaker and Phil Stenton. 1988. Cues and control in expert-client dialogues. In Proc. of the 26th Annual Meeting of the Association for Computational Linguistics, pages 123-130.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>