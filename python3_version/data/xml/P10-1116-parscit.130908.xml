<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000639">
<title confidence="0.9993645">
Topic Models for Word Sense Disambiguation and
Token-based Idiom Detection
</title>
<author confidence="0.983882">
Linlin Li, Benjamin Roth, and Caroline Sporleder
</author>
<affiliation confidence="0.879061">
Saarland University, Postfach 15 11 50
</affiliation>
<address confidence="0.560133">
66041 Saarbr¨ucken, Germany
</address>
<email confidence="0.994498">
{linlin, beroth, csporled}@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.993852" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951631578947">
This paper presents a probabilistic model
for sense disambiguation which chooses
the best sense based on the conditional
probability of sense paraphrases given a
context. We use a topic model to decom-
pose this conditional probability into two
conditional probabilities with latent vari-
ables. We propose three different instanti-
ations of the model for solving sense dis-
ambiguation problems with different de-
grees of resource availability. The pro-
posed models are tested on three different
tasks: coarse-grained word sense disam-
biguation, fine-grained word sense disam-
biguation, and detection of literal vs. non-
literal usages of potentially idiomatic ex-
pressions. In all three cases, we outper-
form state-of-the-art systems either quan-
titatively or statistically significantly.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999916847457627">
Word sense disambiguation (WSD) is the task of
automatically determining the correct sense for a
target word given the context in which it occurs.
WSD is an important problem in NLP and an es-
sential preprocessing step for many applications,
including machine translation, question answering
and information extraction. However, WSD is a
difficult task, and despite the fact that it has been
the focus of much research over the years, state-
of-the-art systems are still often not good enough
for real-world applications. One major factor that
makes WSD difficult is a relative lack of manu-
ally annotated corpora, which hampers the perfor-
mance of supervised systems.
To address this problem, there has been a
significant amount of work on unsupervised
WSD that does not require manually sense-
disambiguated training data (see McCarthy (2009)
for an overview). Recently, several researchers
have experimented with topic models (Brody and
Lapata, 2009; Boyd-Graber et al., 2007; Boyd-
Graber and Blei, 2007; Cai et al., 2007) for sense
disambiguation and induction. Topic models are
generative probabilistic models of text corpora in
which each document is modelled as a mixture
over (latent) topics, which are in turn represented
by a distribution over words.
Previous approaches using topic models for
sense disambiguation either embed topic features
in a supervised model (Cai et al., 2007) or rely
heavily on the structure of hierarchical lexicons
such as WordNet (Boyd-Graber et al., 2007). In
this paper, we propose a novel framework which
is fairly resource-poor in that it requires only 1)
a large unlabelled corpus from which to estimate
the topics distributions, and 2) paraphrases for the
possible target senses. The paraphrases can be
user-supplied or can be taken from existing re-
sources.
We approach the sense disambiguation task by
choosing the best sense based on the conditional
probability of sense paraphrases given a context.
We propose three models which are suitable for
different situations: Model I requires knowledge
of the prior distribution over senses and directly
maximizes the conditional probability of a sense
given the context; Model II maximizes this condi-
tional probability by maximizing the cosine value
of two topic-document vectors (one for the sense
and one for the context). We apply these models
to coarse- and fine-grained WSD and find that they
outperform comparable systems for both tasks.
We also test our framework on the related task
of idiom detection, which involves distinguishing
literal and nonliteral usages of potentially ambigu-
ous expressions such as rock the boat. For this
task, we propose a third model. Model III cal-
culates the probability of a sense given a context
according to the component words of the sense
</bodyText>
<page confidence="0.950411">
1138
</page>
<note confidence="0.942585">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1138–1147,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9977485">
paraphrase. Specifically, it chooses the sense type
which maximizes the probability (given the con-
text) of the paraphrase component word with the
highest likelihood of occurring in that context.
This model also outperforms state-of-the-art sys-
tems.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99998225974026">
There is a large body of work on WSD, cover-
ing supervised, unsupervised (word sense induc-
tion) and knowledge-based approaches (see Mc-
Carthy (2009) for an overview). While most su-
pervised approaches treat the task as a classifica-
tion task and use hand-labelled corpora as train-
ing data, most unsupervised systems automatically
group word tokens into similar groups using clus-
tering algorithms, and then assign labels to each
sense cluster. Knowledge-based approaches ex-
ploit information contained in existing resources.
They can be combined with supervised machine-
learning models to assemble semi-supervised ap-
proaches.
Recently, a number of systems have been pro-
posed that make use of topic models for sense
disambiguation. Cai et al. (2007), for example,
use LDA to capture global context. They com-
pute topic models from a large unlabelled corpus
and include them as features in a supervised sys-
tem. Boyd-Graber and Blei (2007) propose an un-
supervised approach that integrates McCarthy et
al.’s (2004) method for finding predominant word
senses into a topic modelling framework. In ad-
dition to generating a topic from the document’s
topic distribution and sampling a word from that
topic, the enhanced model also generates a distri-
butional neighbour for the chosen word and then
assigns a sense based on the word, its neighbour
and the topic. Boyd-Graber and Blei (2007) test
their method on WSD and information retrieval
tasks and find that it can lead to modest improve-
ments over state-of-the-art results.
In another unsupervised system, Boyd-Graber
et al. (2007) enhance the basic LDA algorithm by
incorporating WordNet senses as an additional la-
tent variable. Instead of generating words directly
from a topic, each topic is associated with a ran-
dom walk through the WordNet hierarchy which
generates the observed word. Topics and synsets
are then inferred together. While Boyd-Graber
et al. (2007) show that this method can lead to
improvements in accuracy, they also find that id-
iosyncracies in the hierarchical structure of Word-
Net can harm performance. This is a general prob-
lem for methods which use hierarchical lexicons
to model semantic distance (Budanitsky and Hirst,
2006). In our approach, we circumvent this prob-
lem by exploiting paraphrase information for the
target senses rather than relying on the structure
of WordNet as a whole.
Topic models have also been applied to the re-
lated task of word sense induction. Brody and
Lapata (2009) propose a method that integrates a
number of different linguistic features into a single
generative model.
Topic models have been previously consid-
ered for metaphor extraction and estimating the
frequency of metaphors (Klebanov et al., 2009;
Bethard et al., 2009). However, we have a differ-
ent focus in this paper, which aims to distinguish
literal and nonliteral usages of potential idiomatic
expressions. A number of methods have been ap-
plied to this task. Katz and Giesbrecht (2006)
devise a supervised method in which they com-
pute the meaning vectors for the literal and non-
literal usages of a given expression in the trainning
data. Birke and Sarkar (2006) use a clustering al-
gorithm which compares test instances to two au-
tomatically constructed seed sets (one literal and
one nonliteral), assigning the label of the closest
set. An unsupervised method that computes co-
hesive links between the component words of the
target expression and its context have been pro-
posed (Sporleder and Li, 2009; Li and Sporleder,
2009). Their system predicts literal usages when
strong links can be found.
</bodyText>
<sectionHeader confidence="0.964419" genericHeader="method">
3 The Sense Disambiguation Model
</sectionHeader>
<subsectionHeader confidence="0.956058">
3.1 Topic Model
</subsectionHeader>
<bodyText confidence="0.999893583333333">
As pointed out by Hofmann (1999), the starting
point of topic models is to decompose the con-
ditional word-document probability distribution
p(w1d) into two different distributions: the word-
topic distribution p(w1z), and the topic-document
distribution p(zld) (see Equation 1). This allows
each semantic topic z to be represented as a multi-
nominal distribution of words p(wjz), and each
document d to be represented as a multinominal
distribution of semantic topics p(zld). The model
introduces a conditional independence assumption
that document d and word w are independent con-
</bodyText>
<page confidence="0.99224">
1139
</page>
<bodyText confidence="0.96343">
ditioned on the hidden variable, topic z.
</bodyText>
<equation confidence="0.9713455">
p(w|d) = E p(z|d)p(w|z) (1)
z
</equation>
<bodyText confidence="0.999214875">
LDA is a Bayesian version of this framework with
Dirichlet hyper-parameters (Blei et al., 2003).
The inference of the two distributions given an
observed corpus can be done through Gibbs Sam-
pling (Geman and Geman, 1987; Griffiths and
Steyvers, 2004). For each turn of the sampling,
each word in each document is assigned a seman-
tic topic based on the current word-topic distribu-
tion and topic-document distribution. The result-
ing topic assignments are then used to re-estimate
a new word-topic distribution and topic-document
distribution for the next turn. This process re-
peats until convergence. To avoid statistical co-
incidence, the final estimation of the distributions
is made by the average of all the turns after con-
vergence.
</bodyText>
<subsectionHeader confidence="0.999373">
3.2 The Sense Disambiguation Model
</subsectionHeader>
<bodyText confidence="0.9996735">
Assigning the correct sense s to a target word w
occurring in a context c involves finding the sense
which maximizes the conditional probability of
senses given a context:
</bodyText>
<equation confidence="0.894682">
s = arg max p(si|c) (2)
si
</equation>
<bodyText confidence="0.98789575">
In our model, we represent a sense (si) as a col-
lection of ‘paraphrases’ that capture (some aspect
of) the meaning of the sense. These paraphrases
can be taken from an existing resource such as
WordNet (Miller, 1995) or supplied by the user
(see Section 4).
This conditional probability is decomposed by
incorporating a hidden variable, topic z, intro-
duced by the topic model. We propose three varia-
tions of the basic model, depending on how much
background information is available, i.e., knowl-
edge of the prior sense distribution available and
type of sense paraphrases used. In Model I and
Model II, the sense paraphrases are obtained from
WordNet, and both the context and the sense para-
phrases are treated as documents, c = dc and
</bodyText>
<subsectionHeader confidence="0.474225">
s = ds.
</subsectionHeader>
<bodyText confidence="0.999918789473684">
WordNet is a fairly rich resource which pro-
vides detailed information about word senses
(glosses, example sentences, synsets, semantic re-
lations between senses, etc.). Sometimes such de-
tailed information may not be available, for in-
stance for languages for which such a resource
does not exist or for expressions that are not
very well covered in WordNet, such as idioms.
For those situations, we propose another model,
Model III, in which contexts are treated as docu-
ments while sense paraphrases are treated as se-
quences of independent words.1
Model I directly maximizes the conditional
probability of the sense given the context, where
the sense is modeled as a ‘paraphrase document’
ds and the context as a ‘context document’ dc.
The conditional probability of sense given context
p(ds|dc) can be rewritten as a joint probability di-
vided by a normalization factor:
</bodyText>
<equation confidence="0.996554">
p(ds|dc) = p(ds, dc) (3)
p(dc)
</equation>
<bodyText confidence="0.9998695">
This joint probability can be rewritten as a gen-
erative process by introducing a hidden variable z.
We make the conditional independence assump-
tion that, conditioned on the topic z, a paraphrase
document ds is generated independently of the
specific context document dc:
</bodyText>
<equation confidence="0.7230685">
p(ds, dc) = E p(ds)p(z|ds)p(dc|z) (4)
z
</equation>
<bodyText confidence="0.9991795">
We apply the same process to the conditional
probability p(dc|z). It can be rewritten as:
</bodyText>
<equation confidence="0.9932835">
p(dc|z) = p(dc)p(z|dc) (5)
p(z)
</equation>
<bodyText confidence="0.993115666666667">
Now, the disambiguation model p(ds|dc) can be
rewritten as a prior p(ds) times a topic function
f(z):
</bodyText>
<equation confidence="0.9660026">
E
p(ds|dc) = p(ds)
z
As p(z) is a uniform distribution according to
the uniform Dirichlet priors assumption, Equation
6 can be rewritten as:
p(ds|dc) ∝ p(ds) E p(z|dc)p(z|ds) (7)
z
Model I:
p(z|dc)p(z|dsi) (8)
</equation>
<bodyText confidence="0.98695">
Model I has the disadvantage that it requires
information about the prior distribution of senses
</bodyText>
<footnote confidence="0.9383975">
1The idea is that these key words capture the meaning of
the idioms.
</footnote>
<figure confidence="0.9934385">
p(z|dc)p(z|ds)
(6)
p(z)
arg max E
dsi p(dsi)
z
</figure>
<page confidence="0.957213">
1140
</page>
<bodyText confidence="0.986119045454546">
p(ds), which is not always available. We use sense
frequency information from WordNet to estimate
the prior sense distribution, although it must be
kept in mind that, depending on the genre of the
texts, it is possible that the distribution of senses
in the testing corpus may diverge greatly from the
WordNet-based estimation. If there is no means
for estimating the prior sense distribution of an
experimental corpus, generally a uniform distri-
bution must be assumed. However, this assump-
tion does not hold, as the true distribution of word
senses is often highly skewed (McCarthy, 2009).
To overcome this problem, we propose Model
II, which indirectly maximizes the sense-context
probability by maximizing the cosine value of two
document vectors that encode the document-topic
frequencies from sampling, v(z|dc) and v(z|ds).
The document vectors are represented by topics,
with each dimension representing the number of
times that the tokens in this document are assigned
to a certain topic.
Model II:
</bodyText>
<equation confidence="0.892084">
arg max cos(v(z|dc), v(z|dsz)) (9)
dsi
</equation>
<bodyText confidence="0.991266472222222">
If the prior distribution of senses is known, Model
I is the best choice. However, Model II has to be
chosen instead when this knowledge is not avail-
able. In our experiments, we test the performance
of both models (see Section 5).
If the sense paraphrases are very short, it is diffi-
cult to reliably estimate p(z|ds). In order to solve
this problem, we treat the sense paraphrase ds as
a ‘query’, a concept which is used in information
retrieval. One model from information retrieval
takes the conditional probability of the query given
the document as a product of all the conditional
probabilities of words in the query given the doc-
ument. The assumption is that the query is gener-
ated by a collection of conditionally independent
words (Song and Croft, 1999).
We make the same assumption here. How-
ever, instead of taking the product of all the condi-
tional probabilities of words given the document,
we take the maximum. There are two reasons for
this: (i) taking the product may penalize longer
paraphrases since the product of probabilities de-
creases as there are more words; (ii) we do not
want to model the probability of generating spe-
cific paraphrases, but rather the probability of gen-
erating a sense, which might only be represented
by one or two words in the paraphrases (e.g., the
potentially idiomatic phrase ‘rock the boat’ can be
paraphrased as ‘break the norm’ or ‘cause trou-
ble’. A similar topic distribution to that of the
individual words ‘norm’ or ‘trouble’ would be
strong supporting evidence of the corresponding
idiomatic reading.). We propose Model III:
p(wz|z)p(z|dc) (10)
where qs is a collection of words contained in the
sense paraphrases.
</bodyText>
<subsectionHeader confidence="0.955586">
3.3 Inference
</subsectionHeader>
<bodyText confidence="0.999996666666667">
One possible inference approach is to combine the
context documents and sense paraphrases into a
corpus and run Gibbs sampling on this corpus.
The problem with this approach is that the test set
and sense paraphrase set are relatively small, and
topic models running on a small corpus are less
likely to capture rich semantic topics. One sim-
ple explanation for this is that a small corpus usu-
ally has a relatively small vocabulary, which is less
representative of topics, i.e., p(w|z) cannot be es-
timated reliably.
In order to overcome this problem, we infer the
word-topic distribution from a very large corpus
(Wikipedia dump, see Section 4). All the follow-
ing inference experiments on the test corpus are
based on the assumption that the word-topic dis-
tribution p(w|z) is the same as the one estimated
from the Wikipedia dump. Inference of topic-
document distributions for context and sense para-
phrases is done by fixing the word-topic distribu-
tion as a constant.
</bodyText>
<sectionHeader confidence="0.999312" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9999699375">
We evaluate our models on three different tasks:
coarse-grained WSD, fine-grained WSD and lit-
eral vs. nonliteral sense detection. In this section
we discuss our experimental set-up. We start by
describing the three datasets for evaluation and an-
other dataset for probability estimation. We also
discuss how we choose sense paraphrases and in-
stance contexts.
Data We use three datasets for evaluation. The
coarse-grained task is evaluated on the Semeval-
2007 Task-07 benchmark dataset released by Nav-
igli et al. (2009). The dataset consists of 5377
words of running text from five different articles:
the first three were obtained from the WSJ cor-
pus, the fourth was the Wikipedia entry for com-
puter programming, and the fifth was an excerpt of
</bodyText>
<figure confidence="0.979101666666667">
arg max �max
qsi wiEqs
z
</figure>
<page confidence="0.97852">
1141
</page>
<bodyText confidence="0.999885387755102">
Amy Steedman’s Knights of the Art, biographies
of Italian painters. The proportion of the non news
text, the last two articles, constitutes 51.87% of the
whole testing set. It consists of 1108 nouns, 591
verbs, 362 adjectives, and 208 adverbs. The data
were annotated with coarse-grained senses which
were obtained by clustering senses from the Word-
Net 2.1 sense inventory based on the procedure
proposed by Navigli (2006).
To determine whether our model is also suitable
for fine-grained WSD, we test on the data provided
by Pradhan et al. (2009) for the Semeval-2007
Task-17 (English fine-grained all-words task).
This dataset is a subset of the set from Task-07. It
comprises the three WSJ articles from Navigli et
al. (2009). A total of 465 lemmas were selected as
instances from about 3500 words of text. There are
10 instances marked as ‘U’ (undecided sense tag).
Of the remaining 455 instances, 159 are nouns and
296 are verbs. The sense inventory is from Word-
Net 2.1.
Finally, we test our model on the related sense
disambiguation task of distinguishing literal and
nonliteral usages of potentially ambiguous expres-
sions such as break the ice. For this, we use the
dataset from Sporleder and Li (2009) as a test set.
This dataset consists of 3964 instances of 17 po-
tential English idioms which were manually anno-
tated as literal or nonliteral.
A Wikipedia dump2 is used to estimate the
multinomial word-topic distribution. This dataset,
which consists of 320,000 articles,3 is significantly
larger than SemCor, which is the dataset used by
Boyd-Graber et al. (2007). All markup from the
Wikipedia dump was stripped off using the same
filter as the ESA implementation (Sorg and Cimi-
ano, 2008), and stopwords were filtered out using
the Snowball (Porter, October 2001) stopword list.
In addition, words with a Wikipedia document fre-
quency of 1 were filtered out. The lemmatized
version of the corpus consists of 299,825 lexical
units.
The test sets were POS-tagged and lemmatized
using RASP (Briscoe and Carroll, 2006). The in-
ference processes are run on the lemmatized ver-
sion of the corpus. For the Semeval-2007 Task 17
English all-words, the organizers do not supply the
part-of-speech and lemma information of the tar-
get instances. In order to avoid the wrong predic-
</bodyText>
<footnote confidence="0.949236">
2We use the English snapshot of 2009-07-13
3All articles of fewer than 100 words were discarded.
</footnote>
<bodyText confidence="0.999958">
tions caused by tagging or lemmatization errors,
we manually corrected any bad tags and lemmas
for the target instances.4
Sense Paraphrases For word sense disam-
biguation tasks, the paraphrases of the sense keys
are represented by information from WordNet 2.1.
(Miller, 1995). To obtain the paraphrases, we use
the word forms, glosses and example sentences
of the synset itself and a set of selected reference
synsets (i.e., synsets linked to the target synset by
specific semantic relations, see Table 1). We ex-
cluded the ‘hypernym reference synsets’, since in-
formation common to all of the child synsets may
confuse the disambiguation process.
For the literal vs. nonliteral sense detection
task, we selected the paraphrases of the nonlit-
eral meaning from several online idiom dictionar-
ies. For the literal senses, we used 2-3 manu-
ally selected words with which we tried to cap-
ture (aspects of) the literal meaning of the expres-
sion.5 For instance, the literal ‘paraphrases’ that
we chose for ‘break the ice’ were ice, water and
snow. The paraphrases are shorter for the idiom
task than for the WSD task, because the mean-
ing descriptions from the idiom dictionaries are
shorter than what we get from WordNet. In the
latter case, each sense can be represented by its
synset as well as its reference synsets.
Instance Context We experimented with differ-
ent context sizes for the disambiguation task. The
five different context settings that we used for the
WSD tasks are: collocations (1w), ±5-word win-
dow (5w), ±10-word window (10w), current sen-
tence, and whole text. Because the idiom corpus
also includes explicitly marked paragraph bound-
aries, we included ‘paragraph’ as a sixth type of
context size for the idiom sense detection task.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999859333333333">
As mentioned above, we test our proposed sense
disambiguation framework on three tasks. We
start by describing the sampling experiments for
</bodyText>
<footnote confidence="0.970218272727272">
4This was done by comparing the predicted sense keys
and the gold standard sense keys. We only checked instances
for which the POS-tags in the predicted sense keys are not
consistent with those in the gold standard. This was the case
for around 20 instances.
5Note that we use the word ‘paraphrase’ in a fairly wide
sense in this paper. Sometimes it is not possible to obtain ex-
act paraphrases. This applies especially to the task of distin-
guishing literal from nonliteral senses of multi-word expres-
sions. In this case we take as paraphrases some key words
which capture salient aspects of the meaning.
</footnote>
<page confidence="0.96845">
1142
</page>
<table confidence="0.999715">
POS Paraphrase reference synsets
N hyponyms, instance hyponyms, member holonyms, substance holonyms, part holonyms,
member meronyms, part meronyms, substance meronyms, attributes, topic members,
region members, usage members, topics, regions, usages
V Troponyms, entailments, outcomes, phrases, verb groups, topics, regions, usages, sentence frames
A similar, pertainym, attributes, related, topics, regions, usages
R pertainyms, topics, regions, usages
</table>
<tableCaption confidence="0.9912135">
Table 1: Selected reference synsets from WordNet that were used for different parts-of-speech to obtain
word sense paraphrase. N(noun), V(verb), A(adj), R(adv).
</tableCaption>
<bodyText confidence="0.999777076923077">
estimating the word-topic distribution from the
Wikipedia dump. We used the package provided
by Wang et al. (2009) with the suggested Dirich-
let hyper-parameters 6. In order to avoid statistical
instability, the final result is averaged over the last
50 iterations. We did four rounds of sampling with
1000, 500, 250, and 125 topics respectively. The
final word-topic distribution is a normalized con-
catenate of the four distributions estimated in each
round. In average, the sampling program run on
the Wikipedia dump consumed 20G memory, and
each round took about one week on a single AMD
Dual-Core 1000MHZ processor.
</bodyText>
<subsectionHeader confidence="0.945804">
5.1 Coarse-Grained WSD
</subsectionHeader>
<bodyText confidence="0.999831458333333">
In this section we first describe the landscape of
similar systems against which we compare our
models, then present the results of the comparison.
The systems that participated in the SemEval-2007
coarse-grained WSD task (Task-07) can be di-
vided into three categories, depending on whether
training data is needed and whether other types
of background knowledge are required: What we
call Type I includes all the systems that need an-
notated training data. All the participating sys-
tems that have the mark TR fall into this cate-
gory (see Navigli et al. (2009) for the evaluation
for all the participating systems). Type II con-
sists of systems that do not need training data but
require prior knowledge of the sense distribution
(estimated sense frequency). All the participating
systems that have the mark MFS belong to this cat-
egory. Systems that need neither training data nor
prior sense distribution knowledge are categorized
as Type III.
We make this distinction based on two princi-
ples: (i) the cost of building a system; (ii) the
portability of the established resource. Type III
is the cheapest system to build, while Type I and
</bodyText>
<footnote confidence="0.607818">
6They were set as: α = 50
</footnote>
<equation confidence="0.489584">
#c����� and ,Q = 0.01.
</equation>
<bodyText confidence="0.999856615384615">
Type II both need extra resources. Type II has
an advantage over Type I since the prior knowl-
edge of the sense distribution can be estimated
from annotated corpora (e.g.: SemCor, Senseval).
In contrast, training data in Type I may be sys-
tem specific (e.g.: different input format, different
annotation guidelines). McCarthy (2009) also ad-
dresses the issue of performance and cost by com-
paring supervised word sense disambiguation sys-
tems with unsupervised ones.
We exclude the system provided by one of
the organizers (UoR-SSI) from our categorization.
The reason is that although this system is claimed
to be unsupervised, and it performs better than
all the participating systems (including the super-
vised systems) in the SemEval-2007 shared task, it
still needs to incorporate a lot of prior knowledge,
specifically information about co-occurrences be-
tween different word senses, which was obtained
from a number of resources (SSI+LKB) includ-
ing: (i) SemCor (manually annotated); (ii) LDC-
DSO (partly manually annotated); (iii) collocation
dictionaries which are then disambiguated semi-
automatically. Even though the system is not
“trained”, it needs a lot of information which is
largely dependent on manually annotated data, so
it does not fit neatly into the categories Type II or
Type III either.
Table 2 lists the best participating systems of
each type in the SemEval-2007 task (Type I:
NUS-PT (Chan et al., 2007); Type II: UPV-WSD
(Buscaldi and Rosso, 2007); Type III: TKB-UO
(Anaya-S´anchez et al., 2007)). Our Model I be-
longs to Type II, and our Model II belongs to Type
III.
Table 2 compares the performance of our mod-
els with the Semeval-2007 participating systems.
We only compare the F-score, since all the com-
pared systems have an attempted rate7 of 1.0,
</bodyText>
<footnote confidence="0.9909015">
7Attempted rate is defined as the total number of disam-
biguated output instances divided by the total number of input
</footnote>
<page confidence="0.989862">
1143
</page>
<bodyText confidence="0.992182490196079">
which makes both the precision and recall rates the
same as the F-score. We focus on comparisons be-
tween our models and the best SemEval-2007 par-
ticipating systems within the same type. Model I is
compared with UPV-WSD, and Model II is com-
pared with TKB-UO. In addition, we also compare
our system with the most frequent sense baseline
which was not outperformed by any of the systems
of Type II and Type III in the SemEval-2007 task.
Comparison on Type III is marked with&apos;, while
comparison on Type II is marked with *. We find
that Model II performs statistically significantly
better than the best participating system of the
same type TKB-UO (p&lt;&lt;0.01, x2 test). When
encoded with the prior knowledge of sense distri-
bution, Model I outperforms by 1.36% the best
Type II system UPV-WSD, although the differ-
ence is not statistically significant. Furthermore,
Model I also quantitatively outperforms the most
frequent sense baseline BL,,,f,, which, as men-
tioned above, was not beat by any participating
systems that do not use training data.
We also find that our model works best for
nouns. The unsupervised Type III model Model
II achieves better results than the most frequent
sense baseline on nouns, but not on other parts-
of-speech. This is in line with results obtained
by previous systems (Griffiths et al., 2005; Boyd-
Graber and Blei, 2008; Cai et al., 2007). While the
performance on verbs can be increased to outper-
form the most frequent sense baseline by including
the prior sense probability, the performance on ad-
jectives and adverbs remains below the most fre-
quent sense baseline. We think that there are three
reasons for this: first, adjectives and adverbs have
fewer reference synsets for paraphrases compared
with nouns and verbs (see Table 1); second, adjec-
tives and adverbs tend to convey less key semantic
content in the document, so they are more difficult
to capture by the topic model; and third, adjectives
and adverbs are a small portion of the test set, so
their performances are statistically unstable. For
example, if ‘already’ appears 10 times out of 20
adverb instances, a system may get bad result on
adverbs only because of its failure to disambiguate
the word ‘already’.
Paraphrase analysis Table 2 also shows the
effect of different ways of choosing sense para-
phrases. MII+ref is the result of including the ref-
erence synsets, while MII-ref excludes the refer-
instances.
</bodyText>
<table confidence="0.999024555555556">
System Noun Verb Adj Adv All
UoR-SSI 84.12 78.34 85.36 88.46 83.21
NUS-PT 82.31 78.51 85.64 89.42 82.50
UPV-WSD 79.33 72.76 84.53 81.52 78.63*
TKB-UO 70.76 62.61 78.73 74.04 70.21&apos;
MII–ref 78.16 70.39 79.56 81.25 76.64
MII+ref 80.05 70.73 82.04 82.21 78.14&apos;
MI+ref 79.96 75.47 83.98 86.06 79.99*
BL,f3 77.44 75.30 84.25 87.50 78.99*
</table>
<tableCaption confidence="0.920146333333333">
Table 2: Model performance (F-score) on the
coarse-grained dataset (context=sentence). Para-
phrases with/without reference synsets (+ref/-ref).
</tableCaption>
<table confidence="0.999814833333333">
Context Ate. Pre. Rec. F1
±1w 91.67 75.05 68.80 71.79
±5w 99.29 77.14 76.60 76.87
±10w 100 77.92 77.92 77.92
text 100 76.86 76.86 76.86
sent. 100 78.14 78.14 78.14
</table>
<tableCaption confidence="0.98114">
Table 3: Model II performance on different con-
</tableCaption>
<bodyText confidence="0.964941814814815">
text size. attempted rate (Ate.), precision (Pre.),
recall (Rec.), F-score (F1).
ence synsets. As can be seen from the table, in-
cluding all reference synsets in sense paraphrases
increases performance. Longer paraphrases con-
tain more information, and they are statistically
more stable for inference.
We find that nouns get the greatest perfor-
mance boost from including reference synsets, as
they have the largest number of different types of
synsets. We also find the ‘similar’ reference synset
for adjectives to be very useful. Performance on
adjectives increases by 2.75% when including this
reference synset.
Context analysis In order to study how the con-
text influences the performance, we experiment
with Model II on different context sizes (see Ta-
ble 3). We find sentence context is the best size for
this disambiguation task. Using a smaller context
not only reduces the precision, but also reduces the
recall rate, which is caused by the all-zero topic as-
signment by the topic model for documents only
containing words that are not in the vocabulary.
As a result, the model is unable to disambiguate.
The context based on the whole text (article) does
not perform well either, possibly because using the
full text folds in too much noisy information.
</bodyText>
<page confidence="0.976237">
1144
</page>
<table confidence="0.9960225">
System F-score
RACAI 52.7 ±4.5
BLmf, 55.91±4.5
MI+ref 56.99±4.5
</table>
<tableCaption confidence="0.9907485">
Table 4: Model performance (F-score) for the fine-
grained word sense disambiguation task.
</tableCaption>
<subsectionHeader confidence="0.995145">
5.2 Fine-grained WSD
</subsectionHeader>
<bodyText confidence="0.999982923076923">
We saw in the previous section that our frame-
work performs well on coarse-grained WSD. Fine-
grained WSD, however, is a more difficult task. To
determine whether our framework is also able to
detect subtler sense distinctions, we tested Model I
on the English all-words subtask of SemEval-2007
Task-17 (see Table 4).
We find that Model I performs better than both
the best unsupervised system, RACAI (Ion and
Tufis¸, 2007) and the most frequent sense baseline
(BLmf,), although these differences are not sta-
tistically significant due to the small size of the
available test data (465).
</bodyText>
<subsectionHeader confidence="0.998511">
5.3 Idiom Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999808642857143">
In the previous section, we provided the results
of applying our framework to coarse- and fine-
grained word sense disambiguation tasks. For
both tasks, our models outperform the state-of-
the-art systems of the same type either quantita-
tively or statistically significantly. In this section,
we apply Model III to another sense disambigua-
tion task, namely distinguishing literal and nonlit-
eral senses of ambiguous expressions.
WordNet has a relatively low coverage for id-
iomatic expressions. In order to represent non-
literal senses, we replace the paraphrases obtained
automatically from WordNet by words selected
manually from online idiom dictionaries (for the
nonliteral sense) and by linguistic introspection
(for the literal sense). We then compare the topic
distributions of literal and nonliteral senses.
As the paraphrases obtained from the idiom dic-
tionary are very short, we treat the paraphrase
as a sequence of independent words instead of
as a document and apply Model III (see Sec-
tion 3). Table 5 shows the results of our pro-
posed model compared with state-of-the-art sys-
tems. We find that the system significantly out-
performs the majority baseline (p&lt;&lt;0.01, x2 test)
and the cohesion-graph based approach proposed
by Sporleder and Li (2009) (p&lt;&lt;0.01, x2 test).
The system also outperforms the bootstrapping
</bodyText>
<table confidence="0.99863">
System Precl Recl Fl Acc.
Basemaj - - - 78.25
co-graph 50.04 69.72 58.26 78.38
boot. 71.86 66.36 69.00 87.03
Model III 67.05 81.07 73.40 87.24
</table>
<tableCaption confidence="0.985846">
Table 5: Performance on the literal or nonliteral
</tableCaption>
<bodyText confidence="0.994061">
sense disambiguation task on idioms. literal pre-
cision (Precl), literal recall (Recl), literal F-score
(Fl), accuracy(Acc.).
system by Li and Sporleder (2009), although not
statistically significantly. This shows how a lim-
ited amount of human knowledge (e.g., para-
phrases) can be added to an unsupervised system
for a strong boost in performance ( Model III com-
pared with the cohesion-graph and the bootstrap-
ping approaches).
For obvious reasons, this approach is sensitive
to the quality of the paraphrases. The paraphrases
chosen to characterise (aspects of) the meaning of
a sense should be non-ambiguous between the lit-
eral or idiomatic meaning. For instance, ‘fire’ is
not a good choice for a paraphrase of the literal
reading of ‘play with fire’, since this word can
be interpreted literally as ‘fire’ or metaphorically
as ‘something dangerous’. The verb component
word ‘play’ is a better literal paraphrase.
For the same reason, this approach works well
for expressions where the literal and nonliteral
readings are well separated (i.e., occur in different
contexts), while the performance drops for expres-
sions whose literal and idiomatic readings can ap-
pear in a similar context. We test the performance
on individual idioms on the five most frequent id-
ioms in our corpus8 (see Table 6). We find that
‘drop the ball’ is a difficult case. The words ‘fault’,
‘mistake’, ‘fail’ or ‘miss’ can be used as the nonlit-
eral paraphrases. However, it is also highly likely
that these words are used to describe a scenario in
a baseball game, in which ‘drop the ball’ is used
literally. In contrast, the performance on ‘rock the
boat’ is much better, since the nonliteral reading
of the phrases ‘break the norm’ or ‘cause trouble’
are less likely to be linked with the literal reading
‘boat’. This may also be because ‘boat’ is not of-
ten used metaphorically in the corpus.
As the topic distribution of nouns and verbs
exhibit different properties, topic comparisons
across parts-of-speech do not make sense. We
</bodyText>
<footnote confidence="0.9554735">
8We tested only on the most frequent idioms in order to
avoid statistically unreliable observations.
</footnote>
<page confidence="0.943272">
1145
</page>
<table confidence="0.7880455">
Idiom Acc.
drop the ball 75.86
play with fire 91.17
break the ice 87.43
rock the boat 95.82
set in stone 89.39
</table>
<tableCaption confidence="0.986151">
Table 6: Performance on individual idioms.
</tableCaption>
<bodyText confidence="0.999893333333333">
make the topic distributions comparable by mak-
ing sure each type of paraphrase contains the same
sets of parts-of-speech. For instance, we do not
permit combinations of literal paraphrases which
only consist of nouns and nonliteral paraphrases
which only consist of verbs.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999141371428572">
We propose three models for sense disambigua-
tion on words and multi-word expressions. The
basic idea of these models is to compare the topic
distribution of a target instance with the candidate
sense paraphrases and choose the most probable
one. While Model I and Model III model the
problem in a probabilistic way, Model II uses a
vector space model by comparing the cosine val-
ues of two topic vectors. Model II and Model III
are completely unsupervised, while Model I needs
the prior sense distribution. Model I and Model
II treat the sense paraphrases as documents, while
Model III treats the sense paraphrases as a collec-
tion of independent words.
We test the proposed models on three tasks. We
apply Model I and Model II to the WSD tasks due
to the availability of more paraphrase information.
Model III is applied to the idiom detection task
since the paraphrases from the idiom dictionary
are smaller. We find that all models outperform
comparable state-of-the-art systems either quanti-
tatively or statistically significantly.
By testing our framework on three different
sense disambiguation tasks, we show that the
framework can be used flexibly in different ap-
plication tasks. The system also points out a
promising way of solving the granularity problem
of word sense disambiguation, as new application
tasks which need different sense granularities can
utilize this framework when new paraphrases of
sense clusters are available. In addition, this sys-
tem can also be used in a larger context such as
figurative language identification (literal or figu-
rative) and sentiment detection (positive or nega-
tive).
</bodyText>
<sectionHeader confidence="0.995241" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999370666666667">
This work was funded by the DFG within the
Cluster of Excellence “Multimodal Computing
and Interaction”.
</bodyText>
<sectionHeader confidence="0.990232" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999476142857143">
H. Anaya-S´anchez, A. Pons-Porrata, R. Berlanga-
Llavori. 2007. TKB-UO: using sense clustering for
WSD. In SemEval ’07: Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, 322–
325.
S. Bethard, V. T. Lai, J. H. Martin. 2009. Topic model
analysis of metaphor frequency for psycholinguistic
stimuli. In CALC ’09: Proceedings of the Workshop
on Computational Approaches to Linguistic Creativ-
ity, 9–16, Morristown, NJ, USA. Association for
Computational Linguistics.
J. Birke, A. Sarkar. 2006. A clustering approach
for the nearly unsupervised recognition of nonliteral
language. In Proceedings of EACL-06.
D. M. Blei, A. Y. Ng, M. I. Jordan. 2003. Latent
dirichlet allocation. Journal of Machine Learning
Reseach, 3:993–1022.
J. Boyd-Graber, D. Blei. 2007. PUTOP: turning
predominant senses into a topic model for word
sense disambiguation. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), 277–281.
J. Boyd-Graber, D. Blei. 2008. Syntactic topic models.
Computational Linguistics.
J. Boyd-Graber, D. Blei, X. Zhu. 2007. A topic
model for word sense disambiguation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), 1024–1033.
T. Briscoe, J. Carroll. 2006. Evaluating the accuracy
of an unlexicalized statistical parser on the PARC
DepBank. In Proceedings of the COLING/ACL on
Main conference poster sessions, 41–48.
S. Brody, M. Lapata. 2009. Bayesian word sense
induction. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
103–111.
A. Budanitsky, G. Hirst. 2006. Evaluating wordnet-
based measures of lexical semantic relatedness.
Computational Linguistics, 32(1):13–47.
D. Buscaldi, P. Rosso. 2007. UPV-WSD: Combining
different WSD methods by means of Fuzzy Borda
Voting. In SemEval ’07: Proceedings of the 4th
International Workshop on Semantic Evaluations,
434–437.
J. Cai, W. S. Lee, Y. W. Teh. 2007. Improving word
sense disambiguation using topic features. In Pro-
ceedings of the 2007 Joint Conference on Empirical
</reference>
<page confidence="0.860577">
1146
</page>
<reference confidence="0.9997023">
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), 1015–1023.
Y. S. Chan, H. T. Ng, Z. Zhong. 2007. NUS-PT: ex-
ploiting parallel texts for word sense disambiguation
in the English all-words tasks. In SemEval ’07: Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, 253–256.
S. Geman, D. Geman. 1987. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration
of images. In Readings in computer vision: is-
sues, problems, principles, and paradigms, 564–
584. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
T. L. Griffiths, M. Steyvers. 2004. Finding scientific
topics. Proceedings of the National Academy of Sci-
ences, 101(Suppl. 1):5228–5235.
T. L. Griffiths, M. Steyvers, D. M. Blei, J. B. Tenen-
baum. 2005. Integrating topics and syntax. In In
Advances in Neural Information Processing Systems
17, 537–544. MIT Press.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In SIGIR ’99: Proceedings of the 22nd an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
50–57.
R. Ion, D. Tufis¸. 2007. Racai: meaning affinity mod-
els. In SemEval ’07: Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, 282–
287, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
G. Katz, E. Giesbrecht. 2006. Automatic identifi-
cation of non-compositional multi-word expressions
using latent semantic analysis. In Proceedings of the
ACL/COLING-06 Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties.
B. B. Klebanov, E. Beigman, D. Diermeier. 2009. Dis-
course topics and metaphors. In CALC ’09: Pro-
ceedings of the Workshop on Computational Ap-
proaches to Linguistic Creativity, 1–8, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
L. Li, C. Sporleder. 2009. Contextual idiom detection
without labelled data. In Proceedings of EMNLP-
09.
D. McCarthy, R. Koeling, J. Weeds, J. Carroll. 2004.
Finding predominant word senses in untagged text.
In Proceedings of the 42nd Meeting of the Associa-
tion for Computational Linguistics (ACL’04), Main
Volume, 279–286.
D. McCarthy. 2009. Word sense disambiguation:
An overview. Language and Linguistics Compass,
3(2):537–558.
G. A. Miller. 1995. WordNet: a lexical database for
english. Commun. ACM, 38(11):39–41.
R. Navigli, K. C. Litkowski, O. Hargraves. 2009.
SemEval-2007 Task 07: Coarse-grained English all-
words task. In Proceedings of the 4th International
Workshop on Semantic Evaluation (SemEval-2007).
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In Proceedings of the 44th Annual Meeting
of the Association for Computational Liguistics joint
with the 21st International Conference on Computa-
tional Liguistics (COLING-ACL 2006).
M. Porter. October 2001. Snowball: A lan-
guage for stemming algorithms. http:
//snowball.tartarus.org/texts/
introduction.html.
S. S. Pradhan, E. Loper, D. Dligach, M. Palmer. 2009.
SemEval-2007 Task 07: Coarse-grained English all-
words task. In Proceedings of the 4th International
Workshop on Semantic Evaluation (SemEval-2007).
F. Song, W. B. Croft. 1999. A general language model
for information retrieval (poster abstract). In Re-
search and Development in Information Retrieval,
279–280.
P. Sorg, P. Cimiano. 2008. Cross-lingual information
retrieval with explicit semantic analysis. In In Work-
ing Notes for the CLEF 2008 Workshop.
C. Sporleder, L. Li. 2009. Unsupervised recognition of
literal and non-literal use of idiomatic expressions.
In Proceedings of EACL-09.
Y. Wang, H. Bai, M. Stanton, W.-Y. Chen, E. Y. Chang.
2009. Plda: Parallel latent dirichlet allocation for
large-scale applications. In Proc. of 5th Interna-
tional Conference on Algorithmic Aspects in Infor-
mation and Management. Software available at
http://code.google.com/p/plda.
</reference>
<page confidence="0.994847">
1147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948978">
<title confidence="0.997639">Topic Models for Word Sense Disambiguation and Token-based Idiom Detection</title>
<author confidence="0.996672">Linlin Li</author>
<author confidence="0.996672">Benjamin Roth</author>
<author confidence="0.996672">Caroline Sporleder</author>
<affiliation confidence="0.988118">Saarland University, Postfach 15 11 50</affiliation>
<address confidence="0.99809">66041 Saarbr¨ucken, Germany</address>
<email confidence="0.998237">beroth,</email>
<abstract confidence="0.9985346">This paper presents a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrases given a context. We use a topic model to decompose this conditional probability into two conditional probabilities with latent variables. We propose three different instantiations of the model for solving sense disambiguation problems with different degrees of resource availability. The proposed models are tested on three different tasks: coarse-grained word sense disambiguation, fine-grained word sense disambiguation, and detection of literal vs. nonliteral usages of potentially idiomatic expressions. In all three cases, we outperform state-of-the-art systems either quantitatively or statistically significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Anaya-S´anchez</author>
<author>A Pons-Porrata</author>
<author>R BerlangaLlavori</author>
</authors>
<title>TKB-UO: using sense clustering for WSD.</title>
<date>2007</date>
<booktitle>In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations, 322–</booktitle>
<pages>325</pages>
<marker>Anaya-S´anchez, Pons-Porrata, BerlangaLlavori, 2007</marker>
<rawString>H. Anaya-S´anchez, A. Pons-Porrata, R. BerlangaLlavori. 2007. TKB-UO: using sense clustering for WSD. In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations, 322– 325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bethard</author>
<author>V T Lai</author>
<author>J H Martin</author>
</authors>
<title>Topic model analysis of metaphor frequency for psycholinguistic stimuli.</title>
<date>2009</date>
<booktitle>In CALC ’09: Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, 9–16,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6980" citStr="Bethard et al., 2009" startWordPosition="1078" endWordPosition="1081"> which use hierarchical lexicons to model semantic distance (Budanitsky and Hirst, 2006). In our approach, we circumvent this problem by exploiting paraphrase information for the target senses rather than relying on the structure of WordNet as a whole. Topic models have also been applied to the related task of word sense induction. Brody and Lapata (2009) propose a method that integrates a number of different linguistic features into a single generative model. Topic models have been previously considered for metaphor extraction and estimating the frequency of metaphors (Klebanov et al., 2009; Bethard et al., 2009). However, we have a different focus in this paper, which aims to distinguish literal and nonliteral usages of potential idiomatic expressions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An unsupervised method that</context>
</contexts>
<marker>Bethard, Lai, Martin, 2009</marker>
<rawString>S. Bethard, V. T. Lai, J. H. Martin. 2009. Topic model analysis of metaphor frequency for psycholinguistic stimuli. In CALC ’09: Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, 9–16, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Birke</author>
<author>A Sarkar</author>
</authors>
<title>A clustering approach for the nearly unsupervised recognition of nonliteral language.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL-06.</booktitle>
<contexts>
<context position="7378" citStr="Birke and Sarkar (2006)" startWordPosition="1146" endWordPosition="1149">ber of different linguistic features into a single generative model. Topic models have been previously considered for metaphor extraction and estimating the frequency of metaphors (Klebanov et al., 2009; Bethard et al., 2009). However, we have a different focus in this paper, which aims to distinguish literal and nonliteral usages of potential idiomatic expressions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An unsupervised method that computes cohesive links between the component words of the target expression and its context have been proposed (Sporleder and Li, 2009; Li and Sporleder, 2009). Their system predicts literal usages when strong links can be found. 3 The Sense Disambiguation Model 3.1 Topic Model As pointed out by Hofmann (1999), the starting point of topic models is to decompose the conditional word-document pr</context>
</contexts>
<marker>Birke, Sarkar, 2006</marker>
<rawString>J. Birke, A. Sarkar. 2006. A clustering approach for the nearly unsupervised recognition of nonliteral language. In Proceedings of EACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Reseach,</journal>
<pages>3--993</pages>
<contexts>
<context position="8611" citStr="Blei et al., 2003" startWordPosition="1340" endWordPosition="1343">ibution p(w1d) into two different distributions: the wordtopic distribution p(w1z), and the topic-document distribution p(zld) (see Equation 1). This allows each semantic topic z to be represented as a multinominal distribution of words p(wjz), and each document d to be represented as a multinominal distribution of semantic topics p(zld). The model introduces a conditional independence assumption that document d and word w are independent con1139 ditioned on the hidden variable, topic z. p(w|d) = E p(z|d)p(w|z) (1) z LDA is a Bayesian version of this framework with Dirichlet hyper-parameters (Blei et al., 2003). The inference of the two distributions given an observed corpus can be done through Gibbs Sampling (Geman and Geman, 1987; Griffiths and Steyvers, 2004). For each turn of the sampling, each word in each document is assigned a semantic topic based on the current word-topic distribution and topic-document distribution. The resulting topic assignments are then used to re-estimate a new word-topic distribution and topic-document distribution for the next turn. This process repeats until convergence. To avoid statistical coincidence, the final estimation of the distributions is made by the averag</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. M. Blei, A. Y. Ng, M. I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Reseach, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>D Blei</author>
</authors>
<title>PUTOP: turning predominant senses into a topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>277--281</pages>
<contexts>
<context position="5188" citStr="Boyd-Graber and Blei (2007)" startWordPosition="791" endWordPosition="794">ed systems automatically group word tokens into similar groups using clustering algorithms, and then assign labels to each sense cluster. Knowledge-based approaches exploit information contained in existing resources. They can be combined with supervised machinelearning models to assemble semi-supervised approaches. Recently, a number of systems have been proposed that make use of topic models for sense disambiguation. Cai et al. (2007), for example, use LDA to capture global context. They compute topic models from a large unlabelled corpus and include them as features in a supervised system. Boyd-Graber and Blei (2007) propose an unsupervised approach that integrates McCarthy et al.’s (2004) method for finding predominant word senses into a topic modelling framework. In addition to generating a topic from the document’s topic distribution and sampling a word from that topic, the enhanced model also generates a distributional neighbour for the chosen word and then assigns a sense based on the word, its neighbour and the topic. Boyd-Graber and Blei (2007) test their method on WSD and information retrieval tasks and find that it can lead to modest improvements over state-of-the-art results. In another unsuperv</context>
</contexts>
<marker>Boyd-Graber, Blei, 2007</marker>
<rawString>J. Boyd-Graber, D. Blei. 2007. PUTOP: turning predominant senses into a topic model for word sense disambiguation. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), 277–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>D Blei</author>
</authors>
<title>Syntactic topic models. Computational Linguistics.</title>
<date>2008</date>
<marker>Boyd-Graber, Blei, 2008</marker>
<rawString>J. Boyd-Graber, D. Blei. 2008. Syntactic topic models. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>D Blei</author>
<author>X Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>1024--1033</pages>
<contexts>
<context position="2009" citStr="Boyd-Graber et al., 2007" startWordPosition="294" endWordPosition="297">task, and despite the fact that it has been the focus of much research over the years, stateof-the-art systems are still often not good enough for real-world applications. One major factor that makes WSD difficult is a relative lack of manually annotated corpora, which hampers the performance of supervised systems. To address this problem, there has been a significant amount of work on unsupervised WSD that does not require manually sensedisambiguated training data (see McCarthy (2009) for an overview). Recently, several researchers have experimented with topic models (Brody and Lapata, 2009; Boyd-Graber et al., 2007; BoydGraber and Blei, 2007; Cai et al., 2007) for sense disambiguation and induction. Topic models are generative probabilistic models of text corpora in which each document is modelled as a mixture over (latent) topics, which are in turn represented by a distribution over words. Previous approaches using topic models for sense disambiguation either embed topic features in a supervised model (Cai et al., 2007) or rely heavily on the structure of hierarchical lexicons such as WordNet (Boyd-Graber et al., 2007). In this paper, we propose a novel framework which is fairly resource-poor in that i</context>
<context position="5826" citStr="Boyd-Graber et al. (2007)" startWordPosition="893" endWordPosition="896">nsupervised approach that integrates McCarthy et al.’s (2004) method for finding predominant word senses into a topic modelling framework. In addition to generating a topic from the document’s topic distribution and sampling a word from that topic, the enhanced model also generates a distributional neighbour for the chosen word and then assigns a sense based on the word, its neighbour and the topic. Boyd-Graber and Blei (2007) test their method on WSD and information retrieval tasks and find that it can lead to modest improvements over state-of-the-art results. In another unsupervised system, Boyd-Graber et al. (2007) enhance the basic LDA algorithm by incorporating WordNet senses as an additional latent variable. Instead of generating words directly from a topic, each topic is associated with a random walk through the WordNet hierarchy which generates the observed word. Topics and synsets are then inferred together. While Boyd-Graber et al. (2007) show that this method can lead to improvements in accuracy, they also find that idiosyncracies in the hierarchical structure of WordNet can harm performance. This is a general problem for methods which use hierarchical lexicons to model semantic distance (Budani</context>
<context position="18150" citStr="Boyd-Graber et al. (2007)" startWordPosition="2919" endWordPosition="2922">entory is from WordNet 2.1. Finally, we test our model on the related sense disambiguation task of distinguishing literal and nonliteral usages of potentially ambiguous expressions such as break the ice. For this, we use the dataset from Sporleder and Li (2009) as a test set. This dataset consists of 3964 instances of 17 potential English idioms which were manually annotated as literal or nonliteral. A Wikipedia dump2 is used to estimate the multinomial word-topic distribution. This dataset, which consists of 320,000 articles,3 is significantly larger than SemCor, which is the dataset used by Boyd-Graber et al. (2007). All markup from the Wikipedia dump was stripped off using the same filter as the ESA implementation (Sorg and Cimiano, 2008), and stopwords were filtered out using the Snowball (Porter, October 2001) stopword list. In addition, words with a Wikipedia document frequency of 1 were filtered out. The lemmatized version of the corpus consists of 299,825 lexical units. The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). The inference processes are run on the lemmatized version of the corpus. For the Semeval-2007 Task 17 English all-words, the organizers do not supp</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>J. Boyd-Graber, D. Blei, X. Zhu. 2007. A topic model for word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), 1024–1033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="18601" citStr="Briscoe and Carroll, 2006" startWordPosition="2992" endWordPosition="2995">inomial word-topic distribution. This dataset, which consists of 320,000 articles,3 is significantly larger than SemCor, which is the dataset used by Boyd-Graber et al. (2007). All markup from the Wikipedia dump was stripped off using the same filter as the ESA implementation (Sorg and Cimiano, 2008), and stopwords were filtered out using the Snowball (Porter, October 2001) stopword list. In addition, words with a Wikipedia document frequency of 1 were filtered out. The lemmatized version of the corpus consists of 299,825 lexical units. The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). The inference processes are run on the lemmatized version of the corpus. For the Semeval-2007 Task 17 English all-words, the organizers do not supply the part-of-speech and lemma information of the target instances. In order to avoid the wrong predic2We use the English snapshot of 2009-07-13 3All articles of fewer than 100 words were discarded. tions caused by tagging or lemmatization errors, we manually corrected any bad tags and lemmas for the target instances.4 Sense Paraphrases For word sense disambiguation tasks, the paraphrases of the sense keys are represented by information from Word</context>
</contexts>
<marker>Briscoe, Carroll, 2006</marker>
<rawString>T. Briscoe, J. Carroll. 2006. Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank. In Proceedings of the COLING/ACL on Main conference poster sessions, 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>M Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009),</booktitle>
<pages>103--111</pages>
<contexts>
<context position="1983" citStr="Brody and Lapata, 2009" startWordPosition="290" endWordPosition="293">ver, WSD is a difficult task, and despite the fact that it has been the focus of much research over the years, stateof-the-art systems are still often not good enough for real-world applications. One major factor that makes WSD difficult is a relative lack of manually annotated corpora, which hampers the performance of supervised systems. To address this problem, there has been a significant amount of work on unsupervised WSD that does not require manually sensedisambiguated training data (see McCarthy (2009) for an overview). Recently, several researchers have experimented with topic models (Brody and Lapata, 2009; Boyd-Graber et al., 2007; BoydGraber and Blei, 2007; Cai et al., 2007) for sense disambiguation and induction. Topic models are generative probabilistic models of text corpora in which each document is modelled as a mixture over (latent) topics, which are in turn represented by a distribution over words. Previous approaches using topic models for sense disambiguation either embed topic features in a supervised model (Cai et al., 2007) or rely heavily on the structure of hierarchical lexicons such as WordNet (Boyd-Graber et al., 2007). In this paper, we propose a novel framework which is fair</context>
<context position="6716" citStr="Brody and Lapata (2009)" startWordPosition="1038" endWordPosition="1041">nd synsets are then inferred together. While Boyd-Graber et al. (2007) show that this method can lead to improvements in accuracy, they also find that idiosyncracies in the hierarchical structure of WordNet can harm performance. This is a general problem for methods which use hierarchical lexicons to model semantic distance (Budanitsky and Hirst, 2006). In our approach, we circumvent this problem by exploiting paraphrase information for the target senses rather than relying on the structure of WordNet as a whole. Topic models have also been applied to the related task of word sense induction. Brody and Lapata (2009) propose a method that integrates a number of different linguistic features into a single generative model. Topic models have been previously considered for metaphor extraction and estimating the frequency of metaphors (Klebanov et al., 2009; Bethard et al., 2009). However, we have a different focus in this paper, which aims to distinguish literal and nonliteral usages of potential idiomatic expressions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a g</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>S. Brody, M. Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), 103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating wordnetbased measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="6447" citStr="Budanitsky and Hirst, 2006" startWordPosition="992" endWordPosition="995">(2007) enhance the basic LDA algorithm by incorporating WordNet senses as an additional latent variable. Instead of generating words directly from a topic, each topic is associated with a random walk through the WordNet hierarchy which generates the observed word. Topics and synsets are then inferred together. While Boyd-Graber et al. (2007) show that this method can lead to improvements in accuracy, they also find that idiosyncracies in the hierarchical structure of WordNet can harm performance. This is a general problem for methods which use hierarchical lexicons to model semantic distance (Budanitsky and Hirst, 2006). In our approach, we circumvent this problem by exploiting paraphrase information for the target senses rather than relying on the structure of WordNet as a whole. Topic models have also been applied to the related task of word sense induction. Brody and Lapata (2009) propose a method that integrates a number of different linguistic features into a single generative model. Topic models have been previously considered for metaphor extraction and estimating the frequency of metaphors (Klebanov et al., 2009; Bethard et al., 2009). However, we have a different focus in this paper, which aims to d</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky, G. Hirst. 2006. Evaluating wordnetbased measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Buscaldi</author>
<author>P Rosso</author>
</authors>
<title>UPV-WSD: Combining different WSD methods by means of Fuzzy Borda Voting.</title>
<date>2007</date>
<booktitle>In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>434--437</pages>
<contexts>
<context position="25361" citStr="Buscaldi and Rosso, 2007" startWordPosition="4076" endWordPosition="4079">ccurrences between different word senses, which was obtained from a number of resources (SSI+LKB) including: (i) SemCor (manually annotated); (ii) LDCDSO (partly manually annotated); (iii) collocation dictionaries which are then disambiguated semiautomatically. Even though the system is not “trained”, it needs a lot of information which is largely dependent on manually annotated data, so it does not fit neatly into the categories Type II or Type III either. Table 2 lists the best participating systems of each type in the SemEval-2007 task (Type I: NUS-PT (Chan et al., 2007); Type II: UPV-WSD (Buscaldi and Rosso, 2007); Type III: TKB-UO (Anaya-S´anchez et al., 2007)). Our Model I belongs to Type II, and our Model II belongs to Type III. Table 2 compares the performance of our models with the Semeval-2007 participating systems. We only compare the F-score, since all the compared systems have an attempted rate7 of 1.0, 7Attempted rate is defined as the total number of disambiguated output instances divided by the total number of input 1143 which makes both the precision and recall rates the same as the F-score. We focus on comparisons between our models and the best SemEval-2007 participating systems within t</context>
</contexts>
<marker>Buscaldi, Rosso, 2007</marker>
<rawString>D. Buscaldi, P. Rosso. 2007. UPV-WSD: Combining different WSD methods by means of Fuzzy Borda Voting. In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations, 434–437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cai</author>
<author>W S Lee</author>
<author>Y W Teh</author>
</authors>
<title>Improving word sense disambiguation using topic features.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>1015--1023</pages>
<contexts>
<context position="2055" citStr="Cai et al., 2007" startWordPosition="303" endWordPosition="306"> of much research over the years, stateof-the-art systems are still often not good enough for real-world applications. One major factor that makes WSD difficult is a relative lack of manually annotated corpora, which hampers the performance of supervised systems. To address this problem, there has been a significant amount of work on unsupervised WSD that does not require manually sensedisambiguated training data (see McCarthy (2009) for an overview). Recently, several researchers have experimented with topic models (Brody and Lapata, 2009; Boyd-Graber et al., 2007; BoydGraber and Blei, 2007; Cai et al., 2007) for sense disambiguation and induction. Topic models are generative probabilistic models of text corpora in which each document is modelled as a mixture over (latent) topics, which are in turn represented by a distribution over words. Previous approaches using topic models for sense disambiguation either embed topic features in a supervised model (Cai et al., 2007) or rely heavily on the structure of hierarchical lexicons such as WordNet (Boyd-Graber et al., 2007). In this paper, we propose a novel framework which is fairly resource-poor in that it requires only 1) a large unlabelled corpus f</context>
<context position="5001" citStr="Cai et al. (2007)" startWordPosition="759" endWordPosition="762">s (see McCarthy (2009) for an overview). While most supervised approaches treat the task as a classification task and use hand-labelled corpora as training data, most unsupervised systems automatically group word tokens into similar groups using clustering algorithms, and then assign labels to each sense cluster. Knowledge-based approaches exploit information contained in existing resources. They can be combined with supervised machinelearning models to assemble semi-supervised approaches. Recently, a number of systems have been proposed that make use of topic models for sense disambiguation. Cai et al. (2007), for example, use LDA to capture global context. They compute topic models from a large unlabelled corpus and include them as features in a supervised system. Boyd-Graber and Blei (2007) propose an unsupervised approach that integrates McCarthy et al.’s (2004) method for finding predominant word senses into a topic modelling framework. In addition to generating a topic from the document’s topic distribution and sampling a word from that topic, the enhanced model also generates a distributional neighbour for the chosen word and then assigns a sense based on the word, its neighbour and the topi</context>
<context position="27154" citStr="Cai et al., 2007" startWordPosition="4383" endWordPosition="4386">y 1.36% the best Type II system UPV-WSD, although the difference is not statistically significant. Furthermore, Model I also quantitatively outperforms the most frequent sense baseline BL,,,f,, which, as mentioned above, was not beat by any participating systems that do not use training data. We also find that our model works best for nouns. The unsupervised Type III model Model II achieves better results than the most frequent sense baseline on nouns, but not on other partsof-speech. This is in line with results obtained by previous systems (Griffiths et al., 2005; BoydGraber and Blei, 2008; Cai et al., 2007). While the performance on verbs can be increased to outperform the most frequent sense baseline by including the prior sense probability, the performance on adjectives and adverbs remains below the most frequent sense baseline. We think that there are three reasons for this: first, adjectives and adverbs have fewer reference synsets for paraphrases compared with nouns and verbs (see Table 1); second, adjectives and adverbs tend to convey less key semantic content in the document, so they are more difficult to capture by the topic model; and third, adjectives and adverbs are a small portion of</context>
</contexts>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>J. Cai, W. S. Lee, Y. W. Teh. 2007. Improving word sense disambiguation using topic features. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), 1015–1023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
<author>Z Zhong</author>
</authors>
<title>NUS-PT: exploiting parallel texts for word sense disambiguation in the English all-words tasks.</title>
<date>2007</date>
<booktitle>In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>253--256</pages>
<contexts>
<context position="25316" citStr="Chan et al., 2007" startWordPosition="4069" endWordPosition="4072">e, specifically information about co-occurrences between different word senses, which was obtained from a number of resources (SSI+LKB) including: (i) SemCor (manually annotated); (ii) LDCDSO (partly manually annotated); (iii) collocation dictionaries which are then disambiguated semiautomatically. Even though the system is not “trained”, it needs a lot of information which is largely dependent on manually annotated data, so it does not fit neatly into the categories Type II or Type III either. Table 2 lists the best participating systems of each type in the SemEval-2007 task (Type I: NUS-PT (Chan et al., 2007); Type II: UPV-WSD (Buscaldi and Rosso, 2007); Type III: TKB-UO (Anaya-S´anchez et al., 2007)). Our Model I belongs to Type II, and our Model II belongs to Type III. Table 2 compares the performance of our models with the Semeval-2007 participating systems. We only compare the F-score, since all the compared systems have an attempted rate7 of 1.0, 7Attempted rate is defined as the total number of disambiguated output instances divided by the total number of input 1143 which makes both the precision and recall rates the same as the F-score. We focus on comparisons between our models and the bes</context>
</contexts>
<marker>Chan, Ng, Zhong, 2007</marker>
<rawString>Y. S. Chan, H. T. Ng, Z. Zhong. 2007. NUS-PT: exploiting parallel texts for word sense disambiguation in the English all-words tasks. In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations, 253–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images.</title>
<date>1987</date>
<booktitle>In Readings in computer vision: issues, problems, principles, and paradigms, 564– 584.</booktitle>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="8734" citStr="Geman and Geman, 1987" startWordPosition="1361" endWordPosition="1364">p(zld) (see Equation 1). This allows each semantic topic z to be represented as a multinominal distribution of words p(wjz), and each document d to be represented as a multinominal distribution of semantic topics p(zld). The model introduces a conditional independence assumption that document d and word w are independent con1139 ditioned on the hidden variable, topic z. p(w|d) = E p(z|d)p(w|z) (1) z LDA is a Bayesian version of this framework with Dirichlet hyper-parameters (Blei et al., 2003). The inference of the two distributions given an observed corpus can be done through Gibbs Sampling (Geman and Geman, 1987; Griffiths and Steyvers, 2004). For each turn of the sampling, each word in each document is assigned a semantic topic based on the current word-topic distribution and topic-document distribution. The resulting topic assignments are then used to re-estimate a new word-topic distribution and topic-document distribution for the next turn. This process repeats until convergence. To avoid statistical coincidence, the final estimation of the distributions is made by the average of all the turns after convergence. 3.2 The Sense Disambiguation Model Assigning the correct sense s to a target word w o</context>
</contexts>
<marker>Geman, Geman, 1987</marker>
<rawString>S. Geman, D. Geman. 1987. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. In Readings in computer vision: issues, problems, principles, and paradigms, 564– 584. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="8765" citStr="Griffiths and Steyvers, 2004" startWordPosition="1365" endWordPosition="1368">. This allows each semantic topic z to be represented as a multinominal distribution of words p(wjz), and each document d to be represented as a multinominal distribution of semantic topics p(zld). The model introduces a conditional independence assumption that document d and word w are independent con1139 ditioned on the hidden variable, topic z. p(w|d) = E p(z|d)p(w|z) (1) z LDA is a Bayesian version of this framework with Dirichlet hyper-parameters (Blei et al., 2003). The inference of the two distributions given an observed corpus can be done through Gibbs Sampling (Geman and Geman, 1987; Griffiths and Steyvers, 2004). For each turn of the sampling, each word in each document is assigned a semantic topic based on the current word-topic distribution and topic-document distribution. The resulting topic assignments are then used to re-estimate a new word-topic distribution and topic-document distribution for the next turn. This process repeats until convergence. To avoid statistical coincidence, the final estimation of the distributions is made by the average of all the turns after convergence. 3.2 The Sense Disambiguation Model Assigning the correct sense s to a target word w occurring in a context c involve</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths, M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>D M Blei</author>
<author>J B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In In Advances in Neural Information Processing Systems 17,</booktitle>
<pages>537--544</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="27108" citStr="Griffiths et al., 2005" startWordPosition="4374" endWordPosition="4377">wledge of sense distribution, Model I outperforms by 1.36% the best Type II system UPV-WSD, although the difference is not statistically significant. Furthermore, Model I also quantitatively outperforms the most frequent sense baseline BL,,,f,, which, as mentioned above, was not beat by any participating systems that do not use training data. We also find that our model works best for nouns. The unsupervised Type III model Model II achieves better results than the most frequent sense baseline on nouns, but not on other partsof-speech. This is in line with results obtained by previous systems (Griffiths et al., 2005; BoydGraber and Blei, 2008; Cai et al., 2007). While the performance on verbs can be increased to outperform the most frequent sense baseline by including the prior sense probability, the performance on adjectives and adverbs remains below the most frequent sense baseline. We think that there are three reasons for this: first, adjectives and adverbs have fewer reference synsets for paraphrases compared with nouns and verbs (see Table 1); second, adjectives and adverbs tend to convey less key semantic content in the document, so they are more difficult to capture by the topic model; and third,</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>T. L. Griffiths, M. Steyvers, D. M. Blei, J. B. Tenenbaum. 2005. Integrating topics and syntax. In In Advances in Neural Information Processing Systems 17, 537–544. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="7893" citStr="Hofmann (1999)" startWordPosition="1232" endWordPosition="1233"> literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An unsupervised method that computes cohesive links between the component words of the target expression and its context have been proposed (Sporleder and Li, 2009; Li and Sporleder, 2009). Their system predicts literal usages when strong links can be found. 3 The Sense Disambiguation Model 3.1 Topic Model As pointed out by Hofmann (1999), the starting point of topic models is to decompose the conditional word-document probability distribution p(w1d) into two different distributions: the wordtopic distribution p(w1z), and the topic-document distribution p(zld) (see Equation 1). This allows each semantic topic z to be represented as a multinominal distribution of words p(wjz), and each document d to be represented as a multinominal distribution of semantic topics p(zld). The model introduces a conditional independence assumption that document d and word w are independent con1139 ditioned on the hidden variable, topic z. p(w|d) </context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>T. Hofmann. 1999. Probabilistic latent semantic indexing. In SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ion</author>
<author>D Tufis¸</author>
</authors>
<title>Racai: meaning affinity models.</title>
<date>2007</date>
<booktitle>In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations, 282– 287,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Ion, Tufis¸, 2007</marker>
<rawString>R. Ion, D. Tufis¸. 2007. Racai: meaning affinity models. In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations, 282– 287, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Katz</author>
<author>E Giesbrecht</author>
</authors>
<title>Automatic identification of non-compositional multi-word expressions using latent semantic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/COLING-06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties.</booktitle>
<contexts>
<context position="7202" citStr="Katz and Giesbrecht (2006)" startWordPosition="1115" endWordPosition="1118">he structure of WordNet as a whole. Topic models have also been applied to the related task of word sense induction. Brody and Lapata (2009) propose a method that integrates a number of different linguistic features into a single generative model. Topic models have been previously considered for metaphor extraction and estimating the frequency of metaphors (Klebanov et al., 2009; Bethard et al., 2009). However, we have a different focus in this paper, which aims to distinguish literal and nonliteral usages of potential idiomatic expressions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An unsupervised method that computes cohesive links between the component words of the target expression and its context have been proposed (Sporleder and Li, 2009; Li and Sporleder, 2009). Their system predicts literal usages when strong links can </context>
</contexts>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>G. Katz, E. Giesbrecht. 2006. Automatic identification of non-compositional multi-word expressions using latent semantic analysis. In Proceedings of the ACL/COLING-06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B B Klebanov</author>
<author>E Beigman</author>
<author>D Diermeier</author>
</authors>
<title>Discourse topics and metaphors.</title>
<date>2009</date>
<booktitle>In CALC ’09: Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, 1–8,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6957" citStr="Klebanov et al., 2009" startWordPosition="1074" endWordPosition="1077">ral problem for methods which use hierarchical lexicons to model semantic distance (Budanitsky and Hirst, 2006). In our approach, we circumvent this problem by exploiting paraphrase information for the target senses rather than relying on the structure of WordNet as a whole. Topic models have also been applied to the related task of word sense induction. Brody and Lapata (2009) propose a method that integrates a number of different linguistic features into a single generative model. Topic models have been previously considered for metaphor extraction and estimating the frequency of metaphors (Klebanov et al., 2009; Bethard et al., 2009). However, we have a different focus in this paper, which aims to distinguish literal and nonliteral usages of potential idiomatic expressions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An u</context>
</contexts>
<marker>Klebanov, Beigman, Diermeier, 2009</marker>
<rawString>B. B. Klebanov, E. Beigman, D. Diermeier. 2009. Discourse topics and metaphors. In CALC ’09: Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, 1–8, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Li</author>
<author>C Sporleder</author>
</authors>
<title>Contextual idiom detection without labelled data.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP09.</booktitle>
<contexts>
<context position="7741" citStr="Li and Sporleder, 2009" startWordPosition="1205" endWordPosition="1208">ions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An unsupervised method that computes cohesive links between the component words of the target expression and its context have been proposed (Sporleder and Li, 2009; Li and Sporleder, 2009). Their system predicts literal usages when strong links can be found. 3 The Sense Disambiguation Model 3.1 Topic Model As pointed out by Hofmann (1999), the starting point of topic models is to decompose the conditional word-document probability distribution p(w1d) into two different distributions: the wordtopic distribution p(w1z), and the topic-document distribution p(zld) (see Equation 1). This allows each semantic topic z to be represented as a multinominal distribution of words p(wjz), and each document d to be represented as a multinominal distribution of semantic topics p(zld). The mod</context>
<context position="32603" citStr="Li and Sporleder (2009)" startWordPosition="5252" endWordPosition="5255"> compared with state-of-the-art systems. We find that the system significantly outperforms the majority baseline (p&lt;&lt;0.01, x2 test) and the cohesion-graph based approach proposed by Sporleder and Li (2009) (p&lt;&lt;0.01, x2 test). The system also outperforms the bootstrapping System Precl Recl Fl Acc. Basemaj - - - 78.25 co-graph 50.04 69.72 58.26 78.38 boot. 71.86 66.36 69.00 87.03 Model III 67.05 81.07 73.40 87.24 Table 5: Performance on the literal or nonliteral sense disambiguation task on idioms. literal precision (Precl), literal recall (Recl), literal F-score (Fl), accuracy(Acc.). system by Li and Sporleder (2009), although not statistically significantly. This shows how a limited amount of human knowledge (e.g., paraphrases) can be added to an unsupervised system for a strong boost in performance ( Model III compared with the cohesion-graph and the bootstrapping approaches). For obvious reasons, this approach is sensitive to the quality of the paraphrases. The paraphrases chosen to characterise (aspects of) the meaning of a sense should be non-ambiguous between the literal or idiomatic meaning. For instance, ‘fire’ is not a good choice for a paraphrase of the literal reading of ‘play with fire’, since</context>
</contexts>
<marker>Li, Sporleder, 2009</marker>
<rawString>L. Li, C. Sporleder. 2009. Contextual idiom detection without labelled data. In Proceedings of EMNLP09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Koeling</author>
<author>J Weeds</author>
<author>J Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>279--286</pages>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>D. McCarthy, R. Koeling, J. Weeds, J. Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, 279–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
</authors>
<title>Word sense disambiguation: An overview.</title>
<date>2009</date>
<journal>Language and Linguistics Compass,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="1875" citStr="McCarthy (2009)" startWordPosition="277" endWordPosition="278">many applications, including machine translation, question answering and information extraction. However, WSD is a difficult task, and despite the fact that it has been the focus of much research over the years, stateof-the-art systems are still often not good enough for real-world applications. One major factor that makes WSD difficult is a relative lack of manually annotated corpora, which hampers the performance of supervised systems. To address this problem, there has been a significant amount of work on unsupervised WSD that does not require manually sensedisambiguated training data (see McCarthy (2009) for an overview). Recently, several researchers have experimented with topic models (Brody and Lapata, 2009; Boyd-Graber et al., 2007; BoydGraber and Blei, 2007; Cai et al., 2007) for sense disambiguation and induction. Topic models are generative probabilistic models of text corpora in which each document is modelled as a mixture over (latent) topics, which are in turn represented by a distribution over words. Previous approaches using topic models for sense disambiguation either embed topic features in a supervised model (Cai et al., 2007) or rely heavily on the structure of hierarchical le</context>
<context position="4406" citStr="McCarthy (2009)" startWordPosition="669" endWordPosition="671"> the sense 1138 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1138–1147, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics paraphrase. Specifically, it chooses the sense type which maximizes the probability (given the context) of the paraphrase component word with the highest likelihood of occurring in that context. This model also outperforms state-of-the-art systems. 2 Related Work There is a large body of work on WSD, covering supervised, unsupervised (word sense induction) and knowledge-based approaches (see McCarthy (2009) for an overview). While most supervised approaches treat the task as a classification task and use hand-labelled corpora as training data, most unsupervised systems automatically group word tokens into similar groups using clustering algorithms, and then assign labels to each sense cluster. Knowledge-based approaches exploit information contained in existing resources. They can be combined with supervised machinelearning models to assemble semi-supervised approaches. Recently, a number of systems have been proposed that make use of topic models for sense disambiguation. Cai et al. (2007), for</context>
<context position="12679" citStr="McCarthy, 2009" startWordPosition="2016" endWordPosition="2017">g max E dsi p(dsi) z 1140 p(ds), which is not always available. We use sense frequency information from WordNet to estimate the prior sense distribution, although it must be kept in mind that, depending on the genre of the texts, it is possible that the distribution of senses in the testing corpus may diverge greatly from the WordNet-based estimation. If there is no means for estimating the prior sense distribution of an experimental corpus, generally a uniform distribution must be assumed. However, this assumption does not hold, as the true distribution of word senses is often highly skewed (McCarthy, 2009). To overcome this problem, we propose Model II, which indirectly maximizes the sense-context probability by maximizing the cosine value of two document vectors that encode the document-topic frequencies from sampling, v(z|dc) and v(z|ds). The document vectors are represented by topics, with each dimension representing the number of times that the tokens in this document are assigned to a certain topic. Model II: arg max cos(v(z|dc), v(z|dsz)) (9) dsi If the prior distribution of senses is known, Model I is the best choice. However, Model II has to be chosen instead when this knowledge is not </context>
<context position="24225" citStr="McCarthy (2009)" startWordPosition="3899" endWordPosition="3900">on knowledge are categorized as Type III. We make this distinction based on two principles: (i) the cost of building a system; (ii) the portability of the established resource. Type III is the cheapest system to build, while Type I and 6They were set as: α = 50 #c����� and ,Q = 0.01. Type II both need extra resources. Type II has an advantage over Type I since the prior knowledge of the sense distribution can be estimated from annotated corpora (e.g.: SemCor, Senseval). In contrast, training data in Type I may be system specific (e.g.: different input format, different annotation guidelines). McCarthy (2009) also addresses the issue of performance and cost by comparing supervised word sense disambiguation systems with unsupervised ones. We exclude the system provided by one of the organizers (UoR-SSI) from our categorization. The reason is that although this system is claimed to be unsupervised, and it performs better than all the participating systems (including the supervised systems) in the SemEval-2007 shared task, it still needs to incorporate a lot of prior knowledge, specifically information about co-occurrences between different word senses, which was obtained from a number of resources (</context>
</contexts>
<marker>McCarthy, 2009</marker>
<rawString>D. McCarthy. 2009. Word sense disambiguation: An overview. Language and Linguistics Compass, 3(2):537–558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: a lexical database for english.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="9699" citStr="Miller, 1995" startWordPosition="1523" endWordPosition="1524"> repeats until convergence. To avoid statistical coincidence, the final estimation of the distributions is made by the average of all the turns after convergence. 3.2 The Sense Disambiguation Model Assigning the correct sense s to a target word w occurring in a context c involves finding the sense which maximizes the conditional probability of senses given a context: s = arg max p(si|c) (2) si In our model, we represent a sense (si) as a collection of ‘paraphrases’ that capture (some aspect of) the meaning of the sense. These paraphrases can be taken from an existing resource such as WordNet (Miller, 1995) or supplied by the user (see Section 4). This conditional probability is decomposed by incorporating a hidden variable, topic z, introduced by the topic model. We propose three variations of the basic model, depending on how much background information is available, i.e., knowledge of the prior sense distribution available and type of sense paraphrases used. In Model I and Model II, the sense paraphrases are obtained from WordNet, and both the context and the sense paraphrases are treated as documents, c = dc and s = ds. WordNet is a fairly rich resource which provides detailed information ab</context>
<context position="19224" citStr="Miller, 1995" startWordPosition="3095" endWordPosition="3096">rence processes are run on the lemmatized version of the corpus. For the Semeval-2007 Task 17 English all-words, the organizers do not supply the part-of-speech and lemma information of the target instances. In order to avoid the wrong predic2We use the English snapshot of 2009-07-13 3All articles of fewer than 100 words were discarded. tions caused by tagging or lemmatization errors, we manually corrected any bad tags and lemmas for the target instances.4 Sense Paraphrases For word sense disambiguation tasks, the paraphrases of the sense keys are represented by information from WordNet 2.1. (Miller, 1995). To obtain the paraphrases, we use the word forms, glosses and example sentences of the synset itself and a set of selected reference synsets (i.e., synsets linked to the target synset by specific semantic relations, see Table 1). We excluded the ‘hypernym reference synsets’, since information common to all of the child synsets may confuse the disambiguation process. For the literal vs. nonliteral sense detection task, we selected the paraphrases of the nonliteral meaning from several online idiom dictionaries. For the literal senses, we used 2-3 manually selected words with which we tried to</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. A. Miller. 1995. WordNet: a lexical database for english. Commun. ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>K C Litkowski</author>
<author>O Hargraves</author>
</authors>
<title>SemEval-2007 Task 07: Coarse-grained English allwords task.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluation (SemEval-2007).</booktitle>
<contexts>
<context position="16320" citStr="Navigli et al. (2009)" startWordPosition="2611" endWordPosition="2615"> and sense paraphrases is done by fixing the word-topic distribution as a constant. 4 Experimental Setup We evaluate our models on three different tasks: coarse-grained WSD, fine-grained WSD and literal vs. nonliteral sense detection. In this section we discuss our experimental set-up. We start by describing the three datasets for evaluation and another dataset for probability estimation. We also discuss how we choose sense paraphrases and instance contexts. Data We use three datasets for evaluation. The coarse-grained task is evaluated on the Semeval2007 Task-07 benchmark dataset released by Navigli et al. (2009). The dataset consists of 5377 words of running text from five different articles: the first three were obtained from the WSJ corpus, the fourth was the Wikipedia entry for computer programming, and the fifth was an excerpt of arg max �max qsi wiEqs z 1141 Amy Steedman’s Knights of the Art, biographies of Italian painters. The proportion of the non news text, the last two articles, constitutes 51.87% of the whole testing set. It consists of 1108 nouns, 591 verbs, 362 adjectives, and 208 adverbs. The data were annotated with coarse-grained senses which were obtained by clustering senses from th</context>
<context position="23268" citStr="Navigli et al. (2009)" startWordPosition="3736" endWordPosition="3739">ingle AMD Dual-Core 1000MHZ processor. 5.1 Coarse-Grained WSD In this section we first describe the landscape of similar systems against which we compare our models, then present the results of the comparison. The systems that participated in the SemEval-2007 coarse-grained WSD task (Task-07) can be divided into three categories, depending on whether training data is needed and whether other types of background knowledge are required: What we call Type I includes all the systems that need annotated training data. All the participating systems that have the mark TR fall into this category (see Navigli et al. (2009) for the evaluation for all the participating systems). Type II consists of systems that do not need training data but require prior knowledge of the sense distribution (estimated sense frequency). All the participating systems that have the mark MFS belong to this category. Systems that need neither training data nor prior sense distribution knowledge are categorized as Type III. We make this distinction based on two principles: (i) the cost of building a system; (ii) the portability of the established resource. Type III is the cheapest system to build, while Type I and 6They were set as: α =</context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2009</marker>
<rawString>R. Navigli, K. C. Litkowski, O. Hargraves. 2009. SemEval-2007 Task 07: Coarse-grained English allwords task. In Proceedings of the 4th International Workshop on Semantic Evaluation (SemEval-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
</authors>
<title>Meaningful clustering of senses helps boost word sense disambiguation performance.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Liguistics joint with the 21st International Conference on Computational Liguistics (COLING-ACL</booktitle>
<contexts>
<context position="16999" citStr="Navigli (2006)" startWordPosition="2729" endWordPosition="2730">fferent articles: the first three were obtained from the WSJ corpus, the fourth was the Wikipedia entry for computer programming, and the fifth was an excerpt of arg max �max qsi wiEqs z 1141 Amy Steedman’s Knights of the Art, biographies of Italian painters. The proportion of the non news text, the last two articles, constitutes 51.87% of the whole testing set. It consists of 1108 nouns, 591 verbs, 362 adjectives, and 208 adverbs. The data were annotated with coarse-grained senses which were obtained by clustering senses from the WordNet 2.1 sense inventory based on the procedure proposed by Navigli (2006). To determine whether our model is also suitable for fine-grained WSD, we test on the data provided by Pradhan et al. (2009) for the Semeval-2007 Task-17 (English fine-grained all-words task). This dataset is a subset of the set from Task-07. It comprises the three WSJ articles from Navigli et al. (2009). A total of 465 lemmas were selected as instances from about 3500 words of text. There are 10 instances marked as ‘U’ (undecided sense tag). Of the remaining 455 instances, 159 are nouns and 296 are verbs. The sense inventory is from WordNet 2.1. Finally, we test our model on the related sens</context>
</contexts>
<marker>Navigli, 2006</marker>
<rawString>R. Navigli. 2006. Meaningful clustering of senses helps boost word sense disambiguation performance. In Proceedings of the 44th Annual Meeting of the Association for Computational Liguistics joint with the 21st International Conference on Computational Liguistics (COLING-ACL 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>Snowball: A language for stemming algorithms.</title>
<date>2001</date>
<note>http: //snowball.tartarus.org/texts/ introduction.html.</note>
<marker>Porter, 2001</marker>
<rawString>M. Porter. October 2001. Snowball: A language for stemming algorithms. http: //snowball.tartarus.org/texts/ introduction.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Pradhan</author>
<author>E Loper</author>
<author>D Dligach</author>
<author>M Palmer</author>
</authors>
<title>SemEval-2007 Task 07: Coarse-grained English allwords task.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluation (SemEval-2007).</booktitle>
<contexts>
<context position="17124" citStr="Pradhan et al. (2009)" startWordPosition="2749" endWordPosition="2752">rogramming, and the fifth was an excerpt of arg max �max qsi wiEqs z 1141 Amy Steedman’s Knights of the Art, biographies of Italian painters. The proportion of the non news text, the last two articles, constitutes 51.87% of the whole testing set. It consists of 1108 nouns, 591 verbs, 362 adjectives, and 208 adverbs. The data were annotated with coarse-grained senses which were obtained by clustering senses from the WordNet 2.1 sense inventory based on the procedure proposed by Navigli (2006). To determine whether our model is also suitable for fine-grained WSD, we test on the data provided by Pradhan et al. (2009) for the Semeval-2007 Task-17 (English fine-grained all-words task). This dataset is a subset of the set from Task-07. It comprises the three WSJ articles from Navigli et al. (2009). A total of 465 lemmas were selected as instances from about 3500 words of text. There are 10 instances marked as ‘U’ (undecided sense tag). Of the remaining 455 instances, 159 are nouns and 296 are verbs. The sense inventory is from WordNet 2.1. Finally, we test our model on the related sense disambiguation task of distinguishing literal and nonliteral usages of potentially ambiguous expressions such as break the </context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2009</marker>
<rawString>S. S. Pradhan, E. Loper, D. Dligach, M. Palmer. 2009. SemEval-2007 Task 07: Coarse-grained English allwords task. In Proceedings of the 4th International Workshop on Semantic Evaluation (SemEval-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Song</author>
<author>W B Croft</author>
</authors>
<title>A general language model for information retrieval (poster abstract).</title>
<date>1999</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>279--280</pages>
<contexts>
<context position="13896" citStr="Song and Croft, 1999" startWordPosition="2214" endWordPosition="2217">not available. In our experiments, we test the performance of both models (see Section 5). If the sense paraphrases are very short, it is difficult to reliably estimate p(z|ds). In order to solve this problem, we treat the sense paraphrase ds as a ‘query’, a concept which is used in information retrieval. One model from information retrieval takes the conditional probability of the query given the document as a product of all the conditional probabilities of words in the query given the document. The assumption is that the query is generated by a collection of conditionally independent words (Song and Croft, 1999). We make the same assumption here. However, instead of taking the product of all the conditional probabilities of words given the document, we take the maximum. There are two reasons for this: (i) taking the product may penalize longer paraphrases since the product of probabilities decreases as there are more words; (ii) we do not want to model the probability of generating specific paraphrases, but rather the probability of generating a sense, which might only be represented by one or two words in the paraphrases (e.g., the potentially idiomatic phrase ‘rock the boat’ can be paraphrased as ‘</context>
</contexts>
<marker>Song, Croft, 1999</marker>
<rawString>F. Song, W. B. Croft. 1999. A general language model for information retrieval (poster abstract). In Research and Development in Information Retrieval, 279–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sorg</author>
<author>P Cimiano</author>
</authors>
<title>Cross-lingual information retrieval with explicit semantic analysis.</title>
<date>2008</date>
<booktitle>In In Working Notes for the CLEF</booktitle>
<note>Workshop.</note>
<contexts>
<context position="18276" citStr="Sorg and Cimiano, 2008" startWordPosition="2940" endWordPosition="2944">nliteral usages of potentially ambiguous expressions such as break the ice. For this, we use the dataset from Sporleder and Li (2009) as a test set. This dataset consists of 3964 instances of 17 potential English idioms which were manually annotated as literal or nonliteral. A Wikipedia dump2 is used to estimate the multinomial word-topic distribution. This dataset, which consists of 320,000 articles,3 is significantly larger than SemCor, which is the dataset used by Boyd-Graber et al. (2007). All markup from the Wikipedia dump was stripped off using the same filter as the ESA implementation (Sorg and Cimiano, 2008), and stopwords were filtered out using the Snowball (Porter, October 2001) stopword list. In addition, words with a Wikipedia document frequency of 1 were filtered out. The lemmatized version of the corpus consists of 299,825 lexical units. The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). The inference processes are run on the lemmatized version of the corpus. For the Semeval-2007 Task 17 English all-words, the organizers do not supply the part-of-speech and lemma information of the target instances. In order to avoid the wrong predic2We use the English sna</context>
</contexts>
<marker>Sorg, Cimiano, 2008</marker>
<rawString>P. Sorg, P. Cimiano. 2008. Cross-lingual information retrieval with explicit semantic analysis. In In Working Notes for the CLEF 2008 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>L Li</author>
</authors>
<title>Unsupervised recognition of literal and non-literal use of idiomatic expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL-09.</booktitle>
<contexts>
<context position="7716" citStr="Sporleder and Li, 2009" startWordPosition="1201" endWordPosition="1204">ential idiomatic expressions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An unsupervised method that computes cohesive links between the component words of the target expression and its context have been proposed (Sporleder and Li, 2009; Li and Sporleder, 2009). Their system predicts literal usages when strong links can be found. 3 The Sense Disambiguation Model 3.1 Topic Model As pointed out by Hofmann (1999), the starting point of topic models is to decompose the conditional word-document probability distribution p(w1d) into two different distributions: the wordtopic distribution p(w1z), and the topic-document distribution p(zld) (see Equation 1). This allows each semantic topic z to be represented as a multinominal distribution of words p(wjz), and each document d to be represented as a multinominal distribution of semant</context>
<context position="17786" citStr="Sporleder and Li (2009)" startWordPosition="2861" endWordPosition="2864">fine-grained all-words task). This dataset is a subset of the set from Task-07. It comprises the three WSJ articles from Navigli et al. (2009). A total of 465 lemmas were selected as instances from about 3500 words of text. There are 10 instances marked as ‘U’ (undecided sense tag). Of the remaining 455 instances, 159 are nouns and 296 are verbs. The sense inventory is from WordNet 2.1. Finally, we test our model on the related sense disambiguation task of distinguishing literal and nonliteral usages of potentially ambiguous expressions such as break the ice. For this, we use the dataset from Sporleder and Li (2009) as a test set. This dataset consists of 3964 instances of 17 potential English idioms which were manually annotated as literal or nonliteral. A Wikipedia dump2 is used to estimate the multinomial word-topic distribution. This dataset, which consists of 320,000 articles,3 is significantly larger than SemCor, which is the dataset used by Boyd-Graber et al. (2007). All markup from the Wikipedia dump was stripped off using the same filter as the ESA implementation (Sorg and Cimiano, 2008), and stopwords were filtered out using the Snowball (Porter, October 2001) stopword list. In addition, words </context>
<context position="32185" citStr="Sporleder and Li (2009)" startWordPosition="5187" endWordPosition="5190">ne idiom dictionaries (for the nonliteral sense) and by linguistic introspection (for the literal sense). We then compare the topic distributions of literal and nonliteral senses. As the paraphrases obtained from the idiom dictionary are very short, we treat the paraphrase as a sequence of independent words instead of as a document and apply Model III (see Section 3). Table 5 shows the results of our proposed model compared with state-of-the-art systems. We find that the system significantly outperforms the majority baseline (p&lt;&lt;0.01, x2 test) and the cohesion-graph based approach proposed by Sporleder and Li (2009) (p&lt;&lt;0.01, x2 test). The system also outperforms the bootstrapping System Precl Recl Fl Acc. Basemaj - - - 78.25 co-graph 50.04 69.72 58.26 78.38 boot. 71.86 66.36 69.00 87.03 Model III 67.05 81.07 73.40 87.24 Table 5: Performance on the literal or nonliteral sense disambiguation task on idioms. literal precision (Precl), literal recall (Recl), literal F-score (Fl), accuracy(Acc.). system by Li and Sporleder (2009), although not statistically significantly. This shows how a limited amount of human knowledge (e.g., paraphrases) can be added to an unsupervised system for a strong boost in perfor</context>
</contexts>
<marker>Sporleder, Li, 2009</marker>
<rawString>C. Sporleder, L. Li. 2009. Unsupervised recognition of literal and non-literal use of idiomatic expressions. In Proceedings of EACL-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>H Bai</author>
<author>M Stanton</author>
<author>W-Y Chen</author>
<author>E Y Chang</author>
</authors>
<title>Plda: Parallel latent dirichlet allocation for large-scale applications.</title>
<date>2009</date>
<booktitle>In Proc. of 5th International Conference on Algorithmic Aspects in Information and Management. Software available at http://code.google.com/p/plda.</booktitle>
<contexts>
<context position="22181" citStr="Wang et al. (2009)" startWordPosition="3559" endWordPosition="3562">yms, member meronyms, part meronyms, substance meronyms, attributes, topic members, region members, usage members, topics, regions, usages V Troponyms, entailments, outcomes, phrases, verb groups, topics, regions, usages, sentence frames A similar, pertainym, attributes, related, topics, regions, usages R pertainyms, topics, regions, usages Table 1: Selected reference synsets from WordNet that were used for different parts-of-speech to obtain word sense paraphrase. N(noun), V(verb), A(adj), R(adv). estimating the word-topic distribution from the Wikipedia dump. We used the package provided by Wang et al. (2009) with the suggested Dirichlet hyper-parameters 6. In order to avoid statistical instability, the final result is averaged over the last 50 iterations. We did four rounds of sampling with 1000, 500, 250, and 125 topics respectively. The final word-topic distribution is a normalized concatenate of the four distributions estimated in each round. In average, the sampling program run on the Wikipedia dump consumed 20G memory, and each round took about one week on a single AMD Dual-Core 1000MHZ processor. 5.1 Coarse-Grained WSD In this section we first describe the landscape of similar systems again</context>
</contexts>
<marker>Wang, Bai, Stanton, Chen, Chang, 2009</marker>
<rawString>Y. Wang, H. Bai, M. Stanton, W.-Y. Chen, E. Y. Chang. 2009. Plda: Parallel latent dirichlet allocation for large-scale applications. In Proc. of 5th International Conference on Algorithmic Aspects in Information and Management. Software available at http://code.google.com/p/plda.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>