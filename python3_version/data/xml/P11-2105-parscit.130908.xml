<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000312">
<title confidence="0.991295">
Hierarchical Text Classification with Latent Concepts
</title>
<author confidence="0.99898">
Xipeng Qiu, Xuanjing Huang, Zhao Liu and Jinlong Zhou
</author>
<affiliation confidence="0.998782">
School of Computer Science, Fudan University
</affiliation>
<email confidence="0.992567">
{xpqiu,xjhuang}@fudan.edu.cn, {zliu.fd,abc9703}@gmail.com
</email>
<sectionHeader confidence="0.994682" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998575285714286">
Recently, hierarchical text classification has
become an active research topic. The essential
idea is that the descendant classes can share
the information of the ancestor classes in a
predefined taxonomy. In this paper, we claim
that each class has several latent concepts and
its subclasses share information with these d-
ifferent concepts respectively. Then, we pro-
pose a variant Passive-Aggressive (PA) algo-
rithm for hierarchical text classification with
latent concepts. Experimental results show
that the performance of our algorithm is com-
petitive with the recently proposed hierarchi-
cal classification algorithms.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938581818182">
Text classification is a crucial and well-proven
method for organizing the collection of large scale
documents. The predefined categories are formed
by different criterions, e.g. “Entertainment”, “Sport-
s” and “Education” in news classification, “Junk E-
mail” and “Ordinary Email” in email classification.
In the literature, many algorithms (Sebastiani, 2002;
Yang and Liu, 1999; Yang and Pedersen, 1997) have
been proposed, such as Support Vector Machines
(SVM), k-Nearest Neighbor (kNN), Naive Bayes
(NB) and so on. Empirical evaluations have shown
that most of these methods are quite effective in tra-
ditional text classification applications.
In past serval years, hierarchical text classification
has become an active research topic in database area
(Koller and Sahami, 1997; Weigend et al., 1999)
and machine learning area (Rousu et al., 2006; Cai
and Hofmann, 2007). Different with traditional clas-
sification, the document collections are organized
as hierarchical class structure in many application
fields: web taxonomies (i.e. the Yahoo! Directory
http://dir.yahoo.com/ and the Open Direc-
tory Project (ODP) http://dmoz.org/), email
folders and product catalogs.
The approaches of hierarchical text classification
can be divided in three ways: flat, local and global
approaches.
The flat approach is traditional multi-class classi-
fication in flat fashion without hierarchical class in-
formation, which only uses the classes in leaf nodes
in taxonomy(Yang and Liu, 1999; Yang and Peder-
sen, 1997; Qiu et al., 2011).
The local approach proceeds in a top-down fash-
ion, which firstly picks the most relevant categories
of the top level and then recursively making the
choice among the low-level categories(Sun and Lim,
2001; Liu et al., 2005).
The global approach builds only one classifier to
discriminate all categories in a hierarchy(Cai and
Hofmann, 2004; Rousu et al., 2006; Miao and Qiu,
2009; Qiu et al., 2009). The essential idea of global
approach is that the close classes have some com-
mon underlying factors. Especially, the descendan-
t classes can share the characteristics of the ances-
tor classes, which is similar with multi-task learn-
ing(Caruana, 1997; Xue et al., 2007).
Because the global hierarchical categorization can
avoid the drawbacks about those high-level irrecov-
erable error, it is more popular in the machine learn-
ing domain.
However, the taxonomy is defined artificially and
is usually very difficult to organize for large scale
taxonomy. The subclasses of the same parent class
may be dissimilar and can be grouped in differen-
t concepts, so it bring great challenge to hierarchi-
</bodyText>
<page confidence="0.95878">
598
</page>
<note confidence="0.780436">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 598–602,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.997503380952381">
(a) (b)
Sports
Swimming
Basketball
Football
College
Surfing
High
School
Water
Acade
my
por
Ball
Swimming
Basketball
Football
Surfing
College
High
School
</figure>
<bodyText confidence="0.935909666666667">
where Λ(y) = (λ1(y), · · · , λm(y))T E Rm and ®
is the Kronecker product.
We can define
</bodyText>
<equation confidence="0.8445315">
{ _ ti if ωi E y
λi(y) 0 otherwise , (2)
</equation>
<bodyText confidence="0.999955666666667">
where ti &gt;= 0 is the attribute value for node v. In
the simplest case, ti can be set to a constant, like 1.
Thus, we can classify x with a score function,
</bodyText>
<equation confidence="0.965236">
yˆ = arg max F(w, b(x, y)), (3)
</equation>
<figureCaption confidence="0.999851">
Figure 1: Example of latent nodes in taxonomy Y
</figureCaption>
<bodyText confidence="0.996098625">
cal classification. For example, the “Sports” node
in a taxonomy have six subclasses (Fig. 1a), but
these subclass can be grouped into three unobserv-
able concepts (Fig. 1b). These concepts can show
the underlying factors more clearly.
In this paper, we claim that each class may have
several latent concepts and its subclasses share in-
formation with these different concepts respectively.
Then we propose a variant Passive-Aggressive (PA)
algorithm to maximizes the margins between latent
paths.
The rest of the paper is organized as follows. Sec-
tion 2 describes the basic model of hierarchical clas-
sification. Then we propose our algorithm in section
3. Section 4 gives experimental analysis. Section 5
concludes the paper.
</bodyText>
<sectionHeader confidence="0.938723" genericHeader="method">
2 Hierarchical Text Classification
</sectionHeader>
<bodyText confidence="0.999514928571429">
In text classification, the documents are often rep-
resented with vector space model (VSM) (Salton et
al., 1975). Following (Cai and Hofmann, 2007),
we incorporate the hierarchical information in fea-
ture representation. The basic idea is that the notion
of class attributes will allow generalization to take
place across (similar) categories and not just across
training examples belonging to the same category.
Assuming that the categories is Q =
[ω1, · · · , ωm], where m is the number of the
categories, which are organized in hierarchical
structure, such as tree or DAG.
Give a sample x with its class path in the taxono-
my y, we define the feature is
</bodyText>
<equation confidence="0.978941">
b(x, y) = Λ(y) ® x, (1)
</equation>
<bodyText confidence="0.995556">
where w is the parameter of F(·).
</bodyText>
<sectionHeader confidence="0.973686" genericHeader="method">
3 Hierarchical Text Classification with
</sectionHeader>
<subsectionHeader confidence="0.541069">
Latent Concepts
</subsectionHeader>
<bodyText confidence="0.999814">
In this section, we first extent the Passive-
Aggressive (PA) algorithm to the hierarchical clas-
sification (HPA), then we modify it to incorporate
latent concepts (LHPA).
</bodyText>
<subsectionHeader confidence="0.999858">
3.1 Hierarchical Passive-Aggressive Algorithm
</subsectionHeader>
<bodyText confidence="0.999935">
The PA algorithm is an online learning algorithm,
which aims to find the new weight vector wt+1 to be
the solution to the following constrained optimiza-
tion problem in round t.
</bodyText>
<equation confidence="0.997471">
1
wt+1 = arg min  ||w − wt   ||2 + Cξ
.S∈R- 2
s.t. ℓ(w; (xt, yt)) &lt;= ξ and ξ &gt;= 0. (4)
</equation>
<bodyText confidence="0.999825076923077">
where ℓ(w; (xt, yt)) is the hinge-loss function and ξ
is slack variable.
Since the hierarchical text classification is loss-
sensitive based on the hierarchical structure. We
need discriminate the misclassification from “near-
ly correct” to “clearly incorrect”. Here we use tree
induced error ∆(y, y′), which is the shortest path
connecting the nodes yleaf and y′leaf. yleaf repre-
sents the leaf node in path y.
Given a example (x, y), we look for the w to
maximize the separation margin γ(w; (x, y)) be-
tween the score of the correct path y and the closest
error path ˆy.
</bodyText>
<equation confidence="0.984949">
γ(w; (x, y)) = wTb(x, y) − wTb(x, ˆy), (5)
</equation>
<page confidence="0.978789">
599
</page>
<bodyText confidence="0.9442085">
where yˆ = arg maxz̸=y wTΦ(x, z) and Φ is a fea-
ture function.
Unlike the standard PA algorithm, which achieve
a margin of at least 1 as often as possible, we wish
the margin is related to tree induced error ∆(y, ˆy).
This loss is defined by the following function,
</bodyText>
<equation confidence="0.982578">
ℓ(w; (x, y)) =
f 0, γ (w; (x, y)) &gt; ∆(y, ˆy) (6)
l ∆(y, ˆy) γ (w; (x, y)), otherwise
</equation>
<bodyText confidence="0.998661333333333">
We abbreviate ℓ(w; (x, y)) to ℓ. If ℓ = 0 then wt
itself satisfies the constraint in Eq. (4) and is clearly
the optimal solution. We therefore concentrate on
the case where ℓ &gt; 0.
First, we define the Lagrangian of the optimiza-
tion problem in Eq. (4) to be,
</bodyText>
<equation confidence="0.999735666666667">
L(w, ξ, α, β) = 2||w−wt||2+Cξ+α(ℓ−ξ)−βξ
1
s.t. α, β &gt;= 0. (7)
</equation>
<bodyText confidence="0.993404">
where α, β is a Lagrange multiplier.
We set the gradient of Eq. (7) respect to ξ to zero.
</bodyText>
<equation confidence="0.990221">
α + β = C. (8)
</equation>
<bodyText confidence="0.80391">
The gradient of w should be zero.
</bodyText>
<equation confidence="0.890151294117647">
w − wt − α(Φ(x, y) − Φ(x, ˆy)) = 0 (9)
Then we get,
w = wt + α(Φ(x, y) − Φ(x, ˆy)). (10)
Substitute Eq. (8) and Eq. (10) to objective func-
tion Eq. (7), we get
L(α)= −2α2||Φ(x,y) − Φ(x, ˆy)||2
1
+ αwt(Φ(x, y) − Φ(x, ˆy))) − α∆(y, ˆy) (11)
Differentiate Eq. (11 with α, and set it to zero, we
get
∆(y, ˆy) − wt(Φ(x, y) − Φ(x, ˆy)))
α∗ = (12)
||Φ(x, y) − Φ(x, ˆy)||2
From α + β = C, we know that α &lt; C, so
α* = min(C, ∆(y,ˆy) − wt(Φ(x,y) − Φ(x, ˆy)))
).
|Φ(x, y) − Φ(x, ˆy) |P
</equation>
<subsectionHeader confidence="0.995192">
3.2 Hierarchical Passive-Aggressive Algorithm
with Latent Concepts
</subsectionHeader>
<bodyText confidence="0.9998027">
For the hierarchical taxonomy Ω = (ω1, · · · , ω&apos;),
we define that each class ωi has a set Hu,, =
h1�z, · · · , hw with m latent concepts, which are un-
observable.
Given a label path y, it has a set of several latent
paths Hy. For a latent path z ∈ Hy, a function
Proj(z) .= y is the projection from a latent path z
to its corresponding path y.
Then we can define the predict latent path h∗ and
the most correct latent path ˆh:
</bodyText>
<equation confidence="0.998117">
hˆ = arg max wTΦ(x, z), (14)
pro9(z)̸=y
wTΦ(x, z). (15)
</equation>
<bodyText confidence="0.959236">
Similar to the above analysis of HPA, we re-define
the margin
</bodyText>
<equation confidence="0.99363225">
γ(w; (x, y) = wTΦ(x, h∗) − wT Φ(x, ˆh), (16)
then we get the optimal update step
ℓ(wt; (x, y))
α∗ � = min(C,
</equation>
<bodyText confidence="0.763056">
Finally, we get update strategy,
</bodyText>
<equation confidence="0.998925">
w = wt + α∗ �(Φ(x, h∗) − Φ(x, ˆh)). (18)
</equation>
<bodyText confidence="0.9988205">
Our hierarchical passive-aggressive algorithm
with latent concepts (LHPA) is shown in Algorith-
m 1. In this paper, we use two latent concepts for
each class.
</bodyText>
<sectionHeader confidence="0.997937" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.944239">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.99995475">
We evaluate our proposed algorithm on two datasets
with hierarchical category structure.
WIPO-alpha dataset The dataset&apos; consisted of the
1372 training and 358 testing document com-
prising the D section of the hierarchy. The
number of nodes in the hierarchy was 188, with
maximum depth 3. The dataset was processed
into bag-of-words representation with TF·IDF
</bodyText>
<equation confidence="0.892105333333333">
&apos;World Intellectual Property Organization, http://www.
h∗ = arg max
pro9(z)=y
||Φ(x, h∗) − Φ(x,
ˆh)||2). (17)
(13) wipo.int/classifications/en
</equation>
<page confidence="0.86106">
600
</page>
<bodyText confidence="0.387976">
input : training data set: (xn, yn), n = 1, · · · , N,
and parameters: C, K
output: w
</bodyText>
<equation confidence="0.984303333333333">
Initialize: cw +- 0,;
for k = 0···K − 1 do
w0 +- 0 ;
for t = 0···T − 1 do
get (xt, yt) from data set;
predict ˆh, h*;
calculate γ(w; (x, y)) and∆(yt,ˆyt);
if γ(w; (x, y)) &lt; ∆(yt,ˆyt) then
calculate αL by Eq. (17);
update wt+1 by Eq. (18). ;
end
end
cw = cw + wT ;
end
w = cw/K ;
</equation>
<bodyText confidence="0.946871166666667">
Algorithm 1: Hierarchical PA algorithm with la-
tent concepts
weighting. No word stemming or stop-word
removal was performed. This dataset is used
in (Rousu et al., 2006).
LSHTC dataset The dataset2 has been constructed
by crawling web pages that are found in the
Open Directory Project (ODP) and translating
them into feature vectors (content vectors) and
splitting the set of Web pages into a training,
a validation and a test set, per ODP category.
Here, we use the dry-run dataset(task 1).
</bodyText>
<subsectionHeader confidence="0.992591">
4.2 Performance Measurement
</subsectionHeader>
<bodyText confidence="0.9992057">
Macro Precision, Macro Recall and Macro F 1 are
the most widely used performance measurements
for text classification problems nowadays. The
macro strategy computes macro precision and re-
call scores by averaging the precision/recall of each
category, which is preferred because the categories
are usually unbalanced and give more challenges to
classifiers. The Macro F1 score is computed using
the standard formula applied to the macro-level pre-
cision and recall scores.
</bodyText>
<equation confidence="0.997626">
P � R
MacroF1 = P + R, (19)
</equation>
<footnote confidence="0.934878">
2Large Scale Hierarchical Text classification Pascal Chal-
lenge,http://lshtc.iit.demokritos.gr
</footnote>
<tableCaption confidence="0.7888565">
Table 1: Results on WIPO-alpha Dataset.“-” means that
the result is not available in the author’s paper.
</tableCaption>
<table confidence="0.998992833333333">
Accuracy F1 Precision Recall TIE
PA 49.16 40.71 43.27 38.44 2.06
HPA 50.84 40.26 43.23 37.67 1.92
LHPA 51.96 41.84 45.56 38.69 1.87
HSVM 23.8 - - - -
HM3 35.0 - - - -
</table>
<tableCaption confidence="0.995176">
Table 2: Results on LSHTC dry-run Dataset
</tableCaption>
<table confidence="0.99644325">
Accuracy F1 Precision Recall TIE
PA 47.36 44.63 52.64 38.73 3.68
HPA 46.88 43.78 51.26 38.2 3.73
LHPA 48.39 46.26 53.82 40.56 3.43
</table>
<bodyText confidence="0.996769">
where P is the Macro Precision and R is the Macro
Recall. We also use tree induced error (TIE) in the
experiments.
</bodyText>
<subsectionHeader confidence="0.512087">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999959071428571">
We implement three algorithms3: PA(Flat PA), H-
PA(Hierarchical PA) and LHPA(Hierarchical PA
with latent concepts). The results are shown in Table
1 and 2. For WIPO-alpha dataset, we also compared
LHPA with two algorithms used in (Rousu et al.,
2006): HSVM and HM3.
We can see that LHPA has better performances
than the other methods. From Table 2, we can see
that it is not always useful to incorporate the hierar-
chical information. Though the subclasses can share
information with their parent class, the shared infor-
mation may be different for each subclass. So we
should decompose the underlying factors into dif-
ferent latent concepts.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9998722">
In this paper, we propose a variant Passive-
Aggressive algorithm for hierarchical text classifi-
cation with latent concepts. In the future, we will
investigate our method in the larger and more noisy
data.
</bodyText>
<sectionHeader confidence="0.998037" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.986462">
This work was (partially) funded by NSFC (No.
61003091 and No. 61073069), 973 Program (No.
</bodyText>
<footnote confidence="0.936485">
3Source codes are available in FudanNLP toolkit, http:
//code.google.com/p/fudannlp/
</footnote>
<page confidence="0.99349">
601
</page>
<bodyText confidence="0.8002055">
2010CB327906) and Shanghai Committee of Sci-
ence and Technology(No. 10511500703).
</bodyText>
<sectionHeader confidence="0.973311" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999754071428571">
L. Cai and T. Hofmann. 2004. Hierarchical document
categorization with support vector machines. In Pro-
ceedings of CIKM.
L. Cai and T. Hofmann. 2007. Exploiting known tax-
onomies in learning overlapping concepts. In Pro-
ceedings of International Joint Conferences on Arti-
ficial Intelligence.
R. Caruana. 1997. Multi-task learning. Machine Learn-
ing, 28(1):41–75.
D. Koller and M Sahami. 1997. Hierarchically classify-
ing documents using very few words. In Proceedings
of the International Conference on Machine Learning
(ICML).
T.Y. Liu, Y. Yang, H. Wan, H.J. Zeng, Z. Chen, and W.Y.
Ma. 2005. Support vector machines classification
with a very large-scale taxonomy. ACM SIGKDD Ex-
plorations Newsletter, 7(1):43.
Youdong Miao and Xipeng Qiu. 2009. Hierarchical
centroid-based classifier for large scale text classifica-
tion. In Large Scale Hierarchical Text classification
(LSHTC) Pascal Challenge.
Xipeng Qiu, Wenjun Gao, and Xuanjing Huang. 2009.
Hierarchical multi-class text categorization with glob-
al margin maximization. In Proceedings of the ACL-
IJCNLP 2009 Conference, pages 165–168, Suntec,
Singapore, August. Association for Computational
Linguistics.
Xipeng Qiu, Jinlong Zhou, and Xuanjing Huang. 2011.
An effective feature selection method for text catego-
rization. In Proceedings of the 15th Pacific-Asia Con-
ference on Knowledge Discovery and Data Mining.
Juho Rousu, Craig Saunders, Sandor Szedmak, and John
Shawe-Taylor. 2006. Kernel-based learning of hierar-
chical multilabel classification models. In Journal of
Machine Learning Research.
G. Salton, A. Wong, and CS Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18(11):613–620.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM computing surveys, 34(1):1–47.
A. Sun and E.-P Lim. 2001. Hierarchical text classi-
fication and evaluation. In Proceedings of the IEEE
International Conference on Data Mining.
A. Weigend, E. Wiener, and J Pedersen. 1999. Exploit-
ing hierarchy in text categorization. In Information
Retrieval.
Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. 2007.
Multi-task learning for classification with dirichlet
process priors. The Journal of Machine Learning Re-
search, 8:63.
Y. Yang and X. Liu. 1999. A re-examination of text
categorization methods. In Proc. of SIGIR. ACM Press
New York, NY, USA.
Y. Yang and J.O. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proc. of
Int. Conf. on Mach. Learn. (ICML), volume 97.
</reference>
<page confidence="0.99818">
602
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945283">
<title confidence="0.999888">Hierarchical Text Classification with Latent Concepts</title>
<author confidence="0.993163">Xuanjing Huang Qiu</author>
<author confidence="0.993163">Zhao Liu</author>
<affiliation confidence="0.999956">School of Computer Science, Fudan University</affiliation>
<abstract confidence="0.996119333333333">Recently, hierarchical text classification has become an active research topic. The essential idea is that the descendant classes can share the information of the ancestor classes in a predefined taxonomy. In this paper, we claim that each class has several latent concepts and its subclasses share information with these different concepts respectively. Then, we propose a variant Passive-Aggressive (PA) algorithm for hierarchical text classification with latent concepts. Experimental results show that the performance of our algorithm is competitive with the recently proposed hierarchical classification algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Cai</author>
<author>T Hofmann</author>
</authors>
<title>Hierarchical document categorization with support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="2715" citStr="Cai and Hofmann, 2004" startWordPosition="388" endWordPosition="391"> three ways: flat, local and global approaches. The flat approach is traditional multi-class classification in flat fashion without hierarchical class information, which only uses the classes in leaf nodes in taxonomy(Yang and Liu, 1999; Yang and Pedersen, 1997; Qiu et al., 2011). The local approach proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories(Sun and Lim, 2001; Liu et al., 2005). The global approach builds only one classifier to discriminate all categories in a hierarchy(Cai and Hofmann, 2004; Rousu et al., 2006; Miao and Qiu, 2009; Qiu et al., 2009). The essential idea of global approach is that the close classes have some common underlying factors. Especially, the descendant classes can share the characteristics of the ancestor classes, which is similar with multi-task learning(Caruana, 1997; Xue et al., 2007). Because the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain. However, the taxonomy is defined artificially and is usually very difficult to organize for large scale ta</context>
</contexts>
<marker>Cai, Hofmann, 2004</marker>
<rawString>L. Cai and T. Hofmann. 2004. Hierarchical document categorization with support vector machines. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Cai</author>
<author>T Hofmann</author>
</authors>
<title>Exploiting known taxonomies in learning overlapping concepts.</title>
<date>2007</date>
<booktitle>In Proceedings of International Joint Conferences on Artificial Intelligence.</booktitle>
<contexts>
<context position="1726" citStr="Cai and Hofmann, 2007" startWordPosition="241" endWordPosition="244">nk Email” and “Ordinary Email” in email classification. In the literature, many algorithms (Sebastiani, 2002; Yang and Liu, 1999; Yang and Pedersen, 1997) have been proposed, such as Support Vector Machines (SVM), k-Nearest Neighbor (kNN), Naive Bayes (NB) and so on. Empirical evaluations have shown that most of these methods are quite effective in traditional text classification applications. In past serval years, hierarchical text classification has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Different with traditional classification, the document collections are organized as hierarchical class structure in many application fields: web taxonomies (i.e. the Yahoo! Directory http://dir.yahoo.com/ and the Open Directory Project (ODP) http://dmoz.org/), email folders and product catalogs. The approaches of hierarchical text classification can be divided in three ways: flat, local and global approaches. The flat approach is traditional multi-class classification in flat fashion without hierarchical class information, which only uses the classes in leaf nodes in taxonomy(Yang and Liu, </context>
<context position="5091" citStr="Cai and Hofmann, 2007" startWordPosition="780" endWordPosition="783">several latent concepts and its subclasses share information with these different concepts respectively. Then we propose a variant Passive-Aggressive (PA) algorithm to maximizes the margins between latent paths. The rest of the paper is organized as follows. Section 2 describes the basic model of hierarchical classification. Then we propose our algorithm in section 3. Section 4 gives experimental analysis. Section 5 concludes the paper. 2 Hierarchical Text Classification In text classification, the documents are often represented with vector space model (VSM) (Salton et al., 1975). Following (Cai and Hofmann, 2007), we incorporate the hierarchical information in feature representation. The basic idea is that the notion of class attributes will allow generalization to take place across (similar) categories and not just across training examples belonging to the same category. Assuming that the categories is Q = [ω1, · · · , ωm], where m is the number of the categories, which are organized in hierarchical structure, such as tree or DAG. Give a sample x with its class path in the taxonomy y, we define the feature is b(x, y) = Λ(y) ® x, (1) where w is the parameter of F(·). 3 Hierarchical Text Classification</context>
</contexts>
<marker>Cai, Hofmann, 2007</marker>
<rawString>L. Cai and T. Hofmann. 2007. Exploiting known taxonomies in learning overlapping concepts. In Proceedings of International Joint Conferences on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Caruana</author>
</authors>
<date>1997</date>
<booktitle>Multi-task learning. Machine Learning,</booktitle>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="3022" citStr="Caruana, 1997" startWordPosition="440" endWordPosition="442">top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories(Sun and Lim, 2001; Liu et al., 2005). The global approach builds only one classifier to discriminate all categories in a hierarchy(Cai and Hofmann, 2004; Rousu et al., 2006; Miao and Qiu, 2009; Qiu et al., 2009). The essential idea of global approach is that the close classes have some common underlying factors. Especially, the descendant classes can share the characteristics of the ancestor classes, which is similar with multi-task learning(Caruana, 1997; Xue et al., 2007). Because the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain. However, the taxonomy is defined artificially and is usually very difficult to organize for large scale taxonomy. The subclasses of the same parent class may be dissimilar and can be grouped in different concepts, so it bring great challenge to hierarchi598 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 598–602, Portland, Oregon, June 19-24, 2011. c�2</context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>R. Caruana. 1997. Multi-task learning. Machine Learning, 28(1):41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>M Sahami</author>
</authors>
<title>Hierarchically classifying documents using very few words.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="1633" citStr="Koller and Sahami, 1997" startWordPosition="225" endWordPosition="228">fferent criterions, e.g. “Entertainment”, “Sports” and “Education” in news classification, “Junk Email” and “Ordinary Email” in email classification. In the literature, many algorithms (Sebastiani, 2002; Yang and Liu, 1999; Yang and Pedersen, 1997) have been proposed, such as Support Vector Machines (SVM), k-Nearest Neighbor (kNN), Naive Bayes (NB) and so on. Empirical evaluations have shown that most of these methods are quite effective in traditional text classification applications. In past serval years, hierarchical text classification has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Different with traditional classification, the document collections are organized as hierarchical class structure in many application fields: web taxonomies (i.e. the Yahoo! Directory http://dir.yahoo.com/ and the Open Directory Project (ODP) http://dmoz.org/), email folders and product catalogs. The approaches of hierarchical text classification can be divided in three ways: flat, local and global approaches. The flat approach is traditional multi-class classification in flat fashion without hierarc</context>
</contexts>
<marker>Koller, Sahami, 1997</marker>
<rawString>D. Koller and M Sahami. 1997. Hierarchically classifying documents using very few words. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Y Liu</author>
<author>Y Yang</author>
<author>H Wan</author>
<author>H J Zeng</author>
<author>Z Chen</author>
<author>W Y Ma</author>
</authors>
<title>Support vector machines classification with a very large-scale taxonomy.</title>
<date>2005</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="2599" citStr="Liu et al., 2005" startWordPosition="371" endWordPosition="374">.org/), email folders and product catalogs. The approaches of hierarchical text classification can be divided in three ways: flat, local and global approaches. The flat approach is traditional multi-class classification in flat fashion without hierarchical class information, which only uses the classes in leaf nodes in taxonomy(Yang and Liu, 1999; Yang and Pedersen, 1997; Qiu et al., 2011). The local approach proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories(Sun and Lim, 2001; Liu et al., 2005). The global approach builds only one classifier to discriminate all categories in a hierarchy(Cai and Hofmann, 2004; Rousu et al., 2006; Miao and Qiu, 2009; Qiu et al., 2009). The essential idea of global approach is that the close classes have some common underlying factors. Especially, the descendant classes can share the characteristics of the ancestor classes, which is similar with multi-task learning(Caruana, 1997; Xue et al., 2007). Because the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learnin</context>
</contexts>
<marker>Liu, Yang, Wan, Zeng, Chen, Ma, 2005</marker>
<rawString>T.Y. Liu, Y. Yang, H. Wan, H.J. Zeng, Z. Chen, and W.Y. Ma. 2005. Support vector machines classification with a very large-scale taxonomy. ACM SIGKDD Explorations Newsletter, 7(1):43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youdong Miao</author>
<author>Xipeng Qiu</author>
</authors>
<title>Hierarchical centroid-based classifier for large scale text classification. In Large Scale Hierarchical Text classification (LSHTC) Pascal Challenge.</title>
<date>2009</date>
<contexts>
<context position="2755" citStr="Miao and Qiu, 2009" startWordPosition="396" endWordPosition="399">hes. The flat approach is traditional multi-class classification in flat fashion without hierarchical class information, which only uses the classes in leaf nodes in taxonomy(Yang and Liu, 1999; Yang and Pedersen, 1997; Qiu et al., 2011). The local approach proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories(Sun and Lim, 2001; Liu et al., 2005). The global approach builds only one classifier to discriminate all categories in a hierarchy(Cai and Hofmann, 2004; Rousu et al., 2006; Miao and Qiu, 2009; Qiu et al., 2009). The essential idea of global approach is that the close classes have some common underlying factors. Especially, the descendant classes can share the characteristics of the ancestor classes, which is similar with multi-task learning(Caruana, 1997; Xue et al., 2007). Because the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain. However, the taxonomy is defined artificially and is usually very difficult to organize for large scale taxonomy. The subclasses of the same paren</context>
</contexts>
<marker>Miao, Qiu, 2009</marker>
<rawString>Youdong Miao and Xipeng Qiu. 2009. Hierarchical centroid-based classifier for large scale text classification. In Large Scale Hierarchical Text classification (LSHTC) Pascal Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xipeng Qiu</author>
<author>Wenjun Gao</author>
<author>Xuanjing Huang</author>
</authors>
<title>Hierarchical multi-class text categorization with global margin maximization.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACLIJCNLP 2009 Conference,</booktitle>
<pages>165--168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2774" citStr="Qiu et al., 2009" startWordPosition="400" endWordPosition="403">ch is traditional multi-class classification in flat fashion without hierarchical class information, which only uses the classes in leaf nodes in taxonomy(Yang and Liu, 1999; Yang and Pedersen, 1997; Qiu et al., 2011). The local approach proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories(Sun and Lim, 2001; Liu et al., 2005). The global approach builds only one classifier to discriminate all categories in a hierarchy(Cai and Hofmann, 2004; Rousu et al., 2006; Miao and Qiu, 2009; Qiu et al., 2009). The essential idea of global approach is that the close classes have some common underlying factors. Especially, the descendant classes can share the characteristics of the ancestor classes, which is similar with multi-task learning(Caruana, 1997; Xue et al., 2007). Because the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain. However, the taxonomy is defined artificially and is usually very difficult to organize for large scale taxonomy. The subclasses of the same parent class may be diss</context>
</contexts>
<marker>Qiu, Gao, Huang, 2009</marker>
<rawString>Xipeng Qiu, Wenjun Gao, and Xuanjing Huang. 2009. Hierarchical multi-class text categorization with global margin maximization. In Proceedings of the ACLIJCNLP 2009 Conference, pages 165–168, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xipeng Qiu</author>
<author>Jinlong Zhou</author>
<author>Xuanjing Huang</author>
</authors>
<title>An effective feature selection method for text categorization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Pacific-Asia Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="2374" citStr="Qiu et al., 2011" startWordPosition="334" endWordPosition="337">assification, the document collections are organized as hierarchical class structure in many application fields: web taxonomies (i.e. the Yahoo! Directory http://dir.yahoo.com/ and the Open Directory Project (ODP) http://dmoz.org/), email folders and product catalogs. The approaches of hierarchical text classification can be divided in three ways: flat, local and global approaches. The flat approach is traditional multi-class classification in flat fashion without hierarchical class information, which only uses the classes in leaf nodes in taxonomy(Yang and Liu, 1999; Yang and Pedersen, 1997; Qiu et al., 2011). The local approach proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories(Sun and Lim, 2001; Liu et al., 2005). The global approach builds only one classifier to discriminate all categories in a hierarchy(Cai and Hofmann, 2004; Rousu et al., 2006; Miao and Qiu, 2009; Qiu et al., 2009). The essential idea of global approach is that the close classes have some common underlying factors. Especially, the descendant classes can share the characteristics of the ancestor classes, which i</context>
</contexts>
<marker>Qiu, Zhou, Huang, 2011</marker>
<rawString>Xipeng Qiu, Jinlong Zhou, and Xuanjing Huang. 2011. An effective feature selection method for text categorization. In Proceedings of the 15th Pacific-Asia Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juho Rousu</author>
<author>Craig Saunders</author>
<author>Sandor Szedmak</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Kernel-based learning of hierarchical multilabel classification models.</title>
<date>2006</date>
<journal>In Journal of Machine Learning Research.</journal>
<contexts>
<context position="1702" citStr="Rousu et al., 2006" startWordPosition="237" endWordPosition="240"> classification, “Junk Email” and “Ordinary Email” in email classification. In the literature, many algorithms (Sebastiani, 2002; Yang and Liu, 1999; Yang and Pedersen, 1997) have been proposed, such as Support Vector Machines (SVM), k-Nearest Neighbor (kNN), Naive Bayes (NB) and so on. Empirical evaluations have shown that most of these methods are quite effective in traditional text classification applications. In past serval years, hierarchical text classification has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Different with traditional classification, the document collections are organized as hierarchical class structure in many application fields: web taxonomies (i.e. the Yahoo! Directory http://dir.yahoo.com/ and the Open Directory Project (ODP) http://dmoz.org/), email folders and product catalogs. The approaches of hierarchical text classification can be divided in three ways: flat, local and global approaches. The flat approach is traditional multi-class classification in flat fashion without hierarchical class information, which only uses the classes in leaf nodes in</context>
<context position="10104" citStr="Rousu et al., 2006" startWordPosition="1746" endWordPosition="1749">rg max pro9(z)=y ||Φ(x, h∗) − Φ(x, ˆh)||2). (17) (13) wipo.int/classifications/en 600 input : training data set: (xn, yn), n = 1, · · · , N, and parameters: C, K output: w Initialize: cw +- 0,; for k = 0···K − 1 do w0 +- 0 ; for t = 0···T − 1 do get (xt, yt) from data set; predict ˆh, h*; calculate γ(w; (x, y)) and∆(yt,ˆyt); if γ(w; (x, y)) &lt; ∆(yt,ˆyt) then calculate αL by Eq. (17); update wt+1 by Eq. (18). ; end end cw = cw + wT ; end w = cw/K ; Algorithm 1: Hierarchical PA algorithm with latent concepts weighting. No word stemming or stop-word removal was performed. This dataset is used in (Rousu et al., 2006). LSHTC dataset The dataset2 has been constructed by crawling web pages that are found in the Open Directory Project (ODP) and translating them into feature vectors (content vectors) and splitting the set of Web pages into a training, a validation and a test set, per ODP category. Here, we use the dry-run dataset(task 1). 4.2 Performance Measurement Macro Precision, Macro Recall and Macro F 1 are the most widely used performance measurements for text classification problems nowadays. The macro strategy computes macro precision and recall scores by averaging the precision/recall of each categor</context>
<context position="11869" citStr="Rousu et al., 2006" startWordPosition="2037" endWordPosition="2040">2 LHPA 51.96 41.84 45.56 38.69 1.87 HSVM 23.8 - - - - HM3 35.0 - - - - Table 2: Results on LSHTC dry-run Dataset Accuracy F1 Precision Recall TIE PA 47.36 44.63 52.64 38.73 3.68 HPA 46.88 43.78 51.26 38.2 3.73 LHPA 48.39 46.26 53.82 40.56 3.43 where P is the Macro Precision and R is the Macro Recall. We also use tree induced error (TIE) in the experiments. 4.3 Results We implement three algorithms3: PA(Flat PA), HPA(Hierarchical PA) and LHPA(Hierarchical PA with latent concepts). The results are shown in Table 1 and 2. For WIPO-alpha dataset, we also compared LHPA with two algorithms used in (Rousu et al., 2006): HSVM and HM3. We can see that LHPA has better performances than the other methods. From Table 2, we can see that it is not always useful to incorporate the hierarchical information. Though the subclasses can share information with their parent class, the shared information may be different for each subclass. So we should decompose the underlying factors into different latent concepts. 5 Conclusion In this paper, we propose a variant PassiveAggressive algorithm for hierarchical text classification with latent concepts. In the future, we will investigate our method in the larger and more noisy</context>
</contexts>
<marker>Rousu, Saunders, Szedmak, Shawe-Taylor, 2006</marker>
<rawString>Juho Rousu, Craig Saunders, Sandor Szedmak, and John Shawe-Taylor. 2006. Kernel-based learning of hierarchical multilabel classification models. In Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>CS Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="5056" citStr="Salton et al., 1975" startWordPosition="775" endWordPosition="778">e claim that each class may have several latent concepts and its subclasses share information with these different concepts respectively. Then we propose a variant Passive-Aggressive (PA) algorithm to maximizes the margins between latent paths. The rest of the paper is organized as follows. Section 2 describes the basic model of hierarchical classification. Then we propose our algorithm in section 3. Section 4 gives experimental analysis. Section 5 concludes the paper. 2 Hierarchical Text Classification In text classification, the documents are often represented with vector space model (VSM) (Salton et al., 1975). Following (Cai and Hofmann, 2007), we incorporate the hierarchical information in feature representation. The basic idea is that the notion of class attributes will allow generalization to take place across (similar) categories and not just across training examples belonging to the same category. Assuming that the categories is Q = [ω1, · · · , ωm], where m is the number of the categories, which are organized in hierarchical structure, such as tree or DAG. Give a sample x with its class path in the taxonomy y, we define the feature is b(x, y) = Λ(y) ® x, (1) where w is the parameter of F(·).</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and CS Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM computing surveys,</journal>
<pages>34--1</pages>
<contexts>
<context position="1212" citStr="Sebastiani, 2002" startWordPosition="163" endWordPosition="164">riant Passive-Aggressive (PA) algorithm for hierarchical text classification with latent concepts. Experimental results show that the performance of our algorithm is competitive with the recently proposed hierarchical classification algorithms. 1 Introduction Text classification is a crucial and well-proven method for organizing the collection of large scale documents. The predefined categories are formed by different criterions, e.g. “Entertainment”, “Sports” and “Education” in news classification, “Junk Email” and “Ordinary Email” in email classification. In the literature, many algorithms (Sebastiani, 2002; Yang and Liu, 1999; Yang and Pedersen, 1997) have been proposed, such as Support Vector Machines (SVM), k-Nearest Neighbor (kNN), Naive Bayes (NB) and so on. Empirical evaluations have shown that most of these methods are quite effective in traditional text classification applications. In past serval years, hierarchical text classification has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Different with traditional classification, the document collections are organized as</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>F. Sebastiani. 2002. Machine learning in automated text categorization. ACM computing surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sun</author>
<author>E-P Lim</author>
</authors>
<title>Hierarchical text classification and evaluation.</title>
<date>2001</date>
<booktitle>In Proceedings of the IEEE International Conference on Data Mining.</booktitle>
<contexts>
<context position="2580" citStr="Sun and Lim, 2001" startWordPosition="367" endWordPosition="370">t (ODP) http://dmoz.org/), email folders and product catalogs. The approaches of hierarchical text classification can be divided in three ways: flat, local and global approaches. The flat approach is traditional multi-class classification in flat fashion without hierarchical class information, which only uses the classes in leaf nodes in taxonomy(Yang and Liu, 1999; Yang and Pedersen, 1997; Qiu et al., 2011). The local approach proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories(Sun and Lim, 2001; Liu et al., 2005). The global approach builds only one classifier to discriminate all categories in a hierarchy(Cai and Hofmann, 2004; Rousu et al., 2006; Miao and Qiu, 2009; Qiu et al., 2009). The essential idea of global approach is that the close classes have some common underlying factors. Especially, the descendant classes can share the characteristics of the ancestor classes, which is similar with multi-task learning(Caruana, 1997; Xue et al., 2007). Because the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in </context>
</contexts>
<marker>Sun, Lim, 2001</marker>
<rawString>A. Sun and E.-P Lim. 2001. Hierarchical text classification and evaluation. In Proceedings of the IEEE International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Weigend</author>
<author>E Wiener</author>
<author>J Pedersen</author>
</authors>
<title>Exploiting hierarchy in text categorization.</title>
<date>1999</date>
<booktitle>In Information Retrieval.</booktitle>
<contexts>
<context position="1656" citStr="Weigend et al., 1999" startWordPosition="229" endWordPosition="232">“Entertainment”, “Sports” and “Education” in news classification, “Junk Email” and “Ordinary Email” in email classification. In the literature, many algorithms (Sebastiani, 2002; Yang and Liu, 1999; Yang and Pedersen, 1997) have been proposed, such as Support Vector Machines (SVM), k-Nearest Neighbor (kNN), Naive Bayes (NB) and so on. Empirical evaluations have shown that most of these methods are quite effective in traditional text classification applications. In past serval years, hierarchical text classification has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Different with traditional classification, the document collections are organized as hierarchical class structure in many application fields: web taxonomies (i.e. the Yahoo! Directory http://dir.yahoo.com/ and the Open Directory Project (ODP) http://dmoz.org/), email folders and product catalogs. The approaches of hierarchical text classification can be divided in three ways: flat, local and global approaches. The flat approach is traditional multi-class classification in flat fashion without hierarchical class information</context>
</contexts>
<marker>Weigend, Wiener, Pedersen, 1999</marker>
<rawString>A. Weigend, E. Wiener, and J Pedersen. 1999. Exploiting hierarchy in text categorization. In Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Xue</author>
<author>X Liao</author>
<author>L Carin</author>
<author>B Krishnapuram</author>
</authors>
<title>Multi-task learning for classification with dirichlet process priors.</title>
<date>2007</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>8</volume>
<contexts>
<context position="3041" citStr="Xue et al., 2007" startWordPosition="443" endWordPosition="446">n, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories(Sun and Lim, 2001; Liu et al., 2005). The global approach builds only one classifier to discriminate all categories in a hierarchy(Cai and Hofmann, 2004; Rousu et al., 2006; Miao and Qiu, 2009; Qiu et al., 2009). The essential idea of global approach is that the close classes have some common underlying factors. Especially, the descendant classes can share the characteristics of the ancestor classes, which is similar with multi-task learning(Caruana, 1997; Xue et al., 2007). Because the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain. However, the taxonomy is defined artificially and is usually very difficult to organize for large scale taxonomy. The subclasses of the same parent class may be dissimilar and can be grouped in different concepts, so it bring great challenge to hierarchi598 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 598–602, Portland, Oregon, June 19-24, 2011. c�2011 Association for</context>
</contexts>
<marker>Xue, Liao, Carin, Krishnapuram, 2007</marker>
<rawString>Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. 2007. Multi-task learning for classification with dirichlet process priors. The Journal of Machine Learning Research, 8:63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>X Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In Proc. of SIGIR.</booktitle>
<publisher>ACM Press</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1232" citStr="Yang and Liu, 1999" startWordPosition="165" endWordPosition="168">essive (PA) algorithm for hierarchical text classification with latent concepts. Experimental results show that the performance of our algorithm is competitive with the recently proposed hierarchical classification algorithms. 1 Introduction Text classification is a crucial and well-proven method for organizing the collection of large scale documents. The predefined categories are formed by different criterions, e.g. “Entertainment”, “Sports” and “Education” in news classification, “Junk Email” and “Ordinary Email” in email classification. In the literature, many algorithms (Sebastiani, 2002; Yang and Liu, 1999; Yang and Pedersen, 1997) have been proposed, such as Support Vector Machines (SVM), k-Nearest Neighbor (kNN), Naive Bayes (NB) and so on. Empirical evaluations have shown that most of these methods are quite effective in traditional text classification applications. In past serval years, hierarchical text classification has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Different with traditional classification, the document collections are organized as hierarchical class </context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Y. Yang and X. Liu. 1999. A re-examination of text categorization methods. In Proc. of SIGIR. ACM Press New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proc. of Int. Conf. on Mach. Learn. (ICML),</booktitle>
<volume>97</volume>
<contexts>
<context position="1258" citStr="Yang and Pedersen, 1997" startWordPosition="169" endWordPosition="172">m for hierarchical text classification with latent concepts. Experimental results show that the performance of our algorithm is competitive with the recently proposed hierarchical classification algorithms. 1 Introduction Text classification is a crucial and well-proven method for organizing the collection of large scale documents. The predefined categories are formed by different criterions, e.g. “Entertainment”, “Sports” and “Education” in news classification, “Junk Email” and “Ordinary Email” in email classification. In the literature, many algorithms (Sebastiani, 2002; Yang and Liu, 1999; Yang and Pedersen, 1997) have been proposed, such as Support Vector Machines (SVM), k-Nearest Neighbor (kNN), Naive Bayes (NB) and so on. Empirical evaluations have shown that most of these methods are quite effective in traditional text classification applications. In past serval years, hierarchical text classification has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Different with traditional classification, the document collections are organized as hierarchical class structure in many applicat</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Y. Yang and J.O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proc. of Int. Conf. on Mach. Learn. (ICML), volume 97.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>