<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<note confidence="0.3860026">
MT Evaluation: Human-like vs. Human Acceptable
Enrique Amig´o , Jes´us Gim´enez , Julio Gonzalo , and Lluis M`arquez
Departamento de Lenguajes y Sistemas Informiticos
Universidad Nacional de Educaci´on a Distancia
Juan del Rosal, 16, E-28040, Madrid
</note>
<email confidence="0.749323">
enrique,julio @lsi.uned.es
</email>
<address confidence="0.541798333333333">
TALP Research Center, LSI Department
Universitat Polit`ecnica de Catalunya
Jordi Girona Salgado, 1–3, E-08034, Barcelona
</address>
<email confidence="0.995713">
jgimenez,lluism @lsi.upc.edu
</email>
<sectionHeader confidence="0.993755" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993725">
We present a comparative study on Ma-
chine Translation Evaluation according to
two different criteria: Human Likeness
and Human Acceptability. We provide
empirical evidence that there is a relation-
ship between these two kinds of evalu-
ation: Human Likeness implies Human
Acceptability but the reverse is not true.
From the point of view of automatic eval-
uation this implies that metrics based on
Human Likeness are more reliable for sys-
tem tuning.
Our results also show that current evalua-
tion metrics are not always able to distin-
guish between automatic and human trans-
lations. In order to improve the descrip-
tive power of current metrics we propose
the use of additional syntax-based met-
rics, and metric combinations inside the
QARLA Framework.
</bodyText>
<sectionHeader confidence="0.999121" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999841538461539">
Current approaches to Automatic Machine Trans-
lation (MT) Evaluation are mostly based on met-
rics which determine the quality of a given transla-
tion according to its similarity to a given set of ref-
erence translations. The commonly accepted crite-
rion that defines the quality of an evaluation metric
is its level of correlation with human evaluators.
High levels of correlation (Pearson over 0.9) have
been attained at the system level (Eck and Hori,
2005). But this is an average effect: the degree of
correlation achieved at the sentence level, crucial
for an accurate error analysis, is much lower.
We argue that there is two main reasons that ex-
plain this fact:
Firstly, current MT evaluation metrics are based
on shallow features. Most metrics work only at the
lexical level. However, natural languages are rich
and ambiguous, allowing for many possible differ-
ent ways of expressing the same idea. In order to
capture this flexibility, these metrics would require
a combinatorial number of reference translations,
when indeed in most cases only a single reference
is available. Therefore, metrics with higher de-
scriptive power are required.
Secondly, there exists, indeed, two different
evaluation criteria: (i) Human Acceptability, i.e.,
to what extent an automatic translation could be
considered acceptable by humans; and (ii) Human
Likeness, i.e., to what extent an automatic transla-
tion could have been generated by a human trans-
lator. Most approaches to automatic MT evalu-
ation implicitly assume that both criteria should
lead to the same results; but this assumption has
not been proved empirically or even discussed.
In this work, we analyze this issue through em-
pirical evidence. First, in Section 2, we inves-
tigate to what extent current evaluation metrics
are able to distinguish between human and auto-
matic translations (Human Likeness). As individ-
ual metrics do not capture such distinction well, in
Section 3 we study how to improve the descrip-
tive power of current metrics by means of met-
ric combinations inside the QARLA Framework
(Amig´o et al., 2005), including a new family of
metrics based on syntactic criteria. Second, we
claim that the two evaluation criteria (Human Ac-
ceptability and Human Likeness) are indeed of a
different nature, and may lead to different results
(Section 4). However, translations exhibiting a
high level of Human Likeness obtain good results
inhuman judges. Therefore, automatic evaluation
metrics based on similarity to references should be
</bodyText>
<page confidence="0.986262">
17
</page>
<bodyText confidence="0.789125">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 17–24,
Sydney, July 2006. c�2006 Association for Computational Linguistics
☎optimized over their capacity to represent Human
Likeness. See conclusions in Section 5.
</bodyText>
<sectionHeader confidence="0.871424" genericHeader="method">
2 Descriptive Power of Standard Metrics
</sectionHeader>
<bodyText confidence="0.999971">
In this section we perform a simple experiment in
order to measure the descriptive power of current
state-of-the-art metrics, i.e., their ability to capture
the features which characterize human translations
with respect to automatic ones.
</bodyText>
<subsectionHeader confidence="0.987096">
2.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999843269230769">
We use the data from the Openlab 2006 Initiative1
promoted by the TC-STAR Consortium2. This
test suite is entirely based on European Parlia-
ment Proceedings3, covering April 1996 to May
2005. We focus on the Spanish-to-English transla-
tion task. For the purpose of evaluation we use the
development set which consists of 1008 sentences.
However, due to lack of available MT outputs for
the whole set we used only a subset of 504 sen-
tences corresponding to the first half of the devel-
opment set. Three human references per sentence
are available.
We employ ten system outputs; nine are based
on Statistical Machine Translation (SMT) sys-
tems (Gim´enez and M`arquez, 2005; Crego et al.,
2005), and one is obtained from the free Sys-
tran4 on-line rule-based MT engine. Evalua-
tion results have been computed by means of the
IQMT5 Framework for Automatic MT Evaluation
(Gim´enez and Amig´o, 2006).
We have selected a representative set of 22 met-
ric variants corresponding to six different fami-
lies: BLEU (Papineni et al., 2001), NIST (Dodding-
ton, 2002), GTM (Melamed et al., 2003), mPER
(Leusch et al., 2003), mWER (Nießen et al., 2000)
and ROUGE (Lin and Och, 2004a).
</bodyText>
<subsectionHeader confidence="0.999399">
2.2 Measuring Descriptive Power of
Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999363333333333">
Our main assumption is that if an evaluation met-
ric is able to characterize human translations, then,
human references should be closer to each other
than automatic translations to other human refer-
ences. Based on this assumption we introduce two
measures (ORANGE and KING) which analyze
</bodyText>
<footnote confidence="0.985389166666667">
1http://tc-star.itc.it/openlab2006/
2http://www.tc-star.org/
3http://www.europarl.eu.int/
4http://www.systransoft.com.
5The IQMT Framework may be freely downloaded at
http://www.lsi.upc.edu/˜nlp/IQMT.
</footnote>
<bodyText confidence="0.9777285">
the descriptive power of evaluation metrics from
diferent points of view.
</bodyText>
<subsectionHeader confidence="0.703019">
ORANGE Measure
</subsectionHeader>
<bodyText confidence="0.999143133333333">
ORANGE compares automatic and manual
translations one-on-one. Let and be the sets
of automatic and reference translations, respec-
tively, and an evaluation metric which out-
puts the quality of an automatic translation
by comparison to . ORANGE measures the de-
scriptive power as the probability that a human ref-
erence is more similar than an automatic transla-
tion to the rest of human references:
ORANGE was introduced by Lin and Och
(2004b)6 for the meta-evaluation of MT evalua-
tion metrics. The measure provides
information about the average behavior of auto-
matic and manual translations regarding an eval-
uation metric.
</bodyText>
<sectionHeader confidence="0.669191" genericHeader="method">
KING Measure
</sectionHeader>
<bodyText confidence="0.999595583333333">
However, ORANGE does not provide informa-
tion about how many manual translations are dis-
cernible from automatic translations. The
measure complements the ORANGE, tackling
these two issues by universally quantifying on
variable :
KING represents the probability that, for a
given evaluation metric, a human reference is
more similar to the rest of human references than
any automatic translation 7.
KING does not depend on the distribution of
automatic translations, and identifies the cases for
</bodyText>
<footnote confidence="0.927258333333333">
6They defined this measure as the average rank of the ref-
erence translations within the combined machine and refer-
ence translations list.
7Originally KING is defined over the evaluation metric
QUEEN, satisfying some restrictions which are not relevant
in our context (Amig´o et al., 2005).
</footnote>
<page confidence="0.998437">
18
</page>
<bodyText confidence="0.9833488">
●which the given metric has been able to discern
human translations from automatic ones. That
is, it measures how many manual translations
can be used as gold-standard for system evalua-
tion/improvement purposes.
</bodyText>
<subsectionHeader confidence="0.873413">
2.3 Results
</subsectionHeader>
<bodyText confidence="0.996155909090909">
Figure 1 shows the descriptive power, in terms of
the ORANGE and KING measures, over the test
set described in Subsection 2.1.
the manual references; as a result, automatic trans-
lations are closer to each manual summary than
manual summaries to each other (see leftmost rep-
resentation in Figure 2).
In other words, automatic translations tend to
share (lexical) features with most of the refer-
ences, but not to match exactly any of them. This
is a combined effect of:
The nature of MT systems, mostly statisti-
cal, which compute their estimates based on
the number of occurrences of words, tend-
ing to rely more on events that occur more
often. Consequently, automatic translations
typically consist of frequent words, which are
likely to appear in most of the references.
The shallowness of current metrics, which
are not able to identify the common proper-
ties of manual translations with regard to au-
tomatic translations.
</bodyText>
<figureCaption confidence="0.999123333333333">
Figure 1: ORANGE and KING values for standard
metrics.
Figure 2: ORANGE and KING behavior.
</figureCaption>
<sectionHeader confidence="0.953464" genericHeader="method">
ORANGE Results
</sectionHeader>
<bodyText confidence="0.99996725">
All values of the ORANGE measure are lower
than 0.5, which is the ORANGE value that a ran-
dom metric would obtain (see central representa-
tion in Figure 2). This is a rather counterintu-
itive result. A reasonable explanation, however,
is that automatic translations behave as centroids
with respect to human translations, because they
somewhat average the vocabulary distribution in
</bodyText>
<sectionHeader confidence="0.987008" genericHeader="method">
KING Results
</sectionHeader>
<bodyText confidence="0.999892875">
KING values, on the other hand, are slightly
higher than the value that a random metric would
obtain ( ). This means that every stan-
dard metric is able to discriminate a certain num-
ber of manual translations from the set of auto-
matic translations; for instance, GTM-3 identifies
19% of the manual references. For the remain-
ing 81% of the test cases, however, GTM-3 cannot
make the distinction, and therefore cannot be used
to detect and improve weaknesses of the automatic
MT systems.
These results provide an explanation for the
low correlation between automatic evaluation met-
rics and human judgements at the sentence level.
The necessary conclusion is that new metrics with
higher descriptive power are required.
</bodyText>
<sectionHeader confidence="0.952595" genericHeader="method">
3 Improving Descriptive Power
</sectionHeader>
<bodyText confidence="0.999973888888889">
The design of a metric that is able to capture all
the linguistic aspects that distinguish human trans-
lations from automatic ones is a difficult path to
trace. We approach this challenge by following a
‘divide and conquer’ strategy. We suggest to build
a set of specialized similarity metrics devoted to
the evaluation of partial aspects of MT quality.
The challenge is then how to combine a set of sim-
ilarity metrics into a single evaluation measure of
</bodyText>
<page confidence="0.995411">
19
</page>
<bodyText confidence="0.994864">
❚MT quality. The QARLA framework provides a
solution for this challenge.
</bodyText>
<subsectionHeader confidence="0.868201">
3.1 Similarity Metric Combinations inside
QARLA
</subsectionHeader>
<bodyText confidence="0.997097538461538">
The QARLA Framework permits to combine sev-
eral similarity metrics into a single quality mea-
sure (QUEEN). Besides considering the similarity
of automatic translations to human references, the
QUEEN measure additionally considers the distri-
bution of similarities among human references.
The QUEEN measure operates under the as-
sumption that a good translation must be similar
to human references ( ) according to all similar-
ity metrics. QUEEN is defined as the probabil-
ity, over , that for every metric in a
given metric set the automatic translation is
more similar to a human reference than two other
references to each other:
where is the automatic translation being eval-
uated, are three different human refer-
ences in , and stands for the similarity of
to .
In the case of Openlab data, we can count only
on three human references per sentence. In order
to increase the number of samples for QUEEN es-
timation we can use reference similarities
between manual translation pairs from other sen-
tences, assuming that the distances between man-
ual references are relatively stable across exam-
ples.
</bodyText>
<subsectionHeader confidence="0.998552">
3.2 Similarity Metrics
</subsectionHeader>
<bodyText confidence="0.999683538461539">
We begin by defining a set of 22 similarity metrics
taken from the list of standard evaluation metrics
in Subsection 2.1. Evaluation metrics can be tuned
into similarity metrics simply by considering only
one reference when computing its value.
Secondly, we explore the possibility of design-
ing complementary similarity metrics that exploit
linguistic information at levels further than lexi-
cal. Inspired in the work by Liu and Gildea (2005),
who introduced a series of metrics based on con-
stituent/dependency syntactic matching, we have
designed three subgroups of syntactic similarity
metrics. To compute them, we have used the de-
pendency trees provided by the MINIPAR depen-
dency parser (Lin, 1998). These metrics com-
pute the level of word overlapping (unigram preci-
sion/recall) between dependency trees associated
to automatic and reference translations, from three
different points of view:
TREE-X overlapping between the words hanging
from non-terminal nodes of type of the
tree. For instance, the metric TREE PRED re-
flects the proportion of word overlapping be-
tween subtrees of type ‘pred’ (predicate of a
clause).
GRAM-X overlapping between the words with
the grammatical category . For instance,
the metric GRAM A reflects the proportion of
word overlapping between terminal nodes of
type ‘A’ (Adjective/Adverbs).
LEVEL-X overlapping between the words hang-
ing at a certain level of the tree, or deeper.
For instance, LEVEL-1 would consider over-
lapping between all the words in the sen-
tences.
In addition, we also consider three coarser met-
rics, namely TREE, GRAM and LEVEL, which cor-
respond to the average value of the finer metrics
corresponding to each subfamily.
</bodyText>
<subsectionHeader confidence="0.998482">
3.3 Metric Set Selection
</subsectionHeader>
<bodyText confidence="0.999934117647059">
We can compute KING over combinations of
metrics by directly replacing the similarity met-
ric with the QUEEN measure. This cor-
responds exactly to the KING measure used in
QARLA:
KING represents the probability that, for a
given set of human references , and a set of met-
rics , the QUEEN quality of a human reference
is greater than the QUEEN quality of any auto-
matic translation in .
The similarity metrics based on standard evalu-
ation measures together with the two new families
of similarity metrics form a set of 104 metrics. Our
goal is to obtain the subset of metrics with highest
descriptive power; for this, we rely on the KING
probability. A brute force exploration of all possi-
ble metric combinations is not viable. In order to
</bodyText>
<figure confidence="0.919442666666667">
QUEEN
KING
QUEEN QUEEN
</figure>
<page confidence="0.932683">
20
</page>
<bodyText confidence="0.997767">
⑦perform an approximate search for a local maxi-
mum in KING over all the possible metric combi-
nations defined by , we have used the following
greedy heuristic:
</bodyText>
<listItem confidence="0.88601">
1. Individual metrics are ranked by their KING
value.
2. In decreasing rank order, metrics are individ-
ually added to the set of optimal metrics if,
</listItem>
<bodyText confidence="0.908593666666667">
and only if, the global KING is increased.
After applying the algorithm we have obtained
the optimal metric set:
</bodyText>
<construct confidence="0.43823075">
GTM-1, NIST-2, GRAM A, GRAM N,
GRAM AUX, GRAM BE, TREE, TREE AUX,
TREE PNMOD, TREE PRED, TREE REL, TREE S
and TREE WHN
</construct>
<bodyText confidence="0.999950533333333">
which has a KING value of 0.29. This is signif-
icantly higher than the maximum KING obtained
by any individual standard metric (which was 0.19
for GTM-3).
As to the probability ORANGE that a reference
translation attains a higher score than an automatic
translation, this metric set obtains a value of 0.49
vs. 0.42. This means that still the metrics are,
on average, unable to discriminate between human
references and automatic translations. However,
the proportion of sentences for which the metrics
are able to discriminate (KING value) is signifi-
cantly higher.
The metric set with highest descriptive power
contains metrics at different linguistic levels.
For instance, GTM-1 and NIST-2 reward n-gram
matches at the lexical level. GRAM A, GRAM N,
GRAM AUX and GRAM BE capture word overlap-
ping for nouns, auxiliary verbs, adjectives and
adverbs, and auxiliary uses of the verb ‘to be’,
respectively. TREE, TREE AUX, TREE PNMOD,
TREE PRED, TREE REL, TREE S and TREE WHN
reward lexical overlapping over different types of
dependency subtrees: surface subjects, relative
clauses, predicates, auxiliary verbs, postnominal
modifiers, and whn-elements at C-spec positions,
respectively.
These results are a clear indication that features
from several linguistic levels are useful for the
characterization of human translations.
</bodyText>
<sectionHeader confidence="0.951865" genericHeader="method">
4 Human-like vs. Human Acceptable
</sectionHeader>
<bodyText confidence="0.999861333333333">
In this section we analyze the relationship be-
tween the two different kinds of MT evaluation
presented: (i) the ability of MT systems to gen-
erate human-like translations, and (ii) the ability
of MT systems to generate translations that look
acceptable to human judges.
</bodyText>
<subsectionHeader confidence="0.989278">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.9998738">
The ideal test set to study this dichotomy inside
the QARLA Framework would consist of a large
number of human references per sentence, and au-
tomatic outputs generated by heterogeneous MT
systems.
</bodyText>
<subsectionHeader confidence="0.9766115">
4.2 Descriptive Power vs. Correlation with
Human Judgements
</subsectionHeader>
<bodyText confidence="0.998253147058824">
We use the data and results from the IWSLT04
Evaluation Campaign8. We focus on the evalu-
ation of the Chinese-to-English (CE) translation
task, in which a set of 500 short sentences from the
Basic Travel Expressions Corpus (BTEC) were
translated (Akiba et al., 2004). For purposes of au-
tomatic evaluation, 16 reference translations and
outputs by 20 different MT systems are available
for each sentence. Moreover, each of these out-
puts was evaluated by three judges on the basis
of adequacy and fluency (LDC, 2002). In our ex-
periments we consider the sum of adequacy and
fluency assessments.
However, the BTEC corpus has a serious draw-
back: sentences are very short (8 word length in
average). In order to consider a sentence adequate
we are practically forcing it to match exactly some
of the human references. To alleviate this effect
we selected sentences consisting of at least ten
words. A total of 94 sentences (of 13 words length
in average) satisfied this constraint.
Figure 3 shows, for all metrics, the relationship
between the power of characterization of human
references (KING, horizontal axis) and the corre-
lation with human judgements (Pearson correla-
tion, vertical axis). Data are plotted in three differ-
ent groups: original standard metrics, single met-
rics inside QARLA (QUEEN measure), and the
optimal metric combination according to KING.
The optimal set is:
GRAM N, LEVEL 2, LEVEL 4, NIST-1, NIST-
3, NIST-4, and 1-WER
This set suggests that all kinds of n-grams play
an important role in the characterization of human
</bodyText>
<footnote confidence="0.977509">
8http://www.slt.atr.co.jp/IWSLT2004/
</footnote>
<page confidence="0.999404">
21
</page>
<bodyText confidence="0.995395578947368">
❶translations. The metric GRAM N reflects the im-
portance of noun translations. Unlike the Openlab
corpus, levels of the dependency tree (LEVEL 2
and LEVEL 4) are descriptive features, but depen-
dency relations are not (TREE metrics). This is
probably due to the small average sentence length
in IWSLT.
Metrics exhibiting a high level of correlation
outside QARLA, such as NIST-3, also exhibit a
high descriptive power (KING). There is also a
tendency for metrics with a KING value around
0.6 to concentrate at a level of Pearson correlation
around 0.5.
But the main point is the fact that the QUEEN
measure obtained by the metric combination with
highest KING does not yield the highest level of
correlation with human assessments, which is ob-
tained by standard metrics outside QARLA (0.5
vs. 0.7).
</bodyText>
<figureCaption confidence="0.9971794">
Figure 3: Human characterization vs. correlation
with human judgements for IWSLT’04 CE trans-
lation task.
Figure 4: QUEEN values vs. human judgements
for IWSLT’04 CE translation task.
</figureCaption>
<subsectionHeader confidence="0.8518305">
4.3 Human Judgements vs. Similarity to
References
</subsectionHeader>
<bodyText confidence="0.999933184210526">
In order to explain the above results, we have ana-
lyzed the relationship between human assessments
and the QUEEN values obtained by the best com-
bination of metrics for every individual transla-
tion.
Figure 4 shows that high values of QUEEN
(i.e., similarity to references) imply high values
of human judgements. But the reverse is not true.
There are translations acceptable to a human judge
but not similar to human translations according
to QUEEN. This fact can be understood by in-
specting a few particular cases. Table 1 shows
two cases of translations exhibiting a very low
QUEEN value and very high human judgment
score. The two cases present the same kind of
problem: there exists some word or phrase ab-
sent from all human references. In the first exam-
ple, the automatic translation uses the expression
“seats” to make a reservation, where humans in-
variably choose “table”. In the second example,
the automatic translation users “rack” as the place
to put a bag, while humans choose “overhead bin”,
“overhead compartment”, but never “rack”.
Therefore, the QUEEN measure discriminates
these automatic translations regarding to all hu-
man references, thus assigning them a low value.
However, human judges find the translation still
acceptable and informative, although not strictly
human-like.
These results suggest that inside the set of
human acceptable translations, which includes
human-like translations, there is also a subset of
translations unlikely to have been produced by a
human translator. This is a drawback of MT eval-
uation based on human references when the evalu-
ation criteria is Human Acceptability. The good
news are that when Human Likeness increases,
Human Acceptability increases as well.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999924888888889">
We have analyzed the ability of current MT eval-
uation metrics to characterize human translations
(as opposed to automatic translations), and the re-
lationship between MT evaluation based on Hu-
man Acceptability and Human Likeness.
The first conclusion is that, over a limited num-
ber of references, standard metrics are unable to
identify the features that characterize human trans-
lations. Instead, systems behave as centroids with
</bodyText>
<page confidence="0.986674">
22
</page>
<bodyText confidence="0.99995109375">
❷respect to human references. This is due, among
other reasons, to the combined effect of the shal-
lowness of current MT evaluation metrics (mostly
lexical), and the fact that the choice of lexical
items is mostly based on statistical methods. We
suggest two complementary ways of solving this
problem. First, we introduce a new family of
syntax-based metrics covering partial aspects of
MT quality. Second, we use the QARLA Frame-
work to combine multiple metrics into a single
measure of quality. In the future we will study
the design of new metrics working at different lin-
guistic levels. For instance, we are currently de-
veloping a new family of metrics based on shallow
parsing (i.e., part-of-speech, lemma, and chunk in-
formation).
Second, our results suggest that there exists a
clear relation between the two kinds of MT eval-
uation described. While Human Likeness is a
sufficient condition to get Human Acceptability,
Human Acceptability does not guarantee Human
Likeness. Human judges may consider acceptable
automatic translations that would never be gener-
ated by a human translator.
Considering these results, we claim that im-
proving metrics according to their descriptive
power (Human Likeness) is more reliable than
improving metrics based on correlation with hu-
man judges. First, because this correlation is not
granted, since automatic metrics are based on sim-
ilarity to models. Second, because high Human
Likeness ensures high scores from human judges.
</bodyText>
<sectionHeader confidence="0.998518" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999747904109589">
Yasuhiro Akiba, Marcello Federico, Noriko Kando, Hi-
romi Nakaiwa, Michael Paul, and Jun’ichi Tsujii.
2004. Overview of the IWSLT04 Evaluation Cam-
paign. In Proceedings of the International Work-
shop on Spoken Language Translation, pages 1–12,
Kyoto, Japan.
Enrique Amig´o, Julio Gonzalo, Anselmo Pe˜nas, and
Felisa Verdejo. 2005. QARLA: a Framework for
the Evaluation of Automatic Sumarization. In Pro-
ceedings of the 43th Annual Meeting of the Associa-
tion for Computational Linguistics, Michigan, June.
Association for Computational Linguistics.
J.M. Crego, Costa juss`a M.R., J.B. Mari˜no, and Fonol-
losa J.A.R. 2005. Ngram-based versus Phrase-
based Statistical Machine Translation. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Technology (IWSLT’05).
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternation Conference on Human Language Technol-
ogy, pages 138–145.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 Evaluation Campaign. In Proceedings
of the International Workshop on Spoken Language
Translation, Carnegie Mellon University, Pittsburgh,
PA.
Jes´us Gim´enez and Enrique Amig´o. 2006. IQMT:
A Framework for Automatic Machine Translation
Evaluation. In Proceedings of the 5th LREC.
Jes´us Gim´enez and Lluis M`arquez. 2005. Combining
Linguistic Data Views for Phrase-based SMT. In
Proceedings of the Workshop on Building and Using
Parallel Texts, ACL.
LDC. 2002. Linguistic Data Annotation Specification:
Assessment of Fluency and Adequacy in Chinese-
English Translations Revision 1.0. Technical
report, Linguistic Data Consortium. http://-
www.ldc.upenn.edu/Projects/TIDES/Translation/-
TransAssess02.pdf.
G. Leusch, N. Ueffing, and H. Ney. 2003. A Novel
String-to-String Distance Measure with Applica-
tions to Machine Translation Evaluation. In Pro-
ceedings ofMT Summit IX.
Chin-Yew Lin and Franz Josef Och. 2004a. Au-
tomatic Evaluation of Machine Translation Qual-
ity Using Longest Common Subsequence and Skip-
Bigram Statics. In Proceedings ofACL.
Chin-Yew Lin and Franz Josef Och. 2004b. OR-
ANGE: a Method for Evaluating Automatic Evalu-
ation Metrics for Machine Translation. In Proceed-
ings of COLING.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Proceedings of the Workshop on the
Evaluation of Parsing Systems.
Ding Liu and Daniel Gildea. 2005. Syntactic Fea-
tures for Evaluation of Machine Translation. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation.
In Proceedings ofHLT/NAACL.
S. Nießen, F.J. Och, G. Leusch, and H. Ney. 2000.
Evaluation Tool for Machine Translation: Fast Eval-
uation for MT Research. In Proceedings of the 2nd
International Conference on Language Resources
and Evaluation.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation, IBM Research Re-
port, RC22176. Technical report, IBM T.J. Watson
Research Center.
</reference>
<page confidence="0.99966">
23
</page>
<tableCaption confidence="0.999605">
Table 1: Automatic translations with high score in human judgements and low QUEEN value.
</tableCaption>
<bodyText confidence="0.815129238095238">
Automatic
Translation:
my name is endo i ’ve reserved seats for nine o’clock
this is endo i booked a table at nine o’clock
i reserved a table for nine o’clock and my name is endo
my name is endo and i made a reservation for a table at nine o’clock
i am endo and i have a reservation for a table at nine pm
my name is endo and i booked a table at nine o’clock
this is endo i reserved a table for nine o’clock
my name is endo and i reserved a table with you for nine o’clock
i ’ve booked a table under endo for nine o’clock
my name is endo and i have a table reserved for nine o’clock
i ’m endo and i have a reservation for a table at nine o’clock
my name is endo and i reserved a table for nine o’clock
the name is endo and i have a reservation for nine
i have a table reserved for nine under the name of endo
hello my name is endo i reserved a table for nine o’clock
my name is endo and i have a table reserved for nine o’clock
my name is endo and i made a reservation for nine o’clock
Human
Reference 1:
Automatic
Translation:
could you help me put my bag on the rack please
could you help me put my bag in the overhead bin
can you help me to get my bag into the overhead bin
would you give me a hand with getting my bag into the overhead bin
would you mind assisting me to put my bag into the overhead bin
could you give me a hand putting my bag in the overhead compartment
please help me put my bag in the overhead bin
would you mind helping me put my bag in the overhead compartment
do you mind helping me put my bag in the overhead compartment
could i get a hand with putting my bag in the overhead compartment
could i ask you to help me put my bag in the overhead compartment
please help me put my bag in the overhead bin
would you mind helping me put my bag in the overhead compartment
i ’d like you to help me put my bag in the overhead compartment
would you mind helping get my bag up into the overhead storage compartment
may i get some assistance getting my bag into the overhead storage compartment
please help me put my into the overhead storage compartment
Human
Reference 1:
</bodyText>
<page confidence="0.993012">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.439962">
<title confidence="0.991896">MT Evaluation: Human-like vs. Human Acceptable</title>
<author confidence="0.992753">Julio Gonzalo</author>
<affiliation confidence="0.9380535">Departamento de Lenguajes y Sistemas Informiticos Universidad Nacional de Educaci´on a Distancia</affiliation>
<address confidence="0.802704">Juan del Rosal, 16, E-28040, Madrid</address>
<email confidence="0.803101">enrique,julio@lsi.uned.es</email>
<affiliation confidence="0.9914185">TALP Research Center, LSI Department Universitat Polit`ecnica de Catalunya</affiliation>
<address confidence="0.98632">Jordi Girona Salgado, 1–3, E-08034, Barcelona</address>
<email confidence="0.999545">jgimenez,lluism@lsi.upc.edu</email>
<abstract confidence="0.983418666666667">We present a comparative study on Machine Translation Evaluation according to two different criteria: Human Likeness and Human Acceptability. We provide empirical evidence that there is a relationship between these two kinds of evaluation: Human Likeness implies Human Acceptability but the reverse is not true. From the point of view of automatic evaluation this implies that metrics based on Human Likeness are more reliable for system tuning. Our results also show that current evaluation metrics are not always able to distinguish between automatic and human translations. In order to improve the descriptive power of current metrics we propose the use of additional syntax-based metrics, and metric combinations inside the QARLA Framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasuhiro Akiba</author>
<author>Marcello Federico</author>
<author>Noriko Kando</author>
<author>Hiromi Nakaiwa</author>
<author>Michael Paul</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of the IWSLT04 Evaluation Campaign.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>1--12</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="16704" citStr="Akiba et al., 2004" startWordPosition="2650" endWordPosition="2653"> systems to generate translations that look acceptable to human judges. 4.1 Experimental Setting The ideal test set to study this dichotomy inside the QARLA Framework would consist of a large number of human references per sentence, and automatic outputs generated by heterogeneous MT systems. 4.2 Descriptive Power vs. Correlation with Human Judgements We use the data and results from the IWSLT04 Evaluation Campaign8. We focus on the evaluation of the Chinese-to-English (CE) translation task, in which a set of 500 short sentences from the Basic Travel Expressions Corpus (BTEC) were translated (Akiba et al., 2004). For purposes of automatic evaluation, 16 reference translations and outputs by 20 different MT systems are available for each sentence. Moreover, each of these outputs was evaluated by three judges on the basis of adequacy and fluency (LDC, 2002). In our experiments we consider the sum of adequacy and fluency assessments. However, the BTEC corpus has a serious drawback: sentences are very short (8 word length in average). In order to consider a sentence adequate we are practically forcing it to match exactly some of the human references. To alleviate this effect we selected sentences consist</context>
</contexts>
<marker>Akiba, Federico, Kando, Nakaiwa, Paul, Tsujii, 2004</marker>
<rawString>Yasuhiro Akiba, Marcello Federico, Noriko Kando, Hiromi Nakaiwa, Michael Paul, and Jun’ichi Tsujii. 2004. Overview of the IWSLT04 Evaluation Campaign. In Proceedings of the International Workshop on Spoken Language Translation, pages 1–12, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
<author>Anselmo Pe˜nas</author>
<author>Felisa Verdejo</author>
</authors>
<title>QARLA: a Framework for the Evaluation of Automatic Sumarization.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Michigan,</location>
<marker>Amig´o, Gonzalo, Pe˜nas, Verdejo, 2005</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Anselmo Pe˜nas, and Felisa Verdejo. 2005. QARLA: a Framework for the Evaluation of Automatic Sumarization. In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>Costa juss`a M R</author>
<author>J B Mari˜no</author>
<author>J A R Fonollosa</author>
</authors>
<title>Ngram-based versus Phrasebased Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Technology (IWSLT’05).</booktitle>
<marker>Crego, R, Mari˜no, Fonollosa, 2005</marker>
<rawString>J.M. Crego, Costa juss`a M.R., J.B. Mari˜no, and Fonollosa J.A.R. 2005. Ngram-based versus Phrasebased Statistical Machine Translation. In Proceedings of the International Workshop on Spoken Language Technology (IWSLT’05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd Internation Conference on Human Language Technology,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="5278" citStr="Doddington, 2002" startWordPosition="824" endWordPosition="826"> sentences corresponding to the first half of the development set. Three human references per sentence are available. We employ ten system outputs; nine are based on Statistical Machine Translation (SMT) systems (Gim´enez and M`arquez, 2005; Crego et al., 2005), and one is obtained from the free Systran4 on-line rule-based MT engine. Evaluation results have been computed by means of the IQMT5 Framework for Automatic MT Evaluation (Gim´enez and Amig´o, 2006). We have selected a representative set of 22 metric variants corresponding to six different families: BLEU (Papineni et al., 2001), NIST (Doddington, 2002), GTM (Melamed et al., 2003), mPER (Leusch et al., 2003), mWER (Nießen et al., 2000) and ROUGE (Lin and Och, 2004a). 2.2 Measuring Descriptive Power of Evaluation Metrics Our main assumption is that if an evaluation metric is able to characterize human translations, then, human references should be closer to each other than automatic translations to other human references. Based on this assumption we introduce two measures (ORANGE and KING) which analyze 1http://tc-star.itc.it/openlab2006/ 2http://www.tc-star.org/ 3http://www.europarl.eu.int/ 4http://www.systransoft.com. 5The IQMT Framework ma</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics. In Proceedings of the 2nd Internation Conference on Human Language Technology, pages 138–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Chiori Hori</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1650" citStr="Eck and Hori, 2005" startWordPosition="250" endWordPosition="253">e the descriptive power of current metrics we propose the use of additional syntax-based metrics, and metric combinations inside the QARLA Framework. 1 Introduction Current approaches to Automatic Machine Translation (MT) Evaluation are mostly based on metrics which determine the quality of a given translation according to its similarity to a given set of reference translations. The commonly accepted criterion that defines the quality of an evaluation metric is its level of correlation with human evaluators. High levels of correlation (Pearson over 0.9) have been attained at the system level (Eck and Hori, 2005). But this is an average effect: the degree of correlation achieved at the sentence level, crucial for an accurate error analysis, is much lower. We argue that there is two main reasons that explain this fact: Firstly, current MT evaluation metrics are based on shallow features. Most metrics work only at the lexical level. However, natural languages are rich and ambiguous, allowing for many possible different ways of expressing the same idea. In order to capture this flexibility, these metrics would require a combinatorial number of reference translations, when indeed in most cases only a sing</context>
</contexts>
<marker>Eck, Hori, 2005</marker>
<rawString>Matthias Eck and Chiori Hori. 2005. Overview of the IWSLT 2005 Evaluation Campaign. In Proceedings of the International Workshop on Spoken Language Translation, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Enrique Amig´o</author>
</authors>
<title>IQMT: A Framework for Automatic Machine Translation Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th LREC.</booktitle>
<marker>Gim´enez, Amig´o, 2006</marker>
<rawString>Jes´us Gim´enez and Enrique Amig´o. 2006. IQMT: A Framework for Automatic Machine Translation Evaluation. In Proceedings of the 5th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Combining Linguistic Data Views for Phrase-based SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Building and Using Parallel Texts, ACL.</booktitle>
<marker>Gim´enez, M`arquez, 2005</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2005. Combining Linguistic Data Views for Phrase-based SMT. In Proceedings of the Workshop on Building and Using Parallel Texts, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC</author>
</authors>
<title>Linguistic Data Annotation Specification: Assessment of Fluency and Adequacy in ChineseEnglish Translations Revision 1.0. Technical report, Linguistic Data Consortium.</title>
<date>2002</date>
<note>http://-www.ldc.upenn.edu/Projects/TIDES/Translation/-TransAssess02.pdf.</note>
<contexts>
<context position="16952" citStr="LDC, 2002" startWordPosition="2693" endWordPosition="2694">nerated by heterogeneous MT systems. 4.2 Descriptive Power vs. Correlation with Human Judgements We use the data and results from the IWSLT04 Evaluation Campaign8. We focus on the evaluation of the Chinese-to-English (CE) translation task, in which a set of 500 short sentences from the Basic Travel Expressions Corpus (BTEC) were translated (Akiba et al., 2004). For purposes of automatic evaluation, 16 reference translations and outputs by 20 different MT systems are available for each sentence. Moreover, each of these outputs was evaluated by three judges on the basis of adequacy and fluency (LDC, 2002). In our experiments we consider the sum of adequacy and fluency assessments. However, the BTEC corpus has a serious drawback: sentences are very short (8 word length in average). In order to consider a sentence adequate we are practically forcing it to match exactly some of the human references. To alleviate this effect we selected sentences consisting of at least ten words. A total of 94 sentences (of 13 words length in average) satisfied this constraint. Figure 3 shows, for all metrics, the relationship between the power of characterization of human references (KING, horizontal axis) and th</context>
</contexts>
<marker>LDC, 2002</marker>
<rawString>LDC. 2002. Linguistic Data Annotation Specification: Assessment of Fluency and Adequacy in ChineseEnglish Translations Revision 1.0. Technical report, Linguistic Data Consortium. http://-www.ldc.upenn.edu/Projects/TIDES/Translation/-TransAssess02.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leusch</author>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>A Novel String-to-String Distance Measure with Applications to Machine Translation Evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings ofMT Summit IX.</booktitle>
<contexts>
<context position="5334" citStr="Leusch et al., 2003" startWordPosition="833" endWordPosition="836">elopment set. Three human references per sentence are available. We employ ten system outputs; nine are based on Statistical Machine Translation (SMT) systems (Gim´enez and M`arquez, 2005; Crego et al., 2005), and one is obtained from the free Systran4 on-line rule-based MT engine. Evaluation results have been computed by means of the IQMT5 Framework for Automatic MT Evaluation (Gim´enez and Amig´o, 2006). We have selected a representative set of 22 metric variants corresponding to six different families: BLEU (Papineni et al., 2001), NIST (Doddington, 2002), GTM (Melamed et al., 2003), mPER (Leusch et al., 2003), mWER (Nießen et al., 2000) and ROUGE (Lin and Och, 2004a). 2.2 Measuring Descriptive Power of Evaluation Metrics Our main assumption is that if an evaluation metric is able to characterize human translations, then, human references should be closer to each other than automatic translations to other human references. Based on this assumption we introduce two measures (ORANGE and KING) which analyze 1http://tc-star.itc.it/openlab2006/ 2http://www.tc-star.org/ 3http://www.europarl.eu.int/ 4http://www.systransoft.com. 5The IQMT Framework may be freely downloaded at http://www.lsi.upc.edu/˜nlp/IQ</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2003</marker>
<rawString>G. Leusch, N. Ueffing, and H. Ney. 2003. A Novel String-to-String Distance Measure with Applications to Machine Translation Evaluation. In Proceedings ofMT Summit IX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and SkipBigram Statics.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="5391" citStr="Lin and Och, 2004" startWordPosition="844" endWordPosition="847">ble. We employ ten system outputs; nine are based on Statistical Machine Translation (SMT) systems (Gim´enez and M`arquez, 2005; Crego et al., 2005), and one is obtained from the free Systran4 on-line rule-based MT engine. Evaluation results have been computed by means of the IQMT5 Framework for Automatic MT Evaluation (Gim´enez and Amig´o, 2006). We have selected a representative set of 22 metric variants corresponding to six different families: BLEU (Papineni et al., 2001), NIST (Doddington, 2002), GTM (Melamed et al., 2003), mPER (Leusch et al., 2003), mWER (Nießen et al., 2000) and ROUGE (Lin and Och, 2004a). 2.2 Measuring Descriptive Power of Evaluation Metrics Our main assumption is that if an evaluation metric is able to characterize human translations, then, human references should be closer to each other than automatic translations to other human references. Based on this assumption we introduce two measures (ORANGE and KING) which analyze 1http://tc-star.itc.it/openlab2006/ 2http://www.tc-star.org/ 3http://www.europarl.eu.int/ 4http://www.systransoft.com. 5The IQMT Framework may be freely downloaded at http://www.lsi.upc.edu/˜nlp/IQMT. the descriptive power of evaluation metrics from dife</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004a. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and SkipBigram Statics. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="5391" citStr="Lin and Och, 2004" startWordPosition="844" endWordPosition="847">ble. We employ ten system outputs; nine are based on Statistical Machine Translation (SMT) systems (Gim´enez and M`arquez, 2005; Crego et al., 2005), and one is obtained from the free Systran4 on-line rule-based MT engine. Evaluation results have been computed by means of the IQMT5 Framework for Automatic MT Evaluation (Gim´enez and Amig´o, 2006). We have selected a representative set of 22 metric variants corresponding to six different families: BLEU (Papineni et al., 2001), NIST (Doddington, 2002), GTM (Melamed et al., 2003), mPER (Leusch et al., 2003), mWER (Nießen et al., 2000) and ROUGE (Lin and Och, 2004a). 2.2 Measuring Descriptive Power of Evaluation Metrics Our main assumption is that if an evaluation metric is able to characterize human translations, then, human references should be closer to each other than automatic translations to other human references. Based on this assumption we introduce two measures (ORANGE and KING) which analyze 1http://tc-star.itc.it/openlab2006/ 2http://www.tc-star.org/ 3http://www.europarl.eu.int/ 4http://www.systransoft.com. 5The IQMT Framework may be freely downloaded at http://www.lsi.upc.edu/˜nlp/IQMT. the descriptive power of evaluation metrics from dife</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on the Evaluation of Parsing Systems.</booktitle>
<contexts>
<context position="12229" citStr="Lin, 1998" startWordPosition="1923" endWordPosition="1924">tion metrics in Subsection 2.1. Evaluation metrics can be tuned into similarity metrics simply by considering only one reference when computing its value. Secondly, we explore the possibility of designing complementary similarity metrics that exploit linguistic information at levels further than lexical. Inspired in the work by Liu and Gildea (2005), who introduced a series of metrics based on constituent/dependency syntactic matching, we have designed three subgroups of syntactic similarity metrics. To compute them, we have used the dependency trees provided by the MINIPAR dependency parser (Lin, 1998). These metrics compute the level of word overlapping (unigram precision/recall) between dependency trees associated to automatic and reference translations, from three different points of view: TREE-X overlapping between the words hanging from non-terminal nodes of type of the tree. For instance, the metric TREE PRED reflects the proportion of word overlapping between subtrees of type ‘pred’ (predicate of a clause). GRAM-X overlapping between the words with the grammatical category . For instance, the metric GRAM A reflects the proportion of word overlapping between terminal nodes of type ‘A’</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based Evaluation of MINIPAR. In Proceedings of the Workshop on the Evaluation of Parsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="11970" citStr="Liu and Gildea (2005)" startWordPosition="1881" endWordPosition="1884">larities between manual translation pairs from other sentences, assuming that the distances between manual references are relatively stable across examples. 3.2 Similarity Metrics We begin by defining a set of 22 similarity metrics taken from the list of standard evaluation metrics in Subsection 2.1. Evaluation metrics can be tuned into similarity metrics simply by considering only one reference when computing its value. Secondly, we explore the possibility of designing complementary similarity metrics that exploit linguistic information at levels further than lexical. Inspired in the work by Liu and Gildea (2005), who introduced a series of metrics based on constituent/dependency syntactic matching, we have designed three subgroups of syntactic similarity metrics. To compute them, we have used the dependency trees provided by the MINIPAR dependency parser (Lin, 1998). These metrics compute the level of word overlapping (unigram precision/recall) between dependency trees associated to automatic and reference translations, from three different points of view: TREE-X overlapping between the words hanging from non-terminal nodes of type of the tree. For instance, the metric TREE PRED reflects the proporti</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph P Turian</author>
</authors>
<title>Precision and Recall of Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<contexts>
<context position="5306" citStr="Melamed et al., 2003" startWordPosition="828" endWordPosition="831"> to the first half of the development set. Three human references per sentence are available. We employ ten system outputs; nine are based on Statistical Machine Translation (SMT) systems (Gim´enez and M`arquez, 2005; Crego et al., 2005), and one is obtained from the free Systran4 on-line rule-based MT engine. Evaluation results have been computed by means of the IQMT5 Framework for Automatic MT Evaluation (Gim´enez and Amig´o, 2006). We have selected a representative set of 22 metric variants corresponding to six different families: BLEU (Papineni et al., 2001), NIST (Doddington, 2002), GTM (Melamed et al., 2003), mPER (Leusch et al., 2003), mWER (Nießen et al., 2000) and ROUGE (Lin and Och, 2004a). 2.2 Measuring Descriptive Power of Evaluation Metrics Our main assumption is that if an evaluation metric is able to characterize human translations, then, human references should be closer to each other than automatic translations to other human references. Based on this assumption we introduce two measures (ORANGE and KING) which analyze 1http://tc-star.itc.it/openlab2006/ 2http://www.tc-star.org/ 3http://www.europarl.eu.int/ 4http://www.systransoft.com. 5The IQMT Framework may be freely downloaded at ht</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, and Joseph P. Turian. 2003. Precision and Recall of Machine Translation. In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nießen</author>
<author>F J Och</author>
<author>G Leusch</author>
<author>H Ney</author>
</authors>
<title>Evaluation Tool for Machine Translation: Fast Evaluation for MT Research.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="5362" citStr="Nießen et al., 2000" startWordPosition="838" endWordPosition="841">ferences per sentence are available. We employ ten system outputs; nine are based on Statistical Machine Translation (SMT) systems (Gim´enez and M`arquez, 2005; Crego et al., 2005), and one is obtained from the free Systran4 on-line rule-based MT engine. Evaluation results have been computed by means of the IQMT5 Framework for Automatic MT Evaluation (Gim´enez and Amig´o, 2006). We have selected a representative set of 22 metric variants corresponding to six different families: BLEU (Papineni et al., 2001), NIST (Doddington, 2002), GTM (Melamed et al., 2003), mPER (Leusch et al., 2003), mWER (Nießen et al., 2000) and ROUGE (Lin and Och, 2004a). 2.2 Measuring Descriptive Power of Evaluation Metrics Our main assumption is that if an evaluation metric is able to characterize human translations, then, human references should be closer to each other than automatic translations to other human references. Based on this assumption we introduce two measures (ORANGE and KING) which analyze 1http://tc-star.itc.it/openlab2006/ 2http://www.tc-star.org/ 3http://www.europarl.eu.int/ 4http://www.systransoft.com. 5The IQMT Framework may be freely downloaded at http://www.lsi.upc.edu/˜nlp/IQMT. the descriptive power of</context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>S. Nießen, F.J. Och, G. Leusch, and H. Ney. 2000. Evaluation Tool for Machine Translation: Fast Evaluation for MT Research. In Proceedings of the 2nd International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation,</title>
<date>2001</date>
<journal>IBM Research</journal>
<tech>Report, RC22176. Technical report, IBM</tech>
<institution>T.J. Watson Research Center.</institution>
<contexts>
<context position="5253" citStr="Papineni et al., 2001" startWordPosition="819" endWordPosition="822">t we used only a subset of 504 sentences corresponding to the first half of the development set. Three human references per sentence are available. We employ ten system outputs; nine are based on Statistical Machine Translation (SMT) systems (Gim´enez and M`arquez, 2005; Crego et al., 2005), and one is obtained from the free Systran4 on-line rule-based MT engine. Evaluation results have been computed by means of the IQMT5 Framework for Automatic MT Evaluation (Gim´enez and Amig´o, 2006). We have selected a representative set of 22 metric variants corresponding to six different families: BLEU (Papineni et al., 2001), NIST (Doddington, 2002), GTM (Melamed et al., 2003), mPER (Leusch et al., 2003), mWER (Nießen et al., 2000) and ROUGE (Lin and Och, 2004a). 2.2 Measuring Descriptive Power of Evaluation Metrics Our main assumption is that if an evaluation metric is able to characterize human translations, then, human references should be closer to each other than automatic translations to other human references. Based on this assumption we introduce two measures (ORANGE and KING) which analyze 1http://tc-star.itc.it/openlab2006/ 2http://www.tc-star.org/ 3http://www.europarl.eu.int/ 4http://www.systransoft.co</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation, IBM Research Report, RC22176. Technical report, IBM T.J. Watson Research Center.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>