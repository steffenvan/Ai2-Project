<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000973">
<title confidence="0.982682">
Cross-Domain Co-Extraction of Sentiment and Topic Lexicons
</title>
<author confidence="0.995523">
Fangtao Li§, Sinno Jialin Pant, Ou Jing, Qiang Yangt and Xiaoyan Zhu§
</author>
<affiliation confidence="0.998782">
§Department of Computer Science and Technology, Tsinghua University, Beijing, China
</affiliation>
<email confidence="0.8994">
§{fangtao06@gmail.com, zxy-dcs@tsinghua.edu.cn}
</email>
<affiliation confidence="0.449758">
tInstitute for Infocomm Research, Singapore
</affiliation>
<email confidence="0.884892">
tjspan@i2r.a-star.edu.sg
</email>
<affiliation confidence="0.785248">
tHong Kong University of Science and Technology, Hong Kong, China
</affiliation>
<email confidence="0.992986">
t{kingomiga@gmail.com, qyang@cse.ust.hk}
</email>
<sectionHeader confidence="0.993831" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999856272727273">
Extracting sentiment and topic lexicons is im-
portant for opinion mining. Previous works
have showed that supervised learning methods
are superior for this task. However, the perfor-
mance of supervised methods highly relies on
manually labeled training data. In this paper,
we propose a domain adaptation framework
for sentiment- and topic- lexicon co-extraction
in a domain of interest where we do not re-
quire any labeled data, but have lots of labeled
data in another related domain. The frame-
work is twofold. In the first step, we gener-
ate a few high-confidence sentiment and topic
seeds in the target domain. In the second
step, we propose a novel Relational Adaptive
bootstraPping (RAP) algorithm to expand the
seeds in the target domain by exploiting the
labeled source domain data and the relation-
ships between topic and sentiment words. Ex-
perimental results show that our domain adap-
tation framework can extract precise lexicons
in the target domain without any annotation.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985816326531">
In the past few years, opinion mining and senti-
ment analysis have attracted much attention in Natu-
ral Language Processing (NLP) and Information Re-
trieval (IR) (Pang and Lee, 2008; Liu, 2010). Senti-
ment lexicon construction and topic lexicon extrac-
tion are two fundamental subtasks for opinion min-
ing (Qiu et al., 2009). A sentiment lexicon is a list
of sentiment expressions, which are used to indicate
sentiment polarity (e.g., positive or negative). The
sentiment lexicon is domain dependent as users may
use different sentiment words to express their opin-
ion in different domains (e.g., different products). A
topic lexicon is a list of topic expressions, on which
the sentiment words are expressed. Extracting the
topic lexicon from a specific domain is important
because users not only care about the overall senti-
ment polarity of a review but also care about which
aspects are mentioned in review. Note that, similar
to sentiment lexicons, different domains may have
very different topic lexicons.
Recently, Jin and Ho (2009) and Li et al. (2010a)
showed that supervised learning methods can
achieve state-of-the-art results for lexicon extrac-
tion. However, the performance of these meth-
ods highly relies on manually annotated training
data. In most cases, the labeling work may be time-
consuming and expensive. It is impossible to anno-
tate each domain of interest to build precise domain-
dependent lexicons. It is more desirable to automat-
ically construct precise lexicons in domains of inter-
est by transferring knowledge from other domains.
In this paper, we focus on the co-extraction task
of sentiment and topic lexicons in a target domain
where we do not have any labeled data, but have
plenty of labeled data in a source domain. Our
goal is to leverage the knowledge extracted from the
source domain to help lexicon co-extraction in the
target domain. To address this problem, we propose
a two-stage domain adaptation method. In the first
step, we build a bridge between the source and tar-
get domains by identifying some common sentiment
words as sentiment seeds in the target domain, such
as “good”, “bad”, “nice”, etc. After that, we gener-
ate topic seeds in the target domain by mining some
general syntactic relation patterns between the sen-
timent and topic words from the source domain. In
the second step, we propose a Relational Adaptive
bootstraPping (RAP) algorithm to expand the seeds
in the target domain. Our proposed method can uti-
</bodyText>
<page confidence="0.963094">
410
</page>
<note confidence="0.9858255">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 410–419,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999441785714286">
lize useful labeled data from the source domain as
well as exploit the relationships between the topic
and sentiment words to propagate information for
lexicon construction in the target domain. Experi-
mental results show that our proposed method is ef-
fective for cross-domain lexicon co-extraction.
In summary, we have three main contributions: 1)
We give a systematic study on cross-domain senti-
ment analysis in word level. While, most of previous
work focused on document level; 2) A new two-step
domain adaptation framework, with a novel RAP al-
gorithm for seed expansion, is proposed. 3) We con-
duct extensive evaluation, and the experimental re-
sults demonstrate the effectiveness of our methods.
</bodyText>
<sectionHeader confidence="0.999924" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.997159">
2.1 Sentiment or Topic Lexicon Extraction
</subsectionHeader>
<bodyText confidence="0.994293303030303">
Sentiment or topic lexicon extraction is to iden-
tify the sentiment or topic words from text. In the
past, many machine learning techniques have been
proposed for this task. Hu and Liu et al. (2004)
proposed an association-rule-based method to ex-
tract topic words and a dictionary-based method to
identify sentiment words, independently. Wiebe et
al. (2004) and Rioff et al. (2003) proposed to
identify subjective adjectives and nouns using word
clustering based on their distributional similarity.
Popescu and Etzioni (2005) proposed a relaxed la-
beling approach to utilize linguistic rules for opinion
polarity detection. Some researchers also proposed
to use topic modeling to identify implicit topics and
sentiment words (Mei et al., 2007; Titov and Mc-
Donald, 2008; Zhao et al., 2010; Li et al., 2010b),
where a topic is a cluster of words, which is differ-
ent from our fine-grained topic-word extraction.
Jin and Ho (2009) and Li et al. (2010a) both pro-
posed to use supervised sequential labeling methods
for topic and opinion extraction. Experimental re-
sults showed that the supervised learning methods
can achieve state-of-the-art performance on lexicon
extraction. However, these methods need to manu-
ally annotate a lot of training data in each domain.
Recently, Qiu et al. (2009) proposed a rule-based
semi-supervised learning methods for lexicon ex-
traction. However, their method requires to manu-
ally define some general syntactic rules among sen-
timent and topic words. In addition, it still requires
some annotated words in the target domain. In this
paper, we do not assume any predefined rules and
labeled data be available in the target domain.
</bodyText>
<subsectionHeader confidence="0.996905">
2.2 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999978172413793">
Domain adaptation aims at transferring knowledge
across domains where data distributions may be dif-
ferent (Pan and Yang, 2010). In the past few years,
domain adaptation techniques have been widely ap-
plied to various NLP tasks, such as part-of-speech
tagging (Ando and Zhang, 2005; Jiang and Zhai,
2007; Daum´e III, 2007), named-entity recognition
and shallow parsing (Daum´e III, 2007; Jiang and
Zhai, 2007; Wu et al., 2009). There are also
lots of studies for cross-domain sentiment analy-
sis (Blitzer et al., 2007; Tan et al., 2007; Li et al.,
2009; Pan et al., 2010; Bollegala et al., 2011; He
et al., 2011; Glorot et al., 2011). However, most
of them focused on coarse-grained document-level
sentiment classification, which is different from our
fine-grained word-level extraction. Our work is sim-
ilar to Jakob and Gurevych (2010) which proposed a
Conditional Random Field (CRF) for cross-domain
topic word extraction. However, the performance
of their method highly depends on the manually de-
signed features. In our experiments, we compare our
method with theirs, and find that ours can achieve
much better results on cross-domain lexicon extrac-
tion. Note that our work is also different from a re-
cent work (Du et al., 2010), which focused on identi-
fying the polarity of adjective words by using cross-
domain knowledge. While we extract both topic and
sentiment words and allow non-adjective sentiment
words, which is more practical.
</bodyText>
<sectionHeader confidence="0.998677" genericHeader="method">
3 Cross-Domain Lexicon Co-Extraction
</sectionHeader>
<subsectionHeader confidence="0.999863">
3.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.999062625">
Recall that, we focus on the setting where we have
no labeled data in the target domain, while we have
plenty of labeled data in the source domain. De-
note DS = {(wSi, ySi)}n�
i�1 the source domain data,
where wSi represents a word in the source domain.
ySi ∈ Y is the corresponding label of wSi. Simi-
larly, we denote DT = {wTj}n2
j�1 the target domain
data, where the input wTj is a word in the target do-
main. In lexicon extraction, Y ∈ {1, 2, 3}, where
yi = 1 denotes the corresponding word wi a sen-
timent word, yi = 2 denotes wi a topic word, and
yi = 3 denotes wi neither a sentiment nor topic
word. Our goal is to predict labels on DT to extract
topic and sentiment words for constructing topic and
</bodyText>
<page confidence="0.996761">
411
</page>
<bodyText confidence="0.774343">
sentiment lexicons, respectively.
</bodyText>
<subsectionHeader confidence="0.999867">
3.2 Motivating Examples
</subsectionHeader>
<bodyText confidence="0.997803666666667">
In this section, we use some examples to introduce
the motivation behind our proposed method. Table 1
shows several reviews from two domains: movie and
camera. From the table, we can observe that there
are some common sentiment words across different
domains, such as “great”, “excellent” and “amaz-
ing”. However, the topic words may be different.
For example, in the movie domain, topic words in-
clude “movie” and “script”. While in the camera do-
main, topic words include “camera” and “photos”.
Domain Review
camera The camera is great.
it is a very amazing product.
i highly recommend this camera.
takes excellent photos.
photos had some artifacts and noise.
movie This movie has good script, great
casting, excellent acting.
I love this movie.
Godfather was the most amazing movie.
The movie is excellent.
</bodyText>
<tableCaption confidence="0.9909455">
Table 1: Reviews in camera and movie domains. Bold-
faces are topic words and Italics are sentiment words.
</tableCaption>
<bodyText confidence="0.999824068965517">
Based on the observations, we can build a connec-
tion between the source and target domains by iden-
tifying the common sentiment words. Furthermore,
intuitively, there are some general syntactic relation-
ships or patterns between topic and sentiment words
across different domains. Therefore, if we can mine
the patterns from the source and target domain data,
then we are able to construct an indirect connection
between topic words across domains by using the
common sentiment words as a bridge, which makes
knowledge transfer across domains possible.
Figure 1 shows two dependency trees for the sen-
tence “the camera is great” in the camera domain
and the sentence “the movie is excellent” in the
movie domain, respectively. As can be observed, the
relationships between the topic and sentiment words
in the two sentences are the same. They both share
a “TOPIC-nsubj-SENTIMENT” relation. Let the
camera domain be the source domain and the movie
domain be the target domain. If the word “excel-
lent” is identified as a common sentiment word, and
the “TOPIC-nsubj-SENTIMENT” relation extracted
from the camera domain is recognized as a common
syntactic pattern, then the word “movie” can be pre-
dicted as a topic word in the movie domain with high
probability. After new topic words are extracted in
the movie domain, we can apply the same syntac-
tic pattern or other syntactic patterns to extract new
sentiment and topic words iteratively.
</bodyText>
<figureCaption confidence="0.999656">
Figure 1: Examples of dependency tree structure.
</figureCaption>
<bodyText confidence="0.999880666666667">
More specifically, we use the shortest path be-
tween a topic word and a sentiment word in the cor-
responding dependency tree to denote the relation
between them. To get more general paths, we do
not take original words in the path into considera-
tion, but use their POS tags instead, such as “NN”,
“VB”, “JJ”, etc. As an example shown in Figure 2,
we can extract two paths or relationships between
topic and sentiment words from the dependency tree
of the sentence “The movie has good script”: “NN-
amod-JJ” from “script” and “good”, and “NN-nsubj-
VB-dobj-NN-amod-JJ” from “movie” and “good”.
</bodyText>
<figureCaption confidence="0.997992">
Figure 2: Example of pattern extraction.
</figureCaption>
<bodyText confidence="0.9994506">
In the following sections, we present the proposed
two-stage domain adaptation framework: 1) gener-
ating some sentiment and topic seeds in the target
domain; and 2) expanding the seeds in the target do-
main to construct sentiment and topic lexicons.
</bodyText>
<sectionHeader confidence="0.976794" genericHeader="method">
4 Seed Generation
</sectionHeader>
<bodyText confidence="0.999955166666667">
Our basic idea is to first identify several common
sentiment words across domains as sentiment seeds.
Meanwhile, we mine some general patterns between
sentiment and topic words from the source domain.
Finally, we use the sentiment seeds and general pat-
terns to generate topic seeds in the target domain.
</bodyText>
<figure confidence="0.999235111111111">
(a) Camera domain. (b) Movie domain.
camera is
nsubi cop
det
great
The
movie is
nsubi cop
det
excellent
The
script(NN)
dobj nsubj
amod det
has(VB)
good(JJ)
movie(NN)
the(DT)
</figure>
<page confidence="0.977628">
412
</page>
<subsectionHeader confidence="0.997348">
4.1 Sentiment Seed Generation
</subsectionHeader>
<bodyText confidence="0.9995175">
To identify common sentiment words across do-
mains, we extract all sentiment words from the
source domain as candidates. For each candidate,
we calculate its score based on the following metric:
</bodyText>
<equation confidence="0.99937">
S1(wi) = (pS(wi) + pT(wi))e(−|pS(wi)−pT (wi)|), (1)
</equation>
<bodyText confidence="0.998542375">
where pS(wi) and pT(wi) are the probabilities of the
word wi occurring in the source and target domains,
respectively. If a word wi has high S1 score, which
implies that the word wi occurs frequently and simi-
larly in both domains, then it can be considered as a
common sentiment word (Pan et al., 2010; Blitzer et
al., 2007). We select top r candidates with highest
S1 scores as sentiment seeds.
</bodyText>
<subsectionHeader confidence="0.988144">
4.2 Topic Seed Generation
</subsectionHeader>
<bodyText confidence="0.9989455">
We extract all patterns between sentiment and topic
words in the source domain as candidates. For each
pattern candidate, we calculate its score based on a
metric defined in AutoSlog-TS (Riloff, 1996):
</bodyText>
<equation confidence="0.999258">
S2(Rj) = Acc(Rj) x log2(Freq(Rj)), (2)
</equation>
<bodyText confidence="0.9999583">
where Acc(Rj) is the accuracy of the pattern Rj in
the source domain, and Freq(Rj) is the frequency
of the pattern Rj observed in target domain. This
metric aims to identify the patterns that are precise
in the source domain and observed frequently in the
target domain. We also select the top r patterns
with highest S2 scores. With the patterns and sen-
timent seeds, we extract topic-word candidates and
measure their scores based on a variant metric of
quadratic combination (Zhang and Ye, 2008):
</bodyText>
<equation confidence="0.871355">
�S3(wk) = (S2(Rj) x S1(wi)), (3)
RHEA, wiEB
</equation>
<bodyText confidence="0.999932666666667">
where B is a set of sentiment seeds and A is a set of
patterns which the words wi and wk satisfy. We then
select the top r candidates as topic seeds.
</bodyText>
<sectionHeader confidence="0.976692" genericHeader="method">
5 Seed Expansion
</sectionHeader>
<bodyText confidence="0.999897510638298">
After generating the topic and sentiment seeds, we
aim to expand them in the target domain to construct
topic and sentiment lexicons. In this section, we pro-
pose a new bootstrapping-based method to address
this problem.
Bootstrapping is the process of improving the per-
formance of a weak classifier by iteratively adding
training data and retraining the classifier. More
specifically, bootstrapping starts with a small set
of labeled “seeds”, and iteratively adds unlabeled
data that are labeled by the classifier to the train-
ing set based on some selection criterion, and retrain
the classifier. Many bootstrapping-based algorithms
have been proposed to information extraction and
other NLP tasks (Blum and Mitchell, 1998; Riloff
and Jones, 1999; Jones et al., 1999; Wu et al., 2009).
One important issue in bootstrapping is how to
design a criterion to select unlabeled data to be
added to the training set iteratively. Our proposed
bootstrapping for cross-domain lexicon extraction
is based on the following two observations: 1) Al-
though the source and target domains are different,
part of source domain labeled data is still useful for
lexicon extraction in the target domain after some
adaptation; 2) The syntactic relationships among
sentiment and topic words can be used to expand the
seeds in the target domain for lexicon construction.
Based on the two observations, we propose a
new bootstrapping-based method named Relational
Adaptive bootstraPping (RAP), as summarized in
Algorithm 1, for expanding lexicons across do-
mains. In each iteration, we employ a cross-domain
classifier trained on the source domain lexicons and
the extracted target domain lexicons to predict the
labels of the target unlabeled data, and select top k2
predicted topic and sentiment words as candidates
based on confidence. With the extracted syntactic
patterns in the previous iterations, we construct a
bipartite graph between sentiment and topic words
on the extracted target domain lexicons and candi-
dates. After that, a graph-based score refinement al-
gorithm is performed on the graph, and the top k1
candidates are added to the extracted lexicons based
on the final scores. Accordingly, with the new ex-
tracted lexicons, we update the syntactic patterns in
each iteration. The details of RAP are presented in
the following sections.
</bodyText>
<subsectionHeader confidence="0.993763">
5.1 Cross-Domain Classifier
</subsectionHeader>
<bodyText confidence="0.9997931">
In this paper, we employ Transfer AdaBoost (TrAd-
aBoost) (Dai et al., 2007) as the cross-domain learn-
ing algorithm in RAP. In TrAdaBoost, each word
wSi (or wT,) is represented by a feature vector xSi
(or xT,). A classifier trained on the source domain
data DS = {(xSi, ySi)} may perform poor on xT,
because of domain difference. The main idea of
TrAdaBoost is to re-weight the source domain data
based on a few of target domain labeled data, which
is referred to as seeds in our task. The re-weighting
</bodyText>
<page confidence="0.997023">
413
</page>
<bodyText confidence="0.999957583333334">
aims to reduce the effect of the “bad” source do-
main data while encourage the “good” ones to get
a more precise classifier in target domain. In each
iteration of RAP, we train cross-domain classifiers
fT� and fT� for sentiment- and topic- word extrac-
tion using TrAdaBoost separately (taking sentiment
or topic words as positive instances). We use linear
Support Vector Machines (SVMs) as the base clas-
sifier in TrAdaBoost. For features to represent each
word, we use lexicon features, such as the previous,
current and next words, and POS tag features, such
as the previous, current and next words’ POS tags.
</bodyText>
<figure confidence="0.94627725">
Algorithm 1 Relational Adaptive bootstraPping
S Du
Require: Target domain data DT = Dl T , where Dl
T T
</figure>
<bodyText confidence="0.844471666666667">
consists of sentiment seeds B and topic seeds C and their
initial scores S1(wi), Vwi E B and S3(wj), Vwj E C, DuT
is the set of unlabeled target domain data; labeled source
domain data DS; a cross-domain classifier; iteration num-
ber M and candidate selection number k1, k2.
Ensure: Expand C and B in the target domain.
</bodyText>
<listItem confidence="0.964405666666666">
1: Initialize a pattern set A = 0, eS1(wi) = S1(wi), wi E B
and eS3(wj) = S3(wj), wj E C. Consider all patterns
observed in the source domain as pattern candidates P.
2: form= 1 ... M do
3: Extract new pattern candidates to P with DlT in target
domain, update pattern score eS2(Rj), where Rj E P,
based on Eq. (4), and select the top k1 patterns to the
pattern set A.
4: Learn the cross-domain classifiers fTO and fTP for
sentiment- and topic- word extraction with DS S DlT
separately. Predict the sentiment score hTfO (wTj ) and
topic score hT f� (wTj) on DuT, and select k2 sentiment
words and topic words with highest scores as candidates.
5: Construct a bipartite graph between sentiment and topic
words on DlT and the k2 sentiment- and topic- word can-
didates, and calculate the normalized weights Oij’s for
each edge of the graph. e
6: Refine the scores S1 and S3 of the k2 sentiment and
topic word candidates using Eqs. (5) and (6) iteratively.
7: Select k1 new sentiment words and k1 new topic words
with the final scores, and add them to lexicons B and C.
Update eS1(wi) and eS3(wj) accordingly.
8: end for
9: return Expanded lexicons B and C.
</listItem>
<subsectionHeader confidence="0.990543">
5.2 Graph Construction
</subsectionHeader>
<bodyText confidence="0.993868">
Based on the cross-domain classifiers fT� and fT� ,
</bodyText>
<equation confidence="0.6660365">
we can predict the sentiment label score hT(wTi )
fO
</equation>
<bodyText confidence="0.98950405">
and topic label score hT(wTi) for the target domain
fP
data wTi. According to all predicted values, we re-
spectively select top k2 new sentiment- and topic-
words as candidates. Together with the extracted
sentiment and topic lexicons in the target domain,
we build a bipartite graph among them as shown in
Figure 3. In the bipartite graph, one set of nodes
represents topic words, including new topic candi-
dates and words in the lexicon C, and the other set
of nodes represents sentiment words, including new
sentiment candidates and words in the lexicon B.
For a pair of sentiment and topic words wd and wT,
if there is a pattern Rj in the pattern set A that they
can satisfy, then there exists an edge eij between
them. Furthermore, each edge eij is associated with
a nonnegative weight θij, which is measured as fol-
lows, θij = ERkEE S2(Rk), where S2 is the pattern
score. Similar to the metric defined in Eq. (3), the
pattern score is defined as:
</bodyText>
<equation confidence="0.992327">
� �
�S1(wi) X �S3(w�) , (4)
</equation>
<bodyText confidence="0.9890128">
where E = {{wi, wjlj, wi E B, wj E C and
wi, wj satisfy Rj, Rj E Al. Note that in the be-
ion, S2 is updated based on the
S1 and topic score S3. We fur-
ginning of each iterat
</bodyText>
<figure confidence="0.565771">
ther normalize θij by �θij = θij/(Eij θij).
Topic words Sentiment words
</figure>
<figureCaption confidence="0.998846">
Figure 3: Topic and sentiment word graph.
</figureCaption>
<subsectionHeader confidence="0.993526">
5.3 Score Computation
</subsectionHeader>
<bodyText confidence="0.9999145">
We construct the bipartite graph to exploit the re-
lationships between sentiment and topic words to
propagate information for lexicon extraction. We
use the following reinforcement formulas to itera-
tively update the final sentiment score �S1(wTj) and
topic score S3(wTi), respectively:
</bodyText>
<equation confidence="0.9921505">
Wij + (1 − µ)hfO
T(wTj ), (5)
S3(wTi) = µ � �S1(wTj )�θij + (1 − µ)hTf�(wTi), (6)
j
</equation>
<bodyText confidence="0.999779666666667">
where µ is a trade-off parameter between the pre-
dicted value by cross-domain classifier and the re-
inforcement scores from other nodes connected by
</bodyText>
<figure confidence="0.983439555555556">
script
movie
music
NN-nsubj-VB-dobj-NN-amod-JJ
NN-amod-JJ
recommend
boring
good
�
</figure>
<equation confidence="0.7255266">
�S2(Rj) =
{wi,wk}EE
new sentiment score
��S1(wTj) = µ �S3(wTi)
i
</equation>
<page confidence="0.987826">
414
</page>
<bodyText confidence="0.9999807">
edge ezj. Here p is empirically set to be 0.5. With
Eqs. (5) and (6), the sentiment scores and topic
scores are iteratively refined until the state of the
graph trends to be stable. This can be considered
as an extension to the HITS algorithm(Kleinberg,
1999). Finally, we select k1 ≪ k2 sentiment and
topic words from the k2 candidates based on their
refined scores, and add them to the target domain
lexicons, respectively. We also update the sentiment
score �S1 and topic score S3 for next iteration.
</bodyText>
<subsectionHeader confidence="0.960661">
5.4 Special Cases
</subsectionHeader>
<bodyText confidence="0.999992071428571">
We now introduce two special cases of the RAP al-
gorithm. In Eqs. (5) and (6), if the parameter p = 1,
then RAP only uses the relationships between sen-
timent and topic words with their patterns to propa-
gate label information in the target domain without
using the cross-domain classifier. We call this reduc-
tion relational bootstrapping. If p = 0, then RAP
only utilizes useful source domain labeled data to as-
sist learning of the target domain classifier without
considering the relationships between sentiment and
topic words. We call this reduction adaptive boot-
strapping, which can be considered as a bootstrap-
ping version of TrAdaBoost. We also empirically
study these two special cases in experiments.
</bodyText>
<sectionHeader confidence="0.997493" genericHeader="method">
6 Experiments on Lexicon Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999936">
6.1 Data Set and Evaluation Criteria
</subsectionHeader>
<bodyText confidence="0.999985357142857">
We use the review dataset from (Li et al., 2010a),
which contains 500 movie and 601 product reviews,
for evaluation. The sentiment and topic words are
manually annotated. In this dataset, all types of
sentiment words are annotated instead of adjective
words only. For example, the verbs, such as “like”,
“recommend”, and nouns, such as “masterpiece”,
are also labeled as sentiment words. We construct
two cross-domain lexicon extraction tasks: “prod-
uct vs. movie” and “movie vs. product”, where the
word before “vs.” corresponds with the source do-
main and the word after “vs.” corresponds with the
target domain. We evaluate our methods in terms of
precision, recall and F-score (F1).
</bodyText>
<subsectionHeader confidence="0.99775">
6.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999787833333333">
The results of in-domain classifiers, which are
trained on plenty of target domain labeled data, can
be treated as upper-bounds. We denote iSVM and
iCRF the in-domain SVM and CRF classifiers in
experiments, and compare our proposed methods,
RAP, relational bootstrapping, and adaptive boot-
strapping, with the following baselines,
Unsupervised Method (Un) we implement a rule-
based method for lexicon extraction based on (Hu
and Liu, 2004), where adjective words that match
a rule is recognized as sentiment words, and nouns
that match a rule are recognized as topic words.
Semi-Supervised Method (Semi) we implement
the double propagation model proposed in (Qiu et
al., 2009). Since this method requires some target
domain labeled data, we manually label 30 senti-
ment words in the target domain.
Cross-Domain CRF (Cross-CRF) we implement
a cross-domain CRF algorithm proposed by (Jakob
and Gurevych, 2010).
TrAdaBoost We apply TrAdaBoost (Dai et al.,
2007) on the source domain labeled data and the
generated seeds in the target domain to train a lexi-
con extractor.
</bodyText>
<subsectionHeader confidence="0.99916">
6.3 Comparison Results
</subsectionHeader>
<bodyText confidence="0.999972259259259">
Comparison results on lexicon extraction are shown
in Table 2 and Table 3. From Table 2, we can ob-
serve that our proposed methods are effective for
sentiment lexicon extraction. The relational boot-
strapping method performs better than the unsuper-
vised method, TrAdaBoost and the cross-domain
CRF algorithm, and achieves comparable results
with the semi-supervised method. However, com-
pared to the semi-supervised method, our proposed
relational bootstrapping method does not require any
labeled data in the target domain. We can also ob-
serve that the adaptive bootstrapping method and the
RAP method perform much better than other meth-
ods in terms of F-score. The reason is that part of
the source domain labeled data may be useful for
learning the target classifier after reweighting. In
addition, we also observe that embedding the TrAd-
aBoost algorithm into a bootstrapping process can
further boost the performance of the classifier for
sentiment lexicon extraction.
Table 3 shows the comparison results on topic lex-
icon extraction. From the table, we can observe that
different from the sentiment lexicon extraction task,
the relational bootstrapping method performs better
than the adaptive bootstrapping method slightly. The
reason may be that for the sentiment lexicon extrac-
tion task, there exist some common sentiment words
</bodyText>
<page confidence="0.997928">
415
</page>
<table confidence="0.999660272727273">
product vs. movie movie vs. product
Prec. Rec. F1 Prec. Rec. F1
Un 0.82 0.31 0.45 0.74 0.23 0.35
Semi 0.71 0.44 0.54 0.62 0.45 0.52
Cross-CRF 0.69 0.40 0.51 0.65 0.34 0.45
Tradaboost 0.73 0.41 0.52 0.72 0.42 0.52
Adaptive 0.68 0.53 0.59 0.63 0.52 0.57
Relational 0.55 0.51 0.53 0.57 0.51 0.54
RAP 0.69 0.59 0.64 0.66 0.59 0.62
iSVM 0.82 0.60 0.70 0.80 0.61 0.68
iCRF 0.80 0.66 0.72 0.80 0.62 0.69
</table>
<tableCaption confidence="0.992819">
Table 2: Results on sentiment lexicon extraction. Num-
bers in boldface denote significant improvement.
</tableCaption>
<table confidence="0.999478818181818">
product vs. movie movie vs. product
Prec. Rec. F1 Prec. Rec. F1
Un 0.41 0.32 0.36 0.53 0.35 0.41
Semi 0.54 0.59 0.56 0.75 0.50 0.60
Cross-CRF 0.70 0.23 0.34 0.80 0.24 0.37
Tradaboost 0.64 0.45 0.53 0.57 0.47 0.51
Adaptive 0.76 0.44 0.56 0.70 0.52 0.59
Relational 0.57 0.58 0.58 0.61 0.57 0.59
RAP 0.80 0.56 0.66 0.73 0.58 0.65
iSVM 0.83 0.73 0.78 0.85 0.70 0.77
iCRF 0.84 0.78 0.81 0.87 0.73 0.80
</table>
<tableCaption confidence="0.993291">
Table 3: Results on topic lexicon extraction. Numbers in
boldface denote significant improvement.
</tableCaption>
<bodyText confidence="0.999969818181818">
across domains, thus part of the labeled source do-
main data may be useful for the target learning task.
However, for the topic lexicon extraction task, the
topic words may be totally different, and as a result,
we may not be able to find useful source domain
labeled data to boost the performance for lexicon
extraction in the target domain. In this case, mu-
tual label propagation between sentiment and topic
words may be more reasonable for knowledge trans-
fer. RAP absorbs the advantages of the adaptive and
relational bootstrapping methods, thus can get the
best results in both lexicon extraction tasks.
We also observe that relational bootstrapping can
get better recall, but lower precision, compared to
adaptive bootstrapping. This is because relational
bootstrapping only utilizes the patterns to propagate
label information, which may cover more topic and
sentiment seeds, but include some noisy words. For
example, given two phases “like the camera” and
“recommend the camera”, we can extract a pattern
“VB-dobj-NN”. However, by using this pattern and
the topic word “camera”, we may extract “take” as
a sentiment word from another phase “take the cam-
era”, which is incorrect. The adaptive bootstrapping
method can utilize various features to make predic-
tions more precisely, which may have higher preci-
sion, but encounter the lower recall problem. For ex-
ample, “flash” is not identified as a topic word in the
target product domain (camera domain). Our RAP
method can exploit both relationships between sen-
timent and topic words and part of labeled source
domain data for cross-domain lexicon extraction. It
can correctly identify the above two cases.
</bodyText>
<subsectionHeader confidence="0.875315">
6.3.1 Parameter Sensitivity Study
</subsectionHeader>
<bodyText confidence="0.999869058823529">
In this section, we conduct experiments to study
the effect of different parameter settings. There are
several parameters in the framework: the number
of generated seeds r, the number of new candidates
k2 and the number of selections k in each iteration,
and the number of iterations M (µ is empirically set
to 0.5 ). For the parameter k2, we just set it to a
large number (k2 = 100) such that have rich candi-
dates to build the bipartite graph. In the experiments
reported in the previous section, we set r = 20,
k1 = 10 and M = 50. Figures 4(a) and 4(b) show
the results under varying values of r in the “product
vs. movie” task. Observe that for sentiment word
extraction, the results of the proposed methods are
not sensitive to the values of r. While for the topic
word extraction, the proposed methods perform well
when r falls in the range from 15 to 20.
</bodyText>
<figureCaption confidence="0.998939">
Figure 4: Results on varying values of r.
</figureCaption>
<figure confidence="0.999394066666667">
0.65
0.65
0.6
0.55
0.55
0.5
0.5
0.45
Relational
Adaptive
RAP
0.4
0 10 20 30 40 50
Number of iterations
(a) Sentiment word extraction (b) Topic word extraction
</figure>
<figureCaption confidence="0.999944">
Figure 5: Results on varying values of M.
</figureCaption>
<bodyText confidence="0.998909">
We also test the sensitivity of the parameter k1
and find that the proposed methods work well and
robust when k1 falls in the range from 10 to 20.
</bodyText>
<figure confidence="0.999366064516129">
0.7
0.65
0.6
0.6
0.55
0.5
0.55
0.5
Relational
Adaptive
RAP
Relational
Adaptive
0.45
5 10 15 20 25 30
Values of r
(a) Sentiment word extraction
0.45
5 10 15 20 25 30
Values of r
(b) Topic word extraction
0.7
0.65
0.6
0.45
0.7
0.4
0 10 20 30 40
Number of iterations
Relational
Adaptive
</figure>
<page confidence="0.998689">
416
</page>
<bodyText confidence="0.9986">
Figures 5(a) and 5(b) show the results under vary-
ing numbers of iterations in the “product vs. movie”
task. As we can see, our proposed methods converge
well when M ≥ 40.
</bodyText>
<sectionHeader confidence="0.920596" genericHeader="method">
7 Application: Sentiment Classification
</sectionHeader>
<bodyText confidence="0.998764333333333">
To further verify the usefulness of the lexicons ex-
tracted by the RAP method, we apply the extracted
sentiment lexicon for sentiment classification.
</bodyText>
<subsectionHeader confidence="0.998678">
7.1 Experiment Setting
</subsectionHeader>
<bodyText confidence="0.999967681818182">
Our work is motivated by the work of (Pang and
Lee, 2004), which only used subjective sentences
for document-level sentiment classification, instead
of using all sentences. In this experiment, we only
use sentiment related words as features to represent
opinion documents for classification, instead of us-
ing all words. Our goal is compare the sentiment
lexicon constructed by the RAP method with other
general lexicons on the impact of for sentiment clas-
sification. The general lexicons used for comparison
are described in Table 4.
We use the dataset from (Blitzer et al., 2007) for
sentiment classification. It contains a collection of
product reviews from Amazon.com. The reviews are
about four product domains: books, dvds, electron-
ics and kitchen appliance. In each domain, there are
1000 positive and 1000 negative reviews. To con-
struct domain specific sentiment lexicons, we apply
RAP on each product domain with the movie domain
described in Section 6.1 as the source domain. Fi-
nally, we use linear SVM as the classifier and the
classification accuracy as the evaluate criterion.
</bodyText>
<table confidence="0.999658">
Lexicon Name Size Description
Senti-WordNet 6957 Words with a subjective score &gt; 0.6
(Esuli and Sebastiani, 2006)
HowNet 4619 Eng. translation of subj. Chinese
words (Dong and Dong, 2006)
Subj. Clues 6878 Lexicons from (Wilson et al., 2005)
</table>
<tableCaption confidence="0.999568">
Table 4: Description of different lexicons.
</tableCaption>
<subsectionHeader confidence="0.995802">
7.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999891166666667">
Experimental results on sentiment classification are
shown in Table 5, where we denote “All” using all
unigram and bigram features instead of using sub-
jective words. As we can see that a classifier trained
with features constructed by our RAP method per-
formance best in all domains. Note that the num-
ber of features (sentiment words) constructed by our
method is much smaller than that of all unigram
and bigram features, which can reduce the classi-
fier training time dramatically. These promising re-
sults imply that our RAP can be applied for senti-
ment classification effectively and efficiently.
</bodyText>
<table confidence="0.9972334">
All Senti HowNet Subj. Clue Ours
dvd 82.55 79.80 80.57 80.93 84.05
book 80.71 76.22 78.22 79.48 81.65
electronic 84.43 82.42 83.05 83.22 86.71
kitchen 87.70 81.78 84.17 84.23 88.83
</table>
<tableCaption confidence="0.9963685">
Table 5: Sentiment classification results (accuracy in %).
Numbers in boldface denotes significant improvement.
</tableCaption>
<sectionHeader confidence="0.995884" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999986">
In this paper, we propose a two-stage framework for
co-extraction of sentiment and topic lexicons across
domains where we have no labeled data in the tar-
get domain but have plenty of labeled data in an-
other domain. In the first stage, we propose a sim-
ple strategy to generate a few high-quality sentiment
and topic seeds for the target domain. In the second
stage, we propose a novel Relational Adaptive boot-
straPping (RAP) method to expand the seeds, which
can exploit the relationships between topic and opin-
ion words, and make use of part of useful source do-
main labeled data for help. Extensive experimental
results show our proposed method can extract pre-
cise sentiment and topic lexicons from the target do-
main. Furthermore, the extracted sentiment lexicon
can be applied to sentiment classification effectively.
In the future work, besides the heterogeneous
relationships between topic and sentiment words,
we intend to investigate the homogeneous relation-
ships among topic words and those among sentiment
words (Qiu et al., 2009) to further boost the perfor-
mance of RAP method. Furthermore, in our frame-
work, we do not identify the polarity of the extracted
sentiment lexicon. We also plan to embed this com-
ponent into our unified framework. Finally, it is also
interesting to exploit multi-domain knowledge (Li
and Zong, 2008; Bollegala et al., 2011) for cross-
domain lexicon extraction.
</bodyText>
<sectionHeader confidence="0.991179" genericHeader="acknowledgments">
9 Acknowledgement
</sectionHeader>
<bodyText confidence="0.644825">
This work was supported by the Chinese Natu-
ral Science Foundation No.60973104, National Key
Basic Research Program 2012CB316301, and Hong
Kong RGC GRF Projects 621010 and 621211.
</bodyText>
<page confidence="0.996762">
417
</page>
<sectionHeader confidence="0.985274" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998959423076923">
Rie K. Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks and
unlabeled data. J. Mach. Learn. Res., 6:1817–1853.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 432–439,
Prague, Czech Republic. ACL.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
Learning Theory, pages 92–100.
Danushka Bollegala, David Weir, and John Carroll.
2011. Using multiple sources to construct a sentiment
sensitive thesaurus for cross-domain sentiment clas-
sification. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 132–141, Port-
land, Oregon. ACL.
Wenyuan Dai, Qiang Yang, Guirong Xue, and Yong Yu.
2007. Boosting for transfer learning. In Proceed-
ings of the 24th International Conference on Machine
Learning, pages 193–200, Corvalis, Oregon, USA,
June. ACM.
Hal Daum´e III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256–
263, Prague, Czech Republic. ACL.
Zhendong Dong and Qiang Dong, editors. 2006.
HOWNET and the computation of meaning. World
Scientific Publishers, Norwell, MA, USA.
Weifu Du, Songbo Tan, Xueqi Cheng, and Xiaochun
Yun. 2010. Adapting information bottleneck method
for automatic construction of domain-oriented senti-
ment lexicon. In Proceedings of the 3rd ACM inter-
national conference on Web search and data mining,
pages 111–120, New York, NY, USA. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource for
opinion mining. In In Proceedings of the 5th Confer-
ence on Language Resources and Evaluation, pages
417–422.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning, pages 513–520, Bellevue, Washing-
ton, USA.
Yulan He, Chenghua Lin, and Harith Alani. 2011. Auto-
matically extracting polarity-bearing topics for cross-
domain sentiment classification. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 123–131, Portland, Oregon. ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177, Seat-
tle, WA, USA. ACM.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single- and cross-domain setting
with conditional random fields. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1035–1045, Cambridge,
Massachusetts, USA. ACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 264–271, Prague, Czech
Republic. ACL.
Wei Jin and Hung Hay Ho. 2009. A novel lexical-
ized HMM-based learning framework for web opinion
mining. In Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, pages 465–
472, Montreal, Quebec, Canada. ACM.
Rosie Jones, Andrew Mccallum, Kamal Nigam, and
Ellen Riloff. 1999. Bootstrapping for text learning
tasks. In In IJCAI-99 Workshop on Text Mining: Foun-
dations, Techniques and Applications, pages 52–63.
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. J. ACM, 46:604–632, Sept.
Shoushan Li and Chengqing Zong. 2008. Multi-domain
sentiment classification. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics on Human Language Technologies: Short
Papers, pages 257–260, Columbus, Ohio, USA. ACL.
Tao Li, Vikas Sindhwani, Chris Ding, and Yi Zhang.
2009. Knowledge transformation for cross-domain
sentiment classification. In Proceedings of the 32nd
international ACM SIGIR conference on Research and
development in information retrieval, pages 716–717,
Boston, MA, USA. ACM.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010a.
Structure-aware review mining and summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 653–661, Beijing,
China.
Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010b.
Sentiment analysis with global topics and local de-
pendency. In Proceedings of the Twenty-Fourth AAAI
Conference on Artificial Intelligence, Atlanta, Geor-
gia, USA. AAAI Press.
</reference>
<page confidence="0.981271">
418
</page>
<reference confidence="0.999713043956044">
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing, Second
Edition.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Pro-
ceedings of the 16th international conference on World
Wide Web, pages 171–180, Banff, Alberta, Canada.
ACM.
Sinno Jialin Pan and Qiang Yang. 2010. A survey
on transfer learning. IEEE Trans. Knowl. Data Eng.,
22(10):1345–1359, Oct.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Chen Zheng. 2010. Cross-domain senti-
ment classification via spectral feature alignment. In
Proceedings of the 19th International Conference on
World Wide Web, pages 751–760, Raleigh, NC, USA,
Apr. ACM.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, Barcelona, Spain. ACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing, pages 339–346, Vancouver, British
Columbia, Canada. ACL.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, pages 1199–
1204, Pasadena, California, USA. Morgan Kaufmann
Publishers Inc.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the 6th national con-
ference on Artificial intelligence, pages 474–479, Or-
lando, Florida, United States. AAAI.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the 7th conference
on natural language learning, pages 25–32, Edmon-
ton, Canada. ACL.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings of
the Thirteenth National Conference on Artificial In-
telligence, pages 1044–1049, Portland, Oregon, USA.
AAAI Press/MIT Press.
Songbo Tan, Gaowei Wu, Huifeng Tang, and Xueqi
Cheng. 2007. A novel scheme for domain-transfer
problem in the context of sentiment analysis. In Pro-
ceedings of the 16th ACM conference on Conference
on information and knowledge management, pages
979–982, Lisbon, Portugal. ACM.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of the 46th Annual Meeting of the As-
sociation of Computational Linguistics: Human Lan-
guage Technologies, pages 308–316, Columbus, Ohio,
USA. ACL.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Comput. Linguist., 30:277–308, Sept.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347–354,
Vancouver, British Columbia, Canada. ACL.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1523–1532, Singapore. ACL.
Min Zhang and Xingyao Ye. 2008. A generation
model to unify topic relevance and lexicon-based sen-
timent for opinion retrieval. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 411–418, Singapore. ACM.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a MaxEnt-LDA hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 56–65, Cambridge, Mas-
sachusetts, USA. ACL.
</reference>
<page confidence="0.99867">
419
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.827881">
<title confidence="0.999895">Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</title>
<author confidence="0.998649">Sinno Jialin Ou Qiang Xiaoyan</author>
<affiliation confidence="0.936731333333333">of Computer Science and Technology, Tsinghua University, Beijing, for Infocomm Research, Kong University of Science and Technology, Hong Kong,</affiliation>
<abstract confidence="0.999579086956522">Extracting sentiment and topic lexicons is important for opinion mining. Previous works have showed that supervised learning methods are superior for this task. However, the performance of supervised methods highly relies on manually labeled training data. In this paper, we propose a domain adaptation framework for sentimentand topiclexicon co-extraction in a domain of interest where we do not require any labeled data, but have lots of labeled data in another related domain. The framework is twofold. In the first step, we generate a few high-confidence sentiment and topic seeds in the target domain. In the second step, we propose a novel Relational Adaptive bootstraPping (RAP) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie K Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled</title>
<date>2005</date>
<pages>6--1817</pages>
<contexts>
<context position="6805" citStr="Ando and Zhang, 2005" startWordPosition="1064" endWordPosition="1067">ods for lexicon extraction. However, their method requires to manually define some general syntactic rules among sentiment and topic words. In addition, it still requires some annotated words in the target domain. In this paper, we do not assume any predefined rules and labeled data be available in the target domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CR</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie K. Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. J. Mach. Learn. Res., 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>432--439</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7040" citStr="Blitzer et al., 2007" startWordPosition="1102" endWordPosition="1105">not assume any predefined rules and labeled data be available in the target domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better res</context>
<context position="13160" citStr="Blitzer et al., 2007" startWordPosition="2126" endWordPosition="2129">ovie(NN) the(DT) 412 4.1 Sentiment Seed Generation To identify common sentiment words across domains, we extract all sentiment words from the source domain as candidates. For each candidate, we calculate its score based on the following metric: S1(wi) = (pS(wi) + pT(wi))e(−|pS(wi)−pT (wi)|), (1) where pS(wi) and pT(wi) are the probabilities of the word wi occurring in the source and target domains, respectively. If a word wi has high S1 score, which implies that the word wi occurs frequently and similarly in both domains, then it can be considered as a common sentiment word (Pan et al., 2010; Blitzer et al., 2007). We select top r candidates with highest S1 scores as sentiment seeds. 4.2 Topic Seed Generation We extract all patterns between sentiment and topic words in the source domain as candidates. For each pattern candidate, we calculate its score based on a metric defined in AutoSlog-TS (Riloff, 1996): S2(Rj) = Acc(Rj) x log2(Freq(Rj)), (2) where Acc(Rj) is the accuracy of the pattern Rj in the source domain, and Freq(Rj) is the frequency of the pattern Rj observed in target domain. This metric aims to identify the patterns that are precise in the source domain and observed frequently in the targe</context>
<context position="30949" citStr="Blitzer et al., 2007" startWordPosition="5133" endWordPosition="5136">t classification. 7.1 Experiment Setting Our work is motivated by the work of (Pang and Lee, 2004), which only used subjective sentences for document-level sentiment classification, instead of using all sentences. In this experiment, we only use sentiment related words as features to represent opinion documents for classification, instead of using all words. Our goal is compare the sentiment lexicon constructed by the RAP method with other general lexicons on the impact of for sentiment classification. The general lexicons used for comparison are described in Table 4. We use the dataset from (Blitzer et al., 2007) for sentiment classification. It contains a collection of product reviews from Amazon.com. The reviews are about four product domains: books, dvds, electronics and kitchen appliance. In each domain, there are 1000 positive and 1000 negative reviews. To construct domain specific sentiment lexicons, we apply RAP on each product domain with the movie domain described in Section 6.1 as the source domain. Finally, we use linear SVM as the classifier and the classification accuracy as the evaluate criterion. Lexicon Name Size Description Senti-WordNet 6957 Words with a subjective score &gt; 0.6 (Esuli</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 432–439, Prague, Czech Republic. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="14931" citStr="Blum and Mitchell, 1998" startWordPosition="2419" endWordPosition="2422">ruct topic and sentiment lexicons. In this section, we propose a new bootstrapping-based method to address this problem. Bootstrapping is the process of improving the performance of a weak classifier by iteratively adding training data and retraining the classifier. More specifically, bootstrapping starts with a small set of labeled “seeds”, and iteratively adds unlabeled data that are labeled by the classifier to the training set based on some selection criterion, and retrain the classifier. Many bootstrapping-based algorithms have been proposed to information extraction and other NLP tasks (Blum and Mitchell, 1998; Riloff and Jones, 1999; Jones et al., 1999; Wu et al., 2009). One important issue in bootstrapping is how to design a criterion to select unlabeled data to be added to the training set iteratively. Our proposed bootstrapping for cross-domain lexicon extraction is based on the following two observations: 1) Although the source and target domains are different, part of source domain labeled data is still useful for lexicon extraction in the target domain after some adaptation; 2) The syntactic relationships among sentiment and topic words can be used to expand the seeds in the target domain fo</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>David Weir</author>
<author>John Carroll</author>
</authors>
<title>Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>132--141</pages>
<publisher>ACL.</publisher>
<location>Portland, Oregon.</location>
<contexts>
<context position="7117" citStr="Bollegala et al., 2011" startWordPosition="1118" endWordPosition="1121"> domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better results on cross-domain lexicon extraction. Note that our work is also different</context>
</contexts>
<marker>Bollegala, Weir, Carroll, 2011</marker>
<rawString>Danushka Bollegala, David Weir, and John Carroll. 2011. Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 132–141, Portland, Oregon. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenyuan Dai</author>
<author>Qiang Yang</author>
<author>Guirong Xue</author>
<author>Yong Yu</author>
</authors>
<title>Boosting for transfer learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning,</booktitle>
<pages>193--200</pages>
<publisher>ACM.</publisher>
<location>Corvalis, Oregon, USA,</location>
<contexts>
<context position="16637" citStr="Dai et al., 2007" startWordPosition="2688" endWordPosition="2691"> With the extracted syntactic patterns in the previous iterations, we construct a bipartite graph between sentiment and topic words on the extracted target domain lexicons and candidates. After that, a graph-based score refinement algorithm is performed on the graph, and the top k1 candidates are added to the extracted lexicons based on the final scores. Accordingly, with the new extracted lexicons, we update the syntactic patterns in each iteration. The details of RAP are presented in the following sections. 5.1 Cross-Domain Classifier In this paper, we employ Transfer AdaBoost (TrAdaBoost) (Dai et al., 2007) as the cross-domain learning algorithm in RAP. In TrAdaBoost, each word wSi (or wT,) is represented by a feature vector xSi (or xT,). A classifier trained on the source domain data DS = {(xSi, ySi)} may perform poor on xT, because of domain difference. The main idea of TrAdaBoost is to re-weight the source domain data based on a few of target domain labeled data, which is referred to as seeds in our task. The re-weighting 413 aims to reduce the effect of the “bad” source domain data while encourage the “good” ones to get a more precise classifier in target domain. In each iteration of RAP, we</context>
<context position="24287" citStr="Dai et al., 2007" startWordPosition="4008" endWordPosition="4011">ised Method (Un) we implement a rulebased method for lexicon extraction based on (Hu and Liu, 2004), where adjective words that match a rule is recognized as sentiment words, and nouns that match a rule are recognized as topic words. Semi-Supervised Method (Semi) we implement the double propagation model proposed in (Qiu et al., 2009). Since this method requires some target domain labeled data, we manually label 30 sentiment words in the target domain. Cross-Domain CRF (Cross-CRF) we implement a cross-domain CRF algorithm proposed by (Jakob and Gurevych, 2010). TrAdaBoost We apply TrAdaBoost (Dai et al., 2007) on the source domain labeled data and the generated seeds in the target domain to train a lexicon extractor. 6.3 Comparison Results Comparison results on lexicon extraction are shown in Table 2 and Table 3. From Table 2, we can observe that our proposed methods are effective for sentiment lexicon extraction. The relational bootstrapping method performs better than the unsupervised method, TrAdaBoost and the cross-domain CRF algorithm, and achieves comparable results with the semi-supervised method. However, compared to the semi-supervised method, our proposed relational bootstrapping method d</context>
</contexts>
<marker>Dai, Yang, Xue, Yu, 2007</marker>
<rawString>Wenyuan Dai, Qiang Yang, Guirong Xue, and Yong Yu. 2007. Boosting for transfer learning. In Proceedings of the 24th International Conference on Machine Learning, pages 193–200, Corvalis, Oregon, USA, June. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic.</location>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256– 263, Prague, Czech Republic. ACL.</rawString>
</citation>
<citation valid="true">
<date>2006</date>
<booktitle>HOWNET and the computation of meaning. World Scientific Publishers,</booktitle>
<editor>Zhendong Dong and Qiang Dong, editors.</editor>
<location>Norwell, MA, USA.</location>
<marker>2006</marker>
<rawString>Zhendong Dong and Qiang Dong, editors. 2006. HOWNET and the computation of meaning. World Scientific Publishers, Norwell, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weifu Du</author>
<author>Songbo Tan</author>
<author>Xueqi Cheng</author>
<author>Xiaochun Yun</author>
</authors>
<title>Adapting information bottleneck method for automatic construction of domain-oriented sentiment lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd ACM international conference on Web search and data mining,</booktitle>
<pages>111--120</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7754" citStr="Du et al., 2010" startWordPosition="1219" endWordPosition="1222">orot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better results on cross-domain lexicon extraction. Note that our work is also different from a recent work (Du et al., 2010), which focused on identifying the polarity of adjective words by using crossdomain knowledge. While we extract both topic and sentiment words and allow non-adjective sentiment words, which is more practical. 3 Cross-Domain Lexicon Co-Extraction 3.1 Problem Definition Recall that, we focus on the setting where we have no labeled data in the target domain, while we have plenty of labeled data in the source domain. Denote DS = {(wSi, ySi)}n� i�1 the source domain data, where wSi represents a word in the source domain. ySi ∈ Y is the corresponding label of wSi. Similarly, we denote DT = {wTj}n2 j</context>
</contexts>
<marker>Du, Tan, Cheng, Yun, 2010</marker>
<rawString>Weifu Du, Songbo Tan, Xueqi Cheng, and Xiaochun Yun. 2010. Adapting information bottleneck method for automatic construction of domain-oriented sentiment lexicon. In Proceedings of the 3rd ACM international conference on Web search and data mining, pages 111–120, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SENTIWORDNET: A publicly available lexical resource for opinion mining. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation,</booktitle>
<pages>417--422</pages>
<contexts>
<context position="31571" citStr="Esuli and Sebastiani, 2006" startWordPosition="5230" endWordPosition="5233">2007) for sentiment classification. It contains a collection of product reviews from Amazon.com. The reviews are about four product domains: books, dvds, electronics and kitchen appliance. In each domain, there are 1000 positive and 1000 negative reviews. To construct domain specific sentiment lexicons, we apply RAP on each product domain with the movie domain described in Section 6.1 as the source domain. Finally, we use linear SVM as the classifier and the classification accuracy as the evaluate criterion. Lexicon Name Size Description Senti-WordNet 6957 Words with a subjective score &gt; 0.6 (Esuli and Sebastiani, 2006) HowNet 4619 Eng. translation of subj. Chinese words (Dong and Dong, 2006) Subj. Clues 6878 Lexicons from (Wilson et al., 2005) Table 4: Description of different lexicons. 7.2 Experimental Results Experimental results on sentiment classification are shown in Table 5, where we denote “All” using all unigram and bigram features instead of using subjective words. As we can see that a classifier trained with features constructed by our RAP method performance best in all domains. Note that the number of features (sentiment words) constructed by our method is much smaller than that of all unigram an</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. SENTIWORDNET: A publicly available lexical resource for opinion mining. In In Proceedings of the 5th Conference on Language Resources and Evaluation, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning,</booktitle>
<pages>513--520</pages>
<location>Bellevue, Washington, USA.</location>
<contexts>
<context position="7156" citStr="Glorot et al., 2011" startWordPosition="1126" endWordPosition="1129">ptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better results on cross-domain lexicon extraction. Note that our work is also different from a recent work (Du et al., 2010), </context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th International Conference on Machine Learning, pages 513–520, Bellevue, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Chenghua Lin</author>
<author>Harith Alani</author>
</authors>
<title>Automatically extracting polarity-bearing topics for crossdomain sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>123--131</pages>
<publisher>ACL.</publisher>
<location>Portland, Oregon.</location>
<contexts>
<context position="7134" citStr="He et al., 2011" startWordPosition="1122" endWordPosition="1125">tation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better results on cross-domain lexicon extraction. Note that our work is also different from a recent wo</context>
</contexts>
<marker>He, Lin, Alani, 2011</marker>
<rawString>Yulan He, Chenghua Lin, and Harith Alani. 2011. Automatically extracting polarity-bearing topics for crossdomain sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 123–131, Portland, Oregon. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="23769" citStr="Hu and Liu, 2004" startWordPosition="3927" endWordPosition="3930">rresponds with the source domain and the word after “vs.” corresponds with the target domain. We evaluate our methods in terms of precision, recall and F-score (F1). 6.2 Baselines The results of in-domain classifiers, which are trained on plenty of target domain labeled data, can be treated as upper-bounds. We denote iSVM and iCRF the in-domain SVM and CRF classifiers in experiments, and compare our proposed methods, RAP, relational bootstrapping, and adaptive bootstrapping, with the following baselines, Unsupervised Method (Un) we implement a rulebased method for lexicon extraction based on (Hu and Liu, 2004), where adjective words that match a rule is recognized as sentiment words, and nouns that match a rule are recognized as topic words. Semi-Supervised Method (Semi) we implement the double propagation model proposed in (Qiu et al., 2009). Since this method requires some target domain labeled data, we manually label 30 sentiment words in the target domain. Cross-Domain CRF (Cross-CRF) we implement a cross-domain CRF algorithm proposed by (Jakob and Gurevych, 2010). TrAdaBoost We apply TrAdaBoost (Dai et al., 2007) on the source domain labeled data and the generated seeds in the target domain to</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177, Seattle, WA, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niklas Jakob</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting opinion targets in a single- and cross-domain setting with conditional random fields.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1035--1045</pages>
<publisher>ACL.</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="7359" citStr="Jakob and Gurevych (2010)" startWordPosition="1154" endWordPosition="1157">various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better results on cross-domain lexicon extraction. Note that our work is also different from a recent work (Du et al., 2010), which focused on identifying the polarity of adjective words by using crossdomain knowledge. While we extract both topic and sentiment words and allow non-adjective sentiment words, which is more practic</context>
<context position="24236" citStr="Jakob and Gurevych, 2010" startWordPosition="4000" endWordPosition="4003">ptive bootstrapping, with the following baselines, Unsupervised Method (Un) we implement a rulebased method for lexicon extraction based on (Hu and Liu, 2004), where adjective words that match a rule is recognized as sentiment words, and nouns that match a rule are recognized as topic words. Semi-Supervised Method (Semi) we implement the double propagation model proposed in (Qiu et al., 2009). Since this method requires some target domain labeled data, we manually label 30 sentiment words in the target domain. Cross-Domain CRF (Cross-CRF) we implement a cross-domain CRF algorithm proposed by (Jakob and Gurevych, 2010). TrAdaBoost We apply TrAdaBoost (Dai et al., 2007) on the source domain labeled data and the generated seeds in the target domain to train a lexicon extractor. 6.3 Comparison Results Comparison results on lexicon extraction are shown in Table 2 and Table 3. From Table 2, we can observe that our proposed methods are effective for sentiment lexicon extraction. The relational bootstrapping method performs better than the unsupervised method, TrAdaBoost and the cross-domain CRF algorithm, and achieves comparable results with the semi-supervised method. However, compared to the semi-supervised met</context>
</contexts>
<marker>Jakob, Gurevych, 2010</marker>
<rawString>Niklas Jakob and Iryna Gurevych. 2010. Extracting opinion targets in a single- and cross-domain setting with conditional random fields. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1035–1045, Cambridge, Massachusetts, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>264--271</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6827" citStr="Jiang and Zhai, 2007" startWordPosition="1068" endWordPosition="1071">tion. However, their method requires to manually define some general syntactic rules among sentiment and topic words. In addition, it still requires some annotated words in the target domain. In this paper, we do not assume any predefined rules and labeled data be available in the target domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain to</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271, Prague, Czech Republic. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Jin</author>
<author>Hung Hay Ho</author>
</authors>
<title>A novel lexicalized HMM-based learning framework for web opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>465--472</pages>
<publisher>ACM.</publisher>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="2472" citStr="Jin and Ho (2009)" startWordPosition="372" endWordPosition="375">olarity (e.g., positive or negative). The sentiment lexicon is domain dependent as users may use different sentiment words to express their opinion in different domains (e.g., different products). A topic lexicon is a list of topic expressions, on which the sentiment words are expressed. Extracting the topic lexicon from a specific domain is important because users not only care about the overall sentiment polarity of a review but also care about which aspects are mentioned in review. Note that, similar to sentiment lexicons, different domains may have very different topic lexicons. Recently, Jin and Ho (2009) and Li et al. (2010a) showed that supervised learning methods can achieve state-of-the-art results for lexicon extraction. However, the performance of these methods highly relies on manually annotated training data. In most cases, the labeling work may be timeconsuming and expensive. It is impossible to annotate each domain of interest to build precise domaindependent lexicons. It is more desirable to automatically construct precise lexicons in domains of interest by transferring knowledge from other domains. In this paper, we focus on the co-extraction task of sentiment and topic lexicons in</context>
<context position="5772" citStr="Jin and Ho (2009)" startWordPosition="902" endWordPosition="905">ntiment words, independently. Wiebe et al. (2004) and Rioff et al. (2003) proposed to identify subjective adjectives and nouns using word clustering based on their distributional similarity. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection. Some researchers also proposed to use topic modeling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both proposed to use supervised sequential labeling methods for topic and opinion extraction. Experimental results showed that the supervised learning methods can achieve state-of-the-art performance on lexicon extraction. However, these methods need to manually annotate a lot of training data in each domain. Recently, Qiu et al. (2009) proposed a rule-based semi-supervised learning methods for lexicon extraction. However, their method requires to manually define some general syntactic rules among sentiment and topic words. In addition, it still requires some annotated w</context>
</contexts>
<marker>Jin, Ho, 2009</marker>
<rawString>Wei Jin and Hung Hay Ho. 2009. A novel lexicalized HMM-based learning framework for web opinion mining. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 465– 472, Montreal, Quebec, Canada. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Andrew Mccallum</author>
<author>Kamal Nigam</author>
<author>Ellen Riloff</author>
</authors>
<title>Bootstrapping for text learning tasks.</title>
<date>1999</date>
<booktitle>In In IJCAI-99 Workshop on Text Mining: Foundations, Techniques and Applications,</booktitle>
<pages>52--63</pages>
<contexts>
<context position="14975" citStr="Jones et al., 1999" startWordPosition="2427" endWordPosition="2430">n, we propose a new bootstrapping-based method to address this problem. Bootstrapping is the process of improving the performance of a weak classifier by iteratively adding training data and retraining the classifier. More specifically, bootstrapping starts with a small set of labeled “seeds”, and iteratively adds unlabeled data that are labeled by the classifier to the training set based on some selection criterion, and retrain the classifier. Many bootstrapping-based algorithms have been proposed to information extraction and other NLP tasks (Blum and Mitchell, 1998; Riloff and Jones, 1999; Jones et al., 1999; Wu et al., 2009). One important issue in bootstrapping is how to design a criterion to select unlabeled data to be added to the training set iteratively. Our proposed bootstrapping for cross-domain lexicon extraction is based on the following two observations: 1) Although the source and target domains are different, part of source domain labeled data is still useful for lexicon extraction in the target domain after some adaptation; 2) The syntactic relationships among sentiment and topic words can be used to expand the seeds in the target domain for lexicon construction. Based on the two obs</context>
</contexts>
<marker>Jones, Mccallum, Nigam, Riloff, 1999</marker>
<rawString>Rosie Jones, Andrew Mccallum, Kamal Nigam, and Ellen Riloff. 1999. Bootstrapping for text learning tasks. In In IJCAI-99 Workshop on Text Mining: Foundations, Techniques and Applications, pages 52–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon M Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>J. ACM,</journal>
<pages>46--604</pages>
<contexts>
<context position="21590" citStr="Kleinberg, 1999" startWordPosition="3577" endWordPosition="3578"> (5) S3(wTi) = µ � �S1(wTj )�θij + (1 − µ)hTf�(wTi), (6) j where µ is a trade-off parameter between the predicted value by cross-domain classifier and the reinforcement scores from other nodes connected by script movie music NN-nsubj-VB-dobj-NN-amod-JJ NN-amod-JJ recommend boring good � �S2(Rj) = {wi,wk}EE new sentiment score ��S1(wTj) = µ �S3(wTi) i 414 edge ezj. Here p is empirically set to be 0.5. With Eqs. (5) and (6), the sentiment scores and topic scores are iteratively refined until the state of the graph trends to be stable. This can be considered as an extension to the HITS algorithm(Kleinberg, 1999). Finally, we select k1 ≪ k2 sentiment and topic words from the k2 candidates based on their refined scores, and add them to the target domain lexicons, respectively. We also update the sentiment score �S1 and topic score S3 for next iteration. 5.4 Special Cases We now introduce two special cases of the RAP algorithm. In Eqs. (5) and (6), if the parameter p = 1, then RAP only uses the relationships between sentiment and topic words with their patterns to propagate label information in the target domain without using the cross-domain classifier. We call this reduction relational bootstrapping. </context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. J. ACM, 46:604–632, Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shoushan Li</author>
<author>Chengqing Zong</author>
</authors>
<title>Multi-domain sentiment classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers,</booktitle>
<pages>257--260</pages>
<publisher>ACL.</publisher>
<location>Columbus, Ohio, USA.</location>
<marker>Li, Zong, 2008</marker>
<rawString>Shoushan Li and Chengqing Zong. 2008. Multi-domain sentiment classification. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 257–260, Columbus, Ohio, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Li</author>
<author>Vikas Sindhwani</author>
<author>Chris Ding</author>
<author>Yi Zhang</author>
</authors>
<title>Knowledge transformation for cross-domain sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>716--717</pages>
<publisher>ACM.</publisher>
<location>Boston, MA, USA.</location>
<contexts>
<context position="7075" citStr="Li et al., 2009" startWordPosition="1110" endWordPosition="1113">led data be available in the target domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better results on cross-domain lexicon extrac</context>
</contexts>
<marker>Li, Sindhwani, Ding, Zhang, 2009</marker>
<rawString>Tao Li, Vikas Sindhwani, Chris Ding, and Yi Zhang. 2009. Knowledge transformation for cross-domain sentiment classification. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 716–717, Boston, MA, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Chao Han</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
<author>Ying-Ju Xia</author>
<author>Shu Zhang</author>
<author>Hao Yu</author>
</authors>
<title>Structure-aware review mining and summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>653--661</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2492" citStr="Li et al. (2010" startWordPosition="377" endWordPosition="380">e or negative). The sentiment lexicon is domain dependent as users may use different sentiment words to express their opinion in different domains (e.g., different products). A topic lexicon is a list of topic expressions, on which the sentiment words are expressed. Extracting the topic lexicon from a specific domain is important because users not only care about the overall sentiment polarity of a review but also care about which aspects are mentioned in review. Note that, similar to sentiment lexicons, different domains may have very different topic lexicons. Recently, Jin and Ho (2009) and Li et al. (2010a) showed that supervised learning methods can achieve state-of-the-art results for lexicon extraction. However, the performance of these methods highly relies on manually annotated training data. In most cases, the labeling work may be timeconsuming and expensive. It is impossible to annotate each domain of interest to build precise domaindependent lexicons. It is more desirable to automatically construct precise lexicons in domains of interest by transferring knowledge from other domains. In this paper, we focus on the co-extraction task of sentiment and topic lexicons in a target domain whe</context>
<context position="5650" citStr="Li et al., 2010" startWordPosition="881" endWordPosition="884">al. (2004) proposed an association-rule-based method to extract topic words and a dictionary-based method to identify sentiment words, independently. Wiebe et al. (2004) and Rioff et al. (2003) proposed to identify subjective adjectives and nouns using word clustering based on their distributional similarity. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection. Some researchers also proposed to use topic modeling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both proposed to use supervised sequential labeling methods for topic and opinion extraction. Experimental results showed that the supervised learning methods can achieve state-of-the-art performance on lexicon extraction. However, these methods need to manually annotate a lot of training data in each domain. Recently, Qiu et al. (2009) proposed a rule-based semi-supervised learning methods for lexicon extraction. However, their method requires to man</context>
<context position="22680" citStr="Li et al., 2010" startWordPosition="3758" endWordPosition="3761">information in the target domain without using the cross-domain classifier. We call this reduction relational bootstrapping. If p = 0, then RAP only utilizes useful source domain labeled data to assist learning of the target domain classifier without considering the relationships between sentiment and topic words. We call this reduction adaptive bootstrapping, which can be considered as a bootstrapping version of TrAdaBoost. We also empirically study these two special cases in experiments. 6 Experiments on Lexicon Evaluation 6.1 Data Set and Evaluation Criteria We use the review dataset from (Li et al., 2010a), which contains 500 movie and 601 product reviews, for evaluation. The sentiment and topic words are manually annotated. In this dataset, all types of sentiment words are annotated instead of adjective words only. For example, the verbs, such as “like”, “recommend”, and nouns, such as “masterpiece”, are also labeled as sentiment words. We construct two cross-domain lexicon extraction tasks: “product vs. movie” and “movie vs. product”, where the word before “vs.” corresponds with the source domain and the word after “vs.” corresponds with the target domain. We evaluate our methods in terms o</context>
</contexts>
<marker>Li, Han, Huang, Zhu, Xia, Zhang, Yu, 2010</marker>
<rawString>Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010a. Structure-aware review mining and summarization. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 653–661, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Sentiment analysis with global topics and local dependency.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence,</booktitle>
<publisher>AAAI Press.</publisher>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="2492" citStr="Li et al. (2010" startWordPosition="377" endWordPosition="380">e or negative). The sentiment lexicon is domain dependent as users may use different sentiment words to express their opinion in different domains (e.g., different products). A topic lexicon is a list of topic expressions, on which the sentiment words are expressed. Extracting the topic lexicon from a specific domain is important because users not only care about the overall sentiment polarity of a review but also care about which aspects are mentioned in review. Note that, similar to sentiment lexicons, different domains may have very different topic lexicons. Recently, Jin and Ho (2009) and Li et al. (2010a) showed that supervised learning methods can achieve state-of-the-art results for lexicon extraction. However, the performance of these methods highly relies on manually annotated training data. In most cases, the labeling work may be timeconsuming and expensive. It is impossible to annotate each domain of interest to build precise domaindependent lexicons. It is more desirable to automatically construct precise lexicons in domains of interest by transferring knowledge from other domains. In this paper, we focus on the co-extraction task of sentiment and topic lexicons in a target domain whe</context>
<context position="5650" citStr="Li et al., 2010" startWordPosition="881" endWordPosition="884">al. (2004) proposed an association-rule-based method to extract topic words and a dictionary-based method to identify sentiment words, independently. Wiebe et al. (2004) and Rioff et al. (2003) proposed to identify subjective adjectives and nouns using word clustering based on their distributional similarity. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection. Some researchers also proposed to use topic modeling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both proposed to use supervised sequential labeling methods for topic and opinion extraction. Experimental results showed that the supervised learning methods can achieve state-of-the-art performance on lexicon extraction. However, these methods need to manually annotate a lot of training data in each domain. Recently, Qiu et al. (2009) proposed a rule-based semi-supervised learning methods for lexicon extraction. However, their method requires to man</context>
<context position="22680" citStr="Li et al., 2010" startWordPosition="3758" endWordPosition="3761">information in the target domain without using the cross-domain classifier. We call this reduction relational bootstrapping. If p = 0, then RAP only utilizes useful source domain labeled data to assist learning of the target domain classifier without considering the relationships between sentiment and topic words. We call this reduction adaptive bootstrapping, which can be considered as a bootstrapping version of TrAdaBoost. We also empirically study these two special cases in experiments. 6 Experiments on Lexicon Evaluation 6.1 Data Set and Evaluation Criteria We use the review dataset from (Li et al., 2010a), which contains 500 movie and 601 product reviews, for evaluation. The sentiment and topic words are manually annotated. In this dataset, all types of sentiment words are annotated instead of adjective words only. For example, the verbs, such as “like”, “recommend”, and nouns, such as “masterpiece”, are also labeled as sentiment words. We construct two cross-domain lexicon extraction tasks: “product vs. movie” and “movie vs. product”, where the word before “vs.” corresponds with the source domain and the word after “vs.” corresponds with the target domain. We evaluate our methods in terms o</context>
</contexts>
<marker>Li, Huang, Zhu, 2010</marker>
<rawString>Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010b. Sentiment analysis with global topics and local dependency. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, Atlanta, Georgia, USA. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and subjectivity.</title>
<date>2010</date>
<booktitle>Handbook of Natural Language Processing, Second Edition.</booktitle>
<contexts>
<context position="1631" citStr="Liu, 2010" startWordPosition="240" endWordPosition="241">d topic seeds in the target domain. In the second step, we propose a novel Relational Adaptive bootstraPping (RAP) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation. 1 Introduction In the past few years, opinion mining and sentiment analysis have attracted much attention in Natural Language Processing (NLP) and Information Retrieval (IR) (Pang and Lee, 2008; Liu, 2010). Sentiment lexicon construction and topic lexicon extraction are two fundamental subtasks for opinion mining (Qiu et al., 2009). A sentiment lexicon is a list of sentiment expressions, which are used to indicate sentiment polarity (e.g., positive or negative). The sentiment lexicon is domain dependent as users may use different sentiment words to express their opinion in different domains (e.g., different products). A topic lexicon is a list of topic expressions, on which the sentiment words are expressed. Extracting the topic lexicon from a specific domain is important because users not only</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>Bing Liu. 2010. Sentiment analysis and subjectivity. Handbook of Natural Language Processing, Second Edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>171--180</pages>
<publisher>ACM.</publisher>
<location>Banff, Alberta, Canada.</location>
<contexts>
<context position="5588" citStr="Mei et al., 2007" startWordPosition="868" endWordPosition="871">ing techniques have been proposed for this task. Hu and Liu et al. (2004) proposed an association-rule-based method to extract topic words and a dictionary-based method to identify sentiment words, independently. Wiebe et al. (2004) and Rioff et al. (2003) proposed to identify subjective adjectives and nouns using word clustering based on their distributional similarity. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection. Some researchers also proposed to use topic modeling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both proposed to use supervised sequential labeling methods for topic and opinion extraction. Experimental results showed that the supervised learning methods can achieve state-of-the-art performance on lexicon extraction. However, these methods need to manually annotate a lot of training data in each domain. Recently, Qiu et al. (2009) proposed a rule-based semi-supervised learning methods</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the 16th international conference on World Wide Web, pages 171–180, Banff, Alberta, Canada. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Qiang Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>IEEE Trans. Knowl. Data Eng.,</journal>
<volume>22</volume>
<issue>10</issue>
<contexts>
<context position="6652" citStr="Pan and Yang, 2010" startWordPosition="1039" endWordPosition="1042">methods need to manually annotate a lot of training data in each domain. Recently, Qiu et al. (2009) proposed a rule-based semi-supervised learning methods for lexicon extraction. However, their method requires to manually define some general syntactic rules among sentiment and topic words. In addition, it still requires some annotated words in the target domain. In this paper, we do not assume any predefined rules and labeled data be available in the target domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which</context>
</contexts>
<marker>Pan, Yang, 2010</marker>
<rawString>Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22(10):1345–1359, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Qiang Yang</author>
<author>Chen Zheng</author>
</authors>
<title>Cross-domain sentiment classification via spectral feature alignment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International Conference on World Wide Web,</booktitle>
<pages>751--760</pages>
<publisher>ACM.</publisher>
<location>Raleigh, NC, USA,</location>
<contexts>
<context position="7093" citStr="Pan et al., 2010" startWordPosition="1114" endWordPosition="1117">able in the target domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better results on cross-domain lexicon extraction. Note that ou</context>
<context position="13137" citStr="Pan et al., 2010" startWordPosition="2122" endWordPosition="2125">has(VB) good(JJ) movie(NN) the(DT) 412 4.1 Sentiment Seed Generation To identify common sentiment words across domains, we extract all sentiment words from the source domain as candidates. For each candidate, we calculate its score based on the following metric: S1(wi) = (pS(wi) + pT(wi))e(−|pS(wi)−pT (wi)|), (1) where pS(wi) and pT(wi) are the probabilities of the word wi occurring in the source and target domains, respectively. If a word wi has high S1 score, which implies that the word wi occurs frequently and similarly in both domains, then it can be considered as a common sentiment word (Pan et al., 2010; Blitzer et al., 2007). We select top r candidates with highest S1 scores as sentiment seeds. 4.2 Topic Seed Generation We extract all patterns between sentiment and topic words in the source domain as candidates. For each pattern candidate, we calculate its score based on a metric defined in AutoSlog-TS (Riloff, 1996): S2(Rj) = Acc(Rj) x log2(Freq(Rj)), (2) where Acc(Rj) is the accuracy of the pattern Rj in the source domain, and Freq(Rj) is the frequency of the pattern Rj observed in target domain. This metric aims to identify the patterns that are precise in the source domain and observed </context>
</contexts>
<marker>Pan, Ni, Sun, Yang, Zheng, 2010</marker>
<rawString>Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, and Chen Zheng. 2010. Cross-domain sentiment classification via spectral feature alignment. In Proceedings of the 19th International Conference on World Wide Web, pages 751–760, Raleigh, NC, USA, Apr. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<publisher>ACL.</publisher>
<location>Barcelona,</location>
<contexts>
<context position="30426" citStr="Pang and Lee, 2004" startWordPosition="5052" endWordPosition="5055">f r (a) Sentiment word extraction 0.45 5 10 15 20 25 30 Values of r (b) Topic word extraction 0.7 0.65 0.6 0.45 0.7 0.4 0 10 20 30 40 Number of iterations Relational Adaptive 416 Figures 5(a) and 5(b) show the results under varying numbers of iterations in the “product vs. movie” task. As we can see, our proposed methods converge well when M ≥ 40. 7 Application: Sentiment Classification To further verify the usefulness of the lexicons extracted by the RAP method, we apply the extracted sentiment lexicon for sentiment classification. 7.1 Experiment Setting Our work is motivated by the work of (Pang and Lee, 2004), which only used subjective sentences for document-level sentiment classification, instead of using all sentences. In this experiment, we only use sentiment related words as features to represent opinion documents for classification, instead of using all words. Our goal is compare the sentiment lexicon constructed by the RAP method with other general lexicons on the impact of for sentiment classification. The general lexicons used for comparison are described in Table 4. We use the dataset from (Blitzer et al., 2007) for sentiment classification. It contains a collection of product reviews fr</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, Barcelona, Spain. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="1619" citStr="Pang and Lee, 2008" startWordPosition="236" endWordPosition="239">fidence sentiment and topic seeds in the target domain. In the second step, we propose a novel Relational Adaptive bootstraPping (RAP) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation. 1 Introduction In the past few years, opinion mining and sentiment analysis have attracted much attention in Natural Language Processing (NLP) and Information Retrieval (IR) (Pang and Lee, 2008; Liu, 2010). Sentiment lexicon construction and topic lexicon extraction are two fundamental subtasks for opinion mining (Qiu et al., 2009). A sentiment lexicon is a list of sentiment expressions, which are used to indicate sentiment polarity (e.g., positive or negative). The sentiment lexicon is domain dependent as users may use different sentiment words to express their opinion in different domains (e.g., different products). A topic lexicon is a list of topic expressions, on which the sentiment words are expressed. Extracting the topic lexicon from a specific domain is important because us</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>339--346</pages>
<publisher>ACL.</publisher>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="5372" citStr="Popescu and Etzioni (2005)" startWordPosition="835" endWordPosition="838">emonstrate the effectiveness of our methods. 2 Related Work 2.1 Sentiment or Topic Lexicon Extraction Sentiment or topic lexicon extraction is to identify the sentiment or topic words from text. In the past, many machine learning techniques have been proposed for this task. Hu and Liu et al. (2004) proposed an association-rule-based method to extract topic words and a dictionary-based method to identify sentiment words, independently. Wiebe et al. (2004) and Rioff et al. (2003) proposed to identify subjective adjectives and nouns using word clustering based on their distributional similarity. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection. Some researchers also proposed to use topic modeling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both proposed to use supervised sequential labeling methods for topic and opinion extraction. Experimental results showed that the supervised learning methods can achieve state-</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 339–346, Vancouver, British Columbia, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Expanding domain sentiment lexicon through double propagation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st international jont conference on Artifical intelligence,</booktitle>
<pages>1199--1204</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>Pasadena, California, USA.</location>
<contexts>
<context position="1759" citStr="Qiu et al., 2009" startWordPosition="259" endWordPosition="262">thm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation. 1 Introduction In the past few years, opinion mining and sentiment analysis have attracted much attention in Natural Language Processing (NLP) and Information Retrieval (IR) (Pang and Lee, 2008; Liu, 2010). Sentiment lexicon construction and topic lexicon extraction are two fundamental subtasks for opinion mining (Qiu et al., 2009). A sentiment lexicon is a list of sentiment expressions, which are used to indicate sentiment polarity (e.g., positive or negative). The sentiment lexicon is domain dependent as users may use different sentiment words to express their opinion in different domains (e.g., different products). A topic lexicon is a list of topic expressions, on which the sentiment words are expressed. Extracting the topic lexicon from a specific domain is important because users not only care about the overall sentiment polarity of a review but also care about which aspects are mentioned in review. Note that, sim</context>
<context position="6133" citStr="Qiu et al. (2009)" startWordPosition="958" endWordPosition="961">deling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both proposed to use supervised sequential labeling methods for topic and opinion extraction. Experimental results showed that the supervised learning methods can achieve state-of-the-art performance on lexicon extraction. However, these methods need to manually annotate a lot of training data in each domain. Recently, Qiu et al. (2009) proposed a rule-based semi-supervised learning methods for lexicon extraction. However, their method requires to manually define some general syntactic rules among sentiment and topic words. In addition, it still requires some annotated words in the target domain. In this paper, we do not assume any predefined rules and labeled data be available in the target domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to</context>
<context position="24006" citStr="Qiu et al., 2009" startWordPosition="3965" endWordPosition="3968">n plenty of target domain labeled data, can be treated as upper-bounds. We denote iSVM and iCRF the in-domain SVM and CRF classifiers in experiments, and compare our proposed methods, RAP, relational bootstrapping, and adaptive bootstrapping, with the following baselines, Unsupervised Method (Un) we implement a rulebased method for lexicon extraction based on (Hu and Liu, 2004), where adjective words that match a rule is recognized as sentiment words, and nouns that match a rule are recognized as topic words. Semi-Supervised Method (Semi) we implement the double propagation model proposed in (Qiu et al., 2009). Since this method requires some target domain labeled data, we manually label 30 sentiment words in the target domain. Cross-Domain CRF (Cross-CRF) we implement a cross-domain CRF algorithm proposed by (Jakob and Gurevych, 2010). TrAdaBoost We apply TrAdaBoost (Dai et al., 2007) on the source domain labeled data and the generated seeds in the target domain to train a lexicon extractor. 6.3 Comparison Results Comparison results on lexicon extraction are shown in Table 2 and Table 3. From Table 2, we can observe that our proposed methods are effective for sentiment lexicon extraction. The rela</context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2009</marker>
<rawString>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double propagation. In Proceedings of the 21st international jont conference on Artifical intelligence, pages 1199– 1204, Pasadena, California, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the 6th national conference on Artificial intelligence,</booktitle>
<pages>474--479</pages>
<publisher>AAAI.</publisher>
<location>Orlando, Florida, United States.</location>
<contexts>
<context position="14955" citStr="Riloff and Jones, 1999" startWordPosition="2423" endWordPosition="2426">lexicons. In this section, we propose a new bootstrapping-based method to address this problem. Bootstrapping is the process of improving the performance of a weak classifier by iteratively adding training data and retraining the classifier. More specifically, bootstrapping starts with a small set of labeled “seeds”, and iteratively adds unlabeled data that are labeled by the classifier to the training set based on some selection criterion, and retrain the classifier. Many bootstrapping-based algorithms have been proposed to information extraction and other NLP tasks (Blum and Mitchell, 1998; Riloff and Jones, 1999; Jones et al., 1999; Wu et al., 2009). One important issue in bootstrapping is how to design a criterion to select unlabeled data to be added to the training set iteratively. Our proposed bootstrapping for cross-domain lexicon extraction is based on the following two observations: 1) Although the source and target domains are different, part of source domain labeled data is still useful for lexicon extraction in the target domain after some adaptation; 2) The syntactic relationships among sentiment and topic words can be used to expand the seeds in the target domain for lexicon construction. </context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the 6th national conference on Artificial intelligence, pages 474–479, Orlando, Florida, United States. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning subjective nouns using extraction pattern bootstrapping.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th conference on natural language learning,</booktitle>
<pages>25--32</pages>
<publisher>ACL.</publisher>
<location>Edmonton, Canada.</location>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. Learning subjective nouns using extraction pattern bootstrapping. In Proceedings of the 7th conference on natural language learning, pages 25–32, Edmonton, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1044--1049</pages>
<publisher>AAAI Press/MIT Press.</publisher>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="13458" citStr="Riloff, 1996" startWordPosition="2176" endWordPosition="2177">wi) and pT(wi) are the probabilities of the word wi occurring in the source and target domains, respectively. If a word wi has high S1 score, which implies that the word wi occurs frequently and similarly in both domains, then it can be considered as a common sentiment word (Pan et al., 2010; Blitzer et al., 2007). We select top r candidates with highest S1 scores as sentiment seeds. 4.2 Topic Seed Generation We extract all patterns between sentiment and topic words in the source domain as candidates. For each pattern candidate, we calculate its score based on a metric defined in AutoSlog-TS (Riloff, 1996): S2(Rj) = Acc(Rj) x log2(Freq(Rj)), (2) where Acc(Rj) is the accuracy of the pattern Rj in the source domain, and Freq(Rj) is the frequency of the pattern Rj observed in target domain. This metric aims to identify the patterns that are precise in the source domain and observed frequently in the target domain. We also select the top r patterns with highest S2 scores. With the patterns and sentiment seeds, we extract topic-word candidates and measure their scores based on a variant metric of quadratic combination (Zhang and Ye, 2008): �S3(wk) = (S2(Rj) x S1(wi)), (3) RHEA, wiEB where B is a set</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1044–1049, Portland, Oregon, USA. AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Songbo Tan</author>
<author>Gaowei Wu</author>
<author>Huifeng Tang</author>
<author>Xueqi Cheng</author>
</authors>
<title>A novel scheme for domain-transfer problem in the context of sentiment analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th ACM conference on Conference on information and knowledge management,</booktitle>
<pages>979--982</pages>
<publisher>ACM.</publisher>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="7058" citStr="Tan et al., 2007" startWordPosition="1106" endWordPosition="1109">ned rules and labeled data be available in the target domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better results on cross-doma</context>
</contexts>
<marker>Tan, Wu, Tang, Cheng, 2007</marker>
<rawString>Songbo Tan, Gaowei Wu, Huifeng Tang, and Xueqi Cheng. 2007. A novel scheme for domain-transfer problem in the context of sentiment analysis. In Proceedings of the 16th ACM conference on Conference on information and knowledge management, pages 979–982, Lisbon, Portugal. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies,</booktitle>
<pages>308--316</pages>
<publisher>ACL.</publisher>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="5614" citStr="Titov and McDonald, 2008" startWordPosition="872" endWordPosition="876">e been proposed for this task. Hu and Liu et al. (2004) proposed an association-rule-based method to extract topic words and a dictionary-based method to identify sentiment words, independently. Wiebe et al. (2004) and Rioff et al. (2003) proposed to identify subjective adjectives and nouns using word clustering based on their distributional similarity. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection. Some researchers also proposed to use topic modeling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both proposed to use supervised sequential labeling methods for topic and opinion extraction. Experimental results showed that the supervised learning methods can achieve state-of-the-art performance on lexicon extraction. However, these methods need to manually annotate a lot of training data in each domain. Recently, Qiu et al. (2009) proposed a rule-based semi-supervised learning methods for lexicon extraction. H</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies, pages 308–316, Columbus, Ohio, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Comput. Linguist.,</journal>
<pages>30--277</pages>
<contexts>
<context position="5204" citStr="Wiebe et al. (2004)" startWordPosition="811" endWordPosition="814">-step domain adaptation framework, with a novel RAP algorithm for seed expansion, is proposed. 3) We conduct extensive evaluation, and the experimental results demonstrate the effectiveness of our methods. 2 Related Work 2.1 Sentiment or Topic Lexicon Extraction Sentiment or topic lexicon extraction is to identify the sentiment or topic words from text. In the past, many machine learning techniques have been proposed for this task. Hu and Liu et al. (2004) proposed an association-rule-based method to extract topic words and a dictionary-based method to identify sentiment words, independently. Wiebe et al. (2004) and Rioff et al. (2003) proposed to identify subjective adjectives and nouns using word clustering based on their distributional similarity. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection. Some researchers also proposed to use topic modeling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both prop</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Comput. Linguist., 30:277–308, Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<publisher>ACL.</publisher>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="31698" citStr="Wilson et al., 2005" startWordPosition="5251" endWordPosition="5254">mains: books, dvds, electronics and kitchen appliance. In each domain, there are 1000 positive and 1000 negative reviews. To construct domain specific sentiment lexicons, we apply RAP on each product domain with the movie domain described in Section 6.1 as the source domain. Finally, we use linear SVM as the classifier and the classification accuracy as the evaluate criterion. Lexicon Name Size Description Senti-WordNet 6957 Words with a subjective score &gt; 0.6 (Esuli and Sebastiani, 2006) HowNet 4619 Eng. translation of subj. Chinese words (Dong and Dong, 2006) Subj. Clues 6878 Lexicons from (Wilson et al., 2005) Table 4: Description of different lexicons. 7.2 Experimental Results Experimental results on sentiment classification are shown in Table 5, where we denote “All” using all unigram and bigram features instead of using subjective words. As we can see that a classifier trained with features constructed by our RAP method performance best in all domains. Note that the number of features (sentiment words) constructed by our method is much smaller than that of all unigram and bigram features, which can reduce the classifier training time dramatically. These promising results imply that our RAP can b</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354, Vancouver, British Columbia, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Wu</author>
<author>Wee Sun Lee</author>
<author>Nan Ye</author>
<author>Hai Leong Chieu</author>
</authors>
<title>Domain adaptive bootstrapping for named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1523--1532</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="6950" citStr="Wu et al., 2009" startWordPosition="1087" endWordPosition="1090">on, it still requires some annotated words in the target domain. In this paper, we do not assume any predefined rules and labeled data be available in the target domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our expe</context>
<context position="14993" citStr="Wu et al., 2009" startWordPosition="2431" endWordPosition="2434">bootstrapping-based method to address this problem. Bootstrapping is the process of improving the performance of a weak classifier by iteratively adding training data and retraining the classifier. More specifically, bootstrapping starts with a small set of labeled “seeds”, and iteratively adds unlabeled data that are labeled by the classifier to the training set based on some selection criterion, and retrain the classifier. Many bootstrapping-based algorithms have been proposed to information extraction and other NLP tasks (Blum and Mitchell, 1998; Riloff and Jones, 1999; Jones et al., 1999; Wu et al., 2009). One important issue in bootstrapping is how to design a criterion to select unlabeled data to be added to the training set iteratively. Our proposed bootstrapping for cross-domain lexicon extraction is based on the following two observations: 1) Although the source and target domains are different, part of source domain labeled data is still useful for lexicon extraction in the target domain after some adaptation; 2) The syntactic relationships among sentiment and topic words can be used to expand the seeds in the target domain for lexicon construction. Based on the two observations, we prop</context>
</contexts>
<marker>Wu, Lee, Ye, Chieu, 2009</marker>
<rawString>Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu. 2009. Domain adaptive bootstrapping for named entity recognition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1523–1532, Singapore. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Xingyao Ye</author>
</authors>
<title>A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>411--418</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13996" citStr="Zhang and Ye, 2008" startWordPosition="2265" endWordPosition="2268">e, we calculate its score based on a metric defined in AutoSlog-TS (Riloff, 1996): S2(Rj) = Acc(Rj) x log2(Freq(Rj)), (2) where Acc(Rj) is the accuracy of the pattern Rj in the source domain, and Freq(Rj) is the frequency of the pattern Rj observed in target domain. This metric aims to identify the patterns that are precise in the source domain and observed frequently in the target domain. We also select the top r patterns with highest S2 scores. With the patterns and sentiment seeds, we extract topic-word candidates and measure their scores based on a variant metric of quadratic combination (Zhang and Ye, 2008): �S3(wk) = (S2(Rj) x S1(wi)), (3) RHEA, wiEB where B is a set of sentiment seeds and A is a set of patterns which the words wi and wk satisfy. We then select the top r candidates as topic seeds. 5 Seed Expansion After generating the topic and sentiment seeds, we aim to expand them in the target domain to construct topic and sentiment lexicons. In this section, we propose a new bootstrapping-based method to address this problem. Bootstrapping is the process of improving the performance of a weak classifier by iteratively adding training data and retraining the classifier. More specifically, bo</context>
</contexts>
<marker>Zhang, Ye, 2008</marker>
<rawString>Min Zhang and Xingyao Ye. 2008. A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 411–418, Singapore. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
<author>Hongfei Yan</author>
<author>Xiaoming Li</author>
</authors>
<title>Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>56--65</pages>
<publisher>ACL.</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="5633" citStr="Zhao et al., 2010" startWordPosition="877" endWordPosition="880">ask. Hu and Liu et al. (2004) proposed an association-rule-based method to extract topic words and a dictionary-based method to identify sentiment words, independently. Wiebe et al. (2004) and Rioff et al. (2003) proposed to identify subjective adjectives and nouns using word clustering based on their distributional similarity. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection. Some researchers also proposed to use topic modeling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both proposed to use supervised sequential labeling methods for topic and opinion extraction. Experimental results showed that the supervised learning methods can achieve state-of-the-art performance on lexicon extraction. However, these methods need to manually annotate a lot of training data in each domain. Recently, Qiu et al. (2009) proposed a rule-based semi-supervised learning methods for lexicon extraction. However, their metho</context>
</contexts>
<marker>Zhao, Jiang, Yan, Li, 2010</marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010. Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 56–65, Cambridge, Massachusetts, USA. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>