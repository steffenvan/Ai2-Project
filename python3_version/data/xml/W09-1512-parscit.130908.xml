<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.045383">
<title confidence="0.9965025">
Integrating High Precision Rules with Statistical Sequence Classifiers for
Accuracy and Speed
</title>
<author confidence="0.986714">
Wenhui Liao, Marc Light, and Sriharsha Veeramachaneni
</author>
<affiliation confidence="0.5457435">
Research and Development,Thomson Reuters
610 Opperman Drive, Eagan MN 55123
</affiliation>
<sectionHeader confidence="0.975303" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99968125">
Integrating rules and statistical systems is a
challenge often faced by natural language pro-
cessing system builders. A common sub-
class is integrating high precision rules with a
Markov statistical sequence classifier. In this
paper we suggest that using such rules to con-
strain the sequence classifier decoder results
in superior accuracy and efficiency. In a case
study of a named entity tagging system, we
provide evidence that this method of combina-
tion does prove efficient than other methods.
The accuracy was the same.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992979245283">
Sequence classification lies at the core of several
natural language processing applications, such as
named entity extraction, Asian language segmen-
tation, Germanic language noun decompounding,
and event identification. Statistical models with a
Markov dependency have been successful employed
to perform these tasks, e.g., hidden Markov mod-
els (HMMs)(Rabiner, 1989) and conditional random
fields (CRFs)(Lafferty et al., 2001). These statistical
systems employ a Viterbi (Forney, 1973) decoder at
runtime to efficiently calculate the most likely la-
bel sequence based on the observed sequence and
model. Statistical machine translation systems make
use of similar decoders.
In many situations it is beneficial, and some-
times required, for these systems to respect con-
straints from high precision rules. And thus when
building working sequence labeling systems, re-
searchers/software engineers are often faced with
the task of combining these two approaches. In
this paper we argue for a particular method of com-
bining statistical models with Markov dependencies
and high precision rules. We outline a number of
ways to do this and then argue that guiding the de-
coder of the statistical system has many advantages
over other methods of combination.
But first, does the problem of combining multi-
ple approaches really happen? In our experience the
need arises in the following way: a statistical ap-
proach with a Markov component is chosen because
it has the best precision/recall characteristics and has
reasonable speed. However, a number of rules arise
for varied reasons. For example, the customer pro-
vides domain knowledge not present in the training
data or a particular output characteristic is more im-
portant that accuracy. Consider the following ficti-
tious but plausible situation: A named entity tagging
system is built using a CRF. The customer then pro-
vides a number of company names that cannot be
missed, i.e., false negatives for these companies are
catastrophic but false positives can be tolerated. In
addition, it is known that, unlike in the training data,
the runtime data will have a company name immedi-
ately before every ticker symbol. The question fac-
ing the builder of the system is how to combine the
CRF with rules based on the must-find company list
and the company-name-before-every-ticker-symbol
fact.
Similar situations arise for the other sequence tag-
ging situations mentioned above and for machine
translation. We suspect that even for non-language
applications, such as gene sequence labeling, similar
situations arise.
</bodyText>
<page confidence="0.988512">
74
</page>
<note confidence="0.795861">
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 74–77,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999926833333333">
In the next section we will discuss a number of
methods for combining statistical systems and high
precision rules and argue for guiding the decoder
of the statistical model. Then in section 3, we de-
scribe an implementation of the approach and give
evidence that the speed benefits are substantial.
</bodyText>
<sectionHeader confidence="0.987615" genericHeader="method">
2 Methods for Combining a Markov
</sectionHeader>
<subsectionHeader confidence="0.891632">
Statistical System and High Precision
Rules
</subsectionHeader>
<bodyText confidence="0.992341411764706">
One method of combination is to encode high preci-
sion rules as features and then train a new model that
includes these features. One advantage is that the
system stays a straightforward statistical system. In
addition, the rules are fully integrated into the sys-
tem allowing the statistical model weigh the rules
against other evidence. However, the model may
not give the rules high weight if training data does
not bear out their high precision or if the rule trig-
ger does not occur often enough in the training data.
Thus, despite a “rule” feature being on, the system
may not “follow” the rule in its result labeling. Also,
addition or modification of a rule would require a
retraining of the model for optimal accuracy. The
retraining process may be costly and/or may not be
possible in the operational environment.
Another method is to run both the statistical sys-
tem and the rules and then merge the resulting labels
giving preference to the labels resulting from the
high precision rules. The benefits are that the rules
are always followed. However, the statistical system
does not have the information needed to give an op-
timal solution based on the results of the high preci-
sion rules. In other words, the results will be incon-
sistent from the view of the statistical system; i.e., if
it had know what the rules were going to say, then it
would have calculated the remaining part of the label
sequence differently. In addition, the decoder con-
siders part of the label sequence search space that is
only going to be ruled out, pun intended, later.
Now for the preferred method: run the rules first,
then use their output to guide the decoder for the
statistical model. The benefits of this method are
that the rules are followed, the statistical system is
informed of constraints imposed by the rules and
thus the statistical system calculates optimal paths
given these constraints. In addition, the decoder
considers only those label sequences consistent with
these constraints, resulting in a smaller search space.
Thus, we would expect this method to produce both
a more accurate and a faster implementation.
Consider Figure 1 which shows a lattice that rep-
resents all the labeling sequences for the input ...
Microsoft on Monday announced a ... The possible
labels are O (out), P (person), C (company), L (lo-
cation) . Assume Microsoft is in a list of must-find
companies and that on and Monday are part of a rule
that makes them NOT names in this context. The
bold points are constraints from the high-precision
rules. In other words, only sequences that include
these bold points need to be considered.
</bodyText>
<figureCaption confidence="0.8679475">
Figure 1: Guiding decoding with high-precision rules
Figure 1 also illustrates how the constraints re-
</figureCaption>
<bodyText confidence="0.990342625">
duce the search space. Without constraints, the
search space includes 46 = 4096 sequences, while
with constraints, it includes only 43 = 64.
It should also be noted that we do not claim to
have invented the idea of constraining the decoder.
For example, in the context of active learning, where
a human corrects some of the errors made by a CRF
sequence classifier, (Culota et al., 2006) proposed a
constrained Viterbi algorithm that finds the path with
maximum probability that passes through the labels
assigned by the human. They showed that constrain-
ing the path to respect the human labeling consider-
ably improves the accuracy on the remaining tokens
in the sequence. Our contribution is noticing that
constraining the decoder is a good way to integrate
rule output.
</bodyText>
<sectionHeader confidence="0.997214" genericHeader="method">
3 A Case Study: Named Entity
Recognition
</sectionHeader>
<bodyText confidence="0.99956925">
In this section, we flesh out the discussion of named
entity (NE) tagging started above. Since the entity
type of a word is determined mostly by the context
of the word, NE tagging is often posed as a sequence
</bodyText>
<page confidence="0.997416">
75
</page>
<bodyText confidence="0.9974005">
classification problem and solved by Markov statis-
tical systems.
</bodyText>
<subsectionHeader confidence="0.997061">
3.1 A Named Entity Recognition System
</subsectionHeader>
<bodyText confidence="0.999866642857143">
The system described here starts with a CRF which
was chosen because it allows for the use of numer-
ous and arbitrary features of the input sequence and
it can be efficiently trained and decoded. We used
the Mallet toolkit (McCallum, 2002) for training the
CRF but implemented our own feature extraction
and runtime system. We used standard features such
as the current word, the word to the right/left, ortho-
graphic shape of the word, membership in word sets
(e.g., common last names), features of neighboring
words, etc.
The system was designed to run on news wire text
and based on this data’s characteristics, we designed
a handful of high precision rules including:
</bodyText>
<construct confidence="0.865996789473684">
Rule 1: if a token is in a must-tag list, this token
should be marked as Company no matter what the
context is.
Rule 2: if a capitalized word is followed by cer-
tain company suffix such as Ltd, Inc, Corp, etc., la-
bel both as Company.
Rule 3: if a token sequence is in a company list
and the length of the sequence is larger than 3, label
them as Company.
Rule 4: if a token does not include any uppercase
letters, is not pure number, and is not in an excep-
tions list, label it as not part of a name. (The ex-
ceptions list includes around 70 words that are not
capitalized but still could be an NE, such as al, at,
in, -, etc.)
Rule 5: if a token does not satisfy rule 4 but its
neighboring tokens satisfy rule 4, then if this token
is a time related word, label it as not part of a name.
(Example time tokens are January and Monday.)
</construct>
<bodyText confidence="0.999920823529412">
The first three rules aim to find company names
and the last two to find tokens that are not part of a
name.
These rules are integrated into the system as de-
scribed in section 2: we apply the rules to the input
token sequence and then use the resulting labels, if
any, to constrain the Viterbi decoder for the CRF.
A further optimization of the system is based on
the following observation: features need not be cal-
culated for tokens that have already received labels
from the rules. (An exception to this is when fea-
tures are copied to a neighbor, e.g., the token to my
left is a number.) Thus, we do not calculate many
features of rule-labeled tokens. Note that feature ex-
traction can often be a major portion of the compu-
tational cost of sequence labeling systems (see Table
1(b))
</bodyText>
<subsectionHeader confidence="0.963108666666667">
3.2 Evidence of Computational Savings
Resulting from Our Proposed Method of
Integration
</subsectionHeader>
<bodyText confidence="0.9996731875">
We compare the results when high-precision rules
are integrated into CRF for name entity extraction
(company, person, and location) in terms of both ac-
curacy and speed for different corpora. Three cor-
pora are used, CoNLL (CoNLL 2003 English shared
task official test set), MUC (Message Understanding
Conference), and TF (includes around 1000 news ar-
ticles from Thomson Financial).
Table 1(a) shows the results for each corpora re-
spectively. The baseline method does not use any
high-precision rules, the Post-corr uses the high-
precision rules to correct the labeling from the CRF,
and Constr-viti uses the rules to constrain the label
sequences considered by the Viterbi decoder. In gen-
eral, Constr-viti achieves slightly better precision
and recall.
</bodyText>
<figureCaption confidence="0.848575">
Figure 2: (b) A test example : (a) without constraints; (b)
with constraints
</figureCaption>
<bodyText confidence="0.9494895">
To better understand how our strategy could im-
prove the accuracy, we did some analysis on the
</bodyText>
<page confidence="0.991492">
76
</page>
<tableCaption confidence="0.999475">
Table 1: Experiment Results
</tableCaption>
<table confidence="0.970118363636364">
Database Methods Precision Recall F1 Methods Rules Features Viterbi Overall
CoNLL Baseline 84.38 83.02 83.69 Baseline 0 0.78 0.22 1
Post-corr 85.87 84.86 85.36 Post-corr 0.08 0.78 0.22 1.08
Constr-viti 85.98 85.55 85.76 Constr-vite 0.08 0.35 0.13 0.56
TF Baseline 88.39 82.42 85.30 Baseline 0 0.85 0.15 1
Post-corr 87.69 88.30 87.99 Post-Corr 0.14 0.85 0.15 1.14
Constr-viti 88.02 88.54 88.28 Constr-vite 0.14 0.38 0.1 0.62
MUC Baseline 92.22 88.72 90.43 Baseline 0 0.79 0.21 1
Post-Corr 91.28 88.87 90.06 Post-corr 0.12 0.79 0.21 1.12
Constr-viti 90.86 89.37 90.11 Constr-vite 0.12 0.36 0.12 0.60
(a)Precision and Recall (b)Time Efficiency
</table>
<bodyText confidence="0.99920290625">
testing data. In one example as shown in Figure 2,
Steel works as an attorney, without high-precision
rules, Steel works is tagged as a company since it is
in our company list. Post-correction changes the la-
bel of works to O, but it is unable to fix Steel. With
our strategy, since works is pinned as O in the Vert-
ibi algorithm, Steel is tagged as Per. Thus, com-
pared to post-correction, the advantage of constrain-
ing Viterbi is that it is able to affect the whole path
where the token is, instead a token itself. However,
the improvements were not significant in our case
study. We have not done an error analysis. We can
only speculate that the high precision rules do not
have perfect precision and thus create a number of
errors that the statistical model would not have made
on its own.
We also measured how much the constrained
Viterbi method improves efficiency. We divide the
computational time to three parts: time in applying
rules, time in feature extraction, and time in Viterbi
computation. Table 1(b) lists the time efficiency. In-
stead using specific time unit (e.g. second), we use
ratio instead by assuming the overall time for the
baseline method is 1. As shown in the table, for
the three data sets, the overall time of our method
is 0.56, 0.62, and 0.60 of the time of the baseline
algorithm respectively. The post-correction method
is the most expensive one because of the extra time
spending in rules. Overall, the constrained Viterbi
method is substantially faster than the Baseline and
Post-corr methods in addition to being more accu-
rate.
</bodyText>
<sectionHeader confidence="0.999873" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999955818181818">
The contribution of this paper is the repurposing of
the idea of constraining a decoder: we constrain the
decoder as a way to integrate high precision rules
with a statistical sequence classifier. In a case study
of named entity tagging, we show that this method
of combination does in fact increase efficiency more
than competing methods without any lose of ac-
curacy. We believe analogous situations exist for
other sequence classifying tasks such as Asian lan-
guage segmentation, Germanic language noun de-
compounding, and event identification.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9986733125">
Aron Culota, Trausti Kristjansson, Andrew McCallum,
and Paul Viola. 2006. Corrective feedback and per-
sistent learning for information extraction. Artificial
Intelligence Journal, 170:1101–1122.
G. D. Forney. 1973. The viterbi algorithm. Proceedings
of the IEEE, 61(3):268–278.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282–289.
A.K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Lawrence R. Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, pages 257–286.
</reference>
<page confidence="0.99912">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.299267">
<title confidence="0.9945805">Integrating High Precision Rules with Statistical Sequence Classifiers for Accuracy and Speed</title>
<author confidence="0.8936">Wenhui Liao</author>
<author confidence="0.8936">Marc Light</author>
<author confidence="0.8936">Sriharsha Veeramachaneni</author>
<affiliation confidence="0.642347">Research and Development,Thomson Reuters</affiliation>
<address confidence="0.986318">610 Opperman Drive, Eagan MN 55123</address>
<abstract confidence="0.959298153846154">Integrating rules and statistical systems is a challenge often faced by natural language processing system builders. A common subclass is integrating high precision rules with a Markov statistical sequence classifier. In this paper we suggest that using such rules to constrain the sequence classifier decoder results in superior accuracy and efficiency. In a case study of a named entity tagging system, we provide evidence that this method of combination does prove efficient than other methods. The accuracy was the same.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aron Culota</author>
<author>Trausti Kristjansson</author>
<author>Andrew McCallum</author>
<author>Paul Viola</author>
</authors>
<title>Corrective feedback and persistent learning for information extraction.</title>
<date>2006</date>
<journal>Artificial Intelligence Journal,</journal>
<pages>170--1101</pages>
<contexts>
<context position="7025" citStr="Culota et al., 2006" startWordPosition="1125" endWordPosition="1128">are constraints from the high-precision rules. In other words, only sequences that include these bold points need to be considered. Figure 1: Guiding decoding with high-precision rules Figure 1 also illustrates how the constraints reduce the search space. Without constraints, the search space includes 46 = 4096 sequences, while with constraints, it includes only 43 = 64. It should also be noted that we do not claim to have invented the idea of constraining the decoder. For example, in the context of active learning, where a human corrects some of the errors made by a CRF sequence classifier, (Culota et al., 2006) proposed a constrained Viterbi algorithm that finds the path with maximum probability that passes through the labels assigned by the human. They showed that constraining the path to respect the human labeling considerably improves the accuracy on the remaining tokens in the sequence. Our contribution is noticing that constraining the decoder is a good way to integrate rule output. 3 A Case Study: Named Entity Recognition In this section, we flesh out the discussion of named entity (NE) tagging started above. Since the entity type of a word is determined mostly by the context of the word, NE t</context>
</contexts>
<marker>Culota, Kristjansson, McCallum, Viola, 2006</marker>
<rawString>Aron Culota, Trausti Kristjansson, Andrew McCallum, and Paul Viola. 2006. Corrective feedback and persistent learning for information extraction. Artificial Intelligence Journal, 170:1101–1122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Forney</author>
</authors>
<title>The viterbi algorithm.</title>
<date>1973</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>61--3</pages>
<contexts>
<context position="1258" citStr="Forney, 1973" startWordPosition="179" endWordPosition="180">ide evidence that this method of combination does prove efficient than other methods. The accuracy was the same. 1 Introduction Sequence classification lies at the core of several natural language processing applications, such as named entity extraction, Asian language segmentation, Germanic language noun decompounding, and event identification. Statistical models with a Markov dependency have been successful employed to perform these tasks, e.g., hidden Markov models (HMMs)(Rabiner, 1989) and conditional random fields (CRFs)(Lafferty et al., 2001). These statistical systems employ a Viterbi (Forney, 1973) decoder at runtime to efficiently calculate the most likely label sequence based on the observed sequence and model. Statistical machine translation systems make use of similar decoders. In many situations it is beneficial, and sometimes required, for these systems to respect constraints from high precision rules. And thus when building working sequence labeling systems, researchers/software engineers are often faced with the task of combining these two approaches. In this paper we argue for a particular method of combining statistical models with Markov dependencies and high precision rules.</context>
</contexts>
<marker>Forney, 1973</marker>
<rawString>G. D. Forney. 1973. The viterbi algorithm. Proceedings of the IEEE, 61(3):268–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1199" citStr="Lafferty et al., 2001" startWordPosition="169" endWordPosition="172">fficiency. In a case study of a named entity tagging system, we provide evidence that this method of combination does prove efficient than other methods. The accuracy was the same. 1 Introduction Sequence classification lies at the core of several natural language processing applications, such as named entity extraction, Asian language segmentation, Germanic language noun decompounding, and event identification. Statistical models with a Markov dependency have been successful employed to perform these tasks, e.g., hidden Markov models (HMMs)(Rabiner, 1989) and conditional random fields (CRFs)(Lafferty et al., 2001). These statistical systems employ a Viterbi (Forney, 1973) decoder at runtime to efficiently calculate the most likely label sequence based on the observed sequence and model. Statistical machine translation systems make use of similar decoders. In many situations it is beneficial, and sometimes required, for these systems to respect constraints from high precision rules. And thus when building working sequence labeling systems, researchers/software engineers are often faced with the task of combining these two approaches. In this paper we argue for a particular method of combining statistica</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="8005" citStr="McCallum, 2002" startWordPosition="1292" endWordPosition="1293">egrate rule output. 3 A Case Study: Named Entity Recognition In this section, we flesh out the discussion of named entity (NE) tagging started above. Since the entity type of a word is determined mostly by the context of the word, NE tagging is often posed as a sequence 75 classification problem and solved by Markov statistical systems. 3.1 A Named Entity Recognition System The system described here starts with a CRF which was chosen because it allows for the use of numerous and arbitrary features of the input sequence and it can be efficiently trained and decoded. We used the Mallet toolkit (McCallum, 2002) for training the CRF but implemented our own feature extraction and runtime system. We used standard features such as the current word, the word to the right/left, orthographic shape of the word, membership in word sets (e.g., common last names), features of neighboring words, etc. The system was designed to run on news wire text and based on this data’s characteristics, we designed a handful of high precision rules including: Rule 1: if a token is in a must-tag list, this token should be marked as Company no matter what the context is. Rule 2: if a capitalized word is followed by certain com</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>A.K. McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="1139" citStr="Rabiner, 1989" startWordPosition="163" endWordPosition="164">lassifier decoder results in superior accuracy and efficiency. In a case study of a named entity tagging system, we provide evidence that this method of combination does prove efficient than other methods. The accuracy was the same. 1 Introduction Sequence classification lies at the core of several natural language processing applications, such as named entity extraction, Asian language segmentation, Germanic language noun decompounding, and event identification. Statistical models with a Markov dependency have been successful employed to perform these tasks, e.g., hidden Markov models (HMMs)(Rabiner, 1989) and conditional random fields (CRFs)(Lafferty et al., 2001). These statistical systems employ a Viterbi (Forney, 1973) decoder at runtime to efficiently calculate the most likely label sequence based on the observed sequence and model. Statistical machine translation systems make use of similar decoders. In many situations it is beneficial, and sometimes required, for these systems to respect constraints from high precision rules. And thus when building working sequence labeling systems, researchers/software engineers are often faced with the task of combining these two approaches. In this pa</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, pages 257–286.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>