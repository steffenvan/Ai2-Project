<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000024">
<title confidence="0.9752995">
Automatic detecting/correcting errors in Chinese text by an
approximate word-matching algorithm
</title>
<author confidence="0.992236">
Lei Zhang
</author>
<affiliation confidence="0.9980345">
Dept. of Computer Science and Technology
Tsinghua University
</affiliation>
<address confidence="0.968562">
Beijing, China, 1000 84
</address>
<email confidence="0.996458">
zhl@s1000e.cs.tsinghua.edu.cn
</email>
<author confidence="0.956034">
Changning Huang
</author>
<affiliation confidence="0.969716">
Microsoft Research China
</affiliation>
<address confidence="0.902365">
Beijing, China, 1000 80
</address>
<email confidence="0.949289">
cnhuang@microsoft.com
</email>
<author confidence="0.817697">
Ming Zhou
</author>
<affiliation confidence="0.877256">
Microsoft Research China
</affiliation>
<address confidence="0.88539">
Beijing, China, 1000 80
</address>
<email confidence="0.981769">
mingzhou@microsoft.com
</email>
<author confidence="0.876514">
Haihua Pan
</author>
<affiliation confidence="0.99144">
Dept. of Chinese, Translation and Linguistics
City University of Hong Kong
Kowlong, Hong Kong, China
</affiliation>
<email confidence="0.955061">
cthpan@cityu.edu.hk
</email>
<sectionHeader confidence="0.996022" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999869333333333">
An approximate word-matching algorithm
for Chinese is presented. Based on this
algorithm, an effective approach to Chinese
spelling error detection and correction is
implemented. With a word tri-gram
language model, the optimal string is
searched from all possible derivation of the
input sentence using operations of character
substitution, insertion, and deletion.
Comparing the original sentence with the
optimal string, spelling error detection and
correction is realized simultaneously.
</bodyText>
<sectionHeader confidence="0.978186" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999961681818182">
No system aiming at automatic detecting
and correcting errors in Chinese text achieves
satisfying result today. One representative
approach is confusing character substitution
method (Chang, 1994), where confusing
characters are used to replace every character in
the input sentence, and a &amp;quot;correct&amp;quot; result with
highest evaluation score is searched from all
paths. While achieving relatively good result, it
has obvious weakness: only character
substitution errors can be detected and corrected,
other kinds of errors can not be handled,
including character deletion, character insertion,
and string substitution errors.
There is a clear two-level structure in
English spelling error detection and correction,
&amp;quot;non-word&amp;quot; error and &amp;quot;real-word&amp;quot; error (Kukich,
1992). Things are different in Chinese. Although
many approaches find that most errors in
Chinese cause segmentation abnormal (Sun,
1997 and Zhang, 199 8), no one stress on such
&amp;quot;non-word&amp;quot; error and the two-level structure is
not adopted in Chinese. Following are possible
reasons for this situation. There is no obvious
word boundary in Chinese text, so automatic
word segmentation must be introduced. If error
exists, segmentation result could be weird. For
example, the segmentation of ,&apos;VJ AkAk, which is
a character deletion error for the word MJ1,L4
Ak, may appear like,&apos;J&apos;./I(k/AC, including three
one-character words. It&apos;s easy for human to
judge thatTJ kAk is some kind of &amp;quot;non-word&amp;quot;
error, but it&apos;s difficult for computer to make such
a decision because most Chinese characters
could be used as a one-character word.
A fast approximate Chinese word-matching
algorithm is presented. Based on this algorithm,
a new automatic error detection and correction
approach using confusing word substitution is
implemented. Compared with the approach of
(Chang, 94), its distinguished feature is that not
only character substitution error, but also
character insertion or deletion error and string
substitution error could be handled.
</bodyText>
<sectionHeader confidence="0.503515" genericHeader="method">
1 Fast approximate Chinese word-
matching algorithm
</sectionHeader>
<subsectionHeader confidence="0.685641">
1.1 Error types in Chinese text
</subsectionHeader>
<bodyText confidence="0.991791428571429">
When classifying errors in Chinese text,
most papers prefer to categorize errors on the
character level. In our opinion, this kind of
classification contributes little to improve the
performance for Chinese text error detection and
correction. Referring to what&apos;s common in
English spelling error detection and correction,
we classify errors as followed:
Non-word errors: The string mapping to
a word in the correct text can&apos;t be treated as a
word in the corresponding error text. This kind
of error can be further classified into:
Character substitution error. A correct character
is replaced by another character. Such as *!�t
� !WM ���� ! ����.
String substitution error. A correct string is
replaced by another string, and at least one of
the two strings consists of more than one
character. Such as~~~!~~~.
Character insertion error. Such as &apos; ; )�MA !
~~~~. Character deletion error. Such
as,T,,b kAk ! &apos;kkk `_V `.
Real-word errors. This kind of error
could be further classified into: Word
substitution error. Such as �_*Mw ! 300.
Word insertion error. Such as AT !���
�. Word-Deletion error. Such as ��� !
��.
</bodyText>
<subsectionHeader confidence="0.8631375">
1.2 Fast precise Chinese word-matching
algorithm
</subsectionHeader>
<bodyText confidence="0.999604133333334">
Dictionary organized in (deterministic)
finite state recognizer (FSR) format is used to
implement the fast precise Chinese word-
matching algorithm. FSR = (Q, A, EI, F) with Q
denoting the set of states, A denoting the input
alphabet, which are all Chinese characters for a
Chinese dictionary, E I: Q❑A!Q denoting the
state transition function and F ❑ Q denoting the
final states. Besides, let stStart denote the
starting state of a FSR, stError denote the state
when input character cannot be accepted.
Precise Chinese word match is to find all
the strings, starting from a specific position in a
sentence, which are items in a dictionary. For
example, with the dictionary including words:
rP rPq rPgA P9 A AVQ, at the
beginning of the sentence rpMAvl� !&amp;quot;
# , three words of different length &amp;quot;rp&amp;quot;, &amp;quot;rp
P9&amp;quot;, &amp;quot;rPP9A&amp;quot; should be matched. Figure 1
shows the algorithm of fast precise Chinese
word match. Where dict=(Q, A, EI, F) is a
Chinese dictionary, state is its current state, str
is the string currently read by dict, sentence is
the input sentence, idx is the subscript in the
sentence of the character that should be read
immediately. All matched words are put into
result. The initial values when calling this
recursive function should assure: str=nil,
state=stStart, result=[]. This function is used by
approximate word-matching algorithm later.
</bodyText>
<subsectionHeader confidence="0.8331135">
1.3 Approximate Chinese word-matching
algorithm
</subsectionHeader>
<bodyText confidence="0.999876944444444">
Approximate word match based-on FSR is
used in English spelling error detection and
correction to find all words in a dictionary
whose minimum edit distance to a given string is
less than a threshold. However, there are great
differences between the approximate word
match of Chinese and English. First, the length
of the English string to be matched is
determinate. Because there are no obvious word
boundaries in Chinese text, the length of the
Chinese string to be matched is unknown. So
approximate Chinese word match must find
words that are similar to all strings of different
length. For example, when approximately
matching the sentence &amp;quot;$%&amp;&apos;()*+,&amp;quot;
at its beginning, the algorithm should give out
all Chinese words that are similar to &amp;quot;$&amp;quot;, &amp;quot;$
%&amp;quot;, &amp;quot;$%&amp;&amp;quot;, &amp;quot;$%&amp;&apos;&amp;quot;, &amp;quot;$%&amp;&apos;(&amp;quot; �
</bodyText>
<construct confidence="0.519911">
Procedure CnPreciseMatch(dfa, state, str, sentence, idx, Var result)
begin
if ( idx is not the end of sentence) and ((state&apos;&amp;quot;EI(state,sentence[idx])) ❑stError) then
begin
if (state&apos;[] F ) then
</construct>
<equation confidence="0.7797125">
result&amp;quot;result U { str + sentence[idx] };
CnPreciseMatch( dfa, state&apos;, str+sentence[idx], sentence, idx+1, result);
end;
end;
</equation>
<figureCaption confidence="0.978296">
Figure 1. Fast precise Chinese word-matching algorithm
</figureCaption>
<bodyText confidence="0.999983913043479">
Second, character is the basic unit in the English,
and its approximate word-matching algorithm
can adopt a cut-off method (Oflazer, 1996)
depending on the string currently read and only
one character to be read next time. This
decreases the search space greatly. As
mentioned before, errors in Chinese text may be
caused by string replacement where the lengths
of the correct string and its corresponding error
string may be different. This means, no matter
how far the FSR has read from the beginning
position, how dissimilar the string currently read
is to all possible words, we can&apos;t determine
whether or not words that are similar to the
target string could be found if more characters
are read later. For example, the error -.//
4 -./0123, which is caused because
the Five-stroke input code are similar between
/(tdi) and 0123(tmdi). Although -./
012 is dissimilar to all words in dictionary,
-./0123 is similar to word -.// .
The direct way to implement the
approximate Chinese word match is: for each
substring with length 1, 2, 3, ..., N, starting at
specified position in sentence, browse the
dictionary and count their similarity or edit
distance to every word in it. This approach will
face two problems: High computational
complexity. A common Chinese dictionary with
normal size contains about 60,000 words.
Besides, the evaluation of the similarity between
two strings is also time-consuming. Its
computing cost is far from the real-time
requirement of error detection and correction.
What is the maximum substring length N?
One may think that 4 or 5 is enough. Let&apos;s look
at an extraordinary example. Imaging an
inexperienced Five-Stroke typewriter is
supposed to input a four-character word and he
makes mistakes on every character. More
unfortunately, his carelessness causes every
single character transformed to a four-character
word. Finally he gets a 16-character string
instead of the expected four-character word.
Here, N should be at least 16 to get the proper
match.
</bodyText>
<subsubsectionHeader confidence="0.981029">
1.3.1 Definition of the distance
</subsubsectionHeader>
<bodyText confidence="0.986118451612903">
To implement approximate match, distance
of two strings should be defined. Some
predefined distances of two strings is called
meta-distance, denoted with MetaD. We define
MetaD(X,Y) between two Chinese strings X and
Y according to their grapheme, pronunciation
and input code: The similarity of their Pinyin
input code. For characters X and Y with same
pronunciation, for example 4 (tong) and 31
(tong), MetaD(X,Y)=30. For characters X and Y
different only on surd/sonant, for example 5
(shan) and 6 (san), MetaD(X,Y)=40. The
similarity of their Five-Stroke input code. For
strings X and Y that one input code could be
transformed to another at most by one
substitution operation, for example78(ahnm)
and 9:;(ahnn), MetaD(X,Y)=30. For strings
X and Y that one input code could be
transformed to another only by one insertion,
deletion or transposition operation, for example
&lt; (aa) and 9 (aaa), = (bwj) and &gt; (wbj),
MetaD(X,Y)=40. Some rules learned from
text errors reflecting common human confusion
on Chinese characters. For example, MetaD(?,
@)=26. In addition, let ε mean null string, for
each character z, let MetaD(ε,ε) = MetaD(z, z) =
0, MetaD(z, ε) = Metad(ε, z) = 50. For those
string pairs X and Y that there is no meta-
distance definition between them, just let
MetaD(X,Y)=+ ∞. Define the set of meta-strings
Mas:
</bodyText>
<equation confidence="0.83086425">
M = { X I X ≠ ε, ∃Y ≠ X, MetaD(X, Y) &lt; +∞ }
For each meta-string X∈M, define its
confusing string set cfs(X) as:
cfs(X)={ Y I Y∈M, Y ≠ ε, MetaD(X, Y) &lt; +∞}
</equation>
<bodyText confidence="0.706758833333333">
And each Y∈cfs(X) is called confusing
string of X. When character transposition error&apos;
m
is not considered, the distance of two strings X,
and Y,n, where m and n is the length of X and Y
respectively, can be defined as:
</bodyText>
<equation confidence="0.9886525">
WetaD (X; , Pn )............MetaD (X; , Pn
d(X; ,Y,n) =  MIN {d(X;,Y,&apos;) +d(X;m,Yjm)} ...... other
≤
)
</equation>
<bodyText confidence="0.9982035">
This recursive definition is not easy to
compute. Fortunately, our approximate match
algorithm use a heuristic expanding method and
avoid the computing cost.
</bodyText>
<figure confidence="0.942863">
&apos; Character transposition error is rare in Chinese text.
&lt;
+∞
1m
≤
≤
n.1
j
n
1.3.2 Fast approximate Chinese word-matching
algorithm
</figure>
<bodyText confidence="0.999497625">
Chinese word approximate match is to find,
from a specific position in the target sentence,
all words that has a distance less than a threshold
tw to substrings beginning at the position with
different length 1, 2, 3, .... At the same time, the
beginning position of the match next time and
the distance d between the matched word and its
corresponding original string should also be
given. In our implementation, the set of meta-
string M is also organized in a FSR format. In
M&apos;s every final state representing a meta-string
X, all strings in cfs(,Y) and their distances to X is
recorded. Figure 2 shows the approximate
Chinese word-matching algorithm. Where
cdfa=(Q, A, L7, F) is M. dict=(Q, A, Q F) is
Chinese dictionary. state is the current state of
dict. str is the string currently read by dict.
sentence is the target sentence. idx is the
subscript in the sentence of the character to be
read next time. diff is the distance of two partial
strings already matched. result is a set of 3-tuple
elements like (word, next, d), where word is the
approximate matched word, next is the position
where the approximate match should start next
time. d is the distance between the matched
word and its corresponding original string. The
initial values when calling this algorithm should
assure: str–nil, state=�tStart, dif�0, result=[].
The algorithm could restore character insertion,
deletion, substitution errors as well as string
substitution errors. The threshold tw decreases
the search space.
For example, when to approximate match
at the beginning of &amp;quot;$A&amp;&apos;()*+, &amp;quot;,
where A is a character substitution error of %,
the result may be contains ($, 2, 0), ( , 2, 30),
(B, 2, 30), (KC, 2, 40), ..., ($%, 3, 0), ($%
&amp;, 4, 0), ($D, 2, 50), ($E, 2, 50),..., (F$ ,
2, 50), (G$, 2, 50)..., etc. The correct word $
%&amp; is in the list.
</bodyText>
<sectionHeader confidence="0.906547" genericHeader="method">
2 Confusing word substitution approach
</sectionHeader>
<bodyText confidence="0.994443933333333">
Confusing character substitution approach
(Chang, 1994) got a relatively good result, but
can not deal with errors of character insertion,
character deletion and string substitution. Our
confusing word substitution approach is an
improvement on the confusing character
substitution approach by mending such
disability. It is based on the fast approximate
Chinese word-matching algorithm. In this
approach, a given sentence is approximately
segmented from all possible derivation of the
input sentence using operations of substitution,
Procedure CnFussyMatch( dict, state, str, sentence, idx, diff, tw cdfa, Var result)
begin
if ( idx is the end of sentence) then return
</bodyText>
<equation confidence="0.960285368421052">
if ( diff+MetaD(sentence[idx],C) ❑ tw) then //try to delete a Chinese character
CnFussyMatch(dict, state, str, sentence, idx+l, diff+MetaD(sentence[idx], C), tw, cdfa, result);
Foreach { x I (state&apos;FC(state, x)) ❑stError } do //try to insert a Chinese character
if ( diff+MetaD(x, C) ❑ tw then
begin
if ( state&apos;❑F)then
result +-- { (str+x, idx, diff+MetaD(x, C)) };
CnFussyMatch(dict, state&apos;, str+x, sentence, idx, Diff+MetaD(x, C), tw, cdfa, result);
end;
CnPreciseMatch( cdfa, stStart, nil, sentence, idx, set--[] ); //get all possible meta-strings into set
Foreach { X I X ❑ set } do
Foreach { Y I Y ❑ cfs(X) } do //try to replace X with its similar string Y
If ( diff+MetaD(X,Y)❑ tw ) and ( (state&apos;FEstate, Y)) ❑ stError ) then
begin
if ( state&apos;❑F ) then
result +-- { (str+Y, idx+IXI, diff+MetaD(X,Y)) };
CnFussyMatch(dict, state&apos;, str+ Y, sentence, idx+IXI, diff+MetaD(X, Y), tw, cdfa, result);
end;
end;
</equation>
<figureCaption confidence="0.992465">
Figure 2. Fast approximate Chinese word-matching algorithm
</figureCaption>
<figure confidence="0.5634612">
Procedure CnFussySeg( dict, path, sentence, idx, diff, cdfa, Var result)
begin
if idx is the end of sentence then
result += path;
else begin
CnFussyMatch( dict, stStart, nil, sentence, idx, 0, min{ t,-diff, tc }, cdfa, fussyWords=❑);
foreach ( (word, next, d) in fussyWords) do
CnFussySeg( dict, path+{(word, next, d)}, sentence, next, diff+d, cdfa, result);
end;
end;
</figure>
<figureCaption confidence="0.998925">
Figure 3. Complete approximate segmentation algorithm
</figureCaption>
<bodyText confidence="0.9996346">
insertion, and deletion. Paths are then evaluated
using base language model and distance
discount. The optimal path with highest score is
searched and treated as the correction of the
original sentence.
</bodyText>
<subsectionHeader confidence="0.989132">
2.1 Approximate segmentation
</subsectionHeader>
<bodyText confidence="0.999958416666667">
As what happens in precise segmentation,
approximate segmentation is to give a
segmentation path for a input sentence, but with
error tolerant ability. On the path, each word is
similar to its corresponding original string in the
input sentence. That&apos;s why we call this confusing
word substitution approach. For a input sentence,
let threshold ts❑ t,,,. It&apos;s required that the sum of
distances between all the words W&apos; on an
approximate segmentation path and their
corresponding original string W can not be
greater than ts:
</bodyText>
<equation confidence="0.85213775">
❑ d W W �
( &apos;, )
W path
&apos;�
</equation>
<bodyText confidence="0.999730583333333">
The reason for using ts is that there are
always little errors in one sentence and it could
decrease the space of the approximate
segmentation paths. Figure 3 shows the
algorithm of the approximate segmentation
listing all possible paths. Where the path is an
approximate segmentation for input sentence,
it&apos;s an array of elements like (word, next, d).
result is the set of all possible approximate
segmentation paths. Other symbols, such as dict
and cdfa, are of same meaning as in section
1.3.2.
</bodyText>
<subsectionHeader confidence="0.996244">
2.2 Path evaluation
</subsectionHeader>
<bodyText confidence="0.971683666666667">
The evaluation of paths consists basic
language model evaluation and distance discount.
For a given approximate segmentation path,
basic language model evaluation can adopt N-
Gram models of character, word, POS tag or
word class. Denote the score with
ModelScore(path). The distance discount
multiplies the ModelScore with a discount,
valued from 0 to 1, according to the distance
between the path and the input sentence. The
final score of a path FS is:
FS(path, sentence)=ModelScore(path) �
discount(path, sentence)
In this paper, we use:
discount(path, sentence)= ❑f 14n,d A&amp;quot;,W Ell
</bodyText>
<subsectionHeader confidence="0.67112">
W Opath
</subsectionHeader>
<bodyText confidence="0.99996075">
Where l is the length of W&apos;, n is the number
of the segment units when its corresponding
original string W is segmented. d( W &apos;, W) is the
distance between W&apos; and W. f is the discount
function. Generally, the value of f should be
closer to 1.0 if d is less, l is longer or n is
greater.
Because the number of possible paths in an
approximate segmentation is very large, to avoid
the computational complexity, dynamic
programming is adopted and some changes are
made to the segmentation algorithm.
</bodyText>
<sectionHeader confidence="0.995577" genericHeader="evaluation">
3 Experiment and results
</sectionHeader>
<bodyText confidence="0.9949527">
First define some evaluating indicator for
the automatic error detection and correction task,
let:
A=number of errors in target text.
B=number of warnings the proofreading
approach given
C=number of errors the proofreading
approach detected
D=number of errors the proofreading
approach corrected
</bodyText>
<table confidence="0.931983888888889">
is
* Character String Character Character Total Number of Recall Precise Correction
substitution substitution deletion insertion warnings rate rate rate
error error error error
e
Test text 395 76 29 35 535
(Chang 1994) 315 / 297 17 / 0 3 / 0 6 / 0 341 / 297 622 64 % 55 % 56%
This approach 317 / 298 66 / 63 19 / 18 27 / 24 420 / 403 656 79 % 64 % 75%
*(number of error warned/ number of error corrected)
</table>
<figureCaption confidence="0.994306">
Figure 4. Error distribution and experiment result
</figureCaption>
<bodyText confidence="0.998648564102564">
Then, recall rate = C/A*100%, precise rate =
C/B*100%, correction rate= D/A*100%.
In the experiment, a text containing 535
errors is to be detected and corrected. The
corpus to train the word tri-gram language
model is about 200M bytes, including people&apos;s
daily 93 and 94, 10 years reader, BaiJiaBao&apos;94,
ShiChangBao&apos;94. Two thresholds is and tw are
set 59 and 50 respectively. Zhang Zhaohuang&apos;s
approach is also applied on the test text as a
comparison. The distribution of different kinds
of errors and the experiment results are shown in
Figure 4.
From the result, we can see that our
approach using similar word substitution has the
same ability to detect and correct the character
substitution errors as (Zhang 1994) approach.
But its ability to detect and correct character
insertion error, character deletion error and
string substitution error are highly enhanced.
The result shows that our approach has great
practicability.
The reason for those incorrect warnings and
undetected or uncorrected errors mainly on:
Insufficient of the similar string set. When a
correct string is not included in the similar string
set, some errors will not be detected and
corrected. For example, the error &amp;quot;HI4bJ
K&amp;quot;�&amp;quot;HL4�L�JK&amp;quot; did not detected in our
experiment because &amp;quot;I&amp;quot; does not in the similar
string set of &amp;quot;L&amp;quot;. More complete similar string
set will detect and correct more errors, yet they
may also cause more incorrect warnings, and
increase the computing cost. Language Mode
deficiency. Tri-gram only has local linguistic
constraints. It&apos;s necessary to adopt long-distance
constraints. Incomplete of the dictionary.
Data sparseness. Larger corpus needed in
training.
</bodyText>
<sectionHeader confidence="0.998637" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.998811">
A fast approximate Chinese word-matching
algorithm is presented in this paper. Based on
the algorithm, an automatic Chinese Spelling
Error Correction approach using similar word
</bodyText>
<table confidence="0.843163555555555">
Error sentences Detection and Correction results
��� ������... ��� ������...
—,RX- &amp;quot;T&amp;Vc T7I7ffl!�!FLit... —,RX- 45M Wr]W+�!FLit
rTii�FfiNA421-k VA !�&amp;quot;N#VU$ rTii*MiM421 AFW0A !Fff&amp;quot;N#VU$
%&amp;&apos;(�)*+,-./ %&amp;&apos;(�)*+,-./
1~2345 67~8 1~2345 67~98
:;&lt;=&gt; ?@�ABCDEFGHIJ :;&lt;=&gt; ?@ABCDEFGHIJK
K
LMNOPQRCSTUV� WXTUY LMNOPQRCSTUV� ZTUY
</table>
<figureCaption confidence="0.997132">
Figure 5. Examples of the test result using our error detection and correction approach
</figureCaption>
<bodyText confidence="0.999975913043478">
substitution and language model evaluation is
designed. Compared with Zhang Chao-Huang&apos;s
confusing character substitution method, this
new approach can deal with not only character
substitution errors but also insertion, deletion
and string substitution errors. Because no word
boundaries in Chinese text, there is not a two-
level structure of &amp;quot;non-word&amp;quot; and &amp;quot;real-word&amp;quot;
errors in Chinese spelling correction task like
that in English spelling correction. The fast
approximate Chinese word-matching algorithm
can handle Chinese &amp;quot;non-word&amp;quot; error efficiently,
making it easy to establish a two-level structure
in Chinese spelling correction.
The future research may include: Pruning
the approximate word matching result before
they take part in the approximate segmentation.
This will decrease the computing cost.
Introducing long distance constraints. What&apos;s
need to point out is that dynamic programming
may dislike this kind of long distance constrains.
So they are more suitable in the pruning and
discounting.
</bodyText>
<sectionHeader confidence="0.999059" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996398190476191">
Chang Chao-Huang (1994) A Pilot Study on
Automatic Chinese Spelling Error Correction.
Communication of COLIPS, 4/2, pp. 143-149
Karen Kukich (1992) Techniques for automatically
correcting words in text. ACM Computing Surveys,
24/4, pp. 377-439
Kemal Oflazer (1996) Error-tolerant Finite-state
Recognition with Applications to Morphological
Analysis and Spelling Correction. Computational
Linguistics, Computational Linguistics, 22/1, pp.
73-89
Sun Cai (1997) Research on Lexical Error Detection
and Correction of Chinese Text: [Master
Dissertation]. Tsinghua University, Beijing. 96p
Zhang Yansen, Ding Bingqing (1998). Research and
Practice on the Lexical Error Detecting System
Based on &amp;quot;Banding and Filtering&amp;quot; in Chinese Text
Automatic Proofread. In Proc. of International
Conference on Chinese Information Processing,
Tsinghua University Publishers, Beijing, China, pp.
392-437
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.503034">
<title confidence="0.9580435">Automatic detecting/correcting errors in Chinese text by an approximate word-matching algorithm</title>
<author confidence="0.999843">Lei Zhang</author>
<affiliation confidence="0.999864">Dept. of Computer Science and Technology Tsinghua University</affiliation>
<address confidence="0.998729">Beijing, China, 1000 84</address>
<email confidence="0.881042">zhl@s1000e.cs.tsinghua.edu.cn</email>
<author confidence="0.946785">Changning Huang</author>
<affiliation confidence="0.999753">Microsoft Research China</affiliation>
<address confidence="0.986963">Beijing, China, 1000 80</address>
<email confidence="0.999273">cnhuang@microsoft.com</email>
<author confidence="0.999474">Ming Zhou</author>
<affiliation confidence="0.999895">Microsoft Research China</affiliation>
<address confidence="0.953051">Beijing, China, 1000 80</address>
<email confidence="0.999471">mingzhou@microsoft.com</email>
<author confidence="0.998388">Haihua Pan</author>
<affiliation confidence="0.8552785">Dept. of Chinese, Translation and Linguistics City University of Hong Kong</affiliation>
<address confidence="0.992539">Kowlong, Hong Kong, China</address>
<email confidence="0.996306">cthpan@cityu.edu.hk</email>
<abstract confidence="0.998203153846154">An approximate word-matching algorithm for Chinese is presented. Based on this algorithm, an effective approach to Chinese spelling error detection and correction is implemented. With a word tri-gram language model, the optimal string is searched from all possible derivation of the input sentence using operations of character substitution, insertion, and deletion. Comparing the original sentence with the optimal string, spelling error detection and correction is realized simultaneously.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chang Chao-Huang</author>
</authors>
<title>A Pilot Study on Automatic Chinese Spelling Error Correction.</title>
<date>1994</date>
<journal>Communication of COLIPS,</journal>
<volume>4</volume>
<pages>143--149</pages>
<marker>Chao-Huang, 1994</marker>
<rawString>Chang Chao-Huang (1994) A Pilot Study on Automatic Chinese Spelling Error Correction. Communication of COLIPS, 4/2, pp. 143-149</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys,</journal>
<volume>24</volume>
<pages>377--439</pages>
<contexts>
<context position="1799" citStr="Kukich, 1992" startWordPosition="238" endWordPosition="239">ng character substitution method (Chang, 1994), where confusing characters are used to replace every character in the input sentence, and a &amp;quot;correct&amp;quot; result with highest evaluation score is searched from all paths. While achieving relatively good result, it has obvious weakness: only character substitution errors can be detected and corrected, other kinds of errors can not be handled, including character deletion, character insertion, and string substitution errors. There is a clear two-level structure in English spelling error detection and correction, &amp;quot;non-word&amp;quot; error and &amp;quot;real-word&amp;quot; error (Kukich, 1992). Things are different in Chinese. Although many approaches find that most errors in Chinese cause segmentation abnormal (Sun, 1997 and Zhang, 199 8), no one stress on such &amp;quot;non-word&amp;quot; error and the two-level structure is not adopted in Chinese. Following are possible reasons for this situation. There is no obvious word boundary in Chinese text, so automatic word segmentation must be introduced. If error exists, segmentation result could be weird. For example, the segmentation of ,&apos;VJ AkAk, which is a character deletion error for the word MJ1,L4 Ak, may appear like,&apos;J&apos;./I(k/AC, including three </context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich (1992) Techniques for automatically correcting words in text. ACM Computing Surveys, 24/4, pp. 377-439</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Error-tolerant Finite-state Recognition with Applications to Morphological Analysis and Spelling Correction.</title>
<date>1996</date>
<journal>Computational Linguistics, Computational Linguistics,</journal>
<volume>22</volume>
<pages>73--89</pages>
<contexts>
<context position="6966" citStr="Oflazer, 1996" startWordPosition="1057" endWordPosition="1058">e algorithm should give out all Chinese words that are similar to &amp;quot;$&amp;quot;, &amp;quot;$ %&amp;quot;, &amp;quot;$%&amp;&amp;quot;, &amp;quot;$%&amp;&apos;&amp;quot;, &amp;quot;$%&amp;&apos;(&amp;quot; � Procedure CnPreciseMatch(dfa, state, str, sentence, idx, Var result) begin if ( idx is not the end of sentence) and ((state&apos;&amp;quot;EI(state,sentence[idx])) ❑stError) then begin if (state&apos;[] F ) then result&amp;quot;result U { str + sentence[idx] }; CnPreciseMatch( dfa, state&apos;, str+sentence[idx], sentence, idx+1, result); end; end; Figure 1. Fast precise Chinese word-matching algorithm Second, character is the basic unit in the English, and its approximate word-matching algorithm can adopt a cut-off method (Oflazer, 1996) depending on the string currently read and only one character to be read next time. This decreases the search space greatly. As mentioned before, errors in Chinese text may be caused by string replacement where the lengths of the correct string and its corresponding error string may be different. This means, no matter how far the FSR has read from the beginning position, how dissimilar the string currently read is to all possible words, we can&apos;t determine whether or not words that are similar to the target string could be found if more characters are read later. For example, the error -.// 4 </context>
</contexts>
<marker>Oflazer, 1996</marker>
<rawString>Kemal Oflazer (1996) Error-tolerant Finite-state Recognition with Applications to Morphological Analysis and Spelling Correction. Computational Linguistics, Computational Linguistics, 22/1, pp. 73-89</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sun Cai</author>
</authors>
<title>Research on Lexical Error Detection and Correction of Chinese Text: [Master Dissertation]. Tsinghua University,</title>
<date>1997</date>
<location>Beijing. 96p</location>
<marker>Cai, 1997</marker>
<rawString>Sun Cai (1997) Research on Lexical Error Detection and Correction of Chinese Text: [Master Dissertation]. Tsinghua University, Beijing. 96p</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhang Yansen</author>
</authors>
<title>Ding Bingqing</title>
<date>1998</date>
<booktitle>In Proc. of International Conference on Chinese Information Processing,</booktitle>
<pages>392--437</pages>
<institution>Tsinghua University Publishers,</institution>
<location>Beijing, China,</location>
<marker>Yansen, 1998</marker>
<rawString>Zhang Yansen, Ding Bingqing (1998). Research and Practice on the Lexical Error Detecting System Based on &amp;quot;Banding and Filtering&amp;quot; in Chinese Text Automatic Proofread. In Proc. of International Conference on Chinese Information Processing, Tsinghua University Publishers, Beijing, China, pp. 392-437</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>