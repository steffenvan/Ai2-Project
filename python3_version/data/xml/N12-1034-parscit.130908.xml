<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007606">
<title confidence="0.99696">
Using paraphrases for improving first story detection in news and Twitter
</title>
<author confidence="0.991276">
Saˇsa Petrovi´c Miles Osborne Victor Lavrenko
</author>
<affiliation confidence="0.9908625">
School of Informatics School of Informatics School of Informatics
University of Edinburgh University of Edinburgh University of Edinburgh
</affiliation>
<email confidence="0.996431">
sasa.petrovic@ed.ac.uk miles@inf.ed.ac.uk vlavrenk@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.995603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99966355">
First story detection (FSD) involves identify-
ing first stories about events from a continuous
stream of documents. A major problem in this
task is the high degree of lexical variation in
documents which makes it very difficult to de-
tect stories that talk about the same event but
expressed using different words. We suggest
using paraphrases to alleviate this problem,
making this the first work to use paraphrases
for FSD. We show a novel way of integrat-
ing paraphrases with locality sensitive hashing
(LSH) in order to obtain an efficient FSD sys-
tem that can scale to very large datasets. Our
system achieves state-of-the-art results on the
first story detection task, beating both the best
supervised and unsupervised systems. To test
our approach on large data, we construct a cor-
pus of events for Twitter, consisting of 50 mil-
lion documents, and show that paraphrasing is
also beneficial in this domain.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977155555556">
First story detection (FSD), sometimes also called
new event detection (NED), is the task of detecting
the first story about a new event from a stream of
documents. It began as one of the tasks in Topic
Detection and Tracking (TDT) (Allan, 2002) where
the overall goal of the project was to improve tech-
nologies related to event-based information organi-
zation tasks. Of the five TDT tasks, first story de-
tection is considered the most difficult one (Allan
et al., 2000a). A good FSD system would be very
useful for business or intelligence analysts where
timely discovery of events is crucial. With the sig-
nificant increase in the amount of information being
produced and consumed every day, a crucial require-
ment for a modern FSD system to be useful is effi-
ciency. This means that the system should be able
to work in a streaming setting where documents are
constantly coming in at a high rate, while still pro-
ducing good results. While previous work has ad-
dressed the efficiency (Petrovi´c et al., 2010) aspect,
there has been little work on improving FSD perfor-
mance in the past few years. A major obstacle is the
high degree of lexical variation in documents that
cover the same event. Here we address this problem,
while keeping in mind the efficiency constraints.
The problem of lexical variation plagues many IR
and NLP tasks, and one way it has been addressed
in the past is through the use of paraphrases. Para-
phrases are alternative ways of expressing the same
meaning in the same language. For example, the
phrase he got married can be paraphrased as he tied
the knot. Paraphrases were already shown to help
in a number of tasks: for machine translation to
translate unknown phrases by translating their para-
phrases (Callison-Burch et al., 2006), for query ex-
pansion in information retrieval (Sp¨arck Jones and
Tait, 1984; Jones et al., 2006), or for improving
question answering (Riezler et al., 2007). A much
more detailed discussion on the use of paraphrases
and ways to extract them is given in (Madnani and
Dorr, 2010). Here, we present the first work to use
paraphrases for improving first story detection. Us-
ing paraphrases, we are able to detect that some doc-
uments previously thought to be about new events
are actually paraphrases of the documents already
</bodyText>
<page confidence="0.655968333333333">
338
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 338–346,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999906971428571">
seen. Our approach is simple and we show a novel
way of integrating paraphrases with locality sen-
sitive hashing (LSH) (Indyk and Motwani, 1998).
This way we obtain a very efficient FSD system with
all the benefits of using paraphrases, while avoid-
ing computationally expensive topic modeling ap-
proaches such as Ahmed et al. (2011).
First story detection was introduced as a task be-
fore the popularization of social media. Event de-
tection in social media, especially Twitter is a very
good fit: we cover a much larger set of events than
would be possible by using newswire, and the sto-
ries are reported in real time, often much sooner
than in news. Of course, social media carries ad-
ditional problems not found in traditional media: we
have to deal with huge amounts of data, the data is
very noisy (both due to spam and due to spelling
and grammar errors), and in the case of Twitter, doc-
uments are extremely short. There has been little
effort in solving these problems for FSD. Arguably
the main reason for this is the lack of a TDT-style
corpus for Twitter that researchers could use to test
their approaches. Here we build such a corpus and
use it to measure the performance of TDT systems
on Twitter.
Our main contributions are: i) we create a first
corpus of events on Twitter, ii) we show how to use
paraphrases in FSD, and how to combine it with
LSH to handle high-volume streams, iii) our unsu-
pervised system that uses paraphrases achieves the
highest reported results on the TDT5 corpus, beat-
ing both the supervised and unsupervised state of the
art, while still keeping a constant per-document time
complexity, and iv) we show that paraphrases also
help in Twitter, although less than in TDT.
</bodyText>
<sectionHeader confidence="0.939044" genericHeader="introduction">
2 Paraphrasing and FSD
</sectionHeader>
<subsectionHeader confidence="0.998487">
2.1 Current approaches to efficient FSD
</subsectionHeader>
<bodyText confidence="0.999724428571429">
State-of-the-art FSD systems (Allan et al., 2000b)
use a fairly simple approach. Documents are repre-
sented as TF-IDF weighted vectors, their distance is
measured in terms of the cosine distance, and they
use a k-nearest neighbors clustering algorithm, with
k usually set to 1. The novelty score for a document
is the cosine distance to the nearest neighbor:
</bodyText>
<equation confidence="0.916549">
score(d) = 1 − max
d&apos;EDt
</equation>
<bodyText confidence="0.99968775">
Dt is the set of all documents up to time t when
document d arrived.
Because the max in equation (1) takes O(|Dt|)
time to compute in the worst case, Petrovi´c et al.
(2010) introduced a way of using locality sensitive
hashing (LSH) to make this time O(1), while retain-
ing the same accuracy level. In particular, instead
of computing the max over the entire set Dt, like
in (1), they compute it over a smaller set 5 of poten-
tial nearest neighbors. The set 5 is the set of docu-
ments that collide with the current document under
a certain type of hash function:
</bodyText>
<equation confidence="0.949597">
5(x) = {y : hij(y) = hij(x), ]i E [1..L], bj E [1..k]},
(2)
</equation>
<bodyText confidence="0.89079">
where the hash functions hij are defined as:
</bodyText>
<equation confidence="0.992141">
hij(x) = sgn(ufixx), (3)
</equation>
<bodyText confidence="0.9999585">
with the random vectors uij being drawn indepen-
dently for each i and j. The efficiency of this algo-
rithm stems from the fact that it can be shown that
the set 5 of potential nearest neighbors can be made
constant in size, while still containing the nearest
neighbor with high probability.
</bodyText>
<subsectionHeader confidence="0.994222">
2.2 Paraphrases
</subsectionHeader>
<bodyText confidence="0.9999815">
There are several levels of paraphrasing – lexical
paraphrases, where the relationship is restricted to
individual lexical items, phrasal paraphrases, where
longer phrases are considered, and sentential para-
phrases, where entire sentences are in a paraphrastic
relationship. Here we use the simplest form, lexi-
cal paraphrases, but our approach, described in sec-
tion 2.3, is general and it would be trivial to use
phrasal paraphrases in the same way – we leave this
for future work.
We use three sources of paraphrases: Word-
net (Fellbaum, 1998), a carefully curated lexical
database of English containing synonym sets,
Microsoft Research paraphrase tables (Quirk et
al., 2004), a set of paraphrase pairs automatically
extracted from news texts, and syntactically-
constrained paraphrases from Callison-Burch
(2008) which are extracted from parallel text. We
also considered using paraphrases from Cohn et
al. (2008), but using them provided only minor
improvement over the baseline model. This is likely
due to the small size of that corpus (a total of 7
</bodyText>
<equation confidence="0.936424">
cos(d,d&apos;). (1)
</equation>
<page confidence="0.994242">
339
</page>
<bodyText confidence="0.999640142857143">
thousand pairs). We do not show results for this
paraphrase corpus in our results section.
Wordnet paraphrases contained 150 thousand
word pairs extracted from Wordnet’s synsets, where
all the pairs of words within one synset were con-
sidered to be paraphrases. MSR paraphrases were
extracted from the phrase tables provided by MSR.
Two words were considered paraphrases if they were
aligned at least once in the most probable alignment,
with the probability of both backward and forward
alignment of at least 0.2. In our initial experiments
we varied this threshold and found it has little ef-
fect on results. Using this method, we extracted
50 thousand paraphrase pairs. Finally, we use the
method of Callison-Burch (2008) to extract syntac-
tically constrained paraphrases from a parallel cor-
pus. This method requires that phrases and their
paraphrases be the same syntactic type, and has been
shown to substantially improve the quality of ex-
tracted paraphrases (Callison-Burch, 2008). We ex-
tracted paraphrases for all the words that appeared
in the MSR paraphrase corpus, and then kept all the
pairs that had the paraphrase probability of at least
0.2. This way, we extracted 48 thousand pairs. All
three resources we use are very different: they come
from different domains (news text, legal text, gen-
eral English), and they have very little overlap (less
than 5% of pairs are shared by any two resources).
</bodyText>
<subsectionHeader confidence="0.999754">
2.3 Efficient paraphrasing in FSD
</subsectionHeader>
<bodyText confidence="0.999843214285714">
In this section, we explain how to use paraphrases
in a first story detection system. We account for
paraphrases by changing how we compute the co-
sine in equation (1). Because the cosine measure
depends on the underlying inner product, we change
the way the inner product is computed. We model
paraphrasing by using a binary word-to-word ma-
trix of paraphrases Q. An entry of 1 at row i and
column j in the matrix indicates that words i and j
are paraphrases of each other.1 Note, however, that
our approach is not limited to using single words – if
the document representation includes n-grams with
n &gt; 1, the matrix Q can contain phrases, and thus
we can capture non-compositional paraphrases like
</bodyText>
<footnote confidence="0.60877375">
1This is of course a simplification – in general, one might
like the entries in the matrix to be real numbers corresponding
to the probability that the two words are paraphrases. We leave
this for future work.
</footnote>
<bodyText confidence="0.9525285">
he died H he kicked the bucket. We use the matrix
Q to define a new inner product space:2
</bodyText>
<equation confidence="0.994309">
(x,y)Q = yT Qx. (4)
</equation>
<bodyText confidence="0.99996825">
This way of using paraphrases basically achieves ex-
pansion of the terms in documents with their para-
phrases. Thus, if two documents have no terms in
common, but one has the term explosion and the
other has the term blast, by knowing that the two
terms are paraphrases, their similarity will be differ-
ent from zero, which would have been the case if no
paraphrasing was used. Alternatively, the new inner
product in equation (4) can also be seen as introduc-
ing a linear kernel.
One problem with using Q as defined in (4) is that
it is not very suitable for use in an online setting.
In particular, if documents come in one at a time
and we have to store each one, only for it to be re-
trieved at some later point, simply storing them and
computing the inner product as in (4) would lead to
frequent matrix-vector multiplications. Even though
Q is sparse, these multiplications become expen-
sive when done often, as is the case in first story
detection. We thus have to store a modified docu-
ment vector x, call it x0, such that when we compute
(x0, y0) we get (x, y)Q. Note that the inner product
between x0 and y0 is computed in the original inner
product space. It is clear that by using:
</bodyText>
<equation confidence="0.8581215">
x0 = Q1/2x (5)
(we have achieved our goal: (x0, y0) = y0Tx0 =
(Q1/2y)T(Q1/2x) = (yTQ1/2T)(Q1/2x) =
yTQx = (x, y)Q. Again, if we view equation (4)
</equation>
<bodyText confidence="0.995995866666667">
as defining a kernel, we can think of equation (5)
as performing an explicit mapping into the feature
space defined by the kernel. Because ours is a linear
kernel, performing this mapping is fairly efficient.
Unfortunately, the square root of the paraphrasing
matrix Q is in general dense (and can even contain
complex entries), which would make our approach
infeasible in practice because we would have to ex-
pand every document with all (or a very large num-
ber of) the words in the vocabulary. Thus, we have to
2Equation (4) does not define a proper inner product in the
strict technical sense because the positive definiteness property
does not hold. However, because vectors x and y in practice
always have positive entries, equation (4) behaves like a proper
inner product for all practical purposes.
</bodyText>
<page confidence="0.984069">
340
</page>
<bodyText confidence="0.999192666666667">
approximate Q1/2 with a sparse matrix, preferably
one that is as sparse as the original Q matrix. To this
end, we introduce the following approximation:
</bodyText>
<equation confidence="0.9971845">
1/2 = Qij (6)
Qij V/ &amp;(Qik + Qkj)/2
</equation>
<bodyText confidence="0.999944785714286">
To see how we arrive at this approximation, consider
the paraphrase matrix Q. If there was no polysemy
in the language, Q would be a block matrix, where
each non-zero submatrix would correspond to a sin-
gle meaning. The square root of such a matrix would
be given exactly by (6). While this approximation
is somewhat simplistic, it has two major advantages
over the exact Q1/2: i) it is very easy to compute
and, with proper implementation, takes O(n2) time,
as opposed to O(n3) for Q1/2, making it scalable to
very large matrices, and ii) matrix Q1/2 is guaran-
teed to be as sparse as Q, whereas Q1/2 will in most
cases become dense, which would make it unusable
in real applications.
</bodyText>
<subsectionHeader confidence="0.98366">
2.4 Locality-sensitive hashing with
paraphrasing
</subsectionHeader>
<bodyText confidence="0.99999425">
Here we explain how to integrate paraphrasing with
efficient FSD, using LSH described in section 2.1.
As we mentioned before, a single hash function hij
in the original LSH scheme hashes the vector x to:
</bodyText>
<equation confidence="0.998168">
h(x) = sgn(uTx), (7)
</equation>
<bodyText confidence="0.999964666666667">
where u is a (dense) random vector. If we want to
use paraphrases with LSH, we simply change the
hash function to
</bodyText>
<equation confidence="0.999285">
h1(x) = sgn(uT(�Q1/2x)). (8)
</equation>
<bodyText confidence="0.999908428571429">
It is not difficult to show that by doing this, the LSH
bounds for probability of collision hold in the new
inner product space defined by the matrix Q. We
omit this proof due to space constraints.
Space efficient LSH. While LSH can significantly
reduce the running time, it is fairly expensive
memory-wise. This memory overhead is due to the
random vectors u being very large. To solve this
problem, (Van Durme and Lall, 2010) used a hash-
ing trick for space-efficient storing of these vectors.
They showed that it is possible to project the vectors
onto a much smaller random subspace, while still re-
taining good properties of LSH. They proposed the
following hash function for a vector x:
</bodyText>
<equation confidence="0.999174">
h2(x) = sgn(uT(Ax)), (9)
</equation>
<bodyText confidence="0.999948142857143">
where A is a random binary matrix with exactly one
non-zero element in each column. This approach
guarantees a constant space use which is bounded by
the number of rows in the A matrix. Here we show
that our paraphrasing approach can be easily used
together with this space-saving approach by defin-
ing the following hash function for x:
</bodyText>
<equation confidence="0.997811">
h3(x) = sgn(uT(A�Q1/2x)). (10)
</equation>
<bodyText confidence="0.999897285714286">
This way we get the benefits of the hashing trick
(the constant space use), while also being able to use
paraphrases. The hash function in (10) is the actual
hash function we use in our system. Together with
the heuristics from Petrovi´c et al. (2010), it guaran-
tees that our FSD system will use constant space and
will take constant time to process each document.
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="method">
3 Twitter Event Corpus
</sectionHeader>
<subsectionHeader confidence="0.999723">
3.1 Event detection on Twitter
</subsectionHeader>
<bodyText confidence="0.999985043478261">
As we mentioned before, research on event detec-
tion in social media is hampered by the lack of a
corpus that could be used to measure performance.
The need for a standard corpus is evident from the
related work on event detection in Twitter. For ex-
ample, (Petrovi´c et al., 2010) address the scaling
problem in social media and present a system that
runs in constant time per document, but the evalua-
tion of their system on Twitter data was limited to
very high-volume events. The only attempt in creat-
ing a corpus of events for Twitter that we are aware
of was presented in Becker et al. (2011). Unfor-
tunately, that corpus is not suitable for FSD eval-
uation for two main reasons: i) the events were
picked from the highest-volume events identified by
the system (similar to what was done in Petrovi´c et
al. (2010)), introducing not only a bias towards high-
volume events, but also a bias toward the kinds of
events that their system can detect, and ii) the au-
thors only considered tweets by users who set their
location to New York, which introduces a strong bias
towards the type of events that can appear in the cor-
pus. While these problems were not relevant to the
</bodyText>
<page confidence="0.993874">
341
</page>
<bodyText confidence="0.999698">
work of (Becker et al., 2011) because the corpus was
only used to compare different cluster representa-
tion techniques, they would certainly pose a serious
problem if we wanted to use the corpus to compare
FSD systems. In this paper we present a new corpus
of tweets with labeled events by taking a very sim-
ilar approach to that taken by NIST when creating
the TDT corpora.
</bodyText>
<subsectionHeader confidence="0.999879">
3.2 Annotating the Tweets
</subsectionHeader>
<bodyText confidence="0.999962628571428">
In this section we describe the annotation process for
our event corpus. Note that due to Twitter’s terms of
service, we distribute the corpus as a set of tweet
IDs and the corresponding annotations – users will
have to crawl the tweets themselves, but this can
be easily done using any one of the freely available
crawlers for Twitter. This is the same method that
the TREC microblog track3 used to distribute their
data. All our Twitter data was collected from the
streaming API4 and consists of tweets from begin-
ning of July 2011 until mid-September 2011. After
removing non-English tweets, our corpus consists of
50 million tweets.
In our annotation process, we have adopted the
approach used by the National Institute of Standards
and Technology (NIST) in labeling the data for TDT
competitions. First, we defined a set of events that
we want to find in the data, thus avoiding the bias of
using events that are the output of any particular sys-
tem. We choose the events from the set of important
events for our time period according to Wikipedia.5
Additionally, we used common knowledge of impor-
tant events at that time to define more events. In
total, we define 27 events, with an average of 112
on-topic tweets. This is comparable to the first TDT
corpus which contained 25 events and average of 45
on-topic documents. However, in terms of the to-
tal number of documents, our corpus is three orders
of magnitude larger than the first TDT corpus, and
two orders of magnitude larger than the biggest TDT
corpus (TDT5). Our corpus contains very different
events, such as the death of Amy Winehouse, down-
grading of US credit rating, increasing of US debt
ceiling, earthquake in Virginia, London riots, terror-
ist attacks in Norway, Google announcing plans to
</bodyText>
<footnote confidence="0.999679333333333">
3http://trec.nist.gov/data/tweets/
4https://stream.twitter.com/
5http://en.wikipedia.org/wiki/2011
</footnote>
<bodyText confidence="0.999855833333333">
buy Motorola Mobility, etc. The event with the most
on-topic tweets had over 1,000 tweets (death of Amy
Winehouse), and the smallest event had only 2 on-
topic tweets (arrest of Goran Hadzic).
We faced the same problems as NIST when label-
ing the events – there were far too many stories to ac-
tually read each one and decide which (if any) events
it corresponds to. In order to narrow down the set
of candidates for each event, we use the same pro-
cedure as used by NIST. The annotator would first
read a description of the event, and from that de-
scription compile a set of keywords to retrieve pos-
sibly relevant tweets. He would then read through
this set, labeling each tweet as on- or off-topic, and
also adding new keywords for retrieving a new batch
of tweets. After labeling all the tweets in one batch,
the newly added keywords were used to retrieve the
next batch, and this procedure was repeated until no
new keywords were added. Unlike in TDT, however,
when retrieving tweets matching a keyword, we do
not search through the whole corpus, as this would
return far too many candidates than is feasible to la-
bel. Instead, we limit the search to a time window of
one day around the time the event happened.
Finally, the annotator guidelines contained some
Twitter-specific instructions. Links in tweets were
not taken into account (the annotator would not click
on links in the tweets), but retweets were (if the
retweet was cut off because of the 140 character
limit, the annotator would label the original tweet).
Furthermore, hashtags were taken into account, so
tweets like #Amywinehouseisdead were labeled as
normal sentences. Also, to be labeled on-topic, the
tweet would have to explicitly mention the event and
the annotator should be able to infer what happened
from the tweet alone, without any outside knowl-
edge. This means that tweets like Just heard about
Lokomotiv, this is a terrible summer for hockey! are
off topic, even though they refer to the plane crash
in which the Lokomotiv hockey team died.
In total, our corpus contains over 50 million
tweets, of which 3035 tweets were labeled as be-
ing on-topic for one of the 27 events. While search-
ing for first tweets (i.e., tweets that first mention an
event), fake first tweets were sometimes discovered.
For example, in the case of the death of Richard
Bowes (victim of London riots), a Telegraph jour-
nalist posted a tweet informing of the man’s death
</bodyText>
<page confidence="0.996401">
342
</page>
<bodyText confidence="0.999962">
more than 12 hours before he actually died. This
tweet was later retracted by the journalist for being
incorrect, but the man then died a few hours later.
Cases like this were labeled off-topic.
</bodyText>
<sectionHeader confidence="0.999037" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.94857">
4.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.999953866666667">
In the official TDT evaluation, each FSD system is
required to assign a score between 0 and 1 to ev-
ery document upon its arrival. Lower scores corre-
spond to old stories, and vice versa. Evaluation is
then carried out by first sorting all stories accord-
ing to their scores and then performing a threshold
sweep. For each value of the threshold, stories with
a score above the threshold are considered new, and
all others are considered old. Therefore, for each
threshold value, one can compute the probability of
a false alarm, i.e., probability of declaring a story
new when it is actually not, and the miss probability,
i.e., probability of declaring a new story old (miss-
ing a new story). Using the false alarm and the miss
rate, the cost Cdet is defined as follows:
</bodyText>
<equation confidence="0.539961">
Cdet = Cmiss*Pmiss*Ptarget+CFA*PFA*Pnon−target,
</equation>
<bodyText confidence="0.999808">
where Cmiss and CFA are costs of miss and false
alarm (0.02 and 0.98, respectively), Pmiss and PFA
are the miss and false alarm rate, and Ptarget and
Pnon−target are the prior target and non-target prob-
abilities. Different FSD systems are compared on
the minimal cost Cmin, which is the minimal value
of Cdet over all threshold values. This means that in
FSD evaluation, a lower value of Cmin indicates a
better system.
</bodyText>
<subsectionHeader confidence="0.990103">
4.2 TDT results
</subsectionHeader>
<bodyText confidence="0.999981561403509">
For the TDT experiments, we use the English por-
tion of TDT-5 dataset, consisting of 126 topics in
278,108 documents. Similar to (Petrovi´c et al.,
2010), we compare our approach to a state-of-the-
art FSD system, namely the UMass system (Allan et
al., 2000b). This system always scored high in the
TDT competitions and is known to perform at least
as well as other systems that also took part in the
competition (Fiscus, 2001). Our system is based on
the streaming FSD system of (Petrovi´c et al., 2010)
which has a constant per-document time complex-
ity. We use stemming (Porter, 1980) and, the same
as (Petrovi´c et al., 2010), we use 13 bits per key
and 70 hash tables for LSH. Additionally, we use the
hashing trick described in section 2.4 with a pool of
size 218. Paraphrasing is implemented in this system
as described in section 2.4.
While the UMass system was among the best sys-
tems that took part in the TDT competitions, there
has been research in event detection since the com-
petitions stopped. Recent work on event detec-
tion includes a hybrid clustering and topic model
with rich features such as entities, time, and top-
ics (Ahmed et al., 2011). We do not compare our
system to Ahmed et al. (2011) because in terms of
the numerical Cmin score, their approach does not
outperform the UMass system. This is not surpris-
ing as the primary goal in Ahmed et al. (2011) was
not to improve FSD performance, but rather to cre-
ate storylines and support structured browsing.
We compare our approach to the best reported re-
sult in the literature on the TDT5 data. To the best of
our knowledge, the highest reported results in FSD
come from a supervised system described in Ku-
maran and Allan (2005). This system uses an SVM
classifier with the features being FSD scores from
unsupervised systems (the authors used scores com-
puted in the same way as is done in the UMass sys-
tem) computed using i) full text, ii) only named en-
tities in the document, and iii) only topic terms. The
classifier was trained on TDT3 and TDT4 corpora
and tested on TDT5.
Table 1 shows the results for TDT5 data. UMass
1000 is the run that was submitted as the official
run in the TDT competition.6 We can see that us-
ing paraphrases improves the results over the unsu-
pervised state of the art, regardless of which source
of paraphrasing is used. However, it is clear that
not all types of paraphrases are equally helpful. In
particular, the automatically extracted paraphrases
from Callison-Burch (2008) seem to be the most
helpful, and by using them our unsupervised sys-
tem is able to beat even the best known supervised
FSD system. This is a very promising result because
it indicates that we can use automatically extracted
paraphrases and do not have to rely on hand-crafted
resources like Wordnet as our source of paraphrases.
</bodyText>
<footnote confidence="0.98399">
6Our experiments, and experiments in Allan et al. (2000b)
showed that keeping full documents does not improve results,
while increasing running time.
</footnote>
<page confidence="0.994667">
343
</page>
<table confidence="0.998803142857143">
System C&apos;ni.
UMass 100 0.721
UMass 1000 0.706
Best supervised system 0.661
Wordnet 0.657
MSR Paraphrases 0.642
Syntactic paraphrases 0.575
</table>
<tableCaption confidence="0.918468">
Table 1: TDT FSD results for different systems, lower is
better. The number next to UMass system indicates the
number of features kept for each document (selected ac-
cording to their TFIDF). All paraphrasing systems work
with full documents. Results for the best supervised sys-
tem were taken from Kumaran and Allan (2005).
</tableCaption>
<bodyText confidence="0.9997335">
The difference between our system and the UMass
system is significant at p = 0.05 using a paired t-test
over the individual topic costs. We were not able to
test significance against the supervised state-of-the-
art because we did not have access to this system. In
terms of efficiency, our approach is still O(1), like
the approach in Petrovi´c et al. (2010), but in practice
it is somewhat slower because hashing the expanded
documents takes more time. We measured the run-
ning time of our system, and it is 3.5 times slower
than the basic approach of Petrovi´c et al. (2010), but
also 3.5 times faster than the UMass system, while
outperforming both of these systems.
How does quality of paraphrases affect results?
We have shown that using automatically obtained
paraphrases to expand documents is beneficial in
first story detection. Because there are different
ways of extracting paraphrases, some of which are
targeted more towards recall, and some towards pre-
cision, we want to know which techniques would be
more suitable to extract paraphrases for use in FSD.
Here, precision is the ratio between extracted word
pairs that are actual paraphrases and all the word
pairs extracted, and recall is the ratio between ex-
tracted word pairs that are actual paraphrases, and
all the possible paraphrase pairs that could have been
extracted. In this experiment we focus on the syn-
tactic paraphrases which yielded the best results. To
lower recall, we randomly remove paraphrase pairs
from the corpus, and to lower precision, we add ran-
dom paraphrase pairs to our table. All the results
are shown in Table 2. Numbers next to precision
</bodyText>
<table confidence="0.999823272727273">
Paraphrasing resource Cmin
Precision 0.1 0.603
Precision 0.2 0.672
Precision 0.3 0.565
Precision 0.4 0.603
Precision 0.5 0.626
Recall 0.9 0.609
Recall 0.8 0.606
Recall 0.7 0.632
Recall 0.6 0.610
Recall 0.5 0.626
</table>
<tableCaption confidence="0.82310825">
Table 2: Effect of paraphrase precision and recall on FSD
performance. Numbers next to recall and precision indi-
cate the sampling rate and the proportion of added ran-
dom pairs, respectively.
</tableCaption>
<bodyText confidence="0.999985666666667">
and recall indicate the proportion of added random
pairs and the proportion of removed pairs, respec-
tively (e.g., recall 0.4 means that 40% of pairs were
removed from the original resource). We can see
that the results are much more stable with respect to
recall – there is an initial drop in performance when
we remove the first 10% of paraphrases, but after
that removing more paraphrases does not affect per-
formance very much. On the other hand, changing
the precision has a bigger impact on the results. For
example, we can see that our system using a para-
phrase corpus with 30% of pairs added at random
performs even better than the system that uses the
original corpus. On the other hand, adding 20% of
random pairs performs substantially worse than the
original corpus. These results show that it is more
important for the paraphrases to have good precision
than to have good recall.
</bodyText>
<subsectionHeader confidence="0.996719">
4.3 Twitter results
</subsectionHeader>
<bodyText confidence="0.999905">
Because the Twitter event corpus that we use con-
sists of over 50 million documents, we cannot
use the UMass system here due to its linear per-
document time complexity. Instead, our baseline
system here is the FSD system of (Petrovi´c et
al., 2010), without any paraphrasing. This sys-
tem uses the same approach as the UMass system,
and (Petrovi´c et al., 2010) showed that it achieves
very similar results. This means that our baseline, al-
</bodyText>
<page confidence="0.996179">
344
</page>
<bodyText confidence="0.999958895833334">
though coming from a different system, is still state-
of-the-art. We make some Twitter-specific modifi-
cation to the baseline system that slightly improve
the results. Specifically, the baseline uses no stem-
ming, ignores links, @-mentions, and treats hash-
tags as normal words (i.e., removes the leading ‘#’
character). While removing links and @-mentions
was also done in (Petrovi´c et al., 2010), our pre-
liminary experiments showed that keeping hashtags,
only without the hash sign improves the results. Ad-
ditionally, we limit the number of documents in a
bucket to at most 30% of the expected number of
collisions for a single day (we assume one million
documents per day).
Results for the different systems are shown in Ta-
ble 3. First, we can see that not using stemming is
much better than using it, which is the opposite from
what is the case in TDT. Second, we can see that the
improvements from using paraphrases that we had in
TDT data are different here. Syntactic paraphrases
and the MSR paraphrases do not help, whereas the
paraphrases extracted from Wordnet did improve the
results, although the gains are not as large as in TDT.
A paired t-test revealed that none of the differences
between the baseline system and the systems that
use paraphrases were significant at p = 0.05.
To gain more insight into why the results are dif-
ferent here, we look at the proportion of words in
the documents that are being paraphrased, i.e., the
coverage of the paraphrasing resource. We can see
from Table 4 that the situation in TDT and Twit-
ter is very different. Coverage of MSR and syntac-
tic paraphrases was lower in Twitter than in TDT,
whereas Wordnet coverage was better on Twitter.
While it seems that the benefits of using paraphrases
in Twitter are not as clear as in news, our efficient
approach enables us to answer questions like these,
which could not be answered otherwise.
To illustrate how paraphrases help detect old
tweets, consider the tweet According to Russian avi-
ation officials, two passengers survived the crash,
but are in critical condition. Before paraphrasing,
the closest tweet returned by our system was Shaz-
aad Hussein has died in Birmingham after being
run over, two others are in critical condition, which
is not very related. After applying paraphrasing,
in particular knowing that officials is a paraphrase
of authorities, the closest tweet returned was Some
</bodyText>
<table confidence="0.740495166666667">
System C&apos;ni.
Baseline system (stemming) 0.756
Baseline system (no stemming) 0.694
Wordnet 0.679
MSR Paraphrases 0.739
Syntactic paraphrases 0.729
</table>
<tableCaption confidence="0.977315">
Table 3: Twitter FSD results for different systems, lower
is better. The baseline system is that of (Petrovi´c et al.,
2010).
</tableCaption>
<table confidence="0.963978">
Paraphrases Coverage TDT (%) Coverage Twitter (%)
Wordnet 52.5 56.1
MSR 33.5 31.0
Syntactic 35.6 31.7
</table>
<tableCaption confidence="0.999711">
Table 4: Coverage of different resources.
</tableCaption>
<bodyText confidence="0.999285545454545">
Russian authorities are reporting one survivor, oth-
ers are saying there are three. There were 37 total
on board, which is on the same event. There are also
cases where paraphrases hurt. For example, before
paraphrasing the tweet Top News #debt #deal #ceil-
ing #party had the nearest neighbor New debt ceiling
deal explained, whereas after paraphrasing, because
the word roof is a paraphrase of ceiling, the nearest
neighbor was The roof the roof the roof is on fire!.
Cases like this could be fixed by looking at the con-
text of the word, but we leave this for future work.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999992625">
We present a way of incorporating paraphrase infor-
mation in a streaming first story detection system.
To the best of our knowledge, this is the first work
to use paraphrases in first story detection, and also
the first work to combine paraphrases with locality-
sensitive hashing to achieve fast retrieval of doc-
uments that are written with different words, but
talk about the same thing. We compare different
sources of paraphrases and show that our unsuper-
vised FSD system that uses syntactically constrained
paraphrases achieves state-of-the-art results, beating
both the best supervised and unsupervised systems.
To test our approach on very large data, we construct
a corpus of events for Twitter. Our approach scales
well on this data both in terms of time and mem-
ory, and we show that paraphrases again help, but
</bodyText>
<page confidence="0.996576">
345
</page>
<bodyText confidence="0.99994">
this time the paraphrase sources yield different im-
provements from TDT data. We find that this differ-
ence can be explained by the different coverage of
the paraphrasing resources.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99987175">
The authors would like to thank Mirella Lapata
for her help with paraphrasing resources. We also
acknowledge financial support from EPSRC grant
EP/J020664/1.
</bodyText>
<sectionHeader confidence="0.999188" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999815">
Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing,
Alex Smola, and Choon Hui Teo. 2011. Unified
analysis of streaming news. In Proceedings of WWW,
pages 267–276. ACM.
James Allan, Victor Lavrenko, and Hubert Jin. 2000a.
First story detection in tdt is hard. In Proceedings of
the CIKM, pages 374–381. ACM.
James Allan, Victor Lavrenko, Daniella Malin, and Rus-
sell Swan. 2000b. Detections, bounds, and timelines:
Umass and tdt-3. In Proceedings of Topic Detection
and Tracking Workshop, pages 167–174.
James Allan. 2002. Topic detection and tracking: event-
based information organization. Kluwer Academic
Publishers.
Hila Becker, Mor Naaman, and Luis Gravano. 2011. Se-
lecting quality twitter content for events. In Proceed-
ings of ICWSM.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of NAACL, pages
17–24. Association for Computational Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 196–205. Asso-
ciation for Computational Linguistics.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4):597–614.
Christiane Fellbaum. 1998. WordNet: An electronic lex-
ical database. The MIT press.
Jonathan Fiscus. 2001. Overview of results (nist). In
Proceedings of the TDT 2001 Workshop.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In STOC ’98: Proceedings of the thirti-
eth annual ACM symposium on Theory of computing,
pages 604–613, New York, NY, USA. ACM.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of the 15th international conference on
World Wide Web, pages 387–396. ACM.
Giridhar Kumaran and James Allan. 2005. Using names
and topics for new event detection. In Proceedings
of EMNLP, pages 121–128. Association for Computa-
tional Linguistics.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341–387.
Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In Proceedings of the 11th annual confer-
ence of the North American Chapter of the ACL, pages
181–189.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In proceedings of EMNLP, pages 142–149.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of ACL, volume 45, page 464.
Karen Sp¨arck Jones and John Tait. 1984. Automatic
search term variant generation. Journal of Documen-
tation, 40(1):50–66.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In In Pro-
ceedings of the 19th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 4–11.
</reference>
<page confidence="0.9991">
346
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.884328">
<title confidence="0.999879">Using paraphrases for improving first story detection in news and Twitter</title>
<author confidence="0.998322">Saˇsa Petrovi´c Miles Osborne Victor Lavrenko</author>
<affiliation confidence="0.9989655">School of Informatics School of Informatics School of Informatics University of Edinburgh University of Edinburgh University of</affiliation>
<email confidence="0.898895">sasa.petrovic@ed.ac.ukmiles@inf.ed.ac.ukvlavrenk@inf.ed.ac.uk</email>
<abstract confidence="0.999363095238095">First story detection (FSD) involves identifying first stories about events from a continuous stream of documents. A major problem in this task is the high degree of lexical variation in documents which makes it very difficult to detect stories that talk about the same event but expressed using different words. We suggest using paraphrases to alleviate this problem, making this the first work to use paraphrases for FSD. We show a novel way of integrating paraphrases with locality sensitive hashing (LSH) in order to obtain an efficient FSD system that can scale to very large datasets. Our system achieves state-of-the-art results on the first story detection task, beating both the best supervised and unsupervised systems. To test our approach on large data, we construct a corpus of events for Twitter, consisting of 50 million documents, and show that paraphrasing is also beneficial in this domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amr Ahmed</author>
<author>Qirong Ho</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Eric Xing, Alex Smola, and Choon Hui Teo.</title>
<date>2011</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>267--276</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4090" citStr="Ahmed et al. (2011)" startWordPosition="661" endWordPosition="664">new events are actually paraphrases of the documents already 338 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 338–346, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics seen. Our approach is simple and we show a novel way of integrating paraphrases with locality sensitive hashing (LSH) (Indyk and Motwani, 1998). This way we obtain a very efficient FSD system with all the benefits of using paraphrases, while avoiding computationally expensive topic modeling approaches such as Ahmed et al. (2011). First story detection was introduced as a task before the popularization of social media. Event detection in social media, especially Twitter is a very good fit: we cover a much larger set of events than would be possible by using newswire, and the stories are reported in real time, often much sooner than in news. Of course, social media carries additional problems not found in traditional media: we have to deal with huge amounts of data, the data is very noisy (both due to spam and due to spelling and grammar errors), and in the case of Twitter, documents are extremely short. There has been</context>
<context position="23859" citStr="Ahmed et al., 2011" startWordPosition="4089" endWordPosition="4092"> time complexity. We use stemming (Porter, 1980) and, the same as (Petrovi´c et al., 2010), we use 13 bits per key and 70 hash tables for LSH. Additionally, we use the hashing trick described in section 2.4 with a pool of size 218. Paraphrasing is implemented in this system as described in section 2.4. While the UMass system was among the best systems that took part in the TDT competitions, there has been research in event detection since the competitions stopped. Recent work on event detection includes a hybrid clustering and topic model with rich features such as entities, time, and topics (Ahmed et al., 2011). We do not compare our system to Ahmed et al. (2011) because in terms of the numerical Cmin score, their approach does not outperform the UMass system. This is not surprising as the primary goal in Ahmed et al. (2011) was not to improve FSD performance, but rather to create storylines and support structured browsing. We compare our approach to the best reported result in the literature on the TDT5 data. To the best of our knowledge, the highest reported results in FSD come from a supervised system described in Kumaran and Allan (2005). This system uses an SVM classifier with the features bein</context>
</contexts>
<marker>Ahmed, Ho, Eisenstein, 2011</marker>
<rawString>Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing, Alex Smola, and Choon Hui Teo. 2011. Unified analysis of streaming news. In Proceedings of WWW, pages 267–276. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Victor Lavrenko</author>
<author>Hubert Jin</author>
</authors>
<title>First story detection in tdt is hard.</title>
<date>2000</date>
<booktitle>In Proceedings of the CIKM,</booktitle>
<pages>374--381</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1723" citStr="Allan et al., 2000" startWordPosition="269" endWordPosition="272">ruct a corpus of events for Twitter, consisting of 50 million documents, and show that paraphrasing is also beneficial in this domain. 1 Introduction First story detection (FSD), sometimes also called new event detection (NED), is the task of detecting the first story about a new event from a stream of documents. It began as one of the tasks in Topic Detection and Tracking (TDT) (Allan, 2002) where the overall goal of the project was to improve technologies related to event-based information organization tasks. Of the five TDT tasks, first story detection is considered the most difficult one (Allan et al., 2000a). A good FSD system would be very useful for business or intelligence analysts where timely discovery of events is crucial. With the significant increase in the amount of information being produced and consumed every day, a crucial requirement for a modern FSD system to be useful is efficiency. This means that the system should be able to work in a streaming setting where documents are constantly coming in at a high rate, while still producing good results. While previous work has addressed the efficiency (Petrovi´c et al., 2010) aspect, there has been little work on improving FSD performanc</context>
<context position="5575" citStr="Allan et al., 2000" startWordPosition="922" endWordPosition="925">Twitter. Our main contributions are: i) we create a first corpus of events on Twitter, ii) we show how to use paraphrases in FSD, and how to combine it with LSH to handle high-volume streams, iii) our unsupervised system that uses paraphrases achieves the highest reported results on the TDT5 corpus, beating both the supervised and unsupervised state of the art, while still keeping a constant per-document time complexity, and iv) we show that paraphrases also help in Twitter, although less than in TDT. 2 Paraphrasing and FSD 2.1 Current approaches to efficient FSD State-of-the-art FSD systems (Allan et al., 2000b) use a fairly simple approach. Documents are represented as TF-IDF weighted vectors, their distance is measured in terms of the cosine distance, and they use a k-nearest neighbors clustering algorithm, with k usually set to 1. The novelty score for a document is the cosine distance to the nearest neighbor: score(d) = 1 − max d&apos;EDt Dt is the set of all documents up to time t when document d arrived. Because the max in equation (1) takes O(|Dt|) time to compute in the worst case, Petrovi´c et al. (2010) introduced a way of using locality sensitive hashing (LSH) to make this time O(1), while re</context>
<context position="22959" citStr="Allan et al., 2000" startWordPosition="3929" endWordPosition="3932">spectively), Pmiss and PFA are the miss and false alarm rate, and Ptarget and Pnon−target are the prior target and non-target probabilities. Different FSD systems are compared on the minimal cost Cmin, which is the minimal value of Cdet over all threshold values. This means that in FSD evaluation, a lower value of Cmin indicates a better system. 4.2 TDT results For the TDT experiments, we use the English portion of TDT-5 dataset, consisting of 126 topics in 278,108 documents. Similar to (Petrovi´c et al., 2010), we compare our approach to a state-of-theart FSD system, namely the UMass system (Allan et al., 2000b). This system always scored high in the TDT competitions and is known to perform at least as well as other systems that also took part in the competition (Fiscus, 2001). Our system is based on the streaming FSD system of (Petrovi´c et al., 2010) which has a constant per-document time complexity. We use stemming (Porter, 1980) and, the same as (Petrovi´c et al., 2010), we use 13 bits per key and 70 hash tables for LSH. Additionally, we use the hashing trick described in section 2.4 with a pool of size 218. Paraphrasing is implemented in this system as described in section 2.4. While the UMass</context>
<context position="25564" citStr="Allan et al. (2000" startWordPosition="4389" endWordPosition="4392">rvised state of the art, regardless of which source of paraphrasing is used. However, it is clear that not all types of paraphrases are equally helpful. In particular, the automatically extracted paraphrases from Callison-Burch (2008) seem to be the most helpful, and by using them our unsupervised system is able to beat even the best known supervised FSD system. This is a very promising result because it indicates that we can use automatically extracted paraphrases and do not have to rely on hand-crafted resources like Wordnet as our source of paraphrases. 6Our experiments, and experiments in Allan et al. (2000b) showed that keeping full documents does not improve results, while increasing running time. 343 System C&apos;ni. UMass 100 0.721 UMass 1000 0.706 Best supervised system 0.661 Wordnet 0.657 MSR Paraphrases 0.642 Syntactic paraphrases 0.575 Table 1: TDT FSD results for different systems, lower is better. The number next to UMass system indicates the number of features kept for each document (selected according to their TFIDF). All paraphrasing systems work with full documents. Results for the best supervised system were taken from Kumaran and Allan (2005). The difference between our system and th</context>
</contexts>
<marker>Allan, Lavrenko, Jin, 2000</marker>
<rawString>James Allan, Victor Lavrenko, and Hubert Jin. 2000a. First story detection in tdt is hard. In Proceedings of the CIKM, pages 374–381. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Victor Lavrenko</author>
<author>Daniella Malin</author>
<author>Russell Swan</author>
</authors>
<title>Detections, bounds, and timelines: Umass and tdt-3.</title>
<date>2000</date>
<booktitle>In Proceedings of Topic Detection and Tracking Workshop,</booktitle>
<pages>167--174</pages>
<contexts>
<context position="1723" citStr="Allan et al., 2000" startWordPosition="269" endWordPosition="272">ruct a corpus of events for Twitter, consisting of 50 million documents, and show that paraphrasing is also beneficial in this domain. 1 Introduction First story detection (FSD), sometimes also called new event detection (NED), is the task of detecting the first story about a new event from a stream of documents. It began as one of the tasks in Topic Detection and Tracking (TDT) (Allan, 2002) where the overall goal of the project was to improve technologies related to event-based information organization tasks. Of the five TDT tasks, first story detection is considered the most difficult one (Allan et al., 2000a). A good FSD system would be very useful for business or intelligence analysts where timely discovery of events is crucial. With the significant increase in the amount of information being produced and consumed every day, a crucial requirement for a modern FSD system to be useful is efficiency. This means that the system should be able to work in a streaming setting where documents are constantly coming in at a high rate, while still producing good results. While previous work has addressed the efficiency (Petrovi´c et al., 2010) aspect, there has been little work on improving FSD performanc</context>
<context position="5575" citStr="Allan et al., 2000" startWordPosition="922" endWordPosition="925">Twitter. Our main contributions are: i) we create a first corpus of events on Twitter, ii) we show how to use paraphrases in FSD, and how to combine it with LSH to handle high-volume streams, iii) our unsupervised system that uses paraphrases achieves the highest reported results on the TDT5 corpus, beating both the supervised and unsupervised state of the art, while still keeping a constant per-document time complexity, and iv) we show that paraphrases also help in Twitter, although less than in TDT. 2 Paraphrasing and FSD 2.1 Current approaches to efficient FSD State-of-the-art FSD systems (Allan et al., 2000b) use a fairly simple approach. Documents are represented as TF-IDF weighted vectors, their distance is measured in terms of the cosine distance, and they use a k-nearest neighbors clustering algorithm, with k usually set to 1. The novelty score for a document is the cosine distance to the nearest neighbor: score(d) = 1 − max d&apos;EDt Dt is the set of all documents up to time t when document d arrived. Because the max in equation (1) takes O(|Dt|) time to compute in the worst case, Petrovi´c et al. (2010) introduced a way of using locality sensitive hashing (LSH) to make this time O(1), while re</context>
<context position="22959" citStr="Allan et al., 2000" startWordPosition="3929" endWordPosition="3932">spectively), Pmiss and PFA are the miss and false alarm rate, and Ptarget and Pnon−target are the prior target and non-target probabilities. Different FSD systems are compared on the minimal cost Cmin, which is the minimal value of Cdet over all threshold values. This means that in FSD evaluation, a lower value of Cmin indicates a better system. 4.2 TDT results For the TDT experiments, we use the English portion of TDT-5 dataset, consisting of 126 topics in 278,108 documents. Similar to (Petrovi´c et al., 2010), we compare our approach to a state-of-theart FSD system, namely the UMass system (Allan et al., 2000b). This system always scored high in the TDT competitions and is known to perform at least as well as other systems that also took part in the competition (Fiscus, 2001). Our system is based on the streaming FSD system of (Petrovi´c et al., 2010) which has a constant per-document time complexity. We use stemming (Porter, 1980) and, the same as (Petrovi´c et al., 2010), we use 13 bits per key and 70 hash tables for LSH. Additionally, we use the hashing trick described in section 2.4 with a pool of size 218. Paraphrasing is implemented in this system as described in section 2.4. While the UMass</context>
<context position="25564" citStr="Allan et al. (2000" startWordPosition="4389" endWordPosition="4392">rvised state of the art, regardless of which source of paraphrasing is used. However, it is clear that not all types of paraphrases are equally helpful. In particular, the automatically extracted paraphrases from Callison-Burch (2008) seem to be the most helpful, and by using them our unsupervised system is able to beat even the best known supervised FSD system. This is a very promising result because it indicates that we can use automatically extracted paraphrases and do not have to rely on hand-crafted resources like Wordnet as our source of paraphrases. 6Our experiments, and experiments in Allan et al. (2000b) showed that keeping full documents does not improve results, while increasing running time. 343 System C&apos;ni. UMass 100 0.721 UMass 1000 0.706 Best supervised system 0.661 Wordnet 0.657 MSR Paraphrases 0.642 Syntactic paraphrases 0.575 Table 1: TDT FSD results for different systems, lower is better. The number next to UMass system indicates the number of features kept for each document (selected according to their TFIDF). All paraphrasing systems work with full documents. Results for the best supervised system were taken from Kumaran and Allan (2005). The difference between our system and th</context>
</contexts>
<marker>Allan, Lavrenko, Malin, Swan, 2000</marker>
<rawString>James Allan, Victor Lavrenko, Daniella Malin, and Russell Swan. 2000b. Detections, bounds, and timelines: Umass and tdt-3. In Proceedings of Topic Detection and Tracking Workshop, pages 167–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
</authors>
<title>Topic detection and tracking: eventbased information organization.</title>
<date>2002</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1500" citStr="Allan, 2002" startWordPosition="233" endWordPosition="234">can scale to very large datasets. Our system achieves state-of-the-art results on the first story detection task, beating both the best supervised and unsupervised systems. To test our approach on large data, we construct a corpus of events for Twitter, consisting of 50 million documents, and show that paraphrasing is also beneficial in this domain. 1 Introduction First story detection (FSD), sometimes also called new event detection (NED), is the task of detecting the first story about a new event from a stream of documents. It began as one of the tasks in Topic Detection and Tracking (TDT) (Allan, 2002) where the overall goal of the project was to improve technologies related to event-based information organization tasks. Of the five TDT tasks, first story detection is considered the most difficult one (Allan et al., 2000a). A good FSD system would be very useful for business or intelligence analysts where timely discovery of events is crucial. With the significant increase in the amount of information being produced and consumed every day, a crucial requirement for a modern FSD system to be useful is efficiency. This means that the system should be able to work in a streaming setting where </context>
</contexts>
<marker>Allan, 2002</marker>
<rawString>James Allan. 2002. Topic detection and tracking: eventbased information organization. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hila Becker</author>
<author>Mor Naaman</author>
<author>Luis Gravano</author>
</authors>
<title>Selecting quality twitter content for events.</title>
<date>2011</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<contexts>
<context position="15967" citStr="Becker et al. (2011)" startWordPosition="2729" endWordPosition="2732">ent detection on Twitter As we mentioned before, research on event detection in social media is hampered by the lack of a corpus that could be used to measure performance. The need for a standard corpus is evident from the related work on event detection in Twitter. For example, (Petrovi´c et al., 2010) address the scaling problem in social media and present a system that runs in constant time per document, but the evaluation of their system on Twitter data was limited to very high-volume events. The only attempt in creating a corpus of events for Twitter that we are aware of was presented in Becker et al. (2011). Unfortunately, that corpus is not suitable for FSD evaluation for two main reasons: i) the events were picked from the highest-volume events identified by the system (similar to what was done in Petrovi´c et al. (2010)), introducing not only a bias towards highvolume events, but also a bias toward the kinds of events that their system can detect, and ii) the authors only considered tweets by users who set their location to New York, which introduces a strong bias towards the type of events that can appear in the corpus. While these problems were not relevant to the 341 work of (Becker et al.</context>
</contexts>
<marker>Becker, Naaman, Gravano, 2011</marker>
<rawString>Hila Becker, Mor Naaman, and Luis Gravano. 2011. Selecting quality twitter content for events. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3011" citStr="Callison-Burch et al., 2006" startWordPosition="490" endWordPosition="493"> of lexical variation in documents that cover the same event. Here we address this problem, while keeping in mind the efficiency constraints. The problem of lexical variation plagues many IR and NLP tasks, and one way it has been addressed in the past is through the use of paraphrases. Paraphrases are alternative ways of expressing the same meaning in the same language. For example, the phrase he got married can be paraphrased as he tied the knot. Paraphrases were already shown to help in a number of tasks: for machine translation to translate unknown phrases by translating their paraphrases (Callison-Burch et al., 2006), for query expansion in information retrieval (Sp¨arck Jones and Tait, 1984; Jones et al., 2006), or for improving question answering (Riezler et al., 2007). A much more detailed discussion on the use of paraphrases and ways to extract them is given in (Madnani and Dorr, 2010). Here, we present the first work to use paraphrases for improving first story detection. Using paraphrases, we are able to detect that some documents previously thought to be about new events are actually paraphrases of the documents already 338 2012 Conference of the North American Chapter of the Association for Comput</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of NAACL, pages 17–24. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>196--205</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7716" citStr="Callison-Burch (2008)" startWordPosition="1287" endWordPosition="1288">d sentential paraphrases, where entire sentences are in a paraphrastic relationship. Here we use the simplest form, lexical paraphrases, but our approach, described in section 2.3, is general and it would be trivial to use phrasal paraphrases in the same way – we leave this for future work. We use three sources of paraphrases: Wordnet (Fellbaum, 1998), a carefully curated lexical database of English containing synonym sets, Microsoft Research paraphrase tables (Quirk et al., 2004), a set of paraphrase pairs automatically extracted from news texts, and syntacticallyconstrained paraphrases from Callison-Burch (2008) which are extracted from parallel text. We also considered using paraphrases from Cohn et al. (2008), but using them provided only minor improvement over the baseline model. This is likely due to the small size of that corpus (a total of 7 cos(d,d&apos;). (1) 339 thousand pairs). We do not show results for this paraphrase corpus in our results section. Wordnet paraphrases contained 150 thousand word pairs extracted from Wordnet’s synsets, where all the pairs of words within one synset were considered to be paraphrases. MSR paraphrases were extracted from the phrase tables provided by MSR. Two word</context>
<context position="8959" citStr="Callison-Burch, 2008" startWordPosition="1487" endWordPosition="1488">phrases if they were aligned at least once in the most probable alignment, with the probability of both backward and forward alignment of at least 0.2. In our initial experiments we varied this threshold and found it has little effect on results. Using this method, we extracted 50 thousand paraphrase pairs. Finally, we use the method of Callison-Burch (2008) to extract syntactically constrained paraphrases from a parallel corpus. This method requires that phrases and their paraphrases be the same syntactic type, and has been shown to substantially improve the quality of extracted paraphrases (Callison-Burch, 2008). We extracted paraphrases for all the words that appeared in the MSR paraphrase corpus, and then kept all the pairs that had the paraphrase probability of at least 0.2. This way, we extracted 48 thousand pairs. All three resources we use are very different: they come from different domains (news text, legal text, general English), and they have very little overlap (less than 5% of pairs are shared by any two resources). 2.3 Efficient paraphrasing in FSD In this section, we explain how to use paraphrases in a first story detection system. We account for paraphrases by changing how we compute t</context>
<context position="25180" citStr="Callison-Burch (2008)" startWordPosition="4325" endWordPosition="4326">in the UMass system) computed using i) full text, ii) only named entities in the document, and iii) only topic terms. The classifier was trained on TDT3 and TDT4 corpora and tested on TDT5. Table 1 shows the results for TDT5 data. UMass 1000 is the run that was submitted as the official run in the TDT competition.6 We can see that using paraphrases improves the results over the unsupervised state of the art, regardless of which source of paraphrasing is used. However, it is clear that not all types of paraphrases are equally helpful. In particular, the automatically extracted paraphrases from Callison-Burch (2008) seem to be the most helpful, and by using them our unsupervised system is able to beat even the best known supervised FSD system. This is a very promising result because it indicates that we can use automatically extracted paraphrases and do not have to rely on hand-crafted resources like Wordnet as our source of paraphrases. 6Our experiments, and experiments in Allan et al. (2000b) showed that keeping full documents does not improve results, while increasing running time. 343 System C&apos;ni. UMass 100 0.721 UMass 1000 0.706 Best supervised system 0.661 Wordnet 0.657 MSR Paraphrases 0.642 Syntac</context>
</contexts>
<marker>Callison-Burch, 2008</marker>
<rawString>Chris Callison-Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 196–205. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Chris Callison-Burch</author>
<author>Mirella Lapata</author>
</authors>
<title>Constructing corpora for the development and evaluation of paraphrase systems.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="7817" citStr="Cohn et al. (2008)" startWordPosition="1301" endWordPosition="1304">plest form, lexical paraphrases, but our approach, described in section 2.3, is general and it would be trivial to use phrasal paraphrases in the same way – we leave this for future work. We use three sources of paraphrases: Wordnet (Fellbaum, 1998), a carefully curated lexical database of English containing synonym sets, Microsoft Research paraphrase tables (Quirk et al., 2004), a set of paraphrase pairs automatically extracted from news texts, and syntacticallyconstrained paraphrases from Callison-Burch (2008) which are extracted from parallel text. We also considered using paraphrases from Cohn et al. (2008), but using them provided only minor improvement over the baseline model. This is likely due to the small size of that corpus (a total of 7 cos(d,d&apos;). (1) 339 thousand pairs). We do not show results for this paraphrase corpus in our results section. Wordnet paraphrases contained 150 thousand word pairs extracted from Wordnet’s synsets, where all the pairs of words within one synset were considered to be paraphrases. MSR paraphrases were extracted from the phrase tables provided by MSR. Two words were considered paraphrases if they were aligned at least once in the most probable alignment, with</context>
</contexts>
<marker>Cohn, Callison-Burch, Lapata, 2008</marker>
<rawString>Trevor Cohn, Chris Callison-Burch, and Mirella Lapata. 2008. Constructing corpora for the development and evaluation of paraphrase systems. Computational Linguistics, 34(4):597–614.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>The MIT press.</publisher>
<contexts>
<context position="7448" citStr="Fellbaum, 1998" startWordPosition="1252" endWordPosition="1253"> containing the nearest neighbor with high probability. 2.2 Paraphrases There are several levels of paraphrasing – lexical paraphrases, where the relationship is restricted to individual lexical items, phrasal paraphrases, where longer phrases are considered, and sentential paraphrases, where entire sentences are in a paraphrastic relationship. Here we use the simplest form, lexical paraphrases, but our approach, described in section 2.3, is general and it would be trivial to use phrasal paraphrases in the same way – we leave this for future work. We use three sources of paraphrases: Wordnet (Fellbaum, 1998), a carefully curated lexical database of English containing synonym sets, Microsoft Research paraphrase tables (Quirk et al., 2004), a set of paraphrase pairs automatically extracted from news texts, and syntacticallyconstrained paraphrases from Callison-Burch (2008) which are extracted from parallel text. We also considered using paraphrases from Cohn et al. (2008), but using them provided only minor improvement over the baseline model. This is likely due to the small size of that corpus (a total of 7 cos(d,d&apos;). (1) 339 thousand pairs). We do not show results for this paraphrase corpus in ou</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An electronic lexical database. The MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Fiscus</author>
</authors>
<title>Overview of results (nist).</title>
<date>2001</date>
<booktitle>In Proceedings of the TDT</booktitle>
<note>Workshop.</note>
<contexts>
<context position="23129" citStr="Fiscus, 2001" startWordPosition="3961" endWordPosition="3962"> on the minimal cost Cmin, which is the minimal value of Cdet over all threshold values. This means that in FSD evaluation, a lower value of Cmin indicates a better system. 4.2 TDT results For the TDT experiments, we use the English portion of TDT-5 dataset, consisting of 126 topics in 278,108 documents. Similar to (Petrovi´c et al., 2010), we compare our approach to a state-of-theart FSD system, namely the UMass system (Allan et al., 2000b). This system always scored high in the TDT competitions and is known to perform at least as well as other systems that also took part in the competition (Fiscus, 2001). Our system is based on the streaming FSD system of (Petrovi´c et al., 2010) which has a constant per-document time complexity. We use stemming (Porter, 1980) and, the same as (Petrovi´c et al., 2010), we use 13 bits per key and 70 hash tables for LSH. Additionally, we use the hashing trick described in section 2.4 with a pool of size 218. Paraphrasing is implemented in this system as described in section 2.4. While the UMass system was among the best systems that took part in the TDT competitions, there has been research in event detection since the competitions stopped. Recent work on event</context>
</contexts>
<marker>Fiscus, 2001</marker>
<rawString>Jonathan Fiscus. 2001. Overview of results (nist). In Proceedings of the TDT 2001 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Indyk</author>
<author>Rajeev Motwani</author>
</authors>
<title>Approximate nearest neighbors: towards removing the curse of dimensionality.</title>
<date>1998</date>
<booktitle>In STOC ’98: Proceedings of the thirtieth annual ACM symposium on Theory of computing,</booktitle>
<pages>604--613</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3903" citStr="Indyk and Motwani, 1998" startWordPosition="630" endWordPosition="633">orr, 2010). Here, we present the first work to use paraphrases for improving first story detection. Using paraphrases, we are able to detect that some documents previously thought to be about new events are actually paraphrases of the documents already 338 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 338–346, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics seen. Our approach is simple and we show a novel way of integrating paraphrases with locality sensitive hashing (LSH) (Indyk and Motwani, 1998). This way we obtain a very efficient FSD system with all the benefits of using paraphrases, while avoiding computationally expensive topic modeling approaches such as Ahmed et al. (2011). First story detection was introduced as a task before the popularization of social media. Event detection in social media, especially Twitter is a very good fit: we cover a much larger set of events than would be possible by using newswire, and the stories are reported in real time, often much sooner than in news. Of course, social media carries additional problems not found in traditional media: we have to </context>
</contexts>
<marker>Indyk, Motwani, 1998</marker>
<rawString>Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards removing the curse of dimensionality. In STOC ’98: Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604–613, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Benjamin Rey</author>
<author>Omid Madani</author>
<author>Wiley Greiner</author>
</authors>
<title>Generating query substitutions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web,</booktitle>
<pages>387--396</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3108" citStr="Jones et al., 2006" startWordPosition="506" endWordPosition="509">in mind the efficiency constraints. The problem of lexical variation plagues many IR and NLP tasks, and one way it has been addressed in the past is through the use of paraphrases. Paraphrases are alternative ways of expressing the same meaning in the same language. For example, the phrase he got married can be paraphrased as he tied the knot. Paraphrases were already shown to help in a number of tasks: for machine translation to translate unknown phrases by translating their paraphrases (Callison-Burch et al., 2006), for query expansion in information retrieval (Sp¨arck Jones and Tait, 1984; Jones et al., 2006), or for improving question answering (Riezler et al., 2007). A much more detailed discussion on the use of paraphrases and ways to extract them is given in (Madnani and Dorr, 2010). Here, we present the first work to use paraphrases for improving first story detection. Using paraphrases, we are able to detect that some documents previously thought to be about new events are actually paraphrases of the documents already 338 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 338–346, Montr´eal, Canada, June 3-8, 201</context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating query substitutions. In Proceedings of the 15th international conference on World Wide Web, pages 387–396. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giridhar Kumaran</author>
<author>James Allan</author>
</authors>
<title>Using names and topics for new event detection.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>121--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24400" citStr="Kumaran and Allan (2005)" startWordPosition="4186" endWordPosition="4190">ic model with rich features such as entities, time, and topics (Ahmed et al., 2011). We do not compare our system to Ahmed et al. (2011) because in terms of the numerical Cmin score, their approach does not outperform the UMass system. This is not surprising as the primary goal in Ahmed et al. (2011) was not to improve FSD performance, but rather to create storylines and support structured browsing. We compare our approach to the best reported result in the literature on the TDT5 data. To the best of our knowledge, the highest reported results in FSD come from a supervised system described in Kumaran and Allan (2005). This system uses an SVM classifier with the features being FSD scores from unsupervised systems (the authors used scores computed in the same way as is done in the UMass system) computed using i) full text, ii) only named entities in the document, and iii) only topic terms. The classifier was trained on TDT3 and TDT4 corpora and tested on TDT5. Table 1 shows the results for TDT5 data. UMass 1000 is the run that was submitted as the official run in the TDT competition.6 We can see that using paraphrases improves the results over the unsupervised state of the art, regardless of which source of</context>
<context position="26122" citStr="Kumaran and Allan (2005)" startWordPosition="4476" endWordPosition="4479">raphrases. 6Our experiments, and experiments in Allan et al. (2000b) showed that keeping full documents does not improve results, while increasing running time. 343 System C&apos;ni. UMass 100 0.721 UMass 1000 0.706 Best supervised system 0.661 Wordnet 0.657 MSR Paraphrases 0.642 Syntactic paraphrases 0.575 Table 1: TDT FSD results for different systems, lower is better. The number next to UMass system indicates the number of features kept for each document (selected according to their TFIDF). All paraphrasing systems work with full documents. Results for the best supervised system were taken from Kumaran and Allan (2005). The difference between our system and the UMass system is significant at p = 0.05 using a paired t-test over the individual topic costs. We were not able to test significance against the supervised state-of-theart because we did not have access to this system. In terms of efficiency, our approach is still O(1), like the approach in Petrovi´c et al. (2010), but in practice it is somewhat slower because hashing the expanded documents takes more time. We measured the running time of our system, and it is 3.5 times slower than the basic approach of Petrovi´c et al. (2010), but also 3.5 times fas</context>
</contexts>
<marker>Kumaran, Allan, 2005</marker>
<rawString>Giridhar Kumaran and James Allan. 2005. Using names and topics for new event detection. In Proceedings of EMNLP, pages 121–128. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of data-driven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="3289" citStr="Madnani and Dorr, 2010" startWordPosition="537" endWordPosition="540">es. Paraphrases are alternative ways of expressing the same meaning in the same language. For example, the phrase he got married can be paraphrased as he tied the knot. Paraphrases were already shown to help in a number of tasks: for machine translation to translate unknown phrases by translating their paraphrases (Callison-Burch et al., 2006), for query expansion in information retrieval (Sp¨arck Jones and Tait, 1984; Jones et al., 2006), or for improving question answering (Riezler et al., 2007). A much more detailed discussion on the use of paraphrases and ways to extract them is given in (Madnani and Dorr, 2010). Here, we present the first work to use paraphrases for improving first story detection. Using paraphrases, we are able to detect that some documents previously thought to be about new events are actually paraphrases of the documents already 338 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 338–346, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics seen. Our approach is simple and we show a novel way of integrating paraphrases with locality sensitive hashing (LSH) (Indyk and </context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics, 36(3):341–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Petrovi´c</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Streaming first story detection with application to twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th annual conference of the North American Chapter of the ACL,</booktitle>
<pages>181--189</pages>
<marker>Petrovi´c, Osborne, Lavrenko, 2010</marker>
<rawString>Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to twitter. In Proceedings of the 11th annual conference of the North American Chapter of the ACL, pages 181–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="23288" citStr="Porter, 1980" startWordPosition="3988" endWordPosition="3989">etter system. 4.2 TDT results For the TDT experiments, we use the English portion of TDT-5 dataset, consisting of 126 topics in 278,108 documents. Similar to (Petrovi´c et al., 2010), we compare our approach to a state-of-theart FSD system, namely the UMass system (Allan et al., 2000b). This system always scored high in the TDT competitions and is known to perform at least as well as other systems that also took part in the competition (Fiscus, 2001). Our system is based on the streaming FSD system of (Petrovi´c et al., 2010) which has a constant per-document time complexity. We use stemming (Porter, 1980) and, the same as (Petrovi´c et al., 2010), we use 13 bits per key and 70 hash tables for LSH. Additionally, we use the hashing trick described in section 2.4 with a pool of size 218. Paraphrasing is implemented in this system as described in section 2.4. While the UMass system was among the best systems that took part in the TDT competitions, there has been research in event detection since the competitions stopped. Recent work on event detection includes a hybrid clustering and topic model with rich features such as entities, time, and topics (Ahmed et al., 2011). We do not compare our syste</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation. In</title>
<date>2004</date>
<booktitle>In proceedings of EMNLP,</booktitle>
<pages>142--149</pages>
<contexts>
<context position="7580" citStr="Quirk et al., 2004" startWordPosition="1268" endWordPosition="1271">hrases, where the relationship is restricted to individual lexical items, phrasal paraphrases, where longer phrases are considered, and sentential paraphrases, where entire sentences are in a paraphrastic relationship. Here we use the simplest form, lexical paraphrases, but our approach, described in section 2.3, is general and it would be trivial to use phrasal paraphrases in the same way – we leave this for future work. We use three sources of paraphrases: Wordnet (Fellbaum, 1998), a carefully curated lexical database of English containing synonym sets, Microsoft Research paraphrase tables (Quirk et al., 2004), a set of paraphrase pairs automatically extracted from news texts, and syntacticallyconstrained paraphrases from Callison-Burch (2008) which are extracted from parallel text. We also considered using paraphrases from Cohn et al. (2008), but using them provided only minor improvement over the baseline model. This is likely due to the small size of that corpus (a total of 7 cos(d,d&apos;). (1) 339 thousand pairs). We do not show results for this paraphrase corpus in our results section. Wordnet paraphrases contained 150 thousand word pairs extracted from Wordnet’s synsets, where all the pairs of wo</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual machine translation for paraphrase generation. In In proceedings of EMNLP, pages 142–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical machine translation for query expansion in answer retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<volume>45</volume>
<pages>464</pages>
<contexts>
<context position="3168" citStr="Riezler et al., 2007" startWordPosition="515" endWordPosition="518"> variation plagues many IR and NLP tasks, and one way it has been addressed in the past is through the use of paraphrases. Paraphrases are alternative ways of expressing the same meaning in the same language. For example, the phrase he got married can be paraphrased as he tied the knot. Paraphrases were already shown to help in a number of tasks: for machine translation to translate unknown phrases by translating their paraphrases (Callison-Burch et al., 2006), for query expansion in information retrieval (Sp¨arck Jones and Tait, 1984; Jones et al., 2006), or for improving question answering (Riezler et al., 2007). A much more detailed discussion on the use of paraphrases and ways to extract them is given in (Madnani and Dorr, 2010). Here, we present the first work to use paraphrases for improving first story detection. Using paraphrases, we are able to detect that some documents previously thought to be about new events are actually paraphrases of the documents already 338 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 338–346, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics seen. Ou</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical machine translation for query expansion in answer retrieval. In Proceedings of ACL, volume 45, page 464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sp¨arck Jones</author>
<author>John Tait</author>
</authors>
<title>Automatic search term variant generation.</title>
<date>1984</date>
<journal>Journal of Documentation,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="3087" citStr="Jones and Tait, 1984" startWordPosition="502" endWordPosition="505">roblem, while keeping in mind the efficiency constraints. The problem of lexical variation plagues many IR and NLP tasks, and one way it has been addressed in the past is through the use of paraphrases. Paraphrases are alternative ways of expressing the same meaning in the same language. For example, the phrase he got married can be paraphrased as he tied the knot. Paraphrases were already shown to help in a number of tasks: for machine translation to translate unknown phrases by translating their paraphrases (Callison-Burch et al., 2006), for query expansion in information retrieval (Sp¨arck Jones and Tait, 1984; Jones et al., 2006), or for improving question answering (Riezler et al., 2007). A much more detailed discussion on the use of paraphrases and ways to extract them is given in (Madnani and Dorr, 2010). Here, we present the first work to use paraphrases for improving first story detection. Using paraphrases, we are able to detect that some documents previously thought to be about new events are actually paraphrases of the documents already 338 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 338–346, Montr´eal, </context>
</contexts>
<marker>Jones, Tait, 1984</marker>
<rawString>Karen Sp¨arck Jones and John Tait. 1984. Automatic search term variant generation. Journal of Documentation, 40(1):50–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Online generation of locality sensitive hash signatures.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Van Durme, Lall, 2010</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2010. Online generation of locality sensitive hash signatures. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>W Bruce Croft</author>
</authors>
<title>Query expansion using local and global document analysis.</title>
<date>1996</date>
<booktitle>In In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>4--11</pages>
<marker>Xu, Croft, 1996</marker>
<rawString>Jinxi Xu and W. Bruce Croft. 1996. Query expansion using local and global document analysis. In In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 4–11.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>