<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001416">
<title confidence="0.9789645">
Syntax-based Statistical Machine Translation using Tree Automata and Tree
Transducers
</title>
<author confidence="0.998722">
Daniel Emilio Beck
</author>
<affiliation confidence="0.998666">
Computer Science Department
Federal University of S˜ao Carlos
</affiliation>
<email confidence="0.987514">
daniel beck@dc.ufscar.br
</email>
<sectionHeader confidence="0.995282" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996601923076923">
In this paper I present a Master’s thesis
proposal in syntax-based Statistical Machine
Translation. I propose to build discrimina-
tive SMT models using both tree-to-string
and tree-to-tree approaches. Translation and
language models will be represented mainly
through the use of Tree Automata and Tree
Transducers. These formalisms have im-
portant representational properties that makes
them well-suited for syntax modeling. I also
present an experiment plan to evaluate these
models through the use of a parallel corpus
written in English and Brazilian Portuguese.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9992156875">
Statistical Machine Translation (SMT) has domi-
nated Machine Translation (MT) research in the
last two decades. One of its variants, Phrase-based
SMT (PB-SMT), is currently considered the state
of the art in the area. However, since the advent
of PB-SMT by Koehn et al. (2003) and Och and
Ney (2004), purely statistical MT systems have not
achieved considerable improvements. So, new re-
search directions point toward the use of linguistic
resources integrated into SMT systems.
According to Lopez (2008), there are four steps
when building an SMT system: translational equiv-
alence modeling1, parameterization, parameter esti-
mation and decoding. This Master’s thesis proposal
aims to improve SMT systems by including syntac-
tic information in the first and second steps. There-
</bodyText>
<footnote confidence="0.85072">
1For the remainder of this proposal, I will refer to this step
as simply translation model.
</footnote>
<bodyText confidence="0.999955931034483">
fore, I plan to investigate two approaches: the Tree-
to-String (TTS) and the Tree-to-Tree (TTT) models.
In the former, syntactic information is provided only
for the source language while in the latter, it is pro-
vided for both source and target languages.
There are many formal theories to represent
syntax in a language, like Context-free Gram-
mars (CFGs), Tree Substitution Grammars (TSGs),
Tree Adjoining Grammars (TAGs) and all its syn-
chronous counterparts. In this work, I represent each
sentence as a constituent tree and use Tree Automata
(TAs) and Tree Transducers (TTs) in the language
and translation models.
Although this work is mainly language indepen-
dent, proof-of-concept experiments will be executed
on the English and Brazilian Portuguese (en-ptBR)
language pair. Previous research on factored trans-
lation for this pair (using morphological informa-
tion) showed that it improved the results in terms
of BLEU (Papineni et al., 2001) and NIST (Dod-
dington, 2002) scores, as shown in Table 1 (Caseli
and Nunes, 2009). However, even factored transla-
tion models have limitations: many languages (and
Brazilian Portuguese is not an exception) have rela-
tively loose word order constraints and present long-
distance agreements that cannot be efficiently repre-
sented by those models. Such phenomena motivate
the use of more powerful models that take syntactic
information into account.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.995254333333333">
Syntax-based approaches for SMT have been pro-
posed in many ways. Some apply the TTS model:
Yamada and Knight (2001) uses explicit inser-
</bodyText>
<page confidence="0.982897">
36
</page>
<note confidence="0.508898">
Proceedings of the ACL-HLT 2011 Student Session, pages 36–40,
</note>
<table confidence="0.752006">
Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics
en-ptBR ptBR-en
BLEU NIST BLEU NIST
PB-SMT 0,3589 7,8312 0,3903 8,3008
FT 0,3713 7,9813 0,3932 8,4421
</table>
<tableCaption confidence="0.876865666666667">
Table 1: BLEU and NIST scores for PB-SMT and fac-
tored translation experiments for the en-ptBR language
pair
</tableCaption>
<bodyText confidence="0.977229192307692">
tion, reordering and translation rules, Nguyen et al.
(2008) uses synchronous CFGs rules and Liu et al.
(2006) uses TTs. Galley et al. (2006) also uses
transducer rules but extract them from parse trees in
target language instead (the string-to-tree approach
- STT). Works that apply the TTT model include
Gildea (2003) and Zhang et al. (2008). All those
works also include methods and algorithms for ef-
ficient rule extraction since it’s unfeasible to extract
all possible rules from a parsed corpus due to expo-
nential cost.
There have been research efforts to combine
syntax-based systems with phrase-based systems.
These works mainly try to incorporate non-syntatic
phrases into a syntax-based model: while Liu et al.
(2006) integrates bilingual phrase tables as separate
TTS templates, Zhang et al. (2008) uses an algo-
rithm to convert leaves in a parse tree to phrases be-
fore rule extraction.
Language models that take into account syntac-
tic aspects have also been an active research subject.
While works like Post and Gildea (2009) and Van-
deghinste (2009) focus solely on language modeling
itself, Graham and van Genabith (2010) shows an
experiment that incorporates a syntax-based model
into an PB-SMT system.
</bodyText>
<sectionHeader confidence="0.615174" genericHeader="method">
3 Tree automata and tree transducers
</sectionHeader>
<bodyText confidence="0.999371">
Tree Automata are similar to Finite-state Automata
(FSA), except they recognize trees instead of strings
(or sequences of words). Formally, FSA can only
represent Regular Languages and thus, cannot ef-
ficiently model several syntactic features, includ-
ing long-distance agreement. TA recognize the so-
called Regular Tree Languages (RTLs), which can
represent Context-free Languages (CFLs) since a set
of all syntactic trees of a CFL is an RTL (Comon
et al., 2007). However, it is important to note that
the reciprocal is not true: there are RTLs that cannot
be modeled by a CFL because those cannot capture
the inner structure of trees. Figure 1 shows such an
RTL, composed of two trees. If we extract an CFG
from this RTL it would have the recursive rule S —*
SS, which would generate an infinite set of syntac-
tic trees. In other words, there isn’t an CFG capable
to generate only the syntactic trees contained in the
RTL shown in Figure 1. This feature implies that
RTLs have more representational power than CFLs.
</bodyText>
<figure confidence="0.99731">
S S
S
a b, b
</figure>
<figureCaption confidence="0.999915">
Figure 1: An RTL that cannot be modeled by a CFL
</figureCaption>
<bodyText confidence="0.999973107142857">
As a Finite-state Transducer (FST) is an extension
of an FSA that produces strings, a Tree Transducer is
an extension of a TA that produces trees. An FST is
composed by an input RTL, an output RTL and a set
of transformation rules. Restrictions can be added to
the rules, leading to many TT variations, each with
its properties (Graehl et al., 2008). The variations
studied in this work are the xT (extended top-down,
for TTT models) and xTS (extended top-down tree-
to-string, for TTS models).
Top-down (T) transducers processes input trees
starting from its root and descending through its
nodes until it reaches the leaves, in contrast to
bottom-up transducers, which do the opposite. Fig-
ure 2 shows a T rule, where uppercase letters (NP)
represent symbols, lowercase letters (q, r, s) repre-
sent states and x1 and x2 are variables (formal def-
initions can be found in Comon et al. (2007)). De-
fault top-down transducers must have only one sym-
bol on the left-hand sides and thus cannot model
some syntactic transformations (like local reorder-
ing, for example) without relying on copy and delete
operations (Maletti et al., 2009). Extended top-
down transducers allow multiple symbols on left-
hand sides, making them more suited for syntax
modeling. This property is shown on Figure 3
(adapted from Maletti et al. (2009)). Tree-to-string
transducers simply drop the tree structure on right-
</bodyText>
<figure confidence="0.9920895">
S S S
a
</figure>
<page confidence="0.993967">
37
</page>
<bodyText confidence="0.999929">
hand sides, which makes them adequate for transla-
tion models wihtout syntactic information in one of
the languages. Figure 4 shows an example of a xTS
rule, applied for the en-ptBR pair.
</bodyText>
<equation confidence="0.660395666666667">
NP
NP
x1 x2 ��
</equation>
<figureCaption confidence="0.999031">
Figure 2: Example of a T rule
</figureCaption>
<sectionHeader confidence="0.997931" genericHeader="method">
4 SMT Model
</sectionHeader>
<bodyText confidence="0.999973346153846">
The systems will be implemented using a discrim-
inative, log-linear model (Och and Ney, 2002), us-
ing the language and translation models as feature
functions. Settings that uses more features besides
those two models will also be built. In particu-
lar, I will investigate settings that incorporate non-
syntactic phrases, using methods similar to Liu et al.
(2006) and Zhang et al. (2008)
The translation models will be weighted TTs
(Graehl et al., 2008), which add probabilities to the
rules. These probabilities will be learned by an EM
algorithm similar to the one described in Graehl et
al. (2008). Rule extraction for TTS will be similar
to the GHKM algorithm described in Galley et al.
(2004) but I also plan to investigate the approaches
used by Liu et al. (2006) and Nguyen et al. (2008).
For TTT rule extraction, I will use a method similar
to the one described in Zhang et al. (2008).
I also plan to use language models which takes
into account syntactic properties. Although most
works in syntactic language models uses tree gram-
mars like TSGs and TAGs, these can be simulated by
TAs and TTs (Shieber, 2004; Maletti, 2010). This
property can help the systems implementation be-
cause it’s possible to unite language and translation
modeling in one TT toolkit.
</bodyText>
<sectionHeader confidence="0.999288" genericHeader="evaluation">
5 Methods
</sectionHeader>
<bodyText confidence="0.9998695">
In this section, I present the experiments proposed in
my thesis and the materials required, along with the
metrics used for evaluation. This work is planned to
be done over a year.
</bodyText>
<equation confidence="0.986665333333333">
r
SINV
x1 x2 ��
s
SINV
x1 x2 ��
</equation>
<figureCaption confidence="0.993651">
Figure 3: Example of a xT rule and its corresponding T
rules
</figureCaption>
<subsectionHeader confidence="0.915385">
5.1 Materials
</subsectionHeader>
<bodyText confidence="0.9998383">
To implement and evaluate the techniques described,
a parallel corpus with syntactic annotation is re-
quired. As the focus of this thesis is the English and
Brazilian Portuguese language pair, I will use the
PesquisaFAPESP corpus2 in my experiments. This
corpus is composed of 646 scientific papers, origi-
nally written in Brazilian Portuguese and manually
translated into English, resulting in about 17,000
parallel sentences. As for syntactic annotation, I will
use the Berkeley parser (Petrov and Klein, 2007) for
</bodyText>
<footnote confidence="0.695209">
2http://revistapesquisa.fapesp.br
</footnote>
<figure confidence="0.966325971428571">
x1 x2 ��
x2
x1
x1
x2
S
SINV
q
x3
��
q
x3
q
x2
S
VP
q
x1
q
S
r
x2
S
VP
s
q
q
q q
x2 x1
q
x2
q
x1
38
was �� x1 foi x2
</figure>
<figureCaption confidence="0.997306">
Figure 4: Example of a xTS rule (for the en-ptBR lan-
guage pair)
</figureCaption>
<bodyText confidence="0.99652025">
English and the PALAVRAS parser (Bick, 2000) for
Brazilian Portuguese.
In addition to the corpora and parsers, the follow-
ing tools will be used:
</bodyText>
<listItem confidence="0.9997658">
• GIZA++3 (Och and Ney, 2000) for lexical
alignment
• Tiburon4 (May and Knight, 2006) for trans-
ducer training in both TTS and TTT systems
• Moses5 (Koehn et al., 2007) for decoding
</listItem>
<subsectionHeader confidence="0.719334">
5.2 Experiments and evaluation
</subsectionHeader>
<bodyText confidence="0.999856333333333">
Initially the corpus will be parsed using the tools de-
scribed in section 5.1 and divided into a training set
and a test set. For the TTS systems (one for each
translation direction), the training set will be lexi-
cally aligned using GIZA++ and for the TTT system,
its syntactic trees will be aligned using techniques
similar to the ones proposed by Gildea (2003) and
by Zhang et al. (2008). Both TTS and TTT systems
will be implemented using Tiburon and Moses. For
evaluation, BLEU and NIST scores on the test set
will be used. The baseline will be the score for fac-
tored translation, shown in Table 1.
</bodyText>
<sectionHeader confidence="0.99044" genericHeader="conclusions">
6 Contributions
</sectionHeader>
<bodyText confidence="0.952296">
After its conclusion, this thesis will have brought the
following contributions:
</bodyText>
<footnote confidence="0.999917">
3http://www.fjoch.com/GIZA++.html
4http://www.isi.edu/licensed-sw/tiburon
5http://www.statmt.org/moses
</footnote>
<listItem confidence="0.996104">
• Language-independent SMT models which in-
corporates syntactic information in both lan-
guage and translation models.
• Implementations of these models, using the
tools described in Section 5.
• Experimental results for the en-ptBR language
pair.
</listItem>
<bodyText confidence="0.999241">
Technical reports will be written during this thesis
progress and made publicly available. Paper submis-
sion showing intermediate and final results is also
planned.
</bodyText>
<sectionHeader confidence="0.992227" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.979351">
This research is supported by FAPESP (Project
2010/03807-4).
</bodyText>
<sectionHeader confidence="0.993572" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.976604806451613">
Eckhard Bick. 2000. The Parsing System ”Palavras”:
Automatic Grammatical Analysis of Portuguese in
a Constraint Grammar Framework. Ph.D. thesis,
Aarhus University.
Helena De Medeiros Caseli and Israel Aono Nunes.
2009. Traduc¸˜ao Autom´atica Estatistica baseada em
Frases e Fatorada : Experimentos com os idiomas Por-
tuguˆes do Brasil e Inglˆes usando o toolkit Moses.
Hubert Comon, Max Dauchet, Remi Gilleron, Florent
Jacquemard, Denis Lugiez, Christof L¨oding, Sophie
Tison, and Marc Tommasi. 2007. Tree automata tech-
niques and applications, volume 10. Available on:
http://www.grappa.univ-lille3.fr/tata.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 128–132.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. Whats in a translation rule? In
Proceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference (HLT/NAACL 2004), pages 273–
280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
ACL - ACL ’06, pages 961–968.
</reference>
<figure confidence="0.994283666666667">
q
S
V
VP
x2
x1
</figure>
<page confidence="0.992861">
39
</page>
<reference confidence="0.999580280487805">
Daniel Gildea. 2003. Loosely tree-based alignment
for machine translation. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 80–87.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training Tree Transducers. Computational Linguis-
tics, 34:391–427.
Yvette Graham and Josef van Genabith. 2010. Deep
Syntax Language Models and Statistical Machine
Translation. In SSST-4 - 4th Workshop on Syntax and
Structure in Statistical Translation at COLING 2010,
page 118.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology - NAACL ’03,
pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177–
180.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th an-
nual meeting of the ACL - ACL ’06, pages 609–616.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):1–49.
Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. 2009. The power of extended top-
down tree transducers. SIAM Journal on Computing,
39(2):410–430.
Andreas Maletti. 2010. A Tree Transducer Model for
Synchronous Tree-Adjoining Grammars. Computa-
tional Linguistics, pages 1067–1076.
Jonathan May and Kevin Knight. 2006. Tiburon : A
Weighted Tree Automata Toolkit. Grammars.
Thai Phuong Nguyen, Akira Shimazu, Tu-Bao Ho, Minh
Le Nguyen, and Vinh Van Nguyen. 2008. A tree-
to-string phrase-based model for statistical machine
translation. In Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learning -
CoNLL ’08, pages 143–150.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 440–447.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics - ACL ’02, page 295.
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, 30(4):417–449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL, pages 311–318.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL, pages 404–
411.
Matt Post and Daniel Gildea. 2009. Language modeling
with tree substitution grammars. Computing, pages 1–
8.
Stuart M Shieber. 2004. Synchronous Grammars as Tree
Transducers. Applied Sciences, pages 88–95.
Vincent Vandeghinste. 2009. Tree-based target language
modeling. In Proceedings of EAMT, pages 152–159.
Kenji Yamada and Kevin Knight. 2001. A Syntax-based
Statistical Translation Model. In ACL ’01 Proceedings
of the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 523–530.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL-08: HLT, pages 559–567.
</reference>
<page confidence="0.998634">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.314971">
<title confidence="0.9988985">Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</title>
<author confidence="0.999964">Daniel Emilio</author>
<affiliation confidence="0.9965655">Computer Science University of</affiliation>
<email confidence="0.343234">danielbeck@dc.ufscar.br</email>
<abstract confidence="0.994440214285714">In this paper I present a Master’s thesis proposal in syntax-based Statistical Machine Translation. I propose to build discrimina- SMT models using both Translation and language models will be represented mainly through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. I also present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eckhard Bick</author>
</authors>
<title>The Parsing System ”Palavras”: Automatic Grammatical Analysis of Portuguese in a Constraint Grammar Framework.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Aarhus University.</institution>
<contexts>
<context position="9846" citStr="Bick, 2000" startWordPosition="1626" endWordPosition="1627">ian Portuguese language pair, I will use the PesquisaFAPESP corpus2 in my experiments. This corpus is composed of 646 scientific papers, originally written in Brazilian Portuguese and manually translated into English, resulting in about 17,000 parallel sentences. As for syntactic annotation, I will use the Berkeley parser (Petrov and Klein, 2007) for 2http://revistapesquisa.fapesp.br x1 x2 �� x2 x1 x1 x2 S SINV q x3 �� q x3 q x2 S VP q x1 q S r x2 S VP s q q q q x2 x1 q x2 q x1 38 was �� x1 foi x2 Figure 4: Example of a xTS rule (for the en-ptBR language pair) English and the PALAVRAS parser (Bick, 2000) for Brazilian Portuguese. In addition to the corpora and parsers, the following tools will be used: • GIZA++3 (Och and Ney, 2000) for lexical alignment • Tiburon4 (May and Knight, 2006) for transducer training in both TTS and TTT systems • Moses5 (Koehn et al., 2007) for decoding 5.2 Experiments and evaluation Initially the corpus will be parsed using the tools described in section 5.1 and divided into a training set and a test set. For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will</context>
</contexts>
<marker>Bick, 2000</marker>
<rawString>Eckhard Bick. 2000. The Parsing System ”Palavras”: Automatic Grammatical Analysis of Portuguese in a Constraint Grammar Framework. Ph.D. thesis, Aarhus University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helena De Medeiros Caseli</author>
<author>Israel Aono Nunes</author>
</authors>
<title>Traduc¸˜ao Autom´atica Estatistica baseada em Frases e Fatorada : Experimentos com os idiomas Portuguˆes do Brasil e Inglˆes usando o toolkit Moses.</title>
<date>2009</date>
<contexts>
<context position="2671" citStr="Caseli and Nunes, 2009" startWordPosition="401" endWordPosition="404">ng Grammars (TAGs) and all its synchronous counterparts. In this work, I represent each sentence as a constituent tree and use Tree Automata (TAs) and Tree Transducers (TTs) in the language and translation models. Although this work is mainly language independent, proof-of-concept experiments will be executed on the English and Brazilian Portuguese (en-ptBR) language pair. Previous research on factored translation for this pair (using morphological information) showed that it improved the results in terms of BLEU (Papineni et al., 2001) and NIST (Doddington, 2002) scores, as shown in Table 1 (Caseli and Nunes, 2009). However, even factored translation models have limitations: many languages (and Brazilian Portuguese is not an exception) have relatively loose word order constraints and present longdistance agreements that cannot be efficiently represented by those models. Such phenomena motivate the use of more powerful models that take syntactic information into account. 2 Related work Syntax-based approaches for SMT have been proposed in many ways. Some apply the TTS model: Yamada and Knight (2001) uses explicit inser36 Proceedings of the ACL-HLT 2011 Student Session, pages 36–40, Portland, OR, USA 19-2</context>
</contexts>
<marker>Caseli, Nunes, 2009</marker>
<rawString>Helena De Medeiros Caseli and Israel Aono Nunes. 2009. Traduc¸˜ao Autom´atica Estatistica baseada em Frases e Fatorada : Experimentos com os idiomas Portuguˆes do Brasil e Inglˆes usando o toolkit Moses.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hubert Comon</author>
<author>Max Dauchet</author>
</authors>
<title>Remi Gilleron, Florent Jacquemard, Denis Lugiez, Christof L¨oding, Sophie Tison, and Marc Tommasi.</title>
<date>2007</date>
<pages>3</pages>
<marker>Comon, Dauchet, 2007</marker>
<rawString>Hubert Comon, Max Dauchet, Remi Gilleron, Florent Jacquemard, Denis Lugiez, Christof L¨oding, Sophie Tison, and Marc Tommasi. 2007. Tree automata techniques and applications, volume 10. Available on: http://www.grappa.univ-lille3.fr/tata.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<pages>128--132</pages>
<contexts>
<context position="2618" citStr="Doddington, 2002" startWordPosition="392" endWordPosition="394">Tree Substitution Grammars (TSGs), Tree Adjoining Grammars (TAGs) and all its synchronous counterparts. In this work, I represent each sentence as a constituent tree and use Tree Automata (TAs) and Tree Transducers (TTs) in the language and translation models. Although this work is mainly language independent, proof-of-concept experiments will be executed on the English and Brazilian Portuguese (en-ptBR) language pair. Previous research on factored translation for this pair (using morphological information) showed that it improved the results in terms of BLEU (Papineni et al., 2001) and NIST (Doddington, 2002) scores, as shown in Table 1 (Caseli and Nunes, 2009). However, even factored translation models have limitations: many languages (and Brazilian Portuguese is not an exception) have relatively loose word order constraints and present longdistance agreements that cannot be efficiently represented by those models. Such phenomena motivate the use of more powerful models that take syntactic information into account. 2 Related work Syntax-based approaches for SMT have been proposed in many ways. Some apply the TTS model: Yamada and Knight (2001) uses explicit inser36 Proceedings of the ACL-HLT 2011</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, pages 128–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Whats in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL 2004),</booktitle>
<pages>273--280</pages>
<contexts>
<context position="8203" citStr="Galley et al. (2004)" startWordPosition="1324" endWordPosition="1327">Och and Ney, 2002), using the language and translation models as feature functions. Settings that uses more features besides those two models will also be built. In particular, I will investigate settings that incorporate nonsyntactic phrases, using methods similar to Liu et al. (2006) and Zhang et al. (2008) The translation models will be weighted TTs (Graehl et al., 2008), which add probabilities to the rules. These probabilities will be learned by an EM algorithm similar to the one described in Graehl et al. (2008). Rule extraction for TTS will be similar to the GHKM algorithm described in Galley et al. (2004) but I also plan to investigate the approaches used by Liu et al. (2006) and Nguyen et al. (2008). For TTT rule extraction, I will use a method similar to the one described in Zhang et al. (2008). I also plan to use language models which takes into account syntactic properties. Although most works in syntactic language models uses tree grammars like TSGs and TAGs, these can be simulated by TAs and TTs (Shieber, 2004; Maletti, 2010). This property can help the systems implementation because it’s possible to unite language and translation modeling in one TT toolkit. 5 Methods In this section, I </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. Whats in a translation rule? In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL 2004), pages 273– 280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL - ACL ’06,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="3684" citStr="Galley et al. (2006)" startWordPosition="559" endWordPosition="562">proaches for SMT have been proposed in many ways. Some apply the TTS model: Yamada and Knight (2001) uses explicit inser36 Proceedings of the ACL-HLT 2011 Student Session, pages 36–40, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics en-ptBR ptBR-en BLEU NIST BLEU NIST PB-SMT 0,3589 7,8312 0,3903 8,3008 FT 0,3713 7,9813 0,3932 8,4421 Table 1: BLEU and NIST scores for PB-SMT and factored translation experiments for the en-ptBR language pair tion, reordering and translation rules, Nguyen et al. (2008) uses synchronous CFGs rules and Liu et al. (2006) uses TTs. Galley et al. (2006) also uses transducer rules but extract them from parse trees in target language instead (the string-to-tree approach - STT). Works that apply the TTT model include Gildea (2003) and Zhang et al. (2008). All those works also include methods and algorithms for efficient rule extraction since it’s unfeasible to extract all possible rules from a parsed corpus due to exponential cost. There have been research efforts to combine syntax-based systems with phrase-based systems. These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al. (2006) integrates bil</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL - ACL ’06, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Loosely tree-based alignment for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="3862" citStr="Gildea (2003)" startWordPosition="589" endWordPosition="590">Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics en-ptBR ptBR-en BLEU NIST BLEU NIST PB-SMT 0,3589 7,8312 0,3903 8,3008 FT 0,3713 7,9813 0,3932 8,4421 Table 1: BLEU and NIST scores for PB-SMT and factored translation experiments for the en-ptBR language pair tion, reordering and translation rules, Nguyen et al. (2008) uses synchronous CFGs rules and Liu et al. (2006) uses TTs. Galley et al. (2006) also uses transducer rules but extract them from parse trees in target language instead (the string-to-tree approach - STT). Works that apply the TTT model include Gildea (2003) and Zhang et al. (2008). All those works also include methods and algorithms for efficient rule extraction since it’s unfeasible to extract all possible rules from a parsed corpus due to exponential cost. There have been research efforts to combine syntax-based systems with phrase-based systems. These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al. (2006) integrates bilingual phrase tables as separate TTS templates, Zhang et al. (2008) uses an algorithm to convert leaves in a parse tree to phrases before rule extraction. Language models that ta</context>
<context position="10520" citStr="Gildea (2003)" startWordPosition="1744" endWordPosition="1745">sers, the following tools will be used: • GIZA++3 (Och and Ney, 2000) for lexical alignment • Tiburon4 (May and Knight, 2006) for transducer training in both TTS and TTT systems • Moses5 (Koehn et al., 2007) for decoding 5.2 Experiments and evaluation Initially the corpus will be parsed using the tools described in section 5.1 and divided into a training set and a test set. For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al. (2008). Both TTS and TTT systems will be implemented using Tiburon and Moses. For evaluation, BLEU and NIST scores on the test set will be used. The baseline will be the score for factored translation, shown in Table 1. 6 Contributions After its conclusion, this thesis will have brought the following contributions: 3http://www.fjoch.com/GIZA++.html 4http://www.isi.edu/licensed-sw/tiburon 5http://www.statmt.org/moses • Language-independent SMT models which incorporates syntactic information in both language and translation models. • Implementations of these models, using th</context>
</contexts>
<marker>Gildea, 2003</marker>
<rawString>Daniel Gildea. 2003. Loosely tree-based alignment for machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Jonathan May</author>
</authors>
<title>Training Tree Transducers. Computational Linguistics,</title>
<date>2008</date>
<pages>34--391</pages>
<contexts>
<context position="6220" citStr="Graehl et al., 2008" startWordPosition="988" endWordPosition="991">tactic trees. In other words, there isn’t an CFG capable to generate only the syntactic trees contained in the RTL shown in Figure 1. This feature implies that RTLs have more representational power than CFLs. S S S a b, b Figure 1: An RTL that cannot be modeled by a CFL As a Finite-state Transducer (FST) is an extension of an FSA that produces strings, a Tree Transducer is an extension of a TA that produces trees. An FST is composed by an input RTL, an output RTL and a set of transformation rules. Restrictions can be added to the rules, leading to many TT variations, each with its properties (Graehl et al., 2008). The variations studied in this work are the xT (extended top-down, for TTT models) and xTS (extended top-down treeto-string, for TTS models). Top-down (T) transducers processes input trees starting from its root and descending through its nodes until it reaches the leaves, in contrast to bottom-up transducers, which do the opposite. Figure 2 shows a T rule, where uppercase letters (NP) represent symbols, lowercase letters (q, r, s) represent states and x1 and x2 are variables (formal definitions can be found in Comon et al. (2007)). Default top-down transducers must have only one symbol on t</context>
<context position="7959" citStr="Graehl et al., 2008" startWordPosition="1282" endWordPosition="1285">ctic information in one of the languages. Figure 4 shows an example of a xTS rule, applied for the en-ptBR pair. NP NP x1 x2 �� Figure 2: Example of a T rule 4 SMT Model The systems will be implemented using a discriminative, log-linear model (Och and Ney, 2002), using the language and translation models as feature functions. Settings that uses more features besides those two models will also be built. In particular, I will investigate settings that incorporate nonsyntactic phrases, using methods similar to Liu et al. (2006) and Zhang et al. (2008) The translation models will be weighted TTs (Graehl et al., 2008), which add probabilities to the rules. These probabilities will be learned by an EM algorithm similar to the one described in Graehl et al. (2008). Rule extraction for TTS will be similar to the GHKM algorithm described in Galley et al. (2004) but I also plan to investigate the approaches used by Liu et al. (2006) and Nguyen et al. (2008). For TTT rule extraction, I will use a method similar to the one described in Zhang et al. (2008). I also plan to use language models which takes into account syntactic properties. Although most works in syntactic language models uses tree grammars like TSGs</context>
</contexts>
<marker>Graehl, Knight, May, 2008</marker>
<rawString>Jonathan Graehl, Kevin Knight, and Jonathan May. 2008. Training Tree Transducers. Computational Linguistics, 34:391–427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvette Graham</author>
<author>Josef van Genabith</author>
</authors>
<title>Deep Syntax Language Models and Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In SSST-4 - 4th Workshop on Syntax and Structure in Statistical Translation at COLING 2010,</booktitle>
<pages>118</pages>
<marker>Graham, van Genabith, 2010</marker>
<rawString>Yvette Graham and Josef van Genabith. 2010. Deep Syntax Language Models and Statistical Machine Translation. In SSST-4 - 4th Workshop on Syntax and Structure in Statistical Translation at COLING 2010, page 118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - NAACL ’03,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1055" citStr="Koehn et al. (2003)" startWordPosition="149" endWordPosition="152">l be represented mainly through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. I also present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese. 1 Introduction Statistical Machine Translation (SMT) has dominated Machine Translation (MT) research in the last two decades. One of its variants, Phrase-based SMT (PB-SMT), is currently considered the state of the art in the area. However, since the advent of PB-SMT by Koehn et al. (2003) and Och and Ney (2004), purely statistical MT systems have not achieved considerable improvements. So, new research directions point toward the use of linguistic resources integrated into SMT systems. According to Lopez (2008), there are four steps when building an SMT system: translational equivalence modeling1, parameterization, parameter estimation and decoding. This Master’s thesis proposal aims to improve SMT systems by including syntactic information in the first and second steps. There1For the remainder of this proposal, I will refer to this step as simply translation model. fore, I pl</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - NAACL ’03, pages 48–54.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="10114" citStr="Koehn et al., 2007" startWordPosition="1672" endWordPosition="1675">es. As for syntactic annotation, I will use the Berkeley parser (Petrov and Klein, 2007) for 2http://revistapesquisa.fapesp.br x1 x2 �� x2 x1 x1 x2 S SINV q x3 �� q x3 q x2 S VP q x1 q S r x2 S VP s q q q q x2 x1 q x2 q x1 38 was �� x1 foi x2 Figure 4: Example of a xTS rule (for the en-ptBR language pair) English and the PALAVRAS parser (Bick, 2000) for Brazilian Portuguese. In addition to the corpora and parsers, the following tools will be used: • GIZA++3 (Och and Ney, 2000) for lexical alignment • Tiburon4 (May and Knight, 2006) for transducer training in both TTS and TTT systems • Moses5 (Koehn et al., 2007) for decoding 5.2 Experiments and evaluation Initially the corpus will be parsed using the tools described in section 5.1 and divided into a training set and a test set. For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al. (2008). Both TTS and TTT systems will be implemented using Tiburon and Moses. For evaluation, BLEU and NIST scores on the test set will be used. The baseline will be the sco</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177– 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL - ACL ’06,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="3653" citStr="Liu et al. (2006)" startWordPosition="553" endWordPosition="556">Related work Syntax-based approaches for SMT have been proposed in many ways. Some apply the TTS model: Yamada and Knight (2001) uses explicit inser36 Proceedings of the ACL-HLT 2011 Student Session, pages 36–40, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics en-ptBR ptBR-en BLEU NIST BLEU NIST PB-SMT 0,3589 7,8312 0,3903 8,3008 FT 0,3713 7,9813 0,3932 8,4421 Table 1: BLEU and NIST scores for PB-SMT and factored translation experiments for the en-ptBR language pair tion, reordering and translation rules, Nguyen et al. (2008) uses synchronous CFGs rules and Liu et al. (2006) uses TTs. Galley et al. (2006) also uses transducer rules but extract them from parse trees in target language instead (the string-to-tree approach - STT). Works that apply the TTT model include Gildea (2003) and Zhang et al. (2008). All those works also include methods and algorithms for efficient rule extraction since it’s unfeasible to extract all possible rules from a parsed corpus due to exponential cost. There have been research efforts to combine syntax-based systems with phrase-based systems. These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while L</context>
<context position="7869" citStr="Liu et al. (2006)" startWordPosition="1266" endWordPosition="1269">htS S S a 37 hand sides, which makes them adequate for translation models wihtout syntactic information in one of the languages. Figure 4 shows an example of a xTS rule, applied for the en-ptBR pair. NP NP x1 x2 �� Figure 2: Example of a T rule 4 SMT Model The systems will be implemented using a discriminative, log-linear model (Och and Ney, 2002), using the language and translation models as feature functions. Settings that uses more features besides those two models will also be built. In particular, I will investigate settings that incorporate nonsyntactic phrases, using methods similar to Liu et al. (2006) and Zhang et al. (2008) The translation models will be weighted TTs (Graehl et al., 2008), which add probabilities to the rules. These probabilities will be learned by an EM algorithm similar to the one described in Graehl et al. (2008). Rule extraction for TTS will be similar to the GHKM algorithm described in Galley et al. (2004) but I also plan to investigate the approaches used by Liu et al. (2006) and Nguyen et al. (2008). For TTT rule extraction, I will use a method similar to the one described in Zhang et al. (2008). I also plan to use language models which takes into account syntactic</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL - ACL ’06, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Statistical machine translation.</title>
<date>2008</date>
<journal>ACM Computing Surveys,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="1282" citStr="Lopez (2008)" startWordPosition="185" endWordPosition="186">these models through the use of a parallel corpus written in English and Brazilian Portuguese. 1 Introduction Statistical Machine Translation (SMT) has dominated Machine Translation (MT) research in the last two decades. One of its variants, Phrase-based SMT (PB-SMT), is currently considered the state of the art in the area. However, since the advent of PB-SMT by Koehn et al. (2003) and Och and Ney (2004), purely statistical MT systems have not achieved considerable improvements. So, new research directions point toward the use of linguistic resources integrated into SMT systems. According to Lopez (2008), there are four steps when building an SMT system: translational equivalence modeling1, parameterization, parameter estimation and decoding. This Master’s thesis proposal aims to improve SMT systems by including syntactic information in the first and second steps. There1For the remainder of this proposal, I will refer to this step as simply translation model. fore, I plan to investigate two approaches: the Treeto-String (TTS) and the Tree-to-Tree (TTT) models. In the former, syntactic information is provided only for the source language while in the latter, it is provided for both source and </context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Statistical machine translation. ACM Computing Surveys, 40(3):1–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
<author>Jonathan Graehl</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
</authors>
<title>The power of extended topdown tree transducers.</title>
<date>2009</date>
<journal>SIAM Journal on Computing,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="6997" citStr="Maletti et al., 2009" startWordPosition="1116" endWordPosition="1119"> transducers processes input trees starting from its root and descending through its nodes until it reaches the leaves, in contrast to bottom-up transducers, which do the opposite. Figure 2 shows a T rule, where uppercase letters (NP) represent symbols, lowercase letters (q, r, s) represent states and x1 and x2 are variables (formal definitions can be found in Comon et al. (2007)). Default top-down transducers must have only one symbol on the left-hand sides and thus cannot model some syntactic transformations (like local reordering, for example) without relying on copy and delete operations (Maletti et al., 2009). Extended topdown transducers allow multiple symbols on lefthand sides, making them more suited for syntax modeling. This property is shown on Figure 3 (adapted from Maletti et al. (2009)). Tree-to-string transducers simply drop the tree structure on rightS S S a 37 hand sides, which makes them adequate for translation models wihtout syntactic information in one of the languages. Figure 4 shows an example of a xTS rule, applied for the en-ptBR pair. NP NP x1 x2 �� Figure 2: Example of a T rule 4 SMT Model The systems will be implemented using a discriminative, log-linear model (Och and Ney, 2</context>
</contexts>
<marker>Maletti, Graehl, Hopkins, Knight, 2009</marker>
<rawString>Andreas Maletti, Jonathan Graehl, Mark Hopkins, and Kevin Knight. 2009. The power of extended topdown tree transducers. SIAM Journal on Computing, 39(2):410–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>A Tree Transducer Model for Synchronous Tree-Adjoining Grammars. Computational Linguistics,</title>
<date>2010</date>
<pages>1067--1076</pages>
<contexts>
<context position="8638" citStr="Maletti, 2010" startWordPosition="1404" endWordPosition="1405">be learned by an EM algorithm similar to the one described in Graehl et al. (2008). Rule extraction for TTS will be similar to the GHKM algorithm described in Galley et al. (2004) but I also plan to investigate the approaches used by Liu et al. (2006) and Nguyen et al. (2008). For TTT rule extraction, I will use a method similar to the one described in Zhang et al. (2008). I also plan to use language models which takes into account syntactic properties. Although most works in syntactic language models uses tree grammars like TSGs and TAGs, these can be simulated by TAs and TTs (Shieber, 2004; Maletti, 2010). This property can help the systems implementation because it’s possible to unite language and translation modeling in one TT toolkit. 5 Methods In this section, I present the experiments proposed in my thesis and the materials required, along with the metrics used for evaluation. This work is planned to be done over a year. r SINV x1 x2 �� s SINV x1 x2 �� Figure 3: Example of a xT rule and its corresponding T rules 5.1 Materials To implement and evaluate the techniques described, a parallel corpus with syntactic annotation is required. As the focus of this thesis is the English and Brazilian</context>
</contexts>
<marker>Maletti, 2010</marker>
<rawString>Andreas Maletti. 2010. A Tree Transducer Model for Synchronous Tree-Adjoining Grammars. Computational Linguistics, pages 1067–1076.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>Tiburon : A Weighted Tree Automata Toolkit.</title>
<date>2006</date>
<publisher>Grammars.</publisher>
<contexts>
<context position="10032" citStr="May and Knight, 2006" startWordPosition="1656" endWordPosition="1659">ese and manually translated into English, resulting in about 17,000 parallel sentences. As for syntactic annotation, I will use the Berkeley parser (Petrov and Klein, 2007) for 2http://revistapesquisa.fapesp.br x1 x2 �� x2 x1 x1 x2 S SINV q x3 �� q x3 q x2 S VP q x1 q S r x2 S VP s q q q q x2 x1 q x2 q x1 38 was �� x1 foi x2 Figure 4: Example of a xTS rule (for the en-ptBR language pair) English and the PALAVRAS parser (Bick, 2000) for Brazilian Portuguese. In addition to the corpora and parsers, the following tools will be used: • GIZA++3 (Och and Ney, 2000) for lexical alignment • Tiburon4 (May and Knight, 2006) for transducer training in both TTS and TTT systems • Moses5 (Koehn et al., 2007) for decoding 5.2 Experiments and evaluation Initially the corpus will be parsed using the tools described in section 5.1 and divided into a training set and a test set. For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al. (2008). Both TTS and TTT systems will be implemented using Tiburon and Moses. For evaluatio</context>
</contexts>
<marker>May, Knight, 2006</marker>
<rawString>Jonathan May and Kevin Knight. 2006. Tiburon : A Weighted Tree Automata Toolkit. Grammars.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thai Phuong Nguyen</author>
<author>Akira Shimazu</author>
<author>Tu-Bao Ho</author>
<author>Minh Le Nguyen</author>
<author>Vinh Van Nguyen</author>
</authors>
<title>A treeto-string phrase-based model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning -CoNLL ’08,</booktitle>
<pages>143--150</pages>
<marker>Nguyen, Shimazu, Ho, Le Nguyen, Van Nguyen, 2008</marker>
<rawString>Thai Phuong Nguyen, Akira Shimazu, Tu-Bao Ho, Minh Le Nguyen, and Vinh Van Nguyen. 2008. A treeto-string phrase-based model for statistical machine translation. In Proceedings of the Twelfth Conference on Computational Natural Language Learning -CoNLL ’08, pages 143–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="9976" citStr="Och and Ney, 2000" startWordPosition="1647" endWordPosition="1650">tific papers, originally written in Brazilian Portuguese and manually translated into English, resulting in about 17,000 parallel sentences. As for syntactic annotation, I will use the Berkeley parser (Petrov and Klein, 2007) for 2http://revistapesquisa.fapesp.br x1 x2 �� x2 x1 x1 x2 S SINV q x3 �� q x3 q x2 S VP q x1 q S r x2 S VP s q q q q x2 x1 q x2 q x1 38 was �� x1 foi x2 Figure 4: Example of a xTS rule (for the en-ptBR language pair) English and the PALAVRAS parser (Bick, 2000) for Brazilian Portuguese. In addition to the corpora and parsers, the following tools will be used: • GIZA++3 (Och and Ney, 2000) for lexical alignment • Tiburon4 (May and Knight, 2006) for transducer training in both TTS and TTT systems • Moses5 (Koehn et al., 2007) for decoding 5.2 Experiments and evaluation Initially the corpus will be parsed using the tools described in section 5.1 and divided into a training set and a test set. For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al. (2008). Both TTS and TTT systems wi</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL ’02,</booktitle>
<pages>295</pages>
<contexts>
<context position="7601" citStr="Och and Ney, 2002" startWordPosition="1223" endWordPosition="1226"> et al., 2009). Extended topdown transducers allow multiple symbols on lefthand sides, making them more suited for syntax modeling. This property is shown on Figure 3 (adapted from Maletti et al. (2009)). Tree-to-string transducers simply drop the tree structure on rightS S S a 37 hand sides, which makes them adequate for translation models wihtout syntactic information in one of the languages. Figure 4 shows an example of a xTS rule, applied for the en-ptBR pair. NP NP x1 x2 �� Figure 2: Example of a T rule 4 SMT Model The systems will be implemented using a discriminative, log-linear model (Och and Ney, 2002), using the language and translation models as feature functions. Settings that uses more features besides those two models will also be built. In particular, I will investigate settings that incorporate nonsyntactic phrases, using methods similar to Liu et al. (2006) and Zhang et al. (2008) The translation models will be weighted TTs (Graehl et al., 2008), which add probabilities to the rules. These probabilities will be learned by an EM algorithm similar to the one described in Graehl et al. (2008). Rule extraction for TTS will be similar to the GHKM algorithm described in Galley et al. (200</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL ’02, page 295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1078" citStr="Och and Ney (2004)" startWordPosition="154" endWordPosition="157">through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. I also present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese. 1 Introduction Statistical Machine Translation (SMT) has dominated Machine Translation (MT) research in the last two decades. One of its variants, Phrase-based SMT (PB-SMT), is currently considered the state of the art in the area. However, since the advent of PB-SMT by Koehn et al. (2003) and Och and Ney (2004), purely statistical MT systems have not achieved considerable improvements. So, new research directions point toward the use of linguistic resources integrated into SMT systems. According to Lopez (2008), there are four steps when building an SMT system: translational equivalence modeling1, parameterization, parameter estimation and decoding. This Master’s thesis proposal aims to improve SMT systems by including syntactic information in the first and second steps. There1For the remainder of this proposal, I will refer to this step as simply translation model. fore, I plan to investigate two a</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="2590" citStr="Papineni et al., 2001" startWordPosition="386" endWordPosition="389">ke Context-free Grammars (CFGs), Tree Substitution Grammars (TSGs), Tree Adjoining Grammars (TAGs) and all its synchronous counterparts. In this work, I represent each sentence as a constituent tree and use Tree Automata (TAs) and Tree Transducers (TTs) in the language and translation models. Although this work is mainly language independent, proof-of-concept experiments will be executed on the English and Brazilian Portuguese (en-ptBR) language pair. Previous research on factored translation for this pair (using morphological information) showed that it improved the results in terms of BLEU (Papineni et al., 2001) and NIST (Doddington, 2002) scores, as shown in Table 1 (Caseli and Nunes, 2009). However, even factored translation models have limitations: many languages (and Brazilian Portuguese is not an exception) have relatively loose word order constraints and present longdistance agreements that cannot be efficiently represented by those models. Such phenomena motivate the use of more powerful models that take syntactic information into account. 2 Related work Syntax-based approaches for SMT have been proposed in many ways. Some apply the TTS model: Yamada and Knight (2001) uses explicit inser36 Pro</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. In ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In</title>
<date>2007</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="9583" citStr="Petrov and Klein, 2007" startWordPosition="1559" endWordPosition="1562">year. r SINV x1 x2 �� s SINV x1 x2 �� Figure 3: Example of a xT rule and its corresponding T rules 5.1 Materials To implement and evaluate the techniques described, a parallel corpus with syntactic annotation is required. As the focus of this thesis is the English and Brazilian Portuguese language pair, I will use the PesquisaFAPESP corpus2 in my experiments. This corpus is composed of 646 scientific papers, originally written in Brazilian Portuguese and manually translated into English, resulting in about 17,000 parallel sentences. As for syntactic annotation, I will use the Berkeley parser (Petrov and Klein, 2007) for 2http://revistapesquisa.fapesp.br x1 x2 �� x2 x1 x1 x2 S SINV q x3 �� q x3 q x2 S VP q x1 q S r x2 S VP s q q q q x2 x1 q x2 q x1 38 was �� x1 foi x2 Figure 4: Example of a xTS rule (for the en-ptBR language pair) English and the PALAVRAS parser (Bick, 2000) for Brazilian Portuguese. In addition to the corpora and parsers, the following tools will be used: • GIZA++3 (Och and Ney, 2000) for lexical alignment • Tiburon4 (May and Knight, 2006) for transducer training in both TTS and TTT systems • Moses5 (Koehn et al., 2007) for decoding 5.2 Experiments and evaluation Initially the corpus wil</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL, pages 404– 411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Language modeling with tree substitution grammars.</title>
<date>2009</date>
<journal>Computing, pages</journal>
<volume>1</volume>
<contexts>
<context position="4578" citStr="Post and Gildea (2009)" startWordPosition="703" endWordPosition="706">le extraction since it’s unfeasible to extract all possible rules from a parsed corpus due to exponential cost. There have been research efforts to combine syntax-based systems with phrase-based systems. These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al. (2006) integrates bilingual phrase tables as separate TTS templates, Zhang et al. (2008) uses an algorithm to convert leaves in a parse tree to phrases before rule extraction. Language models that take into account syntactic aspects have also been an active research subject. While works like Post and Gildea (2009) and Vandeghinste (2009) focus solely on language modeling itself, Graham and van Genabith (2010) shows an experiment that incorporates a syntax-based model into an PB-SMT system. 3 Tree automata and tree transducers Tree Automata are similar to Finite-state Automata (FSA), except they recognize trees instead of strings (or sequences of words). Formally, FSA can only represent Regular Languages and thus, cannot efficiently model several syntactic features, including long-distance agreement. TA recognize the socalled Regular Tree Languages (RTLs), which can represent Context-free Languages (CFL</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Language modeling with tree substitution grammars. Computing, pages 1– 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Synchronous Grammars as Tree Transducers. Applied Sciences,</title>
<date>2004</date>
<pages>88--95</pages>
<contexts>
<context position="8622" citStr="Shieber, 2004" startWordPosition="1402" endWordPosition="1403">abilities will be learned by an EM algorithm similar to the one described in Graehl et al. (2008). Rule extraction for TTS will be similar to the GHKM algorithm described in Galley et al. (2004) but I also plan to investigate the approaches used by Liu et al. (2006) and Nguyen et al. (2008). For TTT rule extraction, I will use a method similar to the one described in Zhang et al. (2008). I also plan to use language models which takes into account syntactic properties. Although most works in syntactic language models uses tree grammars like TSGs and TAGs, these can be simulated by TAs and TTs (Shieber, 2004; Maletti, 2010). This property can help the systems implementation because it’s possible to unite language and translation modeling in one TT toolkit. 5 Methods In this section, I present the experiments proposed in my thesis and the materials required, along with the metrics used for evaluation. This work is planned to be done over a year. r SINV x1 x2 �� s SINV x1 x2 �� Figure 3: Example of a xT rule and its corresponding T rules 5.1 Materials To implement and evaluate the techniques described, a parallel corpus with syntactic annotation is required. As the focus of this thesis is the Engli</context>
</contexts>
<marker>Shieber, 2004</marker>
<rawString>Stuart M Shieber. 2004. Synchronous Grammars as Tree Transducers. Applied Sciences, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Vandeghinste</author>
</authors>
<title>Tree-based target language modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of EAMT,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="4602" citStr="Vandeghinste (2009)" startWordPosition="708" endWordPosition="710">feasible to extract all possible rules from a parsed corpus due to exponential cost. There have been research efforts to combine syntax-based systems with phrase-based systems. These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al. (2006) integrates bilingual phrase tables as separate TTS templates, Zhang et al. (2008) uses an algorithm to convert leaves in a parse tree to phrases before rule extraction. Language models that take into account syntactic aspects have also been an active research subject. While works like Post and Gildea (2009) and Vandeghinste (2009) focus solely on language modeling itself, Graham and van Genabith (2010) shows an experiment that incorporates a syntax-based model into an PB-SMT system. 3 Tree automata and tree transducers Tree Automata are similar to Finite-state Automata (FSA), except they recognize trees instead of strings (or sequences of words). Formally, FSA can only represent Regular Languages and thus, cannot efficiently model several syntactic features, including long-distance agreement. TA recognize the socalled Regular Tree Languages (RTLs), which can represent Context-free Languages (CFLs) since a set of all sy</context>
</contexts>
<marker>Vandeghinste, 2009</marker>
<rawString>Vincent Vandeghinste. 2009. Tree-based target language modeling. In Proceedings of EAMT, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A Syntax-based Statistical Translation Model.</title>
<date>2001</date>
<booktitle>In ACL ’01 Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="3164" citStr="Yamada and Knight (2001)" startWordPosition="477" endWordPosition="480">e results in terms of BLEU (Papineni et al., 2001) and NIST (Doddington, 2002) scores, as shown in Table 1 (Caseli and Nunes, 2009). However, even factored translation models have limitations: many languages (and Brazilian Portuguese is not an exception) have relatively loose word order constraints and present longdistance agreements that cannot be efficiently represented by those models. Such phenomena motivate the use of more powerful models that take syntactic information into account. 2 Related work Syntax-based approaches for SMT have been proposed in many ways. Some apply the TTS model: Yamada and Knight (2001) uses explicit inser36 Proceedings of the ACL-HLT 2011 Student Session, pages 36–40, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics en-ptBR ptBR-en BLEU NIST BLEU NIST PB-SMT 0,3589 7,8312 0,3903 8,3008 FT 0,3713 7,9813 0,3932 8,4421 Table 1: BLEU and NIST scores for PB-SMT and factored translation experiments for the en-ptBR language pair tion, reordering and translation rules, Nguyen et al. (2008) uses synchronous CFGs rules and Liu et al. (2006) uses TTs. Galley et al. (2006) also uses transducer rules but extract them from parse trees in target language</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A Syntax-based Statistical Translation Model. In ACL ’01 Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proc. ACL-08: HLT,</booktitle>
<pages>559--567</pages>
<contexts>
<context position="3886" citStr="Zhang et al. (2008)" startWordPosition="592" endWordPosition="595">19-24 June 2011. c�2011 Association for Computational Linguistics en-ptBR ptBR-en BLEU NIST BLEU NIST PB-SMT 0,3589 7,8312 0,3903 8,3008 FT 0,3713 7,9813 0,3932 8,4421 Table 1: BLEU and NIST scores for PB-SMT and factored translation experiments for the en-ptBR language pair tion, reordering and translation rules, Nguyen et al. (2008) uses synchronous CFGs rules and Liu et al. (2006) uses TTs. Galley et al. (2006) also uses transducer rules but extract them from parse trees in target language instead (the string-to-tree approach - STT). Works that apply the TTT model include Gildea (2003) and Zhang et al. (2008). All those works also include methods and algorithms for efficient rule extraction since it’s unfeasible to extract all possible rules from a parsed corpus due to exponential cost. There have been research efforts to combine syntax-based systems with phrase-based systems. These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al. (2006) integrates bilingual phrase tables as separate TTS templates, Zhang et al. (2008) uses an algorithm to convert leaves in a parse tree to phrases before rule extraction. Language models that take into account syntacti</context>
<context position="7893" citStr="Zhang et al. (2008)" startWordPosition="1271" endWordPosition="1274">s, which makes them adequate for translation models wihtout syntactic information in one of the languages. Figure 4 shows an example of a xTS rule, applied for the en-ptBR pair. NP NP x1 x2 �� Figure 2: Example of a T rule 4 SMT Model The systems will be implemented using a discriminative, log-linear model (Och and Ney, 2002), using the language and translation models as feature functions. Settings that uses more features besides those two models will also be built. In particular, I will investigate settings that incorporate nonsyntactic phrases, using methods similar to Liu et al. (2006) and Zhang et al. (2008) The translation models will be weighted TTs (Graehl et al., 2008), which add probabilities to the rules. These probabilities will be learned by an EM algorithm similar to the one described in Graehl et al. (2008). Rule extraction for TTS will be similar to the GHKM algorithm described in Galley et al. (2004) but I also plan to investigate the approaches used by Liu et al. (2006) and Nguyen et al. (2008). For TTT rule extraction, I will use a method similar to the one described in Zhang et al. (2008). I also plan to use language models which takes into account syntactic properties. Although mo</context>
<context position="10547" citStr="Zhang et al. (2008)" startWordPosition="1748" endWordPosition="1751">ools will be used: • GIZA++3 (Och and Ney, 2000) for lexical alignment • Tiburon4 (May and Knight, 2006) for transducer training in both TTS and TTT systems • Moses5 (Koehn et al., 2007) for decoding 5.2 Experiments and evaluation Initially the corpus will be parsed using the tools described in section 5.1 and divided into a training set and a test set. For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al. (2008). Both TTS and TTT systems will be implemented using Tiburon and Moses. For evaluation, BLEU and NIST scores on the test set will be used. The baseline will be the score for factored translation, shown in Table 1. 6 Contributions After its conclusion, this thesis will have brought the following contributions: 3http://www.fjoch.com/GIZA++.html 4http://www.isi.edu/licensed-sw/tiburon 5http://www.statmt.org/moses • Language-independent SMT models which incorporates syntactic information in both language and translation models. • Implementations of these models, using the tools described in Sectio</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proc. ACL-08: HLT, pages 559–567.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>