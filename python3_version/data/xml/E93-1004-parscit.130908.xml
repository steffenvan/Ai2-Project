<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.985058">
Talking About Trees
</title>
<author confidence="0.995694">
Patrick Blackburn
</author>
<affiliation confidence="0.99574">
Department of Philosophy, Rijksuniversiteit Utrecht
</affiliation>
<email confidence="0.833695">
Heidelberglaan 8, 3584 CS Utrecht. Email: patrick@phil.ruu.n1
</email>
<author confidence="0.978004">
Claire Gardent
</author>
<affiliation confidence="0.9908705">
GRIL, Universite de Clermont Ferrand, France, and
Department of Computational Linguistics, Universiteit van Amsterdam
</affiliation>
<email confidence="0.68503">
Spuistraat 134, 1012 VB Amsterdam. Email: claire@mars.let.uva.n1
</email>
<author confidence="0.572001">
Wilfried Meyer-Viol
</author>
<affiliation confidence="0.473591">
Centrum voor Wiskunde en Informatica
</affiliation>
<email confidence="0.622177">
Kruislaan 413, 1098 SJ Amsterdam. Email: W.Meyer.Viol@cwi.n1
</email>
<sectionHeader confidence="0.948052" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995682">
In this paper we introduce a modal lan-
guage LT for imposing constraints on trees,
and an extension LT(LF) for imposing con-
straints on trees decorated with feature
structures. The motivation for introducing
these languages is to provide tools for for-
malising grammatical frameworks perspic-
uously, and the paper illustrates this by
showing how the leading ideas of GPSG can
be captured in LT(LF).
In addition, the role of modal languages
(and in particular, what we have called
layered modal languages) as constraint for-
malisms for linguistic theorising is discussed
in some detail.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.935762709090909">
In this paper we introduce a modal language LT
for talking about trees, and an extension LT(LF)
for talking about trees decorated with feature struc-
tures. From a logical point of view this is a nat-
ural thing to do. After all, the trees and feature
structures used in linguistics are simple graphical
objects. To put it another way, they are merely
rather simple Kripke models, and modal languages
are probably the simplest languages in which non-
trivial constraints can be imposed on such structures.
Moreover the approach is also linguistically natural:
many of the things linguists need to say about trees
(and feature structures) give rise to modal operators
rather naturally, and indeed our choice of modalities
has been guided by linguistic practice, not logical
convenience.
There are several reasons why we think this path
is an interesting one to explore, however two are of
particular relevance to the present paper.&apos; First,
we believe that it can lead to relatively simple and
natural formalisations of various grammatical frame-
works. In our view, neither simplicity nor natural-
ness are luxuries: unless a formalisation possesses a
high degree of clarity, it is unrealistic to hope that
it can offer either precise analyses of particular sys-
tems or informative comparisons of different frame-
works. We believe our approach has the requisite
clarity (largely because it arose by abstracting from
linguistic practice in a rather direct manner) and
much of this paper is an attempt to substantiate
this. Second, LT can be combined in a very nat-
ural way with feature logics to yield simple systems
which deal with configurational concepts, complex
categories and their interaction. The key idea is to
perform this combination of logics in a highly con-
strained way which we have called layering. Layer-
ing is a relatively new idea in modal logic (in fact the
only paper devoted exclusively to this topic seems to
be [Finger and Gabbay 1992]), and it seems to pro-
vide the right level of expressive power needed to
model many contemporary grammar formalisms.
The paper is structured as follows. In section 2 we
define the syntax and semantics of LT, our modal
language for imposing constraints on tree structure.
1Lurking in the background are two additional, rather
more technical, reasons for our interest. First, we believe
that being explicit about tree structure in our logical ob-
ject languages (instead of, say, coding tree structure up as
just another complex feature) may make it easier to find
computationally tractable logics for linguistic processing.
Second, we believe that logical methods may interact
fruitfully with the mathematical literature on tree ad-
missibility (see [Peters and Ritchie 1969], [Rounds 1970]
and [Joshi and Levy 1977]). However we won&apos;t explore
these ideas here.
</bodyText>
<page confidence="0.998652">
21
</page>
<bodyText confidence="0.999357333333334">
In section 3 we put LT to work, showing how it can
be used to characterise the parse trees of context free
phrase structure grammars. In section 4 we consider
how the structured categories prevalent in modern
linguistic formalisms are dealt with. Our solution
is to introduce a simple feature logic LF for talk-
ing about complex categories, and then to layer LT
across LF. The resulting system LT(LF) is capable
of formulating constraints involving the interaction
of configurational and categorial information. Sec-
tion 5 illustrates how one might use this expressive
power by formulating some of the leading ideas of
GPSG in LT (LF). We conclude the paper with some
general remarks on the use of modal languages as
constraint formalisms in linguistics.
</bodyText>
<sectionHeader confidence="0.550612" genericHeader="introduction">
2 The language LT
</sectionHeader>
<bodyText confidence="0.958432763636364">
The primitive alphabet of the language LT (Prop)
contains the following items: two constant symbols
.s and t, some truth functionally adequate collection
of Boolean operators,2 two unary modalities 1 and
a binary modality a modality of arbitrary pos-
itive arity •, the left bracket ( and the right bracket
). In addition we have a set of propositional symbols
Prop. We think of the symbols in Prop as given to us
by the linguistic theory under consideration; differ-
ent applications may well result in different choices
of Prop. To give a very simple example, Prop might
be {S, NP, VP, N, V, DET, CONJ }.
The wffs of LT (Prop) are defined as follows. First,
all elements of Prop are LT (Prop) wffs, and so are
the constant symbols s and t. Second, if 0, 0 and
01, • • • , (n 1) are LT (Prop) wffs, then so are —0,
(0 A 0), 10, 1 0, (0 =0) and .(0i, , On). Third,
nothing else is an LT (Prop) wff. In what follows, we
will assume that some choice of Prop has been fixed
and drop all subsequent mention of it. That is, we&apos;ll
usually speak simply of the language LT.
The semantics of LT is given in terms of finite
ordered trees. We regard finite trees T as quadruples
of the form (F, &gt;, Co, root), where F is a finite set of
nodes, &gt; is the &apos;Mother of&apos; relation between nodes
&gt; v&apos; means `u is the mother of v&apos;), 0 (c r) is
the set of terminal nodes and root is the root node of
the tree. As for the precedence ordering, we define:
Definition 2.1 (Finite ordered trees)
A finite ordered tree is a pair 0 = (T, A) where T
is a tree (r, &gt;, 9, root ) and A is a function assign-
ing to each node u in r a finite sequence of nodes
of F. For all nodes u E 1&amp;quot;, A(u) must satisfy two
constraints. First, A(u) must contain no repetitions.
Second, A(u) = (14, uk) iff u &gt; u1, ...,u &gt; uk
&apos;In what follows we treat and A as primitive, and
the other Boolean symbols such as V (disjunction),
(material implication), 4&amp;quot;, (material equivalence), T (con-
stant true) and 1 (constant false) as abbreviations de-
fined in the familiar manner.
and there is no node u&apos; in r such that 14&gt; u&apos; and u&apos;
does not occur in the sequence A(u). 0
(In short, the repetition-free sequence assigned to a
node u by A consists of all and only the nodes im-
mediately dominated by u; the sequence gives us a
precedence ordering on these nodes. Note that it
follows from this definition that A(u) is the null-
sequence if u is a terminal node.) Next, we define
a model M to be a pair (0, V) where 0 is a finite
ordered tree (T, A) and V is a function from Prop to
the powerset of F. That is, V assigns to each proposi-
tional symbol a set of nodes. Given any model M and
any node u of M, the satisfaction relation M, u
(that is, the model M satisfies 0 at node u) is defined
as follows:
</bodyText>
<equation confidence="0.985666285714286">
u E V(p),
for all p E Prop
u = root
u E e
not M,u
m, u
and M, k
3u&apos; E r(u &gt; u&apos;
and M,u&apos; 0)
3u&apos; E r(ui &gt; u
and M,u1 k 0)
Vu&apos; E r(m,
implies M, u&apos;
length(A(u)) = k
</equation>
<construct confidence="0.524198">
and M, A(u)(i) k
for all 1 &lt; i &lt; k
</construct>
<bodyText confidence="0.999678956521739">
(In the satisfaction clause for •, A(u)(i) denotes the
ith element of the sequence assigned to u by A.) If
is satisfied at all nodes of a model M, then we say
that is valid on M and write M 1= .0. The notion of
validity has an important role to play for us. As we
shall see in the next section, we think of a grammar
G as being represented by an LT wff OG. The trees
admitted by the grammar are precisely those models
on which OG is valid.
Note that LT is just a simple formalisation of lin-
guistic discourse concerning tree structure. First, 1
and I enable us to say that a daughter node, or a
mother node, do (or do not) bear certain pieces of
linguistic information. For example, 10 insists that
the information is instantiated on some daughter
node. Second, enables general constraints about
tree structure to be made: to insist that 0 =0 is to
say that any node in the tree that bears the informa-
tion 0 must also bear the information 0. Finally •
enables admissibility conditions on local trees to be
stated. That is, • is a modal operator that embod-
ies the idea of local tree admissibility introduced by
[McCawley 1968].3 It enables us to insist that a node
</bodyText>
<footnote confidence="0.99307525">
3As well as McCawley&apos;s article, see also [Gazdar 1979].
Our treatment of node admissibility conditions has been
heavily influenced by Gazdar&apos;s paper and later work in
the GPSG tradition (for example, [Gazdar et al. 1985]).
</footnote>
<figure confidence="0.561259777777778">
M,ukp if
M,uks iff
M,ukt iff
M,u if
M,ukOAOif
M,u
M, u kto iff
iff
M, u *(01, Ok) if
</figure>
<page confidence="0.992568">
22
</page>
<bodyText confidence="0.998033866666667">
must immediately and exhaustively dominate nodes
bearing the listed information, and in the following
section we&apos;ll see how to put it to work.4
Although the design of LT was guided by linguistic
considerations, with the exception of • it turns out
to be a rather conventional language.5 In particular,
is what modal logicians call strict implication, and
1 and together form a particularly simple example
of what modal logicians like to call a &apos;tense logic&apos;.
In what follows we will occasionally make use
of the following (standard) modal abbreviations:
=de/ T =cb (that is, 4) is satisfied at all nodes)
and 1.0 =def 1-0 (that is, qf is true at the mother
of the node we are evaluating at, if in fact this node
has a mother).
</bodyText>
<sectionHeader confidence="0.955447" genericHeader="method">
3 Talking about trees
</sectionHeader>
<bodyText confidence="0.986807">
In this section we show by example how to use LT to
formulate node admissibility conditions that will pick
out precisely the parse trees of context free gram-
mars. Consider the context free phrase structure
grammar G = (S, N, T, P) where S is the start sym-
bol of G; where N, the set of non-terminal symbols,
is {S, NP, VP, N, V, DET, CONJ }; where T, the
set of terminal symbols, is {the, a, man, woman, don-
key, beat, stroke, and, but); and where P, the set of
productions, is:
</bodyText>
<equation confidence="0.654445">
NP VP I S CONJ S
NP --+ DET N
VP V NP
--+ man I woman I donkey
V beat I stroke
DET --) the I a
CONJ and I but
</equation>
<bodyText confidence="0.998925608695652">
Let&apos;s consider how to capture the parse trees of this
grammar by means of constraints formulated in LT.
The first step is to fix our choice of Prop. We choose
it to be N U T, that is, all the terminal and non-
terminal symbols of G. The second step is to capture
the effect of the productions P. We do so as follows.
Let OP be the conjunction of the following wffs:
4The reader will doubtless be able to think of other
interesting operators to add. For example, adding opera-
tors 1.• and 1* which explore the transitive closures of the
daughter-of and mother-of relations respectively enables
GB discourse concerning command relations to be mod-
eled; while weakening the definition of • to ignore the
precedence ordering on sisters, and adding a new unary
modality to take over the task of regulating linear prece-
dence, permits the IDAP format used in GPSG to be nat-
urally incorporated. Such extensions will be discussed in
[Blackburn et al. forthcoming], however for the purposes
of the present paper we will be content to work with the
simpler set of operators we have defined here.
&apos;Indeed, on closer inspection, even • can be regarded
as an old friend in disguise; such logical issues will be
discussed in [Blackburn et al. forthcoming].
</bodyText>
<equation confidence="0.997663285714286">
S .(NP, VP) V •(S, CONJ, 5)
NP .(DET, N)
VP O(V, NP)
N .(man) V .(woman) V e(donkey)
V .(beat)V .(stroke)
DET e(the) V .(a)
CONJ e(and) V 0(btil)
</equation>
<bodyText confidence="0.990560852941176">
Note that each conjunct licences a certain informa-
tion distribution on local trees. Now, any parse tree
for G can be regarded as an LT model, and because
each conjunct in OP mimics in very obvious fashion
a production in G, each node of an LG parse tree is
licenced by one of the conjuncts. To put it another
way, oP is valid on all the G parse trees.
However we want to capture all and only the parse
trees for G.
This is easily done: we need merely express
in our language certain general facts about parse
trees. First, we insist that each node is labeled
by at least one propositional symbol: 0( \ /
v pENuT P)
achieves this. Second, we insist that each node is
labeled by at most one propositional symbol: (p
AgE((NUT)\{p}) achieves this. Third, we insist
that the root of the tree be decorated by the start
symbol S of the grammar: s = S achieves this.
Fourth, we insist that non-terminal symbols label
only nonterminal nodes: ApeN(p —4) achieves
this. Finally, we insist that terminal symbols label
terminal nodes: ApET(p t) achieves this. Call the
conjunction of these five wffs cbu; note that, mod-
ulo our choice of the particular sets N and T, qP
expresses universal facts about parse trees.
Now, for the final step: let OG be OP A P. That
is, cbG expresses both the productions P of G and
the universal facts about parse tree structure. It is
easy to see that for any model M, M = if M is
(isomorphic to) a parse tree for G. Indeed, it is not
hard to see that the method we used to express G
as a formula generalises to any context free phrase
structure grammar.
</bodyText>
<sectionHeader confidence="0.9450595" genericHeader="method">
4 Trees decorated with feature
structures
</sectionHeader>
<bodyText confidence="0.999871">
The previous section showed that LT is powerful
enough to get to grip with interesting &apos;languages&apos; in
the sense of formal language theory; but although
natural language syntacticians may use tools bor-
rowed from formal language theory, they usually
have a rather different conception of language than
do computer scientists. One important difference is
that linguists typically do not consider either non-
terminal or terminal symbols to be indivisible atoms,
rather they consider them to be conglomerations
of &apos;lower level&apos; information called feature structures.
For example, to say that a node bears the informa-
tion NP is to say that the node actually bears a lot
</bodyText>
<page confidence="0.988225">
23
</page>
<bodyText confidence="0.998685326086957">
of lower level information: for example, the features
+N, —V, and BAR 2. Moreover (as we shall see
in section 5) the constraints that a tree must sat-
isfy in order to be accepted as well formed will typi-
cally involve this lower level information. The feature
co-occurrence restrictions and feature instantiation
principles of GPSG are good examples of this.
Is it possible to extend our framework to deal with
such ideas? More to the point, is there a simple ex-
tension of our framework that can deal with com-
plex categories? Layered modal languages provide
what is needed. Semantically, instead of associating
each tree node with an atomic value, we are going to
associate it with a feature structure. Syntactically,
we are going to replace the propositional symbols of
LT with wffs capable of talking about this additional
structure. To be more precise, instead of defining the
wffs of LT over a base Prop consisting of primitive
propositional symbols, we are going to define them
over a base of modal formulas. That is, we will use
a language with two &apos;layers&apos;. The top layer LT will
talk about tree structure just as before, whereas the
base layer (or feature layer) LF will talk about the
&apos;internal structure&apos; associated with each node.&apos;
Clearly the first thing we have to do is to define our
feature language LF and its semantics. We will as-
sume that the linguistic theory we are working with
tells us what features and atoms may be used. That
is, we assume we have been given a signature (F, A)
where both F and A are non-empty finite or denu-
merably infinite sets, the set of features and the set of
atomic information respectively. Typical elements of
.F might be CASE, NUM, PERSON and AGR; while typ-
ical elements of A might be genitive, singular, plural,
1st, 2nd, and 3rd.
The language LF (of signature (Y, A)) contains
the following items: all the elements of A (which we
will regard as propositional symbols), a truth func-
tionally adequate collection of Boolean connectives,
and all the elements of .F (which we will regard as
one place modal operators). The set of wffs of LF
is the smallest set containing all the propositional
symbols (that is, all the elements of A) closed under
the application of the Boolean and modal operators
(that is, the elements of Y). Thus a typical wff of
LF might be the following:
</bodyText>
<subsectionHeader confidence="0.531782">
(AGR)(PERsoN)3rd A (CASE)genitive.
</subsectionHeader>
<bodyText confidence="0.9907135">
Note that this wff is actually something very familiar,
namely the following Attribute Value Matrix:
</bodyText>
<sectionHeader confidence="0.53318" genericHeader="method">
[AGR [PERSON 3rd]
CASE genitive
</sectionHeader>
<bodyText confidence="0.919042909090909">
6 As was mentioned earlier, at present there is rela-
tively little published literature on layered modal lan-
guages. The most detailed investigation is that of
[Finger and Gabbay 1992], while [de Rijke 1992] gives a
brief account in the course of a general discussion on the
nature of modal logic.
Indeed the wffs of LF are nothing but straightfor-
ward ginearisations&apos; of the traditional two dimen-
sional AVM format. Thus it is unsurprising that the
semantics of LF is given in terms of feature struc-
tures:
</bodyText>
<subsectionHeader confidence="0.674681">
Definition 4.1 (Feature structures)
</subsectionHeader>
<bodyText confidence="0.99391025">
A feature structure of signature (Y, A) is a triple
F of the form (W, V), where W is a non-
empty set, called the set of points; for all f E
is a binary relation on W that is a partial function;
and V is a function that assigns to each propositional
symbol (that is, each a E A), a subset of W.7
Our satisfaction definition for LF wffs is as follows.
For any F = (W, {Rf }iEF, V) and any point w E W:
</bodyText>
<equation confidence="0.4428235">
F,wker if w E V(a),foralla EA
F,w iff not F, w
F,wHkAt,b iff F,w4 and F,w
F, w WO if ate(wRiwi and F, w&apos; k 0)
</equation>
<bodyText confidence="0.9998453">
With LF and its semantics defined, we are ready
to define a language for talking about trees decorated
with feature structures: the language LT(LF), that
is, the language LT layered over the language LF .
That is, we choose Prop to be LF and then make the
LT wffs on top of this base in the usual way.8 As a
result, we&apos;ve given an &apos;internal structure&apos; (namely, a
modal structure, or AVM structure) to the proposi-
tional symbols of LT. This is the syntactical heart
of layering.
</bodyText>
<subsectionHeader confidence="0.714237">
Definition 4.2 (Feature decorated trees)
</subsectionHeader>
<bodyText confidence="0.964376785714286">
By a (finite ordered) feature structure decorated tree
(of signature (.7&amp;quot;, A)) is meant a triple (0, D,
where 0 is a finite ordered tree, D is a function
that assigns to each node u of 0 a feature struc-
ture (of signature (.7&apos;, A)), and d is a function that
assigns to each node u of 0 a point of D(u). That
is, d(u) E D(u).9 0
It is straightforward to interpret LT(LF) wffs on
feature structure decorated trees: indeed all we have
7For detailed discussion of this definition see [Black-
burn 1991, 1992] or [Blackburn and Spaan 1991, 1992].
For present purposes it suffices to note that it includes as
special cases most of the well known definitions of feature
structures, such as that of [Kasper and Rounds 1986].
</bodyText>
<footnote confidence="0.6154515">
8This is worth spelling out in detail. The wffs of the
language LT(LF) (of signature (F, A)) are defined as fol-
</footnote>
<construct confidence="0.5484716">
lows. First, all LF wffs (of signature (Y, A)) are LT(LF)
wffs, and so are the constant symbols s and t. Second,
if 4, ti) and 01, , On are LT(LF) wifs then so are
(/) A tk, To, 10, 0 0 and .(01, , On). Third, nothing
else is an LT(LF) wff.
</construct>
<bodyText confidence="0.918826">
&apos;In a number of recent talks Dov Gabbay has ad-
vocated the idea of &apos;fibering&apos; one set of semantic en-
tities over another. This is precisely what&apos;s going on
here: we&apos;re fibering trees over feature structures. Fibered
structures are the natural semantic domains for layered
languages.
</bodyText>
<page confidence="0.994046">
24
</page>
<bodyText confidence="0.979917666666667">
to do is alter the base clause of the LT definition. So,
let M = (0, D, d) be a feature structure decorated
tree, and u be any node in 0. Then for all wffs
</bodyText>
<equation confidence="0.561887">
E LF:
m, u k :ff D(u), d(u)
</equation>
<bodyText confidence="0.999947774193548">
In short, when in the course of evaluating an LT(LF)
wff at a node u we encounter an LF wff (that is,
when we reach &apos;atomic&apos; level) we go to the feature
structure associated with u (that is, D(u)), and start
evaluating the LI wff at the point d(u). This change
at the atomic level is the only change we need to
make: all the other clauses (that is, the clauses for
and t, the Boolean operators, for /, and .) are
unchanged from the LT satisfaction definition given
in section 2.
To close this section, a general comment. LT(LF)
is merely one, rather minimalist, example of a lay-
ered modal language. The layering concept offers
considerable flexibility. By enriching either the LT
component, the LF component, or both, one can tai-
lor constraint languages for specific applications. In-
deed, it&apos;s worth pointing out that one is not forced
to layer LT over a modal language at all. One could
perfectly well layer LT across a first order feature
logic or over a fragment of such a first order logic
(such as the SchOnfinkel Bernays fragment explored
in [Johnson 19911),10 and doubtless the reader can
imagine other possibilities. That said, we&apos;re struck
by the simplicity of purely modal layered languages
such as LT(LF), and we believe that there are good
theoretical reasons for being interested in modal ap-
proaches (these are discussed at the end of the pa-
per). Moreover, as we shall now see, even the rather
simple collection of operators offered by LT (LF) are
capable of imposing interesting constraints on syn-
tactic structures.
</bodyText>
<sectionHeader confidence="0.969906" genericHeader="evaluation">
5 LT(LF) and linguistic theory
</sectionHeader>
<bodyText confidence="0.999735066666667">
At this stage, it should be intuitively clear why
LT(LF) is well suited for modeling contemporary lin-
guistic theories. On the one hand, the LT part of
the language lets us talk directly about tree struc-
ture, thus clearly it is a suitable tool for imposing
constraints on constituent structure. On the other
hand, the LF part of the language permits the de-
scription of complex (rather than atomic) categories;
and nowadays the use of such categories is standard.
The aim of this section is to give a concrete illustra-
tion of how LT(LF) can be used to model modern
linguistic theories. The theory we have chosen for
this purpose is GPSG. In what follows we sketch how
some of the leading ideas of GPSG can be captured
using LT(LF) wffs.
</bodyText>
<subsectionHeader confidence="0.955341">
5.1 Complex categories
</subsectionHeader>
<bodyText confidence="0.9999866">
One of the fundamental ideas underlying GPSG (and
indeed many other contemporary syntactic theories)
is that a linguistic category is a complex object con-
sisting of feature specifications, where feature speci-
fications are feature/value pairs, and a value is either
an atom or is itself a category. In LT(LF) , this idea
is easily modeled since LT(LF) contains LF, a lan-
guage specifically designed for talking about feature
structures. To give a simple example, consider the
following complex category:
</bodyText>
<equation confidence="0.737008666666667">
[ NOUN
VERB +
BAR two
This is naturally represented by the following
LF wff:
—.noun A verb A (BAR)two
</equation>
<bodyText confidence="0.999776142857143">
where the attribute BAR is represented by a modality
and the atomic symbols and Boolean features are
represented by propositional symbols. This wff is
satisfied at any point w in a feature structure such
that noun is false at w, verb is true at w, and the
propositional information two is reachable by making
a BAR transition from w.
</bodyText>
<subsectionHeader confidence="0.996447">
5.2 Admissibility constraints on local trees
</subsectionHeader>
<bodyText confidence="0.993582526315789">
The heart of GPSG is a collection of interacting
principles governing the proper distribution of fea-
tures within syntactic trees. Central to this the-
ory is the concept of admissibility constraints on
local trees. Very roughly,11 the idea is that a lo-
cal tree is admissible if it is a projection of an im-
mediate dominance rule (that is, each node in the
tree corresponds in some precisely defined way to
exactly one category in the rule) and it satisfies all
of the grammar principles; these include feature co-
occurrence restrictions (FCRs), feature specification
defaults (FSDs), linear precedence (LP) statements,
and universal feature instantiation principles (UIPs).
In what follows, we show how LT(LF) can be used
to model some of these admissibility conditions on
local trees: section 5.2.1 shows how to model phrase
structure restrictions and section 5.2.2 concentrates
on FCRs. Finally, in section 5.2.3 we sketch an
LT(LF) treatment of the GPSG UIPs.
</bodyText>
<subsectionHeader confidence="0.587401">
5.2.1 Phrase structure restrictions
</subsectionHeader>
<bodyText confidence="0.922339555555555">
In GPSG, restrictions on constituent structure are
expressed by a set of ID/LP statements. As the name
indicates, I(mmediate) D(ominance) statements en-
code immediate dominance restrictions on local trees
(for instance, the ID rule A B, C licenses any local
tree consisting of a mother node labeled with cate-
gory A and exactly two daughter nodes labeled with
10Layering over first order languages is treated in 11For a more precise formulation of the constraints on
[Finger and Gabbay 1992]. tree admissibility, see [Gazdar et al. 1985, page 100].
</bodyText>
<page confidence="0.996761">
25
</page>
<bodyText confidence="0.987973761904762">
categories B and C respectively), whereas LP state-
ments define a linear precedence relation between sis-
ter nodes (for example, the LP statement C B
states that in any local tree with sisters labeled B
and C, the C node must precede the B node).
Strictly speaking, such restrictions cannot be mod-
eled in LT (LP) . The reason for this is trivial. As has
already been pointed out, the satisfaction definition
for • makes use of both the immediate dominance
and linear precedence relations. In a full-blooded at-
tempt to model GPSG, we would probably define a
variant modal operator o of • that did not make use
of the precedence relation, and introduce an addi-
tional modal operator (say t&gt;) to control precedence.
However, having made this point, we shall not pur-
sue the issue further. Instead, we will show how the
present version of LT (LP) allows for the encoding of
phrase structure rules involving complex categories.
As was shown in section 3, rules involving atomic
categories can be modeled in a fairly transparent way
using • and V. For instance,
</bodyText>
<equation confidence="0.7260284">
S = *(NP, VP)V e(S, CONJ, S)
captures the import of the following two phrase
structure rules:
S NP VP
S —+ S CONJ S
</equation>
<bodyText confidence="0.887618363636364">
In these rules the information associated with each
node of the tree is propositional in nature, that
is, non-structured. However because LT(LP) allows
one to peer into the internal structure of nodes,
this way of modeling phrase structure rules extends
straightforwardly to rules involving complex cate-
gories: it suffices to replace the propositional sym-
bols by LP wffs. For example, the phrase structure
rule:
[ NOUN
VERB
BAR
NOUN
VERB
BAR zero
SUBCAT trans
can be formulated as the following LT(LP) wff:
(-&apos;noun A verb A (BAR)two)
•((-inoun A verb A (BAR)zero A (suscAT)trans),
(noun A -,verb A (BAR)two))
That is, the LP wffs give the required &apos;internal
structure&apos; in the obvious way.
</bodyText>
<subsectionHeader confidence="0.646134">
5.2.2 Feature co-occurrence restrictions
</subsectionHeader>
<bodyText confidence="0.96893075">
FCRs encode restrictions on the distribution of
features within categories. More specifically, they
express conditional or bi-conditional dependencies
between feature specifications occurring within the
same category. For instance, the FCR:
[INV -I-1 1) [AUX -F, VFORM fin] (FCR1)
states that any category with feature specification
INV + must also contain the feature specifications
AUX -F and VFORM fin. In other words, any inverted
constituent must be a finite auxiliary.
FCRs are naturally expressed in LT (LP) by using
the connective. Thus, FCR1 can be captured by
means of the following schema:
inv = (aux A (VFORM) fin)
This says that for any ordered tree and any node
u in this tree, if the feature structure associated with
u starts with the point w and inv is true at w, then
aux is also true at w and furthermore, the proposi-
tional information fin is reachable from w by making
a (VFORM) transition to some other node w&apos;.
</bodyText>
<subsectionHeader confidence="0.512582">
5.2.3 Universal principles
</subsectionHeader>
<bodyText confidence="0.990656125">
In this section, we show that LT (LP) allows us
to axiomatize the main content of GPSG three fea-
ture instantiation principles namely, the foot feature
principle, the head feature convention and the con-
trol agreement principle.
Consider first the foot feature principle (FFP).
This says that:
Any foot feature specification which is in-
stantiated on a daughter in a local tree must
also be instantiated on the mother cate-
gory in that tree. [Gazdar et al. 1985, page
81] 12
SO, assume that our GPSG theorising has resulted
in signature (F, A) which includes the feature FOOT.
We capture the FFP by means of the following
schema:
(FooT)0 ...ft(FooT)4).
This says that for any node u, if the information (/)
is reachable by making a FOOT transition in the fea-
ture structure associated with u, then it must also
be possible to obtain the information by making
a FOOT transition in the feature structure associ-
ated with the mother of u. That is, FOOT infor-
mation percolates up the tree. So for instance, if
three sister nodes ul, U2 and u3 of a tree bear the
information (Focrr)(01, (FooT)02 and (Focrr)cb3
respectively, then the feature structure associated
with the mother node must bear the information
(Focrr)01 A (F0OT)02 A (FOOT)03. Incidentally, it
then follows from the semantics of LP that this node
bears the information (FooT)(01 A cb2 A 03). That
is, the three pieces of foot information are unified.
</bodyText>
<footnote confidence="0.99188525">
12This axiom is actually a simplified version of the FFP
in that it ignores the distinction between inherited and
instantiated features. See section 5.3 for discussion of
this point.
</footnote>
<figure confidence="0.991930142857143">
•
--+
two
I [ NOUN
VERB
BAR
two
</figure>
<page confidence="0.993004">
26
</page>
<bodyText confidence="0.997219">
Consider now the head feature convention (HFC).
A simplified version of the HFC can be stated as
follows;13
Any head features carried by the head
daughter is carried by the mother and vice-
versa.
Assuming a signature (F, A) which includes the
feature HEAD-FEATURE and the atomic information
head, we capture the HFC by means of the following
schema:
</bodyText>
<figure confidence="0.723680666666667">
(head A (HEAD-FEATURE)) (HEAD-FEATURE)0
A
(head A 1(HEAD-FEATURE)0) (HEAD-FEATURE)0
</figure>
<bodyText confidence="0.999693342857143">
The first conjunct says that whenever the feature
structure associated with a node u marks it as a head
node, and the information 0 is reachable by making
a HEAD-FEATURE transition, then one can also reach
the same information 0 by making a HEAD-FEATURE
transition in the feature structure associated with
the mother of u. The second conjunct works analo-
gously to bring HEAD-FEATURE information down to
the head daughter.
Finally, we sketch how the effect of the more elab-
orate control agreement principle (CAP) can be cap-
tured. GPSG formulates CAP by making use of
the Montagovian semantic type assignments. As we
haven&apos;t discussed semantics, we&apos;re going to assume
that the relevant type information is available inside
our feature structures. With this assumed, our for-
mulation of CAP falls into three steps: first, defining
the notions of controller and controllee (or target in
GPSG terminology); second, defining the notion of a
control feature; and third, defining the instantiation
principle. We consider each in turn. Controller and
controllee are defined as follows:14
A category C is controlled by another cate-
gory C&apos; in a constituent Co if one of the
following situations obtains at a seman-
tic level: either C is a functor that ap-
plies to C&apos; to yield a Co, or else there
is a control mediator C&amp;quot; which combines
with C and C&apos; in that order to yield a Co.
[Gazdar et al. 1985, page 87]
Further, a control mediator is a head category
whose semantic type is (V P,(N P, VP)) where VP
denotes the type of an intransitive verb phase and
NP that of a generalised quantifier. The first step is
to formulate the notions of controller and controllee.
</bodyText>
<tableCaption confidence="0.592017625">
&amp;quot;The exact formulation of the HFC implies that only
free feature specifications are taken into account. See
section 5.3 for discussion of this point.
14Again this is somewhat simplified in that the final
GPSG definition of control only takes into account so-
called x—features so as to ignore perturbations of se-
mantic types introduced by the presence of instantiated
features.
</tableCaption>
<bodyText confidence="0.998894">
We do this with the following three wffs (a and b are
metavariables over semantic types, and np and vp
correspond to the NP and VP above):
</bodyText>
<listItem confidence="0.6031585">
• (TYPE)a/b, (TYPE)a)
.(controllee, controller)
</listItem>
<equation confidence="0.97073775">
e( (TYPE)a, (TYPE)trib)
.(controller, controllee)
s(T, (TYPE)vpl(npl vp), T)
.(controller, T, controllee)
</equation>
<bodyText confidence="0.9995647">
Control features are SLASH and AGR and are not
mutually exclusive. The problem is to decide which
should actually function as control feature when both
of them are present on the controllee category. In ef-
fect, in case of conflict (cf. [Gazdar et a/. 1985, 89]),
SLASH is the control feature if it is inherited, else
AGR is. As we have no way to distinguish here be-
tween inherited and instantiated feature values, we
will (again) give a simplified axiomatisation of con-
trol features, namely:
</bodyText>
<equation confidence="0.9877375">
(SLASH)0 = (coNTRoL_FEAT)0
(sLAK)T A (AGR)cd) (CONTROL_FEAT)0
</equation>
<bodyText confidence="0.996352">
Finally, we turn to the CAP itself. This says
that the value of the control feature of the controllee
is identical with the category of the controller. In
</bodyText>
<figure confidence="0.817164333333333">
(controller)A (controllee))
1 ((controller A 0) 4-0
(controllee A (CONTROL_FEAT)0))
</figure>
<subsectionHeader confidence="0.944748">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999968352941177">
In the preceding sections, we showed how LT(LF)
could be used to capture some of the leading concepts
of GPSG. Although the account involves many sim-
plifications and shortcomings, the examples should
illustrate how to use LT (Li): one expresses linguis-
tic principles as LT(LF) wffs, and only those (deco-
rated) trees validatings all these wffs are considered
well-formed. What we hope to have shown is that
LT(LF) is a very natural language for expressing the
various types of theoretical constructs developed in
GPSG and, more generally, in most modern theories of
grammar. Complex categories can be described us-
ing the LF part of the language while general infor-
mation concerning the geometry of trees and the dis-
tribution of feature specifications within those trees
can be stated using the full language. More specifi-
cally, the bullet operator • provides an easy way to
</bodyText>
<page confidence="0.992674">
27
</page>
<bodyText confidence="0.99997237704918">
express phrase structure constraints while the strict
implication operator allows one to express various
types of constraints on the distribution of features in
trees. When used to connect two LF wffs, = ex-
presses generalisations over the internal structure of
categories (as illustrated in section 5.2.2 on FCRs),
whereas when used together with 1, 1 and • it allows
information sharing between feature structures asso-
ciated with different nodes in the tree (cf. section
5.2.3).
As already repeatedly mentioned, there remain
many shortcomings in our approach to modeling
GPSG. To close this discussion let&apos;s consider them a
little more closely; this will lead to some interesting
questions about the nature of linguistic theorising.
The first type of shortcoming involves lack of ex-
pressivity in LT (LF) and is illustrated by the impos-
sibility of expressing ID /LP statements (cf. section
5.2.1). As already indicated, we don&apos;t regard such
shortcomings as a failure of the general modal ap-
proach being developed here. With a slightly differ-
ent choice of modal language, an adequate modeling
of ID/LP statements could be attained. More gener-
ally, we think it is important to explore a wide range
of modal languages for linguistic theorising, for we
believe that it may be possible to usefully classify
differing linguistic theories in terms of the different
modal operators required to formalise them. A theo-
retical justification for our confidence will be given in
the following section; here we&apos;ll simply say that we
think this is a feasible way of addressing the ques-
tions raised in [Shieber 1988] concerning the com-
parative expressivity and computational complexity
of grammatical formalisms.
The second type of shortcoming is more serious
and potentially far more interesting. Two cases
in point are (i) the distinction made in GPSG be-
tween instantiated and inherited features and (ii) the
GPSG notion of a free feature. Briefly, inherited fea-
tures are features whose presence on categories in
trees is directly determined by an ID rule whereas
instantiated features are non-inherited features (cf.
[Gazdar et al. 1985, page 76]). Furthermore, given
a category C occurring in a tree r such that r is a
projection of some ID rule that satisfies the FFP and
the CAP, a feature specification is said to be free in
C if is is compatible with the information contained
in C (cf. [Gazdar et al. 1985, page 95] for a more
precise definition of free features). The problem in
both cases is that derivational information needs to
be taken into account. In the first case, the source
of the feature specification must be known (does it
stem from an ID rule or from some other source?). In
the second case, we must know that both CAP and
FFP are already being satisfied by the category un-
der consideration. There is an essentially dynamic
flavour to these ideas, something that goes against
the grain of the essentially static tree descriptions
offered by LT(LF). Whether this dynamic aspect is
in fact required, and how it could best be modeled,
we leave here as open research questions.
</bodyText>
<sectionHeader confidence="0.981236" genericHeader="conclusions">
6 But why modal languages?
</sectionHeader>
<bodyText confidence="0.999915833333333">
To close this paper we wish to discuss an issue that
may be bothering some readers: why were modal
languages chosen as the medium for expressing con-
straints on trees and feature structures? A reader
unfamiliar with the developments that have taken
place in modal logic since the early 1970&apos;s, and in
particular, unfamiliar with the emergence of modal
correspondence theory, may find the decision to work
with modal languages rather odd; surely it would
be more straightforward to work in (say) some ap-
propriate first order language? However we believe
that there are general reasons for regarding modal
languages as a particularly natural medium for lin-
guistic theorising, and what follows is an attempt to
make these clear.
The first point that needs to be made about modal
languages is that they are nothing but extremely
simple languages for talking about graphs. Unfortu-
nately, the more philosophical presentations of modal
logic tend to obscure this rather obvious point. In
such presentations the emphasis is on discussing such
ideas as &apos;possible worlds&apos; and Intensions&apos;. Such dis-
cussions have their charms, but they make it very
easy to overlook the fact that the mathematical
structures on which these ideas rest are extremely
simple: just sets of nodes decorated with atomic in-
formation on which a transition relation is defined.
Kripke models are nothing but graphs.
The second point is even more important. Modal
languages are not some strange alternative to classi-
cal languages; rather, they are relatively constrained
fragments of such languages. If a problem has been
modelled in a modal language then it has, ipso facto,
been modeled in a classical language; and moreover,
it has been modeled in a very resource conscious way.
The point deserves a little elaboration. Ever since
the early 1970&apos;s, one of the most important branches
of research in technical modal logic has been modal
correspondence theory (see [van Benthem 1984] and
references therein), the systematic study of the in-
terrelationships between modal languages on the one
hand, and various classical logics (first order, infini-
tary, and second order) on the other. Modal corre-
spondence theory rests on the following simple ob-
servation. It is usually possible to view modal oper-
ators as logical &apos;macros&apos;; essentially modal operators
are a prepackaging of certain forms of quantification
that are available in classical languages. To give a
simple example, we might view a statement of the
form I 4:• as a shorthand for the first order expres-
sion 3y(y &gt; x A w(y)), where w(y) is a certain first
order wff called the standard translation of 41.15 For
&amp;quot;This is somewhat impressionistic; for the full story
consult [van Benthem 1984]. For a discussion of the fun-
</bodyText>
<page confidence="0.993732">
28
</page>
<bodyText confidence="0.99014224137931">
present purposes the details aren&apos;t particularly im-
portant; the key point to note is that the I operator is
essentially a neat notation which embodies a limited
form of first order quantificational power: namely
the ability to quantify over mother nodes. More gen-
erally, modal languages eschew the quantificational
power that classical languages achieve through the
use of variables and binding, in favour of a variable
free syntax in which quantification is performed us-
ing operators. Expressive power is traded for syntac-
tic simplicity.
The relevance of these points for linguistics should
be clear. Linguistic theorizing makes heavy use of
graph structures; trees and feature structures are ob-
vious examples. Thus modal languages can be used
as constraint formalisms; what correspondence the-
ory tells us is that they are particularly interesting
ones, namely formalisms that mesh neatly with the
linguists&apos; quest for revealing descriptions using the
weakest tools possible.
Acknowledgements: We would like to thank Jo-
han van Benthem, Gerald Gazdar, Maarten de Ri-
jke, Albert Visser and the anonymous referees for
their comments on the earlier draft of this paper.
Patrick Blackburn would like to acknowledge the fi-
nancial support of the Netherlands Organization for
the Advancement of Research (project NF 102/62-
356 &apos;Structural and Semantic Parallels in Natural
Languages and Programming Languages).
</bodyText>
<sectionHeader confidence="0.99829" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99903692">
[Blackburn 1991] Blackburn, P.: 1991, Modal Logic
and Attribute Value Structures. To appear in
Diamonds and Defaults, edited by M. de Ri-
jke, Studies in Logic, Language and Informa-
tion, Kluwer.
[Blackburn and Spaan 1991] Blackburn, P. and
Spaan, E.: 1991, On the Complexity of At-
tribute Value Logics. Proceedings of the Eighth
Amsterdam Colloquium, edited by P. Dekker
and M. Stokhof, Philosophy Department, Ams-
terdam University, The Netherlands.
[Blackburn and Spaan 1992] Blackburn, P. and
Spaan, E.: 1992, A Modal Perspective on the
Computational Complexity of Attribute Value
Grammar. To appear in Journal of Logic, Lan-
guage and Information.
[Blackburn 1992] Blackburn, P.: 1992, Structures,
Languages and Translations: the Structural Ap-
proach to Feature Logic. To appear in Con-
straints, Language and Computation, edited by
C. Rupp, M. Rosner and R. Johnson, Academic
Press.
[Blackburn et al. forthcoming] Blackburn, P., Gar-
dent, C., and Meyer-Viol, W.: Modal Phrase
Structure Grammars. In preparation.
damental correspondences involved in feature logic see
[Blackburn 1992].
[de Rijke 1992] de Rijke, M.: 1992, What is Modal
Logic? In Logic at Work, proceedings of the
Applied Logic Conference, CCSOM, University
of Amsterdam, 1992.
[Finger and Gabbay 1992] Finger, M. and Gabbay,
D.: 1992, Adding a Temporal Dimension to a
Logic System. Journal of Logic, Language and
Information, 1, pp. 203-233.
[Gazdar 1979] Gazdar, G.: 1979, Constituent Struc-
tures. Manuscript, Sussex University.
[Gazdar et al. 1985] Gazdar, G.: Klein, E., Pullum,
G., and Sag, S.: 1985, Generalised Phrase Struc-
ture Grammar. Basil Blackwell.
[Johnson 1991] Johnson, M.: 1991, Features and
Formulas, Computational Linguistics, 17, pp.
131-151.
[Joshi and Levy 1977] Joshi, A. and Levy, S.: 1977,
Constraints on Structural Descriptions: Local
Transformations. SIAM Journal of Computing,
6, pp. 272-284.
[Kasper and Rounds 1986] Kasper, R. and Rounds,
W.: 1986, A logical semantics for feature struc-
tures. Proceedings of the 24th Annual Meeting of
the Association for Computational Linguistics,
Columbia University, New York, pp. 257-266.
[McCawley 1968] McCawley, J.: 1968, Concerning
the Base Component of a Transformational
Grammar. Foundations of Language, 4, pp. 55-
81.
[Peters and Ritchie 1969] Peters, S. and Ritchie, R.:
1969, Context-Sensitive Immediate Constituent
Analysis — Context-Free Languages Revisited.
Proceedings ACM Symposium on Theory of
Computing, Association for Computing Machin-
ery, pp. 1— 10.
[Rounds 1970] Rounds, W.: 1970, Tree-Oriented
Proofs of Some Theorems in Context-Free and
Indexed Languages. Proceedings ACM Sympo-
sium on Theory of Computing, Association for
Computing Machinery, pp. 210 — 216.
[Shieber 1988] Shieber, S.: 1988, Separating Lin-
guistic Analyses from Linguistic Theories. In
Natural Language Parsing and Linguistic Theo-
ries, edited by U. Reyle and C. Rohrer, Reidel.
[van Benthem 1984] van Benthem, J.: 1984, Corre-
spondence Theory, in Handbook of Philosophical
Logic, 2, edited by D. Gabbay and F. Guenth-
ner, Reidel.
</reference>
<page confidence="0.999111">
29
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.199722">
<title confidence="0.999907">Talking About Trees</title>
<author confidence="0.999932">Patrick Blackburn</author>
<affiliation confidence="0.997368">Department of Philosophy, Rijksuniversiteit Utrecht</affiliation>
<address confidence="0.718942">Heidelberglaan 8, 3584 CS Utrecht. Email: patrick@phil.ruu.n1</address>
<author confidence="0.977214">Claire Gardent</author>
<affiliation confidence="0.831667">de Clermont Ferrand, France, and Department of Computational Linguistics, Universiteit van Amsterdam</affiliation>
<address confidence="0.870028">Spuistraat 134, 1012 VB Amsterdam. Email: claire@mars.let.uva.n1</address>
<affiliation confidence="0.5262695">Wilfried Meyer-Viol Centrum voor Wiskunde en Informatica</affiliation>
<address confidence="0.591733">Kruislaan 413, 1098 SJ Amsterdam. Email: W.Meyer.Viol@cwi.n1</address>
<abstract confidence="0.999124625">In this paper we introduce a modal lanimposing constraints on trees, an extension imposing constraints on trees decorated with feature structures. The motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously, and the paper illustrates this by how the leading ideas of captured in In addition, the role of modal languages (and in particular, what we have called modal languages) constraint formalisms for linguistic theorising is discussed in some detail.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Blackburn</author>
</authors>
<title>Modal Logic and Attribute Value Structures. To appear in Diamonds and Defaults, edited by M. de Rijke, Studies in Logic, Language and Information,</title>
<date>1991</date>
<publisher>Kluwer.</publisher>
<marker>[Blackburn 1991]</marker>
<rawString>Blackburn, P.: 1991, Modal Logic and Attribute Value Structures. To appear in Diamonds and Defaults, edited by M. de Rijke, Studies in Logic, Language and Information, Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blackburn</author>
<author>E Spaan</author>
</authors>
<title>On the Complexity of Attribute Value Logics.</title>
<date>1991</date>
<booktitle>Proceedings of the Eighth Amsterdam Colloquium, edited</booktitle>
<institution>Philosophy Department, Amsterdam University, The Netherlands.</institution>
<marker>[Blackburn and Spaan 1991]</marker>
<rawString>Blackburn, P. and Spaan, E.: 1991, On the Complexity of Attribute Value Logics. Proceedings of the Eighth Amsterdam Colloquium, edited by P. Dekker and M. Stokhof, Philosophy Department, Amsterdam University, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blackburn</author>
<author>E Spaan</author>
</authors>
<title>A Modal Perspective on the Computational Complexity of Attribute Value Grammar.</title>
<date>1992</date>
<journal>Journal of Logic, Language and Information.</journal>
<note>To appear in</note>
<marker>[Blackburn and Spaan 1992]</marker>
<rawString>Blackburn, P. and Spaan, E.: 1992, A Modal Perspective on the Computational Complexity of Attribute Value Grammar. To appear in Journal of Logic, Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blackburn</author>
</authors>
<title>Structures, Languages and Translations: the Structural Approach to Feature Logic.</title>
<date>1992</date>
<publisher>Academic Press.</publisher>
<note>To appear in Constraints, Language and Computation, edited by</note>
<marker>[Blackburn 1992]</marker>
<rawString>Blackburn, P.: 1992, Structures, Languages and Translations: the Structural Approach to Feature Logic. To appear in Constraints, Language and Computation, edited by C. Rupp, M. Rosner and R. Johnson, Academic Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Blackburn</author>
<author>C Gardent</author>
<author>W Meyer-Viol</author>
</authors>
<title>Modal Phrase Structure Grammars. In preparation. damental correspondences involved in feature logic see</title>
<marker>[Blackburn et al. forthcoming]</marker>
<rawString>Blackburn, P., Gardent, C., and Meyer-Viol, W.: Modal Phrase Structure Grammars. In preparation. damental correspondences involved in feature logic see</rawString>
</citation>
<citation valid="false">
<marker>[Blackburn 1992]</marker>
<rawString>.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M de Rijke</author>
</authors>
<title>What is Modal Logic?</title>
<date>1992</date>
<booktitle>In Logic at Work, proceedings of the Applied Logic Conference, CCSOM,</booktitle>
<institution>University of Amsterdam,</institution>
<marker>[de Rijke 1992]</marker>
<rawString>de Rijke, M.: 1992, What is Modal Logic? In Logic at Work, proceedings of the Applied Logic Conference, CCSOM, University of Amsterdam, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Finger</author>
<author>D Gabbay</author>
</authors>
<title>Adding a Temporal Dimension to a Logic System.</title>
<date>1992</date>
<journal>Journal of Logic, Language and Information,</journal>
<volume>1</volume>
<pages>203--233</pages>
<marker>[Finger and Gabbay 1992]</marker>
<rawString>Finger, M. and Gabbay, D.: 1992, Adding a Temporal Dimension to a Logic System. Journal of Logic, Language and Information, 1, pp. 203-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<date>1979</date>
<institution>Constituent Structures. Manuscript, Sussex University.</institution>
<marker>[Gazdar 1979]</marker>
<rawString>Gazdar, G.: 1979, Constituent Structures. Manuscript, Sussex University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Klein Gazdar</author>
<author>E Pullum</author>
<author>G</author>
<author>S Sag</author>
</authors>
<title>Generalised Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Basil Blackwell.</publisher>
<marker>[Gazdar et al. 1985]</marker>
<rawString>Gazdar, G.: Klein, E., Pullum, G., and Sag, S.: 1985, Generalised Phrase Structure Grammar. Basil Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Features and Formulas,</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<pages>131--151</pages>
<marker>[Johnson 1991]</marker>
<rawString>Johnson, M.: 1991, Features and Formulas, Computational Linguistics, 17, pp. 131-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>S Levy</author>
</authors>
<title>Constraints on Structural Descriptions: Local Transformations.</title>
<date>1977</date>
<journal>SIAM Journal of Computing,</journal>
<volume>6</volume>
<pages>272--284</pages>
<marker>[Joshi and Levy 1977]</marker>
<rawString>Joshi, A. and Levy, S.: 1977, Constraints on Structural Descriptions: Local Transformations. SIAM Journal of Computing, 6, pp. 272-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kasper</author>
<author>W Rounds</author>
</authors>
<title>A logical semantics for feature structures.</title>
<date>1986</date>
<booktitle>Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>257--266</pages>
<institution>Columbia University,</institution>
<location>New York,</location>
<marker>[Kasper and Rounds 1986]</marker>
<rawString>Kasper, R. and Rounds, W.: 1986, A logical semantics for feature structures. Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, Columbia University, New York, pp. 257-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCawley</author>
</authors>
<title>Concerning the Base Component of a Transformational Grammar.</title>
<date>1968</date>
<journal>Foundations of Language,</journal>
<volume>4</volume>
<pages>55--81</pages>
<marker>[McCawley 1968]</marker>
<rawString>McCawley, J.: 1968, Concerning the Base Component of a Transformational Grammar. Foundations of Language, 4, pp. 55-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Peters</author>
<author>R Ritchie</author>
</authors>
<title>Context-Sensitive Immediate Constituent Analysis — Context-Free Languages Revisited.</title>
<date>1969</date>
<booktitle>Proceedings ACM Symposium on Theory of Computing, Association for Computing Machinery,</booktitle>
<pages>1--10</pages>
<marker>[Peters and Ritchie 1969]</marker>
<rawString>Peters, S. and Ritchie, R.: 1969, Context-Sensitive Immediate Constituent Analysis — Context-Free Languages Revisited. Proceedings ACM Symposium on Theory of Computing, Association for Computing Machinery, pp. 1— 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Rounds</author>
</authors>
<title>Tree-Oriented Proofs of Some Theorems in Context-Free and Indexed Languages.</title>
<date>1970</date>
<booktitle>Proceedings ACM Symposium on Theory of Computing, Association for Computing Machinery,</booktitle>
<pages>210--216</pages>
<marker>[Rounds 1970]</marker>
<rawString>Rounds, W.: 1970, Tree-Oriented Proofs of Some Theorems in Context-Free and Indexed Languages. Proceedings ACM Symposium on Theory of Computing, Association for Computing Machinery, pp. 210 — 216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>Separating Linguistic Analyses from Linguistic Theories. In Natural Language Parsing and Linguistic Theories, edited by U. Reyle</title>
<date>1988</date>
<location>Reidel.</location>
<marker>[Shieber 1988]</marker>
<rawString>Shieber, S.: 1988, Separating Linguistic Analyses from Linguistic Theories. In Natural Language Parsing and Linguistic Theories, edited by U. Reyle and C. Rohrer, Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J van Benthem</author>
</authors>
<title>Correspondence Theory,</title>
<date>1984</date>
<booktitle>in Handbook of Philosophical Logic,</booktitle>
<volume>2</volume>
<location>Reidel.</location>
<marker>[van Benthem 1984]</marker>
<rawString>van Benthem, J.: 1984, Correspondence Theory, in Handbook of Philosophical Logic, 2, edited by D. Gabbay and F. Guenthner, Reidel.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>