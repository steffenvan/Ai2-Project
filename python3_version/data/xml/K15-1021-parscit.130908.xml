<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029247">
<title confidence="0.994889">
Multichannel Variable-Size Convolution for Sentence Classification
</title>
<author confidence="0.992592">
Wenpeng Yin and Hinrich Sch¨utze
</author>
<affiliation confidence="0.995685">
Center for Information and Language Processing
University of Munich, Germany
</affiliation>
<email confidence="0.972221">
wenpeng@cis.uni-muenchen.de
</email>
<sectionHeader confidence="0.994254" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999738615384616">
We propose MVCNN, a convolution neu-
ral network (CNN) architecture for sen-
tence classification. It (i) combines di-
verse versions of pretrained word embed-
dings and (ii) extracts features of multi-
granular phrases with variable-size convo-
lution filters. We also show that pretrain-
ing MVCNN is critical for good perfor-
mance. MVCNN achieves state-of-the-art
performance on four tasks: on small-scale
binary, small-scale multi-class and large-
scale Twitter sentiment prediction and on
subjectivity classification.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944846153846">
Different sentence classification tasks are crucial
for many Natural Language Processing (NLP) ap-
plications. Natural language sentences have com-
plicated structures, both sequential and hierarchi-
cal, that are essential for understanding them. In
addition, how to decode and compose the features
of component units, including single words and
variable-size phrases, is central to the sentence
classification problem.
In recent years, deep learning models have
achieved remarkable results in computer vision
(Krizhevsky et al., 2012), speech recognition
(Graves et al., 2013) and NLP (Collobert and We-
ston, 2008). A problem largely specific to NLP is
how to detect features of linguistic units, how to
conduct composition over variable-size sequences
and how to use them for NLP tasks (Collobert et
al., 2011; Kalchbrenner et al., 2014; Kim, 2014).
Socher et al. (2011a) proposed recursive neural
networks to form phrases based on parsing trees.
This approach depends on the availability of a well
performing parser; for many languages and do-
mains, especially noisy domains, reliable parsing
is difficult. Hence, convolution neural networks
(CNN) are getting increasing attention, for they
are able to model long-range dependencies in sen-
tences via hierarchical structures (Dos Santos and
Gatti, 2014; Kim, 2014; Denil et al., 2014). Cur-
rent CNN systems usually implement a convolu-
tion layer with fixed-size filters (i.e., feature detec-
tors), in which the concrete filter size is a hyper-
parameter. They essentially split a sentence into
multiple sub-sentences by a sliding window, then
determine the sentence label by using the domi-
nant label across all sub-sentences. The underly-
ing assumption is that the sub-sentence with that
granularity is potentially good enough to represent
the whole sentence. However, it is hard to find the
granularity of a “good sub-sentence” that works
well across sentences. This motivates us to imple-
ment variable-size filters in a convolution layer in
order to extract features of multigranular phrases.
Breakthroughs of deep learning in NLP are also
based on learning distributed word representations
– also called “word embeddings” – by neural lan-
guage models (Bengio et al., 2003; Mnih and Hin-
ton, 2009; Mikolov et al., 2010; Mikolov, 2012;
Mikolov et al., 2013a). Word embeddings are de-
rived by projecting words from a sparse, 1-of-V
encoding (V : vocabulary size) onto a lower di-
mensional and dense vector space via hidden lay-
ers and can be interpreted as feature extractors that
encode semantic and syntactic features of words.
Many papers study the comparative perfor-
mance of different versions of word embed-
dings, usually learned by different neural net-
work (NN) architectures. For example, Chen et
al. (2013) compared HLBL (Mnih and Hinton,
2009), SENNA (Collobert and Weston, 2008),
Turian (Turian et al., 2010) and Huang (Huang
et al., 2012), showing great variance in quality
and characteristics of the semantics captured by
the tested embedding versions. Hill et al. (2014)
showed that embeddings learned by neural ma-
chine translation models outperform three repre-
</bodyText>
<page confidence="0.981139">
204
</page>
<note confidence="0.980001">
Proceedings of the 19th Conference on Computational Language Learning, pages 204–214,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999829395348837">
sentative monolingual embedding versions: skip-
gram (Mikolov et al., 2013b), GloVe (Pennington
et al., 2014) and C&amp;W (Collobert et al., 2011) in
some cases. These prior studies motivate us to ex-
plore combining multiple versions of word embed-
dings, treating each of them as a distinct descrip-
tion of words. Our expectation is that the com-
bination of these embedding versions, trained by
different NNs on different corpora, should contain
more information than each version individually.
We want to leverage this diversity of different em-
bedding versions to extract higher quality sentence
features and thereby improve sentence classifica-
tion performance.
The letters “M” and “V” in the name
“MVCNN” of our architecture denote the multi-
channel and variable-size convolution filters, re-
spectively. “Multichannel” employs language
from computer vision where a color image has red,
green and blue channels. Here, a channel is a de-
scription by an embedding version.
For many sentence classification tasks, only rel-
atively small training sets are available. MVCNN
has a large number of parameters, so that overfit-
ting is a danger when they are trained on small
training sets. We address this problem by pre-
training MVCNN on unlabeled data. These pre-
trained weights can then be fine-tuned for the spe-
cific classification task.
In sum, we attribute the success of MVCNN
to: (i) designing variable-size convolution filters
to extract variable-range features of sentences and
(ii) exploring the combination of multiple pub-
lic embedding versions to initialize words in sen-
tences. We also employ two “tricks” to further en-
hance system performance: mutual learning and
pretraining.
In remaining parts, Section 2 presents related
work. Section 3 gives details of our classification
model. Section 4 introduces two tricks that en-
hance system performance: mutual-learning and
pretraining. Section 5 reports experimental re-
sults. Section 6 concludes this work.
</bodyText>
<sectionHeader confidence="0.999807" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996508228070176">
Much prior work has exploited deep neural net-
works to model sentences.
Blacoe and Lapata (2012) represented a sen-
tence by element-wise addition, multiplication, or
recursive autoencoder over embeddings of com-
ponent single words. Yin and Sch¨utze (2014) ex-
tended this approach by composing on words and
phrases instead of only single words.
Collobert and Weston (2008) and Yu et al.
(2014) used one layer of convolution over phrases
detected by a sliding window on a target sentence,
then used max- or average-pooling to form a sen-
tence representation.
Kalchbrenner et al. (2014) stacked multiple lay-
ers of one-dimensional convolution by dynamic k-
max pooling to model sentences. We also adopt
dynamic k-max pooling while our convolution
layer has variable-size filters.
Kim (2014) also studied multichannel repre-
sentation and variable-size filters. Differently,
their multichannel relies on a single version of
pretrained embeddings (i.e., pretrained Word2Vec
embeddings) with two copies: one is kept stable
and the other one is fine-tuned by backpropaga-
tion. We develop this insight by incorporating di-
verse embedding versions. Additionally, their idea
of variable-size filters is further developed.
Le and Mikolov (2014) initialized the represen-
tation of a sentence as a parameter vector, treat-
ing it as a global feature and combining this vec-
tor with the representations of context words to
do word prediction. Finally, this fine-tuned vec-
tor is used as representation of this sentence. Ap-
parently, this method can only produce generic
sentence representations which encode no task-
specific features.
Our work is also inspired by studies that com-
pared the performance of different word embed-
ding versions or investigated the combination of
them. For example, Turian et al. (2010) compared
Brown clusters, C&amp;W embeddings and HLBL em-
beddings in NER and chunking tasks. They found
that Brown clusters and word embeddings both
can improve the accuracy of supervised NLP sys-
tems; and demonstrated empirically that combin-
ing different word representations is beneficial.
Luo et al. (2014) adapted CBOW (Mikolov et
al., 2013a) to train word embeddings on differ-
ent datasets: free text documents from Wikipedia,
search click-through data and user query data,
showing that combining them gets stronger results
than using individual word embeddings in web
search ranking and word similarity task. How-
ever, these two papers either learned word repre-
sentations on the same corpus (Turian et al., 2010)
or enhanced the embedding quality by extending
training corpora, not learning algorithms (Luo et
</bodyText>
<page confidence="0.997741">
205
</page>
<bodyText confidence="0.999653">
al., 2014). In our work, there is no limit to the
type of embedding versions we can use and they
leverage not only the diversity of corpora, but also
the different principles of learning algorithms.
</bodyText>
<sectionHeader confidence="0.962887" genericHeader="method">
3 Model Description
</sectionHeader>
<bodyText confidence="0.999695466666666">
We now describe the architecture of our model
MVCNN, illustrated in Figure 1.
Multichannel Input. The input of MVCNN in-
cludes multichannel feature maps of a considered
sentence, each is a matrix initialized by a differ-
ent embedding version. Let s be sentence length,
d dimension of word embeddings and c the to-
tal number of different embedding versions (i.e.,
channels). Hence, the whole initialized input is a
three-dimensional array of size c x d x s. Figure 1
depicts a sentence with s = 12 words. Each word
is initialized by c = 5 embeddings, each com-
ing from a different channel. In implementation,
sentences in a mini-batch will be padded to the
same length, and unknown words for correspond-
ing channel are randomly initialized or can acquire
good initialization from the mutual-learning phase
described in next section.
Multichannel initialization brings two advan-
tages: 1) a frequent word can have c representa-
tions in the beginning (instead of only one), which
means it has more available information to lever-
age; 2) a rare word missed in some embedding
versions can be “made up” by others (we call it
“partially known word”). Therefore, this kind of
initialization is able to make use of information
about partially known words, without having to
employ full random initialization or removal of
unknown words. The vocabulary of the binary
sentiment prediction task described in experimen-
tal part contains 5232 words unknown in HLBL
embeddings, 4273 in Huang embeddings, 3299 in
GloVe embeddings, 4136 in SENNA embeddings
and 2257 in Word2Vec embeddings. But only
1824 words find no embedding from any chan-
nel! Hence, multichannel initialization can con-
siderably reduce the number of unknown words.
Convolution Layer (Conv). For convenience,
we first introduce how this work uses a convo-
lution layer on one input feature map to gener-
ate one higher-level feature map. Given a sen-
tence of length s: w1, w2, ... , ws; wi E Rd de-
notes the embedding of word wi; a convolution
layer uses sliding filters to extract local features
of that sentence. The filter width l is a param-
</bodyText>
<figureCaption confidence="0.9373375">
Figure 1: MVCNN: supervised classification and
pretraining.
</figureCaption>
<bodyText confidence="0.993571142857143">
eter. We first concatenate the initialized embed-
dings of l consecutive words (wi−l+1, ... , wi) as
ci E Rld (1 &lt; i &lt; s + l), then generate the fea-
ture value of this phrase as pi (the whole vector
p E Rs+l−1 contains all the local features) using
a tanh activation function and a linear projection
vector v E Rld as:
</bodyText>
<equation confidence="0.986843">
pi = tanh(vTci + b) (1)
</equation>
<bodyText confidence="0.999714818181818">
More generally, convolution operation can deal
with multiple input feature maps and can be
stacked to yield feature maps of increasing layers.
In each layer, there are usually multiple filters of
the same size, but with different weights (Kalch-
brenner et al., 2014). We refer to a filter with a
specific set of weights as a kernel. The goal is
often to train a model in which different kernels
detect different kinds of features of a local region.
However, this traditional way can not detect the
features of regions of different granularity. Hence
</bodyText>
<page confidence="0.992143">
206
</page>
<bodyText confidence="0.994592625">
we keep the property of multi-kernel while extend-
ing it to variable-size in the same layer.
As in CNN for object recognition, to increase
the number of kernels of a certain layer, multiple
feature maps may be computed in parallel at the
same layer. Further, to increase the size diversity
of kernels in the same layer, more feature maps
containing various-range dependency features can
be learned. We denote a feature map of the ith
layer by Fi, and assume totally n feature maps ex-
ist in layer i − 1: F1 i−1, ... , Fni−1. Considering
a specific filter size l in layer i, each feature map
Fji,l is computed by convolving a distinct set of fil-
ters of size l, arranged in a matrix Vj,k
i,l , with each
feature map Fki−1 and summing the results:
</bodyText>
<equation confidence="0.977526333333333">
Vj,k
i,l * Fk (2)
i−1
</equation>
<bodyText confidence="0.990305195652174">
where * indicates the convolution operation and
j is the index of a feature map in layer i. The
weights in V form a rank 4 tensor.
Note that we use wide convolution in this work:
it means word representations wg for g G 0 or
g &gt; s+1 are actually zero embeddings. Wide con-
volution enables that each word can be detected by
all filter weights in V.
In Figure 1, the first convolution layer deals
with an input with n = 5 feature maps.1 Its filters
have sizes 3 and 5 respectively (i.e., l = 3, 5), and
each filter has j = 3 kernels. This means this con-
volution layer can detect three kinds of features of
phrases with length 3 and 5, respectively.
DCNN in (Kalchbrenner et al., 2014) used one-
dimensional convolution: each higher-order fea-
ture is produced from values of a single dimen-
sion in the lower-layer feature map. Even though
that work proposed folding operation to model
the dependencies between adjacent dimensions,
this type of dependency modeling is still lim-
ited. Differently, convolution in present work is
able to model dependency across dimensions as
well as adjacent words, which obviates the need
for a folding step. This change also means our
model has substantially fewer parameters than the
DCNN since the output of each convolution layer
is smaller by a factor of d.
1A reviewer expresses surprise at such a small number of
maps. However, we will use four variable sizes (see below),
so that the overall number of maps is 20. We use a small
number of maps partly because training times for a network
are on the order of days, so limiting the number of parameters
is important.
Dynamic k-max Pooling. Kalchbrenner et al.
(2014) pool the k most active features compared
with simple max (1-max) pooling (Collobert and
Weston, 2008). This property enables it to con-
nect multiple convolution layers to form a deep
architecture to extract high-level abstract features.
In this work, we directly use it to extract features
for variable-size feature maps. For a given feature
map in layer i, dynamic k-max pooling extracts ki
top values from each dimension and ktop top val-
ues in the top layer. We set
—
</bodyText>
<equation confidence="0.9895315">
ki = max(kto L i
p, r L sI) (3)
</equation>
<bodyText confidence="0.999908789473684">
where i E 11, 2,... L} is the order of convolution
layer from bottom to top in Figure 1; L is the total
numbers of convolution layers; ktop is a constant
determined empirically, we set it to 4 as (Kalch-
brenner et al., 2014).
As a result, the second convolution layer in Fig-
ure 1 has an input with two same-size feature
maps, one results from filter size 3, one from filter
size 5. The values in the two feature maps are for
phrases with different granularity. The motivation
of this convolution layer lies in that a feature re-
flected by a short phrase may be not trustworthy
while the longer phrase containing the short one is
trustworthy, or the long phrase has no trustworthy
feature while its component short phrase is more
reliable. This and even higher-order convolution
layers therefore can make a trade-off between the
features of different granularity.
Hidden Layer. On the top of the final k-
max pooling, we stack a fully connected layer to
learn sentence representation with given dimen-
sion (e.g., d).
Logistic Regression Layer. Finally, sentence
representation is forwarded into logistic regression
layer for classification.
In brief, our MVCNN model learns from
(Kalchbrenner et al., 2014) to use dynamic k-
max pooling to stack multiple convolution layers,
and gets insight from (Kim, 2014) to investigate
variable-size filters in a convolution layer. Com-
pared to (Kalchbrenner et al., 2014), MVCNN
has rich feature maps as input and as output of
each convolution layer. Its convolution opera-
tion is not only more flexible to extract features
of variable-range phrases, but also able to model
dependency among all dimensions of representa-
tions. MVCNN extends the network in (Kim,
2014) by hierarchical convolution architecture and
</bodyText>
<equation confidence="0.982682666666667">
n
Fji,l =
k=1
</equation>
<page confidence="0.973309">
207
</page>
<bodyText confidence="0.999068">
further exploration of multichannel and variable-
size feature detectors.
</bodyText>
<sectionHeader confidence="0.945411" genericHeader="method">
4 Model Enhancements
</sectionHeader>
<bodyText confidence="0.998747363636364">
This part introduces two training tricks that en-
hance the performance of MVCNN in practice.
Mutual-Learning of Embedding Versions.
One observation in using multiple embedding ver-
sions is that they have different vocabulary cover-
age. An unknown word in an embedding version
may be a known word in another version. Thus,
there exists a proportion of words that can only
be partially initialized by certain versions of word
embeddings, which means these words lack the
description from other versions.
To alleviate this problem, we design a mutual-
learning regime to predict representations of un-
known words for each embedding version by
learning projections between versions. Asa result,
all embedding versions have the same vocabulary.
This processing ensures that more words in each
embedding version receive a good representation,
and is expected to give most words occurring in a
classification dataset more comprehensive initial-
ization (as opposed to just being randomly initial-
ized).
</bodyText>
<equation confidence="0.869622">
Let c be the number of embedding versions in
consideration, V1, V2,. .. , Vi, ... , Vc their vocab-
ularies, V * = Uci=1Vi their union, and V −
i =
V *\Vi (i = 1,... , c) the vocabulary of unknown
</equation>
<bodyText confidence="0.998487571428572">
words for embedding version i. Our goal is to
learn embeddings for the words in V −
i by knowl-
edge from the other c − 1 embedding versions.
We use the overlapping vocabulary between Vi
and Vj, denoted as Vij, as training set, formalizing
a projection fij from space Vi to space Vj (i =�
</bodyText>
<equation confidence="0.908679">
j; i, j E 11, 2, ... , c}) as follows:
ˆwj = Mijwi (4)
</equation>
<bodyText confidence="0.99790346031746">
where Mij E Rdxd, wi E Rd denotes the rep-
resentation of word w in space Vi and ˆwj is the
projected (or learned) representation of word w in
space Vj. Squared error between wj and ˆwj is the
training loss to minimize. We use ˆwj = fij(wi)
to reformat Equation 4. Totally c(c − 1)/2 projec-
tions fij are trained, each on the vocabulary inter-
section Vij.
Let w be a word that is unknown in Vi, but is
known in V1, V2,..., Vk. To compute an embed-
ding for w in Vi, we first compute the k projections
f1i(w1), f2i(w2), ..., fki(wk) from the source
spaces V1, V2, ... , Vk to the target space Vi. Then,
the element-wise average of f1i(w1), f2i(w2), . . .,
fki(wk) is treated as the representation of w in Vi.
Our motivation is that – assuming there is a true
representation of w in Vi (e.g., the one we would
have obtained by training embeddings on a much
larger corpus) and assuming the projections were
learned well – we would expect all the projected
vectors to be close to the true representation. Also,
each source space contributes potentially comple-
mentary information. Hence averaging them is a
balance of knowledge from all source spaces.
As discussed in Section 3, we found that for
the binary sentiment classification dataset, many
words were unknown in at least one embedding
version. But of these words, a total of 5022 words
did have coverage in another embedding version
and so will benefit from mutual-learning. In the
experiments, we will show that this is a very ef-
fective method to learn representations for un-
known words that increases system performance if
learned representations are used for initialization.
Pretraining. Sentence classification systems
are usually implemented as supervised training
regimes where training loss is between true la-
bel distribution and predicted label distribution. In
this work, we use pretraining on the unlabeled data
of each task and show that it can increase the per-
formance of classification systems.
Figure 1 shows our pretraining setup. The
“sentence representation” – the output of “Fully
connected” hidden layer – is used to predict the
component words (“on” in the figure) in the sen-
tence (instead of predicting the sentence label Y/N
as in supervised learning). Concretely, the sen-
tence representation is averaged with representa-
tions of some surrounding words (“the”, “cat”,
“sat”, “the”, “mat”, “,” in the figure) to predict the
middle word (“on”).
Given sentence representation s E Rd and ini-
tialized representations of 2t context words (t left
words and t right words): wi−t, ..., wi−1, wi+1,
. . ., wi+t; wi E Rd, we average the total 2t + 1
vectors element-wise, depicted as “Average” op-
eration in Figure 1. Then, this resulting vector is
treated as a predicted representation of the mid-
dle word and is used to find the true middle word
by means of noise-contrastive estimation (NCE)
(Mnih and Teh, 2012). For each true example, 10
noise words are sampled.
Note that in pretraining, there are three places
</bodyText>
<page confidence="0.992093">
208
</page>
<bodyText confidence="0.999974392857143">
where each word needs initialization. (i) Each
word in the sentence is initialized in the “Multi-
channel input” layer to the whole network. (ii)
Each context word is initialized as input to the av-
erage layer (“Average” in the figure). (iii) Each tar-
get word is initialized as the output of the “NCE”
layer (“on” in the figure). In this work, we use
multichannel initialization for case (i) and random
initialization for cases (ii) and (iii). Only fine-
tuned multichannel representations (case (i)) are
kept for subsequent supervised training.
The rationale for this pretraining is similar
to auto-encoder: for an object composed of
smaller-granular elements, the representations of
the whole object and its components can learn
each other. The CNN architecture learns sentence
features layer by layer, then those features are jus-
tified by all constituent words.
During pretraining, all the model parameters,
including mutichannel input, convolution parame-
ters and fully connected layer, will be updated un-
til they are mature to extract the sentence features.
Subsequently, the same sets of parameters will be
fine-tuned for supervised classification tasks.
In sum, this pretraining is designed to produce
good initial values for both model parameters and
word embeddings. It is especially helpful for pre-
training the embeddings of unknown words.
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99999125">
We test the network on four classification tasks.
We begin by specifying aspects of the implemen-
tation and the training of the network. We then
report the results of the experiments.
</bodyText>
<subsectionHeader confidence="0.990016">
5.1 Hyperparameters and Training
</subsectionHeader>
<bodyText confidence="0.999964631578947">
In each of the experiments, the top of the net-
work is a logistic regression that predicts the
probability distribution over classes given the in-
put sentence. The network is trained to mini-
mize cross-entropy of predicted and true distri-
butions; the objective includes an L2 regulariza-
tion term over the parameters. The set of param-
eters comprises the word embeddings, all filter
weights and the weights in fully connected layers.
A dropout operation (Hinton et al., 2012) is put be-
fore the logistic regression layer. The network is
trained by back-propagation in mini-batches and
the gradient-based optimization is performed us-
ing the AdaGrad update rule (Duchi et al., 2011)
In all data sets, the initial learning rate is 0.01,
dropout probability is 0.8, L2 weight is 5 · 10−3,
batch size is 50. In each convolution layer, filter
sizes are {3, 5, 7, 9} and each filter has five kernels
(independent of filter size).
</bodyText>
<subsectionHeader confidence="0.990948">
5.2 Datasets and Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999910216216216">
Standard Sentiment Treebank (Socher et al.,
2013). This small-scale dataset includes two tasks
predicting the sentiment of movie reviews. The
output variable is binary in one experiment and
can have five possible outcomes in the other:
{negative, somewhat negative, neutral, somewhat
positive, positive}. In the binary case, we use
the given split of 6920 training, 872 development
and 1821 test sentences. Likewise, in the fine-
grained case, we use the standard 8544/1101/2210
split. Socher et al. (2013) used the Stanford Parser
(Klein and Manning, 2003) to parse each sentence
into subphrases. The subphrases were then labeled
by human annotators in the same way as the sen-
tences were labeled. Labeled phrases that occur
as subparts of the training sentences are treated
as independent training instances as in (Le and
Mikolov, 2014; Kalchbrenner et al., 2014).
Sentiment1402 (Go et al., 2009). This is a
large-scale dataset of tweets about sentiment clas-
sification, where a tweet is automatically labeled
as positive or negative depending on the emoticon
that occurs in it. The training set consists of 1.6
million tweets with emoticon-based labels and the
test set of about 400 hand-annotated tweets. We
preprocess the tweets minimally as follows. 1)
The equivalence class symbol “url” (resp. “user-
name”) replaces all URLs (resp. all words that
start with the @ symbol, e.g., @thomasss). 2) A
sequence of k &gt; 2 repetitions of a letter c (e.g.,
“cooooooool”) is replaced by two occurrences of
c (e.g., “cool”). 3) All tokens are lowercased.
Subj. Subjectivity classification dataset3 re-
leased by (Pang and Lee, 2004) has 5000 sub-
jective sentences and 5000 objective sentences.
We report the result of 10-fold cross validation as
baseline systems did.
</bodyText>
<subsectionHeader confidence="0.614711">
5.2.1 Pretrained Word Vectors
</subsectionHeader>
<bodyText confidence="0.999723">
In this work, we use five embedding versions, as
shown in Table 1, to initialize words. Four of
them are directly downloaded from the Internet.
</bodyText>
<footnote confidence="0.986619">
2http://help.sentiment140.com/for-students
3http://www.cs.cornell.edu/people/pabo/movie-review-
data/
</footnote>
<page confidence="0.994413">
209
</page>
<table confidence="0.997662333333333">
Set Training Data Vocab Size Dimensionality Source
HLBL Reuters English newswire 246,122 50 download
Huang Wikipedia (April 2010 snapshot) 100,232 50 download
Glove Twitter 1,193,514 50 download
SENNA Wikipedia 130,000 50 download
Word2Vec English Gigawords 418,129 50 trained from scratch
</table>
<tableCaption confidence="0.994923">
Table 1: Description of five versions of word embedding.
</tableCaption>
<table confidence="0.9991362">
Binary Fine-grained Senti140 Subj
HLBL 5,232 5,562 344,632 8,621
Huang 4,273 4,523 327,067 6,382
Glove 3,299 3,485 257,376 5,237
SENNA 4,136 4,371 323,501 6,162
W2V 2257 2,409 288,257 4,217
Voc size 18,876 19,612 387,877 23,926
Full hit 12,030 12,357 30,010 13,742
Partial hit 5,022 5,312 121,383 6,580
No hit 1,824 1,943 236,484 3,604
</table>
<tableCaption confidence="0.989774">
Table 2: Statistics of five embedding versions for
</tableCaption>
<bodyText confidence="0.99566625">
four tasks. The first block with five rows provides
the number of unknown words of each task when
using corresponding version to initialize. Voc size:
vocabulary size. Full hit: embedding in all 5 ver-
sions. Partial hit: embedding in 1–4 versions, No
hit: not present in any of the 5 versions.
(i) HLBL. Hierarchical log-bilinear model pre-
sented by Mnih and Hinton (2009) and released
by Turian et al. (2010);4 size: 246,122 word em-
beddings; training corpus: RCV1 corpus, one year
of Reuters English newswire from August 1996 to
August 1997. (ii) Huang.5 Huang et al. (2012) in-
corporated global context to deal with challenges
raised by words with multiple meanings; size:
100,232 word embeddings; training corpus: April
2010 snapshot of Wikipedia. (iii) GloVe.6 Size:
1,193,514 word embeddings; training corpus: a
Twitter corpus of 2B tweets with 27B tokens. (iv)
SENNA.7 Size: 130,000 word embeddings; train-
ing corpus: Wikipedia. Note that we use their 50-
dimensional embeddings. (v) Word2Vec. It has
no 50-dimensional embeddings available online.
We use released code8 to train skip-gram on En-
glish Gigaword Corpus (Parker et al., 2009) with
</bodyText>
<footnote confidence="0.999929">
4http://metaoptimize.com/projects/wordreprs/
5http://ai.stanford.edu/ ehhuang/
6http://nlp.stanford.edu/projects/glove/
7http://ml.nec-labs.com/senna/
8http://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.999866958333333">
setup: window size 5, negative sampling, sam-
pling rate 10−3, threads 12. It is worth empha-
sizing that above embeddings sets are derived on
different corpora with different algorithms. This is
the very property that we want to make use of to
promote the system performance.
Table 2 shows the number of unknown words
in each task when using corresponding embed-
ding version to initialize (rows “HLBL”, “Huang”,
“Glove”, “SENNA”, “W2V”) and the number of
words fully initialized by five embedding versions
(“Full hit” row), the number of words partially
initialized (“Partial hit” row) and the number of
words that cannot be initialized by any of the em-
bedding versions (“No hit” row).
About 30% of words in each task have partially
initialized embeddings and our mutual-learning is
able to initialize the missing embeddings through
projections. Pretraining is expected to learn good
representations for all words, but pretraining is es-
pecially important for words without initialization
(“no hit”); a particularly clear example for this is
the Senti140 task: 236,484 of 387,877 words or
61% are in the “no hit” category.
</bodyText>
<subsubsectionHeader confidence="0.568768">
5.2.2 Results and Analysis
</subsubsectionHeader>
<bodyText confidence="0.9998833125">
Table 3 compares results on test of MVCNN and
its variants with other baselines in the four sen-
tence classification tasks. Row 34, “MVCNN
(overall)”, shows performance of the best config-
uration of MVCNN, optimized on dev. This ver-
sion uses five versions of word embeddings, four
filter sizes (3, 5, 7, 9), both mutual-learning and
pretraining, three convolution layers for Senti140
task and two convolution layers for the other tasks.
Overall, our system gets the best results, beating
all baselines.
The table contains five blocks from top to bot-
tom. Each block investigates one specific config-
urational aspect of the system. All results in the
five blocks are with respect to row 34, “MVCNN
(overall)”; e.g., row 19 shows what happens when
</bodyText>
<page confidence="0.994015">
210
</page>
<table confidence="0.999434538461539">
Model Binary Fine-grained Senti140 Subj
1 RAE (Socher et al., 2011b) 82.4 43.2 – –
2 MV-RNN (Socher et al., 2012) 82.9 44.4 – –
3 RNTN (Socher et al., 2013) 85.4 45.7 – –
4 DCNN (Kalchbrenner et al., 2014) 86.8 48.5 87.4 –
5 Paragraph-Vec (Le and Mikolov, 2014) 87.7 48.7 – –
baselines 6 CNN-rand (Kim, 2014) 82.7 45.0 – 89.6
7 CNN-static (Kim, 2014) 86.8 45.5 – 93.0
8 CNN-non-static (Kim, 2014) 87.2 48.0 – 93.4
9 CNN-multichannel (Kim, 2014) 88.1 47.4 – 93.2
10 NBSVM (Wang and Manning, 2012) – – – 93.2
11 MNB (Wang and Manning, 2012) – – – 93.6
12 G-Dropout (Wang and Manning, 2013) – – – 93.4
13 F-Dropout (Wang and Manning, 2013) – – – 93.6
14 SVM (Go et al., 2009) – – 81.6 –
15 BINB (Go et al., 2009) – – 82.7 –
16 MAX-TDNN (Kalchbrenner et al., 2014) – – 78.8 –
17 NBOW (Kalchbrenner et al., 2014) – – 80.9 –
18 MAXENT (Go et al., 2009) – – 83.0 –
19 MVCNN (-HLBL) 88.5 48.7 88.0 93.6
20 MVCNN (-Huang) 89.2 49.2 88.1 93.7
versions 21 MVCNN (-Glove) 88.3 48.6 87.4 93.6
22 MVCNN (-SENNA) 89.3 49.1 87.9 93.4
23 MVCNN (-Word2Vec) 88.4 48.2 87.6 93.4
24 MVCNN (-3) 89.1 49.2 88.0 93.6
25 MVCNN (-5) 88.7 49.0 87.5 93.4
filters MVCNN (-7) 87.8 48.9 87.5 93.1
26
27 MVCNN (-9) 88.6 49.2 87.8 93.3
28 MVCNN (-mutual-learning) 88.2 49.2 87.8 93.5
tricks MVCNN (-pretraining) 87.6 48.9 87.6 93.2
29
30 MVCNN (1) 89.0 49.3 86.8 93.8
31 MVCNN (2) 89.4 49.6 87.6 93.9
layers MVCNN (3) 88.2
32
88.6 48.6 93.1
33 MVCNN (4) 87.9 48.2 88.0 92.4
34 MVCNN (overall) 89.4 49.6 88.2 93.9
</table>
<tableCaption confidence="0.999102">
Table 3: Test set results of our CNN model against other methods. RAE: Recursive Autoencoders
</tableCaption>
<bodyText confidence="0.9591406875">
with pretrained word embeddings from Wikipedia (Socher et al., 2011b). MV-RNN: Matrix-Vector
Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Net-
work with tensor-based feature function and parse trees (Socher et al., 2013). DCNN, MAX-TDNN,
NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with
Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014).
Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB,
MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maxi-
mum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with
uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with
word embeddings randomly initialized / initialized by pretrained vectors and kept static during training
/ initialized with two copies (each is a “channel”) of pretrained embeddings / initialized with pretrained
embeddings while fine-tuned during training (Kim, 2014). G-Dropout, F-Dropout: Gaussian Dropout
and Fast Dropout from Wang and Manning (2013). Minus sign “-” in MVCNN (-Huang) etc. means
“Huang” is not used. “versions / filters / tricks / layers” denote the MVCNN variants with differ-
ent setups: discard certain embedding version / discard certain filter size / discard mutual-learning or
pretraining / different numbers of convolution layer.
</bodyText>
<page confidence="0.996167">
211
</page>
<bodyText confidence="0.999952729166667">
HLBL is removed from row 34, row 28 shows
what happens when mutual learning is removed
from row 34 etc.
The block “baselines” (1–18) lists some sys-
tems representative of previous work on the cor-
responding datasets, including the state-of-the-art
systems (marked as italic). The block “versions”
(19–23) shows the results of our system when one
of the embedding versions was not used during
training. We want to explore to what extend dif-
ferent embedding versions contribute to perfor-
mance. The block “filters” (24–27) gives the re-
sults when individual filter width is discarded. It
also tells us how much a filter with specific size
influences. The block “tricks” (28–29) shows the
system performance when no mutual-learning or
no pretraining is used. The block “layers” (30–33)
demonstrates how the system performs when it has
different numbers of convolution layers.
From the “layers” block, we can see that our
system performs best with two layers of convo-
lution in Standard Sentiment Treebank and Sub-
jectivity Classification tasks (row 31), but with
three layers of convolution in Sentiment140 (row
32). This is probably due to Sentiment140 being a
much larger dataset; in such a case deeper neural
networks are beneficial.
The block “tricks” demonstrates the effect of
mutual-learning and pretraining. Apparently, pre-
training has a bigger impact on performance than
mutual-learning. We speculate that it is be-
cause pretraining can influence more words and all
learned word embeddings are tuned on the dataset
after pretraining.
The block “filters” indicates the contribution of
each filter size. The system benefits from filters
of each size. Sizes 5 and 7 are most important for
high performance, especially 7 (rows 25 and 26).
In the block “versions”, we see that each em-
bedding version is crucial for good performance:
performance drops in every single case. Though it
is not easy to compare fairly different embedding
versions in NLP tasks, especially when those em-
beddings were trained on different corpora of dif-
ferent sizes using different algorithms, our results
are potentially instructive for researchers making
decision on which embeddings to use for their own
tasks.
</bodyText>
<sectionHeader confidence="0.998211" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998303">
This work presented MVCNN, a novel CNN ar-
chitecture for sentence classification. It com-
bines multichannel initialization – diverse ver-
sions of pretrained word embeddings are used –
and variable-size filters – features of multigranu-
lar phrases are extracted with variable-size convo-
lution filters. We demonstrated that multichannel
initialization and variable-size filters enhance sys-
tem performance on sentiment classification and
subjectivity classification tasks.
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999970148148148">
As pointed out by the reviewers the success of the
multichannel approach is likely due to a combina-
tion of several quite different effects.
First, there is the effect of the embedding learn-
ing algorithm. These algorithms differ in many as-
pects, including in sensitivity to word order (e.g.,
SENNA: yes, word2vec: no), in objective func-
tion and in their treatment of ambiguity (explicitly
modeled only by Huang et al. (2012).
Second, there is the effect of the corpus. We
would expect the size and genre of the corpus to
have a big effect even though we did not analyze
this effect in this paper.
Third, complementarity of word embeddings is
likely to be more useful for some tasks than for
others. Sentiment is a good application for com-
plementary word embeddings because solving this
task requires drawing on heterogeneous sources
of information, including syntax, semantics and
genre as well as the core polarity of a word. Other
tasks like part of speech (POS) tagging may bene-
fit less from heterogeneity since the benefit of em-
beddings in POS often comes down to making a
correct choice between two alternatives – a single
embedding version may be sufficient for this.
We plan to pursue these questions in future
work.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.991902166666667">
Thanks to CIS members and anonymous re-
viewers for constructive comments. This work
was supported by Baidu (through a Baidu
scholarship awarded to Wenpeng Yin) and by
Deutsche Forschungsgemeinschaft (grant DFG
SCHU 2246/8-2, SPP 1335).
</bodyText>
<page confidence="0.996859">
212
</page>
<sectionHeader confidence="0.983405" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999461293577981">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546–556. Association for Compu-
tational Linguistics.
Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, and
Steven Skiena. 2013. The expressive power of word
embeddings. In ICML Workshop on Deep Learning
for Audio, Speech, and Language Processing.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil
Blunsom, and Nando de Freitas. 2014. Modelling,
visualising and summarising documents with a sin-
gle convolutional neural network. arXiv preprint
arXiv:1406.3830.
Cıcero Nogueira Dos Santos and Maıra Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of the 25th In-
ternational Conference on Computational Linguis-
tics.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1–12.
Alex Graves, A-R Mohamed, and Geoffrey Hinton.
2013. Speech recognition with deep recurrent neural
networks. In Acoustics, Speech and Signal Process-
ing, 2013 IEEE International Conference on, pages
6645–6649. IEEE.
Felix Hill, KyungHyun Cho, Sebastien Jean, Coline
Devin, and Yoshua Bengio. 2014. Not all neural
embeddings are born equal. In NIPS Workshop on
Learning Semantics.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics. Association for Computational
Linguistics.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, October.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.
Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of the 31st international conference on Ma-
chine learning.
Yong Luo, Jian Tang, Jun Yan, Chao Xu, and Zheng
Chen. 2014. Pre-trained multi-view word embed-
ding using two-side neural network. In Twenty-
Eighth AAAI Conference on Artificial Intelligence.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010,
pages 1045–1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Tomas Mikolov. 2012. Statistical language models
based on neural networks. Presentation at Google,
Mountain View, 2nd April.
</reference>
<page confidence="0.989971">
213
</page>
<reference confidence="0.996229246575342">
Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081–1088.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of the 29th In-
ternational Conference on Machine Learning, pages
1751–1758.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd annual meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.
Robert Parker, Linguistic Data Consortium, et al.
2009. English gigaword fourth edition. Linguistic
Data Consortium.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. Proceedings of the Empiricial Meth-
ods in Natural Language Processing, 12.
Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011a. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural In-
formation Processing Systems, pages 801–809.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161. Association for
Computational Linguistics.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. Association for Computational Linguis-
tics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the conference on em-
pirical methods in natural language processing, vol-
ume 1631, page 1642. Citeseer.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Short Papers-Volume 2, pages 90–94. As-
sociation for Computational Linguistics.
Sida Wang and Christopher Manning. 2013. Fast
dropout training. In Proceedings of the 30th In-
ternational Conference on Machine Learning, pages
118–126.
Wenpeng Yin and Hinrich Sch¨utze. 2014. An explo-
ration of embeddings for generalized phrases. Pro-
ceedings of the 52nd annual meeting of the associ-
ation for computational linguistics, student research
workshop, pages 41–47.
Lei Yu, Karl Moritz Hermann, Phil Blunsom, and
Stephen Pulman. 2014. Deep learning for answer
sentence selection. NIPS deep learning workshop.
</reference>
<page confidence="0.998952">
214
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.939809">
<title confidence="0.99973">Multichannel Variable-Size Convolution for Sentence Classification</title>
<author confidence="0.982658">Yin</author>
<affiliation confidence="0.999252">Center for Information and Language University of Munich,</affiliation>
<email confidence="0.997963">wenpeng@cis.uni-muenchen.de</email>
<abstract confidence="0.997036785714286">propose a convolution neural network (CNN) architecture for sentence classification. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convofilters. We also show that pretrainis critical for good performance. MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and largescale Twitter sentiment prediction and on subjectivity classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2946" citStr="Bengio et al., 2003" startWordPosition="432" endWordPosition="435"> the sentence label by using the dominant label across all sub-sentences. The underlying assumption is that the sub-sentence with that granularity is potentially good enough to represent the whole sentence. However, it is hard to find the granularity of a “good sub-sentence” that works well across sentences. This motivates us to implement variable-size filters in a convolution layer in order to extract features of multigranular phrases. Breakthroughs of deep learning in NLP are also based on learning distributed word representations – also called “word embeddings” – by neural language models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a). Word embeddings are derived by projecting words from a sparse, 1-of-V encoding (V : vocabulary size) onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words. Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and West</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6066" citStr="Blacoe and Lapata (2012)" startWordPosition="918" endWordPosition="921">ble-range features of sentences and (ii) exploring the combination of multiple public embedding versions to initialize words in sentences. We also employ two “tricks” to further enhance system performance: mutual learning and pretraining. In remaining parts, Section 2 presents related work. Section 3 gives details of our classification model. Section 4 introduces two tricks that enhance system performance: mutual-learning and pretraining. Section 5 reports experimental results. Section 6 concludes this work. 2 Related Work Much prior work has exploited deep neural networks to model sentences. Blacoe and Lapata (2012) represented a sentence by element-wise addition, multiplication, or recursive autoencoder over embeddings of component single words. Yin and Sch¨utze (2014) extended this approach by composing on words and phrases instead of only single words. Collobert and Weston (2008) and Yu et al. (2014) used one layer of convolution over phrases detected by a sliding window on a target sentence, then used max- or average-pooling to form a sentence representation. Kalchbrenner et al. (2014) stacked multiple layers of one-dimensional convolution by dynamic kmax pooling to model sentences. We also adopt dyn</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanqing Chen</author>
<author>Bryan Perozzi</author>
<author>Rami Al-Rfou</author>
<author>Steven Skiena</author>
</authors>
<title>The expressive power of word embeddings.</title>
<date>2013</date>
<booktitle>In ICML Workshop on Deep Learning for Audio, Speech, and Language Processing.</booktitle>
<contexts>
<context position="3481" citStr="Chen et al. (2013)" startWordPosition="521" endWordPosition="524">ns – also called “word embeddings” – by neural language models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a). Word embeddings are derived by projecting words from a sparse, 1-of-V encoding (V : vocabulary size) onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words. Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. Hill et al. (2014) showed that embeddings learned by neural machine translation models outperform three repre204 Proceedings of the 19th Conference on Computational Language Learning, pages 204–214, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics sentative monolingual embedding versions: skipgram (Mikolov et al.,</context>
</contexts>
<marker>Chen, Perozzi, Al-Rfou, Skiena, 2013</marker>
<rawString>Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2013. The expressive power of word embeddings. In ICML Workshop on Deep Learning for Audio, Speech, and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1344" citStr="Collobert and Weston, 2008" startWordPosition="180" endWordPosition="184">n. 1 Introduction Different sentence classification tasks are crucial for many Natural Language Processing (NLP) applications. Natural language sentences have complicated structures, both sequential and hierarchical, that are essential for understanding them. In addition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem. In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008). A problem largely specific to NLP is how to detect features of linguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long</context>
<context position="3555" citStr="Collobert and Weston, 2008" startWordPosition="532" endWordPosition="535">engio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a). Word embeddings are derived by projecting words from a sparse, 1-of-V encoding (V : vocabulary size) onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words. Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. Hill et al. (2014) showed that embeddings learned by neural machine translation models outperform three repre204 Proceedings of the 19th Conference on Computational Language Learning, pages 204–214, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics sentative monolingual embedding versions: skipgram (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and C&amp;W (Collobert et al., 2011) </context>
<context position="6338" citStr="Collobert and Weston (2008)" startWordPosition="959" endWordPosition="962">presents related work. Section 3 gives details of our classification model. Section 4 introduces two tricks that enhance system performance: mutual-learning and pretraining. Section 5 reports experimental results. Section 6 concludes this work. 2 Related Work Much prior work has exploited deep neural networks to model sentences. Blacoe and Lapata (2012) represented a sentence by element-wise addition, multiplication, or recursive autoencoder over embeddings of component single words. Yin and Sch¨utze (2014) extended this approach by composing on words and phrases instead of only single words. Collobert and Weston (2008) and Yu et al. (2014) used one layer of convolution over phrases detected by a sliding window on a target sentence, then used max- or average-pooling to form a sentence representation. Kalchbrenner et al. (2014) stacked multiple layers of one-dimensional convolution by dynamic kmax pooling to model sentences. We also adopt dynamic k-max pooling while our convolution layer has variable-size filters. Kim (2014) also studied multichannel representation and variable-size filters. Differently, their multichannel relies on a single version of pretrained embeddings (i.e., pretrained Word2Vec embeddin</context>
<context position="14327" citStr="Collobert and Weston, 2008" startWordPosition="2315" endWordPosition="2318">lding step. This change also means our model has substantially fewer parameters than the DCNN since the output of each convolution layer is smaller by a factor of d. 1A reviewer expresses surprise at such a small number of maps. However, we will use four variable sizes (see below), so that the overall number of maps is 20. We use a small number of maps partly because training times for a network are on the order of days, so limiting the number of parameters is important. Dynamic k-max Pooling. Kalchbrenner et al. (2014) pool the k most active features compared with simple max (1-max) pooling (Collobert and Weston, 2008). This property enables it to connect multiple convolution layers to form a deep architecture to extract high-level abstract features. In this work, we directly use it to extract features for variable-size feature maps. For a given feature map in layer i, dynamic k-max pooling extracts ki top values from each dimension and ktop top values in the top layer. We set — ki = max(kto L i p, r L sI) (3) where i E 11, 2,... L} is the order of convolution layer from bottom to top in Figure 1; L is the total numbers of convolution layers; ktop is a constant determined empirically, we set it to 4 as (Kal</context>
<context position="31587" citStr="Collobert and Weston, 2008" startWordPosition="5164" endWordPosition="5167">9 layers MVCNN (3) 88.2 32 88.6 48.6 93.1 33 MVCNN (4) 87.9 48.2 88.0 92.4 34 MVCNN (overall) 89.4 49.6 88.2 93.9 Table 3: Test set results of our CNN model against other methods. RAE: Recursive Autoencoders with pretrained word embeddings from Wikipedia (Socher et al., 2011b). MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a “channel”) of pretraine</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1540" citStr="Collobert et al., 2011" startWordPosition="214" endWordPosition="217">d hierarchical, that are essential for understanding them. In addition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem. In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008). A problem largely specific to NLP is how to detect features of linguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures (Dos Santos and Gatti, 2014; Kim, 2014; Denil et al., 2014). Current CNN systems usually implement a convolution layer with fixed-size </context>
<context position="4154" citStr="Collobert et al., 2011" startWordPosition="619" endWordPosition="622">lobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. Hill et al. (2014) showed that embeddings learned by neural machine translation models outperform three repre204 Proceedings of the 19th Conference on Computational Language Learning, pages 204–214, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics sentative monolingual embedding versions: skipgram (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and C&amp;W (Collobert et al., 2011) in some cases. These prior studies motivate us to explore combining multiple versions of word embeddings, treating each of them as a distinct description of words. Our expectation is that the combination of these embedding versions, trained by different NNs on different corpora, should contain more information than each version individually. We want to leverage this diversity of different embedding versions to extract higher quality sentence features and thereby improve sentence classification performance. The letters “M” and “V” in the name “MVCNN” of our architecture denote the multichannel</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Misha Denil</author>
<author>Alban Demiraj</author>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
<author>Nando de Freitas</author>
</authors>
<title>Modelling, visualising and summarising documents with a single convolutional neural network. arXiv preprint arXiv:1406.3830.</title>
<date>2014</date>
<marker>Denil, Demiraj, Kalchbrenner, Blunsom, de Freitas, 2014</marker>
<rawString>Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil Blunsom, and Nando de Freitas. 2014. Modelling, visualising and summarising documents with a single convolutional neural network. arXiv preprint arXiv:1406.3830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cıcero Nogueira Dos Santos</author>
<author>Maıra Gatti</author>
</authors>
<title>Deep convolutional neural networks for sentiment analysis of short texts.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="2032" citStr="Santos and Gatti, 2014" startWordPosition="287" endWordPosition="290">inguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures (Dos Santos and Gatti, 2014; Kim, 2014; Denil et al., 2014). Current CNN systems usually implement a convolution layer with fixed-size filters (i.e., feature detectors), in which the concrete filter size is a hyperparameter. They essentially split a sentence into multiple sub-sentences by a sliding window, then determine the sentence label by using the dominant label across all sub-sentences. The underlying assumption is that the sub-sentence with that granularity is potentially good enough to represent the whole sentence. However, it is hard to find the granularity of a “good sub-sentence” that works well across senten</context>
</contexts>
<marker>Santos, Gatti, 2014</marker>
<rawString>Cıcero Nogueira Dos Santos and Maıra Gatti. 2014. Deep convolutional neural networks for sentiment analysis of short texts. In Proceedings of the 25th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="23326" citStr="Duchi et al., 2011" startWordPosition="3828" endWordPosition="3831"> logistic regression that predicts the probability distribution over classes given the input sentence. The network is trained to minimize cross-entropy of predicted and true distributions; the objective includes an L2 regularization term over the parameters. The set of parameters comprises the word embeddings, all filter weights and the weights in fully connected layers. A dropout operation (Hinton et al., 2012) is put before the logistic regression layer. The network is trained by back-propagation in mini-batches and the gradient-based optimization is performed using the AdaGrad update rule (Duchi et al., 2011) In all data sets, the initial learning rate is 0.01, dropout probability is 0.8, L2 weight is 5 · 10−3, batch size is 50. In each convolution layer, filter sizes are {3, 5, 7, 9} and each filter has five kernels (independent of filter size). 5.2 Datasets and Experimental Setup Standard Sentiment Treebank (Socher et al., 2013). This small-scale dataset includes two tasks predicting the sentiment of movie reviews. The output variable is binary in one experiment and can have five possible outcomes in the other: {negative, somewhat negative, neutral, somewhat positive, positive}. In the binary ca</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N Project Report,</title>
<date>2009</date>
<pages>1--12</pages>
<location>Stanford,</location>
<contexts>
<context position="24500" citStr="Go et al., 2009" startWordPosition="4018" endWordPosition="4021">what positive, positive}. In the binary case, we use the given split of 6920 training, 872 development and 1821 test sentences. Likewise, in the finegrained case, we use the standard 8544/1101/2210 split. Socher et al. (2013) used the Stanford Parser (Klein and Manning, 2003) to parse each sentence into subphrases. The subphrases were then labeled by human annotators in the same way as the sentences were labeled. Labeled phrases that occur as subparts of the training sentences are treated as independent training instances as in (Le and Mikolov, 2014; Kalchbrenner et al., 2014). Sentiment1402 (Go et al., 2009). This is a large-scale dataset of tweets about sentiment classification, where a tweet is automatically labeled as positive or negative depending on the emoticon that occurs in it. The training set consists of 1.6 million tweets with emoticon-based labels and the test set of about 400 hand-annotated tweets. We preprocess the tweets minimally as follows. 1) The equivalence class symbol “url” (resp. “username”) replaces all URLs (resp. all words that start with the @ symbol, e.g., @thomasss). 2) A sequence of k &gt; 2 repetitions of a letter c (e.g., “cooooooool”) is replaced by two occurrences of</context>
<context position="30266" citStr="Go et al., 2009" startWordPosition="4931" endWordPosition="4934"> al., 2011b) 82.4 43.2 – – 2 MV-RNN (Socher et al., 2012) 82.9 44.4 – – 3 RNTN (Socher et al., 2013) 85.4 45.7 – – 4 DCNN (Kalchbrenner et al., 2014) 86.8 48.5 87.4 – 5 Paragraph-Vec (Le and Mikolov, 2014) 87.7 48.7 – – baselines 6 CNN-rand (Kim, 2014) 82.7 45.0 – 89.6 7 CNN-static (Kim, 2014) 86.8 45.5 – 93.0 8 CNN-non-static (Kim, 2014) 87.2 48.0 – 93.4 9 CNN-multichannel (Kim, 2014) 88.1 47.4 – 93.2 10 NBSVM (Wang and Manning, 2012) – – – 93.2 11 MNB (Wang and Manning, 2012) – – – 93.6 12 G-Dropout (Wang and Manning, 2013) – – – 93.4 13 F-Dropout (Wang and Manning, 2013) – – – 93.6 14 SVM (Go et al., 2009) – – 81.6 – 15 BINB (Go et al., 2009) – – 82.7 – 16 MAX-TDNN (Kalchbrenner et al., 2014) – – 78.8 – 17 NBOW (Kalchbrenner et al., 2014) – – 80.9 – 18 MAXENT (Go et al., 2009) – – 83.0 – 19 MVCNN (-HLBL) 88.5 48.7 88.0 93.6 20 MVCNN (-Huang) 89.2 49.2 88.1 93.7 versions 21 MVCNN (-Glove) 88.3 48.6 87.4 93.6 22 MVCNN (-SENNA) 89.3 49.1 87.9 93.4 23 MVCNN (-Word2Vec) 88.4 48.2 87.6 93.4 24 MVCNN (-3) 89.1 49.2 88.0 93.6 25 MVCNN (-5) 88.7 49.0 87.5 93.4 filters MVCNN (-7) 87.8 48.9 87.5 93.1 26 27 MVCNN (-9) 88.6 49.2 87.8 93.3 28 MVCNN (-mutual-learning) 88.2 49.2 87.8 93.5 tricks MVCNN (-pretra</context>
<context position="31864" citStr="Go et al., 2009" startWordPosition="5204" endWordPosition="5207">atrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a “channel”) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014). G-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from Wang and Manning (2013). Minus sign “-” in MVCNN (-Huang) etc. means “Huang” is not used. “versions / filters / tric</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>A-R Mohamed</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Speech recognition with deep recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Acoustics, Speech and Signal Processing, 2013 IEEE International Conference on,</booktitle>
<pages>6645--6649</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1307" citStr="Graves et al., 2013" startWordPosition="174" endWordPosition="177"> on subjectivity classification. 1 Introduction Different sentence classification tasks are crucial for many Natural Language Processing (NLP) applications. Natural language sentences have complicated structures, both sequential and hierarchical, that are essential for understanding them. In addition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem. In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008). A problem largely specific to NLP is how to detect features of linguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks (CNN) are getting increasing atten</context>
</contexts>
<marker>Graves, Mohamed, Hinton, 2013</marker>
<rawString>Alex Graves, A-R Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent neural networks. In Acoustics, Speech and Signal Processing, 2013 IEEE International Conference on, pages 6645–6649. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>KyungHyun Cho</author>
<author>Sebastien Jean</author>
<author>Coline Devin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Not all neural embeddings are born equal.</title>
<date>2014</date>
<booktitle>In NIPS Workshop on Learning Semantics.</booktitle>
<contexts>
<context position="3750" citStr="Hill et al. (2014)" startWordPosition="563" endWordPosition="566"> onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words. Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. Hill et al. (2014) showed that embeddings learned by neural machine translation models outperform three repre204 Proceedings of the 19th Conference on Computational Language Learning, pages 204–214, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics sentative monolingual embedding versions: skipgram (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and C&amp;W (Collobert et al., 2011) in some cases. These prior studies motivate us to explore combining multiple versions of word embeddings, treating each of them as a distinct description of words. Our expectation is that the com</context>
</contexts>
<marker>Hill, Cho, Jean, Devin, Bengio, 2014</marker>
<rawString>Felix Hill, KyungHyun Cho, Sebastien Jean, Coline Devin, and Yoshua Bengio. 2014. Not all neural embeddings are born equal. In NIPS Workshop on Learning Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580.</title>
<date>2012</date>
<contexts>
<context position="23122" citStr="Hinton et al., 2012" startWordPosition="3796" endWordPosition="3799">ifying aspects of the implementation and the training of the network. We then report the results of the experiments. 5.1 Hyperparameters and Training In each of the experiments, the top of the network is a logistic regression that predicts the probability distribution over classes given the input sentence. The network is trained to minimize cross-entropy of predicted and true distributions; the objective includes an L2 regularization term over the parameters. The set of parameters comprises the word embeddings, all filter weights and the weights in fully connected layers. A dropout operation (Hinton et al., 2012) is put before the logistic regression layer. The network is trained by back-propagation in mini-batches and the gradient-based optimization is performed using the AdaGrad update rule (Duchi et al., 2011) In all data sets, the initial learning rate is 0.01, dropout probability is 0.8, L2 weight is 5 · 10−3, batch size is 50. In each convolution layer, filter sizes are {3, 5, 7, 9} and each filter has five kernels (independent of filter size). 5.2 Datasets and Experimental Setup Standard Sentiment Treebank (Socher et al., 2013). This small-scale dataset includes two tasks predicting the sentime</context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3616" citStr="Huang et al., 2012" startWordPosition="543" endWordPosition="546">lov, 2012; Mikolov et al., 2013a). Word embeddings are derived by projecting words from a sparse, 1-of-V encoding (V : vocabulary size) onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words. Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. Hill et al. (2014) showed that embeddings learned by neural machine translation models outperform three repre204 Proceedings of the 19th Conference on Computational Language Learning, pages 204–214, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics sentative monolingual embedding versions: skipgram (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and C&amp;W (Collobert et al., 2011) in some cases. These prior studies motivate us to explore com</context>
<context position="26944" citStr="Huang et al. (2012)" startWordPosition="4397" endWordPosition="4400"> 2: Statistics of five embedding versions for four tasks. The first block with five rows provides the number of unknown words of each task when using corresponding version to initialize. Voc size: vocabulary size. Full hit: embedding in all 5 versions. Partial hit: embedding in 1–4 versions, No hit: not present in any of the 5 versions. (i) HLBL. Hierarchical log-bilinear model presented by Mnih and Hinton (2009) and released by Turian et al. (2010);4 size: 246,122 word embeddings; training corpus: RCV1 corpus, one year of Reuters English newswire from August 1996 to August 1997. (ii) Huang.5 Huang et al. (2012) incorporated global context to deal with challenges raised by words with multiple meanings; size: 100,232 word embeddings; training corpus: April 2010 snapshot of Wikipedia. (iii) GloVe.6 Size: 1,193,514 word embeddings; training corpus: a Twitter corpus of 2B tweets with 27B tokens. (iv) SENNA.7 Size: 130,000 word embeddings; training corpus: Wikipedia. Note that we use their 50- dimensional embeddings. (v) Word2Vec. It has no 50-dimensional embeddings available online. We use released code8 to train skip-gram on English Gigaword Corpus (Parker et al., 2009) with 4http://metaoptimize.com/pro</context>
<context position="35777" citStr="Huang et al. (2012)" startWordPosition="5807" endWordPosition="5810">ion filters. We demonstrated that multichannel initialization and variable-size filters enhance system performance on sentiment classification and subjectivity classification tasks. 7 Future Work As pointed out by the reviewers the success of the multichannel approach is likely due to a combination of several quite different effects. First, there is the effect of the embedding learning algorithm. These algorithms differ in many aspects, including in sensitivity to word order (e.g., SENNA: yes, word2vec: no), in objective function and in their treatment of ambiguity (explicitly modeled only by Huang et al. (2012). Second, there is the effect of the corpus. We would expect the size and genre of the corpus to have a big effect even though we did not analyze this effect in this paper. Third, complementarity of word embeddings is likely to be more useful for some tasks than for others. Sentiment is a good application for complementary word embeddings because solving this task requires drawing on heterogeneous sources of information, including syntax, semantics and genre as well as the core polarity of a word. Other tasks like part of speech (POS) tagging may benefit less from heterogeneity since the benef</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873–882. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1567" citStr="Kalchbrenner et al., 2014" startWordPosition="218" endWordPosition="221"> essential for understanding them. In addition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem. In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008). A problem largely specific to NLP is how to detect features of linguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures (Dos Santos and Gatti, 2014; Kim, 2014; Denil et al., 2014). Current CNN systems usually implement a convolution layer with fixed-size filters (i.e., feature dete</context>
<context position="6549" citStr="Kalchbrenner et al. (2014)" startWordPosition="995" endWordPosition="998">. Section 6 concludes this work. 2 Related Work Much prior work has exploited deep neural networks to model sentences. Blacoe and Lapata (2012) represented a sentence by element-wise addition, multiplication, or recursive autoencoder over embeddings of component single words. Yin and Sch¨utze (2014) extended this approach by composing on words and phrases instead of only single words. Collobert and Weston (2008) and Yu et al. (2014) used one layer of convolution over phrases detected by a sliding window on a target sentence, then used max- or average-pooling to form a sentence representation. Kalchbrenner et al. (2014) stacked multiple layers of one-dimensional convolution by dynamic kmax pooling to model sentences. We also adopt dynamic k-max pooling while our convolution layer has variable-size filters. Kim (2014) also studied multichannel representation and variable-size filters. Differently, their multichannel relies on a single version of pretrained embeddings (i.e., pretrained Word2Vec embeddings) with two copies: one is kept stable and the other one is fine-tuned by backpropagation. We develop this insight by incorporating diverse embedding versions. Additionally, their idea of variable-size filters </context>
<context position="11527" citStr="Kalchbrenner et al., 2014" startWordPosition="1810" endWordPosition="1814">ion and pretraining. eter. We first concatenate the initialized embeddings of l consecutive words (wi−l+1, ... , wi) as ci E Rld (1 &lt; i &lt; s + l), then generate the feature value of this phrase as pi (the whole vector p E Rs+l−1 contains all the local features) using a tanh activation function and a linear projection vector v E Rld as: pi = tanh(vTci + b) (1) More generally, convolution operation can deal with multiple input feature maps and can be stacked to yield feature maps of increasing layers. In each layer, there are usually multiple filters of the same size, but with different weights (Kalchbrenner et al., 2014). We refer to a filter with a specific set of weights as a kernel. The goal is often to train a model in which different kernels detect different kinds of features of a local region. However, this traditional way can not detect the features of regions of different granularity. Hence 206 we keep the property of multi-kernel while extending it to variable-size in the same layer. As in CNN for object recognition, to increase the number of kernels of a certain layer, multiple feature maps may be computed in parallel at the same layer. Further, to increase the size diversity of kernels in the same </context>
<context position="13261" citStr="Kalchbrenner et al., 2014" startWordPosition="2138" endWordPosition="2141">e index of a feature map in layer i. The weights in V form a rank 4 tensor. Note that we use wide convolution in this work: it means word representations wg for g G 0 or g &gt; s+1 are actually zero embeddings. Wide convolution enables that each word can be detected by all filter weights in V. In Figure 1, the first convolution layer deals with an input with n = 5 feature maps.1 Its filters have sizes 3 and 5 respectively (i.e., l = 3, 5), and each filter has j = 3 kernels. This means this convolution layer can detect three kinds of features of phrases with length 3 and 5, respectively. DCNN in (Kalchbrenner et al., 2014) used onedimensional convolution: each higher-order feature is produced from values of a single dimension in the lower-layer feature map. Even though that work proposed folding operation to model the dependencies between adjacent dimensions, this type of dependency modeling is still limited. Differently, convolution in present work is able to model dependency across dimensions as well as adjacent words, which obviates the need for a folding step. This change also means our model has substantially fewer parameters than the DCNN since the output of each convolution layer is smaller by a factor o</context>
<context position="14950" citStr="Kalchbrenner et al., 2014" startWordPosition="2432" endWordPosition="2436">08). This property enables it to connect multiple convolution layers to form a deep architecture to extract high-level abstract features. In this work, we directly use it to extract features for variable-size feature maps. For a given feature map in layer i, dynamic k-max pooling extracts ki top values from each dimension and ktop top values in the top layer. We set — ki = max(kto L i p, r L sI) (3) where i E 11, 2,... L} is the order of convolution layer from bottom to top in Figure 1; L is the total numbers of convolution layers; ktop is a constant determined empirically, we set it to 4 as (Kalchbrenner et al., 2014). As a result, the second convolution layer in Figure 1 has an input with two same-size feature maps, one results from filter size 3, one from filter size 5. The values in the two feature maps are for phrases with different granularity. The motivation of this convolution layer lies in that a feature reflected by a short phrase may be not trustworthy while the longer phrase containing the short one is trustworthy, or the long phrase has no trustworthy feature while its component short phrase is more reliable. This and even higher-order convolution layers therefore can make a trade-off between t</context>
<context position="24467" citStr="Kalchbrenner et al., 2014" startWordPosition="4013" endWordPosition="4016">{negative, somewhat negative, neutral, somewhat positive, positive}. In the binary case, we use the given split of 6920 training, 872 development and 1821 test sentences. Likewise, in the finegrained case, we use the standard 8544/1101/2210 split. Socher et al. (2013) used the Stanford Parser (Klein and Manning, 2003) to parse each sentence into subphrases. The subphrases were then labeled by human annotators in the same way as the sentences were labeled. Labeled phrases that occur as subparts of the training sentences are treated as independent training instances as in (Le and Mikolov, 2014; Kalchbrenner et al., 2014). Sentiment1402 (Go et al., 2009). This is a large-scale dataset of tweets about sentiment classification, where a tweet is automatically labeled as positive or negative depending on the emoticon that occurs in it. The training set consists of 1.6 million tweets with emoticon-based labels and the test set of about 400 hand-annotated tweets. We preprocess the tweets minimally as follows. 1) The equivalence class symbol “url” (resp. “username”) replaces all URLs (resp. all words that start with the @ symbol, e.g., @thomasss). 2) A sequence of k &gt; 2 repetitions of a letter c (e.g., “cooooooool”) </context>
<context position="29799" citStr="Kalchbrenner et al., 2014" startWordPosition="4838" endWordPosition="4841">g and pretraining, three convolution layers for Senti140 task and two convolution layers for the other tasks. Overall, our system gets the best results, beating all baselines. The table contains five blocks from top to bottom. Each block investigates one specific configurational aspect of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when 210 Model Binary Fine-grained Senti140 Subj 1 RAE (Socher et al., 2011b) 82.4 43.2 – – 2 MV-RNN (Socher et al., 2012) 82.9 44.4 – – 3 RNTN (Socher et al., 2013) 85.4 45.7 – – 4 DCNN (Kalchbrenner et al., 2014) 86.8 48.5 87.4 – 5 Paragraph-Vec (Le and Mikolov, 2014) 87.7 48.7 – – baselines 6 CNN-rand (Kim, 2014) 82.7 45.0 – 89.6 7 CNN-static (Kim, 2014) 86.8 45.5 – 93.0 8 CNN-non-static (Kim, 2014) 87.2 48.0 – 93.4 9 CNN-multichannel (Kim, 2014) 88.1 47.4 – 93.2 10 NBSVM (Wang and Manning, 2012) – – – 93.2 11 MNB (Wang and Manning, 2012) – – – 93.6 12 G-Dropout (Wang and Manning, 2013) – – – 93.4 13 F-Dropout (Wang and Manning, 2013) – – – 93.6 14 SVM (Go et al., 2009) – – 81.6 – 15 BINB (Go et al., 2009) – – 82.7 – 16 MAX-TDNN (Kalchbrenner et al., 2014) – – 78.8 – 17 NBOW (Kalchbrenner et al., 201</context>
<context position="31643" citStr="Kalchbrenner et al., 2014" startWordPosition="5171" endWordPosition="5174">.9 48.2 88.0 92.4 34 MVCNN (overall) 89.4 49.6 88.2 93.9 Table 3: Test set results of our CNN model against other methods. RAE: Recursive Autoencoders with pretrained word embeddings from Wikipedia (Socher et al., 2011b). MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a “channel”) of pretrained embeddings / initialized with pretrained embeddings wh</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="1579" citStr="Kim, 2014" startWordPosition="222" endWordPosition="223">g them. In addition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem. In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008). A problem largely specific to NLP is how to detect features of linguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures (Dos Santos and Gatti, 2014; Kim, 2014; Denil et al., 2014). Current CNN systems usually implement a convolution layer with fixed-size filters (i.e., feature detectors), in w</context>
<context position="6750" citStr="Kim (2014)" startWordPosition="1027" endWordPosition="1028">sive autoencoder over embeddings of component single words. Yin and Sch¨utze (2014) extended this approach by composing on words and phrases instead of only single words. Collobert and Weston (2008) and Yu et al. (2014) used one layer of convolution over phrases detected by a sliding window on a target sentence, then used max- or average-pooling to form a sentence representation. Kalchbrenner et al. (2014) stacked multiple layers of one-dimensional convolution by dynamic kmax pooling to model sentences. We also adopt dynamic k-max pooling while our convolution layer has variable-size filters. Kim (2014) also studied multichannel representation and variable-size filters. Differently, their multichannel relies on a single version of pretrained embeddings (i.e., pretrained Word2Vec embeddings) with two copies: one is kept stable and the other one is fine-tuned by backpropagation. We develop this insight by incorporating diverse embedding versions. Additionally, their idea of variable-size filters is further developed. Le and Mikolov (2014) initialized the representation of a sentence as a parameter vector, treating it as a global feature and combining this vector with the representations of con</context>
<context position="16027" citStr="Kim, 2014" startWordPosition="2611" endWordPosition="2612">e its component short phrase is more reliable. This and even higher-order convolution layers therefore can make a trade-off between the features of different granularity. Hidden Layer. On the top of the final kmax pooling, we stack a fully connected layer to learn sentence representation with given dimension (e.g., d). Logistic Regression Layer. Finally, sentence representation is forwarded into logistic regression layer for classification. In brief, our MVCNN model learns from (Kalchbrenner et al., 2014) to use dynamic kmax pooling to stack multiple convolution layers, and gets insight from (Kim, 2014) to investigate variable-size filters in a convolution layer. Compared to (Kalchbrenner et al., 2014), MVCNN has rich feature maps as input and as output of each convolution layer. Its convolution operation is not only more flexible to extract features of variable-range phrases, but also able to model dependency among all dimensions of representations. MVCNN extends the network in (Kim, 2014) by hierarchical convolution architecture and n Fji,l = k=1 207 further exploration of multichannel and variablesize feature detectors. 4 Model Enhancements This part introduces two training tricks that en</context>
<context position="29902" citStr="Kim, 2014" startWordPosition="4859" endWordPosition="4860">our system gets the best results, beating all baselines. The table contains five blocks from top to bottom. Each block investigates one specific configurational aspect of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when 210 Model Binary Fine-grained Senti140 Subj 1 RAE (Socher et al., 2011b) 82.4 43.2 – – 2 MV-RNN (Socher et al., 2012) 82.9 44.4 – – 3 RNTN (Socher et al., 2013) 85.4 45.7 – – 4 DCNN (Kalchbrenner et al., 2014) 86.8 48.5 87.4 – 5 Paragraph-Vec (Le and Mikolov, 2014) 87.7 48.7 – – baselines 6 CNN-rand (Kim, 2014) 82.7 45.0 – 89.6 7 CNN-static (Kim, 2014) 86.8 45.5 – 93.0 8 CNN-non-static (Kim, 2014) 87.2 48.0 – 93.4 9 CNN-multichannel (Kim, 2014) 88.1 47.4 – 93.2 10 NBSVM (Wang and Manning, 2012) – – – 93.2 11 MNB (Wang and Manning, 2012) – – – 93.6 12 G-Dropout (Wang and Manning, 2013) – – – 93.4 13 F-Dropout (Wang and Manning, 2013) – – – 93.6 14 SVM (Go et al., 2009) – – 81.6 – 15 BINB (Go et al., 2009) – – 82.7 – 16 MAX-TDNN (Kalchbrenner et al., 2014) – – 78.8 – 17 NBOW (Kalchbrenner et al., 2014) – – 80.9 – 18 MAXENT (Go et al., 2009) – – 83.0 – 19 MVCNN (-HLBL) 88.5 48.7 88.0 93.6 20 MVCNN (-Hu</context>
<context position="32285" citStr="Kim, 2014" startWordPosition="5262" endWordPosition="5263">sion on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a “channel”) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014). G-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from Wang and Manning (2013). Minus sign “-” in MVCNN (-Huang) etc. means “Huang” is not used. “versions / filters / tricks / layers” denote the MVCNN variants with different setups: discard certain embedding version / discard certain filter size / discard mutual-learning or pretraining / different numbers of convolution layer. 211 HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some systems representative of previous work on the corresponding dat</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24160" citStr="Klein and Manning, 2003" startWordPosition="3963" endWordPosition="3966">ls (independent of filter size). 5.2 Datasets and Experimental Setup Standard Sentiment Treebank (Socher et al., 2013). This small-scale dataset includes two tasks predicting the sentiment of movie reviews. The output variable is binary in one experiment and can have five possible outcomes in the other: {negative, somewhat negative, neutral, somewhat positive, positive}. In the binary case, we use the given split of 6920 training, 872 development and 1821 test sentences. Likewise, in the finegrained case, we use the standard 8544/1101/2210 split. Socher et al. (2013) used the Stanford Parser (Klein and Manning, 2003) to parse each sentence into subphrases. The subphrases were then labeled by human annotators in the same way as the sentences were labeled. Labeled phrases that occur as subparts of the training sentences are treated as independent training instances as in (Le and Mikolov, 2014; Kalchbrenner et al., 2014). Sentiment1402 (Go et al., 2009). This is a large-scale dataset of tweets about sentiment classification, where a tweet is automatically labeled as positive or negative depending on the emoticon that occurs in it. The training set consists of 1.6 million tweets with emoticon-based labels and</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>1097--1105</pages>
<contexts>
<context position="1265" citStr="Krizhevsky et al., 2012" startWordPosition="168" endWordPosition="171">nd largescale Twitter sentiment prediction and on subjectivity classification. 1 Introduction Different sentence classification tasks are crucial for many Natural Language Processing (NLP) applications. Natural language sentences have complicated structures, both sequential and hierarchical, that are essential for understanding them. In addition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem. In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008). A problem largely specific to NLP is how to detect features of linguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural n</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st international conference on Machine learning.</booktitle>
<contexts>
<context position="7192" citStr="Mikolov (2014)" startWordPosition="1089" endWordPosition="1090">ne-dimensional convolution by dynamic kmax pooling to model sentences. We also adopt dynamic k-max pooling while our convolution layer has variable-size filters. Kim (2014) also studied multichannel representation and variable-size filters. Differently, their multichannel relies on a single version of pretrained embeddings (i.e., pretrained Word2Vec embeddings) with two copies: one is kept stable and the other one is fine-tuned by backpropagation. We develop this insight by incorporating diverse embedding versions. Additionally, their idea of variable-size filters is further developed. Le and Mikolov (2014) initialized the representation of a sentence as a parameter vector, treating it as a global feature and combining this vector with the representations of context words to do word prediction. Finally, this fine-tuned vector is used as representation of this sentence. Apparently, this method can only produce generic sentence representations which encode no taskspecific features. Our work is also inspired by studies that compared the performance of different word embedding versions or investigated the combination of them. For example, Turian et al. (2010) compared Brown clusters, C&amp;W embeddings </context>
<context position="24439" citStr="Mikolov, 2014" startWordPosition="4011" endWordPosition="4012"> in the other: {negative, somewhat negative, neutral, somewhat positive, positive}. In the binary case, we use the given split of 6920 training, 872 development and 1821 test sentences. Likewise, in the finegrained case, we use the standard 8544/1101/2210 split. Socher et al. (2013) used the Stanford Parser (Klein and Manning, 2003) to parse each sentence into subphrases. The subphrases were then labeled by human annotators in the same way as the sentences were labeled. Labeled phrases that occur as subparts of the training sentences are treated as independent training instances as in (Le and Mikolov, 2014; Kalchbrenner et al., 2014). Sentiment1402 (Go et al., 2009). This is a large-scale dataset of tweets about sentiment classification, where a tweet is automatically labeled as positive or negative depending on the emoticon that occurs in it. The training set consists of 1.6 million tweets with emoticon-based labels and the test set of about 400 hand-annotated tweets. We preprocess the tweets minimally as follows. 1) The equivalence class symbol “url” (resp. “username”) replaces all URLs (resp. all words that start with the @ symbol, e.g., @thomasss). 2) A sequence of k &gt; 2 repetitions of a le</context>
<context position="29855" citStr="Mikolov, 2014" startWordPosition="4850" endWordPosition="4851">o convolution layers for the other tasks. Overall, our system gets the best results, beating all baselines. The table contains five blocks from top to bottom. Each block investigates one specific configurational aspect of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when 210 Model Binary Fine-grained Senti140 Subj 1 RAE (Socher et al., 2011b) 82.4 43.2 – – 2 MV-RNN (Socher et al., 2012) 82.9 44.4 – – 3 RNTN (Socher et al., 2013) 85.4 45.7 – – 4 DCNN (Kalchbrenner et al., 2014) 86.8 48.5 87.4 – 5 Paragraph-Vec (Le and Mikolov, 2014) 87.7 48.7 – – baselines 6 CNN-rand (Kim, 2014) 82.7 45.0 – 89.6 7 CNN-static (Kim, 2014) 86.8 45.5 – 93.0 8 CNN-non-static (Kim, 2014) 87.2 48.0 – 93.4 9 CNN-multichannel (Kim, 2014) 88.1 47.4 – 93.2 10 NBSVM (Wang and Manning, 2012) – – – 93.2 11 MNB (Wang and Manning, 2012) – – – 93.6 12 G-Dropout (Wang and Manning, 2013) – – – 93.4 13 F-Dropout (Wang and Manning, 2013) – – – 93.6 14 SVM (Go et al., 2009) – – 81.6 – 15 BINB (Go et al., 2009) – – 82.7 – 16 MAX-TDNN (Kalchbrenner et al., 2014) – – 78.8 – 17 NBOW (Kalchbrenner et al., 2014) – – 80.9 – 18 MAXENT (Go et al., 2009) – – 83.0 – 19 </context>
<context position="31730" citStr="Mikolov, 2014" startWordPosition="5185" endWordPosition="5186"> against other methods. RAE: Recursive Autoencoders with pretrained word embeddings from Wikipedia (Socher et al., 2011b). MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a “channel”) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014). G-Dropout, F-Dropout: Gaussian Dropout and </context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of the 31st international conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yong Luo</author>
<author>Jian Tang</author>
<author>Jun Yan</author>
<author>Chao Xu</author>
<author>Zheng Chen</author>
</authors>
<title>Pre-trained multi-view word embedding using two-side neural network.</title>
<date>2014</date>
<booktitle>In TwentyEighth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="8054" citStr="Luo et al. (2014)" startWordPosition="1225" endWordPosition="1228">tion of this sentence. Apparently, this method can only produce generic sentence representations which encode no taskspecific features. Our work is also inspired by studies that compared the performance of different word embedding versions or investigated the combination of them. For example, Turian et al. (2010) compared Brown clusters, C&amp;W embeddings and HLBL embeddings in NER and chunking tasks. They found that Brown clusters and word embeddings both can improve the accuracy of supervised NLP systems; and demonstrated empirically that combining different word representations is beneficial. Luo et al. (2014) adapted CBOW (Mikolov et al., 2013a) to train word embeddings on different datasets: free text documents from Wikipedia, search click-through data and user query data, showing that combining them gets stronger results than using individual word embeddings in web search ranking and word similarity task. However, these two papers either learned word representations on the same corpus (Turian et al., 2010) or enhanced the embedding quality by extending training corpora, not learning algorithms (Luo et 205 al., 2014). In our work, there is no limit to the type of embedding versions we can use and</context>
</contexts>
<marker>Luo, Tang, Yan, Xu, Chen, 2014</marker>
<rawString>Yong Luo, Jian Tang, Jun Yan, Chao Xu, and Zheng Chen. 2014. Pre-trained multi-view word embedding using two-side neural network. In TwentyEighth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>1045--1048</pages>
<location>Makuhari, Chiba, Japan,</location>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR.</booktitle>
<contexts>
<context position="3028" citStr="Mikolov et al., 2013" startWordPosition="447" endWordPosition="450">erlying assumption is that the sub-sentence with that granularity is potentially good enough to represent the whole sentence. However, it is hard to find the granularity of a “good sub-sentence” that works well across sentences. This motivates us to implement variable-size filters in a convolution layer in order to extract features of multigranular phrases. Breakthroughs of deep learning in NLP are also based on learning distributed word representations – also called “word embeddings” – by neural language models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a). Word embeddings are derived by projecting words from a sparse, 1-of-V encoding (V : vocabulary size) onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words. Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing gr</context>
<context position="8089" citStr="Mikolov et al., 2013" startWordPosition="1231" endWordPosition="1234">y, this method can only produce generic sentence representations which encode no taskspecific features. Our work is also inspired by studies that compared the performance of different word embedding versions or investigated the combination of them. For example, Turian et al. (2010) compared Brown clusters, C&amp;W embeddings and HLBL embeddings in NER and chunking tasks. They found that Brown clusters and word embeddings both can improve the accuracy of supervised NLP systems; and demonstrated empirically that combining different word representations is beneficial. Luo et al. (2014) adapted CBOW (Mikolov et al., 2013a) to train word embeddings on different datasets: free text documents from Wikipedia, search click-through data and user query data, showing that combining them gets stronger results than using individual word embeddings in web search ranking and word similarity task. However, these two papers either learned word representations on the same corpus (Turian et al., 2010) or enhanced the embedding quality by extending training corpora, not learning algorithms (Luo et 205 al., 2014). In our work, there is no limit to the type of embedding versions we can use and they leverage not only the diversi</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="3028" citStr="Mikolov et al., 2013" startWordPosition="447" endWordPosition="450">erlying assumption is that the sub-sentence with that granularity is potentially good enough to represent the whole sentence. However, it is hard to find the granularity of a “good sub-sentence” that works well across sentences. This motivates us to implement variable-size filters in a convolution layer in order to extract features of multigranular phrases. Breakthroughs of deep learning in NLP are also based on learning distributed word representations – also called “word embeddings” – by neural language models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a). Word embeddings are derived by projecting words from a sparse, 1-of-V encoding (V : vocabulary size) onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words. Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing gr</context>
<context position="8089" citStr="Mikolov et al., 2013" startWordPosition="1231" endWordPosition="1234">y, this method can only produce generic sentence representations which encode no taskspecific features. Our work is also inspired by studies that compared the performance of different word embedding versions or investigated the combination of them. For example, Turian et al. (2010) compared Brown clusters, C&amp;W embeddings and HLBL embeddings in NER and chunking tasks. They found that Brown clusters and word embeddings both can improve the accuracy of supervised NLP systems; and demonstrated empirically that combining different word representations is beneficial. Luo et al. (2014) adapted CBOW (Mikolov et al., 2013a) to train word embeddings on different datasets: free text documents from Wikipedia, search click-through data and user query data, showing that combining them gets stronger results than using individual word embeddings in web search ranking and word similarity task. However, these two papers either learned word representations on the same corpus (Turian et al., 2010) or enhanced the embedding quality by extending training corpora, not learning algorithms (Luo et 205 al., 2014). In our work, there is no limit to the type of embedding versions we can use and they leverage not only the diversi</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
</authors>
<title>Statistical language models based on neural networks. Presentation at Google,</title>
<date>2012</date>
<location>Mountain View, 2nd</location>
<contexts>
<context position="3006" citStr="Mikolov, 2012" startWordPosition="445" endWordPosition="446">tences. The underlying assumption is that the sub-sentence with that granularity is potentially good enough to represent the whole sentence. However, it is hard to find the granularity of a “good sub-sentence” that works well across sentences. This motivates us to implement variable-size filters in a convolution layer in order to extract features of multigranular phrases. Breakthroughs of deep learning in NLP are also based on learning distributed word representations – also called “word embeddings” – by neural language models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a). Word embeddings are derived by projecting words from a sparse, 1-of-V encoding (V : vocabulary size) onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words. Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et </context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tomas Mikolov. 2012. Statistical language models based on neural networks. Presentation at Google, Mountain View, 2nd April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2009</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="2969" citStr="Mnih and Hinton, 2009" startWordPosition="436" endWordPosition="440">y using the dominant label across all sub-sentences. The underlying assumption is that the sub-sentence with that granularity is potentially good enough to represent the whole sentence. However, it is hard to find the granularity of a “good sub-sentence” that works well across sentences. This motivates us to implement variable-size filters in a convolution layer in order to extract features of multigranular phrases. Breakthroughs of deep learning in NLP are also based on learning distributed word representations – also called “word embeddings” – by neural language models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a). Word embeddings are derived by projecting words from a sparse, 1-of-V encoding (V : vocabulary size) onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words. Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turi</context>
<context position="26741" citStr="Mnih and Hinton (2009)" startWordPosition="4363" endWordPosition="4366">4,136 4,371 323,501 6,162 W2V 2257 2,409 288,257 4,217 Voc size 18,876 19,612 387,877 23,926 Full hit 12,030 12,357 30,010 13,742 Partial hit 5,022 5,312 121,383 6,580 No hit 1,824 1,943 236,484 3,604 Table 2: Statistics of five embedding versions for four tasks. The first block with five rows provides the number of unknown words of each task when using corresponding version to initialize. Voc size: vocabulary size. Full hit: embedding in all 5 versions. Partial hit: embedding in 1–4 versions, No hit: not present in any of the 5 versions. (i) HLBL. Hierarchical log-bilinear model presented by Mnih and Hinton (2009) and released by Turian et al. (2010);4 size: 246,122 word embeddings; training corpus: RCV1 corpus, one year of Reuters English newswire from August 1996 to August 1997. (ii) Huang.5 Huang et al. (2012) incorporated global context to deal with challenges raised by words with multiple meanings; size: 100,232 word embeddings; training corpus: April 2010 snapshot of Wikipedia. (iii) GloVe.6 Size: 1,193,514 word embeddings; training corpus: a Twitter corpus of 2B tweets with 27B tokens. (iv) SENNA.7 Size: 130,000 word embeddings; training corpus: Wikipedia. Note that we use their 50- dimensional </context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2009. A scalable hierarchical distributed language model. In Advances in neural information processing systems, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning,</booktitle>
<pages>1751--1758</pages>
<contexts>
<context position="20972" citStr="Mnih and Teh, 2012" startWordPosition="3454" endWordPosition="3457">esentation is averaged with representations of some surrounding words (“the”, “cat”, “sat”, “the”, “mat”, “,” in the figure) to predict the middle word (“on”). Given sentence representation s E Rd and initialized representations of 2t context words (t left words and t right words): wi−t, ..., wi−1, wi+1, . . ., wi+t; wi E Rd, we average the total 2t + 1 vectors element-wise, depicted as “Average” operation in Figure 1. Then, this resulting vector is treated as a predicted representation of the middle word and is used to find the true middle word by means of noise-contrastive estimation (NCE) (Mnih and Teh, 2012). For each true example, 10 noise words are sampled. Note that in pretraining, there are three places 208 where each word needs initialization. (i) Each word in the sentence is initialized in the “Multichannel input” layer to the whole network. (ii) Each context word is initialized as input to the average layer (“Average” in the figure). (iii) Each target word is initialized as the output of the “NCE” layer (“on” in the figure). In this work, we use multichannel initialization for case (i) and random initialization for cases (ii) and (iii). Only finetuned multichannel representations (case (i)</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proceedings of the 29th International Conference on Machine Learning, pages 1751–1758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25224" citStr="Pang and Lee, 2004" startWordPosition="4136" endWordPosition="4139">labeled as positive or negative depending on the emoticon that occurs in it. The training set consists of 1.6 million tweets with emoticon-based labels and the test set of about 400 hand-annotated tweets. We preprocess the tweets minimally as follows. 1) The equivalence class symbol “url” (resp. “username”) replaces all URLs (resp. all words that start with the @ symbol, e.g., @thomasss). 2) A sequence of k &gt; 2 repetitions of a letter c (e.g., “cooooooool”) is replaced by two occurrences of c (e.g., “cool”). 3) All tokens are lowercased. Subj. Subjectivity classification dataset3 released by (Pang and Lee, 2004) has 5000 subjective sentences and 5000 objective sentences. We report the result of 10-fold cross validation as baseline systems did. 5.2.1 Pretrained Word Vectors In this work, we use five embedding versions, as shown in Table 1, to initialize words. Four of them are directly downloaded from the Internet. 2http://help.sentiment140.com/for-students 3http://www.cs.cornell.edu/people/pabo/movie-reviewdata/ 209 Set Training Data Vocab Size Dimensionality Source HLBL Reuters English newswire 246,122 50 download Huang Wikipedia (April 2010 snapshot) 100,232 50 download Glove Twitter 1,193,514 50 d</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
</authors>
<title>Linguistic Data Consortium, et al.</title>
<date>2009</date>
<marker>Parker, 2009</marker>
<rawString>Robert Parker, Linguistic Data Consortium, et al. 2009. English gigaword fourth edition. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing,</booktitle>
<pages>12</pages>
<contexts>
<context position="4121" citStr="Pennington et al., 2014" startWordPosition="613" endWordPosition="616">Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. Hill et al. (2014) showed that embeddings learned by neural machine translation models outperform three repre204 Proceedings of the 19th Conference on Computational Language Learning, pages 204–214, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics sentative monolingual embedding versions: skipgram (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and C&amp;W (Collobert et al., 2011) in some cases. These prior studies motivate us to explore combining multiple versions of word embeddings, treating each of them as a distinct description of words. Our expectation is that the combination of these embedding versions, trained by different NNs on different corpora, should contain more information than each version individually. We want to leverage this diversity of different embedding versions to extract higher quality sentence features and thereby improve sentence classification performance. The letters “M” and “V” in the name “MVCNN” of our arc</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>801--809</pages>
<contexts>
<context position="1600" citStr="Socher et al. (2011" startWordPosition="224" endWordPosition="227">ddition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem. In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008). A problem largely specific to NLP is how to detect features of linguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures (Dos Santos and Gatti, 2014; Kim, 2014; Denil et al., 2014). Current CNN systems usually implement a convolution layer with fixed-size filters (i.e., feature detectors), in which the concrete fil</context>
<context position="29660" citStr="Socher et al., 2011" startWordPosition="4808" endWordPosition="4811"> of MVCNN, optimized on dev. This version uses five versions of word embeddings, four filter sizes (3, 5, 7, 9), both mutual-learning and pretraining, three convolution layers for Senti140 task and two convolution layers for the other tasks. Overall, our system gets the best results, beating all baselines. The table contains five blocks from top to bottom. Each block investigates one specific configurational aspect of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when 210 Model Binary Fine-grained Senti140 Subj 1 RAE (Socher et al., 2011b) 82.4 43.2 – – 2 MV-RNN (Socher et al., 2012) 82.9 44.4 – – 3 RNTN (Socher et al., 2013) 85.4 45.7 – – 4 DCNN (Kalchbrenner et al., 2014) 86.8 48.5 87.4 – 5 Paragraph-Vec (Le and Mikolov, 2014) 87.7 48.7 – – baselines 6 CNN-rand (Kim, 2014) 82.7 45.0 – 89.6 7 CNN-static (Kim, 2014) 86.8 45.5 – 93.0 8 CNN-non-static (Kim, 2014) 87.2 48.0 – 93.4 9 CNN-multichannel (Kim, 2014) 88.1 47.4 – 93.2 10 NBSVM (Wang and Manning, 2012) – – – 93.2 11 MNB (Wang and Manning, 2012) – – – 93.6 12 G-Dropout (Wang and Manning, 2013) – – – 93.4 13 F-Dropout (Wang and Manning, 2013) – – – 93.6 14 SVM (Go et al.,</context>
<context position="31235" citStr="Socher et al., 2011" startWordPosition="5116" endWordPosition="5119">VCNN (-Word2Vec) 88.4 48.2 87.6 93.4 24 MVCNN (-3) 89.1 49.2 88.0 93.6 25 MVCNN (-5) 88.7 49.0 87.5 93.4 filters MVCNN (-7) 87.8 48.9 87.5 93.1 26 27 MVCNN (-9) 88.6 49.2 87.8 93.3 28 MVCNN (-mutual-learning) 88.2 49.2 87.8 93.5 tricks MVCNN (-pretraining) 87.6 48.9 87.6 93.2 29 30 MVCNN (1) 89.0 49.3 86.8 93.8 31 MVCNN (2) 89.4 49.6 87.6 93.9 layers MVCNN (3) 88.2 32 88.6 48.6 93.1 33 MVCNN (4) 87.9 48.2 88.0 92.4 34 MVCNN (overall) 89.4 49.6 88.2 93.9 Table 3: Test set results of our CNN model against other methods. RAE: Recursive Autoencoders with pretrained word embeddings from Wikipedia (Socher et al., 2011b). MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maxi</context>
</contexts>
<marker>Socher, Huang, Pennin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1600" citStr="Socher et al. (2011" startWordPosition="224" endWordPosition="227">ddition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem. In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008). A problem largely specific to NLP is how to detect features of linguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures (Dos Santos and Gatti, 2014; Kim, 2014; Denil et al., 2014). Current CNN systems usually implement a convolution layer with fixed-size filters (i.e., feature detectors), in which the concrete fil</context>
<context position="29660" citStr="Socher et al., 2011" startWordPosition="4808" endWordPosition="4811"> of MVCNN, optimized on dev. This version uses five versions of word embeddings, four filter sizes (3, 5, 7, 9), both mutual-learning and pretraining, three convolution layers for Senti140 task and two convolution layers for the other tasks. Overall, our system gets the best results, beating all baselines. The table contains five blocks from top to bottom. Each block investigates one specific configurational aspect of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when 210 Model Binary Fine-grained Senti140 Subj 1 RAE (Socher et al., 2011b) 82.4 43.2 – – 2 MV-RNN (Socher et al., 2012) 82.9 44.4 – – 3 RNTN (Socher et al., 2013) 85.4 45.7 – – 4 DCNN (Kalchbrenner et al., 2014) 86.8 48.5 87.4 – 5 Paragraph-Vec (Le and Mikolov, 2014) 87.7 48.7 – – baselines 6 CNN-rand (Kim, 2014) 82.7 45.0 – 89.6 7 CNN-static (Kim, 2014) 86.8 45.5 – 93.0 8 CNN-non-static (Kim, 2014) 87.2 48.0 – 93.4 9 CNN-multichannel (Kim, 2014) 88.1 47.4 – 93.2 10 NBSVM (Wang and Manning, 2012) – – – 93.2 11 MNB (Wang and Manning, 2012) – – – 93.6 12 G-Dropout (Wang and Manning, 2013) – – – 93.4 13 F-Dropout (Wang and Manning, 2013) – – – 93.6 14 SVM (Go et al.,</context>
<context position="31235" citStr="Socher et al., 2011" startWordPosition="5116" endWordPosition="5119">VCNN (-Word2Vec) 88.4 48.2 87.6 93.4 24 MVCNN (-3) 89.1 49.2 88.0 93.6 25 MVCNN (-5) 88.7 49.0 87.5 93.4 filters MVCNN (-7) 87.8 48.9 87.5 93.1 26 27 MVCNN (-9) 88.6 49.2 87.8 93.3 28 MVCNN (-mutual-learning) 88.2 49.2 87.8 93.5 tricks MVCNN (-pretraining) 87.6 48.9 87.6 93.2 29 30 MVCNN (1) 89.0 49.3 86.8 93.8 31 MVCNN (2) 89.4 49.6 87.6 93.9 layers MVCNN (3) 88.2 32 88.6 48.6 93.1 33 MVCNN (4) 87.9 48.2 88.0 92.4 34 MVCNN (overall) 89.4 49.6 88.2 93.9 Table 3: Test set results of our CNN model against other methods. RAE: Recursive Autoencoders with pretrained word embeddings from Wikipedia (Socher et al., 2011b). MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maxi</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011b. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="29707" citStr="Socher et al., 2012" startWordPosition="4818" endWordPosition="4821"> five versions of word embeddings, four filter sizes (3, 5, 7, 9), both mutual-learning and pretraining, three convolution layers for Senti140 task and two convolution layers for the other tasks. Overall, our system gets the best results, beating all baselines. The table contains five blocks from top to bottom. Each block investigates one specific configurational aspect of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when 210 Model Binary Fine-grained Senti140 Subj 1 RAE (Socher et al., 2011b) 82.4 43.2 – – 2 MV-RNN (Socher et al., 2012) 82.9 44.4 – – 3 RNTN (Socher et al., 2013) 85.4 45.7 – – 4 DCNN (Kalchbrenner et al., 2014) 86.8 48.5 87.4 – 5 Paragraph-Vec (Le and Mikolov, 2014) 87.7 48.7 – – baselines 6 CNN-rand (Kim, 2014) 82.7 45.0 – 89.6 7 CNN-static (Kim, 2014) 86.8 45.5 – 93.0 8 CNN-non-static (Kim, 2014) 87.2 48.0 – 93.4 9 CNN-multichannel (Kim, 2014) 88.1 47.4 – 93.2 10 NBSVM (Wang and Manning, 2012) – – – 93.2 11 MNB (Wang and Manning, 2012) – – – 93.6 12 G-Dropout (Wang and Manning, 2013) – – – 93.4 13 F-Dropout (Wang and Manning, 2013) – – – 93.6 14 SVM (Go et al., 2009) – – 81.6 – 15 BINB (Go et al., 2009) – –</context>
<context position="31324" citStr="Socher et al., 2012" startWordPosition="5128" endWordPosition="5131">7 49.0 87.5 93.4 filters MVCNN (-7) 87.8 48.9 87.5 93.1 26 27 MVCNN (-9) 88.6 49.2 87.8 93.3 28 MVCNN (-mutual-learning) 88.2 49.2 87.8 93.5 tricks MVCNN (-pretraining) 87.6 48.9 87.6 93.2 29 30 MVCNN (1) 89.0 49.3 86.8 93.8 31 MVCNN (2) 89.4 49.6 87.6 93.9 layers MVCNN (3) 88.2 32 88.6 48.6 93.1 33 MVCNN (4) 87.9 48.2 88.0 92.4 34 MVCNN (overall) 89.4 49.6 88.2 93.9 Table 3: Test set results of our CNN model against other methods. RAE: Recursive Autoencoders with pretrained word embeddings from Wikipedia (Socher et al., 2011b). MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes wi</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing,</booktitle>
<volume>1631</volume>
<pages>1642</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="23654" citStr="Socher et al., 2013" startWordPosition="3885" endWordPosition="3888">hts and the weights in fully connected layers. A dropout operation (Hinton et al., 2012) is put before the logistic regression layer. The network is trained by back-propagation in mini-batches and the gradient-based optimization is performed using the AdaGrad update rule (Duchi et al., 2011) In all data sets, the initial learning rate is 0.01, dropout probability is 0.8, L2 weight is 5 · 10−3, batch size is 50. In each convolution layer, filter sizes are {3, 5, 7, 9} and each filter has five kernels (independent of filter size). 5.2 Datasets and Experimental Setup Standard Sentiment Treebank (Socher et al., 2013). This small-scale dataset includes two tasks predicting the sentiment of movie reviews. The output variable is binary in one experiment and can have five possible outcomes in the other: {negative, somewhat negative, neutral, somewhat positive, positive}. In the binary case, we use the given split of 6920 training, 872 development and 1821 test sentences. Likewise, in the finegrained case, we use the standard 8544/1101/2210 split. Socher et al. (2013) used the Stanford Parser (Klein and Manning, 2003) to parse each sentence into subphrases. The subphrases were then labeled by human annotators </context>
<context position="29750" citStr="Socher et al., 2013" startWordPosition="4828" endWordPosition="4831">ter sizes (3, 5, 7, 9), both mutual-learning and pretraining, three convolution layers for Senti140 task and two convolution layers for the other tasks. Overall, our system gets the best results, beating all baselines. The table contains five blocks from top to bottom. Each block investigates one specific configurational aspect of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when 210 Model Binary Fine-grained Senti140 Subj 1 RAE (Socher et al., 2011b) 82.4 43.2 – – 2 MV-RNN (Socher et al., 2012) 82.9 44.4 – – 3 RNTN (Socher et al., 2013) 85.4 45.7 – – 4 DCNN (Kalchbrenner et al., 2014) 86.8 48.5 87.4 – 5 Paragraph-Vec (Le and Mikolov, 2014) 87.7 48.7 – – baselines 6 CNN-rand (Kim, 2014) 82.7 45.0 – 89.6 7 CNN-static (Kim, 2014) 86.8 45.5 – 93.0 8 CNN-non-static (Kim, 2014) 87.2 48.0 – 93.4 9 CNN-multichannel (Kim, 2014) 88.1 47.4 – 93.2 10 NBSVM (Wang and Manning, 2012) – – – 93.2 11 MNB (Wang and Manning, 2012) – – – 93.6 12 G-Dropout (Wang and Manning, 2013) – – – 93.4 13 F-Dropout (Wang and Manning, 2013) – – – 93.6 14 SVM (Go et al., 2009) – – 81.6 – 15 BINB (Go et al., 2009) – – 82.7 – 16 MAX-TDNN (Kalchbrenner et al., 2</context>
<context position="31436" citStr="Socher et al., 2013" startWordPosition="5145" endWordPosition="5148">earning) 88.2 49.2 87.8 93.5 tricks MVCNN (-pretraining) 87.6 48.9 87.6 93.2 29 30 MVCNN (1) 89.0 49.3 86.8 93.8 31 MVCNN (2) 89.4 49.6 87.6 93.9 layers MVCNN (3) 88.2 32 88.6 48.6 93.1 33 MVCNN (4) 87.9 48.2 88.0 92.4 34 MVCNN (overall) 89.4 49.6 88.2 93.9 Table 3: Test set results of our CNN model against other methods. RAE: Recursive Autoencoders with pretrained word embeddings from Wikipedia (Socher et al., 2011b). MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings ra</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing, volume 1631, page 1642. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3585" citStr="Turian et al., 2010" startWordPosition="537" endWordPosition="540">2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a). Word embeddings are derived by projecting words from a sparse, 1-of-V encoding (V : vocabulary size) onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words. Many papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. Hill et al. (2014) showed that embeddings learned by neural machine translation models outperform three repre204 Proceedings of the 19th Conference on Computational Language Learning, pages 204–214, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics sentative monolingual embedding versions: skipgram (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and C&amp;W (Collobert et al., 2011) in some cases. These prior stu</context>
<context position="7751" citStr="Turian et al. (2010)" startWordPosition="1178" endWordPosition="1181">able-size filters is further developed. Le and Mikolov (2014) initialized the representation of a sentence as a parameter vector, treating it as a global feature and combining this vector with the representations of context words to do word prediction. Finally, this fine-tuned vector is used as representation of this sentence. Apparently, this method can only produce generic sentence representations which encode no taskspecific features. Our work is also inspired by studies that compared the performance of different word embedding versions or investigated the combination of them. For example, Turian et al. (2010) compared Brown clusters, C&amp;W embeddings and HLBL embeddings in NER and chunking tasks. They found that Brown clusters and word embeddings both can improve the accuracy of supervised NLP systems; and demonstrated empirically that combining different word representations is beneficial. Luo et al. (2014) adapted CBOW (Mikolov et al., 2013a) to train word embeddings on different datasets: free text documents from Wikipedia, search click-through data and user query data, showing that combining them gets stronger results than using individual word embeddings in web search ranking and word similarit</context>
<context position="26778" citStr="Turian et al. (2010)" startWordPosition="4370" endWordPosition="4373">9 288,257 4,217 Voc size 18,876 19,612 387,877 23,926 Full hit 12,030 12,357 30,010 13,742 Partial hit 5,022 5,312 121,383 6,580 No hit 1,824 1,943 236,484 3,604 Table 2: Statistics of five embedding versions for four tasks. The first block with five rows provides the number of unknown words of each task when using corresponding version to initialize. Voc size: vocabulary size. Full hit: embedding in all 5 versions. Partial hit: embedding in 1–4 versions, No hit: not present in any of the 5 versions. (i) HLBL. Hierarchical log-bilinear model presented by Mnih and Hinton (2009) and released by Turian et al. (2010);4 size: 246,122 word embeddings; training corpus: RCV1 corpus, one year of Reuters English newswire from August 1996 to August 1997. (ii) Huang.5 Huang et al. (2012) incorporated global context to deal with challenges raised by words with multiple meanings; size: 100,232 word embeddings; training corpus: April 2010 snapshot of Wikipedia. (iii) GloVe.6 Size: 1,193,514 word embeddings; training corpus: a Twitter corpus of 2B tweets with 27B tokens. (iv) SENNA.7 Size: 130,000 word embeddings; training corpus: Wikipedia. Note that we use their 50- dimensional embeddings. (v) Word2Vec. It has no 5</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>90--94</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="30089" citStr="Wang and Manning, 2012" startWordPosition="4891" endWordPosition="4894"> system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when 210 Model Binary Fine-grained Senti140 Subj 1 RAE (Socher et al., 2011b) 82.4 43.2 – – 2 MV-RNN (Socher et al., 2012) 82.9 44.4 – – 3 RNTN (Socher et al., 2013) 85.4 45.7 – – 4 DCNN (Kalchbrenner et al., 2014) 86.8 48.5 87.4 – 5 Paragraph-Vec (Le and Mikolov, 2014) 87.7 48.7 – – baselines 6 CNN-rand (Kim, 2014) 82.7 45.0 – 89.6 7 CNN-static (Kim, 2014) 86.8 45.5 – 93.0 8 CNN-non-static (Kim, 2014) 87.2 48.0 – 93.4 9 CNN-multichannel (Kim, 2014) 88.1 47.4 – 93.2 10 NBSVM (Wang and Manning, 2012) – – – 93.2 11 MNB (Wang and Manning, 2012) – – – 93.6 12 G-Dropout (Wang and Manning, 2013) – – – 93.4 13 F-Dropout (Wang and Manning, 2013) – – – 93.6 14 SVM (Go et al., 2009) – – 81.6 – 15 BINB (Go et al., 2009) – – 82.7 – 16 MAX-TDNN (Kalchbrenner et al., 2014) – – 78.8 – 17 NBOW (Kalchbrenner et al., 2014) – – 80.9 – 18 MAXENT (Go et al., 2009) – – 83.0 – 19 MVCNN (-HLBL) 88.5 48.7 88.0 93.6 20 MVCNN (-Huang) 89.2 49.2 88.1 93.7 versions 21 MVCNN (-Glove) 88.3 48.6 87.4 93.6 22 MVCNN (-SENNA) 89.3 49.1 87.9 93.4 23 MVCNN (-Word2Vec) 88.4 48.2 87.6 93.4 24 MVCNN (-3) 89.1 49.2 88.0 93.6 25</context>
<context position="31967" citStr="Wang and Manning (2012)" startWordPosition="5220" endWordPosition="5223">ural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a “channel”) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014). G-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from Wang and Manning (2013). Minus sign “-” in MVCNN (-Huang) etc. means “Huang” is not used. “versions / filters / tricks / layers” denote the MVCNN variants with different setups: discard certain embedding version / disca</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida Wang and Christopher D Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 90–94. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher Manning</author>
</authors>
<title>Fast dropout training.</title>
<date>2013</date>
<booktitle>In Proceedings of the 30th International Conference on Machine Learning,</booktitle>
<pages>118--126</pages>
<contexts>
<context position="30181" citStr="Wang and Manning, 2013" startWordPosition="4911" endWordPosition="4914"> row 19 shows what happens when 210 Model Binary Fine-grained Senti140 Subj 1 RAE (Socher et al., 2011b) 82.4 43.2 – – 2 MV-RNN (Socher et al., 2012) 82.9 44.4 – – 3 RNTN (Socher et al., 2013) 85.4 45.7 – – 4 DCNN (Kalchbrenner et al., 2014) 86.8 48.5 87.4 – 5 Paragraph-Vec (Le and Mikolov, 2014) 87.7 48.7 – – baselines 6 CNN-rand (Kim, 2014) 82.7 45.0 – 89.6 7 CNN-static (Kim, 2014) 86.8 45.5 – 93.0 8 CNN-non-static (Kim, 2014) 87.2 48.0 – 93.4 9 CNN-multichannel (Kim, 2014) 88.1 47.4 – 93.2 10 NBSVM (Wang and Manning, 2012) – – – 93.2 11 MNB (Wang and Manning, 2012) – – – 93.6 12 G-Dropout (Wang and Manning, 2013) – – – 93.4 13 F-Dropout (Wang and Manning, 2013) – – – 93.6 14 SVM (Go et al., 2009) – – 81.6 – 15 BINB (Go et al., 2009) – – 82.7 – 16 MAX-TDNN (Kalchbrenner et al., 2014) – – 78.8 – 17 NBOW (Kalchbrenner et al., 2014) – – 80.9 – 18 MAXENT (Go et al., 2009) – – 83.0 – 19 MVCNN (-HLBL) 88.5 48.7 88.0 93.6 20 MVCNN (-Huang) 89.2 49.2 88.1 93.7 versions 21 MVCNN (-Glove) 88.3 48.6 87.4 93.6 22 MVCNN (-SENNA) 89.3 49.1 87.9 93.4 23 MVCNN (-Word2Vec) 88.4 48.2 87.6 93.4 24 MVCNN (-3) 89.1 49.2 88.0 93.6 25 MVCNN (-5) 88.7 49.0 87.5 93.4 filters MVCNN (-7) 87.8 48.9 87.5 93.1 26 27 MVCNN (-9) 88.6</context>
<context position="32371" citStr="Wang and Manning (2013)" startWordPosition="5272" endWordPosition="5275">T: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a “channel”) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014). G-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from Wang and Manning (2013). Minus sign “-” in MVCNN (-Huang) etc. means “Huang” is not used. “versions / filters / tricks / layers” denote the MVCNN variants with different setups: discard certain embedding version / discard certain filter size / discard mutual-learning or pretraining / different numbers of convolution layer. 211 HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as italic). The block “versions”</context>
</contexts>
<marker>Wang, Manning, 2013</marker>
<rawString>Sida Wang and Christopher Manning. 2013. Fast dropout training. In Proceedings of the 30th International Conference on Machine Learning, pages 118–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenpeng Yin</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>An exploration of embeddings for generalized phrases.</title>
<date>2014</date>
<booktitle>Proceedings of the 52nd annual</booktitle>
<pages>41--47</pages>
<marker>Yin, Sch¨utze, 2014</marker>
<rawString>Wenpeng Yin and Hinrich Sch¨utze. 2014. An exploration of embeddings for generalized phrases. Proceedings of the 52nd annual meeting of the association for computational linguistics, student research workshop, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Yu</author>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
<author>Stephen Pulman</author>
</authors>
<title>Deep learning for answer sentence selection. NIPS deep learning workshop.</title>
<date>2014</date>
<contexts>
<context position="6359" citStr="Yu et al. (2014)" startWordPosition="964" endWordPosition="967"> gives details of our classification model. Section 4 introduces two tricks that enhance system performance: mutual-learning and pretraining. Section 5 reports experimental results. Section 6 concludes this work. 2 Related Work Much prior work has exploited deep neural networks to model sentences. Blacoe and Lapata (2012) represented a sentence by element-wise addition, multiplication, or recursive autoencoder over embeddings of component single words. Yin and Sch¨utze (2014) extended this approach by composing on words and phrases instead of only single words. Collobert and Weston (2008) and Yu et al. (2014) used one layer of convolution over phrases detected by a sliding window on a target sentence, then used max- or average-pooling to form a sentence representation. Kalchbrenner et al. (2014) stacked multiple layers of one-dimensional convolution by dynamic kmax pooling to model sentences. We also adopt dynamic k-max pooling while our convolution layer has variable-size filters. Kim (2014) also studied multichannel representation and variable-size filters. Differently, their multichannel relies on a single version of pretrained embeddings (i.e., pretrained Word2Vec embeddings) with two copies: </context>
</contexts>
<marker>Yu, Hermann, Blunsom, Pulman, 2014</marker>
<rawString>Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep learning for answer sentence selection. NIPS deep learning workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>