<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.998243">
Learning Bilingual Linguistic Reordering Model for Statistical
Machine Translation
</title>
<author confidence="0.99733">
Han-Bin Chen, Jian-Cheng Wu and Jason S. Chang
</author>
<affiliation confidence="0.9933695">
Department of Computer Science
National Tsing Hua University
</affiliation>
<address confidence="0.66737">
101, Guangfu Road, Hsinchu, Taiwan
</address>
<email confidence="0.992832">
{hanbin,d928322,jschang}@cs.nthu.edu.tw
</email>
<sectionHeader confidence="0.996587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997936529411765">
In this paper, we propose a method for learn-
ing reordering model for BTG-based statisti-
cal machine translation (SMT). The model
focuses on linguistic features from bilingual
phrases. Our method involves extracting reor-
dering examples as well as features such as
part-of-speech and word class from aligned
parallel sentences. The features are classified
with special considerations of phrase lengths.
We then use these features to train the maxi-
mum entropy (ME) reordering model. With
the model, we performed Chinese-to-English
translation tasks. Experimental results show
that our bilingual linguistic model outper-
forms the state-of-the-art phrase-based and
BTG-based SMT systems by improvements of
2.41 and 1.31 BLEU points respectively.
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9957744">
Bracketing Transduction Grammar (BTG) is a spe-
cial case of Synchronous Context Free Grammar
(SCFG), with binary branching rules that are either
straight or inverted. BTG is widely adopted in
SMT systems, because of its good trade-off be-
tween efficiency and expressiveness (Wu, 1996).
In BTG, the ratio of legal alignments and all possi-
ble alignment in a translation pair drops drastically
especially for long sentences, yet it still covers
most of the syntactic diversities between two lan-
guages.
It is common to utilize phrase translation in
BTG systems. For example in (Xiong et al., 2006),
source sentences are segmented into phrases. Each
sequences of consecutive phrases, mapping to cells
in a CKY matrix, are then translated through a bi-
lingual phrase table and scored as implemented in
(Koehn et al., 2005; Chiang, 2005). In other words,
their system shares the same phrase table with
standard phrase-based SMT systems.
</bodyText>
<figure confidence="0.998896166666667">
three 3 年 前 after
years three
ago years
A1
A2
(a) (b)
</figure>
<figureCaption confidence="0.8597745">
Figure 1: Two reordering examples, with straight
rule applied in (a), and inverted rule in (b).
</figureCaption>
<bodyText confidence="0.999949631578947">
On the other hand, there are various proposed
BTG reordering models to predict correct orienta-
tions between neighboring blocks (bilingual
phrases). In Figure 1, for example, the role of reor-
dering model is to predict correct orientations of
neighboring blocks A1 and A2. In flat model (Wu,
1996; Zens et al., 2004; Kumar and Byrne, 2005),
reordering probabilities are assigned uniformly
during decoding, and can be tuned depending on
different language pairs. It is clear, however, that
this kind of model would suffer when the dominant
rule is wrongly applied.
Predicting orientations in BTG depending on
context information can be achieved with lexical
features. For example, Xiong et al. (2006) pro-
posed MEBTG, based on maximum entropy (ME)
classification with words as features. In MEBTG,
first words of blocks are considered as the fea-
tures, which are then used to train a ME model
</bodyText>
<figure confidence="0.887591666666667">
3 年 後
A1
A2
</figure>
<page confidence="0.981577">
254
</page>
<note confidence="0.890045">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 254–262,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.958982217391305">
for predicting orientations of neighboring blocks.
Xiong et al. (2008b) proposed a linguistically an-
notated BTG (LABTG), in which linguistic fea-
tures such as POS and syntactic labels from
source-side parse trees are used. Both MEBTG
and LABTG achieved significant improvements
over phrase-based Pharaoh (Koehn, 2004) and
Moses (Koehn et al., 2007) respectively, on Chi-
nese-to-English translation tasks.
51 �r_q p FMAJ IYJ ft-I rrj
Nes Nf Nv DE Na
tion on target one, as shown in Figure 2. Addition-
ally, features are extracted and classified
depending on lengths of blocks in order to obtain a
more informed model.
The rest of this paper is organized as follows.
Section 2 reviews the related work. Section 3 de-
scribes the model used in our BTG-based SMT
systems. Section 4 formally describes our bilingual
linguistic reordering model. Section 5 and Section
6 explain the implementation of our systems. We
show the experimental results in Section 7 and
make the conclusion in Section 8.
</bodyText>
<figure confidence="0.9839462">
the details of A2 2 Related Work
14 49 50
A1
the plan
14 18
</figure>
<figureCaption confidence="0.930182333333333">
Figure 2: An inversion reordering example, with
POS below source words, and class numbers below
target words.
</figureCaption>
<bodyText confidence="0.999681692307693">
However, current BTG-based reordering meth-
ods have been limited by the features used. Infor-
mation might not be sufficient or representative, if
only the first (or tail) words are used as features.
For example, in Figure 2, consider target first-word
features extracted from an inverted reordering ex-
ample (Xiong et al., 2006) in MEBTG, in which
first words on two blocks are both &amp;quot;the&amp;quot;. This kind
of feature set is too common and not representative
enough to predict the correct orientation. Intui-
tively, one solution is to extend the feature set by
considering both boundary words, forming a more
complete boundary description. However, this
method is still based on lexicalized features, which
causes data sparseness problem and fails to gener-
alize. In Figure 2, for example, the orientation
should basically be the same, when the
source/target words &amp;quot;ptj/plan&amp;quot; from block A1 is
replaced by other similar nouns and translations
(e.g. &amp;quot;plans&amp;quot;, &amp;quot;events&amp;quot; or &amp;quot;meetings&amp;quot;). However,
such features would be treated as unseen by the
current ME model, since the training data can not
possibly cover all such similar cases.
In this paper we present an improved reorder-
ing model based on BTG, with bilingual linguistic
features from neighboring blocks. To avoid data
sparseness problem, both source and target words
are classified; we perform part-of-speech (POS)
tagging on source language, and word classifica-
In statistical machine translation, reordering model
is concerned with predicting correct orders of tar-
get language sentence given a source language one
and translation pairs. For example, in phrase-based
SMT systems (Koehn et al., 2003; Koehn, 2004),
distortion model is used, in which reordering prob-
abilities depend on relative positions of target side
phrases between adjacent blocks. However, distor-
tion model can not model long-distance reordering,
due to the lack of context information, thus is diffi-
cult to predict correct orders under different cir-
cumstances. Therefore, while phrase-based SMT
moves from words to phrases as the basic unit of
translation, implying effective local reordering
within phrases, it suffers when determining phrase
reordering, especially when phrases are longer than
three words (Koehn et al., 2003).
There have been much effort made to improve
reordering model in SMT. For example, research-
ers have been studying CKY parsing over the last
decade, which considers translations and orienta-
tions of two neighboring block according to
grammar rules or context information. In hierar-
chical phrase-based systems (Chiang, 2005), for
example, SCFG rules are automatically learned
from aligned bilingual corpus, and are applied in
CKY style decoding.
As an another application of CKY parsing tech-
nique is BTG-based SMT. Xiong et al. (2006) and
Xiong et al. (2008a) developed MEBTG systems,
in which first or tail words from reordering exam-
ples are used as features to train ME-based reorder-
ing models.
Similarly, Zhang et al. (2007) proposed a model
similar to BTG, which uses first/tail words of
phrases, and syntactic labels (e.g. NP and VP)
</bodyText>
<page confidence="0.996386">
255
</page>
<bodyText confidence="0.999905451612903">
from source parse trees as features. In their work,
however, inverted rules are allowed to apply only
when source phrases are syntactic; for non-
syntactic ones, blocks are combined straight with a
constant score.
More recently, Xiong et al. (2008b) proposed
LABTG, which incorporates linguistic knowledge
by adding features such as syntactic labels and
POS from source trees to improve their MEBTG.
Different from Zhang&apos;s work, their model do not
restrict non-syntactic phrases, and applies inverted
rules on any pair of neighboring blocks.
Although POS information is used in LABTG
and Zhang&apos;s work, their models are syntax-oriented,
since they focus on syntactic labels. Boundary POS
is considered in LABTG only when source phrases
are not syntactic phrases.
In contrast to the previous works, we present a
reordering model for BTG that uses bilingual in-
formation including class-level features of POS
and word classes. Moreover, our model is dedi-
cated to boundary features and considers different
combinations of phrase lengths, rather than only
first/tail words. In addition, current state-of-the-art
Chinese parsers, including the one used in LABTG
(Xiong et al., 2005), lag beyond in inaccuracy,
compared with English parsers (Klein and Man-
ning, 2003; Petrov and Klein 2007). In our work,
we only use more reliable information such as
Chinese word segmentation and POS tagging (Ma
and Chen, 2003).
</bodyText>
<sectionHeader confidence="0.997929" genericHeader="method">
3 The Model
</sectionHeader>
<bodyText confidence="0.998186333333333">
Following Wu (1996) and Xiong et al. (2006), we
implement BTG-based SMT as our system, in
which three rules are applied during decoding:
</bodyText>
<equation confidence="0.9665965">
A A1 A2 (1)
A x/y (3)
</equation>
<bodyText confidence="0.981530368421053">
where A1 and A2 are blocks in source order. Straight
rule (1) and inverted rule (2) are reordering rules.
They are applied for predicting target-side order
when combining two blocks, and form the reorder-
ing model with the distributions
A1, A2, order)&apos;
where order {straight, inverted}.
In MEBTG, a ME reordering model is trained
using features extracted from reordering examples
of aligned parallel corpus. First words on neighbor-
ing blocks are used as features. In reordering ex-
ample (a), for example, the feature set is
{&amp;quot;S1L=three&amp;quot;, &amp;quot;S2L=ago&amp;quot;, &amp;quot;T1L=3&amp;quot;, &amp;quot;T2L=前&amp;quot;}
where &amp;quot;S1&amp;quot; and &amp;quot;T1&amp;quot; denote source and target
phrases from the block A1.
Rule (3) is lexical translation rule, which trans-
lates source phrase x into target phrase y. We use
the same feature functions as typical phrase-based
SMT systems (Koehn et al., 2005):
</bodyText>
<equation confidence="0.8794122">
Ptrans
4 e5 e y 6
where plw (x  |y) 3  plw (y  |x)4 , 5
e and 6
e y 
</equation>
<bodyText confidence="0.999878714285714">
are lexical translation probabilities in both direc-
tions, phrase penalty and word penalty.
During decoding, the blocks are produced by
applying either one of two reordering rules on two
smaller blocks, or applying lexical rule (3) on
some source phrase. Therefore, the score of a block
A is defined as
</bodyText>
<equation confidence="0.995438875">
P( ) P( ) P( )
A  A A

1 2
)  P ( , , )
lm A A order
reo 1 2
P(A)  Plm (A)lm  Ptrans (x  |y
</equation>
<bodyText confidence="0.978519666666667">
lm are respec-
tively the usual and incremental score of language
model.
To tune all lambda weights above, we perform
minimum error rate training (Och, 2003) on the
development set described in Section 7.
Let B be the set of all blocks with source side
sentence C. Then the best translation of C is the
target side of the block A , where
</bodyText>
<figure confidence="0.979455842105263">
Preo(
( x  |y ) 

|
2
1

3

(
 |y
 |y
(y
p
p
x
x
x

)
(
)
)
 plw
plw
(  |)
y x
or Plm (A1 , A2
reo
)
A  A1 A2 (2)
where Plm ( ) and
A  lm P lm( 1, 2)
A A
256
A argmax P( A

AB
</figure>
<sectionHeader confidence="0.955892" genericHeader="method">
4 Bilingual Linguistic Model
</sectionHeader>
<bodyText confidence="0.9983575">
In this section, we formally describe the problem
we want to address and the proposed method.
</bodyText>
<subsectionHeader confidence="0.996544">
4.1 Problem Statement
</subsectionHeader>
<bodyText confidence="0.995576857142857">
We focus on extracting features representative of
the two neighboring blocks being considered for
reordering by the decoder, as described in Section
3. We define S(A) and T(A) as the information on
source and target side of a block A. For two
neighboring blocks A1 and A2, the set of features
extracted from information of them is denoted as
feature set function F(S(A1), S(A2), T(A1), S(A2)). In
Figure 1 (b), for example, S(A1) and T(A1) are sim-
ply the both sides sentences &amp;quot;3 年&amp;quot; and &amp;quot;three
years&amp;quot;, and F(S(A1), S(A2), T(A1), S(A2)) is
{&amp;quot;S1L=three&amp;quot;, &amp;quot;S2L=after&amp;quot;, &amp;quot;T1L=3&amp;quot;, &amp;quot;T2L=後&amp;quot;}
where &amp;quot;S1L&amp;quot; denotes the first source word on the
block A1, and &amp;quot;T2L&amp;quot; denotes the first target word
on the block A2.
Given the adjacent blocks A1 and A2, our goal
includes (1) adding more linguistic and representa-
tive information to A1 and A2 and (2) finding a fea-
ture set function F&apos; based on added linguistic
information in order to train a more linguistically
motivated and effective model.
</bodyText>
<subsectionHeader confidence="0.978458">
4.2 Word Classification
</subsectionHeader>
<bodyText confidence="0.99927176923077">
As described in Section 1, designing a more com-
plete feature set causes data sparseness problem, if
we use lexical features. One natural solution is us-
ing POS and word class features.
In our model, we perform Chinese POS tagging
on source language. In Xiong et al. (2008b) and
Zhang et al. (2007), Chinese parsers with Penn
Chinese Treebank (Xue et al., 2005) style are used
to derive source parse trees, from which source-
side features such as POS are extracted. However,
due to the relatively low accuracy of current Chi-
nese parsers compared with English ones, we in-
stead use CKIP Chinese word segmentation system
(Ma and Chen, 2003) in order to derive Chinese
tags with high accuracy. Moreover, compared with
the Treebank Chinese tagset, the CKIP tagset pro-
vides more fine-grained tags, including many tags
with semantic information (e.g., Nc for place
nouns, Nd for time nouns), and verb transitivity
and subcategorization (e.g., VA for intransitive
verbs, VC for transitive verbs, VK for verbs that
take a clause as object).
On the other hand, using the POS features in
combination with the lexical features in target lan-
guage will cause another sparseness problem in the
phrase table, since one source phrase would map to
multiple target ones with different POS sequences.
As an alternative, we use mkcls toolkit (Och,
1999), which uses maximum-likelihood principle
to perform classification on target side. After clas-
sification, the toolkit produces a many-to-one
mapping between English tokens and class num-
bers. Therefore, there is no ambiguity of word
class in target phrases and word class features can
be used independently to avoid data sparseness
problem and the phrase table remains unchanged.
As mentioned in Section 1, features based on
words are not representative enough in some cases,
and tend to cause sparseness problem. By classify-
ing words we are able to linguistically generalize
the features, and hence predict the rules more
robustly. In Figure 2, for example, the target words
are converted to corresponding classes, and form
the more complete boundary feature set
{&amp;quot;T1L=14&amp;quot;, &amp;quot;T1R=18&amp;quot;, &amp;quot;T2L=14&amp;quot;, &amp;quot;T2R=50&amp;quot;} (4)
In the feature set (4), #14 is the class containing
&amp;quot;the&amp;quot;, #18 is the class containing &amp;quot;plans&amp;quot;, and #50
is the class containing &amp;quot;of.&amp;quot; Note that we add last-
word features &amp;quot;T1R=18&amp;quot; and &amp;quot;T2R=50&amp;quot;. As men-
tioned in Section 1, the word &amp;quot;plan&amp;quot; from block A1
is replaceable with similar nouns. This extends to
other nominal word classes to realize the general
rule of inverting &amp;quot;the ... NOUN&amp;quot; and &amp;quot;the ... of&amp;quot;.
It is hard to achieve this kind of generality using
only lexicalized feature. With word classification,
we gather feature sets with similar concepts from
the training data. Table 1 shows the word classes
can be used effectively to cope with data sparse-
ness. For example, the feature set (4) occurs 309
times in our training data, and only 2 of them are
straight, with the remaining 307 inverted examples,
implying that similar features based on word
classes lead to similar orientation. Additional ex-
amples of similar feature sets with different word
classes are shown in Table 1.
</bodyText>
<equation confidence="0.491829">
)
</equation>
<page confidence="0.916733">
257
</page>
<table confidence="0.994780666666667">
A1 A2 A1 A2
class X T1R = X straight/inverted
9 graph, government 2/488 我 認為 基於 這些 原因
18 plans, events 2/307 Nh VE P Neqa Na
20 bikes, motors 0/694
I think for these reasons
(a) (b)
M class L class
48 day, month, year 4/510
</table>
<tableCaption confidence="0.8660585">
Table 1: List of feature sets in the form of
{&amp;quot;T1L=14&amp;quot;, &amp;quot;T1R=X&amp;quot;, &amp;quot;T2L=14&amp;quot;, &amp;quot;T2R=50&amp;quot;}. A1 A2 A1 A2
</tableCaption>
<table confidence="0.822252">
技術 和 設備 在 約旦 舉行 會議
Na Caa Na P Nc VC Na
4.3 Feature with Length Consideration technology and equipment in Jordan hold meeting
</table>
<bodyText confidence="0.995479">
Boundary features using both the first and last
words provide more detailed descriptions of
neighboring blocks. However, we should take the
special case blocks with length 1 into consideration.
For example, consider two features sets from
straight and inverted reordering examples (a) and
(b) in Figure 3. There are two identical source fea-
tures in both feature set, since first words on block
A1 and last words on block A2 are the same:
</bodyText>
<equation confidence="0.600896">
{&amp;quot;S1L=P&amp;quot;,&amp;quot;S2R=Na&amp;quot;} F(S(A1),S(A2),T(A1), S(A2))
</equation>
<bodyText confidence="0.999938875">
Therefore, without distinguishing the special case,
the features would represent quite different cases
with the same feature, possibly leading to failure to
predict orientations of two blocks.
We propose a method to alleviate the problem of
features with considerations of lengths of two ad-
jacent phrases by classifying both the both source
and target phrase pairs into one of four classes: M,
L, R and B, corresponding to different combina-
tions of phrase lengths.
Suppose we are given two neighboring blocks
A1 and A2, with source phrases P1 and P2 respec-
tively. Then the feature set from source side is
classified into one of the classes as follows. We
give examples of feature set for each class accord-
ing to Figure 4.
</bodyText>
<figure confidence="0.988016">
(a) (b)
</figure>
<figureCaption confidence="0.992799">
Figure 3: Two reordering examples with ambigu-
ous features on source side.
</figureCaption>
<figure confidence="0.994568">
(c) (d)
R class B class
</figure>
<figureCaption confidence="0.9997665">
Figure 4: Examples of different length combina-
tions, mapping to four classes.
1. M class. The lengths of P1 and P2 are both 1. In
Figure 4 (a), for example, the feature set is
</figureCaption>
<bodyText confidence="0.790423">
{&amp;quot;M1=Nh&amp;quot;, &amp;quot;M2=VE&amp;quot;}
</bodyText>
<listItem confidence="0.840554">
2. L class. The length of P1 is 1, and the length of
P2 is greater than 1. In Figure 4 (b), for exam-
ple, the feature set is
{&amp;quot;L1=P&amp;quot;, &amp;quot;L2=Neqa&amp;quot;, &amp;quot;L3=Na&amp;quot;}
3. R class. The length of P1 is greater than 1, and
the length of P2 is 1. In Figure 4 (c), for exam-
ple, the feature set is
</listItem>
<bodyText confidence="0.9592535">
{&amp;quot;R1=Na&amp;quot;, &amp;quot;R2=Caa&amp;quot;, &amp;quot;R3=Na&amp;quot;}
4. B class. The lengths of P1 and P2 are both
greater than 1. In Figure 4 (d), for example, the
feature set is
{&amp;quot;B1=P&amp;quot;, &amp;quot;B2=Nc&amp;quot;, &amp;quot;B3=VC&amp;quot;, &amp;quot;B4=Na&amp;quot;}
We use the same scheme to classify the two tar-
get phrases. Since both source and target words are
classified as described in Section 4.2, the feature
sets are more representative and tend to lead to
consistent prediction of orientation. Additionally,
the length-based features are easy to fit into mem-
ory, in contrast to lexical features in MEBTG.
To summarize, we extract features based on
word lengths, target-language word classes, and
fine-grained, semantic oriented parts of speech. To
illustrate, we use the neighboring blocks from Fig-
</bodyText>
<figure confidence="0.983449789473684">
for
M411-
P
A1
&apos;&amp;quot;L_ 11�
Neqa Na
A2
hold
meeting
in
jordan
these
reasons
在 約旦
P Nc
舉行 會議
VC Na
A2
A1
</figure>
<page confidence="0.993172">
258
</page>
<bodyText confidence="0.989282666666667">
ure 2 to show an example of complete bilingual
linguistic feature set:
{&amp;quot;S.B1=Nes&amp;quot;, &amp;quot;S.B2=Nv&amp;quot;, &amp;quot;S.B3=DE&amp;quot;,
&amp;quot;S.B4=Na&amp;quot;, &amp;quot;T.B1=14&amp;quot;, &amp;quot;T.B2=18&amp;quot;, &amp;quot;T.B3=14&amp;quot;,
&amp;quot;T.B4=50&amp;quot;}
where &amp;quot;S.&amp;quot; and &amp;quot;T.&amp;quot; denote source and target sides.
In the next section, we describe the process of
preparing the feature data and training an ME
model. In Section 7, we perform evaluations of this
ME-based reordering model against standard
phrase-based SMT and previous work based on
ME and BTG.
</bodyText>
<sectionHeader confidence="0.991007" genericHeader="method">
5 Training
</sectionHeader>
<bodyText confidence="0.999986352941176">
In order to train the translation and reordering
model, we first set up Moses SMT system (Koehn
et al., 2007). We obtain aligned parallel sentences
and the phrase table after the training of Moses,
which includes running GIZA++ (Och and Ney,
2003), grow-diagonal-final symmetrization and
phrase extraction (Koehn et al., 2005). Our system
shares the same translation model with Moses,
since we directly use the phrase table to apply
translation rules (3).
On the other side, we use the aligned parallel
sentences to train our reordering model, which in-
cludes classifying words, extracting bilingual
phrase samples with orientation information, and
training an ME model for predicting orientation.
To perform word classification, the source sen-
tences are tagged and segmented before the Moses
training. As for target side, we ran the Moses
scripts to classify target language words using the
mkcls toolkit before running GIZA++. Therefore,
we directly use its classification result, which gen-
erate 50 classes with 2 optimization runs on the
target sentences.
To extract the reordering examples, we choose
sentence pairs with top 50% alignment scores pro-
vided by GIZA++, in order to fit into memory.
Then the extraction is performed on these aligned
sentence pairs, together with POS tags and word
classes, using basically the algorithm presented in
Xiong et al. (2006). However, we enumerate all
reordering examples, rather than only extract the
smallest straight and largest inverted examples.
Finally, we use the toolkit by Zhang (2004) to train
the ME model with extracted reordering examples.
</bodyText>
<sectionHeader confidence="0.97894" genericHeader="method">
6 Decoding
</sectionHeader>
<bodyText confidence="0.9964621">
We develop a bottom-up CKY style decoder in our
system, similar to Chiang (2005). For a Chinese
sentence C, the decoder finds its best translation on
the block with entire C on source side. The decoder
first applies translation rules (3) on cells in a CKY
matrix. Each cell denotes a sequence of source
phrases, and contains all of the blocks with possi-
ble translations. The longest length of source
phrase to be applied translations rules is restricted
to 7 words, in accordance with the default settings
of Moses training scripts.
To reduce the search space, we apply threshold
pruning and histogram pruning, in which the block
scoring worse than 10-2 times the best block in the
same cell or scoring worse than top 40 highest
scores would be pruned. These pruning techniques
are common in SMT systems. We also apply re-
combination, which distinguish blocks in a cell
only by 3 leftmost and rightmost target words, as
suggested in (Xiong et al., 2006).
</bodyText>
<sectionHeader confidence="0.983002" genericHeader="evaluation">
7 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999547115384615">
We perform Chinese-to-English translation task
on NIST MT-06 test set, and use Moses and
MEBTG as our competitors.
The bilingual training data containing 2.2M sen-
tences pairs from Hong Kong Parallel Text
(LDC2004T08) and Xinhua News Agency
(LDC2007T09), with length shorter than 60, is
used to train the translation and reordering model.
The source sentences are tagged and segmented
with CKIP Chinese word segmentation system (Ma
and Chen, 2003).
About 35M reordering examples are extracted
from top 1.1M sentence pairs with higher align-
ment scores. We generate 171K features for lexi-
calized model used in MEBTG system, and 1.41K
features for our proposed reordering model.
For our language model, we use Xinhua news
from English Gigaword Third Edition
(LDC2007T07) to build a trigram model with
SRILM toolkit (Stolcke, 2002).
Our development set for running minimum error
rate training is NIST MT-08 test set, with sentence
lengths no more than 20. We report the experimen-
tal results on NIST MT-06 test set. Our evaluation
metric is BLEU (Papineni et al., 2002) with case-
insensitive matching from unigram to four-gram.
</bodyText>
<page confidence="0.995021">
259
</page>
<table confidence="0.9754048">
System BLEU-4
Moses(distortion) 22.55
Moses(lexicalized) 23.42
MEBTG 23.65
WC+LC 24.96
</table>
<tableCaption confidence="0.955355">
Table 2: Performances of various systems.
</tableCaption>
<table confidence="0.994797333333333">
System Feature size BLEU-4
MEBTG 171K 23.65
WC+MEBTG 0.24K 23.79
</table>
<tableCaption confidence="0.9032335">
Table 3: Performances of lexicalized and word
classified MEBTG.
The overall result of our experiment is shown in
Table 2. The lexicalized MEBTG system proposed
</tableCaption>
<bodyText confidence="0.991816692307692">
by Xiong et al. (2006) uses first words on adjacent
blocks as lexical features, and outperforms phrase-
based Moses with default distortion model and en-
hanced lexicalized model, by 1.1 and 0.23 BLEU
points respectively. This suggests lexicalized
Moses and MEBTG with context information out-
performs distance-based distortion model. Besides,
MEBTG with structure constraints has better
global reordering estimation than unstructured
Moses, while incorporating their local reordering
ability by using phrase tables.
The proposed reordering model trained with
word classification (WC) and length consideration
(LC) described in Section 4 outperforms MEBTG
by 1.31 point. This suggests our proposed model
not only reduces the model size by using 1% fewer
features than MEBTG, but also improves the trans-
lation quality.
We also evaluate the impacts of WC and LC
separately and show the results in Table 3-5. Table
3 shows the result of MEBTG with word classified
features. While classified MEBTG only improves
0.14 points over original lexicalized one, it drasti-
cally reduces the feature size. This implies WC
alleviates data sparseness by generalizing the ob-
served features.
Table 4 compares different length considerations,
including boundary model demonstrated in Section
4.2, and the proposed LC in Section 4.3. Although
boundary model describes features better than us-
ing only first words, which we will show later, it
suffers from data sparseness with twice feature size
of MEBTG. The LC model has the largest feature
size but performs best among three systems, sug-
gesting the effectiveness of our LC.
In Table 5 we show the impacts of WC and LC
together. Note that all the systems with WC sig-
nificantly reduce the size of features compared to
lexicalized ones.
</bodyText>
<table confidence="0.9799255">
System Feature size BLEU-4
MEBTG 171K 23.65
Boundary 349K 23.42
LC 780K 23.86
</table>
<tableCaption confidence="0.9925865">
Table 4: Performances of BTG systems with dif-
ferent representativeness.
</tableCaption>
<table confidence="0.9931516">
System Feature size BLEU-4
MEBTG 171K 23.65
WC+MEBTG 0.24K 23.79
WC+Bounary 0.48K 24.29
WC+LC 1.41K 24.96
</table>
<tableCaption confidence="0.817673">
Table 5: Different representativeness with word
classification.
</tableCaption>
<bodyText confidence="0.9992946">
While boundary model is worse than first-word
MEBTG in Table 4, it outperforms the latter when
both are performed WC. We obtain the best result
that outperforms the baseline MEBTG by more
than 1 point when we apply WC and LC together.
Our experimental results show that we are able
to ameliorate the sparseness problem by classifying
words, and produce more representative features
by considering phrase length. Moreover, they are
both important, in that we are unable to outperform
our competitors by a large margin unless we com-
bine both WC and LC. In conclusion, while de-
signing more representative features of reordering
model in SMT, we have to find solutions to gener-
alize them.
</bodyText>
<sectionHeader confidence="0.989672" genericHeader="conclusions">
8 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.999831125">
We have proposed a bilingual linguistic reordering
model to improve current BTG-based SMT sys-
tems, based on two drawbacks of previously pro-
posed reordering model, which are sparseness and
representative problem.
First, to solve the sparseness problem in previ-
ously proposed lexicalized model, we perform
word classification on both sides.
</bodyText>
<page confidence="0.977125">
260
</page>
<bodyText confidence="0.999955083333333">
Secondly, we present a more representative fea-
ture extraction method. This involves considering
length combinations of adjacent phrases.
The experimental results of Chinese-to-English
task show that our model outperforms baseline
phrase-based and BTG systems.
We will investigate more linguistic ways to clas-
sify words in future work, especially on target lan-
guage. For example, using word hierarchical
structures in WordNet (Fellbaum, 1998) system
provides more linguistic and semantic information
than statistically-motivated classification tools.
</bodyText>
<sectionHeader confidence="0.994125" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.978111333333333">
This work was supported by National Science
Council of Taiwan grant NSC 95-2221-E-007-182-
MY3.
</bodyText>
<sectionHeader confidence="0.997361" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999904936708861">
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pp. 263-270.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Massachusetts.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT/NAACL 2003.
Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder
for Phrased-Based Statistical Machine Translation
Models. In Proceedings of AMTA 2004.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In International Workshop on Spoken Language
Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan,Wade Shen, Christine Moran, Rich-
ard Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
strantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL 2007, Demonstration Session.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of ACL 2003.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of HLT-EMNLP 2005.
Wei-Yun Ma and Keh-Jiann Chen. 2003. Introduction
to CKIP Chinese Word Segmentation System for the
First International Chinese Word Segmentation
Bakeoff. In Proceedings of ACL, Second SIGHAN
Workshop on Chinese Language Processing, pp168-
171.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In EACL ’99: Ninth
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 71–76,
Bergen, Norway, June.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29:19-51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL 2003, pages 160-167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the ACL, pages 311–318.
Slav Petrov and Dan Klein. 2007. Improved Inference-
for Unlexicalized Parsing. In Proceedings of HLT-
NAACL 2007.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901–904.
Dekai Wu. 1996. A Polynomial-Time Algorithm for
Statistical Machine Translation. In Proceedings of
ACL 1996.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the Penn Chinese tree-
bank with semantic knowledge. In Proceedings of
IJCNLP 2005, pages 70-81.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for
Statistical Machine Translation. In Proceedings of
ACL-COLING 2006.
Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu,
and Shouxun Liu. 2008a. Refinements in BTG-based
statistical machine translation. In Proceedings of
IJCNLP 2008, pp. 505-512.
Deyi Xiong, Min Zhang, Ai Ti Aw, and Haizhou Li.
2008b. Linguistically Annotated BTG for Statistical
Machine Translation. In Proceedings of COLING
2008.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
</reference>
<page confidence="0.964532">
261
</page>
<reference confidence="0.994688307692308">
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004.
Reordering Constraints for Phrase-Based Statistical
Machine Translation. In Proceedings of CoLing 2004,
Geneva, Switzerland, pp. 205-211.
Le Zhang. 2004. Maximum Entropy Modeling Toolkit
for Python and C++. Available at http://homepa
ges.inf.ed.ac.uk/s0450736/maxent_toolkit.html.
Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou.
2007. Phrase Reordering Model Integrating Syntactic
Knowledge for SMT. In Proceedings of EMNLP-
CoNLL 2007.
</reference>
<page confidence="0.997291">
262
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.822268">
<title confidence="0.9990825">Learning Bilingual Linguistic Reordering Model for Machine Translation</title>
<author confidence="0.999978">Wu S Chang</author>
<affiliation confidence="0.960917">Department of Computer National Tsing Hua</affiliation>
<address confidence="0.997919">101, Guangfu Road, Hsinchu,</address>
<email confidence="0.981357">hanbin@cs.nthu.edu.tw</email>
<email confidence="0.981357">d928322@cs.nthu.edu.tw</email>
<email confidence="0.981357">jschang@cs.nthu.edu.tw</email>
<abstract confidence="0.993066222222222">In this paper, we propose a method for learning reordering model for BTG-based statistical machine translation (SMT). The model focuses on linguistic features from bilingual phrases. Our method involves extracting reordering examples as well as features such as part-of-speech and word class from aligned parallel sentences. The features are classified with special considerations of phrase lengths. We then use these features to train the maximum entropy (ME) reordering model. With the model, we performed Chinese-to-English translation tasks. Experimental results show that our bilingual linguistic model outperforms the state-of-the-art phrase-based and BTG-based SMT systems by improvements of 2.41 and 1.31 BLEU points respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1856" citStr="Chiang, 2005" startWordPosition="273" endWordPosition="274"> its good trade-off between efficiency and expressiveness (Wu, 1996). In BTG, the ratio of legal alignments and all possible alignment in a translation pair drops drastically especially for long sentences, yet it still covers most of the syntactic diversities between two languages. It is common to utilize phrase translation in BTG systems. For example in (Xiong et al., 2006), source sentences are segmented into phrases. Each sequences of consecutive phrases, mapping to cells in a CKY matrix, are then translated through a bilingual phrase table and scored as implemented in (Koehn et al., 2005; Chiang, 2005). In other words, their system shares the same phrase table with standard phrase-based SMT systems. three 3 年 前 after years three ago years A1 A2 (a) (b) Figure 1: Two reordering examples, with straight rule applied in (a), and inverted rule in (b). On the other hand, there are various proposed BTG reordering models to predict correct orientations between neighboring blocks (bilingual phrases). In Figure 1, for example, the role of reordering model is to predict correct orientations of neighboring blocks A1 and A2. In flat model (Wu, 1996; Zens et al., 2004; Kumar and Byrne, 2005), reordering </context>
<context position="6918" citStr="Chiang, 2005" startWordPosition="1081" endWordPosition="1082">ifferent circumstances. Therefore, while phrase-based SMT moves from words to phrases as the basic unit of translation, implying effective local reordering within phrases, it suffers when determining phrase reordering, especially when phrases are longer than three words (Koehn et al., 2003). There have been much effort made to improve reordering model in SMT. For example, researchers have been studying CKY parsing over the last decade, which considers translations and orientations of two neighboring block according to grammar rules or context information. In hierarchical phrase-based systems (Chiang, 2005), for example, SCFG rules are automatically learned from aligned bilingual corpus, and are applied in CKY style decoding. As an another application of CKY parsing technique is BTG-based SMT. Xiong et al. (2006) and Xiong et al. (2008a) developed MEBTG systems, in which first or tail words from reordering examples are used as features to train ME-based reordering models. Similarly, Zhang et al. (2007) proposed a model similar to BTG, which uses first/tail words of phrases, and syntactic labels (e.g. NP and VP) 255 from source parse trees as features. In their work, however, inverted rules are a</context>
<context position="20318" citStr="Chiang (2005)" startWordPosition="3385" endWordPosition="3386">eordering examples, we choose sentence pairs with top 50% alignment scores provided by GIZA++, in order to fit into memory. Then the extraction is performed on these aligned sentence pairs, together with POS tags and word classes, using basically the algorithm presented in Xiong et al. (2006). However, we enumerate all reordering examples, rather than only extract the smallest straight and largest inverted examples. Finally, we use the toolkit by Zhang (2004) to train the ME model with extracted reordering examples. 6 Decoding We develop a bottom-up CKY style decoder in our system, similar to Chiang (2005). For a Chinese sentence C, the decoder finds its best translation on the block with entire C on source side. The decoder first applies translation rules (3) on cells in a CKY matrix. Each cell denotes a sequence of source phrases, and contains all of the blocks with possible translations. The longest length of source phrase to be applied translations rules is restricted to 7 words, in accordance with the default settings of Moses training scripts. To reduce the search space, we apply threshold pruning and histogram pruning, in which the block scoring worse than 10-2 times the best block in th</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL 2005, pp. 263-270.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL</booktitle>
<contexts>
<context position="5995" citStr="Koehn et al., 2003" startWordPosition="941" endWordPosition="944">by the current ME model, since the training data can not possibly cover all such similar cases. In this paper we present an improved reordering model based on BTG, with bilingual linguistic features from neighboring blocks. To avoid data sparseness problem, both source and target words are classified; we perform part-of-speech (POS) tagging on source language, and word classificaIn statistical machine translation, reordering model is concerned with predicting correct orders of target language sentence given a source language one and translation pairs. For example, in phrase-based SMT systems (Koehn et al., 2003; Koehn, 2004), distortion model is used, in which reordering probabilities depend on relative positions of target side phrases between adjacent blocks. However, distortion model can not model long-distance reordering, due to the lack of context information, thus is difficult to predict correct orders under different circumstances. Therefore, while phrase-based SMT moves from words to phrases as the basic unit of translation, implying effective local reordering within phrases, it suffers when determining phrase reordering, especially when phrases are longer than three words (Koehn et al., 2003</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of HLT/NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a Beam Search Decoder for Phrased-Based Statistical Machine Translation Models.</title>
<date>2004</date>
<booktitle>In Proceedings of AMTA</booktitle>
<contexts>
<context position="3517" citStr="Koehn, 2004" startWordPosition="539" endWordPosition="540">words of blocks are considered as the features, which are then used to train a ME model 3 年 後 A1 A2 254 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 254–262, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics for predicting orientations of neighboring blocks. Xiong et al. (2008b) proposed a linguistically annotated BTG (LABTG), in which linguistic features such as POS and syntactic labels from source-side parse trees are used. Both MEBTG and LABTG achieved significant improvements over phrase-based Pharaoh (Koehn, 2004) and Moses (Koehn et al., 2007) respectively, on Chinese-to-English translation tasks. 51 �r_q p FMAJ IYJ ft-I rrj Nes Nf Nv DE Na tion on target one, as shown in Figure 2. Additionally, features are extracted and classified depending on lengths of blocks in order to obtain a more informed model. The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 describes the model used in our BTG-based SMT systems. Section 4 formally describes our bilingual linguistic reordering model. Section 5 and Section 6 explain the implementation of our systems. We show the ex</context>
<context position="6009" citStr="Koehn, 2004" startWordPosition="945" endWordPosition="946">del, since the training data can not possibly cover all such similar cases. In this paper we present an improved reordering model based on BTG, with bilingual linguistic features from neighboring blocks. To avoid data sparseness problem, both source and target words are classified; we perform part-of-speech (POS) tagging on source language, and word classificaIn statistical machine translation, reordering model is concerned with predicting correct orders of target language sentence given a source language one and translation pairs. For example, in phrase-based SMT systems (Koehn et al., 2003; Koehn, 2004), distortion model is used, in which reordering probabilities depend on relative positions of target side phrases between adjacent blocks. However, distortion model can not model long-distance reordering, due to the lack of context information, thus is difficult to predict correct orders under different circumstances. Therefore, while phrase-based SMT moves from words to phrases as the basic unit of translation, implying effective local reordering within phrases, it suffers when determining phrase reordering, especially when phrases are longer than three words (Koehn et al., 2003). There have </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder for Phrased-Based Statistical Machine Translation Models. In Proceedings of AMTA 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>Edinburgh System Description for the</booktitle>
<contexts>
<context position="1841" citStr="Koehn et al., 2005" startWordPosition="269" endWordPosition="272"> systems, because of its good trade-off between efficiency and expressiveness (Wu, 1996). In BTG, the ratio of legal alignments and all possible alignment in a translation pair drops drastically especially for long sentences, yet it still covers most of the syntactic diversities between two languages. It is common to utilize phrase translation in BTG systems. For example in (Xiong et al., 2006), source sentences are segmented into phrases. Each sequences of consecutive phrases, mapping to cells in a CKY matrix, are then translated through a bilingual phrase table and scored as implemented in (Koehn et al., 2005; Chiang, 2005). In other words, their system shares the same phrase table with standard phrase-based SMT systems. three 3 年 前 after years three ago years A1 A2 (a) (b) Figure 1: Two reordering examples, with straight rule applied in (a), and inverted rule in (b). On the other hand, there are various proposed BTG reordering models to predict correct orientations between neighboring blocks (bilingual phrases). In Figure 1, for example, the role of reordering model is to predict correct orientations of neighboring blocks A1 and A2. In flat model (Wu, 1996; Zens et al., 2004; Kumar and Byrne, 200</context>
<context position="9836" citStr="Koehn et al., 2005" startWordPosition="1550" endWordPosition="1553">ring model with the distributions A1, A2, order)&apos; where order {straight, inverted}. In MEBTG, a ME reordering model is trained using features extracted from reordering examples of aligned parallel corpus. First words on neighboring blocks are used as features. In reordering example (a), for example, the feature set is {&amp;quot;S1L=three&amp;quot;, &amp;quot;S2L=ago&amp;quot;, &amp;quot;T1L=3&amp;quot;, &amp;quot;T2L=前&amp;quot;} where &amp;quot;S1&amp;quot; and &amp;quot;T1&amp;quot; denote source and target phrases from the block A1. Rule (3) is lexical translation rule, which translates source phrase x into target phrase y. We use the same feature functions as typical phrase-based SMT systems (Koehn et al., 2005): Ptrans 4 e5 e y 6 where plw (x |y) 3  plw (y |x)4 , 5 e and 6 e y  are lexical translation probabilities in both directions, phrase penalty and word penalty. During decoding, the blocks are produced by applying either one of two reordering rules on two smaller blocks, or applying lexical rule (3) on some source phrase. Therefore, the score of a block A is defined as P( ) P( ) P( ) A  A A  1 2 )  P ( , , ) lm A A order reo 1 2 P(A)  Plm (A)lm  Ptrans (x |y lm are respectively the usual and incremental score of language model. To tune all lambda weights above, we perform minimu</context>
<context position="18957" citStr="Koehn et al., 2005" startWordPosition="3169" endWordPosition="3172">&amp;quot;T.&amp;quot; denote source and target sides. In the next section, we describe the process of preparing the feature data and training an ME model. In Section 7, we perform evaluations of this ME-based reordering model against standard phrase-based SMT and previous work based on ME and BTG. 5 Training In order to train the translation and reordering model, we first set up Moses SMT system (Koehn et al., 2007). We obtain aligned parallel sentences and the phrase table after the training of Moses, which includes running GIZA++ (Och and Ney, 2003), grow-diagonal-final symmetrization and phrase extraction (Koehn et al., 2005). Our system shares the same translation model with Moses, since we directly use the phrase table to apply translation rules (3). On the other side, we use the aligned parallel sentences to train our reordering model, which includes classifying words, extracting bilingual phrase samples with orientation information, and training an ME model for predicting orientation. To perform word classification, the source sentences are tagged and segmented before the Moses training. As for target side, we ran the Moses scripts to classify target language words using the mkcls toolkit before running GIZA++</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL 2007, Demonstration Session.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constrantin, and</location>
<contexts>
<context position="3548" citStr="Koehn et al., 2007" startWordPosition="543" endWordPosition="546">idered as the features, which are then used to train a ME model 3 年 後 A1 A2 254 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 254–262, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics for predicting orientations of neighboring blocks. Xiong et al. (2008b) proposed a linguistically annotated BTG (LABTG), in which linguistic features such as POS and syntactic labels from source-side parse trees are used. Both MEBTG and LABTG achieved significant improvements over phrase-based Pharaoh (Koehn, 2004) and Moses (Koehn et al., 2007) respectively, on Chinese-to-English translation tasks. 51 �r_q p FMAJ IYJ ft-I rrj Nes Nf Nv DE Na tion on target one, as shown in Figure 2. Additionally, features are extracted and classified depending on lengths of blocks in order to obtain a more informed model. The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 describes the model used in our BTG-based SMT systems. Section 4 formally describes our bilingual linguistic reordering model. Section 5 and Section 6 explain the implementation of our systems. We show the experimental results in Section 7</context>
<context position="18740" citStr="Koehn et al., 2007" startWordPosition="3138" endWordPosition="3141"> reasons 在 約旦 P Nc 舉行 會議 VC Na A2 A1 258 ure 2 to show an example of complete bilingual linguistic feature set: {&amp;quot;S.B1=Nes&amp;quot;, &amp;quot;S.B2=Nv&amp;quot;, &amp;quot;S.B3=DE&amp;quot;, &amp;quot;S.B4=Na&amp;quot;, &amp;quot;T.B1=14&amp;quot;, &amp;quot;T.B2=18&amp;quot;, &amp;quot;T.B3=14&amp;quot;, &amp;quot;T.B4=50&amp;quot;} where &amp;quot;S.&amp;quot; and &amp;quot;T.&amp;quot; denote source and target sides. In the next section, we describe the process of preparing the feature data and training an ME model. In Section 7, we perform evaluations of this ME-based reordering model against standard phrase-based SMT and previous work based on ME and BTG. 5 Training In order to train the translation and reordering model, we first set up Moses SMT system (Koehn et al., 2007). We obtain aligned parallel sentences and the phrase table after the training of Moses, which includes running GIZA++ (Och and Ney, 2003), grow-diagonal-final symmetrization and phrase extraction (Koehn et al., 2005). Our system shares the same translation model with Moses, since we directly use the phrase table to apply translation rules (3). On the other side, we use the aligned parallel sentences to train our reordering model, which includes classifying words, extracting bilingual phrase samples with orientation information, and training an ME model for predicting orientation. To perform w</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constrantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL 2007, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="8692" citStr="Klein and Manning, 2003" startWordPosition="1358" endWordPosition="1362">, since they focus on syntactic labels. Boundary POS is considered in LABTG only when source phrases are not syntactic phrases. In contrast to the previous works, we present a reordering model for BTG that uses bilingual information including class-level features of POS and word classes. Moreover, our model is dedicated to boundary features and considers different combinations of phrase lengths, rather than only first/tail words. In addition, current state-of-the-art Chinese parsers, including the one used in LABTG (Xiong et al., 2005), lag beyond in inaccuracy, compared with English parsers (Klein and Manning, 2003; Petrov and Klein 2007). In our work, we only use more reliable information such as Chinese word segmentation and POS tagging (Ma and Chen, 2003). 3 The Model Following Wu (1996) and Xiong et al. (2006), we implement BTG-based SMT as our system, in which three rules are applied during decoding: A A1 A2 (1) A x/y (3) where A1 and A2 are blocks in source order. Straight rule (1) and inverted rule (2) are reordering rules. They are applied for predicting target-side order when combining two blocks, and form the reordering model with the distributions A1, A2, order)&apos; where order {straight, i</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<contexts>
<context position="2443" citStr="Kumar and Byrne, 2005" startWordPosition="371" endWordPosition="374"> (Koehn et al., 2005; Chiang, 2005). In other words, their system shares the same phrase table with standard phrase-based SMT systems. three 3 年 前 after years three ago years A1 A2 (a) (b) Figure 1: Two reordering examples, with straight rule applied in (a), and inverted rule in (b). On the other hand, there are various proposed BTG reordering models to predict correct orientations between neighboring blocks (bilingual phrases). In Figure 1, for example, the role of reordering model is to predict correct orientations of neighboring blocks A1 and A2. In flat model (Wu, 1996; Zens et al., 2004; Kumar and Byrne, 2005), reordering probabilities are assigned uniformly during decoding, and can be tuned depending on different language pairs. It is clear, however, that this kind of model would suffer when the dominant rule is wrongly applied. Predicting orientations in BTG depending on context information can be achieved with lexical features. For example, Xiong et al. (2006) proposed MEBTG, based on maximum entropy (ME) classification with words as features. In MEBTG, first words of blocks are considered as the features, which are then used to train a ME model 3 年 後 A1 A2 254 Human Language Technologies: The 2</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proceedings of HLT-EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Yun Ma</author>
<author>Keh-Jiann Chen</author>
</authors>
<title>Introduction to CKIP Chinese Word Segmentation System for the First International Chinese Word Segmentation Bakeoff.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL, Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>168--171</pages>
<contexts>
<context position="8838" citStr="Ma and Chen, 2003" startWordPosition="1384" endWordPosition="1387">ious works, we present a reordering model for BTG that uses bilingual information including class-level features of POS and word classes. Moreover, our model is dedicated to boundary features and considers different combinations of phrase lengths, rather than only first/tail words. In addition, current state-of-the-art Chinese parsers, including the one used in LABTG (Xiong et al., 2005), lag beyond in inaccuracy, compared with English parsers (Klein and Manning, 2003; Petrov and Klein 2007). In our work, we only use more reliable information such as Chinese word segmentation and POS tagging (Ma and Chen, 2003). 3 The Model Following Wu (1996) and Xiong et al. (2006), we implement BTG-based SMT as our system, in which three rules are applied during decoding: A A1 A2 (1) A x/y (3) where A1 and A2 are blocks in source order. Straight rule (1) and inverted rule (2) are reordering rules. They are applied for predicting target-side order when combining two blocks, and form the reordering model with the distributions A1, A2, order)&apos; where order {straight, inverted}. In MEBTG, a ME reordering model is trained using features extracted from reordering examples of aligned parallel corpus. First words on </context>
<context position="12616" citStr="Ma and Chen, 2003" startWordPosition="2090" endWordPosition="2093">Section 1, designing a more complete feature set causes data sparseness problem, if we use lexical features. One natural solution is using POS and word class features. In our model, we perform Chinese POS tagging on source language. In Xiong et al. (2008b) and Zhang et al. (2007), Chinese parsers with Penn Chinese Treebank (Xue et al., 2005) style are used to derive source parse trees, from which sourceside features such as POS are extracted. However, due to the relatively low accuracy of current Chinese parsers compared with English ones, we instead use CKIP Chinese word segmentation system (Ma and Chen, 2003) in order to derive Chinese tags with high accuracy. Moreover, compared with the Treebank Chinese tagset, the CKIP tagset provides more fine-grained tags, including many tags with semantic information (e.g., Nc for place nouns, Nd for time nouns), and verb transitivity and subcategorization (e.g., VA for intransitive verbs, VC for transitive verbs, VK for verbs that take a clause as object). On the other hand, using the POS features in combination with the lexical features in target language will cause another sparseness problem in the phrase table, since one source phrase would map to multipl</context>
<context position="21664" citStr="Ma and Chen, 2003" startWordPosition="3606" endWordPosition="3609">also apply recombination, which distinguish blocks in a cell only by 3 leftmost and rightmost target words, as suggested in (Xiong et al., 2006). 7 Experiments and Results We perform Chinese-to-English translation task on NIST MT-06 test set, and use Moses and MEBTG as our competitors. The bilingual training data containing 2.2M sentences pairs from Hong Kong Parallel Text (LDC2004T08) and Xinhua News Agency (LDC2007T09), with length shorter than 60, is used to train the translation and reordering model. The source sentences are tagged and segmented with CKIP Chinese word segmentation system (Ma and Chen, 2003). About 35M reordering examples are extracted from top 1.1M sentence pairs with higher alignment scores. We generate 171K features for lexicalized model used in MEBTG system, and 1.41K features for our proposed reordering model. For our language model, we use Xinhua news from English Gigaword Third Edition (LDC2007T07) to build a trigram model with SRILM toolkit (Stolcke, 2002). Our development set for running minimum error rate training is NIST MT-08 test set, with sentence lengths no more than 20. We report the experimental results on NIST MT-06 test set. Our evaluation metric is BLEU (Papin</context>
</contexts>
<marker>Ma, Chen, 2003</marker>
<rawString>Wei-Yun Ma and Keh-Jiann Chen. 2003. Introduction to CKIP Chinese Word Segmentation System for the First International Chinese Word Segmentation Bakeoff. In Proceedings of ACL, Second SIGHAN Workshop on Chinese Language Processing, pp168-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In EACL ’99: Ninth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>71--76</pages>
<location>Bergen, Norway,</location>
<contexts>
<context position="13311" citStr="Och, 1999" startWordPosition="2204" endWordPosition="2205">bank Chinese tagset, the CKIP tagset provides more fine-grained tags, including many tags with semantic information (e.g., Nc for place nouns, Nd for time nouns), and verb transitivity and subcategorization (e.g., VA for intransitive verbs, VC for transitive verbs, VK for verbs that take a clause as object). On the other hand, using the POS features in combination with the lexical features in target language will cause another sparseness problem in the phrase table, since one source phrase would map to multiple target ones with different POS sequences. As an alternative, we use mkcls toolkit (Och, 1999), which uses maximum-likelihood principle to perform classification on target side. After classification, the toolkit produces a many-to-one mapping between English tokens and class numbers. Therefore, there is no ambiguity of word class in target phrases and word class features can be used independently to avoid data sparseness problem and the phrase table remains unchanged. As mentioned in Section 1, features based on words are not representative enough in some cases, and tend to cause sparseness problem. By classifying words we are able to linguistically generalize the features, and hence p</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz Josef Och. 1999. An efficient method for determining bilingual word classes. In EACL ’99: Ninth Conference of the European Chapter of the Association for Computational Linguistics, pages 71–76, Bergen, Norway, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="18878" citStr="Och and Ney, 2003" startWordPosition="3160" endWordPosition="3163">B3=DE&amp;quot;, &amp;quot;S.B4=Na&amp;quot;, &amp;quot;T.B1=14&amp;quot;, &amp;quot;T.B2=18&amp;quot;, &amp;quot;T.B3=14&amp;quot;, &amp;quot;T.B4=50&amp;quot;} where &amp;quot;S.&amp;quot; and &amp;quot;T.&amp;quot; denote source and target sides. In the next section, we describe the process of preparing the feature data and training an ME model. In Section 7, we perform evaluations of this ME-based reordering model against standard phrase-based SMT and previous work based on ME and BTG. 5 Training In order to train the translation and reordering model, we first set up Moses SMT system (Koehn et al., 2007). We obtain aligned parallel sentences and the phrase table after the training of Moses, which includes running GIZA++ (Och and Ney, 2003), grow-diagonal-final symmetrization and phrase extraction (Koehn et al., 2005). Our system shares the same translation model with Moses, since we directly use the phrase table to apply translation rules (3). On the other side, we use the aligned parallel sentences to train our reordering model, which includes classifying words, extracting bilingual phrase samples with orientation information, and training an ME model for predicting orientation. To perform word classification, the source sentences are tagged and segmented before the Moses training. As for target side, we ran the Moses scripts </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29:19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="10469" citStr="Och, 2003" startWordPosition="1686" endWordPosition="1687">where plw (x |y) 3  plw (y |x)4 , 5 e and 6 e y  are lexical translation probabilities in both directions, phrase penalty and word penalty. During decoding, the blocks are produced by applying either one of two reordering rules on two smaller blocks, or applying lexical rule (3) on some source phrase. Therefore, the score of a block A is defined as P( ) P( ) P( ) A  A A  1 2 )  P ( , , ) lm A A order reo 1 2 P(A)  Plm (A)lm  Ptrans (x |y lm are respectively the usual and incremental score of language model. To tune all lambda weights above, we perform minimum error rate training (Och, 2003) on the development set described in Section 7. Let B be the set of all blocks with source side sentence C. Then the best translation of C is the target side of the block A , where Preo( ( x |y )   | 2 1  3  ( |y |y (y p p x x x  ) ( ) )  plw plw ( |) y x or Plm (A1 , A2 reo ) A  A1 A2 (2) where Plm ( ) and A  lm P lm( 1, 2) A A 256 A argmax P( A  AB 4 Bilingual Linguistic Model In this section, we formally describe the problem we want to address and the proposed method. 4.1 Problem Statement We focus on extracting features representative of the two neighboring blocks being cons</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL 2003, pages 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="22281" citStr="Papineni et al., 2002" startWordPosition="3707" endWordPosition="3710">2003). About 35M reordering examples are extracted from top 1.1M sentence pairs with higher alignment scores. We generate 171K features for lexicalized model used in MEBTG system, and 1.41K features for our proposed reordering model. For our language model, we use Xinhua news from English Gigaword Third Edition (LDC2007T07) to build a trigram model with SRILM toolkit (Stolcke, 2002). Our development set for running minimum error rate training is NIST MT-08 test set, with sentence lengths no more than 20. We report the experimental results on NIST MT-06 test set. Our evaluation metric is BLEU (Papineni et al., 2002) with caseinsensitive matching from unigram to four-gram. 259 System BLEU-4 Moses(distortion) 22.55 Moses(lexicalized) 23.42 MEBTG 23.65 WC+LC 24.96 Table 2: Performances of various systems. System Feature size BLEU-4 MEBTG 171K 23.65 WC+MEBTG 0.24K 23.79 Table 3: Performances of lexicalized and word classified MEBTG. The overall result of our experiment is shown in Table 2. The lexicalized MEBTG system proposed by Xiong et al. (2006) uses first words on adjacent blocks as lexical features, and outperforms phrasebased Moses with default distortion model and enhanced lexicalized model, by 1.1 a</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inferencefor Unlexicalized Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL</booktitle>
<contexts>
<context position="8716" citStr="Petrov and Klein 2007" startWordPosition="1363" endWordPosition="1366">tactic labels. Boundary POS is considered in LABTG only when source phrases are not syntactic phrases. In contrast to the previous works, we present a reordering model for BTG that uses bilingual information including class-level features of POS and word classes. Moreover, our model is dedicated to boundary features and considers different combinations of phrase lengths, rather than only first/tail words. In addition, current state-of-the-art Chinese parsers, including the one used in LABTG (Xiong et al., 2005), lag beyond in inaccuracy, compared with English parsers (Klein and Manning, 2003; Petrov and Klein 2007). In our work, we only use more reliable information such as Chinese word segmentation and POS tagging (Ma and Chen, 2003). 3 The Model Following Wu (1996) and Xiong et al. (2006), we implement BTG-based SMT as our system, in which three rules are applied during decoding: A A1 A2 (1) A x/y (3) where A1 and A2 are blocks in source order. Straight rule (1) and inverted rule (2) are reordering rules. They are applied for predicting target-side order when combining two blocks, and form the reordering model with the distributions A1, A2, order)&apos; where order {straight, inverted}. In MEBTG, a ME</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inferencefor Unlexicalized Parsing. In Proceedings of HLTNAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="22044" citStr="Stolcke, 2002" startWordPosition="3668" endWordPosition="3669">T08) and Xinhua News Agency (LDC2007T09), with length shorter than 60, is used to train the translation and reordering model. The source sentences are tagged and segmented with CKIP Chinese word segmentation system (Ma and Chen, 2003). About 35M reordering examples are extracted from top 1.1M sentence pairs with higher alignment scores. We generate 171K features for lexicalized model used in MEBTG system, and 1.41K features for our proposed reordering model. For our language model, we use Xinhua news from English Gigaword Third Edition (LDC2007T07) to build a trigram model with SRILM toolkit (Stolcke, 2002). Our development set for running minimum error rate training is NIST MT-08 test set, with sentence lengths no more than 20. We report the experimental results on NIST MT-06 test set. Our evaluation metric is BLEU (Papineni et al., 2002) with caseinsensitive matching from unigram to four-gram. 259 System BLEU-4 Moses(distortion) 22.55 Moses(lexicalized) 23.42 MEBTG 23.65 WC+LC 24.96 Table 2: Performances of various systems. System Feature size BLEU-4 MEBTG 171K 23.65 WC+MEBTG 0.24K 23.79 Table 3: Performances of lexicalized and word classified MEBTG. The overall result of our experiment is sho</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A Polynomial-Time Algorithm for Statistical Machine Translation.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1311" citStr="Wu, 1996" startWordPosition="183" endWordPosition="184">to train the maximum entropy (ME) reordering model. With the model, we performed Chinese-to-English translation tasks. Experimental results show that our bilingual linguistic model outperforms the state-of-the-art phrase-based and BTG-based SMT systems by improvements of 2.41 and 1.31 BLEU points respectively. 1 Introduction Bracketing Transduction Grammar (BTG) is a special case of Synchronous Context Free Grammar (SCFG), with binary branching rules that are either straight or inverted. BTG is widely adopted in SMT systems, because of its good trade-off between efficiency and expressiveness (Wu, 1996). In BTG, the ratio of legal alignments and all possible alignment in a translation pair drops drastically especially for long sentences, yet it still covers most of the syntactic diversities between two languages. It is common to utilize phrase translation in BTG systems. For example in (Xiong et al., 2006), source sentences are segmented into phrases. Each sequences of consecutive phrases, mapping to cells in a CKY matrix, are then translated through a bilingual phrase table and scored as implemented in (Koehn et al., 2005; Chiang, 2005). In other words, their system shares the same phrase t</context>
<context position="8871" citStr="Wu (1996)" startWordPosition="1392" endWordPosition="1393">for BTG that uses bilingual information including class-level features of POS and word classes. Moreover, our model is dedicated to boundary features and considers different combinations of phrase lengths, rather than only first/tail words. In addition, current state-of-the-art Chinese parsers, including the one used in LABTG (Xiong et al., 2005), lag beyond in inaccuracy, compared with English parsers (Klein and Manning, 2003; Petrov and Klein 2007). In our work, we only use more reliable information such as Chinese word segmentation and POS tagging (Ma and Chen, 2003). 3 The Model Following Wu (1996) and Xiong et al. (2006), we implement BTG-based SMT as our system, in which three rules are applied during decoding: A A1 A2 (1) A x/y (3) where A1 and A2 are blocks in source order. Straight rule (1) and inverted rule (2) are reordering rules. They are applied for predicting target-side order when combining two blocks, and form the reordering model with the distributions A1, A2, order)&apos; where order {straight, inverted}. In MEBTG, a ME reordering model is trained using features extracted from reordering examples of aligned parallel corpus. First words on neighboring blocks are used as fe</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Dekai Wu. 1996. A Polynomial-Time Algorithm for Statistical Machine Translation. In Proceedings of ACL 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
<author>Yueliang Qian</author>
</authors>
<title>Parsing the Penn Chinese treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<pages>70--81</pages>
<contexts>
<context position="8610" citStr="Xiong et al., 2005" startWordPosition="1346" endWordPosition="1349">nformation is used in LABTG and Zhang&apos;s work, their models are syntax-oriented, since they focus on syntactic labels. Boundary POS is considered in LABTG only when source phrases are not syntactic phrases. In contrast to the previous works, we present a reordering model for BTG that uses bilingual information including class-level features of POS and word classes. Moreover, our model is dedicated to boundary features and considers different combinations of phrase lengths, rather than only first/tail words. In addition, current state-of-the-art Chinese parsers, including the one used in LABTG (Xiong et al., 2005), lag beyond in inaccuracy, compared with English parsers (Klein and Manning, 2003; Petrov and Klein 2007). In our work, we only use more reliable information such as Chinese word segmentation and POS tagging (Ma and Chen, 2003). 3 The Model Following Wu (1996) and Xiong et al. (2006), we implement BTG-based SMT as our system, in which three rules are applied during decoding: A A1 A2 (1) A x/y (3) where A1 and A2 are blocks in source order. Straight rule (1) and inverted rule (2) are reordering rules. They are applied for predicting target-side order when combining two blocks, and form the</context>
</contexts>
<marker>Xiong, Li, Liu, Lin, Qian, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian. 2005. Parsing the Penn Chinese treebank with semantic knowledge. In Proceedings of IJCNLP 2005, pages 70-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-COLING</booktitle>
<contexts>
<context position="1620" citStr="Xiong et al., 2006" startWordPosition="233" endWordPosition="236">espectively. 1 Introduction Bracketing Transduction Grammar (BTG) is a special case of Synchronous Context Free Grammar (SCFG), with binary branching rules that are either straight or inverted. BTG is widely adopted in SMT systems, because of its good trade-off between efficiency and expressiveness (Wu, 1996). In BTG, the ratio of legal alignments and all possible alignment in a translation pair drops drastically especially for long sentences, yet it still covers most of the syntactic diversities between two languages. It is common to utilize phrase translation in BTG systems. For example in (Xiong et al., 2006), source sentences are segmented into phrases. Each sequences of consecutive phrases, mapping to cells in a CKY matrix, are then translated through a bilingual phrase table and scored as implemented in (Koehn et al., 2005; Chiang, 2005). In other words, their system shares the same phrase table with standard phrase-based SMT systems. three 3 年 前 after years three ago years A1 A2 (a) (b) Figure 1: Two reordering examples, with straight rule applied in (a), and inverted rule in (b). On the other hand, there are various proposed BTG reordering models to predict correct orientations between neighb</context>
<context position="4682" citStr="Xiong et al., 2006" startWordPosition="736" endWordPosition="739">lain the implementation of our systems. We show the experimental results in Section 7 and make the conclusion in Section 8. the details of A2 2 Related Work 14 49 50 A1 the plan 14 18 Figure 2: An inversion reordering example, with POS below source words, and class numbers below target words. However, current BTG-based reordering methods have been limited by the features used. Information might not be sufficient or representative, if only the first (or tail) words are used as features. For example, in Figure 2, consider target first-word features extracted from an inverted reordering example (Xiong et al., 2006) in MEBTG, in which first words on two blocks are both &amp;quot;the&amp;quot;. This kind of feature set is too common and not representative enough to predict the correct orientation. Intuitively, one solution is to extend the feature set by considering both boundary words, forming a more complete boundary description. However, this method is still based on lexicalized features, which causes data sparseness problem and fails to generalize. In Figure 2, for example, the orientation should basically be the same, when the source/target words &amp;quot;ptj/plan&amp;quot; from block A1 is replaced by other similar nouns and translat</context>
<context position="7128" citStr="Xiong et al. (2006)" startWordPosition="1113" endWordPosition="1116">se reordering, especially when phrases are longer than three words (Koehn et al., 2003). There have been much effort made to improve reordering model in SMT. For example, researchers have been studying CKY parsing over the last decade, which considers translations and orientations of two neighboring block according to grammar rules or context information. In hierarchical phrase-based systems (Chiang, 2005), for example, SCFG rules are automatically learned from aligned bilingual corpus, and are applied in CKY style decoding. As an another application of CKY parsing technique is BTG-based SMT. Xiong et al. (2006) and Xiong et al. (2008a) developed MEBTG systems, in which first or tail words from reordering examples are used as features to train ME-based reordering models. Similarly, Zhang et al. (2007) proposed a model similar to BTG, which uses first/tail words of phrases, and syntactic labels (e.g. NP and VP) 255 from source parse trees as features. In their work, however, inverted rules are allowed to apply only when source phrases are syntactic; for nonsyntactic ones, blocks are combined straight with a constant score. More recently, Xiong et al. (2008b) proposed LABTG, which incorporates linguist</context>
<context position="8895" citStr="Xiong et al. (2006)" startWordPosition="1395" endWordPosition="1398">ses bilingual information including class-level features of POS and word classes. Moreover, our model is dedicated to boundary features and considers different combinations of phrase lengths, rather than only first/tail words. In addition, current state-of-the-art Chinese parsers, including the one used in LABTG (Xiong et al., 2005), lag beyond in inaccuracy, compared with English parsers (Klein and Manning, 2003; Petrov and Klein 2007). In our work, we only use more reliable information such as Chinese word segmentation and POS tagging (Ma and Chen, 2003). 3 The Model Following Wu (1996) and Xiong et al. (2006), we implement BTG-based SMT as our system, in which three rules are applied during decoding: A A1 A2 (1) A x/y (3) where A1 and A2 are blocks in source order. Straight rule (1) and inverted rule (2) are reordering rules. They are applied for predicting target-side order when combining two blocks, and form the reordering model with the distributions A1, A2, order)&apos; where order {straight, inverted}. In MEBTG, a ME reordering model is trained using features extracted from reordering examples of aligned parallel corpus. First words on neighboring blocks are used as features. In reordering ex</context>
<context position="19998" citStr="Xiong et al. (2006)" startWordPosition="3333" endWordPosition="3336">s are tagged and segmented before the Moses training. As for target side, we ran the Moses scripts to classify target language words using the mkcls toolkit before running GIZA++. Therefore, we directly use its classification result, which generate 50 classes with 2 optimization runs on the target sentences. To extract the reordering examples, we choose sentence pairs with top 50% alignment scores provided by GIZA++, in order to fit into memory. Then the extraction is performed on these aligned sentence pairs, together with POS tags and word classes, using basically the algorithm presented in Xiong et al. (2006). However, we enumerate all reordering examples, rather than only extract the smallest straight and largest inverted examples. Finally, we use the toolkit by Zhang (2004) to train the ME model with extracted reordering examples. 6 Decoding We develop a bottom-up CKY style decoder in our system, similar to Chiang (2005). For a Chinese sentence C, the decoder finds its best translation on the block with entire C on source side. The decoder first applies translation rules (3) on cells in a CKY matrix. Each cell denotes a sequence of source phrases, and contains all of the blocks with possible tra</context>
<context position="22719" citStr="Xiong et al. (2006)" startWordPosition="3772" endWordPosition="3775">ining is NIST MT-08 test set, with sentence lengths no more than 20. We report the experimental results on NIST MT-06 test set. Our evaluation metric is BLEU (Papineni et al., 2002) with caseinsensitive matching from unigram to four-gram. 259 System BLEU-4 Moses(distortion) 22.55 Moses(lexicalized) 23.42 MEBTG 23.65 WC+LC 24.96 Table 2: Performances of various systems. System Feature size BLEU-4 MEBTG 171K 23.65 WC+MEBTG 0.24K 23.79 Table 3: Performances of lexicalized and word classified MEBTG. The overall result of our experiment is shown in Table 2. The lexicalized MEBTG system proposed by Xiong et al. (2006) uses first words on adjacent blocks as lexical features, and outperforms phrasebased Moses with default distortion model and enhanced lexicalized model, by 1.1 and 0.23 BLEU points respectively. This suggests lexicalized Moses and MEBTG with context information outperforms distance-based distortion model. Besides, MEBTG with structure constraints has better global reordering estimation than unstructured Moses, while incorporating their local reordering ability by using phrase tables. The proposed reordering model trained with word classification (WC) and length consideration (LC) described in</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In Proceedings of ACL-COLING 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
<author>Shouxun Liu</author>
</authors>
<title>Refinements in BTG-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<pages>505--512</pages>
<contexts>
<context position="3270" citStr="Xiong et al. (2008" startWordPosition="501" endWordPosition="504">rongly applied. Predicting orientations in BTG depending on context information can be achieved with lexical features. For example, Xiong et al. (2006) proposed MEBTG, based on maximum entropy (ME) classification with words as features. In MEBTG, first words of blocks are considered as the features, which are then used to train a ME model 3 年 後 A1 A2 254 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 254–262, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics for predicting orientations of neighboring blocks. Xiong et al. (2008b) proposed a linguistically annotated BTG (LABTG), in which linguistic features such as POS and syntactic labels from source-side parse trees are used. Both MEBTG and LABTG achieved significant improvements over phrase-based Pharaoh (Koehn, 2004) and Moses (Koehn et al., 2007) respectively, on Chinese-to-English translation tasks. 51 �r_q p FMAJ IYJ ft-I rrj Nes Nf Nv DE Na tion on target one, as shown in Figure 2. Additionally, features are extracted and classified depending on lengths of blocks in order to obtain a more informed model. The rest of this paper is organized as follows. Section</context>
<context position="7151" citStr="Xiong et al. (2008" startWordPosition="1118" endWordPosition="1121">y when phrases are longer than three words (Koehn et al., 2003). There have been much effort made to improve reordering model in SMT. For example, researchers have been studying CKY parsing over the last decade, which considers translations and orientations of two neighboring block according to grammar rules or context information. In hierarchical phrase-based systems (Chiang, 2005), for example, SCFG rules are automatically learned from aligned bilingual corpus, and are applied in CKY style decoding. As an another application of CKY parsing technique is BTG-based SMT. Xiong et al. (2006) and Xiong et al. (2008a) developed MEBTG systems, in which first or tail words from reordering examples are used as features to train ME-based reordering models. Similarly, Zhang et al. (2007) proposed a model similar to BTG, which uses first/tail words of phrases, and syntactic labels (e.g. NP and VP) 255 from source parse trees as features. In their work, however, inverted rules are allowed to apply only when source phrases are syntactic; for nonsyntactic ones, blocks are combined straight with a constant score. More recently, Xiong et al. (2008b) proposed LABTG, which incorporates linguistic knowledge by adding </context>
<context position="12252" citStr="Xiong et al. (2008" startWordPosition="2028" endWordPosition="2031">denotes the first target word on the block A2. Given the adjacent blocks A1 and A2, our goal includes (1) adding more linguistic and representative information to A1 and A2 and (2) finding a feature set function F&apos; based on added linguistic information in order to train a more linguistically motivated and effective model. 4.2 Word Classification As described in Section 1, designing a more complete feature set causes data sparseness problem, if we use lexical features. One natural solution is using POS and word class features. In our model, we perform Chinese POS tagging on source language. In Xiong et al. (2008b) and Zhang et al. (2007), Chinese parsers with Penn Chinese Treebank (Xue et al., 2005) style are used to derive source parse trees, from which sourceside features such as POS are extracted. However, due to the relatively low accuracy of current Chinese parsers compared with English ones, we instead use CKIP Chinese word segmentation system (Ma and Chen, 2003) in order to derive Chinese tags with high accuracy. Moreover, compared with the Treebank Chinese tagset, the CKIP tagset provides more fine-grained tags, including many tags with semantic information (e.g., Nc for place nouns, Nd for t</context>
</contexts>
<marker>Xiong, Zhang, Aw, Mi, Liu, Liu, 2008</marker>
<rawString>Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu, and Shouxun Liu. 2008a. Refinements in BTG-based statistical machine translation. In Proceedings of IJCNLP 2008, pp. 505-512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Ai Ti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>Linguistically Annotated BTG for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="3270" citStr="Xiong et al. (2008" startWordPosition="501" endWordPosition="504">rongly applied. Predicting orientations in BTG depending on context information can be achieved with lexical features. For example, Xiong et al. (2006) proposed MEBTG, based on maximum entropy (ME) classification with words as features. In MEBTG, first words of blocks are considered as the features, which are then used to train a ME model 3 年 後 A1 A2 254 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 254–262, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics for predicting orientations of neighboring blocks. Xiong et al. (2008b) proposed a linguistically annotated BTG (LABTG), in which linguistic features such as POS and syntactic labels from source-side parse trees are used. Both MEBTG and LABTG achieved significant improvements over phrase-based Pharaoh (Koehn, 2004) and Moses (Koehn et al., 2007) respectively, on Chinese-to-English translation tasks. 51 �r_q p FMAJ IYJ ft-I rrj Nes Nf Nv DE Na tion on target one, as shown in Figure 2. Additionally, features are extracted and classified depending on lengths of blocks in order to obtain a more informed model. The rest of this paper is organized as follows. Section</context>
<context position="7151" citStr="Xiong et al. (2008" startWordPosition="1118" endWordPosition="1121">y when phrases are longer than three words (Koehn et al., 2003). There have been much effort made to improve reordering model in SMT. For example, researchers have been studying CKY parsing over the last decade, which considers translations and orientations of two neighboring block according to grammar rules or context information. In hierarchical phrase-based systems (Chiang, 2005), for example, SCFG rules are automatically learned from aligned bilingual corpus, and are applied in CKY style decoding. As an another application of CKY parsing technique is BTG-based SMT. Xiong et al. (2006) and Xiong et al. (2008a) developed MEBTG systems, in which first or tail words from reordering examples are used as features to train ME-based reordering models. Similarly, Zhang et al. (2007) proposed a model similar to BTG, which uses first/tail words of phrases, and syntactic labels (e.g. NP and VP) 255 from source parse trees as features. In their work, however, inverted rules are allowed to apply only when source phrases are syntactic; for nonsyntactic ones, blocks are combined straight with a constant score. More recently, Xiong et al. (2008b) proposed LABTG, which incorporates linguistic knowledge by adding </context>
<context position="12252" citStr="Xiong et al. (2008" startWordPosition="2028" endWordPosition="2031">denotes the first target word on the block A2. Given the adjacent blocks A1 and A2, our goal includes (1) adding more linguistic and representative information to A1 and A2 and (2) finding a feature set function F&apos; based on added linguistic information in order to train a more linguistically motivated and effective model. 4.2 Word Classification As described in Section 1, designing a more complete feature set causes data sparseness problem, if we use lexical features. One natural solution is using POS and word class features. In our model, we perform Chinese POS tagging on source language. In Xiong et al. (2008b) and Zhang et al. (2007), Chinese parsers with Penn Chinese Treebank (Xue et al., 2005) style are used to derive source parse trees, from which sourceside features such as POS are extracted. However, due to the relatively low accuracy of current Chinese parsers compared with English ones, we instead use CKIP Chinese word segmentation system (Ma and Chen, 2003) in order to derive Chinese tags with high accuracy. Moreover, compared with the Treebank Chinese tagset, the CKIP tagset provides more fine-grained tags, including many tags with semantic information (e.g., Nc for place nouns, Nd for t</context>
</contexts>
<marker>Xiong, Zhang, Aw, Li, 2008</marker>
<rawString>Deyi Xiong, Min Zhang, Ai Ti Aw, and Haizhou Li. 2008b. Linguistically Annotated BTG for Statistical Machine Translation. In Proceedings of COLING 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="12341" citStr="Xue et al., 2005" startWordPosition="2043" endWordPosition="2046">al includes (1) adding more linguistic and representative information to A1 and A2 and (2) finding a feature set function F&apos; based on added linguistic information in order to train a more linguistically motivated and effective model. 4.2 Word Classification As described in Section 1, designing a more complete feature set causes data sparseness problem, if we use lexical features. One natural solution is using POS and word class features. In our model, we perform Chinese POS tagging on source language. In Xiong et al. (2008b) and Zhang et al. (2007), Chinese parsers with Penn Chinese Treebank (Xue et al., 2005) style are used to derive source parse trees, from which sourceside features such as POS are extracted. However, due to the relatively low accuracy of current Chinese parsers compared with English ones, we instead use CKIP Chinese word segmentation system (Ma and Chen, 2003) in order to derive Chinese tags with high accuracy. Moreover, compared with the Treebank Chinese tagset, the CKIP tagset provides more fine-grained tags, including many tags with semantic information (e.g., Nc for place nouns, Nd for time nouns), and verb transitivity and subcategorization (e.g., VA for intransitive verbs,</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
<author>T Watanabe</author>
<author>E Sumita</author>
</authors>
<title>Reordering Constraints for Phrase-Based Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of CoLing 2004,</booktitle>
<pages>205--211</pages>
<location>Geneva,</location>
<contexts>
<context position="2419" citStr="Zens et al., 2004" startWordPosition="367" endWordPosition="370">d as implemented in (Koehn et al., 2005; Chiang, 2005). In other words, their system shares the same phrase table with standard phrase-based SMT systems. three 3 年 前 after years three ago years A1 A2 (a) (b) Figure 1: Two reordering examples, with straight rule applied in (a), and inverted rule in (b). On the other hand, there are various proposed BTG reordering models to predict correct orientations between neighboring blocks (bilingual phrases). In Figure 1, for example, the role of reordering model is to predict correct orientations of neighboring blocks A1 and A2. In flat model (Wu, 1996; Zens et al., 2004; Kumar and Byrne, 2005), reordering probabilities are assigned uniformly during decoding, and can be tuned depending on different language pairs. It is clear, however, that this kind of model would suffer when the dominant rule is wrongly applied. Predicting orientations in BTG depending on context information can be achieved with lexical features. For example, Xiong et al. (2006) proposed MEBTG, based on maximum entropy (ME) classification with words as features. In MEBTG, first words of blocks are considered as the features, which are then used to train a ME model 3 年 後 A1 A2 254 Human Lang</context>
</contexts>
<marker>Zens, Ney, Watanabe, Sumita, 2004</marker>
<rawString>R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Reordering Constraints for Phrase-Based Statistical Machine Translation. In Proceedings of CoLing 2004, Geneva, Switzerland, pp. 205-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhang</author>
</authors>
<title>Maximum Entropy Modeling Toolkit for Python and C++. Available at http://homepa ges.inf.ed.ac.uk/s0450736/maxent_toolkit.html.</title>
<date>2004</date>
<marker>Le Zhang, 2004</marker>
<rawString>Le Zhang. 2004. Maximum Entropy Modeling Toolkit for Python and C++. Available at http://homepa ges.inf.ed.ac.uk/s0450736/maxent_toolkit.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Chi-Ho Li</author>
<author>Ming Zhou</author>
</authors>
<title>Phrase Reordering Model Integrating Syntactic Knowledge for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL</booktitle>
<contexts>
<context position="7321" citStr="Zhang et al. (2007)" startWordPosition="1146" endWordPosition="1149">studying CKY parsing over the last decade, which considers translations and orientations of two neighboring block according to grammar rules or context information. In hierarchical phrase-based systems (Chiang, 2005), for example, SCFG rules are automatically learned from aligned bilingual corpus, and are applied in CKY style decoding. As an another application of CKY parsing technique is BTG-based SMT. Xiong et al. (2006) and Xiong et al. (2008a) developed MEBTG systems, in which first or tail words from reordering examples are used as features to train ME-based reordering models. Similarly, Zhang et al. (2007) proposed a model similar to BTG, which uses first/tail words of phrases, and syntactic labels (e.g. NP and VP) 255 from source parse trees as features. In their work, however, inverted rules are allowed to apply only when source phrases are syntactic; for nonsyntactic ones, blocks are combined straight with a constant score. More recently, Xiong et al. (2008b) proposed LABTG, which incorporates linguistic knowledge by adding features such as syntactic labels and POS from source trees to improve their MEBTG. Different from Zhang&apos;s work, their model do not restrict non-syntactic phrases, and ap</context>
<context position="12278" citStr="Zhang et al. (2007)" startWordPosition="2033" endWordPosition="2036">word on the block A2. Given the adjacent blocks A1 and A2, our goal includes (1) adding more linguistic and representative information to A1 and A2 and (2) finding a feature set function F&apos; based on added linguistic information in order to train a more linguistically motivated and effective model. 4.2 Word Classification As described in Section 1, designing a more complete feature set causes data sparseness problem, if we use lexical features. One natural solution is using POS and word class features. In our model, we perform Chinese POS tagging on source language. In Xiong et al. (2008b) and Zhang et al. (2007), Chinese parsers with Penn Chinese Treebank (Xue et al., 2005) style are used to derive source parse trees, from which sourceside features such as POS are extracted. However, due to the relatively low accuracy of current Chinese parsers compared with English ones, we instead use CKIP Chinese word segmentation system (Ma and Chen, 2003) in order to derive Chinese tags with high accuracy. Moreover, compared with the Treebank Chinese tagset, the CKIP tagset provides more fine-grained tags, including many tags with semantic information (e.g., Nc for place nouns, Nd for time nouns), and verb trans</context>
</contexts>
<marker>Zhang, Li, Li, Zhou, 2007</marker>
<rawString>Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou. 2007. Phrase Reordering Model Integrating Syntactic Knowledge for SMT. In Proceedings of EMNLPCoNLL 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>