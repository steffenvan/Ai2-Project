<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.993453">
Constituent Parsing with Incremental Sigmoid Belief Networks
</title>
<author confidence="0.997687">
Ivan Titov
</author>
<affiliation confidence="0.9980565">
Department of Computer Science
University of Geneva
</affiliation>
<address confidence="0.655956">
24, rue G´en´eral Dufour
CH-1211 Gen`eve 4, Switzerland
</address>
<email confidence="0.918522">
ivan.titov@cui.unige.ch
</email>
<author confidence="0.980159">
James Henderson
</author>
<affiliation confidence="0.9987085">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.7334645">
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
</address>
<email confidence="0.992448">
james.henderson@ed.ac.uk
</email>
<sectionHeader confidence="0.994898" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999804">
We introduce a framework for syntactic
parsing with latent variables based on a form
of dynamic Sigmoid Belief Networks called
Incremental Sigmoid Belief Networks. We
demonstrate that a previous feed-forward
neural network parsing model can be viewed
as a coarse approximation to inference with
this class of graphical model. By construct-
ing a more accurate but still tractable ap-
proximation, we significantly improve pars-
ing accuracy, suggesting that ISBNs provide
a good idealization for parsing. This gener-
ative model of parsing achieves state-of-the-
art results on WSJ text and 8% error reduc-
tion over the baseline neural network parser.
</bodyText>
<sectionHeader confidence="0.998114" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99959196">
Latent variable models have recently been of in-
creasing interest in Natural Language Processing,
and in parsing in particular (e.g. (Koo and Collins,
2005; Matsuzaki et al., 2005; Riezler et al., 2002)).
Latent variables provide a principled way to in-
clude features in a probability model without need-
ing to have data labeled with those features in ad-
vance. Instead, a labeling with these features can
be induced as part of the training process. The
difficulty with latent variable models is that even
small numbers of latent variables can lead to com-
putationally intractable inference (a.k.a. decoding,
parsing). In this paper we propose a solution to
this problem based on dynamic Sigmoid Belief Net-
works (SBNs) (Neal, 1992). The dynamic SBNs
632
which we peopose, called Incremental Sigmoid Be-
lief Networks (ISBNs) have large numbers of latent
variables, which makes exact inference intractable.
However, they can be approximated sufficiently well
to build fast and accurate statistical parsers which in-
duce features during training.
We use SBNs in a generative history-based model
of constituent structure parsing. The probability of
an unbounded structure is decomposed into a se-
quence of probabilities for individual derivation de-
cisions, each decision conditioned on the unbounded
history of previous decisions. The most common ap-
proach to handling the unbounded nature of the his-
tories is to choose a pre-defined set of features which
can be unambiguously derived from the history (e.g.
(Charniak, 2000; Collins, 1999)). Decision prob-
abilities are then assumed to be independent of all
information not represented by this finite set of fea-
tures. Another previous approach is to use neural
networks to compute a compressed representation of
the history and condition decisions on this represen-
tation (Henderson, 2003; Henderson, 2004). It is
possible that an unbounded amount of information
is encoded in the compressed representation via its
continuous values, but it is not clear whether this is
actually happening due to the lack of any principled
interpretation for these continuous values.
Like the former approach, we assume that there
are a finite set of features which encode the relevant
information about the parse history. But unlike that
approach, we allow feature values to be ambiguous,
and represent each feature as a distribution over (bi-
nary) values. In other words, these history features
are treated as latent variables. Unfortunately, inter-
</bodyText>
<note confidence="0.98351">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 632–639,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999967863636364">
preting the history representations as distributions
over discrete values of latent variables makes the ex-
act computation of decision probabilities intractable.
Exact computation requires marginalizing out the la-
tent variables, which involves summing over all pos-
sible vectors of discrete values, which is exponential
in the length of the vector.
We propose two forms of approximation for dy-
namic SBNs, a neural network approximation and
a form of mean field approximation (Saul and Jor-
dan, 1999). We first show that the previous neural
network model of (Henderson, 2003) can be viewed
as a coarse approximation to inference with ISBNs.
We then propose an incremental mean field method,
which results in an improved approximation over
the neural network but remains tractable. The re-
sulting parser achieves significantly higher accuracy
than the neural network parser (90.0% F-measure vs
89.1%). We argue that this correlation between bet-
ter approximation and better accuracy suggests that
dynamic SBNs are a good abstract model for natural
language parsing.
</bodyText>
<sectionHeader confidence="0.926679" genericHeader="method">
2 Sigmoid Belief Networks
</sectionHeader>
<bodyText confidence="0.9999538">
A belief network, or a Bayesian network, is a di-
rected acyclic graph which encodes statistical de-
pendencies between variables. Each variable Si in
the graph has an associated conditional probability
distributions P(Si|Par(Si)) over its values given
the values of its parents Par(Si) in the graph. A
Sigmoid Belief Network (Neal, 1992) is a particu-
lar type of belief networks with binary variables and
conditional probability distributions in the form of
the logistic sigmoid function:
</bodyText>
<equation confidence="0.9991965">
P(Si=1|Par(Si)) = ,
1+exp(− ESjEPar(Si) JijSj)
</equation>
<bodyText confidence="0.99992515">
where Jij is the weight for the edge from variable
Sj to variable Si. In this paper we consider a gen-
eralized version of SBNs where we allow variables
with any range of discrete values. We thus general-
ize the logistic sigmoid function to the normalized
exponential (a.k.a. softmax) function to define the
conditional probabilities for non-binary variables.
Exact inference with all but very small SBNs
is not tractable. Initially sampling methods were
used (Neal, 1992), but this is also not feasible for
large networks, especially for the dynamic models
of the type described in section 2.2. Variational
methods have also been proposed for approximat-
ing SBNs (Saul and Jordan, 1999). The main idea of
variational methods (Jordan et al., 1999) is, roughly,
to construct a tractable approximate model with a
number of free parameters. The free parameters are
set so that the resulting approximate model is as
close as possible to the original graphical model for
a given inference problem.
</bodyText>
<subsectionHeader confidence="0.995564">
2.1 Mean Field Approximation Methods
</subsectionHeader>
<bodyText confidence="0.982958375">
The simplest example of a variation method is the
mean field method, originally introduced in statis-
tical mechanics and later applied to unsupervised
neural networks in (Hinton et al., 1995). Let us de-
note the set of visible variables in the model (i.e. the
inputs and outputs) by V and hidden variables by
H = h1,. . . , hl. The mean field method uses a fully
factorized distribution Q as the approximate model:
</bodyText>
<equation confidence="0.967056">
�
Q(H|V ) = Qi(hi|V ).
i
</equation>
<bodyText confidence="0.99959775">
where each Qi is the distribution of an individual
latent variable. The independence between the vari-
ables hi in this approximate distribution Q does not
imply independence of the free parameters which
define the Qi. These parameters are set to min-
imize the Kullback-Leibler divergence (Cover and
Thomas, 1991) between the approximate distribu-
tion Q(H|V ) and the true distribution P(H|V ):
</bodyText>
<equation confidence="0.949204833333333">
Q(H|V ) ln Q(H|V )
P (H|V ), (1)
or, equivalently, to maximize the expression:
P(H, V )
Q(H|V ) ln. (2)
Q(H|V )
</equation>
<bodyText confidence="0.9989165">
The expression LV is a lower bound on the log-
likelihood lnP(V ). It is used in the mean field
theory (Saul and Jordan, 1999) to approximate the
likelihood. However, in our case of dynamic graph-
ical models, we have to use a different approach
which allows us to construct an incremental parsing
method without needing to introduce the additional
parameters proposed in (Saul and Jordan, 1999).
We will describe our modification of the mean field
method in section 3.3.
</bodyText>
<equation confidence="0.8627505">
1
�
KL(QkP ) =
H
�
LV =
H
633
</equation>
<bodyText confidence="0.977036">
2.2 Dynamics
Dynamic Bayesian networks are Bayesian networks
applied to arbitrarily long sequences. A new set of
variables is instantiated for each position in the se-
quence, but the edges and weights for these variables
are the same as in other positions. The edges which
connect variables instantiated for different positions
must be directed forward in the sequence, thereby
allowing a temporal interpretation of the sequence.
Typically a dynamic Bayesian Network will only in-
volve edges between adjacent positions in the se-
quence (i.e. they are Markovian), but in our parsing
models the pattern of interconnection is determined
by structural locality, rather than sequence locality,
as in the neural networks of (Henderson, 2003).
Using structural locality to define the graph in a
dynamic SBN means that the subgraph of edges with
destinations at a given position cannot be determined
until all the parser decisions for previous positions
have been chosen. We therefore call these models
Incremental SBNs, because, at any given position
in the parse, we only know the graph of edges for
that position and previous positions in the parse. For
example in figure 1, discussed below, it would not
be possible to draw the portion of the graph after t,
because we do not yet know the decision dtk.
The incremental specification of model structure
means that we cannot use an undirected graphical
model, such as Conditional Random Fields. With
a directed dynamic model, all edges connecting the
known portion of the graph to the unknown portion
of the graph are directed toward the unknown por-
tion. Also there are no variables in the unknown
portion of the graph whose values are known (i.e. no
visible variables), because at each step in a history-
based model the decision probability is conditioned
only on the parsing history. Only visible variables
can result in information being reflected backward
through a directed edge, so it is impossible for any-
thing in the unknown portion of the graph to affect
the probabilities in the known portion of the graph.
Therefore inference can be performed by simply ig-
noring the unknown portion of the graph, and there
is no need to sum over all possible structures for the
unknown portion of the graph, as would be neces-
sary for an undirected graphical model.
634
Figure 1: Illustration of an ISBN.
3 The Probabilistic Model of Parsing
In this section we present our framework for syn-
tactic parsing with dynamic Sigmoid Belief Net-
works. We first specify the form of SBN we propose,
namely ISBNs, and then two methods for approx-
imating the inference problems required for pars-
ing. We only consider generative models of pars-
ing, since generative probability models are simpler
and we are focused on probability estimation, not
decision making. Although the most accurate pars-
ing models (Charniak and Johnson, 2005; Hender-
son, 2004; Collins, 2000) are discriminative, all the
most accurate discriminative models make use of a
generative model. More accurate generative models
should make the discriminative models which use
them more accurate as well. Also, there are some
applications, such as language modeling, which re-
quire generative models.
3.1 The Graphical Model
In ISBNs, we use a history-based model, which de-
composes the probability of the parse as:
P (T ) = P (D1, ..., Dm) = � P (Dt|D1,..., Dt−1),
t
where T is the parse tree and D1, ... , Dm is its
equivalent sequence of parser decisions. Instead of
treating each Dt as atomic decisions, it is convenient
to further split them into a sequence of elementary
decisions Dt = dt1, ... , dtn:
P D D ,...,Dt�) II
( tI =
— P(dtk|h(t, k)),
k
where h(t, k) denotes the parsing history
D1,..., Dt−1, dt1, ... , dtk−1. For example, a
decision to create a new constituent can be divided
in two elementary decisions: deciding to create a
constituent and deciding which label to assign to it.
We use a graphical model to define our proposed
class of probability models. An example graphical
model for the computation of P(dtk|h(t, k)) is
illustrated in figure 1.
The graphical model is organized into vectors
of variables: latent state variable vectors St0 =
st01 , ... , st0n, representing an intermediate state of the
parser at derivation step t0, and decision variable
vectors Dt0 = dt01 , ... , dt0
l , representing a parser de-
cision at derivation step t0, where t0 &lt; t. Variables
whose value are given at the current decision (t, k)
are shaded in figure 1, latent and output variables are
left unshaded.
As illustrated by the arrows in figure 1, the prob-
ability of each state variable st0
i depends on all the
variables in a finite set of relevant previous state and
decision vectors, but there are no direct dependen-
cies between the different variables in a single state
vector. Which previous state and decision vectors
are connected to the current state vector is deter-
mined by a set of structural relations specified by
the parser designer. For example, we could select
the most recent state where the same constituent was
on the top of the stack, and a decision variable rep-
resenting the constituent’s label. Each such selected
relation has its own distinct weight matrix for the
resulting edges in the graph, but the same weight
matrix is used at each derivation position where the
relation is relevant.
As indicated in figure 1, the probability of each
elementary decision dt0k depends both on the current
state vector St0 and on the previously chosen ele-
mentary action dt0k−1 from Dt0. This probability dis-
tribution has the form of a normalized
</bodyText>
<equation confidence="0.885128">
d exponential:
L�j Wdjst0
j
P(dt0 k = d|St0, dt0 k−1)= Φh(t0,k)(d) e t0, (3)
EW, s
Ed0Φh(t0,k)(d0) e j d j j
</equation>
<bodyText confidence="0.9959963">
where Φh(t0,k) is the indicator function of a set of
elementary decisions that may possibly follow the
parsing history h(t0, k), and the Wdj are the weights.
For our experiments, we replicated the same pat-
tern of interconnection between state variables as
described in (Henderson, 2003).1 We also used the
&apos;In the neural network of (Henderson, 2003), our variables
same left-corner parsing strategy, and the same set of
decisions, features, and states. We refer the reader to
(Henderson, 2003) for details.
Exact computation with this model is not
tractable. Sampling of parse trees from the model
is not feasible, because a generative model defines a
joint model of both a sentence and a tree, thereby re-
quiring sampling over the space of sentences. Gibbs
sampling (Geman and Geman, 1984) is also impos-
sible, because of the huge space of variables and
need to resample after making each new decision in
the sequence. Thus, we know of no reasonable alter-
natives to the use of variational methods.
</bodyText>
<subsectionHeader confidence="0.999655">
3.2 A Feed-Forward Approximation
</subsectionHeader>
<bodyText confidence="0.96784871875">
The first model we consider is a strictly incremental
computation of a variational approximation, which
we will call the feed-forward approximation. It can
be viewed as the simplest form of mean field approx-
imation. As in any mean field approximation, each
of the latent variables is independently distributed.
But unlike the general case of mean field approxi-
mation, in the feed-forward approximation we only
allow the parameters of the distributions Qi to de-
pend on the distributions of their parents. This addi-
tional constraint increases the potential for a large
Kullback-Leibler divergence with the true model,
defined in expression (1), but it significantly simpli-
fies the computations.
The set of hidden variables H in our graphical
model consists of all the state vectors St0, t0 &lt; t,
and the last decision dtk. All the previously observed
decisions h(t, k) comprise the set of visible vari-
ables V . The approximate fully factorisable distri-
bution Q(H|V ) can be written as:
11 t t&apos; .
Q(H|V ) = qt k(dt k) (µz0)3 &apos; (1 − µz0)1—si
t0,i
where µt0
i is the free parameter which determines the
distribution of state variable i at position t0, namely
its mean, and qtk(dtk) is the free parameter which de-
termines the distribution over decisions dtk.
Because we are only allowed to use information
about the distributions of the parent variables to
map to their “units”, and our dependencies/edges map to their
“links”.
</bodyText>
<equation confidence="0.9320532">
635
E
Bτ(t0,t00)
idt00
k , (4)
</equation>
<bodyText confidence="0.999941222222222">
where RS(t&apos;) is the set of previous positions with
edges from their state vectors to the state vector at t&apos;,
RD(t&apos;) is the set of previous positions with edges
from their decision vectors to the state vector at t&apos;,
τ(t&apos;, t&apos;&apos;) is the relevant relation between the position
t&apos;&apos; and the position t&apos;, and Jτij and Bτid are weight
matrices.
In order to maximize (2), the approximate distri-
bution of the next decisions qtk(d) should be set to
</bodyText>
<equation confidence="0.998654666666667">
_ 4bh(t k) (d) eEj W djµtj
qtk(d) (5)
Ed0 Φh(t k) (d&apos;) eEj Wd&apos;&apos;µj &apos;
</equation>
<bodyText confidence="0.997665">
as follows from expression (3). The resulting esti-
mate of the tree probability is given by:
</bodyText>
<equation confidence="0.996335454545455">
E
τ(t0 t00) t00
Jij,µj +
P(T) ≈ fj qtk(dtk).
t,k
Lt,k = E
V
i
E+µt iηt i+
k0&lt;k
� �
</equation>
<bodyText confidence="0.772992666666667">
compute the free parameters µt0
i ,the optimal assign-
ment of values to the µt0
</bodyText>
<equation confidence="0.890389">
i is:
µZ0 = σ (ηi
</equation>
<bodyText confidence="0.992670037037037">
where σ denotes the logistic sigmoid function and
ηt0
i is a weighted sum of the parent variables’ means:
we will call the mean field approximation. Again,
we are interested in finding the distribution Q which
maximizes the quantity LV in expression (2). The
decision distribution qtk(dtk) maximizes LV when it
has the same dependence on the state vector means
µtk as in the feed-forward approximation, namely ex-
pression (5). However, as we mentioned above, the
feed-forward computation does not allow us to com-
pute the optimal values of state means µt0
i .
Optimally, after each new decision dtk, we should
recompute all the means µt0
i for all the state vec-
tors St0, t&apos; ≤ t. However, this would make the
method intractable, due to the length of derivations
in constituent parsing and the interdependence be-
tween these means. Instead, after making each deci-
sion dtk and adding it to the set of visible variables V ,
we recompute only means of the current state vector
St.
The denominator of the normalized exponential
function in (3) does not allow us to compute LV ex-
actly. Instead, we use a simple first order approxi-
mation:
</bodyText>
<equation confidence="0.399902333333333">
EQ[ln E Φh(t,k) (d) exp( E Wdjstj)]
d j
E≈ ln Φh(t,k)(d) exp( E Wdjµtj), (6)
</equation>
<bodyText confidence="0.964714636363636">
d j
where the expectation EQ[...] is taken over the state
vector St distributed according to the approximate
distribution Q.
Unfortunately, even with this assumption there is
no analytic way to maximize LV with respect to the
means µtk, so we need to use numerical methods.
Assuming (6), we can rewrite the expression (2) as
follows, substituting the true P(H,V ) defined by
the graphical model and the approximate distribu-
tion Q(H|V ), omitting parts independent of µtk:
</bodyText>
<equation confidence="0.995133857142857">
t0= Eηi E
t00ERS(t0) j
t00ERD(t0) k
Φh(t,k0)(dt k0) E Wdt k0jµt j
j
( �
−µt i ln µt i − (1 − µt i) ln 1 − µt i
</equation>
<bodyText confidence="0.999824411764706">
This approximation method replicates exactly the
computation of the feed-forward neural network
in (Henderson, 2003), where the above means µt0
i
are equivalent to the neural network hidden unit acti-
vations. Thus, that neural network probability model
can be regarded as a simple approximation to the
graphical model introduced in section 3.1.
In addition to the drawbacks shared by any mean
field approximation method, this feed-forward ap-
proximation cannot capture backward reasoning.
By backward (a.k.a. top-down) reasoning we mean
the need to update the state vector means µt0
i after
observing a decision dtk, for t&apos; ≤ t. The next section
discusses how backward reasoning can be incorpo-
rated in the approximate model.
</bodyText>
<subsectionHeader confidence="0.51987">
3.3 A Mean Field Approximation
</subsectionHeader>
<bodyText confidence="0.792239">
This section proposes a more accurate way to ap-
proximate ISBNs with mean field methods, which
</bodyText>
<note confidence="0.46165">
kE �E E
ln Φh(t,k0)(d)exp( Wdjµt j)�, (7)
d j
</note>
<bodyText confidence="0.901982">
here, ηti is computed from the previous relevant state
means and decisions as in (4). This expression is
</bodyText>
<page confidence="0.873767">
636
</page>
<bodyText confidence="0.9955256">
concave with respect to the parameters µti, so the
global maximum can be found. We use coordinate-
wise ascent, where each µti is selected by an efficient
line search (Press et al., 1996), while keeping other
µti, fixed.
</bodyText>
<subsectionHeader confidence="0.989596">
3.4 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.981228904761905">
We train these models to maximize the fit of the
approximate model to the data. We use gradient
descent and a maximum likelihood objective func-
tion. This requires computation of the gradient of
the approximate log-likelihood with respect to the
model parameters. In order to compute these deriva-
tives, the error should be propagated all the way
back through the structure of the graphical model.
For the feed-forward approximation, computation of
the derivatives is straightforward, as in neural net-
works. But for the mean field approximation, it re-
quires computation of the derivatives of the means
µti with respect to the other parameters in expres-
sion (7). The use of a numerical search in the mean
field approximation makes the analytical computa-
tion of these derivatives impossible, so a different
method needs to be used to compute their values. If
maximization of Lt,kV is done until convergence, then
the derivatives of Lt,kV with respect to µti are close to
zero:
Fit aL t Pz� 0 for all i.
</bodyText>
<subsectionHeader confidence="0.419698">
∂µi
</subsectionHeader>
<bodyText confidence="0.9930345">
This system of equations allows us to use implicit
differentiation to compute the needed derivatives.
</bodyText>
<sectionHeader confidence="0.993765" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999220928571429">
In this section we evaluate the two approximations
to dynamic SBNs discussed in the previous section,
the feed-forward method equivalent to the neural
network of (Henderson, 2003) (NN method) and the
mean field method (MF method). The hypothesis
we wish to test is that the more accurate approxima-
tion of dynamic SBNs will result in a more accurate
model of constituent structure parsing. If this is true,
then it suggests that dynamic SBNs of the form pro-
posed here are a good abstract model of the nature
of natural language parsing.
We used the Penn Treebank WSJ corpus (Marcus
et al., 1993) to perform the empirical evaluation of
the considered approaches. It is expensive to train
</bodyText>
<table confidence="0.999701142857143">
R P F1
Bikel, 2004 87.9 88.8 88.3
Taskar et al., 2004 89.1 89.1 89.1
NN method 89.1 89.2 89.1
Turian and Melamed, 2006 89.3 89.6 89.4
MF method 89.3 90.7 90.0
Charniak, 2000 90.0 90.2 90.1
</table>
<tableCaption confidence="0.910041666666667">
Table 1: Percentage labeled constituent recall (R),
precision (P), combination of both (F1) on the test-
ing set.
</tableCaption>
<bodyText confidence="0.982294376344086">
the MF approximation on the whole WSJ corpus, so
instead we use only sentences of length at most 15,
as in (Taskar et al., 2004) and (Turian and Melamed,
2006). The standard split of the corpus into training
(sections 2–22, 9,753 sentences), validation (section
24, 321 sentences), and testing (section 23, 603 sen-
tences) was performed.2
As in (Henderson, 2003; Turian and Melamed,
2006) we used a publicly available tagger (Ratna-
parkhi, 1996) to provide the part-of-speech tag for
each word in the sentence. For each tag, there is an
unknown-word vocabulary item which is used for all
those words which are not sufficiently frequent with
that tag to be included individually in the vocabu-
lary. We only included a specific tag-word pair in the
vocabulary if it occurred at least 20 time in the train-
ing set, which (with tag-unknown-word pairs) led to
the very small vocabulary of 567 tag-word pairs.
During parsing with both the NN method and the
MF method, we used beam search with a post-word
beam of 10. Increasing the beam size beyond this
value did not significantly effect parsing accuracy.
For both of the models, the state vector size of 40
was used. All the parameters for both the NN and
MF models were tuned on the validation set. A sin-
gle best model of each type was then applied to the
final testing set.
Table 1 lists the results of the NN approximation
and the MF approximation, along with results of dif-
2Training of our MF method on this subset of WSJ took less
than 6 days on a standard desktop PC. We would expect that
a model for the entire WSJ corpus can be trained in about 3
months time. The training time is about linear with the num-
ber of words, but a larger state vector is needed to accommo-
date all the information. The long training times on the entire
WSJ would not allow us to tune the model parameters properly,
which would have increased the randomness of the empirical
comparison, although it would be feasible for building a sys-
tem.
637
ferent generative and discriminative parsing meth- tional Random Fields, are the standard tools for shal-
ods (Bikel, 2004; Taskar et al., 2004; Turian and low parsing (Sha and Pereira, 2003). However, shal-
Melamed, 2006; Charniak, 2000) evaluated in the low parsing is effectively a sequence labeling prob-
same experimental setup. The MF model improves lem and therefore differs significantly from full pars-
over the baseline NN approximation, with an error ing. As discussed in section 2.2, undirected graph-
reduction in F-measure exceeding 8%. This im- ical models do not seem to be suitable for history-
provement is statically significant.3 The MF model based full parsing models.
achieves results which do not appear to be signifi- Sigmoid Belief Networks were used originally
cantly different from the results of the best model for character recognition tasks, but later a dynamic
in the list (Charniak, 2000). It should also be noted modification of this model was applied to the rein-
that the model (Charniak, 2000) is the most accu- forcement learning task (Sallans, 2002). However,
rate generative model on the standard WSJ parsing their graphical model, approximation method, and
benchmark, which confirms the viability of our gen- learning method differ significantly from those of
erative model. this paper.
These experimental results suggest that Incre- 6 Conclusions
mental Sigmoid Belief Networks are an appropriate This paper proposes a new generative framework
model for natural language parsing. Even approxi- for constituent parsing based on dynamic Sigmoid
mations such as those tested here, with a very strong Belief Networks with vectors of latent variables.
factorisability assumption, allow us to build quite Exact inference with the proposed graphical model
accurate parsing models. The main drawback of our (called Incremental Sigmoid Belief Networks) is
proposed mean field approach is the relative compu- not tractable, but two approximations are consid-
tational complexity of the numerical procedure used ered. First, it is shown that the neural network
to maximize Lv . But this approximation has suc- parser of (Henderson, 2003) can be considered as a
ceeded in showing that a more accurate approxima- simple feed-forward approximation to the graphical
tion of ISBNs results in a more accurate parser. We model. Second, a more accurate but still tractable
believe this provides strong justification for more ac- approximation based on mean field theory is pro-
curate approximations of ISBNs for parsing. posed. Both methods are empirically compared, and
5 Related Work the mean field approach achieves significantly better
There has not been much previous work on graph- results, which are non-significantly different from
ical models for full parsing, although recently sev- the results of the most accurate generative parsing
eral latent variable models for parsing have been model (Charniak, 2000) on our testing set. The fact
proposed (Koo and Collins, 2005; Matsuzaki et al., that a more accurate approximation leads to a more
2005; Riezler et al., 2002). In (Koo and Collins, accurate parser suggests that ISBNs are a good ab-
2005), an undirected graphical model is used for stract model for constituent structure parsing. This
parse reranking. Dependency parsing with dynamic empirical result motivates research into more accu-
Bayesian networks was considered in (Peshkin and rate approximations of dynamic SBNs.
Savova, 2005), with limited success. Their model We focused in this paper on generative models
is very different from ours. Roughly, it considered of parsing. The results of such a generative model
the whole sentence at a time, with the graphical can be easily improved by a discriminative rerank-
model being used to decide which words correspond ing model, even without any additional feature en-
to leaves of the tree. The chosen words are then gineering. For example, the discriminative train-
removed from the sentence and the model is recur- ing techniques successfully applied in (Henderson,
sively applied to the reduced sentence. 2004) to the feed-forward neural network model can
Undirected graphical models, in particular Condi- be directly applied to the mean field model pro-
posed in this paper. The same is true for rerank-
ing with data-defined kernels, with which we would
3We measured significance of all the experiments in this pa-
per with the randomized significance test (Yeh, 2000).
638
expect similar improvements as were achieved with
the neural network parser (Henderson and Titov,
2005). Such improvements should situate the result-
ing model among the best current parsing models.
</bodyText>
<sectionHeader confidence="0.998474" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999949228915663">
Dan M. Bikel. 2004. Intricacies of Collins’ parsing
model. Computational Linguistics, 30(4).
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. ACL, pages 173–180, Ann Arbor, MI.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. ACL, pages 132–139, Seattle, Wash-
ington.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. ICML, pages 175–182,
Stanford, CA.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
ofInformation Theory. John Wiley, New York, NY.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721–741.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilistic
models. In Proc. ACL, Ann Arbor, MI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
HLT-NAACL, pages 103–110, Edmonton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. ACL,
Barcelona, Spain.
G. Hinton, P. Dayan, B. Frey, and R. Neal. 1995.
The wake-sleep algorithm for unsupervised neural net-
works. Science, 268:1158–1161.
M. I. Jordan, Z.Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods for
graphical models. In Michael I. Jordan, editor, Learn-
ing in Graphical Models. MIT Press, Cambridge, MA.
Terry Koo and Michael Collins. 2005. Hidden-variable
models for discriminative reranking. In Proc. EMNLP,
Vancouver, B.C., Canada.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. ACL, Ann Arbor, MI.
Radford Neal. 1992. Connectionist learning of belief
networks. Artificial Intelligence, 56:71–113.
Leon Peshkin and Virginia Savova. 2005. Dependency
parsing with dynamic bayesian network. In AAAI,
20th National Conference on Artificial Intelligence,
Pittsburgh, Pennsylvania.
W. Press, B. Flannery, S. Teukolsky, and W. Vetterling.
1996. Numerical Recipes. Cambridge University
Press, Cambridge, UK.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proc. EMNLP, pages
133–142, Univ. of Pennsylvania, PA.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proc. ACL, Philadelphia, PA.
Brian Sallans. 2002. Reinforcement Learning for Fac-
tored Markov Decision Processes. Ph.D. thesis, Uni-
versity of Toronto, Toronto, Canada.
Lawrence K. Saul and Michael I. Jordan. 1999. A
mean field learning algorithm for unsupervised neu-
ral networks. In Michael I. Jordan, editor, Learning in
Graphical Models, pages 541–554. MIT Press, Cam-
bridge, MA.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proc. HLT-NAACL,
Edmonton, Canada.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proc. EMNLP, Barcelona, Spain.
Joseph Turian and Dan Melamed. 2006. Advances in
discriminative parsing. In Proc. COLING-ACL, Syd-
ney, Australia.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of the result differences. In Proc.
COLING, pages 947–953, Saarbruken, Germany.
</reference>
<page confidence="0.951471">
639
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.714781">
<title confidence="0.999943">Constituent Parsing with Incremental Sigmoid Belief Networks</title>
<author confidence="0.999913">Ivan Titov</author>
<affiliation confidence="0.99997">Department of Computer Science University of Geneva</affiliation>
<address confidence="0.9945165">24, rue G´en´eral Dufour CH-1211 Gen`eve 4, Switzerland</address>
<email confidence="0.959088">ivan.titov@cui.unige.ch</email>
<author confidence="0.999908">James Henderson</author>
<affiliation confidence="0.939009333333333">School of Informatics University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.997476">Edinburgh EH8 9LW, United Kingdom</address>
<email confidence="0.999077">james.henderson@ed.ac.uk</email>
<abstract confidence="0.9951270625">We introduce a framework for syntactic parsing with latent variables based on a form of dynamic Sigmoid Belief Networks called Incremental Sigmoid Belief Networks. We demonstrate that a previous feed-forward neural network parsing model can be viewed as a coarse approximation to inference with this class of graphical model. By constructing a more accurate but still tractable approximation, we significantly improve parsing accuracy, suggesting that ISBNs provide a good idealization for parsing. This generative model of parsing achieves state-of-theart results on WSJ text and 8% error reduction over the baseline neural network parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dan M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="21527" citStr="Bikel, 2004" startWordPosition="3586" endWordPosition="3587">section, the feed-forward method equivalent to the neural network of (Henderson, 2003) (NN method) and the mean field method (MF method). The hypothesis we wish to test is that the more accurate approximation of dynamic SBNs will result in a more accurate model of constituent structure parsing. If this is true, then it suggests that dynamic SBNs of the form proposed here are a good abstract model of the nature of natural language parsing. We used the Penn Treebank WSJ corpus (Marcus et al., 1993) to perform the empirical evaluation of the considered approaches. It is expensive to train R P F1 Bikel, 2004 87.9 88.8 88.3 Taskar et al., 2004 89.1 89.1 89.1 NN method 89.1 89.2 89.1 Turian and Melamed, 2006 89.3 89.6 89.4 MF method 89.3 90.7 90.0 Charniak, 2000 90.0 90.2 90.1 Table 1: Percentage labeled constituent recall (R), precision (P), combination of both (F1) on the testing set. the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al., 2004) and (Turian and Melamed, 2006). The standard split of the corpus into training (sections 2–22, 9,753 sentences), validation (section 24, 321 sentences), and testing (section 23, 603 senten</context>
<context position="23900" citStr="Bikel, 2004" startWordPosition="4006" endWordPosition="4007">han 6 days on a standard desktop PC. We would expect that a model for the entire WSJ corpus can be trained in about 3 months time. The training time is about linear with the number of words, but a larger state vector is needed to accommodate all the information. The long training times on the entire WSJ would not allow us to tune the model parameters properly, which would have increased the randomness of the empirical comparison, although it would be feasible for building a system. 637 ferent generative and discriminative parsing meth- tional Random Fields, are the standard tools for shalods (Bikel, 2004; Taskar et al., 2004; Turian and low parsing (Sha and Pereira, 2003). However, shalMelamed, 2006; Charniak, 2000) evaluated in the low parsing is effectively a sequence labeling probsame experimental setup. The MF model improves lem and therefore differs significantly from full parsover the baseline NN approximation, with an error ing. As discussed in section 2.2, undirected graphreduction in F-measure exceeding 8%. This im- ical models do not seem to be suitable for historyprovement is statically significant.3 The MF model based full parsing models. achieves results which do not appear to be</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Dan M. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="10594" citStr="Charniak and Johnson, 2005" startWordPosition="1691" endWordPosition="1694">ion of the graph, as would be necessary for an undirected graphical model. 634 Figure 1: Illustration of an ISBN. 3 The Probabilistic Model of Parsing In this section we present our framework for syntactic parsing with dynamic Sigmoid Belief Networks. We first specify the form of SBN we propose, namely ISBNs, and then two methods for approximating the inference problems required for parsing. We only consider generative models of parsing, since generative probability models are simpler and we are focused on probability estimation, not decision making. Although the most accurate parsing models (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000) are discriminative, all the most accurate discriminative models make use of a generative model. More accurate generative models should make the discriminative models which use them more accurate as well. Also, there are some applications, such as language modeling, which require generative models. 3.1 The Graphical Model In ISBNs, we use a history-based model, which decomposes the probability of the parse as: P (T ) = P (D1, ..., Dm) = � P (Dt|D1,..., Dt−1), t where T is the parse tree and D1, ... , Dm is its equivalent sequence of parser decisions. Instead of</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proc. ACL, pages 173–180, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="2518" citStr="Charniak, 2000" startWordPosition="379" endWordPosition="380">e. However, they can be approximated sufficiently well to build fast and accurate statistical parsers which induce features during training. We use SBNs in a generative history-based model of constituent structure parsing. The probability of an unbounded structure is decomposed into a sequence of probabilities for individual derivation decisions, each decision conditioned on the unbounded history of previous decisions. The most common approach to handling the unbounded nature of the histories is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. Another previous approach is to use neural networks to compute a compressed representation of the history and condition decisions on this representation (Henderson, 2003; Henderson, 2004). It is possible that an unbounded amount of information is encoded in the compressed representation via its continuous values, but it is not clear whether this is actually happening due to the lack of any principled interpretation for these continuous values. Like the</context>
<context position="21682" citStr="Charniak, 2000" startWordPosition="3615" endWordPosition="3616"> we wish to test is that the more accurate approximation of dynamic SBNs will result in a more accurate model of constituent structure parsing. If this is true, then it suggests that dynamic SBNs of the form proposed here are a good abstract model of the nature of natural language parsing. We used the Penn Treebank WSJ corpus (Marcus et al., 1993) to perform the empirical evaluation of the considered approaches. It is expensive to train R P F1 Bikel, 2004 87.9 88.8 88.3 Taskar et al., 2004 89.1 89.1 89.1 NN method 89.1 89.2 89.1 Turian and Melamed, 2006 89.3 89.6 89.4 MF method 89.3 90.7 90.0 Charniak, 2000 90.0 90.2 90.1 Table 1: Percentage labeled constituent recall (R), precision (P), combination of both (F1) on the testing set. the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al., 2004) and (Turian and Melamed, 2006). The standard split of the corpus into training (sections 2–22, 9,753 sentences), validation (section 24, 321 sentences), and testing (section 23, 603 sentences) was performed.2 As in (Henderson, 2003; Turian and Melamed, 2006) we used a publicly available tagger (Ratnaparkhi, 1996) to provide the part-of-speec</context>
<context position="24014" citStr="Charniak, 2000" startWordPosition="4024" endWordPosition="4025">about 3 months time. The training time is about linear with the number of words, but a larger state vector is needed to accommodate all the information. The long training times on the entire WSJ would not allow us to tune the model parameters properly, which would have increased the randomness of the empirical comparison, although it would be feasible for building a system. 637 ferent generative and discriminative parsing meth- tional Random Fields, are the standard tools for shalods (Bikel, 2004; Taskar et al., 2004; Turian and low parsing (Sha and Pereira, 2003). However, shalMelamed, 2006; Charniak, 2000) evaluated in the low parsing is effectively a sequence labeling probsame experimental setup. The MF model improves lem and therefore differs significantly from full parsover the baseline NN approximation, with an error ing. As discussed in section 2.2, undirected graphreduction in F-measure exceeding 8%. This im- ical models do not seem to be suitable for historyprovement is statically significant.3 The MF model based full parsing models. achieves results which do not appear to be signifi- Sigmoid Belief Networks were used originally cantly different from the results of the best model for cha</context>
<context position="26702" citStr="Charniak, 2000" startWordPosition="4436" endWordPosition="4437"> accurate parser. We model. Second, a more accurate but still tractable believe this provides strong justification for more ac- approximation based on mean field theory is procurate approximations of ISBNs for parsing. posed. Both methods are empirically compared, and 5 Related Work the mean field approach achieves significantly better There has not been much previous work on graph- results, which are non-significantly different from ical models for full parsing, although recently sev- the results of the most accurate generative parsing eral latent variable models for parsing have been model (Charniak, 2000) on our testing set. The fact proposed (Koo and Collins, 2005; Matsuzaki et al., that a more accurate approximation leads to a more 2005; Riezler et al., 2002). In (Koo and Collins, accurate parser suggests that ISBNs are a good ab2005), an undirected graphical model is used for stract model for constituent structure parsing. This parse reranking. Dependency parsing with dynamic empirical result motivates research into more accuBayesian networks was considered in (Peshkin and rate approximations of dynamic SBNs. Savova, 2005), with limited success. Their model We focused in this paper on gener</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proc. ACL, pages 132–139, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2534" citStr="Collins, 1999" startWordPosition="381" endWordPosition="382"> can be approximated sufficiently well to build fast and accurate statistical parsers which induce features during training. We use SBNs in a generative history-based model of constituent structure parsing. The probability of an unbounded structure is decomposed into a sequence of probabilities for individual derivation decisions, each decision conditioned on the unbounded history of previous decisions. The most common approach to handling the unbounded nature of the histories is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. Another previous approach is to use neural networks to compute a compressed representation of the history and condition decisions on this representation (Henderson, 2003; Henderson, 2004). It is possible that an unbounded amount of information is encoded in the compressed representation via its continuous values, but it is not clear whether this is actually happening due to the lack of any principled interpretation for these continuous values. Like the former approach</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>175--182</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="10627" citStr="Collins, 2000" startWordPosition="1698" endWordPosition="1699">n undirected graphical model. 634 Figure 1: Illustration of an ISBN. 3 The Probabilistic Model of Parsing In this section we present our framework for syntactic parsing with dynamic Sigmoid Belief Networks. We first specify the form of SBN we propose, namely ISBNs, and then two methods for approximating the inference problems required for parsing. We only consider generative models of parsing, since generative probability models are simpler and we are focused on probability estimation, not decision making. Although the most accurate parsing models (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000) are discriminative, all the most accurate discriminative models make use of a generative model. More accurate generative models should make the discriminative models which use them more accurate as well. Also, there are some applications, such as language modeling, which require generative models. 3.1 The Graphical Model In ISBNs, we use a history-based model, which decomposes the probability of the parse as: P (T ) = P (D1, ..., Dm) = � P (Dt|D1,..., Dt−1), t where T is the parse tree and D1, ... , Dm is its equivalent sequence of parser decisions. Instead of treating each Dt as atomic decis</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proc. ICML, pages 175–182, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements ofInformation Theory.</title>
<date>1991</date>
<publisher>John Wiley,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="7053" citStr="Cover and Thomas, 1991" startWordPosition="1093" endWordPosition="1096"> applied to unsupervised neural networks in (Hinton et al., 1995). Let us denote the set of visible variables in the model (i.e. the inputs and outputs) by V and hidden variables by H = h1,. . . , hl. The mean field method uses a fully factorized distribution Q as the approximate model: � Q(H|V ) = Qi(hi|V ). i where each Qi is the distribution of an individual latent variable. The independence between the variables hi in this approximate distribution Q does not imply independence of the free parameters which define the Qi. These parameters are set to minimize the Kullback-Leibler divergence (Cover and Thomas, 1991) between the approximate distribution Q(H|V ) and the true distribution P(H|V ): Q(H|V ) ln Q(H|V ) P (H|V ), (1) or, equivalently, to maximize the expression: P(H, V ) Q(H|V ) ln. (2) Q(H|V ) The expression LV is a lower bound on the loglikelihood lnP(V ). It is used in the mean field theory (Saul and Jordan, 1999) to approximate the likelihood. However, in our case of dynamic graphical models, we have to use a different approach which allows us to construct an incremental parsing method without needing to introduce the additional parameters proposed in (Saul and Jordan, 1999). We will descri</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 1991. Elements ofInformation Theory. John Wiley, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="14187" citStr="Geman and Geman, 1984" startWordPosition="2312" endWordPosition="2315">xperiments, we replicated the same pattern of interconnection between state variables as described in (Henderson, 2003).1 We also used the &apos;In the neural network of (Henderson, 2003), our variables same left-corner parsing strategy, and the same set of decisions, features, and states. We refer the reader to (Henderson, 2003) for details. Exact computation with this model is not tractable. Sampling of parse trees from the model is not feasible, because a generative model defines a joint model of both a sentence and a tree, thereby requiring sampling over the space of sentences. Gibbs sampling (Geman and Geman, 1984) is also impossible, because of the huge space of variables and need to resample after making each new decision in the sequence. Thus, we know of no reasonable alternatives to the use of variational methods. 3.2 A Feed-Forward Approximation The first model we consider is a strictly incremental computation of a variational approximation, which we will call the feed-forward approximation. It can be viewed as the simplest form of mean field approximation. As in any mean field approximation, each of the latent variables is independently distributed. But unlike the general case of mean field approx</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Ivan Titov</author>
</authors>
<title>Data-defined kernels for parse reranking derived from probabilistic models.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<location>Ann Arbor, MI.</location>
<marker>Henderson, Titov, 2005</marker>
<rawString>James Henderson and Ivan Titov. 2005. Data-defined kernels for parse reranking derived from probabilistic models. In Proc. ACL, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Inducing history representations for broad coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>103--110</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2831" citStr="Henderson, 2003" startWordPosition="427" endWordPosition="428">for individual derivation decisions, each decision conditioned on the unbounded history of previous decisions. The most common approach to handling the unbounded nature of the histories is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. Another previous approach is to use neural networks to compute a compressed representation of the history and condition decisions on this representation (Henderson, 2003; Henderson, 2004). It is possible that an unbounded amount of information is encoded in the compressed representation via its continuous values, but it is not clear whether this is actually happening due to the lack of any principled interpretation for these continuous values. Like the former approach, we assume that there are a finite set of features which encode the relevant information about the parse history. But unlike that approach, we allow feature values to be ambiguous, and represent each feature as a distribution over (binary) values. In other words, these history features are treat</context>
<context position="4233" citStr="Henderson, 2003" startWordPosition="640" endWordPosition="641">07 Association for Computational Linguistics preting the history representations as distributions over discrete values of latent variables makes the exact computation of decision probabilities intractable. Exact computation requires marginalizing out the latent variables, which involves summing over all possible vectors of discrete values, which is exponential in the length of the vector. We propose two forms of approximation for dynamic SBNs, a neural network approximation and a form of mean field approximation (Saul and Jordan, 1999). We first show that the previous neural network model of (Henderson, 2003) can be viewed as a coarse approximation to inference with ISBNs. We then propose an incremental mean field method, which results in an improved approximation over the neural network but remains tractable. The resulting parser achieves significantly higher accuracy than the neural network parser (90.0% F-measure vs 89.1%). We argue that this correlation between better approximation and better accuracy suggests that dynamic SBNs are a good abstract model for natural language parsing. 2 Sigmoid Belief Networks A belief network, or a Bayesian network, is a directed acyclic graph which encodes sta</context>
<context position="8476" citStr="Henderson, 2003" startWordPosition="1336" endWordPosition="1337">iables is instantiated for each position in the sequence, but the edges and weights for these variables are the same as in other positions. The edges which connect variables instantiated for different positions must be directed forward in the sequence, thereby allowing a temporal interpretation of the sequence. Typically a dynamic Bayesian Network will only involve edges between adjacent positions in the sequence (i.e. they are Markovian), but in our parsing models the pattern of interconnection is determined by structural locality, rather than sequence locality, as in the neural networks of (Henderson, 2003). Using structural locality to define the graph in a dynamic SBN means that the subgraph of edges with destinations at a given position cannot be determined until all the parser decisions for previous positions have been chosen. We therefore call these models Incremental SBNs, because, at any given position in the parse, we only know the graph of edges for that position and previous positions in the parse. For example in figure 1, discussed below, it would not be possible to draw the portion of the graph after t, because we do not yet know the decision dtk. The incremental specification of mod</context>
<context position="13684" citStr="Henderson, 2003" startWordPosition="2231" endWordPosition="2232"> 1, the probability of each elementary decision dt0k depends both on the current state vector St0 and on the previously chosen elementary action dt0k−1 from Dt0. This probability distribution has the form of a normalized d exponential: L�j Wdjst0 j P(dt0 k = d|St0, dt0 k−1)= Φh(t0,k)(d) e t0, (3) EW, s Ed0Φh(t0,k)(d0) e j d j j where Φh(t0,k) is the indicator function of a set of elementary decisions that may possibly follow the parsing history h(t0, k), and the Wdj are the weights. For our experiments, we replicated the same pattern of interconnection between state variables as described in (Henderson, 2003).1 We also used the &apos;In the neural network of (Henderson, 2003), our variables same left-corner parsing strategy, and the same set of decisions, features, and states. We refer the reader to (Henderson, 2003) for details. Exact computation with this model is not tractable. Sampling of parse trees from the model is not feasible, because a generative model defines a joint model of both a sentence and a tree, thereby requiring sampling over the space of sentences. Gibbs sampling (Geman and Geman, 1984) is also impossible, because of the huge space of variables and need to resample after making eac</context>
<context position="18561" citStr="Henderson, 2003" startWordPosition="3089" endWordPosition="3090">ording to the approximate distribution Q. Unfortunately, even with this assumption there is no analytic way to maximize LV with respect to the means µtk, so we need to use numerical methods. Assuming (6), we can rewrite the expression (2) as follows, substituting the true P(H,V ) defined by the graphical model and the approximate distribution Q(H|V ), omitting parts independent of µtk: t0= Eηi E t00ERS(t0) j t00ERD(t0) k Φh(t,k0)(dt k0) E Wdt k0jµt j j ( � −µt i ln µt i − (1 − µt i) ln 1 − µt i This approximation method replicates exactly the computation of the feed-forward neural network in (Henderson, 2003), where the above means µt0 i are equivalent to the neural network hidden unit activations. Thus, that neural network probability model can be regarded as a simple approximation to the graphical model introduced in section 3.1. In addition to the drawbacks shared by any mean field approximation method, this feed-forward approximation cannot capture backward reasoning. By backward (a.k.a. top-down) reasoning we mean the need to update the state vector means µt0 i after observing a decision dtk, for t&apos; ≤ t. The next section discusses how backward reasoning can be incorporated in the approximate </context>
<context position="21002" citStr="Henderson, 2003" startWordPosition="3492" endWordPosition="3493">he mean field approximation makes the analytical computation of these derivatives impossible, so a different method needs to be used to compute their values. If maximization of Lt,kV is done until convergence, then the derivatives of Lt,kV with respect to µti are close to zero: Fit aL t Pz� 0 for all i. ∂µi This system of equations allows us to use implicit differentiation to compute the needed derivatives. 4 Experimental Evaluation In this section we evaluate the two approximations to dynamic SBNs discussed in the previous section, the feed-forward method equivalent to the neural network of (Henderson, 2003) (NN method) and the mean field method (MF method). The hypothesis we wish to test is that the more accurate approximation of dynamic SBNs will result in a more accurate model of constituent structure parsing. If this is true, then it suggests that dynamic SBNs of the form proposed here are a good abstract model of the nature of natural language parsing. We used the Penn Treebank WSJ corpus (Marcus et al., 1993) to perform the empirical evaluation of the considered approaches. It is expensive to train R P F1 Bikel, 2004 87.9 88.8 88.3 Taskar et al., 2004 89.1 89.1 89.1 NN method 89.1 89.2 89.1</context>
<context position="25931" citStr="Henderson, 2003" startWordPosition="4318" endWordPosition="4319">t parsing based on dynamic Sigmoid mations such as those tested here, with a very strong Belief Networks with vectors of latent variables. factorisability assumption, allow us to build quite Exact inference with the proposed graphical model accurate parsing models. The main drawback of our (called Incremental Sigmoid Belief Networks) is proposed mean field approach is the relative compu- not tractable, but two approximations are considtational complexity of the numerical procedure used ered. First, it is shown that the neural network to maximize Lv . But this approximation has suc- parser of (Henderson, 2003) can be considered as a ceeded in showing that a more accurate approxima- simple feed-forward approximation to the graphical tion of ISBNs results in a more accurate parser. We model. Second, a more accurate but still tractable believe this provides strong justification for more ac- approximation based on mean field theory is procurate approximations of ISBNs for parsing. posed. Both methods are empirically compared, and 5 Related Work the mean field approach achieves significantly better There has not been much previous work on graph- results, which are non-significantly different from ical m</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>James Henderson. 2003. Inducing history representations for broad coverage statistical parsing. In Proc. HLT-NAACL, pages 103–110, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In Proc. ACL,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2849" citStr="Henderson, 2004" startWordPosition="429" endWordPosition="430">rivation decisions, each decision conditioned on the unbounded history of previous decisions. The most common approach to handling the unbounded nature of the histories is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. Another previous approach is to use neural networks to compute a compressed representation of the history and condition decisions on this representation (Henderson, 2003; Henderson, 2004). It is possible that an unbounded amount of information is encoded in the compressed representation via its continuous values, but it is not clear whether this is actually happening due to the lack of any principled interpretation for these continuous values. Like the former approach, we assume that there are a finite set of features which encode the relevant information about the parse history. But unlike that approach, we allow feature values to be ambiguous, and represent each feature as a distribution over (binary) values. In other words, these history features are treated as latent varia</context>
<context position="10611" citStr="Henderson, 2004" startWordPosition="1695" endWordPosition="1697">e necessary for an undirected graphical model. 634 Figure 1: Illustration of an ISBN. 3 The Probabilistic Model of Parsing In this section we present our framework for syntactic parsing with dynamic Sigmoid Belief Networks. We first specify the form of SBN we propose, namely ISBNs, and then two methods for approximating the inference problems required for parsing. We only consider generative models of parsing, since generative probability models are simpler and we are focused on probability estimation, not decision making. Although the most accurate parsing models (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000) are discriminative, all the most accurate discriminative models make use of a generative model. More accurate generative models should make the discriminative models which use them more accurate as well. Also, there are some applications, such as language modeling, which require generative models. 3.1 The Graphical Model In ISBNs, we use a history-based model, which decomposes the probability of the parse as: P (T ) = P (D1, ..., Dm) = � P (Dt|D1,..., Dt−1), t where T is the parse tree and D1, ... , Dm is its equivalent sequence of parser decisions. Instead of treating each Dt</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Discriminative training of a neural network statistical parser. In Proc. ACL, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
<author>P Dayan</author>
<author>B Frey</author>
<author>R Neal</author>
</authors>
<title>The wake-sleep algorithm for unsupervised neural networks.</title>
<date>1995</date>
<journal>Science,</journal>
<pages>268--1158</pages>
<contexts>
<context position="6495" citStr="Hinton et al., 1995" startWordPosition="993" endWordPosition="996">iational methods have also been proposed for approximating SBNs (Saul and Jordan, 1999). The main idea of variational methods (Jordan et al., 1999) is, roughly, to construct a tractable approximate model with a number of free parameters. The free parameters are set so that the resulting approximate model is as close as possible to the original graphical model for a given inference problem. 2.1 Mean Field Approximation Methods The simplest example of a variation method is the mean field method, originally introduced in statistical mechanics and later applied to unsupervised neural networks in (Hinton et al., 1995). Let us denote the set of visible variables in the model (i.e. the inputs and outputs) by V and hidden variables by H = h1,. . . , hl. The mean field method uses a fully factorized distribution Q as the approximate model: � Q(H|V ) = Qi(hi|V ). i where each Qi is the distribution of an individual latent variable. The independence between the variables hi in this approximate distribution Q does not imply independence of the free parameters which define the Qi. These parameters are set to minimize the Kullback-Leibler divergence (Cover and Thomas, 1991) between the approximate distribution Q(H|</context>
</contexts>
<marker>Hinton, Dayan, Frey, Neal, 1995</marker>
<rawString>G. Hinton, P. Dayan, B. Frey, and R. Neal. 1995. The wake-sleep algorithm for unsupervised neural networks. Science, 268:1158–1161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M I Jordan</author>
<author>T S Jaakkola Z Ghahramani</author>
<author>L K Saul</author>
</authors>
<title>An introduction to variational methods for graphical models.</title>
<date>1999</date>
<booktitle>Learning in Graphical Models.</booktitle>
<editor>In Michael I. Jordan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6022" citStr="Jordan et al., 1999" startWordPosition="918" endWordPosition="921">Ns where we allow variables with any range of discrete values. We thus generalize the logistic sigmoid function to the normalized exponential (a.k.a. softmax) function to define the conditional probabilities for non-binary variables. Exact inference with all but very small SBNs is not tractable. Initially sampling methods were used (Neal, 1992), but this is also not feasible for large networks, especially for the dynamic models of the type described in section 2.2. Variational methods have also been proposed for approximating SBNs (Saul and Jordan, 1999). The main idea of variational methods (Jordan et al., 1999) is, roughly, to construct a tractable approximate model with a number of free parameters. The free parameters are set so that the resulting approximate model is as close as possible to the original graphical model for a given inference problem. 2.1 Mean Field Approximation Methods The simplest example of a variation method is the mean field method, originally introduced in statistical mechanics and later applied to unsupervised neural networks in (Hinton et al., 1995). Let us denote the set of visible variables in the model (i.e. the inputs and outputs) by V and hidden variables by H = h1,. .</context>
</contexts>
<marker>Jordan, Ghahramani, Saul, 1999</marker>
<rawString>M. I. Jordan, Z.Ghahramani, T. S. Jaakkola, and L. K. Saul. 1999. An introduction to variational methods for graphical models. In Michael I. Jordan, editor, Learning in Graphical Models. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Hidden-variable models for discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. EMNLP,</booktitle>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="1162" citStr="Koo and Collins, 2005" startWordPosition="164" endWordPosition="167">vious feed-forward neural network parsing model can be viewed as a coarse approximation to inference with this class of graphical model. By constructing a more accurate but still tractable approximation, we significantly improve parsing accuracy, suggesting that ISBNs provide a good idealization for parsing. This generative model of parsing achieves state-of-theart results on WSJ text and 8% error reduction over the baseline neural network parser. 1 Introduction Latent variable models have recently been of increasing interest in Natural Language Processing, and in parsing in particular (e.g. (Koo and Collins, 2005; Matsuzaki et al., 2005; Riezler et al., 2002)). Latent variables provide a principled way to include features in a probability model without needing to have data labeled with those features in advance. Instead, a labeling with these features can be induced as part of the training process. The difficulty with latent variable models is that even small numbers of latent variables can lead to computationally intractable inference (a.k.a. decoding, parsing). In this paper we propose a solution to this problem based on dynamic Sigmoid Belief Networks (SBNs) (Neal, 1992). The dynamic SBNs 632 which</context>
<context position="26763" citStr="Koo and Collins, 2005" startWordPosition="4445" endWordPosition="4448"> still tractable believe this provides strong justification for more ac- approximation based on mean field theory is procurate approximations of ISBNs for parsing. posed. Both methods are empirically compared, and 5 Related Work the mean field approach achieves significantly better There has not been much previous work on graph- results, which are non-significantly different from ical models for full parsing, although recently sev- the results of the most accurate generative parsing eral latent variable models for parsing have been model (Charniak, 2000) on our testing set. The fact proposed (Koo and Collins, 2005; Matsuzaki et al., that a more accurate approximation leads to a more 2005; Riezler et al., 2002). In (Koo and Collins, accurate parser suggests that ISBNs are a good ab2005), an undirected graphical model is used for stract model for constituent structure parsing. This parse reranking. Dependency parsing with dynamic empirical result motivates research into more accuBayesian networks was considered in (Peshkin and rate approximations of dynamic SBNs. Savova, 2005), with limited success. Their model We focused in this paper on generative models is very different from ours. Roughly, it conside</context>
</contexts>
<marker>Koo, Collins, 2005</marker>
<rawString>Terry Koo and Michael Collins. 2005. Hidden-variable models for discriminative reranking. In Proc. EMNLP, Vancouver, B.C., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="21417" citStr="Marcus et al., 1993" startWordPosition="3565" endWordPosition="3568">4 Experimental Evaluation In this section we evaluate the two approximations to dynamic SBNs discussed in the previous section, the feed-forward method equivalent to the neural network of (Henderson, 2003) (NN method) and the mean field method (MF method). The hypothesis we wish to test is that the more accurate approximation of dynamic SBNs will result in a more accurate model of constituent structure parsing. If this is true, then it suggests that dynamic SBNs of the form proposed here are a good abstract model of the nature of natural language parsing. We used the Penn Treebank WSJ corpus (Marcus et al., 1993) to perform the empirical evaluation of the considered approaches. It is expensive to train R P F1 Bikel, 2004 87.9 88.8 88.3 Taskar et al., 2004 89.1 89.1 89.1 NN method 89.1 89.2 89.1 Turian and Melamed, 2006 89.3 89.6 89.4 MF method 89.3 90.7 90.0 Charniak, 2000 90.0 90.2 90.1 Table 1: Percentage labeled constituent recall (R), precision (P), combination of both (F1) on the testing set. the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al., 2004) and (Turian and Melamed, 2006). The standard split of the corpus into training</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1186" citStr="Matsuzaki et al., 2005" startWordPosition="168" endWordPosition="171">al network parsing model can be viewed as a coarse approximation to inference with this class of graphical model. By constructing a more accurate but still tractable approximation, we significantly improve parsing accuracy, suggesting that ISBNs provide a good idealization for parsing. This generative model of parsing achieves state-of-theart results on WSJ text and 8% error reduction over the baseline neural network parser. 1 Introduction Latent variable models have recently been of increasing interest in Natural Language Processing, and in parsing in particular (e.g. (Koo and Collins, 2005; Matsuzaki et al., 2005; Riezler et al., 2002)). Latent variables provide a principled way to include features in a probability model without needing to have data labeled with those features in advance. Instead, a labeling with these features can be induced as part of the training process. The difficulty with latent variable models is that even small numbers of latent variables can lead to computationally intractable inference (a.k.a. decoding, parsing). In this paper we propose a solution to this problem based on dynamic Sigmoid Belief Networks (SBNs) (Neal, 1992). The dynamic SBNs 632 which we peopose, called Incr</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc. ACL, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford Neal</author>
</authors>
<title>Connectionist learning of belief networks.</title>
<date>1992</date>
<journal>Artificial Intelligence,</journal>
<pages>56--71</pages>
<contexts>
<context position="1734" citStr="Neal, 1992" startWordPosition="260" endWordPosition="261">particular (e.g. (Koo and Collins, 2005; Matsuzaki et al., 2005; Riezler et al., 2002)). Latent variables provide a principled way to include features in a probability model without needing to have data labeled with those features in advance. Instead, a labeling with these features can be induced as part of the training process. The difficulty with latent variable models is that even small numbers of latent variables can lead to computationally intractable inference (a.k.a. decoding, parsing). In this paper we propose a solution to this problem based on dynamic Sigmoid Belief Networks (SBNs) (Neal, 1992). The dynamic SBNs 632 which we peopose, called Incremental Sigmoid Belief Networks (ISBNs) have large numbers of latent variables, which makes exact inference intractable. However, they can be approximated sufficiently well to build fast and accurate statistical parsers which induce features during training. We use SBNs in a generative history-based model of constituent structure parsing. The probability of an unbounded structure is decomposed into a sequence of probabilities for individual derivation decisions, each decision conditioned on the unbounded history of previous decisions. The mos</context>
<context position="5081" citStr="Neal, 1992" startWordPosition="770" endWordPosition="771">ignificantly higher accuracy than the neural network parser (90.0% F-measure vs 89.1%). We argue that this correlation between better approximation and better accuracy suggests that dynamic SBNs are a good abstract model for natural language parsing. 2 Sigmoid Belief Networks A belief network, or a Bayesian network, is a directed acyclic graph which encodes statistical dependencies between variables. Each variable Si in the graph has an associated conditional probability distributions P(Si|Par(Si)) over its values given the values of its parents Par(Si) in the graph. A Sigmoid Belief Network (Neal, 1992) is a particular type of belief networks with binary variables and conditional probability distributions in the form of the logistic sigmoid function: P(Si=1|Par(Si)) = , 1+exp(− ESjEPar(Si) JijSj) where Jij is the weight for the edge from variable Sj to variable Si. In this paper we consider a generalized version of SBNs where we allow variables with any range of discrete values. We thus generalize the logistic sigmoid function to the normalized exponential (a.k.a. softmax) function to define the conditional probabilities for non-binary variables. Exact inference with all but very small SBNs </context>
</contexts>
<marker>Neal, 1992</marker>
<rawString>Radford Neal. 1992. Connectionist learning of belief networks. Artificial Intelligence, 56:71–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Peshkin</author>
<author>Virginia Savova</author>
</authors>
<title>Dependency parsing with dynamic bayesian network.</title>
<date>2005</date>
<booktitle>In AAAI, 20th National Conference on Artificial Intelligence,</booktitle>
<location>Pittsburgh, Pennsylvania.</location>
<marker>Peshkin, Savova, 2005</marker>
<rawString>Leon Peshkin and Virginia Savova. 2005. Dependency parsing with dynamic bayesian network. In AAAI, 20th National Conference on Artificial Intelligence, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Press</author>
<author>B Flannery</author>
<author>S Teukolsky</author>
<author>W Vetterling</author>
</authors>
<title>Numerical Recipes.</title>
<date>1996</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="19633" citStr="Press et al., 1996" startWordPosition="3268" endWordPosition="3271">r means µt0 i after observing a decision dtk, for t&apos; ≤ t. The next section discusses how backward reasoning can be incorporated in the approximate model. 3.3 A Mean Field Approximation This section proposes a more accurate way to approximate ISBNs with mean field methods, which kE �E E ln Φh(t,k0)(d)exp( Wdjµt j)�, (7) d j here, ηti is computed from the previous relevant state means and decisions as in (4). This expression is 636 concave with respect to the parameters µti, so the global maximum can be found. We use coordinatewise ascent, where each µti is selected by an efficient line search (Press et al., 1996), while keeping other µti, fixed. 3.4 Parameter Estimation We train these models to maximize the fit of the approximate model to the data. We use gradient descent and a maximum likelihood objective function. This requires computation of the gradient of the approximate log-likelihood with respect to the model parameters. In order to compute these derivatives, the error should be propagated all the way back through the structure of the graphical model. For the feed-forward approximation, computation of the derivatives is straightforward, as in neural networks. But for the mean field approximatio</context>
</contexts>
<marker>Press, Flannery, Teukolsky, Vetterling, 1996</marker>
<rawString>W. Press, B. Flannery, S. Teukolsky, and W. Vetterling. 1996. Numerical Recipes. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>133--142</pages>
<location>Univ. of Pennsylvania, PA.</location>
<contexts>
<context position="22253" citStr="Ratnaparkhi, 1996" startWordPosition="3708" endWordPosition="3710">.6 89.4 MF method 89.3 90.7 90.0 Charniak, 2000 90.0 90.2 90.1 Table 1: Percentage labeled constituent recall (R), precision (P), combination of both (F1) on the testing set. the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al., 2004) and (Turian and Melamed, 2006). The standard split of the corpus into training (sections 2–22, 9,753 sentences), validation (section 24, 321 sentences), and testing (section 23, 603 sentences) was performed.2 As in (Henderson, 2003; Turian and Melamed, 2006) we used a publicly available tagger (Ratnaparkhi, 1996) to provide the part-of-speech tag for each word in the sentence. For each tag, there is an unknown-word vocabulary item which is used for all those words which are not sufficiently frequent with that tag to be included individually in the vocabulary. We only included a specific tag-word pair in the vocabulary if it occurred at least 20 time in the training set, which (with tag-unknown-word pairs) led to the very small vocabulary of 567 tag-word pairs. During parsing with both the NN method and the MF method, we used beam search with a post-word beam of 10. Increasing the beam size beyond this</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proc. EMNLP, pages 133–142, Univ. of Pennsylvania, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1209" citStr="Riezler et al., 2002" startWordPosition="172" endWordPosition="175"> can be viewed as a coarse approximation to inference with this class of graphical model. By constructing a more accurate but still tractable approximation, we significantly improve parsing accuracy, suggesting that ISBNs provide a good idealization for parsing. This generative model of parsing achieves state-of-theart results on WSJ text and 8% error reduction over the baseline neural network parser. 1 Introduction Latent variable models have recently been of increasing interest in Natural Language Processing, and in parsing in particular (e.g. (Koo and Collins, 2005; Matsuzaki et al., 2005; Riezler et al., 2002)). Latent variables provide a principled way to include features in a probability model without needing to have data labeled with those features in advance. Instead, a labeling with these features can be induced as part of the training process. The difficulty with latent variable models is that even small numbers of latent variables can lead to computationally intractable inference (a.k.a. decoding, parsing). In this paper we propose a solution to this problem based on dynamic Sigmoid Belief Networks (SBNs) (Neal, 1992). The dynamic SBNs 632 which we peopose, called Incremental Sigmoid Belief </context>
<context position="26861" citStr="Riezler et al., 2002" startWordPosition="4462" endWordPosition="4465">an field theory is procurate approximations of ISBNs for parsing. posed. Both methods are empirically compared, and 5 Related Work the mean field approach achieves significantly better There has not been much previous work on graph- results, which are non-significantly different from ical models for full parsing, although recently sev- the results of the most accurate generative parsing eral latent variable models for parsing have been model (Charniak, 2000) on our testing set. The fact proposed (Koo and Collins, 2005; Matsuzaki et al., that a more accurate approximation leads to a more 2005; Riezler et al., 2002). In (Koo and Collins, accurate parser suggests that ISBNs are a good ab2005), an undirected graphical model is used for stract model for constituent structure parsing. This parse reranking. Dependency parsing with dynamic empirical result motivates research into more accuBayesian networks was considered in (Peshkin and rate approximations of dynamic SBNs. Savova, 2005), with limited success. Their model We focused in this paper on generative models is very different from ours. Roughly, it considered of parsing. The results of such a generative model the whole sentence at a time, with the grap</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proc. ACL, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Sallans</author>
</authors>
<title>Reinforcement Learning for Factored Markov Decision Processes.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Toronto,</institution>
<location>Toronto, Canada.</location>
<contexts>
<context position="24853" citStr="Sallans, 2002" startWordPosition="4159" endWordPosition="4160">As discussed in section 2.2, undirected graphreduction in F-measure exceeding 8%. This im- ical models do not seem to be suitable for historyprovement is statically significant.3 The MF model based full parsing models. achieves results which do not appear to be signifi- Sigmoid Belief Networks were used originally cantly different from the results of the best model for character recognition tasks, but later a dynamic in the list (Charniak, 2000). It should also be noted modification of this model was applied to the reinthat the model (Charniak, 2000) is the most accu- forcement learning task (Sallans, 2002). However, rate generative model on the standard WSJ parsing their graphical model, approximation method, and benchmark, which confirms the viability of our gen- learning method differ significantly from those of erative model. this paper. These experimental results suggest that Incre- 6 Conclusions mental Sigmoid Belief Networks are an appropriate This paper proposes a new generative framework model for natural language parsing. Even approxi- for constituent parsing based on dynamic Sigmoid mations such as those tested here, with a very strong Belief Networks with vectors of latent variables.</context>
</contexts>
<marker>Sallans, 2002</marker>
<rawString>Brian Sallans. 2002. Reinforcement Learning for Factored Markov Decision Processes. Ph.D. thesis, University of Toronto, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence K Saul</author>
<author>Michael I Jordan</author>
</authors>
<title>A mean field learning algorithm for unsupervised neural networks.</title>
<date>1999</date>
<booktitle>Learning in Graphical Models,</booktitle>
<pages>541--554</pages>
<editor>In Michael I. Jordan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4158" citStr="Saul and Jordan, 1999" startWordPosition="625" endWordPosition="629">Computational Linguistics, pages 632–639, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics preting the history representations as distributions over discrete values of latent variables makes the exact computation of decision probabilities intractable. Exact computation requires marginalizing out the latent variables, which involves summing over all possible vectors of discrete values, which is exponential in the length of the vector. We propose two forms of approximation for dynamic SBNs, a neural network approximation and a form of mean field approximation (Saul and Jordan, 1999). We first show that the previous neural network model of (Henderson, 2003) can be viewed as a coarse approximation to inference with ISBNs. We then propose an incremental mean field method, which results in an improved approximation over the neural network but remains tractable. The resulting parser achieves significantly higher accuracy than the neural network parser (90.0% F-measure vs 89.1%). We argue that this correlation between better approximation and better accuracy suggests that dynamic SBNs are a good abstract model for natural language parsing. 2 Sigmoid Belief Networks A belief ne</context>
<context position="5962" citStr="Saul and Jordan, 1999" startWordPosition="908" endWordPosition="911">able Si. In this paper we consider a generalized version of SBNs where we allow variables with any range of discrete values. We thus generalize the logistic sigmoid function to the normalized exponential (a.k.a. softmax) function to define the conditional probabilities for non-binary variables. Exact inference with all but very small SBNs is not tractable. Initially sampling methods were used (Neal, 1992), but this is also not feasible for large networks, especially for the dynamic models of the type described in section 2.2. Variational methods have also been proposed for approximating SBNs (Saul and Jordan, 1999). The main idea of variational methods (Jordan et al., 1999) is, roughly, to construct a tractable approximate model with a number of free parameters. The free parameters are set so that the resulting approximate model is as close as possible to the original graphical model for a given inference problem. 2.1 Mean Field Approximation Methods The simplest example of a variation method is the mean field method, originally introduced in statistical mechanics and later applied to unsupervised neural networks in (Hinton et al., 1995). Let us denote the set of visible variables in the model (i.e. the</context>
<context position="7370" citStr="Saul and Jordan, 1999" startWordPosition="1155" endWordPosition="1158"> each Qi is the distribution of an individual latent variable. The independence between the variables hi in this approximate distribution Q does not imply independence of the free parameters which define the Qi. These parameters are set to minimize the Kullback-Leibler divergence (Cover and Thomas, 1991) between the approximate distribution Q(H|V ) and the true distribution P(H|V ): Q(H|V ) ln Q(H|V ) P (H|V ), (1) or, equivalently, to maximize the expression: P(H, V ) Q(H|V ) ln. (2) Q(H|V ) The expression LV is a lower bound on the loglikelihood lnP(V ). It is used in the mean field theory (Saul and Jordan, 1999) to approximate the likelihood. However, in our case of dynamic graphical models, we have to use a different approach which allows us to construct an incremental parsing method without needing to introduce the additional parameters proposed in (Saul and Jordan, 1999). We will describe our modification of the mean field method in section 3.3. 1 � KL(QkP ) = H � LV = H 633 2.2 Dynamics Dynamic Bayesian networks are Bayesian networks applied to arbitrarily long sequences. A new set of variables is instantiated for each position in the sequence, but the edges and weights for these variables are th</context>
</contexts>
<marker>Saul, Jordan, 1999</marker>
<rawString>Lawrence K. Saul and Michael I. Jordan. 1999. A mean field learning algorithm for unsupervised neural networks. In Michael I. Jordan, editor, Learning in Graphical Models, pages 541–554. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="23969" citStr="Sha and Pereira, 2003" startWordPosition="4016" endWordPosition="4019">a model for the entire WSJ corpus can be trained in about 3 months time. The training time is about linear with the number of words, but a larger state vector is needed to accommodate all the information. The long training times on the entire WSJ would not allow us to tune the model parameters properly, which would have increased the randomness of the empirical comparison, although it would be feasible for building a system. 637 ferent generative and discriminative parsing meth- tional Random Fields, are the standard tools for shalods (Bikel, 2004; Taskar et al., 2004; Turian and low parsing (Sha and Pereira, 2003). However, shalMelamed, 2006; Charniak, 2000) evaluated in the low parsing is effectively a sequence labeling probsame experimental setup. The MF model improves lem and therefore differs significantly from full parsover the baseline NN approximation, with an error ing. As discussed in section 2.2, undirected graphreduction in F-measure exceeding 8%. This im- ical models do not seem to be suitable for historyprovement is statically significant.3 The MF model based full parsing models. achieves results which do not appear to be signifi- Sigmoid Belief Networks were used originally cantly differe</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proc. HLT-NAACL, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael Collins</author>
<author>Daphne Koller</author>
<author>Christopher Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="21562" citStr="Taskar et al., 2004" startWordPosition="3591" endWordPosition="3594">ethod equivalent to the neural network of (Henderson, 2003) (NN method) and the mean field method (MF method). The hypothesis we wish to test is that the more accurate approximation of dynamic SBNs will result in a more accurate model of constituent structure parsing. If this is true, then it suggests that dynamic SBNs of the form proposed here are a good abstract model of the nature of natural language parsing. We used the Penn Treebank WSJ corpus (Marcus et al., 1993) to perform the empirical evaluation of the considered approaches. It is expensive to train R P F1 Bikel, 2004 87.9 88.8 88.3 Taskar et al., 2004 89.1 89.1 89.1 NN method 89.1 89.2 89.1 Turian and Melamed, 2006 89.3 89.6 89.4 MF method 89.3 90.7 90.0 Charniak, 2000 90.0 90.2 90.1 Table 1: Percentage labeled constituent recall (R), precision (P), combination of both (F1) on the testing set. the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al., 2004) and (Turian and Melamed, 2006). The standard split of the corpus into training (sections 2–22, 9,753 sentences), validation (section 24, 321 sentences), and testing (section 23, 603 sentences) was performed.2 As in (Henders</context>
<context position="23921" citStr="Taskar et al., 2004" startWordPosition="4008" endWordPosition="4011"> a standard desktop PC. We would expect that a model for the entire WSJ corpus can be trained in about 3 months time. The training time is about linear with the number of words, but a larger state vector is needed to accommodate all the information. The long training times on the entire WSJ would not allow us to tune the model parameters properly, which would have increased the randomness of the empirical comparison, although it would be feasible for building a system. 637 ferent generative and discriminative parsing meth- tional Random Fields, are the standard tools for shalods (Bikel, 2004; Taskar et al., 2004; Turian and low parsing (Sha and Pereira, 2003). However, shalMelamed, 2006; Charniak, 2000) evaluated in the low parsing is effectively a sequence labeling probsame experimental setup. The MF model improves lem and therefore differs significantly from full parsover the baseline NN approximation, with an error ing. As discussed in section 2.2, undirected graphreduction in F-measure exceeding 8%. This im- ical models do not seem to be suitable for historyprovement is statically significant.3 The MF model based full parsing models. achieves results which do not appear to be signifi- Sigmoid Bel</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and Christopher Manning. 2004. Max-margin parsing. In Proc. EMNLP, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Dan Melamed</author>
</authors>
<title>Advances in discriminative parsing.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="21627" citStr="Turian and Melamed, 2006" startWordPosition="3603" endWordPosition="3606">(NN method) and the mean field method (MF method). The hypothesis we wish to test is that the more accurate approximation of dynamic SBNs will result in a more accurate model of constituent structure parsing. If this is true, then it suggests that dynamic SBNs of the form proposed here are a good abstract model of the nature of natural language parsing. We used the Penn Treebank WSJ corpus (Marcus et al., 1993) to perform the empirical evaluation of the considered approaches. It is expensive to train R P F1 Bikel, 2004 87.9 88.8 88.3 Taskar et al., 2004 89.1 89.1 89.1 NN method 89.1 89.2 89.1 Turian and Melamed, 2006 89.3 89.6 89.4 MF method 89.3 90.7 90.0 Charniak, 2000 90.0 90.2 90.1 Table 1: Percentage labeled constituent recall (R), precision (P), combination of both (F1) on the testing set. the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al., 2004) and (Turian and Melamed, 2006). The standard split of the corpus into training (sections 2–22, 9,753 sentences), validation (section 24, 321 sentences), and testing (section 23, 603 sentences) was performed.2 As in (Henderson, 2003; Turian and Melamed, 2006) we used a publicly available </context>
</contexts>
<marker>Turian, Melamed, 2006</marker>
<rawString>Joseph Turian and Dan Melamed. 2006. Advances in discriminative parsing. In Proc. COLING-ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of the result differences.</title>
<date>2000</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>947--953</pages>
<location>Saarbruken, Germany.</location>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of the result differences. In Proc. COLING, pages 947–953, Saarbruken, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>