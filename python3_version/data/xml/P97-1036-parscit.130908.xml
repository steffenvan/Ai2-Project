<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000098">
<title confidence="0.778815">
Unification-based Multimodal Integration
</title>
<author confidence="0.8824225">
Michael Johnston, Philip R. Cohen, David McGee,
Sharon L. Oviatt, James A. Pittman, Ira Smith
</author>
<affiliation confidence="0.968971333333333">
Center for Human Computer Communication
Department of Computer Science and Engineering
Oregon Graduate Institute, PO BOX 91000, Portland, OR 97291, USA.
</affiliation>
<email confidence="0.941431">
{johnston,pcohen,dmcgee,oviatt,jay,ira}Ocse.ogi.edu
</email>
<sectionHeader confidence="0.994287" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999550941176471">
Recent empirical research has shown con-
clusive advantages of multimodal interac-
tion over speech-only interaction for map-
based tasks. This paper describes a mul-
timodal language processing architecture
which supports interfaces allowing simulta-
neous input from speech and gesture recog-
nition. Integration of spoken and gestural
input is driven by unification of typed fea-
ture structures representing the semantic
contributions of the different modes. This
integration method allows the component
modalities to mutually compensate for each
others&apos; errors. It is implemented in Quick-
Set, a multimodal (pen/voice) system that
enables users to set up and control dis-
tributed interactive simulations.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999786659574468">
By providing a number of channels through which
information may pass between user and computer,
multimodal interfaces promise to significantly in-
crease the bandwidth and fluidity of the interface
between humans and machines. In this work, we are
concerned with the addition of multimodal input to
the interface. In particular, we focus on interfaces
which support simultaneous input from speech and
pen, utilizing speech recognition and recognition of
gestures and drawings made with a pen on a complex
visual display, such as a map.
Our focus on multimodal interfaces is motivated,
in part, by the trend toward portable computing de-
vices for which complex graphical user interfaces are
infeasible. For such devices, speech and gesture will
be the primary means of user input. Recent em-
pirical results (Oviatt 1996) demonstrate clear task
performance and user preference advantages for mul-
timodal interfaces over speech only interfaces, in par-
ticular for spatial tasks such as those involving maps.
Specifically, in a within-subject experiment during
which the same users performed the same tasks in
various conditions using only speech, only pen, or
both speech and pen-based input, users&apos; multimodal
input to maps resulted in 10% faster task comple-
tion time, 23% fewer words, 35% fewer spoken dis-
fluencies, and 36% fewer task errors compared to
unimodal spoken input. Of the user errors, 48% in-
volved location errors on the mapâ€”errors that were
nearly eliminated by the simple ability to use pen-
based input. Finally, 100% of users indicated a pref-
erence for multimodal interaction over speech-only
interaction with maps. These results indicate that
for map-based tasks, users would both perform bet-
ter and be more satisfied when using a multimodal
interface. As an illustrative example, in the dis-
tributed simulation application we describe in this
paper, one user task is to add a &amp;quot;phase line&amp;quot; to a
map. In the existing unimodal interface for this ap-
plication (CommandTalk, Moore 1997), this is ac-
complished with a spoken utterance such as &apos;CRE-
ATE A LINE FROM COORDINATES NINE FOUR
THREE NINE THREE ONE TO NINE EIGHT
NINE NINE FIVE ZERO AND CALL IT PHASE
LINE GREEN&apos;. In contrast the same task can be ac-
complished by saying &apos;PHASE LINE GREEN&apos; and
simultaneously drawing the gesture in Figure 1.
</bodyText>
<figureCaption confidence="0.998272">
Figure 1: Line gesture
</figureCaption>
<bodyText confidence="0.99993">
The multimodal command involves speech recog-
nition of only a three word phrase, while the equiva-
lent unimodal speech command involves recognition
of a complex twenty four word expression. Further-
more, using unimodal speech to indicate more corn-
</bodyText>
<page confidence="0.994654">
281
</page>
<bodyText confidence="0.999536681818182">
plex spatial features such as routes and areas is prac-
tically infeasible if accuracy of shape is important.
Another significant advantage of multimodal over
unimodal speech is that it allows the user to switch
modes when environmental noise or security con-
cerns make speech an unacceptable input medium,
or for avoiding and repairing recognition errors (Ovi-
att and Van Gent 1996). Multimodality also offers
the potential for input modes to mutually compen-
sate for each others&apos; errors. We will demonstrate
in our system, multimodal integration allows
speech input to compensate for errors in gesture
recognition and vice versa.
Systems capable of integration of speech and ges-
ture have existed since the early 80&apos;s. One of the
first such systems was the &amp;quot;Put-That-There&amp;quot; sys-
tem (Bolt 1980). However, in the sixteen years since
then, research on multimodal integration has not
yielded a reusable scalable architecture for the con-
struction of multimodal systems that integrate ges-
ture and voice. There are four major limiting factors
in previous approaches to multimodal integration:
</bodyText>
<listItem confidence="0.995223904761905">
(i) The majority of approaches limit the bandwidth
of the gestural mode to simple deictic pointing
gestures made with a mouse (Neal and Shapiro
1991, Cohen 1991, Cohen 1992, Brison and
Vigouroux (ms.), Wauchope 1994) or with the
hand (Koons et al 19931).
(ii) Most previous approaches have been primarily
speech-driven2 , treating gesture as a secondary
dependent mode (Neal and Shapiro 1991, Co-
hen 1991, Cohen 1992, Brison and Vigouroux
(ms.), Koons et al 1993, Wauchope 1994). In
these systems, integration of gesture is triggered
by the appearance of expressions in the speech
stream whose reference needs to be resolved,
such as definite and deictic noun phrases (e.g.
&apos;this one&apos;, &apos;the red cube&apos;).
(iii) None of the existing approaches provide a well-
understood generally applicable common mean-
ing representation for the different modes, or,
(iv) A general and formally-well defined mechanism
for multimodal integration.
</listItem>
<tableCaption confidence="0.855800777777778">
&apos;Koons et al 1993 describe two different systems. The
first uses input from hand gestures and eye gaze in order
to aid in determining the reference of noun phrases in the
speech stream. The second allows users to manipulate
objects in a blocks world using iconic and pantomimic
gestures in addition to deictic gestures.
2More precisely, they are &apos;verbal language&apos;-driven.
Either spoken or typed linguistic expressions are the
driving force of interpretation.
</tableCaption>
<bodyText confidence="0.999928777777778">
We present an approach to multimodal integra-
tion which overcomes these limiting factors. A wide
base of continuous gestural input is supported and
integration may be driven by either mode. Typed
feature structures (Carpenter 1992) are used to pro-
vide a clearly defined and well understood common
meaning representation for the modes, and multi-
modal integration is accomplished through unifica-
tion.
</bodyText>
<sectionHeader confidence="0.947959666666667" genericHeader="method">
2 Quickset: A Multimodal Interface
for Distributed Interactive
Simulation
</sectionHeader>
<bodyText confidence="0.999894382352941">
The initial application of our multimodal interface
architecture has been in the development of the
QuickSet system, an interface for setting up and
interacting with distributed interactive simulations.
QuickSet provides a portal into LeatherNee , a sim-
ulation system used for the training of US Marine
Corps platoon leaders. LeatherNet simulates train-
ing exercises using the ModSAF simulator (Courte-
manche and Ceranowicz 1995) and supports 3D vi-
sualization of the simulated exercises using Com-
mandVu (Clarkson and Yi 1996). SRI Interna-
tional&apos;s CommandTalk provides a unimodal spoken
interface to LeatherNet (Moore et al 1997).
QuickSet is a distributed system consisting of a
collection of agents that communicate through the
Open Agent Architecture&apos; (Cohen et al 1994). It
runs on both desktop and hand-held PCs under Win-
dows 95, communicating over wired and wireless
LANs (respectively), or modem links. The wire-
less hand-held unit is a 3-lb Fujitsu Stylistic 1000
(Figure 2). We have also developed a Java-based
QuickSet agent that provides a portal to the simula-
tion over the World Wide Web. The QuickSet user
interface displays a map of the terrain on which the
simulated military exercise is to take place (Figure
2). The user can gesture and draw directly on the
map with the pen and simultaneously issue spoken
commands. Units and objectives can be laid down
on the map by speaking their name and gesturing
on the desired location. The map can also be an-
notated with line features such as barbed wire and
fortified lines, and area features such as minefields
and landing zones. These are created by drawing the
appropriate spatial feature on the map and speak-
</bodyText>
<construct confidence="0.424502571428572">
3LeatherNet is currently being developed by the
Naval Command, Control and Ocean Surveillance Cen-
ter (NCCOSC) Research. Development, Test and Eval-
uation Division (NRaD) in coordination with a number
of contractors.
40pen Agent Architecture is a trademark of SRI
International.
</construct>
<page confidence="0.990205">
282
</page>
<figureCaption confidence="0.999843">
Figure 2: The QuickSet user interface
</figureCaption>
<bodyText confidence="0.999670967741935">
ing its name. Units, objectives, and lines can also
be generated using unimodal gestures by drawing
their map symbols in the desired location. Orders
can be assigned to units, for example, in Figure 2
an M1A1 platoon on the bottom left has been as-
signed a route to follow. This order is created mul-
timodally by drawing the curved route and saying
&apos;WHISKEY FOUR SIX FOLLOW THIS ROUTE&apos;.
As entities are created and assigned orders they are
displayed on the UI and automatically instantiated
in a simulation database maintained by the ModSAF
simulator.
Speech recognition operates in either a click-to-
speak mode, in which the microphone is activated
when the pen is placed on the screen, or open micro-
phone mode. The speech recognition agent is built
using a continuous speaker-independent recognizer
commercially available from IBM.
When the user draws or gestures on the map, the
resulting electronic &apos;ink&apos; is passed to a gesture recog-
nition agent, which utilizes both a neural network
and a set of hidden Markov models. The ink is size-
normalized, centered in a 2D image, and fed into the
neural network as pixels, as well as being smoothed,
resampled, converted to deltas, and fed to the HMM
recognizer. The gesture recognizer currently recog-
nizes a total of twenty six different gestures, some of
which are illustrated in Figure 3. They include var-
ious military map symbols such as platoon, mortar,
and fortified line, editing gestures such as deletion,
and spatial features such as routes and areas.
</bodyText>
<figureCaption confidence="0.993559">
Figure 3: Example symbols and gestures
</figureCaption>
<bodyText confidence="0.898407">
As with all recognition technologies, gesture
recognition may result in errors. One of the factors
</bodyText>
<figure confidence="0.99736475">
--..----1---&amp;quot;1-- â€¢
line
â€¢ It 0 are a point
FEDI
tank me chanized
platoon company
mortar deletion
fortfied line barbed vire
</figure>
<page confidence="0.995762">
283
</page>
<bodyText confidence="0.999222333333333">
contributing to this is that routes and areas do not
have signature shapes that can be used to identify
them and are frequently confused (Figure 4).
</bodyText>
<figureCaption confidence="0.997123">
Figure 4: Pen drawings of routes and areas
</figureCaption>
<bodyText confidence="0.99972044">
Another contributing factor is that users&apos; pen in-
put is often sloppy (Figure 5) and map symbols can
be confused among themselves and with route and
area gestures.
1989, Moshier 1988). Unification is an operation
that determines the consistency of two pieces of par-
tial information, and if they are consistent combines
them into a single result. As such, it is ideally suited
to the task at hand, in which we want to determine
whether a given piece of gestural input is compatible
with a given piece of spoken input, and if they are
compatible, to combine the two inputs into a single
result that can be interpreted by the system.
The use of feature structures as a semantic rep-
resentation framework facilitates the specification of
partial meanings. Spoken or gestural input which
partially specifies a command can be represented
as an underspecified feature structure in which cer-
tain features are not instantiated. The adoption of
typed feature structures facilitates the statement of
constraints on integration. For example, if a given
speech input can be integrated with a line gesture,
it can be assigned a feature structure with an under-
specified location feature whose value is required to
be of type line.
</bodyText>
<figure confidence="0.99148425">
Cpe FC1.7eri
Client
mortar tank deletion
platoon
</figure>
<figureCaption confidence="0.992761">
Figure 5: Typical pen input from
</figureCaption>
<figure confidence="0.9671665">
me chanized
company
real users
Speech Gesture
Recognition Recognition
Agent Agent
</figure>
<bodyText confidence="0.999860166666667">
Given the potential for error, the gesture recog-
nizer issues not just a single interpretation, but a
series of potential interpretations ranked with re-
spect to probability. The correct interpretation is
frequently determined as a result of multimodal in-
tegration, as illustrated below5.
</bodyText>
<sectionHeader confidence="0.974365" genericHeader="method">
3 A Unification-based Architecture
for Multimodal Integration
</sectionHeader>
<bodyText confidence="0.931718307692308">
One the most significant challenges facing the devel-
opment of effective multimodal interfaces concerns
the integration of input from different modes. In-
put signals from each of the modes can be assigned
meanings. The problem is to work out how to com-
bine the meanings contributed by each of the modes
in order to determine what the user actually intends
to communicate.
To model this integration, we utilize a unification
operation over typed feature structures (Carpenter
1990, 1992, Pollard and Sag 1987, Calder 1987, King
&apos;See Wahlster 1991 for discussion of the role of dialog
in resolving ambiguous gestures.
</bodyText>
<figure confidence="0.7162095">
Natural
Langua
Agentge
Multimodal
Integration
Agent
Bridge
Agent
</figure>
<figureCaption confidence="0.997454">
Figure 6: Multimodal integration architecture
</figureCaption>
<bodyText confidence="0.938201375">
Figure 6 presents the main agents involved in the
QuickSet system. Spoken and gestural input orig-
inates in the user interface client agent and it is
passed on to the speech recognition and gesture
recognition agents respectively. The natural lan-
guage agent uses a parser implemented in Prolog to
parse strings that originate from the speech recog-
nition agent and assign typed feature structures to
</bodyText>
<page confidence="0.988886">
284
</page>
<bodyText confidence="0.9999214375">
them. The potential interpretations of gesture from
the gesture recognition agent are also represented as
typed feature structures. The multimodal integra-
tion agent determines and ranks potential unifica-
tions of spoken and gestural input and issues com-
plete commands to the bridge agent. The bridge
agent accepts commands in the form of typed fea-
ture structures and translates them into commands
for whichever applications the system is providing
an interface to.
For example, if the user utters `M1A1 PLA-
TOON&apos;, the name of a particular type of tank pla-
toon, the natural language agent assigns this phrase
the feature structure in Figure 7. The type of each
feature structure is indicated in italics at its bottom
right or left corner.
</bodyText>
<equation confidence="0.6198895">
[ object :
create_unit location:
</equation>
<figureCaption confidence="0.999328">
Figure 7: Feature structure for `M1A1 PLATOON&apos;
</figureCaption>
<bodyText confidence="0.999847928571429">
Since QuickSet is a task-based system directed to-
ward setting up a scenario for simulation, this phrase
is interpreted as a partially specified unit creation
command. Before it can be executed, it needs a lo-
cation feature indicating where to create the unit,
which is provided by the user&apos;s gesturing on the
screen. The user&apos;s ink is likely to be assigned a num-
ber of interpretations, for example, both a point in-
terpretation and a line interpretation, which the ges-
ture recognition agent assigns typed feature struc-
tures (see Figures 8 and 9). Interpretations of ges-
tures as location features are assigned a general com-
mand type which unifies with all of commands taken
by the system.
</bodyText>
<equation confidence="0.698651">
[ xcoord : 95305
xcoord : 94365 I
point
</equation>
<figureCaption confidence="0.981042">
Figure 8: Point interpretation of gesture
</figureCaption>
<bodyText confidence="0.99996369047619">
issue a typed feature structure representing the pre-
ferred interpretation to the bridge agent, which will
execute the command. This involves parsing of the
speech and gesture streams in order to determine po-
tential multimodal integrations. Two factors guide
this: tagging of speech and gesture as either com-
plete or partial and examination of time stamps as-
sociated with speech and gesture.
Speech or gesture input is marked as complete if it
provides a full command specification and therefore
does not need to be integrated with another mode.
Speech or gesture marked as partial needs to be in-
tegrated with another mode in order to derive an
executable command.
Empirical study of the nature of multimodal inter-
action has shown that speech typically follows ges-
ture within a window of a three to four seconds while
gesture following speech is very uncommon (Oviatt
et al 97). Therefore, in our multimodal architec-
ture, the integrator temporally licenses integration
of speech and gesture if their time intervals overlap,
or if the onset of the speech signal is within a brief
time window following the end of gesture. Speech
and gesture are integrated appropriately even if the
integrator agent receives them in a different order
from their actual order of occurrence. If speech is
temporally compatible with gesture, in this respect,
then the integrator takes the sets of interpretations
for both speech and gesture, and for each pairing
in the product set attempts to unify the two fea-
ture structures. The probability of each multimodal
interpretation in the resulting set licensed by unifi-
cation is determined by multiplying the probabilities
assigned to the speech and gesture interpretations.
In the example case above, both speech and
gesture have only partial interpretations, one for
speech, and two for gesture. Since the speech in-
terpretation (Figure 7) requires its location feature
to be of type point, only unification with the point
interpretation of the gesture will succeed and be
passed on as a valid multimodal interpretation (Fig-
ure 10).
</bodyText>
<figure confidence="0.84176225">
type : mlal
echelon : platoon
I point
unit
command[ location:
[ location :
command
coordlist :
[(95301, 94360),
(95305, 94365),
(95310, 94380)]
object :
location:
create_unit
[ type : mlal
echelon : platoon
unit
[xcoord : 95305
xcoord : 94365 I
point
</figure>
<figureCaption confidence="0.999288">
Figure 9: Line interpretation of gesture
</figureCaption>
<bodyText confidence="0.951994">
The task of the integrator agent is to field incom-
ing typed feature structures representing interpreta-
tions of speech and of gesture, identify the best po-
tential interpretation, multimodal or unimodal, and
</bodyText>
<figureCaption confidence="0.576635">
Figure 10: Multimodal interpretation
</figureCaption>
<bodyText confidence="0.9993185">
The ambiguity of interpretation of the gesture was
resolved by integration with speech which in this
case required a location feature of type point. If
the spoken command had instead been &apos;BARBED
</bodyText>
<page confidence="0.995531">
285
</page>
<bodyText confidence="0.99981225">
WIRE&apos; it would have been assigned the feature
structure in Figure 11. This structure would only
unify with the line interpretation of gesture result-
ing in the interpretation in Figure 12.
</bodyText>
<equation confidence="0.677611333333333">
[ object :
location :
create Jine
</equation>
<figureCaption confidence="0.886300125">
Figure 11: Feature structure for &apos;BARBED WIRE&apos;
create _line object : style : barbed_wire Isne
location : color : red itne-obi
coordlist :
[(95301, 94360),
(95305, 94365),
(95310, 94380)]
Figure 12: Multimodal line creation
</figureCaption>
<bodyText confidence="0.987848760869566">
Similarly, if the spoken command described an
area, for example an &apos;ANTI TANK MINEFIELD&apos; ,
it would only unify with an interpretation of gesture
as an area designation. In each case the unification-
based integration strategy compensates for errors in
gesture recognition through type constraints on the
values of features.
Gesture also compensates for errors in speech
recognition. In the open microphone mode, where
the user does not have to gesture in order to speak,
spurious speech recognition errors are more common
than with click-to-speak, but are frequently rejected
by the system because of the absence of a compatible
gesture for integration. For example, if the system
spuriously recognizes `M1A1 PLATOON&apos;, but there
is no overlapping or immediately preceding gesture
to provide the location, the speech will be ignored.
The architecture also supports selection among n-
best speech recognition results on the basis of the
preferred gesture recognition. In the future, n-best
recognition results will be available from the recog-
nizer, and we will further examine the potential for
gesture to help select among speech recognition al-
ternatives.
Since speech may follow gesture, and since even si-
multaneously produced speech and gesture are pro-
cessed sequentially, the integrator cannot execute
what appears to be a complete unimodal command
on receiving it, in case it is immediately followed by
input from the other mode suggesting a multimodal
interpretation. If a given speech or gesture input
has a set of interpretations including both partial
and complete interpretations, the integrator agent
waits for an incoming signal from the other mode. If
no signal is forthcoming from the other mode within
the time window, or if interpretations from the other
mode do not integrate with any interpretations in
the set, then the best of the complete unimodal
interpretations from the original set is sent to the
bridge agent.
For example, the gesture in Figure 13 is used for
unimodal specification of the location of a fortified
line. If recognition is successful the gesture agent
would assign the gesture an interpretation like that
in Figure 14.
Aii&amp;quot;Ln_11,
</bodyText>
<figureCaption confidence="0.955075">
Figure 13: Fortified line gesture
</figureCaption>
<table confidence="0.906505833333333">
create _line object : style : fortified _line line Itne_obj
location : color : blue
coordlist :
[(93000,94360),
(93025,94365),
_ (93112, 94362)] _
</table>
<figureCaption confidence="0.974759">
Figure 14: Unimodal fortified line feature structure
</figureCaption>
<bodyText confidence="0.995823333333333">
However, it might also receive an additional po-
tential interpretation as a location feature of a more
general line type (Figure 15).
</bodyText>
<figureCaption confidence="0.847529">
Figure 15: Line feature structure
</figureCaption>
<bodyText confidence="0.999935454545455">
On receiving this set of interpretations, the in-
tegrator cannot immediately execute the complete
interpretation to create a fortified line, even if it is
assigned the highest probability by the recognizer,
since speech contradicting this may immediately fol-
low. For example, if overlapping with or just after
the gesture, the user said &apos;BARBED WIRE&apos; then
the line feature interpretation would be preferred. If
speech does not follow within the three to four sec-
ond window, or following speech does not integrate
with the gesture, then the unimodal interpretation
</bodyText>
<table confidence="0.577249166666667">
[ style : barbed_wire
color : red
coordlist :
[(93000, 94360),
location : (93025,94365),
command _ (93112, 94362)] line -
</table>
<page confidence="0.996539">
286
</page>
<bodyText confidence="0.999883111111111">
is chosen. This approach embodies a preference for
multimodal interpretations over unimodal ones, mo-
tivated by the possibility of unintended complete
unimodal interpretations of gestures. After more
detailed empirical investigation, this will be refined
so that the possibility of integration weighs in favor
of the multimodal interpretation, but it can still be
beaten by a unimodal gestural interpretation with a
significantly higher probability.
</bodyText>
<sectionHeader confidence="0.996526" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99999722972973">
We have presented an architecture for multimodal
interfaces in which integration of speech and ges-
ture is mediated and constrained by a unification
operation over typed feature structures. Our ap-
proach supports a full spectrum of gestural input,
not just deixis. It also can be driven by either mode
and enables a wide and flexible range of interactions.
Complete commands can originate in a single mode
yielding unimodal spoken and gestural commands,
or in a combination of modes yielding multimodal
commands, in which speech and gesture are able to
contribute either the predicate or the arguments of
the command. This architecture allows the modes
to synergistically mutual compensate for each oth-
ers&apos; errors. We have informally observed that inte-
gration with speech does succeed in resolving am-
biguous gestures. In the majority of cases, gestures
will have multiple interpretations, but this is rarely
apparent to the user, because the erroneous inter-
pretations of gesture are screened out by the unifi-
cation process. We have also observed that in the
open microphone mode multimodality allows erro-
neous speech recognition results to be screened out.
For the application tasks described here, we have
observed a reduction in the length and complexity
of spoken input, compared to the unimodal spoken
interface to LeatherNet, informally reconfirming the
empirical results of Oviatt et al 1997. For this fam-
ily of applications at least, it appears to be the case
that as part of a multimodal architecture, current
speech recognition technology is sufficiently robust
to support easy-to-use interfaces.
Vo and Wood 1996 present an approach to mul-
timodal integration similar in spirit to that pre-
sented here in that it accepts a variety of gestures
and is not solely speech-driven. However, we be-
lieve that unification of typed feature structures
provides a more general, formally well-understood,
and reusable mechanism for multimodal integration
than the frame merging strategy that they describe.
Cheyer and Julia (1995) sketch a system based on
Oviatt&apos;s (1996) results but describe neither the in-
tegration strategy nor multimodal compensation.
QuickSet has undergone a form of pro-active eval-
uation in that its design is informed by detailed pre-
dictive modeling of how users interact multimodally
and it incorporates the results of existing empirical
studies of multimodal interaction (Oviatt 1996, Ovi-
att et al 1997). It has also undergone participatory
design and user testing with the US Marine Corps
at their training base at 29 Palms, California, with
the US Army at the Royal Dragon exercise at Fort
Bragg, North Carolina, and as part of the Command
Center of the Future at NRaD.
Our initial application of this architecture has
been to map-based tasks such as distributed simula-
tion. It supports a fully-implemented usable system
in which hundreds of different kinds of entities can
be created and manipulated. We believe that the
unification-based method described here will read-
ily scale to larger tasks and is sufficiently general
to support a wide variety of other application areas,
including graphically-based information systems and
editing of textual and graphical content. The archi-
tecture has already been successfully re-deployed in
the construction of multimodal interface to health
care information.
We are actively pursuing incorporation of
statistically-derived heuristics and a more sophisti-
cated dialogue model into the integration architec-
ture. We are also developing a capability for auto-
matic logging of spoken and gestural input in order
to collect more fine-grained empirical data on the
nature of multimodal interaction.
</bodyText>
<sectionHeader confidence="0.999369" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999628375">
This work is supported in part by the Informa-
tion Technology and Information Systems offices of
DARPA under contract number DABT63-95-C-007,
in part by ONR grant number N00014-95-1-1164,
and has been done in collaboration with the US
Navy&apos;s NCCOSC RDT&amp;E Division (NRaD), Ascent
Technologies, Mitre Corp., MRJ Corp., and SRI In-
ternational.
</bodyText>
<sectionHeader confidence="0.987212" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.856310222222222">
Bolt, R. A., 1980. &amp;quot;Put-That-There&amp;quot; :Voice and ges-
ture at the graphics interface. Computer Graph-
ics, 14.3:262-270.
Brison, E., and N. Vigouroux. (unpublished ms.).
Multimodal references: A generic fusion pro-
cess. URIT-URA CNRS. Universit Paul Sabatier,
Toulouse, France.
Calder, J. 1987. Typed unification for natural lan-
guage processing. In E. Klein and J. van Benthem,
</bodyText>
<page confidence="0.993443">
287
</page>
<reference confidence="0.963044076190477">
editors, Categories, Polymorphisms, and Unifica-
tion, pages 65-72. Centre for Cognitive Science,
University of Edinburgh, Edinburgh.
Carpenter, R. 1990. Typed feature structures: In-
heritance, (In)equality, and Extensionality. In
W. Daelemans and G. Gazdar, editors, Proceed-
ings of the ITK Workshop: Inheritance in Natural
Language Processing, pages 9-18, Tilburg. Insti-
tute for Language Technology and Artificial Intel-
ligence, Tilburg University, Tilburg.
Carpenter, R. 1992. The logic of typed feature struc-
tures. Cambridge University Press, Cambridge,
England.
Cheyer, A., and L. Julia. 1995. Multimodal maps:
An agent-based approach. In International Con-
ference on Cooperative Multimodal Communica-
tion (CMC/95), pages 24-26, May 1995. Eind-
hoven, The Netherlands.
Clarkson, J. D., and J. Yi. 1996. LeatherNet: A
synthetic forces tactical training system for the
USMC commander. In Proceedings of the Sixth
Conference on Computer Generated Forces and
Behavioral Representation, pages 275-281. Insti-
tute for simulation and training. Technical Report
IST-TR-96-18.
Cohen, P. R. 1991. Integrated interfaces for decision
support with simulation. In B. Nelson, W. D. Kel-
ton, and G. M. Clark, editors, Proceedings of the
Winter Simulation Conference, pages 1066-1072.
ACM, New York.
Cohen, P. R. 1992. The role of natural language in a
multimodal interface. In Proceedings of UIST&apos;92,
pages 143-149, ACM Press, New York.
Cohen, P. R., A. Cheyer, M. Wang, and S. C. Baeg.
1994. An open agent architecture. In Working
Notes of the AAAI Spring Symposium on Soft-
ware Agents (March 21-22, Stanford University,
Stanford, California), pages 1-8.
Courtemanche, A. J., and A. Ceranowicz. 1995.
ModSAF development status. In Proceedings
of the Fifth Conference on Computer Generated
Forces and Behavioral Representation, pages 3-13,
May 9-11, Orlando, Florida. University of Central
Florida, Florida.
King, P. 1989. A logical formalism for head-driven
phrase structure grammar. Ph.D. Thesis, Univer-
sity of Manchester, Manchester, England.
Koons, D. B., C. J. Sparrell, and K. R. Thorisson.
1993. Integrating simultaneous input from speech,
gaze, and hand gestures. In M. T. Maybury, edi-
tor, Intelligent Multimedia Interfaces, pages 257-
276. AAAI Press/ MIT Press, Cambridge, Mas-
sachusetts.
Moore, R. C., J. Dowding, H. Bratt, J. M. Gawron,
Y. Gorfu, and A. Cheyer 1997. CommandTalk:
A Spoken-Language Interface for Battlefield Sim-
ulations. In Proceedings of Fifth Conference on
Applied Natural Language Processing, pages 1-7,
Washington, D.C. Association for Computational
Linguistics, Morristown, New Jersey.
Moshier, D. 1988. Extensions to unification gram-
mar for the description of programming languages.
Ph.D. Thesis, University of Michigan, Ann Arbor,
Michigan.
Neal, J. G., and S. C. Shapiro. 1991. Intelligent
multi-media interface technology. In J. W. Sul-
livan and S. W. Tyler, editors, Intelligent User
Interfaces, pages 45-68. ACM Press, Frontier Se-
ries, Addison Wesley Publishing Co., New York,
New York.
Oviatt, S. L. 1996. Multimodal interfaces for dy-
namic interactive maps. In Proceedings of Con-
ference on Human Factors in Computing Systems:
CHI &apos;96, pages 95-102, Vancouver, Canada. ACM
Press, New York.
Oviatt, S. L., A. DeAngeli, and K. Kuhn. 1997. In-
tegration and synchronization of input modes dur-
ing multimodal human-computer interaction. In
Proceedings of the Conference on Human Factors
in Computing Systems: CHI &apos;97, pages 415-422,
Atlanta, Georgia. ACM Press, New York.
Oviatt, S. L., and R. van Gent. 1996. Error resolu-
tion during multimodal human-computer interac-
tion. In Proceedings of International Conference
on Spoken Language Processing, vol 1, pages 204-
207, Philadelphia, Pennsylvania.
Pollard, C. J., and I. A. Sag. 1987. Information-
based syntax and semantics: Volume I, Funda-
mentals., Volume 13 of CSLI Lecture Notes. Cen-
ter for the Study of Language and Information,
Stanford University, Stanford, California.
Vo, M. T., and C. Wood. 1996. Building an appli-
cation framework for speech and pen input inte-
gration in multimodal learning interfaces. In Pro-
ceedings of International Conference on Acoustics,
Speech, and Signal Processing, Atlanta, GA.
Wahlster, W. 1991. User and discourse models for
multimodal communication. In J. Sullivan and S.
Tyler, editors, Intelligent User Interfaces, ACM
Press, Addison Wesley Publishing Co., New York,
New York.
Wauchope, K. 1994. Eucalyptus: Integrating
natural language input with a graphical user
interface. Naval Research Laboratory, Report
NRL/FR/5510-94-9711.
</reference>
<page confidence="0.996587">
288
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.852130">
<title confidence="0.999304">Unification-based Multimodal Integration</title>
<author confidence="0.9996035">Michael Johnston</author>
<author confidence="0.9996035">Philip R Cohen</author>
<author confidence="0.9996035">David McGee</author>
<author confidence="0.9996035">Sharon L Oviatt</author>
<author confidence="0.9996035">James A Pittman</author>
<author confidence="0.9996035">Ira Smith</author>
<affiliation confidence="0.9994475">Center for Human Computer Communication Department of Computer Science and Engineering</affiliation>
<address confidence="0.87734">Oregon Graduate Institute, PO BOX 91000, Portland, OR 97291, USA.</address>
<email confidence="0.999879">johnstonOcse.ogi.edu</email>
<email confidence="0.999879">pcohenOcse.ogi.edu</email>
<email confidence="0.999879">dmcgeeOcse.ogi.edu</email>
<email confidence="0.999879">oviattOcse.ogi.edu</email>
<email confidence="0.999879">jayOcse.ogi.edu</email>
<email confidence="0.999879">iraOcse.ogi.edu</email>
<abstract confidence="0.998537611111111">Recent empirical research has shown conclusive advantages of multimodal interaction over speech-only interaction for mapbased tasks. This paper describes a multimodal language processing architecture which supports interfaces allowing simultaneous input from speech and gesture recognition. Integration of spoken and gestural input is driven by unification of typed feature structures representing the semantic contributions of the different modes. This integration method allows the component modalities to mutually compensate for each others&apos; errors. It is implemented in Quick- Set, a multimodal (pen/voice) system that enables users to set up and control distributed interactive simulations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<pages>65--72</pages>
<editor>editors, Categories, Polymorphisms, and Unification,</editor>
<institution>Centre for Cognitive Science, University of Edinburgh,</institution>
<location>Edinburgh.</location>
<marker></marker>
<rawString>editors, Categories, Polymorphisms, and Unification, pages 65-72. Centre for Cognitive Science, University of Edinburgh, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Carpenter</author>
</authors>
<title>Typed feature structures: Inheritance, (In)equality, and Extensionality.</title>
<date>1990</date>
<booktitle>Proceedings of the ITK Workshop: Inheritance in Natural Language Processing,</booktitle>
<pages>9--18</pages>
<editor>In W. Daelemans and G. Gazdar, editors,</editor>
<location>Tilburg.</location>
<contexts>
<context position="12713" citStr="Carpenter 1990" startWordPosition="2014" endWordPosition="2015">y determined as a result of multimodal integration, as illustrated below5. 3 A Unification-based Architecture for Multimodal Integration One the most significant challenges facing the development of effective multimodal interfaces concerns the integration of input from different modes. Input signals from each of the modes can be assigned meanings. The problem is to work out how to combine the meanings contributed by each of the modes in order to determine what the user actually intends to communicate. To model this integration, we utilize a unification operation over typed feature structures (Carpenter 1990, 1992, Pollard and Sag 1987, Calder 1987, King &apos;See Wahlster 1991 for discussion of the role of dialog in resolving ambiguous gestures. Natural Langua Agentge Multimodal Integration Agent Bridge Agent Figure 6: Multimodal integration architecture Figure 6 presents the main agents involved in the QuickSet system. Spoken and gestural input originates in the user interface client agent and it is passed on to the speech recognition and gesture recognition agents respectively. The natural language agent uses a parser implemented in Prolog to parse strings that originate from the speech recognition</context>
</contexts>
<marker>Carpenter, 1990</marker>
<rawString>Carpenter, R. 1990. Typed feature structures: Inheritance, (In)equality, and Extensionality. In W. Daelemans and G. Gazdar, editors, Proceedings of the ITK Workshop: Inheritance in Natural Language Processing, pages 9-18, Tilburg. Institute for Language Technology and Artificial Intelligence, Tilburg University, Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Carpenter</author>
</authors>
<title>The logic of typed feature structures.</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="6321" citStr="Carpenter 1992" startWordPosition="979" endWordPosition="980">m hand gestures and eye gaze in order to aid in determining the reference of noun phrases in the speech stream. The second allows users to manipulate objects in a blocks world using iconic and pantomimic gestures in addition to deictic gestures. 2More precisely, they are &apos;verbal language&apos;-driven. Either spoken or typed linguistic expressions are the driving force of interpretation. We present an approach to multimodal integration which overcomes these limiting factors. A wide base of continuous gestural input is supported and integration may be driven by either mode. Typed feature structures (Carpenter 1992) are used to provide a clearly defined and well understood common meaning representation for the modes, and multimodal integration is accomplished through unification. 2 Quickset: A Multimodal Interface for Distributed Interactive Simulation The initial application of our multimodal interface architecture has been in the development of the QuickSet system, an interface for setting up and interacting with distributed interactive simulations. QuickSet provides a portal into LeatherNee , a simulation system used for the training of US Marine Corps platoon leaders. LeatherNet simulates training ex</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Carpenter, R. 1992. The logic of typed feature structures. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cheyer</author>
<author>L Julia</author>
</authors>
<title>Multimodal maps: An agent-based approach.</title>
<date>1995</date>
<booktitle>In International Conference on Cooperative Multimodal Communication (CMC/95),</booktitle>
<pages>24--26</pages>
<location>Eindhoven, The Netherlands.</location>
<contexts>
<context position="23926" citStr="Cheyer and Julia (1995)" startWordPosition="3796" endWordPosition="3799"> this family of applications at least, it appears to be the case that as part of a multimodal architecture, current speech recognition technology is sufficiently robust to support easy-to-use interfaces. Vo and Wood 1996 present an approach to multimodal integration similar in spirit to that presented here in that it accepts a variety of gestures and is not solely speech-driven. However, we believe that unification of typed feature structures provides a more general, formally well-understood, and reusable mechanism for multimodal integration than the frame merging strategy that they describe. Cheyer and Julia (1995) sketch a system based on Oviatt&apos;s (1996) results but describe neither the integration strategy nor multimodal compensation. QuickSet has undergone a form of pro-active evaluation in that its design is informed by detailed predictive modeling of how users interact multimodally and it incorporates the results of existing empirical studies of multimodal interaction (Oviatt 1996, Oviatt et al 1997). It has also undergone participatory design and user testing with the US Marine Corps at their training base at 29 Palms, California, with the US Army at the Royal Dragon exercise at Fort Bragg, North </context>
</contexts>
<marker>Cheyer, Julia, 1995</marker>
<rawString>Cheyer, A., and L. Julia. 1995. Multimodal maps: An agent-based approach. In International Conference on Cooperative Multimodal Communication (CMC/95), pages 24-26, May 1995. Eindhoven, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Clarkson</author>
<author>J Yi</author>
</authors>
<title>LeatherNet: A synthetic forces tactical training system for the USMC commander.</title>
<date>1996</date>
<booktitle>In Proceedings of the Sixth Conference on Computer Generated Forces and Behavioral Representation,</booktitle>
<tech>Technical Report IST-TR-96-18.</tech>
<pages>275--281</pages>
<contexts>
<context position="7086" citStr="Clarkson and Yi 1996" startWordPosition="1091" endWordPosition="1094">ed through unification. 2 Quickset: A Multimodal Interface for Distributed Interactive Simulation The initial application of our multimodal interface architecture has been in the development of the QuickSet system, an interface for setting up and interacting with distributed interactive simulations. QuickSet provides a portal into LeatherNee , a simulation system used for the training of US Marine Corps platoon leaders. LeatherNet simulates training exercises using the ModSAF simulator (Courtemanche and Ceranowicz 1995) and supports 3D visualization of the simulated exercises using CommandVu (Clarkson and Yi 1996). SRI International&apos;s CommandTalk provides a unimodal spoken interface to LeatherNet (Moore et al 1997). QuickSet is a distributed system consisting of a collection of agents that communicate through the Open Agent Architecture&apos; (Cohen et al 1994). It runs on both desktop and hand-held PCs under Windows 95, communicating over wired and wireless LANs (respectively), or modem links. The wireless hand-held unit is a 3-lb Fujitsu Stylistic 1000 (Figure 2). We have also developed a Java-based QuickSet agent that provides a portal to the simulation over the World Wide Web. The QuickSet user interfac</context>
</contexts>
<marker>Clarkson, Yi, 1996</marker>
<rawString>Clarkson, J. D., and J. Yi. 1996. LeatherNet: A synthetic forces tactical training system for the USMC commander. In Proceedings of the Sixth Conference on Computer Generated Forces and Behavioral Representation, pages 275-281. Institute for simulation and training. Technical Report IST-TR-96-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
</authors>
<title>Integrated interfaces for decision support with simulation.</title>
<date>1991</date>
<booktitle>Proceedings of the Winter Simulation Conference,</booktitle>
<pages>1066--1072</pages>
<editor>In B. Nelson, W. D. Kelton, and G. M. Clark, editors,</editor>
<publisher>ACM,</publisher>
<location>New York.</location>
<contexts>
<context position="4868" citStr="Cohen 1991" startWordPosition="754" endWordPosition="755">ems capable of integration of speech and gesture have existed since the early 80&apos;s. One of the first such systems was the &amp;quot;Put-That-There&amp;quot; system (Bolt 1980). However, in the sixteen years since then, research on multimodal integration has not yielded a reusable scalable architecture for the construction of multimodal systems that integrate gesture and voice. There are four major limiting factors in previous approaches to multimodal integration: (i) The majority of approaches limit the bandwidth of the gestural mode to simple deictic pointing gestures made with a mouse (Neal and Shapiro 1991, Cohen 1991, Cohen 1992, Brison and Vigouroux (ms.), Wauchope 1994) or with the hand (Koons et al 19931). (ii) Most previous approaches have been primarily speech-driven2 , treating gesture as a secondary dependent mode (Neal and Shapiro 1991, Cohen 1991, Cohen 1992, Brison and Vigouroux (ms.), Koons et al 1993, Wauchope 1994). In these systems, integration of gesture is triggered by the appearance of expressions in the speech stream whose reference needs to be resolved, such as definite and deictic noun phrases (e.g. &apos;this one&apos;, &apos;the red cube&apos;). (iii) None of the existing approaches provide a wellunders</context>
</contexts>
<marker>Cohen, 1991</marker>
<rawString>Cohen, P. R. 1991. Integrated interfaces for decision support with simulation. In B. Nelson, W. D. Kelton, and G. M. Clark, editors, Proceedings of the Winter Simulation Conference, pages 1066-1072. ACM, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
</authors>
<title>The role of natural language in a multimodal interface.</title>
<date>1992</date>
<booktitle>In Proceedings of UIST&apos;92,</booktitle>
<pages>143--149</pages>
<publisher>ACM Press,</publisher>
<location>New York.</location>
<contexts>
<context position="4880" citStr="Cohen 1992" startWordPosition="756" endWordPosition="757">of integration of speech and gesture have existed since the early 80&apos;s. One of the first such systems was the &amp;quot;Put-That-There&amp;quot; system (Bolt 1980). However, in the sixteen years since then, research on multimodal integration has not yielded a reusable scalable architecture for the construction of multimodal systems that integrate gesture and voice. There are four major limiting factors in previous approaches to multimodal integration: (i) The majority of approaches limit the bandwidth of the gestural mode to simple deictic pointing gestures made with a mouse (Neal and Shapiro 1991, Cohen 1991, Cohen 1992, Brison and Vigouroux (ms.), Wauchope 1994) or with the hand (Koons et al 19931). (ii) Most previous approaches have been primarily speech-driven2 , treating gesture as a secondary dependent mode (Neal and Shapiro 1991, Cohen 1991, Cohen 1992, Brison and Vigouroux (ms.), Koons et al 1993, Wauchope 1994). In these systems, integration of gesture is triggered by the appearance of expressions in the speech stream whose reference needs to be resolved, such as definite and deictic noun phrases (e.g. &apos;this one&apos;, &apos;the red cube&apos;). (iii) None of the existing approaches provide a wellunderstood general</context>
</contexts>
<marker>Cohen, 1992</marker>
<rawString>Cohen, P. R. 1992. The role of natural language in a multimodal interface. In Proceedings of UIST&apos;92, pages 143-149, ACM Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>A Cheyer</author>
<author>M Wang</author>
<author>S C Baeg</author>
</authors>
<title>An open agent architecture.</title>
<date>1994</date>
<booktitle>In Working Notes of the AAAI Spring Symposium on Software Agents</booktitle>
<pages>1--8</pages>
<institution>Stanford University,</institution>
<location>Stanford, California),</location>
<contexts>
<context position="7333" citStr="Cohen et al 1994" startWordPosition="1128" endWordPosition="1131">racting with distributed interactive simulations. QuickSet provides a portal into LeatherNee , a simulation system used for the training of US Marine Corps platoon leaders. LeatherNet simulates training exercises using the ModSAF simulator (Courtemanche and Ceranowicz 1995) and supports 3D visualization of the simulated exercises using CommandVu (Clarkson and Yi 1996). SRI International&apos;s CommandTalk provides a unimodal spoken interface to LeatherNet (Moore et al 1997). QuickSet is a distributed system consisting of a collection of agents that communicate through the Open Agent Architecture&apos; (Cohen et al 1994). It runs on both desktop and hand-held PCs under Windows 95, communicating over wired and wireless LANs (respectively), or modem links. The wireless hand-held unit is a 3-lb Fujitsu Stylistic 1000 (Figure 2). We have also developed a Java-based QuickSet agent that provides a portal to the simulation over the World Wide Web. The QuickSet user interface displays a map of the terrain on which the simulated military exercise is to take place (Figure 2). The user can gesture and draw directly on the map with the pen and simultaneously issue spoken commands. Units and objectives can be laid down on</context>
</contexts>
<marker>Cohen, Cheyer, Wang, Baeg, 1994</marker>
<rawString>Cohen, P. R., A. Cheyer, M. Wang, and S. C. Baeg. 1994. An open agent architecture. In Working Notes of the AAAI Spring Symposium on Software Agents (March 21-22, Stanford University, Stanford, California), pages 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Courtemanche</author>
<author>A Ceranowicz</author>
</authors>
<title>ModSAF development status.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fifth Conference on Computer Generated Forces and Behavioral Representation,</booktitle>
<pages>3--13</pages>
<institution>Orlando, Florida. University of Central Florida, Florida.</institution>
<contexts>
<context position="6990" citStr="Courtemanche and Ceranowicz 1995" startWordPosition="1074" endWordPosition="1078">ed and well understood common meaning representation for the modes, and multimodal integration is accomplished through unification. 2 Quickset: A Multimodal Interface for Distributed Interactive Simulation The initial application of our multimodal interface architecture has been in the development of the QuickSet system, an interface for setting up and interacting with distributed interactive simulations. QuickSet provides a portal into LeatherNee , a simulation system used for the training of US Marine Corps platoon leaders. LeatherNet simulates training exercises using the ModSAF simulator (Courtemanche and Ceranowicz 1995) and supports 3D visualization of the simulated exercises using CommandVu (Clarkson and Yi 1996). SRI International&apos;s CommandTalk provides a unimodal spoken interface to LeatherNet (Moore et al 1997). QuickSet is a distributed system consisting of a collection of agents that communicate through the Open Agent Architecture&apos; (Cohen et al 1994). It runs on both desktop and hand-held PCs under Windows 95, communicating over wired and wireless LANs (respectively), or modem links. The wireless hand-held unit is a 3-lb Fujitsu Stylistic 1000 (Figure 2). We have also developed a Java-based QuickSet ag</context>
</contexts>
<marker>Courtemanche, Ceranowicz, 1995</marker>
<rawString>Courtemanche, A. J., and A. Ceranowicz. 1995. ModSAF development status. In Proceedings of the Fifth Conference on Computer Generated Forces and Behavioral Representation, pages 3-13, May 9-11, Orlando, Florida. University of Central Florida, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P King</author>
</authors>
<title>A logical formalism for head-driven phrase structure grammar.</title>
<date>1989</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Manchester,</institution>
<location>Manchester, England.</location>
<marker>King, 1989</marker>
<rawString>King, P. 1989. A logical formalism for head-driven phrase structure grammar. Ph.D. Thesis, University of Manchester, Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B Koons</author>
<author>C J Sparrell</author>
<author>K R Thorisson</author>
</authors>
<title>Integrating simultaneous input from speech, gaze, and hand gestures.</title>
<date>1993</date>
<booktitle>Intelligent Multimedia Interfaces,</booktitle>
<pages>257--276</pages>
<editor>In M. T. Maybury, editor,</editor>
<publisher>AAAI Press/ MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="4959" citStr="Koons et al 1993" startWordPosition="768" endWordPosition="771">ne of the first such systems was the &amp;quot;Put-That-There&amp;quot; system (Bolt 1980). However, in the sixteen years since then, research on multimodal integration has not yielded a reusable scalable architecture for the construction of multimodal systems that integrate gesture and voice. There are four major limiting factors in previous approaches to multimodal integration: (i) The majority of approaches limit the bandwidth of the gestural mode to simple deictic pointing gestures made with a mouse (Neal and Shapiro 1991, Cohen 1991, Cohen 1992, Brison and Vigouroux (ms.), Wauchope 1994) or with the hand (Koons et al 19931). (ii) Most previous approaches have been primarily speech-driven2 , treating gesture as a secondary dependent mode (Neal and Shapiro 1991, Cohen 1991, Cohen 1992, Brison and Vigouroux (ms.), Koons et al 1993, Wauchope 1994). In these systems, integration of gesture is triggered by the appearance of expressions in the speech stream whose reference needs to be resolved, such as definite and deictic noun phrases (e.g. &apos;this one&apos;, &apos;the red cube&apos;). (iii) None of the existing approaches provide a wellunderstood generally applicable common meaning representation for the different modes, or, (iv) A</context>
</contexts>
<marker>Koons, Sparrell, Thorisson, 1993</marker>
<rawString>Koons, D. B., C. J. Sparrell, and K. R. Thorisson. 1993. Integrating simultaneous input from speech, gaze, and hand gestures. In M. T. Maybury, editor, Intelligent Multimedia Interfaces, pages 257-276. AAAI Press/ MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>J Dowding</author>
<author>H Bratt</author>
<author>J M Gawron</author>
<author>Y Gorfu</author>
<author>A Cheyer</author>
</authors>
<title>CommandTalk: A Spoken-Language Interface for Battlefield Simulations.</title>
<date>1997</date>
<booktitle>In Proceedings of Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Washington, D.C.</location>
<contexts>
<context position="7189" citStr="Moore et al 1997" startWordPosition="1106" endWordPosition="1109">al application of our multimodal interface architecture has been in the development of the QuickSet system, an interface for setting up and interacting with distributed interactive simulations. QuickSet provides a portal into LeatherNee , a simulation system used for the training of US Marine Corps platoon leaders. LeatherNet simulates training exercises using the ModSAF simulator (Courtemanche and Ceranowicz 1995) and supports 3D visualization of the simulated exercises using CommandVu (Clarkson and Yi 1996). SRI International&apos;s CommandTalk provides a unimodal spoken interface to LeatherNet (Moore et al 1997). QuickSet is a distributed system consisting of a collection of agents that communicate through the Open Agent Architecture&apos; (Cohen et al 1994). It runs on both desktop and hand-held PCs under Windows 95, communicating over wired and wireless LANs (respectively), or modem links. The wireless hand-held unit is a 3-lb Fujitsu Stylistic 1000 (Figure 2). We have also developed a Java-based QuickSet agent that provides a portal to the simulation over the World Wide Web. The QuickSet user interface displays a map of the terrain on which the simulated military exercise is to take place (Figure 2). T</context>
</contexts>
<marker>Moore, Dowding, Bratt, Gawron, Gorfu, Cheyer, 1997</marker>
<rawString>Moore, R. C., J. Dowding, H. Bratt, J. M. Gawron, Y. Gorfu, and A. Cheyer 1997. CommandTalk: A Spoken-Language Interface for Battlefield Simulations. In Proceedings of Fifth Conference on Applied Natural Language Processing, pages 1-7, Washington, D.C. Association for Computational Linguistics, Morristown, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moshier</author>
</authors>
<title>Extensions to unification grammar for the description of programming languages.</title>
<date>1988</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Michigan,</institution>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="10691" citStr="Moshier 1988" startWordPosition="1692" endWordPosition="1693">and gestures As with all recognition technologies, gesture recognition may result in errors. One of the factors --..----1---&amp;quot;1-- â€¢ line â€¢ It 0 are a point FEDI tank me chanized platoon company mortar deletion fortfied line barbed vire 283 contributing to this is that routes and areas do not have signature shapes that can be used to identify them and are frequently confused (Figure 4). Figure 4: Pen drawings of routes and areas Another contributing factor is that users&apos; pen input is often sloppy (Figure 5) and map symbols can be confused among themselves and with route and area gestures. 1989, Moshier 1988). Unification is an operation that determines the consistency of two pieces of partial information, and if they are consistent combines them into a single result. As such, it is ideally suited to the task at hand, in which we want to determine whether a given piece of gestural input is compatible with a given piece of spoken input, and if they are compatible, to combine the two inputs into a single result that can be interpreted by the system. The use of feature structures as a semantic representation framework facilitates the specification of partial meanings. Spoken or gestural input which p</context>
</contexts>
<marker>Moshier, 1988</marker>
<rawString>Moshier, D. 1988. Extensions to unification grammar for the description of programming languages. Ph.D. Thesis, University of Michigan, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Neal</author>
<author>S C Shapiro</author>
</authors>
<title>Intelligent multi-media interface technology.</title>
<date>1991</date>
<booktitle>Intelligent User Interfaces,</booktitle>
<pages>45--68</pages>
<editor>In J. W. Sullivan and S. W. Tyler, editors,</editor>
<publisher>ACM Press, Frontier Series, Addison Wesley Publishing Co.,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="4856" citStr="Neal and Shapiro 1991" startWordPosition="750" endWordPosition="753">on and vice versa. Systems capable of integration of speech and gesture have existed since the early 80&apos;s. One of the first such systems was the &amp;quot;Put-That-There&amp;quot; system (Bolt 1980). However, in the sixteen years since then, research on multimodal integration has not yielded a reusable scalable architecture for the construction of multimodal systems that integrate gesture and voice. There are four major limiting factors in previous approaches to multimodal integration: (i) The majority of approaches limit the bandwidth of the gestural mode to simple deictic pointing gestures made with a mouse (Neal and Shapiro 1991, Cohen 1991, Cohen 1992, Brison and Vigouroux (ms.), Wauchope 1994) or with the hand (Koons et al 19931). (ii) Most previous approaches have been primarily speech-driven2 , treating gesture as a secondary dependent mode (Neal and Shapiro 1991, Cohen 1991, Cohen 1992, Brison and Vigouroux (ms.), Koons et al 1993, Wauchope 1994). In these systems, integration of gesture is triggered by the appearance of expressions in the speech stream whose reference needs to be resolved, such as definite and deictic noun phrases (e.g. &apos;this one&apos;, &apos;the red cube&apos;). (iii) None of the existing approaches provide </context>
</contexts>
<marker>Neal, Shapiro, 1991</marker>
<rawString>Neal, J. G., and S. C. Shapiro. 1991. Intelligent multi-media interface technology. In J. W. Sullivan and S. W. Tyler, editors, Intelligent User Interfaces, pages 45-68. ACM Press, Frontier Series, Addison Wesley Publishing Co., New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
</authors>
<title>Multimodal interfaces for dynamic interactive maps.</title>
<date>1996</date>
<booktitle>In Proceedings of Conference on Human Factors in Computing Systems: CHI &apos;96,</booktitle>
<pages>95--102</pages>
<publisher>ACM Press,</publisher>
<location>Vancouver, Canada.</location>
<contexts>
<context position="1873" citStr="Oviatt 1996" startWordPosition="270" endWordPosition="271"> humans and machines. In this work, we are concerned with the addition of multimodal input to the interface. In particular, we focus on interfaces which support simultaneous input from speech and pen, utilizing speech recognition and recognition of gestures and drawings made with a pen on a complex visual display, such as a map. Our focus on multimodal interfaces is motivated, in part, by the trend toward portable computing devices for which complex graphical user interfaces are infeasible. For such devices, speech and gesture will be the primary means of user input. Recent empirical results (Oviatt 1996) demonstrate clear task performance and user preference advantages for multimodal interfaces over speech only interfaces, in particular for spatial tasks such as those involving maps. Specifically, in a within-subject experiment during which the same users performed the same tasks in various conditions using only speech, only pen, or both speech and pen-based input, users&apos; multimodal input to maps resulted in 10% faster task completion time, 23% fewer words, 35% fewer spoken disfluencies, and 36% fewer task errors compared to unimodal spoken input. Of the user errors, 48% involved location err</context>
<context position="24304" citStr="Oviatt 1996" startWordPosition="3855" endWordPosition="3856"> we believe that unification of typed feature structures provides a more general, formally well-understood, and reusable mechanism for multimodal integration than the frame merging strategy that they describe. Cheyer and Julia (1995) sketch a system based on Oviatt&apos;s (1996) results but describe neither the integration strategy nor multimodal compensation. QuickSet has undergone a form of pro-active evaluation in that its design is informed by detailed predictive modeling of how users interact multimodally and it incorporates the results of existing empirical studies of multimodal interaction (Oviatt 1996, Oviatt et al 1997). It has also undergone participatory design and user testing with the US Marine Corps at their training base at 29 Palms, California, with the US Army at the Royal Dragon exercise at Fort Bragg, North Carolina, and as part of the Command Center of the Future at NRaD. Our initial application of this architecture has been to map-based tasks such as distributed simulation. It supports a fully-implemented usable system in which hundreds of different kinds of entities can be created and manipulated. We believe that the unification-based method described here will readily scale </context>
</contexts>
<marker>Oviatt, 1996</marker>
<rawString>Oviatt, S. L. 1996. Multimodal interfaces for dynamic interactive maps. In Proceedings of Conference on Human Factors in Computing Systems: CHI &apos;96, pages 95-102, Vancouver, Canada. ACM Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
<author>A DeAngeli</author>
<author>K Kuhn</author>
</authors>
<title>Integration and synchronization of input modes during multimodal human-computer interaction.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Human Factors in Computing Systems: CHI &apos;97,</booktitle>
<pages>415--422</pages>
<publisher>ACM Press,</publisher>
<location>Atlanta,</location>
<contexts>
<context position="23298" citStr="Oviatt et al 1997" startWordPosition="3698" endWordPosition="3701">ceed in resolving ambiguous gestures. In the majority of cases, gestures will have multiple interpretations, but this is rarely apparent to the user, because the erroneous interpretations of gesture are screened out by the unification process. We have also observed that in the open microphone mode multimodality allows erroneous speech recognition results to be screened out. For the application tasks described here, we have observed a reduction in the length and complexity of spoken input, compared to the unimodal spoken interface to LeatherNet, informally reconfirming the empirical results of Oviatt et al 1997. For this family of applications at least, it appears to be the case that as part of a multimodal architecture, current speech recognition technology is sufficiently robust to support easy-to-use interfaces. Vo and Wood 1996 present an approach to multimodal integration similar in spirit to that presented here in that it accepts a variety of gestures and is not solely speech-driven. However, we believe that unification of typed feature structures provides a more general, formally well-understood, and reusable mechanism for multimodal integration than the frame merging strategy that they descr</context>
</contexts>
<marker>Oviatt, DeAngeli, Kuhn, 1997</marker>
<rawString>Oviatt, S. L., A. DeAngeli, and K. Kuhn. 1997. Integration and synchronization of input modes during multimodal human-computer interaction. In Proceedings of the Conference on Human Factors in Computing Systems: CHI &apos;97, pages 415-422, Atlanta, Georgia. ACM Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
<author>R van Gent</author>
</authors>
<title>Error resolution during multimodal human-computer interaction.</title>
<date>1996</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<volume>1</volume>
<pages>204--207</pages>
<location>Philadelphia, Pennsylvania.</location>
<marker>Oviatt, van Gent, 1996</marker>
<rawString>Oviatt, S. L., and R. van Gent. 1996. Error resolution during multimodal human-computer interaction. In Proceedings of International Conference on Spoken Language Processing, vol 1, pages 204-207, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Pollard</author>
<author>I A Sag</author>
</authors>
<title>Informationbased syntax and semantics: Volume I, Fundamentals., Volume 13 of CSLI Lecture Notes. Center for the Study of Language and Information,</title>
<date>1987</date>
<location>Stanford University, Stanford, California.</location>
<contexts>
<context position="12741" citStr="Pollard and Sag 1987" startWordPosition="2017" endWordPosition="2020">lt of multimodal integration, as illustrated below5. 3 A Unification-based Architecture for Multimodal Integration One the most significant challenges facing the development of effective multimodal interfaces concerns the integration of input from different modes. Input signals from each of the modes can be assigned meanings. The problem is to work out how to combine the meanings contributed by each of the modes in order to determine what the user actually intends to communicate. To model this integration, we utilize a unification operation over typed feature structures (Carpenter 1990, 1992, Pollard and Sag 1987, Calder 1987, King &apos;See Wahlster 1991 for discussion of the role of dialog in resolving ambiguous gestures. Natural Langua Agentge Multimodal Integration Agent Bridge Agent Figure 6: Multimodal integration architecture Figure 6 presents the main agents involved in the QuickSet system. Spoken and gestural input originates in the user interface client agent and it is passed on to the speech recognition and gesture recognition agents respectively. The natural language agent uses a parser implemented in Prolog to parse strings that originate from the speech recognition agent and assign typed feat</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, C. J., and I. A. Sag. 1987. Informationbased syntax and semantics: Volume I, Fundamentals., Volume 13 of CSLI Lecture Notes. Center for the Study of Language and Information, Stanford University, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Vo</author>
<author>C Wood</author>
</authors>
<title>Building an application framework for speech and pen input integration in multimodal learning interfaces.</title>
<date>1996</date>
<booktitle>In Proceedings of International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="23523" citStr="Vo and Wood 1996" startWordPosition="3734" endWordPosition="3737">fication process. We have also observed that in the open microphone mode multimodality allows erroneous speech recognition results to be screened out. For the application tasks described here, we have observed a reduction in the length and complexity of spoken input, compared to the unimodal spoken interface to LeatherNet, informally reconfirming the empirical results of Oviatt et al 1997. For this family of applications at least, it appears to be the case that as part of a multimodal architecture, current speech recognition technology is sufficiently robust to support easy-to-use interfaces. Vo and Wood 1996 present an approach to multimodal integration similar in spirit to that presented here in that it accepts a variety of gestures and is not solely speech-driven. However, we believe that unification of typed feature structures provides a more general, formally well-understood, and reusable mechanism for multimodal integration than the frame merging strategy that they describe. Cheyer and Julia (1995) sketch a system based on Oviatt&apos;s (1996) results but describe neither the integration strategy nor multimodal compensation. QuickSet has undergone a form of pro-active evaluation in that its desig</context>
</contexts>
<marker>Vo, Wood, 1996</marker>
<rawString>Vo, M. T., and C. Wood. 1996. Building an application framework for speech and pen input integration in multimodal learning interfaces. In Proceedings of International Conference on Acoustics, Speech, and Signal Processing, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
</authors>
<title>User and discourse models for multimodal communication.</title>
<date>1991</date>
<booktitle>Intelligent User Interfaces,</booktitle>
<editor>In J. Sullivan and S. Tyler, editors,</editor>
<publisher>ACM Press, Addison Wesley Publishing Co.,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="12779" citStr="Wahlster 1991" startWordPosition="2025" endWordPosition="2026">below5. 3 A Unification-based Architecture for Multimodal Integration One the most significant challenges facing the development of effective multimodal interfaces concerns the integration of input from different modes. Input signals from each of the modes can be assigned meanings. The problem is to work out how to combine the meanings contributed by each of the modes in order to determine what the user actually intends to communicate. To model this integration, we utilize a unification operation over typed feature structures (Carpenter 1990, 1992, Pollard and Sag 1987, Calder 1987, King &apos;See Wahlster 1991 for discussion of the role of dialog in resolving ambiguous gestures. Natural Langua Agentge Multimodal Integration Agent Bridge Agent Figure 6: Multimodal integration architecture Figure 6 presents the main agents involved in the QuickSet system. Spoken and gestural input originates in the user interface client agent and it is passed on to the speech recognition and gesture recognition agents respectively. The natural language agent uses a parser implemented in Prolog to parse strings that originate from the speech recognition agent and assign typed feature structures to 284 them. The potent</context>
</contexts>
<marker>Wahlster, 1991</marker>
<rawString>Wahlster, W. 1991. User and discourse models for multimodal communication. In J. Sullivan and S. Tyler, editors, Intelligent User Interfaces, ACM Press, Addison Wesley Publishing Co., New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wauchope</author>
</authors>
<title>Eucalyptus: Integrating natural language input with a graphical user interface.</title>
<date>1994</date>
<journal>Naval Research Laboratory, Report</journal>
<pages>5510--94</pages>
<contexts>
<context position="4924" citStr="Wauchope 1994" startWordPosition="762" endWordPosition="763">e existed since the early 80&apos;s. One of the first such systems was the &amp;quot;Put-That-There&amp;quot; system (Bolt 1980). However, in the sixteen years since then, research on multimodal integration has not yielded a reusable scalable architecture for the construction of multimodal systems that integrate gesture and voice. There are four major limiting factors in previous approaches to multimodal integration: (i) The majority of approaches limit the bandwidth of the gestural mode to simple deictic pointing gestures made with a mouse (Neal and Shapiro 1991, Cohen 1991, Cohen 1992, Brison and Vigouroux (ms.), Wauchope 1994) or with the hand (Koons et al 19931). (ii) Most previous approaches have been primarily speech-driven2 , treating gesture as a secondary dependent mode (Neal and Shapiro 1991, Cohen 1991, Cohen 1992, Brison and Vigouroux (ms.), Koons et al 1993, Wauchope 1994). In these systems, integration of gesture is triggered by the appearance of expressions in the speech stream whose reference needs to be resolved, such as definite and deictic noun phrases (e.g. &apos;this one&apos;, &apos;the red cube&apos;). (iii) None of the existing approaches provide a wellunderstood generally applicable common meaning representation </context>
</contexts>
<marker>Wauchope, 1994</marker>
<rawString>Wauchope, K. 1994. Eucalyptus: Integrating natural language input with a graphical user interface. Naval Research Laboratory, Report NRL/FR/5510-94-9711.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>