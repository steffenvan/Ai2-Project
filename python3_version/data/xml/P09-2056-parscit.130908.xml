<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008438">
<title confidence="0.908745">
CATiB: The Columbia Arabic Treebank
</title>
<author confidence="0.991047">
Nizar Habash and Ryan M. Roth
</author>
<affiliation confidence="0.983185">
Center for Computational Learning Systems
Columbia University, New York, USA
</affiliation>
<email confidence="0.996647">
{habash,ryanr}@ccls.columbia.edu
</email>
<sectionHeader confidence="0.993772" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999394538461538">
The Columbia Arabic Treebank (CATiB)
is a database of syntactic analyses of Ara-
bic sentences. CATiB contrasts with pre-
vious approaches to Arabic treebanking
in its emphasis on speed with some con-
straints on linguistic richness. Two ba-
sic ideas inspire the CATiB approach: no
annotation of redundant information and
using representations and terminology in-
spired by traditional Arabic syntax. We
describe CATiB’s representation and an-
notation procedure, and report on inter-
annotator agreement and speed.
</bodyText>
<sectionHeader confidence="0.985922" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.997473923076923">
Treebanks are collections of manually-annotated
syntactic analyses of sentences. They are pri-
marily intended for building models for statis-
tical parsing; however, they are often enriched
for general natural language processing purposes.
For Arabic, two important treebanking efforts ex-
ist: the Penn Arabic Treebank (PATB) (Maamouri
et al., 2004) and the Prague Arabic Dependency
Treebank (PADT) (Smrž and Hajiˇc, 2006). In
addition to syntactic annotations, both resources
are annotated with rich morphological and seman-
tic information such as full part-of-speech (POS)
tags, lemmas, semantic roles, and diacritizations.
This allows these treebanks to be used for training
a variety of applications other than parsing, such
as tokenization, diacritization, POS tagging, mor-
phological disambiguation, base phrase chunking,
and semantic role labeling.
In this paper, we describe a new Arabic tree-
banking effort: the Columbia Arabic Treebank
(CATiB).1 CATiB is motivated by the following
three observations. First, as far as parsing Arabic
research, much of the non-syntactic rich annota-
tions are not used. For example, PATB has over
400 tags, but they are typically reduced to around
36 tags in training and testing parsers (Kulick et
</bodyText>
<footnote confidence="0.921148">
1This work was supported by Defense Advanced Re-
search Projects Agency Contract No. HR0011-08-C-0110.
</footnote>
<bodyText confidence="0.999108772727272">
al., 2006). The reduction addresses the fact that
sub-tags indicating case and other similar features
are essentially determined syntactically and are
hard to automatically tag accurately. Second, un-
der time restrictions, the creation of a treebank
faces a tradeoff between linguistic richness and
treebank size. The richer the annotations, the
slower the annotation process, the smaller the re-
sulting treebank. Obviously, bigger treebanks are
desirable for building better parsers. Third, both
PATB and PADT use complex syntactic represen-
tations that come from modern linguistic traditions
that differ from Arabic’s long history of syntac-
tic studies. The use of these representations puts
higher requirements on the kind of annotators to
hire and the length of their initial training.
CATiB contrasts with PATB and PADT in
putting an emphasis on annotation speed for the
specific task of parser training. Two basic ideas
inspire the CATiB approach. First, CATiB avoids
annotation of redundant linguistic information or
information not targeted in current parsing re-
search. For example, nominal case markers in
Arabic have been shown to be automatically de-
terminable from syntax and word morphology and
needn’t be manually annotated (Habash et al.,
2007a). Also, phrasal co-indexation, empty pro-
nouns, and full lemma disambiguation are not
currently used in parsing research so we do not
include them in CATiB. Second, CATiB uses a
simple intuitive dependency representation and
terminology inspired by Arabic’s long tradition
of syntactic studies. For example, CATiB rela-
tion labels include tamyiz (specification) and idafa
(possessive construction) in addition to universal
predicate-argument structure labels such as sub-
ject, object and modifier. These representation
choices make it easier to train annotators without
being restricted to hire people who have degrees
in linguistics.
This paper briefly describes CATiB’s repre-
sentation and annotation procedure, and reports
on produced data, achieved inter-annotator agree-
ment and annotation speeds.
</bodyText>
<page confidence="0.977235">
221
</page>
<note confidence="0.9639565">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 221–224,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<sectionHeader confidence="0.438049" genericHeader="method">
2 CATiB: Columbia Arabic Treebank
</sectionHeader>
<bodyText confidence="0.993107612244898">
CATiB uses the same basic tokenization scheme
used by PATB and PADT. However, the CATiB
POS tag set is much smaller than the PATB’s.
Whereas PATB uses over 400 tags specifying
every aspect of Arabic word morphology such
as definiteness, gender, number, person, mood,
voice and case, CATiB uses 6 POS tags: NOM
(non-proper nominals including nouns, pronouns,
adjectives and adverbs), PROP (proper nouns),
VRB (active-voice verbs), VRB-PASS (passive-
voice verbs), PRT (particles such as prepositions
or conjunctions) and PNX (punctuation).2
CATiB’s dependency links are labeled with one
of eight relation labels: SBJ (subject of verb
or topic of simple nominal sentence), OBJ (ob-
ject of verb, preposition, or deverbal noun), TPC
(topic in complex nominal sentences containing
an explicit pronominal referent), PRD (predicate
marking the complement of the extended cop-
ular constructions for kAn3 Aî~E@ñ �k@ð uA¿ and An
Aî�E@ñ �k@ð v@), IDF (relation between the posses-
sor [dependent] to the possessed [head] in the
idafa/possesive nominal construction), TMZ (re-
lation of the specifier [dependent] to the specified
[head] in the tamyiz/specification nominal con-
structions), MOD (general modifier of verbs or
nouns), and — (marking flatness inside construc-
tions such as first-last proper name sequences).
This relation label set is much smaller than the
twenty or so dashtags used in PATB to mark syn-
tactic and semantic functions. No empty cate-
gories and no phrase co-indexation are made ex-
plicit. No semantic relations (such as time and
place) are annotated.
Figure 1 presents an example of a tree in CATiB
annotation. In this example, the verb @ðP@P� zArwA
‘visited’ heads a subject, an object and a prepo-
sitional phrase. The subject includes a com-
plex number construction formed using idafa and
tamyiz and headed by the number vñ‚Ôg� xmswn
‘fifty’, which is the only carrier of the subject’s
syntactic nominative case here. The preposition ú�¯�
fy heads the prepositional phrase, whose object is
a proper noun, �PñÖß tmwz ‘July’ with an adjectival
~
modifier, úæ;,AÖÏ@ AlmADy ‘last’. See Habash et al.
�
(2009) for a full description of CATiB’s guidelines
and a detailed comparison with PATB and PADT.
</bodyText>
<footnote confidence="0.945368">
2We are able to reproduce a parsing-tailored tag set [size
36] (Kulick et al., 2006) automatically at 98.5% accuracy us-
ing features from the annotated trees. Details of this result
will be presented in a future publication.
3Arabic transliterations are in the Habash-Soudi-
Buckwalter transliteration scheme (Habash et al., 2007b).
</footnote>
<figure confidence="0.522955">
VRB
@ðP@P� zArwA
‘visited’
</figure>
<figureCaption confidence="0.973301">
Figure 1: CATiB annotation for the sentence
</figureCaption>
<bodyText confidence="0.799045">
ú•AÖÏ@ �ñÖ~ß ú
-Ë@ �àñ‚Ô g
æAƒ �
�¯� �àA�JJ.Ë @ðP@ P l~
�
xmswn Alf sAˆyH zArwA lbnAn fy tmwz AlmADy
‘50 thousand tourists visited Lebanon last July.’
</bodyText>
<sectionHeader confidence="0.986678" genericHeader="method">
3 Annotation Procedure
</sectionHeader>
<bodyText confidence="0.999675363636364">
Although CATiB is independent of previous anno-
tation projects, it builds on existing resources and
lessons learned. For instance, CATiB’s pipeline
uses PATB-trained tools for tokenization, POS-
tagging and parsing. We also use the TrEd anno-
tation interface developed in coordination with the
PADT. Similarly, our annotation manual is guided
by the wonderfully detailed manual of the PATB
for coverage (Maamouri et al., 2008).
Annotators Our five annotators and their super-
visor are all educated native Arabic speakers. An-
notators are hired on a part-time basis and are not
required to be on-site. The annotation files are ex-
changed electronically. This arrangement allows
more annotators to participate, and reduces logis-
tical problems. However, having no full-time an-
notators limits the overall weekly annotation rate.
Annotator training took about two months (150
hrs/annotator on average). This training time is
much shorter than the PATB’s six-month training
period.4
Below, we describe our pipeline in some detail
including the different resources we use.
Data Preparation The data to annotate is split
into batches of 3-5 documents each, with each
document containing around 15-20 sentences
(400-600 tokens). Each annotator works on one
batch at a time. This procedure and the size
of the batches was determined to be optimal for
both the software and the annotators’ productivity.
To track the annotation quality, several key doc-
uments are selected for inter-annotator agreement
(IAA) checks. The IAA documents are chosen to
</bodyText>
<footnote confidence="0.485928">
4Personal communication with Mohamed Maamouri.
</footnote>
<figure confidence="0.99082009375">
SBJ
NOM
àñ‚Ô g
�� xmswn
‘fifty’
TMZ
NOM
-Ë@ Alf
�
‘thousand’
IDF
NOM
l�� Aƒ sAˆyH
‘tourist’
MOD
PRT
ú¯� fy
‘in’
OBJ
PROP
PñÖ ß tmwz
~
‘July’
MOD
NOM
ú•AÖÏ@ AlmADy
�æ�
‘last’
OBJ
PROP
,)q lbnAn
‘Lebanon’
</figure>
<page confidence="0.995503">
222
</page>
<bodyText confidence="0.995065413043478">
cover a range of sources and to be of average doc-
ument size. These documents (collectively about
10% of the token volume) are seeded throughout
the batches. Every annotator eventually annotates
each one of the IAA documents, but is never told
which documents are for IAA.
Automatic Tokenization and POS Tagging We
use the MADA&amp;TOKAN toolkit (Habash and
Rambow, 2005) for initial tokenization and POS
tagging. The tokenization F-score is 99.1% and
the POS tagging accuracy (on the CATiB POS tag
set; with gold tokenization) is above 97.7%.
Manual Tokenization Correction Tokeniza-
tion decisions are manually checked and corrected
by the annotation supervisor. New POS tags are
assigned manually only for corrected tokens. Full
POS tag correction is done as part of the manual
annotation step (see below). The speed of this step
is well over 6K tokens/hour.
Automatic Parsing Initial dependency parsing
in CATiB is conducted using MaltParser (Nivre et
al., 2007). An initial parsing model was built using
an automatic constituency-to-dependency conver-
sion of a section of PATB part 3 (PATB3-Train,
339K tokens). The quality of the automatic con-
version step is measured against a hand-annotated
version of an automatically converted held-out
section of PATB3 (PATB3-Dev, 31K tokens). The
results are 87.2%, 93.16% and 83.2% for attach-
ment (ATT), label (LAB) and labeled attachment
(LABATT) accuracies, respectively. These num-
bers are 95%, 98% and 94% (respectively) of the
IAA scores on that set.5 At the production mid-
point another parsing model was trained by adding
all the CATiB annotations generated up to that
point (513K tokens total). An evaluation of the
parser against the CATiB version of PATB3-Dev
shows the ATT, LAB and LABATT accuracies
are 81.7%, 91.1% and 77.4% respectively.6
Manual Annotation CATiB uses the TrEd tool
as a visual interface for annotation.7 The parsed
trees are converted to TrEd format and delivered
to the annotators. The annotators are asked to only
correct the POS, syntactic structure and relation
labels. Once annotated (i.e. corrected), the docu-
ments are returned to be packaged for release.
</bodyText>
<footnote confidence="0.9993314">
5Conversion will be discussed in a future publication.
6Since CATiB POS tag set is rather small, we extend it
automatically deterministically to a larger tag set for parsing
purposes. Details will be presented in a future publication.
7http://ufal.mff.cuni.cz/∼pajas/tred
</footnote>
<table confidence="0.9987928">
IAA Set Sents POS ATT LAB LABATT
PATB3-Dev All 98.6 91.5 95.3 88.8
&lt; 40 98.7 91.7 94.7 88.6
PROD All 97.6 89.2 93.0 85.0
&lt; 40 97.7 91.5 94.1 87.7
</table>
<tableCaption confidence="0.9748416">
Table 1: Average pairwise IAA accuracies for 5
annotators. The Sents column indicates which
sentences were evaluated, based on token length.
The sizes of the sets are 2.4K (PATB3-Dev) and
3.8K (PROD) tokens.
</tableCaption>
<sectionHeader confidence="0.99925" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.991123171428572">
Data Sets CATiB annotated data is taken
from the following LDC-provided resources:8
LDC2007E46, LDC2007E87, GALE-DEV07,
MT05 test set, MT06 test set, and PATB (part 3).
These datasets are 2004-2007 newswire feeds col-
lected from different news agencies and news pa-
pers, such as Agence France Presse, Xinhua, Al-
Hayat, Al-Asharq Al-Awsat, Al-Quds Al-Arabi,
An-Nahar, Al-Ahram and As-Sabah. The CATiB-
annotated PATB3 portion is extracted from An-
Nahar news articles from 2002. Headlines, date-
lines and bylines are not annotated and some sen-
tences are excluded for excessive (&gt;300 tokens)
length and formatting problems. Over 273K to-
kens (228K words, 7,121 trees) of data were anno-
tated, not counting IAA duplications. In addition,
the PATB part 1, part 2 and part 3 data is automat-
ically converted into CATiB representation. This
converted data contributes an additional 735K to-
kens (613K words, 24,198 trees). Collectively, the
CATiB version 1.0 release contains over 1M to-
kens (841K words, 31,319 trees), including anno-
tated and converted data.
Annotator Speeds Our POS and syntax annota-
tion rate is 540 tokens/hour (with some reaching
rates as high as 715 tokens/hour). However, due
to the current part-time arrangement, annotators
worked an average of only 6 hours/week, which
meant that data was annotated at an average rate of
15K tokens/week. These speeds are much higher
than reported speeds for complete (POS+syntax)
annotation in PATB (around 250-300 tokens/hour)
and PADT (around 75 tokens/hour).9
Basic Inter-Annotator Agreement We present
IAA scores for ATT, LAB and LABATT on IAA
</bodyText>
<footnote confidence="0.9893705">
8http://www.ldc.upenn.edu/
9Extrapolated from personal communications, Mohamed
Maamouri and Otakar Smrž. In the PATB, the syntactic anno-
tation step alone has similar speed to CATiB’s full POS and
syntax annotation. The POS annotation step is what slows
down the whole process in PATB.
</footnote>
<page confidence="0.993729">
223
</page>
<table confidence="0.9986502">
IAA File Toks/hr POS ATT LAB LABATT
HI 398 97.0 94.7 96.1 91.2
HI-S 956 97.0 97.8 97.9 95.7
LO 476 98.3 88.8 91.7 82.3
LO-S 944 97.7 91.0 93.8 85.8
</table>
<tableCaption confidence="0.971558">
Table 2: Highest and lowest average pairwise IAA
</tableCaption>
<bodyText confidence="0.997022078125">
accuracies for 5 annotators achieved on a single
document – before and after serial annotation. The
“-S” suffix indicates the result after the second an-
notation.
subsets from two data sets in Table 1: PATB3-
Dev is based on an automatically converted PATB
set and PROD refers to all the new CATiB data.
We compare the IAA scores for all sentences and
for sentences of token length ≤ 40 tokens. The
IAA scores in PROD are lower than PATB3-Dev,
this is understandable given that the error rate of
the conversion from a manual annotation (starting
point of PATB3-Dev) is lower than parsing (start-
ing point for PROD). Length seems to make a big
difference in performance for PROD, but less so
for PATB3-Dev, which makes sense given their
origins. Annotation training did not include very
long sentences. Excluding long sentences during
production was not possible because the data has a
high proportion of very long sentences: for PROD
set, 41% of sentences had &gt;40 tokens and they
constituted over 61% of all tokens.
The best reported IAA number for PATB
is 94.3% F-measure after extensive efforts
(Maamouri et al., 2008). This number does not in-
clude dashtags, empty categories or indices. Our
numbers cannot be directly compared to their
number because of the different metrics used for
different representations.
Serial Inter-Annotator Agreement We test the
value of serial annotation, a procedure in which
the output of annotation is passed again as input to
another annotator in an attempt to improve it. The
IAA documents with the highest (HI, 333 tokens)
and lowest (LO, 350 tokens) agreement scores in
PROD are selected. The results, shown in Table 2,
indicate that serial annotation is very helpful re-
ducing LABATT error by 20-50%. The reduction
in LO is not as large as that in HI, unfortunately.
The second round of annotation is almost twice as
fast as the first round. The overall reduction in
speed (end-to-end) is around 30%.
Disagreement Analysis We conduct an error
analysis of the basic-annotation disagreements in
HI and LO. The two sets differ in sentence length,
source and genre: HI has 28 tokens/sentence and
contains AFP general news, while LO has 58 to-
kens/sentence and contains Xinhua financial news.
The most common POS disagreement in both sets
is NOM/PROP confusion, a common issue in Ara-
bic POS tagging in general. The most common
attachment disagreements in LO are as follows:
prepositional phrase (PP) and nominal modifiers
(8% of the words had at least one dissenting an-
notation), complex constructions (dates, proper
nouns, numbers and currencies) (6%), subordina-
tion/coordination (4%), among others. The re-
spective proportions for HI are 5%, 5% and 1%.
Label disagreements are mostly in nominal modi-
fication (MOD/TMZ/IDF/—) (LO 10%, HI 5% of
the words had at least one dissenting annotation).
The error differences between HI and LO seem
to primarily correlate with length difference and
less with genre and source differences.
</bodyText>
<sectionHeader confidence="0.994506" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999975875">
We presented CATiB, a treebank for Arabic pars-
ing built with faster annotation speed in mind. In
the future, we plan to extend our annotation guide-
lines focusing on longer sentences and specific
complex constructions, introduce serial annotation
as a standard part of the annotation pipeline, and
enrich the treebank with automatically generated
morphological information.
</bodyText>
<sectionHeader confidence="0.999097" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99984575">
N. Habash, R. Faraj and R. Roth. 2009. Syntactic Annota-
tion in the Columbia Arabic Treebank. In Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
N. Habash and O. Rambow. 2005. Arabic Tokenization,
Part-of-Speech Tagging and Morphological Disambigua-
tion in One Fell Swoop. In ACL’05, Ann Arbor, Michi-
gan.
N. Habash, R. Gabbard, O. Rambow, S. Kulick, and M. Mar-
cus. 2007a. Determining case in Arabic: Learning com-
plex linguistic behavior requires complex linguistic fea-
tures. In EMNLP’07, Prague, Czech Republic.
N. Habash, A. Soudi, and T. Buckwalter. 2007b. On Ara-
bic Transliteration. In A. van den Bosch and A. Soudi,
editors, Arabic Computational Morphology. Springer.
S. Kulick, R. Gabbard, and M. Marcus. 2006. Parsing the
Arabic Treebank: Analysis and Improvements. In Tree-
banks and Linguistic Theories Conference, Prague, Czech
Republic.
M. Maamouri, A. Bies, and T. Buckwalter. 2004. The Penn
Arabic Treebank: Building a large-scale annotated Arabic
corpus. In Conference on Arabic Language Resources and
Tools, Cairo, Egypt.
M. Maamouri, A. Bies and S. Kulick. 2008. Enhancing the
Arabic treebank: a collaborative effort toward new anno-
tation guidelines. In LREC’08, Marrakech, Morocco.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. Kubler,
S. Marinov, and E. Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency parsing.
Natural Language Engineering, 13(2):95–135.
O. Smrž and J. Hajiˇc. 2006. The Other Arabic Treebank:
Prague Dependencies and Functions. In Ali Farghaly, edi-
tor, Arabic Computational Linguistics. CSLI Publications.
</reference>
<page confidence="0.998712">
224
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.798325">
<title confidence="0.998704">CATiB: The Columbia Arabic Treebank</title>
<author confidence="0.999682">Nizar Habash</author>
<author confidence="0.999682">Ryan M Roth</author>
<affiliation confidence="0.9173895">Center for Computational Learning Systems Columbia University, New York, USA</affiliation>
<email confidence="0.999682">habash@ccls.columbia.edu</email>
<email confidence="0.999682">ryanr@ccls.columbia.edu</email>
<abstract confidence="0.996878857142857">The Columbia Arabic Treebank (CATiB) is a database of syntactic analyses of Arabic sentences. CATiB contrasts with previous approaches to Arabic treebanking in its emphasis on speed with some constraints on linguistic richness. Two basic ideas inspire the CATiB approach: no annotation of redundant information and using representations and terminology inspired by traditional Arabic syntax. We describe CATiB’s representation and annotation procedure, and report on interannotator agreement and speed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>R Faraj</author>
<author>R Roth</author>
</authors>
<title>Syntactic Annotation in the Columbia Arabic Treebank.</title>
<date>2009</date>
<booktitle>In Conference on Arabic Language Resources and Tools,</booktitle>
<location>Cairo, Egypt.</location>
<marker>Habash, Faraj, Roth, 2009</marker>
<rawString>N. Habash, R. Faraj and R. Roth. 2009. Syntactic Annotation in the Columbia Arabic Treebank. In Conference on Arabic Language Resources and Tools, Cairo, Egypt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop.</title>
<date>2005</date>
<booktitle>In ACL’05,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="9197" citStr="Habash and Rambow, 2005" startWordPosition="1408" endWordPosition="1411"> to 4Personal communication with Mohamed Maamouri. SBJ NOM àñ‚Ô g �� xmswn ‘fifty’ TMZ NOM -Ë@ Alf � ‘thousand’ IDF NOM l�� Aƒ sAˆyH ‘tourist’ MOD PRT ú¯� fy ‘in’ OBJ PROP PñÖ ß tmwz ~ ‘July’ MOD NOM ú•AÖÏ@ AlmADy �æ� ‘last’ OBJ PROP ,)q lbnAn ‘Lebanon’ 222 cover a range of sources and to be of average document size. These documents (collectively about 10% of the token volume) are seeded throughout the batches. Every annotator eventually annotates each one of the IAA documents, but is never told which documents are for IAA. Automatic Tokenization and POS Tagging We use the MADA&amp;TOKAN toolkit (Habash and Rambow, 2005) for initial tokenization and POS tagging. The tokenization F-score is 99.1% and the POS tagging accuracy (on the CATiB POS tag set; with gold tokenization) is above 97.7%. Manual Tokenization Correction Tokenization decisions are manually checked and corrected by the annotation supervisor. New POS tags are assigned manually only for corrected tokens. Full POS tag correction is done as part of the manual annotation step (see below). The speed of this step is well over 6K tokens/hour. Automatic Parsing Initial dependency parsing in CATiB is conducted using MaltParser (Nivre et al., 2007). An in</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>N. Habash and O. Rambow. 2005. Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop. In ACL’05, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>R Gabbard</author>
<author>O Rambow</author>
<author>S Kulick</author>
<author>M Marcus</author>
</authors>
<title>Determining case in Arabic: Learning complex linguistic behavior requires complex linguistic features.</title>
<date>2007</date>
<booktitle>In EMNLP’07,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3305" citStr="Habash et al., 2007" startWordPosition="486" endWordPosition="489">yntactic studies. The use of these representations puts higher requirements on the kind of annotators to hire and the length of their initial training. CATiB contrasts with PATB and PADT in putting an emphasis on annotation speed for the specific task of parser training. Two basic ideas inspire the CATiB approach. First, CATiB avoids annotation of redundant linguistic information or information not targeted in current parsing research. For example, nominal case markers in Arabic have been shown to be automatically determinable from syntax and word morphology and needn’t be manually annotated (Habash et al., 2007a). Also, phrasal co-indexation, empty pronouns, and full lemma disambiguation are not currently used in parsing research so we do not include them in CATiB. Second, CATiB uses a simple intuitive dependency representation and terminology inspired by Arabic’s long tradition of syntactic studies. For example, CATiB relation labels include tamyiz (specification) and idafa (possessive construction) in addition to universal predicate-argument structure labels such as subject, object and modifier. These representation choices make it easier to train annotators without being restricted to hire people</context>
<context position="6798" citStr="Habash et al., 2007" startWordPosition="1022" endWordPosition="1025">ive case here. The preposition ú�¯� fy heads the prepositional phrase, whose object is a proper noun, �PñÖß tmwz ‘July’ with an adjectival ~ modifier, úæ;,AÖÏ@ AlmADy ‘last’. See Habash et al. � (2009) for a full description of CATiB’s guidelines and a detailed comparison with PATB and PADT. 2We are able to reproduce a parsing-tailored tag set [size 36] (Kulick et al., 2006) automatically at 98.5% accuracy using features from the annotated trees. Details of this result will be presented in a future publication. 3Arabic transliterations are in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007b). VRB @ðP@P� zArwA ‘visited’ Figure 1: CATiB annotation for the sentence ú•AÖÏ@ �ñÖ~ß ú -Ë@ �àñ‚Ô g æAƒ � �¯� �àA�JJ.Ë @ðP@ P l~ � xmswn Alf sAˆyH zArwA lbnAn fy tmwz AlmADy ‘50 thousand tourists visited Lebanon last July.’ 3 Annotation Procedure Although CATiB is independent of previous annotation projects, it builds on existing resources and lessons learned. For instance, CATiB’s pipeline uses PATB-trained tools for tokenization, POStagging and parsing. We also use the TrEd annotation interface developed in coordination with the PADT. Similarly, our annotation manual is guided by the wonde</context>
</contexts>
<marker>Habash, Gabbard, Rambow, Kulick, Marcus, 2007</marker>
<rawString>N. Habash, R. Gabbard, O. Rambow, S. Kulick, and M. Marcus. 2007a. Determining case in Arabic: Learning complex linguistic behavior requires complex linguistic features. In EMNLP’07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>A Soudi</author>
<author>T Buckwalter</author>
</authors>
<title>On Arabic Transliteration.</title>
<date>2007</date>
<booktitle>Arabic Computational Morphology.</booktitle>
<editor>In A. van den Bosch and A. Soudi, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="3305" citStr="Habash et al., 2007" startWordPosition="486" endWordPosition="489">yntactic studies. The use of these representations puts higher requirements on the kind of annotators to hire and the length of their initial training. CATiB contrasts with PATB and PADT in putting an emphasis on annotation speed for the specific task of parser training. Two basic ideas inspire the CATiB approach. First, CATiB avoids annotation of redundant linguistic information or information not targeted in current parsing research. For example, nominal case markers in Arabic have been shown to be automatically determinable from syntax and word morphology and needn’t be manually annotated (Habash et al., 2007a). Also, phrasal co-indexation, empty pronouns, and full lemma disambiguation are not currently used in parsing research so we do not include them in CATiB. Second, CATiB uses a simple intuitive dependency representation and terminology inspired by Arabic’s long tradition of syntactic studies. For example, CATiB relation labels include tamyiz (specification) and idafa (possessive construction) in addition to universal predicate-argument structure labels such as subject, object and modifier. These representation choices make it easier to train annotators without being restricted to hire people</context>
<context position="6798" citStr="Habash et al., 2007" startWordPosition="1022" endWordPosition="1025">ive case here. The preposition ú�¯� fy heads the prepositional phrase, whose object is a proper noun, �PñÖß tmwz ‘July’ with an adjectival ~ modifier, úæ;,AÖÏ@ AlmADy ‘last’. See Habash et al. � (2009) for a full description of CATiB’s guidelines and a detailed comparison with PATB and PADT. 2We are able to reproduce a parsing-tailored tag set [size 36] (Kulick et al., 2006) automatically at 98.5% accuracy using features from the annotated trees. Details of this result will be presented in a future publication. 3Arabic transliterations are in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007b). VRB @ðP@P� zArwA ‘visited’ Figure 1: CATiB annotation for the sentence ú•AÖÏ@ �ñÖ~ß ú -Ë@ �àñ‚Ô g æAƒ � �¯� �àA�JJ.Ë @ðP@ P l~ � xmswn Alf sAˆyH zArwA lbnAn fy tmwz AlmADy ‘50 thousand tourists visited Lebanon last July.’ 3 Annotation Procedure Although CATiB is independent of previous annotation projects, it builds on existing resources and lessons learned. For instance, CATiB’s pipeline uses PATB-trained tools for tokenization, POStagging and parsing. We also use the TrEd annotation interface developed in coordination with the PADT. Similarly, our annotation manual is guided by the wonde</context>
</contexts>
<marker>Habash, Soudi, Buckwalter, 2007</marker>
<rawString>N. Habash, A. Soudi, and T. Buckwalter. 2007b. On Arabic Transliteration. In A. van den Bosch and A. Soudi, editors, Arabic Computational Morphology. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kulick</author>
<author>R Gabbard</author>
<author>M Marcus</author>
</authors>
<title>Parsing the Arabic Treebank: Analysis and Improvements.</title>
<date>2006</date>
<booktitle>In Treebanks and Linguistic Theories Conference,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6556" citStr="Kulick et al., 2006" startWordPosition="987" endWordPosition="990">heads a subject, an object and a prepositional phrase. The subject includes a complex number construction formed using idafa and tamyiz and headed by the number vñ‚Ôg� xmswn ‘fifty’, which is the only carrier of the subject’s syntactic nominative case here. The preposition ú�¯� fy heads the prepositional phrase, whose object is a proper noun, �PñÖß tmwz ‘July’ with an adjectival ~ modifier, úæ;,AÖÏ@ AlmADy ‘last’. See Habash et al. � (2009) for a full description of CATiB’s guidelines and a detailed comparison with PATB and PADT. 2We are able to reproduce a parsing-tailored tag set [size 36] (Kulick et al., 2006) automatically at 98.5% accuracy using features from the annotated trees. Details of this result will be presented in a future publication. 3Arabic transliterations are in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007b). VRB @ðP@P� zArwA ‘visited’ Figure 1: CATiB annotation for the sentence ú•AÖÏ@ �ñÖ~ß ú -Ë@ �àñ‚Ô g æAƒ � �¯� �àA�JJ.Ë @ðP@ P l~ � xmswn Alf sAˆyH zArwA lbnAn fy tmwz AlmADy ‘50 thousand tourists visited Lebanon last July.’ 3 Annotation Procedure Although CATiB is independent of previous annotation projects, it builds on existing resources and lessons le</context>
</contexts>
<marker>Kulick, Gabbard, Marcus, 2006</marker>
<rawString>S. Kulick, R. Gabbard, and M. Marcus. 2006. Parsing the Arabic Treebank: Analysis and Improvements. In Treebanks and Linguistic Theories Conference, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
<author>T Buckwalter</author>
</authors>
<title>The Penn Arabic Treebank: Building a large-scale annotated Arabic corpus.</title>
<date>2004</date>
<booktitle>In Conference on Arabic Language Resources and Tools,</booktitle>
<location>Cairo, Egypt.</location>
<contexts>
<context position="1063" citStr="Maamouri et al., 2004" startWordPosition="148" endWordPosition="151">CATiB approach: no annotation of redundant information and using representations and terminology inspired by traditional Arabic syntax. We describe CATiB’s representation and annotation procedure, and report on interannotator agreement and speed. 1 Introduction and Motivation Treebanks are collections of manually-annotated syntactic analyses of sentences. They are primarily intended for building models for statistical parsing; however, they are often enriched for general natural language processing purposes. For Arabic, two important treebanking efforts exist: the Penn Arabic Treebank (PATB) (Maamouri et al., 2004) and the Prague Arabic Dependency Treebank (PADT) (Smrž and Hajiˇc, 2006). In addition to syntactic annotations, both resources are annotated with rich morphological and semantic information such as full part-of-speech (POS) tags, lemmas, semantic roles, and diacritizations. This allows these treebanks to be used for training a variety of applications other than parsing, such as tokenization, diacritization, POS tagging, morphological disambiguation, base phrase chunking, and semantic role labeling. In this paper, we describe a new Arabic treebanking effort: the Columbia Arabic Treebank (CATiB</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, 2004</marker>
<rawString>M. Maamouri, A. Bies, and T. Buckwalter. 2004. The Penn Arabic Treebank: Building a large-scale annotated Arabic corpus. In Conference on Arabic Language Resources and Tools, Cairo, Egypt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
<author>S Kulick</author>
</authors>
<title>Enhancing the Arabic treebank: a collaborative effort toward new annotation guidelines.</title>
<date>2008</date>
<booktitle>In LREC’08,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="7469" citStr="Maamouri et al., 2008" startWordPosition="1129" endWordPosition="1132">notation for the sentence ú•AÖÏ@ �ñÖ~ß ú -Ë@ �àñ‚Ô g æAƒ � �¯� �àA�JJ.Ë @ðP@ P l~ � xmswn Alf sAˆyH zArwA lbnAn fy tmwz AlmADy ‘50 thousand tourists visited Lebanon last July.’ 3 Annotation Procedure Although CATiB is independent of previous annotation projects, it builds on existing resources and lessons learned. For instance, CATiB’s pipeline uses PATB-trained tools for tokenization, POStagging and parsing. We also use the TrEd annotation interface developed in coordination with the PADT. Similarly, our annotation manual is guided by the wonderfully detailed manual of the PATB for coverage (Maamouri et al., 2008). Annotators Our five annotators and their supervisor are all educated native Arabic speakers. Annotators are hired on a part-time basis and are not required to be on-site. The annotation files are exchanged electronically. This arrangement allows more annotators to participate, and reduces logistical problems. However, having no full-time annotators limits the overall weekly annotation rate. Annotator training took about two months (150 hrs/annotator on average). This training time is much shorter than the PATB’s six-month training period.4 Below, we describe our pipeline in some detail inclu</context>
<context position="14788" citStr="Maamouri et al., 2008" startWordPosition="2309" endWordPosition="2312">he conversion from a manual annotation (starting point of PATB3-Dev) is lower than parsing (starting point for PROD). Length seems to make a big difference in performance for PROD, but less so for PATB3-Dev, which makes sense given their origins. Annotation training did not include very long sentences. Excluding long sentences during production was not possible because the data has a high proportion of very long sentences: for PROD set, 41% of sentences had &gt;40 tokens and they constituted over 61% of all tokens. The best reported IAA number for PATB is 94.3% F-measure after extensive efforts (Maamouri et al., 2008). This number does not include dashtags, empty categories or indices. Our numbers cannot be directly compared to their number because of the different metrics used for different representations. Serial Inter-Annotator Agreement We test the value of serial annotation, a procedure in which the output of annotation is passed again as input to another annotator in an attempt to improve it. The IAA documents with the highest (HI, 333 tokens) and lowest (LO, 350 tokens) agreement scores in PROD are selected. The results, shown in Table 2, indicate that serial annotation is very helpful reducing LABA</context>
</contexts>
<marker>Maamouri, Bies, Kulick, 2008</marker>
<rawString>M. Maamouri, A. Bies and S. Kulick. 2008. Enhancing the Arabic treebank: a collaborative effort toward new annotation guidelines. In LREC’08, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S Kubler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>MaltParser: A languageindependent system for data-driven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="9790" citStr="Nivre et al., 2007" startWordPosition="1501" endWordPosition="1504"> (Habash and Rambow, 2005) for initial tokenization and POS tagging. The tokenization F-score is 99.1% and the POS tagging accuracy (on the CATiB POS tag set; with gold tokenization) is above 97.7%. Manual Tokenization Correction Tokenization decisions are manually checked and corrected by the annotation supervisor. New POS tags are assigned manually only for corrected tokens. Full POS tag correction is done as part of the manual annotation step (see below). The speed of this step is well over 6K tokens/hour. Automatic Parsing Initial dependency parsing in CATiB is conducted using MaltParser (Nivre et al., 2007). An initial parsing model was built using an automatic constituency-to-dependency conversion of a section of PATB part 3 (PATB3-Train, 339K tokens). The quality of the automatic conversion step is measured against a hand-annotated version of an automatically converted held-out section of PATB3 (PATB3-Dev, 31K tokens). The results are 87.2%, 93.16% and 83.2% for attachment (ATT), label (LAB) and labeled attachment (LABATT) accuracies, respectively. These numbers are 95%, 98% and 94% (respectively) of the IAA scores on that set.5 At the production midpoint another parsing model was trained by a</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, Kubler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. Kubler, S. Marinov, and E. Marsi. 2007. MaltParser: A languageindependent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Smrž</author>
<author>J Hajiˇc</author>
</authors>
<title>The Other Arabic Treebank: Prague Dependencies and Functions.</title>
<date>2006</date>
<booktitle>Arabic Computational Linguistics.</booktitle>
<editor>In Ali Farghaly, editor,</editor>
<publisher>CSLI Publications.</publisher>
<marker>Smrž, Hajiˇc, 2006</marker>
<rawString>O. Smrž and J. Hajiˇc. 2006. The Other Arabic Treebank: Prague Dependencies and Functions. In Ali Farghaly, editor, Arabic Computational Linguistics. CSLI Publications.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>