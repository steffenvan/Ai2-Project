<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000215">
<title confidence="0.996668">
A Unigram Orientation Model for Statistical Machine Translation
</title>
<author confidence="0.872451">
Christoph Tillmann
</author>
<affiliation confidence="0.598388">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.591207">
Yorktown Heights, NY 10598
</address>
<email confidence="0.960485">
ctill@us.ibm.com
</email>
<figure confidence="0.949256363636364">
Lebanese
violate
warplanes
Israeli
airspace
b5
b4
b3
b2
b1
A
l
T
A
}
r
A
t
A
l
H
r
b
y
P
y
l
y
P
A
l
A
s
</figure>
<bodyText confidence="0.789401814814815">
r
A
}
t
n
t
h
k
A
l
m
j
A
l
A
l
j
w
y
A
l
l
b
n
A
n
y
</bodyText>
<sectionHeader confidence="0.929951" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909125">
In this paper, we present a unigram segmen-
tation model for statistical machine transla-
tion where the segmentation units are blocks:
pairs of phrases without internal structure. The
segmentation model uses a novel orientation
component to handle swapping of neighbor
blocks. During training, we collect block un-
igram counts with orientation: we count how
often a block occurs to the left or to the right of
some predecessor block. The orientation model
is shown to improve translation performance
over two models: 1) no block re-ordering is
used, and 2) the block swapping is controlled
only by a language model. We show exper-
imental results on a standard Arabic-English
translation task.
</bodyText>
<sectionHeader confidence="0.997616" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986968833333333">
In recent years, phrase-based systems for statistical ma-
chine translation (Och et al., 1999; Koehn et al., 2003;
Venugopal et al., 2003) have delivered state-of-the-art
performance on standard translation tasks. In this pa-
per, we present a phrase-based unigram system similar
to the one in (Tillmann and Xia, 2003), which is ex-
tended by an unigram orientation model. The units of
translation are blocks, pairs of phrases without internal
structure. Fig. 1 shows an example block translation us-
ing five Arabic-English blocks . The unigram
orientation model is trained from word-aligned training
data. During decoding, we view translation as a block
segmentation process, where the input sentence is seg-
mented from left to right and the target sentence is gener-
ated from bottom to top, one block at a time. A monotone
block sequence is generated except for the possibility to
swap a pair of neighbor blocks. The novel orientation
model is used to assist the block swapping: as shown in
</bodyText>
<figureCaption confidence="0.564703333333333">
Figure 1: An Arabic-English block translation example
taken from the devtest set. The Arabic words are roman-
ized.
</figureCaption>
<bodyText confidence="0.999260173913044">
section 3, block swapping where only a trigram language
model is used to compute probabilities between neighbor
blocks fails to improve translation performance. (Wu,
1996; Zens and Ney, 2003) present re-ordering models
that make use of a straight/inverted orientation model that
is related to our work. Here, we investigate in detail
the effect of restricting the word re-ordering to neighbor
block swapping only.
In this paper, we assume a block generation process that
generates block sequences from bottom to top, one block
at a time. The score of a successor block depends on its
predecessor block and on its orientation relative to the
block . In Fig. 1 for example, block is the predeces-
sor of block , and block is the predecessor of block
. The target clump of a predecessor block is adja-
cent to the target clump of a successor block . A right
adjacent predecessor block is a block where addition-
ally the source clumps are adjacent and the source clump
of occurs to the right of the source clump of . A left
adjacent predecessor block is defined accordingly.
During decoding, we compute the score of a
block sequence with orientation as a product of
block bigram scores:
</bodyText>
<equation confidence="0.941907">
(1)
</equation>
<bodyText confidence="0.993215909090909">
where is a block and is a three-valued
orientation component linked to the block (the orienta-
tion of the predecessor block is ignored.). A block
has right orientation ( ) if it has a left adjacent
predecessor. Accordingly, a block has left orientation
( ) if it has a right adjacent predecessor. If a block
has neither a left or right adjacent predecessor, its orien-
tation is neutral ( ). The neutral orientation is not
modeled explicitly in this paper, rather it is handled as a
default case as explained below. In Fig. 1, the orienta-
tion sequence is , i.e. block and
block are generated using left orientation. During de-
coding most blocks have right orientation , since
the block translations are mostly monotone.
We try to find a block sequence with orientation
that maximizes . The following three types
of parameters are used to model the block bigram score
in Eq. 1:
Two unigram count-based models: and
. We compute the unigram probability of
a block based on its occurrence count . The
blocks are counted from word-aligned training data.
We also collect unigram counts with orientation: a
left count and a right count . These
counts are defined via an enumeration process and
are used to define the orientation model :
Trigram language model: The block language
model score is computed as the proba-
bility of the first target word in the target clump of
given the final two words of the target clump of
.
The three models are combined in a log-linear way, as
shown in the following section.
</bodyText>
<sectionHeader confidence="0.992758" genericHeader="method">
2 Orientation Unigram Model
</sectionHeader>
<bodyText confidence="0.968990576923077">
The basic idea of the orientation model can be illustrated
as follows: In the example translation in Fig. 1, block
occurs to the left of block . Although the joint block
consisting of the two smaller blocks and
has not been seen in the training data, we can still profit
from the fact that block occurs more frequently with
left than with right orientation. In our Arabic-English
training data, block has been seen times
with left orientation, and with right orien-
tation, i.e. it is always involved in swapping. This intu-
ition is formalized using unigram counts with orientation.
The orientation model is related to the distortion model
in (Brown et al., 1993), but we do not compute a block
alignment during training. We rather enumerate all rele-
vant blocks in some order. Enumeration does not allow
us to capture position dependent distortion probabilities,
but we can compute statistics about adjacent block prede-
cessors.
Our baseline model is the unigram monotone model de-
scribed in (Tillmann and Xia, 2003). Here, we select
blocks from word-aligned training data and unigram
block occurrence counts are computed: all blocks
for a training sentence pair are enumerated in some order
and we count how often a given block occurs in the par-
allel training data 1. The training algorithm yields a list
of about blocks per training sentence pair. In this pa-
per, we make extended use of the baseline enumeration
procedure: for each block , we additionally enumerate
all its left and right predecessors . No optimal block
segmentation is needed to compute the predecessors: for
each block , we check for adjacent predecessor blocks
that also occur in the enumeration list. We compute left
orientation counts as follows:
Here, we enumerate all adjacent predecessors of block
over all training sentence pairs. The identity of is ig-
nored. is the number of times the block succeeds
some right adjacent predecessor block . The ’right’ ori-
entation count is defined accordingly. Note, that
in general the unigram count :
during enumeration, a block might have both left and
right adjacent predecessors, either a left or a right adja-
cent predecessor, or no adjacent predecessors at all. The
orientation count collection is illustrated in Fig. 2: each
time a block has a left or right adjacent predecessor in
the parallel training data, the orientation counts are incre-
mented accordingly.
The decoding orientation restrictions are illustrated in
Fig 3: a monotone block sequence with right (
&apos;We keep all blocks for which and the phrase
length is less or equal . No other selection criteria are applied.
For the model, we keep all blocks for which
.
</bodyText>
<figure confidence="0.943354323529412">
right adjacent predecessor of
)
N (b) += 1
R
b4
b4
b3
o =N
4
b3
o =L
3
b2
o =R
3
b1
o =R
2
o =N
2
b1
b2
o =R
1
o =R
1
o =R
4
N (b) += 1
L
b
b’
b
b’
</figure>
<figureCaption confidence="0.999186">
Figure 2: During training, blocks are enumerated in some
</figureCaption>
<bodyText confidence="0.948385138888889">
order: for each block , we look for left and right adjacent
predecessors .
orientation is generated. If a block is skipped e.g. block
in Fig 3 by first generating block then block , the
block is generated using left orientation . Since
the block translation is generated from bottom-to-top, the
blocks and do not have adjacent predecessors below
them: they are generated by a default model
without orientation component. The orientation model
is given in Eq. 2, the default model is given in Eq. 3.
The block bigram model in
Eq. 1 is defined as:
where and the orientation of the
predecessor is ignored. The are chosen to be optimal
on the devtest set (the optimal parameter setting is shown
in Table. 1). Only two parameters have to be optimized
due to the constraint that the have to sum to . The
default model is
defined as:
.
Straightforward normalization over all successor blocks
in Eq. 2 and in Eq. 3 is not feasible: there are tens of mil-
lions of possible successor blocks . In future work, nor-
malization over a restricted successor set, e.g. for a given
source input sentence, all blocks that match this sen-
tence might be useful for both training and decoding. The
segmentation model in Eq. 1 naturally prefers translations
that make use of a smaller number of blocks which leads
to a smaller number of factors in Eq. 1. Using fewer ’big-
ger’ blocks to carry out the translation generally seems
to improve translation performance. Since normalization
does not influence the number of blocks used to carry out
the translation, it might be less important for our segmen-
tation model.
We use a DP-based beam search procedure similar to the
one presented in (Tillmann and Xia, 2003). We maximize
</bodyText>
<figureCaption confidence="0.6693636">
Figure 3: During decoding, a mostly monotone block se-
quence with orientation is generated as shown
in the left picture. In the right picture, block swapping
generates block to the left of block . The blocks
and do not have a left or right adjacent predecessor.
</figureCaption>
<bodyText confidence="0.954300909090909">
over all block segmentations with orientation for
which the source phrases yield a segmentation of the in-
put sentence. Swapping involves only blocks for
which for the successor block , e.g. the
blocks and in Fig 1. We tried several thresholds for
, and performance is reduced significantly only if
. No other parameters are used to control
the block swapping. In particular the orientation of the
predecessor block is ignored: in future work, we might
take into account that a certain predecessor block typi-
cally precedes other blocks.
</bodyText>
<sectionHeader confidence="0.99222" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.987169615384615">
The translation system is tested on an Arabic-to-English
translation task. The training data comes from the UN
news sources: million Arabic and million En-
glish words. The training data is sentence-aligned yield-
ing million training sentence pairs. The Arabic data
is romanized, some punctuation tokenization and some
number classing are carried out on the English and the
Arabic training data. As devtest set, we use testing
data provided by LDC, which consists of sen-
tences with Arabic words with reference trans-
lations. As a blind test set, we use MT 03 Arabic-English
DARPA evaluation test set consisting of sentences
with Arabic words.
Three systems are evaluated in our experiments: is the
baseline block unigram model without re-ordering. Here,
monotone block alignments are generated: the blocks
have only left predecessors (no blocks are swapped).
This is the model presented in (Tillmann and Xia, 2003).
For the model, the sentence is translated mostly
monotonously, and only neighbor blocks are allowed to
be swapped (at most block is skipped). The
model allows for the same block swapping as the
model, but additionally uses the orientation component
described in Section 2: the block swapping is controlled
where . The are not optimized sepa-
rately, rather we define:
</bodyText>
<tableCaption confidence="0.872314666666667">
Table 1: Effect of the orientation model on Arabic-
English test data: LDC devtest set and DARPA MT 03
blind test set.
</tableCaption>
<table confidence="0.99905275">
Test Unigram Setting BLEUr4n4
Model
Dev test
Test
</table>
<tableCaption confidence="0.7051164">
Table 2: Arabic-English example blocks from the de-
vtest set: the Arabic phrases are romanized. The example
blocks were swapped in the development test set transla-
tions. The counts are obtained from the parallel training
data.
</tableCaption>
<table confidence="0.980322666666667">
Arabic-English blocks
(’exhibition’ ’mErD’) 97 32
(’added’ ’wADAP) 285 68
(’said’ ’wqAl’) 872 801
(’suggested ’AqtrH’) 356 729
(’terrorist attacks’ hjmAt ArhAbyp’) 14 27
</table>
<bodyText confidence="0.995744825">
by the unigram orientation counts. The and mod-
els use the block bigram model in Eq. 3: all blocks
are generated with neutral orientation , and only
two components, the block unigram model and the
block bigram score are used.
Experimental results are reported in Table 1: three BLEU
results are presented for both devtest set and blind test
set. Two scaling parameters are set on the devtest set and
copied for use on the blind test set. The second column
shows the model name, the third column presents the op-
timal weighting as obtained from the devtest set by car-
rying out an exhaustive grid search. The fourth column
shows BLEU results together with confidence intervals
(Here, the word casing is ignored). The block swapping
model obtains a statistical significant improve-
ment over the baseline model. Interestingly, the swap-
ping model without orientation performs worse than
the baseline model: the word-based trigram language
model alone is too weak to control the block swapping:
the model is too unrestrictive to handle the block swap-
ping reliably. Additionally, Table 2 presents devtest set
example blocks that have actually been swapped. The
training data is unsegmented, as can be seen from the
first two blocks. The block in the first line has been seen
times more often with left than with right orientation.
Blocks for which the ratio is bigger than
are likely candidates for swapping in our Arabic-English
experiments. The ratio itself is not currently used in the
orientation model. The orientation model mostly effects
blocks where the Arabic and English words are verbs or
nouns. As shown in Fig. 1, the orientation model uses
the orientation probability for the noun block ,
and only the default model for the adjective block . Al-
though the noun block might occur by itself without ad-
jective, the swapping is not controlled by the occurrence
of the adjective block (which does not have adjacent
predecessors). We rather model the fact that a noun block
is typically preceded by some block . This situation
seems typical for the block swapping that occurs on the
evaluation test set.
</bodyText>
<sectionHeader confidence="0.982705" genericHeader="method">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9992665">
This work was partially supported by DARPA and mon-
itored by SPAWAR under contract No. N66001-99-2-
8916. The paper has greatly profited from discussion with
Kishore Papineni and Fei Xia.
</bodyText>
<sectionHeader confidence="0.999062" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997295">
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263–311.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the HLT-NAACL 2003 conference, pages 127–133,
Edmonton, Canada, May.
Franz-Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical Ma-
chine Translation. In Proc. of the Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC 99), pages 20–28,
College Park, MD, June.
Christoph Tillmann and Fei Xia. 2003. A Phrase-based
Unigram Model for Statistical Machine Translation. In
Companian Vol. of the Joint HLT and NAACL Confer-
ence (HLT 03), pages 106–108, Edmonton, Canada,
June.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective Phrase Translation Extraction from
Alignment Models. In Proc. of the 41st Annual Conf.
of the Association for Computational Linguistics (ACL
03), pages 319–326, Sapporo, Japan, July.
Dekai Wu. 1996. A Polynomial-Time Algorithm for Sta-
tistical Machine Translation. In Proc. of the 34th An-
nual Conf. of the Association for Computational Lin-
guistics (ACL 96), pages 152–158, Santa Cruz, CA,
June.
Richard Zens and Hermann Ney. 2003. A Comparative
Study on Reordering Constraints in Statistical Machine
Translation. In Proc. of the 41st Annual Conf. of the
Association for Computational Linguistics (ACL 03),
pages 144–151, Sapporo, Japan, July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.597493">
<title confidence="0.999942">A Unigram Orientation Model for Statistical Machine Translation</title>
<author confidence="0.995714">Christoph</author>
<affiliation confidence="0.89811">IBM T.J. Watson Research Yorktown Heights, NY</affiliation>
<email confidence="0.974683">ctill@us.ibm.com</email>
<abstract confidence="0.995774430555556">Lebanese violate warplanes Israeli airspace A l T A } r A t A l H r b y P y l y P A l A s r A } t n t h k A l m j A l A l j w y A l l b n A n y Abstract In this paper, we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure. The segmentation model uses a novel orientation component to handle swapping of neighbor blocks. During training, we collect block uncounts with we count how often a block occurs to the left or to the right of some predecessor block. The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model. We show experimental results on a standard Arabic-English translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5495" citStr="Brown et al., 1993" startWordPosition="953" endWordPosition="956">an be illustrated as follows: In the example translation in Fig. 1, block occurs to the left of block . Although the joint block consisting of the two smaller blocks and has not been seen in the training data, we can still profit from the fact that block occurs more frequently with left than with right orientation. In our Arabic-English training data, block has been seen times with left orientation, and with right orientation, i.e. it is always involved in swapping. This intuition is formalized using unigram counts with orientation. The orientation model is related to the distortion model in (Brown et al., 1993), but we do not compute a block alignment during training. We rather enumerate all relevant blocks in some order. Enumeration does not allow us to capture position dependent distortion probabilities, but we can compute statistics about adjacent block predecessors. Our baseline model is the unigram monotone model described in (Tillmann and Xia, 2003). Here, we select blocks from word-aligned training data and unigram block occurrence counts are computed: all blocks for a training sentence pair are enumerated in some order and we count how often a given block occurs in the parallel training data</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the HLT-NAACL 2003 conference,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1141" citStr="Koehn et al., 2003" startWordPosition="207" endWordPosition="210">del uses a novel orientation component to handle swapping of neighbor blocks. During training, we collect block unigram counts with orientation: we count how often a block occurs to the left or to the right of some predecessor block. The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model. We show experimental results on a standard Arabic-English translation task. 1 Introduction In recent years, phrase-based systems for statistical machine translation (Och et al., 1999; Koehn et al., 2003; Venugopal et al., 2003) have delivered state-of-the-art performance on standard translation tasks. In this paper, we present a phrase-based unigram system similar to the one in (Tillmann and Xia, 2003), which is extended by an unigram orientation model. The units of translation are blocks, pairs of phrases without internal structure. Fig. 1 shows an example block translation using five Arabic-English blocks . The unigram orientation model is trained from word-aligned training data. During decoding, we view translation as a block segmentation process, where the input sentence is segmented fro</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. of the HLT-NAACL 2003 conference, pages 127–133, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz-Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Alignment Models for Statistical Machine Translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC 99),</booktitle>
<pages>20--28</pages>
<location>College Park, MD,</location>
<contexts>
<context position="1121" citStr="Och et al., 1999" startWordPosition="203" endWordPosition="206">he segmentation model uses a novel orientation component to handle swapping of neighbor blocks. During training, we collect block unigram counts with orientation: we count how often a block occurs to the left or to the right of some predecessor block. The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model. We show experimental results on a standard Arabic-English translation task. 1 Introduction In recent years, phrase-based systems for statistical machine translation (Och et al., 1999; Koehn et al., 2003; Venugopal et al., 2003) have delivered state-of-the-art performance on standard translation tasks. In this paper, we present a phrase-based unigram system similar to the one in (Tillmann and Xia, 2003), which is extended by an unigram orientation model. The units of translation are blocks, pairs of phrases without internal structure. Fig. 1 shows an example block translation using five Arabic-English blocks . The unigram orientation model is trained from word-aligned training data. During decoding, we view translation as a block segmentation process, where the input sente</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz-Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved Alignment Models for Statistical Machine Translation. In Proc. of the Joint Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC 99), pages 20–28, College Park, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Fei Xia</author>
</authors>
<title>A Phrase-based Unigram Model for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Companian Vol. of the Joint HLT and NAACL Conference (HLT 03),</booktitle>
<pages>106--108</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1344" citStr="Tillmann and Xia, 2003" startWordPosition="238" endWordPosition="241">he right of some predecessor block. The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model. We show experimental results on a standard Arabic-English translation task. 1 Introduction In recent years, phrase-based systems for statistical machine translation (Och et al., 1999; Koehn et al., 2003; Venugopal et al., 2003) have delivered state-of-the-art performance on standard translation tasks. In this paper, we present a phrase-based unigram system similar to the one in (Tillmann and Xia, 2003), which is extended by an unigram orientation model. The units of translation are blocks, pairs of phrases without internal structure. Fig. 1 shows an example block translation using five Arabic-English blocks . The unigram orientation model is trained from word-aligned training data. During decoding, we view translation as a block segmentation process, where the input sentence is segmented from left to right and the target sentence is generated from bottom to top, one block at a time. A monotone block sequence is generated except for the possibility to swap a pair of neighbor blocks. The nove</context>
<context position="5846" citStr="Tillmann and Xia, 2003" startWordPosition="1009" endWordPosition="1012"> data, block has been seen times with left orientation, and with right orientation, i.e. it is always involved in swapping. This intuition is formalized using unigram counts with orientation. The orientation model is related to the distortion model in (Brown et al., 1993), but we do not compute a block alignment during training. We rather enumerate all relevant blocks in some order. Enumeration does not allow us to capture position dependent distortion probabilities, but we can compute statistics about adjacent block predecessors. Our baseline model is the unigram monotone model described in (Tillmann and Xia, 2003). Here, we select blocks from word-aligned training data and unigram block occurrence counts are computed: all blocks for a training sentence pair are enumerated in some order and we count how often a given block occurs in the parallel training data 1. The training algorithm yields a list of about blocks per training sentence pair. In this paper, we make extended use of the baseline enumeration procedure: for each block , we additionally enumerate all its left and right predecessors . No optimal block segmentation is needed to compute the predecessors: for each block , we check for adjacent pr</context>
<context position="9361" citStr="Tillmann and Xia, 2003" startWordPosition="1636" endWordPosition="1639"> source input sentence, all blocks that match this sentence might be useful for both training and decoding. The segmentation model in Eq. 1 naturally prefers translations that make use of a smaller number of blocks which leads to a smaller number of factors in Eq. 1. Using fewer ’bigger’ blocks to carry out the translation generally seems to improve translation performance. Since normalization does not influence the number of blocks used to carry out the translation, it might be less important for our segmentation model. We use a DP-based beam search procedure similar to the one presented in (Tillmann and Xia, 2003). We maximize Figure 3: During decoding, a mostly monotone block sequence with orientation is generated as shown in the left picture. In the right picture, block swapping generates block to the left of block . The blocks and do not have a left or right adjacent predecessor. over all block segmentations with orientation for which the source phrases yield a segmentation of the input sentence. Swapping involves only blocks for which for the successor block , e.g. the blocks and in Fig 1. We tried several thresholds for , and performance is reduced significantly only if . No other parameters are u</context>
<context position="11105" citStr="Tillmann and Xia, 2003" startWordPosition="1919" endWordPosition="1922">uation tokenization and some number classing are carried out on the English and the Arabic training data. As devtest set, we use testing data provided by LDC, which consists of sentences with Arabic words with reference translations. As a blind test set, we use MT 03 Arabic-English DARPA evaluation test set consisting of sentences with Arabic words. Three systems are evaluated in our experiments: is the baseline block unigram model without re-ordering. Here, monotone block alignments are generated: the blocks have only left predecessors (no blocks are swapped). This is the model presented in (Tillmann and Xia, 2003). For the model, the sentence is translated mostly monotonously, and only neighbor blocks are allowed to be swapped (at most block is skipped). The model allows for the same block swapping as the model, but additionally uses the orientation component described in Section 2: the block swapping is controlled where . The are not optimized separately, rather we define: Table 1: Effect of the orientation model on ArabicEnglish test data: LDC devtest set and DARPA MT 03 blind test set. Test Unigram Setting BLEUr4n4 Model Dev test Test Table 2: Arabic-English example blocks from the devtest set: the </context>
</contexts>
<marker>Tillmann, Xia, 2003</marker>
<rawString>Christoph Tillmann and Fei Xia. 2003. A Phrase-based Unigram Model for Statistical Machine Translation. In Companian Vol. of the Joint HLT and NAACL Conference (HLT 03), pages 106–108, Edmonton, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Effective Phrase Translation Extraction from Alignment Models.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Conf. of the Association for Computational Linguistics (ACL 03),</booktitle>
<pages>319--326</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1166" citStr="Venugopal et al., 2003" startWordPosition="211" endWordPosition="214">entation component to handle swapping of neighbor blocks. During training, we collect block unigram counts with orientation: we count how often a block occurs to the left or to the right of some predecessor block. The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model. We show experimental results on a standard Arabic-English translation task. 1 Introduction In recent years, phrase-based systems for statistical machine translation (Och et al., 1999; Koehn et al., 2003; Venugopal et al., 2003) have delivered state-of-the-art performance on standard translation tasks. In this paper, we present a phrase-based unigram system similar to the one in (Tillmann and Xia, 2003), which is extended by an unigram orientation model. The units of translation are blocks, pairs of phrases without internal structure. Fig. 1 shows an example block translation using five Arabic-English blocks . The unigram orientation model is trained from word-aligned training data. During decoding, we view translation as a block segmentation process, where the input sentence is segmented from left to right and the t</context>
</contexts>
<marker>Venugopal, Vogel, Waibel, 2003</marker>
<rawString>Ashish Venugopal, Stephan Vogel, and Alex Waibel. 2003. Effective Phrase Translation Extraction from Alignment Models. In Proc. of the 41st Annual Conf. of the Association for Computational Linguistics (ACL 03), pages 319–326, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A Polynomial-Time Algorithm for Statistical Machine Translation.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Conf. of the Association for Computational Linguistics (ACL 96),</booktitle>
<pages>152--158</pages>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="2298" citStr="Wu, 1996" startWordPosition="395" endWordPosition="396"> process, where the input sentence is segmented from left to right and the target sentence is generated from bottom to top, one block at a time. A monotone block sequence is generated except for the possibility to swap a pair of neighbor blocks. The novel orientation model is used to assist the block swapping: as shown in Figure 1: An Arabic-English block translation example taken from the devtest set. The Arabic words are romanized. section 3, block swapping where only a trigram language model is used to compute probabilities between neighbor blocks fails to improve translation performance. (Wu, 1996; Zens and Ney, 2003) present re-ordering models that make use of a straight/inverted orientation model that is related to our work. Here, we investigate in detail the effect of restricting the word re-ordering to neighbor block swapping only. In this paper, we assume a block generation process that generates block sequences from bottom to top, one block at a time. The score of a successor block depends on its predecessor block and on its orientation relative to the block . In Fig. 1 for example, block is the predecessor of block , and block is the predecessor of block . The target clump of a </context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Dekai Wu. 1996. A Polynomial-Time Algorithm for Statistical Machine Translation. In Proc. of the 34th Annual Conf. of the Association for Computational Linguistics (ACL 96), pages 152–158, Santa Cruz, CA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>A Comparative Study on Reordering Constraints in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Conf. of the Association for Computational Linguistics (ACL 03),</booktitle>
<pages>144--151</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="2319" citStr="Zens and Ney, 2003" startWordPosition="397" endWordPosition="400">where the input sentence is segmented from left to right and the target sentence is generated from bottom to top, one block at a time. A monotone block sequence is generated except for the possibility to swap a pair of neighbor blocks. The novel orientation model is used to assist the block swapping: as shown in Figure 1: An Arabic-English block translation example taken from the devtest set. The Arabic words are romanized. section 3, block swapping where only a trigram language model is used to compute probabilities between neighbor blocks fails to improve translation performance. (Wu, 1996; Zens and Ney, 2003) present re-ordering models that make use of a straight/inverted orientation model that is related to our work. Here, we investigate in detail the effect of restricting the word re-ordering to neighbor block swapping only. In this paper, we assume a block generation process that generates block sequences from bottom to top, one block at a time. The score of a successor block depends on its predecessor block and on its orientation relative to the block . In Fig. 1 for example, block is the predecessor of block , and block is the predecessor of block . The target clump of a predecessor block is </context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Richard Zens and Hermann Ney. 2003. A Comparative Study on Reordering Constraints in Statistical Machine Translation. In Proc. of the 41st Annual Conf. of the Association for Computational Linguistics (ACL 03), pages 144–151, Sapporo, Japan, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>