<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001059">
<title confidence="0.9947295">
An Annotation Tool for Multimodal Dialogue Corpora
using Global Document Annotation
</title>
<author confidence="0.854257">
Kazunari ITO and Hiroaki SAITO
</author>
<affiliation confidence="0.901109">
Keio University
Department of Science for Open and Environmental Systems
</affiliation>
<address confidence="0.860987">
3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan, 223-8522
</address>
<email confidence="0.990646">
{k_ito , hxs}@nak.ics.keio.ac.jp
</email>
<sectionHeader confidence="0.998528" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997647">
This paper reports a tool which assists the
user in annotating a video corpus and en-
ables the user to search for a semantic or
pragmatic structure in a GDA tagged cor-
pus. An XQL format is allowed for search
patterns as well as a plain phrase. This
tool is capable of generating a GDA time-
stamped corpus from a video file manu-
ally. It will be publicly available for aca-
demic purposes.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999905208333333">
To achieve a natural communication environment
between computers and the users, many interactive
prototype systems that can talk with the user have
been developed using multimodal information
(face expressions, voice tones, gestures, etc.).
Since multimodalness of these systems is manually
built in, achieving free and effective communica-
tion or enhancing communication ablilities is not
easy. Thus automatic learning from huge data is
hoped for.
Recently such various video data as TV dramas,
news, and language teaching materials are avail-
able, from which natural interactiveness should be
extracted. Such interactiveness from an intellectual
content is also valuable for the fields of machine
translation, information retrieval, handling ques-
tion responses, and knowledge discovery systems.
GDA1 (Global Document Annotation), which is
an XML tag set, adds information on syntax, se-
mantics, and pragmatics to texts (Hashida 1998).
The texts with GDA organically corresponding to
voice and a video will contribute to the basic re-
search into these technologies and promote the ap-
plication development.
</bodyText>
<sectionHeader confidence="0.964045" genericHeader="introduction">
2 GDA tagged corpus
</sectionHeader>
<bodyText confidence="0.999660666666667">
This chapter explains the GDA tag set and a
method which relates tagged data with the video
image.
</bodyText>
<subsectionHeader confidence="0.568348">
2.1 GDA
</subsectionHeader>
<bodyText confidence="0.999556555555556">
The GDA Initiative aims at having Internet authors
annotate their electronic documents with a com-
mon standard tag set which allows machines to
automatically recognize the semantic and prag-
matic structures of the documents. A huge amount
of annotated data is expected to emerge, which
should serve not just as tagged linguistic corpora
but also as a worldwide, self-extending knowl-
edge-base mainly consisting of examples of how
our knowledge manifests. It describes the meaning
of sentence analysis (semantics and pragmatics)
basically. It also describes information on the sub-
ject role, the rhetoric relation, and correspondence.
Figure 1 shows an example of the text “
(an ear is covered)” tagged with GDA.
Note that GDA is totally language independent,
although all the following examples include Japa-
nese texts.
</bodyText>
<footnote confidence="0.593463">
1 http://www.i-content.org/GDA/tagset.html
</footnote>
<figure confidence="0.927491461538461">
&lt;q who=&amp;quot;A&amp;quot;&gt;
&lt;su syn=”fc” id=&amp;quot;kakure&amp;quot;&gt;
&lt;vp syn=&amp;quot;f&amp;quot;&gt;
&lt;n arg=&amp;quot;X&amp;quot;&gt; &lt;/n&gt;
&lt;ad sem=&amp;quot;obj&amp;quot;&gt; &lt;/ad&gt;
&lt;adp syn=&amp;quot;f&amp;quot;&gt;
&lt;v&gt; &lt;/v&gt;
&lt;ad&gt; &lt;/ad&gt;
&lt;/adp&gt;
&lt;v&gt; &lt;/v&gt;
&lt;/vp&gt;
&lt;/su&gt;
&lt;/q&gt;
</figure>
<figureCaption confidence="0.999986">
Figure 1: A fragment of GDA corpus
</figureCaption>
<bodyText confidence="0.999969846153846">
In Figure 1, &lt;q&gt; represents a word where that
part is spoken by someone, and the value of who
attribute shows who utters it. &lt;su&gt; indicates a sen-
tence, having no syntactic relation to other parts of
the utterance. Attribute “syn” means it is a syntax
structure of the sentence, and “fc” means a forward
link dependency. &lt;v&gt; and &lt;vp&gt; elements mean a
verb and a verb phrase, respectively. &lt;n&gt; element
represents a noun or a noun phrase. &lt;ad&gt; and
&lt;adp&gt; elements are an adverb, and a postpositional
phrase. As these examples display, the GDA tag
set has been determined to show syntactic structure
effectively where a word is assumed to be a unit.
</bodyText>
<subsectionHeader confidence="0.9577765">
2.2 Adding Time -information to GDA tagged
corpora
</subsectionHeader>
<bodyText confidence="0.9654554">
When you relate the video image file with its text
file, it is widely used to embed the time informa-
tion indicating when an utterance is spoken. We
define the following two kinds of new formats to
relate a video file with its GDA corpus file .
1. btime and etime attributes: These attributes can
be added to an arbitrary tag. Attribute btime shows
the start time of an utterance. Attribute etime
shows the finished time. The format is described as
follows.
</bodyText>
<construct confidence="0.6929105">
&lt;any btime=”utterance start time(sec)”
etime=”utterance end time(sec)” &gt;
sentence
&lt;/any&gt;
</construct>
<bodyText confidence="0.855633230769231">
With these attributes, the voiceless section of an
utterance can be precisely indicated and the over-
lap event is detected in a multi-speaker environ-
ment.
2. tst (time stamp) tag: Tag “tst” is an empty ele-
ment tag and described in the following format.
&lt;tst val = “utterance start time
(sec)” /&gt;
A tst tag can be inserted in an arbitrary place. The
ending time of an utterance can be determined
from the value of the next btime attribute or val
attribute of the next tst tag. Moreover, it is also
possible that the tst tag has btime and etime attrib-
</bodyText>
<figureCaption confidence="0.761323">
ute. Figure 2 shows an example when time infor-
mation is added to the GDA tagged corpus in
Figure 1. You can see a man allocated “A” speaks
</figureCaption>
<figure confidence="0.984432578947369">
a word “ (an ear)” from 60.81 to 61.03 sec, a
word “ (is)” from 61.10 to 61.90 sec.
&lt;q who=”A”&gt;
&lt;su id=”kakure”&gt;
&lt;vp syn=”f”&gt;
&lt;n arg=&amp;quot;X&amp;quot; btime=”60.815”
etime=&amp;quot;61.034”&gt;
&lt;/n&gt;
&lt;ad sem=”obj”&gt;
&lt;tst val=”61.100”/&gt;&lt;/ad&gt;
&lt;adp syn=”f”&gt;
&lt;v&gt;
&lt;tst val=&amp;quot;61.907&amp;quot;/&gt;&lt;/v&gt;
&lt;ad&gt; &lt;tst val=&amp;quot;62.192&amp;quot;/&gt;&lt;/ad&gt;
&lt;/adp&gt;
&lt;v&gt; &lt;tst val=&amp;quot;62.383&amp;quot;/&gt;&lt;/v&gt;
&lt;/vp&gt;
&lt;/su&gt;
&lt;/q&gt;
</figure>
<figureCaption confidence="0.9050965">
Figure 2: An example of GDA corpus with time-
stamp
</figureCaption>
<bodyText confidence="0.9995335">
These time annotation is often inserted after
GDA tagging, because time information is inde-
pendent from the standard syntactic/pragmatic
GDA.
</bodyText>
<sectionHeader confidence="0.770067" genericHeader="method">
3 An annotation tool for multimodal dia-
logue corpus
</sectionHeader>
<bodyText confidence="0.988311">
We have developed a multimodal annotation tool
for a video corpus and its annotated data (JEITA
2001). This tool runs on any platform that accom-
modates Java2 and Java Media Framework (JMF2)
ver.2.0 or higher. It also requires Java-based XML
parser Xerces-J 3 and Java-based XQL engine
</bodyText>
<footnote confidence="0.996658">
2 http://java.sun.com/products/java-media/jmf/
3 http://xml.apache.org/xerces-j/
</footnote>
<bodyText confidence="0.999557333333334">
GMD-IPSI XQL4. Moreover, users can easily ex-
tend functions by mounting plug-ins. The proto-
type is equipped with two different types of plug-
ins. One is “XQL search plug-in” in which the user
can search for various syntactic and semantic pat-
terns in a GDA corpus. An XQL format query as
well as a plain word is allowed for search patterns.
Another is “Annotation plug-in” which assists the
annotator in generating a time-stamped GDA cor-
pus from a video file.
Figure 4 shows the screenshot when the XQL
search plug-in is loaded, you can see new window
appears at the bottom left of the tool. The user can
type a query into the text field at the bottom of the
new window.
</bodyText>
<sectionHeader confidence="0.983314" genericHeader="method">
4 Basic Functions
</sectionHeader>
<figureCaption confidence="0.979593666666667">
A screenshot of basic composition is shown in
Figure 3. The whole window is composed of sev-
eral internal windows.
Figure 4: Screenshot when XQL search plug-in is
loaded
Figure 3: Screenshot of basic composition
</figureCaption>
<bodyText confidence="0.999988375">
The left internal window is “time table window”
which displays each sentence in a table format.
When the user clicks one of the columns in the
table, the video corresponding to the sentence is
played. Rows of the table are highlighted consecu-
tively during that part of the video is being dis-
played. The top right is “video image window”. It
displays and plays the video image.
</bodyText>
<sectionHeader confidence="0.99848" genericHeader="method">
5 Extended Functions
</sectionHeader>
<bodyText confidence="0.999709">
This section explains the outline of two plug-ins
first and usage of these after that.
</bodyText>
<footnote confidence="0.6499835">
5.1 XQL search plug-in
4 http://xml.darmstadt.gmd.de/xql/
</footnote>
<bodyText confidence="0.998264428571429">
An acceptable query format is a plain text or a
query text defined by XQL (XML Query Lan-
guage). XQL is a subset of query language XQuery
that uses XML as a data model, a recommendation
by W3C. XQL has already been mounted on soft-
ware over many fields like Web browsers, docu-
ment archiving systems, XML middleware, Perl
libraries, and a command line utility. XQL always
returns a part of the document. In XQL, the hierar-
chy of the node is written by ‘/’, an arbitrary hier-
archy is written by ‘//’, the attribute name by
‘@name’, the tag name by ‘name’ as it is. A regu-
lar expression and a conditional expression are also
acceptable. For a exmple, ‘ //q[@who=&apos;A&apos;]’ returns
‘q’ node with the value of attribute name ‘who’
equals ‘A’.
Figure 5 shows search results. In Figure 5, five
words [ (a hair), (a forelock),
(a prominent forehead), (a face), (an
ear)] are matched on the condition that the part of
speech is noun and the value of the attribute “arg”
equals to “X” (see the fourth line of Figure 2).
Moreover, since the label “X” is attached to the
utterance &amp;quot; (this person)&amp;quot; in this GDA file,
this query becomes a meaning of extracting a noun
phrase whose subject is “this person”. The user can
search for the subject even if an object is omitted
in suited sentences.
</bodyText>
<figureCaption confidence="0.998259">
Figure 6: Screenshot when Annoation plug-in is loaded
</figureCaption>
<bodyText confidence="0.99787725">
A query of extracting a synonym of a certain
word, only the utterance of a specific speaker to
another, and a sentence that of the response for a
certain utterance, etc. are possible. Semantic or
discourse structures are also extractable if such
information is annotated in the file. Clicking any
column of the table, the corresponding media sec-
tion is played like the “time table Window”.
</bodyText>
<figureCaption confidence="0.850049">
Figure 5: Result by query [a noun and value of at-
tribute “arg” equals “X”]
</figureCaption>
<subsectionHeader confidence="0.998206">
5.2 Annotation plug-in
</subsectionHeader>
<bodyText confidence="0.999874769230769">
Creating a GDA file with timestamps record auto-
matically with less time and less labor is indispen-
sable to make large quantities of them and to
spread them. From the accuracy of the current
speech recognition technology, it is difficult to at-
tach timestamps record automatically and accu-
rately by taking synchronization of a video image.
Annotation plug-in increases the efficiency of
time-stamping a GDA file by visual operation.
Figure 6 shows the screenshot when it is loaded.
The window located on the lower part of Figure
6 called “Annotation board” which displays infor-
mation on a GDA file with a time-stamp visually.
You can also see a horizontal axis which expresses
the time on media, and two layers in the board.
Upper layer displays utterances of “Speaker A”,
lower layer displays “Speaker B” in this case. Rec-
tangles on one layer represent each speaker&apos;s utter-
ances according to the time series. The utterance
itseft is displayed in the rectangle, color of which
is different for each speaker. Length and the posi-
tion indicate time information of the utterance. A
person edits the annotation by operating two kinds
of lines on the board. &amp;quot;Current line&amp;quot; shows a cur-
rent playback position on the media. &amp;quot;Base line&amp;quot;
indicates the start or the end point of the time-
stamp when the utterance sentence is newly in-
serted. Various functions (change of line’s position,
and deletion or insertion of an utterance) can be
executed by mouse operation. When the annotator
clicks on the board, the “Current line” moves and a
frame to which a video image corresponds is dis-
played. Thus, an annotator can attach the informa-
tion of the start time and the end time and utterance
text itself manually. A prototype system which
automatically converts the GDA file from raw text
files with a morphological analysis and a parsing
tool in addition to the original filter has already
been proposed (Suzuki 2001).
</bodyText>
<sectionHeader confidence="0.939709" genericHeader="method">
6 Current development
</sectionHeader>
<bodyText confidence="0.999985">
The core functions of the tool are complete and
stable. Still, there is much room for expansion. The
following functions are being developed.
</bodyText>
<subsectionHeader confidence="0.999937">
6.1 User-friendly GUI-based search interface
</subsectionHeader>
<bodyText confidence="0.9999596">
Needless to say, there is no guarantee that aclause
or a sentence which agrees with an XQL query
exists. Moreover the user has to know the XQL
expression for search. We believe it necessary that
a retrieval way by the top-down philosophy which
narrows the candidates while presenting suited
clauses sequentially.
It is very difficult at present for an XQL to ex-
press dependency relations among the search con-
dition. Now, a query language of XML has been
integrated into Xquery. Hence, we are scheduled to
bulid an Xquery engine. A user-friendly GUI-
based search window for the retrieval which does
not require an explicit XQL query is currently be-
ing developed.
</bodyText>
<subsectionHeader confidence="0.999969">
6.2 Uniting with other multimodal data
</subsectionHeader>
<bodyText confidence="0.999975125">
There are many kinds of specifications to describe
multimodal data. For example, J-ToBI(Venditti,
1995) which describes prosodic information of
voice, FACS(Ekman, etc. 1978) which annotates
person&apos;s expression, etc. We are scheduled to de-
sign the specification to integrate these information
into GDA in the XML format. As a result, the user
will be able to present a condition, for example, of
a word Truth’ of doubt type or ‘I see’ of hesita-
tion.
It is also scheduled to relate visual information
of video data with GDA. They can contain infor-
mation on motion, glance and the place of the ob-
ject in video image. A reverse-search which
extracts a corresponding text from visual informa-
tion in a video image becomes available, too.
</bodyText>
<subsectionHeader confidence="0.997213">
6.3 Coordinated functions
</subsectionHeader>
<bodyText confidence="0.999896571428571">
We intend to enhance a relation with other annota-
tion tools. Concretely, various formats of output
files can be taken in this tool in XML formats. We
shall define a DTD (data conversion definition) to
enable export and import to/from other tools. A
function of date exchange enables the user to en-
hance flexibility and accessbility of this tool.
</bodyText>
<subsectionHeader confidence="0.927397">
6.4 Retrieval for multiple files
</subsectionHeader>
<bodyText confidence="0.999940333333333">
The user can retrieve only a single file in a local
machine at present. This tool will cope with the
client-server model that it requests retrieval de-
mand to the corpora database servers on a network,
downloads only necessary files to the local ma-
chine and analyzes them.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="method">
7 Related works
</sectionHeader>
<bodyText confidence="0.999738903225806">
Most of recent multilmodal annotation tool pro-
jects are almost Java-based, use XML for file ex-
change and have an object-oriented design:
MATE (Carletta, etc. 1999) is an annotation
workbench that allows highly generic specification
via stylesheets that determine shape and
functionality of the user’s implemation. Speed and
stability for the tool are both still problematic for
real annotation. Also, the generic approach
increases the initial effort to set up the tool since
you basically have to write your own tool using the
MATE style language.
EUDICO (Brugman et al. 2000) is an effort to
allow multi-user annotation of a centrally localted
corpus via a web-based interface. The tools that are
available to work with the multimodal corpus
make it possible to analyze their content and to add
new free defined annotations to them. A EUDICO
client can choose a subset of the corpus data.
Anvil (Kipp, 2001) is a generic video annota-
tion tool which allows frame-accurate, hierarchical
multi-layered annotation with objects that contain
attribute-value pairs. Layers and attributes are all
user-defineable. A time-aligned view and some
configuration options make coding work quite in-
tuitive.
ATLAS project (Steven, etc. 2000) deals with
all types of annotation and is theoretically based on
the idea of annotation graphs where nodes repre-
sent time points and edges indicate annotation la-
bels.
</bodyText>
<sectionHeader confidence="0.998593" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999953181818182">
We have reported an annotation tool for multimo-
dal dialogue corpora. This tool enables semantic
and pragmatic search from a video data with anno-
tated texts in the GDA format. This tool is plat-
form-independent and equipped with easy-to-use
interface. It will be of use to researchers dealing
with multimodal dialogue for exploratory studies
as well as annotation. Core functions are complete
and various extension facilities are now being de-
veloped. This prototype will be publicly available
soon.
</bodyText>
<sectionHeader confidence="0.998892" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999858166666667">
We wish to express our gratitute for the members
of the committee on methods and standards of dia-
logic content in Japan Electronics and Information
Technology Industries Association (JEITA). Espe-
cially we would like to thank Koichi Hashida for
insightful advices.
</bodyText>
<sectionHeader confidence="0.99908" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999546903225806">
Hasida, K. (1998) Intellectual contents of all-round
based on GDA meaning modification. The transac-
tion of Japanese Society for Artificial Intelligence.,
Vol. 13, No.4, pp.528-535 (in Japanese).
JEITA (2001) Servey Report about natural language
processing system. pp.49-56(in Japanese).
J. Suzuki and K. Hasida (2001) Proposal of answer ex-
traction system using GDA tag. The 7th annual meet-
ing of Language Processing Society (in Japanese).
Ekman, P. and Friesen, W.(1978) Facial action coding
system : a technique for the measurement of facial-
movement, Consulting Psychologists Press, 1978
J.J.Venditti,(1995) Japanese ToBI Labelling Guidelines.
Ohio-State University,Columbus,U.S.A.,1995.
Michael Kipp (2001) Anvil - A Generic Annotation
Tool for Multimodal Dialogue, Proceedings of Eu-
rospeech 2001, pp.1367-1370.
Carletta, J. and Isard, A (1999) The MATE Annotation
Workbench, In Proceedings of the ACL Workshop,
Towards Standards and Tools for Discourse Tagging.,
pp.11-17.
H. Brugman, A. Russel, D. Broeder, and P.Wittenburg
(2000) EUDICO. Annotation and Exploitation of
Multi Media Corpora, Proceedings of LREC 2000
Workshop.
Steven Bird, David Day, John Garofolo, John Hender-
son, Christophe Laprun, and Mark Liberman (2000)
ATLAS: A Flexible and Extensible Architecture for
Linguistic Annotation, Proceedings of the Second In-
ternational Conference on Language Resource and
Evaluation, pp.1699-1706.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.841586">
<title confidence="0.996647">An Annotation Tool for Multimodal Dialogue Corpora using Global Document Annotation</title>
<author confidence="0.985739">Kazunari ITO</author>
<author confidence="0.985739">Hiroaki SAITO</author>
<affiliation confidence="0.998981">Keio University Department of Science for Open and Environmental Systems</affiliation>
<address confidence="0.943513">3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan, 223-8522</address>
<email confidence="0.967462">k_ito@nak.ics.keio.ac.jp</email>
<email confidence="0.967462">hxs@nak.ics.keio.ac.jp</email>
<abstract confidence="0.994621090909091">This paper reports a tool which assists the user in annotating a video corpus and enables the user to search for a semantic or pragmatic structure in a GDA tagged corpus. An XQL format is allowed for search patterns as well as a plain phrase. This tool is capable of generating a GDA timestamped corpus from a video file manually. It will be publicly available for academic purposes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Hasida</author>
</authors>
<title>Intellectual contents of all-round based on GDA meaning modification.</title>
<date>1998</date>
<journal>The transaction of Japanese Society for Artificial Intelligence.,</journal>
<volume>13</volume>
<pages>528--535</pages>
<note>(in Japanese).</note>
<marker>Hasida, 1998</marker>
<rawString>Hasida, K. (1998) Intellectual contents of all-round based on GDA meaning modification. The transaction of Japanese Society for Artificial Intelligence., Vol. 13, No.4, pp.528-535 (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>JEITA</author>
</authors>
<title>Servey Report about natural language processing system.</title>
<date>2001</date>
<pages>49--56</pages>
<note>Japanese).</note>
<contexts>
<context position="5571" citStr="JEITA 2001" startWordPosition="903" endWordPosition="904">o 61.90 sec. &lt;q who=”A”&gt; &lt;su id=”kakure”&gt; &lt;vp syn=”f”&gt; &lt;n arg=&amp;quot;X&amp;quot; btime=”60.815” etime=&amp;quot;61.034”&gt; &lt;/n&gt; &lt;ad sem=”obj”&gt; &lt;tst val=”61.100”/&gt;&lt;/ad&gt; &lt;adp syn=”f”&gt; &lt;v&gt; &lt;tst val=&amp;quot;61.907&amp;quot;/&gt;&lt;/v&gt; &lt;ad&gt; &lt;tst val=&amp;quot;62.192&amp;quot;/&gt;&lt;/ad&gt; &lt;/adp&gt; &lt;v&gt; &lt;tst val=&amp;quot;62.383&amp;quot;/&gt;&lt;/v&gt; &lt;/vp&gt; &lt;/su&gt; &lt;/q&gt; Figure 2: An example of GDA corpus with timestamp These time annotation is often inserted after GDA tagging, because time information is independent from the standard syntactic/pragmatic GDA. 3 An annotation tool for multimodal dialogue corpus We have developed a multimodal annotation tool for a video corpus and its annotated data (JEITA 2001). This tool runs on any platform that accommodates Java2 and Java Media Framework (JMF2) ver.2.0 or higher. It also requires Java-based XML parser Xerces-J 3 and Java-based XQL engine 2 http://java.sun.com/products/java-media/jmf/ 3 http://xml.apache.org/xerces-j/ GMD-IPSI XQL4. Moreover, users can easily extend functions by mounting plug-ins. The prototype is equipped with two different types of plugins. One is “XQL search plug-in” in which the user can search for various syntactic and semantic patterns in a GDA corpus. An XQL format query as well as a plain word is allowed for search pattern</context>
</contexts>
<marker>JEITA, 2001</marker>
<rawString>JEITA (2001) Servey Report about natural language processing system. pp.49-56(in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>K Hasida</author>
</authors>
<title>Proposal of answer extraction system using GDA tag. The 7th annual meeting of Language Processing Society</title>
<date>2001</date>
<note>(in Japanese).</note>
<marker>Suzuki, Hasida, 2001</marker>
<rawString>J. Suzuki and K. Hasida (2001) Proposal of answer extraction system using GDA tag. The 7th annual meeting of Language Processing Society (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
<author>Friesen</author>
</authors>
<title>Facial action coding system : a technique for the measurement of facialmovement, Consulting Psychologists Press,</title>
<date>1978</date>
<marker>Ekman, Friesen, 1978</marker>
<rawString>Ekman, P. and Friesen, W.(1978) Facial action coding system : a technique for the measurement of facialmovement, Consulting Psychologists Press, 1978</rawString>
</citation>
<citation valid="false">
<institution>J.J.Venditti,(1995) Japanese ToBI Labelling Guidelines. Ohio-State University,Columbus,U.S.A.,1995.</institution>
<marker></marker>
<rawString>J.J.Venditti,(1995) Japanese ToBI Labelling Guidelines. Ohio-State University,Columbus,U.S.A.,1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kipp</author>
</authors>
<title>Anvil - A Generic Annotation Tool for Multimodal Dialogue,</title>
<date>2001</date>
<booktitle>Proceedings of Eurospeech</booktitle>
<pages>1367--1370</pages>
<contexts>
<context position="14233" citStr="Kipp, 2001" startWordPosition="2371" endWordPosition="2372">’s implemation. Speed and stability for the tool are both still problematic for real annotation. Also, the generic approach increases the initial effort to set up the tool since you basically have to write your own tool using the MATE style language. EUDICO (Brugman et al. 2000) is an effort to allow multi-user annotation of a centrally localted corpus via a web-based interface. The tools that are available to work with the multimodal corpus make it possible to analyze their content and to add new free defined annotations to them. A EUDICO client can choose a subset of the corpus data. Anvil (Kipp, 2001) is a generic video annotation tool which allows frame-accurate, hierarchical multi-layered annotation with objects that contain attribute-value pairs. Layers and attributes are all user-defineable. A time-aligned view and some configuration options make coding work quite intuitive. ATLAS project (Steven, etc. 2000) deals with all types of annotation and is theoretically based on the idea of annotation graphs where nodes represent time points and edges indicate annotation labels. 8 Conclusion We have reported an annotation tool for multimodal dialogue corpora. This tool enables semantic and pr</context>
</contexts>
<marker>Kipp, 2001</marker>
<rawString>Michael Kipp (2001) Anvil - A Generic Annotation Tool for Multimodal Dialogue, Proceedings of Eurospeech 2001, pp.1367-1370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>A Isard</author>
</authors>
<title>The MATE Annotation Workbench,</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL Workshop, Towards Standards and Tools for Discourse Tagging.,</booktitle>
<pages>11--17</pages>
<marker>Carletta, Isard, 1999</marker>
<rawString>Carletta, J. and Isard, A (1999) The MATE Annotation Workbench, In Proceedings of the ACL Workshop, Towards Standards and Tools for Discourse Tagging., pp.11-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Brugman</author>
<author>A Russel</author>
<author>D Broeder</author>
</authors>
<title>and P.Wittenburg (2000) EUDICO. Annotation and Exploitation of Multi Media Corpora,</title>
<date>2000</date>
<booktitle>Proceedings of LREC</booktitle>
<note>Workshop.</note>
<contexts>
<context position="13901" citStr="Brugman et al. 2000" startWordPosition="2312" endWordPosition="2315">machine and analyzes them. 7 Related works Most of recent multilmodal annotation tool projects are almost Java-based, use XML for file exchange and have an object-oriented design: MATE (Carletta, etc. 1999) is an annotation workbench that allows highly generic specification via stylesheets that determine shape and functionality of the user’s implemation. Speed and stability for the tool are both still problematic for real annotation. Also, the generic approach increases the initial effort to set up the tool since you basically have to write your own tool using the MATE style language. EUDICO (Brugman et al. 2000) is an effort to allow multi-user annotation of a centrally localted corpus via a web-based interface. The tools that are available to work with the multimodal corpus make it possible to analyze their content and to add new free defined annotations to them. A EUDICO client can choose a subset of the corpus data. Anvil (Kipp, 2001) is a generic video annotation tool which allows frame-accurate, hierarchical multi-layered annotation with objects that contain attribute-value pairs. Layers and attributes are all user-defineable. A time-aligned view and some configuration options make coding work q</context>
</contexts>
<marker>Brugman, Russel, Broeder, 2000</marker>
<rawString>H. Brugman, A. Russel, D. Broeder, and P.Wittenburg (2000) EUDICO. Annotation and Exploitation of Multi Media Corpora, Proceedings of LREC 2000 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>David Day</author>
<author>John Garofolo</author>
<author>John Henderson</author>
<author>Christophe Laprun</author>
<author>Mark Liberman</author>
</authors>
<title>ATLAS: A Flexible and Extensible Architecture for Linguistic Annotation,</title>
<date>2000</date>
<booktitle>Proceedings of the Second International Conference on Language Resource and Evaluation,</booktitle>
<pages>1699--1706</pages>
<marker>Bird, Day, Garofolo, Henderson, Laprun, Liberman, 2000</marker>
<rawString>Steven Bird, David Day, John Garofolo, John Henderson, Christophe Laprun, and Mark Liberman (2000) ATLAS: A Flexible and Extensible Architecture for Linguistic Annotation, Proceedings of the Second International Conference on Language Resource and Evaluation, pp.1699-1706.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>