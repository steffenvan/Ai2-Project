<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.275206">
<title confidence="0.968095">
Lexical Normalisation of Short Text Messages: Makn Sens a #twitter
</title>
<author confidence="0.996223">
Bo Han and Timothy Baldwin
</author>
<affiliation confidence="0.997230333333333">
NICTA Victoria Research Laboratory
Department of Computer Science and Software Engineering
The University of Melbourne
</affiliation>
<email confidence="0.996023">
hanb@student.unimelb.edu.au tb@ldwin.net
</email>
<sectionHeader confidence="0.995601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999548">
Twitter provides access to large volumes of
data in real time, but is notoriously noisy,
hampering its utility for NLP. In this paper, we
target out-of-vocabulary words in short text
messages and propose a method for identify-
ing and normalising ill-formed words. Our
method uses a classifier to detect ill-formed
words, and generates correction candidates
based on morphophonemic similarity. Both
word similarity and context are then exploited
to select the most probable correction can-
didate for the word. The proposed method
doesn’t require any annotations, and achieves
state-of-the-art performance over an SMS cor-
pus and a novel dataset based on Twitter.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999143196078431">
Twitter and other micro-blogging services are highly
attractive for information extraction and text mining
purposes, as they offer large volumes of real-time
data, with around 65 millions tweets posted on Twit-
ter per day in June 2010 (Twitter, 2010). The quality
of messages varies significantly, however, ranging
from high quality newswire-like text to meaningless
strings. Typos, ad hoc abbreviations, phonetic sub-
stitutions, ungrammatical structures and emoticons
abound in short text messages, causing grief for text
processing tools (Sproat et al., 2001; Ritter et al.,
2010). For instance, presented with the input u must
be talkin bout the paper but I was thinkin movies
(“You must be talking about the paper but I was
thinking movies”),&apos; the Stanford parser (Klein and
&apos;Throughout the paper, we will provide a normalised version
of examples as a gloss in double quotes.
Manning, 2003; de Marneffe et al., 2006) analyses
bout the paper and thinkin movies as a clause and
noun phrase, respectively, rather than a prepositional
phrase and verb phrase. If there were some way of
preprocessing the message to produce a more canon-
ical lexical rendering, we would expect the quality
of the parser to improve appreciably. Our aim in this
paper is this task of lexical normalisation of noisy
English text, with a particular focus on Twitter and
SMS messages. In this paper, we will collectively
refer to individual instances of typos, ad hoc abbre-
viations, unconventional spellings, phonetic substi-
tutions and other causes of lexical deviation as “ill-
formed words”.
The message normalisation task is challenging.
It has similarities with spell checking (Peterson,
1980), but differs in that ill-formedness in text mes-
sages is often intentional, whether due to the desire
to save characters/keystrokes, for social identity, or
due to convention in this text sub-genre. We propose
to go beyond spell checkers, in performing deabbre-
viation when appropriate, and recovering the canon-
ical word form of commonplace shorthands like b4
“before”, which tend to be considered beyond the
remit of spell checking (Aw et al., 2006). The free
writing style of text messages makes the task even
more complex, e.g. with word lengthening such as
goooood being commonplace for emphasis. In ad-
dition, the detection of ill-formed words is difficult
due to noisy context.
Our objective is to restore ill-formed words to
their canonical lexical forms in standard English.
Through a pilot study, we compared OOV words in
Twitter and SMS data with other domain corpora,
</bodyText>
<page confidence="0.976221">
368
</page>
<note confidence="0.979598">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 368–378,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996042">
revealing their characteristics in OOV word distri-
bution. We found Twitter data to have an unsur-
prisingly long tail of OOV words, suggesting that
conventional supervised learning will not perform
well due to data sparsity. Additionally, many ill-
formed words are ambiguous, and require context
to disambiguate. For example, Gooood may refer to
Good or God depending on context. This provides
the motivation to develop a method which does not
require annotated training data, but is able to lever-
age context for lexical normalisation. Our approach
first generates a list of candidate canonical lexical
forms, based on morphological and phonetic vari-
ation. Then, all candidates are ranked according
to a list of features generated from noisy context
and similarity between ill-formed words and can-
didates. Our proposed cascaded method is shown
to achieve state-of-the-art results on both SMS and
Twitter data.
Our contributions in this paper are as follows: (1)
we conduct a pilot study on the OOV word distri-
bution of Twitter and other text genres, and anal-
yse different sources of non-standard orthography in
Twitter; (2) we generate a text normalisation dataset
based on Twitter data; (3) we propose a novel nor-
malisation approach that exploits dictionary lookup,
word similarity and word context, without requir-
ing annotated data; and (4) we demonstrate that our
method achieves state-of-the-art accuracy over both
SMS and Twitter data.
</bodyText>
<sectionHeader confidence="0.999496" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999962516129032">
The noisy channel model (Shannon, 1948) has tradi-
tionally been the primary approach to tackling text
normalisation. Suppose the ill-formed text is T
and its corresponding standard form is S, the ap-
proach aims to find arg max P(S|T) by comput-
ing arg maxP(T|S)P(S), in which P(S) is usu-
ally a language model and P (T |S) is an error model.
Brill and Moore (2000) characterise the error model
by computing the product of operation probabilities
on slice-by-slice string edits. Toutanova and Moore
(2002) improve the model by incorporating pronun-
ciation information. Choudhury et al. (2007) model
the word-level text generation process for SMS mes-
sages, by considering graphemic/phonetic abbrevi-
ations and unintentional typos as hidden Markov
model (HMM) state transitions and emissions, re-
spectively (Rabiner, 1989). Cook and Stevenson
(2009) expand the error model by introducing infer-
ence from different erroneous formation processes,
according to the sampled error distribution. While
the noisy channel model is appropriate for text nor-
malisation, P(T|S), which encodes the underlying
error production process, is hard to approximate
accurately. Additionally, these methods make the
strong assumption that a token tz E T only depends
on sz E S, ignoring the context around the token,
which could be utilised to help in resolving ambigu-
ity.
Statistical machine translation (SMT) has been
proposed as a means of context-sensitive text nor-
malisation, by treating the ill-formed text as the
source language, and the standard form as the target
language. For example, Aw et al. (2006) propose a
phrase-level SMT SMS normalisation method with
bootstrapped phrase alignments. SMT approaches
tend to suffer from a critical lack of training data,
however. It is labor intensive to construct an anno-
tated corpus to sufficiently cover ill-formed words
and context-appropriate corrections. Furthermore,
it is hard to harness SMT for the lexical normali-
sation problem, as even if phrase-level re-ordering
is suppressed by constraints on phrase segmenta-
tion, word-level re-orderings within a phrase are still
prevalent.
Some researchers have also formulated text nor-
malisation as a speech recognition problem. For ex-
ample, Kobus et al. (2008) firstly convert input text
tokens into phonetic tokens and then restore them to
words by phonetic dictionary lookup. Beaufort et al.
(2010) use finite state methods to perform French
SMS normalisation, combining the advantages of
SMT and the noisy channel model. Kaufmann and
Kalita (2010) exploit a machine translation approach
with a preprocessor for syntactic (rather than lexical)
normalisation.
Predominantly, however, these methods require
large-scale annotated training data, limiting their
adaptability to new domains or languages. In con-
trast, our proposed method doesn’t require annotated
data. It builds on the work on SMS text normalisa-
tion, and adapts it to Twitter data, exploiting multi-
ple data sources for normalisation.
</bodyText>
<page confidence="0.999193">
369
</page>
<figureCaption confidence="0.990147">
Figure 1: Out-of-vocabulary word distribution in English Gigaword (NYT), Twitter and SMS data
</figureCaption>
<sectionHeader confidence="0.975178" genericHeader="method">
3 Scoping Text Normalisation
</sectionHeader>
<subsectionHeader confidence="0.999818">
3.1 Task Definition of Lexical Normalisation
</subsectionHeader>
<bodyText confidence="0.9994232">
We define the task of text normalisation to be a map-
ping from “ill-formed” OOV lexical items to their
standard lexical forms, focusing exclusively on En-
glish for the purposes of this paper. We define the
task as follows:
</bodyText>
<listItem confidence="0.9879737">
• only OOV words are considered for normalisa-
tion;
• normalisation must be to a single-token word,
meaning that we would normalise smokin to
smoking, but not imo to in my opinion; a side-
effect of this is to permit lower-register contrac-
tions such as gonna as the canonical form of
gunna (given that going to is out of scope as a
normalisation candidate, on the grounds of be-
ing multi-token).
</listItem>
<bodyText confidence="0.999779948717949">
Given this definition, our first step is to identify
candidate tokens for lexical normalisation, where
we examine all tokens that consist of alphanumeric
characters, and categorise them into in-vocabulary
(IV) and out-of-vocabulary (OOV) words, relative to
a dictionary. The OOV word definition is somewhat
rough, because it includes neologisms and proper
nouns like hopeable or WikiLeaks which have not
made their way into the dictionary. However, it
greatly simplifies the candidate identification task,
at the cost of pushing complexity downstream to
the word detection task, in that we need to explic-
itly distinguish between correct OOV words and ill-
formed OOV words such as typos (e.g. earthquak
“earthquake”), register-specific single-word abbre-
viations (e.g. lv “love”), and phonetic substitutions
(e.g. 2morrow “tomorrow”).
An immediate implication of our task definition is
that ill-formed words which happen to coincide with
an IV word (e.g. the misspelling of can’t as cant) are
outside the scope of this research. We also consider
that deabbreviation largely falls outside the scope of
text normalisation, as abbreviations can be formed
freely in standard English. Note that single-word
abbreviations such as govt “government” are very
much within the scope of lexical normalisation, as
they are OOV and match to a single token in their
standard lexical form.
Throughout this paper, we use the GNU aspell
dictionary (v0.60.6)2 to determine whether a token
is OOV. In tokenising the text, hyphenanted tokens
and tokens containing apostrophes (e.g. take-off and
won’t, resp.) are treated as a single token. Twit-
ter mentions (e.g. @twitter), hashtags (e.g. #twitter)
and urls (e.g. twitter.com) are excluded from consid-
eration for normalisation, but left in situ for context
modelling purposes. Dictionary lookup of Internet
slang is performed relative to a dictionary of 5021
items collected from the Internet.3
</bodyText>
<subsectionHeader confidence="0.998763">
3.2 OOV Word Distribution and Types
</subsectionHeader>
<bodyText confidence="0.999988875">
To get a sense of the relative need for lexical nor-
malisation, we perform analysis of the distribution
of OOV words in different text types. In particular,
we calculate the proportion of OOV tokens per mes-
sage (or sentence, in the case of edited text), bin the
messages according to the OOV token proportion,
and plot the probability mass contained in each bin
for a given text type. The three corpora we compare
</bodyText>
<footnote confidence="0.998998">
2We remove all one character tokens, except a and I, and
treat RT as an IV word.
3http://www.noslang.com
</footnote>
<page confidence="0.996841">
370
</page>
<bodyText confidence="0.99997056097561">
are the New York Times (NYT),4 SMS,5 and Twit-
ter.6 The results are presented in Figure 1.
Both SMS and Twitter have a relatively flat distri-
bution, with Twitter having a particularly large tail:
around 15% of tweets have 50% or more OOV to-
kens. This has implications for any context mod-
elling, as we cannot rely on having only isolated oc-
currences of OOV words. In contrast, NYT shows a
more Zipfian distribution, despite the large number
of proper names it contains.
While this analysis confirms that Twitter and SMS
are similar in being heavily laden with OOV tokens,
it does not shed any light on the relative similarity in
the makeup of OOV tokens in each case. To further
analyse the two data sources, we extracted the set
of OOV terms found exclusively in SMS and Twit-
ter, and analysed each. Manual analysis of the two
sets revealed that most OOV words found only in
SMS were personal names. The Twitter-specific set,
on the other hand, contained a heterogeneous col-
lection of ill-formed words and proper nouns. This
suggests that Twitter is a richer/noisier data source,
and that text normalisation for Twitter needs to be
more nuanced than for SMS.
To further analyse the ill-formed words in Twit-
ter, we randomly selected 449 tweets and manu-
ally analysed the sources of lexical variation, to
determine the phenomena that lexical normalisa-
tion needs to deal with. We identified 254 to-
ken instances of lexical normalisation, and broke
them down into categories, as listed in Table 1.
“Letter” refers to instances where letters are miss-
ing or there are extraneous letters, but the lexi-
cal correspondence to the target word form is triv-
ially accessible (e.g. shuld “should”). “Number
Substitution” refers to instances of letter–number
substitution, where numbers have been substituted
for phonetically-similar sequences of letters (e.g. 4
“for”). “Letter&amp;Number” refers to instances which
have both extra/missing letters and number substitu-
tion (e.g. b4 “before”). “Slang” refers to instances
</bodyText>
<footnote confidence="0.989297571428571">
4Based on 44 million sentences from English Gigaword.
5Based on 12.6 thousand SMS messages from How and Kan
(2005) and Choudhury et al. (2007).
6Based on 1.37 million tweets collected from the Twitter
streaming API from Aug to Oct 2010, and filtered for mono-
lingual English messages; see Section 5.1 for details of the lan-
guage filtering methodology.
</footnote>
<table confidence="0.995984833333333">
Category Ratio
Letter&amp;Number 2.36%
Letter 72.44%
Number Substitution 2.76%
Slang 12.20%
Other 10.24%
</table>
<tableCaption confidence="0.999921">
Table 1: Ill-formed word distribution
</tableCaption>
<bodyText confidence="0.999871357142857">
of Internet slang (e.g. lol “laugh out loud”), as found
in a slang dictionary (see Section 3.1). “Other” is
the remainder of the instances, which is predomi-
nantly made up of occurrences of spaces having be-
ing deleted between words (e.g. sucha “such a”). If
a given instance belongs to multiple error categories
(e.g. “Letter&amp;Number” and it is also found in a slang
dictionary), we classify it into the higher-occurring
category in Table 1.
From Table 1, it is clear that “Letter” accounts
for the majority of ill-formed words in Twitter, and
that most ill-formed words are based on morpho-
phonemic variations. This empirical finding assists
in shaping our strategy for lexical normalisation.
</bodyText>
<sectionHeader confidence="0.977562" genericHeader="method">
4 Lexical normalisation
</sectionHeader>
<bodyText confidence="0.999986294117647">
Our proposed lexical normalisation strategy in-
volves three general steps: (1) confusion set gen-
eration, where we identify normalisation candidates
for a given word; (2) ill-formed word identification,
where we classify a word as being ill-formed or not,
relative to its confusion set; and (3) candidate selec-
tion, where we select the standard form for tokens
which have been classified as being ill formed. In
confusion set generation, we generate a set of IV
normalisation candidates for each OOV word type
based on morphophonemic variation. We call this
set the confusion set of that OOV word, and aim to
include all feasible normalisation candidates for the
word type in the confusion set. The confusion can-
didates are then filtered for each token occurrence of
a given OOV word, based on their local context fit
with a language model.
</bodyText>
<subsectionHeader confidence="0.998378">
4.1 Confusion Set Generation
</subsectionHeader>
<bodyText confidence="0.999884">
Revisiting our manual analysis from Section 3.2,
most ill-formed tokens in Twitter are morphophone-
mically derived. First, inspired by Kaufmann and
Kalita (2010), any repititions of more than 3 let-
ters are reduced back to 3 letters (e.g. cooool is re-
</bodyText>
<page confidence="0.994916">
371
</page>
<table confidence="0.999756">
Criterion Recall Average
Candidates
T� &lt; 1 40.4% 24
T� &lt; 2 76.6% 240
T, = 0 55.4% 65
T, &lt; 1 83.4% 1248
T, &lt; 2 91.0% 9694
T� &lt; 2 V T, &lt; 1 88.8% 1269
T� &lt; 2 V T, &lt; 2 92.7% 9515
</table>
<tableCaption confidence="0.996114">
Table 2: Recall and average number of candidates for dif-
ferent confusion set generation strategies
</tableCaption>
<bodyText confidence="0.999798071428571">
duced to coool). Second, IV words within a thresh-
old T, character edit distance of the given OOV
word are calculated, as is widely used in spell check-
ers. Third, the double metaphone algorithm (Philips,
2000) is used to decode the pronunciation of all IV
words, and IV words within a threshold Tp edit dis-
tance of the given OOV word under phonemic tran-
scription, are included in the confusion set; this al-
lows us to capture OOV words such as earthquick
“earthquake”. In Table 2, we list the recall and av-
erage size of the confusion set generated by the fi-
nal two strategies with different threshold settings,
based on our evaluation dataset (see Section 5.1).
The recall for lexical edit distance with T, &lt; 2 is
moderately high, but it is unable to detect the correct
candidate for about one quarter of words. The com-
bination of the lexical and phonemic strategies with
T, &lt; 2 V Tp &lt; 2 is more impressive, but the number
of candidates has also soared. Note that increasing
the edit distance further in both cases leads to an ex-
plosion in the average number of candidates, with
serious computational implications for downstream
processing. Thankfully, T, &lt; 2 V Tp &lt; 1 leads to an
extra increment in recall to 88.8%, with only a slight
increase in the average number of candidates. Based
on these results, we use T, &lt; 2VTp &lt; 1 as the basis
for confusion set generation.
Examples of ill-formed words where we are un-
able to generate the standard lexical form are clip-
pings such as fav “favourite” and convo “conversa-
tion”.
In addition to generating the confusion set, we
rank the candidates based on a trigram language
model trained over 1.5GB of clean Twitter data, i.e.
tweets which consist of all IV words: despite the
prevalence of OOV words in Twitter, the sheer vol-
ume of the data means that it is relatively easy to col-
lect large amounts of all-IV messages. To train the
language model, we used SRILM (Stolcke, 2002)
with the -&lt;unk&gt; option. If we truncate the ranking
to the top 10% of candidates, the recall drops back
to 84% with a 90% reduction in candidates.
</bodyText>
<subsectionHeader confidence="0.970595">
4.2 Ill-formed Word Detection
</subsectionHeader>
<bodyText confidence="0.999971775">
The next step is to detect whether a given OOV word
in context is actually an ill-formed word or not, rel-
ative to its confusion set. To the best of our knowl-
edge, we are the first to target the task of ill-formed
word detection in the context of short text messages,
although related work exists for text with lower rel-
ative occurrences of OOV words (Izumi et al., 2003;
Sun et al., 2007). Due to the noisiness of the data, it
is impractical to use full-blown syntactic or seman-
tic features. The most direct source of evidence is
IV words around an OOV word. Inspired by work
on labelled sequential pattern extraction (Sun et al.,
2007), we exploit large-scale edited corpus data to
construct dependency-based features.
First, we use the Stanford parser (Klein and Man-
ning, 2003; de Marneffe et al., 2006) to extract de-
pendencies from the NYT corpus (see Section 3.2).
For example, from a sentence such as One obvious
difference is the way they look, we would extract
dependencies such as rcmod(way-6,look-8)
and nsubj(look-8,they-7). We then trans-
form the dependencies into relational features for
each OOV word. Assuming that way were an OOV
word, e.g., we would extract dependencies of the
form (look,way,+2), indicating that look oc-
curs 2 words after way. We choose dependencies to
represent context because they are an effective way
of capturing key relationships between words, and
similar features can easily be extracted from tweets.
Note that we don’t record the dependency type here,
because we have no intention of dependency parsing
text messages, due to their noisiness and the volume
of the data. The counts of dependency forms are
combined together to derive a confidence score, and
the scored dependencies are stored in a dependency
bank.
Given the dependency-based features, a linear
kernel SVM classifier (Fan et al., 2008) is trained
on clean Twitter data, i.e. the subset of Twitter mes-
sages without OOV words. Each word is repre-
</bodyText>
<page confidence="0.992582">
372
</page>
<bodyText confidence="0.999501474576271">
sented by its IV words within a context window
of three words to either side of the target word,
together with their relative positions in the form
of (word1,word2,position) tuples, and their
score in the dependency bank. These form the pos-
itive training exemplars. Negative exemplars are
automatically constructed by replacing target words
with highly-ranked candidates from their confusion
set. Note that the classifier does not require any hand
annotation, as all training exemplars are constructed
automatically.
To predict whether a given OOV word is
ill-formed, we form an exemplar for each
of its confusion candidates, and extract
(word1,word2,position) features. If
all its candidates are predicted to be negative by the
model, we mark it as correct; otherwise, we treat
it as ill-formed, and pass all candidates (not just
positively-classified candidates) on to the candidate
selection step. For example, given the message
way yu lookin shuld be a sin and the OOV word
lookin, we would generate context features for each
candidate word such as (way,looking,-2),
and classify each such candidate.
In training, it is possible for the exact same fea-
ture vector to occur as both positive and negative ex-
emplars. To prevent positive exemplars being con-
taminated from the automatic generation, we re-
move all negative instances in such cases. The
(word1,word2,position) features are sparse
and sometimes lead to conservative results in ill-
formed word detection. That is, without valid fea-
tures, the SVM classifier tends to label uncertain
cases as correct rather than ill-formed words. This
is arguably the right approach to normalisation, in
choosing to under- rather than over-normalise in
cases of uncertainty.
As the context for a target word often contains
OOV words which don’t occur in the dependency
bank, we expand the dependency features to include
context tokens up to a phonemic edit distance of 1
from context tokens in the dependency bank. In
this way, we generate dependency-based features
for context words such as seee “see” in (seee,
flm, +2) (based on the target word flm in the
context of flm to seee). However, expanded depen-
dency features may introduce noise, and we there-
fore introduce expanded dependency weights wd E
10.0, 0.5, 1.0} to ameliorate the effects of noise: a
weight of wd = 0.0 means no expansion, while 1.0
means expanded dependencies are indistinguishable
from non-expanded (strict match) dependencies.
We separately introduce a threshold td E
11, 2, ...,10} on the number of positive predictions
returned by the detection classifier over the set of
normalisation candidates for a given OOV token: the
token is considered to be ill-formed iff td or more
candidates are positively classified, i.e. predicted to
be correct candidates.
</bodyText>
<subsectionHeader confidence="0.999334">
4.3 Candidate Selection
</subsectionHeader>
<bodyText confidence="0.999948942857143">
For OOV words which are predicted to be ill-
formed, we select the most likely candidate from the
confusion set as the basis of normalisation. The final
selection is based on the following features, in line
with previous work (Wong et al., 2006; Cook and
Stevenson, 2009).
Lexical edit distance, phonemic edit distance,
prefix substring, suffix substring, and the longest
common subsequence (LCS) are exploited to cap-
ture morphophonemic similarity. Both lexical and
phonemic edit distance (ED) are normalised by the
reciprocal of exp(ED). The prefix and suffix fea-
tures are intended to capture the fact that leading
and trailing characters are frequently dropped from
words, e.g. in cases such as ish and talkin. We cal-
culate the ratio of the LCS over the maximum string
length between ill-formed word and the candidate,
since the ill-formed word can be either longer or
shorter than (or the same size as) the standard form.
For example, mve can be restored to either me or
move, depending on context. We normalise these ra-
tios following Cook and Stevenson (2009).
For context inference, we employ both language
model- and dependency-based frequency features.
Ranking by language model score is intuitively ap-
pealing for candidate selection, but our trigram
model is trained only on clean Twitter data and ill-
formed words often don’t have sufficient context for
the language model to operate effectively, as in bt
“but” in say 2 sum1 bt nt gonna say “say to some-
one but not going to say”. To consolidate the con-
text modelling, we obtain dependencies from the de-
pendency bank used in ill-formed word detection.
Although text messages are of a different genre to
edited newswire text, we assume they form similar
</bodyText>
<page confidence="0.998426">
373
</page>
<bodyText confidence="0.999868">
dependencies based on the common goal of getting
across the message effectively. The dependency fea-
tures can be used in noisy contexts and are robust
to the effects of other ill-formed words, as they do
not rely on contiguity. For example, uz “use” in i
did #tt uz me and yu, dependencies can capture rela-
tionships like aux(use-4, do-2), which is be-
yond the capabilities of the language model due to
the hashtag being treated as a correct OOV word.
</bodyText>
<sectionHeader confidence="0.999831" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999069">
5.1 Dataset and baselines
</subsectionHeader>
<bodyText confidence="0.9966851875">
The aim of our experiments is to compare the effec-
tiveness of different methodologies over text mes-
sages, based on two datasets: (1) an SMS corpus
(Choudhury et al., 2007); and (2) a novel Twitter
dataset developed as part of this research, based on
a random sampling of 549 English tweets. The En-
glish tweets were annotated by three independent
annotators. All OOV words were pre-identified,
and the annotators were requested to determine: (a)
whether each OOV word was ill-formed or not; and
(b) what the standard form was for ill-formed words,
subject to the task definition outlined in Section 3.1.
The total number of ill-formed words contained in
the SMS and Twitter datasets were 3849 and 1184,
respectively.&apos;
The language filtering of Twitter to automatically
identify English tweets was based on the language
identification method of Baldwin and Lui (2010),
using the EuroGOV dataset as training data, a mixed
unigram/bigram/trigram byte feature representation,
and a skew divergence nearest prototype classifier.
We reimplemented the state-of-art noisy channel
model of Cook and Stevenson (2009) and SMT ap-
proach of Aw et al. (2006) as benchmark meth-
ods. We implement the SMT approach in Moses
(Koehn et al., 2007), with synthetic training and
tuning data of 90,000 and 1000 sentence pairs, re-
spectively. This data is randomly sampled from the
1.5GB of clean Twitter data, and errors are gener-
ated according to distribution of SMS corpus. The
10-fold cross-validated BLEU score (Papineni et al.,
2002) over this data is 0.81.
</bodyText>
<footnote confidence="0.930003333333333">
7The Twitter dataset is available at http://www.
csse.unimelb.edu.au/research/lt/resources/
lexnorm/.
</footnote>
<bodyText confidence="0.999896">
In addition to comparing our method with com-
petitor methods, we also study the contribution of
different feature groups. We separately compare dic-
tionary lookup over our Internet slang dictionary,
the contextual feature model, and the word similar-
ity feature model, as well as combinations of these
three.
</bodyText>
<subsectionHeader confidence="0.998977">
5.2 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.9999526">
The evaluation of lexical normalisation consists of
two stages (Hirst and Budanitsky, 2005): (1) ill-
formed word detection, and (2) candidate selection.
In terms of detection, we want to make sense of how
well the system can identify ill-formed words and
leave correct OOV words untouched. This step is
crucial to further normalisation, because if correct
OOV words are identified as ill-formed, the candi-
date selection step can never be correct. Conversely,
if an ill-formed word is predicted to be correct, the
candidate selection will have no chance to normalise
it.
We evaluate detection performance by token-level
precision, recall and F-score (Q = 1). Previous work
over the SMS corpus has assumed perfect ill-formed
word detection and focused only on the candidate
selection step, so we evaluate ill-formed word de-
tection for the Twitter data only.
For candidate selection, we once again evalu-
ate using token-level precision, recall and F-score.
Additionally, we evaluate using the BLEU score
over the normalised form of each message, as the
SMT method can lead to perturbations of the token
stream, vexing standard precision, recall and F-score
evaluation.
</bodyText>
<subsectionHeader confidence="0.984366">
5.3 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999973">
First, we test the impact of the wd and td values
on ill-formed word detection effectiveness, based on
dependencies from either the Spinn3r blog corpus
(Blog: Burton et al. (2009)) or NYT. The results for
precision, recall and F-score are presented in Fig-
ure 2.
Some conclusions can be drawn from the graphs.
First, higher detection threshold values (td) give bet-
ter precision but lower recall. Generally, as td is
raised from 1 to 10, the precision improves slightly
but recall drops dramatically, with the net effect that
the F-score decreases monotonically. Thus, we use a
</bodyText>
<page confidence="0.998178">
374
</page>
<figureCaption confidence="0.994879">
Figure 2: Ill-formed word detection precision, recall and
F-score
</figureCaption>
<bodyText confidence="0.999984233766234">
smaller threshold, i.e. td = 1. Second, there are dif-
ferences between the two corpora, with dependen-
cies from the Blog corpus producing slightly lower
precision but higher recall, compared with the NYT
corpus. The lower precision for the Blog corpus ap-
pears to be due to the text not being as clean as NYT,
introducing parser errors. Nevertheless, the differ-
ence in F-score between the two corpora is insignif-
icant. Third, we obtain the best results, especially
in terms of precision, for wd = 0.5, i.e. with ex-
panded dependencies, but penalised relative to non-
expanded dependencies.
Overall, the best F-score is 71.2%, with a preci-
sion of 61.1% and recall of 85.3%, obtained over
the Blog corpus with td = 1 and wd = 0.5. Clearly
there is significant room for immprovements in these
results. We leave the improvement of ill-formed
word detection for future work, and perform eval-
uation of candidate selection for Twitter assuming
perfect ill-formed word detection, as for the SMS
data.
From Table 3, we see that the general perfor-
mance of our proposed method on Twitter is better
than that on SMS. To better understand this trend,
we examined the annotations in the SMS corpus, and
found them to be looser than ours, because they have
different task specifications than our lexical normal-
isation. In our annotation, the annotators only nor-
malised ill-formed word if they had high confidence
of how to normalise, as with talkin “talking”. For
ill-formed words where they couldn’t be certain of
the standard form, the tokens were left untouched.
However, in the SMS corpus, annotations such as
sammis “same” are also included. This leads to a
performance drop for our method over the SMS cor-
pus.
The noisy channel method of Cook and Stevenson
(2009) shares similar features with word similarity
(“WS”), However, when word similarity and con-
text support are combined (“WS+CS”), our method
outperforms the noisy channel method by about 7%
and 12% in F-score over SMS and Twitter corpora,
respectively. This can be explained as follows. First,
the Cook and Stevenson (2009) method is type-
based, so all token instances of a given ill-formed
word will be normalised identically. In the Twit-
ter data, however, the same word can be normalised
differently depending on context, e.g. hw “how” in
so hw many time remaining so I can calculate it?
vs. hw “homework” in I need to finish my hw first.
Second, the noisy channel method was developed
specifically for SMS normalisation, in which clip-
ping is the most prevalent form of lexical variation,
while in the Twitter data, we commonly have in-
stances of word lengthening for emphasis, such as
moviiie “movie”. Having said this, our method is
superior to the noisy channel method over both the
SMS and Twitter data.
The SMT approach is relatively stable on the two
datasets, but well below the performance of our
method. This is due to the limitations of the training
data: we obtain the ill-formed words and their stan-
dard forms from the SMS corpus, but the ill-formed
words in the SMS corpus are not sufficient to cover
those in the Twitter data (and we don’t have suffi-
cient Twitter data to train the SMT method directly).
Thus, novel ill-formed words are missed in normal-
isation. This shows the shortcoming of supervised
data-driven approaches that require annotated data
to cover all possibilities of ill-formed words in Twit-
ter.
The dictionary lookup method (“DL”) unsurpris-
ingly achieves the best precision, but the recall
on Twitter is not competitive. Consequently, the
Twitter normalisation cannot be tackled with dictio-
nary lookup alone, although it is an effective pre-
processing strategy when combined with more ro-
</bodyText>
<page confidence="0.995639">
375
</page>
<table confidence="0.999506222222222">
Dataset Evaluation NC MT DL WS CS WS+CS DL+WS+CS
SMS Precision 0.465 — 0.927 0.521 0.116 0.532 0.756
Recall 0.464 — 0.597 0.520 0.116 0.531 0.754
F-score 0.464 — 0.726 0.520 0.116 0.531 0.755
BLEU 0.746 0.700 0.801 0.764 0.612 0.772 0.876
Twitter Precision 0.452 — 0.961 0.551 0.194 0.571 0.753
Recall 0.452 — 0.460 0.551 0.194 0.571 0.753
F-score 0.452 — 0.622 0.551 0.194 0.571 0.753
BLEU 0.857 0.728 0.861 0.878 0.797 0.884 0.934
</table>
<tableCaption confidence="0.986105">
Table 3: Candidate selection effectiveness on different datasets (NC = noisy channel model (Cook and Stevenson,
2009); MT = SMT (Aw et al., 2006); DL = dictionary lookup; WS = word similarity; CS = context support)
</tableCaption>
<bodyText confidence="0.999874114285714">
bust techniques such as our proposed method, and
effective at capturing common abbreviations such as
gf “girlfriend”.
Of the component methods proposed in this re-
search, word similarity (“WS”) achieves higher pre-
cision and recall than context support (“CS”), sig-
nifying that many of the ill-formed words emanate
from morphophonemic variations. However, when
combined with word similarity features, context
support improves over the basic method at a level of
statistical significance (based on randomised estima-
tion, p &lt; 0.05: Yeh (2000)), indicating the comple-
mentarity of the two methods, especially on Twitter
data. The best F-score is achieved when combin-
ing dictionary lookup, word similarity and context
support (“DL+WS+CS”), in which ill-formed words
are first looked up in the slang dictionary, and only
if no match is found do we apply our normalisation
method.
We found several limitations in our proposed ap-
proach by analysing the output of our method. First,
not all ill-formed words offer useful context. Some
highly noisy tweets contain almost all misspellings
and unique symbols, and thus no context features
can be extracted. This also explains why “CS” fea-
tures often fail. For such cases, the method falls back
to context-independent normalisation. We found
that only 32.6% ill-formed words have all IV words
in their context windows. Moreover, the IV words
may not occur in the dependency bank, further de-
creasing the effectiveness of context support fea-
tures. Second, the different features are linearly
combined, where a weighted combination is likely
to give better results, although it also requires a cer-
tain amount of well-sampled annotations for tuning.
</bodyText>
<sectionHeader confidence="0.996631" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999979833333333">
In this paper, we have proposed the task of lexi-
cal normalisation for short text messages, as found
in Twitter and SMS data. We found that most ill-
formed words are based on morphophonemic varia-
tion and proposed a cascaded method to detect and
normalise ill-formed words. Our ill-formed word
detector requires no explicit annotations, and the
dependency-based features were shown to be some-
what effective, however, there was still a lot of
room for improvement at ill-formed word detection.
In normalisation, we compared our method with
two benchmark methods from the literature, and
achieved that highest F-score and BLEU score by
integrating dictionary lookup, word similarity and
context support modelling.
In future work, we propose to pursue a number of
directions. First, we plan to improve our ill-formed
word detection classifier by introducing an OOV
word whitelist. Furthermore, we intend to allevi-
ate noisy contexts with a bootstrapping approach, in
which ill-formed words with high confidence and no
ambiguity will be replaced by their standard forms,
and fed into the normalisation model as new training
data.
</bodyText>
<sectionHeader confidence="0.994792" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.90556675">
NICTA is funded by the Australian government as rep-
resented by Department of Broadband, Communication
and Digital Economy, and the Australian Research Coun-
cil through the ICT centre of Excellence programme.
</bodyText>
<sectionHeader confidence="0.991908" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.796305666666667">
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. In Proceedings of the 21st International Con-
</reference>
<page confidence="0.995044">
376
</page>
<reference confidence="0.989534457943925">
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 33–40, Sydney, Australia.
Timothy Baldwin and Marco Lui. 2010. Language iden-
tification: The long and the short of the matter. In
HLT ’10: Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
229–237, Los Angeles, USA.
Richard Beaufort, Sophie Roekhaut, Louise-Am´elie
Cougnon, and C´edrick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing SMS messages. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 770–779, Uppsala, Sweden.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
ACL ’00: Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, pages
286–293, Hong Kong.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia, San Jose, USA.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10:157–174.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
CALC ’09: Proceedings of the Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71–
78, Boulder, USA.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006),
Genoa, Italy.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexical
cohesion. Natural Language Engineering, 11:87–111.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In Human Computer Interfaces International
(HCII 05), Las Vegas, USA.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic er-
ror detection in the Japanese learners’ English spoken
data. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics - Volume 2,
pages 145–148, Sapporo, Japan.
Joseph Kaufmann and Jugal Kalita. 2010. Syntactic nor-
malization of Twitter messages. In International Con-
ference on Natural Language Processing, Kharagpur,
India.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15 (NIPS 2002), pages 3–10, Whistler,
Canada.
Catherine Kobus, Franois Yvon, and Graldine Damnati.
2008. Transcrire les SMS comme on reconnat la pa-
role. In Actes de la Confrence sur le Traitement Au-
tomatique des Langues (TALN’08), pages 128–138.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177–
180, Prague, Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311–318, Philadelphia, USA.
James L. Peterson. 1980. Computer programs for de-
tecting and correcting spelling errors. Commun. ACM,
23:676–687, December.
Lawrence Philips. 2000. The double metaphone search
algorithm. C/C++ Users Journal, 18:38–43.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257–286.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In HLT
’10: Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
172–180, Los Angeles, USA.
Claude Elwood Shannon. 1948. A mathematical the-
ory of communication. Bell System Technical Journal,
27:379–423, 623–656.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287 – 333.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
</reference>
<page confidence="0.980181">
377
</page>
<reference confidence="0.999427928571429">
ken Language Processing, pages 901–904, Denver,
USA.
Guihua Sun, Gao Cong, Xiaohua Liu, Chin-Yew Lin, and
Ming Zhou. 2007. Mining sequential patterns and tree
patterns to detect erroneous sentences. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 81–88, Prague, Czech
Republic.
Kristina Toutanova and Robert C. Moore. 2002. Pro-
nunciation modeling for improved spelling correction.
In Proceedings of the 40th Annual Meeting on Associ-
ation for Computational Linguistics, ACL ’02, pages
144–151, Philadelphia, USA.
Twitter. 2010. Big goals, big game, big records.
http://blog.twitter.com/2010/06/
big-goals-big-game-big-records.html.
Retrieved 4 August 2010.
Wilson Wong, Wei Liu, and Mohammed Bennamoun.
2006. Integrated scoring for spelling error correction,
abbreviation expansion and case restoration in dirty
text. In Proceedings of the Fifth Australasian Con-
ference on Data Mining and Analytics, pages 83–89,
Sydney, Australia.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th Conference on Computational Linguistics -
Volume 2, COLING ’00, pages 947–953, Saarbr¨ucken,
Germany.
</reference>
<page confidence="0.99828">
378
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.344350">
<title confidence="0.998514">Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</title>
<author confidence="0.831978">Han</author>
<affiliation confidence="0.924100666666667">NICTA Victoria Research Department of Computer Science and Software The University of</affiliation>
<abstract confidence="0.964499117647059">hanb@student.unimelb.edu.au tb@ldwin.net Abstract Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AiTi Aw</author>
<author>Min Zhang</author>
<author>Juan Xiao</author>
<author>Jian Su</author>
</authors>
<title>A phrase-based statistical model for SMS text normalization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3054" citStr="Aw et al., 2006" startWordPosition="467" endWordPosition="470">ons and other causes of lexical deviation as “illformed words”. The message normalisation task is challenging. It has similarities with spell checking (Peterson, 1980), but differs in that ill-formedness in text messages is often intentional, whether due to the desire to save characters/keystrokes, for social identity, or due to convention in this text sub-genre. We propose to go beyond spell checkers, in performing deabbreviation when appropriate, and recovering the canonical word form of commonplace shorthands like b4 “before”, which tend to be considered beyond the remit of spell checking (Aw et al., 2006). The free writing style of text messages makes the task even more complex, e.g. with word lengthening such as goooood being commonplace for emphasis. In addition, the detection of ill-formed words is difficult due to noisy context. Our objective is to restore ill-formed words to their canonical lexical forms in standard English. Through a pilot study, we compared OOV words in Twitter and SMS data with other domain corpora, 368 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 368–378, Portland, Oregon, June 19-24, 2011. c�2011 Association for Compu</context>
<context position="6707" citStr="Aw et al. (2006)" startWordPosition="1038" endWordPosition="1041">r distribution. While the noisy channel model is appropriate for text normalisation, P(T|S), which encodes the underlying error production process, is hard to approximate accurately. Additionally, these methods make the strong assumption that a token tz E T only depends on sz E S, ignoring the context around the token, which could be utilised to help in resolving ambiguity. Statistical machine translation (SMT) has been proposed as a means of context-sensitive text normalisation, by treating the ill-formed text as the source language, and the standard form as the target language. For example, Aw et al. (2006) propose a phrase-level SMT SMS normalisation method with bootstrapped phrase alignments. SMT approaches tend to suffer from a critical lack of training data, however. It is labor intensive to construct an annotated corpus to sufficiently cover ill-formed words and context-appropriate corrections. Furthermore, it is hard to harness SMT for the lexical normalisation problem, as even if phrase-level re-ordering is suppressed by constraints on phrase segmentation, word-level re-orderings within a phrase are still prevalent. Some researchers have also formulated text normalisation as a speech reco</context>
<context position="26147" citStr="Aw et al. (2006)" startWordPosition="4234" endWordPosition="4237">or ill-formed words, subject to the task definition outlined in Section 3.1. The total number of ill-formed words contained in the SMS and Twitter datasets were 3849 and 1184, respectively.&apos; The language filtering of Twitter to automatically identify English tweets was based on the language identification method of Baldwin and Lui (2010), using the EuroGOV dataset as training data, a mixed unigram/bigram/trigram byte feature representation, and a skew divergence nearest prototype classifier. We reimplemented the state-of-art noisy channel model of Cook and Stevenson (2009) and SMT approach of Aw et al. (2006) as benchmark methods. We implement the SMT approach in Moses (Koehn et al., 2007), with synthetic training and tuning data of 90,000 and 1000 sentence pairs, respectively. This data is randomly sampled from the 1.5GB of clean Twitter data, and errors are generated according to distribution of SMS corpus. The 10-fold cross-validated BLEU score (Papineni et al., 2002) over this data is 0.81. 7The Twitter dataset is available at http://www. csse.unimelb.edu.au/research/lt/resources/ lexnorm/. In addition to comparing our method with competitor methods, we also study the contribution of different</context>
<context position="33053" citStr="Aw et al., 2006" startWordPosition="5368" endWordPosition="5371">trategy when combined with more ro375 Dataset Evaluation NC MT DL WS CS WS+CS DL+WS+CS SMS Precision 0.465 — 0.927 0.521 0.116 0.532 0.756 Recall 0.464 — 0.597 0.520 0.116 0.531 0.754 F-score 0.464 — 0.726 0.520 0.116 0.531 0.755 BLEU 0.746 0.700 0.801 0.764 0.612 0.772 0.876 Twitter Precision 0.452 — 0.961 0.551 0.194 0.571 0.753 Recall 0.452 — 0.460 0.551 0.194 0.571 0.753 F-score 0.452 — 0.622 0.551 0.194 0.571 0.753 BLEU 0.857 0.728 0.861 0.878 0.797 0.884 0.934 Table 3: Candidate selection effectiveness on different datasets (NC = noisy channel model (Cook and Stevenson, 2009); MT = SMT (Aw et al., 2006); DL = dictionary lookup; WS = word similarity; CS = context support) bust techniques such as our proposed method, and effective at capturing common abbreviations such as gf “girlfriend”. Of the component methods proposed in this research, word similarity (“WS”) achieves higher precision and recall than context support (“CS”), signifying that many of the ill-formed words emanate from morphophonemic variations. However, when combined with word similarity features, context support improves over the basic method at a level of statistical significance (based on randomised estimation, p &lt; 0.05: Yeh</context>
</contexts>
<marker>Aw, Zhang, Xiao, Su, 2006</marker>
<rawString>AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for SMS text normalization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 33–40, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Marco Lui</author>
</authors>
<title>Language identification: The long and the short of the matter.</title>
<date>2010</date>
<booktitle>In HLT ’10: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>229--237</pages>
<location>Los Angeles, USA.</location>
<contexts>
<context position="25870" citStr="Baldwin and Lui (2010)" startWordPosition="4193" endWordPosition="4196">sed on a random sampling of 549 English tweets. The English tweets were annotated by three independent annotators. All OOV words were pre-identified, and the annotators were requested to determine: (a) whether each OOV word was ill-formed or not; and (b) what the standard form was for ill-formed words, subject to the task definition outlined in Section 3.1. The total number of ill-formed words contained in the SMS and Twitter datasets were 3849 and 1184, respectively.&apos; The language filtering of Twitter to automatically identify English tweets was based on the language identification method of Baldwin and Lui (2010), using the EuroGOV dataset as training data, a mixed unigram/bigram/trigram byte feature representation, and a skew divergence nearest prototype classifier. We reimplemented the state-of-art noisy channel model of Cook and Stevenson (2009) and SMT approach of Aw et al. (2006) as benchmark methods. We implement the SMT approach in Moses (Koehn et al., 2007), with synthetic training and tuning data of 90,000 and 1000 sentence pairs, respectively. This data is randomly sampled from the 1.5GB of clean Twitter data, and errors are generated according to distribution of SMS corpus. The 10-fold cros</context>
</contexts>
<marker>Baldwin, Lui, 2010</marker>
<rawString>Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In HLT ’10: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 229–237, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Beaufort</author>
<author>Sophie Roekhaut</author>
<author>Louise-Am´elie Cougnon</author>
<author>C´edrick Fairon</author>
</authors>
<title>A hybrid rule/model-based finite-state framework for normalizing SMS messages.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>770--779</pages>
<location>Uppsala,</location>
<contexts>
<context position="7496" citStr="Beaufort et al. (2010)" startWordPosition="1156" endWordPosition="1159"> It is labor intensive to construct an annotated corpus to sufficiently cover ill-formed words and context-appropriate corrections. Furthermore, it is hard to harness SMT for the lexical normalisation problem, as even if phrase-level re-ordering is suppressed by constraints on phrase segmentation, word-level re-orderings within a phrase are still prevalent. Some researchers have also formulated text normalisation as a speech recognition problem. For example, Kobus et al. (2008) firstly convert input text tokens into phonetic tokens and then restore them to words by phonetic dictionary lookup. Beaufort et al. (2010) use finite state methods to perform French SMS normalisation, combining the advantages of SMT and the noisy channel model. Kaufmann and Kalita (2010) exploit a machine translation approach with a preprocessor for syntactic (rather than lexical) normalisation. Predominantly, however, these methods require large-scale annotated training data, limiting their adaptability to new domains or languages. In contrast, our proposed method doesn’t require annotated data. It builds on the work on SMS text normalisation, and adapts it to Twitter data, exploiting multiple data sources for normalisation. 36</context>
</contexts>
<marker>Beaufort, Roekhaut, Cougnon, Fairon, 2010</marker>
<rawString>Richard Beaufort, Sophie Roekhaut, Louise-Am´elie Cougnon, and C´edrick Fairon. 2010. A hybrid rule/model-based finite-state framework for normalizing SMS messages. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 770–779, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>In ACL ’00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>286--293</pages>
<location>Hong Kong.</location>
<contexts>
<context position="5488" citStr="Brill and Moore (2000)" startWordPosition="856" endWordPosition="859"> propose a novel normalisation approach that exploits dictionary lookup, word similarity and word context, without requiring annotated data; and (4) we demonstrate that our method achieves state-of-the-art accuracy over both SMS and Twitter data. 2 Related work The noisy channel model (Shannon, 1948) has traditionally been the primary approach to tackling text normalisation. Suppose the ill-formed text is T and its corresponding standard form is S, the approach aims to find arg max P(S|T) by computing arg maxP(T|S)P(S), in which P(S) is usually a language model and P (T |S) is an error model. Brill and Moore (2000) characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. Toutanova and Moore (2002) improve the model by incorporating pronunciation information. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov model (HMM) state transitions and emissions, respectively (Rabiner, 1989). Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sampled e</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. In ACL ’00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 286–293, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Burton</author>
<author>Akshay Java</author>
<author>Ian Soboroff</author>
</authors>
<date>2009</date>
<booktitle>The ICWSM 2009 Spinn3r Dataset. In Proceedings of the Third Annual Conference on Weblogs and Social Media,</booktitle>
<location>San Jose, USA.</location>
<contexts>
<context position="28340" citStr="Burton et al. (2009)" startWordPosition="4577" endWordPosition="4580"> only on the candidate selection step, so we evaluate ill-formed word detection for the Twitter data only. For candidate selection, we once again evaluate using token-level precision, recall and F-score. Additionally, we evaluate using the BLEU score over the normalised form of each message, as the SMT method can lead to perturbations of the token stream, vexing standard precision, recall and F-score evaluation. 5.3 Results and Analysis First, we test the impact of the wd and td values on ill-formed word detection effectiveness, based on dependencies from either the Spinn3r blog corpus (Blog: Burton et al. (2009)) or NYT. The results for precision, recall and F-score are presented in Figure 2. Some conclusions can be drawn from the graphs. First, higher detection threshold values (td) give better precision but lower recall. Generally, as td is raised from 1 to 10, the precision improves slightly but recall drops dramatically, with the net effect that the F-score decreases monotonically. Thus, we use a 374 Figure 2: Ill-formed word detection precision, recall and F-score smaller threshold, i.e. td = 1. Second, there are differences between the two corpora, with dependencies from the Blog corpus produci</context>
</contexts>
<marker>Burton, Java, Soboroff, 2009</marker>
<rawString>Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The ICWSM 2009 Spinn3r Dataset. In Proceedings of the Third Annual Conference on Weblogs and Social Media, San Jose, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Animesh Mukherjee</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Investigation and modeling of the structure of texting language.</title>
<date>2007</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<pages>10--157</pages>
<contexts>
<context position="5714" citStr="Choudhury et al. (2007)" startWordPosition="887" endWordPosition="890">SMS and Twitter data. 2 Related work The noisy channel model (Shannon, 1948) has traditionally been the primary approach to tackling text normalisation. Suppose the ill-formed text is T and its corresponding standard form is S, the approach aims to find arg max P(S|T) by computing arg maxP(T|S)P(S), in which P(S) is usually a language model and P (T |S) is an error model. Brill and Moore (2000) characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. Toutanova and Moore (2002) improve the model by incorporating pronunciation information. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov model (HMM) state transitions and emissions, respectively (Rabiner, 1989). Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sampled error distribution. While the noisy channel model is appropriate for text normalisation, P(T|S), which encodes the underlying error production process, is hard to approximate accurately. Additionally, these methods make the str</context>
<context position="13498" citStr="Choudhury et al. (2007)" startWordPosition="2119" endWordPosition="2122">es where letters are missing or there are extraneous letters, but the lexical correspondence to the target word form is trivially accessible (e.g. shuld “should”). “Number Substitution” refers to instances of letter–number substitution, where numbers have been substituted for phonetically-similar sequences of letters (e.g. 4 “for”). “Letter&amp;Number” refers to instances which have both extra/missing letters and number substitution (e.g. b4 “before”). “Slang” refers to instances 4Based on 44 million sentences from English Gigaword. 5Based on 12.6 thousand SMS messages from How and Kan (2005) and Choudhury et al. (2007). 6Based on 1.37 million tweets collected from the Twitter streaming API from Aug to Oct 2010, and filtered for monolingual English messages; see Section 5.1 for details of the language filtering methodology. Category Ratio Letter&amp;Number 2.36% Letter 72.44% Number Substitution 2.76% Slang 12.20% Other 10.24% Table 1: Ill-formed word distribution of Internet slang (e.g. lol “laugh out loud”), as found in a slang dictionary (see Section 3.1). “Other” is the remainder of the instances, which is predominantly made up of occurrences of spaces having being deleted between words (e.g. sucha “such a”)</context>
<context position="25176" citStr="Choudhury et al., 2007" startWordPosition="4082" endWordPosition="4085"> of getting across the message effectively. The dependency features can be used in noisy contexts and are robust to the effects of other ill-formed words, as they do not rely on contiguity. For example, uz “use” in i did #tt uz me and yu, dependencies can capture relationships like aux(use-4, do-2), which is beyond the capabilities of the language model due to the hashtag being treated as a correct OOV word. 5 Experiments 5.1 Dataset and baselines The aim of our experiments is to compare the effectiveness of different methodologies over text messages, based on two datasets: (1) an SMS corpus (Choudhury et al., 2007); and (2) a novel Twitter dataset developed as part of this research, based on a random sampling of 549 English tweets. The English tweets were annotated by three independent annotators. All OOV words were pre-identified, and the annotators were requested to determine: (a) whether each OOV word was ill-formed or not; and (b) what the standard form was for ill-formed words, subject to the task definition outlined in Section 3.1. The total number of ill-formed words contained in the SMS and Twitter datasets were 3849 and 1184, respectively.&apos; The language filtering of Twitter to automatically ide</context>
</contexts>
<marker>Choudhury, Saraf, Jain, Mukherjee, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. International Journal on Document Analysis and Recognition, 10:157–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An unsupervised model for text message normalization.</title>
<date>2009</date>
<booktitle>In CALC ’09: Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,</booktitle>
<pages>71--78</pages>
<location>Boulder, USA.</location>
<contexts>
<context position="5967" citStr="Cook and Stevenson (2009)" startWordPosition="922" endWordPosition="925">nd arg max P(S|T) by computing arg maxP(T|S)P(S), in which P(S) is usually a language model and P (T |S) is an error model. Brill and Moore (2000) characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. Toutanova and Moore (2002) improve the model by incorporating pronunciation information. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov model (HMM) state transitions and emissions, respectively (Rabiner, 1989). Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sampled error distribution. While the noisy channel model is appropriate for text normalisation, P(T|S), which encodes the underlying error production process, is hard to approximate accurately. Additionally, these methods make the strong assumption that a token tz E T only depends on sz E S, ignoring the context around the token, which could be utilised to help in resolving ambiguity. Statistical machine translation (SMT) has been proposed as a means of context-sensitive text normal</context>
<context position="23072" citStr="Cook and Stevenson, 2009" startWordPosition="3728" endWordPosition="3731">. We separately introduce a threshold td E 11, 2, ...,10} on the number of positive predictions returned by the detection classifier over the set of normalisation candidates for a given OOV token: the token is considered to be ill-formed iff td or more candidates are positively classified, i.e. predicted to be correct candidates. 4.3 Candidate Selection For OOV words which are predicted to be illformed, we select the most likely candidate from the confusion set as the basis of normalisation. The final selection is based on the following features, in line with previous work (Wong et al., 2006; Cook and Stevenson, 2009). Lexical edit distance, phonemic edit distance, prefix substring, suffix substring, and the longest common subsequence (LCS) are exploited to capture morphophonemic similarity. Both lexical and phonemic edit distance (ED) are normalised by the reciprocal of exp(ED). The prefix and suffix features are intended to capture the fact that leading and trailing characters are frequently dropped from words, e.g. in cases such as ish and talkin. We calculate the ratio of the LCS over the maximum string length between ill-formed word and the candidate, since the ill-formed word can be either longer or </context>
<context position="26110" citStr="Cook and Stevenson (2009)" startWordPosition="4225" endWordPosition="4228">d or not; and (b) what the standard form was for ill-formed words, subject to the task definition outlined in Section 3.1. The total number of ill-formed words contained in the SMS and Twitter datasets were 3849 and 1184, respectively.&apos; The language filtering of Twitter to automatically identify English tweets was based on the language identification method of Baldwin and Lui (2010), using the EuroGOV dataset as training data, a mixed unigram/bigram/trigram byte feature representation, and a skew divergence nearest prototype classifier. We reimplemented the state-of-art noisy channel model of Cook and Stevenson (2009) and SMT approach of Aw et al. (2006) as benchmark methods. We implement the SMT approach in Moses (Koehn et al., 2007), with synthetic training and tuning data of 90,000 and 1000 sentence pairs, respectively. This data is randomly sampled from the 1.5GB of clean Twitter data, and errors are generated according to distribution of SMS corpus. The 10-fold cross-validated BLEU score (Papineni et al., 2002) over this data is 0.81. 7The Twitter dataset is available at http://www. csse.unimelb.edu.au/research/lt/resources/ lexnorm/. In addition to comparing our method with competitor methods, we als</context>
<context position="30555" citStr="Cook and Stevenson (2009)" startWordPosition="4950" endWordPosition="4953">s trend, we examined the annotations in the SMS corpus, and found them to be looser than ours, because they have different task specifications than our lexical normalisation. In our annotation, the annotators only normalised ill-formed word if they had high confidence of how to normalise, as with talkin “talking”. For ill-formed words where they couldn’t be certain of the standard form, the tokens were left untouched. However, in the SMS corpus, annotations such as sammis “same” are also included. This leads to a performance drop for our method over the SMS corpus. The noisy channel method of Cook and Stevenson (2009) shares similar features with word similarity (“WS”), However, when word similarity and context support are combined (“WS+CS”), our method outperforms the noisy channel method by about 7% and 12% in F-score over SMS and Twitter corpora, respectively. This can be explained as follows. First, the Cook and Stevenson (2009) method is typebased, so all token instances of a given ill-formed word will be normalised identically. In the Twitter data, however, the same word can be normalised differently depending on context, e.g. hw “how” in so hw many time remaining so I can calculate it? vs. hw “homew</context>
<context position="33025" citStr="Cook and Stevenson, 2009" startWordPosition="5361" endWordPosition="5364">gh it is an effective preprocessing strategy when combined with more ro375 Dataset Evaluation NC MT DL WS CS WS+CS DL+WS+CS SMS Precision 0.465 — 0.927 0.521 0.116 0.532 0.756 Recall 0.464 — 0.597 0.520 0.116 0.531 0.754 F-score 0.464 — 0.726 0.520 0.116 0.531 0.755 BLEU 0.746 0.700 0.801 0.764 0.612 0.772 0.876 Twitter Precision 0.452 — 0.961 0.551 0.194 0.571 0.753 Recall 0.452 — 0.460 0.551 0.194 0.571 0.753 F-score 0.452 — 0.622 0.551 0.194 0.571 0.753 BLEU 0.857 0.728 0.861 0.878 0.797 0.884 0.934 Table 3: Candidate selection effectiveness on different datasets (NC = noisy channel model (Cook and Stevenson, 2009); MT = SMT (Aw et al., 2006); DL = dictionary lookup; WS = word similarity; CS = context support) bust techniques such as our proposed method, and effective at capturing common abbreviations such as gf “girlfriend”. Of the component methods proposed in this research, word similarity (“WS”) achieves higher precision and recall than context support (“CS”), signifying that many of the ill-formed words emanate from morphophonemic variations. However, when combined with word similarity features, context support improves over the basic method at a level of statistical significance (based on randomis</context>
</contexts>
<marker>Cook, Stevenson, 2009</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. In CALC ’09: Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, pages 71– 78, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Genoa, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="19893" citStr="Fan et al., 2008" startWordPosition="3216" endWordPosition="3219">that look occurs 2 words after way. We choose dependencies to represent context because they are an effective way of capturing key relationships between words, and similar features can easily be extracted from tweets. Note that we don’t record the dependency type here, because we have no intention of dependency parsing text messages, due to their noisiness and the volume of the data. The counts of dependency forms are combined together to derive a confidence score, and the scored dependencies are stored in a dependency bank. Given the dependency-based features, a linear kernel SVM classifier (Fan et al., 2008) is trained on clean Twitter data, i.e. the subset of Twitter messages without OOV words. Each word is repre372 sented by its IV words within a context window of three words to either side of the target word, together with their relative positions in the form of (word1,word2,position) tuples, and their score in the dependency bank. These form the positive training exemplars. Negative exemplars are automatically constructed by replacing target words with highly-ranked candidates from their confusion set. Note that the classifier does not require any hand annotation, as all training exemplars ar</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>Alexander Budanitsky</author>
</authors>
<title>Correcting real-word spelling errors by restoring lexical cohesion.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<pages>11--87</pages>
<contexts>
<context position="27063" citStr="Hirst and Budanitsky, 2005" startWordPosition="4373" endWordPosition="4376">on of SMS corpus. The 10-fold cross-validated BLEU score (Papineni et al., 2002) over this data is 0.81. 7The Twitter dataset is available at http://www. csse.unimelb.edu.au/research/lt/resources/ lexnorm/. In addition to comparing our method with competitor methods, we also study the contribution of different feature groups. We separately compare dictionary lookup over our Internet slang dictionary, the contextual feature model, and the word similarity feature model, as well as combinations of these three. 5.2 Evaluation metrics The evaluation of lexical normalisation consists of two stages (Hirst and Budanitsky, 2005): (1) illformed word detection, and (2) candidate selection. In terms of detection, we want to make sense of how well the system can identify ill-formed words and leave correct OOV words untouched. This step is crucial to further normalisation, because if correct OOV words are identified as ill-formed, the candidate selection step can never be correct. Conversely, if an ill-formed word is predicted to be correct, the candidate selection will have no chance to normalise it. We evaluate detection performance by token-level precision, recall and F-score (Q = 1). Previous work over the SMS corpus </context>
</contexts>
<marker>Hirst, Budanitsky, 2005</marker>
<rawString>Graeme Hirst and Alexander Budanitsky. 2005. Correcting real-word spelling errors by restoring lexical cohesion. Natural Language Engineering, 11:87–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijue How</author>
<author>Min-Yen Kan</author>
</authors>
<title>Optimizing predictive text entry for short message service on mobile phones.</title>
<date>2005</date>
<booktitle>In Human Computer Interfaces International (HCII 05),</booktitle>
<location>Las Vegas, USA.</location>
<contexts>
<context position="13470" citStr="How and Kan (2005)" startWordPosition="2114" endWordPosition="2117">tter” refers to instances where letters are missing or there are extraneous letters, but the lexical correspondence to the target word form is trivially accessible (e.g. shuld “should”). “Number Substitution” refers to instances of letter–number substitution, where numbers have been substituted for phonetically-similar sequences of letters (e.g. 4 “for”). “Letter&amp;Number” refers to instances which have both extra/missing letters and number substitution (e.g. b4 “before”). “Slang” refers to instances 4Based on 44 million sentences from English Gigaword. 5Based on 12.6 thousand SMS messages from How and Kan (2005) and Choudhury et al. (2007). 6Based on 1.37 million tweets collected from the Twitter streaming API from Aug to Oct 2010, and filtered for monolingual English messages; see Section 5.1 for details of the language filtering methodology. Category Ratio Letter&amp;Number 2.36% Letter 72.44% Number Substitution 2.76% Slang 12.20% Other 10.24% Table 1: Ill-formed word distribution of Internet slang (e.g. lol “laugh out loud”), as found in a slang dictionary (see Section 3.1). “Other” is the remainder of the instances, which is predominantly made up of occurrences of spaces having being deleted between</context>
</contexts>
<marker>How, Kan, 2005</marker>
<rawString>Yijue How and Min-Yen Kan. 2005. Optimizing predictive text entry for short message service on mobile phones. In Human Computer Interfaces International (HCII 05), Las Vegas, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emi Izumi</author>
<author>Kiyotaka Uchimoto</author>
<author>Toyomi Saiga</author>
<author>Thepchai Supnithi</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Automatic error detection in the Japanese learners’ English spoken data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -</booktitle>
<volume>2</volume>
<pages>145--148</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="18421" citStr="Izumi et al., 2003" startWordPosition="2978" endWordPosition="2981">-IV messages. To train the language model, we used SRILM (Stolcke, 2002) with the -&lt;unk&gt; option. If we truncate the ranking to the top 10% of candidates, the recall drops back to 84% with a 90% reduction in candidates. 4.2 Ill-formed Word Detection The next step is to detect whether a given OOV word in context is actually an ill-formed word or not, relative to its confusion set. To the best of our knowledge, we are the first to target the task of ill-formed word detection in the context of short text messages, although related work exists for text with lower relative occurrences of OOV words (Izumi et al., 2003; Sun et al., 2007). Due to the noisiness of the data, it is impractical to use full-blown syntactic or semantic features. The most direct source of evidence is IV words around an OOV word. Inspired by work on labelled sequential pattern extraction (Sun et al., 2007), we exploit large-scale edited corpus data to construct dependency-based features. First, we use the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006) to extract dependencies from the NYT corpus (see Section 3.2). For example, from a sentence such as One obvious difference is the way they look, we would extract d</context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Supnithi, Isahara, 2003</marker>
<rawString>Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Supnithi, and Hitoshi Isahara. 2003. Automatic error detection in the Japanese learners’ English spoken data. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 2, pages 145–148, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Kaufmann</author>
<author>Jugal Kalita</author>
</authors>
<title>Syntactic normalization of Twitter messages.</title>
<date>2010</date>
<booktitle>In International Conference on Natural Language Processing,</booktitle>
<location>Kharagpur, India.</location>
<contexts>
<context position="7646" citStr="Kaufmann and Kalita (2010)" startWordPosition="1179" endWordPosition="1182">t is hard to harness SMT for the lexical normalisation problem, as even if phrase-level re-ordering is suppressed by constraints on phrase segmentation, word-level re-orderings within a phrase are still prevalent. Some researchers have also formulated text normalisation as a speech recognition problem. For example, Kobus et al. (2008) firstly convert input text tokens into phonetic tokens and then restore them to words by phonetic dictionary lookup. Beaufort et al. (2010) use finite state methods to perform French SMS normalisation, combining the advantages of SMT and the noisy channel model. Kaufmann and Kalita (2010) exploit a machine translation approach with a preprocessor for syntactic (rather than lexical) normalisation. Predominantly, however, these methods require large-scale annotated training data, limiting their adaptability to new domains or languages. In contrast, our proposed method doesn’t require annotated data. It builds on the work on SMS text normalisation, and adapts it to Twitter data, exploiting multiple data sources for normalisation. 369 Figure 1: Out-of-vocabulary word distribution in English Gigaword (NYT), Twitter and SMS data 3 Scoping Text Normalisation 3.1 Task Definition of Le</context>
<context position="15589" citStr="Kaufmann and Kalita (2010)" startWordPosition="2452" endWordPosition="2455">med. In confusion set generation, we generate a set of IV normalisation candidates for each OOV word type based on morphophonemic variation. We call this set the confusion set of that OOV word, and aim to include all feasible normalisation candidates for the word type in the confusion set. The confusion candidates are then filtered for each token occurrence of a given OOV word, based on their local context fit with a language model. 4.1 Confusion Set Generation Revisiting our manual analysis from Section 3.2, most ill-formed tokens in Twitter are morphophonemically derived. First, inspired by Kaufmann and Kalita (2010), any repititions of more than 3 letters are reduced back to 3 letters (e.g. cooool is re371 Criterion Recall Average Candidates T� &lt; 1 40.4% 24 T� &lt; 2 76.6% 240 T, = 0 55.4% 65 T, &lt; 1 83.4% 1248 T, &lt; 2 91.0% 9694 T� &lt; 2 V T, &lt; 1 88.8% 1269 T� &lt; 2 V T, &lt; 2 92.7% 9515 Table 2: Recall and average number of candidates for different confusion set generation strategies duced to coool). Second, IV words within a threshold T, character edit distance of the given OOV word are calculated, as is widely used in spell checkers. Third, the double metaphone algorithm (Philips, 2000) is used to decode the pr</context>
</contexts>
<marker>Kaufmann, Kalita, 2010</marker>
<rawString>Joseph Kaufmann and Jugal Kalita. 2010. Syntactic normalization of Twitter messages. In International Conference on Natural Language Processing, Kharagpur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15 (NIPS</booktitle>
<pages>3--10</pages>
<location>Whistler, Canada.</location>
<contexts>
<context position="18830" citStr="Klein and Manning, 2003" startWordPosition="3045" endWordPosition="3049">wledge, we are the first to target the task of ill-formed word detection in the context of short text messages, although related work exists for text with lower relative occurrences of OOV words (Izumi et al., 2003; Sun et al., 2007). Due to the noisiness of the data, it is impractical to use full-blown syntactic or semantic features. The most direct source of evidence is IV words around an OOV word. Inspired by work on labelled sequential pattern extraction (Sun et al., 2007), we exploit large-scale edited corpus data to construct dependency-based features. First, we use the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006) to extract dependencies from the NYT corpus (see Section 3.2). For example, from a sentence such as One obvious difference is the way they look, we would extract dependencies such as rcmod(way-6,look-8) and nsubj(look-8,they-7). We then transform the dependencies into relational features for each OOV word. Assuming that way were an OOV word, e.g., we would extract dependencies of the form (look,way,+2), indicating that look occurs 2 words after way. We choose dependencies to represent context because they are an effective way of capturing key relationships between w</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15 (NIPS 2002), pages 3–10, Whistler, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Kobus</author>
<author>Franois Yvon</author>
<author>Graldine Damnati</author>
</authors>
<title>Transcrire les SMS comme on reconnat la parole.</title>
<date>2008</date>
<booktitle>In Actes de la Confrence sur le Traitement Automatique des Langues (TALN’08),</booktitle>
<pages>128--138</pages>
<contexts>
<context position="7356" citStr="Kobus et al. (2008)" startWordPosition="1134" endWordPosition="1137">S normalisation method with bootstrapped phrase alignments. SMT approaches tend to suffer from a critical lack of training data, however. It is labor intensive to construct an annotated corpus to sufficiently cover ill-formed words and context-appropriate corrections. Furthermore, it is hard to harness SMT for the lexical normalisation problem, as even if phrase-level re-ordering is suppressed by constraints on phrase segmentation, word-level re-orderings within a phrase are still prevalent. Some researchers have also formulated text normalisation as a speech recognition problem. For example, Kobus et al. (2008) firstly convert input text tokens into phonetic tokens and then restore them to words by phonetic dictionary lookup. Beaufort et al. (2010) use finite state methods to perform French SMS normalisation, combining the advantages of SMT and the noisy channel model. Kaufmann and Kalita (2010) exploit a machine translation approach with a preprocessor for syntactic (rather than lexical) normalisation. Predominantly, however, these methods require large-scale annotated training data, limiting their adaptability to new domains or languages. In contrast, our proposed method doesn’t require annotated </context>
</contexts>
<marker>Kobus, Yvon, Damnati, 2008</marker>
<rawString>Catherine Kobus, Franois Yvon, and Graldine Damnati. 2008. Transcrire les SMS comme on reconnat la parole. In Actes de la Confrence sur le Traitement Automatique des Langues (TALN’08), pages 128–138.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="26229" citStr="Koehn et al., 2007" startWordPosition="4249" endWordPosition="4252">e total number of ill-formed words contained in the SMS and Twitter datasets were 3849 and 1184, respectively.&apos; The language filtering of Twitter to automatically identify English tweets was based on the language identification method of Baldwin and Lui (2010), using the EuroGOV dataset as training data, a mixed unigram/bigram/trigram byte feature representation, and a skew divergence nearest prototype classifier. We reimplemented the state-of-art noisy channel model of Cook and Stevenson (2009) and SMT approach of Aw et al. (2006) as benchmark methods. We implement the SMT approach in Moses (Koehn et al., 2007), with synthetic training and tuning data of 90,000 and 1000 sentence pairs, respectively. This data is randomly sampled from the 1.5GB of clean Twitter data, and errors are generated according to distribution of SMS corpus. The 10-fold cross-validated BLEU score (Papineni et al., 2002) over this data is 0.81. 7The Twitter dataset is available at http://www. csse.unimelb.edu.au/research/lt/resources/ lexnorm/. In addition to comparing our method with competitor methods, we also study the contribution of different feature groups. We separately compare dictionary lookup over our Internet slang d</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177– 180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="26516" citStr="Papineni et al., 2002" startWordPosition="4295" endWordPosition="4298">set as training data, a mixed unigram/bigram/trigram byte feature representation, and a skew divergence nearest prototype classifier. We reimplemented the state-of-art noisy channel model of Cook and Stevenson (2009) and SMT approach of Aw et al. (2006) as benchmark methods. We implement the SMT approach in Moses (Koehn et al., 2007), with synthetic training and tuning data of 90,000 and 1000 sentence pairs, respectively. This data is randomly sampled from the 1.5GB of clean Twitter data, and errors are generated according to distribution of SMS corpus. The 10-fold cross-validated BLEU score (Papineni et al., 2002) over this data is 0.81. 7The Twitter dataset is available at http://www. csse.unimelb.edu.au/research/lt/resources/ lexnorm/. In addition to comparing our method with competitor methods, we also study the contribution of different feature groups. We separately compare dictionary lookup over our Internet slang dictionary, the contextual feature model, and the word similarity feature model, as well as combinations of these three. 5.2 Evaluation metrics The evaluation of lexical normalisation consists of two stages (Hirst and Budanitsky, 2005): (1) illformed word detection, and (2) candidate sel</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James L Peterson</author>
</authors>
<title>Computer programs for detecting and correcting spelling errors.</title>
<date>1980</date>
<journal>Commun. ACM,</journal>
<pages>23--676</pages>
<contexts>
<context position="2605" citStr="Peterson, 1980" startWordPosition="397" endWordPosition="398">se. If there were some way of preprocessing the message to produce a more canonical lexical rendering, we would expect the quality of the parser to improve appreciably. Our aim in this paper is this task of lexical normalisation of noisy English text, with a particular focus on Twitter and SMS messages. In this paper, we will collectively refer to individual instances of typos, ad hoc abbreviations, unconventional spellings, phonetic substitutions and other causes of lexical deviation as “illformed words”. The message normalisation task is challenging. It has similarities with spell checking (Peterson, 1980), but differs in that ill-formedness in text messages is often intentional, whether due to the desire to save characters/keystrokes, for social identity, or due to convention in this text sub-genre. We propose to go beyond spell checkers, in performing deabbreviation when appropriate, and recovering the canonical word form of commonplace shorthands like b4 “before”, which tend to be considered beyond the remit of spell checking (Aw et al., 2006). The free writing style of text messages makes the task even more complex, e.g. with word lengthening such as goooood being commonplace for emphasis. </context>
</contexts>
<marker>Peterson, 1980</marker>
<rawString>James L. Peterson. 1980. Computer programs for detecting and correcting spelling errors. Commun. ACM, 23:676–687, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Philips</author>
</authors>
<title>The double metaphone search algorithm.</title>
<date>2000</date>
<journal>C/C++ Users Journal,</journal>
<pages>18--38</pages>
<contexts>
<context position="16164" citStr="Philips, 2000" startWordPosition="2571" endWordPosition="2572"> inspired by Kaufmann and Kalita (2010), any repititions of more than 3 letters are reduced back to 3 letters (e.g. cooool is re371 Criterion Recall Average Candidates T� &lt; 1 40.4% 24 T� &lt; 2 76.6% 240 T, = 0 55.4% 65 T, &lt; 1 83.4% 1248 T, &lt; 2 91.0% 9694 T� &lt; 2 V T, &lt; 1 88.8% 1269 T� &lt; 2 V T, &lt; 2 92.7% 9515 Table 2: Recall and average number of candidates for different confusion set generation strategies duced to coool). Second, IV words within a threshold T, character edit distance of the given OOV word are calculated, as is widely used in spell checkers. Third, the double metaphone algorithm (Philips, 2000) is used to decode the pronunciation of all IV words, and IV words within a threshold Tp edit distance of the given OOV word under phonemic transcription, are included in the confusion set; this allows us to capture OOV words such as earthquick “earthquake”. In Table 2, we list the recall and average size of the confusion set generated by the final two strategies with different threshold settings, based on our evaluation dataset (see Section 5.1). The recall for lexical edit distance with T, &lt; 2 is moderately high, but it is unable to detect the correct candidate for about one quarter of words</context>
</contexts>
<marker>Philips, 2000</marker>
<rawString>Lawrence Philips. 2000. The double metaphone search algorithm. C/C++ Users Journal, 18:38–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="5940" citStr="Rabiner, 1989" startWordPosition="920" endWordPosition="921">roach aims to find arg max P(S|T) by computing arg maxP(T|S)P(S), in which P(S) is usually a language model and P (T |S) is an error model. Brill and Moore (2000) characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. Toutanova and Moore (2002) improve the model by incorporating pronunciation information. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov model (HMM) state transitions and emissions, respectively (Rabiner, 1989). Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sampled error distribution. While the noisy channel model is appropriate for text normalisation, P(T|S), which encodes the underlying error production process, is hard to approximate accurately. Additionally, these methods make the strong assumption that a token tz E T only depends on sz E S, ignoring the context around the token, which could be utilised to help in resolving ambiguity. Statistical machine translation (SMT) has been proposed as a means of co</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of Twitter conversations.</title>
<date>2010</date>
<booktitle>In HLT ’10: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>172--180</pages>
<location>Los Angeles, USA.</location>
<contexts>
<context position="1517" citStr="Ritter et al., 2010" startWordPosition="217" endWordPosition="220">ataset based on Twitter. 1 Introduction Twitter and other micro-blogging services are highly attractive for information extraction and text mining purposes, as they offer large volumes of real-time data, with around 65 millions tweets posted on Twitter per day in June 2010 (Twitter, 2010). The quality of messages varies significantly, however, ranging from high quality newswire-like text to meaningless strings. Typos, ad hoc abbreviations, phonetic substitutions, ungrammatical structures and emoticons abound in short text messages, causing grief for text processing tools (Sproat et al., 2001; Ritter et al., 2010). For instance, presented with the input u must be talkin bout the paper but I was thinkin movies (“You must be talking about the paper but I was thinking movies”),&apos; the Stanford parser (Klein and &apos;Throughout the paper, we will provide a normalised version of examples as a gloss in double quotes. Manning, 2003; de Marneffe et al., 2006) analyses bout the paper and thinkin movies as a clause and noun phrase, respectively, rather than a prepositional phrase and verb phrase. If there were some way of preprocessing the message to produce a more canonical lexical rendering, we would expect the qual</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of Twitter conversations. In HLT ’10: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 172–180, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude Elwood Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>623--656</pages>
<contexts>
<context position="5167" citStr="Shannon, 1948" startWordPosition="799" endWordPosition="800">n both SMS and Twitter data. Our contributions in this paper are as follows: (1) we conduct a pilot study on the OOV word distribution of Twitter and other text genres, and analyse different sources of non-standard orthography in Twitter; (2) we generate a text normalisation dataset based on Twitter data; (3) we propose a novel normalisation approach that exploits dictionary lookup, word similarity and word context, without requiring annotated data; and (4) we demonstrate that our method achieves state-of-the-art accuracy over both SMS and Twitter data. 2 Related work The noisy channel model (Shannon, 1948) has traditionally been the primary approach to tackling text normalisation. Suppose the ill-formed text is T and its corresponding standard form is S, the approach aims to find arg max P(S|T) by computing arg maxP(T|S)P(S), in which P(S) is usually a language model and P (T |S) is an error model. Brill and Moore (2000) characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. Toutanova and Moore (2002) improve the model by incorporating pronunciation information. Choudhury et al. (2007) model the word-level text generation process for SMS</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Claude Elwood Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal, 27:379–423, 623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Alan W Black</author>
<author>Stanley Chen</author>
<author>Shankar Kumar</author>
<author>Mari Ostendorf</author>
<author>Christopher Richards</author>
</authors>
<title>Normalization of non-standard words.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<volume>15</volume>
<issue>3</issue>
<pages>333</pages>
<contexts>
<context position="1495" citStr="Sproat et al., 2001" startWordPosition="213" endWordPosition="216"> corpus and a novel dataset based on Twitter. 1 Introduction Twitter and other micro-blogging services are highly attractive for information extraction and text mining purposes, as they offer large volumes of real-time data, with around 65 millions tweets posted on Twitter per day in June 2010 (Twitter, 2010). The quality of messages varies significantly, however, ranging from high quality newswire-like text to meaningless strings. Typos, ad hoc abbreviations, phonetic substitutions, ungrammatical structures and emoticons abound in short text messages, causing grief for text processing tools (Sproat et al., 2001; Ritter et al., 2010). For instance, presented with the input u must be talkin bout the paper but I was thinkin movies (“You must be talking about the paper but I was thinking movies”),&apos; the Stanford parser (Klein and &apos;Throughout the paper, we will provide a normalised version of examples as a gloss in double quotes. Manning, 2003; de Marneffe et al., 2006) analyses bout the paper and thinkin movies as a clause and noun phrase, respectively, rather than a prepositional phrase and verb phrase. If there were some way of preprocessing the message to produce a more canonical lexical rendering, we</context>
</contexts>
<marker>Sproat, Black, Chen, Kumar, Ostendorf, Richards, 2001</marker>
<rawString>Richard Sproat, Alan W. Black, Stanley Chen, Shankar Kumar, Mari Ostendorf, and Christopher Richards. 2001. Normalization of non-standard words. Computer Speech and Language, 15(3):287 – 333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, USA.</location>
<contexts>
<context position="17875" citStr="Stolcke, 2002" startWordPosition="2879" endWordPosition="2880">e use T, &lt; 2VTp &lt; 1 as the basis for confusion set generation. Examples of ill-formed words where we are unable to generate the standard lexical form are clippings such as fav “favourite” and convo “conversation”. In addition to generating the confusion set, we rank the candidates based on a trigram language model trained over 1.5GB of clean Twitter data, i.e. tweets which consist of all IV words: despite the prevalence of OOV words in Twitter, the sheer volume of the data means that it is relatively easy to collect large amounts of all-IV messages. To train the language model, we used SRILM (Stolcke, 2002) with the -&lt;unk&gt; option. If we truncate the ranking to the top 10% of candidates, the recall drops back to 84% with a 90% reduction in candidates. 4.2 Ill-formed Word Detection The next step is to detect whether a given OOV word in context is actually an ill-formed word or not, relative to its confusion set. To the best of our knowledge, we are the first to target the task of ill-formed word detection in the context of short text messages, although related work exists for text with lower relative occurrences of OOV words (Izumi et al., 2003; Sun et al., 2007). Due to the noisiness of the data,</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In International Conference on Spoken Language Processing, pages 901–904, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guihua Sun</author>
<author>Gao Cong</author>
<author>Xiaohua Liu</author>
<author>Chin-Yew Lin</author>
<author>Ming Zhou</author>
</authors>
<title>Mining sequential patterns and tree patterns to detect erroneous sentences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>81--88</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="18440" citStr="Sun et al., 2007" startWordPosition="2982" endWordPosition="2985">in the language model, we used SRILM (Stolcke, 2002) with the -&lt;unk&gt; option. If we truncate the ranking to the top 10% of candidates, the recall drops back to 84% with a 90% reduction in candidates. 4.2 Ill-formed Word Detection The next step is to detect whether a given OOV word in context is actually an ill-formed word or not, relative to its confusion set. To the best of our knowledge, we are the first to target the task of ill-formed word detection in the context of short text messages, although related work exists for text with lower relative occurrences of OOV words (Izumi et al., 2003; Sun et al., 2007). Due to the noisiness of the data, it is impractical to use full-blown syntactic or semantic features. The most direct source of evidence is IV words around an OOV word. Inspired by work on labelled sequential pattern extraction (Sun et al., 2007), we exploit large-scale edited corpus data to construct dependency-based features. First, we use the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006) to extract dependencies from the NYT corpus (see Section 3.2). For example, from a sentence such as One obvious difference is the way they look, we would extract dependencies such as</context>
</contexts>
<marker>Sun, Cong, Liu, Lin, Zhou, 2007</marker>
<rawString>Guihua Sun, Gao Cong, Xiaohua Liu, Chin-Yew Lin, and Ming Zhou. 2007. Mining sequential patterns and tree patterns to detect erroneous sentences. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Robert C Moore</author>
</authors>
<title>Pronunciation modeling for improved spelling correction.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>144--151</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="5628" citStr="Toutanova and Moore (2002)" startWordPosition="875" endWordPosition="878">ata; and (4) we demonstrate that our method achieves state-of-the-art accuracy over both SMS and Twitter data. 2 Related work The noisy channel model (Shannon, 1948) has traditionally been the primary approach to tackling text normalisation. Suppose the ill-formed text is T and its corresponding standard form is S, the approach aims to find arg max P(S|T) by computing arg maxP(T|S)P(S), in which P(S) is usually a language model and P (T |S) is an error model. Brill and Moore (2000) characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. Toutanova and Moore (2002) improve the model by incorporating pronunciation information. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov model (HMM) state transitions and emissions, respectively (Rabiner, 1989). Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sampled error distribution. While the noisy channel model is appropriate for text normalisation, P(T|S), which encodes the underlying error productio</context>
</contexts>
<marker>Toutanova, Moore, 2002</marker>
<rawString>Kristina Toutanova and Robert C. Moore. 2002. Pronunciation modeling for improved spelling correction. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 144–151, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Twitter</author>
</authors>
<title>Big goals, big game, big records. http://blog.twitter.com/2010/06/ big-goals-big-game-big-records.html.</title>
<date>2010</date>
<journal>Retrieved</journal>
<volume>4</volume>
<contexts>
<context position="1186" citStr="Twitter, 2010" startWordPosition="173" endWordPosition="174">med words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter. 1 Introduction Twitter and other micro-blogging services are highly attractive for information extraction and text mining purposes, as they offer large volumes of real-time data, with around 65 millions tweets posted on Twitter per day in June 2010 (Twitter, 2010). The quality of messages varies significantly, however, ranging from high quality newswire-like text to meaningless strings. Typos, ad hoc abbreviations, phonetic substitutions, ungrammatical structures and emoticons abound in short text messages, causing grief for text processing tools (Sproat et al., 2001; Ritter et al., 2010). For instance, presented with the input u must be talkin bout the paper but I was thinkin movies (“You must be talking about the paper but I was thinking movies”),&apos; the Stanford parser (Klein and &apos;Throughout the paper, we will provide a normalised version of examples </context>
</contexts>
<marker>Twitter, 2010</marker>
<rawString>Twitter. 2010. Big goals, big game, big records. http://blog.twitter.com/2010/06/ big-goals-big-game-big-records.html. Retrieved 4 August 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson Wong</author>
<author>Wei Liu</author>
<author>Mohammed Bennamoun</author>
</authors>
<title>Integrated scoring for spelling error correction, abbreviation expansion and case restoration in dirty text.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth Australasian Conference on Data Mining and Analytics,</booktitle>
<pages>83--89</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="23045" citStr="Wong et al., 2006" startWordPosition="3724" endWordPosition="3727">match) dependencies. We separately introduce a threshold td E 11, 2, ...,10} on the number of positive predictions returned by the detection classifier over the set of normalisation candidates for a given OOV token: the token is considered to be ill-formed iff td or more candidates are positively classified, i.e. predicted to be correct candidates. 4.3 Candidate Selection For OOV words which are predicted to be illformed, we select the most likely candidate from the confusion set as the basis of normalisation. The final selection is based on the following features, in line with previous work (Wong et al., 2006; Cook and Stevenson, 2009). Lexical edit distance, phonemic edit distance, prefix substring, suffix substring, and the longest common subsequence (LCS) are exploited to capture morphophonemic similarity. Both lexical and phonemic edit distance (ED) are normalised by the reciprocal of exp(ED). The prefix and suffix features are intended to capture the fact that leading and trailing characters are frequently dropped from words, e.g. in cases such as ish and talkin. We calculate the ratio of the LCS over the maximum string length between ill-formed word and the candidate, since the ill-formed wo</context>
</contexts>
<marker>Wong, Liu, Bennamoun, 2006</marker>
<rawString>Wilson Wong, Wei Liu, and Mohammed Bennamoun. 2006. Integrated scoring for spelling error correction, abbreviation expansion and case restoration in dirty text. In Proceedings of the Fifth Australasian Conference on Data Mining and Analytics, pages 83–89, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics -Volume 2, COLING ’00,</booktitle>
<pages>947--953</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="33660" citStr="Yeh (2000)" startWordPosition="5463" endWordPosition="5464">06); DL = dictionary lookup; WS = word similarity; CS = context support) bust techniques such as our proposed method, and effective at capturing common abbreviations such as gf “girlfriend”. Of the component methods proposed in this research, word similarity (“WS”) achieves higher precision and recall than context support (“CS”), signifying that many of the ill-formed words emanate from morphophonemic variations. However, when combined with word similarity features, context support improves over the basic method at a level of statistical significance (based on randomised estimation, p &lt; 0.05: Yeh (2000)), indicating the complementarity of the two methods, especially on Twitter data. The best F-score is achieved when combining dictionary lookup, word similarity and context support (“DL+WS+CS”), in which ill-formed words are first looked up in the slang dictionary, and only if no match is found do we apply our normalisation method. We found several limitations in our proposed approach by analysing the output of our method. First, not all ill-formed words offer useful context. Some highly noisy tweets contain almost all misspellings and unique symbols, and thus no context features can be extrac</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th Conference on Computational Linguistics -Volume 2, COLING ’00, pages 947–953, Saarbr¨ucken, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>