<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000049">
<title confidence="0.969538">
A Detailed, Accurate, Extensive, Available English Lexical Database
</title>
<author confidence="0.959882">
Adam Kilgarriff
</author>
<affiliation confidence="0.891101">
Lexical Computing Ltd
</affiliation>
<address confidence="0.53203">
Brighton, UK
</address>
<email confidence="0.997054">
adam@lexmasterclass.com
</email>
<sectionHeader confidence="0.993854" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999416611111111">
We present an English lexical database which
is fuller, more accurate and more consistent
than any other. We believe this to be so be-
cause the project has been well-planned, with
a 12-month intensive planning phase prior to
the lexicography beginning; well-resourced,
employing a team of fifteen highly experi-
enced lexicographers for a thirty-month main
phase; it has had access to the latest corpus
and dictionary-editing technology; it has not
been constrained to meet any goals other than
an accurate description of the language; and
it has been led by a team with singular expe-
rience in delivering high-quality and innova-
tive resources. The lexicon will be complete
in Summer 2010 and will be available for NLP
groups, on terms designed to encourage its re-
search use.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999417689655172">
Most NLP applications need lexicons. NLP re-
searchers have used databases from dictionary pub-
lishers (Boguraev and Briscoe, 1989; Wilks et al.,
1996), or developed NLP resources (COMLEX
(Macleod et al., 1994), XTAG (Doran et al., 1994))
or used WordNet,(Fellbaum, 1998) or have switched
to fully corpus-based strategies which need no lex-
icons. However the publishers’ dictionaries were
pre-corpus, often inconsistent, and licencing con-
straints were in the end fatal. COMLEX and XTAG
address only syntax; WordNet, only semantics. Also
these resources were not produced by experienced
lexicographers, nor according to a detailed, stringent
‘style guide’ specifying how to handle all the phe-
nomena (in orthography, morphology, syntax, se-
mantics and pragmatics, from spelling variation to
register to collocation to sense distinction) that make
lexicography complex. Unsupervised corpus meth-
ods are intellectually exciting but do not provide the
lexical facts that many applications need.
We present DANTE (Database of Analysed Texts
of English), an English lexical database. For the
commonest 50,000 words of English, it gives a de-
tailed account of the word’s meaning(s), grammar,
phraseology and collocation and any noteworthy
facts about its pragmatics or distribution.
In outline this is what dictionaries have been do-
ing for many years. This database is of more interest
to NLP than others (for English) because of its:
</bodyText>
<listItem confidence="0.987675727272727">
• quality and consistency
• level of detail
• number of examples
• accountability to the corpus
• purity: it has been created only as an anal-
ysis of English, and has not been compro-
mised by publishing constraints or other non-
lexicographic goals
• availability, on licencing terms that promote its
research use and also the re-use of enhanced
versions created by NLP groups.
</listItem>
<sectionHeader confidence="0.990555" genericHeader="introduction">
2 The Project
</sectionHeader>
<bodyText confidence="0.9987295">
The overall project is the preparation of a New En-
glish Irish Dictionary, and is funded by Foras na
Gaeilge, the official body for the (Gaelic) Irish lan-
guage.1 The project was designed according to a
</bodyText>
<footnote confidence="0.9855525">
1FnG was set up following the Good Friday Agreement of
1998 on Northern Ireland, between the Governments of the Re-
</footnote>
<page confidence="0.991298">
21
</page>
<note confidence="0.460342">
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 21–24,
</note>
<page confidence="0.3086">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</page>
<bodyText confidence="0.9999917">
model where the first stage of the production of
a blingual dictionary is a target-language-neutral
monolingual analysis of the source language listing
all the phenomena that might possibly have an unex-
pected translation. (The next stages are then trans-
lation and ‘finishing’.) The 2.3 MEuro contract for
the analysis of English was won by Lexicography
MasterClass Ltd in 2007.2 The lexicographers are
working on the letter ‘s’ at time of writing and the
database will be complete in Summer 2010.
</bodyText>
<sectionHeader confidence="0.995283" genericHeader="method">
3 Lexicography
</sectionHeader>
<bodyText confidence="0.986021787234043">
Writing a dictionary is a large and complex under-
taking. Planning is paramount.
In the planning phase, we identified all the as-
pects of the behaviour of English words which
a full account of the lexicon should cover. We
then found words exemplifying all aspects, and pre-
pared a sample of one hundred model entries, where
the hundred words chosen covered all the prin-
cipal phenomena (Atkins and Grundy, 2006). A
detailed style guide and corresponding DTD were
written. We created the New Corpus for Ire-
land (NCI) (Kilgarriff, 2006), and set up a corpus
query system (Lexical Computing’s Sketch Engine;
http://www.sketchengine.co.uk) and dictionary edit-
ing system (IDM’s DPS: http://www.idm.fr) for the
project to use. 50,000 headwords were identified
and each was classified into one of eighteen cate-
gories according to type and complexity. This sup-
ported detailed planning of lexicographers’ work-
loads and hence, scheduling, as well as adding to the
richness of the data. Template entries (Atkins and
Rundell, 2008, pp123-128) were developed for 68
lexical sets and for words belonging to these sets, the
template was automatically inserted into the draft
dictionary, saving lexicographer time and encourag-
ing consistency.
We identified forty syntactic patterns for verbs,
eighteen for nouns and eighteen for adjectives. Lexi-
cographers were required to note all the patterns that
applied for each word sense.
The lexicographers were all known to the man-
agement team beforehand for their high-quality
public of Ireland and the UK. FnaG is an institution of the two
countries.
2Lexicography MasterClass had also previously undertaken
the planning of the project.
work. They were trained in the dictionary style
at two workshops, and their work was thoroughly
checked throughout the project, with failings re-
ported back and progress monitored.
A typical short entry is honeymoon (shown here
in full but for truncated examples). Note the level
of detail including senses, subsenses, grammatical
structures and collocations. All points are exem-
plified by one or usually more corpus example sen-
tences. (The style guide, available online, states the
conditions for giving one, two or three examples for
</bodyText>
<figure confidence="0.849363176470588">
a phenomenon.)
honeymoon
• n holiday after wedding
Following the wedding day, Jane and ...
Upon your return from honeymoon ...
Lee and Zoe left for a honeymoon in ...
SUPPORT VERB spend
They now live in Cumbernauld after spending...
Their honeymoon was spent at Sandals ...
SUPPORT VERB have
I hope that you have an absolutely fantastic ...
The reception was held at the local pub and ...
SUPPORT PREP on
I have a ring on my left hand which Martha ...
The groom whisked the bride off on honeymoon ...
This particular portrait was a festive affair, .. .
STRUCTURE N premod
</figure>
<bodyText confidence="0.937992428571429">
destination hotel suite holiday night couple
Classic honeymoon destinations like the ...
We can help and recommend all types of ...
We were staying in the honeymoon suite ...
A magical honeymoon holiday in the beautiful ...
Our honeymoon packages offer a wide range of ...
It is the favourite of our many honeymoon couples.
</bodyText>
<figure confidence="0.4361315">
• v spend one’s honeymoon
STRUCTURE Particle (locative)
They’ll be honeymooning in Paris (ooh, la la).
Mr and Mrs Maunder will honeymoon in ...
The couple spent the early part of their ...
A Dave Lister from five years in the future is ...
• n period of grace
VARIANT FORM honeymoon period
</figure>
<bodyText confidence="0.590897666666667">
Since his May 1997 landslide election, Blair has ...
The UN and Europe were pan national organisations
CHUNK the honeymoon is over
VARIANT the honey moon period is over
The shortest post-election honeymoon is over.
Could the honeymoon period be over that quickly?
</bodyText>
<page confidence="0.994669">
22
</page>
<sectionHeader confidence="0.742586" genericHeader="method">
4 Corpus strategy and innovation
</sectionHeader>
<bodyText confidence="0.999990472222222">
The project team combined expertise in corpora,
computational linguistics and lexicography, and
from the outset the project was to be solidly corpus-
based. In the planning phase we had built the NCI:
by the time the compilation phase started, in 2007, it
was evident not only that the NCI would no longer
capture current English, but also that the field had
moved on and at 250m words, it was too small.
We appended the Irish English data from the NCI
to the much larger and newer UKWaC (Ferraresi et
al., 2008) and added some contemporary American
newspaper text to create the project corpus, which
was then pos-tagged with TreeTagger 3 and loaded
into the Sketch Engine.
The distinctive feature of the Sketch Engine is
‘word sketches’: one-page, corpus-driven sum-
maries of a word’s grammatical and collocational
behaviour. The corpus is parsed and a table of col-
locations is given for each grammatical relation. For
DANTE, the set of grammatical relations was de-
fined to give an exact match to the grammatical pat-
terns that the lexicographers were to record. The
same names were used. The word sketch for the
word would, in so far as the POS-tagging, parsing,
and statistics worked correctly, identify precisely the
grammatical patterns and collocations that the lexi-
cographer needed to note in the dictionary.
As is evident, a very large number of corpus sen-
tences needed taking from the corpus into the dic-
tionary. This was streamlined with two processes:
GDEX, for sorting the examples so that the ‘best’
(according to a set of heuristics) are shown to the
lexicographer first (Kilgarriff et al., 2008), and ‘one-
click-copying’ of sentences onto the clipboard (in-
cluding highlighting the nodeword). (In contrast to
a finished dictionary, examples were not edited.)
</bodyText>
<sectionHeader confidence="0.740839" genericHeader="method">
5 XML-based dictionary preparation
</sectionHeader>
<bodyText confidence="0.998621666666667">
The document type definition uses seventy-two el-
ements. It is as restrictive as possible, given that
accuracy and then clarity take priority. Lexicogra-
phers were not permitted to submit work which did
not validate. Wherever there was a fixed range of
possible values for an information field, the list was
</bodyText>
<footnote confidence="0.737911">
3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
</footnote>
<bodyText confidence="0.999864454545455">
included in the DTD as possible values for an at-
tribute and the lexicographer used menu-selection
rather than text-entry.
The database was also used for checking potential
problems in a number of ways. For example, there
are some word senses where examples are not re-
quired, but it is unusual for both senses of a two-or-
more-sense word not to need examples, so we rou-
tinely used XML searching to check lexicographers’
work for any such cases and scrutinised them prior
to approval.
</bodyText>
<sectionHeader confidence="0.65315" genericHeader="method">
6 None of the usual constraints
</sectionHeader>
<bodyText confidence="0.999897">
Most dictionary projects are managed by publishers
who are focused on the final (usually print) product,
so constraints such as fitting in limited page-space,
or using simplified codes to help naive users, or re-
sponding to the marketing department, or tailoring
the analysis according to the specialist interests of
some likely users, or features of the target language
(for a bilingual dictionary) usually play a large role
in the instructions given to lexicographers. In this
project, with the separation of the project team from
the publisher, we were unusually free of such com-
promising factors.
</bodyText>
<sectionHeader confidence="0.988479" genericHeader="method">
7 Leadership
</sectionHeader>
<bodyText confidence="0.99383825">
Many lexicographic projects take years or decades
longer than scheduled, and suffer changes of intel-
lectual leadership, or are buffeted by political and
economic constraints, all of which produce grave in-
consistencies of style, scale and quality between dif-
ferent sections of the data. A consistent lexicon is
impossible without consistent and rigorous manage-
ment. The credentials of the managers are an indi-
cator of the likely quality of the data.
Sue Atkins, the project manager, has been
the driving force behind the Collins-Robert En-
glish/French Dictionaries (first two editions), the
COBUILD project (with John Sinclair), The Euro-
pean Association for Lexicography (with Reinhart
Hartmann), the British National Corpus, the Ox-
ford Hachette English/French dictionaries (assisted
by Valerie Grundy, DANTE Chief Editor) and with
Charles Fillmore, FrameNet. She has co-published
the Oxford Guide to Practical Lexicography with
Michael Rundell, another of the project management
</bodyText>
<page confidence="0.995524">
23
</page>
<bodyText confidence="0.999636">
team, who has been Managing Editor of a large num-
ber of dictionaries at Longman and Macmillan.
</bodyText>
<sectionHeader confidence="0.919521" genericHeader="evaluation">
8 Licencing
</sectionHeader>
<bodyText confidence="0.999991363636363">
In the late 1980s it seemed likely that Longman Dic-
tionary of Contemporary English (LDOCE) would
have a great impact on NLP. But its star rose, but
then promptly fell. As a Longman employee with
the task of developing LDOCE use within NLP, the
first author investigated the reasons long and hard.
The problem was that NLP groups could not
do anything with their LDOCE-based work. They
could describe the work in papers, but the work it-
self was embedded in enhanced versions of LDOCE,
or LDOCE-derived resources, and the licence that
allowed them to use LDOCE did not alow them to
publish or licence or give away any such resource.
So LDOCE research, for academics, was a dead end.
A high-quality dictionary represents an invest-
ment of millions so one cannot expect its owners to
give it away. The challenge then is to arrive at a
model for a dictionary’s use in which its exploration
and enhancement is encouraged, and is not a dead
end, and also in which the owner’s interest in a re-
turn on investment is respected.
DANTE will be made available in a way designed
to meet these goals. It will be licenced for NLP re-
search for no fee. The licence will not allow the
licencee to pass on the resource, but will include an
undertaking from the owner to pass on the licencee’s
enhanced version to other groups on the same terms
(provided it passes quality tests). The owner, or its
agent, will also, where possible, integrate and cross-
validate enhancements from different users. The
owner will retain the right to licence the enhanced
data, for a fee, for commercial use. The model is
presented fully in (Kilgarriff, 1998).
</bodyText>
<sectionHeader confidence="0.996093" genericHeader="conclusions">
9 DANTE Disambiguation
</sectionHeader>
<bodyText confidence="0.999937769230769">
‘DANTE disambiguation’ is a program currently in
preparation which takes arbitrary text and, for each
content word in the text, identifies the DANTE pat-
terns it matches and thereby assigns it to one of the
word’s senses in the DANTE database. It is designed
to demonstrate the potential that DANTE has for
NLP, and to undertake in a systematic way a piece
of work that many DANTE users would otherwise
need to do themselves: converting as many DANTE
data fields as possible into methods which either do
or do not match a particular instance of the word.
The program will be freely available alongside the
database.
</bodyText>
<sectionHeader confidence="0.996648" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.969082">
Thanks to colleagues on the project, particularly the
management team of Sue Atkins, Michael Rundell,
Valerie Grundy, Diana Rawlinson and Cathal Con-
very.
</bodyText>
<sectionHeader confidence="0.996764" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999463058823529">
Sue Atkins and Valerie Grundy. 2006. Lexicographic
profiling: an aid to consistency in dictionary entry de-
sign. In Proc. Euralex, Torino.
Sue Atkins and Michael Rundell. 2008. Oxford Guide to
Practical Lexicography. OUP, Oxford.
Bran Boguraev and Ted Briscoe, editors. 1989. Compu-
tational lexicographyfor natural language processing.
Longman, London.
Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srini-
vas, and Martin Zaidel. 1994. Xtag system: a wide
coverage grammar for english. In Proc. COLING,
pages 922–928.
Christiane Fellbaum, editor. 1998. WordNet, an elec-
tronic lexical database. MIT Press.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
UKWaC, a very large web-derived corpus of English.
In Proc ˙WAC4, LREC, Marrakesh.
Adam Kilgarriff, Milos Husak, Katy McAdam, Michael
Rundell, and Pavel Rychly. 2008. Gdex: Automati-
cally finding good dictionary examples in a corpus. In
Proc. Euralex, Barcelona.
Adam Kilgarriff. 1998. Business models for dictioanries
and NLP. Int Jnl Lexicography, 13(2):107–118.
Adam Kilgarriff. 2006. Efficient corpus development
for lexicography: building the new corpus for ireland.
Language Resources and Evaluation Journal.
Catherine Macleod, Ralph Grishman, and Adam Mey-
ers. 1994. The comlex syntax project: the first year.
In Proc ˙Human Language Technology workshop, pages
8–12.
Yorick Wilks, Brian Slator, and Louise Guthrie. 1996.
Electric words: dictionaries, computers, and mean-
ings. MIT Press, Cambridge, MA, USA.
</reference>
<page confidence="0.999177">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.593014">
<title confidence="0.999164">A Detailed, Accurate, Extensive, Available English Lexical Database</title>
<author confidence="0.974553">Adam</author>
<affiliation confidence="0.993082">Lexical Computing</affiliation>
<address confidence="0.626077">Brighton,</address>
<email confidence="0.999626">adam@lexmasterclass.com</email>
<abstract confidence="0.998711736842105">We present an English lexical database which is fuller, more accurate and more consistent than any other. We believe this to be so because the project has been well-planned, with a 12-month intensive planning phase prior to the lexicography beginning; well-resourced, employing a team of fifteen highly experienced lexicographers for a thirty-month main phase; it has had access to the latest corpus and dictionary-editing technology; it has not been constrained to meet any goals other than an accurate description of the language; and it has been led by a team with singular experience in delivering high-quality and innovative resources. The lexicon will be complete in Summer 2010 and will be available for NLP groups, on terms designed to encourage its research use.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sue Atkins</author>
<author>Valerie Grundy</author>
</authors>
<title>Lexicographic profiling: an aid to consistency in dictionary entry design.</title>
<date>2006</date>
<booktitle>In Proc. Euralex,</booktitle>
<location>Torino.</location>
<contexts>
<context position="4140" citStr="Atkins and Grundy, 2006" startWordPosition="654" endWordPosition="657">tract for the analysis of English was won by Lexicography MasterClass Ltd in 2007.2 The lexicographers are working on the letter ‘s’ at time of writing and the database will be complete in Summer 2010. 3 Lexicography Writing a dictionary is a large and complex undertaking. Planning is paramount. In the planning phase, we identified all the aspects of the behaviour of English words which a full account of the lexicon should cover. We then found words exemplifying all aspects, and prepared a sample of one hundred model entries, where the hundred words chosen covered all the principal phenomena (Atkins and Grundy, 2006). A detailed style guide and corresponding DTD were written. We created the New Corpus for Ireland (NCI) (Kilgarriff, 2006), and set up a corpus query system (Lexical Computing’s Sketch Engine; http://www.sketchengine.co.uk) and dictionary editing system (IDM’s DPS: http://www.idm.fr) for the project to use. 50,000 headwords were identified and each was classified into one of eighteen categories according to type and complexity. This supported detailed planning of lexicographers’ workloads and hence, scheduling, as well as adding to the richness of the data. Template entries (Atkins and Rundel</context>
</contexts>
<marker>Atkins, Grundy, 2006</marker>
<rawString>Sue Atkins and Valerie Grundy. 2006. Lexicographic profiling: an aid to consistency in dictionary entry design. In Proc. Euralex, Torino.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sue Atkins</author>
<author>Michael Rundell</author>
</authors>
<date>2008</date>
<booktitle>Oxford Guide to Practical Lexicography. OUP,</booktitle>
<location>Oxford.</location>
<contexts>
<context position="4747" citStr="Atkins and Rundell, 2008" startWordPosition="746" endWordPosition="749">and Grundy, 2006). A detailed style guide and corresponding DTD were written. We created the New Corpus for Ireland (NCI) (Kilgarriff, 2006), and set up a corpus query system (Lexical Computing’s Sketch Engine; http://www.sketchengine.co.uk) and dictionary editing system (IDM’s DPS: http://www.idm.fr) for the project to use. 50,000 headwords were identified and each was classified into one of eighteen categories according to type and complexity. This supported detailed planning of lexicographers’ workloads and hence, scheduling, as well as adding to the richness of the data. Template entries (Atkins and Rundell, 2008, pp123-128) were developed for 68 lexical sets and for words belonging to these sets, the template was automatically inserted into the draft dictionary, saving lexicographer time and encouraging consistency. We identified forty syntactic patterns for verbs, eighteen for nouns and eighteen for adjectives. Lexicographers were required to note all the patterns that applied for each word sense. The lexicographers were all known to the management team beforehand for their high-quality public of Ireland and the UK. FnaG is an institution of the two countries. 2Lexicography MasterClass had also prev</context>
</contexts>
<marker>Atkins, Rundell, 2008</marker>
<rawString>Sue Atkins and Michael Rundell. 2008. Oxford Guide to Practical Lexicography. OUP, Oxford.</rawString>
</citation>
<citation valid="true">
<date>1989</date>
<booktitle>Computational lexicographyfor natural language processing. Longman,</booktitle>
<editor>Bran Boguraev and Ted Briscoe, editors.</editor>
<location>London.</location>
<marker>1989</marker>
<rawString>Bran Boguraev and Ted Briscoe, editors. 1989. Computational lexicographyfor natural language processing. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christy Doran</author>
<author>Dania Egedi</author>
<author>Beth Ann Hockey</author>
<author>B Srinivas</author>
<author>Martin Zaidel</author>
</authors>
<title>Xtag system: a wide coverage grammar for english.</title>
<date>1994</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>922--928</pages>
<contexts>
<context position="1173" citStr="Doran et al., 1994" startWordPosition="179" endWordPosition="182">d dictionary-editing technology; it has not been constrained to meet any goals other than an accurate description of the language; and it has been led by a team with singular experience in delivering high-quality and innovative resources. The lexicon will be complete in Summer 2010 and will be available for NLP groups, on terms designed to encourage its research use. 1 Introduction Most NLP applications need lexicons. NLP researchers have used databases from dictionary publishers (Boguraev and Briscoe, 1989; Wilks et al., 1996), or developed NLP resources (COMLEX (Macleod et al., 1994), XTAG (Doran et al., 1994)) or used WordNet,(Fellbaum, 1998) or have switched to fully corpus-based strategies which need no lexicons. However the publishers’ dictionaries were pre-corpus, often inconsistent, and licencing constraints were in the end fatal. COMLEX and XTAG address only syntax; WordNet, only semantics. Also these resources were not produced by experienced lexicographers, nor according to a detailed, stringent ‘style guide’ specifying how to handle all the phenomena (in orthography, morphology, syntax, semantics and pragmatics, from spelling variation to register to collocation to sense distinction) that</context>
</contexts>
<marker>Doran, Egedi, Hockey, Srinivas, Zaidel, 1994</marker>
<rawString>Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srinivas, and Martin Zaidel. 1994. Xtag system: a wide coverage grammar for english. In Proc. COLING, pages 922–928.</rawString>
</citation>
<citation valid="true">
<title>WordNet, an electronic lexical database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet, an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating UKWaC, a very large web-derived corpus of English.</title>
<date>2008</date>
<booktitle>In Proc ˙WAC4, LREC, Marrakesh.</booktitle>
<contexts>
<context position="7925" citStr="Ferraresi et al., 2008" startWordPosition="1280" endWordPosition="1283">election honeymoon is over. Could the honeymoon period be over that quickly? 22 4 Corpus strategy and innovation The project team combined expertise in corpora, computational linguistics and lexicography, and from the outset the project was to be solidly corpusbased. In the planning phase we had built the NCI: by the time the compilation phase started, in 2007, it was evident not only that the NCI would no longer capture current English, but also that the field had moved on and at 250m words, it was too small. We appended the Irish English data from the NCI to the much larger and newer UKWaC (Ferraresi et al., 2008) and added some contemporary American newspaper text to create the project corpus, which was then pos-tagged with TreeTagger 3 and loaded into the Sketch Engine. The distinctive feature of the Sketch Engine is ‘word sketches’: one-page, corpus-driven summaries of a word’s grammatical and collocational behaviour. The corpus is parsed and a table of collocations is given for each grammatical relation. For DANTE, the set of grammatical relations was defined to give an exact match to the grammatical patterns that the lexicographers were to record. The same names were used. The word sketch for the </context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating UKWaC, a very large web-derived corpus of English. In Proc ˙WAC4, LREC, Marrakesh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Milos Husak</author>
<author>Katy McAdam</author>
<author>Michael Rundell</author>
<author>Pavel Rychly</author>
</authors>
<title>Gdex: Automatically finding good dictionary examples in a corpus.</title>
<date>2008</date>
<booktitle>In Proc. Euralex,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="9021" citStr="Kilgarriff et al., 2008" startWordPosition="1459" endWordPosition="1462">act match to the grammatical patterns that the lexicographers were to record. The same names were used. The word sketch for the word would, in so far as the POS-tagging, parsing, and statistics worked correctly, identify precisely the grammatical patterns and collocations that the lexicographer needed to note in the dictionary. As is evident, a very large number of corpus sentences needed taking from the corpus into the dictionary. This was streamlined with two processes: GDEX, for sorting the examples so that the ‘best’ (according to a set of heuristics) are shown to the lexicographer first (Kilgarriff et al., 2008), and ‘oneclick-copying’ of sentences onto the clipboard (including highlighting the nodeword). (In contrast to a finished dictionary, examples were not edited.) 5 XML-based dictionary preparation The document type definition uses seventy-two elements. It is as restrictive as possible, given that accuracy and then clarity take priority. Lexicographers were not permitted to submit work which did not validate. Wherever there was a fixed range of possible values for an information field, the list was 3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ included in the DTD as possible val</context>
</contexts>
<marker>Kilgarriff, Husak, McAdam, Rundell, Rychly, 2008</marker>
<rawString>Adam Kilgarriff, Milos Husak, Katy McAdam, Michael Rundell, and Pavel Rychly. 2008. Gdex: Automatically finding good dictionary examples in a corpus. In Proc. Euralex, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Business models for dictioanries and NLP.</title>
<date>1998</date>
<journal>Int Jnl Lexicography,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="13418" citStr="Kilgarriff, 1998" startWordPosition="2175" endWordPosition="2176">nt is respected. DANTE will be made available in a way designed to meet these goals. It will be licenced for NLP research for no fee. The licence will not allow the licencee to pass on the resource, but will include an undertaking from the owner to pass on the licencee’s enhanced version to other groups on the same terms (provided it passes quality tests). The owner, or its agent, will also, where possible, integrate and crossvalidate enhancements from different users. The owner will retain the right to licence the enhanced data, for a fee, for commercial use. The model is presented fully in (Kilgarriff, 1998). 9 DANTE Disambiguation ‘DANTE disambiguation’ is a program currently in preparation which takes arbitrary text and, for each content word in the text, identifies the DANTE patterns it matches and thereby assigns it to one of the word’s senses in the DANTE database. It is designed to demonstrate the potential that DANTE has for NLP, and to undertake in a systematic way a piece of work that many DANTE users would otherwise need to do themselves: converting as many DANTE data fields as possible into methods which either do or do not match a particular instance of the word. The program will be f</context>
</contexts>
<marker>Kilgarriff, 1998</marker>
<rawString>Adam Kilgarriff. 1998. Business models for dictioanries and NLP. Int Jnl Lexicography, 13(2):107–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Efficient corpus development for lexicography: building the new corpus for ireland. Language Resources and Evaluation Journal.</title>
<date>2006</date>
<contexts>
<context position="4263" citStr="Kilgarriff, 2006" startWordPosition="676" endWordPosition="677">s’ at time of writing and the database will be complete in Summer 2010. 3 Lexicography Writing a dictionary is a large and complex undertaking. Planning is paramount. In the planning phase, we identified all the aspects of the behaviour of English words which a full account of the lexicon should cover. We then found words exemplifying all aspects, and prepared a sample of one hundred model entries, where the hundred words chosen covered all the principal phenomena (Atkins and Grundy, 2006). A detailed style guide and corresponding DTD were written. We created the New Corpus for Ireland (NCI) (Kilgarriff, 2006), and set up a corpus query system (Lexical Computing’s Sketch Engine; http://www.sketchengine.co.uk) and dictionary editing system (IDM’s DPS: http://www.idm.fr) for the project to use. 50,000 headwords were identified and each was classified into one of eighteen categories according to type and complexity. This supported detailed planning of lexicographers’ workloads and hence, scheduling, as well as adding to the richness of the data. Template entries (Atkins and Rundell, 2008, pp123-128) were developed for 68 lexical sets and for words belonging to these sets, the template was automaticall</context>
</contexts>
<marker>Kilgarriff, 2006</marker>
<rawString>Adam Kilgarriff. 2006. Efficient corpus development for lexicography: building the new corpus for ireland. Language Resources and Evaluation Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
</authors>
<title>The comlex syntax project: the first year.</title>
<date>1994</date>
<booktitle>In Proc ˙Human Language Technology workshop,</booktitle>
<pages>8--12</pages>
<contexts>
<context position="1146" citStr="Macleod et al., 1994" startWordPosition="174" endWordPosition="177">ccess to the latest corpus and dictionary-editing technology; it has not been constrained to meet any goals other than an accurate description of the language; and it has been led by a team with singular experience in delivering high-quality and innovative resources. The lexicon will be complete in Summer 2010 and will be available for NLP groups, on terms designed to encourage its research use. 1 Introduction Most NLP applications need lexicons. NLP researchers have used databases from dictionary publishers (Boguraev and Briscoe, 1989; Wilks et al., 1996), or developed NLP resources (COMLEX (Macleod et al., 1994), XTAG (Doran et al., 1994)) or used WordNet,(Fellbaum, 1998) or have switched to fully corpus-based strategies which need no lexicons. However the publishers’ dictionaries were pre-corpus, often inconsistent, and licencing constraints were in the end fatal. COMLEX and XTAG address only syntax; WordNet, only semantics. Also these resources were not produced by experienced lexicographers, nor according to a detailed, stringent ‘style guide’ specifying how to handle all the phenomena (in orthography, morphology, syntax, semantics and pragmatics, from spelling variation to register to collocation</context>
</contexts>
<marker>Macleod, Grishman, Meyers, 1994</marker>
<rawString>Catherine Macleod, Ralph Grishman, and Adam Meyers. 1994. The comlex syntax project: the first year. In Proc ˙Human Language Technology workshop, pages 8–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
<author>Brian Slator</author>
<author>Louise Guthrie</author>
</authors>
<title>Electric words: dictionaries, computers, and meanings.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1087" citStr="Wilks et al., 1996" startWordPosition="165" endWordPosition="168">exicographers for a thirty-month main phase; it has had access to the latest corpus and dictionary-editing technology; it has not been constrained to meet any goals other than an accurate description of the language; and it has been led by a team with singular experience in delivering high-quality and innovative resources. The lexicon will be complete in Summer 2010 and will be available for NLP groups, on terms designed to encourage its research use. 1 Introduction Most NLP applications need lexicons. NLP researchers have used databases from dictionary publishers (Boguraev and Briscoe, 1989; Wilks et al., 1996), or developed NLP resources (COMLEX (Macleod et al., 1994), XTAG (Doran et al., 1994)) or used WordNet,(Fellbaum, 1998) or have switched to fully corpus-based strategies which need no lexicons. However the publishers’ dictionaries were pre-corpus, often inconsistent, and licencing constraints were in the end fatal. COMLEX and XTAG address only syntax; WordNet, only semantics. Also these resources were not produced by experienced lexicographers, nor according to a detailed, stringent ‘style guide’ specifying how to handle all the phenomena (in orthography, morphology, syntax, semantics and pra</context>
</contexts>
<marker>Wilks, Slator, Guthrie, 1996</marker>
<rawString>Yorick Wilks, Brian Slator, and Louise Guthrie. 1996. Electric words: dictionaries, computers, and meanings. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>