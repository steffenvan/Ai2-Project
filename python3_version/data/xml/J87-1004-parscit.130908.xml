<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.868349">
AN EFFICIENT AUGMENTED-CONTEXT-FREE PARSING ALGORITHM&apos;
</title>
<author confidence="0.942773">
Masaru Tomita
</author>
<affiliation confidence="0.874287">
Computer Science Department
</affiliation>
<email confidence="0.18438">
and
</email>
<sectionHeader confidence="0.434818" genericHeader="abstract">
Center for Machine Translation
Carnegie-Mellon University
Pittsburgh, PA 15213
</sectionHeader>
<bodyText confidence="0.987908411764706">
An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to
on-line natural language interfaces discussed. The algorithm is a generalized LR parsing algorithm, which
precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented
context-free grammar. Unlike the standard LR parsing algorithm, it can handle arbitrary context-free
grammars, including ambiguous grammars, while most of the LR efficiency is preserved by introducing the
concept of a &amp;quot;graph-structured stack&amp;quot;. The graph-structured stack allows an LR shift-reduce parser to
maintain multiple parses without parsing any part of the input twice in the same way. We can also view our
parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables. The algo-
rithm is fast, due to the LR table precomputation. In several experiments with different English grammars
and sentences, timings indicate a five- to tenfold speed advantage over Earley&apos;s context-free parsing algo-
rithm.
The algorithm parses a sentence strictly from left to right on-line, that is, it starts parsing as soon as the
user types in the first word of a sentence, without waiting for completion of the sentence. A practical
on-line parser based on the algorithm has been implemented in Common Lisp, and running on Symbolics
and HP Al workstations. The parser is used in the multi-lingual machine translation project at CMU. Also,
a commercial on-line parser for Japanese language is being built by Intelligent Technology Incorporation,
based on the technique developed at CMU.
</bodyText>
<sectionHeader confidence="0.998381" genericHeader="introduction">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.9999361">
Parsing efficiency is crucial when building practical
natural language systems on smaller computers such as
personal workstations. This is especially the case for
interactive systems such as natural language database
access, interfaces to expert systems, and interactive
machine translation. This paper introduces an efficient
on-line parsing algorithm, and focuses on its practical
application to natural language interfaces.
The algorithm can be viewed as a generalized LR pars-
ing algorithm that can handle arbitrary context-free
grammars, including ambiguous grammars. Section 2
describes the algorithm by extending the standard LR
parsing algorithm with the idea of a &amp;quot;graph-structured
stack&amp;quot;. Section 3 describes how to represent parse trees
efficiently, so that all possible parse trees (the parse
forest) take at most polynomial space as the ambiguity of
a sentence grows exponentially. In section 4, several
examples are given. Section 5 presents several empirical
results of the algorithm&apos;s practical performance, including
comparison with Earley&apos;s algorithm. In section 6, we
discuss how to enhance the algorithm to handle
augmented context-free grammars rather than pure
context-free grammars. Section 7 describes the concept
of on-line parsing, taking advantage of left-to-right oper-
ation of our parsing algorithm. The on-line parser parses
a sentence strictly from left to right, and starts parsing as
soon as the user types in the first word, without waiting
for the end of line. Benefits of on-line parsing are then
discussed. Finally, several versions of on-line parser have
been implemented, and they are mentioned in section 8.
</bodyText>
<sectionHeader confidence="0.994161" genericHeader="method">
2 THE CONTEXT-FREE PARSING ALGORITHM
</sectionHeader>
<bodyText confidence="0.911170714285714">
The LR parsing algorithms (Aho and Ullman 1972, Aho
and Johnson 1974) were developed originally for
programming languages. An LR parsing algorithm is a
Copyright 1987 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that
the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy
otherwise, or to republish, requires a fee and/or specific permission.
0362-613X/87 /010031-46$ 03.00
</bodyText>
<note confidence="0.7799085">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 31
Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm
</note>
<bodyText confidence="0.99981327027027">
shift-reduce parsing algorithm deterministically guided by
a parsing table indicating what action should be taken
next. The parsing table can be obtained automatically
from a context-free phrase structure grammar, using an
algorithm first developed by DeRemer (1969, 1971). We
do not describe the algorithms here, referring the reader
to chapter 6 in Aho and Ullman (1977). We assume that
the reader is familiar with the standard LR parsing algo-
rithm (not necessarily with the parsing table construction
algorithm).
The LR paring algorithm is one of the most efficient
parsing algorithms. It is totally deterministic, and no
backtracking or search is involved. Unfortunately, we
cannot directly adopt the LR parsing technique for
natural languages, because it is applicable only to a small
subset of context-free grammars called LR grammars,
and it is almost certain that any practical natural
language grammars are not LR. If a grammar is non-LR,
its parsing table will have multiple entries;1 one or more
of the action table entries will be multiply defined (Shie-
ber 1983). Figures 2.1 and 2.2 show an example of a
non-LR grammar and its parsing table. Grammar symbols
starting with &amp;quot;s&amp;quot; represent pre-terminals. Entries &amp;quot;sh n&amp;quot;
in the action table (the left part of the table) indicate the
action &amp;quot;shift one word from input buffer onto the stack,
and go to state n&amp;quot;. Entries &amp;quot;re n&amp;quot; indicate the action
&amp;quot;reduce constituents on the stack using rule n&amp;quot;. The
entry &amp;quot;acc&amp;quot; stands for the action &amp;quot;accept&amp;quot;, and blank
spaces represent &amp;quot;error&amp;quot;. The goto table (the right part
of the table) decides to what state the parser should go
after a reduce action. These operations shall become
clear when we trace the algorithm with example
sentences in section 4. The exact definition and operation
of the LR parser can be found in Aho and Ullman
(1977).
We can see that there are two multiple entries in the
action table; on the rows of state 11 and 12 at the
</bodyText>
<listItem confidence="0.999152428571428">
(1) S --&gt; NP VP
(2) S --&gt; S PP
(3) NP --&gt; on
(4) NP --&gt; odet on
(5) NP --&gt; NP PP
(6) PP --&gt; *prep NP
(7) VP --&gt; ov NP
</listItem>
<figureCaption confidence="0.985453">
Figure 2.1. An example ambiguous grammar.
</figureCaption>
<bodyText confidence="0.999940222222222">
column labeled &amp;quot;prep&amp;quot;. Roughly speaking, this is the
situation where the parser encounters a preposition of a
PP right after a NP. If this PP does not modify the NP,
then the parser can go ahead to reduce the NP into a
higher nonterminal such as PP or VP, using rule 6 or 7,
respectively (re&amp; and re7 in the multiple entries). If, on
the other hand, the PP does modify the NP, then the
parser must wait (sh6) until the PP is completed so it can
build a higher NP using rule 5.
It has been thought that, for LR parsing, multiple
entries are fatal because once a parsing table has multiple
entries, deterministic parsing is no longer possible and
some kind of non-determinism is necessary. We handle
multiple entries with a special technique, named a graph-
structured stack. In order to introduce the concept, we
first give a simpler form of non-determinism, and make
refinements on it. Subsection 2.1 describes a simple and
straightforward non-deterministic technique, that is,
pseudo-parallelism (breadth-first search), in which the
system maintains a number of stacks simultaneously,
called the Stack List. A disadvantage of the stack list is
then described. The next subsection describes the idea of
stack combination, which was introduced in the author&apos;s
earlier research (Tomita 1984), to make the algorithm
much more efficient. With this idea, stacks are repres-
ented as trees (or a forest). Finally, a further refinement,
the graph-structured stack, is described to make the algo-
</bodyText>
<figure confidence="0.9610845">
State *det on ov *prep $ NP PP VP S
0 sh3 sh4 2 1
1 sh6 acc 5
2 sh7 sh6 9 8
3 sh10
4 re3 re3 re3
5 re2 re2
6 sh3 sh4 11
7 sh3 sh4 12
8 rel rel
9 re5 re5 re5
10 re4 re4 re4
11 re6 re6,sh6 re6 9
12 re7,sh6 rel
</figure>
<figureCaption confidence="0.998108">
Figure 2.2. LR parsing table with multiple entries.
</figureCaption>
<page confidence="0.881899">
32 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.467097">
Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm
</note>
<bodyText confidence="0.9708835">
rithm even more efficient; efficient enough to run in
polynomial time.
</bodyText>
<subsectionHeader confidence="0.904051">
2.1 HANDLING MULTIPLE ENTRIES WITH STACK LIST
</subsectionHeader>
<bodyText confidence="0.994045716981132">
The simplest idea would be to handle multiple entries
non-deterministically. We adopt pseudo-parallelism
(breadth-first search), maintaining a list of stacks (the
Stack List). The pseudo-parallelism works as follows.
A number of processes are operated in parallel. Each
process has a stack and behaves basically the same as in
standard LR parsing. When a process encounters a multi-
ple entry, the process is split into several processes (one
for each entry), by replicating its stack. When a process
encounters an error entry, the process is killed, by
removing its stack from the stack list. All processes are
synchronized; they shift a word at the same time so that
they always look at the same word. Thus, if a process
encounters a shift action, it waits until all other processes
also encounter a (possibly different) shift action.
Figure 2.3 shows a snapshot of the stack list right after
shifting the word with in the sentence I saw a man on the
bed in the apartment with a telescope using the grammar in
Figure 2.1 and the parsing table in Figure 2.2. For the
sake of convenience, we denote a stack with vertices and
edges. The leftmost vertex is the bottom of the stack,
and the rightmost vertex is the top of the stack. Vertices
represented by a circle are called state vertices, and they
represent a state number. Vertices represented by a
square are called symbol vertices, and they represent a
grammar symbol. Each stack is exactly the same as a
stack in the standard LR parsing algorithm. The distance
between vertices (length of an edge) does not have any
significance, except it may help the reader understand the
status of the stacks. In the figures, &amp;quot;p&amp;quot; stands for *prep,
and &amp;quot;d&amp;quot; stands for *det throughout this paper.
Since the sentence is 14-way ambiguous, the stack has
been split into 14 stacks. For example, the sixth stack
(05 1 *p 6 NP 11 *p 6)
is in the status where I saw a man on the bed has been
reduced into S, and the apartment has been reduced into
NP. From the LR parsing table, we know that the top of
the stack, state 6, is expecting *det or *n and eventually a
NP. Thus, after a telescope comes in, a PP with a telescope
will be formed, and the PP will modify the NP the apart-
ment, and in the apartment will modify the S I saw a man.
We notice that some stacks in the stack list appear to
be identical. This is because they have reached the
current state in different ways. For example, the sixth
and seventh stacks are identical, because I saw a man on
the bed has been reduced into S in two different ways.
A disadvantage of the stack list method is that there
are no interconnections between stacks (processes), and
there is no way in which a process can utilize what other
processes have done already. The number of stacks in the
stack list grows exponentially as ambiguities are
encountered.3 For example, these 14 processes in Figure
2.3 will parse the rest of the sentence the telescope 14
</bodyText>
<figure confidence="0.97572635">
S 1 *P 6
S 1 •
*p 6
s •
*p 6
s 1 •
• p 6
S U.
ap
S 1 *P 6 NP 11 •
S 1 IFP 6 NP 11 U.
*p 6
NP 2 *v 7 NP 12 13) 6 NP 11 •
412 6
NP 2 4v 7 NP 12 6 NP 11 111) 6 NP 11 ap 6
S 1 *P 6 NP 11 *p 6 NP 11 *P 6
NP 2 *v 7 NP 12 *p 6 NP 11 *P 6
*p 6 NP 11 *P
NP 2 *v NP 1 *P
NP 2 *v NP II *P
</figure>
<figureCaption confidence="0.999465">
Figure 2.3. Stack list: after shifting with in
</figureCaption>
<bodyText confidence="0.993584">
I saw a man on the bed in the apartment with a telescope
(with the the grammar and the table in Figures 2.1 and 2.2).
times in exactly the same way. This can be avoided by
using a tree-structured stack, which is described in the
following subsection.
</bodyText>
<subsectionHeader confidence="0.999646">
2.2 WITH A TREE-STRUCTURE STACK
</subsectionHeader>
<bodyText confidence="0.999691333333333">
If two processes are in a common state, that is, if two
stacks have a common state number at the rightmost
vertex, they will behave in exactly the same manner until
the vertex is popped from the stacks by a reduce action.
To avoid this redundant operation, these processes are
unified into one process by combining their stacks.
Whenever two or more processes have a common state
number on the top of their stacks, the top vertices are
unified, and these stacks are represented as a tree, where
the top vertex corresponds to the root of the tree. We call
this a tree-structured stack. When the top vertex is
popped, the tree-structured stack is split into the original
number of stacks. In general, the system maintains a
number of tree-structured stacks in parallel, so stacks are
represented as a forest. Figure 2.4 shows a snapshot of
the tree-structured stack immediately after shifting the
word with. In contrast to the previous example, the tele-
scope will be parsed only once.
Although the amount of computation is significantly
reduced by the stack combination technique, the number
of branches of the tree-structured stack (the number of
</bodyText>
<figure confidence="0.985151363636364">
Ot3 OC2 OC/ 0° 0° 0° .°
o
0
0
0
•
0
0
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 33
An Efficient Augmented-Context-Free Parsing Algorithm
Masaru Tomita
</figure>
<figureCaption confidence="0.999863">
Figure 2.4. A tree-structured stack.
</figureCaption>
<bodyText confidence="0.9998678">
bottoms of the stack) that must be maintained still grows
exponentially as ambiguities are encountered. In the next
subsection, we describe a further modification in which
stacks are represented as a directed acyclic graph, in
order to avoid such inefficiency.
</bodyText>
<subsectionHeader confidence="0.998728">
2.3 WITH A GRAPH-STRUCTURE STACK
</subsectionHeader>
<bodyText confidence="0.99736364516129">
So far, when a stack is split, a copy of the whole stack is
made. However, we do not necessarily have to copy the
whole stack: even after different parallel operations on
the tree-structured stack, the bottom portion of the stack
may remain the same. Only the necessary portion of the
stack should therefore be split. When a stack is split, the
stack is thus represented as a tree, where the bottom of
the stack corresponds to the root of the tree. With the
stack combination technique described in the previous
subsection, stacks are represented as a directed acyclic
graph. Figure 2.5 shows a snapshot of the graph stack. It
is easy to show that the algorithm with the graph-struc-
tured stack does not parse any part of an input sentence
more than once in the same way. This is because, if two
processes had parsed a part of a sentence in the same
way, they would have been in the same state, and they
would have been combined as one process.
The graph-structured stack looks very similar to a
chart in chart parsing. In fact, one can also view our
algorithm as an extended chart parsing algorithm that is
guided by LR parsing tables. The major extension is that
nodes in the chart contain more information (LR state
numbers) than in conventional chart parsing. In this
paper, however, we describe the algorithm as a general-
ized LR parsing algorithm only.
So far, we have focussed on how to accept or reject a
sentence. In practice, however, the parser must not only
accept or reject sentences but also build the syntactic
structure(s) of the sentence (parse forest). The next
section describes how to represent the parse forest and
how to build it with our parsing algorithm.
</bodyText>
<figureCaption confidence="0.985727">
Figure 2.5. A graph-structured stack.
</figureCaption>
<sectionHeader confidence="0.992449" genericHeader="method">
3 AN EFFICIENT REPRESENTATION OF A PARSE FOREST
</sectionHeader>
<bodyText confidence="0.999891190476191">
Our parsing algorithm is an all-path parsing algorithm;
that is, it produces all possible parses in case an input
sentence is ambiguous. Such all-path parsing is of ten
needed in natural language processing to manage tempo-
rarily or absolutely ambiguous input sentences. The
ambiguity (the number of parses) of a sentence may
grow exponentially as the length of a sentence grows
(Church and Patil 1982). Thus, one might notice that,
even with an efficient parsing algorithm such as the one
we described, the parser would take exponential time
because exponential time would be required merely to
print out all parse trees (parse forest). We must therefore
provide an efficient representation so that the size of the
parse forest does not grow exponentially.
This section describes two techniques for providing an
efficient representation: subtree sharing and local ambi-
guity packing. It should be mentioned that these two
techniques are not completely new ideas, and some exist-
ing systems (e.g., Earley&apos;s (1970) algorithm) have
already adopted these techniques, either implicitly or
explicitly.
</bodyText>
<subsectionHeader confidence="0.998316">
3.1 SUB-TREE SHARING
</subsectionHeader>
<bodyText confidence="0.9998946">
If two or more trees have a common subtree, the subtree
should be represented only once. For example, the parse
forest for the sentence I saw a man in the park with a
telescope should be represented as in Figure 3.1.
To implement this, we no longer push grammatical
symbols on the stack; instead, we push pointers to a node
of the shared forest.4 When the parser &amp;quot;shifts&amp;quot; a word, it
creates a leaf node labeled with the word and the pre-ter-
minal, and, instead of the pre-terminal symbol, a pointer
to the newly created leaf node is pushed onto the stack.
lithe exact same leaf node (i.e., the node labeled with
the same word and the same pre-terminal) already exists,
a pointer to this existing node is pushed onto the stack,
without creating another node. When the parser
&amp;quot;reduces&amp;quot; the stack, it pops pointers from the stack,
</bodyText>
<page confidence="0.89646">
34 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<figure confidence="0.9845495">
An Efficient Augmented-Context-Free Parsing Algorithm
Masaru Tomita
</figure>
<figureCaption confidence="0.82769">
Figure 3.1. Unpacked shared forest.
</figureCaption>
<figure confidence="0.999777">
PP
NP
*n 4113
a mon in
NP
/\
ad an
the apt
PP
1/NP
ap ad an
with a tel
NP
an
*v
saw
</figure>
<figureCaption confidence="0.999577">
Figure 3.2. Packed shared forest.
</figureCaption>
<bodyText confidence="0.99944225">
creates a new node whose successive nodes are pointed
to by those popped pointers, and pushes a pointer to the
newly created node onto the stack.
Using this relatively simple procedure, our parsing
algorithm can produce the shared forest as its output
without any other special book-keeping mechanism,
because it never does the same reduce action twice in the
same manner.
</bodyText>
<subsectionHeader confidence="0.999525">
3.2 LOCAL AMBIGUITY PACKING
</subsectionHeader>
<bodyText confidence="0.99999090625">
We say that two or more subtrees represent local ambigu-
ity if they have common leaf nodes and their top nodes
are labeled with the same non-terminal symbol. That is to
say, a fragment of a sentence is locally ambiguous if the
fragment can be reduced to a certain non-terminal
symbol in two or more ways. If a sentence has many local
ambiguities, the total ambiguity would grow exponential-
ly. To avoid this, we use a technique called local ambigui-
ty packing, which works in the following way. The top
nodes of subtrees that represent local ambiguity are
merged and treated by higher-level structures as if there
were only one node. Such a node is called a packed node,
and nodes before packing are called subnodes of the
packed node. An example of a shared-packed forest is
shown in Figure 3.2. Packed nodes are represented by
boxes. We have three packed nodes in Figure 3.2; one
with three subnodes and two with two subnodes.
Local ambiguity packing can be easily implemented
with our parsing algorithm as follows. In the graph-struc-
tured stack, if two or more symbol vertices have a
common state vertex immediately on their left and a
common state vertex immediately on their right, they
represent local ambiguity. Nodes pointed to by these
symbol vertices are to be packed as one node. In Figure
2.5, for example, we see one 5-way local ambiguity and
two 2-way local ambiguities. The algorithm is made clear
by the example in the following section.
Recently, the author (Tomita 1986) suggested a tech-
nique to disambiguate a sentence out of the shared-
packed forest representation by asking the user a minimal
number of questions in natural language (without show-
ing any tree structures).
</bodyText>
<sectionHeader confidence="0.994676" genericHeader="method">
4 EXAMPLES
</sectionHeader>
<bodyText confidence="0.999755125">
This section presents three examples. The first example,
using the sentence I saw a man in the apartment with a
telescope, is intended to help the reader understand the
algorithm mOre clearly.
The second example, with the sentence That informa-
tion is important is doubtful, is presented to demonstrate
that our algorithm is able to handle multi-part-of-speech
words without any special mechanism. In the sentence,
that is a multi-part-of-speech word, because it could also
be a determiner or a pronoun.
The third example is provided to show that the algo-
rithm is also able to handle unknown words by consider-
ing an unknown word as a special multi-part-of-speech
word whose part of speech can be anything. We use an
example sentence / * a *, where *s represent unknown
words.
</bodyText>
<subsectionHeader confidence="0.997481">
4.1 THE EXAMPLE
</subsectionHeader>
<bodyText confidence="0.999202">
This subsection gives a trace of the algorithm with the
grammar in Figure 2.1, the parsing table in Figure 2.2,
and the sentence I saw a man in the park with a telescope.
At the very beginning, the stack contains only one
vertex labeled 0, and the parse forest contains nothing.
By looking at the action table, the next action, &amp;quot;shift 4&amp;quot;,
is determined as in standard LR parsing.
</bodyText>
<subsectionHeader confidence="0.282323">
Next Word = T•
</subsectionHeader>
<bodyText confidence="0.914436111111111">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 35
Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm
When shifting the word /, the algorithm creates a leaf
node in the parse forest labeled with the word / and its
preterminal *n, and pushes a pointer to the leaf node
onto the stack. The next action, &amp;quot;reduce 3, is determined
from the action table.
Next Word = &apos;saw&apos; 0 0 4
9-11F-40 (p.31 0 [6n &apos; I&apos; ]
We reduce the stack basically in the same manner as
standard LR parsing. It pops the top vertex &amp;quot;4&amp;quot; and the
pointer &amp;quot;0&amp;quot; from the stack, and creates a new node in the
parse forest whose successor is the node pointed to by
the pointer. The newly created node is labeled with the
left-hand side symbol of rule 3, namely &amp;quot;NP&amp;quot;. The point-
er to this newly created node, namely &amp;quot;1&amp;quot;, is pushed onto
the stack. The action, &amp;quot;shift 7&amp;quot;, is determined as the next
action. Now, we have
</bodyText>
<figure confidence="0.999417368421052">
0 &apos;1&apos;3
I [NP (0)]
0 1 2
0-0-41# Lob 71
Next Word = &apos;saw&apos;
Next Word = &apos;a&apos;
0 [on &apos;I&apos;]
I [NP (0)]
2 [6v &apos;saw&apos;]
0 1 2 2 7
fa—IIF&amp;quot;.&apos;dlF. lob 31
After executing &amp;quot;shift 7&amp;quot;, we have
0 [&apos;n &apos;I&apos;]
1 [NP (0)]
2 [6v &apos;saw&apos;]
3 Not &apos;a&apos;l
0 1 2 2 7 3 3
101
After executing &amp;quot;shift 3&amp;quot;, we have
Next Word = &apos;man&apos;
Next Word = &apos;in&apos;
0 [&apos;n &apos;I&apos;]
I [NP (0)]
2 [6v &apos;saw&apos;]
3 [*dot &apos;a&apos;]
4 [6n &apos;man&apos;]
After executing &amp;quot;shift 10&amp;quot;, we have
0 1 2 2 7 3 3 4 10
410-1111-1111-111-10-11-410-6-411/ fry 411
0 &apos;I&apos;]
I [NP (0)]
2 [6v &apos;saw&apos;]
3 [&apos;dot &apos;a&apos;]
4 {6n &apos;man&apos;]
5 [NP (3 4)]
Next Word = &apos;in&apos;
o ,n1 2 2 7 5 12 (r.7)
Wili-0-0-0-111—W Lab
</figure>
<figureCaption confidence="0.719317666666667">
The next action is &amp;quot;reduce 4&amp;quot;. It pops pointers, &amp;quot;3&amp;quot;
and &amp;quot;4&amp;quot;, and creates a new node in the parse forest such
that node 3 and node 4 are its successors. The newly
created node is labeled with the left-hand side symbol of
rule 4, i.e., &amp;quot;NP&amp;quot;. The pointer to this newly created node,
&amp;quot;5&amp;quot;, is pushed onto the stack. We now have
</figureCaption>
<bodyText confidence="0.999967125">
At this point, we encounter a multiple entry, &amp;quot;reduce
7&amp;quot; and &amp;quot;shift 6&amp;quot;, and both actions are to be executed.
Reduce actions are always executed first, and shift
actions are executed only when there is no remaining
reduce action to execute. In this way, the parser works
strictly from left to right; it does everything that can be
done before shifting the next word. After executing
&amp;quot;reduce 7&amp;quot;, the stack and the parse forest look like the
following. The top vertex labeled &amp;quot;12&amp;quot; is not popped
away, because it still has an action not yet executed. Such
a top vertex, or more generally, vertices with one or more
actions yet to be executed, are called &amp;quot;active&amp;quot;. Thus, we
have two active vertices in the stack above: one labeled
&amp;quot;12&amp;quot;, and the other labeled &amp;quot;8&amp;quot;. The action &amp;quot;reduce 1&amp;quot;
is determined from the action table, and is associated
with the latter vertex.
</bodyText>
<page confidence="0.797588">
36 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<figure confidence="0.9711178">
Masaru Tomita An Efficient Augmented-Context--Free Parsing Algorithm
Next Word = &apos;in&apos;
0 E•is &apos;V]
0 1 2 2 7 5 12 1 [NP (0)3
4861 2 [l&apos;it &apos;saw&apos;)
8 3 (*dot &apos;al
ip frill 4 [&apos;n &apos;Illan&apos;j
5 [NP (3 4)]
6 [VP (2 5)]
Because reduce actions have a higher priority than the vertex labeled &amp;quot;8&amp;quot;. The action &amp;quot;shift 6&amp;quot; is deter-
</figure>
<figureCaption confidence="0.268663">
shift actions, the algorithm next executes &amp;quot;reduce 1&amp;quot; on mined from the action table.
</figureCaption>
<table confidence="0.90843495">
Next Word = In&apos; 0 NI &apos;I&apos;)
1 [NP (0)]
2 (6v &apos;saw&apos;)
3 (*dot &apos;al
4 (n &apos;man&apos;]
5 [NP (3 4)]
1 [VP (2 5)]
7 [5 (1 6)]
Now we have two &amp;quot;shift 6&amp;quot;s. The parser, however, stack wherever possible. The stack and the parse forest
creates only one new leaf node in the parse forest. After look like the following, and &amp;quot;shift 3&amp;quot; is determined from
executing two shift actions, it combines vertices in the the action table as the next action.
Next Word = &apos;the&apos; Ish 3) 0 &apos;I&apos;]
1 [NP (0)]
2 [6v &apos;saw&apos;]
3.(•clot &apos;a&apos;)
4 (sn &apos;man&apos;)
5 [NP (3 4)]
6 [VP (2 5)]
7 [5 (1 6)]
3 [&apos;prop &apos;in&apos;]
</table>
<bodyText confidence="0.915465">
After about 20 steps (see below), the action &amp;quot;accept&amp;quot;
is finally executed. It returns &amp;quot;25&amp;quot; as the top node of the
parse forest, and halts the process.
</bodyText>
<figure confidence="0.98890944">
I 2 2 7 1 12
•
10e 101
0 2 2 7 31 • to 10
• ei It It
We&amp;quot;
&apos;2 kb 51
• (i&apos; ll
• Ine 21
II et It el
LaS GI
&apos;2 190 61
.5511
0. 21
kb GI
2 2 7
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 37
An Efficient Augmented-Context-Free Parsing Algorithm
1
Masaru Tomita
22751 11 IS 21 11
I,. 71
ir• 21
6 n
*-48-4D tj
</figure>
<table confidence="0.95507435">
to (&apos;n &apos;par) 20 [PP (18 19)] 22) (7 23)]
11 CUP (9 10)1 21 [NP (11 20)]
12 [PP (6 11)] 22 [NP (13 20)]
13 [NP (6 12)] 23 (PP (8 21)]
14 [VP (2 13)] 24 (VP (2 22)]
15 CS (1 14) (7 12)] 25 CS (1 24) (15
18 [&apos;prep &apos;with.]
17 [&apos;dot &apos;al
18 (611 sc008.1
19 CNP (17 18)]
The final parse forest is 0 C44+ &apos;I.]
1 CNP (0)1
2 C*6 &apos;sale]
3 C•dot &apos;al
4 Celt &apos;maw]
5 (NP (3 4)]
8 (VP (2 8)]
7 CS (1 8)1
8 (*prep &apos;In&apos;)
9 [&apos;dot &apos;mi)
</table>
<subsectionHeader confidence="0.789381">
4.2 MANAGING MULTI-PART-OF-SPEECH WORDS
</subsectionHeader>
<bodyText confidence="0.999259">
This subsection gives a trace of the algorithm with the
sentence That information is important is doubtful, to
demonstrate that our algorithm can handle multi-part-of-
speech words (in this sentence, that) just like multiple
entries without any special mechanism. We use the gram-
mar at the right and the parsing table below.
</bodyText>
<table confidence="0.856257923076923">
S --&gt; NP VP
NP --&gt; *det *11
NP --&gt; *n
NP --&gt; *that S
VP --&gt; be *adj
State •adj be •det *n •that $
NP S VP
sh5 sh4 sh3 2 1
1 ace
2 sh6 7
3 sh5 sh4 sh3 2 8
4 re3
sh9
</table>
<footnote confidence="0.48514">
6 sh10
7 rot rel
re4
9 re2
10 re5 re5
</footnote>
<page confidence="0.724134">
38 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.660888">
Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm
</note>
<bodyText confidence="0.999396">
At the very beginning, the parse forest contains noth-
ing, and the stack contains only one vertex, labeled 0.
The first word of the sentence is that, which can be cate-
gorized as *that, *det or *n. The action table tells us that
</bodyText>
<subsectionHeader confidence="0.775678">
Next Word = &apos;that&apos;
</subsectionHeader>
<bodyText confidence="0.9973795">
all of these categories are legal. Thus, the algorithm
behaves as if a multiple entry is encountered. Three
actions, &amp;quot;shift 3&amp;quot;, &amp;quot;shift 4&amp;quot;, and &amp;quot;shift 5&amp;quot;, are to be
executed.
</bodyText>
<table confidence="0.96279375">
0 NO 31
4, Ish 41
Ish 51
After executing those three shift actions, we have 03
Next Word = &apos;information&apos;
0/I 4 Ish 41 0 (*that &apos;that&apos;]
\2 5 I 1 [on &apos;that&apos;]
Ish 91 2 (*dolt &apos;that&apos;)
</table>
<bodyText confidence="0.9988446">
Note that three different leaf nodes have been created
in the parse forest. One of the three possibilities, that as
a noun, is discarded immediately after the parser sees the
next word information. After executing the two shift
actions, we have
</bodyText>
<figure confidence="0.994391578947368">
0 3 3 4 It. 31 0 [&apos;that &apos;that&apos;)
9 Ire 21 1 [6:1 &apos;that&apos;]
2 [&apos;dot &apos;that&apos;]
3 [on &apos;information&apos;)
Next Word = &apos;is&apos;
Ish 101
0 [&apos;that &apos;that&apos;)
1 [on &apos;that&apos;]
2 [*dot &apos;that&apos;]
3 [&apos;n &apos;information&apos;)
4 [NP (3)]
6 [NP (2 3)]
6 [&apos;be &apos;is&apos;]
After executing &amp;quot;shift 6&amp;quot;, we have
Next Word = &apos;important&apos;
Ire 51
Next Word = &apos;is&apos;
0 [&apos;that &apos;that&apos;]
1 [6n &apos;that&apos;]
</figure>
<footnote confidence="0.598636166666667">
2 (6det &apos;that&apos;)
3 [&apos;n &apos;information&apos;]
4 [NP (3)]
[NP (2 3)]
6 (*be &apos;is&apos;]
7 [6adj &apos;important&apos;]
</footnote>
<bodyText confidence="0.911689">
After executing &amp;quot;shift 10&amp;quot;, we have
This time, only one leaf node has been created in the
parse forest, because both shift actions regarded the
word as belonging to the same category, i.e., noun. Now
we have two active vertices, and &amp;quot;reduce 3&amp;quot; is arbitrarily
chosen as the next action to execute. After executing
</bodyText>
<table confidence="0.7034315">
&amp;quot;reduce 3&amp;quot;, we have
0 3 4 2 0 [&apos;that &apos;that&apos;]
Ish 61 1 [611 &apos;that&apos;]
2 [&apos;dot &apos;that&apos;]
2 5 3 9 3 [&apos;n &apos;information&apos;]
Ire 21 4 [NP (3)]
0 0 3 4 2 Ish 61 0 [6that &apos;that&apos;]
1 [6n &apos;that&apos;]
2 [&apos;dot &apos;that&apos;]
3 [&apos;n &apos;information&apos;]
4 [NP (3)]
5 [NP (2 3)]
Next Word = &apos;is&apos;
After executing &amp;quot;reduce 2&amp;quot;, we have
Next Word = &apos;is&apos;
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 39
Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm
After executing &amp;quot;reduce 5&amp;quot;, we have Ire I] 0 [&apos;that &apos;that&apos;]
Next Word = &apos;is&apos; 1 (*n &apos;that&apos;)
2 (&amp;quot;dia. &apos;that&apos;)
3 [&apos;n &apos;information&apos;]
4 [NP (3)]
5 [NP (2 3)]
8 (&apos;be &apos;is&apos;)
7 (&amp;quot;adj &apos;importantl
8 [VP (6 7)]
Now, there are two ways to execute the action
&amp;quot;reduce 1&amp;quot;. After executing &amp;quot;reduce 1&amp;quot; in both ways, 0
we have 1
2
</table>
<figure confidence="0.985706981132075">
0 p 3 a 8 3
Ire 41 4
lo 1 5
8
1 7
8
9
Next Word = &apos;is&apos;
[&apos;that &apos;that&apos;]
[&apos;n &apos;that&apos;)
(&amp;quot;det &apos;that&apos;]
[&apos;n &apos;information&apos;]
[PIP (3)]
[NP (2 3)]
[&apos;be &apos;is&apos;]
[•adj &apos;important&apos;)
[VP (8 7)]
[S (4 8)]
An error action is finally found for the possibility, that 0 11 2 0 [&apos;that &apos;that&apos;] 8 [VP (8 7)]
as a determiner. After executing &amp;quot;reduce 4&amp;quot;, we have e-iFe 1st, LI 1 (&amp;quot;n &apos;that&apos;) 9 [S (4 8)]
Next Word = &apos;is&apos; 2 (&amp;quot;det &apos;that&apos;] 10 CS (5 8)]
3 [&apos;n &apos;information&apos;] 11 [NP (0 0)]
4 [NP (3)]
5 [NP (2 3)/
8 (*be &apos;is&apos;]
7 [sadj &apos;important&apos;]
After executing &amp;quot;shift 6&amp;quot;, and several steps later, we
have
o tt 2 12 6
111H1-1/-111-10 Ish 101
O 11 2 1, 6 13 10
Ire 51
0 [&apos;that &apos;that&apos;]
1 (&amp;quot;n &apos;that&apos;]
2 (&amp;quot;dot &apos;that&apos;]
3 [on &apos;information&apos;]
4 [NP (3)]
5 [NP (2 3)]
6 (&amp;quot;be &apos;is&apos;]
7 padj &apos;important&apos;]
O 2 14 7
111-11-1,—.11, Ire 1
O 15
0—E-4111 lacci
8 [VP (8 7)]
9 [S (4 8)]
10 [S (5 8)]
11 [NP (0 9)]
12 [&apos;be &apos;is&apos;]
13 padj &apos;doubtful&apos;)
14 [VP (12 13)]
15 CS (11 14)]
Next Word = &apos;$&apos;
</figure>
<bodyText confidence="0.8593675">
The parser accepts the sentence, and returns &amp;quot;15&amp;quot; as
the top node of the parse forest. The forest consists of
only one tree which is the desired structure for That
information is important is doubtful.
</bodyText>
<subsectionHeader confidence="0.98174">
4.3 MANAGING UNKNOWN WORDS
</subsectionHeader>
<bodyText confidence="0.994757833333333">
In the previous subsection, we saw the parsing algorithm
handling a multi-part-of-speech word just like multiple
entries without any special mechanism. That capability
can also be applied to handle unknown words (words
whose categories are unknown). An unknown word can
be thought of as a special type of a multi-part-of-speech
word whose categories can be anything. In the following,
we present another trace of the parser with the sentence
/ * a *, where *s represent an unknown word. We use
the same grammar and parsing table as in the first exam-
ple (Figures 2.1 and 2.2).
At the very beginning, we have
</bodyText>
<figure confidence="0.7825215">
NIEWLVVord = 1&apos;
0
4, Ish 4
40 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
0 (*n &apos;11
An Efficient Augmented-Context-Free Parsing Algorithm
0 0 4
411P-111-11 Ire 31
MasantTomita
After executing &amp;quot;shift 4&amp;quot;, we have
Next Word =
At this point, the parser is looking at the unknown have only one kind of action, &amp;quot;reduce 3&amp;quot;. Thus the algo-
</figure>
<table confidence="0.914386714285714">
word, &amp;quot;*&amp;quot;; in other words, a word whose categories are rithm executes only the action &amp;quot;reduce 3&amp;quot;, after which
*det, *n, *v and *prep. On row 4 of the action table, we we have
Next Word = 0 1 2 /sh 0 [&apos;n &apos;11
40AI-40 fsh 71 1 [NP (0)]
On row 2 of the action table, there are two kinds of word has two possibilities, as a preposition or a verb.
actions, &amp;quot;shift 6&amp;quot; and &amp;quot;shift 7&amp;quot;. This means the unknown After executing both actions, we have
Next Word = &apos;a&apos; 2 6 (sit 31 0 [&apos;n &apos;I&apos;]
ti_44(111-10 Ish 31 1 [NP (0)]
,3 7 2 [&apos;prep &apos;0&apos;]
3 (&amp;quot;1/ •e&apos;]
After executing &amp;quot;shift 3&amp;quot; twice, we have 2 6 0 [&apos;n &apos;I&apos;]
Next Word = 0 1 2 /111-111\ 4 3 1 (MP (0)]
3 7 /11-11. (IA 10/ 2 [&apos;prep
1.-111( 3 [61,
</table>
<figure confidence="0.944307534883721">
4 [&apos;det &apos;a1
At this point, the parser is again looking at the mine the category of the unknown word, which is a noun.
unknown word, &amp;quot;*&amp;quot;. However, since there is only one After shifting the unknown word as a noun, we have
entry on row 3 in the action table, we can uniquely deter-
Next Word =
After executing &amp;quot;reduce 4&amp;quot;, we have 1 2 2 6 6 11 Ire 61 0 [&apos;n &apos;I&apos;]
3 7 6 12 fre 71 1 [11P (0)]
2 [&apos;prep **1
3 1.,, .4.1
4 &amp;quot;dot &apos;a&apos;]
5 &apos;n &apos;01
6 NP (4 5)]
Next Word = &apos;$&apos;
7,
Ire 51
1 2
8
Ire 11
[&apos;n &apos;&apos;&apos;)
[MP (4 5)]
[PP (2 6)]
[VP (3 6)]
0 [&apos;n &apos;I&apos;]
1 [NP (0)]
2 [&apos;prep
3 (&amp;quot;1/
4 [&apos;dot &apos;a&apos;]
6 [&apos;n &apos;&apos;&apos;]
After executing both &amp;quot;reduce 6&amp;quot; and &amp;quot;reduce 7&amp;quot;, we
have
Next Word =
0 [&apos;n &apos;11
1 [NP (0)]
2 [&apos;prep &apos;&apos;&apos;]
3 Cov &apos;0&apos;)
4 [&apos;dot &apos;a&apos;)
5
6
7
8
2 6
I 2 4 3 5 10
re 41
</figure>
<table confidence="0.968218375">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 41
Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm
After executing both &amp;quot;reduce 5&amp;quot; and &amp;quot;reduce 1&amp;quot;, we 9 2 1
have 0 •ri &apos;I&apos;]
Next Word = 1 NP (0)]
2 *prep &apos;•&apos;]
3 &amp;quot;&apos;v &apos;1&amp;quot;]
4 •det &apos;al
5 *ri &amp;quot;&amp;quot;)
6 NP (4 51
7 [PP (2 6)
8 [VP (3 6)
9 [NP (1 7)]
10 [S (1 8)]
(error)
lace)
</table>
<bodyText confidence="0.999479166666667">
The possibility of the first unknown word being a
preposition has now disappeared. The parser accepts the
sentence in only one way, and returns &amp;quot;10&amp;quot; as the root
node of the parse forest.
We have shown that our parsing algorithm can handle
unknown words without any special mechanism.
</bodyText>
<sectionHeader confidence="0.985536" genericHeader="method">
5 EMPIRICAL RESULTS
</sectionHeader>
<bodyText confidence="0.999941266666667">
In this section, we present some empirical results of the
algorithm&apos;s practical performance. Since space is limited,
we only show the highlights of the results, referring the
reader to chapter 6 of Tomita (1985) for more detail.
Figure 5.1 shows the relationship between parsing time
of the Tomita algorithm and the length of input sentence,
and Figure 5.2 shows the comparison with Earley&apos;s algo-
rithm (or active chart parsing), using a sample English
grammar that consists of 220 context-free rules and 40
sample sentences taken from actual publications. All
programs are run on DEC-20 and written in MacLisp, but
not compiled. Although the experiment is informal, the
result show that the Tomita algorithm is about 5 to 10
times faster than Earley&apos;s algorithm, due to the pre-com-
pilation of the grammar into the LR table. The
</bodyText>
<figure confidence="0.8670525">
O 5 10 15 20 25 30 35
Sentence Length (words)
</figure>
<figureCaption confidence="0.976878">
Figure 5.1. Parsing time and sentence length.
</figureCaption>
<figure confidence="0.952212916666667">
.2 30
ri!
25 0
s 20
15 0
000
10 0 09000 .00 g 0 0 0
e 0 0
0 0 0 0 0
5
5 10 15 20 25 30 35
Sentence Length (words)
</figure>
<figureCaption confidence="0.987943">
Figure 5.2. Earley/Tomita ratio.
</figureCaption>
<page confidence="0.917292833333333">
14
12
8
6
4
2
</page>
<figure confidence="0.982082833333333">
-4 800
700
• 600
1500
E. 400
300
t&apos; ▪ 200
.6 100
4.4
0
O 50 100 150 200 250 300 350 400 10 100 WOO 10000 100000 1000000
Grammar Size (The number of Rules) Sentence Ambiguity (the number of parses)
</figure>
<figureCaption confidence="0.981029">
Figure 5.3. Earley/Tomita ratio and grammar size. Figure 5.4. Size of parse forest and ambiguity.
</figureCaption>
<page confidence="0.953662">
42 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.882689">
Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm
</note>
<bodyText confidence="0.99959805">
Earley/Tomita ratio seems to increase as the size of
grammar grows as shown in Figure 5.3. Figure 5.4 shows
the relationship between the size of a produced shared-
packed forest representation (in terms of the number of
nodes) and the ambiguity of its input sentence (the
number of possible parses). The sample sentences are
created from the following schema.
noun verb det noun (prep det noun)n-1
An example sentence with this structure is
I saw a man in the park on the hill with a telescope.
The result shows that all possible parses can be repres-
ented in almost 0(log n) space, where n is the number of
possible parses in a sentence.5
Figure 5.5 shows the relationship between the parsing
time and the ambiguity of a sentence. Recall that within
the given time the algorithm produces all possible parses
in the shared-packed forest representation. It is
concluded that our algorithm can parse (and produce a
forest for) a very ambiguous sentence with a million
possible parses in a reasonable time.
</bodyText>
<figure confidence="0.9232075">
10 100 1000 10000 100000 1000000
Sentence Ambiguity (the number of parses)
</figure>
<figureCaption confidence="0.974132">
Figure 5.5. Parsing time and ambiguity.
</figureCaption>
<sectionHeader confidence="0.960532" genericHeader="method">
6 AUGMENTED CONTEXT-FREE GRAMMARS
</sectionHeader>
<bodyText confidence="0.999966363636364">
So far, we have described the algorithm as a pure
context-free parsing algorithm. In practice, it is often
desired for each grammar nonterminal to have attributes,
and for each grammar rule to have an augmentation to
define, pass, and test the attribute values. It is also
desired to produce a functional structure (in the sense of
functional grammar formalism (Kay 1984, Bresnan and
Kaplan 1982) rather than the context-free forest.
Subsection 6.1 describes the augmentation, and
subsection 6.2 discusses the shared-packed represen-
tation for functional structures.
</bodyText>
<subsectionHeader confidence="0.998199">
6.1 THE AUGMENTATION
</subsectionHeader>
<bodyText confidence="0.999980647058824">
We attach a Lisp function to each grammar rule for this
augmentation. Whenever the parser reduces constituents
into a higher-level nonterminal using a phrase structure
rule, the Lisp program associated with the rule is evalu-
ated. The Lisp program handles such aspects as
construction of a syntax/semantic representation of the
input sentence, passing attribute values among constitu-
ents at different levels and checking syntactic/semantic
constraints such as subject-verb agreement.
If the Lisp function returns NIL, the parser does not
do the reduce action with the rule. If the Lisp function
returns a non-NIL value, then this value is given to the
newly created non-terminal. The value includes attributes
of the nonterminal and a partial syntactic/semantic
representation constructed thus far. Notice that those
Lisp functions can be precompiled into machine code by
the standard Lisp compiler.
</bodyText>
<subsectionHeader confidence="0.999852">
6.2 SHARING AND PACKING FUNCTIONAL STRUCTURES
</subsectionHeader>
<bodyText confidence="0.997456853658537">
A functional structure used in the functional grammar
formalisms (Kay 1984, Bresnan and Kaplan 1982, Shie-
ber 1985) is in general a directed acyclic graph (dag)
rather than a tree. This is because some value may be
shared by two different attributes in the same sentence
(e.g., the &amp;quot;agreement&amp;quot; attributes of subject and main
verb). Pereira (1985) introduced a method to share dag
structures. However, the dag structure sharing method is
much more complex and computationally expensive than
tree structure sharing. Therefore, we handle only tree-
structured functional structures for the sake of efficiency
and simplicity.6 In the example, the &amp;quot;agreement&amp;quot; attri-
butes of subject and main verb may thus have two differ-
ent values. The identity of these two values is tested
explicitly by a test in the augmentation. Sharing tree-
structured functional structures requires only a minor
modification on the subtree sharing method for the
shared-packed forest representation described in
subsection 3.1.
Local ambiguity packing for augmented context-free
grammars is not as easy. Suppose two certain nodes have
been packed into one packed node. Although these two
nodes have the same category name (e.g., NP), they may
have different attribute values. When a certain test in the
Lisp function refers to an attribute of the packed node,
its value may not be uniquely determined. In this case,
the parser can no longer treat the packed node as one
node, and the parser will unpack the packed node into
two individual nodes again. The question, then, is how
often this unpacking needs to take place in practice. The
more frequently it takes place, the less significant it is to
do local ambiguity packing. However, most of sentence
ambiguity comes from such phenomena as PP-attachment
and conjunction scoping, and it is unlikely to require
unpacking in these cases. For instance, consider the noun
phrase:
a man in the park with a telescope,
which is locally ambiguous (whether telescope modifies
man or park). Two NP nodes (one for each interpreta-
tion) will be packed into one node, but it is unlikely that
the two NP nodes have different attribute values which
</bodyText>
<figure confidence="0.984278833333333">
600
500
*O&amp;quot;
400
300
&apos;al 200
Co
Co
100
Co
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 43
Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm
</figure>
<bodyText confidence="0.999730428571429">
are referred to later by some tests in the augmentation.
The same argument holds with the noun phrases:
pregnant women and children
large file equipment
Although more comprehensive experiments are desired, it
is expected that only a few packed nodes need to be
unpacked in practical applications.
</bodyText>
<subsectionHeader confidence="0.988827">
6.3 THE LFG COMPILER
</subsectionHeader>
<bodyText confidence="0.999980590909091">
It is in general very painful to create, extend, and modify
augmentations written in Lisp. The Lisp functions should
be generated automatically from more abstract specifica-
tions. We have implemented the LFG compiler that
compiles augmentations in a higher level notation into
Lisp functions. The notation is similar to the Lexical
Functional Grammar (LFG) formalism (Bresnan and
Kaplan 1982) and PATR-1I (Shieber 1984). An example
of the LFG-like notation and its compiled Lisp function
are shown in Figures 6.1 and 6.2. We generate only
non-destructive functions with no side-effects to make
sure that a process never alters other processes or the
parser&apos;s control flow. A generated function takes a list of
arguments, each of which is a value associated with each
right-hand side symbol, and returns a value to be associ-
ated with the left-hand side symbol. Each value is a list
of f-structures, in case of disjunction and local ambiguity.
That a semantic grammar in the LFG-like notation can
also be generated automatically from a domain semantics
specification and a purely syntactic grammar is discussed
further in Tomita and Carbonell (1986). The discussion
is, however, beyond the scope of this paper.
</bodyText>
<figure confidence="0.9950596">
(&lt;S&gt; &lt;==&gt; ( &lt;NP&gt; &lt;VP&gt;)
(((xl case) = nom)
((x2 form) =c finite)
(4TW&amp;quot;
(((x2 :time) present)
((xl agr) = (x2 agr)))
(((x2 :time) = past)))
((x0) = (x2))
((x0 :mood) = dec)
((x0 subj) = (x1))))
</figure>
<figureCaption confidence="0.9830195">
Figure 6.1. Example grammar rule in the
LFG-like notation.
</figureCaption>
<figure confidence="0.647916">
(&lt;S&gt; &lt;==&gt; (&lt;NP&gt; &lt;VP&gt;)
(LAMBDA (X1 X2)
</figure>
<sectionHeader confidence="0.994481125" genericHeader="method">
(LET ((X (LIST (LIST (CONS (QUOTE X2) X2) (CONS (QUOTE X1) X1)))))
(AND
(SETQ X (UNIFYSETVALUE* (QUOTE (X1 CASE)) (QUOTE (NOM))))
(SETQ X (C-UNIFYSETVALUE* (QUOTE (X2 FORM)) (QUOTE (FINITE))))
(SETQ X (APPEND
(LET ((X X))
(SETQ X (UNIFYSETVALUE* (QUOTE (X2 :TIME)) (QUOTE (PRESENT)))))
(SETQ X (UNIFYVALUE* (QUOTE (X2 AGR)) (QUOTE (X1 AGR))))
X)
(LET ((X X))
(SETQ X (UNIFYSETVALUE* (QUOTE (X2 :TIME)) (QUOTE (PAST))))
X)))
(SETQ X (UNIFYVALUE* (QUOTE (X0)) (QUOTE (X2))))
(SETQ X (UNIFYSETVALUE* (QUOTE (X0 :MOOD)) (QUOTE (DEC))))
(SETQ X (UNIFYVALUE* (QUOTE (X0 SUBJ)) (QUOTE (X1))))
(GETVALUE* X (QUOTE (X0)))))))
</sectionHeader>
<figureCaption confidence="0.88664">
Figure 6.2. The compiled grammar rule.
</figureCaption>
<page confidence="0.948973">
44 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.891183">
Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm
</note>
<sectionHeader confidence="0.970612" genericHeader="method">
7 THE ON-LINE PARSER
</sectionHeader>
<bodyText confidence="0.967031833333333">
Our parsing algorithm parses a sentence strictly from left
to right. This characteristics makes on-line parsing possi-
ble; i.e., to parse a sentence as the user types it in, with-
out waiting for completion of the sentence. An example
session of on-line parsing is presented in Figure 7.1 for
the sample sentence I saw a man with a telescope.
</bodyText>
<figure confidence="0.987487976744186">
&gt;- Starts accepting a sentence.
&gt;I_
&gt;1 Starts parsing &amp;quot;I&amp;quot;.
&gt;1 s_
&gt;1 so_
&gt;1 saw_
&gt;1 saw _ Starts parsing &amp;quot;saw&amp;quot;.
&gt;1 saw a_
&gt;1 saw a _ Starts parsing &amp;quot;a&amp;quot;.
&gt;1 saw a b_
&gt;1 saw a bi_
&gt;1 saw a big_
&gt;1 saw a big _ Starts parsing &amp;quot;big&amp;quot;.
&gt;1 saw a big m_
&gt;1 saw a big ma_ User changes his mind.
&gt;1 saw a big m_
&gt;1 saw a big _
&gt;1 saw a big_ Starts unparsing &amp;quot;big&amp;quot;.
&gt;1 saw a bi_
&gt;1 saw a b_
&gt;1 saw a _
&gt;1 saw a m_
&gt;1 saw a ma_
&gt;1 saw a man_
&gt;I saw a man Starts parsing &amp;quot;man&amp;quot;.
&gt;1 saw a man w_
&gt;1 saw a man wi_
&gt;1 saw a man wit_
&gt;1 saw a man with
&gt;1 saw a man with Starts parsing &amp;quot;with&amp;quot;.
&gt;1 saw a man with a_
&gt;1 saw a man with a _ Starts parsing &amp;quot;a&amp;quot;.
&gt;1 saw a man with a t_
&gt;1 saw a man with a te_
&gt;1 saw a man with a tel_
&gt;1 saw a man with a • tel e_
&gt;1 saw a man with a teles_
&gt;1 saw a man with a telesc_
&gt;1 saw a man with a telesco_
&gt;1 saw a man with a telescop_
&gt;1 saw a man with a telescope_
&gt;1 saw a man with a telescope._ Starts parsing &amp;quot;telescope&amp;quot;.
&gt;1 saw a man with a telescope. User hits (return). Ends parsing
</figure>
<figureCaption confidence="0.999889">
Figure 7.1. Example of on-line parsing.
</figureCaption>
<bodyText confidence="0.999986222222222">
As in this example, the user often wants to hit the
&amp;quot;backspace&amp;quot; key to correct previously input words. In
the case in which these words have already been proc-
essed by the parser, the parser must be able to &amp;quot;un-
parse&amp;quot; the words, without parsing the sentence from the
beginning all over again. To implement unparsing, the
parser needs to store system status each time a word is
parsed. Fortunately, this can be nicely done with our
parsing algorithm; only pointers to the graph-structured
stack and the parse forest need to be stored.
It should be noted that our parsing algorithm is not the
only algorithm that parses a sentence strictly from left to
right; Other left-to-right algorithms include Earley&apos;s
(1970) algorithm, the active chart parsing algorithm
(Winograd 1983), and a breadth-first version of ATN
(Woods 1970). Despite the availability of left-to-right
algorithms, surprisingly few on-line parsers exist.
NLMenu (Tennant et al. 1983) adopted on-line parsing
for a menu-based system but not for typed inputs.
In the rest of this section, we discuss two benefits of
on-line parsing, quicker response time and early error
detection. One obvious benefit of on-line parsing is that
it reduces the parser&apos;s response time significantly. When
the user finishes typing a whole sentence, most of the
input sentence has been already processed by the parser.
Although this does not affect CPU time, it could reduce
response time from the user&apos;s point of view significantly.
On-line parsing is therefore useful in interactive systems
in which input sentences are typed in by the user on-line;
it is not particularly useful in batch systems in which
input sentences are provided in a file.
Another benefit of on-line parsing is that it can detect
an error almost as soon as the error occurs, and it can
warn the user immediately. In this way, on-line parsing
could provide better man-machine communication.
Further studies on human factors are necessary.
</bodyText>
<sectionHeader confidence="0.997912" genericHeader="conclusions">
8 CONCLUSION
</sectionHeader>
<bodyText confidence="0.959801131578947">
This paper has introduced an efficient context-free pars-
ing algorithm, and its application to on-line natural
language interfaces has been discussed.
A pilot on-line parser was first implemented in
MacLisp at the Computer Science Department, Carne-
gie-Mellon University (CMU) as a part of the author&apos;s
thesis work (Tomita 1985). The empirical results in
section 5 are based on this parser.
CMU&apos;s machine translation project (Carbonell and
Tomita 1986) adopts on-line parsing for multiple
languages. It can parse unsegmented sentences (with no
spaces between words, typical in Japanese). To handle
unsegmented sentences, its grammar is written in a char-
acter-based manner; all terminal symbols in the grammar
are characters rather than words. Thus, morphological
rules, as well as syntactic rules, are written in the
augmented context-free grammar. The parser takes
about 1-3 seconds CPU time per sentence on a Symbol-
ics 3600 with about 800 grammar rules; its response time
(real time), however, is less than a second due to on-line
parsing. This speed does not seem to be affected very
much by the length of sentence or the size of grammar,
as discussed in section 5. We expect further improve-
ments for fully segmented sentences (such as English)
where words rather then characters are the atomic units.
A commercial on-line parser for Japanese language is
being developed in Common Lisp jointly by Intelligent
Technology Incorporation (ITI) and Carnegie Group
Incorporation (CGI), based on the technique developed
at CMU.
Finally, in the continuous speech recognition project
at CMU (Hayes et al. 1986), the on-line parsing algo-
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 45
Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm
rithm is being extended to handle speech input, to make
the speech parsing process efficient and capable of being
pipelined with lower level processes such as
acoustic/phonetic level recognition (Tomita 1986).
</bodyText>
<sectionHeader confidence="0.935301" genericHeader="acknowledgments">
AKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.999877153846154">
I would like to thank Jaime Carbonell, Phil Hayes, James
Allen, Herb Simon, Hozumi Tanaka, and Ralph Grish-
man for their helpful comments on the early version of
this paper. Kazuhiro Toyoshima and Hideto Kagamida
have implemented the runtime parser and the LR table
compiler, respectively, in Common Lisp. Lori Levin,
Teruko Watanabe, Peggy Anderson, and Donna Gates
have developed Japanese and English grammars in the
LFG-like notation. Hiroaki Saito has implemented the
algorithm for speech input. Ron Kaplan, Martin Kay,
Lauri Karttunen, and Stuart Shieber provided useful
comments on the implementation of LFG and dag struc-
ture sharing.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.997167024691358">
Aho, A.V. and Johnson, S.C. 1974 LR Parsing. Computing Survey
6(2): 99-124.
Aho, A.V. and Ullman, J.D. 1972 The Theory of Parsing, Translation,
and Compiling. Prentice-Hall, Englewood Cliffs, New Jersey.
Aho, A.V. and Ullman, J.D. 1977 Principles of Compiler Design. Addi-
son Wesley.
Bresnan, J. and Kaplan, R. 1982 Lexical-Functional Grammar: A
Formal System for Grammatical Representation. The Mental Repre-
sentation of Grammatical Relations. MIT Press, Cambridge, Massa-
chusetts: 173-281.
Carbonell, J.G. and Tomita, M. 1986 Knowledge-Based Machine
Translation, the CMU Approach. Machine Translation: Theoretical
and Methodological Issues. Cambridge University Press.
Church, K. and Patil, R. 1982 Coping with Syntactic Ambiguity, or
How to Put the Block in the Box on the Table. Technical Report
MIT/LC5/TM-216, Laboratory for Computer Science, Massachusetts
Institute of Technology, Cambridge, Massachusetts.
DeRemer, F.L. 1969 Practical Translators for LR(k) Languages.
Ph.D. thesis, Massachusetts Institute of Technology, Cambridge,
Massachusetts.
DeRemer, F.L. 1971 Simple LR(k) Grammars. Communications ACM
14(7): 453-460.
Earley, J. 1970 An Efficient Context-Free Parsing Algorithm.
Communications ACM 6(8): 94-102.
Hayes, P.J.; Hauptmann, A.G.; Carbonell, J.G.; and Tomita, M. 1986
Parsing Spoken Language: A Semantic Caseframe Approach. In
Proceedings of the 11th International Conference on Computation
Linguistics (COLING86).
Kay, M. 1984 Functional Unification Grammar: A Formalism for
Machine Translation. In Proceedings of the 10th International
Conference on Computational Linguistics: 75-78.
Pereira, F.C.N. 1985 A Structure-Sharing Representation for Unifica-
tion-Based Grammar Formalisms. In Proceedings of the 23rd Annual
Meeting of the Association for Computational Linguistics: 137-144.
Shieber, S.M. 1983 Sentence Disambiguation by a Shift-Reduce Pars-
ing Technique. In Proceedings of the 21st Annual Meeting of the
Association for Computational Linguistics: 113-118.
Shieber, S.M. 1984 The Design of a Computer Language for Linguis-
tic Information. In Proceedings of the 10th International Conference
on Computational Linguistics: 362-366.
Shieber, S.M. 1985 Using Restriction to Extend Parsing Algorithms
for Complex-Feature-Based Grammar Formalisms, In Proceedings
of the 23rd Annual Meeting of the Association for Computational
Linguistics: 145-152.
Tennant, H.R.; Ross, KM.; Saenz, R.M.; Thompson, C.W.; and Miller,
J.R. 1983 Menu-Based Natural Language Understanding. In
Proceedings of the 21st Annual Meeting of the Association for Compu-
tational Linguistics: 151-158.
Tomita, M. 1984 LR Parsers for Natural Language. In Proceedings of
the 10th International Conference on Computational Linguistics
(COLING84).
Tomita, M. 1985 Efficient Parsing for Natural Language: A Fast Algo-
rithm for Practical Systems. Kluwer Academic Publishers, Boston,
Massachusetts.
Tomita, M. 1986a Sentence Disambiguation by Asking. Computers and
Translation 1(1): 39-51.
Tomita, M. 1986b An Efficient Word Lattice Parsing Algorithm for
Continuous Speech Recognition. In Proceedings of IEEE-IECE-ASJ
International conference on Acoustics, Speech, and Signal Processing
(ICASSP86).
Winograd, T. 1983 Language as a Cognitive Process. Addison Wesley.
Woods, W.A. 1970 Transition Network Grammars for Natural
Language Analysis. Communications of ACM 13: 591-606.
NOTES
1. This research was sponsored by the Defense Advanced Research
Projects Agency (DOD), ARPA Order No. 3597, monitored by the
Air Force Avionics Laboratory under Contract F33615-
81-K-1539.
The views and conclusions contained in this document are those
of the author and should not be interpreted as representing the offi-
cial policies, either expressed or implied, of the Defense Advanced
Research Projects Agency or the US Government.
2. The situation is often called conflict.
3. Although it is possibly reduced if some processes reach error entries
and die.
4. The term node is used for forest representation, whereas the term
vertex is used for graph-structured stack representation.
5. In practice; not in theory.
6. Although we plan to handle dag structures in the future, tree struc-
tures may be adequate, as GPSGs use tree structures rather than dag
structures.
</reference>
<page confidence="0.988583">
46 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.467124">
<title confidence="0.997575">AN EFFICIENT AUGMENTED-CONTEXT-FREE PARSING ALGORITHM&apos;</title>
<author confidence="0.997499">Masaru Tomita</author>
<affiliation confidence="0.987923333333333">Computer Science Center for Machine Carnegie-Mellon</affiliation>
<address confidence="0.999072">Pittsburgh, PA 15213</address>
<abstract confidence="0.951044411764706">An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed. The algorithm is a generalized LR parsing algorithm, which precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented context-free grammar. Unlike the standard LR parsing algorithm, it can handle arbitrary context-free grammars, including ambiguous grammars, while most of the LR efficiency is preserved by introducing the concept of a &amp;quot;graph-structured stack&amp;quot;. The graph-structured stack allows an LR shift-reduce parser to maintain multiple parses without parsing any part of the input twice in the same way. We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables. The algorithm is fast, due to the LR table precomputation. In several experiments with different English grammars and sentences, timings indicate a fiveto tenfold speed advantage over Earley&apos;s context-free parsing algorithm. algorithm parses a sentence strictly from left to right that is, starts parsing as soon as the user types in the first word of a sentence, without waiting for completion of the sentence. A practical on-line parser based on the algorithm has been implemented in Common Lisp, and running on Symbolics and HP Al workstations. The parser is used in the multi-lingual machine translation project at CMU. Also, a commercial on-line parser for Japanese language is being built by Intelligent Technology Incorporation, on the technique developed at</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>S C Johnson</author>
</authors>
<date>1974</date>
<journal>LR Parsing. Computing Survey</journal>
<volume>6</volume>
<issue>2</issue>
<pages>99--124</pages>
<contexts>
<context position="3566" citStr="Aho and Johnson 1974" startWordPosition="522" endWordPosition="525">e augmented context-free grammars rather than pure context-free grammars. Section 7 describes the concept of on-line parsing, taking advantage of left-to-right operation of our parsing algorithm. The on-line parser parses a sentence strictly from left to right, and starts parsing as soon as the user types in the first word, without waiting for the end of line. Benefits of on-line parsing are then discussed. Finally, several versions of on-line parser have been implemented, and they are mentioned in section 8. 2 THE CONTEXT-FREE PARSING ALGORITHM The LR parsing algorithms (Aho and Ullman 1972, Aho and Johnson 1974) were developed originally for programming languages. An LR parsing algorithm is a Copyright 1987 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87 /010031-46$ 03.00 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 31 Masaru Tomita An Efficient Augmented-Context-Free </context>
</contexts>
<marker>Aho, Johnson, 1974</marker>
<rawString>Aho, A.V. and Johnson, S.C. 1974 LR Parsing. Computing Survey 6(2): 99-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling. Prentice-Hall,</booktitle>
<location>Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="3543" citStr="Aho and Ullman 1972" startWordPosition="518" endWordPosition="521">he algorithm to handle augmented context-free grammars rather than pure context-free grammars. Section 7 describes the concept of on-line parsing, taking advantage of left-to-right operation of our parsing algorithm. The on-line parser parses a sentence strictly from left to right, and starts parsing as soon as the user types in the first word, without waiting for the end of line. Benefits of on-line parsing are then discussed. Finally, several versions of on-line parser have been implemented, and they are mentioned in section 8. 2 THE CONTEXT-FREE PARSING ALGORITHM The LR parsing algorithms (Aho and Ullman 1972, Aho and Johnson 1974) were developed originally for programming languages. An LR parsing algorithm is a Copyright 1987 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87 /010031-46$ 03.00 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 31 Masaru Tomita An Efficient </context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, A.V. and Ullman, J.D. 1972 The Theory of Parsing, Translation, and Compiling. Prentice-Hall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<title>Principles of Compiler Design.</title>
<date>1977</date>
<publisher>Addison Wesley.</publisher>
<contexts>
<context position="4556" citStr="Aho and Ullman (1977)" startWordPosition="667" endWordPosition="670">o copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87 /010031-46$ 03.00 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 31 Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm shift-reduce parsing algorithm deterministically guided by a parsing table indicating what action should be taken next. The parsing table can be obtained automatically from a context-free phrase structure grammar, using an algorithm first developed by DeRemer (1969, 1971). We do not describe the algorithms here, referring the reader to chapter 6 in Aho and Ullman (1977). We assume that the reader is familiar with the standard LR parsing algorithm (not necessarily with the parsing table construction algorithm). The LR paring algorithm is one of the most efficient parsing algorithms. It is totally deterministic, and no backtracking or search is involved. Unfortunately, we cannot directly adopt the LR parsing technique for natural languages, because it is applicable only to a small subset of context-free grammars called LR grammars, and it is almost certain that any practical natural language grammars are not LR. If a grammar is non-LR, its parsing table will h</context>
<context position="6021" citStr="Aho and Ullman (1977)" startWordPosition="913" endWordPosition="916">tries &amp;quot;sh n&amp;quot; in the action table (the left part of the table) indicate the action &amp;quot;shift one word from input buffer onto the stack, and go to state n&amp;quot;. Entries &amp;quot;re n&amp;quot; indicate the action &amp;quot;reduce constituents on the stack using rule n&amp;quot;. The entry &amp;quot;acc&amp;quot; stands for the action &amp;quot;accept&amp;quot;, and blank spaces represent &amp;quot;error&amp;quot;. The goto table (the right part of the table) decides to what state the parser should go after a reduce action. These operations shall become clear when we trace the algorithm with example sentences in section 4. The exact definition and operation of the LR parser can be found in Aho and Ullman (1977). We can see that there are two multiple entries in the action table; on the rows of state 11 and 12 at the (1) S --&gt; NP VP (2) S --&gt; S PP (3) NP --&gt; on (4) NP --&gt; odet on (5) NP --&gt; NP PP (6) PP --&gt; *prep NP (7) VP --&gt; ov NP Figure 2.1. An example ambiguous grammar. column labeled &amp;quot;prep&amp;quot;. Roughly speaking, this is the situation where the parser encounters a preposition of a PP right after a NP. If this PP does not modify the NP, then the parser can go ahead to reduce the NP into a higher nonterminal such as PP or VP, using rule 6 or 7, respectively (re&amp; and re7 in the multiple entries). If, o</context>
</contexts>
<marker>Aho, Ullman, 1977</marker>
<rawString>Aho, A.V. and Ullman, J.D. 1977 Principles of Compiler Design. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bresnan</author>
<author>R Kaplan</author>
</authors>
<title>Lexical-Functional Grammar: A Formal System for Grammatical Representation. The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<pages>173--281</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="35723" citStr="Bresnan and Kaplan 1982" startWordPosition="6448" endWordPosition="6451">a very ambiguous sentence with a million possible parses in a reasonable time. 10 100 1000 10000 100000 1000000 Sentence Ambiguity (the number of parses) Figure 5.5. Parsing time and ambiguity. 6 AUGMENTED CONTEXT-FREE GRAMMARS So far, we have described the algorithm as a pure context-free parsing algorithm. In practice, it is often desired for each grammar nonterminal to have attributes, and for each grammar rule to have an augmentation to define, pass, and test the attribute values. It is also desired to produce a functional structure (in the sense of functional grammar formalism (Kay 1984, Bresnan and Kaplan 1982) rather than the context-free forest. Subsection 6.1 describes the augmentation, and subsection 6.2 discusses the shared-packed representation for functional structures. 6.1 THE AUGMENTATION We attach a Lisp function to each grammar rule for this augmentation. Whenever the parser reduces constituents into a higher-level nonterminal using a phrase structure rule, the Lisp program associated with the rule is evaluated. The Lisp program handles such aspects as construction of a syntax/semantic representation of the input sentence, passing attribute values among constituents at different levels an</context>
<context position="36958" citStr="Bresnan and Kaplan 1982" startWordPosition="6628" endWordPosition="6631">yntactic/semantic constraints such as subject-verb agreement. If the Lisp function returns NIL, the parser does not do the reduce action with the rule. If the Lisp function returns a non-NIL value, then this value is given to the newly created non-terminal. The value includes attributes of the nonterminal and a partial syntactic/semantic representation constructed thus far. Notice that those Lisp functions can be precompiled into machine code by the standard Lisp compiler. 6.2 SHARING AND PACKING FUNCTIONAL STRUCTURES A functional structure used in the functional grammar formalisms (Kay 1984, Bresnan and Kaplan 1982, Shieber 1985) is in general a directed acyclic graph (dag) rather than a tree. This is because some value may be shared by two different attributes in the same sentence (e.g., the &amp;quot;agreement&amp;quot; attributes of subject and main verb). Pereira (1985) introduced a method to share dag structures. However, the dag structure sharing method is much more complex and computationally expensive than tree structure sharing. Therefore, we handle only treestructured functional structures for the sake of efficiency and simplicity.6 In the example, the &amp;quot;agreement&amp;quot; attributes of subject and main verb may thus ha</context>
<context position="39897" citStr="Bresnan and Kaplan 1982" startWordPosition="7100" endWordPosition="7103"> the noun phrases: pregnant women and children large file equipment Although more comprehensive experiments are desired, it is expected that only a few packed nodes need to be unpacked in practical applications. 6.3 THE LFG COMPILER It is in general very painful to create, extend, and modify augmentations written in Lisp. The Lisp functions should be generated automatically from more abstract specifications. We have implemented the LFG compiler that compiles augmentations in a higher level notation into Lisp functions. The notation is similar to the Lexical Functional Grammar (LFG) formalism (Bresnan and Kaplan 1982) and PATR-1I (Shieber 1984). An example of the LFG-like notation and its compiled Lisp function are shown in Figures 6.1 and 6.2. We generate only non-destructive functions with no side-effects to make sure that a process never alters other processes or the parser&apos;s control flow. A generated function takes a list of arguments, each of which is a value associated with each right-hand side symbol, and returns a value to be associated with the left-hand side symbol. Each value is a list of f-structures, in case of disjunction and local ambiguity. That a semantic grammar in the LFG-like notation c</context>
</contexts>
<marker>Bresnan, Kaplan, 1982</marker>
<rawString>Bresnan, J. and Kaplan, R. 1982 Lexical-Functional Grammar: A Formal System for Grammatical Representation. The Mental Representation of Grammatical Relations. MIT Press, Cambridge, Massachusetts: 173-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Carbonell</author>
<author>M Tomita</author>
</authors>
<title>Knowledge-Based Machine Translation, the CMU Approach. Machine Translation: Theoretical and Methodological Issues.</title>
<date>1986</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="45622" citStr="Carbonell and Tomita 1986" startWordPosition="8102" endWordPosition="8105">and it can warn the user immediately. In this way, on-line parsing could provide better man-machine communication. Further studies on human factors are necessary. 8 CONCLUSION This paper has introduced an efficient context-free parsing algorithm, and its application to on-line natural language interfaces has been discussed. A pilot on-line parser was first implemented in MacLisp at the Computer Science Department, Carnegie-Mellon University (CMU) as a part of the author&apos;s thesis work (Tomita 1985). The empirical results in section 5 are based on this parser. CMU&apos;s machine translation project (Carbonell and Tomita 1986) adopts on-line parsing for multiple languages. It can parse unsegmented sentences (with no spaces between words, typical in Japanese). To handle unsegmented sentences, its grammar is written in a character-based manner; all terminal symbols in the grammar are characters rather than words. Thus, morphological rules, as well as syntactic rules, are written in the augmented context-free grammar. The parser takes about 1-3 seconds CPU time per sentence on a Symbolics 3600 with about 800 grammar rules; its response time (real time), however, is less than a second due to on-line parsing. This speed</context>
</contexts>
<marker>Carbonell, Tomita, 1986</marker>
<rawString>Carbonell, J.G. and Tomita, M. 1986 Knowledge-Based Machine Translation, the CMU Approach. Machine Translation: Theoretical and Methodological Issues. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>R Patil</author>
</authors>
<title>Coping with Syntactic Ambiguity, or How to Put the Block in the Box on the Table.</title>
<date>1982</date>
<tech>Technical Report MIT/LC5/TM-216,</tech>
<institution>Laboratory for Computer Science, Massachusetts Institute of Technology,</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="15648" citStr="Church and Patil 1982" startWordPosition="2663" endWordPosition="2666"> sentence (parse forest). The next section describes how to represent the parse forest and how to build it with our parsing algorithm. Figure 2.5. A graph-structured stack. 3 AN EFFICIENT REPRESENTATION OF A PARSE FOREST Our parsing algorithm is an all-path parsing algorithm; that is, it produces all possible parses in case an input sentence is ambiguous. Such all-path parsing is of ten needed in natural language processing to manage temporarily or absolutely ambiguous input sentences. The ambiguity (the number of parses) of a sentence may grow exponentially as the length of a sentence grows (Church and Patil 1982). Thus, one might notice that, even with an efficient parsing algorithm such as the one we described, the parser would take exponential time because exponential time would be required merely to print out all parse trees (parse forest). We must therefore provide an efficient representation so that the size of the parse forest does not grow exponentially. This section describes two techniques for providing an efficient representation: subtree sharing and local ambiguity packing. It should be mentioned that these two techniques are not completely new ideas, and some existing systems (e.g., Earley</context>
</contexts>
<marker>Church, Patil, 1982</marker>
<rawString>Church, K. and Patil, R. 1982 Coping with Syntactic Ambiguity, or How to Put the Block in the Box on the Table. Technical Report MIT/LC5/TM-216, Laboratory for Computer Science, Massachusetts Institute of Technology, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F L DeRemer</author>
</authors>
<title>Practical Translators for LR(k) Languages.</title>
<date>1969</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology,</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="4449" citStr="DeRemer (1969" startWordPosition="650" endWordPosition="651">mmercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87 /010031-46$ 03.00 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 31 Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm shift-reduce parsing algorithm deterministically guided by a parsing table indicating what action should be taken next. The parsing table can be obtained automatically from a context-free phrase structure grammar, using an algorithm first developed by DeRemer (1969, 1971). We do not describe the algorithms here, referring the reader to chapter 6 in Aho and Ullman (1977). We assume that the reader is familiar with the standard LR parsing algorithm (not necessarily with the parsing table construction algorithm). The LR paring algorithm is one of the most efficient parsing algorithms. It is totally deterministic, and no backtracking or search is involved. Unfortunately, we cannot directly adopt the LR parsing technique for natural languages, because it is applicable only to a small subset of context-free grammars called LR grammars, and it is almost certai</context>
</contexts>
<marker>DeRemer, 1969</marker>
<rawString>DeRemer, F.L. 1969 Practical Translators for LR(k) Languages. Ph.D. thesis, Massachusetts Institute of Technology, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F L DeRemer</author>
</authors>
<title>Simple LR(k) Grammars.</title>
<date>1971</date>
<journal>Communications ACM</journal>
<volume>14</volume>
<issue>7</issue>
<pages>453--460</pages>
<marker>DeRemer, 1971</marker>
<rawString>DeRemer, F.L. 1971 Simple LR(k) Grammars. Communications ACM 14(7): 453-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm.</title>
<date>1970</date>
<journal>Communications ACM</journal>
<volume>6</volume>
<issue>8</issue>
<pages>94--102</pages>
<marker>Earley, 1970</marker>
<rawString>Earley, J. 1970 An Efficient Context-Free Parsing Algorithm. Communications ACM 6(8): 94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
<author>A G Hauptmann</author>
<author>J G Carbonell</author>
<author>M Tomita</author>
</authors>
<title>Parsing Spoken Language: A Semantic Caseframe Approach.</title>
<date>1986</date>
<booktitle>In Proceedings of the 11th International Conference on Computation Linguistics (COLING86).</booktitle>
<contexts>
<context position="46774" citStr="Hayes et al. 1986" startWordPosition="8285" endWordPosition="8288"> however, is less than a second due to on-line parsing. This speed does not seem to be affected very much by the length of sentence or the size of grammar, as discussed in section 5. We expect further improvements for fully segmented sentences (such as English) where words rather then characters are the atomic units. A commercial on-line parser for Japanese language is being developed in Common Lisp jointly by Intelligent Technology Incorporation (ITI) and Carnegie Group Incorporation (CGI), based on the technique developed at CMU. Finally, in the continuous speech recognition project at CMU (Hayes et al. 1986), the on-line parsing algoComputational Linguistics, Volume 13, Numbers 1-2, January-June 1987 45 Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm rithm is being extended to handle speech input, to make the speech parsing process efficient and capable of being pipelined with lower level processes such as acoustic/phonetic level recognition (Tomita 1986). AKNOWLEDGMENTS I would like to thank Jaime Carbonell, Phil Hayes, James Allen, Herb Simon, Hozumi Tanaka, and Ralph Grishman for their helpful comments on the early version of this paper. Kazuhiro Toyoshima and Hideto Kagami</context>
</contexts>
<marker>Hayes, Hauptmann, Carbonell, Tomita, 1986</marker>
<rawString>Hayes, P.J.; Hauptmann, A.G.; Carbonell, J.G.; and Tomita, M. 1986 Parsing Spoken Language: A Semantic Caseframe Approach. In Proceedings of the 11th International Conference on Computation Linguistics (COLING86).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Functional Unification Grammar: A Formalism for Machine Translation.</title>
<date>1984</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Linguistics:</booktitle>
<pages>75--78</pages>
<contexts>
<context position="35697" citStr="Kay 1984" startWordPosition="6446" endWordPosition="6447">rest for) a very ambiguous sentence with a million possible parses in a reasonable time. 10 100 1000 10000 100000 1000000 Sentence Ambiguity (the number of parses) Figure 5.5. Parsing time and ambiguity. 6 AUGMENTED CONTEXT-FREE GRAMMARS So far, we have described the algorithm as a pure context-free parsing algorithm. In practice, it is often desired for each grammar nonterminal to have attributes, and for each grammar rule to have an augmentation to define, pass, and test the attribute values. It is also desired to produce a functional structure (in the sense of functional grammar formalism (Kay 1984, Bresnan and Kaplan 1982) rather than the context-free forest. Subsection 6.1 describes the augmentation, and subsection 6.2 discusses the shared-packed representation for functional structures. 6.1 THE AUGMENTATION We attach a Lisp function to each grammar rule for this augmentation. Whenever the parser reduces constituents into a higher-level nonterminal using a phrase structure rule, the Lisp program associated with the rule is evaluated. The Lisp program handles such aspects as construction of a syntax/semantic representation of the input sentence, passing attribute values among constitue</context>
<context position="36933" citStr="Kay 1984" startWordPosition="6626" endWordPosition="6627">checking syntactic/semantic constraints such as subject-verb agreement. If the Lisp function returns NIL, the parser does not do the reduce action with the rule. If the Lisp function returns a non-NIL value, then this value is given to the newly created non-terminal. The value includes attributes of the nonterminal and a partial syntactic/semantic representation constructed thus far. Notice that those Lisp functions can be precompiled into machine code by the standard Lisp compiler. 6.2 SHARING AND PACKING FUNCTIONAL STRUCTURES A functional structure used in the functional grammar formalisms (Kay 1984, Bresnan and Kaplan 1982, Shieber 1985) is in general a directed acyclic graph (dag) rather than a tree. This is because some value may be shared by two different attributes in the same sentence (e.g., the &amp;quot;agreement&amp;quot; attributes of subject and main verb). Pereira (1985) introduced a method to share dag structures. However, the dag structure sharing method is much more complex and computationally expensive than tree structure sharing. Therefore, we handle only treestructured functional structures for the sake of efficiency and simplicity.6 In the example, the &amp;quot;agreement&amp;quot; attributes of subject </context>
</contexts>
<marker>Kay, 1984</marker>
<rawString>Kay, M. 1984 Functional Unification Grammar: A Formalism for Machine Translation. In Proceedings of the 10th International Conference on Computational Linguistics: 75-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
</authors>
<title>A Structure-Sharing Representation for Unification-Based Grammar Formalisms.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics:</booktitle>
<pages>137--144</pages>
<contexts>
<context position="37204" citStr="Pereira (1985)" startWordPosition="6672" endWordPosition="6673">l. The value includes attributes of the nonterminal and a partial syntactic/semantic representation constructed thus far. Notice that those Lisp functions can be precompiled into machine code by the standard Lisp compiler. 6.2 SHARING AND PACKING FUNCTIONAL STRUCTURES A functional structure used in the functional grammar formalisms (Kay 1984, Bresnan and Kaplan 1982, Shieber 1985) is in general a directed acyclic graph (dag) rather than a tree. This is because some value may be shared by two different attributes in the same sentence (e.g., the &amp;quot;agreement&amp;quot; attributes of subject and main verb). Pereira (1985) introduced a method to share dag structures. However, the dag structure sharing method is much more complex and computationally expensive than tree structure sharing. Therefore, we handle only treestructured functional structures for the sake of efficiency and simplicity.6 In the example, the &amp;quot;agreement&amp;quot; attributes of subject and main verb may thus have two different values. The identity of these two values is tested explicitly by a test in the augmentation. Sharing treestructured functional structures requires only a minor modification on the subtree sharing method for the shared-packed fore</context>
</contexts>
<marker>Pereira, 1985</marker>
<rawString>Pereira, F.C.N. 1985 A Structure-Sharing Representation for Unification-Based Grammar Formalisms. In Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics: 137-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>Sentence Disambiguation by a Shift-Reduce Parsing Technique.</title>
<date>1983</date>
<booktitle>In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics:</booktitle>
<pages>113--118</pages>
<contexts>
<context position="5258" citStr="Shieber 1983" startWordPosition="781" endWordPosition="783">sarily with the parsing table construction algorithm). The LR paring algorithm is one of the most efficient parsing algorithms. It is totally deterministic, and no backtracking or search is involved. Unfortunately, we cannot directly adopt the LR parsing technique for natural languages, because it is applicable only to a small subset of context-free grammars called LR grammars, and it is almost certain that any practical natural language grammars are not LR. If a grammar is non-LR, its parsing table will have multiple entries;1 one or more of the action table entries will be multiply defined (Shieber 1983). Figures 2.1 and 2.2 show an example of a non-LR grammar and its parsing table. Grammar symbols starting with &amp;quot;s&amp;quot; represent pre-terminals. Entries &amp;quot;sh n&amp;quot; in the action table (the left part of the table) indicate the action &amp;quot;shift one word from input buffer onto the stack, and go to state n&amp;quot;. Entries &amp;quot;re n&amp;quot; indicate the action &amp;quot;reduce constituents on the stack using rule n&amp;quot;. The entry &amp;quot;acc&amp;quot; stands for the action &amp;quot;accept&amp;quot;, and blank spaces represent &amp;quot;error&amp;quot;. The goto table (the right part of the table) decides to what state the parser should go after a reduce action. These operations shall beco</context>
</contexts>
<marker>Shieber, 1983</marker>
<rawString>Shieber, S.M. 1983 Sentence Disambiguation by a Shift-Reduce Parsing Technique. In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics: 113-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>The Design of a Computer Language for Linguistic Information.</title>
<date>1984</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Linguistics:</booktitle>
<pages>362--366</pages>
<contexts>
<context position="39924" citStr="Shieber 1984" startWordPosition="7106" endWordPosition="7107">children large file equipment Although more comprehensive experiments are desired, it is expected that only a few packed nodes need to be unpacked in practical applications. 6.3 THE LFG COMPILER It is in general very painful to create, extend, and modify augmentations written in Lisp. The Lisp functions should be generated automatically from more abstract specifications. We have implemented the LFG compiler that compiles augmentations in a higher level notation into Lisp functions. The notation is similar to the Lexical Functional Grammar (LFG) formalism (Bresnan and Kaplan 1982) and PATR-1I (Shieber 1984). An example of the LFG-like notation and its compiled Lisp function are shown in Figures 6.1 and 6.2. We generate only non-destructive functions with no side-effects to make sure that a process never alters other processes or the parser&apos;s control flow. A generated function takes a list of arguments, each of which is a value associated with each right-hand side symbol, and returns a value to be associated with the left-hand side symbol. Each value is a list of f-structures, in case of disjunction and local ambiguity. That a semantic grammar in the LFG-like notation can also be generated automa</context>
</contexts>
<marker>Shieber, 1984</marker>
<rawString>Shieber, S.M. 1984 The Design of a Computer Language for Linguistic Information. In Proceedings of the 10th International Conference on Computational Linguistics: 362-366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>Using Restriction to Extend Parsing Algorithms for Complex-Feature-Based Grammar Formalisms,</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics:</booktitle>
<pages>145--152</pages>
<contexts>
<context position="36973" citStr="Shieber 1985" startWordPosition="6632" endWordPosition="6634">ints such as subject-verb agreement. If the Lisp function returns NIL, the parser does not do the reduce action with the rule. If the Lisp function returns a non-NIL value, then this value is given to the newly created non-terminal. The value includes attributes of the nonterminal and a partial syntactic/semantic representation constructed thus far. Notice that those Lisp functions can be precompiled into machine code by the standard Lisp compiler. 6.2 SHARING AND PACKING FUNCTIONAL STRUCTURES A functional structure used in the functional grammar formalisms (Kay 1984, Bresnan and Kaplan 1982, Shieber 1985) is in general a directed acyclic graph (dag) rather than a tree. This is because some value may be shared by two different attributes in the same sentence (e.g., the &amp;quot;agreement&amp;quot; attributes of subject and main verb). Pereira (1985) introduced a method to share dag structures. However, the dag structure sharing method is much more complex and computationally expensive than tree structure sharing. Therefore, we handle only treestructured functional structures for the sake of efficiency and simplicity.6 In the example, the &amp;quot;agreement&amp;quot; attributes of subject and main verb may thus have two differen</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Shieber, S.M. 1985 Using Restriction to Extend Parsing Algorithms for Complex-Feature-Based Grammar Formalisms, In Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics: 145-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H R Tennant</author>
<author>KM Ross</author>
<author>R M Saenz</author>
<author>C W Thompson</author>
<author>J R Miller</author>
</authors>
<title>Menu-Based Natural Language Understanding.</title>
<date>1983</date>
<booktitle>In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics:</booktitle>
<pages>151--158</pages>
<contexts>
<context position="44151" citStr="Tennant et al. 1983" startWordPosition="7867" endWordPosition="7870"> needs to store system status each time a word is parsed. Fortunately, this can be nicely done with our parsing algorithm; only pointers to the graph-structured stack and the parse forest need to be stored. It should be noted that our parsing algorithm is not the only algorithm that parses a sentence strictly from left to right; Other left-to-right algorithms include Earley&apos;s (1970) algorithm, the active chart parsing algorithm (Winograd 1983), and a breadth-first version of ATN (Woods 1970). Despite the availability of left-to-right algorithms, surprisingly few on-line parsers exist. NLMenu (Tennant et al. 1983) adopted on-line parsing for a menu-based system but not for typed inputs. In the rest of this section, we discuss two benefits of on-line parsing, quicker response time and early error detection. One obvious benefit of on-line parsing is that it reduces the parser&apos;s response time significantly. When the user finishes typing a whole sentence, most of the input sentence has been already processed by the parser. Although this does not affect CPU time, it could reduce response time from the user&apos;s point of view significantly. On-line parsing is therefore useful in interactive systems in which inp</context>
</contexts>
<marker>Tennant, Ross, Saenz, Thompson, Miller, 1983</marker>
<rawString>Tennant, H.R.; Ross, KM.; Saenz, R.M.; Thompson, C.W.; and Miller, J.R. 1983 Menu-Based Natural Language Understanding. In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics: 151-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>LR Parsers for Natural Language.</title>
<date>1984</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Linguistics (COLING84).</booktitle>
<contexts>
<context position="7576" citStr="Tomita 1984" startWordPosition="1193" endWordPosition="1194">s necessary. We handle multiple entries with a special technique, named a graphstructured stack. In order to introduce the concept, we first give a simpler form of non-determinism, and make refinements on it. Subsection 2.1 describes a simple and straightforward non-deterministic technique, that is, pseudo-parallelism (breadth-first search), in which the system maintains a number of stacks simultaneously, called the Stack List. A disadvantage of the stack list is then described. The next subsection describes the idea of stack combination, which was introduced in the author&apos;s earlier research (Tomita 1984), to make the algorithm much more efficient. With this idea, stacks are represented as trees (or a forest). Finally, a further refinement, the graph-structured stack, is described to make the algoState *det on ov *prep $ NP PP VP S 0 sh3 sh4 2 1 1 sh6 acc 5 2 sh7 sh6 9 8 3 sh10 4 re3 re3 re3 5 re2 re2 6 sh3 sh4 11 7 sh3 sh4 12 8 rel rel 9 re5 re5 re5 10 re4 re4 re4 11 re6 re6,sh6 re6 9 12 re7,sh6 rel Figure 2.2. LR parsing table with multiple entries. 32 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm rith</context>
</contexts>
<marker>Tomita, 1984</marker>
<rawString>Tomita, M. 1984 LR Parsers for Natural Language. In Proceedings of the 10th International Conference on Computational Linguistics (COLING84).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems.</title>
<date>1985</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="32872" citStr="Tomita (1985)" startWordPosition="5951" endWordPosition="5952">] 4 •det &apos;al 5 *ri &amp;quot;&amp;quot;) 6 NP (4 51 7 [PP (2 6) 8 [VP (3 6) 9 [NP (1 7)] 10 [S (1 8)] (error) lace) The possibility of the first unknown word being a preposition has now disappeared. The parser accepts the sentence in only one way, and returns &amp;quot;10&amp;quot; as the root node of the parse forest. We have shown that our parsing algorithm can handle unknown words without any special mechanism. 5 EMPIRICAL RESULTS In this section, we present some empirical results of the algorithm&apos;s practical performance. Since space is limited, we only show the highlights of the results, referring the reader to chapter 6 of Tomita (1985) for more detail. Figure 5.1 shows the relationship between parsing time of the Tomita algorithm and the length of input sentence, and Figure 5.2 shows the comparison with Earley&apos;s algorithm (or active chart parsing), using a sample English grammar that consists of 220 context-free rules and 40 sample sentences taken from actual publications. All programs are run on DEC-20 and written in MacLisp, but not compiled. Although the experiment is informal, the result show that the Tomita algorithm is about 5 to 10 times faster than Earley&apos;s algorithm, due to the pre-compilation of the grammar into t</context>
<context position="45498" citStr="Tomita 1985" startWordPosition="8085" endWordPosition="8086">a file. Another benefit of on-line parsing is that it can detect an error almost as soon as the error occurs, and it can warn the user immediately. In this way, on-line parsing could provide better man-machine communication. Further studies on human factors are necessary. 8 CONCLUSION This paper has introduced an efficient context-free parsing algorithm, and its application to on-line natural language interfaces has been discussed. A pilot on-line parser was first implemented in MacLisp at the Computer Science Department, Carnegie-Mellon University (CMU) as a part of the author&apos;s thesis work (Tomita 1985). The empirical results in section 5 are based on this parser. CMU&apos;s machine translation project (Carbonell and Tomita 1986) adopts on-line parsing for multiple languages. It can parse unsegmented sentences (with no spaces between words, typical in Japanese). To handle unsegmented sentences, its grammar is written in a character-based manner; all terminal symbols in the grammar are characters rather than words. Thus, morphological rules, as well as syntactic rules, are written in the augmented context-free grammar. The parser takes about 1-3 seconds CPU time per sentence on a Symbolics 3600 wi</context>
</contexts>
<marker>Tomita, 1985</marker>
<rawString>Tomita, M. 1985 Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems. Kluwer Academic Publishers, Boston, Massachusetts.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Tomita</author>
</authors>
<title>1986a Sentence Disambiguation by Asking.</title>
<journal>Computers and Translation</journal>
<volume>1</volume>
<issue>1</issue>
<pages>39--51</pages>
<marker>Tomita, </marker>
<rawString>Tomita, M. 1986a Sentence Disambiguation by Asking. Computers and Translation 1(1): 39-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>An Efficient Word Lattice Parsing Algorithm for Continuous Speech Recognition.</title>
<date>1986</date>
<booktitle>In Proceedings of IEEE-IECE-ASJ International conference on Acoustics, Speech, and Signal Processing (ICASSP86).</booktitle>
<contexts>
<context position="19369" citStr="Tomita 1986" startWordPosition="3294" endWordPosition="3295">; one with three subnodes and two with two subnodes. Local ambiguity packing can be easily implemented with our parsing algorithm as follows. In the graph-structured stack, if two or more symbol vertices have a common state vertex immediately on their left and a common state vertex immediately on their right, they represent local ambiguity. Nodes pointed to by these symbol vertices are to be packed as one node. In Figure 2.5, for example, we see one 5-way local ambiguity and two 2-way local ambiguities. The algorithm is made clear by the example in the following section. Recently, the author (Tomita 1986) suggested a technique to disambiguate a sentence out of the sharedpacked forest representation by asking the user a minimal number of questions in natural language (without showing any tree structures). 4 EXAMPLES This section presents three examples. The first example, using the sentence I saw a man in the apartment with a telescope, is intended to help the reader understand the algorithm mOre clearly. The second example, with the sentence That information is important is doubtful, is presented to demonstrate that our algorithm is able to handle multi-part-of-speech words without any special</context>
<context position="45622" citStr="Tomita 1986" startWordPosition="8104" endWordPosition="8105">n the user immediately. In this way, on-line parsing could provide better man-machine communication. Further studies on human factors are necessary. 8 CONCLUSION This paper has introduced an efficient context-free parsing algorithm, and its application to on-line natural language interfaces has been discussed. A pilot on-line parser was first implemented in MacLisp at the Computer Science Department, Carnegie-Mellon University (CMU) as a part of the author&apos;s thesis work (Tomita 1985). The empirical results in section 5 are based on this parser. CMU&apos;s machine translation project (Carbonell and Tomita 1986) adopts on-line parsing for multiple languages. It can parse unsegmented sentences (with no spaces between words, typical in Japanese). To handle unsegmented sentences, its grammar is written in a character-based manner; all terminal symbols in the grammar are characters rather than words. Thus, morphological rules, as well as syntactic rules, are written in the augmented context-free grammar. The parser takes about 1-3 seconds CPU time per sentence on a Symbolics 3600 with about 800 grammar rules; its response time (real time), however, is less than a second due to on-line parsing. This speed</context>
<context position="47148" citStr="Tomita 1986" startWordPosition="8338" endWordPosition="8339">veloped in Common Lisp jointly by Intelligent Technology Incorporation (ITI) and Carnegie Group Incorporation (CGI), based on the technique developed at CMU. Finally, in the continuous speech recognition project at CMU (Hayes et al. 1986), the on-line parsing algoComputational Linguistics, Volume 13, Numbers 1-2, January-June 1987 45 Masaru Tomita An Efficient Augmented-Context-Free Parsing Algorithm rithm is being extended to handle speech input, to make the speech parsing process efficient and capable of being pipelined with lower level processes such as acoustic/phonetic level recognition (Tomita 1986). AKNOWLEDGMENTS I would like to thank Jaime Carbonell, Phil Hayes, James Allen, Herb Simon, Hozumi Tanaka, and Ralph Grishman for their helpful comments on the early version of this paper. Kazuhiro Toyoshima and Hideto Kagamida have implemented the runtime parser and the LR table compiler, respectively, in Common Lisp. Lori Levin, Teruko Watanabe, Peggy Anderson, and Donna Gates have developed Japanese and English grammars in the LFG-like notation. Hiroaki Saito has implemented the algorithm for speech input. Ron Kaplan, Martin Kay, Lauri Karttunen, and Stuart Shieber provided useful comments</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>Tomita, M. 1986b An Efficient Word Lattice Parsing Algorithm for Continuous Speech Recognition. In Proceedings of IEEE-IECE-ASJ International conference on Acoustics, Speech, and Signal Processing (ICASSP86).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Language as a Cognitive Process.</title>
<date>1983</date>
<journal>Communications of ACM</journal>
<volume>13</volume>
<pages>591--606</pages>
<publisher>Addison Wesley. Woods,</publisher>
<location>W.A.</location>
<contexts>
<context position="43978" citStr="Winograd 1983" startWordPosition="7845" endWordPosition="7846">essed by the parser, the parser must be able to &amp;quot;unparse&amp;quot; the words, without parsing the sentence from the beginning all over again. To implement unparsing, the parser needs to store system status each time a word is parsed. Fortunately, this can be nicely done with our parsing algorithm; only pointers to the graph-structured stack and the parse forest need to be stored. It should be noted that our parsing algorithm is not the only algorithm that parses a sentence strictly from left to right; Other left-to-right algorithms include Earley&apos;s (1970) algorithm, the active chart parsing algorithm (Winograd 1983), and a breadth-first version of ATN (Woods 1970). Despite the availability of left-to-right algorithms, surprisingly few on-line parsers exist. NLMenu (Tennant et al. 1983) adopted on-line parsing for a menu-based system but not for typed inputs. In the rest of this section, we discuss two benefits of on-line parsing, quicker response time and early error detection. One obvious benefit of on-line parsing is that it reduces the parser&apos;s response time significantly. When the user finishes typing a whole sentence, most of the input sentence has been already processed by the parser. Although this</context>
</contexts>
<marker>Winograd, 1983</marker>
<rawString>Winograd, T. 1983 Language as a Cognitive Process. Addison Wesley. Woods, W.A. 1970 Transition Network Grammars for Natural Language Analysis. Communications of ACM 13: 591-606.</rawString>
</citation>
<citation valid="false">
<authors>
<author>NOTES</author>
</authors>
<title>This research was sponsored by the Defense Advanced Research Projects Agency (DOD),</title>
<booktitle>ARPA Order No. 3597, monitored by the Air Force Avionics Laboratory under Contract</booktitle>
<pages>33615--81</pages>
<marker>NOTES, </marker>
<rawString>NOTES 1. This research was sponsored by the Defense Advanced Research Projects Agency (DOD), ARPA Order No. 3597, monitored by the Air Force Avionics Laboratory under Contract F33615-81-K-1539.</rawString>
</citation>
<citation valid="false">
<title>The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the US Government. 2. The situation is often called conflict.</title>
<marker></marker>
<rawString>The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the US Government. 2. The situation is often called conflict.</rawString>
</citation>
<citation valid="false">
<title>Although it is possibly reduced if some processes reach error entries and die.</title>
<marker></marker>
<rawString>3. Although it is possibly reduced if some processes reach error entries and die.</rawString>
</citation>
<citation valid="false">
<title>The term node is used for forest representation, whereas the term vertex is used for graph-structured stack representation. 5. In practice; not in theory.</title>
<marker></marker>
<rawString>4. The term node is used for forest representation, whereas the term vertex is used for graph-structured stack representation. 5. In practice; not in theory.</rawString>
</citation>
<citation valid="false">
<title>Although we plan to handle dag structures in the future, tree structures may be adequate, as GPSGs use tree structures rather than dag structures.</title>
<marker></marker>
<rawString>6. Although we plan to handle dag structures in the future, tree structures may be adequate, as GPSGs use tree structures rather than dag structures.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>