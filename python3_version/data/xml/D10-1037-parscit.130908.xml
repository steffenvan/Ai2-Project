<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.890722">
Incorporating Content Structure into Text Analysis Applications
</title>
<author confidence="0.997141">
Christina Sauper, Aria Haghighi, Regina Barzilay
</author>
<affiliation confidence="0.998756">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.995666">
{csauper, aria42, regina}@csail.mit.edu
</email>
<sectionHeader confidence="0.997362" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99903275">
In this paper, we investigate how modeling
content structure can benefit text analysis ap-
plications such as extractive summarization
and sentiment analysis. This follows the lin-
guistic intuition that rich contextual informa-
tion should be useful in these tasks. We
present a framework which combines a su-
pervised text analysis application with the in-
duction of latent content structure. Both of
these elements are learned jointly using the
EM algorithm. The induced content struc-
ture is learned from a large unannotated cor-
pus and biased by the underlying text analysis
task. We demonstrate that exploiting content
structure yields significant improvements over
approaches that rely only on local context.1
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968071428571">
In this paper, we demonstrate that leveraging doc-
ument structure significantly benefits text analysis
applications. As a motivating example, consider
the excerpt from a DVD review shown in Table 1.
This review discusses multiple aspects of a product,
such as audio and video properties. While the word
“pleased” is a strong indicator of positive sentiment,
the sentence in which it appears does not specify the
aspect to which it relates. Resolving this ambiguity
requires information about global document struc-
ture.
A central challenge in utilizing such informa-
tion lies in finding a relevant representation of con-
tent structure for a specific text analysis task. For
</bodyText>
<footnote confidence="0.9599135">
1Code and processed data presented here are available at
http://groups.csail.mit.edu/rbg/code/content structure.html
</footnote>
<bodyText confidence="0.9610555">
Audio Audio choices are English, Spanish and French
Dolby Digital 5.1 ... Bass is still robust and powerful,
giving weight to just about any scene – most notably
the film’s exciting final fight. Fans should be pleased
with the presentation.
Extras This single-disc DVD comes packed in a black
amaray case with a glossy slipcover. Cover art has
clearly been designed to appeal the Twilight crowd ...
Finally, we’ve got a deleted scenes reel. Most of the
excised scenes are actually pretty interesting.
</bodyText>
<tableCaption confidence="0.996649">
Table 1: An excerpt from a DVD review.
</tableCaption>
<bodyText confidence="0.999953590909091">
instance, when performing single-aspect sentiment
analysis, the most relevant aspect of content struc-
ture is whether a given sentence is objective or sub-
jective (Pang and Lee, 2004). In a multi-aspect
setting, however, information about the sentence
topic is required to determine the aspect to which
a sentiment-bearing word relates (Snyder and Barzi-
lay, 2007). As we can see from even these closely re-
lated applications, the content structure representa-
tion should be intimately tied to a specific text anal-
ysis task.
In this work, we present an approach in which a
content model is learned jointly with a text analy-
sis task. We assume complete annotations for the
task itself, but we learn the content model from raw,
unannotated text. Our approach is implemented in
a discriminative framework using latent variables to
represent facets of content structure. In this frame-
work, the original task features (e.g., lexical ones)
are conjoined with latent variables to enrich the fea-
tures with global contextual information. For ex-
ample, in Table 1, the feature associated with the
</bodyText>
<page confidence="0.970819">
377
</page>
<note confidence="0.818015">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 377–387,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999733166666667">
word “pleased” should contribute most strongly to
the sentiment of the audio aspect when it is aug-
mented with a relevant topic indicator.
The coupling of the content model and the task-
specific model allows the two components to mutu-
ally influence each other during learning. The con-
tent model leverages unannotated data to improve
the performance of the task-specific model, while
the task-specific model provides feedback to im-
prove the relevance of the content model. The com-
bined model can be learned effectively using a novel
EM-based method for joint training.
We evaluate our approach on two complementary
text analysis tasks. Our first task is a multi-aspect
sentiment analysis task, where a system predicts the
aspect-specific sentiment ratings (Snyder and Barzi-
lay, 2007). Second, we consider a multi-aspect ex-
tractive summarization task in which a system ex-
tracts key properties for a pre-specified set of as-
pects. On both tasks, our method for incorporating
content structure consistently outperforms structure-
agnostic counterparts. Moreover, jointly learning
content and task parameters yields additional gains
over independently learned models.
</bodyText>
<sectionHeader confidence="0.999924" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999945942028985">
Prior research has demonstrated the usefulness of
content models for discourse-level tasks. Examples
of such tasks include sentence ordering (Barzilay
and Lee, 2004; Elsner et al., 2007), extraction-based
summarization (Haghighi and Vanderwende, 2009)
and text segmentation (Chen et al., 2009). Since
these tasks are inherently tied to document structure,
a content model is essential to performing them suc-
cessfully. In contrast, the applications considered in
this paper are typically developed without any dis-
course information, focusing on capturing sentence-
level relations. Our goal is to augment these models
with document-level content information.
Several applications in information extraction
and sentiment analysis are close in spirit to our
work (Pang and Lee, 2004; Patwardhan and Riloff,
2007; McDonald et al., 2007). These approaches
consider global contextual information when de-
termining whether a given sentence is relevant to
the underlying analysis task. All assume that rele-
vant sentences have been annotated. For instance,
Pang and Lee (2004) refine the accuracy of sen-
timent analysis by considering only the subjective
sentences of a review as determined by an indepen-
dent classifier. Patwardhan and Riloff (2007) take
a similar approach in the context of information ex-
traction. Rather than applying their extractor to all
the sentences in a document, they limit it to event-
relevant sentences. Since these sentences are more
likely to contain information of interest, the extrac-
tion performance increases.
Another approach, taken by Choi and Cardie
(2008) and Somasundaran et al. (2009) uses lin-
guistic resources to create a latent model in a task-
specific fashion to improve performance, rather than
assuming sentence-level task relevancy. Choi and
Cardie (2008) address a sentiment analysis task by
using a heuristic decision process based on word-
level intermediate variables to represent polarity.
Somasundaran et al. (2009) similarly uses a boot-
strapped local polarity classifier to identify sentence
polarity.
McDonald et al. (2007) propose a model
which jointly identifies global polarity as well as
paragraph- and sentence-level polarity, all of which
are observed in training data. While our approach
uses a similar hierarchy, McDonald et al. (2007) is
concerned with recovering the labels at all levels,
whereas in this work we are interested in using la-
tent document content structure as a means to benefit
task predictions.
While our method also incorporates contextual
information into existing text analysis applications,
our approach is markedly different from the above
approaches. First, our representation of context en-
codes more than the relevance-based binary distinc-
tion considered in the past work. Our algorithm ad-
justs the content model dynamically for a given task
rather than pre-specifying it. Second, while previ-
ous work is fully supervised, in our case relevance
annotations are readily available for only a few ap-
plications and are prohibitively expensive to obtain
for many others. To overcome this drawback, our
method induces a content model in an unsupervised
fashion and connects it via latent variables to the
target model. This design not only eliminates the
need for additional annotations, but also allows the
algorithm to leverage large quantities of raw data for
training the content model. The tight coupling of rel-
</bodyText>
<page confidence="0.997886">
378
</page>
<bodyText confidence="0.999805888888889">
evance learning with the target analysis task leads to
further performance gains.
Finally, our work relates to supervised topic mod-
els in Blei and McAullife (2007). In this work, la-
tent topic variables are used to generate text as well
as a supervised sentiment rating for the document.
However, this architecture does not permit the usage
of standard discriminative models which condition
freely on textual features.
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="method">
3 Model
</sectionHeader>
<subsectionHeader confidence="0.999907">
3.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.996394227272727">
In this section, we describe a model which incorpo-
rates content information into a multi-aspect sum-
marization task.2 Our approach assumes that at
training time we have a collection of labeled doc-
uments DL, each consisting of the document text
s and true task-specific labeling y*. For the multi-
aspect summarization task, y* consists of sequence
labels (e.g., value or service) for the tokens of a
document. Specifically, the document text s is
composed of sentences s1, ... , sn and the label-
ings y* consists of corresponding label sequences
y1,...,yn.3
As is common in related work, we model each yi
using a CRF which conditions on the observed doc-
ument text. In this work, we also assume a content
model, which we fix to be the document-level HMM
as used in Barzilay and Lee (2004). In this content
model, each sentence si is associated with a hidden
topic variable Ti which generates the words of the
sentence. We will use T = (T1, ... , Tn) to refer to
the hidden topic sequence for a document. We fix
the number of topics to a pre-specified constant K.
</bodyText>
<subsectionHeader confidence="0.999903">
3.2 Model Overview
</subsectionHeader>
<bodyText confidence="0.9992532">
Our model, depicted in Figure 1, proceeds as fol-
lows: First the document-level HMM generates
a hidden content topic sequence T for the sen-
tences of a document. This content component is
parametrized by θ and decomposes in the standard
</bodyText>
<footnote confidence="0.98376025">
2In Section 3.6, we discuss how this framework can be used
for other text analysis applications.
3Note that each yi is a label sequence across the words in si,
rather than an individual label.
</footnote>
<figureCaption confidence="0.698864">
Figure 1: A graphical depiction of our model for
sequence labeling tasks. The Ti variable represents
the content model topic for the ith sentence si. The
</figureCaption>
<bodyText confidence="0.9760272">
words of si, (w1i , ... , wmi ), each have a task label
(y1 i ,. . . , ym i ). Note that each token label has an
undirected edge to a factor containing the words of
the current sentence, si as well as the topic of the
current sentence Ti.
</bodyText>
<equation confidence="0.979973666666667">
HMM fashion:4
Pθ(s,T) = rln Pθ(Ti|Ti−1) rl Pθ(w|Ti) (1)
i=1 wEsi
</equation>
<bodyText confidence="0.99978225">
Then the label sequences for each sentence in
the document are independently modeled as CRFs
which condition on both the sentence features and
the sentence topic:
</bodyText>
<equation confidence="0.9994815">
Pφ(y|s,T) = rln Pφ(yi|si, Ti) (2)
i=1
</equation>
<bodyText confidence="0.8737395">
Each sentence CRF is parametrized by φ and takes
the standard form:
</bodyText>
<equation confidence="0.9945995">
Pφ(y|s,T) a
φT [fN(yj,s,T) + fE(yj,yj+1)] }
</equation>
<bodyText confidence="0.759059">
4We also utilize a hierarchical emission model so that each
topic distribution interpolates between a topic-specific distribu-
tion as well as a shared background model; this is intended to
capture domain-specific stop words.
</bodyText>
<equation confidence="0.911802">
Ti−1 Ti Ti+1
Si
. . .
(w; = pleased) (Tj = )
w? = pleased
1 2 iM
yi yi ... yi
exp { �
j
</equation>
<page confidence="0.915642">
379
</page>
<figure confidence="0.847605">
Task Labels
</figure>
<figureCaption confidence="0.984991">
Figure 2: A graphical depiction of the generative
</figureCaption>
<bodyText confidence="0.970587428571429">
process for a labeled document at training time (See
Section 3); shaded nodes indicate variables which
are observed at training time. First the latent un-
derlying content structure T is drawn. Then, the
document text s is drawn conditioned on the content
structure utilizing content parameters 0. Finally, the
observed task labels for the document are modeled
given s and T using the task parameters 0. Note that
the arrows for the task labels are undirected since
they are modeled discriminatively.
where fN(·) and fE(·) are feature functions associ-
ated with CRF nodes and transitions respectively.
Allowing the CRF to condition on the sentence
topic Ti permits predictions to be more sensitive to
content. For instance, using the example from Ta-
ble 1, we could have a feature that indicates the word
“pleased” conjoined with the segment topic (see Fig-
ure 1). These topic-specific features serve to disam-
biguate word usage.
This joint process, depicted graphically in Fig-
ure 2, is summarized as:
</bodyText>
<equation confidence="0.99967">
P(T,s,y*) = Pe(T,s)PO(y*|s,T) (3)
</equation>
<bodyText confidence="0.9998965">
Note that this probability decomposes into a
document-level HMM term (the content component)
as well as a product of CRF terms (the task compo-
nent).
</bodyText>
<subsectionHeader confidence="0.999521">
3.3 Learning
</subsectionHeader>
<bodyText confidence="0.999665">
During learning, we would like to find the
document-level HMM parameters 0 and the summa-
rization task CRF parameters 0 which maximize the
likelihood of the labeled documents. The only ob-
served elements of a labeled document are the docu-
ment text s and the aspect labels y*. This objective
is given by:
</bodyText>
<equation confidence="0.9967242">
X
LL(0, 0) =
(s,y*)EDL
X=
(s,y*)EDL
</equation>
<bodyText confidence="0.999909375">
We use the EM algorithm to optimize this objec-
tive.
E-Step The E-Step in EM requires computing the
posterior distribution over latent variables. In this
model, the only latent variables are the sentence top-
ics T. To compute this term, we utilize the decom-
position in Equation (3) and rearrange HMM and
CRF terms to obtain:
</bodyText>
<equation confidence="0.996186428571429">
P(T, s, y*) = Po(T, s)PO(y*|T, s)
!PO(w|Ti) ·
!PO(y*i |si,Ti)
Yn PO(Ti|Ti−1)·
i=1
wEsi
Y !PO(w|Ti)PO(y*i |si, Ti)
</equation>
<bodyText confidence="0.99988">
We note that this expression takes the same form as
the document-level HMM, except that in addition to
emitting the words of a sentence, we also have an
observation associated with the sentence sequence
labeling. We treat each PO(y*i |si, Ti) as part of the
node potential associated with the document-level
HMM. We utilize the Forward-Backward algorithm
as one would with the document-level HMM in iso-
lation, except that each node potential incorporates
this CRF term.
M-Step We perform separate M-Steps for content
and task parameters. The M-Step for the content pa-
rameters is identical to the document-level HMM
</bodyText>
<figure confidence="0.952028368421053">
Content
Structure
T
Text
θ
Content
S Parameters
φ
Task
Parameters
Y∗
log P(s, y*)
X P(T, s, y*)
log
T
Yn P�(Ti|Ti−1) Y
i=1 wEsi
Yn
i=1
</figure>
<page confidence="0.981156">
380
</page>
<bodyText confidence="0.999427916666667">
content model: topic emission and transition dis-
tributions are updated with expected counts derived
from E-Step topic posteriors.
The M-Step for the task parameters does not have
a closed-form solution. Recall that in the M-Step,
we maximize the log probability of all random vari-
ables given expectations of latent variables. Using
the decomposition in Equation (3), it is clear that
the only component of the joint labeled document
probability which relies upon the task parameters is
log Pφ(y*|s, T). Thus for the M-Step, it is sufficient
to optimize the following with respect to 0:
</bodyText>
<equation confidence="0.96617475">
ET|s, y∗ log Pφ(y*|s, T)
ETi|si, y∗i log Pφ(y*i |si, Ti)
XK P(Ti = k|si, y*i ) log Pφ(y*i |si, Ti)
k=1
</equation>
<bodyText confidence="0.999762933333333">
The first equality follows from the decomposition
of the task component into independent CRFs (see
Equation (2)). Optimizing this objective is equiva-
lent to a weighted version of the conditional likeli-
hood objective used to train the CRF in isolation. An
intuitive explanation of this process is that there are
multiple CRF instances, one for each possible hid-
den topic T. Each utilizes different content features
to explain the sentence sequence labeling. These in-
stances are weighted according to the posterior over
T obtained during the E-Step. While this objective
is non-convex due to the summation over T, we can
still optimize it using any gradient-based optimiza-
tion solver; in our experiments, we used the LBFGS
algorithm (Liu et al., 1989).
</bodyText>
<sectionHeader confidence="0.662256" genericHeader="method">
3.4 Inference
</sectionHeader>
<bodyText confidence="0.9999925">
We must predict a label sequence y for each sen-
tence s of the document. We assume a loss function
over a sequence labeling y and a proposed labeling
y, which decomposes as:
</bodyText>
<equation confidence="0.942966666666667">
X
L(y, y) =
j
</equation>
<bodyText confidence="0.910972">
where each position loss is sensitive to the kind of
error which is made. Failing to extract a token is
penalized to a greater extent than extracting it with
an incorrect label:
</bodyText>
<equation confidence="0.992887666666667">
0 if yj = yj
c if yj =6 NONE and yj = NONE
1 otherwise
</equation>
<bodyText confidence="0.928135230769231">
In this definition, NONE represents the background
label which is reserved for tokens which do not cor-
respond to labels of interest. The constant c repre-
sents a user-defined trade-off between precision and
recall errors. For our multi-aspect summarization
task, we select c = 4 for Yelp and c = 5 for Amazon
to combat the high-precision bias typical of condi-
tional likelihood models.
At inference time, we select the single labeling
which minimizes the expected loss with respect to
model posterior over label sequences:
In our case, we must marginalize out the sentence
topic T:
</bodyText>
<equation confidence="0.9957555">
P(yj|s) = X P(yj, T |s)
T
X= Pθ(T|s)Pφ(yj|s, T)
T
</equation>
<bodyText confidence="0.999944076923077">
This minimum risk criterion has been widely used in
NLP applications such as parsing (Goodman, 1999)
and machine translation (DeNero et al., 2009). Note
that the above formulation differs from the stan-
dard CRF due to the latent topic variables. Other-
wise the inference task could be accomplished by
directly obtaining posteriors over each yj state using
the Forward-Backwards algorithm on the sentence
CRF.
Finding y� can be done efficiently. First, we ob-
tain marginal token posteriors as above. Then, the
expected loss of a token prediction is computed as
follows:
</bodyText>
<equation confidence="0.853445">
P(yj|s)L(yj, yj)
</equation>
<bodyText confidence="0.999953">
Once we obtain expected losses of each token pre-
diction, we compute the minimum risk sequence la-
beling by running the Viterbi algorithm. The po-
tential for each position and prediction is given by
</bodyText>
<equation confidence="0.9156848">
= Xn
i=1
= Xn
i=1
L(yj,�yj) =
</equation>
<figure confidence="0.628446363636364">
⎧
⎨⎪
⎪⎩
y� = min Ey|sL(y, y)
y�
X= min Eye|sL(yj, 9j)
y�
j=1
X
yj
L(yj, 9j)
</figure>
<page confidence="0.984872">
381
</page>
<bodyText confidence="0.999180333333333">
the negative expected loss. The maximal scoring se-
quence according to these potentials minimizes the
expected risk.
topic variables become correlated. Thus when learn-
ing, the E-Step requires computing a posterior over
paragraph topic tuples T:
</bodyText>
<subsectionHeader confidence="0.997666">
3.5 Leveraging unannotated data
</subsectionHeader>
<bodyText confidence="0.9997249">
Our model allows us to incorporate unlabeled doc-
uments, denoted DU, to improve the learning of the
content model. For an unlabeled document we only
observe the document text s and assume it is drawn
from the same content model as our labeled docu-
ments. The objective presented in Section 3.3 as-
sumed that all documents were labeled; here we sup-
plement this objective by capturing the likelihood
of unlabeled documents according to the content
model:
</bodyText>
<equation confidence="0.989262">
�
LU(�) =
sEDU
�=
sEDU
</equation>
<bodyText confidence="0.999092333333333">
Our overall objective function is to maximize the
likelihood of both our labeled and unlabeled data.
This objective corresponds to:
</bodyText>
<equation confidence="0.63439">
L(O, 0) =LU(0) + LL(0, 0)
</equation>
<bodyText confidence="0.999700333333333">
This objective can also be optimized using the EM
algorithm, where the E-Step for labeled and unla-
beled documents is outlined above.
</bodyText>
<subsectionHeader confidence="0.918352">
3.6 Generalization
</subsectionHeader>
<bodyText confidence="0.999980625">
The approach outlined can be applied to a wider
range of task components. For instance, in Sec-
tion 4.1 we apply this approach to multi-aspect sen-
timent analysis. In this task, the target y consists of
numeric sentiment ratings (yi, ... , yK) for each of
K aspects. The task component consists of indepen-
dent linear regression models for each aspect sen-
timent rating. For the content model, we associate
a topic with each paragraph; T consists of assign-
ments of topics to each document paragraph.
The model structure still decomposes as in Fig-
ure 2, but the details of learning are slightly differ-
ent. For instance, because the task label (aspect sen-
timent ratings) is not localized to any region of the
document, all content model variables influence the
target response. Conditioned on the target label, all
</bodyText>
<equation confidence="0.839905">
P(T |y, s) oc P(s, T)P(y|T, s)
</equation>
<bodyText confidence="0.999940285714286">
For the case of our multi-aspect sentiment task, this
computation can be done exactly by enumerating
T tuples, since the number of sentences and pos-
sible topics is relatively small. If summation is in-
tractable, the posterior may be approximated using
variational techniques (Bishop, 2006), which is ap-
plicable to a broad range of potential applications.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="method">
4 Experimental Set-Up
</sectionHeader>
<bodyText confidence="0.99982125">
We apply our approach to two text analysis tasks that
stand to benefit from modeling content structure:
multi-aspect sentiment analysis and multi-aspect re-
view summarization.
</bodyText>
<subsectionHeader confidence="0.964996">
4.1 Tasks
</subsectionHeader>
<bodyText confidence="0.999579407407407">
In the following section, we define each task in de-
tail, explain the task-specific adaptation of the model
and describe the data sets used in the experiments.
Table 2 summarizes statistics for all the data sets.
For all tasks, when using a content model with a
task model, we utilize a new set of features which
include all the original features as well as a copy
of each feature conjoined with the content topic as-
signment (see Figure 1). We also include a fea-
ture which indicates whether a given word was most
likely emitted from the underlying topic or from a
background distribution.
Multi-Aspect Sentiment Ranking The goal of
multi-aspect sentiment classification is to predict a
set of numeric ranks that reflects the user satisfaction
for each aspect (Snyder and Barzilay, 2007). One of
the challenges in this task is to attribute sentiment-
bearing words to the aspects they describe. Informa-
tion about document structure has the potential to
greatly reduce this ambiguity.
Following standard sentiment ranking ap-
proaches (Wilson et al., 2004; Pang and Lee, 2005;
Goldberg and Zhu, 2006; Snyder and Barzilay,
2007), we employ ordinary linear regression to
independently map bag-of-words representations
into predicted aspect ranks. In addition to com-
monly used lexical features, this set is augmented
</bodyText>
<equation confidence="0.919191">
log P0(s)
� P0(s,T)
log
T
</equation>
<page confidence="0.978124">
382
</page>
<table confidence="0.999194666666667">
Task Labeled Unlabeled Avg. Size
Train Test Words Sents
Multi-aspect sentiment 600 65 — 1,027 20.5
Multi-aspect summarization
Amazon 35 24 12,684 214 11.7
Yelp 48 48 33,015 178 11.2
</table>
<tableCaption confidence="0.9231045">
Table 2: This table summarizes the size of each corpus. In each case, the unlabeled texts of both labeled and
unlabeled documents are used for training the content model, while only the labeled training corpus is used
</tableCaption>
<bodyText confidence="0.987948428571429">
to train the task model. Note that the entire data set for the multi-aspect sentiment analysis task is labeled.
with content features as described above. For this
application, we fix the number of HMM states to be
equal to the predefined number of aspects.
We test our sentiment ranker on a set of DVD re-
views from the website IGN.com.5 Each review is
accompanied by 1-10 scale ratings in four categories
that assess the quality of a movie’s content, video,
audio, and DVD extras. In this data set, segments
corresponding to each of the aspects are clearly de-
lineated in each document. Therefore, we can com-
pare the performance of the algorithm using auto-
matically induced content models against the gold
standard structural information.
Multi-Aspect Review Summarization The goal
of this task is to extract informative phrases that
identify information relevant to several predefined
aspects of interest. In other words, we would like our
system to both extract important phrases (e.g., cheap
food) and label it with one of the given aspects (e.g.,
value). For concrete examples and lists of aspects
for each data set, see Figures 3b and 3c. Variants of
this task have been considered in review summariza-
tion in previous work (Kim and Hovy, 2006; Brana-
van et al., 2009).
This task has elements of both information extrac-
tion and phrase-based summarization — the phrases
we wish to extract are broader in scope than in stan-
dard template-driven IE, but at the same time, the
type of selected information is restricted to the de-
fined aspects, similar to query-based summarization.
The difficulty here is that phrase selection is highly
context-dependent. For instance, in TV reviews such
as in Figure 3b, the highlighted phrase “easy to read”
might refer to either the menu or the remote; broader
</bodyText>
<footnote confidence="0.722274">
5http://dvd.ign.com/index/reviews.html
</footnote>
<bodyText confidence="0.997506166666667">
context is required for correct labeling.
We evaluated our approach for this task on two
data sets: Amazon TV reviews (Figure 3b) and Yelp
restaurant reviews (Figure 3c). To eliminate noisy
reviews, we only retain documents that have been
rated “helpful” by the users of the site; we also re-
move reviews which are abnormally short or long.
Each data set was manually annotated with aspect
labels using Mechanical Turk, which has been used
in previous work to annotate NLP data (Snow et al.,
2008). Since we cannot select high-quality annota-
tors directly, we included a control document which
had been previously annotated by a native speaker
among the documents assigned to each annotator.
The work of any annotator who exhibited low agree-
ment on the control document annotation was ex-
cluded from the corpus. To test task annotation
agreement, we use Cohen’s Kappa (Cohen, 1960).
On the Amazon data set, two native speakers anno-
tated a set of four documents. The agreement be-
tween the judges was 0.54. On the Yelp data set, we
simply computed the agreement between all pairs of
reviewers who received the same control documents;
the agreement was 0.49.
</bodyText>
<subsectionHeader confidence="0.999422">
4.2 Baseline Comparison and Evaluation
</subsectionHeader>
<bodyText confidence="0.9995819">
Baselines For all the models, we obtain a baseline
system by eliminating content features and only us-
ing a task model with the set of features described
above. We also compare against a simplified vari-
ant of our method wherein a content model is in-
duced in isolation rather than learned jointly in the
context of the underlying task. In our experiments,
we refer to the two methods as the No Content
Model (NoCM) and Independent Content Model
(IndepCM) settings, respectively. The Joint Content
</bodyText>
<page confidence="0.98991">
383
</page>
<bodyText confidence="0.671221">
M This collection certainly offers some nostalgic
fun, but at the end of the day, the shows themselves,
for the most part, just don&apos;t hold up. (5)
V Regardless, this is a fairly solid presentation, but
it&apos;s obvious there was room for improvement. (7)
A Bass is still robust and powerful. Fans should be
pleased with this presentation. (8)
E The deleted scenes were quite lengthy, but only
shelled out a few extra laughs. (4)
</bodyText>
<figure confidence="0.900022">
(a) Sample labeled text from the multi-aspect sentiment corpus
</figure>
<figureCaption confidence="0.844665">
[R Big multifunction remote] with [R easy-to-
read keys]. The on-screen menu is [M easy to
use] and you [M can rename the inputs] to one
of several options (DVD, Cable, etc.).
[I Plenty of inputs], including [I 2 HDMI ports],
which is [E unheard of in this price range].
I bought this TV because the [V overall picture
quality is good] and it&apos;s [A unbelievably thin].
</figureCaption>
<figure confidence="0.936620083333333">
(b) Sample labeled text from the Amazon multi-aspect summa-
rization corpus
[F All the ingredients are fresh], [V the sizes are =
huge] and [V the price is cheap]. =
=
=
=
[A The place is a pretty good size] and
[S the staff is super friendly].
[O This place rocks!] [V Pricey, but worth it] .
(c) Sample labeled text from the Yelp multi-aspect summarization
corpus
</figure>
<figureCaption confidence="0.860627">
Figure 3: Excerpts from the three corpora with the
corresponding labels. Note that sentences from the
multi-aspect summarization corpora generally focus
</figureCaption>
<bodyText confidence="0.992429333333333">
on only one or two aspects. The multi-aspect senti-
ment corpus has labels per paragraph rather than per
sentence.
Model (JointCM) setting refers to our full model de-
scribed in Section 3, where content and task compo-
nents are learned jointly.
Evaluation Metrics For multi-aspect sentiment
ranking, we report the average L2 (squared differ-
ence) and L1 (absolute difference) between system
prediction and true 1-10 sentiment rating across test
documents and aspects.
For the multi-aspect summarization task, we mea-
sure average token precision and recall of the label
assignments (Multi-label). For the Amazon corpus,
we also report a coarser metric which measures ex-
traction precision and recall while ignoring labels
(Binary labels) as well as ROUGE (Lin, 2004). To
compute ROUGE, we control for length by limiting
</bodyText>
<table confidence="0.933412">
L1 L2
NoCM 1.37 3.15
IndepCM 1.28†* 2.80†*
JointCM 1.25† 2.65†*
Gold 1.18†* 2.48†*
</table>
<tableCaption confidence="0.8778098">
Table 3: The error rate on the multi-aspect sentiment
ranking. We report mean L1 and L2 between system
prediction and true values over all aspects. Marked
results are statistically significant with p &lt; 0.05: *
over the previous model and † over NoCM.
</tableCaption>
<table confidence="0.99972825">
F1 F2 Prec. Recall
NoCM 28.8% 34.8% 22.4% 40.3%
IndepCM 37.9% 43.7% 31.1%†* 48.6%†*
JointCM 39.2% 44.4% 32.9%†* 48.6%†
</table>
<tableCaption confidence="0.969832">
Table 4: Results for multi-aspect summarization on
</tableCaption>
<bodyText confidence="0.980900769230769">
the Yelp corpus. Marked precision and recall are
statistically significant with p &lt; 0.05: * over the
previous model and † over NoCM.
each system to predict the same number of tokens as
the original labeled document.
Our metrics of statistical significance vary by
task. For the sentiment task, we use Student’s t-
test. For the multi-aspect summarization task, we
perform chi-square analysis on the ROUGE scores
as well as on precision and recall separately, as
is commonly done in information extraction (Fre-
itag, 2004; Weeds et al., 2004; Finkel and Manning,
2009).
</bodyText>
<sectionHeader confidence="0.999862" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.87217932">
In this section, we present the results of the methods
on the tasks described above (see Tables 3, 4, and 5).
Baseline Comparisons Adding a content model
significantly outperforms the NoCM baseline on
both tasks. The highest F1 error reduction – 14.7%
– is achieved on multi-aspect summarization on the
Yelp corpus, followed by the reduction of 11.5% and
8.75%, on multi-aspect summarization on the Ama-
zon corpus and multi-aspect sentiment ranking, re-
spectively.
We also observe a consistent performance boost
when comparing against the IndepCM baseline.
This result confirms our hypothesis about the ad-
=o ie
= i eo
= io
= x
= emo e
= en
= np
= onom
= i eo
= o n
= ppe n e
= e e
</bodyText>
<figure confidence="0.951248">
oo
mop e e
l e
e i e
e ll
</figure>
<page confidence="0.990191">
384
</page>
<table confidence="0.9988796">
Multi-label Binary labels
F1 F2 Prec. Recall F1 F2 Prec. Recall ROUGE
NoCM 18.9% 18.0% 20.4% 17.5% 35.1% 33.6% 38.1% 32.6% 43.8%
IndepCM 24.5% 23.8% 25.8%†* 23.3%†* 43.0% 41.8% 45.3%†* 40.9%†* 47.4%†*
JointCM 28.2% 31.3% 24.3%† 33.7%†* 47.8% 53.0% 41.2%† 57.1%†* 47.6%†*
</table>
<tableCaption confidence="0.973228">
Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and
recall are statistically significant with p &lt; 0.05: * over the previous model and † over NoCM.
</tableCaption>
<bodyText confidence="0.971257289473684">
vantages of jointly learning the content model in the
context of the underlying task.
Comparison with additional context features
One alternative to an explicit content model is to
simply incorporate additional features into NoCM
as a proxy for contextual information. In the
multi-aspect summarization case, this can be accom-
plished by adding unigram features from the sen-
tences before and after the current one.6
When testing this approach, however, the perfor-
mance of NoCM actually decreases on both Ama-
zon (to 15.0% F1) and Yelp (to 24.5% F1) corpora.
This result is not surprising for this particular task –
by adding these features, we substantially increase
the feature space without increasing the amount of
training data. An advantage of our approach is
that our learned representation of context is coarse,
and we can leverage large quantities of unannotated
training data.
Impact of content model quality on task per-
formance In the multi-aspect sentiment ranking
task, we have access to gold standard document-
level content structure annotation. This affords us
the ability to compare the ideal content structure,
provided by the document authors, with one that is
learned automatically. As Table 3 shows, the manu-
ally created document structure segmentation yields
the best results. However, the performance of our
JointCM model is not far behind the gold standard
content structure.
The quality of the induced content model is de-
termined by the amount of training data. As Fig-
ure 4 shows, the multi-aspect summarizer improves
with the increase in the size of raw data available for
learning content model.
6This type of feature is not applicable to our multi-aspect
sentiment ranking task, as we already use unigram features from
the entire document.
</bodyText>
<figure confidence="0.799577">
0% 50% 100%
Percentage of unlabeled data
</figure>
<figureCaption confidence="0.947306">
Figure 4: Results on the Amazon corpus using the
</figureCaption>
<bodyText confidence="0.932908">
complete annotated set with varying amounts of ad-
ditional unlabeled data.7
Compensating for annotation sparsity We hy-
pothesize that by incorporating rich contextual in-
formation, we can reduce the need for manual task
annotation. We test this by reducing the amount of
annotated data available to the model and measur-
ing performance at several quantities of unannotated
data. As Figure 5 shows, the performance increase
achieved by doubling the amount of annotated data
can also be achieved by adding only 12.5% of the
unlabeled data.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999939428571429">
In this paper, we demonstrate the benefits of incor-
porating content models in text analysis tasks. We
also introduce a framework to allow the joint learn-
ing of an unsupervised latent content model with a
supervised task-specific model. On multiple tasks
and datasets, our results empirically connect model
quality and task performance, suggesting that fur-
</bodyText>
<footnote confidence="0.588286333333333">
7Because we append the unlabeled versions of the labeled
data to the unlabeled set, even with 0% additional unlabeled
data, there is a small data set to train the content model.
</footnote>
<figure confidence="0.943906666666667">
22.8
26.0
28.2
30
Multi-label F1
20
10
385
25
15.1
0% 12.5% 25%
Percentage of unlabeled data
</figure>
<figureCaption confidence="0.667561">
Figure 5: Results on the Amazon corpus using half
of the annotated training documents. The content
model is trained with 0%, 12.5%, and 25% of addi-
tional unlabeled data.7 The dashed horizontal line
represents NoCM with the complete annotated set.
</figureCaption>
<bodyText confidence="0.9397605">
ther improvements in content modeling may yield
even further gains.
</bodyText>
<sectionHeader confidence="0.999051" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99997725">
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168) and NIH (grant 5-
R01-LM009723-02). Thanks to Peter Szolovits and
the MIT NLP group for their helpful comments.
Any opinions, findings, conclusions, or recommen-
dations expressed in this paper are those of the au-
thors, and do not necessarily reflect the views of the
funding organizations.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99877025">
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the NAACL/HLT, pages 113–120.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning (Information Science and Statis-
tics). Springer-Verlag New York, Inc.
David M. Blei and Jon D. McAullife. 2007. Supervised
Topic Models. In NIPS.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2009. Learning document-level se-
mantic properties from free-text annotations. JAIR,
34:569–603.
Harr Chen, S. R. K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Content modeling using la-
tent permutations. JAIR, 36:129–163.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for sub-
sentential sentiment analysis. In Proceedings of the
EMNLP, pages 793–801.
J. Cohen. 1960. A Coefficient of Agreement for Nominal
Scales. Educational and Psychological Measurement,
20(1):37.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of the ACL/IJCNLP, pages 567–575.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for discourse
coherence. In Proceedings of the NAACL/HLT, pages
436–443.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of the NAACL.
Dayne Freitag. 2004. Trained named entity recogni-
tion using distributional clusters. In Proceedings of
the EMNLP, pages 262–269.
Andrew B. Goldberg and Xiaojin Zhu. 2006. See-
ing stars when there aren’t many stars: Graph-based
semi-supervised learning for sentiment categoriza-
tion. In Proceedings of the NAACL/HLT Workshop on
TextGraphs, pages 45–52.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573–605.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of the NAACL/HLT, pages 362–370.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of the COLING/ACL, pages 483–490.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Proceedings of the ACL,
pages 74–81.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-
cedal. 1989. On the limited memory bfgs method for
large scale optimization. Mathematical Programming,
45:503–528.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
the ACL, pages 432–439.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the ACL,
pages 271–278.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL,
pages 115–124.
</reference>
<figure confidence="0.984002857142857">
20
15
Multi-label F1
10
18.9
20.8
22.6
</figure>
<page confidence="0.989452">
386
</page>
<reference confidence="0.992361478260869">
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of the
EMNLP/CoNLL, pages 717–727.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast - but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the EMNLP.
Benjamin Snyder and Regina Barzilay. 2007. Multiple
aspect ranking using the good grief algorithm. In Pro-
ceedings of the NAACL/HLT, pages 300–307.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
the EMNLP, pages 170–179.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional simi-
larity. In Proceedings of the COLING, page 1015.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? finding strong and weak opin-
ion clauses. In Proceedings of the AAAI, pages 761–
769.
</reference>
<page confidence="0.998324">
387
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.375536">
<title confidence="0.999669">Incorporating Content Structure into Text Analysis Applications</title>
<author confidence="0.99912">Christina Sauper</author>
<author confidence="0.99912">Aria Haghighi</author>
<author confidence="0.99912">Regina</author>
<affiliation confidence="0.986025">Computer Science and Artificial Intelligence Massachusetts Institute of</affiliation>
<email confidence="0.590735">aria42,</email>
<abstract confidence="0.979309647058824">In this paper, we investigate how modeling content structure can benefit text analysis applications such as extractive summarization and sentiment analysis. This follows the linguistic intuition that rich contextual information should be useful in these tasks. We present a framework which combines a supervised text analysis application with the induction of latent content structure. Both of these elements are learned jointly using the EM algorithm. The induced content structure is learned from a large unannotated corpus and biased by the underlying text analysis task. We demonstrate that exploiting content structure yields significant improvements over that rely only on local</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the NAACL/HLT,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="4922" citStr="Barzilay and Lee, 2004" startWordPosition="742" endWordPosition="745">the aspect-specific sentiment ratings (Snyder and Barzilay, 2007). Second, we consider a multi-aspect extractive summarization task in which a system extracts key properties for a pre-specified set of aspects. On both tasks, our method for incorporating content structure consistently outperforms structureagnostic counterparts. Moreover, jointly learning content and task parameters yields additional gains over independently learned models. 2 Related Work Prior research has demonstrated the usefulness of content models for discourse-level tasks. Examples of such tasks include sentence ordering (Barzilay and Lee, 2004; Elsner et al., 2007), extraction-based summarization (Haghighi and Vanderwende, 2009) and text segmentation (Chen et al., 2009). Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully. In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations. Our goal is to augment these models with document-level content information. Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang </context>
<context position="9374" citStr="Barzilay and Lee (2004)" startWordPosition="1441" endWordPosition="1444">we have a collection of labeled documents DL, each consisting of the document text s and true task-specific labeling y*. For the multiaspect summarization task, y* consists of sequence labels (e.g., value or service) for the tokens of a document. Specifically, the document text s is composed of sentences s1, ... , sn and the labelings y* consists of corresponding label sequences y1,...,yn.3 As is common in related work, we model each yi using a CRF which conditions on the observed document text. In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004). In this content model, each sentence si is associated with a hidden topic variable Ti which generates the words of the sentence. We will use T = (T1, ... , Tn) to refer to the hidden topic sequence for a document. We fix the number of topics to a pre-specified constant K. 3.2 Model Overview Our model, depicted in Figure 1, proceeds as follows: First the document-level HMM generates a hidden content topic sequence T for the sentences of a document. This content component is parametrized by θ and decomposes in the standard 2In Section 3.6, we discuss how this framework can be used for other te</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of the NAACL/HLT, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning (Information Science and Statistics).</booktitle>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc.</location>
<contexts>
<context position="19546" citStr="Bishop, 2006" startWordPosition="3191" endWordPosition="3192"> still decomposes as in Figure 2, but the details of learning are slightly different. For instance, because the task label (aspect sentiment ratings) is not localized to any region of the document, all content model variables influence the target response. Conditioned on the target label, all P(T |y, s) oc P(s, T)P(y|T, s) For the case of our multi-aspect sentiment task, this computation can be done exactly by enumerating T tuples, since the number of sentences and possible topics is relatively small. If summation is intractable, the posterior may be approximated using variational techniques (Bishop, 2006), which is applicable to a broad range of potential applications. 4 Experimental Set-Up We apply our approach to two text analysis tasks that stand to benefit from modeling content structure: multi-aspect sentiment analysis and multi-aspect review summarization. 4.1 Tasks In the following section, we define each task in detail, explain the task-specific adaptation of the model and describe the data sets used in the experiments. Table 2 summarizes statistics for all the data sets. For all tasks, when using a content model with a task model, we utilize a new set of features which include all the</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Jon D McAullife</author>
</authors>
<title>Supervised Topic Models.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="8304" citStr="Blei and McAullife (2007)" startWordPosition="1260" endWordPosition="1263">ions are readily available for only a few applications and are prohibitively expensive to obtain for many others. To overcome this drawback, our method induces a content model in an unsupervised fashion and connects it via latent variables to the target model. This design not only eliminates the need for additional annotations, but also allows the algorithm to leverage large quantities of raw data for training the content model. The tight coupling of rel378 evance learning with the target analysis task leads to further performance gains. Finally, our work relates to supervised topic models in Blei and McAullife (2007). In this work, latent topic variables are used to generate text as well as a supervised sentiment rating for the document. However, this architecture does not permit the usage of standard discriminative models which condition freely on textual features. 3 Model 3.1 Problem Formulation In this section, we describe a model which incorporates content information into a multi-aspect summarization task.2 Our approach assumes that at training time we have a collection of labeled documents DL, each consisting of the document text s and true task-specific labeling y*. For the multiaspect summarizatio</context>
</contexts>
<marker>Blei, McAullife, 2007</marker>
<rawString>David M. Blei and Jon D. McAullife. 2007. Supervised Topic Models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning document-level semantic properties from free-text annotations.</title>
<date>2009</date>
<journal>JAIR,</journal>
<pages>34--569</pages>
<contexts>
<context position="22826" citStr="Branavan et al., 2009" startWordPosition="3728" endWordPosition="3732">ng automatically induced content models against the gold standard structural information. Multi-Aspect Review Summarization The goal of this task is to extract informative phrases that identify information relevant to several predefined aspects of interest. In other words, we would like our system to both extract important phrases (e.g., cheap food) and label it with one of the given aspects (e.g., value). For concrete examples and lists of aspects for each data set, see Figures 3b and 3c. Variants of this task have been considered in review summarization in previous work (Kim and Hovy, 2006; Branavan et al., 2009). This task has elements of both information extraction and phrase-based summarization — the phrases we wish to extract are broader in scope than in standard template-driven IE, but at the same time, the type of selected information is restricted to the defined aspects, similar to query-based summarization. The difficulty here is that phrase selection is highly context-dependent. For instance, in TV reviews such as in Figure 3b, the highlighted phrase “easy to read” might refer to either the menu or the remote; broader 5http://dvd.ign.com/index/reviews.html context is required for correct labe</context>
</contexts>
<marker>Branavan, Chen, Eisenstein, Barzilay, 2009</marker>
<rawString>S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and Regina Barzilay. 2009. Learning document-level semantic properties from free-text annotations. JAIR, 34:569–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harr Chen</author>
<author>S R K Branavan</author>
<author>Regina Barzilay</author>
<author>David R Karger</author>
</authors>
<title>Content modeling using latent permutations.</title>
<date>2009</date>
<journal>JAIR,</journal>
<pages>36--129</pages>
<contexts>
<context position="5051" citStr="Chen et al., 2009" startWordPosition="759" endWordPosition="762">n which a system extracts key properties for a pre-specified set of aspects. On both tasks, our method for incorporating content structure consistently outperforms structureagnostic counterparts. Moreover, jointly learning content and task parameters yields additional gains over independently learned models. 2 Related Work Prior research has demonstrated the usefulness of content models for discourse-level tasks. Examples of such tasks include sentence ordering (Barzilay and Lee, 2004; Elsner et al., 2007), extraction-based summarization (Haghighi and Vanderwende, 2009) and text segmentation (Chen et al., 2009). Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully. In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations. Our goal is to augment these models with document-level content information. Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang and Lee, 2004; Patwardhan and Riloff, 2007; McDonald et al., 2007). These approaches consider global contextual information when </context>
</contexts>
<marker>Chen, Branavan, Barzilay, Karger, 2009</marker>
<rawString>Harr Chen, S. R. K. Branavan, Regina Barzilay, and David R. Karger. 2009. Content modeling using latent permutations. JAIR, 36:129–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with compositional semantics as structural inference for subsentential sentiment analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>793--801</pages>
<contexts>
<context position="6337" citStr="Choi and Cardie (2008)" startWordPosition="954" endWordPosition="957">ng analysis task. All assume that relevant sentences have been annotated. For instance, Pang and Lee (2004) refine the accuracy of sentiment analysis by considering only the subjective sentences of a review as determined by an independent classifier. Patwardhan and Riloff (2007) take a similar approach in the context of information extraction. Rather than applying their extractor to all the sentences in a document, they limit it to eventrelevant sentences. Since these sentences are more likely to contain information of interest, the extraction performance increases. Another approach, taken by Choi and Cardie (2008) and Somasundaran et al. (2009) uses linguistic resources to create a latent model in a taskspecific fashion to improve performance, rather than assuming sentence-level task relevancy. Choi and Cardie (2008) address a sentiment analysis task by using a heuristic decision process based on wordlevel intermediate variables to represent polarity. Somasundaran et al. (2009) similarly uses a bootstrapped local polarity classifier to identify sentence polarity. McDonald et al. (2007) propose a model which jointly identifies global polarity as well as paragraph- and sentence-level polarity, all of whi</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Yejin Choi and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsentential sentiment analysis. In Proceedings of the EMNLP, pages 793–801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement,</title>
<date>1960</date>
<contexts>
<context position="24268" citStr="Cohen, 1960" startWordPosition="3964" endWordPosition="3965">s of the site; we also remove reviews which are abnormally short or long. Each data set was manually annotated with aspect labels using Mechanical Turk, which has been used in previous work to annotate NLP data (Snow et al., 2008). Since we cannot select high-quality annotators directly, we included a control document which had been previously annotated by a native speaker among the documents assigned to each annotator. The work of any annotator who exhibited low agreement on the control document annotation was excluded from the corpus. To test task annotation agreement, we use Cohen’s Kappa (Cohen, 1960). On the Amazon data set, two native speakers annotated a set of four documents. The agreement between the judges was 0.54. On the Yelp data set, we simply computed the agreement between all pairs of reviewers who received the same control documents; the agreement was 0.49. 4.2 Baseline Comparison and Evaluation Baselines For all the models, we obtain a baseline system by eliminating content features and only using a task model with the set of features described above. We also compare against a simplified variant of our method wherein a content model is induced in isolation rather than learned</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Fast consensus decoding over translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL/IJCNLP,</booktitle>
<pages>567--575</pages>
<contexts>
<context position="16616" citStr="DeNero et al., 2009" startWordPosition="2695" endWordPosition="2698">ts a user-defined trade-off between precision and recall errors. For our multi-aspect summarization task, we select c = 4 for Yelp and c = 5 for Amazon to combat the high-precision bias typical of conditional likelihood models. At inference time, we select the single labeling which minimizes the expected loss with respect to model posterior over label sequences: In our case, we must marginalize out the sentence topic T: P(yj|s) = X P(yj, T |s) T X= Pθ(T|s)Pφ(yj|s, T) T This minimum risk criterion has been widely used in NLP applications such as parsing (Goodman, 1999) and machine translation (DeNero et al., 2009). Note that the above formulation differs from the standard CRF due to the latent topic variables. Otherwise the inference task could be accomplished by directly obtaining posteriors over each yj state using the Forward-Backwards algorithm on the sentence CRF. Finding y� can be done efficiently. First, we obtain marginal token posteriors as above. Then, the expected loss of a token prediction is computed as follows: P(yj|s)L(yj, yj) Once we obtain expected losses of each token prediction, we compute the minimum risk sequence labeling by running the Viterbi algorithm. The potential for each pos</context>
</contexts>
<marker>DeNero, Chiang, Knight, 2009</marker>
<rawString>John DeNero, David Chiang, and Kevin Knight. 2009. Fast consensus decoding over translation forests. In Proceedings of the ACL/IJCNLP, pages 567–575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Joseph Austerweil</author>
<author>Eugene Charniak</author>
</authors>
<title>A unified local and global model for discourse coherence.</title>
<date>2007</date>
<booktitle>In Proceedings of the NAACL/HLT,</booktitle>
<pages>436--443</pages>
<contexts>
<context position="4944" citStr="Elsner et al., 2007" startWordPosition="746" endWordPosition="749">iment ratings (Snyder and Barzilay, 2007). Second, we consider a multi-aspect extractive summarization task in which a system extracts key properties for a pre-specified set of aspects. On both tasks, our method for incorporating content structure consistently outperforms structureagnostic counterparts. Moreover, jointly learning content and task parameters yields additional gains over independently learned models. 2 Related Work Prior research has demonstrated the usefulness of content models for discourse-level tasks. Examples of such tasks include sentence ordering (Barzilay and Lee, 2004; Elsner et al., 2007), extraction-based summarization (Haghighi and Vanderwende, 2009) and text segmentation (Chen et al., 2009). Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully. In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations. Our goal is to augment these models with document-level content information. Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang and Lee, 2004; Patward</context>
</contexts>
<marker>Elsner, Austerweil, Charniak, 2007</marker>
<rawString>Micha Elsner, Joseph Austerweil, and Eugene Charniak. 2007. A unified local and global model for discourse coherence. In Proceedings of the NAACL/HLT, pages 436–443.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint parsing and named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL.</booktitle>
<contexts>
<context position="28332" citStr="Finkel and Manning, 2009" startWordPosition="4643" endWordPosition="4646">†* 48.6%† Table 4: Results for multi-aspect summarization on the Yelp corpus. Marked precision and recall are statistically significant with p &lt; 0.05: * over the previous model and † over NoCM. each system to predict the same number of tokens as the original labeled document. Our metrics of statistical significance vary by task. For the sentiment task, we use Student’s ttest. For the multi-aspect summarization task, we perform chi-square analysis on the ROUGE scores as well as on precision and recall separately, as is commonly done in information extraction (Freitag, 2004; Weeds et al., 2004; Finkel and Manning, 2009). 5 Results In this section, we present the results of the methods on the tasks described above (see Tables 3, 4, and 5). Baseline Comparisons Adding a content model significantly outperforms the NoCM baseline on both tasks. The highest F1 error reduction – 14.7% – is achieved on multi-aspect summarization on the Yelp corpus, followed by the reduction of 11.5% and 8.75%, on multi-aspect summarization on the Amazon corpus and multi-aspect sentiment ranking, respectively. We also observe a consistent performance boost when comparing against the IndepCM baseline. This result confirms our hypothes</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Joint parsing and named entity recognition. In Proceedings of the NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Trained named entity recognition using distributional clusters.</title>
<date>2004</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>262--269</pages>
<contexts>
<context position="28285" citStr="Freitag, 2004" startWordPosition="4636" endWordPosition="4638">* 48.6%†* JointCM 39.2% 44.4% 32.9%†* 48.6%† Table 4: Results for multi-aspect summarization on the Yelp corpus. Marked precision and recall are statistically significant with p &lt; 0.05: * over the previous model and † over NoCM. each system to predict the same number of tokens as the original labeled document. Our metrics of statistical significance vary by task. For the sentiment task, we use Student’s ttest. For the multi-aspect summarization task, we perform chi-square analysis on the ROUGE scores as well as on precision and recall separately, as is commonly done in information extraction (Freitag, 2004; Weeds et al., 2004; Finkel and Manning, 2009). 5 Results In this section, we present the results of the methods on the tasks described above (see Tables 3, 4, and 5). Baseline Comparisons Adding a content model significantly outperforms the NoCM baseline on both tasks. The highest F1 error reduction – 14.7% – is achieved on multi-aspect summarization on the Yelp corpus, followed by the reduction of 11.5% and 8.75%, on multi-aspect summarization on the Amazon corpus and multi-aspect sentiment ranking, respectively. We also observe a consistent performance boost when comparing against the Inde</context>
</contexts>
<marker>Freitag, 2004</marker>
<rawString>Dayne Freitag. 2004. Trained named entity recognition using distributional clusters. In Proceedings of the EMNLP, pages 262–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew B Goldberg</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Seeing stars when there aren’t many stars: Graph-based semi-supervised learning for sentiment categorization.</title>
<date>2006</date>
<booktitle>In Proceedings of the NAACL/HLT Workshop on TextGraphs,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="20911" citStr="Goldberg and Zhu, 2006" startWordPosition="3412" endWordPosition="3415">h indicates whether a given word was most likely emitted from the underlying topic or from a background distribution. Multi-Aspect Sentiment Ranking The goal of multi-aspect sentiment classification is to predict a set of numeric ranks that reflects the user satisfaction for each aspect (Snyder and Barzilay, 2007). One of the challenges in this task is to attribute sentimentbearing words to the aspects they describe. Information about document structure has the potential to greatly reduce this ambiguity. Following standard sentiment ranking approaches (Wilson et al., 2004; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007), we employ ordinary linear regression to independently map bag-of-words representations into predicted aspect ranks. In addition to commonly used lexical features, this set is augmented log P0(s) � P0(s,T) log T 382 Task Labeled Unlabeled Avg. Size Train Test Words Sents Multi-aspect sentiment 600 65 — 1,027 20.5 Multi-aspect summarization Amazon 35 24 12,684 214 11.7 Yelp 48 48 33,015 178 11.2 Table 2: This table summarizes the size of each corpus. In each case, the unlabeled texts of both labeled and unlabeled documents are used for training the content model, wh</context>
</contexts>
<marker>Goldberg, Zhu, 2006</marker>
<rawString>Andrew B. Goldberg and Xiaojin Zhu. 2006. Seeing stars when there aren’t many stars: Graph-based semi-supervised learning for sentiment categorization. In Proceedings of the NAACL/HLT Workshop on TextGraphs, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="16570" citStr="Goodman, 1999" startWordPosition="2690" endWordPosition="2691">els of interest. The constant c represents a user-defined trade-off between precision and recall errors. For our multi-aspect summarization task, we select c = 4 for Yelp and c = 5 for Amazon to combat the high-precision bias typical of conditional likelihood models. At inference time, we select the single labeling which minimizes the expected loss with respect to model posterior over label sequences: In our case, we must marginalize out the sentence topic T: P(yj|s) = X P(yj, T |s) T X= Pθ(T|s)Pφ(yj|s, T) T This minimum risk criterion has been widely used in NLP applications such as parsing (Goodman, 1999) and machine translation (DeNero et al., 2009). Note that the above formulation differs from the standard CRF due to the latent topic variables. Otherwise the inference task could be accomplished by directly obtaining posteriors over each yj state using the Forward-Backwards algorithm on the sentence CRF. Finding y� can be done efficiently. First, we obtain marginal token posteriors as above. Then, the expected loss of a token prediction is computed as follows: P(yj|s)L(yj, yj) Once we obtain expected losses of each token prediction, we compute the minimum risk sequence labeling by running the</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Joshua Goodman. 1999. Semiring parsing. Computational Linguistics, 25(4):573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL/HLT,</booktitle>
<pages>362--370</pages>
<contexts>
<context position="5009" citStr="Haghighi and Vanderwende, 2009" startWordPosition="752" endWordPosition="755">consider a multi-aspect extractive summarization task in which a system extracts key properties for a pre-specified set of aspects. On both tasks, our method for incorporating content structure consistently outperforms structureagnostic counterparts. Moreover, jointly learning content and task parameters yields additional gains over independently learned models. 2 Related Work Prior research has demonstrated the usefulness of content models for discourse-level tasks. Examples of such tasks include sentence ordering (Barzilay and Lee, 2004; Elsner et al., 2007), extraction-based summarization (Haghighi and Vanderwende, 2009) and text segmentation (Chen et al., 2009). Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully. In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations. Our goal is to augment these models with document-level content information. Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang and Lee, 2004; Patwardhan and Riloff, 2007; McDonald et al., 2007). These approaches co</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of the NAACL/HLT, pages 362–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic identification of pro and con reasons in online reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL,</booktitle>
<pages>483--490</pages>
<contexts>
<context position="22802" citStr="Kim and Hovy, 2006" startWordPosition="3724" endWordPosition="3727">of the algorithm using automatically induced content models against the gold standard structural information. Multi-Aspect Review Summarization The goal of this task is to extract informative phrases that identify information relevant to several predefined aspects of interest. In other words, we would like our system to both extract important phrases (e.g., cheap food) and label it with one of the given aspects (e.g., value). For concrete examples and lists of aspects for each data set, see Figures 3b and 3c. Variants of this task have been considered in review summarization in previous work (Kim and Hovy, 2006; Branavan et al., 2009). This task has elements of both information extraction and phrase-based summarization — the phrases we wish to extract are broader in scope than in standard template-driven IE, but at the same time, the type of selected information is restricted to the defined aspects, similar to query-based summarization. The difficulty here is that phrase selection is highly context-dependent. For instance, in TV reviews such as in Figure 3b, the highlighted phrase “easy to read” might refer to either the menu or the remote; broader 5http://dvd.ign.com/index/reviews.html context is r</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Automatic identification of pro and con reasons in online reviews. In Proceedings of the COLING/ACL, pages 483–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="27210" citStr="Lin, 2004" startWordPosition="4460" endWordPosition="4461">ur full model described in Section 3, where content and task components are learned jointly. Evaluation Metrics For multi-aspect sentiment ranking, we report the average L2 (squared difference) and L1 (absolute difference) between system prediction and true 1-10 sentiment rating across test documents and aspects. For the multi-aspect summarization task, we measure average token precision and recall of the label assignments (Multi-label). For the Amazon corpus, we also report a coarser metric which measures extraction precision and recall while ignoring labels (Binary labels) as well as ROUGE (Lin, 2004). To compute ROUGE, we control for length by limiting L1 L2 NoCM 1.37 3.15 IndepCM 1.28†* 2.80†* JointCM 1.25† 2.65†* Gold 1.18†* 2.48†* Table 3: The error rate on the multi-aspect sentiment ranking. We report mean L1 and L2 between system prediction and true values over all aspects. Marked results are statistically significant with p &lt; 0.05: * over the previous model and † over NoCM. F1 F2 Prec. Recall NoCM 28.8% 34.8% 22.4% 40.3% IndepCM 37.9% 43.7% 31.1%†* 48.6%†* JointCM 39.2% 44.4% 32.9%†* 48.6%† Table 4: Results for multi-aspect summarization on the Yelp corpus. Marked precision and reca</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Proceedings of the ACL, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="15404" citStr="Liu et al., 1989" startWordPosition="2477" endWordPosition="2480">jective is equivalent to a weighted version of the conditional likelihood objective used to train the CRF in isolation. An intuitive explanation of this process is that there are multiple CRF instances, one for each possible hidden topic T. Each utilizes different content features to explain the sentence sequence labeling. These instances are weighted according to the posterior over T obtained during the E-Step. While this objective is non-convex due to the summation over T, we can still optimize it using any gradient-based optimization solver; in our experiments, we used the LBFGS algorithm (Liu et al., 1989). 3.4 Inference We must predict a label sequence y for each sentence s of the document. We assume a loss function over a sequence labeling y and a proposed labeling y, which decomposes as: X L(y, y) = j where each position loss is sensitive to the kind of error which is made. Failing to extract a token is penalized to a greater extent than extracting it with an incorrect label: 0 if yj = yj c if yj =6 NONE and yj = NONE 1 otherwise In this definition, NONE represents the background label which is reserved for tokens which do not correspond to labels of interest. The constant c represents a use</context>
</contexts>
<marker>Liu, Nocedal, Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge Nocedal. 1989. On the limited memory bfgs method for large scale optimization. Mathematical Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kerry Hannan</author>
<author>Tyler Neylon</author>
<author>Mike Wells</author>
<author>Jeff Reynar</author>
</authors>
<title>Structured models for fine-to-coarse sentiment analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>432--439</pages>
<contexts>
<context position="5588" citStr="McDonald et al., 2007" startWordPosition="838" endWordPosition="841">marization (Haghighi and Vanderwende, 2009) and text segmentation (Chen et al., 2009). Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully. In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations. Our goal is to augment these models with document-level content information. Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang and Lee, 2004; Patwardhan and Riloff, 2007; McDonald et al., 2007). These approaches consider global contextual information when determining whether a given sentence is relevant to the underlying analysis task. All assume that relevant sentences have been annotated. For instance, Pang and Lee (2004) refine the accuracy of sentiment analysis by considering only the subjective sentences of a review as determined by an independent classifier. Patwardhan and Riloff (2007) take a similar approach in the context of information extraction. Rather than applying their extractor to all the sentences in a document, they limit it to eventrelevant sentences. Since these </context>
<context position="6818" citStr="McDonald et al. (2007)" startWordPosition="1026" endWordPosition="1029">s are more likely to contain information of interest, the extraction performance increases. Another approach, taken by Choi and Cardie (2008) and Somasundaran et al. (2009) uses linguistic resources to create a latent model in a taskspecific fashion to improve performance, rather than assuming sentence-level task relevancy. Choi and Cardie (2008) address a sentiment analysis task by using a heuristic decision process based on wordlevel intermediate variables to represent polarity. Somasundaran et al. (2009) similarly uses a bootstrapped local polarity classifier to identify sentence polarity. McDonald et al. (2007) propose a model which jointly identifies global polarity as well as paragraph- and sentence-level polarity, all of which are observed in training data. While our approach uses a similar hierarchy, McDonald et al. (2007) is concerned with recovering the labels at all levels, whereas in this work we are interested in using latent document content structure as a means to benefit task predictions. While our method also incorporates contextual information into existing text analysis applications, our approach is markedly different from the above approaches. First, our representation of context enc</context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells, and Jeff Reynar. 2007. Structured models for fine-to-coarse sentiment analysis. In Proceedings of the ACL, pages 432–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="2485" citStr="Pang and Lee, 2004" startWordPosition="368" endWordPosition="371">erful, giving weight to just about any scene – most notably the film’s exciting final fight. Fans should be pleased with the presentation. Extras This single-disc DVD comes packed in a black amaray case with a glossy slipcover. Cover art has clearly been designed to appeal the Twilight crowd ... Finally, we’ve got a deleted scenes reel. Most of the excised scenes are actually pretty interesting. Table 1: An excerpt from a DVD review. instance, when performing single-aspect sentiment analysis, the most relevant aspect of content structure is whether a given sentence is objective or subjective (Pang and Lee, 2004). In a multi-aspect setting, however, information about the sentence topic is required to determine the aspect to which a sentiment-bearing word relates (Snyder and Barzilay, 2007). As we can see from even these closely related applications, the content structure representation should be intimately tied to a specific text analysis task. In this work, we present an approach in which a content model is learned jointly with a text analysis task. We assume complete annotations for the task itself, but we learn the content model from raw, unannotated text. Our approach is implemented in a discrimin</context>
<context position="5535" citStr="Pang and Lee, 2004" startWordPosition="830" endWordPosition="833"> 2004; Elsner et al., 2007), extraction-based summarization (Haghighi and Vanderwende, 2009) and text segmentation (Chen et al., 2009). Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully. In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations. Our goal is to augment these models with document-level content information. Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang and Lee, 2004; Patwardhan and Riloff, 2007; McDonald et al., 2007). These approaches consider global contextual information when determining whether a given sentence is relevant to the underlying analysis task. All assume that relevant sentences have been annotated. For instance, Pang and Lee (2004) refine the accuracy of sentiment analysis by considering only the subjective sentences of a review as determined by an independent classifier. Patwardhan and Riloff (2007) take a similar approach in the context of information extraction. Rather than applying their extractor to all the sentences in a document, t</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the ACL, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="20887" citStr="Pang and Lee, 2005" startWordPosition="3408" endWordPosition="3411">clude a feature which indicates whether a given word was most likely emitted from the underlying topic or from a background distribution. Multi-Aspect Sentiment Ranking The goal of multi-aspect sentiment classification is to predict a set of numeric ranks that reflects the user satisfaction for each aspect (Snyder and Barzilay, 2007). One of the challenges in this task is to attribute sentimentbearing words to the aspects they describe. Information about document structure has the potential to greatly reduce this ambiguity. Following standard sentiment ranking approaches (Wilson et al., 2004; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007), we employ ordinary linear regression to independently map bag-of-words representations into predicted aspect ranks. In addition to commonly used lexical features, this set is augmented log P0(s) � P0(s,T) log T 382 Task Labeled Unlabeled Avg. Size Train Test Words Sents Multi-aspect sentiment 600 65 — 1,027 20.5 Multi-aspect summarization Amazon 35 24 12,684 214 11.7 Yelp 48 48 33,015 178 11.2 Table 2: This table summarizes the size of each corpus. In each case, the unlabeled texts of both labeled and unlabeled documents are used for traini</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>Effective information extraction with semantic affinity patterns and relevant regions.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLP/CoNLL,</booktitle>
<pages>717--727</pages>
<contexts>
<context position="5564" citStr="Patwardhan and Riloff, 2007" startWordPosition="834" endWordPosition="837">, 2007), extraction-based summarization (Haghighi and Vanderwende, 2009) and text segmentation (Chen et al., 2009). Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully. In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations. Our goal is to augment these models with document-level content information. Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang and Lee, 2004; Patwardhan and Riloff, 2007; McDonald et al., 2007). These approaches consider global contextual information when determining whether a given sentence is relevant to the underlying analysis task. All assume that relevant sentences have been annotated. For instance, Pang and Lee (2004) refine the accuracy of sentiment analysis by considering only the subjective sentences of a review as determined by an independent classifier. Patwardhan and Riloff (2007) take a similar approach in the context of information extraction. Rather than applying their extractor to all the sentences in a document, they limit it to eventrelevant</context>
</contexts>
<marker>Patwardhan, Riloff, 2007</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2007. Effective information extraction with semantic affinity patterns and relevant regions. In Proceedings of the EMNLP/CoNLL, pages 717–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast - but is it good? evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Multiple aspect ranking using the good grief algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the NAACL/HLT,</booktitle>
<pages>300--307</pages>
<contexts>
<context position="2665" citStr="Snyder and Barzilay, 2007" startWordPosition="394" endWordPosition="398">packed in a black amaray case with a glossy slipcover. Cover art has clearly been designed to appeal the Twilight crowd ... Finally, we’ve got a deleted scenes reel. Most of the excised scenes are actually pretty interesting. Table 1: An excerpt from a DVD review. instance, when performing single-aspect sentiment analysis, the most relevant aspect of content structure is whether a given sentence is objective or subjective (Pang and Lee, 2004). In a multi-aspect setting, however, information about the sentence topic is required to determine the aspect to which a sentiment-bearing word relates (Snyder and Barzilay, 2007). As we can see from even these closely related applications, the content structure representation should be intimately tied to a specific text analysis task. In this work, we present an approach in which a content model is learned jointly with a text analysis task. We assume complete annotations for the task itself, but we learn the content model from raw, unannotated text. Our approach is implemented in a discriminative framework using latent variables to represent facets of content structure. In this framework, the original task features (e.g., lexical ones) are conjoined with latent variab</context>
<context position="4365" citStr="Snyder and Barzilay, 2007" startWordPosition="663" endWordPosition="667">the content model and the taskspecific model allows the two components to mutually influence each other during learning. The content model leverages unannotated data to improve the performance of the task-specific model, while the task-specific model provides feedback to improve the relevance of the content model. The combined model can be learned effectively using a novel EM-based method for joint training. We evaluate our approach on two complementary text analysis tasks. Our first task is a multi-aspect sentiment analysis task, where a system predicts the aspect-specific sentiment ratings (Snyder and Barzilay, 2007). Second, we consider a multi-aspect extractive summarization task in which a system extracts key properties for a pre-specified set of aspects. On both tasks, our method for incorporating content structure consistently outperforms structureagnostic counterparts. Moreover, jointly learning content and task parameters yields additional gains over independently learned models. 2 Related Work Prior research has demonstrated the usefulness of content models for discourse-level tasks. Examples of such tasks include sentence ordering (Barzilay and Lee, 2004; Elsner et al., 2007), extraction-based su</context>
<context position="20604" citStr="Snyder and Barzilay, 2007" startWordPosition="3363" endWordPosition="3366">Table 2 summarizes statistics for all the data sets. For all tasks, when using a content model with a task model, we utilize a new set of features which include all the original features as well as a copy of each feature conjoined with the content topic assignment (see Figure 1). We also include a feature which indicates whether a given word was most likely emitted from the underlying topic or from a background distribution. Multi-Aspect Sentiment Ranking The goal of multi-aspect sentiment classification is to predict a set of numeric ranks that reflects the user satisfaction for each aspect (Snyder and Barzilay, 2007). One of the challenges in this task is to attribute sentimentbearing words to the aspects they describe. Information about document structure has the potential to greatly reduce this ambiguity. Following standard sentiment ranking approaches (Wilson et al., 2004; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007), we employ ordinary linear regression to independently map bag-of-words representations into predicted aspect ranks. In addition to commonly used lexical features, this set is augmented log P0(s) � P0(s,T) log T 382 Task Labeled Unlabeled Avg. Size Train Test Word</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2007. Multiple aspect ranking using the good grief algorithm. In Proceedings of the NAACL/HLT, pages 300–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Galileo Namata</author>
<author>Janyce Wiebe</author>
<author>Lise Getoor</author>
</authors>
<title>Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>170--179</pages>
<contexts>
<context position="6368" citStr="Somasundaran et al. (2009)" startWordPosition="959" endWordPosition="962">e that relevant sentences have been annotated. For instance, Pang and Lee (2004) refine the accuracy of sentiment analysis by considering only the subjective sentences of a review as determined by an independent classifier. Patwardhan and Riloff (2007) take a similar approach in the context of information extraction. Rather than applying their extractor to all the sentences in a document, they limit it to eventrelevant sentences. Since these sentences are more likely to contain information of interest, the extraction performance increases. Another approach, taken by Choi and Cardie (2008) and Somasundaran et al. (2009) uses linguistic resources to create a latent model in a taskspecific fashion to improve performance, rather than assuming sentence-level task relevancy. Choi and Cardie (2008) address a sentiment analysis task by using a heuristic decision process based on wordlevel intermediate variables to represent polarity. Somasundaran et al. (2009) similarly uses a bootstrapped local polarity classifier to identify sentence polarity. McDonald et al. (2007) propose a model which jointly identifies global polarity as well as paragraph- and sentence-level polarity, all of which are observed in training dat</context>
</contexts>
<marker>Somasundaran, Namata, Wiebe, Getoor, 2009</marker>
<rawString>Swapna Somasundaran, Galileo Namata, Janyce Wiebe, and Lise Getoor. 2009. Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. In Proceedings of the EMNLP, pages 170–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of the COLING,</booktitle>
<pages>1015</pages>
<contexts>
<context position="28305" citStr="Weeds et al., 2004" startWordPosition="4639" endWordPosition="4642">CM 39.2% 44.4% 32.9%†* 48.6%† Table 4: Results for multi-aspect summarization on the Yelp corpus. Marked precision and recall are statistically significant with p &lt; 0.05: * over the previous model and † over NoCM. each system to predict the same number of tokens as the original labeled document. Our metrics of statistical significance vary by task. For the sentiment task, we use Student’s ttest. For the multi-aspect summarization task, we perform chi-square analysis on the ROUGE scores as well as on precision and recall separately, as is commonly done in information extraction (Freitag, 2004; Weeds et al., 2004; Finkel and Manning, 2009). 5 Results In this section, we present the results of the methods on the tasks described above (see Tables 3, 4, and 5). Baseline Comparisons Adding a content model significantly outperforms the NoCM baseline on both tasks. The highest F1 error reduction – 14.7% – is achieved on multi-aspect summarization on the Yelp corpus, followed by the reduction of 11.5% and 8.75%, on multi-aspect summarization on the Amazon corpus and multi-aspect sentiment ranking, respectively. We also observe a consistent performance boost when comparing against the IndepCM baseline. This r</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of the COLING, page 1015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Rebecca Hwa</author>
</authors>
<title>Just how mad are you? finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI,</booktitle>
<pages>761--769</pages>
<contexts>
<context position="20867" citStr="Wilson et al., 2004" startWordPosition="3404" endWordPosition="3407">Figure 1). We also include a feature which indicates whether a given word was most likely emitted from the underlying topic or from a background distribution. Multi-Aspect Sentiment Ranking The goal of multi-aspect sentiment classification is to predict a set of numeric ranks that reflects the user satisfaction for each aspect (Snyder and Barzilay, 2007). One of the challenges in this task is to attribute sentimentbearing words to the aspects they describe. Information about document structure has the potential to greatly reduce this ambiguity. Following standard sentiment ranking approaches (Wilson et al., 2004; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007), we employ ordinary linear regression to independently map bag-of-words representations into predicted aspect ranks. In addition to commonly used lexical features, this set is augmented log P0(s) � P0(s,T) log T 382 Task Labeled Unlabeled Avg. Size Train Test Words Sents Multi-aspect sentiment 600 65 — 1,027 20.5 Multi-aspect summarization Amazon 35 24 12,684 214 11.7 Yelp 48 48 33,015 178 11.2 Table 2: This table summarizes the size of each corpus. In each case, the unlabeled texts of both labeled and unlabeled documents</context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004. Just how mad are you? finding strong and weak opinion clauses. In Proceedings of the AAAI, pages 761– 769.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>