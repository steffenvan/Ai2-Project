<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009713">
<sectionHeader confidence="0.925695333333333" genericHeader="method">
TECHNICAL CORRESPONDENCE
PARSING DISCONTINUOUS CONSTITUENTS IN
DEPENDENCY GRAMMAR
</sectionHeader>
<bodyText confidence="0.998007694444444">
Discontinuous constituents—for example, a noun and its
modifying adjective separated by words unrelated to them--
are common in variable-word-order languages; Figure 1
shows examples. But phrase structure grammars, including
ID/LP grammars, require each constituent to be a contigu-
ous series of words. Insofar as standard parsing algorithms
are based on phrase structure rules, they are inadequate for
parsing such languages.1
The algorithm presented here, however, does not require
constituents to be continuous, but merely prefers them so.
It can therefore parse languages in which conventional
parsing techniques do not work. At the same time, because
of its preference for nearby attachments, it prefers to make
constituents continuous when more than one analysis is
possible. The new algorithm has been used successfully to
parse Russian and Latin (Covington 1988, 1990).
This algorithm uses dependency grammar. That is, in-
stead of breaking the sentence into phrases and subphrases,
it establishes links between individual words. Each link
connects a word (the &amp;quot;head&amp;quot;) with one of its &amp;quot;dependents&amp;quot;
(an argument or modifier). Figure 2 shows how this works.
The arrows point from head to dependent; a head can have
many dependents, but each dependent can have only one
head. Of course the same word can be the head in one link
and the dependent in another.2
Dependency grammar is equivalent to an X-bar theory
with only one phrasal bar level (Figure 3)—the dependents
of a word are the heads of its sisters. Thus dependency
grammar captures the increasingly recognized importance
of headship in syntax. At the same time, the absence of
phrasal nodes from the dependency representation stream-
lines the search process during parsing.
The parser presupposes a grammar that specifies which
words can depend on which. In the prototype, the grammar
consists of unification-based dependency rules (called
D-rules) such as:
</bodyText>
<figure confidence="0.500875333333333">
[category:noun
category:verb
person:X
, person:X
number:Y
case:nominative number:Y
</figure>
<bodyText confidence="0.994285571428571">
This rule sanctions a dependency relation between any two
words whose features unify with the structures shown—in
this case, the verb and its subject in a language such as
Russian or Latin. The arrow means &amp;quot;can depend on&amp;quot; and
the word order is not specified. X and Y are variables.
D-rules take the place of the phrase structure rules used by
Shieber (1986) and others; semantic information can easily
be added to them, and the whole power of unification-based
grammar is available.
The parser accepts words from the input string and keeps
track of whether or not each word is &amp;quot;independent&amp;quot; (not yet
known to depend on another word), indicated by + or — in
Figure 4. On accepting a word W, the parser does the
following:
</bodyText>
<listItem confidence="0.990793285714286">
(1) Search the independent words (those marked +),
most recent first, for words that can depend on W. If any
are found, establish the dependencies and change the mark-
ing of the dependents from + to —.
(2) Search all words so far seen, most recent first, for a
word on which Wean depend. If one is found, establish the
dependency and mark Was —. Otherwise mark Was +.
</listItem>
<bodyText confidence="0.997731166666667">
Figure 4 shows the process in action. The first three
words, ultima Cumaei venit, are accepted without creating
any links. Then the parser accepts lam and makes it depend
on venit. Next the parser accepts carminis, on which Cu-
maei, already in the list, depends. Finally it accepts aetas,
which becomes a dependent of venit and the head of ultima
and carminis.
The most-recent-first search order gives the parser its
preference for continuous constituents. The search order is
significant because it is assumed that the parser can back-
track, i.e., whenever there are alternatives it can back up
and try them. This is necessary to avoid &amp;quot;garden paths&amp;quot;
such as taking animalia (ambiguously nominative or accu-
sative) to be the subject of animalia vident pueri &amp;quot;boys see
animals.&amp;quot;
With ordinary sentences, however, backtracking is rela-
tively seldom necessary. Further, there appear to be other
constraints on variable word order. Ades and Steedman
(1982) propose that all discontinuities can be resolved by a
pushdown stack. (For example, pick up ultima, then Cu-
mad, then put down Cumaei next to carminis, then put
down ultima next to aetas. Crossing movements are not
permitted.) Moreover, there appears to be an absolute
constraint against mixing clauses together.3 If these hypoth-
eses hold true, the parser can be modified to restrict the
search process accordingly.
Most dependency parsers have followed a &amp;quot;principle of
adjacency&amp;quot; that requires every word plus all its direct and
indirect dependents to form a contiguous substring (Hays
and Ziehe 1960; Starosta and Nomura 1986; Fraser 1989;
but not Hellwig 1986 and possibly not Jappinen et al.
1986). This is equivalent to requiring constituents to be
continuous. This parser imposes no such requirement. To
add the adjacency requirement, one would modify it as
follows:
( ) When looking for potential dependents of W, never
</bodyText>
<page confidence="0.86051">
234 Computational Linguistics Volume 16, Number 4, December 1990
</page>
<figure confidence="0.9960512">
Technical Correspondence
VP NP
Adj
Adj V Adv
&amp;lima venit lam
Cumaei
last Cumean has come now
aelas
age
1. ultnna
2. &amp;lima Cumaei
3. ultima Cumaei venit
4. &amp;l/ma Cumaei venit lain
5. &amp;l/ma
Cumaei venit mm earnwhis
NP
Aux
NP
The last era of the Cumean song has now arrived&apos;
(Latin; Vergil. Eclogues IV.4)
6. u///ma
Cumaei
earminis aetas
lam
wen&apos;. t
</figure>
<figureCaption confidence="0.98871">
Figure 4. The parser accepts words one by one and
tries to link them together;&apos;+&apos; marks words that
do not (yet) depend on other words.
</figureCaption>
<figure confidence="0.792383333333333">
kurdu—ngku ka a - 12 yi wife —ngku
child dog chase small
the big dog chased the cat
</figure>
<figureCaption confidence="0.815726166666667">
Figure 2. Dependency representation of a
sentence. Arrows point from each word to its
dependents (modifiers or arguments).
skip over an independent word. That is, if an independent
word is found that cannot depend on W, then neither can
any earlier independent word.
</figureCaption>
<bodyText confidence="0.999417933333333">
(2) When looking for the word on which W depends,
consider only the previous word, that word&apos;s head, the
head&apos;s head if any, and so on.
With these requirements added, the algorithm would be
the same as one implemented by Hudson (1989).
Formal complexity analysis has not been carried out, but
my algorithm is simpler, at least conceptually, than the
variable-word-order parsers of Johnson (1985), Kashket
(1986), and Abramson and Dahl (1989). Johnson&apos;s parser
and Abramson and Dahl&apos;s parser use constituency trees
with explicit discontinuity (&amp;quot;tangled trees&amp;quot;), with all their
inherent unwieldiness. Kashket&apos;s parser, though based on
GB theory, is effectively a dependency parser since it relies
on case assignment and subcategorization rather than tree
structure.
</bodyText>
<figure confidence="0.89419">
&apos;The small child is chasing the dog&apos;
(Warlpiri; Siewierska 1988:158. citing Nash)
</figure>
<figureCaption confidence="0.999476">
Figure 1. Examples of discontinuous constituents.
</figureCaption>
<table confidence="0.940272">
Michael A. Covington
Artificial Intelligence Programs
The University of Georgia
Athens, GA 30602
REFERENCES
A
</table>
<figureCaption confidence="0.90842375">
Figure 3. Equivalence of dependency network to Abramson, Harvey and Dahl, Veronica. (1989). Logic Grammars.
X-bar tree. Springer.
Ades, Anthony E. and Steedman, Mark J. (1982). &amp;quot;On the order of
words.&amp;quot; Linguistics and Philosophy, 4: 517-558.
</figureCaption>
<bodyText confidence="0.268792">
Computational Linguistics Volume 16, Number 4, December 1990 235
</bodyText>
<subsectionHeader confidence="0.518768">
Technical Correspondence
</subsectionHeader>
<reference confidence="0.998044574074074">
Covington, Michael A. (1990). &amp;quot;A dependency parser for variable-word-
order languages.&amp;quot; In Computer Assisted Analysis and Modeling on the
IBM 3090, edited by Hilton U. Brown, MIT Press.
Covington, Michael A. (1988). &amp;quot;Parsing variable-word-order languages
with unification-based dependency grammar.&amp;quot; Research report 01-
0022, Artificial Intelligence Programs, The University of Georgia.
Fraser, Norman M. (1989). &amp;quot;Parsing and dependency grammar.&amp;quot; UCL
Working Papers in Linguistics, 1: 296-319.
Hays, David G. (1964). &amp;quot;Dependency theory: a formalism and some
observations.&amp;quot; Language 40: 511-525.
Hays, David G. and Ziehe, T. W. (1960). &amp;quot;Studies in machine translation,
10-Russian sentence-structure determination.&amp;quot; Research memoran-
dum RM-2358, The RAND Corporation, Santa Monica, CA.
Hellwig, Peter. (1986). &amp;quot;Dependency unification grammar.&amp;quot; In Proceed-
ings of the I I th International Conference on Computational Linguis-
tics (COLING-86). 195-198.
Hudson, Richard. (1989). &amp;quot;Towards a computer-testable word grammar
of English.&amp;quot; UCL Working Papers in Linguistics, 1:321-339.
Hudson, Richard. (1984). Word Grammar. Blackwell.
Jappinen, Harri; Lehtola, Aarno; and Valkonen, Kari. (1986). &amp;quot;Func-
tional structures for parsing dependency constraints.&amp;quot; In Proceedings
of the I 1 th International Conference on Computational Linguistics
(COLING-86), 461-463.
Johnson, Mark. (1985). &amp;quot;Parsing with discontinuous constituents.&amp;quot; Pro-
ceedings of the 23rd Annual Meeting of the Association for Computa-
tional Linguistics, 127-132.
Kashket, Michael B. (1986). &amp;quot;Parsing a free-word-order language:
Warlpiri.&amp;quot; Proceedings of the 24th Annual Meeting of the Association
for Computational Linguistics, 60-66.
MeRuk, I. A. (1988). Dependency Syntax: Theory and Practice. State
University Press of New York.
Robinson, Jane J. (1970). &amp;quot;Dependency structures and transformational
rules.&amp;quot; Language 46:259-285.
Schubert, Klaus. (1987). Metataxis: Contrastive Dependency Syntax for
Machine Translation. Foris.
Shieber, Stuart M. (1986). An Introduction to Unification-Based Ap-
proaches to Grammar. (CSLI Lecture Notes, 4.) Stanford: CSLI.
Starosta, Stanley. (1988). The Case for Lexicase. Pinter.
Starosta, Stanley and Nomura, Hirosato. (1986). &amp;quot;Lexicase parsing: a
lexicon-driven approach to syntactic analysis. In Proceedings of the
I 1 th International Conference on Computational Linguistics (COL-
ING-86).
Tesniere, Lucien. (1959). Elements de la Syntaxe Structurale. Klinck-
s ieck.
NOTES
1. The early stages of this work were supported by National Science
Foundation grant IST-85-02477. lam grateful to Norman Fraser and
Richard Hudson for comments and encouragement.
2. On dependency grammar in general see Tesniere 1959, Hays 1964,
Robinson 1970, Hudson 1986, Schubert 1987, Mel&apos;euk 1988, and
Starosta 1988. In Hudson&apos;s system, a single word can have two heads
provided the grammatical relations connecting it to them are distinct.
3. As pointed out by an anonymous reviewer for Computational Linguis-
tics.
</reference>
<page confidence="0.92375">
236 Computational Linguistics Volume 16, Number 4, December 1990
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.022899">
<title confidence="0.909346333333333">TECHNICAL CORRESPONDENCE PARSING DISCONTINUOUS CONSTITUENTS DEPENDENCY GRAMMAR</title>
<abstract confidence="0.97929091503268">Discontinuous constituents—for example, a noun and its modifying adjective separated by words unrelated to them-are common in variable-word-order languages; Figure 1 shows examples. But phrase structure grammars, including ID/LP grammars, require each constituent to be a contiguous series of words. Insofar as standard parsing algorithms are based on phrase structure rules, they are inadequate for such The algorithm presented here, however, does not require constituents to be continuous, but merely prefers them so. It can therefore parse languages in which conventional parsing techniques do not work. At the same time, because of its preference for nearby attachments, it prefers to make constituents continuous when more than one analysis is possible. The new algorithm has been used successfully to parse Russian and Latin (Covington 1988, 1990). This algorithm uses dependency grammar. That is, instead of breaking the sentence into phrases and subphrases, it establishes links between individual words. Each link connects a word (the &amp;quot;head&amp;quot;) with one of its &amp;quot;dependents&amp;quot; (an argument or modifier). Figure 2 shows how this works. The arrows point from head to dependent; a head can have many dependents, but each dependent can have only one head. Of course the same word can be the head in one link the dependent in Dependency grammar is equivalent to an X-bar theory with only one phrasal bar level (Figure 3)—the dependents of a word are the heads of its sisters. Thus dependency grammar captures the increasingly recognized importance of headship in syntax. At the same time, the absence of phrasal nodes from the dependency representation streamlines the search process during parsing. The parser presupposes a grammar that specifies which words can depend on which. In the prototype, the grammar consists of unification-based dependency rules (called D-rules) such as: [category:noun category:verb person:X , person:X number:Y case:nominative number:Y This rule sanctions a dependency relation between any two words whose features unify with the structures shown—in this case, the verb and its subject in a language such as Russian or Latin. The arrow means &amp;quot;can depend on&amp;quot; and word order is not specified. Y are variables. D-rules take the place of the phrase structure rules used by Shieber (1986) and others; semantic information can easily be added to them, and the whole power of unification-based grammar is available. The parser accepts words from the input string and keeps track of whether or not each word is &amp;quot;independent&amp;quot; (not yet known to depend on another word), indicated by + or — in Figure 4. On accepting a word W, the parser does the following: (1) Search the independent words (those marked +), most recent first, for words that can depend on W. If any are found, establish the dependencies and change the marking of the dependents from + to —. (2) Search all words so far seen, most recent first, for a word on which Wean depend. If one is found, establish the dependency and mark Was —. Otherwise mark Was +. Figure 4 shows the process in action. The first three Cumaei venit, accepted without creating links. Then the parser accepts makes it depend the parser accepts which Cuin the list, depends. Finally it accepts becomes a dependent of the head of The most-recent-first search order gives the parser its preference for continuous constituents. The search order is significant because it is assumed that the parser can backtrack, i.e., whenever there are alternatives it can back up and try them. This is necessary to avoid &amp;quot;garden paths&amp;quot; as taking nominative or accuto be the subject of vident pueri see animals.&amp;quot; With ordinary sentences, however, backtracking is relatively seldom necessary. Further, there appear to be other constraints on variable word order. Ades and Steedman (1982) propose that all discontinuities can be resolved by a stack. (For example, pick up Cuput down to put to movements are not permitted.) Moreover, there appears to be an absolute against mixing clauses If these hypotheses hold true, the parser can be modified to restrict the search process accordingly. Most dependency parsers have followed a &amp;quot;principle of adjacency&amp;quot; that requires every word plus all its direct and indirect dependents to form a contiguous substring (Hays and Ziehe 1960; Starosta and Nomura 1986; Fraser 1989; but not Hellwig 1986 and possibly not Jappinen et al. 1986). This is equivalent to requiring constituents to be continuous. This parser imposes no such requirement. To add the adjacency requirement, one would modify it as follows: ( ) When looking for potential dependents of W, never 234 Computational Linguistics Volume 16, Number 4, December 1990 Technical Correspondence VP NP Adj &amp;lima venit lam Cumaei last Cumean has come now aelas age 1. ultnna 2. &amp;lima Cumaei 3. ultima Cumaei venit 4. &amp;l/ma Cumaei venit lain 5. &amp;l/ma Cumaei venit mm earnwhis NP Aux NP The last era of the Cumean song has now arrived&apos; (Latin; Vergil. Eclogues IV.4) 6. u///ma Cumaei earminis aetas lam t 4. parser accepts words one by one tries to link them together;&apos;+&apos; marks words do not (yet) depend on other words. ka 12 —ngku child dog chase small the big dog chased the cat 2. representation of a sentence. Arrows point from each word to its dependents (modifiers or arguments). skip over an independent word. That is, if an independent word is found that cannot depend on W, then neither can any earlier independent word. (2) When looking for the word on which W depends, consider only the previous word, that word&apos;s head, the head&apos;s head if any, and so on. With these requirements added, the algorithm would be the same as one implemented by Hudson (1989). Formal complexity analysis has not been carried out, but my algorithm is simpler, at least conceptually, than the variable-word-order parsers of Johnson (1985), Kashket (1986), and Abramson and Dahl (1989). Johnson&apos;s parser and Abramson and Dahl&apos;s parser use constituency trees with explicit discontinuity (&amp;quot;tangled trees&amp;quot;), with all their inherent unwieldiness. Kashket&apos;s parser, though based on GB theory, is effectively a dependency parser since it relies on case assignment and subcategorization rather than tree structure. &apos;The small child is chasing the dog&apos; (Warlpiri; Siewierska 1988:158. citing Nash) 1. of discontinuous constituents.</abstract>
<author confidence="0.937165">Michael A Covington</author>
<note confidence="0.929959317073171">Artificial Intelligence Programs The University of Georgia Athens, GA 30602 REFERENCES A 3. of dependency network Harvey and Dahl, Veronica. (1989). Springer. X-bar tree. Ades, Anthony E. and Steedman, Mark J. (1982). &amp;quot;On the order of 517-558. Computational Linguistics Volume 16, Number 4, December 1990 235 Technical Correspondence Covington, Michael A. (1990). &amp;quot;A dependency parser for variable-wordlanguages.&amp;quot; In Assisted Analysis and Modeling on the 3090, by Hilton U. Brown, MIT Press. Covington, Michael A. (1988). &amp;quot;Parsing variable-word-order languages with unification-based dependency grammar.&amp;quot; Research report 01- 0022, Artificial Intelligence Programs, The University of Georgia. Norman M. (1989). &amp;quot;Parsing and dependency grammar.&amp;quot; Papers in Linguistics, 1: Hays, David G. (1964). &amp;quot;Dependency theory: a formalism and some 511-525. Hays, David G. and Ziehe, T. W. (1960). &amp;quot;Studies in machine translation, 10-Russian sentence-structure determination.&amp;quot; Research memorandum RM-2358, The RAND Corporation, Santa Monica, CA. Peter. (1986). &amp;quot;Dependency unification grammar.&amp;quot; In Proceedings of the I I th International Conference on Computational Linguis- 195-198. Hudson, Richard. (1989). &amp;quot;Towards a computer-testable word grammar English.&amp;quot; Working Papers in Linguistics, Richard. (1984). Grammar. Jappinen, Harri; Lehtola, Aarno; and Valkonen, Kari. (1986). &amp;quot;Funcstructures for parsing dependency constraints.&amp;quot; In of the I 1 th International Conference on Computational Linguistics (COLING-86), 461-463. Mark. (1985). &amp;quot;Parsing with discontinuous constituents.&amp;quot; Proceedings of the 23rd Annual Meeting of the Association for Computa- Linguistics, Kashket, Michael B. (1986). &amp;quot;Parsing a free-word-order language: of the 24th Annual Meeting of the Association Computational Linguistics, I. A. (1988). Syntax: Theory and Practice. University Press of New York. Robinson, Jane J. (1970). &amp;quot;Dependency structures and transformational Klaus. (1987). Contrastive Dependency Syntax for Translation.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>A dependency parser for variable-wordorder languages.&amp;quot;</title>
<date>1990</date>
<booktitle>In Computer Assisted Analysis and Modeling on the IBM 3090, edited by</booktitle>
<publisher>MIT Press.</publisher>
<marker>Covington, 1990</marker>
<rawString>Covington, Michael A. (1990). &amp;quot;A dependency parser for variable-wordorder languages.&amp;quot; In Computer Assisted Analysis and Modeling on the IBM 3090, edited by Hilton U. Brown, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>Parsing variable-word-order languages with unification-based dependency grammar.&amp;quot; Research report 01-0022, Artificial Intelligence Programs,</title>
<date>1988</date>
<institution>The University of Georgia.</institution>
<contexts>
<context position="950" citStr="Covington 1988" startWordPosition="134" endWordPosition="135">constituent to be a contiguous series of words. Insofar as standard parsing algorithms are based on phrase structure rules, they are inadequate for parsing such languages.1 The algorithm presented here, however, does not require constituents to be continuous, but merely prefers them so. It can therefore parse languages in which conventional parsing techniques do not work. At the same time, because of its preference for nearby attachments, it prefers to make constituents continuous when more than one analysis is possible. The new algorithm has been used successfully to parse Russian and Latin (Covington 1988, 1990). This algorithm uses dependency grammar. That is, instead of breaking the sentence into phrases and subphrases, it establishes links between individual words. Each link connects a word (the &amp;quot;head&amp;quot;) with one of its &amp;quot;dependents&amp;quot; (an argument or modifier). Figure 2 shows how this works. The arrows point from head to dependent; a head can have many dependents, but each dependent can have only one head. Of course the same word can be the head in one link and the dependent in another.2 Dependency grammar is equivalent to an X-bar theory with only one phrasal bar level (Figure 3)—the dependen</context>
</contexts>
<marker>Covington, 1988</marker>
<rawString>Covington, Michael A. (1988). &amp;quot;Parsing variable-word-order languages with unification-based dependency grammar.&amp;quot; Research report 01-0022, Artificial Intelligence Programs, The University of Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman M Fraser</author>
</authors>
<title>Parsing and dependency grammar.&amp;quot;</title>
<date>1989</date>
<journal>UCL Working Papers in Linguistics,</journal>
<volume>1</volume>
<pages>296--319</pages>
<contexts>
<context position="4761" citStr="Fraser 1989" startWordPosition="762" endWordPosition="763">ties can be resolved by a pushdown stack. (For example, pick up ultima, then Cumad, then put down Cumaei next to carminis, then put down ultima next to aetas. Crossing movements are not permitted.) Moreover, there appears to be an absolute constraint against mixing clauses together.3 If these hypotheses hold true, the parser can be modified to restrict the search process accordingly. Most dependency parsers have followed a &amp;quot;principle of adjacency&amp;quot; that requires every word plus all its direct and indirect dependents to form a contiguous substring (Hays and Ziehe 1960; Starosta and Nomura 1986; Fraser 1989; but not Hellwig 1986 and possibly not Jappinen et al. 1986). This is equivalent to requiring constituents to be continuous. This parser imposes no such requirement. To add the adjacency requirement, one would modify it as follows: ( ) When looking for potential dependents of W, never 234 Computational Linguistics Volume 16, Number 4, December 1990 Technical Correspondence VP NP Adj Adj V Adv &amp;lima venit lam Cumaei last Cumean has come now aelas age 1. ultnna 2. &amp;lima Cumaei 3. ultima Cumaei venit 4. &amp;l/ma Cumaei venit lain 5. &amp;l/ma Cumaei venit mm earnwhis NP Aux NP The last era of the Cumea</context>
</contexts>
<marker>Fraser, 1989</marker>
<rawString>Fraser, Norman M. (1989). &amp;quot;Parsing and dependency grammar.&amp;quot; UCL Working Papers in Linguistics, 1: 296-319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Hays</author>
</authors>
<title>Dependency theory: a formalism and some observations.&amp;quot;</title>
<date>1964</date>
<journal>Language</journal>
<volume>40</volume>
<pages>511--525</pages>
<marker>Hays, 1964</marker>
<rawString>Hays, David G. (1964). &amp;quot;Dependency theory: a formalism and some observations.&amp;quot; Language 40: 511-525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Hays</author>
<author>T W Ziehe</author>
</authors>
<title>Studies in machine translation, 10-Russian sentence-structure determination.&amp;quot; Research memorandum RM-2358, The RAND Corporation,</title>
<date>1960</date>
<location>Santa Monica, CA.</location>
<contexts>
<context position="4722" citStr="Hays and Ziehe 1960" startWordPosition="754" endWordPosition="757">nd Steedman (1982) propose that all discontinuities can be resolved by a pushdown stack. (For example, pick up ultima, then Cumad, then put down Cumaei next to carminis, then put down ultima next to aetas. Crossing movements are not permitted.) Moreover, there appears to be an absolute constraint against mixing clauses together.3 If these hypotheses hold true, the parser can be modified to restrict the search process accordingly. Most dependency parsers have followed a &amp;quot;principle of adjacency&amp;quot; that requires every word plus all its direct and indirect dependents to form a contiguous substring (Hays and Ziehe 1960; Starosta and Nomura 1986; Fraser 1989; but not Hellwig 1986 and possibly not Jappinen et al. 1986). This is equivalent to requiring constituents to be continuous. This parser imposes no such requirement. To add the adjacency requirement, one would modify it as follows: ( ) When looking for potential dependents of W, never 234 Computational Linguistics Volume 16, Number 4, December 1990 Technical Correspondence VP NP Adj Adj V Adv &amp;lima venit lam Cumaei last Cumean has come now aelas age 1. ultnna 2. &amp;lima Cumaei 3. ultima Cumaei venit 4. &amp;l/ma Cumaei venit lain 5. &amp;l/ma Cumaei venit mm earnw</context>
</contexts>
<marker>Hays, Ziehe, 1960</marker>
<rawString>Hays, David G. and Ziehe, T. W. (1960). &amp;quot;Studies in machine translation, 10-Russian sentence-structure determination.&amp;quot; Research memorandum RM-2358, The RAND Corporation, Santa Monica, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Hellwig</author>
</authors>
<title>Dependency unification grammar.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings of the I I th International Conference on Computational Linguistics (COLING-86).</booktitle>
<pages>195--198</pages>
<contexts>
<context position="4783" citStr="Hellwig 1986" startWordPosition="766" endWordPosition="767">by a pushdown stack. (For example, pick up ultima, then Cumad, then put down Cumaei next to carminis, then put down ultima next to aetas. Crossing movements are not permitted.) Moreover, there appears to be an absolute constraint against mixing clauses together.3 If these hypotheses hold true, the parser can be modified to restrict the search process accordingly. Most dependency parsers have followed a &amp;quot;principle of adjacency&amp;quot; that requires every word plus all its direct and indirect dependents to form a contiguous substring (Hays and Ziehe 1960; Starosta and Nomura 1986; Fraser 1989; but not Hellwig 1986 and possibly not Jappinen et al. 1986). This is equivalent to requiring constituents to be continuous. This parser imposes no such requirement. To add the adjacency requirement, one would modify it as follows: ( ) When looking for potential dependents of W, never 234 Computational Linguistics Volume 16, Number 4, December 1990 Technical Correspondence VP NP Adj Adj V Adv &amp;lima venit lam Cumaei last Cumean has come now aelas age 1. ultnna 2. &amp;lima Cumaei 3. ultima Cumaei venit 4. &amp;l/ma Cumaei venit lain 5. &amp;l/ma Cumaei venit mm earnwhis NP Aux NP The last era of the Cumean song has now arrived</context>
</contexts>
<marker>Hellwig, 1986</marker>
<rawString>Hellwig, Peter. (1986). &amp;quot;Dependency unification grammar.&amp;quot; In Proceedings of the I I th International Conference on Computational Linguistics (COLING-86). 195-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hudson</author>
</authors>
<title>Towards a computer-testable word grammar of English.&amp;quot; UCL Working Papers in Linguistics,</title>
<date>1989</date>
<pages>1--321</pages>
<contexts>
<context position="6180" citStr="Hudson (1989)" startWordPosition="1010" endWordPosition="1011">yet) depend on other words. kurdu—ngku ka a - 12 yi wife —ngku child dog chase small the big dog chased the cat Figure 2. Dependency representation of a sentence. Arrows point from each word to its dependents (modifiers or arguments). skip over an independent word. That is, if an independent word is found that cannot depend on W, then neither can any earlier independent word. (2) When looking for the word on which W depends, consider only the previous word, that word&apos;s head, the head&apos;s head if any, and so on. With these requirements added, the algorithm would be the same as one implemented by Hudson (1989). Formal complexity analysis has not been carried out, but my algorithm is simpler, at least conceptually, than the variable-word-order parsers of Johnson (1985), Kashket (1986), and Abramson and Dahl (1989). Johnson&apos;s parser and Abramson and Dahl&apos;s parser use constituency trees with explicit discontinuity (&amp;quot;tangled trees&amp;quot;), with all their inherent unwieldiness. Kashket&apos;s parser, though based on GB theory, is effectively a dependency parser since it relies on case assignment and subcategorization rather than tree structure. &apos;The small child is chasing the dog&apos; (Warlpiri; Siewierska 1988:158. c</context>
</contexts>
<marker>Hudson, 1989</marker>
<rawString>Hudson, Richard. (1989). &amp;quot;Towards a computer-testable word grammar of English.&amp;quot; UCL Working Papers in Linguistics, 1:321-339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hudson</author>
</authors>
<date>1984</date>
<publisher>Word Grammar. Blackwell.</publisher>
<marker>Hudson, 1984</marker>
<rawString>Hudson, Richard. (1984). Word Grammar. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harri Jappinen</author>
<author>Aarno Lehtola</author>
<author>Kari Valkonen</author>
</authors>
<title>Functional structures for parsing dependency constraints.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings of the I 1 th International Conference on Computational Linguistics (COLING-86),</booktitle>
<pages>461--463</pages>
<contexts>
<context position="4822" citStr="Jappinen et al. 1986" startWordPosition="771" endWordPosition="774">ple, pick up ultima, then Cumad, then put down Cumaei next to carminis, then put down ultima next to aetas. Crossing movements are not permitted.) Moreover, there appears to be an absolute constraint against mixing clauses together.3 If these hypotheses hold true, the parser can be modified to restrict the search process accordingly. Most dependency parsers have followed a &amp;quot;principle of adjacency&amp;quot; that requires every word plus all its direct and indirect dependents to form a contiguous substring (Hays and Ziehe 1960; Starosta and Nomura 1986; Fraser 1989; but not Hellwig 1986 and possibly not Jappinen et al. 1986). This is equivalent to requiring constituents to be continuous. This parser imposes no such requirement. To add the adjacency requirement, one would modify it as follows: ( ) When looking for potential dependents of W, never 234 Computational Linguistics Volume 16, Number 4, December 1990 Technical Correspondence VP NP Adj Adj V Adv &amp;lima venit lam Cumaei last Cumean has come now aelas age 1. ultnna 2. &amp;lima Cumaei 3. ultima Cumaei venit 4. &amp;l/ma Cumaei venit lain 5. &amp;l/ma Cumaei venit mm earnwhis NP Aux NP The last era of the Cumean song has now arrived&apos; (Latin; Vergil. Eclogues IV.4) 6. u//</context>
</contexts>
<marker>Jappinen, Lehtola, Valkonen, 1986</marker>
<rawString>Jappinen, Harri; Lehtola, Aarno; and Valkonen, Kari. (1986). &amp;quot;Functional structures for parsing dependency constraints.&amp;quot; In Proceedings of the I 1 th International Conference on Computational Linguistics (COLING-86), 461-463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Parsing with discontinuous constituents.&amp;quot;</title>
<date>1985</date>
<booktitle>Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="6341" citStr="Johnson (1985)" startWordPosition="1033" endWordPosition="1034">e. Arrows point from each word to its dependents (modifiers or arguments). skip over an independent word. That is, if an independent word is found that cannot depend on W, then neither can any earlier independent word. (2) When looking for the word on which W depends, consider only the previous word, that word&apos;s head, the head&apos;s head if any, and so on. With these requirements added, the algorithm would be the same as one implemented by Hudson (1989). Formal complexity analysis has not been carried out, but my algorithm is simpler, at least conceptually, than the variable-word-order parsers of Johnson (1985), Kashket (1986), and Abramson and Dahl (1989). Johnson&apos;s parser and Abramson and Dahl&apos;s parser use constituency trees with explicit discontinuity (&amp;quot;tangled trees&amp;quot;), with all their inherent unwieldiness. Kashket&apos;s parser, though based on GB theory, is effectively a dependency parser since it relies on case assignment and subcategorization rather than tree structure. &apos;The small child is chasing the dog&apos; (Warlpiri; Siewierska 1988:158. citing Nash) Figure 1. Examples of discontinuous constituents. Michael A. Covington Artificial Intelligence Programs The University of Georgia Athens, GA 30602 RE</context>
</contexts>
<marker>Johnson, 1985</marker>
<rawString>Johnson, Mark. (1985). &amp;quot;Parsing with discontinuous constituents.&amp;quot; Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics, 127-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael B Kashket</author>
</authors>
<title>Parsing a free-word-order language: Warlpiri.&amp;quot;</title>
<date>1986</date>
<booktitle>Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>60--66</pages>
<contexts>
<context position="6357" citStr="Kashket (1986)" startWordPosition="1035" endWordPosition="1036">from each word to its dependents (modifiers or arguments). skip over an independent word. That is, if an independent word is found that cannot depend on W, then neither can any earlier independent word. (2) When looking for the word on which W depends, consider only the previous word, that word&apos;s head, the head&apos;s head if any, and so on. With these requirements added, the algorithm would be the same as one implemented by Hudson (1989). Formal complexity analysis has not been carried out, but my algorithm is simpler, at least conceptually, than the variable-word-order parsers of Johnson (1985), Kashket (1986), and Abramson and Dahl (1989). Johnson&apos;s parser and Abramson and Dahl&apos;s parser use constituency trees with explicit discontinuity (&amp;quot;tangled trees&amp;quot;), with all their inherent unwieldiness. Kashket&apos;s parser, though based on GB theory, is effectively a dependency parser since it relies on case assignment and subcategorization rather than tree structure. &apos;The small child is chasing the dog&apos; (Warlpiri; Siewierska 1988:158. citing Nash) Figure 1. Examples of discontinuous constituents. Michael A. Covington Artificial Intelligence Programs The University of Georgia Athens, GA 30602 REFERENCES A Figur</context>
</contexts>
<marker>Kashket, 1986</marker>
<rawString>Kashket, Michael B. (1986). &amp;quot;Parsing a free-word-order language: Warlpiri.&amp;quot; Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, 60-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A MeRuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University Press of</publisher>
<location>New York.</location>
<marker>MeRuk, 1988</marker>
<rawString>MeRuk, I. A. (1988). Dependency Syntax: Theory and Practice. State University Press of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane J Robinson</author>
</authors>
<title>Dependency structures and transformational rules.&amp;quot;</title>
<date>1970</date>
<journal>Language</journal>
<pages>46--259</pages>
<marker>Robinson, 1970</marker>
<rawString>Robinson, Jane J. (1970). &amp;quot;Dependency structures and transformational rules.&amp;quot; Language 46:259-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Schubert</author>
</authors>
<title>Metataxis: Contrastive Dependency Syntax for Machine Translation. Foris.</title>
<date>1987</date>
<marker>Schubert, 1987</marker>
<rawString>Schubert, Klaus. (1987). Metataxis: Contrastive Dependency Syntax for Machine Translation. Foris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>An Introduction to Unification-Based Approaches to Grammar.</title>
<date>1986</date>
<journal>CSLI Lecture Notes,</journal>
<volume>4</volume>
<publisher>Pinter.</publisher>
<location>Stanford: CSLI. Starosta, Stanley.</location>
<contexts>
<context position="2441" citStr="Shieber (1986)" startWordPosition="374" endWordPosition="375"> presupposes a grammar that specifies which words can depend on which. In the prototype, the grammar consists of unification-based dependency rules (called D-rules) such as: [category:noun category:verb person:X , person:X number:Y case:nominative number:Y This rule sanctions a dependency relation between any two words whose features unify with the structures shown—in this case, the verb and its subject in a language such as Russian or Latin. The arrow means &amp;quot;can depend on&amp;quot; and the word order is not specified. X and Y are variables. D-rules take the place of the phrase structure rules used by Shieber (1986) and others; semantic information can easily be added to them, and the whole power of unification-based grammar is available. The parser accepts words from the input string and keeps track of whether or not each word is &amp;quot;independent&amp;quot; (not yet known to depend on another word), indicated by + or — in Figure 4. On accepting a word W, the parser does the following: (1) Search the independent words (those marked +), most recent first, for words that can depend on W. If any are found, establish the dependencies and change the marking of the dependents from + to —. (2) Search all words so far seen, m</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, Stuart M. (1986). An Introduction to Unification-Based Approaches to Grammar. (CSLI Lecture Notes, 4.) Stanford: CSLI. Starosta, Stanley. (1988). The Case for Lexicase. Pinter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Starosta</author>
<author>Hirosato Nomura</author>
</authors>
<title>Lexicase parsing: a lexicon-driven approach to syntactic analysis.</title>
<date>1986</date>
<booktitle>In Proceedings of the I 1 th International Conference on Computational Linguistics (COLING-86).</booktitle>
<contexts>
<context position="4748" citStr="Starosta and Nomura 1986" startWordPosition="758" endWordPosition="761">opose that all discontinuities can be resolved by a pushdown stack. (For example, pick up ultima, then Cumad, then put down Cumaei next to carminis, then put down ultima next to aetas. Crossing movements are not permitted.) Moreover, there appears to be an absolute constraint against mixing clauses together.3 If these hypotheses hold true, the parser can be modified to restrict the search process accordingly. Most dependency parsers have followed a &amp;quot;principle of adjacency&amp;quot; that requires every word plus all its direct and indirect dependents to form a contiguous substring (Hays and Ziehe 1960; Starosta and Nomura 1986; Fraser 1989; but not Hellwig 1986 and possibly not Jappinen et al. 1986). This is equivalent to requiring constituents to be continuous. This parser imposes no such requirement. To add the adjacency requirement, one would modify it as follows: ( ) When looking for potential dependents of W, never 234 Computational Linguistics Volume 16, Number 4, December 1990 Technical Correspondence VP NP Adj Adj V Adv &amp;lima venit lam Cumaei last Cumean has come now aelas age 1. ultnna 2. &amp;lima Cumaei 3. ultima Cumaei venit 4. &amp;l/ma Cumaei venit lain 5. &amp;l/ma Cumaei venit mm earnwhis NP Aux NP The last era</context>
</contexts>
<marker>Starosta, Nomura, 1986</marker>
<rawString>Starosta, Stanley and Nomura, Hirosato. (1986). &amp;quot;Lexicase parsing: a lexicon-driven approach to syntactic analysis. In Proceedings of the I 1 th International Conference on Computational Linguistics (COLING-86).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesniere</author>
</authors>
<title>Elements de la Syntaxe Structurale. Klincks ieck.</title>
<date>1959</date>
<marker>Tesniere, 1959</marker>
<rawString>Tesniere, Lucien. (1959). Elements de la Syntaxe Structurale. Klincks ieck.</rawString>
</citation>
<citation valid="false">
<authors>
<author>NOTES</author>
</authors>
<title>The early stages of this work were supported by National Science Foundation grant IST-85-02477. lam grateful to Norman Fraser and Richard Hudson for comments and encouragement.</title>
<marker>NOTES, </marker>
<rawString>NOTES 1. The early stages of this work were supported by National Science Foundation grant IST-85-02477. lam grateful to Norman Fraser and Richard Hudson for comments and encouragement.</rawString>
</citation>
<citation valid="true">
<title>On dependency grammar in general see Tesniere 1959,</title>
<date>1964</date>
<location>Hays</location>
<marker>1964</marker>
<rawString>2. On dependency grammar in general see Tesniere 1959, Hays 1964, Robinson 1970, Hudson 1986, Schubert 1987, Mel&apos;euk 1988, and Starosta 1988. In Hudson&apos;s system, a single word can have two heads provided the grammatical relations connecting it to them are distinct.</rawString>
</citation>
<citation valid="false">
<title>As pointed out by an anonymous reviewer for Computational Linguistics.</title>
<marker></marker>
<rawString>3. As pointed out by an anonymous reviewer for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>