<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.998738">
Don’t ‘have a clue’?
Unsupervised co-learning of downward-entailing operators
</title>
<author confidence="0.996943">
Cristian Danescu-Niculescu-Mizil and Lillian Lee
</author>
<affiliation confidence="0.999543">
Department of Computer Science, Cornell University
</affiliation>
<email confidence="0.994295">
cristian@cs.cornell.edu, llee@cs.cornell.edu
</email>
<sectionHeader confidence="0.994917" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99956615">
Researchers in textual entailment have
begun to consider inferences involving
downward-entailing operators, an inter-
esting and important class of lexical items
that change the way inferences are made.
Recent work proposed a method for learn-
ing English downward-entailing operators
that requires access to a high-quality col-
lection of negative polarity items (NPIs).
However, English is one of the very few
languages for which such a list exists. We
propose the first approach that can be ap-
plied to the many languages for which
there is no pre-existing high-precision
database of NPIs. As a case study, we
apply our method to Romanian and show
that our method yields good results. Also,
we perform a cross-linguistic analysis that
suggests interesting connections to some
findings in linguistic typology.
</bodyText>
<sectionHeader confidence="0.975632" genericHeader="introduction">
1 Introduction
</sectionHeader>
<figure confidence="0.381434">
Cristi: “Nicio” ... is that adjective you’ve mentioned.
Anca: A negative pronominal adjective.
Cristi: You mean there are people who analyze that
kind of thing?
Anca: The Romanian Academy.
Cristi: They’re crazy.
</figure>
<bodyText confidence="0.907152">
—From the movie Police, adjective
Downward-entailing operators are an interest-
ing and varied class of lexical items that change
the default way of dealing with certain types of
inferences. They thus play an important role in
understanding natural language [6, 18–20, etc.].
We explain what downward entailing means by
first demonstrating the “default” behavior, which
is upward entailing. The word ‘observed’ is an
example upward-entailing operator: the statement
</bodyText>
<listItem confidence="0.840012333333333">
(i) ‘Witnesses observed opium use.’
implies
(ii) ‘Witnesses observed narcotic use.’
</listItem>
<bodyText confidence="0.974584675675676">
but not vice versa (we write i ==&gt;. (*) ii). That
is, the truth value is preserved if we replace the
argument of an upward-entailing operator by a su-
perset (a more general version); in our case, the set
‘opium use’ was replaced by the superset ‘narcotic
use’.
Downward-entailing (DE) (also known as
downward monotonic or monotone decreasing)
operators violate this default inference rule: with
DE operators, reasoning instead goes from “sets to
subsets”. An example is the word ‘bans’:
‘The law bans opium use’
==&gt;.� (.&lt;--)
‘The law bans narcotic use’.
Although DE behavior represents an exception to
the default, DE operators are as a class rather com-
mon. They are also quite diverse in sense and
even part of speech. Some are simple negations,
such as ‘not’, but some other English DE opera-
tors are ‘without’, ‘reluctant to’, ‘to doubt’, and
‘to allow’.1 This variety makes them hard to ex-
tract automatically.
Because DE operators violate the default “sets
to supersets” inference, identifying them can po-
tentially improve performance in many NLP tasks.
Perhaps the most obvious such tasks are those in-
volving textual entailment, such as question an-
swering, information extraction, summarization,
and the evaluation of machine translation [4]. Re-
searchers are in fact beginning to build textual-
entailment systems that can handle inferences in-
volving downward-entailing operators other than
simple negations, although these systems almost
all rely on small handcrafted lists of DE operators
[1–3, 15, 16].2 Other application areas are natural-
language generation and human-computer interac-
tion, since downward-entailing inferences induce
</bodyText>
<footnote confidence="0.9881229">
1Some examples showing different constructions for ana-
lyzing these operators: ‘The defendant does not own a blue
car’ gS (.&lt;--) ‘The defendant does not own a car’; ‘They are
reluctant to tango’ gS (.&lt;--) ‘They are reluctant to dance’;
‘Police doubt Smith threatened Jones’ gS (.&lt;--) ‘Police doubt
Smith threatened Jones or Brown’; ‘You are allowed to use
Mastercard’ gS (.&lt;--) ‘You are allowed to use any credit card’.
2The exception [2] employs the list automatically derived
by Danescu-Niculescu-Mizil, Lee, and Ducott [5], described
later.
</footnote>
<page confidence="0.929459">
247
</page>
<note confidence="0.520565">
Proceedings of the ACL 2010 Conference Short Papers, pages 247–252,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999929106382979">
greater cognitive load than inferences in the oppo-
site direction [8].
Most NLP systems for the applications men-
tioned above have only been deployed for a small
subset of languages. A key factor is the lack
of relevant resources for other languages. While
one approach would be to separately develop a
method to acquire such resources for each lan-
guage individually, we instead aim to ameliorate
the resource-scarcity problem in the case of DE
operators wholesale: we propose a single unsuper-
vised method that can extract DE operators in any
language for which raw text corpora exist.
Overview of our work Our approach takes the
English-centric work of Danescu-Niculescu-Mizil
et al. [5] — DLD09 for short — as a starting point,
as they present the first and, until now, only al-
gorithm for automatically extracting DE operators
from data. However, our work departs signifi-
cantly from DLD09 in the following key respect.
DLD09 critically depends on access to a high-
quality, carefully curated collection of negative
polarity items (NPIs) — lexical items such as
‘any’, ‘ever’, or the idiom ‘have a clue’ that tend
to occur only in negative environments (see §2
for more details). DLD09 use NPIs as signals of
the occurrence of downward-entailing operators.
However, almost every language other than En-
glish lacks a high-quality accessible NPI list.
To circumvent this problem, we introduce a
knowledge-lean co-learning approach. Our al-
gorithm is initialized with a very small seed set
of NPIs (which we describe how to generate), and
then iterates between (a) discovering a set of DE
operators using a collection of pseudo-NPIs — a
concept we introduce — and (b) using the newly-
acquired DE operators to detect new pseudo-NPIs.
Why this isn’t obvious Although the algorith-
mic idea sketched above seems quite simple, it is
important to note that prior experiments in that
direction have not proved fruitful. Preliminary
work on learning (German) NPIs using a small
list of simple known DE operators did not yield
strong results [14]. Hoeksema [10] discusses why
NPIs might be hard to learn from data.3 We cir-
cumvent this problem because we are not inter-
ested in learning NPIs per se; rather, for our pur-
</bodyText>
<footnote confidence="0.98998875">
3In fact, humans can have trouble agreeing on NPI-hood;
for instance, Lichte and Soehn [14] mention doubts about
over half of K¨urschner [12]’s 344 manually collected German
NPIs.
</footnote>
<bodyText confidence="0.998881333333333">
poses, pseudo-NPIs suffice. Also, our prelim-
inary work determined that one of the most fa-
mous co-learning algorithms, hubs and authorities
or HITS [11], is poorly suited to our problem.4
Contributions To begin with, we apply our al-
gorithm to produce the first large list of DE opera-
tors for a language other than English. In our case
study on Romanian (§4), we achieve quite high
precisions at k (for example, iteration achieves a
precision at 30 of 87%).
Auxiliary experiments explore the effects of us-
ing a large but noisy NPI list, should one be avail-
able for the language in question. Intriguingly, we
find that co-learning new pseudo-NPIs provides
better results.
Finally (§5), we engage in some cross-linguistic
analysis based on the results of applying our al-
gorithm to English. We find that there are some
suggestive connections with findings in linguistic
typology.
Appendix available A more complete account
of our work and its implications can be found in a
version of this paper containing appendices, avail-
able at www.cs.cornell.edu/˜cristian/acl2010/.
</bodyText>
<sectionHeader confidence="0.527852" genericHeader="method">
2 DLD09: successes and challenges
</sectionHeader>
<bodyText confidence="0.999911111111111">
In this section, we briefly summarize those aspects
of the DLD09 method that are important to under-
standing how our new co-learning method works.
DE operators and NPIs Acquiring DE opera-
tors is challenging because of the complete lack of
annotated data. DLD09’s insight was to make use
of negative polarity items (NPIs), which are words
or phrases that tend to occur only in negative con-
texts. The reason they did so is that Ladusaw’s hy-
pothesis [7, 13] asserts that NPIs only occur within
the scope of DE operators. Figure 1 depicts exam-
ples involving the English NPIs ‘any’5 and ‘have
a clue’ (in the idiomatic sense) that illustrate this
relationship. Some other English NPIs are ‘ever’,
‘yet’ and ‘give a damn’.
Thus, NPIs can be treated as clues that a DE
operator might be present (although DE operators
may also occur without NPIs).
</bodyText>
<footnote confidence="0.771964166666667">
4We explored three different edge-weighting schemes
based on co-occurrence frequencies and seed-set member-
ship, but the results were extremely poor; HITS invariably
retrieved very frequent words.
5The free-choice sense of ‘any’, as in ‘I can skim any pa-
per in five minutes’, is a known exception.
</footnote>
<page confidence="0.98825">
248
</page>
<figure confidence="0.995450923076923">
NPIs
DE operators
any have a clue, idiomatic sense
not or n’t
✓ We do n’t
have any apples ✓ We do n’t
have a clue
doubt
no DE operator
✓I doubt
they have any apples ✓ I doubt
they have a clue
x They have any apples x They have a clue
</figure>
<figureCaption confidence="0.996256">
Figure 1: Examples consistent with Ladusaw’s hypothesis that NPIs can only occur within the scope of
DE operators. A ✓ denotes an acceptable sentence; a x denotes an unacceptable sentence.
</figureCaption>
<bodyText confidence="0.969772321428572">
DLD09 algorithm Potential DE operators are
collected by extracting those words that appear in
an NPI’s context at least once.6 Then, the potential
DE operators x are ranked by
fraction of NPI contexts that contain x
f(x) ��
relative frequency of x in the corpus ,
which compares x’s probability of occurrence
conditioned on the appearance of an NPI with its
probability of occurrence overall.7
The method just outlined requires access to a
list of NPIs. DLD09’s system used a subset of
John Lawler’s carefully curated and “moderately
complete” list of English NPIs.8 The resultant
rankings of candidate English DE operators were
judged to be of high quality.
The challenge in porting to other languages:
cluelessness Can the unsupervised approach of
DLD09 be successfully applied to languages other
than English? Unfortunately, for most other lan-
guages, it does not seem that large, high-quality
NPI lists are available.
One might wonder whether one can circumvent
the NPI-acquisition problem by simply translating
a known English NPI list into the target language.
However, NPI-hood need not be preserved under
translation [17]. Thus, for most languages, we
lack the critical clues that DLD09 depends on.
</bodyText>
<sectionHeader confidence="0.896691" genericHeader="method">
3 Getting a clue
</sectionHeader>
<bodyText confidence="0.7582948">
In this section, we develop an iterative co-
learning algorithm that can extract DE operators
in the many languages where a high-quality NPI
6DLD09 policies: (a) “NPI context” was defined as the
part of the sentence to the left of the NPI up to the first
comma, semi-colon or beginning of sentence; (b) to encour-
age the discovery of new DE operators, those sentences con-
taining one of a list of 10 well-known DE operators were dis-
carded. For Romanian, we treated only negations (‘nu’ and
‘n-’) and questions as well-known environments.
</bodyText>
<footnote confidence="0.973033666666667">
7DLD09 used an additional distilled score, but we found
that the distilled score performed worse on Romanian.
8http://www-personal.umich.edu/—jlawler/aue/npi.html
</footnote>
<bodyText confidence="0.9951055">
database is not available, using Romanian as a
case study.
</bodyText>
<subsectionHeader confidence="0.997522">
3.1 Data and evaluation paradigm
</subsectionHeader>
<bodyText confidence="0.999641857142857">
We used Rada Mihalcea’s corpus of Pz�1.45 million
sentences of raw Romanian newswire articles.
Note that we cannot evaluate impact on textual
inference because, to our knowledge, no publicly
available textual-entailment system or evaluation
data for Romanian exists. We therefore examine
the system outputs directly to determine whether
the top-ranked items are actually DE operators or
not. Our evaluation metric is precision at k of a
given system’s ranked list of candidate DE oper-
ators; it is not possible to evaluate recall since no
list of Romanian DE operators exists (a problem
that is precisely the motivation for this paper).
To evaluate the results, two native Romanian
speakers labeled the system outputs as being
“DE”, “not DE” or “Hard (to decide)”. The la-
beling protocol, which was somewhat complex
to prevent bias, is described in the externally-
available appendices (§7.1). The complete system
output and annotations are publicly available at:
http://www.cs.cornell.edu/˜cristian/acl2010/.
</bodyText>
<subsectionHeader confidence="0.999872">
3.2 Generating a seed set
</subsectionHeader>
<bodyText confidence="0.999933818181818">
Even though, as discussed above, the translation
of an NPI need not be an NPI, a preliminary re-
view of the literature indicates that in many lan-
guages, there is some NPI that can be translated
as ‘any’ or related forms like ‘anybody’. Thus,
with a small amount of effort, one can form a min-
imal NPI seed set for the DLD09 method by us-
ing an appropriate target-language translation of
‘any’. For Romanian, we used ‘vreo’ and ‘vreun’,
which are the feminine and masculine translations
of English ‘any’.
</bodyText>
<subsectionHeader confidence="0.917832">
3.3 DLD09 using the Romanian seed set
</subsectionHeader>
<bodyText confidence="0.972405666666667">
We first check whether DLD09 with the two-
item seed set described in §3.2 performs well on
Romanian. In fact, the results are fairly poor:
</bodyText>
<page confidence="0.984951">
249
</page>
<figure confidence="0.997069235294117">
40
100
90
80
70
Precision at k (in %)
60
50
40
30
20
10
0
k=80
k=50
k=40
25
k=30
20
k=20
k=10
5
0
0 5 9 10 15
Iteration
Number of DE−operators
35
30
15
10
DE
Hard
10 20 30 40 50 60 70 80
k
</figure>
<figureCaption confidence="0.9988525">
Figure 2: Left: Number of DE operators in the top k results returned by the co-learning method at each iteration.
Items labeled “Hard” are not included. Iteration 0 corresponds to DLD09 applied to I‘vreo’, ‘vreun’}. Curves for
k = 60 and 70 omitted for clarity. Right: Precisions at k for the results of the 9th iteration. The bar divisions are:
DE (blue/darkest/largest) and Hard (red/lighter, sometimes non-existent).
</figureCaption>
<bodyText confidence="0.9999115">
for example, the precision at 30 is below 50%.
(See blue/dark bars in figure 3 in the externally-
available appendices for detailed results.)
This relatively unsatisfactory performance may
be a consequence of the very small size of the NPI
list employed, and may therefore indicate that it
would be fruitful to investigate automatically ex-
tending our list of clues.
</bodyText>
<subsectionHeader confidence="0.979634">
3.4 Main idea: a co-learning approach
</subsectionHeader>
<bodyText confidence="0.94707353125">
Our main insight is that not only can NPIs be used
as clues for finding DE operators, as shown by
DLD09, but conversely, DE operators (if known)
can potentially be used to discover new NPI-like
clues, which we refer to as pseudo-NPIs (or pNPIs
for short). By “NPI-like” we mean, “serve as pos-
sible indicators of the presence of DE operators,
regardless of whether they are actually restricted
to negative contexts, as true NPIs are”. For exam-
ple, in English newswire, the words ‘allegation’ or
‘rumor’ tend to occur mainly in DE contexts, like
‘ denied ’ or ‘ dismissed ’, even though they are
clearly not true NPIs (the sentence ‘I heard a ru-
mor’ is fine). Given this insight, we approach the
problem using an iterative co-learning paradigm
that integrates the search for new DE operators
with a search for new pNPIs.
First, we describe an algorithm that is the “re-
verse” of DLD09 (henceforth rDLD), in that it re-
trieves and ranks pNPIs assuming a given list of
DE operators. Potential pNPIs are collected by ex-
tracting those words that appear in a DE context
(defined here, to avoid the problems of parsing or
scope determination, as the part of the sentence to
the right of a DE operator, up to the first comma,
semi-colon or end of sentence); these candidates x
are then ranked by
fraction of DE contexts that contain x
�
relative frequency of x in the corpus
Then, our co-learning algorithm consists of the
iteration of the following two steps:
</bodyText>
<listItem confidence="0.88036225">
• (DE learning) Apply DLD09 using a set N
of pseudo-NPIs to retrieve a list of candidate
DE operators ranked by f (defined in Section
2). Let D be the top n candidates in this list.
• (pNPI learning) Apply rDLD using the set D
to retrieve a list of pNPIs ranked by fr; ex-
tend N with the top nr pNPIs in this list. In-
crement n.
</listItem>
<bodyText confidence="0.999982">
Here, N is initialized with the NPI seed set. At
each iteration, we consider the output of the al-
gorithm to be the ranked list of DE operators re-
trieved in the DE-learning step. In our experi-
ments, we initialized n to 10 and set nr to 1.
</bodyText>
<sectionHeader confidence="0.977702" genericHeader="method">
4 Romanian results
</sectionHeader>
<bodyText confidence="0.999959888888889">
Our results show that there is indeed favorable
synergy between DE-operator and pNPI retrieval.
Figure 2 plots the number of correctly retrieved
DE operators in the top k outputs at each iteration.
The point at iteration 0 corresponds to a datapoint
already discussed above, namely, DLD09 applied
to the two ‘any’-translation NPIs. Clearly, we see
general substantial improvement over DLD09, al-
though the increases level off in later iterations.
</bodyText>
<equation confidence="0.952348">
fr(x) �=
</equation>
<page confidence="0.972392">
250
</page>
<bodyText confidence="0.999942">
(Determining how to choose the optimal number
of iterations is a subject for future research.)
Additional experiments, described in the
externally-available appendices (§7.2), suggest
that pNPIs can even be more effective clues than
a noisy list of NPIs. (Thus, a larger seed set
does not necessarily mean better performance.)
pNPIs also have the advantage of being derivable
automatically, and might be worth investigating
from a linguistic perspective in their own right.
</bodyText>
<sectionHeader confidence="0.92625" genericHeader="method">
5 Cross-linguistic analysis
</sectionHeader>
<bodyText confidence="0.999802567567567">
Applying our algorithm to English: connec-
tions to linguistic typology So far, we have
made no assumptions about the language on which
our algorithm is applied. A valid question is, does
the quality of the results vary with choice of appli-
cation language? In particular, what happens if we
run our algorithm on English?
Note that in some sense, this is a perverse ques-
tion: the motivation behind our algorithm is the
non-existence of a high-quality list of NPIs for
the language in question, and English is essen-
tially the only case that does not fit this descrip-
tion. On the other hand, the fact that DLD09 ap-
plied their method for extraction of DE operators
to English necessitates some form of comparison,
for the sake of experimental completeness.
We thus ran our algorithm on the English
BLLIP newswire corpus with seed set {‘any’} .
We observe that, surprisingly, the iterative addi-
tion of pNPIs has very little effect: the precisions
at k are good at the beginning and stay about the
same across iterations (for details see figure 5 in
in the externally-available appendices). Thus, on
English, co-learning does not hurt performance,
which is good news; but unlike in Romanian, it
does not lead to improvements.
Why is English ‘any’ seemingly so “powerful”,
in contrast to Romanian, where iterating beyond
the initial ‘any’ translations leads to better re-
sults? Interestingly, findings from linguistic typol-
ogy may shed some light on this issue. Haspel-
math [9] compares the functions of indefinite pro-
nouns in 40 languages. He shows that English is
one of the minority of languages (11 out of 40)9 in
which there exists an indefinite pronoun series that
occurs in all (Haspelmath’s) classes of DE con-
texts, and thus can constitute a sufficient seed on
</bodyText>
<page confidence="0.4137855">
9English, Ancash Quechua, Basque, Catalan, French,
Hindi/Urdu, Irish, Portuguese, Swahili, Swedish, Turkish.
</page>
<bodyText confidence="0.999973866666667">
its own. In the other languages (including Roma-
nian),10 no indirect pronoun can serve as a suffi-
cient seed. So, we expect our method to be vi-
able for all languages; while the iterative discov-
ery of pNPIs is not necessary (although neither is
it harmful) for the subset of languages for which a
sufficient seed exists, such as English, it is essen-
tial for the languages for which, like Romanian,
‘any’-equivalents do not suffice.
Using translation Another interesting question
is whether directly translating DE operators from
English is an alternative to our method. First, we
emphasize that there exists no complete list of En-
glish DE operators (the largest available collec-
tion is the one extracted by DLD09). Second, we
do not know whether DE operators in one lan-
guage translate into DE operators in another lan-
guage. Even if that were the case, and we some-
how had access to ideal translations of DLD09’s
list, there would still be considerable value in us-
ing our method: 14 (39%) of our top 36 highest-
ranked Romanian DE operators for iteration 9 do
not, according to the Romanian-speaking author,
have English equivalents appearing on DLD09’s
90-item list. Some examples are: ‘abt¸inut’ (ab-
stained), ‘criticat’ (criticized) and ‘react¸ionat’ (re-
acted). Therefore, a significant fraction of the
DE operators derived by our co-learning algorithm
would have been missed by the translation alterna-
tive even under ideal conditions.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999215">
We have introduced the first method for discov-
ering downward-entailing operators that is univer-
sally applicable. Previous work on automatically
detecting DE operators assumed the existence of
a high-quality collection of NPIs, which renders it
inapplicable in most languages, where such a re-
source does not exist. We overcome this limita-
tion by employing a novel co-learning approach,
and demonstrate its effectiveness on Romanian.
Also, we introduce the concept of pseudo-NPIs.
Auxiliary experiments described in the externally-
available appendices show that pNPIs are actually
more effective seeds than a noisy “true” NPI list.
Finally, we noted some cross-linguistic differ-
ences in performance, and found an interesting
connection between these differences and Haspel-
math’s [9] characterization of cross-linguistic vari-
ation in the occurrence of indefinite pronouns.
</bodyText>
<footnote confidence="0.406631">
10Examples: Chinese, German, Italian, Polish, Serbian.
</footnote>
<page confidence="0.996618">
251
</page>
<bodyText confidence="0.99651975">
Acknowledgments We thank Tudor Marian for
serving as an annotator, Rada Mihalcea for ac-
cess to the Romanian newswire corpus, and Claire
Cardie, Yejin Choi, Effi Georgala, Mark Liber-
man, Myle Ott, Jo˜ao Paula Muchado, Stephen Pur-
pura, Mark Yatskar, Ainur Yessenalina, and the
anonymous reviewers for their helpful comments.
Supported by NSF grant IIS-0910664.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999969511363637">
[1] Roy Bar-Haim, Jonathan Berant, Ido Da-
gan, Iddo Greental, Shachar Mirkin, Eyal
Shnarch, and Idan Szpektor. Efficient seman-
tic deduction and approximate matching over
compact parse forests. In Proceedings of the
Text Analysis Conference (TAC), 2008.
[2] Eric Breck. A simple system for detecting
non-entailment. In Proceedings of the Text
Analysis Conference (TAC), 2009.
[3] Christos Christodoulopoulos. Creating a nat-
ural logic inference system with combinatory
categorial grammar. Master’s thesis, Univer-
sity of Edinburgh, 2008.
[4] Ido Dagan, Oren Glickman, and Bernardo
Magnini. The PASCAL Recognising Textual
Entailment challenge. In Machine Learn-
ing Challenges, Evaluating Predictive Un-
certainty, Visual Object Classification and
Recognizing Textual Entailment, First PAS-
CAL Machine Learning Challenges Work-
shop, pages 177–190. Springer, 2006.
[5] Cristian Danescu-Niculescu-Mizil, Lillian
Lee, and Richard Ducott. Without a ‘doubt’?
Unsupervised discovery of downward-
entailing operators. In Proceedings of
NAACL HLT, 2009.
[6] David Dowty. The role of negative polar-
ity and concord marking in natural language
reasoning. In Mandy Harvey and Lynn San-
telmann, editors, Proceedings of SALT IV,
pages 114–144, 1994.
[7] Gilles Fauconnier. Polarity and the scale
principle. In Proceedings of the Chicago Lin-
guistic Society (CLS), pages 188–199, 1975.
Reprinted in Javier Gutierrez-Rexach (ed.),
Semantics: Critical Concepts in Linguistics,
2003.
[8] Bart Geurts and Frans van der Slik. Mono-
tonicity and processing load. Journal of Se-
mantics, 22(1):97–117, 2005.
[9] Martin Haspelmath. Indefinite Pronouns.
Oxford University Press, 2001.
[10] Jack Hoeksema. Corpus study of negative
polarity items. IV-V Jornades de corpus lin-
guistics 1996-1997, 1997. http://odur.let.rug.
nl/—hoeksema/docs/barcelona.html.
[11] Jon Kleinberg. Authoritative sources in a hy-
perlinked environment. In Proceedings of
the 9th ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 668–677, 1998.
Extended version in Journal of the ACM,
46:604–632, 1999.
[12] Wilfried Kirschner. Studien zur Negation im
Deutschen. Narr, 1983.
[13] William A. Ladusaw. Polarity Sensitivity as
Inherent Scope Relations. Garland Press,
New York, 1980. Ph.D. thesis date 1979.
[14] Timm Lichte and Jan-Philipp Soehn. The re-
trieval and classification of Negative Polar-
ity Items using statistical profiles. In Sam
Featherston and Wolfgang Sternefeld, edi-
tors, Roots: Linguistics in Search of its Ev-
idential Base, pages 249–266. Mouton de
Gruyter, 2007.
[15] Bill MacCartney and Christopher D. Man-
ning. Modeling semantic containment and
exclusion in natural language inference. In
Proceedings of COLING, pages 521–528,
2008.
[16] Rowan Nairn, Cleo Condoravdi, and Lauri
Karttunen. Computing relative polarity for
textual inference. In Proceedings of In-
ference in Computational Semantics (ICoS),
2006.
[17] Frank Richter, Janina Rad´o, and Manfred
Sailer. Negative polarity items: Corpus
linguistics, semantics, and psycholinguis-
tics: Day 2: Corpus linguistics. Tutorial
slides: http://www.sfs.uni-tuebingen.de/∼fr/
esslli/08/byday/day2/day2-part1.pdf, 2008.
[18] Victor S´anchez Valencia. Studies on natural
logic and categorial grammar. PhD thesis,
University of Amsterdam, 1991.
[19] Johan van Benthem. Essays in Logical Se-
mantics. Reidel, Dordrecht, 1986.
[20] Ton van der Wouden. Negative contexts:
Collocation, polarity and multiple negation.
Routledge, 1997.
</reference>
<page confidence="0.997433">
252
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950088">
<title confidence="0.9907125">Don’t ‘have a clue’? Unsupervised co-learning of downward-entailing operators</title>
<author confidence="0.999946">Cristian Danescu-Niculescu-Mizil</author>
<author confidence="0.999946">Lillian Lee</author>
<affiliation confidence="0.999957">Department of Computer Science, Cornell University</affiliation>
<email confidence="0.999646">cristian@cs.cornell.edu,llee@cs.cornell.edu</email>
<abstract confidence="0.998388285714286">Researchers in textual entailment have begun to consider inferences involving an interesting and important class of lexical items that change the way inferences are made. Recent work proposed a method for learning English downward-entailing operators that requires access to a high-quality colof polarity items However, English is one of the very few languages for which such a list exists. We propose the first approach that can be applied to the many languages for which there is no pre-existing high-precision database of NPIs. As a case study, we apply our method to Romanian and show that our method yields good results. Also, we perform a cross-linguistic analysis that suggests interesting connections to some findings in linguistic typology.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Jonathan Berant</author>
</authors>
<title>Ido Dagan, Iddo Greental, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor. Efficient semantic deduction and approximate matching over compact parse forests.</title>
<date>2008</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC),</booktitle>
<marker>[1]</marker>
<rawString>Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Greental, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor. Efficient semantic deduction and approximate matching over compact parse forests. In Proceedings of the Text Analysis Conference (TAC), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
</authors>
<title>A simple system for detecting non-entailment.</title>
<date>2009</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC),</booktitle>
<contexts>
<context position="3887" citStr="[2]" startWordPosition="588" endWordPosition="588">sts of DE operators [1–3, 15, 16].2 Other application areas are naturallanguage generation and human-computer interaction, since downward-entailing inferences induce 1Some examples showing different constructions for analyzing these operators: ‘The defendant does not own a blue car’ gS (.&lt;--) ‘The defendant does not own a car’; ‘They are reluctant to tango’ gS (.&lt;--) ‘They are reluctant to dance’; ‘Police doubt Smith threatened Jones’ gS (.&lt;--) ‘Police doubt Smith threatened Jones or Brown’; ‘You are allowed to use Mastercard’ gS (.&lt;--) ‘You are allowed to use any credit card’. 2The exception [2] employs the list automatically derived by Danescu-Niculescu-Mizil, Lee, and Ducott [5], described later. 247 Proceedings of the ACL 2010 Conference Short Papers, pages 247–252, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics greater cognitive load than inferences in the opposite direction [8]. Most NLP systems for the applications mentioned above have only been deployed for a small subset of languages. A key factor is the lack of relevant resources for other languages. While one approach would be to separately develop a method to acquire such resources for e</context>
</contexts>
<marker>[2]</marker>
<rawString>Eric Breck. A simple system for detecting non-entailment. In Proceedings of the Text Analysis Conference (TAC), 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
</authors>
<title>Creating a natural logic inference system with combinatory categorial grammar. Master’s thesis,</title>
<date>2008</date>
<institution>University of Edinburgh,</institution>
<marker>[3]</marker>
<rawString>Christos Christodoulopoulos. Creating a natural logic inference system with combinatory categorial grammar. Master’s thesis, University of Edinburgh, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment challenge.</title>
<date>2006</date>
<booktitle>In Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop,</booktitle>
<pages>177--190</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="3056" citStr="[4]" startWordPosition="462" endWordPosition="462"> rather common. They are also quite diverse in sense and even part of speech. Some are simple negations, such as ‘not’, but some other English DE operators are ‘without’, ‘reluctant to’, ‘to doubt’, and ‘to allow’.1 This variety makes them hard to extract automatically. Because DE operators violate the default “sets to supersets” inference, identifying them can potentially improve performance in many NLP tasks. Perhaps the most obvious such tasks are those involving textual entailment, such as question answering, information extraction, summarization, and the evaluation of machine translation [4]. Researchers are in fact beginning to build textualentailment systems that can handle inferences involving downward-entailing operators other than simple negations, although these systems almost all rely on small handcrafted lists of DE operators [1–3, 15, 16].2 Other application areas are naturallanguage generation and human-computer interaction, since downward-entailing inferences induce 1Some examples showing different constructions for analyzing these operators: ‘The defendant does not own a blue car’ gS (.&lt;--) ‘The defendant does not own a car’; ‘They are reluctant to tango’ gS (.&lt;--) ‘T</context>
</contexts>
<marker>[4]</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL Recognising Textual Entailment challenge. In Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, pages 177–190. Springer, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Lillian Lee</author>
<author>Richard Ducott</author>
</authors>
<title>Without a ‘doubt’? Unsupervised discovery of downwardentailing operators.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL HLT,</booktitle>
<contexts>
<context position="3974" citStr="[5]" startWordPosition="599" endWordPosition="599">ion and human-computer interaction, since downward-entailing inferences induce 1Some examples showing different constructions for analyzing these operators: ‘The defendant does not own a blue car’ gS (.&lt;--) ‘The defendant does not own a car’; ‘They are reluctant to tango’ gS (.&lt;--) ‘They are reluctant to dance’; ‘Police doubt Smith threatened Jones’ gS (.&lt;--) ‘Police doubt Smith threatened Jones or Brown’; ‘You are allowed to use Mastercard’ gS (.&lt;--) ‘You are allowed to use any credit card’. 2The exception [2] employs the list automatically derived by Danescu-Niculescu-Mizil, Lee, and Ducott [5], described later. 247 Proceedings of the ACL 2010 Conference Short Papers, pages 247–252, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics greater cognitive load than inferences in the opposite direction [8]. Most NLP systems for the applications mentioned above have only been deployed for a small subset of languages. A key factor is the lack of relevant resources for other languages. While one approach would be to separately develop a method to acquire such resources for each language individually, we instead aim to ameliorate the resource-scarcity problem i</context>
</contexts>
<marker>[5]</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Lillian Lee, and Richard Ducott. Without a ‘doubt’? Unsupervised discovery of downwardentailing operators. In Proceedings of NAACL HLT, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<title>The role of negative polarity and concord marking in natural language reasoning.</title>
<date>1994</date>
<booktitle>Proceedings of SALT IV,</booktitle>
<pages>114--144</pages>
<editor>In Mandy Harvey and Lynn Santelmann, editors,</editor>
<marker>[6]</marker>
<rawString>David Dowty. The role of negative polarity and concord marking in natural language reasoning. In Mandy Harvey and Lynn Santelmann, editors, Proceedings of SALT IV, pages 114–144, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilles Fauconnier</author>
</authors>
<title>Polarity and the scale principle.</title>
<date>1975</date>
<booktitle>In Proceedings of the Chicago Linguistic Society (CLS),</booktitle>
<pages>188--199</pages>
<editor>Javier Gutierrez-Rexach (ed.),</editor>
<note>Reprinted in</note>
<contexts>
<context position="8079" citStr="[7, 13]" startWordPosition="1270" endWordPosition="1271">ions can be found in a version of this paper containing appendices, available at www.cs.cornell.edu/˜cristian/acl2010/. 2 DLD09: successes and challenges In this section, we briefly summarize those aspects of the DLD09 method that are important to understanding how our new co-learning method works. DE operators and NPIs Acquiring DE operators is challenging because of the complete lack of annotated data. DLD09’s insight was to make use of negative polarity items (NPIs), which are words or phrases that tend to occur only in negative contexts. The reason they did so is that Ladusaw’s hypothesis [7, 13] asserts that NPIs only occur within the scope of DE operators. Figure 1 depicts examples involving the English NPIs ‘any’5 and ‘have a clue’ (in the idiomatic sense) that illustrate this relationship. Some other English NPIs are ‘ever’, ‘yet’ and ‘give a damn’. Thus, NPIs can be treated as clues that a DE operator might be present (although DE operators may also occur without NPIs). 4We explored three different edge-weighting schemes based on co-occurrence frequencies and seed-set membership, but the results were extremely poor; HITS invariably retrieved very frequent words. 5The free-choice </context>
</contexts>
<marker>[7]</marker>
<rawString>Gilles Fauconnier. Polarity and the scale principle. In Proceedings of the Chicago Linguistic Society (CLS), pages 188–199, 1975. Reprinted in Javier Gutierrez-Rexach (ed.), Semantics: Critical Concepts in Linguistics, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Geurts</author>
<author>Frans van der Slik</author>
</authors>
<title>Monotonicity and processing load.</title>
<date>2005</date>
<journal>Journal of Semantics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="4216" citStr="[8]" startWordPosition="633" endWordPosition="633"> ‘They are reluctant to tango’ gS (.&lt;--) ‘They are reluctant to dance’; ‘Police doubt Smith threatened Jones’ gS (.&lt;--) ‘Police doubt Smith threatened Jones or Brown’; ‘You are allowed to use Mastercard’ gS (.&lt;--) ‘You are allowed to use any credit card’. 2The exception [2] employs the list automatically derived by Danescu-Niculescu-Mizil, Lee, and Ducott [5], described later. 247 Proceedings of the ACL 2010 Conference Short Papers, pages 247–252, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics greater cognitive load than inferences in the opposite direction [8]. Most NLP systems for the applications mentioned above have only been deployed for a small subset of languages. A key factor is the lack of relevant resources for other languages. While one approach would be to separately develop a method to acquire such resources for each language individually, we instead aim to ameliorate the resource-scarcity problem in the case of DE operators wholesale: we propose a single unsupervised method that can extract DE operators in any language for which raw text corpora exist. Overview of our work Our approach takes the English-centric work of Danescu-Niculesc</context>
</contexts>
<marker>[8]</marker>
<rawString>Bart Geurts and Frans van der Slik. Monotonicity and processing load. Journal of Semantics, 22(1):97–117, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Haspelmath</author>
</authors>
<title>Indefinite Pronouns.</title>
<date>2001</date>
<publisher>Oxford University Press,</publisher>
<contexts>
<context position="18387" citStr="[9]" startWordPosition="3016" endWordPosition="3016">ngly, the iterative addition of pNPIs has very little effect: the precisions at k are good at the beginning and stay about the same across iterations (for details see figure 5 in in the externally-available appendices). Thus, on English, co-learning does not hurt performance, which is good news; but unlike in Romanian, it does not lead to improvements. Why is English ‘any’ seemingly so “powerful”, in contrast to Romanian, where iterating beyond the initial ‘any’ translations leads to better results? Interestingly, findings from linguistic typology may shed some light on this issue. Haspelmath [9] compares the functions of indefinite pronouns in 40 languages. He shows that English is one of the minority of languages (11 out of 40)9 in which there exists an indefinite pronoun series that occurs in all (Haspelmath’s) classes of DE contexts, and thus can constitute a sufficient seed on 9English, Ancash Quechua, Basque, Catalan, French, Hindi/Urdu, Irish, Portuguese, Swahili, Swedish, Turkish. its own. In the other languages (including Romanian),10 no indirect pronoun can serve as a sufficient seed. So, we expect our method to be viable for all languages; while the iterative discovery of p</context>
</contexts>
<marker>[9]</marker>
<rawString>Martin Haspelmath. Indefinite Pronouns. Oxford University Press, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Hoeksema</author>
</authors>
<title>Corpus study of negative polarity items. IV-V Jornades de corpus linguistics 1996-1997,</title>
<date>1997</date>
<note>http://odur.let.rug. nl/—hoeksema/docs/barcelona.html.</note>
<contexts>
<context position="6190" citStr="[10]" startWordPosition="957" endWordPosition="957">m is initialized with a very small seed set of NPIs (which we describe how to generate), and then iterates between (a) discovering a set of DE operators using a collection of pseudo-NPIs — a concept we introduce — and (b) using the newlyacquired DE operators to detect new pseudo-NPIs. Why this isn’t obvious Although the algorithmic idea sketched above seems quite simple, it is important to note that prior experiments in that direction have not proved fruitful. Preliminary work on learning (German) NPIs using a small list of simple known DE operators did not yield strong results [14]. Hoeksema [10] discusses why NPIs might be hard to learn from data.3 We circumvent this problem because we are not interested in learning NPIs per se; rather, for our pur3In fact, humans can have trouble agreeing on NPI-hood; for instance, Lichte and Soehn [14] mention doubts about over half of K¨urschner [12]’s 344 manually collected German NPIs. poses, pseudo-NPIs suffice. Also, our preliminary work determined that one of the most famous co-learning algorithms, hubs and authorities or HITS [11], is poorly suited to our problem.4 Contributions To begin with, we apply our algorithm to produce the first larg</context>
</contexts>
<marker>[10]</marker>
<rawString>Jack Hoeksema. Corpus study of negative polarity items. IV-V Jornades de corpus linguistics 1996-1997, 1997. http://odur.let.rug. nl/—hoeksema/docs/barcelona.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1998</date>
<journal>Journal of the ACM,</journal>
<booktitle>In Proceedings of the 9th ACM-SIAM Symposium on Discrete Algorithms (SODA),</booktitle>
<volume>46</volume>
<pages>668--677</pages>
<contexts>
<context position="6677" citStr="[11]" startWordPosition="1039" endWordPosition="1039">arning (German) NPIs using a small list of simple known DE operators did not yield strong results [14]. Hoeksema [10] discusses why NPIs might be hard to learn from data.3 We circumvent this problem because we are not interested in learning NPIs per se; rather, for our pur3In fact, humans can have trouble agreeing on NPI-hood; for instance, Lichte and Soehn [14] mention doubts about over half of K¨urschner [12]’s 344 manually collected German NPIs. poses, pseudo-NPIs suffice. Also, our preliminary work determined that one of the most famous co-learning algorithms, hubs and authorities or HITS [11], is poorly suited to our problem.4 Contributions To begin with, we apply our algorithm to produce the first large list of DE operators for a language other than English. In our case study on Romanian (§4), we achieve quite high precisions at k (for example, iteration achieves a precision at 30 of 87%). Auxiliary experiments explore the effects of using a large but noisy NPI list, should one be available for the language in question. Intriguingly, we find that co-learning new pseudo-NPIs provides better results. Finally (§5), we engage in some cross-linguistic analysis based on the results of </context>
</contexts>
<marker>[11]</marker>
<rawString>Jon Kleinberg. Authoritative sources in a hyperlinked environment. In Proceedings of the 9th ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 668–677, 1998. Extended version in Journal of the ACM, 46:604–632, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilfried Kirschner</author>
</authors>
<title>Studien zur Negation im Deutschen.</title>
<date>1983</date>
<location>Narr,</location>
<contexts>
<context position="6487" citStr="[12]" startWordPosition="1010" endWordPosition="1010">n’t obvious Although the algorithmic idea sketched above seems quite simple, it is important to note that prior experiments in that direction have not proved fruitful. Preliminary work on learning (German) NPIs using a small list of simple known DE operators did not yield strong results [14]. Hoeksema [10] discusses why NPIs might be hard to learn from data.3 We circumvent this problem because we are not interested in learning NPIs per se; rather, for our pur3In fact, humans can have trouble agreeing on NPI-hood; for instance, Lichte and Soehn [14] mention doubts about over half of K¨urschner [12]’s 344 manually collected German NPIs. poses, pseudo-NPIs suffice. Also, our preliminary work determined that one of the most famous co-learning algorithms, hubs and authorities or HITS [11], is poorly suited to our problem.4 Contributions To begin with, we apply our algorithm to produce the first large list of DE operators for a language other than English. In our case study on Romanian (§4), we achieve quite high precisions at k (for example, iteration achieves a precision at 30 of 87%). Auxiliary experiments explore the effects of using a large but noisy NPI list, should one be available fo</context>
</contexts>
<marker>[12]</marker>
<rawString>Wilfried Kirschner. Studien zur Negation im Deutschen. Narr, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Ladusaw</author>
</authors>
<title>Polarity Sensitivity as Inherent Scope Relations.</title>
<date>1980</date>
<publisher>Garland Press,</publisher>
<location>New York,</location>
<note>Ph.D. thesis date</note>
<contexts>
<context position="8079" citStr="[7, 13]" startWordPosition="1270" endWordPosition="1271">ions can be found in a version of this paper containing appendices, available at www.cs.cornell.edu/˜cristian/acl2010/. 2 DLD09: successes and challenges In this section, we briefly summarize those aspects of the DLD09 method that are important to understanding how our new co-learning method works. DE operators and NPIs Acquiring DE operators is challenging because of the complete lack of annotated data. DLD09’s insight was to make use of negative polarity items (NPIs), which are words or phrases that tend to occur only in negative contexts. The reason they did so is that Ladusaw’s hypothesis [7, 13] asserts that NPIs only occur within the scope of DE operators. Figure 1 depicts examples involving the English NPIs ‘any’5 and ‘have a clue’ (in the idiomatic sense) that illustrate this relationship. Some other English NPIs are ‘ever’, ‘yet’ and ‘give a damn’. Thus, NPIs can be treated as clues that a DE operator might be present (although DE operators may also occur without NPIs). 4We explored three different edge-weighting schemes based on co-occurrence frequencies and seed-set membership, but the results were extremely poor; HITS invariably retrieved very frequent words. 5The free-choice </context>
</contexts>
<marker>[13]</marker>
<rawString>William A. Ladusaw. Polarity Sensitivity as Inherent Scope Relations. Garland Press, New York, 1980. Ph.D. thesis date 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timm Lichte</author>
<author>Jan-Philipp Soehn</author>
</authors>
<title>The retrieval and classification of Negative Polarity Items using statistical profiles.</title>
<date>2007</date>
<booktitle>Linguistics in Search of its Evidential Base,</booktitle>
<pages>249--266</pages>
<editor>In Sam Featherston and Wolfgang Sternefeld, editors, Roots:</editor>
<contexts>
<context position="6175" citStr="[14]" startWordPosition="955" endWordPosition="955">h. Our algorithm is initialized with a very small seed set of NPIs (which we describe how to generate), and then iterates between (a) discovering a set of DE operators using a collection of pseudo-NPIs — a concept we introduce — and (b) using the newlyacquired DE operators to detect new pseudo-NPIs. Why this isn’t obvious Although the algorithmic idea sketched above seems quite simple, it is important to note that prior experiments in that direction have not proved fruitful. Preliminary work on learning (German) NPIs using a small list of simple known DE operators did not yield strong results [14]. Hoeksema [10] discusses why NPIs might be hard to learn from data.3 We circumvent this problem because we are not interested in learning NPIs per se; rather, for our pur3In fact, humans can have trouble agreeing on NPI-hood; for instance, Lichte and Soehn [14] mention doubts about over half of K¨urschner [12]’s 344 manually collected German NPIs. poses, pseudo-NPIs suffice. Also, our preliminary work determined that one of the most famous co-learning algorithms, hubs and authorities or HITS [11], is poorly suited to our problem.4 Contributions To begin with, we apply our algorithm to produce</context>
</contexts>
<marker>[14]</marker>
<rawString>Timm Lichte and Jan-Philipp Soehn. The retrieval and classification of Negative Polarity Items using statistical profiles. In Sam Featherston and Wolfgang Sternefeld, editors, Roots: Linguistics in Search of its Evidential Base, pages 249–266. Mouton de Gruyter, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Modeling semantic containment and exclusion in natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>521--528</pages>
<marker>[15]</marker>
<rawString>Bill MacCartney and Christopher D. Manning. Modeling semantic containment and exclusion in natural language inference. In Proceedings of COLING, pages 521–528, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rowan Nairn</author>
<author>Cleo Condoravdi</author>
<author>Lauri Karttunen</author>
</authors>
<title>Computing relative polarity for textual inference.</title>
<date>2006</date>
<booktitle>In Proceedings of Inference in Computational Semantics (ICoS),</booktitle>
<marker>[16]</marker>
<rawString>Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen. Computing relative polarity for textual inference. In Proceedings of Inference in Computational Semantics (ICoS), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Richter</author>
<author>Janina Rad´o</author>
<author>Manfred Sailer</author>
</authors>
<title>Negative polarity items: Corpus linguistics, semantics, and psycholinguistics: Day 2: Corpus linguistics. Tutorial slides: http://www.sfs.uni-tuebingen.de/∼fr/ esslli/08/byday/day2/day2-part1.pdf,</title>
<date>2008</date>
<contexts>
<context position="10318" citStr="[17]" startWordPosition="1645" endWordPosition="1645">oderately complete” list of English NPIs.8 The resultant rankings of candidate English DE operators were judged to be of high quality. The challenge in porting to other languages: cluelessness Can the unsupervised approach of DLD09 be successfully applied to languages other than English? Unfortunately, for most other languages, it does not seem that large, high-quality NPI lists are available. One might wonder whether one can circumvent the NPI-acquisition problem by simply translating a known English NPI list into the target language. However, NPI-hood need not be preserved under translation [17]. Thus, for most languages, we lack the critical clues that DLD09 depends on. 3 Getting a clue In this section, we develop an iterative colearning algorithm that can extract DE operators in the many languages where a high-quality NPI 6DLD09 policies: (a) “NPI context” was defined as the part of the sentence to the left of the NPI up to the first comma, semi-colon or beginning of sentence; (b) to encourage the discovery of new DE operators, those sentences containing one of a list of 10 well-known DE operators were discarded. For Romanian, we treated only negations (‘nu’ and ‘n-’) and questions</context>
</contexts>
<marker>[17]</marker>
<rawString>Frank Richter, Janina Rad´o, and Manfred Sailer. Negative polarity items: Corpus linguistics, semantics, and psycholinguistics: Day 2: Corpus linguistics. Tutorial slides: http://www.sfs.uni-tuebingen.de/∼fr/ esslli/08/byday/day2/day2-part1.pdf, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor S´anchez Valencia</author>
</authors>
<title>Studies on natural logic and categorial grammar.</title>
<date>1991</date>
<tech>PhD thesis,</tech>
<institution>University of Amsterdam,</institution>
<marker>[18]</marker>
<rawString>Victor S´anchez Valencia. Studies on natural logic and categorial grammar. PhD thesis, University of Amsterdam, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan van Benthem</author>
</authors>
<title>Essays in Logical Semantics.</title>
<date>1986</date>
<location>Reidel, Dordrecht,</location>
<marker>[19]</marker>
<rawString>Johan van Benthem. Essays in Logical Semantics. Reidel, Dordrecht, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ton van der Wouden</author>
</authors>
<title>Negative contexts: Collocation, polarity and multiple negation.</title>
<date>1997</date>
<location>Routledge,</location>
<marker>[20]</marker>
<rawString>Ton van der Wouden. Negative contexts: Collocation, polarity and multiple negation. Routledge, 1997.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>