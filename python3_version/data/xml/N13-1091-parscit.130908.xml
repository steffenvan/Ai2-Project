<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000669">
<title confidence="0.990376">
TruthTeller: Annotating Predicate Truth
</title>
<author confidence="0.997916">
Amnon Lotan Asher Stern and Ido Dagan
</author>
<affiliation confidence="0.983646">
Department of Linguistics Department of Computer Science
Tel Aviv University Bar Ilan University
</affiliation>
<email confidence="0.990914">
amnonlot@post.tau.ac.il astern7@gmail.com dagan@cs.biu.ac.il
</email>
<sectionHeader confidence="0.995514" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958076923077">
We propose a novel semantic anno-
tation type of assigning truth values
to predicate occurrences, and present
TRUTHTELLER, a standalone publicly-
available tool that produces such annota-
tions. TRUTHTELLER integrates a range
of semantic phenomena, such as nega-
tion, modality, presupposition, implicativ-
ity, and more, which were dealt only partly
in previous works. Empirical evaluations
against human annotations show satisfac-
tory results and suggest the usefulness of
this new type of tool for NLP.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976882352941">
In a text, the action or relation denoted by ev-
ery predicate can be seen as being either pos-
itively or negatively inferred from its sentence,
or otherwise having an unknown truth status.
Only in (3) below can we infer that Gal sold
her shop, hence the positive truth value of the
predicate sell, while according to (2) and (4) Gal
did not sell it, hence the negative truth values,
and in (1) we do not know if she sold it or not
(the notations PT+, PT- and PT? denote truth
states, defined in Subsection 2.3). Identifying
these predicate truth values is an important sub-
task within many semantic processing scenarios,
including various applications such as Question
Answering (QA), Information Extraction (IE),
paraphrasing and summarization. The follow-
ing examples illustrate the phenomenon:
</bodyText>
<listItem confidence="0.989143625">
(1) Gal made an attempt pt+ to sell pt? her
shop.
(2) Gal did not try pt− to sell pt− her shop af-
ter hearing pt+ the offers.
(3) Maybe Gal wasn’t smart pt? to sell pt+ her
shop.
(4) Gal wasn’t smart pt− enough to sell pt− the
shop that she had bought pt+.
</listItem>
<bodyText confidence="0.999328666666667">
Previous works addressed specific aspects of
the truth detection problem: Nairn et al.
(2006), and later MacCartney &amp; Manning (2007;
2009), were the first to build paraphrasing and
inference systems that combine negation (see try
in (2)), modality (smart in (3)) and “natural
logic”, a recursive truth value calculus (sell in
(1-3)); recently, Mausam et al. (2012) built an
open IE system that identifies granulated vari-
ants of modality and conditions on predicates
(smart in (3)); and Kiparsky &amp; Kiparsky (1970)
and Karttunen (1971; 2012) laid the ground
work for factive and implicative entailment cal-
culus (sell in (1-4)), as well as many generic
constructions of presupposition (hearing in (2)
is presupposed because it heads an adverbial
clause and bought in (4) heads a finite relative
clause), which, to our knowledge, have not yet
been implemented computationally. Notice in
the examples that presuppositions persist under
negation, in questions and if-clauses, while en-
tailments do not. In addition, there is a growing
research line of negation and modality detection.
See, for example, Morante &amp; Daelemans (2012).
</bodyText>
<page confidence="0.9897">
752
</page>
<subsectionHeader confidence="0.293534">
Proceedings of NAACL-HLT 2013, pages 752–757,
</subsectionHeader>
<bodyText confidence="0.983705181818182">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
We present TRUTHTELLERI, a novel algo-
rithm and system that identifies the truth value
of each predicate in a given sentence. It anno-
tates nodes in the text’s dependency parse-tree
via a combination of pattern-based annotation
rules and a recursive algorithm based on natu-
ral logic. In the course of computing truth value,
it also computes the implicativity/factivity sig-
nature of predicates, and their negation and
modality to a basic degree, both of which are
made available in the system output. It ad-
dresses and combines the aforementioned phe-
nomena (see Section 2), many of which weren’t
dealt in previous systems.
TRUTHTELLER is an open source and pub-
licly available annotation tool, offers a relatively
simple algebra for truth value computation, and
is accompanied by a publicly available lexicon
of over 1,700 implicative and factive predicates.
Also, we provide an intuitive GUI for viewing
and modifying the algorithm’s annotation rules.
</bodyText>
<subsectionHeader confidence="0.4948">
2 Annotation Types and Algorithm
</subsectionHeader>
<bodyText confidence="0.9984615">
This section summarizes the annotation algo-
rithm (a detailed report is available with the sys-
tem release). We perform the annotations over
dependency parse trees, generated according to
the Stanford Dependencies standard (de Marn-
effe and Manning, 2008). For all verbs, nouns
and adjectives in a sentence’s parse tree, we pro-
duce the following 4 annotation types, given in
the order they are calculated, as described in the
following subsections:
</bodyText>
<listItem confidence="0.998920454545455">
1. Predicate Implication Signature (sig) - de-
scribes the pattern by which the predi-
cate entails or presupposes its complements,
e.g., the verb refuse entails the negative of
its complements: Ed refused to pay entails
that Ed didn’t pay.
2. Negation and Uncertainty (NU) - indicates
whether the predicate is modified by an un-
certainty modifier like might, probably, etc.,
or whether it’s negated by no, never etc.
3. Clause-Truth (CT) - indicates whether the
</listItem>
<footnote confidence="0.93661">
1http://cs.biu.ac.il/—nlp/downloads/TruthTeller
</footnote>
<bodyText confidence="0.721301">
entire clause headed by the predicate is en-
tailed by the complete sentence
</bodyText>
<listItem confidence="0.686438">
4. Predicate Truth (PT) - indicates whether
the predicate itself is entailed by the sen-
tence, as defined below
</listItem>
<bodyText confidence="0.99999206060606">
Before presenting the detailed definitions and
descriptions below, we give a high-level descrip-
tion of TRUTHTELLER&apos;s algorithm, where each
step relies on the results of its predecessor: a)
every predicate in the parse tree is annotated
with a predicate implication signature, identi-
fied by lexicon lookup; b) NU annotations are
added, according to the presence of uncertainty
modifiers (maybe, might, etc.) and negation
modifies (not, never, etc.); c) predicates in cer-
tain presupposition constructions (e.g., adver-
bial clauses, WH arguments) are annotated with
positive CT values; d) the parse tree is depth-
first scanned, in order to compute both CT and
PT annotations by the recursive effects of fac-
tives and implicatives; e) in conjunction with
the previous step, relative clause constructions
are identified and annotated with CT and PT.
Except for steps a) and d), all of the pro-
cedure is implemented as an ordered sequence
of annotation rule applications. An annotation
rule is a dependency parse tree template, pos-
sibly including variables, which assigns certain
annotations to any parse tree node that matches
against it. Step a) is implemented with signa-
ture lexicon lookups, and step d) is an algorithm
implemented in code.
To illustrate this entire process, Figure 1
presents the annotation process of a sim-
ple sentence, step by step, resulting in
TRUTHTELLER&apos;s complete output, fully speci-
fied below. Most other examples in this paper
show only partial annotations for brevity.
</bodyText>
<subsectionHeader confidence="0.990901">
2.1 Predicate Implication Signature
</subsectionHeader>
<bodyText confidence="0.999968">
Our system marks the signature of each predi-
cate, as defined in Table 1. There, each signa-
ture has a left sign and a right sign. The left sign
determines the clause truth value of the pred-
icate&apos;s complements, when the predicate is in
positive contexts (e.g., not negated), while the
right sign applies in negative contexts (clause
</bodyText>
<page confidence="0.998749">
753
</page>
<note confidence="0.9649353">
# Sig Positive context example Negative context example
1 +/- Ed managed to escape ⇒ Ed escaped Ed didn’t manage to escape ⇒ Ed didn’t escape
2 +/? Ed was forced to sell ⇒ Ed sold Ed wasn’t forced to sell ⇒ no entailments
3 ?/- Ed was allowed to go ⇒ no entailments Ed wasn’t allowed to go ⇒ Ed didn’t go
4 -/+ Ed forgot to pay ⇒ Ed didn’t pay Ed didn’t forget to pay ⇒ Ed paid
5 -/? Ed refused to fight ⇒ Ed didn’t fight Ed didn’t refuse to fight ⇒ no entailments
6 ?/+ Ed hesitated to ask ⇒ no entailments Ed didn’t hesitate to ask ⇒ Ed asked
7 +/+ Ed was glad to come ⇒ Ed came Ed wasn’t glad to come ⇒ Ed came
8 -/- Ed pretended to pay ⇒ Ed didn’t pay Ed didn’t pretend to pay ⇒ Ed didn’t pay
9 ?/? Ed wanted to fly ⇒ no entailments Ed didn’t want to fly ⇒ no entailments
</note>
<tableCaption confidence="0.993396">
Table 1: Implication signatures, based on MacCartney &amp; Manning (2009) and Karttunen (2012). The first
six signatures are named implicatives, and the last three factive, counter factive and regular, respectively.
</tableCaption>
<figure confidence="0.900715461538461">
a) Annotate signatures via lexicons lookup
Gal wasn’t allowed?/− to come?/?
b) Annotate NU
Gal wasn’t allowed?/−,nu− to come?/?,nu+
c) Annotate CT to presupposition constructions
Gal wasn’t allowed?/−,nu−,ct+ to come?/? ,nu+,ct+
d) Recursive CT and PT annotation
Gal wasn’t allowed?/−,nu−,ct+,pt− to
come?/?,nu+,ct−,pt−
e) Annotate CT and PT of relative clauses
(has no effect on this example)
Gal wasn’t allowed?/−,nu−,ct+,pt− to
come?/?,nu+,ct−,pt−
</figure>
<figureCaption confidence="0.999737">
Figure 1: An illustration of the annotation process
</figureCaption>
<bodyText confidence="0.998487954545455">
truth is defined in Subsection 2.3). See exam-
ples for both context types in the table. Each
sign can be either + (positive), - (negative) or
? (unknown). The unknown sign signifies that
the predicate does not entail its complements in
any way.
Signatures are identified via lookup, using two
lexicons, one for single-word predicates and the
other for verb+noun phrasal verbs, e.g., take the
time to X. Our single-word lexicon is similar to
those used in (Nairn et al., 2006) and (Bar-Haim
et al., 2007), but is far greater, holding over
1,700 entries, while each of the previous two has,
to the best of our knowledge, less than 300 en-
tries. It was built semi automatically, out of
a kernel of 320 manually inspected predicates,
which was then expanded with WordNet syn-
onyms (Fellbaum, 1998). The second lexicon
is the implicative phrasal verb lexicon of Kart-
tunen (2012), adapted into our framework. The
+/? implicative serves as the default signature
for all unlisted predicates.
</bodyText>
<listItem confidence="0.90875475">
Signature is also sensitive to the type of the
complement. Consider:
(6) Ed forgot−/+ to call pt− Joe
(7) Ed forgot+/+ that he called pt+ Joe
</listItem>
<bodyText confidence="0.992988285714286">
Therefore, signatures are specified separately for
finite and non finite complements of each pred-
icate.
After the initial signature lookup, two anno-
tation rules correct the signatures of +/+ fac-
tives modified by enough and too, into +/- and
-/+, correspondingly, see Kiparsky &amp; Kiparsky
</bodyText>
<listItem confidence="0.855449">
(1970). Compare:
(8) Ed was mad+/+ to go ⇒ Ed went
(9) Ed was too mad−/+ to go ⇒ Ed didn’t go
</listItem>
<bodyText confidence="0.935943">
In addition, we observed, like Karttunen (2012),
that most verbs that have passive voice and the
into preposition become +/? implicatives, e.g.,
</bodyText>
<listItem confidence="0.907444">
(10) Workers were pushed / maddened /
managed+/&apos; into signing ⇒ They signed
(11) Workers weren’t pushed / maddened /
managed+/&apos; into signing ⇒ It is unknown
whether they signed
</listItem>
<bodyText confidence="0.885313">
so we captured this construction in another rule.
</bodyText>
<page confidence="0.99254">
754
</page>
<subsectionHeader confidence="0.980109">
2.2 Negation and Uncertainty (NU)
</subsectionHeader>
<bodyText confidence="0.988670818181818">
NU takes the values {NU+, NU-, NU?}, stand-
ing for non-negated certain actions, negated cer-
tain actions, and uncertain actions. The first
NU rules match against a closed set of negation
modifiers around the predicate, like not, never,
neither etc. (see (2)), while later rules detect
uncertainty modifiers, like maybe, probably, etc.
Therefore, NU? takes precedence over NU-.
Many constructions of subject-negation,
object-negation and “double negation” are
accounted for in our rules, as in:
</bodyText>
<listItem confidence="0.985904">
(12) Nobody was seennu− at the site
(13) Almost nobody was seennu+ at the site
</listItem>
<subsectionHeader confidence="0.958671">
2.3 Clause Truth and Predicate Truth
</subsectionHeader>
<bodyText confidence="0.998074545454546">
Clause Truth (CT, denoted as ct(p)) corre-
sponds to POLARITY of Nairn et al. (2006). It
represents whether the clause headed by a pred-
icate p is entailed by the sentence, contradicted
or unknown, and thus takes three values {CT+,
CT-, CT?}.
Predicate Truth (PT) (denoted as pt(p)) rep-
resents whether we can infer from the sentence
that the action described by the predicate hap-
pened (or that its relation holds). It is defined
as the binary product of NU and CT:
</bodyText>
<equation confidence="0.589445">
Definition 1. PT = NU · CT
</equation>
<bodyText confidence="0.9996834">
and takes analogous values: {PT+, PT-, PT?}.
Intuitively, the product of two identical posi-
tive/negative values yields PT+, a positive and a
negative yield PT-, and NU? or CT? always yield
PT?. To illustrate these definitions, consider:
</bodyText>
<listItem confidence="0.978606666666667">
(14) Meg may have sleptct+,pt? after
eatingct+,pt+ the meal Ed cookedct+,pt+,
while no one was therect+,pt−
</listItem>
<bodyText confidence="0.987812625">
After signatures and NU are annotated, CT
and PT are calculated. At first, we apply
a set of rules that annotate generic presup-
position constructions with CT+. These in-
clude adverbial clauses opening with {while, be-
fore, after, where, how come, because, since,
owing to, though, despite, yet, therefore...},
WH arguments (who, which, whom, what), and
</bodyText>
<figureCaption confidence="0.998403">
Figure 2: Formula of ct(p), for any predicate p.
ct(gov(p)) is the CT of p&apos;s governing predicate.
</figureCaption>
<bodyText confidence="0.99917412">
parataxis2. See for example the effects of after
and while in (14).
Then, we apply the following recursive se-
quential procedure. The tree root always gets
CT+ (see slept in (14)). The tree is then scanned
downwards, predicate by predicate. At each one,
we compute CT by the formula in Figure 2, as
follows. First, we check if one of the aforemen-
tioned presupposition rules already matched the
node. Second, if none matched, we apply to the
node’s entire subtree another set of rules that
annotate each relative clause with the CT of its
governing noun3, ct(gov(p)) (see failed in (15)).
Third, if no previous rule matched, and p is a
complement of another predicate gov(p), then
compCT(p) is calculated, by the following logic:
when pt(gov(p)) is PT+ or PT-, the correspond-
ing left or right sign of sig(gov(p)) is copied.
Otherwise, if pt(gov(p)) = PT?, CT? is returned,
except when the signature of gov(p) is +/+ (or
-/-) factive, which always yields CT+ (or CT-).
Third, if nothing applied to p, CT? is returned
by default. Finally, PT is set, according to Def-
inition 1.
To illustrate, consider these annotations:
</bodyText>
<listItem confidence="0.907574">
(15) Gal managed+/−,ct+,pt+ a
building+/?,ct+,pt+, which Ginger
failed−/+,ct+,pt+ to sell+/?,ct−,pt−
</listItem>
<bodyText confidence="0.74300125">
First, managed gets CT+ as the tree root. Then,
we get compCT (building) = CT+, as the com-
plement of managed+/−,pt+. Next, a relative
clause rule copies CT+ from building to failed.
</bodyText>
<footnote confidence="0.983984666666667">
2The placing of clauses or phrases one after another,
without words to indicate coordination, as in “veni, vidi,
vici” in contrast to “veni, vidi and vici”.
3 W also annotate nouns and adjectives as predicates
in copular constructions, and in instances where nouns
have complements.
</footnote>
<figure confidence="0.8065286">
ct(p) =
{
compCT(p) :
a complement
CT? : otherwise (default)
CT+ : p was already annotated
by a presupposition rule
p heads a relative
ct(gov(p)) : clause
otherwise, and p is
</figure>
<page confidence="0.996482">
755
</page>
<bodyText confidence="0.9989315">
Finally, compCT(sell) = CT- is calculated, as
the complement of failed−/+,Pt+.
</bodyText>
<sectionHeader confidence="0.994563" genericHeader="introduction">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999982743589744">
To evaluate TRUTHTELLER&apos;s accuracy, we sam-
pled 25 sentences from each of the RTE5 and
RTE6 Test datasets (Bentivogli et al., 2009;
Bentivogli et al., 2010), widely used for textual
inference benchmarks. In these 50 sentences, we
manually annotated each predicate, 153 in to-
tal, forming a gold standard. As baseline, we
report the most frequent value for each annota-
tion. The results, in Table 2, show high accuracy
for all types, reducing the baseline CT and PT
errors by half. Furthermore, most of the remain-
ing errors were due to parser errors, according
to a manual error analysis we conducted.
The baseline for NU annotations shows that
negations are scarce in these RTE datasets,
which was also the case for CT- and PT- an-
notations. Thus, Table 2 mostly indicates
TruthTeller&apos;s performance in distinguishing pos-
itive CT and PT annotations from unknown
ones, the latter constituting —20% of the gold
standard. To further assess CT- and PT- annota-
tions we performed two targeted measurements.
Precision for CT- and PT- was measured by man-
ually judging the correctness of such annotations
by TRUTHTELLER, on a sample from RTE6 Test
including 50 CT- and 124 PT- annotations. This
test yielded 78% and 83% precision, respectively.
PT- is more frequent as it is typically triggered
by CT-, as well as by other constructions involv-
ing negation. Recall was estimated by employ-
ing a human annotator to go through the dataset
and look for CT- and PT- gold standard anno-
tations. The annotator identified 40 “CT-”s and
50 “PT-”s, out of which TRUTHTELLER found
47.5% of the “CT-”s and 74% of the “PT-”s. In
summary, TRUTHTELLER&apos;s performance on our
target PT annotations is quite satisfactory with
89% accuracy overall, having 83% precision and
74% recall estimates specifically for PT-.
</bodyText>
<sectionHeader confidence="0.999115" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999231">
We have presented TRUTHTELLER, a novel algo-
rithm and system that identifies truth values of
</bodyText>
<table confidence="0.999477">
Annotation TruthTeller Baseline
Signature 89.5% 81% (+/?)
NU 98% 97.3% (NU+)
CT 90.8% 78.4% (CT+)
PT 89% 77% (PT+)
</table>
<tableCaption confidence="0.9739505">
Table 2: The accuracy measures for
TRUTHTELLER&apos;s 4 annotations. The right col-
umn gives the accuracy for the corresponding
most-frequent baseline: {+/?, NU+, CT+, PT+}.
</tableCaption>
<bodyText confidence="0.99993812">
predicates, the first such system to a) address or
combine a wide variety of relevant grammatical
constructions; b) be an open source annotation
tool; c) address the truth value annotation task
as an independent tool, which makes it possible
for client systems to use its output, while pre-
vious works only embedded annotations in their
task-specific systems; and d) annotate unknown
truth values extensively and explicitly.
TRUTHTELLER may be used for several pur-
poses, such as inferring parts of a sentence
from the whole and improving textual entail-
ment (and contradiction) detection. It includes
a novel, large and accurate, lexicon of predicate
implication signatures.
While in this paper we evaluated the correct-
ness of TRUTHTELLER as an individual com-
ponent, in the future we propose integrating
it in a state-of-the-art RTE system and report
its impact. One challenge in this scenario is
having other system components interact with
TRUTHTELLER&apos;s decisions, possibly masking its
effects. In addition, we plan to incorporate
monotonicity calculations in the annotation pro-
cess, like in MacCartney and Manning (2009).
</bodyText>
<sectionHeader confidence="0.998888" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998984">
This work was partially supported by the Israel
Science Foundation grant 1112/08 and the Eu-
ropean Community’s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
We thank Roni Katzir and Fred Landman for
useful discussions.
</bodyText>
<page confidence="0.997805">
756
</page>
<sectionHeader confidence="0.995875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989968627451">
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI, pages
871–876.
Luisa Bentivogli, Bernardo Magnini, Ido Dagan,
Hoa Trang Dang, and Danilo Giampiccolo. 2009.
The fifth pascal recognizing textual entailment
challenge. In Preproceedings of the Text Analysis
Conference (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T.
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment chal-
lenge. In The Text Analysis Conference (TAC
2010).
Marie-Catherine de Marneffe and Christopher D.
Manning. 2008. The stanford typed dependencies
representation. In COLING Workshop on Cross-
framework and Cross-domain Parser Evaluation.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database (Language, Speech,
and Communication). MIT Press.
Lauri Karttunen. 1971. Implicative verbs. Lan-
guage, 47:340–358.
Lauri Karttunen. 2012. Simple and phrasal implica-
tives. In *SEM 2012, pages 124–131.
P. Kiparsky and C. Kiparsky. 1970. Fact.
In Progress in Linguistics, pages 143–173. The
Hague: Mouton de Gruyter.
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of ACL workshop on textual entailment and para-
phrasing.
Bill MacCartney and Christopher D. Manning. 2009.
An extended model of natural logic. In Proceed-
ings of the Eighth International Conference on
Computational Semantics (IWCS-8).
Mausam, Michael Schmitz, Stephen Soderland,
Robert Bart, and Oren Etzioni. 2012. Open lan-
guage learning for information extraction. In Pro-
ceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
523–534.
Roser Morante and Walter Daelemans. 2012. An-
notating modality and negation for a machine
reading evaluation. In CLEF (Online Working
Notes/Labs/Workshop).
Rowan Nairn, Cleo Condoravdi, and Lauri Kart-
tunen. 2006. Computing relative polarity for tex-
tual inference. In In Proceedings of ICoS-5 (Infer-
ence in Computational Semantics).
</reference>
<page confidence="0.997688">
757
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.672262">
<title confidence="0.999643">TruthTeller: Annotating Predicate Truth</title>
<author confidence="0.999927">Lotan Asher Stern Dagan</author>
<affiliation confidence="0.99607">Department of Linguistics Department of Computer Science Tel Aviv University Bar Ilan University</affiliation>
<email confidence="0.688216">amnonlot@post.tau.ac.ilastern7@gmail.comdagan@cs.biu.ac.il</email>
<abstract confidence="0.9990415">We propose a novel semantic annotation type of assigning truth values to predicate occurrences, and present a standalone publiclyavailable tool that produces such annotaa range of semantic phenomena, such as negation, modality, presupposition, implicativity, and more, which were dealt only partly in previous works. Empirical evaluations against human annotations show satisfactory results and suggest the usefulness of this new type of tool for NLP.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
</authors>
<title>Ido Dagan, Iddo Greental, and Eyal Shnarch.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>871--876</pages>
<marker>Bar-Haim, 2007</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal Shnarch. 2007. Semantic inference at the lexicalsyntactic level. In Proceedings of AAAI, pages 871–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>The fifth pascal recognizing textual entailment challenge.</title>
<date>2009</date>
<booktitle>In Preproceedings of the Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="14419" citStr="Bentivogli et al., 2009" startWordPosition="2359" endWordPosition="2362">t words to indicate coordination, as in “veni, vidi, vici” in contrast to “veni, vidi and vici”. 3 W also annotate nouns and adjectives as predicates in copular constructions, and in instances where nouns have complements. ct(p) = { compCT(p) : a complement CT? : otherwise (default) CT+ : p was already annotated by a presupposition rule p heads a relative ct(gov(p)) : clause otherwise, and p is 755 Finally, compCT(sell) = CT- is calculated, as the complement of failed−/+,Pt+. 3 Evaluation To evaluate TRUTHTELLER&apos;s accuracy, we sampled 25 sentences from each of the RTE5 and RTE6 Test datasets (Bentivogli et al., 2009; Bentivogli et al., 2010), widely used for textual inference benchmarks. In these 50 sentences, we manually annotated each predicate, 153 in total, forming a gold standard. As baseline, we report the most frequent value for each annotation. The results, in Table 2, show high accuracy for all types, reducing the baseline CT and PT errors by half. Furthermore, most of the remaining errors were due to parser errors, according to a manual error analysis we conducted. The baseline for NU annotations shows that negations are scarce in these RTE datasets, which was also the case for CT- and PT- anno</context>
</contexts>
<marker>Bentivogli, Magnini, Dagan, Dang, Giampiccolo, 2009</marker>
<rawString>Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The fifth pascal recognizing textual entailment challenge. In Preproceedings of the Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
<author>Ido Dagan</author>
<author>Hoa T Dang</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>The sixth PASCAL recognizing textual entailment challenge.</title>
<date>2010</date>
<booktitle>In The Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="14445" citStr="Bentivogli et al., 2010" startWordPosition="2363" endWordPosition="2366">ination, as in “veni, vidi, vici” in contrast to “veni, vidi and vici”. 3 W also annotate nouns and adjectives as predicates in copular constructions, and in instances where nouns have complements. ct(p) = { compCT(p) : a complement CT? : otherwise (default) CT+ : p was already annotated by a presupposition rule p heads a relative ct(gov(p)) : clause otherwise, and p is 755 Finally, compCT(sell) = CT- is calculated, as the complement of failed−/+,Pt+. 3 Evaluation To evaluate TRUTHTELLER&apos;s accuracy, we sampled 25 sentences from each of the RTE5 and RTE6 Test datasets (Bentivogli et al., 2009; Bentivogli et al., 2010), widely used for textual inference benchmarks. In these 50 sentences, we manually annotated each predicate, 153 in total, forming a gold standard. As baseline, we report the most frequent value for each annotation. The results, in Table 2, show high accuracy for all types, reducing the baseline CT and PT errors by half. Furthermore, most of the remaining errors were due to parser errors, according to a manual error analysis we conducted. The baseline for NU annotations shows that negations are scarce in these RTE datasets, which was also the case for CT- and PT- annotations. Thus, Table 2 mos</context>
</contexts>
<marker>Bentivogli, Clark, Dagan, Dang, Giampiccolo, 2010</marker>
<rawString>Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T. Dang, and Danilo Giampiccolo. 2010. The sixth PASCAL recognizing textual entailment challenge. In The Text Analysis Conference (TAC 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In COLING Workshop on Crossframework and Cross-domain Parser Evaluation.</booktitle>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The stanford typed dependencies representation. In COLING Workshop on Crossframework and Cross-domain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Implicative verbs.</title>
<date>1971</date>
<journal>Language,</journal>
<pages>47--340</pages>
<contexts>
<context position="2336" citStr="Karttunen (1971" startWordPosition="373" endWordPosition="374">hop. (4) Gal wasn’t smart pt− enough to sell pt− the shop that she had bought pt+. Previous works addressed specific aspects of the truth detection problem: Nairn et al. (2006), and later MacCartney &amp; Manning (2007; 2009), were the first to build paraphrasing and inference systems that combine negation (see try in (2)), modality (smart in (3)) and “natural logic”, a recursive truth value calculus (sell in (1-3)); recently, Mausam et al. (2012) built an open IE system that identifies granulated variants of modality and conditions on predicates (smart in (3)); and Kiparsky &amp; Kiparsky (1970) and Karttunen (1971; 2012) laid the ground work for factive and implicative entailment calculus (sell in (1-4)), as well as many generic constructions of presupposition (hearing in (2) is presupposed because it heads an adverbial clause and bought in (4) heads a finite relative clause), which, to our knowledge, have not yet been implemented computationally. Notice in the examples that presuppositions persist under negation, in questions and if-clauses, while entailments do not. In addition, there is a growing research line of negation and modality detection. See, for example, Morante &amp; Daelemans (2012). 752 Proc</context>
</contexts>
<marker>Karttunen, 1971</marker>
<rawString>Lauri Karttunen. 1971. Implicative verbs. Language, 47:340–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Simple and phrasal implicatives.</title>
<date>2012</date>
<booktitle>In *SEM 2012,</booktitle>
<pages>124--131</pages>
<contexts>
<context position="7907" citStr="Karttunen (2012)" startWordPosition="1295" endWordPosition="1296"> no entailments Ed wasn’t allowed to go ⇒ Ed didn’t go 4 -/+ Ed forgot to pay ⇒ Ed didn’t pay Ed didn’t forget to pay ⇒ Ed paid 5 -/? Ed refused to fight ⇒ Ed didn’t fight Ed didn’t refuse to fight ⇒ no entailments 6 ?/+ Ed hesitated to ask ⇒ no entailments Ed didn’t hesitate to ask ⇒ Ed asked 7 +/+ Ed was glad to come ⇒ Ed came Ed wasn’t glad to come ⇒ Ed came 8 -/- Ed pretended to pay ⇒ Ed didn’t pay Ed didn’t pretend to pay ⇒ Ed didn’t pay 9 ?/? Ed wanted to fly ⇒ no entailments Ed didn’t want to fly ⇒ no entailments Table 1: Implication signatures, based on MacCartney &amp; Manning (2009) and Karttunen (2012). The first six signatures are named implicatives, and the last three factive, counter factive and regular, respectively. a) Annotate signatures via lexicons lookup Gal wasn’t allowed?/− to come?/? b) Annotate NU Gal wasn’t allowed?/−,nu− to come?/?,nu+ c) Annotate CT to presupposition constructions Gal wasn’t allowed?/−,nu−,ct+ to come?/? ,nu+,ct+ d) Recursive CT and PT annotation Gal wasn’t allowed?/−,nu−,ct+,pt− to come?/?,nu+,ct−,pt− e) Annotate CT and PT of relative clauses (has no effect on this example) Gal wasn’t allowed?/−,nu−,ct+,pt− to come?/?,nu+,ct−,pt− Figure 1: An illustration o</context>
<context position="9402" citStr="Karttunen (2012)" startWordPosition="1529" endWordPosition="1531"> are identified via lookup, using two lexicons, one for single-word predicates and the other for verb+noun phrasal verbs, e.g., take the time to X. Our single-word lexicon is similar to those used in (Nairn et al., 2006) and (Bar-Haim et al., 2007), but is far greater, holding over 1,700 entries, while each of the previous two has, to the best of our knowledge, less than 300 entries. It was built semi automatically, out of a kernel of 320 manually inspected predicates, which was then expanded with WordNet synonyms (Fellbaum, 1998). The second lexicon is the implicative phrasal verb lexicon of Karttunen (2012), adapted into our framework. The +/? implicative serves as the default signature for all unlisted predicates. Signature is also sensitive to the type of the complement. Consider: (6) Ed forgot−/+ to call pt− Joe (7) Ed forgot+/+ that he called pt+ Joe Therefore, signatures are specified separately for finite and non finite complements of each predicate. After the initial signature lookup, two annotation rules correct the signatures of +/+ factives modified by enough and too, into +/- and -/+, correspondingly, see Kiparsky &amp; Kiparsky (1970). Compare: (8) Ed was mad+/+ to go ⇒ Ed went (9) Ed wa</context>
</contexts>
<marker>Karttunen, 2012</marker>
<rawString>Lauri Karttunen. 2012. Simple and phrasal implicatives. In *SEM 2012, pages 124–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kiparsky</author>
<author>C Kiparsky</author>
</authors>
<date>1970</date>
<booktitle>Fact. In Progress in Linguistics,</booktitle>
<pages>143--173</pages>
<note>The Hague: Mouton de Gruyter.</note>
<contexts>
<context position="2316" citStr="Kiparsky &amp; Kiparsky (1970)" startWordPosition="368" endWordPosition="371">n’t smart pt? to sell pt+ her shop. (4) Gal wasn’t smart pt− enough to sell pt− the shop that she had bought pt+. Previous works addressed specific aspects of the truth detection problem: Nairn et al. (2006), and later MacCartney &amp; Manning (2007; 2009), were the first to build paraphrasing and inference systems that combine negation (see try in (2)), modality (smart in (3)) and “natural logic”, a recursive truth value calculus (sell in (1-3)); recently, Mausam et al. (2012) built an open IE system that identifies granulated variants of modality and conditions on predicates (smart in (3)); and Kiparsky &amp; Kiparsky (1970) and Karttunen (1971; 2012) laid the ground work for factive and implicative entailment calculus (sell in (1-4)), as well as many generic constructions of presupposition (hearing in (2) is presupposed because it heads an adverbial clause and bought in (4) heads a finite relative clause), which, to our knowledge, have not yet been implemented computationally. Notice in the examples that presuppositions persist under negation, in questions and if-clauses, while entailments do not. In addition, there is a growing research line of negation and modality detection. See, for example, Morante &amp; Daelem</context>
<context position="9948" citStr="Kiparsky &amp; Kiparsky (1970)" startWordPosition="1616" endWordPosition="1619">. The second lexicon is the implicative phrasal verb lexicon of Karttunen (2012), adapted into our framework. The +/? implicative serves as the default signature for all unlisted predicates. Signature is also sensitive to the type of the complement. Consider: (6) Ed forgot−/+ to call pt− Joe (7) Ed forgot+/+ that he called pt+ Joe Therefore, signatures are specified separately for finite and non finite complements of each predicate. After the initial signature lookup, two annotation rules correct the signatures of +/+ factives modified by enough and too, into +/- and -/+, correspondingly, see Kiparsky &amp; Kiparsky (1970). Compare: (8) Ed was mad+/+ to go ⇒ Ed went (9) Ed was too mad−/+ to go ⇒ Ed didn’t go In addition, we observed, like Karttunen (2012), that most verbs that have passive voice and the into preposition become +/? implicatives, e.g., (10) Workers were pushed / maddened / managed+/&apos; into signing ⇒ They signed (11) Workers weren’t pushed / maddened / managed+/&apos; into signing ⇒ It is unknown whether they signed so we captured this construction in another rule. 754 2.2 Negation and Uncertainty (NU) NU takes the values {NU+, NU-, NU?}, standing for non-negated certain actions, negated certain actions</context>
</contexts>
<marker>Kiparsky, Kiparsky, 1970</marker>
<rawString>P. Kiparsky and C. Kiparsky. 1970. Fact. In Progress in Linguistics, pages 143–173. The Hague: Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Natural logic for textual inference.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL workshop on textual entailment and paraphrasing.</booktitle>
<contexts>
<context position="1935" citStr="MacCartney &amp; Manning (2007" startWordPosition="307" endWordPosition="310">an important subtask within many semantic processing scenarios, including various applications such as Question Answering (QA), Information Extraction (IE), paraphrasing and summarization. The following examples illustrate the phenomenon: (1) Gal made an attempt pt+ to sell pt? her shop. (2) Gal did not try pt− to sell pt− her shop after hearing pt+ the offers. (3) Maybe Gal wasn’t smart pt? to sell pt+ her shop. (4) Gal wasn’t smart pt− enough to sell pt− the shop that she had bought pt+. Previous works addressed specific aspects of the truth detection problem: Nairn et al. (2006), and later MacCartney &amp; Manning (2007; 2009), were the first to build paraphrasing and inference systems that combine negation (see try in (2)), modality (smart in (3)) and “natural logic”, a recursive truth value calculus (sell in (1-3)); recently, Mausam et al. (2012) built an open IE system that identifies granulated variants of modality and conditions on predicates (smart in (3)); and Kiparsky &amp; Kiparsky (1970) and Karttunen (1971; 2012) laid the ground work for factive and implicative entailment calculus (sell in (1-4)), as well as many generic constructions of presupposition (hearing in (2) is presupposed because it heads a</context>
</contexts>
<marker>MacCartney, Manning, 2007</marker>
<rawString>Bill MacCartney and Christopher D. Manning. 2007. Natural logic for textual inference. In Proceedings of ACL workshop on textual entailment and paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>An extended model of natural logic.</title>
<date>2009</date>
<booktitle>In Proceedings of the Eighth International Conference on Computational Semantics (IWCS-8).</booktitle>
<contexts>
<context position="7886" citStr="MacCartney &amp; Manning (2009)" startWordPosition="1290" endWordPosition="1293">nts 3 ?/- Ed was allowed to go ⇒ no entailments Ed wasn’t allowed to go ⇒ Ed didn’t go 4 -/+ Ed forgot to pay ⇒ Ed didn’t pay Ed didn’t forget to pay ⇒ Ed paid 5 -/? Ed refused to fight ⇒ Ed didn’t fight Ed didn’t refuse to fight ⇒ no entailments 6 ?/+ Ed hesitated to ask ⇒ no entailments Ed didn’t hesitate to ask ⇒ Ed asked 7 +/+ Ed was glad to come ⇒ Ed came Ed wasn’t glad to come ⇒ Ed came 8 -/- Ed pretended to pay ⇒ Ed didn’t pay Ed didn’t pretend to pay ⇒ Ed didn’t pay 9 ?/? Ed wanted to fly ⇒ no entailments Ed didn’t want to fly ⇒ no entailments Table 1: Implication signatures, based on MacCartney &amp; Manning (2009) and Karttunen (2012). The first six signatures are named implicatives, and the last three factive, counter factive and regular, respectively. a) Annotate signatures via lexicons lookup Gal wasn’t allowed?/− to come?/? b) Annotate NU Gal wasn’t allowed?/−,nu− to come?/?,nu+ c) Annotate CT to presupposition constructions Gal wasn’t allowed?/−,nu−,ct+ to come?/? ,nu+,ct+ d) Recursive CT and PT annotation Gal wasn’t allowed?/−,nu−,ct+,pt− to come?/?,nu+,ct−,pt− e) Annotate CT and PT of relative clauses (has no effect on this example) Gal wasn’t allowed?/−,nu−,ct+,pt− to come?/?,nu+,ct−,pt− Figure</context>
</contexts>
<marker>MacCartney, Manning, 2009</marker>
<rawString>Bill MacCartney and Christopher D. Manning. 2009. An extended model of natural logic. In Proceedings of the Eighth International Conference on Computational Semantics (IWCS-8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schmitz Mausam</author>
<author>Stephen Soderland</author>
<author>Robert Bart</author>
<author>Oren Etzioni</author>
</authors>
<title>Open language learning for information extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>523--534</pages>
<contexts>
<context position="2168" citStr="Mausam et al. (2012)" startWordPosition="344" endWordPosition="347">(1) Gal made an attempt pt+ to sell pt? her shop. (2) Gal did not try pt− to sell pt− her shop after hearing pt+ the offers. (3) Maybe Gal wasn’t smart pt? to sell pt+ her shop. (4) Gal wasn’t smart pt− enough to sell pt− the shop that she had bought pt+. Previous works addressed specific aspects of the truth detection problem: Nairn et al. (2006), and later MacCartney &amp; Manning (2007; 2009), were the first to build paraphrasing and inference systems that combine negation (see try in (2)), modality (smart in (3)) and “natural logic”, a recursive truth value calculus (sell in (1-3)); recently, Mausam et al. (2012) built an open IE system that identifies granulated variants of modality and conditions on predicates (smart in (3)); and Kiparsky &amp; Kiparsky (1970) and Karttunen (1971; 2012) laid the ground work for factive and implicative entailment calculus (sell in (1-4)), as well as many generic constructions of presupposition (hearing in (2) is presupposed because it heads an adverbial clause and bought in (4) heads a finite relative clause), which, to our knowledge, have not yet been implemented computationally. Notice in the examples that presuppositions persist under negation, in questions and if-cla</context>
</contexts>
<marker>Mausam, Soderland, Bart, Etzioni, 2012</marker>
<rawString>Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learning for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 523–534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>Annotating modality and negation for a machine reading evaluation.</title>
<date>2012</date>
<booktitle>In CLEF (Online Working Notes/Labs/Workshop).</booktitle>
<contexts>
<context position="2926" citStr="Morante &amp; Daelemans (2012)" startWordPosition="462" endWordPosition="465"> Kiparsky (1970) and Karttunen (1971; 2012) laid the ground work for factive and implicative entailment calculus (sell in (1-4)), as well as many generic constructions of presupposition (hearing in (2) is presupposed because it heads an adverbial clause and bought in (4) heads a finite relative clause), which, to our knowledge, have not yet been implemented computationally. Notice in the examples that presuppositions persist under negation, in questions and if-clauses, while entailments do not. In addition, there is a growing research line of negation and modality detection. See, for example, Morante &amp; Daelemans (2012). 752 Proceedings of NAACL-HLT 2013, pages 752–757, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics We present TRUTHTELLERI, a novel algorithm and system that identifies the truth value of each predicate in a given sentence. It annotates nodes in the text’s dependency parse-tree via a combination of pattern-based annotation rules and a recursive algorithm based on natural logic. In the course of computing truth value, it also computes the implicativity/factivity signature of predicates, and their negation and modality to a basic degree, both of which are made</context>
</contexts>
<marker>Morante, Daelemans, 2012</marker>
<rawString>Roser Morante and Walter Daelemans. 2012. Annotating modality and negation for a machine reading evaluation. In CLEF (Online Working Notes/Labs/Workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rowan Nairn</author>
<author>Cleo Condoravdi</author>
<author>Lauri Karttunen</author>
</authors>
<title>Computing relative polarity for textual inference. In</title>
<date>2006</date>
<booktitle>In Proceedings of ICoS-5 (Inference in Computational Semantics).</booktitle>
<contexts>
<context position="1897" citStr="Nairn et al. (2006)" startWordPosition="301" endWordPosition="304">hese predicate truth values is an important subtask within many semantic processing scenarios, including various applications such as Question Answering (QA), Information Extraction (IE), paraphrasing and summarization. The following examples illustrate the phenomenon: (1) Gal made an attempt pt+ to sell pt? her shop. (2) Gal did not try pt− to sell pt− her shop after hearing pt+ the offers. (3) Maybe Gal wasn’t smart pt? to sell pt+ her shop. (4) Gal wasn’t smart pt− enough to sell pt− the shop that she had bought pt+. Previous works addressed specific aspects of the truth detection problem: Nairn et al. (2006), and later MacCartney &amp; Manning (2007; 2009), were the first to build paraphrasing and inference systems that combine negation (see try in (2)), modality (smart in (3)) and “natural logic”, a recursive truth value calculus (sell in (1-3)); recently, Mausam et al. (2012) built an open IE system that identifies granulated variants of modality and conditions on predicates (smart in (3)); and Kiparsky &amp; Kiparsky (1970) and Karttunen (1971; 2012) laid the ground work for factive and implicative entailment calculus (sell in (1-4)), as well as many generic constructions of presupposition (hearing in</context>
<context position="9006" citStr="Nairn et al., 2006" startWordPosition="1460" endWordPosition="1463">ses (has no effect on this example) Gal wasn’t allowed?/−,nu−,ct+,pt− to come?/?,nu+,ct−,pt− Figure 1: An illustration of the annotation process truth is defined in Subsection 2.3). See examples for both context types in the table. Each sign can be either + (positive), - (negative) or ? (unknown). The unknown sign signifies that the predicate does not entail its complements in any way. Signatures are identified via lookup, using two lexicons, one for single-word predicates and the other for verb+noun phrasal verbs, e.g., take the time to X. Our single-word lexicon is similar to those used in (Nairn et al., 2006) and (Bar-Haim et al., 2007), but is far greater, holding over 1,700 entries, while each of the previous two has, to the best of our knowledge, less than 300 entries. It was built semi automatically, out of a kernel of 320 manually inspected predicates, which was then expanded with WordNet synonyms (Fellbaum, 1998). The second lexicon is the implicative phrasal verb lexicon of Karttunen (2012), adapted into our framework. The +/? implicative serves as the default signature for all unlisted predicates. Signature is also sensitive to the type of the complement. Consider: (6) Ed forgot−/+ to call</context>
<context position="11136" citStr="Nairn et al. (2006)" startWordPosition="1813" endWordPosition="1816">ctions, negated certain actions, and uncertain actions. The first NU rules match against a closed set of negation modifiers around the predicate, like not, never, neither etc. (see (2)), while later rules detect uncertainty modifiers, like maybe, probably, etc. Therefore, NU? takes precedence over NU-. Many constructions of subject-negation, object-negation and “double negation” are accounted for in our rules, as in: (12) Nobody was seennu− at the site (13) Almost nobody was seennu+ at the site 2.3 Clause Truth and Predicate Truth Clause Truth (CT, denoted as ct(p)) corresponds to POLARITY of Nairn et al. (2006). It represents whether the clause headed by a predicate p is entailed by the sentence, contradicted or unknown, and thus takes three values {CT+, CT-, CT?}. Predicate Truth (PT) (denoted as pt(p)) represents whether we can infer from the sentence that the action described by the predicate happened (or that its relation holds). It is defined as the binary product of NU and CT: Definition 1. PT = NU · CT and takes analogous values: {PT+, PT-, PT?}. Intuitively, the product of two identical positive/negative values yields PT+, a positive and a negative yield PT-, and NU? or CT? always yield PT?.</context>
</contexts>
<marker>Nairn, Condoravdi, Karttunen, 2006</marker>
<rawString>Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen. 2006. Computing relative polarity for textual inference. In In Proceedings of ICoS-5 (Inference in Computational Semantics).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>