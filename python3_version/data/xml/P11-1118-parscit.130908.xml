<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006527">
<title confidence="0.988701">
Disentangling Chat with Local Coherence Models
</title>
<author confidence="0.997855">
Micha Elsner Eugene Charniak
</author>
<affiliation confidence="0.997851">
School of Informatics Department of Computer Science
University of Edinburgh Brown University, Providence, RI 02912
</affiliation>
<email confidence="0.996186">
melsner0@gmail.com ec@cs.brown.edu
</email>
<sectionHeader confidence="0.996571" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924777777778">
We evaluate several popular models of local
discourse coherence for domain and task gen-
erality by applying them to chat disentangle-
ment. Using experiments on synthetic multi-
party conversations, we show that most mod-
els transfer well from text to dialogue. Co-
herence models improve results overall when
good parses and topic models are available,
and on a constrained task for real chat data.
</bodyText>
<sectionHeader confidence="0.997908" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989826145454546">
One property of a well-written document is coher-
ence, the way each sentence fits into its context— sen-
tences should be interpretable in light of what has
come before, and in turn make it possible to inter-
pret what comes after. Models of coherence have
primarily been used for text-based generation tasks:
ordering units of text for multidocument summariza-
tion or inserting new text into an existing article.
In general, the corpora used consist of informative
writing, and the tasks used for evaluation consider
different ways of reordering the same set of textual
units. But the theoretical concept of coherence goes
beyond both this domain and this task setting— and
so should coherence models.
This paper evaluates a variety of local coher-
ence models on the task of chat disentanglement or
threading: separating a transcript of a multiparty
interaction into independent conversations✶. Such
simultaneous conversations occur in internet chat
✶A public implementation is available via https://
bitbucket.org/melsner/browncoherence.
rooms, and on shared voice channels such as push-
to-talk radio. In these situations, a single, correctly
disentangled, conversational thread will be coherent,
since the speakers involved understand the normal
rules of discourse, but the transcript as a whole will
not be. Thus, a good model of coherence should be
able to disentangle sentences as well as order them.
There are several differences between disentan-
glement and the newswire sentence-ordering tasks
typically used to evaluate coherence models. Inter-
net chat comes from a different domain, one where
topics vary widely and no reliable syntactic annota-
tions are available. The disentanglement task mea-
sures different capabilities of a model, since it com-
pares documents that are not permuted versions of
one another. Finally, full disentanglement requires
a large-scale search, which is computationally dif-
ficult. We move toward disentanglement in stages,
carrying out a series of experiments to measure the
contribution of each of these factors.
As an intermediary between newswire and inter-
net chat, we adopt the SWITCHBOARD (SWBD) cor-
pus. SWBD contains recorded telephone conversa-
tions with known topics and hand-annotated parse
trees; this allows us to control for the performance
of our parser and other informational resources. To
compare the two algorithmic settings, we use SWBD
for ordering experiments, and also artificially entan-
gle pairs of telephone dialogues to create synthetic
transcripts which we can disentangle. Finally, we
present results on actual internet chat corpora.
On synthetic SWBD transcripts, local coherence
models improve performance considerably over our
baseline model, Elsner and Charniak (2008b). On
</bodyText>
<page confidence="0.975195">
1179
</page>
<note confidence="0.97949">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1179–1189,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999964571428571">
internet chat, we continue to do better on a con-
strained disentanglement task, though so far, we are
unable to apply these improvements to the full task.
We suspect that, with better low-level annotation
tools for the chat domain and a good way of integrat-
ing prior information, our improvements on SwBD
could transfer fully to IRC chat.
</bodyText>
<sectionHeader confidence="0.998445" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999977639344263">
There is extensive previous work on coherence mod-
els for text ordering; we describe several specific
models below, in section 2. This study focuses on
models of local coherence, which relate text to its
immediate context. There has also been work on
global coherence, the structure of a document as a
whole (Chen et al., 2009; Eisenstein and Barzilay,
2008; Barzilay and Lee, 2004), typically modeled
in terms of sequential topics. We avoid using them
here, because we do not believe topic sequences are
predictable in conversation and because such models
tend to be algorithmically cumbersome.
In addition to text ordering, local coherence mod-
els have also been used to score the fluency of texts
written by humans or produced by machine (Pitler
and Nenkova, 2008; Lapata, 2006; Miltsakaki and
Kukich, 2004). Like disentanglement, these tasks
provide an algorithmic setting that differs from or-
dering, and so can demonstrate previously unknown
weaknesses in models. However, the target genre is
still informative writing, so they reveal little about
cross-domain flexibility.
The task of disentanglement or threading for
internet chat was introduced by Shen et al. (2006).
Elsner and Charniak (2008b) created the publicly
available #LINUX corpus; the best published re-
sults on this corpus are those of Wang and Oard
(2009). These two studies use overlapping unigrams
to measure similarity between two sentences; Wang
and Oard (2009) use a message expansion tech-
nique to incorporate context beyond a single sen-
tence. Unigram overlaps are used to model coher-
ence, but more sophisticated methods using syntax
(Lapata and Barzilay, 2005) or lexical features (La-
pata, 2003) often outperform them on ordering tasks.
This study compares several of these methods with
Elsner and Charniak (2008b), which we use as a
baseline because there is a publicly available imple-
mentation✷.
Adams (2008) also created and released a disen-
tanglement corpus. They use LDA (Blei et al., 2001)
to discover latent topics in their corpus, then measur-
ing similarity by looking for shared topics. These
features fail to improve their performance, which is
puzzling in light of the success of topic modeling for
other coherence and segmentation problems (Eisen-
stein and Barzilay, 2008; Foltz et al., 1998). The
results of this study suggest that topic models can
help with disentanglement, but that it is difficult to
find useful topics for IRC chat.
A few studies have attempted to disentangle con-
versational speech (Aoki et al., 2003; Aoki et al.,
2006), mostly using temporal features. For the most
part, however, this research has focused on auditory
processing in the context of the cocktail party prob-
lem, the task of attending to a specific speaker in
a noisy room (Haykin and Chen, 2005). Utterance
content has some influence on what the listener per-
ceives, but only for extremely salient cues such as
the listener&apos;s name (Moray, 1959), so cocktail party
research does not typically use lexical models.
</bodyText>
<sectionHeader confidence="0.991061" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.999962923076923">
In this section, we briefly describe the models we in-
tend to evaluate. Most of them are drawn from pre-
vious work; one, the topical entity grid, is a novel
extension of the entity grid. For the experiments be-
low, we train the models on SwBD, sometimes aug-
mented with a larger set of automatically parsed con-
versations from the FISHER corpus. Since the two
corpora are quite similar, FISHER is a useful source
for extra data; McClosky et al. (2010) uses it for
this purpose in parsing experiments. (We continue
to use SwBD/FISHER even for experiments on IRC,
because we do not have enough disentangled train-
ing data to learn lexical relationships.)
</bodyText>
<subsectionHeader confidence="0.998233">
3.1 Entity grid
</subsectionHeader>
<bodyText confidence="0.996383666666667">
The entity grid (Lapata and Barzilay, 2005; Barzilay
and Lapata, 2005) is an attempt to model some prin-
ciples of Centering Theory (Grosz et al., 1995) in a
statistical manner. It represents a document in terms
of entities and their syntactic roles: subject (S), ob-
ject (O), other (X) and not present (-). In each new
</bodyText>
<footnote confidence="0.521198">
2cs.brown.edu/melsner
</footnote>
<page confidence="0.988568">
1180
</page>
<bodyText confidence="0.999985391304348">
utterance, the grid predicts the role in which each
entity will appear, given its history of roles in the
previous sentences, plus a salience feature counting
the total number of times the entity occurs. For in-
stance, for an entity which is the subject of sentence
1, the object of sentence 2, and occurs four times in
total, the grid predicts its role in sentence 3 accord-
ing to the conditional P(✁❥S, O, sal = 4).
As in previous work, we treat each noun in a doc-
ument as denoting a single entity, rather than using
a coreference technique to attempt to resolve them.
In our development experiments, we noticed that
coreferent nouns often occur farther apart in conver-
sation than in newswire, since they are frequently
referred to by pronouns and deictics in the interim.
Therefore, we extend the history to six previous ut-
terances. For robustness with this long history, we
model the conditional probabilities using multilabel
logistic regression rather than maximum likelihood.
This requires the assumption of a linear model, but
makes the estimator less vulnerable to overfitting
due to sparsity, increasing performance by about 2%
in development experiments.
</bodyText>
<subsectionHeader confidence="0.99949">
3.2 Topical entity grid
</subsectionHeader>
<bodyText confidence="0.977736105263158">
This model is a variant of the generative entity
grid, intended to take into account topical informa-
tion. To create the topical entity grid, we learn a
set of topic-to-word distributions for our corpus us-
ing LDA (Blei et al., 2001)3 with 200 latent top-
ics. This model embeds our vocabulary in a low-
dimensional space: we represent each word w as
the vector of topic probabilities p(ti❥w). We ex-
perimented with several ways to measure relation-
ships between words in this space, starting with the
standard cosine. However, the cosine can depend on
small variations in probability (for instance, if w has
most of its mass in dimension 1, then it is sensitive
to the exact weight of v for topic 1, even if this es-
sentially never happens).
To control for this tendency, we instead use the
magnitude of the dimension of greatest similarity:
sim(w, v) = maxi min(wi, vi)
To model coherence, we generalize the binary his-
</bodyText>
<footnote confidence="0.870292">
3www.cs.princeton.edu/blei/
topicmodeling.html
</footnote>
<bodyText confidence="0.999682125">
tory features of the standard entity grid, which de-
tect, for example, whether entity e is the subject of
the previous sentence. In the topical entity grid, we
instead compute a real-valued feature which sums
up the similarity between entity e and the subject(s)
of the previous sentence.
These features can detect a transition like: &amp;quot;The
House voted yesterday. The Senate will consider the
bill today.&amp;quot;. If &amp;quot;House&amp;quot; and &amp;quot;Senate&amp;quot; have a high
similarity, then the feature will have a high value,
predicting that &amp;quot;Senate&amp;quot; is a good subject for the cur-
rent sentence. As in the previous section, we learn
the conditional probabilities with logistic regression;
we train in parallel by splitting the data and averag-
ing (Mann et al., 2009). The topics are trained on
FISHER, and on NANC for news.
</bodyText>
<subsectionHeader confidence="0.973583">
3.3 IBM-1
</subsectionHeader>
<bodyText confidence="0.9999771">
The IBM translation model was first considered for
coherence by Soricut and Marcu (2006), although a
less probabilistically elegant version was proposed
earlier (Lapata, 2003). This model attempts to gen-
erate the content words of the next sentence by trans-
lating them from the words of the previous sentence,
plus a null word; thus, it will learn alignments be-
tween pairs of words that tend to occur in adjacent
sentences. We learn parameters on the FISHER cor-
pus, and on NANC for news.
</bodyText>
<subsectionHeader confidence="0.965211">
3.4 Pronouns
</subsectionHeader>
<bodyText confidence="0.999972466666667">
The use of a generative pronoun resolver for co-
herence modeling originates in Elsner and Char-
niak (2008a). That paper used a supervised model
(Ge et al., 1998), but we adapt a newer, unsuper-
vised model which they also make publicly available
(Charniak and Elsner, 2009)4. They model each pro-
noun as generated by an antecedent somewhere in
the previous two sentences. If a good antecedent is
found, the probability of the pronoun&apos;s occurrence
will be high; otherwise, the probability is low, sig-
naling that the text is less coherent because the pro-
noun is hard to interpret correctly.
We use the model as distributed for news text. For
conversation, we adapt it by running a few iterations
of their EM training algorithm on the FISHER data.
</bodyText>
<footnote confidence="0.4810785">
4bllip.cs.brown.edu/resources.shtml\
#software
</footnote>
<page confidence="0.971353">
1181
</page>
<subsectionHeader confidence="0.940681">
3.5 Discourse-newness
</subsectionHeader>
<bodyText confidence="0.999967454545455">
Building on work from summarization (Nenkova
and McKeown, 2003) and coreference resolution
(Poesio et al., 2005), Elsner and Charniak (2008a)
use a model which recognizes discourse-new versus
old NPs as a coherence model. For instance, the
model can learn that &amp;quot;President Barack Obama&amp;quot; is
a more likely first reference than &amp;quot;Obama&amp;quot;. Follow-
ing their work, we score discourse-newness with a
maximum-entropy classifier using syntactic features
counting different types of NP modifiers, and we use
NP head identity as a proxy for coreference.
</bodyText>
<subsectionHeader confidence="0.901696">
3.6 Chat-specific features
</subsectionHeader>
<bodyText confidence="0.999987783783784">
Most disentanglement models use non-linguistic in-
formation alongside lexical features; in fact, times-
tamps and speaker identities are usually better cues
than words are. We capture three essential non-
linguistic features using simple generative models.
The first feature is the time gap between one utter-
ance and the next within the same thread. Consistent
short gaps are a sign of normal turn-taking behavior;
long pauses do occur, but much more rarely (Aoki et
al., 2003). We round all time gaps to the nearest sec-
ond and model the distribution of time gaps using a
histogram, choosing bucket sizes adaptively so that
each bucket contains at least four datapoints.
The second feature is speaker identity; conver-
sations usually involve a small subset of the to-
tal number of speakers, and a few core speakers
make most of the utterances. We model the distri-
bution of speakers in each conversation using a Chi-
nese Restaurant Process (CRP) (Aldous, 1985) (tun-
ing the dispersion a to maximize development pe-
formance). The CRP&apos;s &amp;quot;rich-get-richer&amp;quot; dynamics
capture our intuitions, favoring conversations domi-
nated by a few vociferous speakers.
Finally, we model name mentioning. Speakers
in IRC chat often use their addressee&apos;s names to co-
ordinate the chat (O&apos;Neill and Martin, 2003), and
this is a powerful source of information (Elsner and
Charniak, 2008b). Our model classifies each utter-
ance into either the start or continuation of a conver-
sational turn, by checking if the previous utterance
had the same speaker. Given this status, it computes
probabilities for three outcomes: no name mention,
a mention of someone who has previously spoken
in the conversation, or a mention of someone else.
(The third option is extremely rare; this accounts
for most of the model&apos;s predictive power). We learn
these probabilities from IRC training data.
</bodyText>
<subsectionHeader confidence="0.993514">
3.7 Model combination
</subsectionHeader>
<bodyText confidence="0.917970916666667">
To combine these different models, we adopt the
log-linear framework of Soricut and Marcu (2006).
Here, each model Pi is assigned a weight Ai, and the
combined score P(d) is proportional to:
❳ Ailog(Pi(d))
i
The weights A can be learned discriminatively,
maximizing the probability of d relative to a task-
specific contrast set. For ordering experiments, the
contrast set is a single random permutation of d; we
explain the training regime for disentanglement be-
low, in subsection 4.1.
</bodyText>
<sectionHeader confidence="0.893995" genericHeader="method">
4 Comparing orderings of SWBD
</sectionHeader>
<bodyText confidence="0.999785730769231">
To measure the differences in performance caused
by moving from news to a conversational domain,
we first compare our models on an ordering task,
discrimination (Barzilay and Lapata, 2005; Karama-
nis et al., 2004). In this task, we take an original
document and randomly permute its sentences, cre-
ating an artificial incoherent document. We then test
to see if our model prefers the coherent original.
For SwBD, rather than compare permutations
of the individual utterances, we permute conversa-
tional turns (sets of consecutive utterances by each
speaker), since turns are natural discourse units in
conversation. We take documents numbered 2000
3999 as training/development and the remainder as
test, yielding 505 training and 153 test documents;
we evaluate 20 permutations per document. As a
comparison, we also show results for the same mod-
els on wSJ, using the train-test split from Elsner and
Charniak (2008a); the test set is sections 14-24, to-
talling 1004 documents.
Purandare and Litman (2008) carry out similar ex-
periments on distinguishing permuted SwBD doc-
uments, using lexical and WordNet features in a
model similar to Lapata (2003). Their accuracy for
this task (which they call &amp;quot;switch-hard&amp;quot;) is roughly
68%.
</bodyText>
<page confidence="0.976868">
1182
</page>
<table confidence="0.999733833333333">
WSJ SWBD
EGrid 76.4$ 86.0
Topical EGrid 71.8$ 70.9$
IBM-1 77.2$ 84.91
Pronouns 69.6$ 71.7$
Disc-new 72.3$ 55.0$
Combined 81.9 88.4
-EGrid 81.0 87.5
-Topical EGrid 82.2 90.5
-IBM-1 79.0$ 88.9
-Pronouns 81.3 88.5
-Disc-new 82.2 88.4
</table>
<tableCaption confidence="0.987116333333333">
Table 1: Discrimination F scores on news and dialogue.
③ indicates a significant difference from the combined
model at p=.01 and ② at p=.05.
</tableCaption>
<bodyText confidence="0.999977764705882">
In Table 1, we show the results for individual
models, for the combined model, and ablation re-
sults for mixtures without each component. WSJ is
more difficult than SWBD overall because, on av-
erage, news articles are shorter than SWBD con-
versations. Short documents are harder, because
permuting disrupts them less. The best SWBD re-
sult is 91%; the best WSJ result is 82% (both for
mixtures without the topical entity grid). The WSJ
result is state-of-the-art for the dataset, improving
slightly on Elsner and Charniak (2008a) at 81%. We
test results for significance using the non-parametric
Mann-Whitney U test.
Controlling for the fact that discrimination is eas-
ier on SWBD, most of the individual models perform
similarly in both corpora. The strongest models in
both cases are the entity grid and IBM-1 (at about
77% for news, 85% for dialogue). Pronouns and the
topical entity grid are weaker. The major outlier is
the discourse-new model, whose performance drops
from 72% for news to only 55%, just above chance,
for conversation.
The model combination results show that all the
models are quite closely correlated, since leaving
out any single model does not degrade the combi-
nation very much (only one of the ablations is sig-
nificantly worse than the combination). The most
critical in news is IBM-1 (decreasing performance
by 3% when removed); in conversation, it is the
entity grid (decreasing by about 1%). The topical
entity grid actually has a (nonsignificant) negative
impact on combined performance, implying that its
predictive power in this setting comes mainly from
information that other models also capture, but that
it is noisier and less reliable. In each domain, the
combined models outperform the best single model,
showing the information provided by the weaker
models is not completely redundant.
Overall, these results suggest that most previ-
ously proposed local coherence models are domain-
general; they work on conversation as well as
news. The exception is the discourse-newness
model, which benefits most from the specific con-
ventions of a written style. Full names with titles
(like &amp;quot;President Barack Obama&amp;quot;) are more common
in news, while conversation tends to involve fewer
completely unfamiliar entities and more cases of
bridging reference, in which grounding information
is given implicitly (Nissim, 2006). Due to its poor
performance, we omit the discourse-newness model
in our remaining experiments.
</bodyText>
<sectionHeader confidence="0.992308" genericHeader="method">
5 Disentangling SWBD
</sectionHeader>
<bodyText confidence="0.99995532">
We now turn to the task of disentanglement, test-
ing whether models that are good at ordering also
do well in this new setting. We would like to hold
the domain constant, but we do not have any disen-
tanglement data recorded from naturally occurring
speech, so we create synthetic instances by merging
pairs of SWBD dialogues. Doing so creates an arti-
ficial transcript in which two pairs of people appear
to be talking simultaneously over a shared channel.
The situation is somewhat contrived in that each
pair of speakers converses only with each other,
never breaking into the other pair&apos;s dialogue and
rarely using devices like name mentioning to make
it clear who they are addressing. Since this makes
speaker identity a perfect cue for disentanglement,
we do not use it in this section. The only chat-
specific model we use is time.
Because we are not using speaker information, we
remove all utterances which do not contain a noun
before constructing synthetic transcripts these are
mostly backchannels like &amp;quot;Yeah&amp;quot;. Such utterances
cannot be correctly assigned by our coherence mod-
els, which deal with content; we suspect most of
them could be dealt with by associating them with
the nearest utterance from the same speaker.
</bodyText>
<page confidence="0.947147">
1183
</page>
<bodyText confidence="0.9999495">
Once the backchannels are stripped, we can cre-
ate a synthetic transcript. For each dialogue, we first
simulate timestamps by sampling the number of sec-
onds between each utterance and the next from a dis-
cretized Gaussian: ❜N(0; 2:5)❝. The interleaving of
the conversations is dictated by the timestamps. We
truncate the longer conversation at the length of the
shorter; this ensures a baseline score of 50% for the
degenerate model that assigns all utterances to the
same conversation.
We create synthetic instances of two types those
where the two entangled conversations had differ-
ent topical prompts and those where they were the
same. (Each dialogue in ❙❲❇❉ focuses on a prese-
lected topic, such as fishing or movies.) We entangle
dialogues from our ordering development set to use
for mixture training and validation; for testing, we
use 100 instances of each type, constructed from di-
alogues in our test set.
When disentangling, we treat each thread as inde-
pendent of the others. In other words, the probability
of the entire transcript is the product of the probabil-
ities of the component threads. Our objective is to
find the set of threads maximizing this. As a com-
parison, we use the model of Elsner and Charniak
(2008b) as a baseline. To make their implementa-
tion comparable to ours, in this section we constrain
it to find only two threads.
</bodyText>
<subsectionHeader confidence="0.995244">
5.1 Disentangling a single utterance
</subsectionHeader>
<bodyText confidence="0.999783555555556">
Our first disentanglement task is to correctly assign
a single utterance, given the true structure of the rest
of the transcript. For each utterance, we compare
two versions of the transcript, the original, and a
version where it is swapped into the other thread.
Our accuracy measures how often our models prefer
the original. Unlike full-scale disentanglement, this
task does not require a computationally demanding
search, so it is possible to run experiments quickly.
We also use it to train our mixture models for disen-
tanglement, by construct a training example for each
utterance i in our training transcripts. Since the El-
sner and Charniak (2008b) model maximizes a cor-
relation clustering objective which sums up indepen-
dent edge weights, we can also use it to disentangle
a single sentence efficiently.
Our results are shown in Table 2. Again, re-
sults for individual models are above the line, then
</bodyText>
<table confidence="0.999863538461539">
Different Same Avg.
EGrid 80.2 72.9 76.6
Topical EGrid 81.7 73.3 77.5
IBM-1 70.4 66.7 68.5
Pronouns 53.1 50.1 51.6
Time 58.5 57.4 57.9
Combined 86.8 79.6 83.2
-EGrid 86.0 79.1 82.6
-Topical EGrid 85.2 78.7 81.9
-IBM-1 86.2 78.7 82.4
-Pronouns 86.8 79.4 83.1
-Time 84.5 76.7 80.6
E+C `08 78.2 73.5 75.8
</table>
<tableCaption confidence="0.987565">
Table 2: Average accuracy for disentanglement of a sin-
gle utterance on 200 synthetic multiparty conversations
from SWBD test.
</tableCaption>
<bodyText confidence="0.9999351">
our combined model, and finally ablation results for
mixtures omitting a single model. The results show
that, for a pair of dialogues that differ in topic, our
best model can assign a single sentence with 87%
accuracy. For the same topic, the accuracy is 80%.
In each case, these results improve on (Elsner and
Charniak, 2008b), which scores 78% and 74%.
Changing to this new task has a substantial im-
pact on performance. The topical model, which per-
formed poorly for ordering, is actually stronger than
the entity grid in this setting. IBM-1 underperforms
either grid model (69% to 77%); on ordering, it was
nearly as good (85% to 86%).
Despite their ordering performance of 72%, pro-
nouns are essentially useless for this task, at 52%.
This decline is due partly to domain, and partly
to task setting. Although ❙❲❇❉ contains more
pronominals than ❲❙❏, many of them are first
and second-person pronouns or deictics, which our
model does not attempt to resolve. Since the disen-
tanglement task involves moving only a single sen-
tence, if moving this sentence does not sever a re-
solvable pronoun from its antecedent, the model will
be unable to make a good decision.
As before, the ablation results show that all the
models are quite correlated, since removing any sin-
gle model from the mixture causes only a small de-
crease in performance. The largest drop (83% to
81%) is caused by removing time; though time is
a weak model on its own, it is completely orthogo-
</bodyText>
<page confidence="0.990279">
1184
</page>
<bodyText confidence="0.999785142857143">
nal to the other models, since unlike them, it does
not depend on the words in the sentences.
Comparing results between &amp;quot;different topic&amp;quot; and
&amp;quot;same topic&amp;quot; instances shows that &amp;quot;same topic&amp;quot; is
harder— by about 7% for the combined model. The
IBM model has a relatively small gap of 3.7%, and
in the ablation results, removing it causes a larger
drop in performance for &amp;quot;same&amp;quot; than &amp;quot;different&amp;quot;;
this suggests it is somewhat more robust to similar-
ity in topic than entity grids.
Disentanglement accuracy is hard to predict given
ordering performance; the two tasks plainly make
different demands on models. One difference is that
the models which use longer histories (the two entity
grids) remain strong, while the models considering
only one or two previous sentences (IBM and pro-
nouns) do not do as well. Since the changes being
considered here affect only a single sentence, while
permutation affects the entire transcript, more his-
tory may help by making the model more sensitive
to small changes.
</bodyText>
<subsectionHeader confidence="0.998532">
5.2 Disentangling an entire transcript
</subsectionHeader>
<bodyText confidence="0.999971565217391">
We now turn to the task of disentangling an entire
transcript at once. This is a practical task, motivated
by applications such as search and information re-
trieval. However, it is more difficult than assign-
ing only a single utterance, because decisions are
interrelated— an error on one utterance may cause
a cascade of poor decisions further down. It is also
computationally harder.
We use tabu search (Glover and Laguna, 1997) to
find a good solution. The search repeatedly finds and
moves the utterance which would most improve the
model score if swapped from one thread to the other.
Unlike greedy search, tabu search is constrained not
to repeat a solution that it has recently visited; this
forces it to keep exploring when it reaches a local
maximum. We run 500 iterations of tabu search
(usually finding the first local maximum after about
100) and return the best solution found.
We measure performance with one-to-one over-
lap, which maps the two clusters to the two gold
dialogues, then measures percent corrects. Our re-
sults (Table 3) show that, for transcripts with dif-
ferent topics, our disentanglement has 68% over-
</bodyText>
<footnote confidence="0.873661">
5The other popular metrics, F and l♦c 3, are correlated.
</footnote>
<table confidence="0.999874625">
Different Same Avg.
EGrid 60.3 57.1 58.7
Topical EGrid 62.3 56.8 59.6
IBM-1 56.5 55.2 55.9
Pronouns 54.5 54.4 54.4
Time 55.4 53.8 54.6
Combined 67.9 59.8 63.9
E+C `08 59.1 57.4 58.3
</table>
<tableCaption confidence="0.993752">
Table 3: One-to-one overlap between disentanglement re-
sults and truth on 200 synthetic multiparty conversations
from SWBD test.
</tableCaption>
<bodyText confidence="0.999958909090909">
lap with truth, extracting about two thirds of the
structure correctly; this is substantially better than
Elsner and Charniak (2008b), which scores 59%.
Where the entangled conversations have the same
topic, performance is lower, about 60%, but still bet-
ter than the comparison model with 57%. Since cor-
relations with the previous section are fairly reliable,
and the disentanglement procedure is computation-
ally intensive, we omit ablation experiments.
As we expect, full disentanglement is more dif-
ficult than single-sentence disentanglement (com-
bined scores drop by about 20%), but the single-
sentence task is a good predictor of relative perfor-
mance. Entity grid models do best, the IBM model
remains useful, but less so than for discrimination,
and pronouns are very weak. The IBM model per-
forms similarly under both metrics (56% and 57%),
while other models perform worse on l♦c 3. This
supports our suggestion that IBM&apos;s decline in per-
formance from ordering is indeed due to its using a
single sentence history; it is still capable of getting
local structures right, but misses global ones.
</bodyText>
<sectionHeader confidence="0.983776" genericHeader="method">
6 IRC data
</sectionHeader>
<bodyText confidence="0.999896090909091">
In this section, we move from synthetic data to
real multiparty discourse recorded from internet chat
rooms. We use two datasets: the #LINUX corpus
(Elsner and Charniak, 2008b), and three larger cor-
pora, #IPHONE, #PHYSICS and #PYTHON (Adams,
2008). We use the 1000-line &amp;quot;development&amp;quot; sec-
tion of #LINUX for tuning our mixture models and
the 800-line &amp;quot;test&amp;quot; section for development experi-
ments. We reserve the Adams (2008) corpora for
testing; together, they consist of 19581 lines of chat,
with each section containing 500 to 1000 lines.
</bodyText>
<page confidence="0.975311">
1185
</page>
<figure confidence="0.902360894736842">
74.0
Chat-specific
+EGrid
+Topical EGrid
79.3
#IPHONE #PHYSICS #PYTHON
+EGrid 92.3 96.6 91.1
E+C `08b 89.0 90.2 88.4
76.8
+IBM-1
+Pronouns
73.9
76.3
Table 5: Average accuracy for disentanglement of a sin-
gle utterance for 19581 total lines from Adams (2008).
78.3
+EGrid/Topic/IBM-1
E+C `08b
76.4
</figure>
<tableCaption confidence="0.9250995">
Table 4: Accuracy for single utterance disentanglement,
averaged over annotations of 800 lines of #LINUx data.
</tableCaption>
<bodyText confidence="0.999892416666667">
In order to use syntactic models like the entity
grid, we parse the transcripts using (McClosky et
al., 2006). Performance is bad, although the parser
does identify most of the NPs; poor results are typi-
cal for a standard parser on chat (Foster, 2010). We
postprocess the parse trees to retag &amp;quot;lol&amp;quot;, &amp;quot;haha&amp;quot; and
&amp;quot;yes&amp;quot; as UH (rather than NN, NNP and JJ).
In this section, we use all three of our chat-
specific models (sec. 2.0.6; time, speaker and men-
tion) as a baseline. This baseline is relatively strong,
so we evaluate our other models in combination with
it.
</bodyText>
<subsectionHeader confidence="0.999655">
6.1 Disentangling a single sentence
</subsectionHeader>
<bodyText confidence="0.99998880952381">
As before, we show results on correctly disentan-
gling a single sentence, given the correct structure
of the rest of the transcript. We average perfor-
mance on each transcript over the different annota-
tions, then average the transcripts, weighing them by
length to give each utterance equal weight.
Table 4 gives results on our development corpus,
#LINUX. Our best result, for the chat-specific fea-
tures plus entity grid, is 79%, improving on the com-
parison model, Elsner and Charniak (2008b), which
gets 76%. (Although the table only presents an av-
erage over all annotations of the dataset, this model
is also more accurate for each individual annota-
tor than the comparison model.) We then ran the
same model, chat-specific features plus entity grid,
on the test corpora from Adams (2008). These re-
sults (Table 5) are also better than Elsner and Char-
niak (2008b), at an average of 93% over 89%.
As pointed out in Elsner and Charniak (2008b),
the chat-specific features are quite powerful in this
domain, and it is hard to improve over them. Elsner
and Charniak (2008b), which has simple lexical fea-
tures, mostly based on unigram overlap, increases
performance over baseline by 2%. Both IBM and
the topical entity grid achieve similar gains. The en-
tity grid does better, increasing performance to 79%.
Pronouns, as before for SwBD, are useless.
We believe that the entity grid&apos;s good perfor-
mance here is due mostly to two factors: its use of
a long history, and its lack of lexicalization. The
grid looks at the previous six sentences, which dif-
ferentiates it from the IBM model and from Elsner
and Charniak (2008b), which treats each pair of sen-
tences independently. Using this long history helps
to distinguish important nouns from unimportant
ones better than frequency alone. We suspect that
our lexicalized models, IBM and the topical entity
grid, are hampered by poor parameter settings, since
their parameters were learned on FISHER rather than
IRC chat. In particular, we believe this explains why
the topical entity grid, which slightly outperformed
the entity grid on SwBD, is much worse here.
</bodyText>
<subsectionHeader confidence="0.998519">
6.2 Full disentanglement
</subsectionHeader>
<bodyText confidence="0.99996925">
Running our tabu search algorithm on the full disen-
tanglement task yields disappointing results. Accu-
racies on the #LINUX dataset are not only worse than
previous work, but also worse than simple baselines
like creating one thread for each speaker. The model
finds far too many threads it detects over 300, when
the true number is about 81 (averaging over annota-
tions). This appears to be related to biases in our
chat-specific models as well as in the entity grid;
the time model (which generates gaps between adja-
cent sentences) and the speaker model (which uses
a CRP) both assign probability 1 to single-utterance
conversations. The entity grid also has a bias toward
short conversations, because unseen entities are em-
pirically more likely to occur toward the beginning
of a conversation than in the middle.
A major weakness in our model is that we aim
only to maximize coherence of the individual con-
versations, with no prior on the likely length or num-
ber of conversations that will appear in the tran-
</bodyText>
<page confidence="0.976492">
1186
</page>
<bodyText confidence="0.999972588235294">
script. This allows the model to create far too many
conversations. Integrating a prior into our frame-
work is not straightforward because we currently
train our mixture to maximize single-utterance dis-
entanglement performance, and the prior is not use-
ful for this task.
We experimented with fixing parts of the tran-
script to the solution obtained by Elsner and Char-
niak (2008b), then using tabu search to fill in the
gaps. This constrains the number of conversations
and their approximate positions. With this structure
in place, we were able to obtain scores comparable
to Elsner and Charniak (2008b), but not improve-
ments. It appears that our performance increase on
single-sentence disentanglement does not transfer to
this task because of cascading errors and the neces-
sity of using external constraints.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999977851851852">
We demonstrate that several popular models of lo-
cal coherence transfer well to the conversational do-
main, suggesting that they do indeed capture coher-
ence in general rather than specific conventions of
newswire text. However, their performance across
tasks is not as stable; in particular, models which
use less history information are worse for disentan-
glement.
Our results study suggest that while sophisticated
coherence models can potentially contribute to dis-
entanglement, they would benefit greatly from im-
proved low-level resources for internet chat. Bet-
ter parsing, or at least NP chunking, would help for
models like the entity grid which rely on syntactic
role information. Larger training sets, or some kind
of transfer learning, could improve the learning of
topics and other lexical parameters. In particular,
our results on SwBD data confirm the conjecture of
(Adams, 2008) that LDA topic modeling is in prin-
ciple a useful tool for disentanglement we believe a
topic-based model could also work on IRC chat, but
would require a better set of extracted topics. With
better parameters for these models and the integra-
tion of a prior, we believe that our good performance
on SwBD and single-utterance disentanglement for
IRC can be extended to full-scale disentanglement
of IRC.
</bodyText>
<sectionHeader confidence="0.980782" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998825833333333">
We are extremely grateful to Regina Barzilay, Mark
Johnson, Rebecca Mason, Ben Swanson and Neal
Fox for their comments, to Craig Martell for the
NPS chat datasets and to three anonymous review-
ers. This work was funded by a Google Fellowship
for Natural Language Processing.
</bodyText>
<sectionHeader confidence="0.996576" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999202853658536">
Paige H. Adams. 2008. Conversation Thread Extraction
and Topic Detection in Text-based Chat. Ph.D. thesis,
Naval Postgraduate School.
David Aldous. 1985. Exchangeability and related top-
ics. In Ecole d&apos;Ete de Probabilities de Saint-Flour
XIII 1983, pages 1198. Springer.
Paul M. Aoki, Matthew Romaine, Margaret H. Szyman-
ski, James D. Thornton, Daniel Wilson, and Allison
Woodruff. 2003. The mad hatter&apos;s cocktail party: a
social mobile audio space supporting multiple simul-
taneous conversations. In CHI &apos;03: Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 425432, New York, NY, USA. ACM
Press.
Paul M. Aoki, Margaret H. Szymanski, Luke D.
Plurkowski, James D. Thornton, Allison Woodruff,
and Weilie Yi. 2006. Where&apos;s the &amp;quot;party&amp;quot; in &amp;quot;multi-
party&amp;quot;?: analyzing the structure of small-group socia-
ble talk. In CSCW &apos;06: Proceedings of the 2006 20th
anniversary conference on Computer supported coop-
erative work, pages 393402, New York, NY, USA.
ACM Press.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL&apos;05).
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113120.
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2001. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:2003.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
EACL, Athens, Greece.
Harr Chen, S.R.K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Global models of document
structure using latent permutations. In Proceedings
of Human Language Technologies: The 2009 Annual
</reference>
<page confidence="0.949497">
1187
</page>
<reference confidence="0.999266179245283">
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 371-
379, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In EMNLP, pages
334-343.
Micha Elsner and Eugene Charniak. 2008a.
Coreference-inspired coherence modeling. In
Proceedings of ACL-08: HLT, Short Papers, pages
41-44, Columbus, Ohio, June. Association for
Computational Linguistics.
Micha Elsner and Eugene Charniak. 2008b. You talk-
ing to me? a corpus and algorithm for conversation
disentanglement. In Proceedings of ACL-08: HLT,
pages 834-842, Columbus, Ohio, June. Association
for Computational Linguistics.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2&amp;3):285-307.
Jennifer Foster. 2010. cba to check the spelling: In-
vestigating parser performance on discussion forum
posts. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
381-384, Los Angeles, California, June. Association
for Computational Linguistics.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 161-171, Orlando, Florida. Harcourt Brace.
Fred Glover and Manuel Laguna. 1997. Tabu Search.
University of Colorado at Boulder.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203-225.
Simon Haykin and Zhe Chen. 2005. The Cocktail Party
Problem. Neural Computation, 17(9):1875-1902.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish,
and Jon Oberlander. 2004. Evaluating centering-
based metrics of coherence. In ACL, pages 391-398.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IICAI, pages 1085-1090.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the annual meeting of ACL, 2003.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall&apos;s tau. Computational Linguis-
tics, 32(4):1-14.
Gideon Mann, Ryan McDonald, Mehryar Mohri, Nathan
Silberman, and Dan Walker. 2009. Efficient large-
scale distributed training of conditional maximum en-
tropy models. In Y. Bengio, D. Schuurmans, J. Laf-
ferty, C. K. I. Williams, and A. Culotta, editors, Ad-
vances in Neural Information Processing Systems 22,
pages 1231-1239.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152-159.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adaptation for parsing. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 28-36,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Eleni Miltsakaki and K. Kukich. 2004. Evaluation of text
coherence for electronic essay scoring systems. Nat.
Lang. Eng., 10(1):25-55.
Neville Moray. 1959. Attention in dichotic listening: Af-
fective cues and the influence of instructions. Quar-
terly Iournal of Experimental Psychology, 11(1):56-
60.
Ani Nenkova and Kathleen McKeown. 2003. Refer-
ences to named entities: a corpus study. In NAACL
&apos;03, pages 70-72.
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of EMNLP, pages
94-102, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Jacki O&apos;Neill and David Martin. 2003. Text chat in ac-
tion. In GROUP &apos;03: Proceedings of the 2003 inter-
national ACM SIGGROUP conference on Supporting
group work, pages 40-49, New York, NY, USA. ACM
Press.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of the 2008 Conference on Empir-
ical Methods in Natural Language Processing, pages
186-195, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Massimo Poesio, Mijail Alexandrov-Kabadjov, Renata
Vieira, Rodrigo Goulart, and Olga Uryupina. 2005.
Does discourse-new detection help definite description
resolution? In Proceedings of the Sixth International
Workshop on Computational Semantics, Tillburg.
Amruta Purandare and Diane J. Litman. 2008. Analyz-
ing dialog coherence using transition patterns in lexi-
cal and semantic features. In FLAIRS Conference&apos;08,
pages 195-200.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
</reference>
<page confidence="0.864396">
1188
</page>
<reference confidence="0.999186363636364">
streams. In SIGIR &apos;06: Proceedings of the 29th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 3542,
New York, NY, USA. ACM.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the Association for Computational Lin-
guistics Conference (ACL-2006).
Lidan Wang and Douglas W. Oard. 2009. Context-based
message expansion for disentanglement of interleaved
text conversations. In Proceedings of NAACL-09.
</reference>
<page confidence="0.996804">
1189
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.914230">
<title confidence="0.99861">Disentangling Chat with Local Coherence Models</title>
<author confidence="0.999817">Micha Elsner Eugene Charniak</author>
<affiliation confidence="0.969347">School of Informatics Department of Computer Science University of Edinburgh Brown University, Providence, RI 02912</affiliation>
<email confidence="0.997538">melsner0@gmail.comec@cs.brown.edu</email>
<abstract confidence="0.9977739">We evaluate several popular models of local discourse coherence for domain and task generality by applying them to chat disentanglement. Using experiments on synthetic multiparty conversations, we show that most models transfer well from text to dialogue. Coherence models improve results overall when good parses and topic models are available, and on a constrained task for real chat data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paige H Adams</author>
</authors>
<title>Conversation Thread Extraction and Topic Detection in Text-based Chat.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Naval Postgraduate School.</institution>
<contexts>
<context position="5809" citStr="Adams (2008)" startWordPosition="888" endWordPosition="889">ts on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measure similarity between two sentences; Wang and Oard (2009) use a message expansion technique to incorporate context beyond a single sentence. Unigram overlaps are used to model coherence, but more sophisticated methods using syntax (Lapata and Barzilay, 2005) or lexical features (Lapata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we use as a baseline because there is a publicly available implementation✷. Adams (2008) also created and released a disentanglement corpus. They use LDA (Blei et al., 2001) to discover latent topics in their corpus, then measuring similarity by looking for shared topics. These features fail to improve their performance, which is puzzling in light of the success of topic modeling for other coherence and segmentation problems (Eisenstein and Barzilay, 2008; Foltz et al., 1998). The results of this study suggest that topic models can help with disentanglement, but that it is difficult to find useful topics for IRC chat. A few studies have attempted to disentangle conversational spe</context>
<context position="28498" citStr="Adams, 2008" startWordPosition="4626" endWordPosition="4627">tion, and pronouns are very weak. The IBM model performs similarly under both metrics (56% and 57%), while other models perform worse on l♦c 3. This supports our suggestion that IBM&apos;s decline in performance from ordering is indeed due to its using a single sentence history; it is still capable of getting local structures right, but misses global ones. 6 IRC data In this section, we move from synthetic data to real multiparty discourse recorded from internet chat rooms. We use two datasets: the #LINUX corpus (Elsner and Charniak, 2008b), and three larger corpora, #IPHONE, #PHYSICS and #PYTHON (Adams, 2008). We use the 1000-line &amp;quot;development&amp;quot; section of #LINUX for tuning our mixture models and the 800-line &amp;quot;test&amp;quot; section for development experiments. We reserve the Adams (2008) corpora for testing; together, they consist of 19581 lines of chat, with each section containing 500 to 1000 lines. 1185 74.0 Chat-specific +EGrid +Topical EGrid 79.3 #IPHONE #PHYSICS #PYTHON +EGrid 92.3 96.6 91.1 E+C `08b 89.0 90.2 88.4 76.8 +IBM-1 +Pronouns 73.9 76.3 Table 5: Average accuracy for disentanglement of a single utterance for 19581 total lines from Adams (2008). 78.3 +EGrid/Topic/IBM-1 E+C `08b 76.4 Table 4: </context>
<context position="30584" citStr="Adams (2008)" startWordPosition="4971" endWordPosition="4972">ver the different annotations, then average the transcripts, weighing them by length to give each utterance equal weight. Table 4 gives results on our development corpus, #LINUX. Our best result, for the chat-specific features plus entity grid, is 79%, improving on the comparison model, Elsner and Charniak (2008b), which gets 76%. (Although the table only presents an average over all annotations of the dataset, this model is also more accurate for each individual annotator than the comparison model.) We then ran the same model, chat-specific features plus entity grid, on the test corpora from Adams (2008). These results (Table 5) are also better than Elsner and Charniak (2008b), at an average of 93% over 89%. As pointed out in Elsner and Charniak (2008b), the chat-specific features are quite powerful in this domain, and it is hard to improve over them. Elsner and Charniak (2008b), which has simple lexical features, mostly based on unigram overlap, increases performance over baseline by 2%. Both IBM and the topical entity grid achieve similar gains. The entity grid does better, increasing performance to 79%. Pronouns, as before for SwBD, are useless. We believe that the entity grid&apos;s good perfo</context>
<context position="34648" citStr="Adams, 2008" startWordPosition="5634" endWordPosition="5635">in particular, models which use less history information are worse for disentanglement. Our results study suggest that while sophisticated coherence models can potentially contribute to disentanglement, they would benefit greatly from improved low-level resources for internet chat. Better parsing, or at least NP chunking, would help for models like the entity grid which rely on syntactic role information. Larger training sets, or some kind of transfer learning, could improve the learning of topics and other lexical parameters. In particular, our results on SwBD data confirm the conjecture of (Adams, 2008) that LDA topic modeling is in principle a useful tool for disentanglement we believe a topic-based model could also work on IRC chat, but would require a better set of extracted topics. With better parameters for these models and the integration of a prior, we believe that our good performance on SwBD and single-utterance disentanglement for IRC can be extended to full-scale disentanglement of IRC. Acknowledgements We are extremely grateful to Regina Barzilay, Mark Johnson, Rebecca Mason, Ben Swanson and Neal Fox for their comments, to Craig Martell for the NPS chat datasets and to three ano</context>
</contexts>
<marker>Adams, 2008</marker>
<rawString>Paige H. Adams. 2008. Conversation Thread Extraction and Topic Detection in Text-based Chat. Ph.D. thesis, Naval Postgraduate School.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Aldous</author>
</authors>
<title>Exchangeability and related topics.</title>
<date>1985</date>
<booktitle>In Ecole d&apos;Ete de Probabilities de Saint-Flour XIII</booktitle>
<pages>1--198</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="13701" citStr="Aldous, 1985" startWordPosition="2198" endWordPosition="2199">hread. Consistent short gaps are a sign of normal turn-taking behavior; long pauses do occur, but much more rarely (Aoki et al., 2003). We round all time gaps to the nearest second and model the distribution of time gaps using a histogram, choosing bucket sizes adaptively so that each bucket contains at least four datapoints. The second feature is speaker identity; conversations usually involve a small subset of the total number of speakers, and a few core speakers make most of the utterances. We model the distribution of speakers in each conversation using a Chinese Restaurant Process (CRP) (Aldous, 1985) (tuning the dispersion a to maximize development peformance). The CRP&apos;s &amp;quot;rich-get-richer&amp;quot; dynamics capture our intuitions, favoring conversations dominated by a few vociferous speakers. Finally, we model name mentioning. Speakers in IRC chat often use their addressee&apos;s names to coordinate the chat (O&apos;Neill and Martin, 2003), and this is a powerful source of information (Elsner and Charniak, 2008b). Our model classifies each utterance into either the start or continuation of a conversational turn, by checking if the previous utterance had the same speaker. Given this status, it computes probab</context>
</contexts>
<marker>Aldous, 1985</marker>
<rawString>David Aldous. 1985. Exchangeability and related topics. In Ecole d&apos;Ete de Probabilities de Saint-Flour XIII 1983, pages 1198. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul M Aoki</author>
<author>Matthew Romaine</author>
<author>Margaret H Szymanski</author>
<author>James D Thornton</author>
<author>Daniel Wilson</author>
<author>Allison Woodruff</author>
</authors>
<title>The mad hatter&apos;s cocktail party: a social mobile audio space supporting multiple simultaneous conversations.</title>
<date>2003</date>
<booktitle>In CHI &apos;03: Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>425--432</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6431" citStr="Aoki et al., 2003" startWordPosition="989" endWordPosition="992"> created and released a disentanglement corpus. They use LDA (Blei et al., 2001) to discover latent topics in their corpus, then measuring similarity by looking for shared topics. These features fail to improve their performance, which is puzzling in light of the success of topic modeling for other coherence and segmentation problems (Eisenstein and Barzilay, 2008; Foltz et al., 1998). The results of this study suggest that topic models can help with disentanglement, but that it is difficult to find useful topics for IRC chat. A few studies have attempted to disentangle conversational speech (Aoki et al., 2003; Aoki et al., 2006), mostly using temporal features. For the most part, however, this research has focused on auditory processing in the context of the cocktail party problem, the task of attending to a specific speaker in a noisy room (Haykin and Chen, 2005). Utterance content has some influence on what the listener perceives, but only for extremely salient cues such as the listener&apos;s name (Moray, 1959), so cocktail party research does not typically use lexical models. 3 Models In this section, we briefly describe the models we intend to evaluate. Most of them are drawn from previous work; o</context>
<context position="13222" citStr="Aoki et al., 2003" startWordPosition="2114" endWordPosition="2117">g syntactic features counting different types of NP modifiers, and we use NP head identity as a proxy for coreference. 3.6 Chat-specific features Most disentanglement models use non-linguistic information alongside lexical features; in fact, timestamps and speaker identities are usually better cues than words are. We capture three essential nonlinguistic features using simple generative models. The first feature is the time gap between one utterance and the next within the same thread. Consistent short gaps are a sign of normal turn-taking behavior; long pauses do occur, but much more rarely (Aoki et al., 2003). We round all time gaps to the nearest second and model the distribution of time gaps using a histogram, choosing bucket sizes adaptively so that each bucket contains at least four datapoints. The second feature is speaker identity; conversations usually involve a small subset of the total number of speakers, and a few core speakers make most of the utterances. We model the distribution of speakers in each conversation using a Chinese Restaurant Process (CRP) (Aldous, 1985) (tuning the dispersion a to maximize development peformance). The CRP&apos;s &amp;quot;rich-get-richer&amp;quot; dynamics capture our intuition</context>
</contexts>
<marker>Aoki, Romaine, Szymanski, Thornton, Wilson, Woodruff, 2003</marker>
<rawString>Paul M. Aoki, Matthew Romaine, Margaret H. Szymanski, James D. Thornton, Daniel Wilson, and Allison Woodruff. 2003. The mad hatter&apos;s cocktail party: a social mobile audio space supporting multiple simultaneous conversations. In CHI &apos;03: Proceedings of the SIGCHI conference on Human factors in computing systems, pages 425432, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul M Aoki</author>
<author>Margaret H Szymanski</author>
<author>Luke D Plurkowski</author>
<author>James D Thornton</author>
<author>Allison Woodruff</author>
<author>Weilie Yi</author>
</authors>
<title>Where&apos;s the &amp;quot;party&amp;quot; in &amp;quot;multiparty&amp;quot;?: analyzing the structure of small-group sociable talk.</title>
<date>2006</date>
<booktitle>In CSCW &apos;06: Proceedings of the 2006 20th anniversary conference on Computer supported cooperative work,</booktitle>
<pages>393--402</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6451" citStr="Aoki et al., 2006" startWordPosition="993" endWordPosition="996">ed a disentanglement corpus. They use LDA (Blei et al., 2001) to discover latent topics in their corpus, then measuring similarity by looking for shared topics. These features fail to improve their performance, which is puzzling in light of the success of topic modeling for other coherence and segmentation problems (Eisenstein and Barzilay, 2008; Foltz et al., 1998). The results of this study suggest that topic models can help with disentanglement, but that it is difficult to find useful topics for IRC chat. A few studies have attempted to disentangle conversational speech (Aoki et al., 2003; Aoki et al., 2006), mostly using temporal features. For the most part, however, this research has focused on auditory processing in the context of the cocktail party problem, the task of attending to a specific speaker in a noisy room (Haykin and Chen, 2005). Utterance content has some influence on what the listener perceives, but only for extremely salient cues such as the listener&apos;s name (Moray, 1959), so cocktail party research does not typically use lexical models. 3 Models In this section, we briefly describe the models we intend to evaluate. Most of them are drawn from previous work; one, the topical enti</context>
</contexts>
<marker>Aoki, Szymanski, Plurkowski, Thornton, Woodruff, Yi, 2006</marker>
<rawString>Paul M. Aoki, Margaret H. Szymanski, Luke D. Plurkowski, James D. Thornton, Allison Woodruff, and Weilie Yi. 2006. Where&apos;s the &amp;quot;party&amp;quot; in &amp;quot;multiparty&amp;quot;?: analyzing the structure of small-group sociable talk. In CSCW &apos;06: Proceedings of the 2006 20th anniversary conference on Computer supported cooperative work, pages 393402, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: an entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05).</booktitle>
<contexts>
<context position="7649" citStr="Barzilay and Lapata, 2005" startWordPosition="1197" endWordPosition="1200"> work; one, the topical entity grid, is a novel extension of the entity grid. For the experiments below, we train the models on SwBD, sometimes augmented with a larger set of automatically parsed conversations from the FISHER corpus. Since the two corpora are quite similar, FISHER is a useful source for extra data; McClosky et al. (2010) uses it for this purpose in parsing experiments. (We continue to use SwBD/FISHER even for experiments on IRC, because we do not have enough disentangled training data to learn lexical relationships.) 3.1 Entity grid The entity grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005) is an attempt to model some principles of Centering Theory (Grosz et al., 1995) in a statistical manner. It represents a document in terms of entities and their syntactic roles: subject (S), object (O), other (X) and not present (-). In each new 2cs.brown.edu/melsner 1180 utterance, the grid predicts the role in which each entity will appear, given its history of roles in the previous sentences, plus a salience feature counting the total number of times the entity occurs. For instance, for an entity which is the subject of sentence 1, the object of sentence 2, and occurs four times in total,</context>
<context position="15315" citStr="Barzilay and Lapata, 2005" startWordPosition="2452" endWordPosition="2455">ut and Marcu (2006). Here, each model Pi is assigned a weight Ai, and the combined score P(d) is proportional to: ❳ Ailog(Pi(d)) i The weights A can be learned discriminatively, maximizing the probability of d relative to a taskspecific contrast set. For ordering experiments, the contrast set is a single random permutation of d; we explain the training regime for disentanglement below, in subsection 4.1. 4 Comparing orderings of SWBD To measure the differences in performance caused by moving from news to a conversational domain, we first compare our models on an ordering task, discrimination (Barzilay and Lapata, 2005; Karamanis et al., 2004). In this task, we take an original document and randomly permute its sentences, creating an artificial incoherent document. We then test to see if our model prefers the coherent original. For SwBD, rather than compare permutations of the individual utterances, we permute conversational turns (sets of consecutive utterances by each speaker), since turns are natural discourse units in conversation. We take documents numbered 2000 3999 as training/development and the remainder as test, yielding 505 training and 153 test documents; we evaluate 20 permutations per documen</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: an entity-based approach. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Proceedings of the Main Conference,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="4304" citStr="Barzilay and Lee, 2004" startWordPosition="652" endWordPosition="655">apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrating prior information, our improvements on SwBD could transfer fully to IRC chat. 2 Related work There is extensive previous work on coherence models for text ordering; we describe several specific models below, in section 2. This study focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the fluency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the t</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL 2004: Proceedings of the Main Conference, pages 113120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2001</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--2003</pages>
<contexts>
<context position="5894" citStr="Blei et al., 2001" startWordPosition="901" endWordPosition="904">lapping unigrams to measure similarity between two sentences; Wang and Oard (2009) use a message expansion technique to incorporate context beyond a single sentence. Unigram overlaps are used to model coherence, but more sophisticated methods using syntax (Lapata and Barzilay, 2005) or lexical features (Lapata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we use as a baseline because there is a publicly available implementation✷. Adams (2008) also created and released a disentanglement corpus. They use LDA (Blei et al., 2001) to discover latent topics in their corpus, then measuring similarity by looking for shared topics. These features fail to improve their performance, which is puzzling in light of the success of topic modeling for other coherence and segmentation problems (Eisenstein and Barzilay, 2008; Foltz et al., 1998). The results of this study suggest that topic models can help with disentanglement, but that it is difficult to find useful topics for IRC chat. A few studies have attempted to disentangle conversational speech (Aoki et al., 2003; Aoki et al., 2006), mostly using temporal features. For the m</context>
<context position="9343" citStr="Blei et al., 2001" startWordPosition="1481" endWordPosition="1484">tory to six previous utterances. For robustness with this long history, we model the conditional probabilities using multilabel logistic regression rather than maximum likelihood. This requires the assumption of a linear model, but makes the estimator less vulnerable to overfitting due to sparsity, increasing performance by about 2% in development experiments. 3.2 Topical entity grid This model is a variant of the generative entity grid, intended to take into account topical information. To create the topical entity grid, we learn a set of topic-to-word distributions for our corpus using LDA (Blei et al., 2001)3 with 200 latent topics. This model embeds our vocabulary in a lowdimensional space: we represent each word w as the vector of topic probabilities p(ti❥w). We experimented with several ways to measure relationships between words in this space, starting with the standard cosine. However, the cosine can depend on small variations in probability (for instance, if w has most of its mass in dimension 1, then it is sensitive to the exact weight of v for topic 1, even if this essentially never happens). To control for this tendency, we instead use the magnitude of the dimension of greatest similarit</context>
</contexts>
<marker>Blei, Ng, Jordan, 2001</marker>
<rawString>David Blei, Andrew Y. Ng, and Michael I. Jordan. 2001. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Micha Elsner</author>
</authors>
<title>EM works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="11639" citStr="Charniak and Elsner, 2009" startWordPosition="1868" endWordPosition="1871"> proposed earlier (Lapata, 2003). This model attempts to generate the content words of the next sentence by translating them from the words of the previous sentence, plus a null word; thus, it will learn alignments between pairs of words that tend to occur in adjacent sentences. We learn parameters on the FISHER corpus, and on NANC for news. 3.4 Pronouns The use of a generative pronoun resolver for coherence modeling originates in Elsner and Charniak (2008a). That paper used a supervised model (Ge et al., 1998), but we adapt a newer, unsupervised model which they also make publicly available (Charniak and Elsner, 2009)4. They model each pronoun as generated by an antecedent somewhere in the previous two sentences. If a good antecedent is found, the probability of the pronoun&apos;s occurrence will be high; otherwise, the probability is low, signaling that the text is less coherent because the pronoun is hard to interpret correctly. We use the model as distributed for news text. For conversation, we adapt it by running a few iterations of their EM training algorithm on the FISHER data. 4bllip.cs.brown.edu/resources.shtml\ #software 1181 3.5 Discourse-newness Building on work from summarization (Nenkova and McKeow</context>
</contexts>
<marker>Charniak, Elsner, 2009</marker>
<rawString>Eugene Charniak and Micha Elsner. 2009. EM works for pronoun anaphora resolution. In Proceedings of EACL, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harr Chen</author>
<author>S R K Branavan</author>
<author>Regina Barzilay</author>
<author>David R Karger</author>
</authors>
<title>Global models of document structure using latent permutations.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>371--379</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="4248" citStr="Chen et al., 2009" startWordPosition="644" endWordPosition="647">ntanglement task, though so far, we are unable to apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrating prior information, our improvements on SwBD could transfer fully to IRC chat. 2 Related work There is extensive previous work on coherence models for text ordering; we describe several specific models below, in section 2. This study focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the fluency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate</context>
</contexts>
<marker>Chen, Branavan, Barzilay, Karger, 2009</marker>
<rawString>Harr Chen, S.R.K. Branavan, Regina Barzilay, and David R. Karger. 2009. Global models of document structure using latent permutations. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 371-379, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>334--343</pages>
<contexts>
<context position="4279" citStr="Eisenstein and Barzilay, 2008" startWordPosition="648" endWordPosition="651">hough so far, we are unable to apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrating prior information, our improvements on SwBD could transfer fully to IRC chat. 2 Related work There is extensive previous work on coherence models for text ordering; we describe several specific models below, in section 2. This study focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the fluency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses </context>
<context position="6180" citStr="Eisenstein and Barzilay, 2008" startWordPosition="945" endWordPosition="949">y, 2005) or lexical features (Lapata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we use as a baseline because there is a publicly available implementation✷. Adams (2008) also created and released a disentanglement corpus. They use LDA (Blei et al., 2001) to discover latent topics in their corpus, then measuring similarity by looking for shared topics. These features fail to improve their performance, which is puzzling in light of the success of topic modeling for other coherence and segmentation problems (Eisenstein and Barzilay, 2008; Foltz et al., 1998). The results of this study suggest that topic models can help with disentanglement, but that it is difficult to find useful topics for IRC chat. A few studies have attempted to disentangle conversational speech (Aoki et al., 2003; Aoki et al., 2006), mostly using temporal features. For the most part, however, this research has focused on auditory processing in the context of the cocktail party problem, the task of attending to a specific speaker in a noisy room (Haykin and Chen, 2005). Utterance content has some influence on what the listener perceives, but only for extre</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In EMNLP, pages 334-343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>Coreference-inspired coherence modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>41--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3367" citStr="Elsner and Charniak (2008" startWordPosition="501" endWordPosition="504"> adopt the SWITCHBOARD (SWBD) corpus. SWBD contains recorded telephone conversations with known topics and hand-annotated parse trees; this allows us to control for the performance of our parser and other informational resources. To compare the two algorithmic settings, we use SWBD for ordering experiments, and also artificially entangle pairs of telephone dialogues to create synthetic transcripts which we can disentangle. Finally, we present results on actual internet chat corpora. On synthetic SWBD transcripts, local coherence models improve performance considerably over our baseline model, Elsner and Charniak (2008b). On 1179 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1179–1189, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics internet chat, we continue to do better on a constrained disentanglement task, though so far, we are unable to apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrating prior information, our improvements on SwBD could transfer fully to IRC chat. 2 Related work There is extensive previous work on coherenc</context>
<context position="5124" citStr="Elsner and Charniak (2008" startWordPosition="777" endWordPosition="780">ically cumbersome. In addition to text ordering, local coherence models have also been used to score the fluency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain flexibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available #LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measure similarity between two sentences; Wang and Oard (2009) use a message expansion technique to incorporate context beyond a single sentence. Unigram overlaps are used to model coherence, but more sophisticated methods using syntax (Lapata and Barzilay, 2005) or lexical features (Lapata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we </context>
<context position="11473" citStr="Elsner and Charniak (2008" startWordPosition="1839" endWordPosition="1843">C for news. 3.3 IBM-1 The IBM translation model was first considered for coherence by Soricut and Marcu (2006), although a less probabilistically elegant version was proposed earlier (Lapata, 2003). This model attempts to generate the content words of the next sentence by translating them from the words of the previous sentence, plus a null word; thus, it will learn alignments between pairs of words that tend to occur in adjacent sentences. We learn parameters on the FISHER corpus, and on NANC for news. 3.4 Pronouns The use of a generative pronoun resolver for coherence modeling originates in Elsner and Charniak (2008a). That paper used a supervised model (Ge et al., 1998), but we adapt a newer, unsupervised model which they also make publicly available (Charniak and Elsner, 2009)4. They model each pronoun as generated by an antecedent somewhere in the previous two sentences. If a good antecedent is found, the probability of the pronoun&apos;s occurrence will be high; otherwise, the probability is low, signaling that the text is less coherent because the pronoun is hard to interpret correctly. We use the model as distributed for news text. For conversation, we adapt it by running a few iterations of their EM tr</context>
<context position="14100" citStr="Elsner and Charniak, 2008" startWordPosition="2257" endWordPosition="2260">ly involve a small subset of the total number of speakers, and a few core speakers make most of the utterances. We model the distribution of speakers in each conversation using a Chinese Restaurant Process (CRP) (Aldous, 1985) (tuning the dispersion a to maximize development peformance). The CRP&apos;s &amp;quot;rich-get-richer&amp;quot; dynamics capture our intuitions, favoring conversations dominated by a few vociferous speakers. Finally, we model name mentioning. Speakers in IRC chat often use their addressee&apos;s names to coordinate the chat (O&apos;Neill and Martin, 2003), and this is a powerful source of information (Elsner and Charniak, 2008b). Our model classifies each utterance into either the start or continuation of a conversational turn, by checking if the previous utterance had the same speaker. Given this status, it computes probabilities for three outcomes: no name mention, a mention of someone who has previously spoken in the conversation, or a mention of someone else. (The third option is extremely rare; this accounts for most of the model&apos;s predictive power). We learn these probabilities from IRC training data. 3.7 Model combination To combine these different models, we adopt the log-linear framework of Soricut and Mar</context>
<context position="16041" citStr="Elsner and Charniak (2008" startWordPosition="2567" endWordPosition="2570">ces, creating an artificial incoherent document. We then test to see if our model prefers the coherent original. For SwBD, rather than compare permutations of the individual utterances, we permute conversational turns (sets of consecutive utterances by each speaker), since turns are natural discourse units in conversation. We take documents numbered 2000 3999 as training/development and the remainder as test, yielding 505 training and 153 test documents; we evaluate 20 permutations per document. As a comparison, we also show results for the same models on wSJ, using the train-test split from Elsner and Charniak (2008a); the test set is sections 14-24, totalling 1004 documents. Purandare and Litman (2008) carry out similar experiments on distinguishing permuted SwBD documents, using lexical and WordNet features in a model similar to Lapata (2003). Their accuracy for this task (which they call &amp;quot;switch-hard&amp;quot;) is roughly 68%. 1182 WSJ SWBD EGrid 76.4$ 86.0 Topical EGrid 71.8$ 70.9$ IBM-1 77.2$ 84.91 Pronouns 69.6$ 71.7$ Disc-new 72.3$ 55.0$ Combined 81.9 88.4 -EGrid 81.0 87.5 -Topical EGrid 82.2 90.5 -IBM-1 79.0$ 88.9 -Pronouns 81.3 88.5 -Disc-new 82.2 88.4 Table 1: Discrimination F scores on news and dialogu</context>
<context position="21649" citStr="Elsner and Charniak (2008" startWordPosition="3482" endWordPosition="3485"> and those where they were the same. (Each dialogue in ❙❲❇❉ focuses on a preselected topic, such as fishing or movies.) We entangle dialogues from our ordering development set to use for mixture training and validation; for testing, we use 100 instances of each type, constructed from dialogues in our test set. When disentangling, we treat each thread as independent of the others. In other words, the probability of the entire transcript is the product of the probabilities of the component threads. Our objective is to find the set of threads maximizing this. As a comparison, we use the model of Elsner and Charniak (2008b) as a baseline. To make their implementation comparable to ours, in this section we constrain it to find only two threads. 5.1 Disentangling a single utterance Our first disentanglement task is to correctly assign a single utterance, given the true structure of the rest of the transcript. For each utterance, we compare two versions of the transcript, the original, and a version where it is swapped into the other thread. Our accuracy measures how often our models prefer the original. Unlike full-scale disentanglement, this task does not require a computationally demanding search, so it is pos</context>
<context position="23472" citStr="Elsner and Charniak, 2008" startWordPosition="3787" endWordPosition="3790">4 57.9 Combined 86.8 79.6 83.2 -EGrid 86.0 79.1 82.6 -Topical EGrid 85.2 78.7 81.9 -IBM-1 86.2 78.7 82.4 -Pronouns 86.8 79.4 83.1 -Time 84.5 76.7 80.6 E+C `08 78.2 73.5 75.8 Table 2: Average accuracy for disentanglement of a single utterance on 200 synthetic multiparty conversations from SWBD test. our combined model, and finally ablation results for mixtures omitting a single model. The results show that, for a pair of dialogues that differ in topic, our best model can assign a single sentence with 87% accuracy. For the same topic, the accuracy is 80%. In each case, these results improve on (Elsner and Charniak, 2008b), which scores 78% and 74%. Changing to this new task has a substantial impact on performance. The topical model, which performed poorly for ordering, is actually stronger than the entity grid in this setting. IBM-1 underperforms either grid model (69% to 77%); on ordering, it was nearly as good (85% to 86%). Despite their ordering performance of 72%, pronouns are essentially useless for this task, at 52%. This decline is due partly to domain, and partly to task setting. Although ❙❲❇❉ contains more pronominals than ❲❙❏, many of them are first and second-person pronouns or deictics, which our</context>
<context position="27274" citStr="Elsner and Charniak (2008" startWordPosition="4426" endWordPosition="4429">ures percent corrects. Our results (Table 3) show that, for transcripts with different topics, our disentanglement has 68% over5The other popular metrics, F and l♦c 3, are correlated. Different Same Avg. EGrid 60.3 57.1 58.7 Topical EGrid 62.3 56.8 59.6 IBM-1 56.5 55.2 55.9 Pronouns 54.5 54.4 54.4 Time 55.4 53.8 54.6 Combined 67.9 59.8 63.9 E+C `08 59.1 57.4 58.3 Table 3: One-to-one overlap between disentanglement results and truth on 200 synthetic multiparty conversations from SWBD test. lap with truth, extracting about two thirds of the structure correctly; this is substantially better than Elsner and Charniak (2008b), which scores 59%. Where the entangled conversations have the same topic, performance is lower, about 60%, but still better than the comparison model with 57%. Since correlations with the previous section are fairly reliable, and the disentanglement procedure is computationally intensive, we omit ablation experiments. As we expect, full disentanglement is more difficult than single-sentence disentanglement (combined scores drop by about 20%), but the singlesentence task is a good predictor of relative performance. Entity grid models do best, the IBM model remains useful, but less so than fo</context>
<context position="30285" citStr="Elsner and Charniak (2008" startWordPosition="4919" endWordPosition="4922">aseline. This baseline is relatively strong, so we evaluate our other models in combination with it. 6.1 Disentangling a single sentence As before, we show results on correctly disentangling a single sentence, given the correct structure of the rest of the transcript. We average performance on each transcript over the different annotations, then average the transcripts, weighing them by length to give each utterance equal weight. Table 4 gives results on our development corpus, #LINUX. Our best result, for the chat-specific features plus entity grid, is 79%, improving on the comparison model, Elsner and Charniak (2008b), which gets 76%. (Although the table only presents an average over all annotations of the dataset, this model is also more accurate for each individual annotator than the comparison model.) We then ran the same model, chat-specific features plus entity grid, on the test corpora from Adams (2008). These results (Table 5) are also better than Elsner and Charniak (2008b), at an average of 93% over 89%. As pointed out in Elsner and Charniak (2008b), the chat-specific features are quite powerful in this domain, and it is hard to improve over them. Elsner and Charniak (2008b), which has simple le</context>
<context position="33313" citStr="Elsner and Charniak (2008" startWordPosition="5425" endWordPosition="5429">e beginning of a conversation than in the middle. A major weakness in our model is that we aim only to maximize coherence of the individual conversations, with no prior on the likely length or number of conversations that will appear in the tran1186 script. This allows the model to create far too many conversations. Integrating a prior into our framework is not straightforward because we currently train our mixture to maximize single-utterance disentanglement performance, and the prior is not useful for this task. We experimented with fixing parts of the transcript to the solution obtained by Elsner and Charniak (2008b), then using tabu search to fill in the gaps. This constrains the number of conversations and their approximate positions. With this structure in place, we were able to obtain scores comparable to Elsner and Charniak (2008b), but not improvements. It appears that our performance increase on single-sentence disentanglement does not transfer to this task because of cascading errors and the necessity of using external constraints. 7 Conclusions We demonstrate that several popular models of local coherence transfer well to the conversational domain, suggesting that they do indeed capture coheren</context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008a. Coreference-inspired coherence modeling. In Proceedings of ACL-08: HLT, Short Papers, pages 41-44, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>You talking to me? a corpus and algorithm for conversation disentanglement.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>834--842</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008b. You talking to me? a corpus and algorithm for conversation disentanglement. In Proceedings of ACL-08: HLT, pages 834-842, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas Landauer</author>
</authors>
<title>The measurement of textual coherence with latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="6201" citStr="Foltz et al., 1998" startWordPosition="950" endWordPosition="953">apata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we use as a baseline because there is a publicly available implementation✷. Adams (2008) also created and released a disentanglement corpus. They use LDA (Blei et al., 2001) to discover latent topics in their corpus, then measuring similarity by looking for shared topics. These features fail to improve their performance, which is puzzling in light of the success of topic modeling for other coherence and segmentation problems (Eisenstein and Barzilay, 2008; Foltz et al., 1998). The results of this study suggest that topic models can help with disentanglement, but that it is difficult to find useful topics for IRC chat. A few studies have attempted to disentangle conversational speech (Aoki et al., 2003; Aoki et al., 2006), mostly using temporal features. For the most part, however, this research has focused on auditory processing in the context of the cocktail party problem, the task of attending to a specific speaker in a noisy room (Haykin and Chen, 2005). Utterance content has some influence on what the listener perceives, but only for extremely salient cues suc</context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Peter Foltz, Walter Kintsch, and Thomas Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse Processes, 25(2&amp;3):285-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
</authors>
<title>cba to check the spelling: Investigating parser performance on discussion forum posts.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>381--384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="29451" citStr="Foster, 2010" startWordPosition="4780" endWordPosition="4781"> #IPHONE #PHYSICS #PYTHON +EGrid 92.3 96.6 91.1 E+C `08b 89.0 90.2 88.4 76.8 +IBM-1 +Pronouns 73.9 76.3 Table 5: Average accuracy for disentanglement of a single utterance for 19581 total lines from Adams (2008). 78.3 +EGrid/Topic/IBM-1 E+C `08b 76.4 Table 4: Accuracy for single utterance disentanglement, averaged over annotations of 800 lines of #LINUx data. In order to use syntactic models like the entity grid, we parse the transcripts using (McClosky et al., 2006). Performance is bad, although the parser does identify most of the NPs; poor results are typical for a standard parser on chat (Foster, 2010). We postprocess the parse trees to retag &amp;quot;lol&amp;quot;, &amp;quot;haha&amp;quot; and &amp;quot;yes&amp;quot; as UH (rather than NN, NNP and JJ). In this section, we use all three of our chatspecific models (sec. 2.0.6; time, speaker and mention) as a baseline. This baseline is relatively strong, so we evaluate our other models in combination with it. 6.1 Disentangling a single sentence As before, we show results on correctly disentangling a single sentence, given the correct structure of the rest of the transcript. We average performance on each transcript over the different annotations, then average the transcripts, weighing them by l</context>
</contexts>
<marker>Foster, 2010</marker>
<rawString>Jennifer Foster. 2010. cba to check the spelling: Investigating parser performance on discussion forum posts. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 381-384, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
<author>John Hale</author>
<author>Eugene Charniak</author>
</authors>
<title>A statistical approach to anaphora resolution.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>161--171</pages>
<publisher>Harcourt Brace.</publisher>
<location>Orlando, Florida.</location>
<contexts>
<context position="11529" citStr="Ge et al., 1998" startWordPosition="1850" endWordPosition="1853">red for coherence by Soricut and Marcu (2006), although a less probabilistically elegant version was proposed earlier (Lapata, 2003). This model attempts to generate the content words of the next sentence by translating them from the words of the previous sentence, plus a null word; thus, it will learn alignments between pairs of words that tend to occur in adjacent sentences. We learn parameters on the FISHER corpus, and on NANC for news. 3.4 Pronouns The use of a generative pronoun resolver for coherence modeling originates in Elsner and Charniak (2008a). That paper used a supervised model (Ge et al., 1998), but we adapt a newer, unsupervised model which they also make publicly available (Charniak and Elsner, 2009)4. They model each pronoun as generated by an antecedent somewhere in the previous two sentences. If a good antecedent is found, the probability of the pronoun&apos;s occurrence will be high; otherwise, the probability is low, signaling that the text is less coherent because the pronoun is hard to interpret correctly. We use the model as distributed for news text. For conversation, we adapt it by running a few iterations of their EM training algorithm on the FISHER data. 4bllip.cs.brown.edu</context>
</contexts>
<marker>Ge, Hale, Charniak, 1998</marker>
<rawString>Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical approach to anaphora resolution. In Proceedings of the Sixth Workshop on Very Large Corpora, pages 161-171, Orlando, Florida. Harcourt Brace.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Glover</author>
<author>Manuel Laguna</author>
</authors>
<date>1997</date>
<institution>Tabu Search. University of Colorado at Boulder.</institution>
<contexts>
<context position="26076" citStr="Glover and Laguna, 1997" startWordPosition="4227" endWordPosition="4230">e affect only a single sentence, while permutation affects the entire transcript, more history may help by making the model more sensitive to small changes. 5.2 Disentangling an entire transcript We now turn to the task of disentangling an entire transcript at once. This is a practical task, motivated by applications such as search and information retrieval. However, it is more difficult than assigning only a single utterance, because decisions are interrelated— an error on one utterance may cause a cascade of poor decisions further down. It is also computationally harder. We use tabu search (Glover and Laguna, 1997) to find a good solution. The search repeatedly finds and moves the utterance which would most improve the model score if swapped from one thread to the other. Unlike greedy search, tabu search is constrained not to repeat a solution that it has recently visited; this forces it to keep exploring when it reaches a local maximum. We run 500 iterations of tabu search (usually finding the first local maximum after about 100) and return the best solution found. We measure performance with one-to-one overlap, which maps the two clusters to the two gold dialogues, then measures percent corrects. Our </context>
</contexts>
<marker>Glover, Laguna, 1997</marker>
<rawString>Fred Glover and Manuel Laguna. 1997. Tabu Search. University of Colorado at Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--2</pages>
<contexts>
<context position="7729" citStr="Grosz et al., 1995" startWordPosition="1212" endWordPosition="1215">xperiments below, we train the models on SwBD, sometimes augmented with a larger set of automatically parsed conversations from the FISHER corpus. Since the two corpora are quite similar, FISHER is a useful source for extra data; McClosky et al. (2010) uses it for this purpose in parsing experiments. (We continue to use SwBD/FISHER even for experiments on IRC, because we do not have enough disentangled training data to learn lexical relationships.) 3.1 Entity grid The entity grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005) is an attempt to model some principles of Centering Theory (Grosz et al., 1995) in a statistical manner. It represents a document in terms of entities and their syntactic roles: subject (S), object (O), other (X) and not present (-). In each new 2cs.brown.edu/melsner 1180 utterance, the grid predicts the role in which each entity will appear, given its history of roles in the previous sentences, plus a salience feature counting the total number of times the entity occurs. For instance, for an entity which is the subject of sentence 1, the object of sentence 2, and occurs four times in total, the grid predicts its role in sentence 3 according to the conditional P(✁❥S, O,</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Haykin</author>
<author>Zhe Chen</author>
</authors>
<date>2005</date>
<booktitle>The Cocktail Party Problem. Neural Computation,</booktitle>
<pages>17--9</pages>
<contexts>
<context position="6691" citStr="Haykin and Chen, 2005" startWordPosition="1034" endWordPosition="1037">t of the success of topic modeling for other coherence and segmentation problems (Eisenstein and Barzilay, 2008; Foltz et al., 1998). The results of this study suggest that topic models can help with disentanglement, but that it is difficult to find useful topics for IRC chat. A few studies have attempted to disentangle conversational speech (Aoki et al., 2003; Aoki et al., 2006), mostly using temporal features. For the most part, however, this research has focused on auditory processing in the context of the cocktail party problem, the task of attending to a specific speaker in a noisy room (Haykin and Chen, 2005). Utterance content has some influence on what the listener perceives, but only for extremely salient cues such as the listener&apos;s name (Moray, 1959), so cocktail party research does not typically use lexical models. 3 Models In this section, we briefly describe the models we intend to evaluate. Most of them are drawn from previous work; one, the topical entity grid, is a novel extension of the entity grid. For the experiments below, we train the models on SwBD, sometimes augmented with a larger set of automatically parsed conversations from the FISHER corpus. Since the two corpora are quite si</context>
</contexts>
<marker>Haykin, Chen, 2005</marker>
<rawString>Simon Haykin and Zhe Chen. 2005. The Cocktail Party Problem. Neural Computation, 17(9):1875-1902.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikiforos Karamanis</author>
<author>Massimo Poesio</author>
<author>Chris Mellish</author>
<author>Jon Oberlander</author>
</authors>
<title>Evaluating centeringbased metrics of coherence.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>391--398</pages>
<contexts>
<context position="15340" citStr="Karamanis et al., 2004" startWordPosition="2456" endWordPosition="2460">each model Pi is assigned a weight Ai, and the combined score P(d) is proportional to: ❳ Ailog(Pi(d)) i The weights A can be learned discriminatively, maximizing the probability of d relative to a taskspecific contrast set. For ordering experiments, the contrast set is a single random permutation of d; we explain the training regime for disentanglement below, in subsection 4.1. 4 Comparing orderings of SWBD To measure the differences in performance caused by moving from news to a conversational domain, we first compare our models on an ordering task, discrimination (Barzilay and Lapata, 2005; Karamanis et al., 2004). In this task, we take an original document and randomly permute its sentences, creating an artificial incoherent document. We then test to see if our model prefers the coherent original. For SwBD, rather than compare permutations of the individual utterances, we permute conversational turns (sets of consecutive utterances by each speaker), since turns are natural discourse units in conversation. We take documents numbered 2000 3999 as training/development and the remainder as test, yielding 505 training and 153 test documents; we evaluate 20 permutations per document. As a comparison, we al</context>
</contexts>
<marker>Karamanis, Poesio, Mellish, Oberlander, 2004</marker>
<rawString>Nikiforos Karamanis, Massimo Poesio, Chris Mellish, and Jon Oberlander. 2004. Evaluating centeringbased metrics of coherence. In ACL, pages 391-398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic evaluation of text coherence: Models and representations.</title>
<date>2005</date>
<booktitle>In IICAI,</booktitle>
<pages>1085--1090</pages>
<contexts>
<context position="5559" citStr="Lapata and Barzilay, 2005" startWordPosition="846" endWordPosition="849">ive writing, so they reveal little about cross-domain flexibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available #LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measure similarity between two sentences; Wang and Oard (2009) use a message expansion technique to incorporate context beyond a single sentence. Unigram overlaps are used to model coherence, but more sophisticated methods using syntax (Lapata and Barzilay, 2005) or lexical features (Lapata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we use as a baseline because there is a publicly available implementation✷. Adams (2008) also created and released a disentanglement corpus. They use LDA (Blei et al., 2001) to discover latent topics in their corpus, then measuring similarity by looking for shared topics. These features fail to improve their performance, which is puzzling in light of the success of topic modeling for other coherence and segmentation problems (Eisenste</context>
<context position="7621" citStr="Lapata and Barzilay, 2005" startWordPosition="1193" endWordPosition="1196">hem are drawn from previous work; one, the topical entity grid, is a novel extension of the entity grid. For the experiments below, we train the models on SwBD, sometimes augmented with a larger set of automatically parsed conversations from the FISHER corpus. Since the two corpora are quite similar, FISHER is a useful source for extra data; McClosky et al. (2010) uses it for this purpose in parsing experiments. (We continue to use SwBD/FISHER even for experiments on IRC, because we do not have enough disentangled training data to learn lexical relationships.) 3.1 Entity grid The entity grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005) is an attempt to model some principles of Centering Theory (Grosz et al., 1995) in a statistical manner. It represents a document in terms of entities and their syntactic roles: subject (S), object (O), other (X) and not present (-). In each new 2cs.brown.edu/melsner 1180 utterance, the grid predicts the role in which each entity will appear, given its history of roles in the previous sentences, plus a salience feature counting the total number of times the entity occurs. For instance, for an entity which is the subject of sentence 1, the object of sentence 2, and</context>
</contexts>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>Mirella Lapata and Regina Barzilay. 2005. Automatic evaluation of text coherence: Models and representations. In IICAI, pages 1085-1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of the annual meeting of ACL,</booktitle>
<contexts>
<context position="5594" citStr="Lapata, 2003" startWordPosition="853" endWordPosition="855">omain flexibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available #LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measure similarity between two sentences; Wang and Oard (2009) use a message expansion technique to incorporate context beyond a single sentence. Unigram overlaps are used to model coherence, but more sophisticated methods using syntax (Lapata and Barzilay, 2005) or lexical features (Lapata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we use as a baseline because there is a publicly available implementation✷. Adams (2008) also created and released a disentanglement corpus. They use LDA (Blei et al., 2001) to discover latent topics in their corpus, then measuring similarity by looking for shared topics. These features fail to improve their performance, which is puzzling in light of the success of topic modeling for other coherence and segmentation problems (Eisenstein and Barzilay, 2008; Foltz et al.</context>
<context position="11045" citStr="Lapata, 2003" startWordPosition="1764" endWordPosition="1765">The Senate will consider the bill today.&amp;quot;. If &amp;quot;House&amp;quot; and &amp;quot;Senate&amp;quot; have a high similarity, then the feature will have a high value, predicting that &amp;quot;Senate&amp;quot; is a good subject for the current sentence. As in the previous section, we learn the conditional probabilities with logistic regression; we train in parallel by splitting the data and averaging (Mann et al., 2009). The topics are trained on FISHER, and on NANC for news. 3.3 IBM-1 The IBM translation model was first considered for coherence by Soricut and Marcu (2006), although a less probabilistically elegant version was proposed earlier (Lapata, 2003). This model attempts to generate the content words of the next sentence by translating them from the words of the previous sentence, plus a null word; thus, it will learn alignments between pairs of words that tend to occur in adjacent sentences. We learn parameters on the FISHER corpus, and on NANC for news. 3.4 Pronouns The use of a generative pronoun resolver for coherence modeling originates in Elsner and Charniak (2008a). That paper used a supervised model (Ge et al., 1998), but we adapt a newer, unsupervised model which they also make publicly available (Charniak and Elsner, 2009)4. The</context>
<context position="16274" citStr="Lapata (2003)" startWordPosition="2606" endWordPosition="2607">nces by each speaker), since turns are natural discourse units in conversation. We take documents numbered 2000 3999 as training/development and the remainder as test, yielding 505 training and 153 test documents; we evaluate 20 permutations per document. As a comparison, we also show results for the same models on wSJ, using the train-test split from Elsner and Charniak (2008a); the test set is sections 14-24, totalling 1004 documents. Purandare and Litman (2008) carry out similar experiments on distinguishing permuted SwBD documents, using lexical and WordNet features in a model similar to Lapata (2003). Their accuracy for this task (which they call &amp;quot;switch-hard&amp;quot;) is roughly 68%. 1182 WSJ SWBD EGrid 76.4$ 86.0 Topical EGrid 71.8$ 70.9$ IBM-1 77.2$ 84.91 Pronouns 69.6$ 71.7$ Disc-new 72.3$ 55.0$ Combined 81.9 88.4 -EGrid 81.0 87.5 -Topical EGrid 82.2 90.5 -IBM-1 79.0$ 88.9 -Pronouns 81.3 88.5 -Disc-new 82.2 88.4 Table 1: Discrimination F scores on news and dialogue.  indicates a significant difference from the combined model at p=.01 and  at p=.05. In Table 1, we show the results for individual models, for the combined model, and ablation results for mixtures without each component. WSJ is </context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the annual meeting of ACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Automatic evaluation of information ordering:</title>
<date>2006</date>
<booktitle>Kendall&apos;s tau. Computational Linguistics,</booktitle>
<pages>32--4</pages>
<contexts>
<context position="4701" citStr="Lapata, 2006" startWordPosition="718" endWordPosition="719"> coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the fluency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain flexibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available #LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measu</context>
</contexts>
<marker>Lapata, 2006</marker>
<rawString>Mirella Lapata. 2006. Automatic evaluation of information ordering: Kendall&apos;s tau. Computational Linguistics, 32(4):1-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Mann</author>
<author>Ryan McDonald</author>
<author>Mehryar Mohri</author>
<author>Nathan Silberman</author>
<author>Dan Walker</author>
</authors>
<title>Efficient largescale distributed training of conditional maximum entropy models.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems 22,</booktitle>
<pages>1231--1239</pages>
<editor>In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors,</editor>
<contexts>
<context position="10802" citStr="Mann et al., 2009" startWordPosition="1724" endWordPosition="1727">ious sentence. In the topical entity grid, we instead compute a real-valued feature which sums up the similarity between entity e and the subject(s) of the previous sentence. These features can detect a transition like: &amp;quot;The House voted yesterday. The Senate will consider the bill today.&amp;quot;. If &amp;quot;House&amp;quot; and &amp;quot;Senate&amp;quot; have a high similarity, then the feature will have a high value, predicting that &amp;quot;Senate&amp;quot; is a good subject for the current sentence. As in the previous section, we learn the conditional probabilities with logistic regression; we train in parallel by splitting the data and averaging (Mann et al., 2009). The topics are trained on FISHER, and on NANC for news. 3.3 IBM-1 The IBM translation model was first considered for coherence by Soricut and Marcu (2006), although a less probabilistically elegant version was proposed earlier (Lapata, 2003). This model attempts to generate the content words of the next sentence by translating them from the words of the previous sentence, plus a null word; thus, it will learn alignments between pairs of words that tend to occur in adjacent sentences. We learn parameters on the FISHER corpus, and on NANC for news. 3.4 Pronouns The use of a generative pronoun </context>
</contexts>
<marker>Mann, McDonald, Mohri, Silberman, Walker, 2009</marker>
<rawString>Gideon Mann, Ryan McDonald, Mehryar Mohri, Nathan Silberman, and Dan Walker. 2009. Efficient largescale distributed training of conditional maximum entropy models. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1231-1239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="29309" citStr="McClosky et al., 2006" startWordPosition="4753" endWordPosition="4756">ting; together, they consist of 19581 lines of chat, with each section containing 500 to 1000 lines. 1185 74.0 Chat-specific +EGrid +Topical EGrid 79.3 #IPHONE #PHYSICS #PYTHON +EGrid 92.3 96.6 91.1 E+C `08b 89.0 90.2 88.4 76.8 +IBM-1 +Pronouns 73.9 76.3 Table 5: Average accuracy for disentanglement of a single utterance for 19581 total lines from Adams (2008). 78.3 +EGrid/Topic/IBM-1 E+C `08b 76.4 Table 4: Accuracy for single utterance disentanglement, averaged over annotations of 800 lines of #LINUx data. In order to use syntactic models like the entity grid, we parse the transcripts using (McClosky et al., 2006). Performance is bad, although the parser does identify most of the NPs; poor results are typical for a standard parser on chat (Foster, 2010). We postprocess the parse trees to retag &amp;quot;lol&amp;quot;, &amp;quot;haha&amp;quot; and &amp;quot;yes&amp;quot; as UH (rather than NN, NNP and JJ). In this section, we use all three of our chatspecific models (sec. 2.0.6; time, speaker and mention) as a baseline. This baseline is relatively strong, so we evaluate our other models in combination with it. 6.1 Disentangling a single sentence As before, we show results on correctly disentangling a single sentence, given the correct structure of the rest</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152-159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Automatic domain adaptation for parsing. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="7362" citStr="McClosky et al. (2010)" startWordPosition="1151" endWordPosition="1154">e listener perceives, but only for extremely salient cues such as the listener&apos;s name (Moray, 1959), so cocktail party research does not typically use lexical models. 3 Models In this section, we briefly describe the models we intend to evaluate. Most of them are drawn from previous work; one, the topical entity grid, is a novel extension of the entity grid. For the experiments below, we train the models on SwBD, sometimes augmented with a larger set of automatically parsed conversations from the FISHER corpus. Since the two corpora are quite similar, FISHER is a useful source for extra data; McClosky et al. (2010) uses it for this purpose in parsing experiments. (We continue to use SwBD/FISHER even for experiments on IRC, because we do not have enough disentangled training data to learn lexical relationships.) 3.1 Entity grid The entity grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005) is an attempt to model some principles of Centering Theory (Grosz et al., 1995) in a statistical manner. It represents a document in terms of entities and their syntactic roles: subject (S), object (O), other (X) and not present (-). In each new 2cs.brown.edu/melsner 1180 utterance, the grid predicts the role </context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2010</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adaptation for parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 28-36, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
<author>K Kukich</author>
</authors>
<title>Evaluation of text coherence for electronic essay scoring systems.</title>
<date>2004</date>
<journal>Nat. Lang. Eng.,</journal>
<pages>10--1</pages>
<contexts>
<context position="4731" citStr="Miltsakaki and Kukich, 2004" startWordPosition="720" endWordPosition="723">ich relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the fluency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain flexibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available #LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measure similarity between two sent</context>
</contexts>
<marker>Miltsakaki, Kukich, 2004</marker>
<rawString>Eleni Miltsakaki and K. Kukich. 2004. Evaluation of text coherence for electronic essay scoring systems. Nat. Lang. Eng., 10(1):25-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neville Moray</author>
</authors>
<title>Attention in dichotic listening: Affective cues and the influence of instructions.</title>
<date>1959</date>
<journal>Quarterly Iournal of Experimental Psychology,</journal>
<pages>11--1</pages>
<contexts>
<context position="6839" citStr="Moray, 1959" startWordPosition="1060" endWordPosition="1061">y suggest that topic models can help with disentanglement, but that it is difficult to find useful topics for IRC chat. A few studies have attempted to disentangle conversational speech (Aoki et al., 2003; Aoki et al., 2006), mostly using temporal features. For the most part, however, this research has focused on auditory processing in the context of the cocktail party problem, the task of attending to a specific speaker in a noisy room (Haykin and Chen, 2005). Utterance content has some influence on what the listener perceives, but only for extremely salient cues such as the listener&apos;s name (Moray, 1959), so cocktail party research does not typically use lexical models. 3 Models In this section, we briefly describe the models we intend to evaluate. Most of them are drawn from previous work; one, the topical entity grid, is a novel extension of the entity grid. For the experiments below, we train the models on SwBD, sometimes augmented with a larger set of automatically parsed conversations from the FISHER corpus. Since the two corpora are quite similar, FISHER is a useful source for extra data; McClosky et al. (2010) uses it for this purpose in parsing experiments. (We continue to use SwBD/FI</context>
</contexts>
<marker>Moray, 1959</marker>
<rawString>Neville Moray. 1959. Attention in dichotic listening: Affective cues and the influence of instructions. Quarterly Iournal of Experimental Psychology, 11(1):56-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>References to named entities: a corpus study.</title>
<date>2003</date>
<booktitle>In NAACL &apos;03,</booktitle>
<pages>70--72</pages>
<contexts>
<context position="12247" citStr="Nenkova and McKeown, 2003" startWordPosition="1963" endWordPosition="1966"> and Elsner, 2009)4. They model each pronoun as generated by an antecedent somewhere in the previous two sentences. If a good antecedent is found, the probability of the pronoun&apos;s occurrence will be high; otherwise, the probability is low, signaling that the text is less coherent because the pronoun is hard to interpret correctly. We use the model as distributed for news text. For conversation, we adapt it by running a few iterations of their EM training algorithm on the FISHER data. 4bllip.cs.brown.edu/resources.shtml\ #software 1181 3.5 Discourse-newness Building on work from summarization (Nenkova and McKeown, 2003) and coreference resolution (Poesio et al., 2005), Elsner and Charniak (2008a) use a model which recognizes discourse-new versus old NPs as a coherence model. For instance, the model can learn that &amp;quot;President Barack Obama&amp;quot; is a more likely first reference than &amp;quot;Obama&amp;quot;. Following their work, we score discourse-newness with a maximum-entropy classifier using syntactic features counting different types of NP modifiers, and we use NP head identity as a proxy for coreference. 3.6 Chat-specific features Most disentanglement models use non-linguistic information alongside lexical features; in fact, t</context>
</contexts>
<marker>Nenkova, McKeown, 2003</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2003. References to named entities: a corpus study. In NAACL &apos;03, pages 70-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malvina Nissim</author>
</authors>
<title>Learning information status of discourse entities.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>94--102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="19071" citStr="Nissim, 2006" startWordPosition="3053" endWordPosition="3054">ngle model, showing the information provided by the weaker models is not completely redundant. Overall, these results suggest that most previously proposed local coherence models are domaingeneral; they work on conversation as well as news. The exception is the discourse-newness model, which benefits most from the specific conventions of a written style. Full names with titles (like &amp;quot;President Barack Obama&amp;quot;) are more common in news, while conversation tends to involve fewer completely unfamiliar entities and more cases of bridging reference, in which grounding information is given implicitly (Nissim, 2006). Due to its poor performance, we omit the discourse-newness model in our remaining experiments. 5 Disentangling SWBD We now turn to the task of disentanglement, testing whether models that are good at ordering also do well in this new setting. We would like to hold the domain constant, but we do not have any disentanglement data recorded from naturally occurring speech, so we create synthetic instances by merging pairs of SWBD dialogues. Doing so creates an artificial transcript in which two pairs of people appear to be talking simultaneously over a shared channel. The situation is somewhat c</context>
</contexts>
<marker>Nissim, 2006</marker>
<rawString>Malvina Nissim. 2006. Learning information status of discourse entities. In Proceedings of EMNLP, pages 94-102, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacki O&apos;Neill</author>
<author>David Martin</author>
</authors>
<title>Text chat in action.</title>
<date>2003</date>
<booktitle>In GROUP &apos;03: Proceedings of the 2003 international ACM SIGGROUP conference on Supporting group work,</booktitle>
<pages>40--49</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="14027" citStr="O&apos;Neill and Martin, 2003" startWordPosition="2245" endWordPosition="2248">r datapoints. The second feature is speaker identity; conversations usually involve a small subset of the total number of speakers, and a few core speakers make most of the utterances. We model the distribution of speakers in each conversation using a Chinese Restaurant Process (CRP) (Aldous, 1985) (tuning the dispersion a to maximize development peformance). The CRP&apos;s &amp;quot;rich-get-richer&amp;quot; dynamics capture our intuitions, favoring conversations dominated by a few vociferous speakers. Finally, we model name mentioning. Speakers in IRC chat often use their addressee&apos;s names to coordinate the chat (O&apos;Neill and Martin, 2003), and this is a powerful source of information (Elsner and Charniak, 2008b). Our model classifies each utterance into either the start or continuation of a conversational turn, by checking if the previous utterance had the same speaker. Given this status, it computes probabilities for three outcomes: no name mention, a mention of someone who has previously spoken in the conversation, or a mention of someone else. (The third option is extremely rare; this accounts for most of the model&apos;s predictive power). We learn these probabilities from IRC training data. 3.7 Model combination To combine the</context>
</contexts>
<marker>O&apos;Neill, Martin, 2003</marker>
<rawString>Jacki O&apos;Neill and David Martin. 2003. Text chat in action. In GROUP &apos;03: Proceedings of the 2003 international ACM SIGGROUP conference on Supporting group work, pages 40-49, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>186--195</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="4687" citStr="Pitler and Nenkova, 2008" startWordPosition="714" endWordPosition="717">focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the fluency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain flexibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available #LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping uni</context>
</contexts>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 186-195, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Mijail Alexandrov-Kabadjov</author>
<author>Renata Vieira</author>
<author>Rodrigo Goulart</author>
<author>Olga Uryupina</author>
</authors>
<title>Does discourse-new detection help definite description resolution?</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Workshop on Computational Semantics, Tillburg.</booktitle>
<contexts>
<context position="12296" citStr="Poesio et al., 2005" startWordPosition="1970" endWordPosition="1973">ed by an antecedent somewhere in the previous two sentences. If a good antecedent is found, the probability of the pronoun&apos;s occurrence will be high; otherwise, the probability is low, signaling that the text is less coherent because the pronoun is hard to interpret correctly. We use the model as distributed for news text. For conversation, we adapt it by running a few iterations of their EM training algorithm on the FISHER data. 4bllip.cs.brown.edu/resources.shtml\ #software 1181 3.5 Discourse-newness Building on work from summarization (Nenkova and McKeown, 2003) and coreference resolution (Poesio et al., 2005), Elsner and Charniak (2008a) use a model which recognizes discourse-new versus old NPs as a coherence model. For instance, the model can learn that &amp;quot;President Barack Obama&amp;quot; is a more likely first reference than &amp;quot;Obama&amp;quot;. Following their work, we score discourse-newness with a maximum-entropy classifier using syntactic features counting different types of NP modifiers, and we use NP head identity as a proxy for coreference. 3.6 Chat-specific features Most disentanglement models use non-linguistic information alongside lexical features; in fact, timestamps and speaker identities are usually bett</context>
</contexts>
<marker>Poesio, Alexandrov-Kabadjov, Vieira, Goulart, Uryupina, 2005</marker>
<rawString>Massimo Poesio, Mijail Alexandrov-Kabadjov, Renata Vieira, Rodrigo Goulart, and Olga Uryupina. 2005. Does discourse-new detection help definite description resolution? In Proceedings of the Sixth International Workshop on Computational Semantics, Tillburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amruta Purandare</author>
<author>Diane J Litman</author>
</authors>
<title>Analyzing dialog coherence using transition patterns in lexical and semantic features.</title>
<date>2008</date>
<booktitle>In FLAIRS Conference&apos;08,</booktitle>
<pages>195--200</pages>
<contexts>
<context position="16130" citStr="Purandare and Litman (2008)" startWordPosition="2581" endWordPosition="2584">rs the coherent original. For SwBD, rather than compare permutations of the individual utterances, we permute conversational turns (sets of consecutive utterances by each speaker), since turns are natural discourse units in conversation. We take documents numbered 2000 3999 as training/development and the remainder as test, yielding 505 training and 153 test documents; we evaluate 20 permutations per document. As a comparison, we also show results for the same models on wSJ, using the train-test split from Elsner and Charniak (2008a); the test set is sections 14-24, totalling 1004 documents. Purandare and Litman (2008) carry out similar experiments on distinguishing permuted SwBD documents, using lexical and WordNet features in a model similar to Lapata (2003). Their accuracy for this task (which they call &amp;quot;switch-hard&amp;quot;) is roughly 68%. 1182 WSJ SWBD EGrid 76.4$ 86.0 Topical EGrid 71.8$ 70.9$ IBM-1 77.2$ 84.91 Pronouns 69.6$ 71.7$ Disc-new 72.3$ 55.0$ Combined 81.9 88.4 -EGrid 81.0 87.5 -Topical EGrid 82.2 90.5 -IBM-1 79.0$ 88.9 -Pronouns 81.3 88.5 -Disc-new 82.2 88.4 Table 1: Discrimination F scores on news and dialogue.  indicates a significant difference from the combined model at p=.01 and  at p=.05. </context>
</contexts>
<marker>Purandare, Litman, 2008</marker>
<rawString>Amruta Purandare and Diane J. Litman. 2008. Analyzing dialog coherence using transition patterns in lexical and semantic features. In FLAIRS Conference&apos;08, pages 195-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Qiang Yang</author>
<author>Jian-Tao Sun</author>
<author>Zheng Chen</author>
</authors>
<title>Thread detection in dynamic text message streams.</title>
<date>2006</date>
<booktitle>In SIGIR &apos;06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>35--42</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5097" citStr="Shen et al. (2006)" startWordPosition="773" endWordPosition="776">tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the fluency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain flexibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available #LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measure similarity between two sentences; Wang and Oard (2009) use a message expansion technique to incorporate context beyond a single sentence. Unigram overlaps are used to model coherence, but more sophisticated methods using syntax (Lapata and Barzilay, 2005) or lexical features (Lapata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and </context>
</contexts>
<marker>Shen, Yang, Sun, Chen, 2006</marker>
<rawString>Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen. 2006. Thread detection in dynamic text message streams. In SIGIR &apos;06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 3542, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Discourse generation using utility-trained coherence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics Conference (ACL-2006).</booktitle>
<contexts>
<context position="10958" citStr="Soricut and Marcu (2006)" startWordPosition="1751" endWordPosition="1754">f the previous sentence. These features can detect a transition like: &amp;quot;The House voted yesterday. The Senate will consider the bill today.&amp;quot;. If &amp;quot;House&amp;quot; and &amp;quot;Senate&amp;quot; have a high similarity, then the feature will have a high value, predicting that &amp;quot;Senate&amp;quot; is a good subject for the current sentence. As in the previous section, we learn the conditional probabilities with logistic regression; we train in parallel by splitting the data and averaging (Mann et al., 2009). The topics are trained on FISHER, and on NANC for news. 3.3 IBM-1 The IBM translation model was first considered for coherence by Soricut and Marcu (2006), although a less probabilistically elegant version was proposed earlier (Lapata, 2003). This model attempts to generate the content words of the next sentence by translating them from the words of the previous sentence, plus a null word; thus, it will learn alignments between pairs of words that tend to occur in adjacent sentences. We learn parameters on the FISHER corpus, and on NANC for news. 3.4 Pronouns The use of a generative pronoun resolver for coherence modeling originates in Elsner and Charniak (2008a). That paper used a supervised model (Ge et al., 1998), but we adapt a newer, unsup</context>
<context position="14709" citStr="Soricut and Marcu (2006)" startWordPosition="2354" endWordPosition="2357"> Charniak, 2008b). Our model classifies each utterance into either the start or continuation of a conversational turn, by checking if the previous utterance had the same speaker. Given this status, it computes probabilities for three outcomes: no name mention, a mention of someone who has previously spoken in the conversation, or a mention of someone else. (The third option is extremely rare; this accounts for most of the model&apos;s predictive power). We learn these probabilities from IRC training data. 3.7 Model combination To combine these different models, we adopt the log-linear framework of Soricut and Marcu (2006). Here, each model Pi is assigned a weight Ai, and the combined score P(d) is proportional to: ❳ Ailog(Pi(d)) i The weights A can be learned discriminatively, maximizing the probability of d relative to a taskspecific contrast set. For ordering experiments, the contrast set is a single random permutation of d; we explain the training regime for disentanglement below, in subsection 4.1. 4 Comparing orderings of SWBD To measure the differences in performance caused by moving from news to a conversational domain, we first compare our models on an ordering task, discrimination (Barzilay and Lapata</context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>Radu Soricut and Daniel Marcu. 2006. Discourse generation using utility-trained coherence models. In Proceedings of the Association for Computational Linguistics Conference (ACL-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidan Wang</author>
<author>Douglas W Oard</author>
</authors>
<title>Context-based message expansion for disentanglement of interleaved text conversations.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-09.</booktitle>
<contexts>
<context position="5248" citStr="Wang and Oard (2009)" startWordPosition="798" endWordPosition="801"> by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain flexibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available #LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measure similarity between two sentences; Wang and Oard (2009) use a message expansion technique to incorporate context beyond a single sentence. Unigram overlaps are used to model coherence, but more sophisticated methods using syntax (Lapata and Barzilay, 2005) or lexical features (Lapata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we use as a baseline because there is a publicly available implementation✷. Adams (2008) also created and released a disentangl</context>
</contexts>
<marker>Wang, Oard, 2009</marker>
<rawString>Lidan Wang and Douglas W. Oard. 2009. Context-based message expansion for disentanglement of interleaved text conversations. In Proceedings of NAACL-09.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>