<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003532">
<title confidence="0.991885">
A text-based search interface for Multimedia Dialectics
</title>
<author confidence="0.97394">
Katerina Pastra
</author>
<affiliation confidence="0.933678">
Inst. for Language &amp; Speech Processing
</affiliation>
<address confidence="0.732464">
Athens, Greece
</address>
<email confidence="0.986986">
kpastra@ilsp.gr
</email>
<author confidence="0.828138">
Eirini Balta
</author>
<affiliation confidence="0.802999">
Inst. for Language &amp; Speech Processing
</affiliation>
<address confidence="0.726843">
Athens, Greece
</address>
<email confidence="0.990759">
ebalta@ilsp.gr
</email>
<sectionHeader confidence="0.993643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99982275">
The growing popularity of multimedia
documents requires language technologies
to approach automatic language analysis
and generation from yet another perspec-
tive: that of its use in multimodal commu-
nication. In this paper, we present a sup-
port tool for COSMOROE, a theoretical
framework for modelling multimedia di-
alectics. The tool is a text-based search in-
terface that facilitates the exploration of a
corpus of audiovisual files, annotated with
the COSMOROE relations.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921225">
Online multimedia content becomes more and
more accessible through digital TV, social net-
working sites and searchable digital libraries of
photographs and videos. People of different ages
and cultures attempt to make sense out of this data
and re-package it for their own needs, these being
informative, educational and entertainment ones.
Understanding and generation of multimedia dis-
course requires knowledge and skills related to the
nature of the interacting modalities and their se-
mantic interplay for formulating the multimedia
message.
Within such context, intelligent multimedia sys-
tems are expected to parse/generate such messages
or at least assist humans in these tasks. From an-
other perspective, everyday human communica-
tion is predominantly multimodal; as such, sim-
ilarly intuitive human-computer/robot interaction
demands that intelligent systems master —among
others— the semantic interplay between differ-
ent media and modalities, i.e. they are able to
use/understand natural language and its reference
to objects and activities in the shared, situated
communication space.
It was more than a decade ago, when the lack
of a theory of how different media interact with
one another was indicated (Whittaker and Walker,
1991). Recently, such theoretical framework has
been developed and used for annotating a corpus
of audiovisual documents with the objective of us-
ing such corpus for developing multimedia infor-
mation processing tools (Pastra, 2008). In this pa-
per, we provide a brief overview of the theory and
the corresponding annotated corpus and present
a text-based search interface that has been devel-
oped for the exploration and the automatic expan-
sion/generalisation of the annotated semantic rela-
tions. This search interface is a support tool for
the theory and the related corpus and a first step
towards its computational exploitation.
</bodyText>
<sectionHeader confidence="0.995586" genericHeader="introduction">
2 COSMOROE
</sectionHeader>
<bodyText confidence="0.950924703703703">
The CrOSs-Media inteRactiOn rElations (COS-
MOROE) framework describes multimedia di-
alectics, i.e. the semantic interplay between
images, language and body movements (Pastra,
2008). It uses an analogy to language discourse
analysis for “talking” about multimedia dialectics.
It actually borrows terms that are widely used in
language analysis for describing a number of phe-
nomena (e.g. metonymy, adjuncts etc.) and adopts
a message-formation perspective which is remi-
niscent of structuralistic approaches in language
description. While doing so, inherent character-
istics of the different modalities (e.g. exhaustive
specificity of images) are taken into consideration.
COSMOROE is the result of a thorough,
inter-disciplinary review of image-language and
gesture-language interaction relations and charac-
teristics, as described across a number of disci-
plines from computational and semiotic perspec-
tives. It is also the result of observation and analy-
sis of different types of corpora for different tasks.
COSMOROE was tested for its coverage and de-
scriptive power through the annotation of a corpus
of TV travel documentaries. Figure 1 presents the
COSMOROE relations. There are three main rela-
Proceedings of the EACL 2009 Demonstrations Session, pages 53–56,
Athens, Greece, 3 April 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.99586">
53
</page>
<bodyText confidence="0.988105">
tions: semantic equivalence, complementarity and
independence, each with each own subtypes.
</bodyText>
<figureCaption confidence="0.999714">
Figure 1: The COSMOROE cross-media relations
</figureCaption>
<bodyText confidence="0.999974608695652">
For annotating a corpus with the COSMOROE
relations, a multi-faceted annotation scheme is
employed. COSMOROE relations link two or
more annotation facets, i.e. the modalities of
two or more different media. Time offsets
of the transcribed speech, subtitles, graphic-text
and scene text, body movements, gestures, shots
(with foreground and background distinction) and
keyframe-regions are identified and included in
COSMOROE relations. All visual data have been
labelled by the annotators with one or two-word
action or entity denoting tags. These labels have
resulted from a process of watching only the vi-
sual stream of the file. The labelling followed a
cognitive categorisation approach, that builds on
the “basic level theory” of categorisation (Rosch,
1978). Currently, the annotated corpus consists
of 5 hours of TV travel documentaries in Greek
and 5 hours of TV travel documentaries in En-
glish. Three hours of the Greek files have under-
gone validation and a preliminary inter-annotator
agreement study has also been carried out (Pastra,
2008).
</bodyText>
<sectionHeader confidence="0.994633" genericHeader="method">
3 The COSMOROE Search Interface
</sectionHeader>
<bodyText confidence="0.988684">
Such rich semantically annotated multimedia cor-
pus requires a support tool that will serve the fol-
lowing:
</bodyText>
<listItem confidence="0.960557904761905">
• it will facilitate the active exploration and
presentation of the semantic interplay be-
tween different modalities for any user, illus-
trating the COSMOROE theory through spe-
cific examples from real audiovisual data
• it will serve as simple search interface for
general users, taking advantage of the rich se-
mantic annotation —behind the scenes— for
more precise and intelligent retrieval of au-
diovisual files
• it will allow for observation and educated
decision-taking on how one could proceed
with mining the corpus or using it as train-
ing data for semantic multimedia processing
applications, and
• it will allow interfacing with semantic lexical
resources, computational lexicons, text pro-
cessing components and cross-lingual infor-
mation resources for automatically expand-
ing and generalising the data (semantic rela-
tions) one can mine from the corpus.
</listItem>
<bodyText confidence="0.998160692307692">
We have developed such tool, the COSMOROE
search interface. The interface itself is actually
a text-based search engine, that indexes and re-
trieves information from the COSMOROE anno-
tated corpus. The interface allows for both sim-
ple search and advanced search, depending on the
type and needs of the users. The advanced search
is designed for those who have a special interest
in multimedia semantics and/or ones who want to
develop systems that will be trained on the COS-
MOROE corpus. This advanced version allows
search in a text-based manner, in either of these
ways:
</bodyText>
<listItem confidence="0.982730625">
• Search using single or multiword query terms
(keywords) that are mentioned in the tran-
scribed speech (or other text) of the video
or in the visual labels set of its visual-units,
in order to find instantiations of different se-
mantic relations in which they participate;
• Search using a pair of single or multi-
word query terms (keywords) that are related
through (un)specified semantic relations;
• Search for specific types of relations and find
out how these are realized through actual in-
stances in a certain multimedia context;
• Search for specific modality types (e.g. spe-
cific types of gestures, image-regions etc.)
and find out all the different relations in
which they appear;
</listItem>
<page confidence="0.998067">
54
</page>
<figureCaption confidence="0.877787625">
Figure 2 presents a search example, using the
advanced interface1. The user has opted to search
for all instances of the word “bell” appearing in
the visual label of keyframe regions and/or video
shots and in particular ones in which the bell is
clearly shown either in the foreground or in the
background. In a similar way, the user can search
Figure 2: Search example
</figureCaption>
<bodyText confidence="0.99976472">
for concepts present in the audio part of the video,
through the use of the “Transcribed Text” option or
make a multiple selection. Another possibility is
to use a “Query 2” set, in conjunction, disjunction
or negation with “Query 1”, in order to obtain the
relations through which two categories of concepts
are associated.
Multimedia relations can also be searched in-
dependently of their content, simply denoting the
desired type. Finally, the user can search for spe-
cial types of visual-units, such as body move-
ments, gestures, images, without defining the con-
cept they denote.
After executing the query, the user is presented
with the list of the results, grouped by the semantic
relation in which the visual labels —in the exam-
ple case presented above— participate. Each hit
is accompanied by its transcribed speech. Indica-
tion of the number of results found is given and
the user has also the option to save the results of
the specific search session. By clicking on indi-
vidual hits in the result list, one may investigate
the corresponding relation particulars.
Figure 3 shows such detailed view of one of the
results of the query shown in Figure 2. All relation
</bodyText>
<footnote confidence="0.849171">
1Only part of the advanced search interface is depicted for
the screenshot to be intelligible
</footnote>
<figureCaption confidence="0.998031">
Figure 3: Example result - relation template
</figureCaption>
<bodyText confidence="0.99997976">
components are presented, textual and visual ones.
There are links to the video file from which the re-
lation comes, at the specified time offsets. Also,
the user may watch the video clips of the modali-
ties that participate in the relation (e.g. a particu-
lar gesture) and/or a static image (keyframe) of a
participating image region (e.g. a specific object)
with the contour of the object highlighted.
In this example, one may see that the word
“monastery”, which was mentioned in the tran-
scribed speech of the file, is grounded to the video
sequence depicting a “bell tower” in the back-
ground and to another image of a “bell”, through
a metonymic relation of type “part for whole”.
What is actually happening, from a semantic point
of view, is that although the video talks about a
“monastery”, it never actually shows the building,
it shows a characteristic part of it instead. In this
page, the option to save these relation elements as
a text file, is also provided.
Last, a user may get a quantified profile of the
contents of the database (the annotated corpus) in
terms of number of relations per type, per lan-
guage, per file, or even per file producer, number
of visual objects, gestures of different types, body
</bodyText>
<page confidence="0.996727">
55
</page>
<bodyText confidence="0.9999282">
movements, word tokens, visual labels, frequen-
cies of such data per file/set of files, as well as co-
occurrence statistics on word-visual label pairs per
relation/file/language and other parameters.
For the novice or general user, a simple inter-
face is provided that allows the user to submit
a text query, with no other specifications. The
results consist of a hit list with thumbnails of
the video-clips related to the query and the cor-
responding transcribed utterance. Individual hits
lead to full viewing of the video clip. Further de-
tails on the hit, i.e. information an advanced user
would get, are available following the advance-
information link. The use of semantic relations in
multimedia data, in this case, is hidden in the way
results are sorted in the results list. The sorting fol-
lows a highly to less informative pattern relying
on whether the transcript words or visual labels
matched to the query participate in cross-media
relations or not, and in which relation. Automat-
ing the processing of audiovisual files for the ex-
traction of cross-media semantics, in order to get
this type of “intelligence” in search and retrieval
within digital video archives, is the ultimate ob-
jective of the COSMOROE approach.
</bodyText>
<subsectionHeader confidence="0.995484">
3.1 Technical Details
</subsectionHeader>
<bodyText confidence="0.990769090909091">
In developing the COSMOROE search interface,
specific application needs had to be taken into
consideration. The main goal was to develop a
text-based search engine module capable of han-
dling files in the .xml format and accessed by lo-
cal and remote users. The core implementation
is actually a web application, mainly based on
the Apache Lucene2 search engine library. This
choice is supported by Lucene’s intrinsic charac-
teristics, such as high-performance indexing and
searching, scalability and customization options
and open source, cross-platform implementation,
that render it one of the most suitable solutions for
text-based search.
In particular, we exploited and further devel-
oped the built-in features of Lucene, in order to
meet our design criteria:
• The relation specific .xml files were indexed
in a way that retained their internal tree
structure, while multilingual files can eas-
ily be handled during indexing and searching
phases;
</bodyText>
<footnote confidence="0.905284">
2http://lucene.apache.org/
</footnote>
<listItem confidence="0.6774902">
• The queries are formed in a text-like man-
ner by the user, but are treated in a combined
way by the system, that enables a relational
search, enhanced with linguistic capabilities;
• The results are shown using custom sorting
methods, making them more presentable and
easily browsed by the user;
• Since Lucene is written in Java the applica-
tion is basically platform-independent;
• The implementation of the Lucene search en-
</listItem>
<bodyText confidence="0.950348444444444">
gine as a web application makes it easily ac-
cessible to local and remote users, through a
simple web browser page.
During the results presentation phase, a special
issue had to be taken into consideration, that is
video sharing. Due to performance and security
reasons, the Red53 server is used, which is an open
source flash server, supporting secure streaming
video.
</bodyText>
<sectionHeader confidence="0.6812445" genericHeader="method">
4 Conclusion: towards computational
modelling of multimedia dialectics
</sectionHeader>
<bodyText confidence="0.999986666666667">
The COSMOROE search interface presented in
this paper is the first phase for supporting the
computational modelling of multimedia dialectics.
The tool aims at providing a user-friendly access
to the COSMOROE corpus, illustrating the theory
through specific examples and providing an inter-
face platform for reaching towards computational
linguistic resources and tools that will generalise
over the semantic information provided by the cor-
pus. Last, the tool illustrates the hidden intelli-
gence one could achieve with cross-media seman-
tics in search engines of the future.
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999540090909091">
K. Pastra. 2008. Cosmoroe: A cross-media rela-
tions framework for modelling multimedia dialec-
tics. Multimedia Systems, 14:299–323.
E. Rosch. 1978. Principles of categorization. In
E. Rosch and B. Lloyd, editors, Cognition and Cat-
egorization, chapter 2, pages 27–48. Lawrence Erl-
baum Associates.
S. Whittaker and M. Walker. 1991. Toward a theory
of multi-modal interaction. In Proceedings of the
National Conference on Artificial Intelligence Work-
shop on Multi-modal Interaction.
</reference>
<footnote confidence="0.786139">
3http://osflash.org/red5/
</footnote>
<page confidence="0.996798">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.555096">
<title confidence="0.99399">A text-based search interface for Multimedia Dialectics</title>
<author confidence="0.996481">Katerina Pastra</author>
<affiliation confidence="0.987174">Inst. for Language &amp; Speech Processing</affiliation>
<address confidence="0.979671">Athens, Greece</address>
<email confidence="0.991036">kpastra@ilsp.gr</email>
<affiliation confidence="0.8090465">Eirini Balta Inst. for Language &amp; Speech Processing</affiliation>
<address confidence="0.982797">Athens, Greece</address>
<email confidence="0.995675">ebalta@ilsp.gr</email>
<abstract confidence="0.995527923076923">The growing popularity of multimedia documents requires language technologies to approach automatic language analysis and generation from yet another perspective: that of its use in multimodal communication. In this paper, we present a support tool for COSMOROE, a theoretical framework for modelling multimedia dialectics. The tool is a text-based search interface that facilitates the exploration of a corpus of audiovisual files, annotated with the COSMOROE relations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Pastra</author>
</authors>
<title>Cosmoroe: A cross-media relations framework for modelling multimedia dialectics. Multimedia Systems,</title>
<date>2008</date>
<pages>14--299</pages>
<contexts>
<context position="2183" citStr="Pastra, 2008" startWordPosition="317" endWordPosition="318">ent systems master —among others— the semantic interplay between different media and modalities, i.e. they are able to use/understand natural language and its reference to objects and activities in the shared, situated communication space. It was more than a decade ago, when the lack of a theory of how different media interact with one another was indicated (Whittaker and Walker, 1991). Recently, such theoretical framework has been developed and used for annotating a corpus of audiovisual documents with the objective of using such corpus for developing multimedia information processing tools (Pastra, 2008). In this paper, we provide a brief overview of the theory and the corresponding annotated corpus and present a text-based search interface that has been developed for the exploration and the automatic expansion/generalisation of the annotated semantic relations. This search interface is a support tool for the theory and the related corpus and a first step towards its computational exploitation. 2 COSMOROE The CrOSs-Media inteRactiOn rElations (COSMOROE) framework describes multimedia dialectics, i.e. the semantic interplay between images, language and body movements (Pastra, 2008). It uses an</context>
<context position="5122" citStr="Pastra, 2008" startWordPosition="753" endWordPosition="754">l visual data have been labelled by the annotators with one or two-word action or entity denoting tags. These labels have resulted from a process of watching only the visual stream of the file. The labelling followed a cognitive categorisation approach, that builds on the “basic level theory” of categorisation (Rosch, 1978). Currently, the annotated corpus consists of 5 hours of TV travel documentaries in Greek and 5 hours of TV travel documentaries in English. Three hours of the Greek files have undergone validation and a preliminary inter-annotator agreement study has also been carried out (Pastra, 2008). 3 The COSMOROE Search Interface Such rich semantically annotated multimedia corpus requires a support tool that will serve the following: • it will facilitate the active exploration and presentation of the semantic interplay between different modalities for any user, illustrating the COSMOROE theory through specific examples from real audiovisual data • it will serve as simple search interface for general users, taking advantage of the rich semantic annotation —behind the scenes— for more precise and intelligent retrieval of audiovisual files • it will allow for observation and educated deci</context>
</contexts>
<marker>Pastra, 2008</marker>
<rawString>K. Pastra. 2008. Cosmoroe: A cross-media relations framework for modelling multimedia dialectics. Multimedia Systems, 14:299–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rosch</author>
</authors>
<title>Principles of categorization.</title>
<date>1978</date>
<booktitle>Cognition and Categorization, chapter 2,</booktitle>
<pages>27--48</pages>
<editor>In E. Rosch and B. Lloyd, editors,</editor>
<contexts>
<context position="4834" citStr="Rosch, 1978" startWordPosition="706" endWordPosition="707">ts, i.e. the modalities of two or more different media. Time offsets of the transcribed speech, subtitles, graphic-text and scene text, body movements, gestures, shots (with foreground and background distinction) and keyframe-regions are identified and included in COSMOROE relations. All visual data have been labelled by the annotators with one or two-word action or entity denoting tags. These labels have resulted from a process of watching only the visual stream of the file. The labelling followed a cognitive categorisation approach, that builds on the “basic level theory” of categorisation (Rosch, 1978). Currently, the annotated corpus consists of 5 hours of TV travel documentaries in Greek and 5 hours of TV travel documentaries in English. Three hours of the Greek files have undergone validation and a preliminary inter-annotator agreement study has also been carried out (Pastra, 2008). 3 The COSMOROE Search Interface Such rich semantically annotated multimedia corpus requires a support tool that will serve the following: • it will facilitate the active exploration and presentation of the semantic interplay between different modalities for any user, illustrating the COSMOROE theory through s</context>
</contexts>
<marker>Rosch, 1978</marker>
<rawString>E. Rosch. 1978. Principles of categorization. In E. Rosch and B. Lloyd, editors, Cognition and Categorization, chapter 2, pages 27–48. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Whittaker</author>
<author>M Walker</author>
</authors>
<title>Toward a theory of multi-modal interaction.</title>
<date>1991</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence Workshop on Multi-modal Interaction.</booktitle>
<contexts>
<context position="1958" citStr="Whittaker and Walker, 1991" startWordPosition="282" endWordPosition="285"> parse/generate such messages or at least assist humans in these tasks. From another perspective, everyday human communication is predominantly multimodal; as such, similarly intuitive human-computer/robot interaction demands that intelligent systems master —among others— the semantic interplay between different media and modalities, i.e. they are able to use/understand natural language and its reference to objects and activities in the shared, situated communication space. It was more than a decade ago, when the lack of a theory of how different media interact with one another was indicated (Whittaker and Walker, 1991). Recently, such theoretical framework has been developed and used for annotating a corpus of audiovisual documents with the objective of using such corpus for developing multimedia information processing tools (Pastra, 2008). In this paper, we provide a brief overview of the theory and the corresponding annotated corpus and present a text-based search interface that has been developed for the exploration and the automatic expansion/generalisation of the annotated semantic relations. This search interface is a support tool for the theory and the related corpus and a first step towards its comp</context>
</contexts>
<marker>Whittaker, Walker, 1991</marker>
<rawString>S. Whittaker and M. Walker. 1991. Toward a theory of multi-modal interaction. In Proceedings of the National Conference on Artificial Intelligence Workshop on Multi-modal Interaction.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>