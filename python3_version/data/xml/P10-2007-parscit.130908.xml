<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.079943">
<title confidence="0.962926">
The Same-head Heuristic for Coreference
</title>
<author confidence="0.506766">
Micha Elsner and Eugene Charniak
</author>
<affiliation confidence="0.514664">
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
</affiliation>
<address confidence="0.874288">
Providence, RI 02912
</address>
<email confidence="0.999566">
{melsner,ec}@cs.brown.edu
</email>
<sectionHeader confidence="0.994817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992075">
We investigate coreference relationships
between NPs with the same head noun.
It is relatively common in unsupervised
work to assume that such pairs are
coreferent– but this is not always true, es-
pecially if realistic mention detection is
used. We describe the distribution of non-
coreferent same-head pairs in news text,
and present an unsupervised generative
model which learns not to link some same-
head NPs using syntactic features, improv-
ing precision.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959811320755">
Full NP coreference, the task of discovering which
non-pronominal NPs in a discourse refer to the
same entity, is widely known to be challenging.
In practice, however, most work focuses on the
subtask of linking NPs with different head words.
Decisions involving NPs with the same head word
have not attracted nearly as much attention, and
many systems, especially unsupervised ones, op-
erate under the assumption that all same-head
pairs corefer. This is by no means always the case–
there are several systematic exceptions to the rule.
In this paper, we show that these exceptions are
fairly common, and describe an unsupervised sys-
tem which learns to distinguish them from coref-
erent same-head pairs.
There are several reasons why relatively little
attention has been paid to same-head pairs. Pri-
marily, this is because they are a comparatively
easy subtask in a notoriously difficult area; Stoy-
anov et al. (2009) shows that, among NPs headed
by common nouns, those which have an exact
match earlier in the document are the easiest to
resolve (variant MUC score .82 on MUC-6) and
while those with partial matches are quite a bit
harder (.53), by far the worst performance is on
those without any match at all (.27). This effect
is magnified by most popular metrics for coref-
erence, which reward finding links within large
clusters more than they punish proposing spu-
rious links, making it hard to improve perfor-
mance by linking conservatively. Systems that
use gold mention boundaries (the locations of NPs
marked by annotators)1 have even less need to
worry about same-head relationships, since most
NPs which disobey the conventional assumption
are not marked as mentions.
In this paper, we count how often same-head
pairs fail to corefer in the MUC-6 corpus, show-
ing that gold mention detection hides most such
pairs, but more realistic detection finds large num-
bers. We also present an unsupervised genera-
tive model which learns to make certain same-
head pairs non-coreferent. The model is based
on the idea that pronoun referents are likely to
be salient noun phrases in the discourse, so we
can learn about NP antecedents using pronom-
inal antecedents as a starting point. Pronoun
anaphora, in turn, is learnable from raw data
(Cherry and Bergsma, 2005; Charniak and Elsner,
2009). Since our model links fewer NPs than the
baseline, it improves precision but decreases re-
call. This tradeoff is favorable for CEAF, but not
for V.
</bodyText>
<sectionHeader confidence="0.997632" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.8510515">
Unsupervised systems specify the assumption of
same-head coreference in several ways: by as-
</bodyText>
<footnote confidence="0.997697">
1Gold mention detection means something slightly differ-
ent in the ACE corpus, where the system input contains every
NP annotated with an entity type.
</footnote>
<page confidence="0.99109">
33
</page>
<note confidence="0.509378">
Proceedings of the ACL 2010 Conference Short Papers, pages 33–37,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999889888888889">
sumption (Haghighi and Klein, 2009), using
a head-prediction clause (Poon and Domingos,
2008), and using a sparse Dirichlet prior on word
emissions (Haghighi and Klein, 2007). (These
three systems, perhaps not coincidentally, use gold
mentions.) An exception is Ng (2008), who points
out that head identity is not an entirely reliable cue
and instead uses exact string match (minus deter-
miners) for common NPs and an alias detection
system for proper NPs. This work uses mentions
extracted with an NP chunker. No specific results
are reported for same-head NPs. However, while
using exact string match raises precision, many
non-matching phrases are still coreferent, so this
approach cannot be considered a full solution to
the problem.
Supervised systems do better on the task, but
not perfectly. Recent work (Stoyanov et al., 2009)
attempts to determine the contributions of various
categories of NP to coreference scores, and shows
(as stated above) that common NPs which partially
match an earlier mention are not well resolved by
the state-of-the-art RECONCILE system, which
uses pairwise classification. They also show that
using gold mention boundaries makes the corefer-
ence task substantially easier, and argue that this
experimental setting is “rather unrealistic”.
</bodyText>
<sectionHeader confidence="0.985701" genericHeader="method">
3 Descriptive study: MUC-6
</sectionHeader>
<bodyText confidence="0.997703183098592">
We begin by examining how often non-same-head
pairs appear in the MUC-6 coreference dataset.
To do so, we compare two artificial coreference
systems: the link-all strategy links all, and only,
full (non-pronominal) NP pairs with the same head
which occur within 10 sentences of one another.
The oracle strategy links NP pairs with the same
head which occur within 10 sentences, but only if
they are actually coreferent (according to the gold
annotation)2 The link-all system, in other words,
does what most existing unsupervised systems do
on the same-head subset of NPs, while the oracle
system performs perfectly.
We compare our results to the gold standard us-
ing two metrics. b3(Bagga and Baldwin, 1998)
is a standard metric which calculates a precision
and recall for each mention. The mention CEAF
(Luo, 2005) constructs a maximum-weight bipar-
2The choice of 10 sentences as the window size captures
most, but not all, of the available recall. Using nouns mention
detection, it misses 117 possible same-head links, or about
10%. However, precision drops further as the window size
increases.
tite matching between gold and proposed clusters,
then gives the percentage of entities whose gold
label and proposed label match. b3 gives more
weight to errors involving larger clusters (since
these lower scores for several mentions at once);
for mention CEAF, all mentions are weighted
equally.
We annotate the data with the self-trained Char-
niak parser (McClosky et al., 2006), then extract
mentions using three different methods. The gold
mentions method takes only mentions marked by
annotators. The nps method takes all base noun
phrases detected by the parser. Finally, the nouns
method takes all nouns, even those that do not
head NPs; this method maximizes recall, since it
does not exclude prenominals in phrases like “a
Bush spokesman”. (High-precision models of the
internal structure of flat Penn Treebank-style NPs
were investigated by Vadas and Curran (2007).)
For each experimental setting, we show the num-
ber of mentions detected, and how many of them
are linked to some antecedent by the system.
The data is shown in Table 1. b3 shows a large
drop in precision when all same-head pairs are
linked; in fact, in the nps and nouns settings, only
about half the same-headed NPs are actually coref-
erent (864 real links, 1592 pairs for nps). This
demonstrates that non-coreferent same-head pairs
not only occur, but are actually rather common in
the dataset. The drop in precision is much less
obvious in the gold mentions setting, however;
most unlinked same-head pairs are not annotated
as mentions in the gold data, which is one reason
why systems run in this experimental setting can
afford to ignore them.
Improperly linking same-head pairs causes a
loss in precision, but scores are dominated by re-
call3. Thus, reporting b3 helps to mask the impact
of these pairs when examining the final f-score.
We roughly characterize what sort of same-
headed NPs are non-coreferent by hand-
examining 100 randomly selected pairs. 39
pairs denoted different entities (“recent employ-
ees” vs “employees who have worked for longer”)
disambiguated by modifiers or sometimes by
discourse position. The next largest group (24)
consists of time and measure phrases like “ten
miles”. 12 pairs refer to parts or quantities
</bodyText>
<footnote confidence="0.9981125">
3This bias is exaggerated for systems which only link
same-head pairs, but continues to apply to real systems; for
instance (Haghighi and Klein, 2009) has a b3 precision of 84
and recall of 67.
</footnote>
<page confidence="0.989009">
34
</page>
<table confidence="0.999903615384615">
Mentions Linked V pr rec F mention CEAF
Gold mentions
Oracle 1929 1164 100 32.3 48.8 54.4
Link all 1929 1182 80.6 31.7 45.5 53.8
Alignment 1929 495 93.7 22.1 35.8 40.5
NPs
Oracle 3993 864 100 30.6 46.9 73.4
Link all 3993 1592 67.2 29.5 41.0 62.2
Alignment 3993 518 87.2 24.7 38.5 67.0
Nouns
Oracle 5435 1127 100 41.5 58.6 83.5
Link all 5435 2541 56.6 40.9 45.7 67.0
Alignment 5435 935 83.0 32.8 47.1 74.4
</table>
<tableCaption confidence="0.999669">
Table 1: Oracle, system and baseline scores on MUC-6 test data. Gold mentions leave little room
</tableCaption>
<bodyText confidence="0.9878888">
for improvement between baseline and oracle; detecting more mentions widens the gap between
them. With realistic mention detection, precision and CEAF scores improve over baselines, while recall
and f-scores drop.
(“members of...”), and 12 contained a generic
(“In a corporate campaign, a union tries...”). 9
contained an annotator error. The remaining 4
were mistakes involving proper noun phrases
headed by Inc. and other abbreviations; this case
is easy to handle, but apparently not the primary
cause of errors.
</bodyText>
<sectionHeader confidence="0.997184" genericHeader="method">
4 System
</sectionHeader>
<bodyText confidence="0.999923045454546">
Our system is a version of the popular IBM model
2 for machine translation. To define our generative
model, we assume that the parse trees for the en-
tire document D are given, except for the subtrees
with root nonterminal NP, denoted ni, which our
system will generate. These subtrees are related
by a hidden set of alignments, ai, which link each
NP to another NP (which we call a generator) ap-
pearing somewhere before it in the document, or
to a null antecedent. The set of potential genera-
tors G (which plays the same role as the source-
language text in MT) is taken to be all the NPs
occurring within 10 sentences of the target, plus a
special null antecedent which plays the same role
as the null word in machine translation– it serves
as a dummy generator for NPs which are unrelated
to any real NP in G.
The generative process fills in all the NP nodes
in order, from left to right. This process ensures
that, when generating node ni, we have already
filled in all the NPs in the set G (since these all
precede ni). When deciding on a generator for
NP ni, we can extract features characterizing its
relationship to a potential generator gj. These fea-
tures, which we denote f(ni, gj, D), may depend
on their relative position in the document D, and
on any features of gj, since we have already gener-
ated its tree. However, we cannot extract features
from the subtree under ni, since we have yet to
generate it!
As usual for IBM models, we learn using EM,
and we need to start our alignment function off
with a good initial set of parameters. Since an-
tecedents of NPs and pronouns (both salient NPs)
often occur in similar syntactic environments, we
use an alignment function for pronoun corefer-
ence as a starting point. This alignment can be
learned from raw data, making our approach un-
supervised.
We take the pronoun model of Charniak and El-
sner (2009)4 as our starting point. We re-express
it in the IBM framework, using a log-linear model
for our alignment. Then our alignment (parame-
terized by feature weights w) is:
</bodyText>
<equation confidence="0.898545">
p(ai = j|G, D) a exp(f(ni, gj, D) • w)
</equation>
<bodyText confidence="0.99576375">
The weights w are learned by gradient descent
on the log-likelihood. To use this model within
EM, we alternate an E-step where we calculate
the expected alignments E[ai = j], then an M-
step where we run gradient descent. (We have also
had some success with stepwise EM as in (Liang
and Klein, 2009), but this requires some tuning to
work properly.)
</bodyText>
<footnote confidence="0.993739">
4Downloaded from http://bllip.cs.brown.edu.
</footnote>
<page confidence="0.999207">
35
</page>
<bodyText confidence="0.999804225">
As features, we take the same features as Char-
niak and Elsner (2009): sentence and word-count
distance between ni and gj, sentence position of
each, syntactic role of each, and head type of gj
(proper, common or pronoun). We add binary fea-
tures for the nonterminal directly over gj (NP, VP,
PP, any S type, or other), the type of phrases mod-
ifying gj (proper nouns, phrasals (except QP and
PP), QP, PP-of, PP-other, other modifiers, or noth-
ing), and the type of determiner of gj (possessive,
definite, indefinite, deictic, other, or nothing). We
designed this feature set to distinguish prominent
NPs in the discourse, and also to be able to detect
abstract or partitive phrases by examining modi-
fiers and determiners.
To produce full NPs and learn same-head coref-
erence, we focus on learning a good alignment
using the pronoun model as a starting point. For
translation, we use a trivial model, p(ni|gaz) = 1
if the two have the same head, and 0 otherwise,
except for the null antecedent, which draws heads
from a multinomial distribution over words.
While we could learn an alignment and then
treat all generators as antecedents, so that only
NPs aligned to the null antecedent were not la-
beled coreferent, in practice this model would
align nearly all the same-head pairs. This is
true because many words are “bursty”; the prob-
ability of a second occurrence given the first is
higher than the a priori probability of occurrence
(Church, 2000). Therefore, our model is actually a
mixture of two IBM models, pC and pN, where pC
produces NPs with antecedents and pN produces
pairs that share a head, but are not coreferent. To
break the symmetry, we allow pC to use any pa-
rameters w, while pN uses a uniform alignment,
w = 0. We interpolate between these two models
with a constant A, the single manually set parame-
ter of our system, which we fixed at .9.
The full model, therefore, is:
</bodyText>
<equation confidence="0.9976545">
p(ni|G, D) =ApT(ni|G, D)
+ (1 − A)pN(ni|G, D)
1 1:
pT (ni|G, D) = �
jEG
x ff{head(ni) = head(j)}
</equation>
<bodyText confidence="0.9934044">
NPs for which the maximum-likelihood gener-
ator (the largest term in either of the sums) is from
pT and is not the null antecedent are marked as
coreferent to the generator. Other NPs are marked
not coreferent.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999972666666667">
Our results on the MUC-6 formal test set are
shown in Table 1. In all experimental settings,
the model improves precision over the baseline
while decreasing recall– that is, it misses some le-
gitimate coreferent pairs while correctly exclud-
ing many of the spurious ones. Because of the
precision-recall tradeoff at which the systems op-
erate, this results in reduced V and link F. How-
ever, for the nps and nouns settings, where the
parser is responsible for finding mentions, the
tradeoff is positive for the CEAF metrics. For in-
stance, in the nps setting, it improves over baseline
by 57%.
As expected, the model does poorly in the gold
mentions setting, doing worse than baseline on
both metrics. Although it is possible to get very
high precision in this setting, the model is far too
conservative, linking less than half of the available
mentions to anything, when in fact about 60% of
them are coreferent. As we explain above, this ex-
perimental setting makes it mostly unnecessary to
worry about non-coreferent same-head pairs be-
cause the MUC-6 annotators don’t often mark
them.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999931444444444">
While same-head pairs are easier to resolve than
same-other pairs, they are still non-trivial and de-
serve further attention in coreference research. To
effectively measure their effect on performance,
researchers should report multiple metrics, since
under V the link-all heuristic is extremely diffi-
cult to beat. It is also important to report results
using a realistic mention detector as well as gold
mentions.
</bodyText>
<sectionHeader confidence="0.996174" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997732">
We thank Jean Carletta for the SWITCHBOARD
annotations, and Dan Jurafsky and eight anony-
mous reviewers for their comments and sugges-
tions. This work was funded by a Google graduate
fellowship.
</bodyText>
<equation confidence="0.898958333333333">
1:
pT(ni|G,D) =
jEG
1 ff{head(ni) = head(gj)}
|G|
exp(f(ni, gj, D) • w)
</equation>
<page confidence="0.995729">
36
</page>
<sectionHeader confidence="0.993827" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997683125">
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In LREC Workshop on
Linguistics Coreference, pages 563–566.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
EACL, Athens, Greece.
Colin Cherry and Shane Bergsma. 2005. An Expecta-
tion Maximization approach to pronoun resolution.
In Proceedings of CoNLL, pages 88–95, Ann Arbor,
Michigan.
Kenneth W. Church. 2000. Empirical estimates of
adaptation: the chance of two Noriegas is closer to
p/2 than p2. In Proceedings ofACL, pages 180–186.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of ACL, pages 848–855.
Aria Haghighi and Dan Klein. 2009. Simple corefer-
ence resolution with rich syntactic and semantic fea-
tures. In Proceedings of EMNLP, pages 1152–1161.
Percy Liang and Dan Klein. 2009. Online EM for un-
supervised models. In HLT-NAACL.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of HLT-EMNLP,
pages 25–32, Morristown, NJ, USA. Association for
Computational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of HLT-NAACL, pages 152–159.
Vincent Ng. 2008. Unsupervised models for corefer-
ence resolution. In Proceedings of EMNLP, pages
640–649, Honolulu, Hawaii. Association for Com-
putational Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov Logic.
In Proceedings of EMNLP, pages 650–659, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP, pages
656–664, Suntec, Singapore, August. Association
for Computational Linguistics.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of ACL, pages 240–247, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.999608">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.937865">
<title confidence="0.999456">The Same-head Heuristic for Coreference</title>
<author confidence="0.999363">Micha Elsner</author>
<author confidence="0.999363">Eugene Charniak</author>
<affiliation confidence="0.997709">Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University</affiliation>
<address confidence="0.999335">Providence, RI 02912</address>
<abstract confidence="0.995609923076923">We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In LREC Workshop on Linguistics Coreference,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="5532" citStr="Bagga and Baldwin, 1998" startWordPosition="872" endWordPosition="875">e dataset. To do so, we compare two artificial coreference systems: the link-all strategy links all, and only, full (non-pronominal) NP pairs with the same head which occur within 10 sentences of one another. The oracle strategy links NP pairs with the same head which occur within 10 sentences, but only if they are actually coreferent (according to the gold annotation)2 The link-all system, in other words, does what most existing unsupervised systems do on the same-head subset of NPs, while the oracle system performs perfectly. We compare our results to the gold standard using two metrics. b3(Bagga and Baldwin, 1998) is a standard metric which calculates a precision and recall for each mention. The mention CEAF (Luo, 2005) constructs a maximum-weight bipar2The choice of 10 sentences as the window size captures most, but not all, of the available recall. Using nouns mention detection, it misses 117 possible same-head links, or about 10%. However, precision drops further as the window size increases. tite matching between gold and proposed clusters, then gives the percentage of entities whose gold label and proposed label match. b3 gives more weight to errors involving larger clusters (since these lower sco</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In LREC Workshop on Linguistics Coreference, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Micha Elsner</author>
</authors>
<title>EM works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="2966" citStr="Charniak and Elsner, 2009" startWordPosition="472" endWordPosition="475">ked as mentions. In this paper, we count how often same-head pairs fail to corefer in the MUC-6 corpus, showing that gold mention detection hides most such pairs, but more realistic detection finds large numbers. We also present an unsupervised generative model which learns to make certain samehead pairs non-coreferent. The model is based on the idea that pronoun referents are likely to be salient noun phrases in the discourse, so we can learn about NP antecedents using pronominal antecedents as a starting point. Pronoun anaphora, in turn, is learnable from raw data (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). Since our model links fewer NPs than the baseline, it improves precision but decreases recall. This tradeoff is favorable for CEAF, but not for V. 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using </context>
<context position="11221" citStr="Charniak and Elsner (2009)" startWordPosition="1842" endWordPosition="1846"> the document D, and on any features of gj, since we have already generated its tree. However, we cannot extract features from the subtree under ni, since we have yet to generate it! As usual for IBM models, we learn using EM, and we need to start our alignment function off with a good initial set of parameters. Since antecedents of NPs and pronouns (both salient NPs) often occur in similar syntactic environments, we use an alignment function for pronoun coreference as a starting point. This alignment can be learned from raw data, making our approach unsupervised. We take the pronoun model of Charniak and Elsner (2009)4 as our starting point. We re-express it in the IBM framework, using a log-linear model for our alignment. Then our alignment (parameterized by feature weights w) is: p(ai = j|G, D) a exp(f(ni, gj, D) • w) The weights w are learned by gradient descent on the log-likelihood. To use this model within EM, we alternate an E-step where we calculate the expected alignments E[ai = j], then an Mstep where we run gradient descent. (We have also had some success with stepwise EM as in (Liang and Klein, 2009), but this requires some tuning to work properly.) 4Downloaded from http://bllip.cs.brown.edu. 3</context>
</contexts>
<marker>Charniak, Elsner, 2009</marker>
<rawString>Eugene Charniak and Micha Elsner. 2009. EM works for pronoun anaphora resolution. In Proceedings of EACL, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Shane Bergsma</author>
</authors>
<title>An Expectation Maximization approach to pronoun resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>88--95</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2938" citStr="Cherry and Bergsma, 2005" startWordPosition="468" endWordPosition="471">nal assumption are not marked as mentions. In this paper, we count how often same-head pairs fail to corefer in the MUC-6 corpus, showing that gold mention detection hides most such pairs, but more realistic detection finds large numbers. We also present an unsupervised generative model which learns to make certain samehead pairs non-coreferent. The model is based on the idea that pronoun referents are likely to be salient noun phrases in the discourse, so we can learn about NP antecedents using pronominal antecedents as a starting point. Pronoun anaphora, in turn, is learnable from raw data (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). Since our model links fewer NPs than the baseline, it improves precision but decreases recall. This tradeoff is favorable for CEAF, but not for V. 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics sumption (Haghi</context>
</contexts>
<marker>Cherry, Bergsma, 2005</marker>
<rawString>Colin Cherry and Shane Bergsma. 2005. An Expectation Maximization approach to pronoun resolution. In Proceedings of CoNLL, pages 88–95, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>Empirical estimates of adaptation: the chance of two Noriegas is closer to p/2 than p2.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>180--186</pages>
<contexts>
<context position="13268" citStr="Church, 2000" startWordPosition="2196" endWordPosition="2197">l as a starting point. For translation, we use a trivial model, p(ni|gaz) = 1 if the two have the same head, and 0 otherwise, except for the null antecedent, which draws heads from a multinomial distribution over words. While we could learn an alignment and then treat all generators as antecedents, so that only NPs aligned to the null antecedent were not labeled coreferent, in practice this model would align nearly all the same-head pairs. This is true because many words are “bursty”; the probability of a second occurrence given the first is higher than the a priori probability of occurrence (Church, 2000). Therefore, our model is actually a mixture of two IBM models, pC and pN, where pC produces NPs with antecedents and pN produces pairs that share a head, but are not coreferent. To break the symmetry, we allow pC to use any parameters w, while pN uses a uniform alignment, w = 0. We interpolate between these two models with a constant A, the single manually set parameter of our system, which we fixed at .9. The full model, therefore, is: p(ni|G, D) =ApT(ni|G, D) + (1 − A)pN(ni|G, D) 1 1: pT (ni|G, D) = � jEG x ff{head(ni) = head(j)} NPs for which the maximum-likelihood generator (the largest t</context>
</contexts>
<marker>Church, 2000</marker>
<rawString>Kenneth W. Church. 2000. Empirical estimates of adaptation: the chance of two Noriegas is closer to p/2 than p2. In Proceedings ofACL, pages 180–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric Bayesian model.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>848--855</pages>
<contexts>
<context position="3697" citStr="Haghighi and Klein, 2007" startWordPosition="585" endWordPosition="588">eoff is favorable for CEAF, but not for V. 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause (Poon and Domingos, 2008), and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007). (These three systems, perhaps not coincidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matching phrases are still coreferent, so this approach cannot be considered a full solution to the problem. Supervised systems do better on the </context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric Bayesian model. In Proceedings of ACL, pages 848–855.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1152--1161</pages>
<contexts>
<context position="3558" citStr="Haghighi and Klein, 2009" startWordPosition="564" endWordPosition="567"> 2005; Charniak and Elsner, 2009). Since our model links fewer NPs than the baseline, it improves precision but decreases recall. This tradeoff is favorable for CEAF, but not for V. 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause (Poon and Domingos, 2008), and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007). (These three systems, perhaps not coincidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matchi</context>
<context position="8289" citStr="Haghighi and Klein, 2009" startWordPosition="1319" endWordPosition="1322"> helps to mask the impact of these pairs when examining the final f-score. We roughly characterize what sort of sameheaded NPs are non-coreferent by handexamining 100 randomly selected pairs. 39 pairs denoted different entities (“recent employees” vs “employees who have worked for longer”) disambiguated by modifiers or sometimes by discourse position. The next largest group (24) consists of time and measure phrases like “ten miles”. 12 pairs refer to parts or quantities 3This bias is exaggerated for systems which only link same-head pairs, but continues to apply to real systems; for instance (Haghighi and Klein, 2009) has a b3 precision of 84 and recall of 67. 34 Mentions Linked V pr rec F mention CEAF Gold mentions Oracle 1929 1164 100 32.3 48.8 54.4 Link all 1929 1182 80.6 31.7 45.5 53.8 Alignment 1929 495 93.7 22.1 35.8 40.5 NPs Oracle 3993 864 100 30.6 46.9 73.4 Link all 3993 1592 67.2 29.5 41.0 62.2 Alignment 3993 518 87.2 24.7 38.5 67.0 Nouns Oracle 5435 1127 100 41.5 58.6 83.5 Link all 5435 2541 56.6 40.9 45.7 67.0 Alignment 5435 935 83.0 32.8 47.1 74.4 Table 1: Oracle, system and baseline scores on MUC-6 test data. Gold mentions leave little room for improvement between baseline and oracle; detecti</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of EMNLP, pages 1152–1161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>Online EM for unsupervised models.</title>
<date>2009</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="11725" citStr="Liang and Klein, 2009" startWordPosition="1935" endWordPosition="1938">an be learned from raw data, making our approach unsupervised. We take the pronoun model of Charniak and Elsner (2009)4 as our starting point. We re-express it in the IBM framework, using a log-linear model for our alignment. Then our alignment (parameterized by feature weights w) is: p(ai = j|G, D) a exp(f(ni, gj, D) • w) The weights w are learned by gradient descent on the log-likelihood. To use this model within EM, we alternate an E-step where we calculate the expected alignments E[ai = j], then an Mstep where we run gradient descent. (We have also had some success with stepwise EM as in (Liang and Klein, 2009), but this requires some tuning to work properly.) 4Downloaded from http://bllip.cs.brown.edu. 35 As features, we take the same features as Charniak and Elsner (2009): sentence and word-count distance between ni and gj, sentence position of each, syntactic role of each, and head type of gj (proper, common or pronoun). We add binary features for the nonterminal directly over gj (NP, VP, PP, any S type, or other), the type of phrases modifying gj (proper nouns, phrasals (except QP and PP), QP, PP-of, PP-other, other modifiers, or nothing), and the type of determiner of gj (possessive, definite, </context>
</contexts>
<marker>Liang, Klein, 2009</marker>
<rawString>Percy Liang and Dan Klein. 2009. Online EM for unsupervised models. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5640" citStr="Luo, 2005" startWordPosition="892" endWordPosition="893">ronominal) NP pairs with the same head which occur within 10 sentences of one another. The oracle strategy links NP pairs with the same head which occur within 10 sentences, but only if they are actually coreferent (according to the gold annotation)2 The link-all system, in other words, does what most existing unsupervised systems do on the same-head subset of NPs, while the oracle system performs perfectly. We compare our results to the gold standard using two metrics. b3(Bagga and Baldwin, 1998) is a standard metric which calculates a precision and recall for each mention. The mention CEAF (Luo, 2005) constructs a maximum-weight bipar2The choice of 10 sentences as the window size captures most, but not all, of the available recall. Using nouns mention detection, it misses 117 possible same-head links, or about 10%. However, precision drops further as the window size increases. tite matching between gold and proposed clusters, then gives the percentage of entities whose gold label and proposed label match. b3 gives more weight to errors involving larger clusters (since these lower scores for several mentions at once); for mention CEAF, all mentions are weighted equally. We annotate the data</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of HLT-EMNLP, pages 25–32, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="6302" citStr="McClosky et al., 2006" startWordPosition="994" endWordPosition="997">hoice of 10 sentences as the window size captures most, but not all, of the available recall. Using nouns mention detection, it misses 117 possible same-head links, or about 10%. However, precision drops further as the window size increases. tite matching between gold and proposed clusters, then gives the percentage of entities whose gold label and proposed label match. b3 gives more weight to errors involving larger clusters (since these lower scores for several mentions at once); for mention CEAF, all mentions are weighted equally. We annotate the data with the self-trained Charniak parser (McClosky et al., 2006), then extract mentions using three different methods. The gold mentions method takes only mentions marked by annotators. The nps method takes all base noun phrases detected by the parser. Finally, the nouns method takes all nouns, even those that do not head NPs; this method maximizes recall, since it does not exclude prenominals in phrases like “a Bush spokesman”. (High-precision models of the internal structure of flat Penn Treebank-style NPs were investigated by Vadas and Curran (2007).) For each experimental setting, we show the number of mentions detected, and how many of them are linked</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of HLT-NAACL, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised models for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>640--649</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="3794" citStr="Ng (2008)" startWordPosition="601" endWordPosition="602">d coreference in several ways: by as1Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause (Poon and Domingos, 2008), and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007). (These three systems, perhaps not coincidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matching phrases are still coreferent, so this approach cannot be considered a full solution to the problem. Supervised systems do better on the task, but not perfectly. Recent work (Stoyanov et al., 2009) attempts to determine the contributi</context>
</contexts>
<marker>Ng, 2008</marker>
<rawString>Vincent Ng. 2008. Unsupervised models for coreference resolution. In Proceedings of EMNLP, pages 640–649, Honolulu, Hawaii. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov Logic.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>650--659</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="3616" citStr="Poon and Domingos, 2008" startWordPosition="572" endWordPosition="575">wer NPs than the baseline, it improves precision but decreases recall. This tradeoff is favorable for CEAF, but not for V. 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause (Poon and Domingos, 2008), and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007). (These three systems, perhaps not coincidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matching phrases are still coreferent, so this approach cannot b</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with Markov Logic. In Proceedings of EMNLP, pages 650–659, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>656--664</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1595" citStr="Stoyanov et al. (2009)" startWordPosition="242" endWordPosition="246">ve not attracted nearly as much attention, and many systems, especially unsupervised ones, operate under the assumption that all same-head pairs corefer. This is by no means always the case– there are several systematic exceptions to the rule. In this paper, we show that these exceptions are fairly common, and describe an unsupervised system which learns to distinguish them from coreferent same-head pairs. There are several reasons why relatively little attention has been paid to same-head pairs. Primarily, this is because they are a comparatively easy subtask in a notoriously difficult area; Stoyanov et al. (2009) shows that, among NPs headed by common nouns, those which have an exact match earlier in the document are the easiest to resolve (variant MUC score .82 on MUC-6) and while those with partial matches are quite a bit harder (.53), by far the worst performance is on those without any match at all (.27). This effect is magnified by most popular metrics for coreference, which reward finding links within large clusters more than they punish proposing spurious links, making it hard to improve performance by linking conservatively. Systems that use gold mention boundaries (the locations of NPs marked</context>
<context position="4357" citStr="Stoyanov et al., 2009" startWordPosition="690" endWordPosition="693">incidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matching phrases are still coreferent, so this approach cannot be considered a full solution to the problem. Supervised systems do better on the task, but not perfectly. Recent work (Stoyanov et al., 2009) attempts to determine the contributions of various categories of NP to coreference scores, and shows (as stated above) that common NPs which partially match an earlier mention are not well resolved by the state-of-the-art RECONCILE system, which uses pairwise classification. They also show that using gold mention boundaries makes the coreference task substantially easier, and argue that this experimental setting is “rather unrealistic”. 3 Descriptive study: MUC-6 We begin by examining how often non-same-head pairs appear in the MUC-6 coreference dataset. To do so, we compare two artificial co</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art. In Proceedings of ACL-IJCNLP, pages 656–664, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James Curran</author>
</authors>
<title>Adding noun phrase structure to the penn treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>240--247</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6796" citStr="Vadas and Curran (2007)" startWordPosition="1071" endWordPosition="1074">ntion CEAF, all mentions are weighted equally. We annotate the data with the self-trained Charniak parser (McClosky et al., 2006), then extract mentions using three different methods. The gold mentions method takes only mentions marked by annotators. The nps method takes all base noun phrases detected by the parser. Finally, the nouns method takes all nouns, even those that do not head NPs; this method maximizes recall, since it does not exclude prenominals in phrases like “a Bush spokesman”. (High-precision models of the internal structure of flat Penn Treebank-style NPs were investigated by Vadas and Curran (2007).) For each experimental setting, we show the number of mentions detected, and how many of them are linked to some antecedent by the system. The data is shown in Table 1. b3 shows a large drop in precision when all same-head pairs are linked; in fact, in the nps and nouns settings, only about half the same-headed NPs are actually coreferent (864 real links, 1592 pairs for nps). This demonstrates that non-coreferent same-head pairs not only occur, but are actually rather common in the dataset. The drop in precision is much less obvious in the gold mentions setting, however; most unlinked same-h</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James Curran. 2007. Adding noun phrase structure to the penn treebank. In Proceedings of ACL, pages 240–247, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>