<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000122">
<title confidence="0.999163">
Unsupervised Relation Extraction of In-Domain Data
from Focused Crawls
</title>
<author confidence="0.988854">
Steffen Remus
</author>
<affiliation confidence="0.97661925">
FG Language Technology
Computer Science Department, Technische Universit¨at Darmstadt
Information Center for Education
German Institute for Educational Research (DIPF)
</affiliation>
<email confidence="0.996842">
remus@cs.tu-darmstadt.de
</email>
<sectionHeader confidence="0.993844" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935571428571">
This thesis proposal approaches unsuper-
vised relation extraction from web data,
which is collected by crawling only those
parts of the web that are from the same do-
main as a relatively small reference cor-
pus. The first part of this proposal is con-
cerned with the efficient discovery of web
documents for a particular domain and in
a particular language. We create a com-
bined, focused web crawling system that
automatically collects relevant documents
and minimizes the amount of irrelevant
web content. The collected web data is
semantically processed in order to acquire
rich in-domain knowledge. Here, we focus
on fully unsupervised relation extraction
by employing the extended distributional
hypothesis. We use distributional similar-
ities between two pairs of nominals based
on dependency paths as context and vice
versa for identifying relational structure.
We apply our system for the domain of
educational sciences by focusing primarily
on crawling scientific educational publica-
tions in the web. We are able to produce
promising initial results on relation identi-
fication and we will discuss future direc-
tions.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939805555556">
Knowledge acquisition from written or spoken
text is a field of interest not only for theoretical
reasons but also for practical applications, such as
semantic search, question answering and knowl-
edge management, just to name a few.
In this work, we propose an approach for un-
supervised relation extraction (URE) where we
make use of the Distributional Hypothesis by Har-
ris (1954). The underlying data set is collected
from the world wide web by focusing on web doc-
uments that are from the same domain as a small
initialization data set that is provided beforehand.
We hereby enrich this existing, domain-defining,
corpus with more data of the same kind. This is
needed for practical reasons when working with
the Distributional Hypothesis (Harris, 1954): A lot
of data is required for plausible outcomes and an
appropriate coverage. However, we want as little
irrelevant data as possible. The proposal’s contri-
bution is thus twofold: a) focused crawling, and
b) unsupervised relation extraction. As a partic-
ular use case, we are especially interested in sci-
entific publications from the German educational
domain. However, we would like to point out that
the methodology itself is independent of language
and domain and is generally applicable to any do-
main.
This work is structured as follows: First we will
motivate our combined approach and introduce
each part individually. We then present related
work in Section 2. Section 3 explains the method-
ology of both parts, and in Section 4 we outline the
evaluation procedure of each of the components
individually. This is followed by some prelimi-
nary results in Section 5, and Section 6 concludes
this proposal with some prospects for future work.
</bodyText>
<subsectionHeader confidence="0.99434">
1.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999941083333333">
The identification of relations between entities
solely from text is one of many challenges in
the development of language understanding sys-
tem (Carlson et al., 2010; Etzioni et al., 2008);
and yet it is the one step with the highest informa-
tion gain. It is used e.g. for taxonomy induction
(Hearst, 1992) or ontology accumulation (Mintz et
al., 2009) or even for identifying facts that express
general knowledge and that often recur (Cham-
bers and Jurafsky, 2011). Davidov et al. (2007)
performed unsupervised relation extraction by ac-
tively mining the web and showed major improve-
</bodyText>
<page confidence="0.992761">
11
</page>
<note confidence="0.99515">
Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 11–20,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99998188">
ments in the detection of new facts from only little
initial seed. They used a major web search engine
as a vital component of their system. According to
Kilgarriff (2007), however, this strategy is unreli-
able and should be avoided. Nevertheless, the web
is undeniably the largest source for any kind of
data, and we feel the need for developing easy-to-
use components that make it possible to create cor-
pora from the web with only little effort (cf. e.g.
Biemann et al. (2013)). When it comes to specific
in-domain information, the complete world wide
web is first of all too vast to be processed conve-
niently, and second the gain is little because of too
much irrelevant information. Thus we need meth-
ods for reducing the size of data to process without
losing the focus on the important information and
without using web search engines. The combina-
tion of a focused crawling system with a subse-
quent unsupervised relation extraction system en-
ables the acquisition of richer in-domain knowl-
edge than just relying on little local data, but with-
out having to process petabytes of data and still not
relying of web search. And yet, by using the web
as a resource, our system is generally applicable
and independent of language and target domain.
</bodyText>
<subsectionHeader confidence="0.973823">
1.2 Focused Crawling
</subsectionHeader>
<bodyText confidence="0.9999210625">
The first part of this proposal is concerned with
the efficient discovery of publications in the web
for a particular domain. The domain definition is
given as a limited number of reference documents.
An extra challenge is, that non-negligible amounts
of scientific publications are only available as pdf
documents, which makes the necessity of new fo-
cused crawling techniques even more important.
This holds especially for our target use case, the
German educational domain. In Section 2.1 we
will discuss this issue in more detail. We develop
a focused web crawling system which collects pri-
marily relevant documents and ignores irrelevant
documents and which is particularly suited for har-
vesting documents from a predefined specific do-
main.
</bodyText>
<subsectionHeader confidence="0.990191">
1.3 Unsupervised Relation Extraction
</subsectionHeader>
<bodyText confidence="0.9998733">
The second part of this proposal is the semantic
structuring of texts—in our particular use case
scientific publications from the educational do-
main —by using data-driven techniques of com-
putational semantics. The resulting structure en-
ables forms of post-processing like inference or
reasoning. In the semantic structuring part, the
overall goal is to discover knowledge which can
then be used in further steps. Specifically, we will
focus on unsupervised relation extraction.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.99727">
2.1 Focused Crawling
</subsectionHeader>
<bodyText confidence="0.999974071428571">
The development of high-quality data-driven se-
mantic models relies on corpora of large sizes
(Banko and Brill, 2001; Halevy et al., 2009), and
the world wide web is by far the biggest avail-
able source of textual data. Nowadays, a large
number of research projects rely on corpora that
comes from data in the world wide web. The Web-
as-Corpus Kool Yinitiative1 (WaCKy) (Baroni et
al., 2009) for example produced one of the largest
corpora used in linguistic research which comes
from web documents. Another research initia-
tive which produces a variety of corpora by crawl-
ing the web is the COW2 (corpora from the web)
project (Sch¨afer and Bildhauer, 2012). Currently
one of the largest N-gram corpora coming from
web data is the Google V1 and Google V2 (Lin et
al., 2010), which are used e.g. for improving noun
phrase parsing (Pitler et al., 2010). Also the pre-
decessor Google Web1T (Brants and Franz, 2006),
which is computed from 1 Trillion words from the
web, is heavily used in the community.
All these corpora are generated from general
texts which either come from crawling specific
top-level-domains (tlds) or preprocessing and fil-
tering very large amounts of texts for a specified
language. Additionally, we are not aware of any
corpus that is created by collecting pdf documents.
This is especially an issue when aiming at a cor-
pus of scientific publications, such as e.g. the ACL
anthology3 (Bird et al., 2008). As of today, elec-
tronic publications are primarily distributed as pdf
documents. Usually these are omitted by the par-
ticular crawler because of a number of practical
issues, e.g. difficulties in extracting clean plain-
text.
Further, we are not interested in sheer collec-
tion size, but also in domain specificity. Crawling
is a time-consuming process and it comes with lo-
gistic challenges for processing the resulting data.
While standard breadth-first or depth-first crawl-
ing strategies can be adjusted to include pdf files,
we want to avoid to harvest the huge bulk of data
</bodyText>
<footnote confidence="0.999982666666667">
1http://wacky.sslmit.unibo.it/
2http://hpsg.fu-berlin.de/cow/
3http://acl-arc.comp.nus.edu.sg/
</footnote>
<page confidence="0.998884">
12
</page>
<bodyText confidence="0.999882181818182">
that we are not interested in, namely those docu-
ments that are of a different topical domain as our
initial domain definition.
In focused crawling, which is sometimes also
referred to as topical crawling, web crawlers are
designed to harvest those parts of the web first
that are more interesting for a particular topic
(Chakrabarti et al., 1999). By doing so, task-
specific corpora can be generated fast and ef-
ficient. Typical focused crawlers use machine
learning techniques or heuristics to prioritize
newly discovered URIs (unified resource iden-
tifier) for further crawling (Blum and Mitchell,
1998; Chakrabarti et al., 1999; Menczer et al.,
2004). In our scenario however, we do not rely on
positively and negatively labeled data. The source
documents that serve as the domain definition are
assumed to be given in plain text. The develop-
ment of tools that are able to generate in-domain
web-corpora from focused crawls is the premise
for further generating rich semantic models tai-
lored to a target domain.
</bodyText>
<subsectionHeader confidence="0.99833">
2.2 Unsupervised Relation Extraction
</subsectionHeader>
<bodyText confidence="0.9999794">
The unsupervised relation extraction (URE) part
of this proposal is specifically focused on ex-
tracting relations between nominals. Typically the
choice of the entity type depends merely on the
final task at hand. Kinds of entities which are usu-
ally considered in relation extraction are named
entities like persons or organizations. However,
we will focus on nominals which are much more
general and also include named entities since they
are basically nouns or noun phrases (Nastase et
al., 2013). Nominals are discussed in more de-
tail in Section 3.2. Unsupervised methods for re-
lation extraction is a particularly interesting area
of research because of its applicability across lan-
guages without relying on labeled data. In con-
trast to open information extraction, in unsuper-
vised relation extraction the collected relations are
aggregated in order to identify the most promising
relations for expressing interesting facts. Here, the
grouping is made explicit for further processing.
One possible application of relation extraction
is the establishment of so-called knowledge graphs
(Sowa, 2000), which encode facts that manifest
solely from text. The knowledge graph can then
be used e.g. for reasoning, that is finding new facts
from existing facts.
Many approaches exist for acquiring knowledge
from text. Hearst (1992) first discovered that rela-
tions between entities occur in a handful of well
developed text patterns. For example ’X is a Y’
or ’X and other Ys’ manifest themselves as hy-
ponymic relations. However, not every kind of re-
lation is as easy to identify as those ’is-a’ relations.
Often semantic relations cannot be expressed by
any pattern. A variety of methods were devel-
oped that automatically find new patterns and en-
tities with or without supervision. These methods
reach from bootstrapping methods (Hearst, 1992)
over distant supervision (Mintz et al., 2009) and
latent relational analysis (LRA) (Turney, 2005)
to extreme unsupervised relation extraction (Davi-
dov and Rappoport, 2008a), just to name a few.
The importance of unsupervised methods for re-
lation extraction is obvious: The manual creation
of knowledge resources is time consuming and ex-
pensive in terms of manpower. Though manual re-
sources are typically very precise they are almost
always lacking of lexical and relational coverage.
The extraction of relations between entities is a
crucial process which is performed by every mod-
ern language understanding system like NELL4
(Carlson et al., 2010) or machine reading5, which
evolved among others from TextRunner6 (Etzioni
et al., 2008). The identification of relations in nat-
ural language texts is at the heart of such systems.
</bodyText>
<sectionHeader confidence="0.999713" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999494">
3.1 Focused Crawling
</subsectionHeader>
<bodyText confidence="0.999913266666667">
Language models (LMs) are a rather old but
well understood and generally accepted concept
in Computational Linguistics and Information Re-
trieval. Our focused crawling strategy builds upon
the idea of utilizing a language model to discrim-
inate between relevant and irrelevant web docu-
ments. The key idea of this methodology is that
web pages which come from a certain domain —
which implies the use of a particular vocabulary
(Biber, 1995)—link to other documents of the
same domain. The assumption is that the crawler
will most likely stay in the same topical domain
as the initial language model was generated from.
Thus the crawling process can be terminated when
enough data has been collected.
</bodyText>
<footnote confidence="0.9621282">
4Never Ending Language Learner:
http://rtw.ml.cmu.edu/
5http://ai.cs.washington.edu/
projects/open-information-extraction
6http://openie.cs.washington.edu/
</footnote>
<page confidence="0.99921">
13
</page>
<bodyText confidence="0.999879571428571">
A language model is a statistical model over
short sequences of consecutive tokens called N-
grams. The order of a language model is defined
by the length of such sequences, i.e. the ’N’ in N-
gram. The probability of a sequence of m words,
that could be for example a sentence, is computed
as:
</bodyText>
<equation confidence="0.999346">
p(w1, ..., wm) ≈ �m p(wi|wi−N+1:i−1) , (1)
i=1
</equation>
<bodyText confidence="0.9996535">
where N is the order of the language model and
p(wi|wi−n+1:i−1) is the probability of the particu-
lar N-gram. In the simplest case the probability of
an N-gram is computed as:
</bodyText>
<equation confidence="0.997476">
count(wi−N+1:i)
p(wi|wi−n+1:i−1) = , (2)
count (wi−N+1:i−1)
</equation>
<bodyText confidence="0.99980819047619">
where count(N-gram) is a function that takes as
argument an N-gram of length N or an N-gram
of length N − 1 and returns the frequency of ob-
servations in the source corpus. This model has
some obvious limitations when it comes to out-
of-vocabulary (OOV) terms because of probabil-
ities being zero. Due to this limitation, a number
of LMs were proposed which handle OOV terms
well.
One of the most advanced language models is
the Kneser-Ney language model (Kneser and Ney,
1995), which applies an advanced interpolation
technique for OOV issues. According to Halevy
et al. (2009), simpler models that are trained on
large amounts of data often outperform complex
models with training procedures that are feasible
only for small data. Anyway, we have only little
data in the initial phase, thus we use Kneser and
Ney’s model.
Perplexity is used to measure the amount of
compatibility with another model X:
</bodyText>
<equation confidence="0.998715">
Perplexity(X) = 2H(X) , (3)
</equation>
<bodyText confidence="0.999885941176471">
where H(X) = − |x |Ex∈X log2 p(x) is the
cross entropy of a model X. Using perplexity we
are able to tell how well the language model fits
the data and vice versa.
The key idea is that documents which come
from a certain register or domain — which im-
plies the use of a particular vocabulary (Biber,
1995) —link to other documents of the same reg-
ister. Using perplexity, we are able to rank out-
going links by their deviation from our initial lan-
guage model. Hence weblinks that are extracted
from a highly deviating webpage are less priori-
tized for harvesting. The open source crawler soft-
ware Heritrix7 (Mohr et al., 2004) forms the basis
of our focused crawling strategy, since it provides
a well-established framework which is easily ex-
tensible through its modularity.
</bodyText>
<subsectionHeader confidence="0.999557">
3.2 Identification of Nominals
</subsectionHeader>
<bodyText confidence="0.999978269230769">
Nominals are defined to be expressions which syn-
tactically act like nouns or noun phrases (Quirk
et al., 1985, p.335). Another definition according
to Nastase et al. (2013) is that nominals are de-
fined to be in one of the following classes: a) com-
mon nouns, b) proper nouns, c) multi-word proper
nouns, d) deverbal nouns, e) deadjectival nouns,
or f) non-compositional (adjective) noun phrases.
In this work we will follow the definition given
by Nastase et al. (2013). We will further address
only relations that are at least realized by verbal or
prepositional phrases and ignore relations that are
implicitly present in compounds, which is a task
of its own, cf. (Holz and Biemann, 2008). Note
however we do not ignore relations between com-
pounds, but within compounds.
The identification of nominals can be seen
as the task of identifying reliable multi-word-
expressions (MWEs), which is a research question
of its own right. As a first simplified approach
we only consider nouns and heads of noun com-
pounds to be representatives for nominals. E.g. a
compound is used as an entity, but only the head
is taken into further consideration as a represen-
tative since it encapsulates the main meaning for
that phrase.
</bodyText>
<subsectionHeader confidence="0.996235">
3.3 Unsupervised Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999980785714286">
Our system is founded in the idea of distributional
semantics on the level of dependency parses. The
Distributional Hypothesis by Harris (1954) (cf.
also (Miller and Charles, 1991)) states that words
which tend to occur in similar contexts tend to
have similar meanings. This implies that one can
estimate the meaning of an unknown word by con-
sidering the context in that it occurs. Lin and Pan-
tel (2001) extended this hypothesis to cover short-
est paths in the dependency graph— so-called de-
pendency paths —and introduced the Extended
Distributional Hypothesis. This extended hypoth-
esis states that dependency paths which tend to oc-
cur in similar contexts, i.e. they connect the simi-
</bodyText>
<footnote confidence="0.993245">
7http://crawler.archive.org
</footnote>
<page confidence="0.998588">
14
</page>
<bodyText confidence="0.999924543478261">
lar sets of words, also tend to have similar mean-
ings.
Sun and Grishman (2010) used an agglomera-
tive hierarchical clustering based approach in or-
der to group the patterns found by Lin and Pan-
tel’s method. The clusters are used in a semi-
supervised way to extract relation instances that
are used in a bootstrapping fashion to find new
relations. While Sun and Grishman (2010) per-
formed a hard clustering, meaning every relation is
assigned exactly to one cluster, we argue that rela-
tions are accompanied by a certain degree of am-
biguity. Think for example about the expression
’X comes from Y’ which could be both, a causal
relation or a locational relation depending on the
meaning of X and Y.
That being said, we use the Extended Distri-
butional Hypothesis in order to extract meaning-
ful relations from text. We follow Lin and Pantel
(2001) and use the dependency path between two
entities to identify both, similar entity pairs and
similar dependency paths. Specifically we use the
Stanford Parser8 (Klein and Manning, 2003) to get
a collapsed dependency graph representation of a
sentence, and apply the JoBimText9 (Biemann and
Riedl, 2013) software for computing the distribu-
tional similarities.
By using the JoBimText framework, we ac-
cept their theory, which states that dimensionality-
reduced vector space models are not expressive
enough to capture the full semantics of words,
phrases, sentences, documents or relations. Tur-
ney and Pantel (2010) surveyed that vector space
models are commonly used in computational se-
mantics and that they are able to capture the mean-
ing of words. However, by doing various kinds of
vector space transformations, e.g. dimensionality
reduction with SVD10 important information from
the long tail, i.e. items that do not occur often,
is lost. Instead, Biemann and Riedl (2013) intro-
duced the scalable JoBimText framework, which
makes use of the Distributional Hypothesis. We
take this as a starting point to steer away from the
use of vector space models.
For each entity pair ’X::Y’, where ’X’ and ’Y’
are nominals, we collect all dependency paths that
</bodyText>
<footnote confidence="0.995087">
8http://nlp.stanford.edu/downloads/
lex-parser.shtml
9http://sf.net/p/jobimtext
10Singular Value Decomposition, used for example in la-
tent semantic analysis, latent relational analysis, principal
component analysis and many more.
</footnote>
<figure confidence="0.6760966">
rain:: seawater @1 nsubj
←−−−− comes prep from
−−−−−−−→ @2
dobjnsubj
rain :: seawater @1←−−− causes −−−−→ @2
prep
from nsubj
seawater :: rain @1 ←−−−−−−− comes −−−−→ @2
nsubjdobj
seawater :: rain @1 ←−−−− causes −−−→ @2
</figure>
<figureCaption confidence="0.717709166666667">
Figure 1: Upper12: collapsed dependency parses
of the example sentences ’Rain comes from evapo-
rated seawater.’ and ’Evaporated seawater causes
rain’. Lower: extracted entity pairs plus shortest
dependency paths per entity pair from both sen-
tences.
</figureCaption>
<bodyText confidence="0.9997049">
co-occur with it in the complete dataset. A par-
ticular path for a particular relation instance has
form ’@1-PATH-@2’, where ’-PATH-’ is the in-
stantiation of the directed shortest path in the col-
lapsed dependency path starting from a particu-
lar ’X’ and ending in a particular ’Y’. The @1,
resp. @2, symbolizes the place where ’X’ and ’Y’
were found in the path. Here we restrict the path
to be shorter than five edges and additionally we
ignore paths that have only nn relations, i.e. com-
pound dependency relations. See Figure 1 for an
illustration of this strategy on two small example
sentences. Note that this procedure strongly co-
heres with the methodologies proposed by Lewis
and Steedman (2013) or Akbik et al. (2013).
We then compute the distributional similarities
for both directions: a) similarities of entity pairs
by paths, and b) similarities of paths by entity
pairs. This gives us two different views on the
data.
</bodyText>
<sectionHeader confidence="0.99919" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99996625">
The two major directions of this paper, i.e. the fo-
cused crawling part and the unsupervised relation
extraction part are evaluated individually and in-
dependent of each other. First we will present an
</bodyText>
<footnote confidence="0.940841">
12Images generated with GrammarScope:
http://grammarscope.sf.net.
</footnote>
<page confidence="0.998461">
15
</page>
<bodyText confidence="0.999801666666666">
evaluation methodology to assess the quality of the
crawler and second we will outline the evaluation
of relations. While we can only show anecdotical
evidence of the viability of this approach, since the
work is in progress, we are able to present encour-
aging preliminary results in Section 5.
</bodyText>
<subsectionHeader confidence="0.984058">
4.1 Focused Crawling
</subsectionHeader>
<bodyText confidence="0.998453">
The quality of a focused crawl is measured in
terms of perplexity (cf. Section 3.1) by creating
a language model from the harvested data during
a particular crawl. Perplexity is then calculated
with respect to a held out test set. The follow-
ing three phases describe the evaluation procedure
more precisely:
</bodyText>
<listItem confidence="0.918769533333333">
1. The source corpus is split i.i.d.13 into a train-
ing and test set.
2. We create a language model U of the training
data, which is applied according to Section
3.1 for automatically focusing the crawl. In
order to compare the data of different crawls,
the repeated crawls are initialized with the
same global parameter settings, e.g. polite-
ness settings, seed, etc. are the same, and are
terminated after reaching a certain number of
documents.
3. From the harvested data, another language
model V is produced which is used for the
evaluation of the test data. Here we argue
that a crawl which collects data that is used
</listItem>
<bodyText confidence="0.9543174">
for evaluating V and V results in a lower per-
plexity score, is preferred as it better models
the target domain.
Figure 2 shows a schematic overview of the three
phases of evaluation.
</bodyText>
<subsectionHeader confidence="0.985981">
4.2 Unsupervised Relation Extraction
</subsectionHeader>
<bodyText confidence="0.979318375">
The evaluation of relation extraction is a non-
trivial task, as unsupervised categories do usually
not exactly match the distinctions taken in annota-
tion studies. For the evaluation of our method we
consider the following three approaches:
1. We test our relations directly on datasets that
were provided as relation classification chal-
lenge datasets (Girju et al., 2007; Hendrickx
</bodyText>
<footnote confidence="0.547862">
13independent and identically distributed
</footnote>
<figureCaption confidence="0.873704">
Figure 2: Schematic overview of the evaluation
procedure for a particular crawl.
</figureCaption>
<bodyText confidence="0.999837166666667">
et al., 2010). Whereas the first dataset is pro-
vided as a binary classification task, the sec-
ond is a multi-way classification task. How-
ever, both datasets can be transformed to ad-
dress the one or the other task. This is possi-
ble because the challenge is already finished.
</bodyText>
<listItem confidence="0.7551815">
2. We apply our extracted relations for assisting
classification algorithms for the task of tex-
tual entailment (Dagan et al., 2006).
3. Following Davidov and Rappoport (2008b)
we would further like to apply our system to
the task of question answering.
</listItem>
<bodyText confidence="0.999819">
While the first approach is an intrinsic evaluation,
the other three approaches are extrinsic, i.e. the
extracted relations are used in a particular task
which is then evaluated against some gold stan-
dard.
</bodyText>
<sectionHeader confidence="0.977125" genericHeader="method">
5 Preliminary Results
</sectionHeader>
<subsectionHeader confidence="0.966364">
5.1 Focused crawling
</subsectionHeader>
<bodyText confidence="0.9516346">
Table 1 shows some quantitative characteristics of
a non-focused crawl. Here the crawl was per-
formed as a scoped crawl, which means that it was
bounded to the German top-level-domain ’.de’ and
additionally by a maximum number of 20 hops
from the start seed14. The crawl was terminated
after about two weeks. Although these numbers
14The start seed for the first crawl consists of five web page
urls which are strongly connected to German educational re-
search.
</bodyText>
<figure confidence="0.9727336">
Phase 2
Phase 1
U
Phase 3
V
</figure>
<page confidence="0.983863">
16
</page>
<bodyText confidence="0.91091275">
pdf html
size in GBytes 17 400
number of documents 43K 9M
runtime ≈ 2 weeks
</bodyText>
<tableCaption confidence="0.80364">
Table 1: Numbers are given as approximate num-
bers.
</tableCaption>
<bodyText confidence="0.998922333333333">
do not seem surprising, they do support the main
argument of this proposal. Focused crawling is
necessary in order to reduce the massive load of
irrelevant data.
Initial encouraging results on the comparison of
a focused vs. a non-focused crawl are shown in
Figure 3. The crawls were performed under the
same conditions and we recorded the perplexity
value during the process. We plot the history for
the first 300,000 documents. Although these re-
sults are preliminary, a trend is clearly observable.
The focused crawl harvests more relevant doc-
uments as it proceeds, whereas the non-focused
crawl deviates more as longer the crawl proceeds,
as indicated by higher perplexity values for later
documents — an effect that is likely to increase as
the crawl proceeds. The focused crawl, on the
other hand, stays within low perplexity limits. We
plan to evaluate settings and the interplay between
crawling parameters and language modeling more
thoroughly in future evaluations.
</bodyText>
<subsectionHeader confidence="0.998637">
5.2 Unsupervised Relation Extraction
</subsectionHeader>
<bodyText confidence="0.9997674">
The unsupervised extraction of relations was per-
formed on a small subset of one Million sentences
of the news corpus from the Leipzig Corpora Col-
lection (Richter et al., 2006).
Preliminary example results are shown in Ta-
ble 2 and in Table 3. Table 2 shows selected results
for similar entity pairs, and Table 3 shows selected
results for similar dependency paths.
In Table 2, three example entity pairs are shown
together with their most similar counterparts. It is
interesting to see that the relation of gold to ounce
is the same as stock to share or oil to barrel and
we can easily agree here, since the one is the mea-
suring unit for the other.
Table 3 shows for three example prepositional
paths the similar paths. We have chosen prepo-
sitional phrases here because of their intuitive in-
terpretability. The example output shows that the
similar phrases which were identified by the sys-
tem are also interpretable for humans.
</bodyText>
<figureCaption confidence="0.996964">
Figure 3: Two crawl runs under same conditions
and with same settings. Upper: a focused crawl
run. Lower: a non-focused crawl run.
</figureCaption>
<sectionHeader confidence="0.955419" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.983595476190476">
This research thesis proposal addressed the two
major objectives:
1. crawling with a focus on in-domain data by
using a language model of an initial corpus,
which is small compared to the expected re-
sult of the crawls, in order to discriminate
relevant web documents from irrelevant web
documents, and
2. unsupervised relation extraction by follow-
ing the principles of the Distributional Hy-
pothesis by Harris (1954) resp. the Extended
Distributional Hypothesis by Lin and Pantel
(2001).
The promising preliminary results encourage
us to examine this approach for further direc-
tions. Specifically the yet unaddressed parts of the
evaluation will be investigated. Further, the un-
supervised relation extraction techniques will be
applied on the complete set of in-domain data,
thus finalizing the workflow of enriching a small
amount of domain defining data with web data
</bodyText>
<page confidence="0.962772">
17
</page>
<equation confidence="0.947214222222222">
gold/NN :: ounce/NN
crude/NN:: barrel/NN
oil/NN:: barrel/NN
futures/NNS :: barrel/NN
stock/NN:: share/NN
graduate/NN :: University/NNP
graduate/NN :: School/NNP
graduate/NN :: College/NNP
goals/NNS :: season/NN
</equation>
<tableCaption confidence="0.699748333333333">
points/NNS:: season/NN
points/NNS:: game/NN
touchdowns/NNS:: season/NN
Table 2: Example results for selected entity pairs.
Similar entity pairs with respect to the boldface
pair are shown.
</tableCaption>
<equation confidence="0.996495583333333">
@1 &lt;= prep above = @2
@1 &lt;= prep below = @2
@1 &lt;= nsubj = rose/VBD = dobj =&gt; @2
@1 &lt;= nsubj = dropped/VBD = dobj =&gt; @2
@1 &lt;= nsubj = fell/V BD = dobj =&gt; @2
@1 &lt;= prep regarding = @2
@1 &lt;= prep about = @2
@1 &lt;= prep on = @2
@1 &lt;= prep like = @2
@1 &lt;= prep such as = @2
@1 &lt;= prep including = @2
@1 &lt;= nsubj = are/VBP = prep among =&gt; @2
</equation>
<bodyText confidence="0.937265619047619">
Table 3: Example results for selected dependency
paths. Similar paths with respect to the boldface
path are shown.
from focused crawls in order to extract rich in-
domain knowledge, particularly from the german
educational domain as our application domain.
While we made clear that crawling the web is a
crucial process in order to get the amounts of in-
domain data needed by the unsupervised relation
extraction methods, we did not yet point out that
we will also examine the reverse direction, i.e. the
possibility to use the extracted relations for fur-
ther improving the focused crawler. A focused
crawler that is powered by semantic relations be-
tween entities would raise a new level of semanti-
cally focused crawls. Additionally, we will inves-
tigate possibilities for further narrowing the rela-
tions found by our system. Here it is possible to
further categorize or cluster the relations by using
either the similarity graph or the features itself, as
done by Pantel and Lin (2002).
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999265333333333">
This work has been supported by the German In-
stitute for Educational Research (DIPF) under the
KDSL program.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99640211627907">
Alan Akbik, Larysa Visengeriyeva, Johannes
Kirschnick, and Alexander L¨oser. 2013. Ef-
fective selectional restrictions for unsupervised
relation extraction. In Proceedings of the Sixth In-
ternational Joint Conference on Natural Language
Processing (IJCNLP), pages 1312–1320, Nagoya,
Japan.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting on
Association for Computational Linguistics (ACL),
pages 26–33, Toulouse, France.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
a collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209–226.
Douglas Biber. 1995. Dimensions of Register Varia-
tion: A Cross-Linguistic Comparison. Cambridge
University Press, Cambridge.
Chris Biemann and Martin Riedl. 2013. Text: Now
in 2d! a framework for lexical expansion with con-
textual similarity. Journal of Language Modelling
(JLM), 1(1):55–95.
Chris Biemann, Felix Bildhauer, Stefan Evert, Dirk
Goldhahn, Uwe Quasthoff, Roland Sch¨afer, Jo-
hannes Simon, Swiezinski Swiezinski, and Torsten
Zesch. 2013. Scalable construction of high-quality
web corpora. Journal for Language Technology and
Computational Linguistics (JLCL), 27(2).
Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,
Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir Radev, and Yee Fan Tan. 2008.
The acl anthology reference corpus: A reference
dataset for bibliographic research in computational
linguistics. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the Eleventh Annual Conference on
Computational Learning Theory (COLT), pages 92–
100, Madison, Wisconsin, USA.
</reference>
<page confidence="0.997786">
18
</page>
<table confidence="0.470062166666667">
Thorsten Brants and Alex Franz. 2006. Web 1T5-gram
Version 1. Linguistic Data Consortium, Philadel-
phia.
Alon Halevy, Peter Norvig, and Fernando Pereira.
2009. The unreasonable effectiveness of data. IEEE
Intelligent Systems, 24(2):8–12.
</table>
<reference confidence="0.998864368932039">
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
24th Conference on Artificial Intelligence (AAAI),
Atlanta, GA, USA.
Soumen Chakrabarti, Martin van den Berg, and Byron
Dom. 1999. Focused crawling: a new approach
to topic-specific web resource discovery. Computer
Networks, 31(11–16):1623–1640.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 976–
986, Portland, Oregon, USA.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Machine Learning Challenges. Evalu-
ating Predictive Uncertainty, Visual Object Classifi-
cation, and Recognising Tectual Entailment, volume
3944 of Lecture Notes in Computer Science, pages
177–190. Springer Berlin Heidelberg.
Dmitry Davidov and Ari Rappoport. 2008a. Clas-
sification of semantic relationships between nomi-
nals using pattern clusters. In Proceedings of the
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL-HLT), pages 227–235, Columbus, Ohio.
Dmitry Davidov and Ari Rappoport. 2008b. Unsuper-
vised discovery of generic relationships using pat-
tern clusters and its evaluation by automatically gen-
erated SAT analogy questions. In Proceedings of the
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL-HLT), pages 692–700, Columbus, Ohio.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proceed-
ings of the 45th Annual Meeting of the Association of
Computational Linguistics (ACL), pages 232–239,
Prague, Czech Republic.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extrac-
tion from the web. Communications of the ACM,
51(12):68–74.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic
relations between nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalu-
ation (SemEval), pages 13–18, Prague, Czech Re-
public.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146–162.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 15th Conference on Computational Linguistics
(Coling), pages 539–545, Nantes, France.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian
Pad´o, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. Semeval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of the
Fifth International Workshop on Semantic Evalua-
tion (SemEval), pages 33–38, Los Angeles, Califor-
nia.
Florian Holz and Chris Biemann. 2008. Unsupervised
and knowledge-free learning of compound splits and
periphrases. In CICLing 2008: Proceedings of the
Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 117–127, Haifa, Israel.
Adam Kilgarriff. 2007. Googleology is bad science.
Computational Linguististics (CL), 33(1):147–151.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL), pages 423–430, Sapporo,
Japan.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, pages
181–184, Detroit, Michigan.
Mike Lewis and Mark Steedman. 2013. Unsuper-
vised induction of cross-lingual semantic relations.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 681–692, Seattle, WA, USA.
Dekang Lin and Patrick Pantel. 2001. Dirt - discov-
ery of inference rules from text. In Proceedings of
the Seventh International Conference on Knowledge
Discovery and Data Mining (KDD), pages 323–328,
San Francisco, California.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools
for web-scale n-grams. In Proceedings of the 7th
International Conference on Language Resources
and Evaluation (LREC), pages 2221–2227, Valletta,
Malta.
</reference>
<page confidence="0.983746">
19
</page>
<reference confidence="0.999694215384616">
Filippo Menczer, Gautam Pant, and Padmini Srini-
vasan. 2004. Topical web crawlers: Evaluating
adaptive algorithms. ACM Transactions Internet
Technology (TOIT), 4(4):378–419.
George A Miller and Walter G Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
Cognitive Processes (LCP), 6(1):1–28.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011, Suntec, Singapore.
Gordon Mohr, Michele Kimpton, Micheal Stack, and
Igor Ranitovic. 2004. Introduction to heritrix,
an archival quality web crawler. In Proceedings
of the 4th International Web Archiving Workshop
IWAW’04, Bath, UK.
Vivi Nastase, Preslav Nakov, Diarmuid O´ S´eaghdha,
and Stan Szpakowicz. 2013. Semantic relations be-
tween nominals. In Synthesis Lectures on Human
Language Technologies, volume 6. Morgan &amp; Cay-
pool Publishers.
Patrick Pantel and Dekang Lin. 2002. Document
clustering with committees. In Proceedings of the
25th Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pages 199–206.
Emily Pitler, Shane Bergsma, Dekang Lin, and Ken-
neth Church. 2010. Using web-scale n-grams to im-
prove base np parsing performance. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling), pages 886–894, Beijing,
China.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, London.
Matthias Richter, Uwe Quasthoff, Erla Hallsteinsd´ottir,
and Chris Biemann. 2006. Exploiting the leipzig
corpora collection. In Proceesings of the IS-LTC,
Ljubljana, Slovenia.
Roland Sch¨afer and Felix Bildhauer. 2012. Building
large corpora from the web using a new efficient
tool chain. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC), pages 486–493, Istanbul, Turkey.
John Sowa. 2000. Knowledge Representation: Logi-
cal, Philosophical and Computational Foundations.
Brooks Cole Publishing Co., Pacific Grove, CA.
Ang Sun and Ralph Grishman. 2010. Semi-supervised
semantic pattern discovery with guidance from un-
supervised pattern clusters. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling), pages 1194–1202, Beijing,
China.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal for Artificial Intelligence Research
(JAIR), 37:141–188.
Peter D. Turney. 2005. Measuring semantic similar-
ity by latent relational analysis. In Proceedings of
the 19th International Joint Conference on Artificial
Intelligence (IJCAI), pages 1136–1141, Edinburgh,
Scotland, UK.
</reference>
<page confidence="0.9948">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.797875">
<title confidence="0.99022">Unsupervised Relation Extraction of In-Domain from Focused Crawls</title>
<author confidence="0.998841">Steffen Remus</author>
<affiliation confidence="0.98743775">FG Language Technology Computer Science Department, Technische Universit¨at Darmstadt Information Center for Education German Institute for Educational Research (DIPF)</affiliation>
<email confidence="0.986073">remus@cs.tu-darmstadt.de</email>
<abstract confidence="0.995133172413793">This thesis proposal approaches unsupervised relation extraction from web data, which is collected by crawling only those parts of the web that are from the same domain as a relatively small reference corpus. The first part of this proposal is concerned with the efficient discovery of web documents for a particular domain and in a particular language. We create a combined, focused web crawling system that automatically collects relevant documents and minimizes the amount of irrelevant web content. The collected web data is semantically processed in order to acquire rich in-domain knowledge. Here, we focus on fully unsupervised relation extraction by employing the extended distributional hypothesis. We use distributional similarities between two pairs of nominals based on dependency paths as context and vice versa for identifying relational structure. We apply our system for the domain of educational sciences by focusing primarily on crawling scientific educational publications in the web. We are able to produce promising initial results on relation identification and we will discuss future directions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan Akbik</author>
<author>Larysa Visengeriyeva</author>
<author>Johannes Kirschnick</author>
<author>Alexander L¨oser</author>
</authors>
<title>Effective selectional restrictions for unsupervised relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>1312--1320</pages>
<location>Nagoya, Japan.</location>
<marker>Akbik, Visengeriyeva, Kirschnick, L¨oser, 2013</marker>
<rawString>Alan Akbik, Larysa Visengeriyeva, Johannes Kirschnick, and Alexander L¨oser. 2013. Effective selectional restrictions for unsupervised relation extraction. In Proceedings of the Sixth International Joint Conference on Natural Language Processing (IJCNLP), pages 1312–1320, Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>26--33</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="6637" citStr="Banko and Brill, 2001" startWordPosition="1046" endWordPosition="1049">nd part of this proposal is the semantic structuring of texts—in our particular use case scientific publications from the educational domain —by using data-driven techniques of computational semantics. The resulting structure enables forms of post-processing like inference or reasoning. In the semantic structuring part, the overall goal is to discover knowledge which can then be used in further steps. Specifically, we will focus on unsupervised relation extraction. 2 Related Work 2.1 Focused Crawling The development of high-quality data-driven semantic models relies on corpora of large sizes (Banko and Brill, 2001; Halevy et al., 2009), and the world wide web is by far the biggest available source of textual data. Nowadays, a large number of research projects rely on corpora that comes from data in the world wide web. The Webas-Corpus Kool Yinitiative1 (WaCKy) (Baroni et al., 2009) for example produced one of the largest corpora used in linguistic research which comes from web documents. Another research initiative which produces a variety of corpora by crawling the web is the COW2 (corpora from the web) project (Sch¨afer and Bildhauer, 2012). Currently one of the largest N-gram corpora coming from web</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics (ACL), pages 26–33, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="6910" citStr="Baroni et al., 2009" startWordPosition="1095" endWordPosition="1098">or reasoning. In the semantic structuring part, the overall goal is to discover knowledge which can then be used in further steps. Specifically, we will focus on unsupervised relation extraction. 2 Related Work 2.1 Focused Crawling The development of high-quality data-driven semantic models relies on corpora of large sizes (Banko and Brill, 2001; Halevy et al., 2009), and the world wide web is by far the biggest available source of textual data. Nowadays, a large number of research projects rely on corpora that comes from data in the world wide web. The Webas-Corpus Kool Yinitiative1 (WaCKy) (Baroni et al., 2009) for example produced one of the largest corpora used in linguistic research which comes from web documents. Another research initiative which produces a variety of corpora by crawling the web is the COW2 (corpora from the web) project (Sch¨afer and Bildhauer, 2012). Currently one of the largest N-gram corpora coming from web data is the Google V1 and Google V2 (Lin et al., 2010), which are used e.g. for improving noun phrase parsing (Pitler et al., 2010). Also the predecessor Google Web1T (Brants and Franz, 2006), which is computed from 1 Trillion words from the web, is heavily used in the co</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>Dimensions of Register Variation: A Cross-Linguistic Comparison.</title>
<date>1995</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="12805" citStr="Biber, 1995" startWordPosition="2030" endWordPosition="2031">others from TextRunner6 (Etzioni et al., 2008). The identification of relations in natural language texts is at the heart of such systems. 3 Methodology 3.1 Focused Crawling Language models (LMs) are a rather old but well understood and generally accepted concept in Computational Linguistics and Information Retrieval. Our focused crawling strategy builds upon the idea of utilizing a language model to discriminate between relevant and irrelevant web documents. The key idea of this methodology is that web pages which come from a certain domain — which implies the use of a particular vocabulary (Biber, 1995)—link to other documents of the same domain. The assumption is that the crawler will most likely stay in the same topical domain as the initial language model was generated from. Thus the crawling process can be terminated when enough data has been collected. 4Never Ending Language Learner: http://rtw.ml.cmu.edu/ 5http://ai.cs.washington.edu/ projects/open-information-extraction 6http://openie.cs.washington.edu/ 13 A language model is a statistical model over short sequences of consecutive tokens called Ngrams. The order of a language model is defined by the length of such sequences, i.e. the </context>
<context position="15030" citStr="Biber, 1995" startWordPosition="2401" endWordPosition="2402">unts of data often outperform complex models with training procedures that are feasible only for small data. Anyway, we have only little data in the initial phase, thus we use Kneser and Ney’s model. Perplexity is used to measure the amount of compatibility with another model X: Perplexity(X) = 2H(X) , (3) where H(X) = − |x |Ex∈X log2 p(x) is the cross entropy of a model X. Using perplexity we are able to tell how well the language model fits the data and vice versa. The key idea is that documents which come from a certain register or domain — which implies the use of a particular vocabulary (Biber, 1995) —link to other documents of the same register. Using perplexity, we are able to rank outgoing links by their deviation from our initial language model. Hence weblinks that are extracted from a highly deviating webpage are less prioritized for harvesting. The open source crawler software Heritrix7 (Mohr et al., 2004) forms the basis of our focused crawling strategy, since it provides a well-established framework which is easily extensible through its modularity. 3.2 Identification of Nominals Nominals are defined to be expressions which syntactically act like nouns or noun phrases (Quirk et al</context>
</contexts>
<marker>Biber, 1995</marker>
<rawString>Douglas Biber. 1995. Dimensions of Register Variation: A Cross-Linguistic Comparison. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Martin Riedl</author>
</authors>
<title>Text: Now in 2d! a framework for lexical expansion with contextual similarity.</title>
<date>2013</date>
<journal>Journal of Language Modelling (JLM),</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="18636" citStr="Biemann and Riedl, 2013" startWordPosition="2996" endWordPosition="2999">gree of ambiguity. Think for example about the expression ’X comes from Y’ which could be both, a causal relation or a locational relation depending on the meaning of X and Y. That being said, we use the Extended Distributional Hypothesis in order to extract meaningful relations from text. We follow Lin and Pantel (2001) and use the dependency path between two entities to identify both, similar entity pairs and similar dependency paths. Specifically we use the Stanford Parser8 (Klein and Manning, 2003) to get a collapsed dependency graph representation of a sentence, and apply the JoBimText9 (Biemann and Riedl, 2013) software for computing the distributional similarities. By using the JoBimText framework, we accept their theory, which states that dimensionalityreduced vector space models are not expressive enough to capture the full semantics of words, phrases, sentences, documents or relations. Turney and Pantel (2010) surveyed that vector space models are commonly used in computational semantics and that they are able to capture the meaning of words. However, by doing various kinds of vector space transformations, e.g. dimensionality reduction with SVD10 important information from the long tail, i.e. it</context>
</contexts>
<marker>Biemann, Riedl, 2013</marker>
<rawString>Chris Biemann and Martin Riedl. 2013. Text: Now in 2d! a framework for lexical expansion with contextual similarity. Journal of Language Modelling (JLM), 1(1):55–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Felix Bildhauer</author>
<author>Stefan Evert</author>
<author>Dirk Goldhahn</author>
<author>Uwe Quasthoff</author>
<author>Roland Sch¨afer</author>
<author>Johannes Simon</author>
<author>Swiezinski Swiezinski</author>
<author>Torsten Zesch</author>
</authors>
<title>Scalable construction of high-quality web corpora.</title>
<date>2013</date>
<booktitle>Journal for Language Technology and Computational Linguistics (JLCL),</booktitle>
<pages>27--2</pages>
<marker>Biemann, Bildhauer, Evert, Goldhahn, Quasthoff, Sch¨afer, Simon, Swiezinski, Zesch, 2013</marker>
<rawString>Chris Biemann, Felix Bildhauer, Stefan Evert, Dirk Goldhahn, Uwe Quasthoff, Roland Sch¨afer, Johannes Simon, Swiezinski Swiezinski, and Torsten Zesch. 2013. Scalable construction of high-quality web corpora. Journal for Language Technology and Computational Linguistics (JLCL), 27(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Robert Dale</author>
<author>Bonnie Dorr</author>
<author>Bryan Gibson</author>
<author>Mark Joseph</author>
<author>Min-Yen Kan</author>
<author>Dongwon Lee</author>
<author>Brett Powley</author>
<author>Dragomir Radev</author>
<author>Yee Fan Tan</author>
</authors>
<title>The acl anthology reference corpus: A reference dataset for bibliographic research in computational linguistics.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="7942" citStr="Bird et al., 2008" startWordPosition="1269" endWordPosition="1272">ing noun phrase parsing (Pitler et al., 2010). Also the predecessor Google Web1T (Brants and Franz, 2006), which is computed from 1 Trillion words from the web, is heavily used in the community. All these corpora are generated from general texts which either come from crawling specific top-level-domains (tlds) or preprocessing and filtering very large amounts of texts for a specified language. Additionally, we are not aware of any corpus that is created by collecting pdf documents. This is especially an issue when aiming at a corpus of scientific publications, such as e.g. the ACL anthology3 (Bird et al., 2008). As of today, electronic publications are primarily distributed as pdf documents. Usually these are omitted by the particular crawler because of a number of practical issues, e.g. difficulties in extracting clean plaintext. Further, we are not interested in sheer collection size, but also in domain specificity. Crawling is a time-consuming process and it comes with logistic challenges for processing the resulting data. While standard breadth-first or depth-first crawling strategies can be adjusted to include pdf files, we want to avoid to harvest the huge bulk of data 1http://wacky.sslmit.uni</context>
</contexts>
<marker>Bird, Dale, Dorr, Gibson, Joseph, Kan, Lee, Powley, Radev, Tan, 2008</marker>
<rawString>Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson, Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett Powley, Dragomir Radev, and Yee Fan Tan. 2008. The acl anthology reference corpus: A reference dataset for bibliographic research in computational linguistics. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Eleventh Annual Conference on Computational Learning Theory (COLT),</booktitle>
<pages>92--100</pages>
<location>Madison, Wisconsin, USA.</location>
<contexts>
<context position="9217" citStr="Blum and Mitchell, 1998" startWordPosition="1462" endWordPosition="1465">c.comp.nus.edu.sg/ 12 that we are not interested in, namely those documents that are of a different topical domain as our initial domain definition. In focused crawling, which is sometimes also referred to as topical crawling, web crawlers are designed to harvest those parts of the web first that are more interesting for a particular topic (Chakrabarti et al., 1999). By doing so, taskspecific corpora can be generated fast and efficient. Typical focused crawlers use machine learning techniques or heuristics to prioritize newly discovered URIs (unified resource identifier) for further crawling (Blum and Mitchell, 1998; Chakrabarti et al., 1999; Menczer et al., 2004). In our scenario however, we do not rely on positively and negatively labeled data. The source documents that serve as the domain definition are assumed to be given in plain text. The development of tools that are able to generate in-domain web-corpora from focused crawls is the premise for further generating rich semantic models tailored to a target domain. 2.2 Unsupervised Relation Extraction The unsupervised relation extraction (URE) part of this proposal is specifically focused on extracting relations between nominals. Typically the choice </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory (COLT), pages 92– 100, Madison, Wisconsin, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 24th Conference on Artificial Intelligence (AAAI),</booktitle>
<location>Atlanta, GA, USA.</location>
<contexts>
<context position="3300" citStr="Carlson et al., 2010" startWordPosition="510" endWordPosition="513"> This work is structured as follows: First we will motivate our combined approach and introduce each part individually. We then present related work in Section 2. Section 3 explains the methodology of both parts, and in Section 4 we outline the evaluation procedure of each of the components individually. This is followed by some preliminary results in Section 5, and Section 6 concludes this proposal with some prospects for future work. 1.1 Motivation The identification of relations between entities solely from text is one of many challenges in the development of language understanding system (Carlson et al., 2010; Etzioni et al., 2008); and yet it is the one step with the highest information gain. It is used e.g. for taxonomy induction (Hearst, 1992) or ontology accumulation (Mintz et al., 2009) or even for identifying facts that express general knowledge and that often recur (Chambers and Jurafsky, 2011). Davidov et al. (2007) performed unsupervised relation extraction by actively mining the web and showed major improve11 Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 11–20, Gothenburg, Sweden, April </context>
<context position="12151" citStr="Carlson et al., 2010" startWordPosition="1923" endWordPosition="1926">intz et al., 2009) and latent relational analysis (LRA) (Turney, 2005) to extreme unsupervised relation extraction (Davidov and Rappoport, 2008a), just to name a few. The importance of unsupervised methods for relation extraction is obvious: The manual creation of knowledge resources is time consuming and expensive in terms of manpower. Though manual resources are typically very precise they are almost always lacking of lexical and relational coverage. The extraction of relations between entities is a crucial process which is performed by every modern language understanding system like NELL4 (Carlson et al., 2010) or machine reading5, which evolved among others from TextRunner6 (Etzioni et al., 2008). The identification of relations in natural language texts is at the heart of such systems. 3 Methodology 3.1 Focused Crawling Language models (LMs) are a rather old but well understood and generally accepted concept in Computational Linguistics and Information Retrieval. Our focused crawling strategy builds upon the idea of utilizing a language model to discriminate between relevant and irrelevant web documents. The key idea of this methodology is that web pages which come from a certain domain — which im</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an architecture for neverending language learning. In Proceedings of the 24th Conference on Artificial Intelligence (AAAI), Atlanta, GA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soumen Chakrabarti</author>
<author>Martin van den Berg</author>
<author>Byron Dom</author>
</authors>
<title>Focused crawling: a new approach to topic-specific web resource discovery.</title>
<date>1999</date>
<journal>Computer Networks,</journal>
<pages>31--11</pages>
<marker>Chakrabarti, van den Berg, Dom, 1999</marker>
<rawString>Soumen Chakrabarti, Martin van den Berg, and Byron Dom. 1999. Focused crawling: a new approach to topic-specific web resource discovery. Computer Networks, 31(11–16):1623–1640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Template-based information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>976--986</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="3598" citStr="Chambers and Jurafsky, 2011" startWordPosition="560" endWordPosition="564">s individually. This is followed by some preliminary results in Section 5, and Section 6 concludes this proposal with some prospects for future work. 1.1 Motivation The identification of relations between entities solely from text is one of many challenges in the development of language understanding system (Carlson et al., 2010; Etzioni et al., 2008); and yet it is the one step with the highest information gain. It is used e.g. for taxonomy induction (Hearst, 1992) or ontology accumulation (Mintz et al., 2009) or even for identifying facts that express general knowledge and that often recur (Chambers and Jurafsky, 2011). Davidov et al. (2007) performed unsupervised relation extraction by actively mining the web and showed major improve11 Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 11–20, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics ments in the detection of new facts from only little initial seed. They used a major web search engine as a vital component of their system. According to Kilgarriff (2007), however, this strategy is unreliable and should be avoided. Neve</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Template-based information extraction without the templates. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 976– 986, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment,</title>
<date>2006</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>3944</volume>
<pages>177--190</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="23867" citStr="Dagan et al., 2006" startWordPosition="3837" endWordPosition="3840"> were provided as relation classification challenge datasets (Girju et al., 2007; Hendrickx 13independent and identically distributed Figure 2: Schematic overview of the evaluation procedure for a particular crawl. et al., 2010). Whereas the first dataset is provided as a binary classification task, the second is a multi-way classification task. However, both datasets can be transformed to address the one or the other task. This is possible because the challenge is already finished. 2. We apply our extracted relations for assisting classification algorithms for the task of textual entailment (Dagan et al., 2006). 3. Following Davidov and Rappoport (2008b) we would further like to apply our system to the task of question answering. While the first approach is an intrinsic evaluation, the other three approaches are extrinsic, i.e. the extracted relations are used in a particular task which is then evaluated against some gold standard. 5 Preliminary Results 5.1 Focused crawling Table 1 shows some quantitative characteristics of a non-focused crawl. Here the crawl was performed as a scoped crawl, which means that it was bounded to the German top-level-domain ’.de’ and additionally by a maximum number of </context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, volume 3944 of Lecture Notes in Computer Science, pages 177–190. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Classification of semantic relationships between nominals using pattern clusters.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),</booktitle>
<pages>227--235</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="11673" citStr="Davidov and Rappoport, 2008" startWordPosition="1846" endWordPosition="1850">in a handful of well developed text patterns. For example ’X is a Y’ or ’X and other Ys’ manifest themselves as hyponymic relations. However, not every kind of relation is as easy to identify as those ’is-a’ relations. Often semantic relations cannot be expressed by any pattern. A variety of methods were developed that automatically find new patterns and entities with or without supervision. These methods reach from bootstrapping methods (Hearst, 1992) over distant supervision (Mintz et al., 2009) and latent relational analysis (LRA) (Turney, 2005) to extreme unsupervised relation extraction (Davidov and Rappoport, 2008a), just to name a few. The importance of unsupervised methods for relation extraction is obvious: The manual creation of knowledge resources is time consuming and expensive in terms of manpower. Though manual resources are typically very precise they are almost always lacking of lexical and relational coverage. The extraction of relations between entities is a crucial process which is performed by every modern language understanding system like NELL4 (Carlson et al., 2010) or machine reading5, which evolved among others from TextRunner6 (Etzioni et al., 2008). The identification of relations </context>
<context position="23909" citStr="Davidov and Rappoport (2008" startWordPosition="3843" endWordPosition="3846">ication challenge datasets (Girju et al., 2007; Hendrickx 13independent and identically distributed Figure 2: Schematic overview of the evaluation procedure for a particular crawl. et al., 2010). Whereas the first dataset is provided as a binary classification task, the second is a multi-way classification task. However, both datasets can be transformed to address the one or the other task. This is possible because the challenge is already finished. 2. We apply our extracted relations for assisting classification algorithms for the task of textual entailment (Dagan et al., 2006). 3. Following Davidov and Rappoport (2008b) we would further like to apply our system to the task of question answering. While the first approach is an intrinsic evaluation, the other three approaches are extrinsic, i.e. the extracted relations are used in a particular task which is then evaluated against some gold standard. 5 Preliminary Results 5.1 Focused crawling Table 1 shows some quantitative characteristics of a non-focused crawl. Here the crawl was performed as a scoped crawl, which means that it was bounded to the German top-level-domain ’.de’ and additionally by a maximum number of 20 hops from the start seed14. The crawl w</context>
</contexts>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2008a. Classification of semantic relationships between nominals using pattern clusters. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 227–235, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated SAT analogy questions.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),</booktitle>
<pages>692--700</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="11673" citStr="Davidov and Rappoport, 2008" startWordPosition="1846" endWordPosition="1850">in a handful of well developed text patterns. For example ’X is a Y’ or ’X and other Ys’ manifest themselves as hyponymic relations. However, not every kind of relation is as easy to identify as those ’is-a’ relations. Often semantic relations cannot be expressed by any pattern. A variety of methods were developed that automatically find new patterns and entities with or without supervision. These methods reach from bootstrapping methods (Hearst, 1992) over distant supervision (Mintz et al., 2009) and latent relational analysis (LRA) (Turney, 2005) to extreme unsupervised relation extraction (Davidov and Rappoport, 2008a), just to name a few. The importance of unsupervised methods for relation extraction is obvious: The manual creation of knowledge resources is time consuming and expensive in terms of manpower. Though manual resources are typically very precise they are almost always lacking of lexical and relational coverage. The extraction of relations between entities is a crucial process which is performed by every modern language understanding system like NELL4 (Carlson et al., 2010) or machine reading5, which evolved among others from TextRunner6 (Etzioni et al., 2008). The identification of relations </context>
<context position="23909" citStr="Davidov and Rappoport (2008" startWordPosition="3843" endWordPosition="3846">ication challenge datasets (Girju et al., 2007; Hendrickx 13independent and identically distributed Figure 2: Schematic overview of the evaluation procedure for a particular crawl. et al., 2010). Whereas the first dataset is provided as a binary classification task, the second is a multi-way classification task. However, both datasets can be transformed to address the one or the other task. This is possible because the challenge is already finished. 2. We apply our extracted relations for assisting classification algorithms for the task of textual entailment (Dagan et al., 2006). 3. Following Davidov and Rappoport (2008b) we would further like to apply our system to the task of question answering. While the first approach is an intrinsic evaluation, the other three approaches are extrinsic, i.e. the extracted relations are used in a particular task which is then evaluated against some gold standard. 5 Preliminary Results 5.1 Focused crawling Table 1 shows some quantitative characteristics of a non-focused crawl. Here the crawl was performed as a scoped crawl, which means that it was bounded to the German top-level-domain ’.de’ and additionally by a maximum number of 20 hops from the start seed14. The crawl w</context>
</contexts>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2008b. Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated SAT analogy questions. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 692–700, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
<author>Moshe Koppel</author>
</authors>
<title>Fully unsupervised discovery of conceptspecific relationships by web mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<pages>232--239</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3621" citStr="Davidov et al. (2007)" startWordPosition="565" endWordPosition="568">ed by some preliminary results in Section 5, and Section 6 concludes this proposal with some prospects for future work. 1.1 Motivation The identification of relations between entities solely from text is one of many challenges in the development of language understanding system (Carlson et al., 2010; Etzioni et al., 2008); and yet it is the one step with the highest information gain. It is used e.g. for taxonomy induction (Hearst, 1992) or ontology accumulation (Mintz et al., 2009) or even for identifying facts that express general knowledge and that often recur (Chambers and Jurafsky, 2011). Davidov et al. (2007) performed unsupervised relation extraction by actively mining the web and showed major improve11 Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 11–20, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics ments in the detection of new facts from only little initial seed. They used a major web search engine as a vital component of their system. According to Kilgarriff (2007), however, this strategy is unreliable and should be avoided. Nevertheless, the web is un</context>
</contexts>
<marker>Davidov, Rappoport, Koppel, 2007</marker>
<rawString>Dmitry Davidov, Ari Rappoport, and Moshe Koppel. 2007. Fully unsupervised discovery of conceptspecific relationships by web mining. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL), pages 232–239, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2008</date>
<journal>Communications of the ACM,</journal>
<volume>51</volume>
<issue>12</issue>
<contexts>
<context position="3323" citStr="Etzioni et al., 2008" startWordPosition="514" endWordPosition="517">ed as follows: First we will motivate our combined approach and introduce each part individually. We then present related work in Section 2. Section 3 explains the methodology of both parts, and in Section 4 we outline the evaluation procedure of each of the components individually. This is followed by some preliminary results in Section 5, and Section 6 concludes this proposal with some prospects for future work. 1.1 Motivation The identification of relations between entities solely from text is one of many challenges in the development of language understanding system (Carlson et al., 2010; Etzioni et al., 2008); and yet it is the one step with the highest information gain. It is used e.g. for taxonomy induction (Hearst, 1992) or ontology accumulation (Mintz et al., 2009) or even for identifying facts that express general knowledge and that often recur (Chambers and Jurafsky, 2011). Davidov et al. (2007) performed unsupervised relation extraction by actively mining the web and showed major improve11 Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 11–20, Gothenburg, Sweden, April 26-30 2014. c�2014 Asso</context>
<context position="12239" citStr="Etzioni et al., 2008" startWordPosition="1936" endWordPosition="1939">rvised relation extraction (Davidov and Rappoport, 2008a), just to name a few. The importance of unsupervised methods for relation extraction is obvious: The manual creation of knowledge resources is time consuming and expensive in terms of manpower. Though manual resources are typically very precise they are almost always lacking of lexical and relational coverage. The extraction of relations between entities is a crucial process which is performed by every modern language understanding system like NELL4 (Carlson et al., 2010) or machine reading5, which evolved among others from TextRunner6 (Etzioni et al., 2008). The identification of relations in natural language texts is at the heart of such systems. 3 Methodology 3.1 Focused Crawling Language models (LMs) are a rather old but well understood and generally accepted concept in Computational Linguistics and Information Retrieval. Our focused crawling strategy builds upon the idea of utilizing a language model to discriminate between relevant and irrelevant web documents. The key idea of this methodology is that web pages which come from a certain domain — which implies the use of a particular vocabulary (Biber, 1995)—link to other documents of the sa</context>
</contexts>
<marker>Etzioni, Banko, Soderland, Weld, 2008</marker>
<rawString>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. 2008. Open information extraction from the web. Communications of the ACM, 51(12):68–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>Semeval-2007 task 04: Classification of semantic relations between nominals.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluation (SemEval),</booktitle>
<pages>13--18</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="23328" citStr="Girju et al., 2007" startWordPosition="3750" endWordPosition="3753">h collects data that is used for evaluating V and V results in a lower perplexity score, is preferred as it better models the target domain. Figure 2 shows a schematic overview of the three phases of evaluation. 4.2 Unsupervised Relation Extraction The evaluation of relation extraction is a nontrivial task, as unsupervised categories do usually not exactly match the distinctions taken in annotation studies. For the evaluation of our method we consider the following three approaches: 1. We test our relations directly on datasets that were provided as relation classification challenge datasets (Girju et al., 2007; Hendrickx 13independent and identically distributed Figure 2: Schematic overview of the evaluation procedure for a particular crawl. et al., 2010). Whereas the first dataset is provided as a binary classification task, the second is a multi-way classification task. However, both datasets can be transformed to address the one or the other task. This is possible because the challenge is already finished. 2. We apply our extracted relations for assisting classification algorithms for the task of textual entailment (Dagan et al., 2006). 3. Following Davidov and Rappoport (2008b) we would further</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. Semeval-2007 task 04: Classification of semantic relations between nominals. In Proceedings of the Fourth International Workshop on Semantic Evaluation (SemEval), pages 13–18, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="1801" citStr="Harris (1954)" startWordPosition="268" endWordPosition="270">educational sciences by focusing primarily on crawling scientific educational publications in the web. We are able to produce promising initial results on relation identification and we will discuss future directions. 1 Introduction Knowledge acquisition from written or spoken text is a field of interest not only for theoretical reasons but also for practical applications, such as semantic search, question answering and knowledge management, just to name a few. In this work, we propose an approach for unsupervised relation extraction (URE) where we make use of the Distributional Hypothesis by Harris (1954). The underlying data set is collected from the world wide web by focusing on web documents that are from the same domain as a small initialization data set that is provided beforehand. We hereby enrich this existing, domain-defining, corpus with more data of the same kind. This is needed for practical reasons when working with the Distributional Hypothesis (Harris, 1954): A lot of data is required for plausible outcomes and an appropriate coverage. However, we want as little irrelevant data as possible. The proposal’s contribution is thus twofold: a) focused crawling, and b) unsupervised rela</context>
<context position="16923" citStr="Harris (1954)" startWordPosition="2712" endWordPosition="2713">ification of nominals can be seen as the task of identifying reliable multi-wordexpressions (MWEs), which is a research question of its own right. As a first simplified approach we only consider nouns and heads of noun compounds to be representatives for nominals. E.g. a compound is used as an entity, but only the head is taken into further consideration as a representative since it encapsulates the main meaning for that phrase. 3.3 Unsupervised Relation Extraction Our system is founded in the idea of distributional semantics on the level of dependency parses. The Distributional Hypothesis by Harris (1954) (cf. also (Miller and Charles, 1991)) states that words which tend to occur in similar contexts tend to have similar meanings. This implies that one can estimate the meaning of an unknown word by considering the context in that it occurs. Lin and Pantel (2001) extended this hypothesis to cover shortest paths in the dependency graph— so-called dependency paths —and introduced the Extended Distributional Hypothesis. This extended hypothesis states that dependency paths which tend to occur in similar contexts, i.e. they connect the simi7http://crawler.archive.org 14 lar sets of words, also tend </context>
<context position="27370" citStr="Harris (1954)" startWordPosition="4424" endWordPosition="4425">em are also interpretable for humans. Figure 3: Two crawl runs under same conditions and with same settings. Upper: a focused crawl run. Lower: a non-focused crawl run. 6 Conclusion and Future Work This research thesis proposal addressed the two major objectives: 1. crawling with a focus on in-domain data by using a language model of an initial corpus, which is small compared to the expected result of the crawls, in order to discriminate relevant web documents from irrelevant web documents, and 2. unsupervised relation extraction by following the principles of the Distributional Hypothesis by Harris (1954) resp. the Extended Distributional Hypothesis by Lin and Pantel (2001). The promising preliminary results encourage us to examine this approach for further directions. Specifically the yet unaddressed parts of the evaluation will be investigated. Further, the unsupervised relation extraction techniques will be applied on the complete set of in-domain data, thus finalizing the workflow of enriching a small amount of domain defining data with web data 17 gold/NN :: ounce/NN crude/NN:: barrel/NN oil/NN:: barrel/NN futures/NNS :: barrel/NN stock/NN:: share/NN graduate/NN :: University/NNP graduate</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S. Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 15th Conference on Computational Linguistics (Coling),</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="3440" citStr="Hearst, 1992" startWordPosition="538" endWordPosition="539">k in Section 2. Section 3 explains the methodology of both parts, and in Section 4 we outline the evaluation procedure of each of the components individually. This is followed by some preliminary results in Section 5, and Section 6 concludes this proposal with some prospects for future work. 1.1 Motivation The identification of relations between entities solely from text is one of many challenges in the development of language understanding system (Carlson et al., 2010; Etzioni et al., 2008); and yet it is the one step with the highest information gain. It is used e.g. for taxonomy induction (Hearst, 1992) or ontology accumulation (Mintz et al., 2009) or even for identifying facts that express general knowledge and that often recur (Chambers and Jurafsky, 2011). Davidov et al. (2007) performed unsupervised relation extraction by actively mining the web and showed major improve11 Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 11–20, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics ments in the detection of new facts from only little initial seed. They used a </context>
<context position="10990" citStr="Hearst (1992)" startWordPosition="1740" endWordPosition="1741">beled data. In contrast to open information extraction, in unsupervised relation extraction the collected relations are aggregated in order to identify the most promising relations for expressing interesting facts. Here, the grouping is made explicit for further processing. One possible application of relation extraction is the establishment of so-called knowledge graphs (Sowa, 2000), which encode facts that manifest solely from text. The knowledge graph can then be used e.g. for reasoning, that is finding new facts from existing facts. Many approaches exist for acquiring knowledge from text. Hearst (1992) first discovered that relations between entities occur in a handful of well developed text patterns. For example ’X is a Y’ or ’X and other Ys’ manifest themselves as hyponymic relations. However, not every kind of relation is as easy to identify as those ’is-a’ relations. Often semantic relations cannot be expressed by any pattern. A variety of methods were developed that automatically find new patterns and entities with or without supervision. These methods reach from bootstrapping methods (Hearst, 1992) over distant supervision (Mintz et al., 2009) and latent relational analysis (LRA) (Tur</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 15th Conference on Computational Linguistics (Coling), pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fifth International Workshop on Semantic Evaluation (SemEval),</booktitle>
<pages>33--38</pages>
<location>Los Angeles, California.</location>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2010</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the Fifth International Workshop on Semantic Evaluation (SemEval), pages 33–38, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Holz</author>
<author>Chris Biemann</author>
</authors>
<title>Unsupervised and knowledge-free learning of compound splits and periphrases.</title>
<date>2008</date>
<booktitle>In CICLing 2008: Proceedings of the Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>117--127</pages>
<location>Haifa,</location>
<contexts>
<context position="16218" citStr="Holz and Biemann, 2008" startWordPosition="2596" endWordPosition="2599"> nouns or noun phrases (Quirk et al., 1985, p.335). Another definition according to Nastase et al. (2013) is that nominals are defined to be in one of the following classes: a) common nouns, b) proper nouns, c) multi-word proper nouns, d) deverbal nouns, e) deadjectival nouns, or f) non-compositional (adjective) noun phrases. In this work we will follow the definition given by Nastase et al. (2013). We will further address only relations that are at least realized by verbal or prepositional phrases and ignore relations that are implicitly present in compounds, which is a task of its own, cf. (Holz and Biemann, 2008). Note however we do not ignore relations between compounds, but within compounds. The identification of nominals can be seen as the task of identifying reliable multi-wordexpressions (MWEs), which is a research question of its own right. As a first simplified approach we only consider nouns and heads of noun compounds to be representatives for nominals. E.g. a compound is used as an entity, but only the head is taken into further consideration as a representative since it encapsulates the main meaning for that phrase. 3.3 Unsupervised Relation Extraction Our system is founded in the idea of d</context>
</contexts>
<marker>Holz, Biemann, 2008</marker>
<rawString>Florian Holz and Chris Biemann. 2008. Unsupervised and knowledge-free learning of compound splits and periphrases. In CICLing 2008: Proceedings of the Conference on Intelligent Text Processing and Computational Linguistics, pages 117–127, Haifa, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Googleology is bad science.</title>
<date>2007</date>
<journal>Computational Linguististics (CL),</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="4132" citStr="Kilgarriff (2007)" startWordPosition="644" endWordPosition="645">s that express general knowledge and that often recur (Chambers and Jurafsky, 2011). Davidov et al. (2007) performed unsupervised relation extraction by actively mining the web and showed major improve11 Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 11–20, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics ments in the detection of new facts from only little initial seed. They used a major web search engine as a vital component of their system. According to Kilgarriff (2007), however, this strategy is unreliable and should be avoided. Nevertheless, the web is undeniably the largest source for any kind of data, and we feel the need for developing easy-touse components that make it possible to create corpora from the web with only little effort (cf. e.g. Biemann et al. (2013)). When it comes to specific in-domain information, the complete world wide web is first of all too vast to be processed conveniently, and second the gain is little because of too much irrelevant information. Thus we need methods for reducing the size of data to process without losing the focus</context>
</contexts>
<marker>Kilgarriff, 2007</marker>
<rawString>Adam Kilgarriff. 2007. Googleology is bad science. Computational Linguististics (CL), 33(1):147–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="18519" citStr="Klein and Manning, 2003" startWordPosition="2978" endWordPosition="2981">g, meaning every relation is assigned exactly to one cluster, we argue that relations are accompanied by a certain degree of ambiguity. Think for example about the expression ’X comes from Y’ which could be both, a causal relation or a locational relation depending on the meaning of X and Y. That being said, we use the Extended Distributional Hypothesis in order to extract meaningful relations from text. We follow Lin and Pantel (2001) and use the dependency path between two entities to identify both, similar entity pairs and similar dependency paths. Specifically we use the Stanford Parser8 (Klein and Manning, 2003) to get a collapsed dependency graph representation of a sentence, and apply the JoBimText9 (Biemann and Riedl, 2013) software for computing the distributional similarities. By using the JoBimText framework, we accept their theory, which states that dimensionalityreduced vector space models are not expressive enough to capture the full semantics of words, phrases, sentences, documents or relations. Turney and Pantel (2010) surveyed that vector space models are commonly used in computational semantics and that they are able to capture the meaning of words. However, by doing various kinds of vec</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL), pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling. In</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>181--184</pages>
<location>Detroit, Michigan.</location>
<contexts>
<context position="14271" citStr="Kneser and Ney, 1995" startWordPosition="2265" endWordPosition="2268">e particular N-gram. In the simplest case the probability of an N-gram is computed as: count(wi−N+1:i) p(wi|wi−n+1:i−1) = , (2) count (wi−N+1:i−1) where count(N-gram) is a function that takes as argument an N-gram of length N or an N-gram of length N − 1 and returns the frequency of observations in the source corpus. This model has some obvious limitations when it comes to outof-vocabulary (OOV) terms because of probabilities being zero. Due to this limitation, a number of LMs were proposed which handle OOV terms well. One of the most advanced language models is the Kneser-Ney language model (Kneser and Ney, 1995), which applies an advanced interpolation technique for OOV issues. According to Halevy et al. (2009), simpler models that are trained on large amounts of data often outperform complex models with training procedures that are feasible only for small data. Anyway, we have only little data in the initial phase, thus we use Kneser and Ney’s model. Perplexity is used to measure the amount of compatibility with another model X: Perplexity(X) = 2H(X) , (3) where H(X) = − |x |Ex∈X log2 p(x) is the cross entropy of a model X. Using perplexity we are able to tell how well the language model fits the da</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181–184, Detroit, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Unsupervised induction of cross-lingual semantic relations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>681--692</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="20982" citStr="Lewis and Steedman (2013)" startWordPosition="3364" endWordPosition="3367">ar relation instance has form ’@1-PATH-@2’, where ’-PATH-’ is the instantiation of the directed shortest path in the collapsed dependency path starting from a particular ’X’ and ending in a particular ’Y’. The @1, resp. @2, symbolizes the place where ’X’ and ’Y’ were found in the path. Here we restrict the path to be shorter than five edges and additionally we ignore paths that have only nn relations, i.e. compound dependency relations. See Figure 1 for an illustration of this strategy on two small example sentences. Note that this procedure strongly coheres with the methodologies proposed by Lewis and Steedman (2013) or Akbik et al. (2013). We then compute the distributional similarities for both directions: a) similarities of entity pairs by paths, and b) similarities of paths by entity pairs. This gives us two different views on the data. 4 Evaluation The two major directions of this paper, i.e. the focused crawling part and the unsupervised relation extraction part are evaluated individually and independent of each other. First we will present an 12Images generated with GrammarScope: http://grammarscope.sf.net. 15 evaluation methodology to assess the quality of the crawler and second we will outline th</context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013. Unsupervised induction of cross-lingual semantic relations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 681–692, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>323--328</pages>
<location>San Francisco, California.</location>
<contexts>
<context position="17184" citStr="Lin and Pantel (2001)" startWordPosition="2756" endWordPosition="2760">r nominals. E.g. a compound is used as an entity, but only the head is taken into further consideration as a representative since it encapsulates the main meaning for that phrase. 3.3 Unsupervised Relation Extraction Our system is founded in the idea of distributional semantics on the level of dependency parses. The Distributional Hypothesis by Harris (1954) (cf. also (Miller and Charles, 1991)) states that words which tend to occur in similar contexts tend to have similar meanings. This implies that one can estimate the meaning of an unknown word by considering the context in that it occurs. Lin and Pantel (2001) extended this hypothesis to cover shortest paths in the dependency graph— so-called dependency paths —and introduced the Extended Distributional Hypothesis. This extended hypothesis states that dependency paths which tend to occur in similar contexts, i.e. they connect the simi7http://crawler.archive.org 14 lar sets of words, also tend to have similar meanings. Sun and Grishman (2010) used an agglomerative hierarchical clustering based approach in order to group the patterns found by Lin and Pantel’s method. The clusters are used in a semisupervised way to extract relation instances that are </context>
<context position="27440" citStr="Lin and Pantel (2001)" startWordPosition="4432" endWordPosition="4435"> under same conditions and with same settings. Upper: a focused crawl run. Lower: a non-focused crawl run. 6 Conclusion and Future Work This research thesis proposal addressed the two major objectives: 1. crawling with a focus on in-domain data by using a language model of an initial corpus, which is small compared to the expected result of the crawls, in order to discriminate relevant web documents from irrelevant web documents, and 2. unsupervised relation extraction by following the principles of the Distributional Hypothesis by Harris (1954) resp. the Extended Distributional Hypothesis by Lin and Pantel (2001). The promising preliminary results encourage us to examine this approach for further directions. Specifically the yet unaddressed parts of the evaluation will be investigated. Further, the unsupervised relation extraction techniques will be applied on the complete set of in-domain data, thus finalizing the workflow of enriching a small amount of domain defining data with web data 17 gold/NN :: ounce/NN crude/NN:: barrel/NN oil/NN:: barrel/NN futures/NNS :: barrel/NN stock/NN:: share/NN graduate/NN :: University/NNP graduate/NN :: School/NNP graduate/NN :: College/NNP goals/NNS :: season/NN po</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of inference rules from text. In Proceedings of the Seventh International Conference on Knowledge Discovery and Data Mining (KDD), pages 323–328, San Francisco, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
<author>Heng Ji</author>
<author>Satoshi Sekine</author>
<author>David Yarowsky</author>
<author>Shane Bergsma</author>
<author>Kailash Patil</author>
<author>Emily Pitler</author>
<author>Rachel Lathbury</author>
<author>Vikram Rao</author>
<author>Kapil Dalwani</author>
<author>Sushant Narsale</author>
</authors>
<title>New tools for web-scale n-grams.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>2221--2227</pages>
<location>Valletta,</location>
<contexts>
<context position="7292" citStr="Lin et al., 2010" startWordPosition="1161" endWordPosition="1164">wide web is by far the biggest available source of textual data. Nowadays, a large number of research projects rely on corpora that comes from data in the world wide web. The Webas-Corpus Kool Yinitiative1 (WaCKy) (Baroni et al., 2009) for example produced one of the largest corpora used in linguistic research which comes from web documents. Another research initiative which produces a variety of corpora by crawling the web is the COW2 (corpora from the web) project (Sch¨afer and Bildhauer, 2012). Currently one of the largest N-gram corpora coming from web data is the Google V1 and Google V2 (Lin et al., 2010), which are used e.g. for improving noun phrase parsing (Pitler et al., 2010). Also the predecessor Google Web1T (Brants and Franz, 2006), which is computed from 1 Trillion words from the web, is heavily used in the community. All these corpora are generated from general texts which either come from crawling specific top-level-domains (tlds) or preprocessing and filtering very large amounts of texts for a specified language. Additionally, we are not aware of any corpus that is created by collecting pdf documents. This is especially an issue when aiming at a corpus of scientific publications, s</context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, Patil, Pitler, Lathbury, Rao, Dalwani, Narsale, 2010</marker>
<rawString>Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale. 2010. New tools for web-scale n-grams. In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC), pages 2221–2227, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filippo Menczer</author>
<author>Gautam Pant</author>
<author>Padmini Srinivasan</author>
</authors>
<title>Topical web crawlers: Evaluating adaptive algorithms.</title>
<date>2004</date>
<journal>ACM Transactions Internet Technology (TOIT),</journal>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="9266" citStr="Menczer et al., 2004" startWordPosition="1470" endWordPosition="1473">, namely those documents that are of a different topical domain as our initial domain definition. In focused crawling, which is sometimes also referred to as topical crawling, web crawlers are designed to harvest those parts of the web first that are more interesting for a particular topic (Chakrabarti et al., 1999). By doing so, taskspecific corpora can be generated fast and efficient. Typical focused crawlers use machine learning techniques or heuristics to prioritize newly discovered URIs (unified resource identifier) for further crawling (Blum and Mitchell, 1998; Chakrabarti et al., 1999; Menczer et al., 2004). In our scenario however, we do not rely on positively and negatively labeled data. The source documents that serve as the domain definition are assumed to be given in plain text. The development of tools that are able to generate in-domain web-corpora from focused crawls is the premise for further generating rich semantic models tailored to a target domain. 2.2 Unsupervised Relation Extraction The unsupervised relation extraction (URE) part of this proposal is specifically focused on extracting relations between nominals. Typically the choice of the entity type depends merely on the final ta</context>
</contexts>
<marker>Menczer, Pant, Srinivasan, 2004</marker>
<rawString>Filippo Menczer, Gautam Pant, and Padmini Srinivasan. 2004. Topical web crawlers: Evaluating adaptive algorithms. ACM Transactions Internet Technology (TOIT), 4(4):378–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes (LCP),</booktitle>
<pages>6--1</pages>
<contexts>
<context position="16960" citStr="Miller and Charles, 1991" startWordPosition="2716" endWordPosition="2719"> be seen as the task of identifying reliable multi-wordexpressions (MWEs), which is a research question of its own right. As a first simplified approach we only consider nouns and heads of noun compounds to be representatives for nominals. E.g. a compound is used as an entity, but only the head is taken into further consideration as a representative since it encapsulates the main meaning for that phrase. 3.3 Unsupervised Relation Extraction Our system is founded in the idea of distributional semantics on the level of dependency parses. The Distributional Hypothesis by Harris (1954) (cf. also (Miller and Charles, 1991)) states that words which tend to occur in similar contexts tend to have similar meanings. This implies that one can estimate the meaning of an unknown word by considering the context in that it occurs. Lin and Pantel (2001) extended this hypothesis to cover shortest paths in the dependency graph— so-called dependency paths —and introduced the Extended Distributional Hypothesis. This extended hypothesis states that dependency paths which tend to occur in similar contexts, i.e. they connect the simi7http://crawler.archive.org 14 lar sets of words, also tend to have similar meanings. Sun and Gri</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A Miller and Walter G Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes (LCP), 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>1003--1011</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="3486" citStr="Mintz et al., 2009" startWordPosition="543" endWordPosition="546">ethodology of both parts, and in Section 4 we outline the evaluation procedure of each of the components individually. This is followed by some preliminary results in Section 5, and Section 6 concludes this proposal with some prospects for future work. 1.1 Motivation The identification of relations between entities solely from text is one of many challenges in the development of language understanding system (Carlson et al., 2010; Etzioni et al., 2008); and yet it is the one step with the highest information gain. It is used e.g. for taxonomy induction (Hearst, 1992) or ontology accumulation (Mintz et al., 2009) or even for identifying facts that express general knowledge and that often recur (Chambers and Jurafsky, 2011). Davidov et al. (2007) performed unsupervised relation extraction by actively mining the web and showed major improve11 Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 11–20, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics ments in the detection of new facts from only little initial seed. They used a major web search engine as a vital component o</context>
<context position="11548" citStr="Mintz et al., 2009" startWordPosition="1830" endWordPosition="1833">oaches exist for acquiring knowledge from text. Hearst (1992) first discovered that relations between entities occur in a handful of well developed text patterns. For example ’X is a Y’ or ’X and other Ys’ manifest themselves as hyponymic relations. However, not every kind of relation is as easy to identify as those ’is-a’ relations. Often semantic relations cannot be expressed by any pattern. A variety of methods were developed that automatically find new patterns and entities with or without supervision. These methods reach from bootstrapping methods (Hearst, 1992) over distant supervision (Mintz et al., 2009) and latent relational analysis (LRA) (Turney, 2005) to extreme unsupervised relation extraction (Davidov and Rappoport, 2008a), just to name a few. The importance of unsupervised methods for relation extraction is obvious: The manual creation of knowledge resources is time consuming and expensive in terms of manpower. Though manual resources are typically very precise they are almost always lacking of lexical and relational coverage. The extraction of relations between entities is a crucial process which is performed by every modern language understanding system like NELL4 (Carlson et al., 20</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003–1011, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gordon Mohr</author>
<author>Michele Kimpton</author>
<author>Micheal Stack</author>
<author>Igor Ranitovic</author>
</authors>
<title>Introduction to heritrix, an archival quality web crawler.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Web Archiving Workshop IWAW’04,</booktitle>
<location>Bath, UK.</location>
<contexts>
<context position="15348" citStr="Mohr et al., 2004" startWordPosition="2454" endWordPosition="2457">e H(X) = − |x |Ex∈X log2 p(x) is the cross entropy of a model X. Using perplexity we are able to tell how well the language model fits the data and vice versa. The key idea is that documents which come from a certain register or domain — which implies the use of a particular vocabulary (Biber, 1995) —link to other documents of the same register. Using perplexity, we are able to rank outgoing links by their deviation from our initial language model. Hence weblinks that are extracted from a highly deviating webpage are less prioritized for harvesting. The open source crawler software Heritrix7 (Mohr et al., 2004) forms the basis of our focused crawling strategy, since it provides a well-established framework which is easily extensible through its modularity. 3.2 Identification of Nominals Nominals are defined to be expressions which syntactically act like nouns or noun phrases (Quirk et al., 1985, p.335). Another definition according to Nastase et al. (2013) is that nominals are defined to be in one of the following classes: a) common nouns, b) proper nouns, c) multi-word proper nouns, d) deverbal nouns, e) deadjectival nouns, or f) non-compositional (adjective) noun phrases. In this work we will foll</context>
</contexts>
<marker>Mohr, Kimpton, Stack, Ranitovic, 2004</marker>
<rawString>Gordon Mohr, Michele Kimpton, Micheal Stack, and Igor Ranitovic. 2004. Introduction to heritrix, an archival quality web crawler. In Proceedings of the 4th International Web Archiving Workshop IWAW’04, Bath, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semantic relations between nominals.</title>
<date>2013</date>
<booktitle>In Synthesis Lectures on Human Language Technologies,</booktitle>
<volume>6</volume>
<publisher>Morgan &amp; Caypool Publishers.</publisher>
<marker>Nastase, Nakov, S´eaghdha, Szpakowicz, 2013</marker>
<rawString>Vivi Nastase, Preslav Nakov, Diarmuid O´ S´eaghdha, and Stan Szpakowicz. 2013. Semantic relations between nominals. In Synthesis Lectures on Human Language Technologies, volume 6. Morgan &amp; Caypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Document clustering with committees.</title>
<date>2002</date>
<booktitle>In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>199--206</pages>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Document clustering with committees. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 199–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
</authors>
<title>Using web-scale n-grams to improve base np parsing performance.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling),</booktitle>
<pages>886--894</pages>
<location>Beijing, China.</location>
<contexts>
<context position="7369" citStr="Pitler et al., 2010" startWordPosition="1174" endWordPosition="1177"> a large number of research projects rely on corpora that comes from data in the world wide web. The Webas-Corpus Kool Yinitiative1 (WaCKy) (Baroni et al., 2009) for example produced one of the largest corpora used in linguistic research which comes from web documents. Another research initiative which produces a variety of corpora by crawling the web is the COW2 (corpora from the web) project (Sch¨afer and Bildhauer, 2012). Currently one of the largest N-gram corpora coming from web data is the Google V1 and Google V2 (Lin et al., 2010), which are used e.g. for improving noun phrase parsing (Pitler et al., 2010). Also the predecessor Google Web1T (Brants and Franz, 2006), which is computed from 1 Trillion words from the web, is heavily used in the community. All these corpora are generated from general texts which either come from crawling specific top-level-domains (tlds) or preprocessing and filtering very large amounts of texts for a specified language. Additionally, we are not aware of any corpus that is created by collecting pdf documents. This is especially an issue when aiming at a corpus of scientific publications, such as e.g. the ACL anthology3 (Bird et al., 2008). As of today, electronic p</context>
</contexts>
<marker>Pitler, Bergsma, Lin, Church, 2010</marker>
<rawString>Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth Church. 2010. Using web-scale n-grams to improve base np parsing performance. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling), pages 886–894, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<location>Longman, London.</location>
<contexts>
<context position="15637" citStr="Quirk et al., 1985" startWordPosition="2499" endWordPosition="2502">iber, 1995) —link to other documents of the same register. Using perplexity, we are able to rank outgoing links by their deviation from our initial language model. Hence weblinks that are extracted from a highly deviating webpage are less prioritized for harvesting. The open source crawler software Heritrix7 (Mohr et al., 2004) forms the basis of our focused crawling strategy, since it provides a well-established framework which is easily extensible through its modularity. 3.2 Identification of Nominals Nominals are defined to be expressions which syntactically act like nouns or noun phrases (Quirk et al., 1985, p.335). Another definition according to Nastase et al. (2013) is that nominals are defined to be in one of the following classes: a) common nouns, b) proper nouns, c) multi-word proper nouns, d) deverbal nouns, e) deadjectival nouns, or f) non-compositional (adjective) noun phrases. In this work we will follow the definition given by Nastase et al. (2013). We will further address only relations that are at least realized by verbal or prepositional phrases and ignore relations that are implicitly present in compounds, which is a task of its own, cf. (Holz and Biemann, 2008). Note however we d</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Richter</author>
<author>Uwe Quasthoff</author>
<author>Erla Hallsteinsd´ottir</author>
<author>Chris Biemann</author>
</authors>
<title>Exploiting the leipzig corpora collection.</title>
<date>2006</date>
<booktitle>In Proceesings of the IS-LTC,</booktitle>
<location>Ljubljana, Slovenia.</location>
<marker>Richter, Quasthoff, Hallsteinsd´ottir, Biemann, 2006</marker>
<rawString>Matthias Richter, Uwe Quasthoff, Erla Hallsteinsd´ottir, and Chris Biemann. 2006. Exploiting the leipzig corpora collection. In Proceesings of the IS-LTC, Ljubljana, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Sch¨afer</author>
<author>Felix Bildhauer</author>
</authors>
<title>Building large corpora from the web using a new efficient tool chain.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>486--493</pages>
<location>Istanbul, Turkey.</location>
<marker>Sch¨afer, Bildhauer, 2012</marker>
<rawString>Roland Sch¨afer and Felix Bildhauer. 2012. Building large corpora from the web using a new efficient tool chain. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC), pages 486–493, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sowa</author>
</authors>
<title>Knowledge Representation: Logical, Philosophical and Computational Foundations.</title>
<date>2000</date>
<publisher>Brooks Cole Publishing Co.,</publisher>
<location>Pacific Grove, CA.</location>
<contexts>
<context position="10763" citStr="Sowa, 2000" startWordPosition="1704" endWordPosition="1705">al., 2013). Nominals are discussed in more detail in Section 3.2. Unsupervised methods for relation extraction is a particularly interesting area of research because of its applicability across languages without relying on labeled data. In contrast to open information extraction, in unsupervised relation extraction the collected relations are aggregated in order to identify the most promising relations for expressing interesting facts. Here, the grouping is made explicit for further processing. One possible application of relation extraction is the establishment of so-called knowledge graphs (Sowa, 2000), which encode facts that manifest solely from text. The knowledge graph can then be used e.g. for reasoning, that is finding new facts from existing facts. Many approaches exist for acquiring knowledge from text. Hearst (1992) first discovered that relations between entities occur in a handful of well developed text patterns. For example ’X is a Y’ or ’X and other Ys’ manifest themselves as hyponymic relations. However, not every kind of relation is as easy to identify as those ’is-a’ relations. Often semantic relations cannot be expressed by any pattern. A variety of methods were developed t</context>
</contexts>
<marker>Sowa, 2000</marker>
<rawString>John Sowa. 2000. Knowledge Representation: Logical, Philosophical and Computational Foundations. Brooks Cole Publishing Co., Pacific Grove, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
<author>Ralph Grishman</author>
</authors>
<title>Semi-supervised semantic pattern discovery with guidance from unsupervised pattern clusters.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling),</booktitle>
<pages>1194--1202</pages>
<location>Beijing, China.</location>
<contexts>
<context position="17572" citStr="Sun and Grishman (2010)" startWordPosition="2817" endWordPosition="2820">rles, 1991)) states that words which tend to occur in similar contexts tend to have similar meanings. This implies that one can estimate the meaning of an unknown word by considering the context in that it occurs. Lin and Pantel (2001) extended this hypothesis to cover shortest paths in the dependency graph— so-called dependency paths —and introduced the Extended Distributional Hypothesis. This extended hypothesis states that dependency paths which tend to occur in similar contexts, i.e. they connect the simi7http://crawler.archive.org 14 lar sets of words, also tend to have similar meanings. Sun and Grishman (2010) used an agglomerative hierarchical clustering based approach in order to group the patterns found by Lin and Pantel’s method. The clusters are used in a semisupervised way to extract relation instances that are used in a bootstrapping fashion to find new relations. While Sun and Grishman (2010) performed a hard clustering, meaning every relation is assigned exactly to one cluster, we argue that relations are accompanied by a certain degree of ambiguity. Think for example about the expression ’X comes from Y’ which could be both, a causal relation or a locational relation depending on the mean</context>
</contexts>
<marker>Sun, Grishman, 2010</marker>
<rawString>Ang Sun and Ralph Grishman. 2010. Semi-supervised semantic pattern discovery with guidance from unsupervised pattern clusters. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling), pages 1194–1202, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal for Artificial Intelligence Research (JAIR),</journal>
<pages>37--141</pages>
<contexts>
<context position="18945" citStr="Turney and Pantel (2010)" startWordPosition="3041" endWordPosition="3045">Pantel (2001) and use the dependency path between two entities to identify both, similar entity pairs and similar dependency paths. Specifically we use the Stanford Parser8 (Klein and Manning, 2003) to get a collapsed dependency graph representation of a sentence, and apply the JoBimText9 (Biemann and Riedl, 2013) software for computing the distributional similarities. By using the JoBimText framework, we accept their theory, which states that dimensionalityreduced vector space models are not expressive enough to capture the full semantics of words, phrases, sentences, documents or relations. Turney and Pantel (2010) surveyed that vector space models are commonly used in computational semantics and that they are able to capture the meaning of words. However, by doing various kinds of vector space transformations, e.g. dimensionality reduction with SVD10 important information from the long tail, i.e. items that do not occur often, is lost. Instead, Biemann and Riedl (2013) introduced the scalable JoBimText framework, which makes use of the Distributional Hypothesis. We take this as a starting point to steer away from the use of vector space models. For each entity pair ’X::Y’, where ’X’ and ’Y’ are nominal</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal for Artificial Intelligence Research (JAIR), 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Measuring semantic similarity by latent relational analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1136--1141</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="11600" citStr="Turney, 2005" startWordPosition="1839" endWordPosition="1840">92) first discovered that relations between entities occur in a handful of well developed text patterns. For example ’X is a Y’ or ’X and other Ys’ manifest themselves as hyponymic relations. However, not every kind of relation is as easy to identify as those ’is-a’ relations. Often semantic relations cannot be expressed by any pattern. A variety of methods were developed that automatically find new patterns and entities with or without supervision. These methods reach from bootstrapping methods (Hearst, 1992) over distant supervision (Mintz et al., 2009) and latent relational analysis (LRA) (Turney, 2005) to extreme unsupervised relation extraction (Davidov and Rappoport, 2008a), just to name a few. The importance of unsupervised methods for relation extraction is obvious: The manual creation of knowledge resources is time consuming and expensive in terms of manpower. Though manual resources are typically very precise they are almost always lacking of lexical and relational coverage. The extraction of relations between entities is a crucial process which is performed by every modern language understanding system like NELL4 (Carlson et al., 2010) or machine reading5, which evolved among others </context>
</contexts>
<marker>Turney, 2005</marker>
<rawString>Peter D. Turney. 2005. Measuring semantic similarity by latent relational analysis. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), pages 1136–1141, Edinburgh, Scotland, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>