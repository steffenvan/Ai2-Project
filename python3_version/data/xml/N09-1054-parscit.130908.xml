<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005377">
<title confidence="0.994025">
Predicting Response to Political Blog Posts with Topic Models
</title>
<author confidence="0.995505">
Tae Yano William W. Cohen Noah A. Smith
</author>
<affiliation confidence="0.900084">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998024">
{taey,wcohen,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997678" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.989292214285714">
In this paper we model discussions in online
political blogs. To do this, we extend Latent
Dirichlet Allocation (Blei et al., 2003), in var-
ious ways to capture different characteristics
of the data. Our models jointly describe the
generation of the primary documents (posts)
as well as the authorship and, optionally, the
contents of the blog community’s verbal reac-
tions to each post (comments). We evaluate
our model on a novel comment prediction task
where the models are used to predict which
blog users will leave comments on a given
post. We also provide a qualitative discussion
about what the models discover.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978142857143">
Web logging (blogging) and its social impact have
recently attracted considerable public and scientific
interest. One use of blogs is as a community dis-
cussion forum, especially for political discussion
and debate. Blogging has arguably opened a new
channel for huge numbers of people to express their
views with unprecedented speed and to unprece-
dented audiences. Their collective behavior in the
blogosphere has already been noted in the Ameri-
can political arena (Adamic and Glance, 2005). In
this paper we attempt to deliver a framework useful
for analyzing text in blogs quantitatively as well as
qualitatively. Better blog text analysis could lead to
better automated recommendation, organization, ex-
traction, and retrieval systems, and might facilitate
data-driven research in the social sciences.
Apart from the potential social utility of text pro-
cessing for this domain, we believe blog data is wor-
thy of scientific study in its own right. The sponta-
neous, reactive, and informal nature of the language
in this domain seems to defy conventional analytical
approaches in NLP such as supervised text classifi-
cation (Mullen and Malouf, 2006), yet the data are
rich in argumentative, topical, and temporal struc-
ture that can perhaps be modeled computationally.
We are especially interested in the semi-causal struc-
ture of blog discussions, in which a post “spawns”
comments (or fails to do so), which meander among
topics and asides and show the personality of the
participants and the community.
Our approach is to develop probabilistic mod-
els for the generation of blog posts and comments
jointly within a blog site. The model is an extension
of Latent Dirichlet Allocation (Blei et al., 2003).
Unsupervised topic models can be applied to collec-
tions of unannotated documents, requiring very lit-
tle corpus engineering. They can be easily adapted
to new problems by altering the graphical model,
then applying standard probabilistic inference algo-
rithms. Different models can be compared to ex-
plore the ramifications of different hypotheses about
the data. For example, we will explore whether the
contents of posts a user has commented on in the
past and the words she has used can help predict
which posts she will respond to in the future.
The paper is organized as follows. In §2 we re-
view prior work on topic modeling for document
collections and studies of social media like political
blogs. We then provide a qualitative characterization
of political blogs, highlighting some of the features
we believe a computational model should capture
and discuss our new corpus of political blogs (§3).
We present several different candidate topic models
that aim to capture these ideas in §4. §5 shows our
empirical evaluation on a new comment prediction
task and a qualitative analysis of the models learned.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99990075">
Network analysis, including citation analysis, has
been applied to document collections on the Web
(Cohn and Hofmann, 2001). Adamic and Glance
(2005) applied network analysis to the political bl-
</bodyText>
<page confidence="0.976816">
477
</page>
<note confidence="0.890689">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 477–485,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999936676470589">
ogosphere. The study modeled the large, complex
structure of the political blogosphere as a network
of hyperlinks among the blog sites, demonstrated the
viability of link structure for information discovery,
though their analysis of text content was less exten-
sive. In contrast, the text seems to be of interest
to social scientists studying blogs as an artifact of
the political process. Although attempts to quanti-
tatively analyze the contents of political texts have
been made, results from classical, supervised text
classification experiments are mixed (Mullen and
Malouf, 2006; Malouf and Mullen, 2007). Also, a
consensus on useful, reliable annotation or catego-
rization schemes for political texts, at any level of
granularity, has yet to emerge.
Meanwhile, latent topic modeling has become a
widely used unsupervised text analysis tool. The ba-
sic aim of those models is to discover recurring pat-
terns of “topics” within a text collection. LDA was
introduced by Blei et al. (2003) and has been espe-
cially popular because it can be understood as a gen-
erative model and because it discovers understand-
able topics in many scenarios (Steyvers and Grif-
fiths, 2007). Its declarative specification makes it
easy to extend for new kinds of text collections. The
technique has been applied to Web document collec-
tions, notably for community discovery in social net-
works (Zhang et al., 2007), opinion mining in user
reviews (Titov and McDonald, 2008), and sentiment
discovery in free-text annotations (Branavan et al.,
2008). Dredze et al. (2008) applied LDA to a collec-
tion of email for summary keyword extraction. The
authors evaluated the model with proxy tasks such as
recipient prediction. More closely related to the data
considered in this work, Lin et al. (2008) applied a
variation of LDA to ideological discourse.
A notable trend in the recent research is to aug-
ment the models to describe non-textual evidence
alongside the document collection. Several such
studies are especially relevant to our work. Blei and
Jordan (2003) were one of the earliest results in this
trend. The concept was developed into more general
framework by Blei and McAuliffe (2008). Steyvers
et al. (2004) and Rosen-Zvi et al. (2004) first ex-
tended LDA to explicitly model the influence of au-
thorship, applying the model to a collection of aca-
demic papers from CiteSeer. The model combined
the ideas from the mixture model proposed by Mc-
Callum (1999) and LDA. In this model, an abstract
notion “author” is associated with a distribution over
topics. Another approach to the same document col-
lection based on LDA was used for citation network
analysis. Erosheva et al. (2004), following Cohn and
Hofmann (2001), defined a generative process not
only for each word in the text, but also its citation
to other documents in the collection, thereby cap-
turing the notion of relations between the document
into one generative process. Nallapati and Cohen
(2008) introduced the Link-PLSA-LDA model, in
which the contents of the citing document and the
“influences” on the document (its citations to exist-
ing literature), as well as the contents of the cited
documents, are modeled together. They further ap-
plied the Link-PLSA-LDA model to a blog corpus
to analyze its cross citation structure via hyperlinks.
In this work, we aim to model the data within blog
conversations, focusing on comments left by a blog
community in response to a blogger’s post.
</bodyText>
<sectionHeader confidence="0.966641" genericHeader="method">
3 Political Blog Data
</sectionHeader>
<bodyText confidence="0.995317">
We discuss next the dataset used in our experiments.
</bodyText>
<subsectionHeader confidence="0.997844">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9997979375">
We have collected blog posts and comments from
40 blog sites focusing on American politics during
the period November 2007 to October 2008, con-
temporaneous with the presidential elections. The
discussions on these blogs focus on American poli-
tics, and many themes appear: the Democratic and
Republican candidates, speculation about the results
of various state contests, and various aspects of
international and (more commonly) domestic poli-
tics. The sites were selected to have a variety of
political leanings. From this pool we chose five
blogs which accumulated a large number of posts
during this period: Carpetbagger (CB),1 Daily Kos
(DK),2 Matthew Yglesias (MY),3 Red State (RS),4
and Right Wing News (RWN).5 CB and MY ceased
as independent bloggers in August 2008.6 Because
</bodyText>
<footnote confidence="0.999973428571429">
1http://www.thecarpetbaggerreport.com
2http://www.dailykos.com
3http://matthewyglesias.theatlantic.com
4http://www.redstate.com
5http://www.rightwingnews.com
6The authors of those blogs now write for larger on-
line media, CB for Washingon Monthly at http://www.
</footnote>
<page confidence="0.993462">
478
</page>
<table confidence="0.999832666666667">
MY RWN CB RS DK
Time span (from 11/11/07) –8/2/08 –10/10/08 –8/25/08 –6/26/08 –4/9/08
# training posts 1607 1052 1080 2045 2146
# words (total) 110,788 194,948 183,635 321,699 221,820
(on average per post) (68) (185) (170) (157) (103)
# comments 56,507 34,734 34,244 59,687 425,494
(on average per post) (35) (33) (31) (29) (198)
(unique commenters, on average) (24) (13) (24) (14) (93)
# words in comments (total) 2,287,843 1,073,726 1,411,363 1,675,098 8,359,456
(on average per post) (1423) (1020) (1306) (819) (3895)
(on average per comment) (41) (31) (41) (27) (20)
Post vocabulary size 6,659 9,707 7,579 12,282 10,179
Comment vocabulary size 33,350 22,024 24,702 25,473 58,591
Size of user pool 7,341 963 5,059 2,789 16,849
# test posts 183 113 121 231 240
</table>
<tableCaption confidence="0.999869">
Table 1: Details of the blog data used in this paper.
</tableCaption>
<bodyText confidence="0.999892">
our focus in this paper is on blog posts and their
comments, we discard posts on which no one com-
mented within six days. We also remove posts with
too few words: specifically, we retain a post only
if it has at least five words in the main entry, and
at least five words in the comment section. All
posts are represented as text only (images, hyper-
links, and other non-text contents are ignored). To
standardize the texts, we remove from the text 670
commonly used stop words, non-alphabet symbols
including punctuation marks, and strings consisting
of only symbols and digits. We also discard infre-
quent words from our dataset: for each word in a
post’s main entry, we kept it only if it appears at
least one more time in some main entry. We ap-
ply the same word pruning to the comment section
as well. The corpus size and the vocabulary size of
the five datasets are listed in Table 1. In addition,
each user’s handle is replaced with a unique inte-
ger. The dataset is available for download at http:
//www.ark.cs.cmu.edu/blog-data.
</bodyText>
<subsectionHeader confidence="0.998363">
3.2 Qualitative Properties of Blogs
</subsectionHeader>
<bodyText confidence="0.981448538461538">
We believe that readers’ reactions to blog posts are
an integral part of blogging activity. Often com-
ments are much more substantial and informative
than the post. While circumspective articles limit
themselves to allusions or oblique references, read-
ers’ comments may point to heart of the matter more
washingtonmonthly.com and MY for Think Progress
athttp://yglesias.thinkprogress.org.
boldly. Opinions are expressed more blatantly in
comments. Comments may help a human (or au-
tomated) reader to understand the post more clearly
when the main text is too terse, stylized, or technical.
Although the main entry and its comments are
certainly related and at least partially address similar
topics, they are markedly different in several ways.
First of all, their vocabulary is noticeably different.
Comments are more casual, conversational, and full
of jargon. They are less carefully edited and there-
fore contain more misspellings and typographical er-
rors. There is more diversity among comments than
within the single-author post, both in style of writing
and in what commenters like to talk about. Depend-
ing on the subjects covered in a blog post, different
types of people are inspired to respond. We believe
that analyzing a piece of text based on the reaction
it causes among those who read it is a fascinating
problem for NLP.
Blog sites are also quite distinctive from each
other. Their language, discussion topics, and col-
lective political orientations vary greatly. Their vol-
umes also vary; multi-author sites (such as DK, RS)
may consistently produce over twenty posts per day,
while single-author sites (such as MY, CB) may have
a day with only one post. Single author sites also
tend to have a much smaller vocabulary and range
of interests. The sites are also culturally different
in commenting styles; some sites are full of short
interjections, while others have longer, more analyt-
ical comments. On some sites, users appear to be
</bodyText>
<page confidence="0.993385">
479
</page>
<figure confidence="0.999714315789474">
w w′ u
�
z
N
α
0
)y 7
z′
M
D
w
�
z z′
N M
α
0
7
u
D
</figure>
<figureCaption confidence="0.988614">
Figure 1: Left:
</figureCaption>
<bodyText confidence="0.886863263157895">
LinkLDA (Erosheva
et al., 2004), with
variables reassigned.
Right:
CommentLDA. In
training, w, u, and
(in CommentLDA)
w/ are observed. D is
the number of blog
posts, and N and M
are the word counts
in the post and the all
of its comments,
respectively. Here we
“count by verbosity.”
close-knit, while others have high turnover.
In the next section, we describe how we apply
topic models to political blogs, and how these prob-
abilistic models can put to use to make predictions.
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="method">
4 Generative Models
</sectionHeader>
<bodyText confidence="0.999865">
The first model we consider is LinkLDA, which is
analogous to the model of Erosheva et al. (2004),
though the variables are given different meanings
here.7 The graphical model is depicted in Fig. 1
(left). As in LDA andits many variants, this model
postulates a set of latent “topic” variables, where
each topic k corresponds to a multinomial distribu-
tion βk over the vocabulary. In addition to gener-
ating the words in the post from its topic mixture,
this model also generates a bag of users who respond
to the post, according to a distribution γ over users
given topics. In this model, the topic distribution θ
is all that determines the text content of the post and
which users will respond to the post.
LinkLDA models which users are likely to re-
spond to a post, but it does not model what they
will write. Our new model, ConlnlentLDA, gen-
erates the contents of the comments (see Fig. 1,
right). In order to capture the differences in lan-
guage style between posts and comments, however,
we use a different conditional distribution over com-
ment words given topics, β&apos;. The post text, comment
text, and commenter distributions are all interdepen-
dent through the (latent) topic distribution θ, and a
topic k is defined by:
</bodyText>
<listItem confidence="0.972731869565217">
7Instead of blog commenters, they modeled citations.
• A multinomial distribution βk over post words;
• A multinomial distribution β&apos;k over comment
words; and
• A multinomial distribution γk over blog com-
menters who might react to posts on the topic.
Formally, LinkLDA and CommentLDA generate
blog data as follows: For each blog post (1 to D):
1. Choose a distribution θ over topics according
to Dirichlet distribution α.
2. For i from 1 to Ni (the length of the post):
(a) Choose a topic zi according to θ.
(b) Choose a word wi according to the topic’s
post word distribution βzi.
3. For j from 1 to Mi (the length of the comments
on the post, in words):
(a) Choose a topic z&apos;j.
(b) Choose an author uj from the topic’s com-
menter distribution γz,
7.
(c) (CommentLDA only) Choose a word w&apos;j
according to the topic’s comment word
distribution β&apos;z, .
</listItem>
<page confidence="0.682914">
7
</page>
<subsectionHeader confidence="0.991894">
4.1 Variations on Counting Users
</subsectionHeader>
<bodyText confidence="0.9999342">
As described, CommentLDA associates each com-
ment word token with an independent author. In
both LinkLDA and CommentLDA, this “counting
by verbosity” will force γ to give higher probabil-
ity to users who write longer comments with more
</bodyText>
<page confidence="0.98914">
480
</page>
<bodyText confidence="0.9693473">
words. We consider two alternative ways to count
comments, applicable to both LinkLDA and Com-
mentLDA. These both involve a change to step 3 in
the generative process.
Counting by response (replaces step 3): For j from
1 to Ui (the number of users who respond to the
post): (a) and (b) as before. (c) (CommentLDA only)
For ` from 1 to `i,j (the number of words in uj’s
comments), choose w0� according to the topic’s com-
ment word distribution β0z,. This model collapses all
</bodyText>
<page confidence="0.804207">
7
</page>
<bodyText confidence="0.993926375">
comments by a user into a single bag of words on a
single topic.8
Counting by comments (replaces step 3): For j
from 1 to Ci (the number of comments on the post):
(a) and (b) as before. (c) (CommentLDA only) For `
from 1 to `i,j (the number of words in comment j),
choose w0� according to the topic’s comment word
distribution β0z,. Intuitively, each comment has a
</bodyText>
<page confidence="0.714219">
7
</page>
<bodyText confidence="0.998617909090909">
topic, a user, and a bag of words.
The three variations—counting users by ver-
bosity, response, or comments—correspond to dif-
ferent ways of thinking about topics in political blog
discourse. Counting by verbosity will let garrulous
users define the topics. Counting by response is
more democratic, letting every user who responds
to a blog post get an equal vote in determining what
the post is about, no matter how much that user says.
Counting by comments gives more say to users who
engage in the conversation repeatedly.
</bodyText>
<subsectionHeader confidence="0.929989">
4.2 Implementation
</subsectionHeader>
<bodyText confidence="0.9990826">
We train our model using empirical Bayesian esti-
mation. Specifically, we fix α = 0.1, and we learn
the values of word distributions β and β0 and user
distribution γ by maximizing the likelihood of the
training data:
</bodyText>
<equation confidence="0.991487">
p(w, w1, u  |α, β, β0, γ) (1)
</equation>
<bodyText confidence="0.955458">
(Obviously, β0 is not present in the LinkLDA mod-
els.) This requires an inference step that marginal-
izes out the latent variables, θ, z, and z0, for which
we use Gibbs sampling as implemented by the Hier-
archical Bayes Compiler (Daum´e, 2007). The Gibbs
</bodyText>
<footnote confidence="0.972997666666667">
8The counting-by-response models are deficient, since they
assume each user will only be chosen once per blog post, though
they permit the same user to be chosen repeatedly.
</footnote>
<bodyText confidence="0.986576333333333">
sampling inference algorithm for LDA was first in-
troduced by Griffiths and Steyvers (2004) and has
since been used widely.
</bodyText>
<sectionHeader confidence="0.994614" genericHeader="method">
5 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.998786">
We adopt a typical NLP “train-and-test” strategy that
learns the model parameters on a training dataset
consisting of a collection of blog posts and their
commenters and comments, then considers an un-
seen test dataset from a later time period. Many
kinds of predictions might be made about the test
set and then evaluated against the true comment re-
sponse. For example, the likelihood of a user to
comment on the post, given knowledge of θ can be
estimated as:9
</bodyText>
<equation confidence="0.999273833333333">
K
p(u  |wN1 , γ, θ) = p(u  |z,γ)p(z  |wN1 , θ)
z=1
K
γz,u &apos; θz (2)
z=1
</equation>
<bodyText confidence="0.999457">
The latter is in a sense a “guessing game,” a pre-
diction on who is going to comment on a new blog
post. A similar task was used by Nallapati and Co-
hen (2008) for assessing the performance of Link-
PLSA-LDA: they predicted the presence or absence
of citation links between documents. We report the
performance on this prediction task using our six
blog topic models (LinkLDA and CommentLDA,
with three counting variations each).
Our aim is to explore and compare the effective-
ness of the different models in discovering topics
that are useful for a practical task. We also give a
qualitative analysis of topics learned.
</bodyText>
<subsectionHeader confidence="0.993321">
5.1 Comment Prediction
</subsectionHeader>
<bodyText confidence="0.9999103">
For each political blog, we trained the three varia-
tions each of LinkLDA and CommentLDA. Model
parameters β, γ, and (in CommentLDA) β0 were
learned by maximizing likelihood, with Gibbs sam-
pling for inference, as described in §4.2. The num-
ber of topics K was fixed at 15.
A simple baseline method makes a post-
independent prediction that ranks users by their
comment frequency. Since blogs often have a “core
constituency” of users who post frequently, this is a
</bodyText>
<footnote confidence="0.911389">
9Another approach would attempt to integrate out θ.
</footnote>
<page confidence="0.993732">
481
</page>
<table confidence="0.99645631372549">
n=5 n=10 n=20 n=30 oracle
AY
Freq. 23.93 18.68 14.20 11.65 13.18
NB 25.13 19.28 14.20 11.63 13.54
Link-v 20.10 14.04 11.17 9.23 11.32
Link-r 26.77 18.63 14.64 12.47 14.03
Link-c 25.13 18.85 14.61 11.91 13.84
Com-v 22.84 17.15 12.75 10.69 12.77
Com-r 27.54 20.54 14.61 12.45 14.35
Com-c 22.40 18.50 14.83 12.56 14.20
Max 94.75 89.89 73.63 58.76 92.60
RWN
Freq. 32.56 30.17 22.61 19.7 27.19
NB 25.63 34.86 27.61 22.03 18.28
Link-v 28.14 21.06 17.34 14.51 19.81
Link-r 32.92 29.29 22.61 18.96 26.32
Link-c 32.56 27.43 21.15 17.43 25.09
Com-v 29.02 24.07 19.07 16.04 22.71
Com-r 36.10 29.64 23.8 19.26 25.97
Com-c 32.03 27.43 19.82 16.25 23.88
Max 90.97 76.46 52.56 37.05 96.16
CB
Freq. 33.38 28.84 24.17 20.99 21.63
NB 36.36 31.15 25.08 21.40 23.22
Link-v 32.06 26.11 19.79 17.43 18.31
Link-r 37.02 31.65 24.62 20.85 22.34
Link-c 36.03 32.06 25.28 21.10 23.44
Com-v 32.39 26.36 20.95 18.26 19.85
Com-r 35.53 29.33 24.33 20.22 22.02
Com-c 33.71 29.25 23.80 19.86 21.68
Max 99.66 98.34 88.88 72.53 95.58
RS
Freq. 25.45 16.75 11.42 9.62 17.15
NB 22.07 16.01 11.60 9.76 16.50
Link-v 14.63 11.9 9.13 7.76 11.38
Link-r 25.19 16.92 12.14 9.82 17.98
Link-c 24.50 16.45 11.49 9.32 16.76
Com-v 14.97 10.51 8.46 7.37 11.3 0
Com-r 15.93 11.42 8.37 6.89 10.97
Com-c 17.57 12.46 8.85 7.34 12.14
Max 80.77 62.98 40.95 29.03 91.86
DK
Freq. 24.66 19.08 15.33 13.34 9.64
NB 35.00 27.33 22.25 19.45 13.97
Link-v 20.58 19.79 15.83 13.88 10.35
Link-r 33.83 27.29 21.39 19.09 13.44
Link-c 28.66 22.16 18.33 16.79 12.60
Com-v 22.16 18.00 16.54 14.45 10.92
Com-r 33.08 25.66 20.66 18.29 12.74
Com-c 26.08 20.91 17.47 15.59 11.82
Max 100.00 100.00 100.00 99.09 98.62
</table>
<tableCaption confidence="0.99949">
Table 2: Comment prediction results on 5 blogs. See text.
</tableCaption>
<bodyText confidence="0.9999366">
strong baseline. We also compared to a Naive Bayes
classifier (with word counts in the post’s main en-
try as features). To perform the prediction task with
our models, we took the following steps. First, we
removed the comment section (both the words and
the authorship information) from the test data set.
Then, we ran a Gibbs sampler with the partial data,
fixing the model parameters to their learned values
and the blog post words to their observed values.
This gives a posterior topic mixture for each post (θ
in the above equations).10 We then computed each
user’s comment prediction score for each post as in
Eq. 2. Users are ordered by their posterior probabil-
ities. Note that these posteriors have different mean-
ings for different variations:
</bodyText>
<listItem confidence="0.9726732">
• When counting by verbosity, the value is the prob-
ability that the next (or any) comment word will
be generated by the user, given the blog post.
• When counting by response, the value is the prob-
ability that the user will respond at all, given the
blog post. (Intuitively, this approach best matches
the task at hand.)
• When counting by comments, the value is the
probability that the next (or any) comment will be
generated by the user, given the blog post.
</listItem>
<bodyText confidence="0.9998882">
We compare our commenter ranking-by-
likelihood with the actual commenters in the test
set. We report in Tab. 2 the precision (macro-
averaged across posts) of our predictions at various
cut-offs (n). The oracle column is the precision
where it is equal to the recall, equivalent to the
situation when the true number of commenters
is known. (The performance of random guessing
is well below 1% for all sites at cut-off points
shown.) “Freq.” and “NB” refer to our baseline
methods. “Link” refers to LinkLDA and “Com” to
CommentLDA. The suffixes denote the counting
methods: verbosity (“-v”), response (“-r”), and
comments (“-c”). Recall that we considered only
the comments by the users seen at least once in the
training set, so perfect precision, as well as recall, is
impossible when new users comment on a post; the
Max row shows the maximum performance possible
given the set of commenters recognizable from the
training data.
</bodyText>
<footnote confidence="0.9271455">
10For a few cases we checked the stability of the sampler and
found results varied by less than 1% precision across ten runs.
</footnote>
<page confidence="0.996899">
482
</page>
<bodyText confidence="0.999990780487805">
Our results suggest that, if asked to guess 5 peo-
ple who would comment on a new post given some
site history, we will get 25–37% of them right, de-
pending on the site, given the content of a new post.
We achieved some improvement over both the
baseline and Naive Bayes for some cut-offs on three
of the five sites, though the gains were very small
for and RS and CB. LinkLDA usually works slightly
better than CommentLDA, except for MY, where
CommentLDA is stronger, and RS, where Com-
mentLDA is extremely poor. Differences in com-
menting style are likely to blame: MY has relatively
long comments in comparison to RS, as well as DK.
MY is the only site where CommentLDA variations
consistently outperformed LinkLDA variations, as
well as Naive Bayes classifiers. This suggests that
sites with more terse comments may be too sparse
to support a rich model like CommentLDA.
In general, counting by response works best,
though counting by comments is a close rival in
some cases. We observe that counting by response
tends to help LinkLDA, which is ignorant of the
word contents of the comment, more than it helps
CommentLDA. Varying the counting method can
bring as much as 10% performance gain.
Each of the models we have tested makes differ-
ent assumptions about the behavior of commenters.
Our results suggest that commenters on different
sites behave differently, so that the same modeling
assumptions cannot be made universally. In future
work, we hope to permit blog-specific properties
to be automatically discovered during learning, so
that, for example, the comment words can be ex-
ploited when they are helpful but assumed indepen-
dent when they are not. Of course, improved per-
formance might also be obtained with more topics,
richer priors over topic distributions, or models that
take into account other cues, such as the time of the
post, pages it links to, etc. It is also possible that bet-
ter performance will come from more sophisticated
supervised models that do not use topics.
</bodyText>
<subsectionHeader confidence="0.999209">
5.2 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.992042367346939">
Aside from prediction tasks such as above, the
model parameters by themselves can be informative.
β defines which words are likely to occur in the post
body for a given topic. β&apos; tells which words are
likely to appear in the collective response to a partic-
ular topic. Similarity or divergence of the two dis-
tributions can tell us about differences in language
used by bloggers and their readers. γ expresses
users’ topic preferences. A pair or group of par-
ticipants may be seen as “like-minded” if they have
similar topic preferences (perhaps useful in collabo-
rative filtering).
Following previous work on LDA and its exten-
sions, we show words most strongly associated with
a few topics, arguing that some coherent clusters
have been discovered. Table 3 shows topics discov-
ered in MY using CommentLDA (counting by com-
ments). This is the blog site where our models most
consistently outperformed the Naive Bayes classi-
fiers and LinkLDA, therefore we believe the model
was a good fit for this dataset.
Since the site is concentrated on American pol-
itics, many of the topics look alike. Table 3 shows
the most probable words in the posts, comments, and
both together for five hand-picked topics that were
relatively transparent. The probabilistic scores of
those words are computed with the scoring method
suggested by Blei and Lafferty (in press).
The model clustered words into topics pertain-
ing to religion and domestic policy (first and last
topics in Table 3) quite reasonably. Some of the
religion-related words make sense in light of cur-
rent affairs.11 Some words in the comment sec-
tion are slightly off-topic from the issue of religion,
such as dawkins12 or wright,13 but are relevant in
the context of real-world events. Notice those words
rank highly only in the comment section, showing
differences between discussion in the post and the
comments. This is also noticeable, for example, in
the “primary” topic (second in Table 3), where the
Republican primary receives more discussion in the
main post, and in the “Iraq war” and “energy” top-
ics, where bloggers discuss strategy and commenters
11Mitt Romney was a candidate for the Republican nomi-
nation in 2008 presidential election. He is a member of The
Church of Jesus Christ of Latter-Day Saints. Another candi-
date, Mike Huckabee, is an ordained Southern Baptist minister.
Moktada al-Sadr is an Iraqi theologian and political activist, and
John Hagee is an influential televangelist.
</bodyText>
<footnote confidence="0.863377">
12Richard Dawkins is a well known evolutionary biologist
who is a vocal critic of intelligent design.
13We believe this is a reference to Rev. Jeremiah Wright of
Trinity United Church of Christ, whose inflammatory rhetoric
was negatively associated with then-candidate Barack Obama.
</footnote>
<page confidence="0.994135">
483
</page>
<bodyText confidence="0.999848815789474">
religion; in both: people, just, american, church, believe, god, black, jesus, mormon, faith, jews, right, say,
in posts: mormons, religious, point
in comments:
romney, huckabee, muslim, political, hagee, cabinet, mitt, consider, true, anti, problem,
course, views, life, real, speech, moral, answer, jobs, difference, muslims, hardly, going,
christianity
religion, think, know, really, christian, obama, white, wright, way, said, good, world, science,
time, dawkins, human, man, things, fact, years, mean, atheists, blacks, christians
primary; in both: obama, clinton, mccain, race, win, iowa, delegates, going, people, state, nomination, primary,
in posts: hillary, election, polls, party, states, voters, campaign, michigan, just
in comments:
huckabee, wins, romney, got, percent, lead, barack, point, majority, ohio, big, victory, strong,
pretty, winning, support, primaries, south, rules
vote, think, superdelegates, democratic, candidate, pledged, delegate, independents, votes,
white, democrats, really, way, caucuses, edwards, florida, supporters, wisconsin, count
Iraq war; in american, iran, just, iraq, people, support, point, country, nuclear, world, power, military,
both: really, government, war, army, right, iraqi, think
in posts:
in comments:
kind, united, forces, international, presence, political, states, foreign, countries, role, need,
making, course, problem, shiite, john, understand, level, idea, security, main
israel, sadr, bush, state, way, oil, years, time, going, good, weapons, saddam, know, maliki,
want, say, policy, fact, said, shia, troops
energy; in both: people, just, tax, carbon, think, high, transit, need, live, going, want, problem, way, market,
in posts: money, income, cost, density
in comments:
idea, public, pretty, course, economic, plan, making, climate, spending, economy, reduce,
change, increase, policy, things, stimulus, cuts, low, financial, housing, bad, real
taxes, fuel, years, time, rail, oil, cars, car, energy, good, really, lot, point, better, prices, pay,
city, know, government, price, work, technology
domestic policy; people, public, health, care, insurance, college, schools, education, higher, children, think,
in both: poor, really, just, kids, want, school, going, better
in posts:
in comments:
different, things, point, fact, social, work, large, article, getting, inequality, matt, simply,
percent, tend, hard, increase, huge, costs, course, policy, happen
students, universal, high, good, way, income, money, government, class, problem, pay, amer-
icans, private, plan, american, country, immigrants, time, know, taxes, cost
</bodyText>
<tableCaption confidence="0.995069">
Table 3: The most probable words for some CommentLDA topics (MY).
</tableCaption>
<bodyText confidence="0.999571461538462">
focus on the tangible (oil, taxes, prices, weapons).
While our topic-modeling approach achieves
mixed results on the prediction task, we believe it
holds promise as a way to understand and summa-
rize the data. Without CommentLDA, we would not
be able to easily see the differences noted above in
blogger and commenter language. In future work,
we plan to explore models with weaker indepen-
dence assumptions among users, among blog posts
over time, and even across blogs. This line of re-
search will permit a more nuanced understanding
of language in the blogosphere and in political dis-
course more generally.
</bodyText>
<sectionHeader confidence="0.999498" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999453636363636">
In this paper we applied several probabilistic topic
models to discourse within political blogs. We in-
troduced a novel comment prediction task to assess
these models in an objective evaluation with possi-
ble practical applications. The results show that pre-
dicting political discourse behavior is challenging,
in part because of considerable variation in user be-
havior across different blog sites. Our results show
that using topic modeling, we can begin to make rea-
sonable predictions as well as qualitative discoveries
about language in blogs.
</bodyText>
<sectionHeader confidence="0.999812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.777281333333333">
This research was supported by a gift from Microsoft
Research and NSF IIS-0836431. The authors appreciate
helpful comments from the anonymous reviewers, Ja-Hui
Chang, Hal Daum´e, and Ramesh Nallapati. We thank
Shay Cohen for his help with inference algorithms and
the members of the ARK group for reviewing this paper.
</reference>
<page confidence="0.999448">
484
</page>
<sectionHeader confidence="0.998299" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999616567901234">
L. Adamic and N. Glance. 2005. The political blogo-
sphere and the 2004 U.S. election: Divided they blog.
In Proceedings of the 2nd Annual Workshop on the We-
blogging Ecosystem: Aggregation, Analysis and Dy-
namics.
D. Blei and M. Jordan. 2003. Modeling annotated data.
In Proceedings of the 26th Annual International ACM
SIGIR Conference on Research and Development in
Informaion Retrieval.
D. Blei and J. Lafferty. In press. Topic models. In A. Sri-
vastava and M. Sahami, editors, Text Mining: Theory
and Applications. Taylor and Franci.
D. Blei and J. McAuliffe. 2008. Supervised topic mod-
els. In Advances in Neural Information Processing
Systems 20.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirich-
let allocation. Journal of Machine Learning Research,
3:993–1022.
S. R. K. Branavan, H. Chen, J. Eisenstein, and R. Barzi-
lay. 2008. Learning document-level semantic prop-
erties from free-text annotations. In Proceedings of
ACL-08: HLT.
D. Cohn and T. Hofmann. 2001. The missing link—a
probabilistic model of document content and hyper-
text connectivity. In Neural Information Processing
Systems 13.
H. Daum´e. 2007. HBC: Hierarchical Bayes compiler.
http://www.cs.utah.edu/∼hal/HBC.
M. Dredze, H. M. Wallach, D. Puller, and F. Pereira.
2008. Generating summary keywords for emails us-
ing topics. In Proceedings of the 13th International
Conference on Intelligent User Interfaces.
E. Erosheva, S. Fienberg, and J. Lafferty. 2004. Mixed
membership models of scientific publications. Pro-
ceedings of the National Academy of Sciences, pages
5220–5227, April.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101 Suppl. 1:5228–5235, April.
W.-H. Lin, E. Xing, and A. Hauptmann. 2008. A joint
topic and perspective model for ideological discourse.
In Proceedings of 2008 European Conference on Ma-
chine Learning and Principles and Practice of Knowl-
edge Discovery in Databases.
R. Malouf and T. Mullen. 2007. Graph-based user clas-
sification for informal online political discourse. In
Proceedings of the 1st Workshop on Information Cred-
ibility on the Web.
A. McCallum. 1999. Multi-label text classification with
a mixture model trained by EM. In AAAI Workshop on
Text Learning.
T. Mullen and R. Malouf. 2006. A preliminary investi-
gation into sentiment analysis of informal political dis-
course. In Proceedings of AAAI-2006 Spring Sympo-
sium on Computational Approaches to Analyzing We-
blogs.
R. Nallapati and W. Cohen. 2008. Link-PLSA-LDA: A
new unsupervised model for topics and influence of
blogs. In Proceedings of the 2nd International Con-
ference on Weblogs and Social Media.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P Smyth.
2004. The author-topic model for authors and docu-
ments. In Proceedings of the 20th Conference on Un-
certainty in Artificial Intelligence.
M. Steyvers and T. Griffiths. 2007. Probabilistic topic
models. In T. Landauer, D. Mcnamara, S. Dennis,
and W. Kintsch, editors, Handbook of Latent Semantic
Analysis. Lawrence Erlbaum Associates.
M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. L. Grif-
fiths. 2004. Probabilistic author-topic models for in-
formation discovery. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining.
I. Titov and R. McDonald. 2008. A joint model of text
and aspect ratings for sentiment summarization. In
Proceedings of ACL-08: HLT.
H. Zhang, B. Qiu, C. L. Giles, H. C. Foley, and J. Yen.
2007. An LDA-based community structure discovery
approach for large-scale social networks. In Proceed-
ings of the IEEE International Conference on Intelli-
gence and Security Informatics.
</reference>
<page confidence="0.999075">
485
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984178">
<title confidence="0.999947">Predicting Response to Political Blog Posts with Topic Models</title>
<author confidence="0.999933">Tae Yano William W Cohen Noah A Smith</author>
<affiliation confidence="0.9999575">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999497">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.998981066666667">In this paper we model discussions in online political blogs. To do this, we extend Latent Dirichlet Allocation (Blei et al., 2003), in various ways to capture different characteristics of the data. Our models jointly describe the generation of the primary documents (posts) as well as the authorship and, optionally, the contents of the blog community’s verbal reactions to each post (comments). We evaluate our model on a novel comment prediction task where the models are used to predict which blog users will leave comments on a given post. We also provide a qualitative discussion about what the models discover.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This research was supported by a gift from Microsoft Research and NSF IIS-0836431. The authors appreciate helpful comments from the anonymous reviewers, Ja-Hui Chang, Hal Daum´e, and Ramesh Nallapati. We thank Shay Cohen for his help with inference algorithms and the members of the ARK group for reviewing this paper.</title>
<marker></marker>
<rawString>This research was supported by a gift from Microsoft Research and NSF IIS-0836431. The authors appreciate helpful comments from the anonymous reviewers, Ja-Hui Chang, Hal Daum´e, and Ramesh Nallapati. We thank Shay Cohen for his help with inference algorithms and the members of the ARK group for reviewing this paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Adamic</author>
<author>N Glance</author>
</authors>
<title>The political blogosphere and the</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd Annual Workshop on the Weblogging Ecosystem: Aggregation, Analysis and Dynamics.</booktitle>
<contexts>
<context position="1347" citStr="Adamic and Glance, 2005" startWordPosition="206" endWordPosition="209">og users will leave comments on a given post. We also provide a qualitative discussion about what the models discover. 1 Introduction Web logging (blogging) and its social impact have recently attracted considerable public and scientific interest. One use of blogs is as a community discussion forum, especially for political discussion and debate. Blogging has arguably opened a new channel for huge numbers of people to express their views with unprecedented speed and to unprecedented audiences. Their collective behavior in the blogosphere has already been noted in the American political arena (Adamic and Glance, 2005). In this paper we attempt to deliver a framework useful for analyzing text in blogs quantitatively as well as qualitatively. Better blog text analysis could lead to better automated recommendation, organization, extraction, and retrieval systems, and might facilitate data-driven research in the social sciences. Apart from the potential social utility of text processing for this domain, we believe blog data is worthy of scientific study in its own right. The spontaneous, reactive, and informal nature of the language in this domain seems to defy conventional analytical approaches in NLP such as</context>
<context position="3834" citStr="Adamic and Glance (2005)" startWordPosition="606" endWordPosition="609"> and studies of social media like political blogs. We then provide a qualitative characterization of political blogs, highlighting some of the features we believe a computational model should capture and discuss our new corpus of political blogs (§3). We present several different candidate topic models that aim to capture these ideas in §4. §5 shows our empirical evaluation on a new comment prediction task and a qualitative analysis of the models learned. 2 Related Work Network analysis, including citation analysis, has been applied to document collections on the Web (Cohn and Hofmann, 2001). Adamic and Glance (2005) applied network analysis to the political bl477 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 477–485, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ogosphere. The study modeled the large, complex structure of the political blogosphere as a network of hyperlinks among the blog sites, demonstrated the viability of link structure for information discovery, though their analysis of text content was less extensive. In contrast, the text seems to be of interest to social scientists studying blogs as an artif</context>
</contexts>
<marker>Adamic, Glance, 2005</marker>
<rawString>L. Adamic and N. Glance. 2005. The political blogosphere and the 2004 U.S. election: Divided they blog. In Proceedings of the 2nd Annual Workshop on the Weblogging Ecosystem: Aggregation, Analysis and Dynamics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>M Jordan</author>
</authors>
<title>Modeling annotated data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval.</booktitle>
<contexts>
<context position="6108" citStr="Blei and Jordan (2003)" startWordPosition="965" endWordPosition="968">r reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collection of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of the earliest results in this trend. The concept was developed into more general framework by Blei and McAuliffe (2008). Steyvers et al. (2004) and Rosen-Zvi et al. (2004) first extended LDA to explicitly model the influence of authorship, applying the model to a collection of academic papers from CiteSeer. The model combined the ideas from the mixture model proposed by McCallum (1999) and LDA. In this model, an abstract notion “author” is associated with a distribution over topics. Another approach to the same document collection based on LDA was used for citation network analysis</context>
</contexts>
<marker>Blei, Jordan, 2003</marker>
<rawString>D. Blei and M. Jordan. 2003. Modeling annotated data. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Blei</author>
<author>J Lafferty</author>
</authors>
<title>In press. Topic models.</title>
<editor>In A. Srivastava and M. Sahami, editors, Text</editor>
<marker>Blei, Lafferty, </marker>
<rawString>D. Blei and J. Lafferty. In press. Topic models. In A. Srivastava and M. Sahami, editors, Text Mining: Theory and Applications. Taylor and Franci.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>J McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 20.</booktitle>
<contexts>
<context position="6239" citStr="Blei and McAuliffe (2008)" startWordPosition="987" endWordPosition="990">008) applied LDA to a collection of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of the earliest results in this trend. The concept was developed into more general framework by Blei and McAuliffe (2008). Steyvers et al. (2004) and Rosen-Zvi et al. (2004) first extended LDA to explicitly model the influence of authorship, applying the model to a collection of academic papers from CiteSeer. The model combined the ideas from the mixture model proposed by McCallum (1999) and LDA. In this model, an abstract notion “author” is associated with a distribution over topics. Another approach to the same document collection based on LDA was used for citation network analysis. Erosheva et al. (2004), following Cohn and Hofmann (2001), defined a generative process not only for each word in the text, but a</context>
</contexts>
<marker>Blei, McAuliffe, 2008</marker>
<rawString>D. Blei and J. McAuliffe. 2008. Supervised topic models. In Advances in Neural Information Processing Systems 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2557" citStr="Blei et al., 2003" startWordPosition="400" endWordPosition="403"> such as supervised text classification (Mullen and Malouf, 2006), yet the data are rich in argumentative, topical, and temporal structure that can perhaps be modeled computationally. We are especially interested in the semi-causal structure of blog discussions, in which a post “spawns” comments (or fails to do so), which meander among topics and asides and show the personality of the participants and the community. Our approach is to develop probabilistic models for the generation of blog posts and comments jointly within a blog site. The model is an extension of Latent Dirichlet Allocation (Blei et al., 2003). Unsupervised topic models can be applied to collections of unannotated documents, requiring very little corpus engineering. They can be easily adapted to new problems by altering the graphical model, then applying standard probabilistic inference algorithms. Different models can be compared to explore the ramifications of different hypotheses about the data. For example, we will explore whether the contents of posts a user has commented on in the past and the words she has used can help predict which posts she will respond to in the future. The paper is organized as follows. In §2 we review </context>
<context position="5062" citStr="Blei et al. (2003)" startWordPosition="795" endWordPosition="798"> political process. Although attempts to quantitatively analyze the contents of political texts have been made, results from classical, supervised text classification experiments are mixed (Mullen and Malouf, 2006; Malouf and Mullen, 2007). Also, a consensus on useful, reliable annotation or categorization schemes for political texts, at any level of granularity, has yet to emerge. Meanwhile, latent topic modeling has become a widely used unsupervised text analysis tool. The basic aim of those models is to discover recurring patterns of “topics” within a text collection. LDA was introduced by Blei et al. (2003) and has been especially popular because it can be understood as a generative model and because it discovers understandable topics in many scenarios (Steyvers and Griffiths, 2007). Its declarative specification makes it easy to extend for new kinds of text collections. The technique has been applied to Web document collections, notably for community discovery in social networks (Zhang et al., 2007), opinion mining in user reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collection of email for su</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>H Chen</author>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Learning document-level semantic properties from free-text annotations.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="5596" citStr="Branavan et al., 2008" startWordPosition="881" endWordPosition="884"> patterns of “topics” within a text collection. LDA was introduced by Blei et al. (2003) and has been especially popular because it can be understood as a generative model and because it discovers understandable topics in many scenarios (Steyvers and Griffiths, 2007). Its declarative specification makes it easy to extend for new kinds of text collections. The technique has been applied to Web document collections, notably for community discovery in social networks (Zhang et al., 2007), opinion mining in user reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collection of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of the earliest results in this trend. The concept was developed into more gen</context>
</contexts>
<marker>Branavan, Chen, Eisenstein, Barzilay, 2008</marker>
<rawString>S. R. K. Branavan, H. Chen, J. Eisenstein, and R. Barzilay. 2008. Learning document-level semantic properties from free-text annotations. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cohn</author>
<author>T Hofmann</author>
</authors>
<title>The missing link—a probabilistic model of document content and hypertext connectivity.</title>
<date>2001</date>
<booktitle>In Neural Information Processing Systems 13.</booktitle>
<contexts>
<context position="3808" citStr="Cohn and Hofmann, 2001" startWordPosition="602" endWordPosition="605"> for document collections and studies of social media like political blogs. We then provide a qualitative characterization of political blogs, highlighting some of the features we believe a computational model should capture and discuss our new corpus of political blogs (§3). We present several different candidate topic models that aim to capture these ideas in §4. §5 shows our empirical evaluation on a new comment prediction task and a qualitative analysis of the models learned. 2 Related Work Network analysis, including citation analysis, has been applied to document collections on the Web (Cohn and Hofmann, 2001). Adamic and Glance (2005) applied network analysis to the political bl477 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 477–485, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ogosphere. The study modeled the large, complex structure of the political blogosphere as a network of hyperlinks among the blog sites, demonstrated the viability of link structure for information discovery, though their analysis of text content was less extensive. In contrast, the text seems to be of interest to social scientists </context>
<context position="6767" citStr="Cohn and Hofmann (2001)" startWordPosition="1076" endWordPosition="1079"> this trend. The concept was developed into more general framework by Blei and McAuliffe (2008). Steyvers et al. (2004) and Rosen-Zvi et al. (2004) first extended LDA to explicitly model the influence of authorship, applying the model to a collection of academic papers from CiteSeer. The model combined the ideas from the mixture model proposed by McCallum (1999) and LDA. In this model, an abstract notion “author” is associated with a distribution over topics. Another approach to the same document collection based on LDA was used for citation network analysis. Erosheva et al. (2004), following Cohn and Hofmann (2001), defined a generative process not only for each word in the text, but also its citation to other documents in the collection, thereby capturing the notion of relations between the document into one generative process. Nallapati and Cohen (2008) introduced the Link-PLSA-LDA model, in which the contents of the citing document and the “influences” on the document (its citations to existing literature), as well as the contents of the cited documents, are modeled together. They further applied the Link-PLSA-LDA model to a blog corpus to analyze its cross citation structure via hyperlinks. In this </context>
</contexts>
<marker>Cohn, Hofmann, 2001</marker>
<rawString>D. Cohn and T. Hofmann. 2001. The missing link—a probabilistic model of document content and hypertext connectivity. In Neural Information Processing Systems 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<date>2007</date>
<note>HBC: Hierarchical Bayes compiler. http://www.cs.utah.edu/∼hal/HBC.</note>
<marker>Daum´e, 2007</marker>
<rawString>H. Daum´e. 2007. HBC: Hierarchical Bayes compiler. http://www.cs.utah.edu/∼hal/HBC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dredze</author>
<author>H M Wallach</author>
<author>D Puller</author>
<author>F Pereira</author>
</authors>
<title>Generating summary keywords for emails using topics.</title>
<date>2008</date>
<booktitle>In Proceedings of the 13th International Conference on Intelligent User Interfaces.</booktitle>
<contexts>
<context position="5618" citStr="Dredze et al. (2008)" startWordPosition="885" endWordPosition="888">thin a text collection. LDA was introduced by Blei et al. (2003) and has been especially popular because it can be understood as a generative model and because it discovers understandable topics in many scenarios (Steyvers and Griffiths, 2007). Its declarative specification makes it easy to extend for new kinds of text collections. The technique has been applied to Web document collections, notably for community discovery in social networks (Zhang et al., 2007), opinion mining in user reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collection of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of the earliest results in this trend. The concept was developed into more general framework by Blei</context>
</contexts>
<marker>Dredze, Wallach, Puller, Pereira, 2008</marker>
<rawString>M. Dredze, H. M. Wallach, D. Puller, and F. Pereira. 2008. Generating summary keywords for emails using topics. In Proceedings of the 13th International Conference on Intelligent User Interfaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Erosheva</author>
<author>S Fienberg</author>
<author>J Lafferty</author>
</authors>
<title>Mixed membership models of scientific publications.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>5220--5227</pages>
<contexts>
<context position="6732" citStr="Erosheva et al. (2004)" startWordPosition="1071" endWordPosition="1074">ere one of the earliest results in this trend. The concept was developed into more general framework by Blei and McAuliffe (2008). Steyvers et al. (2004) and Rosen-Zvi et al. (2004) first extended LDA to explicitly model the influence of authorship, applying the model to a collection of academic papers from CiteSeer. The model combined the ideas from the mixture model proposed by McCallum (1999) and LDA. In this model, an abstract notion “author” is associated with a distribution over topics. Another approach to the same document collection based on LDA was used for citation network analysis. Erosheva et al. (2004), following Cohn and Hofmann (2001), defined a generative process not only for each word in the text, but also its citation to other documents in the collection, thereby capturing the notion of relations between the document into one generative process. Nallapati and Cohen (2008) introduced the Link-PLSA-LDA model, in which the contents of the citing document and the “influences” on the document (its citations to existing literature), as well as the contents of the cited documents, are modeled together. They further applied the Link-PLSA-LDA model to a blog corpus to analyze its cross citation</context>
<context position="12572" citStr="Erosheva et al., 2004" startWordPosition="2027" endWordPosition="2030">collective political orientations vary greatly. Their volumes also vary; multi-author sites (such as DK, RS) may consistently produce over twenty posts per day, while single-author sites (such as MY, CB) may have a day with only one post. Single author sites also tend to have a much smaller vocabulary and range of interests. The sites are also culturally different in commenting styles; some sites are full of short interjections, while others have longer, more analytical comments. On some sites, users appear to be 479 w w′ u � z N α 0 )y 7 z′ M D w � z z′ N M α 0 7 u D Figure 1: Left: LinkLDA (Erosheva et al., 2004), with variables reassigned. Right: CommentLDA. In training, w, u, and (in CommentLDA) w/ are observed. D is the number of blog posts, and N and M are the word counts in the post and the all of its comments, respectively. Here we “count by verbosity.” close-knit, while others have high turnover. In the next section, we describe how we apply topic models to political blogs, and how these probabilistic models can put to use to make predictions. 4 Generative Models The first model we consider is LinkLDA, which is analogous to the model of Erosheva et al. (2004), though the variables are given dif</context>
</contexts>
<marker>Erosheva, Fienberg, Lafferty, 2004</marker>
<rawString>E. Erosheva, S. Fienberg, and J. Lafferty. 2004. Mixed membership models of scientific publications. Proceedings of the National Academy of Sciences, pages 5220–5227, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences, 101 Suppl.</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="17530" citStr="Griffiths and Steyvers (2004)" startWordPosition="2897" endWordPosition="2900">β and β0 and user distribution γ by maximizing the likelihood of the training data: p(w, w1, u |α, β, β0, γ) (1) (Obviously, β0 is not present in the LinkLDA models.) This requires an inference step that marginalizes out the latent variables, θ, z, and z0, for which we use Gibbs sampling as implemented by the Hierarchical Bayes Compiler (Daum´e, 2007). The Gibbs 8The counting-by-response models are deficient, since they assume each user will only be chosen once per blog post, though they permit the same user to be chosen repeatedly. sampling inference algorithm for LDA was first introduced by Griffiths and Steyvers (2004) and has since been used widely. 5 Empirical Evaluation We adopt a typical NLP “train-and-test” strategy that learns the model parameters on a training dataset consisting of a collection of blog posts and their commenters and comments, then considers an unseen test dataset from a later time period. Many kinds of predictions might be made about the test set and then evaluated against the true comment response. For example, the likelihood of a user to comment on the post, given knowledge of θ can be estimated as:9 K p(u |wN1 , γ, θ) = p(u |z,γ)p(z |wN1 , θ) z=1 K γz,u &apos; θz (2) z=1 The latter is </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101 Suppl. 1:5228–5235, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W-H Lin</author>
<author>E Xing</author>
<author>A Hauptmann</author>
</authors>
<title>A joint topic and perspective model for ideological discourse.</title>
<date>2008</date>
<booktitle>In Proceedings of 2008 European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases.</booktitle>
<contexts>
<context position="5842" citStr="Lin et al. (2008)" startWordPosition="923" endWordPosition="926">Griffiths, 2007). Its declarative specification makes it easy to extend for new kinds of text collections. The technique has been applied to Web document collections, notably for community discovery in social networks (Zhang et al., 2007), opinion mining in user reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collection of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of the earliest results in this trend. The concept was developed into more general framework by Blei and McAuliffe (2008). Steyvers et al. (2004) and Rosen-Zvi et al. (2004) first extended LDA to explicitly model the influence of authorship, applying the model to a collection of academic papers from CiteSeer. The model com</context>
</contexts>
<marker>Lin, Xing, Hauptmann, 2008</marker>
<rawString>W.-H. Lin, E. Xing, and A. Hauptmann. 2008. A joint topic and perspective model for ideological discourse. In Proceedings of 2008 European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
<author>T Mullen</author>
</authors>
<title>Graph-based user classification for informal online political discourse.</title>
<date>2007</date>
<booktitle>In Proceedings of the 1st Workshop on Information Credibility on the Web.</booktitle>
<contexts>
<context position="4683" citStr="Malouf and Mullen, 2007" startWordPosition="732" endWordPosition="735">nal Linguistics ogosphere. The study modeled the large, complex structure of the political blogosphere as a network of hyperlinks among the blog sites, demonstrated the viability of link structure for information discovery, though their analysis of text content was less extensive. In contrast, the text seems to be of interest to social scientists studying blogs as an artifact of the political process. Although attempts to quantitatively analyze the contents of political texts have been made, results from classical, supervised text classification experiments are mixed (Mullen and Malouf, 2006; Malouf and Mullen, 2007). Also, a consensus on useful, reliable annotation or categorization schemes for political texts, at any level of granularity, has yet to emerge. Meanwhile, latent topic modeling has become a widely used unsupervised text analysis tool. The basic aim of those models is to discover recurring patterns of “topics” within a text collection. LDA was introduced by Blei et al. (2003) and has been especially popular because it can be understood as a generative model and because it discovers understandable topics in many scenarios (Steyvers and Griffiths, 2007). Its declarative specification makes it e</context>
</contexts>
<marker>Malouf, Mullen, 2007</marker>
<rawString>R. Malouf and T. Mullen. 2007. Graph-based user classification for informal online political discourse. In Proceedings of the 1st Workshop on Information Credibility on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>Multi-label text classification with a mixture model trained by EM.</title>
<date>1999</date>
<booktitle>In AAAI Workshop on Text Learning.</booktitle>
<contexts>
<context position="6508" citStr="McCallum (1999)" startWordPosition="1035" endWordPosition="1037">e. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of the earliest results in this trend. The concept was developed into more general framework by Blei and McAuliffe (2008). Steyvers et al. (2004) and Rosen-Zvi et al. (2004) first extended LDA to explicitly model the influence of authorship, applying the model to a collection of academic papers from CiteSeer. The model combined the ideas from the mixture model proposed by McCallum (1999) and LDA. In this model, an abstract notion “author” is associated with a distribution over topics. Another approach to the same document collection based on LDA was used for citation network analysis. Erosheva et al. (2004), following Cohn and Hofmann (2001), defined a generative process not only for each word in the text, but also its citation to other documents in the collection, thereby capturing the notion of relations between the document into one generative process. Nallapati and Cohen (2008) introduced the Link-PLSA-LDA model, in which the contents of the citing document and the “influ</context>
</contexts>
<marker>McCallum, 1999</marker>
<rawString>A. McCallum. 1999. Multi-label text classification with a mixture model trained by EM. In AAAI Workshop on Text Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mullen</author>
<author>R Malouf</author>
</authors>
<title>A preliminary investigation into sentiment analysis of informal political discourse.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI-2006 Spring Symposium on Computational Approaches to Analyzing Weblogs.</booktitle>
<contexts>
<context position="2004" citStr="Mullen and Malouf, 2006" startWordPosition="309" endWordPosition="312">iver a framework useful for analyzing text in blogs quantitatively as well as qualitatively. Better blog text analysis could lead to better automated recommendation, organization, extraction, and retrieval systems, and might facilitate data-driven research in the social sciences. Apart from the potential social utility of text processing for this domain, we believe blog data is worthy of scientific study in its own right. The spontaneous, reactive, and informal nature of the language in this domain seems to defy conventional analytical approaches in NLP such as supervised text classification (Mullen and Malouf, 2006), yet the data are rich in argumentative, topical, and temporal structure that can perhaps be modeled computationally. We are especially interested in the semi-causal structure of blog discussions, in which a post “spawns” comments (or fails to do so), which meander among topics and asides and show the personality of the participants and the community. Our approach is to develop probabilistic models for the generation of blog posts and comments jointly within a blog site. The model is an extension of Latent Dirichlet Allocation (Blei et al., 2003). Unsupervised topic models can be applied to c</context>
<context position="4657" citStr="Mullen and Malouf, 2006" startWordPosition="728" endWordPosition="731">ssociation for Computational Linguistics ogosphere. The study modeled the large, complex structure of the political blogosphere as a network of hyperlinks among the blog sites, demonstrated the viability of link structure for information discovery, though their analysis of text content was less extensive. In contrast, the text seems to be of interest to social scientists studying blogs as an artifact of the political process. Although attempts to quantitatively analyze the contents of political texts have been made, results from classical, supervised text classification experiments are mixed (Mullen and Malouf, 2006; Malouf and Mullen, 2007). Also, a consensus on useful, reliable annotation or categorization schemes for political texts, at any level of granularity, has yet to emerge. Meanwhile, latent topic modeling has become a widely used unsupervised text analysis tool. The basic aim of those models is to discover recurring patterns of “topics” within a text collection. LDA was introduced by Blei et al. (2003) and has been especially popular because it can be understood as a generative model and because it discovers understandable topics in many scenarios (Steyvers and Griffiths, 2007). Its declarativ</context>
</contexts>
<marker>Mullen, Malouf, 2006</marker>
<rawString>T. Mullen and R. Malouf. 2006. A preliminary investigation into sentiment analysis of informal political discourse. In Proceedings of AAAI-2006 Spring Symposium on Computational Approaches to Analyzing Weblogs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nallapati</author>
<author>W Cohen</author>
</authors>
<title>Link-PLSA-LDA: A new unsupervised model for topics and influence of blogs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd International Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="7012" citStr="Nallapati and Cohen (2008)" startWordPosition="1116" endWordPosition="1119">llection of academic papers from CiteSeer. The model combined the ideas from the mixture model proposed by McCallum (1999) and LDA. In this model, an abstract notion “author” is associated with a distribution over topics. Another approach to the same document collection based on LDA was used for citation network analysis. Erosheva et al. (2004), following Cohn and Hofmann (2001), defined a generative process not only for each word in the text, but also its citation to other documents in the collection, thereby capturing the notion of relations between the document into one generative process. Nallapati and Cohen (2008) introduced the Link-PLSA-LDA model, in which the contents of the citing document and the “influences” on the document (its citations to existing literature), as well as the contents of the cited documents, are modeled together. They further applied the Link-PLSA-LDA model to a blog corpus to analyze its cross citation structure via hyperlinks. In this work, we aim to model the data within blog conversations, focusing on comments left by a blog community in response to a blogger’s post. 3 Political Blog Data We discuss next the dataset used in our experiments. 3.1 Corpus We have collected blog</context>
<context position="18273" citStr="Nallapati and Cohen (2008)" startWordPosition="3038" endWordPosition="3042">the model parameters on a training dataset consisting of a collection of blog posts and their commenters and comments, then considers an unseen test dataset from a later time period. Many kinds of predictions might be made about the test set and then evaluated against the true comment response. For example, the likelihood of a user to comment on the post, given knowledge of θ can be estimated as:9 K p(u |wN1 , γ, θ) = p(u |z,γ)p(z |wN1 , θ) z=1 K γz,u &apos; θz (2) z=1 The latter is in a sense a “guessing game,” a prediction on who is going to comment on a new blog post. A similar task was used by Nallapati and Cohen (2008) for assessing the performance of LinkPLSA-LDA: they predicted the presence or absence of citation links between documents. We report the performance on this prediction task using our six blog topic models (LinkLDA and CommentLDA, with three counting variations each). Our aim is to explore and compare the effectiveness of the different models in discovering topics that are useful for a practical task. We also give a qualitative analysis of topics learned. 5.1 Comment Prediction For each political blog, we trained the three variations each of LinkLDA and CommentLDA. Model parameters β, γ, and (</context>
</contexts>
<marker>Nallapati, Cohen, 2008</marker>
<rawString>R. Nallapati and W. Cohen. 2008. Link-PLSA-LDA: A new unsupervised model for topics and influence of blogs. In Proceedings of the 2nd International Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rosen-Zvi</author>
<author>T Griffiths</author>
<author>M Steyvers</author>
<author>P Smyth</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="6291" citStr="Rosen-Zvi et al. (2004)" startWordPosition="996" endWordPosition="999">keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of the earliest results in this trend. The concept was developed into more general framework by Blei and McAuliffe (2008). Steyvers et al. (2004) and Rosen-Zvi et al. (2004) first extended LDA to explicitly model the influence of authorship, applying the model to a collection of academic papers from CiteSeer. The model combined the ideas from the mixture model proposed by McCallum (1999) and LDA. In this model, an abstract notion “author” is associated with a distribution over topics. Another approach to the same document collection based on LDA was used for citation network analysis. Erosheva et al. (2004), following Cohn and Hofmann (2001), defined a generative process not only for each word in the text, but also its citation to other documents in the collectio</context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P Smyth. 2004. The author-topic model for authors and documents. In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steyvers</author>
<author>T Griffiths</author>
</authors>
<title>Probabilistic topic models.</title>
<date>2007</date>
<editor>In T. Landauer, D. Mcnamara, S. Dennis, and W. Kintsch, editors,</editor>
<contexts>
<context position="5241" citStr="Steyvers and Griffiths, 2007" startWordPosition="825" endWordPosition="829">xperiments are mixed (Mullen and Malouf, 2006; Malouf and Mullen, 2007). Also, a consensus on useful, reliable annotation or categorization schemes for political texts, at any level of granularity, has yet to emerge. Meanwhile, latent topic modeling has become a widely used unsupervised text analysis tool. The basic aim of those models is to discover recurring patterns of “topics” within a text collection. LDA was introduced by Blei et al. (2003) and has been especially popular because it can be understood as a generative model and because it discovers understandable topics in many scenarios (Steyvers and Griffiths, 2007). Its declarative specification makes it easy to extend for new kinds of text collections. The technique has been applied to Web document collections, notably for community discovery in social networks (Zhang et al., 2007), opinion mining in user reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collection of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>M. Steyvers and T. Griffiths. 2007. Probabilistic topic models. In T. Landauer, D. Mcnamara, S. Dennis, and W. Kintsch, editors, Handbook of Latent Semantic Analysis. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steyvers</author>
<author>P Smyth</author>
<author>M Rosen-Zvi</author>
<author>T L Griffiths</author>
</authors>
<title>Probabilistic author-topic models for information discovery.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining.</booktitle>
<contexts>
<context position="6263" citStr="Steyvers et al. (2004)" startWordPosition="991" endWordPosition="994">ction of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of the earliest results in this trend. The concept was developed into more general framework by Blei and McAuliffe (2008). Steyvers et al. (2004) and Rosen-Zvi et al. (2004) first extended LDA to explicitly model the influence of authorship, applying the model to a collection of academic papers from CiteSeer. The model combined the ideas from the mixture model proposed by McCallum (1999) and LDA. In this model, an abstract notion “author” is associated with a distribution over topics. Another approach to the same document collection based on LDA was used for citation network analysis. Erosheva et al. (2004), following Cohn and Hofmann (2001), defined a generative process not only for each word in the text, but also its citation to othe</context>
</contexts>
<marker>Steyvers, Smyth, Rosen-Zvi, Griffiths, 2004</marker>
<rawString>M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. L. Griffiths. 2004. Probabilistic author-topic models for information discovery. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>R McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="5522" citStr="Titov and McDonald, 2008" startWordPosition="871" endWordPosition="874">ed text analysis tool. The basic aim of those models is to discover recurring patterns of “topics” within a text collection. LDA was introduced by Blei et al. (2003) and has been especially popular because it can be understood as a generative model and because it discovers understandable topics in many scenarios (Steyvers and Griffiths, 2007). Its declarative specification makes it easy to extend for new kinds of text collections. The technique has been applied to Web document collections, notably for community discovery in social networks (Zhang et al., 2007), opinion mining in user reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collection of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of t</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>I. Titov and R. McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>B Qiu</author>
<author>C L Giles</author>
<author>H C Foley</author>
<author>J Yen</author>
</authors>
<title>An LDA-based community structure discovery approach for large-scale social networks.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Conference on Intelligence and Security Informatics.</booktitle>
<contexts>
<context position="5463" citStr="Zhang et al., 2007" startWordPosition="862" endWordPosition="865">nt topic modeling has become a widely used unsupervised text analysis tool. The basic aim of those models is to discover recurring patterns of “topics” within a text collection. LDA was introduced by Blei et al. (2003) and has been especially popular because it can be understood as a generative model and because it discovers understandable topics in many scenarios (Steyvers and Griffiths, 2007). Its declarative specification makes it easy to extend for new kinds of text collections. The technique has been applied to Web document collections, notably for community discovery in social networks (Zhang et al., 2007), opinion mining in user reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collection of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially</context>
</contexts>
<marker>Zhang, Qiu, Giles, Foley, Yen, 2007</marker>
<rawString>H. Zhang, B. Qiu, C. L. Giles, H. C. Foley, and J. Yen. 2007. An LDA-based community structure discovery approach for large-scale social networks. In Proceedings of the IEEE International Conference on Intelligence and Security Informatics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>