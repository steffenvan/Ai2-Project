<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000024">
<title confidence="0.998458">
A Part of Speech Estimation Method for Japanese Unknown
Words using a Statistical Model of Morphology and Context
</title>
<author confidence="0.675065">
Masaaki NAGATA
</author>
<affiliation confidence="0.487932">
NTT Cyber Space Laboratories
</affiliation>
<address confidence="0.886987">
1-1 Hikari-no-oka Yokosuka-Shi Kanagawa, 239-0847 Japan
</address>
<email confidence="0.825958">
nagatanttn1y.isl.ntt.co.jp
</email>
<sectionHeader confidence="0.903083" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999791846153846">
We present a statistical model of Japanese unknown
words consisting of a set of length and spelling
models classified by the character types that con-
stitute a word. The point is quite simple: differ-
ent character sets should be treated differently and
the changes between character types are very im-
portant because Japanese script has both ideograms
like Chinese (kanji) and phonograms like English
(katakana). Both word segmentation accuracy and
part of speech tagging accuracy are improved by the
proposed model. The model can achieve 96.6% tag-
ging accuracy if unknown words are correctly seg-
mented.
</bodyText>
<sectionHeader confidence="0.990192" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995172972973">
In Japanese, around 95% word segmentation ac-
curacy is reported by using a word-based lan-
guage model and the Viterbi-like dynamic program-
ming procedures (Nagata, 1994; Yamamoto, 1996;
Takeuchi and Matsumoto, 1997; Haruno and Mat-
sumoto, 1997). About the same accuracy is reported
in Chinese by statistical methods (Sproat et al.,
1996). But there has been relatively little improve-
ment in recent years because most of the remaining
errors are due to unknown words.
There are two approaches to solve this problem:
to increase the coverage of the dictionary (Fung and
Wu, 1994; Chang et al., 1995; Mori and Naga.o,
1996) and to design a better model for unknown
words (Nagata, 1996; Sproat et al., 1996). We take
the latter approach. To improve word segmenta-
tion accuracy, (Nagata, 1996) used a single general
purpose unknown word model, while (Sproat et al.,
1996) used a set of specific word models such as for
plurals, personal names, and transliterated foreign
words.
The goal of our research is to assign a correct part
of speech to unknown word as well as identifying it
correctly. In this paper, we present a novel statistical
model for Japanese unknown words. It consists of
a set of word models for each part of speech and
word type. We classified Japanese words into nine
orthographic types based on the character types that
constitute a word. We find that by making different
models for each word type, we can better model the
length and spelling of unknown words.
In the following sections, we first describe the lan-
guage model used for Japanese word segmentation.
We then describe a series of unknown word mod-
els, from the baseline model to the one we propose.
Finally, we prove the effectiveness of the proposed
model by experiment.
</bodyText>
<sectionHeader confidence="0.926295" genericHeader="method">
2 Word Segmentation Model
</sectionHeader>
<subsectionHeader confidence="0.7481045">
2.1 Baseline Language Model and Search
Algorithm
</subsectionHeader>
<bodyText confidence="0.99246375">
Let the input Japanese character sequence be C =
cm, and segment it into word sequence W =
wi wn 1 . The word segmentation task can be de-
fined as finding the word segmentation W that max-
imize the joint probability of word sequence given
character sequence P(WIC). Since the maximiza-
tion is carried out with fixed character sequence C,
the word segmenter only has to maximize the joint
probability of word sequence P(W).
=-- arg mmax P(WIC) = arg P(W) (1)
We call P(W) the segmentation model. We can
use any type of word-based language model for
P(W), such as word ngram and class-based ngram.
We used the word bigram model in this paper. So,
P(W) is approximated by the product of word bi-
gram probabilities P(wilwi—i)•
</bodyText>
<equation confidence="0.9937555">
P(W)
P(wil&lt;bos&gt;)mi=2p(wilwi_op(&lt;eos&gt;iwn) (2)
</equation>
<bodyText confidence="0.9955446">
Here, the special symbols &lt;bos&gt; and &lt;eos&gt; indi-
cate the beginning and the end of a sentence, re-
spectively.
Basically, word bigram probabilities of the word
segmentation model is estimated by computing the
</bodyText>
<footnote confidence="0.6175075">
1 In this paper, we define a word as a combination of its
surface form and part of speech. Two words are considered
to be equal only if they have the same surface form and part
of speech.
</footnote>
<page confidence="0.994524">
277
</page>
<tableCaption confidence="0.825712">
Table 1: Examples of word bigrams including un-
known word tags
</tableCaption>
<bodyText confidence="0.9989695">
relative frequencies of the corresponding events in
the word segmented training corpus, with appropri-
ate smoothing techniques. The maximization search
can be efficiently implemented by using the Viterbi-
like dynamic programming procedure described in
(Nagata, 1994).
</bodyText>
<subsectionHeader confidence="0.9283855">
2.2 Modification to Handle Unknown
Words
</subsectionHeader>
<bodyText confidence="0.974990416666667">
To handle unknown words, we made a slight modi-
fication in the above word segmentation model. We
have introduced unknown word tags &lt;U-t&gt; for each
part of speech t. For example, &lt;U-noun&gt; and &lt;U-
verb&gt; represents an unknown noun and an unknown
verb, respectively.
If wi is an unknown word whose part of speech
is t, the word bigram probability P(wi 1 wi_i) is ap-
proximated as the product of word bigram probabil-
ity P(&lt;U-t&gt;lwi_i) and the probability of w, given
it is an unknown word whose part of speech is t,
P(wil&lt;U-t&gt;).
</bodyText>
<equation confidence="0.9999085">
P(wilwi_i) =
P(&lt;U-t&gt;lwi_i)P(wil&lt;U-t&gt;) (3)
</equation>
<bodyText confidence="0.995490642857143">
Here, we made an assumption that the spelling
of an unknown word solely depends on its part of
speech and is independent of the previous word.
This is the same assumption made in the hidden
Markov model, which is called output independence.
The probabilities P(&lt;U-t&gt;lwi_i) can be esti-
mated from the relative frequencies in the training
corpus whose infrequent words are replaced with
their corresponding unknown word tags based on
their part of speeches 2 .
Table 1 shows examples of word bigrams including
unknown word tags. Here, a word is represented by
a list of surface form, pronunciation, and part of
speech, which are delimited by a slash &apos;/&apos;. The first
</bodyText>
<footnote confidence="0.938462166666667">
2 Throughout in this paper, we use the term &amp;quot;infrequent
words&amp;quot; to represent words that appeared only once in the
corpus. They are also called &amp;quot;hapax legomena&amp;quot; or &amp;quot;hapax
words&amp;quot;. It is well known that the characteristics of hapax
legomena are similar to those of unknown words (Baayen and
Sproat, 1996).
</footnote>
<bodyText confidence="0.992549625">
example &amp;quot;0/no/particle &lt;U-noun&gt;&amp;quot; will appear in
the most frequent form of Japanese noun phrases &amp;quot;A
0) B&amp;quot;, which corresponds to &amp;quot;B of A&amp;quot; in English.
As Table 1 shows, word bigrams whose infrequent
words are replaced with their corresponding part of
speech-based unknown word tags are very important
information source of the contexts where unknown
words appears.
</bodyText>
<sectionHeader confidence="0.99279" genericHeader="method">
3 Unknown Word Model
</sectionHeader>
<subsectionHeader confidence="0.976528">
3.1 Baseline Model
</subsectionHeader>
<bodyText confidence="0.998367444444444">
The simplest unknown word model depends only on
the spelling. We think of an unknown word as a word
having a special part of speech &lt;UNK&gt;. Then, the
unknown word model is formally defined as the joint
probability of the character sequence wi = c1 . • • ck
if it is an unknown word. Without loss of generality,
we decompose it into the product of word length
probability and word spelling probability given its
length,
</bodyText>
<equation confidence="0.999947">
P(wil&lt;UNK&gt;) = P(ci ...ck 1&lt;UNK).) =
P(kl&lt;UNK&gt;)P(ci cklk, &lt;UNK&gt;) (4)
</equation>
<bodyText confidence="0.999630636363636">
where k is the length of the character sequence.
We call P(kl&lt;UNK&gt;) the word length model, and
P(ci ck1k, &lt;UNK&gt;) the word spelling model.
In order to estimate the entropy of English,
(Brown et al., 1992) approximated P(kl&lt;UNK&gt;)
by a Poisson distribution whose parameter is the
average word length A in the training corpus, and
P(ci . ck lk, &lt;UNK&gt;) by the product of character
zerogram probabilities. This means all characters in
the character set are considered to be selected inde-
pendently and uniformly.
</bodyText>
<equation confidence="0.9472455">
Ak
P(ci ckl&lt;UNK&gt;) ve plc
</equation>
<bodyText confidence="0.999834375">
where p is the inverse of the number of characters in
the character set. If we assume JIS-X-0208 is used
as the Japanese character set, p = 1/6879.
Since the Poisson distribution is a single parame-
ter distribution with lower bound, it is appropriate
to use it as a first order approximation to the word
length distribution. But the Brown model has two
problems. It assigns a certain amount of probability
mass to zero-length words, and it is too simple to
express morphology.
For Japanese word segmentation and OCR error
correction, (Nagata, 1996) proposed a modified ver-
sion of the Brown model. Nagata also assumed the
word length probability obeys the Poisson distribu-
tion. But he moved the lower bound from zero to
one.
</bodyText>
<equation confidence="0.969982">
(A — 1)k-1
P(kl&lt;UNK&gt;) (6)
(k —1)!
</equation>
<figure confidence="0.916903266666667">
word bigram
0)/no/particle &lt;U-noun&gt;
&lt;U-verb&gt; 1—/shi/inflection
&lt;U-number&gt; F9/yen/suffix
&lt;U-adjectival-verb&gt; /X/na/inilection
&lt;U-adjective&gt; i yi/inflection
&lt;U-adverb&gt; /to/particle
frequency
6783
1052
407
405
182
139
(5)
</figure>
<page confidence="0.995109">
278
</page>
<bodyText confidence="0.999686">
Instead of zerogram, He approximated the word
spelling probability P(ci ck jk, &lt;UNK&gt;) by the
product of word-based character bigram probabili-
ties, regardless of word length.
</bodyText>
<equation confidence="0.9989845">
P(ci cklk, &lt;UNK&gt;)
P(ci I &lt;bow&gt; ) IIi=2 P(ci )P(&lt;eow&gt; I ck ) (7)
</equation>
<bodyText confidence="0.9998365">
where &lt;bow&gt; and &lt;eow&gt; are special symbols that
indicate the beginning and the end of a word.
</bodyText>
<subsectionHeader confidence="0.998822">
3.2 Correction of Word Spelling
Probabilities
</subsectionHeader>
<bodyText confidence="0.9986518">
We find that Equation (7) assigns too little proba-
bilities to long words (5 or more characters). This is
because the lefthand side of Equation (7) represents
the probability of the string ck in the set of all
strings whose length are k, while the righthand side
represents the probability of the string in the set of
all possible strings (from length zero to infinity).
Let Pk(Ci • Ckl &lt;UNK&gt;) be the probability of
character string c1 ck estimated from the char-
acter bigram model.
</bodyText>
<equation confidence="0.999635">
Pb(ci ...ckj&lt;UNK&gt;) =
P(cil&lt;bow&gt;) fl2 P(cilcz_i)P(&lt;eow&gt;lck) (8)
</equation>
<bodyText confidence="0.96614925">
Let Pk (kI&lt;UNK&gt;) be the sum of the probabilities
of all strings which are generated by the character
bigram model and whose length are k. More appro-
priate estimate for P(ci ck lk, &lt;UNK&gt;) is,
</bodyText>
<equation confidence="0.961457666666667">
P(ci ck lk, &lt;UNK&gt;) Pb(ci ...ck I &lt;UNK&gt;)
Pb(kl&lt;UNK&gt;)
(9)
</equation>
<bodyText confidence="0.999776125">
But how can we estimate Pk (kl&lt;UNK&gt;)? It is
difficult to compute it directly, but we can get a rea-
sonable estimate by considering the unigram case.
If strings are generated by the character unigram
model, the sum of the probabilities of all length k
strings equals to the probability of the event that
the end of word symbol &lt;eow&gt; is selected after a
character other than &lt;eow&gt; is selected k — 1 times.
</bodyText>
<equation confidence="0.996517">
Pb(kl&lt;UNK&gt;) (1 — .P(&lt;eow&gt;))&amp;quot;-P(&lt;eow&gt;) (10)
</equation>
<bodyText confidence="0.9995665">
Throughout in this paper, we used Equation (9)
to compute the word spelling probabilities.
</bodyText>
<subsectionHeader confidence="0.999224">
3.3 Japanese Orthography and Word
Length Distribution
</subsectionHeader>
<bodyText confidence="0.999969714285714">
In word segmentation, one of the major problems of
the word length model of Equation (6) is the decom-
position of unknown words. When a substring of an
unknown word coincides with other word in the dic-
tionary, it is very likely to be decomposed into the
dictionary word and the remaining substring. We
find the reason of the decomposition is that the word
</bodyText>
<figure confidence="0.997436666666667">
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
2 4 6
Word Character Length
</figure>
<figureCaption confidence="0.965559">
Figure 2: Word length distribution of kanji words
and katakana words
</figureCaption>
<bodyText confidence="0.99781825">
length model does not reflect the variation of the
word length distribution resulting from the Japanese
orthography.
Figure 1 shows the word length distribution of in-
frequent words in the EDR corpus, and the estimate
of word length distribution by Equation (6) whose
parameter (A = 4.8) is the average word length of
infrequent words. The empirical and the estimated
distributions agree fairly well. But the estimates
by Poisson are smaller than empirical probabilities
for shorter words (&lt;= 4 characters), and larger for
longer words (&gt; characters). This is because we rep-
</bodyText>
<figure confidence="0.9936384375">
8
10
2 4 6
Word Character Length
Word Length Distribution
0.5
Probs from Raw Counts (hapax words) -.—
Estimates by Poisson (hapax words)
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
</figure>
<figureCaption confidence="0.9933515">
Figure 1: Word length distribution of unknown
words and its estimate by Poisson distribution
</figureCaption>
<figure confidence="0.991065">
0.45
Unknown Word Length Distribution
Kenji -e—
katakana
10
</figure>
<page confidence="0.981263">
279
</page>
<tableCaption confidence="0.9976375">
Table 3: Examples of common character bigrams for
Table 2: Character type configuration of infrequent
</tableCaption>
<figure confidence="0.977323625">
words in the EDR corpus each part of speech in the infrequent words
character type sequence
kanji
katakana
katakana-kanji
kanji-hiragana
hiragana
kanji-katakana
katakana-symbol-katakana
number
kanji-hiragana-kanji
alphabet
kanji-hiragana-kanji-hiragana
hiragana-kanji
percent
45.1%
11.4%
6.5%
5.6%
3.7%
3.4%
3.0%
2.6%
2.4%
2.0%
1.7%
1.3%
examples
KAM
007 0 7
Ws i7.., R 9 Pi
VSOP
</figure>
<figureCaption confidence="0.589252222222222">
resented all unknown words by one length model.
Figure 2 shows the word length distribution of
words consists of only kanji characters and words
consists of only katakana characters. It shows that
the length of kanji words distributes around 3 char-
acters, while that of katakana words distributes
around 5 characters. The empirical word length dis-
tribution of Figure 1 is, in fact, a weighted sum of
these two distributions.
</figureCaption>
<bodyText confidence="0.999946357142857">
In the Japanese writing system, there are at least
five different types of characters other than punc-
tuation marks: kanji, hiragana, katakana, Roman
alphabet, and Arabic numeral. Kanji which means
&apos;Chinese character&apos; is used for both Chinese origin
words and Japanese words semantically equivalent
to Chinese characters. Hiragana and katakana are
syllabaries: The former is used primarily for gram-
matical function words, such as particles and inflec-
tional endings, while the latter is used primarily to
transliterate Western origin words. Roman alphabet
is also used for Western origin words and acronyms.
Arabic numeral is used for numbers.
Most Japanese words are written in kanji, while
more recent loan words are written in katakana.
Katakana words are likely to be used for techni-
cal terms, especially in relatively new fields like
computer science. Kanji words are shorter than
katakana words because kanji is based on a large
(&gt; 6,000) alphabet of ideograms while katakana is
based on a small (&lt; 100) alphabet of phonograms.
Table 2 shows the distribution of character type
sequences that constitute the infrequent words in
the EDR corpus. It shows approximately 65% of
words are constituted by a single character type.
Among the words that are constituted by more than
two character types, only the kanji-hiragana and
hiragana-kanji sequences are morphemes and others
are compound words in a strict sense although they
are identified as words in the EDR corpus 3
Therefore, we classified Japanese words into 9
word types based on the character types that consti-
tute a word: &lt;sym&gt;, &lt;num&gt;, &lt;alpha&gt;, &lt;hira&gt;,
&lt;lcata&gt;, and &lt;Ican&gt; represent a sequence of sym-
bols, numbers, alphabets, hiraganas, katakanas, and
kanjis, respectively. &lt;Ican-hira&gt; and &lt;hira-lcan&gt;
represent a sequence of kanjis followed by hiraganas
and that of hiraganas followed by kanjis, respec-
tively. The rest are classified as &lt;misc&gt;.
The resulting unknown word model is as follows.
We first select the word type, then we select the
length and spelling.
</bodyText>
<equation confidence="0.999032333333333">
P(ci ck I &lt;UNK&gt;) =
P(&lt;WT&gt;I&lt;UNK&gt;)P(ki&lt;WT&gt;, &lt;UNK&gt;)
P(ci ...cklk, &lt;WT&gt;, &lt;UNK&gt;) (11)
</equation>
<subsectionHeader confidence="0.996856">
3.4 Part of Speech and Word Morphology
</subsectionHeader>
<bodyText confidence="0.99988375">
It is obvious that the beginnings and endings of
words play an important role in tagging part of
speech. Table 3 shows examples of common char-
acter bigrams for each part of speech in the infre-
quent words of the EDR corpus. The first example
in Table 3 shows that words ending in &apos; —&apos; are likely
to be nouns. This symbol typically appears at the
end of transliterated Western origin words written
in katakana.
It is natural to make a model for each part of
speech. The resulting unknown word model is as
follows.
</bodyText>
<equation confidence="0.9992785">
P(ci ...ckl&lt;U-t&gt;) =
P(kl&lt;U-t&gt;)P(ci ...cklk, &lt;U-t&gt;) (12)
</equation>
<bodyText confidence="0.999877666666667">
By introducing the distinction of word type to the
model of Equation (12), we can derive a more sophis-
ticated unknown word model that reflects both word
</bodyText>
<footnote confidence="0.995255142857143">
3 When a Chinese character is used to represent a seman-
tically equivalent Japanese verb, its root is written in the
Chinese character and its inflectional suffix is written in hi-
ragana. This results in kanji-hiragana sequence. When a
Chinese character is too difficult to read, it is transliterated
in hiragana. This results in either hiragana-kanji or kanji-
hiragana sequence.
</footnote>
<figure confidence="0.98149775">
part of speech
noun
number
adjectival-verb
verb
adjective
adverb
character bigram
&lt;bow&gt; &lt;eow&gt;
1
&lt;eow&gt;
&lt;eow&gt;
&lt;eow&gt;
&lt;eow&gt;
frequency
1343
</figure>
<page confidence="0.882419833333334">
484
327
213
69
63
280
</page>
<bodyText confidence="0.99984675">
type and part of speech information. This is the un-
known word model we propose in this paper. It first
selects the word type given the part of speech, then
the word length and spelling.
</bodyText>
<equation confidence="0.999743333333333">
P(ci ...ckl&lt;U-t&gt;) =
P(&lt;WT&gt;I&lt;U-t&gt;)P(kl&lt;WT&gt;, &lt;U-t&gt;)
P(ci ...cklk, &lt;WT&gt;, &lt;U-t&gt;) (13)
</equation>
<bodyText confidence="0.999303">
The first factor in the righthand side of Equa-
tion (13) is estimated from the relative frequency
of the corresponding events in the training corpus.
</bodyText>
<equation confidence="0.984106">
C(&lt;WT&gt;, &lt;U-t&gt;)
P(&lt;WT&gt;I&lt;U-t&gt;) = (14)
C(&lt;U-t&gt;)
</equation>
<bodyText confidence="0.999031833333333">
Here, CO represents the counts in the corpus. To
estimate the probabilities of the combinations of
word type and part of speech that did not appeared
in the training corpus, we used the Witten-Bell
method (Witten and Bell, 1991) to obtain an esti-
mate for the sum of the probabilities of unobserved
events. We then redistributed this evenly among all
unobserved events 4 .
The second factor of Equation (13) is estimated
from the Poisson distribution whose parameter
A&lt;WT&gt;,&lt;U-t&gt; is the average length of words whose
word type is &lt;WT&gt; and part of speech is &lt;U-t&gt;.
</bodyText>
<equation confidence="0.9987965">
P(kl&lt;WT&gt;, &lt;U-t&gt;) =
(A&lt;wr&gt;,&lt;u-t&gt;-1)k-i e-0&lt;wr&gt;,&lt;u-t&gt; -1) (15)
</equation>
<bodyText confidence="0.999960777777778">
If the combinations of word type and part of speech
that did not appeared in the training corpus, we used
the average word length of all words.
To compute the third factor of Equation (13), we
have to estimate the character bigram probabilities
that are classified by word type and part of speech.
Basically, they are estimated from the relative fre-
quency of the character bigrams for each word type
and part of speech.
</bodyText>
<equation confidence="0.992319666666667">
Pcilei-i,&lt;WT&gt;,&lt;U-t&gt;) =
c(&lt;wr&gt;,&lt;u-t&gt;,c,_ i,ci)
c(&lt;wT&gt;,&lt;u-t&gt;,.1-1)
</equation>
<bodyText confidence="0.9996466">
However, if we divide the corpus by the combina-
tion of word type and part of speech, the amount of
each training data becomes very small. Therefore,
we linearly interpolated the following five probabili-
ties (Jelinek and Mercer, 1980).
</bodyText>
<equation confidence="0.381718">
P(cilc,-1,&lt;WT&gt;,&lt;U-t&gt;)
</equation>
<footnote confidence="0.949981">
4 The Witten-Bell method estimates the probability of ob-
serving novel events to be r/(n + r), where n is the total num-
ber of events seen previously, and r is the number of symbols
that are distinct. The probability of the event observed c
times is c/(n + r).
</footnote>
<tableCaption confidence="0.998763">
Table 4: The amount of training and test sets
</tableCaption>
<table confidence="0.99467975">
training set test set-1 test set-2
sentences 100,000 100,000 5,000
word tokens 2,460,188 2,465,441 122,064
char tokens 3,897,718 3,906,260 192,818
</table>
<equation confidence="0.908114">
&lt;WT&gt;, &lt;U-t&gt;)
+a2f(c2lci-i,&lt;WT&gt;, &lt;U-t&gt;)
+cts f(c2) + + a5(1/V) (17)
Where
o1+a2+a3+a4+a5 = 1. f (c2, &lt;WT&gt;, &lt;U-t&gt;) and
</equation>
<bodyText confidence="0.995777666666667">
f &lt;WT&gt; , &lt;U-t&gt;) are the relative frequen-
cies of the character unigram and bigram for each
word type and part of speech. f(c1) and f
are the relative frequencies of the character unigram
and bigram. V is the number of characters (not to-
kens but types) appeared in the corpus.
</bodyText>
<sectionHeader confidence="0.999866" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999749">
4.1 Training and Test Data for the
Language Model
</subsectionHeader>
<bodyText confidence="0.999987032258065">
We used the EDR Japanese Corpus Version 1.0
(EDR, 1991) to train the language model. It is a
manually word segmented and tagged corpus of ap-
proximately 5.1 million words (208 thousand sen-
tences). It contains a variety of Japanese sentences
taken from newspapers, magazines, dictionaries, en-
cyclopedias, textbooks, etc..
In this experiment, we randomly selected two sets
of 100 thousand sentences. The first 100 thousand
sentences are used for training the language model.
The second 100 thousand sentences are used for test-
ing. The remaining 8 thousand sentences are used
as a heldout set for smoothing the parameters.
For the evaluation of the word segmentation ac-
curacy, we randomly selected 5 thousand sentences
from the test set of 100 thousand sentences. We
call the first test set (100 thousand sentences) &amp;quot;test
set-1&amp;quot; and the second test set (5 thousand sentences)
&amp;quot;test set-2&amp;quot;. Table 4 shows the number of sentences,
words, and characters of the training and test sets.
There were 94,680 distinct words in the training
test. We discarded the words whose frequency was
one, and made a dictionary of 45,027 words. Af-
ter replacing the words whose frequency was one
with the corresponding unknown word tags, there
were 474,155 distinct word bigrams. We discarded
the bigrams with frequency one, and the remaining
175,527 bigrams were used in the word segmentation
model.
As for the unknown word model, word-based char-
acter bigrams are computed from the words with
</bodyText>
<equation confidence="0.7328">
(16)
</equation>
<page confidence="0.997502">
281
</page>
<tableCaption confidence="0.913756">
Table 5: Cross entropy (CE) per word and character
perplexity (PP) of each unknown word model
</tableCaption>
<bodyText confidence="0.991451846153846">
frequency one (49,653 words). There were 3,120 dis-
tinct character unigrams and 55,486 distinct char-
acter bigrams. We discarded the bigram with fre-
quency one and remaining 20,775 bigrams were used.
There were 12,633 distinct character unigrams and
80,058 distinct character bigrams when we classified
them for each word type and part of speech. We
discarded the bigrams with frequency one and re-
maining 26,633 bigrams were used in the unknown
word model.
Average word lengths for each word type and part
of speech were also computed from the words with
frequency one in the training set.
</bodyText>
<subsectionHeader confidence="0.999902">
4.2 Cross Entropy and Perplexity
</subsectionHeader>
<bodyText confidence="0.999990695652174">
Table 5 shows the cross entropy per word and char-
acter perplexity of three unknown word model. The
first model is Equation (5), which is the combina-
tion of Poisson distribution and character zerogram
(Poisson + zerogram). The second model is the
combination of Poisson distribution (Equation (6))
and character bigram (Equation (7)) (Poisson + bi-
gram). The third model is Equation (11), which is a
set of word models trained for each word type (WT
+ Poisson + bigram). Cross entropy was computed
over the words in test set-1 that were not found
in the dictionary of the word segmentation model
(56,121 words). Character perplexity is more intu-
itive than cross entropy because it shows the average
number of equally probable characters out of 6,879
characters in JIS-X-0208.
Table 5 shows that by changing the word spelling
model from zerogram to bigram, character perplex-
ity is greatly reduced. It also shows that by making
a separate model for each word type, character per-
plexity is reduced by an additional 45% (128 —&gt; 71).
This shows that the word type information is useful
for modeling the morphology of Japanese words.
</bodyText>
<subsectionHeader confidence="0.9996475">
4.3 Part of Speech Prediction Accuracy
without Context
</subsectionHeader>
<bodyText confidence="0.999840571428572">
Figure 3 shows the part of speech prediction accu-
racy of two unknown word model without context.
It shows the accuracies up to the top 10 candidates.
The first model is Equation (12), which is a set of
word models trained for each part of speech (POS
+ Poisson + bigram). The second model is Equa-
tion (13), which is a set of word models trained for
</bodyText>
<figure confidence="0.997207111111111">
0.95
0.9
Accumulated Accuracy 0.85
0.8
0.75
0.7
0.65
2 3 4 5 6 7 8 9 10
Rank
</figure>
<figureCaption confidence="0.999994">
Figure 3: Accuracy of part of speech estimation
</figureCaption>
<bodyText confidence="0.998337444444444">
each part of speech and word type (POS + WT +
Poisson + bigram). The test words are the same
56,121 words used to compute the cross entropy.
Since these unknown word models give the prob-
ability of spelling for each part of speech P(wit), we
used the empirical part of speech probability P(t)
to compute the joint probability P(w, t). The part
of speech t that gives the highest joint probability is
selected.
</bodyText>
<equation confidence="0.816743">
i = arg max P(w, t) = P(t)P(w It) (18)
</equation>
<bodyText confidence="0.99995325">
The part of speech prediction accuracy of the first
and the second model was 67.5% and 74.4%, respec-
tively. As Figure 3 shows, word type information
improves the prediction accuracy significantly.
</bodyText>
<subsectionHeader confidence="0.999523">
4.4 Word Segmentation Accuracy
</subsectionHeader>
<bodyText confidence="0.999869">
Word segmentation accuracy is expressed in terms
of recall and precision as is done in the previous
research (Sproat et al., 1996). Let the number of
words in the manually segmented corpus be Std, the
number of words in the output of the word segmenter
be Sys, and the number of matched words be M.
Recall is defined as M/Std, and precision is defined
as M/Sys. Since it is inconvenient to use both recall
and precision all the time, we also use the F-measure
to indicate the overall performance. It is calculated
by
</bodyText>
<equation confidence="0.981623666666667">
(02 1.0)xPxR
F = (19)
[32 x P + R
</equation>
<bodyText confidence="0.998529">
where P is precision, R is recall, and is the relative
importance given to recall over precision. We set
</bodyText>
<figure confidence="0.994050133333333">
unknown word model
Poisson+zerogram
Poisson+bigram
WT+Poisson+bigram
CE per word
59.4
37.8
33.3
char PP
2032
128
71
Part ot Speech Estimation Accuracy
POS + WT + Poisson + bigram -.--
POS + Poisson + bigram
</figure>
<page confidence="0.989291">
282
</page>
<tableCaption confidence="0.992773">
Table 6: Word segmentation accuracy of all words
</tableCaption>
<table confidence="0.9996974">
rec prec
Poisson+bigram 94.5 93.1 93.8
WT+Poisson+bigram 94.4 93.8 94.1
POS+Poisson+bigram 94.4 93.6 94.0
POS+WT+Poisson+bigram 94.6 93.7 94.1
</table>
<tableCaption confidence="0.915717">
Table 7: Word segmentation accuracy of unknown
words
</tableCaption>
<table confidence="0.99315">
rec prec
Poisson + bigram 31.8 65.0 42.7
WT+Poisson+bigram 45.5 62.0 52.5
POS+Poisson+bigram 39.7 61.5 48.3
POS+WT+Poisson+bigram 42.0 66.4 51.4
</table>
<bodyText confidence="0.999918958333334">
= 1.0 throughout this experiment. That is, we
put equal importance on recall and precision.
Table 6 shows the word segmentation accuracy of
four unknown word models over test set-2. Com-
pared to the baseline model (Poisson + bigram), by
using word type and part of speech information, the
precision of the proposed model (POS + WT + Pois-
son + bigram) is improved by a modest 0.6%. The
impact of the proposed model is small because the
out-of-vocabulary rate of test set-2 is only 3.1%.
To closely investigate the effect of the proposed
unknown word model, we computed the word seg-
mentation accuracy of unknown words. Table 7
shows the results. The accuracy of the proposed
model (POS + WT + Poisson + bigram) is signif-
icantly higher than the baseline model (Poisson +
bigram). Recall is improved from 31.8% to 42.0%
and precision is improved from 65.0% to 66.4%.
Here, recall is the percentage of correctly seg-
mented unknown words in the system output to the
all unknown words in the test sentences. Precision
is the percentage of correctly segmented unknown
words in the system&apos;s output to the all words that
system identified as unknown words.
Table 8 shows the tagging accuracy of unknown
words. Notice that the baseline model (Poisson +
bigram) cannot predict part of speech. To roughly
estimate the amount of improvement brought by the
proposed model, we applied a simple tagging strat-
egy to the output of the baseline model. That is,
words that include numbers are tagged as numbers,
and others are tagged as nouns.
Table 8 shows that by using word type and part
of speech information, recall is improved from 28.1%
to 40.6% and precision is improved from 57.3% to
64.1%.
Other than the usual recall/precision measures,
we defined another precision (prec2 in Table 8),
which roughly correspond to the tagging accuracy
in English where word segmentation is trivial. Prec2
is defined as the percentage of correctly tagged un-
known words to the correctly segmented unknown
words. Table 8 shows that tagging precision is im-
proved from 88.2% to 96.6%. The tagging accuracy
in context (96.6%) is significantly higher than that
without context (74.4%). This shows that the word
bigrams using unknown word tags for each part of
speech are useful to predict the part of speech.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999944268292683">
Since English uses spaces between words, unknown
words can be identified by simple dictionary lookup.
So the topic of interest is part of speech estimation.
Some statistical model to estimate the part of speech
of unknown words from the case of the first letter
and the prefix and suffix is proposed (Weischedel et
al., 1993; Brill, 1995; Ratnaparkhi, 1996; Mikheev,
1997). On the contrary, since Asian languages like
Japanese and Chinese do not put spaces between
words, previous work on unknown word problem is
focused on word segmentation; there are few studies
estimating part of speech of unknown words in Asian
languages.
The cues used for estimating the part of speech of
unknown words for Japanese in this paper are ba-
sically the same for English, namely, the prefix and
suffix of the unknown word as well as the previous
and following part of speech. The contribution of
this paper is in showing the fact that different char-
acter sets behave differently in Japanese and a better
word model can be made by using this fact.
By introducing different length models based on
character sets, the number of decomposition errors
of unknown words are significantly reduced. In other
words, the tendency of over-segmentation is cor-
rected. However, the spelling model, especially the
character bigrams in Equation (17) are hard to es-
timate because of the data sparseness. This is the
main reason of the remaining under-segmented and
over-segmented errors.
To improve the unknown word model, feature-
based approach such as the maximum entropy
method (Ratnaparkhi, 1996) might be useful, be-
cause we don&apos;t have to divide the training data into
several disjoint sets (like we did by part of speech
and word type) and we can incorporate more lin-
guistic and morphological knowledge into the same
probabilistic framework. We are thinking of re-
implementing our unknown word model using the
maximum entropy method as the next step of our
research.
</bodyText>
<page confidence="0.999416">
283
</page>
<tableCaption confidence="0.979531">
Table 8: Part of speech tagging accuracy of unknown words (the last column represents the percentage of
correctly tagged unknown words in the correctly segmented unknown words)
</tableCaption>
<table confidence="0.9993868">
rec prec F prec2
Poisson+bigram 28.1 57.3 37.7 88.2
WT+Poisson+bigram 37.7 51.5 43.5 87.9
P OS +Poisson+bigram 37.5 58.1 45.6 94.3
POS+WT+Poisson+bigram 40.6 64.1 49.7 96.6
</table>
<sectionHeader confidence="0.998663" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999496">
We present a statistical model of Japanese unknown
words using word morphology and word context. We
find that Japanese words are better modeled by clas-
sifying words based on the character sets (kanji, hi-
ragana, katakana, etc.) and its changes. This is
because the different character sets behave differ-
ently in many ways (historical etymology, ideogram
vs. phonogram, etc.). Both word segmentation ac-
curacy and part of speech tagging accuracy are im-
proved by treating them differently.
</bodyText>
<sectionHeader confidence="0.988963" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.694571625">
Harald Baayen and Richard Sproat. 1996. Estimat-
ing lexical priors for low-frequency morphologi-
cally ambiguous forms. Computational Linguis-
tics, 22(2):155-166.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational
Linguistics, 21(4):543-565.
</bodyText>
<reference confidence="0.999018328358209">
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, Jennifer C. Lai, and Robert L.
Mercer. 1992. An estimate of an upper bound for
the entropy of English. Computational Linguis-
tics, 18(1):31-40.
Jing-Shin Chang, Yi-Chung Lin, and Keh-Yih Su.
1995. Automatic construction of a Chinese elec-
tronic dictionary. In Proceedings of the Third
Workshop on Very Large Corpora, pages 107-120.
EDR. 1991. EDR electronic dictionary version
1 technical guide. Technical Report TR2-003,
Japan Electronic Dictionary Research Institute.
Pascale Fung and Dekai Wu. 1994. Statistical aug-
mentation of a Chinese machine-readable dictio-
nary. In Proceedings of the Second Workshop on
Very Large Corpora, pages 69-85.
Masahiko Haruno and Yuji Matsumoto. 1997.
Mistake-driven mixture of hierachical tag context
trees. In Proceedings of the 35th ACL and 8th
EA CL, pages 230-237.
F. Jelinek and R. L. Mercer. 1980. Interpolated esti-
mation of Markov source parameters from sparse
data. In Proceedings of the Workshop on Pattern
Recognition in Practice, pages 381-397.
Andrei Mikheev. 1997. Automatic rule induction for
unknown-word guessing. Computational Linguis-
tics, 23(3):405-423.
Shinsuke Mori and Makoto Nagao. 1996. Word ex-
traction from corpora and its part-of-speech esti-
mation using distributional analysis. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics, pages 1119-1122.
Masaaki Nagata. 1994. A stochastic Japanese mor-
phological analyzer using a forward-dp backward-
A* n-best search algorithm. In Proceedings of the
15th International Conference on Computational
Linguistics, pages 201-207.
Masaaki Nagata. 1996. Context-based spelling cor-
rection for Japanese OCR. In Proceedings of the
16th International Conference on Computational
Linguistics, pages 806-811.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing, pages 133-142.
Richard Sproat, Chilin Shih, William Gale, and
Nancy Chang. 1996. A stochastic finite-state
word-segmentation algorithm for Chinese. Com-
putational Linguistics, 22(3):377-404.
Koichi Takeuchi and Yuji Matsumoto. 1997. HMM
parameter learning for Japanese morphological
analyzer. Transaction of Information Processing
of Japan, 38(3):500-509. (in Japanese).
Ralph Weischedel, Marie Meteer, Richard Schwartz,
Lance Ramshaw, and Jeff Palmucci. 1993. Cop-
ing with ambiguity and unknown words through
probabilistic models. Computational Linguistics,
19(2):359-382.
Ian H. Witten and Timothy C. Bell. 1991. The
zero-frequency problem: Estimating the proba-
bilities of novel events in adaptive text compres-
sion. IEEE Transaction on Information Theory,
37(4):1085-1094.
Mikio Yamamoto. 1996. A re-estimation method for
stochastic language modeling from ambiguous ob-
servations. In Proceedings of the Fourth Workshop
on Very Large Corpora, pages 155-167.
</reference>
<page confidence="0.998354">
284
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.577164">
<title confidence="0.9942775">A Part of Speech Estimation Method for Japanese Unknown Words using a Statistical Model of Morphology and Context</title>
<author confidence="0.992225">Masaaki NAGATA</author>
<affiliation confidence="0.986709">NTT Cyber Space Laboratories</affiliation>
<address confidence="0.837293">1-1 Hikari-no-oka Yokosuka-Shi Kanagawa, 239-0847 Japan</address>
<email confidence="0.793421">nagatanttn1y.isl.ntt.co.jp</email>
<abstract confidence="0.986647214285714">We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that constitute a word. The point is quite simple: different character sets should be treated differently and the changes between character types are very important because Japanese script has both ideograms Chinese phonograms like English word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model. The model can achieve 96.6% tagging accuracy if unknown words are correctly segmented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>An estimate of an upper bound for the entropy of English.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--1</pages>
<contexts>
<context position="6828" citStr="Brown et al., 1992" startWordPosition="1135" endWordPosition="1138"> of an unknown word as a word having a special part of speech &lt;UNK&gt;. Then, the unknown word model is formally defined as the joint probability of the character sequence wi = c1 . • • ck if it is an unknown word. Without loss of generality, we decompose it into the product of word length probability and word spelling probability given its length, P(wil&lt;UNK&gt;) = P(ci ...ck 1&lt;UNK).) = P(kl&lt;UNK&gt;)P(ci cklk, &lt;UNK&gt;) (4) where k is the length of the character sequence. We call P(kl&lt;UNK&gt;) the word length model, and P(ci ck1k, &lt;UNK&gt;) the word spelling model. In order to estimate the entropy of English, (Brown et al., 1992) approximated P(kl&lt;UNK&gt;) by a Poisson distribution whose parameter is the average word length A in the training corpus, and P(ci . ck lk, &lt;UNK&gt;) by the product of character zerogram probabilities. This means all characters in the character set are considered to be selected independently and uniformly. Ak P(ci ckl&lt;UNK&gt;) ve plc where p is the inverse of the number of characters in the character set. If we assume JIS-X-0208 is used as the Japanese character set, p = 1/6879. Since the Poisson distribution is a single parameter distribution with lower bound, it is appropriate to use it as a first o</context>
</contexts>
<marker>Brown, Pietra, Pietra, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Jennifer C. Lai, and Robert L. Mercer. 1992. An estimate of an upper bound for the entropy of English. Computational Linguistics, 18(1):31-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing-Shin Chang</author>
<author>Yi-Chung Lin</author>
<author>Keh-Yih Su</author>
</authors>
<title>Automatic construction of a Chinese electronic dictionary.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>107--120</pages>
<contexts>
<context position="1458" citStr="Chang et al., 1995" startWordPosition="223" endWordPosition="226">segmented. 1 Introduction In Japanese, around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Matsumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996). But there has been relatively little improvement in recent years because most of the remaining errors are due to unknown words. There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Naga.o, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996). We take the latter approach. To improve word segmentation accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words. The goal of our research is to assign a correct part of speech to unknown word as well as identifying it correctly. In this paper, we present a novel statistical model for Japanese unknown words. It consists of a set of word</context>
</contexts>
<marker>Chang, Lin, Su, 1995</marker>
<rawString>Jing-Shin Chang, Yi-Chung Lin, and Keh-Yih Su. 1995. Automatic construction of a Chinese electronic dictionary. In Proceedings of the Third Workshop on Very Large Corpora, pages 107-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EDR</author>
</authors>
<title>EDR electronic dictionary version 1 technical guide.</title>
<date>1991</date>
<tech>Technical Report TR2-003,</tech>
<institution>Japan Electronic Dictionary Research Institute.</institution>
<contexts>
<context position="18392" citStr="EDR, 1991" startWordPosition="3043" endWordPosition="3044">,000 100,000 5,000 word tokens 2,460,188 2,465,441 122,064 char tokens 3,897,718 3,906,260 192,818 &lt;WT&gt;, &lt;U-t&gt;) +a2f(c2lci-i,&lt;WT&gt;, &lt;U-t&gt;) +cts f(c2) + + a5(1/V) (17) Where o1+a2+a3+a4+a5 = 1. f (c2, &lt;WT&gt;, &lt;U-t&gt;) and f &lt;WT&gt; , &lt;U-t&gt;) are the relative frequencies of the character unigram and bigram for each word type and part of speech. f(c1) and f are the relative frequencies of the character unigram and bigram. V is the number of characters (not tokens but types) appeared in the corpus. 4 Experiments 4.1 Training and Test Data for the Language Model We used the EDR Japanese Corpus Version 1.0 (EDR, 1991) to train the language model. It is a manually word segmented and tagged corpus of approximately 5.1 million words (208 thousand sentences). It contains a variety of Japanese sentences taken from newspapers, magazines, dictionaries, encyclopedias, textbooks, etc.. In this experiment, we randomly selected two sets of 100 thousand sentences. The first 100 thousand sentences are used for training the language model. The second 100 thousand sentences are used for testing. The remaining 8 thousand sentences are used as a heldout set for smoothing the parameters. For the evaluation of the word segme</context>
</contexts>
<marker>EDR, 1991</marker>
<rawString>EDR. 1991. EDR electronic dictionary version 1 technical guide. Technical Report TR2-003, Japan Electronic Dictionary Research Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Dekai Wu</author>
</authors>
<title>Statistical augmentation of a Chinese machine-readable dictionary.</title>
<date>1994</date>
<booktitle>In Proceedings of the Second Workshop on Very Large Corpora,</booktitle>
<pages>69--85</pages>
<contexts>
<context position="1438" citStr="Fung and Wu, 1994" startWordPosition="219" endWordPosition="222">ords are correctly segmented. 1 Introduction In Japanese, around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Matsumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996). But there has been relatively little improvement in recent years because most of the remaining errors are due to unknown words. There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Naga.o, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996). We take the latter approach. To improve word segmentation accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words. The goal of our research is to assign a correct part of speech to unknown word as well as identifying it correctly. In this paper, we present a novel statistical model for Japanese unknown words. It consi</context>
</contexts>
<marker>Fung, Wu, 1994</marker>
<rawString>Pascale Fung and Dekai Wu. 1994. Statistical augmentation of a Chinese machine-readable dictionary. In Proceedings of the Second Workshop on Very Large Corpora, pages 69-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masahiko Haruno</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Mistake-driven mixture of hierachical tag context trees.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th ACL and 8th EA CL,</booktitle>
<pages>230--237</pages>
<contexts>
<context position="1106" citStr="Haruno and Matsumoto, 1997" startWordPosition="162" endWordPosition="166">eated differently and the changes between character types are very important because Japanese script has both ideograms like Chinese (kanji) and phonograms like English (katakana). Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model. The model can achieve 96.6% tagging accuracy if unknown words are correctly segmented. 1 Introduction In Japanese, around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Matsumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996). But there has been relatively little improvement in recent years because most of the remaining errors are due to unknown words. There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Naga.o, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996). We take the latter approach. To improve word segmentation accuracy, (Nagata, 1996) used a single general purpose unknown word model, while </context>
</contexts>
<marker>Haruno, Matsumoto, 1997</marker>
<rawString>Masahiko Haruno and Yuji Matsumoto. 1997. Mistake-driven mixture of hierachical tag context trees. In Proceedings of the 35th ACL and 8th EA CL, pages 230-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Proceedings of the Workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<contexts>
<context position="17405" citStr="Jelinek and Mercer, 1980" startWordPosition="2870" endWordPosition="2873">pus, we used the average word length of all words. To compute the third factor of Equation (13), we have to estimate the character bigram probabilities that are classified by word type and part of speech. Basically, they are estimated from the relative frequency of the character bigrams for each word type and part of speech. Pcilei-i,&lt;WT&gt;,&lt;U-t&gt;) = c(&lt;wr&gt;,&lt;u-t&gt;,c,_ i,ci) c(&lt;wT&gt;,&lt;u-t&gt;,.1-1) However, if we divide the corpus by the combination of word type and part of speech, the amount of each training data becomes very small. Therefore, we linearly interpolated the following five probabilities (Jelinek and Mercer, 1980). P(cilc,-1,&lt;WT&gt;,&lt;U-t&gt;) 4 The Witten-Bell method estimates the probability of observing novel events to be r/(n + r), where n is the total number of events seen previously, and r is the number of symbols that are distinct. The probability of the event observed c times is c/(n + r). Table 4: The amount of training and test sets training set test set-1 test set-2 sentences 100,000 100,000 5,000 word tokens 2,460,188 2,465,441 122,064 char tokens 3,897,718 3,906,260 192,818 &lt;WT&gt;, &lt;U-t&gt;) +a2f(c2lci-i,&lt;WT&gt;, &lt;U-t&gt;) +cts f(c2) + + a5(1/V) (17) Where o1+a2+a3+a4+a5 = 1. f (c2, &lt;WT&gt;, &lt;U-t&gt;) and f &lt;WT&gt; </context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>F. Jelinek and R. L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, pages 381-397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
</authors>
<title>Automatic rule induction for unknown-word guessing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="26776" citStr="Mikheev, 1997" startWordPosition="4465" endWordPosition="4466">.6%. The tagging accuracy in context (96.6%) is significantly higher than that without context (74.4%). This shows that the word bigrams using unknown word tags for each part of speech are useful to predict the part of speech. 5 Related Work Since English uses spaces between words, unknown words can be identified by simple dictionary lookup. So the topic of interest is part of speech estimation. Some statistical model to estimate the part of speech of unknown words from the case of the first letter and the prefix and suffix is proposed (Weischedel et al., 1993; Brill, 1995; Ratnaparkhi, 1996; Mikheev, 1997). On the contrary, since Asian languages like Japanese and Chinese do not put spaces between words, previous work on unknown word problem is focused on word segmentation; there are few studies estimating part of speech of unknown words in Asian languages. The cues used for estimating the part of speech of unknown words for Japanese in this paper are basically the same for English, namely, the prefix and suffix of the unknown word as well as the previous and following part of speech. The contribution of this paper is in showing the fact that different character sets behave differently in Japane</context>
</contexts>
<marker>Mikheev, 1997</marker>
<rawString>Andrei Mikheev. 1997. Automatic rule induction for unknown-word guessing. Computational Linguistics, 23(3):405-423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Makoto Nagao</author>
</authors>
<title>Word extraction from corpora and its part-of-speech estimation using distributional analysis.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>1119--1122</pages>
<marker>Mori, Nagao, 1996</marker>
<rawString>Shinsuke Mori and Makoto Nagao. 1996. Word extraction from corpora and its part-of-speech estimation using distributional analysis. In Proceedings of the 16th International Conference on Computational Linguistics, pages 1119-1122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A stochastic Japanese morphological analyzer using a forward-dp backwardA* n-best search algorithm.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="1031" citStr="Nagata, 1994" startWordPosition="154" endWordPosition="155">point is quite simple: different character sets should be treated differently and the changes between character types are very important because Japanese script has both ideograms like Chinese (kanji) and phonograms like English (katakana). Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model. The model can achieve 96.6% tagging accuracy if unknown words are correctly segmented. 1 Introduction In Japanese, around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Matsumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996). But there has been relatively little improvement in recent years because most of the remaining errors are due to unknown words. There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Naga.o, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996). We take the latter approach. To improve word segmentation accura</context>
<context position="4170" citStr="Nagata, 1994" startWordPosition="685" endWordPosition="686">ically, word bigram probabilities of the word segmentation model is estimated by computing the 1 In this paper, we define a word as a combination of its surface form and part of speech. Two words are considered to be equal only if they have the same surface form and part of speech. 277 Table 1: Examples of word bigrams including unknown word tags relative frequencies of the corresponding events in the word segmented training corpus, with appropriate smoothing techniques. The maximization search can be efficiently implemented by using the Viterbilike dynamic programming procedure described in (Nagata, 1994). 2.2 Modification to Handle Unknown Words To handle unknown words, we made a slight modification in the above word segmentation model. We have introduced unknown word tags &lt;U-t&gt; for each part of speech t. For example, &lt;U-noun&gt; and &lt;Uverb&gt; represents an unknown noun and an unknown verb, respectively. If wi is an unknown word whose part of speech is t, the word bigram probability P(wi 1 wi_i) is approximated as the product of word bigram probability P(&lt;U-t&gt;lwi_i) and the probability of w, given it is an unknown word whose part of speech is t, P(wil&lt;U-t&gt;). P(wilwi_i) = P(&lt;U-t&gt;lwi_i)P(wil&lt;U-t&gt;) (</context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-dp backwardA* n-best search algorithm. In Proceedings of the 15th International Conference on Computational Linguistics, pages 201-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>Context-based spelling correction for Japanese OCR.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>806--811</pages>
<contexts>
<context position="1543" citStr="Nagata, 1996" startWordPosition="240" endWordPosition="241"> using a word-based language model and the Viterbi-like dynamic programming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Matsumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996). But there has been relatively little improvement in recent years because most of the remaining errors are due to unknown words. There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Naga.o, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996). We take the latter approach. To improve word segmentation accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words. The goal of our research is to assign a correct part of speech to unknown word as well as identifying it correctly. In this paper, we present a novel statistical model for Japanese unknown words. It consists of a set of word models for each part of speech and word type. We classified Japanese words into nine</context>
<context position="7703" citStr="Nagata, 1996" startWordPosition="1283" endWordPosition="1284">be selected independently and uniformly. Ak P(ci ckl&lt;UNK&gt;) ve plc where p is the inverse of the number of characters in the character set. If we assume JIS-X-0208 is used as the Japanese character set, p = 1/6879. Since the Poisson distribution is a single parameter distribution with lower bound, it is appropriate to use it as a first order approximation to the word length distribution. But the Brown model has two problems. It assigns a certain amount of probability mass to zero-length words, and it is too simple to express morphology. For Japanese word segmentation and OCR error correction, (Nagata, 1996) proposed a modified version of the Brown model. Nagata also assumed the word length probability obeys the Poisson distribution. But he moved the lower bound from zero to one. (A — 1)k-1 P(kl&lt;UNK&gt;) (6) (k —1)! word bigram 0)/no/particle &lt;U-noun&gt; &lt;U-verb&gt; 1—/shi/inflection &lt;U-number&gt; F9/yen/suffix &lt;U-adjectival-verb&gt; /X/na/inilection &lt;U-adjective&gt; i yi/inflection &lt;U-adverb&gt; /to/particle frequency 6783 1052 407 405 182 139 (5) 278 Instead of zerogram, He approximated the word spelling probability P(ci ck jk, &lt;UNK&gt;) by the product of word-based character bigram probabilities, regardless of word l</context>
</contexts>
<marker>Nagata, 1996</marker>
<rawString>Masaaki Nagata. 1996. Context-based spelling correction for Japanese OCR. In Proceedings of the 16th International Conference on Computational Linguistics, pages 806-811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="26760" citStr="Ratnaparkhi, 1996" startWordPosition="4463" endWordPosition="4464">ed from 88.2% to 96.6%. The tagging accuracy in context (96.6%) is significantly higher than that without context (74.4%). This shows that the word bigrams using unknown word tags for each part of speech are useful to predict the part of speech. 5 Related Work Since English uses spaces between words, unknown words can be identified by simple dictionary lookup. So the topic of interest is part of speech estimation. Some statistical model to estimate the part of speech of unknown words from the case of the first letter and the prefix and suffix is proposed (Weischedel et al., 1993; Brill, 1995; Ratnaparkhi, 1996; Mikheev, 1997). On the contrary, since Asian languages like Japanese and Chinese do not put spaces between words, previous work on unknown word problem is focused on word segmentation; there are few studies estimating part of speech of unknown words in Asian languages. The cues used for estimating the part of speech of unknown words for Japanese in this paper are basically the same for English, namely, the prefix and suffix of the unknown word as well as the previous and following part of speech. The contribution of this paper is in showing the fact that different character sets behave diffe</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic finite-state word-segmentation algorithm for Chinese. Computational Linguistics,</title>
<date>1996</date>
<pages>22--3</pages>
<contexts>
<context position="1199" citStr="Sproat et al., 1996" startWordPosition="178" endWordPosition="181"> has both ideograms like Chinese (kanji) and phonograms like English (katakana). Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model. The model can achieve 96.6% tagging accuracy if unknown words are correctly segmented. 1 Introduction In Japanese, around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Matsumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996). But there has been relatively little improvement in recent years because most of the remaining errors are due to unknown words. There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Naga.o, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996). We take the latter approach. To improve word segmentation accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names,</context>
<context position="22989" citStr="Sproat et al., 1996" startWordPosition="3823" endWordPosition="3826">ability of spelling for each part of speech P(wit), we used the empirical part of speech probability P(t) to compute the joint probability P(w, t). The part of speech t that gives the highest joint probability is selected. i = arg max P(w, t) = P(t)P(w It) (18) The part of speech prediction accuracy of the first and the second model was 67.5% and 74.4%, respectively. As Figure 3 shows, word type information improves the prediction accuracy significantly. 4.4 Word Segmentation Accuracy Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996). Let the number of words in the manually segmented corpus be Std, the number of words in the output of the word segmenter be Sys, and the number of matched words be M. Recall is defined as M/Std, and precision is defined as M/Sys. Since it is inconvenient to use both recall and precision all the time, we also use the F-measure to indicate the overall performance. It is calculated by (02 1.0)xPxR F = (19) [32 x P + R where P is precision, R is recall, and is the relative importance given to recall over precision. We set unknown word model Poisson+zerogram Poisson+bigram WT+Poisson+bigram CE pe</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>Richard Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-state word-segmentation algorithm for Chinese. Computational Linguistics, 22(3):377-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichi Takeuchi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>HMM parameter learning for Japanese morphological analyzer.</title>
<date>1997</date>
<booktitle>Transaction of Information Processing of Japan,</booktitle>
<pages>38--3</pages>
<note>(in Japanese).</note>
<contexts>
<context position="1077" citStr="Takeuchi and Matsumoto, 1997" startWordPosition="158" endWordPosition="161">nt character sets should be treated differently and the changes between character types are very important because Japanese script has both ideograms like Chinese (kanji) and phonograms like English (katakana). Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model. The model can achieve 96.6% tagging accuracy if unknown words are correctly segmented. 1 Introduction In Japanese, around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Matsumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996). But there has been relatively little improvement in recent years because most of the remaining errors are due to unknown words. There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Naga.o, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996). We take the latter approach. To improve word segmentation accuracy, (Nagata, 1996) used a single general purpo</context>
</contexts>
<marker>Takeuchi, Matsumoto, 1997</marker>
<rawString>Koichi Takeuchi and Yuji Matsumoto. 1997. HMM parameter learning for Japanese morphological analyzer. Transaction of Information Processing of Japan, 38(3):500-509. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Marie Meteer</author>
<author>Richard Schwartz</author>
<author>Lance Ramshaw</author>
<author>Jeff Palmucci</author>
</authors>
<title>Coping with ambiguity and unknown words through probabilistic models.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="26728" citStr="Weischedel et al., 1993" startWordPosition="4457" endWordPosition="4460">shows that tagging precision is improved from 88.2% to 96.6%. The tagging accuracy in context (96.6%) is significantly higher than that without context (74.4%). This shows that the word bigrams using unknown word tags for each part of speech are useful to predict the part of speech. 5 Related Work Since English uses spaces between words, unknown words can be identified by simple dictionary lookup. So the topic of interest is part of speech estimation. Some statistical model to estimate the part of speech of unknown words from the case of the first letter and the prefix and suffix is proposed (Weischedel et al., 1993; Brill, 1995; Ratnaparkhi, 1996; Mikheev, 1997). On the contrary, since Asian languages like Japanese and Chinese do not put spaces between words, previous work on unknown word problem is focused on word segmentation; there are few studies estimating part of speech of unknown words in Asian languages. The cues used for estimating the part of speech of unknown words for Japanese in this paper are basically the same for English, namely, the prefix and suffix of the unknown word as well as the previous and following part of speech. The contribution of this paper is in showing the fact that diffe</context>
</contexts>
<marker>Weischedel, Meteer, Schwartz, Ramshaw, Palmucci, 1993</marker>
<rawString>Ralph Weischedel, Marie Meteer, Richard Schwartz, Lance Ramshaw, and Jeff Palmucci. 1993. Coping with ambiguity and unknown words through probabilistic models. Computational Linguistics, 19(2):359-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Timothy C Bell</author>
</authors>
<title>The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transaction on Information Theory,</journal>
<pages>37--4</pages>
<contexts>
<context position="16292" citStr="Witten and Bell, 1991" startWordPosition="2688" endWordPosition="2691">we propose in this paper. It first selects the word type given the part of speech, then the word length and spelling. P(ci ...ckl&lt;U-t&gt;) = P(&lt;WT&gt;I&lt;U-t&gt;)P(kl&lt;WT&gt;, &lt;U-t&gt;) P(ci ...cklk, &lt;WT&gt;, &lt;U-t&gt;) (13) The first factor in the righthand side of Equation (13) is estimated from the relative frequency of the corresponding events in the training corpus. C(&lt;WT&gt;, &lt;U-t&gt;) P(&lt;WT&gt;I&lt;U-t&gt;) = (14) C(&lt;U-t&gt;) Here, CO represents the counts in the corpus. To estimate the probabilities of the combinations of word type and part of speech that did not appeared in the training corpus, we used the Witten-Bell method (Witten and Bell, 1991) to obtain an estimate for the sum of the probabilities of unobserved events. We then redistributed this evenly among all unobserved events 4 . The second factor of Equation (13) is estimated from the Poisson distribution whose parameter A&lt;WT&gt;,&lt;U-t&gt; is the average length of words whose word type is &lt;WT&gt; and part of speech is &lt;U-t&gt;. P(kl&lt;WT&gt;, &lt;U-t&gt;) = (A&lt;wr&gt;,&lt;u-t&gt;-1)k-i e-0&lt;wr&gt;,&lt;u-t&gt; -1) (15) If the combinations of word type and part of speech that did not appeared in the training corpus, we used the average word length of all words. To compute the third factor of Equation (13), we have to esti</context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transaction on Information Theory, 37(4):1085-1094.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Yamamoto</author>
</authors>
<title>A re-estimation method for stochastic language modeling from ambiguous observations.</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth Workshop on Very Large Corpora,</booktitle>
<pages>155--167</pages>
<contexts>
<context position="1047" citStr="Yamamoto, 1996" startWordPosition="156" endWordPosition="157"> simple: different character sets should be treated differently and the changes between character types are very important because Japanese script has both ideograms like Chinese (kanji) and phonograms like English (katakana). Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model. The model can achieve 96.6% tagging accuracy if unknown words are correctly segmented. 1 Introduction In Japanese, around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Matsumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996). But there has been relatively little improvement in recent years because most of the remaining errors are due to unknown words. There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Naga.o, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996). We take the latter approach. To improve word segmentation accuracy, (Nagata, 199</context>
</contexts>
<marker>Yamamoto, 1996</marker>
<rawString>Mikio Yamamoto. 1996. A re-estimation method for stochastic language modeling from ambiguous observations. In Proceedings of the Fourth Workshop on Very Large Corpora, pages 155-167.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>