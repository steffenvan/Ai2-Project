<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000174">
<title confidence="0.9844835">
Paraphrase Recognition Using Machine Learning to Combine Similarity
Measures
</title>
<author confidence="0.977467">
Prodromos Malakasiotis
</author>
<affiliation confidence="0.9975105">
Department of Informatics
Athens University of Economics and Business
</affiliation>
<address confidence="0.684171">
Patission 76, GR-104 34 Athens, Greece
</address>
<sectionHeader confidence="0.975383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999089928571429">
This paper presents three methods that can
be used to recognize paraphrases. They
all employ string similarity measures ap-
plied to shallow abstractions of the input
sentences, and a Maximum Entropy clas-
sifier to learn how to combine the result-
ing features. Two of the methods also ex-
ploit WordNet to detect synonyms and one
of them also exploits a dependency parser.
We experiment on two datasets, the MSR
paraphrasing corpus and a dataset that we
automatically created from the MTC cor-
pus. Our system achieves state of the art
or better results.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973888888889">
Recognizing or generating semantically equiva-
lent phrases is of significant importance in many
natural language applications. In question answer-
ing, for example, a question may be phrased dif-
ferently than in a document collection (e.g., “Who
is the author of War and Peace?” vs. “Leo Tol-
stoy is the writer of War and Peace.”), and taking
such variations into account can improve system
performance significantly (Harabagiu et al., 2003;
Harabagiu and Hickl, 2006). A paraphrase gener-
ator, meaning a module that produces new phrases
or patterns that are semantically equivalent (or al-
most equivalant) to a given input phrase or pattern
(e.g., “X is the writer of Y ” q “X wrote Y ” q
“Y was written by X” q “X is the author of Y ”,
or “X produces Y ” q “X manufactures Y ” q
“X is the manufacturer of Y ”) can be used to pro-
duce alternative phrasings of the question, before
matching it against a document collection.
Unlike paraphrase generators, paraphrase rec-
ognizers decide whether or not two given phrases
(or patterns) are paraphrases, possibly by general-
izing over many different training pairs of phrases.
Paraphrase recognizers can be embedded in para-
phrase generators to filter out erroneous generated
paraphrases; but they are also useful on their own.
In question answering, for example, they can be
used to check if a pattern extracted from the ques-
tion (possibly by replacing named entities by their
semantic categories and turning the question into
a statement) matches any patterns extracted from
candidate answers. As a further example, in text
summarization, especially multi-document sum-
marization, a paraphrase recognizer can be used
to check if a sentence is a paraphrase of any other
sentence already present in a partially constructed
summary.
Note that, although “paraphrasing” and “textual
entailment” are sometimes used as synonyms, we
use the former to refer to methods that generate
or recognize semantically equivalent (or almost
equivalent) phrases or patterns, whereas in textual
entailment (Dagan et al., 2006; Bar-Haim et al.,
2006; Giampiccolo et al., 2007) the expressions or
patterns are not necessarily semantically equiva-
lent; it suffices if one entails the other, even if the
reverse direction does not hold. For example, “Y
was written by X” textually entails “Y is the work
of X”, but the reverse direction does not neces-
sarily hold (e.g., if Y is a statue); hence, the two
sentences are not paraphrases.
In this paper, we focus on paraphrase recogni-
tion. We propose three methods that employ string
similarity measures, which are applied to several
abstractions of a pair of input phrases (e.g., the
phrases themselves, their stems, POS tags). The
scores returned by the similarity measures are used
as features in a Maximum Entropy (ME) classifier
(Jaynes, 1957; Good, 1963), which learns to sepa-
rate true paraphrase pairs from false ones. Two of
our methods also exploit WordNet to detect syn-
onyms, and one of them uses additional features
to measure similarities of grammatical relations
</bodyText>
<page confidence="0.983988">
27
</page>
<note confidence="0.996993">
Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 27–35,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999615533333333">
obtained by a dependency parser.1 Our experi-
ments were conducted on two datasets: the pub-
licly available Microsoft Research Paraphrasing
corpus (Dolan et al., 2004) and a dataset that we
constructed from the MTC corpus.2 The experi-
mental results show that our methods perform very
well. Even the simplest one manages to achieve
state of the art results, even though it uses fewer
linguistic resources than other reported systems.
The other two, more elaborate methods perform
even better.
Section 2 presents the three methods, and sec-
tion 3 our experiments. Section 4 covers related
work. Section 5 concludes and proposes further
work.
</bodyText>
<sectionHeader confidence="0.960559" genericHeader="method">
2 The three methods
</sectionHeader>
<bodyText confidence="0.999923363636364">
The main idea underlying our methods is that by
capturing similarities at various shallow abstrac-
tions of the input (e.g., the original sentences, the
stems of their words, their POS tags), we can rec-
ognize paraphrases and textual entailment reason-
ably well, provided that we learn to assign ap-
propriate weights to the resulting features. Fur-
ther improvements are possible by recognizing
synonyms and by employing similarity measures
that operate on the output of dependency grammar
parsers.
</bodyText>
<subsectionHeader confidence="0.993999">
2.1 Method 1(INIT)
</subsectionHeader>
<bodyText confidence="0.997384777777778">
During training, the first method, called INIT, is
given a set {hS1,1, S1,2, y1i ,... , hSn,1, Sn,2, yni},
where Si,1 and Si,2 are sentences (more gener-
ally, phrases), yi = 1 (positive class) if the
two sentences are paraphrases, and yi = −1
(negative class) otherwise. Each pair of sen-
tences hSi,1, Si,2i is converted to a feature vec-
tor ~vi, whose values are scores returned by sim-
ilarity measures that indicate how similar Si,1
and Si,2 are at various levels of abstraction.
The vectors and the corresponding categories
{h~v1, yii , . . . , h ~vn, yni} are given as input to the
ME classifier, which learns how to classify new
vectors ~v, corresponding to unseen pairs of sen-
tences hS1, S2i.
We use nine string similarity measures: Leven-
shtein distance (edit distance), Jaro-Winkler dis-
tance, Manhattan distance, Euclidean distance, co-
</bodyText>
<footnote confidence="0.9980635">
1We use Stanford University’s ME classifier and parser;
seehttp://nlp.stanford.edu/.
2The corpus is available by the LDC, Catalogue Number
LDC2002T01, ISBN 1-58563-217-1.
</footnote>
<bodyText confidence="0.918155888888889">
sine similarity, n-gram distance (with n = 3),
matching coefficient, Dice coefficient, and Jac-
card coefficient. To save space, we do not repeat
the definitions of the similarity measures here,
since they are readily available in the literature
and they are also summarized in our previous work
(Malakasiotis and Androutsopoulos, 2007).
For each pair of input strings hS1, S2i, we form
ten new pairs of strings (s11, s1 � , ... ,
</bodyText>
<equation confidence="0.874989">
s10 �
1 , s10
2 2
</equation>
<bodyText confidence="0.996868">
corresponding to ten different levels of abstraction
of S1 and S2, and we apply the nine similarity
measures to the ten new pairs, resulting in a to-
tal of 90 measurements. These measurements are
then included as features in the vector v~ that cor-
responds to hS1, S2i. The (si1, si � pairs are:
</bodyText>
<equation confidence="0.9875774375">
2
(s11, s1 � : two strings consisting of the original tokens of S1
2
and S2, respectively, with the original order of the to-
kens maintained;3
(s(s(s21, s2 � : as in the previous case, but now the tokens are
2
replaced by their stems;
31, s3 � : as in the previous case, but now the tokens are
2
replaced by their part-of-speech (POS) tags;
41, s4 � : as in the previous case, but now the tokens are
2
replaced by their soundex codes;4
(s51, s5 � : two strings consisting of only the nouns of S1 and
2
</equation>
<bodyText confidence="0.876729">
S2, as identified by a POS-tagger, with the original or-
der of the nouns maintained;
</bodyText>
<equation confidence="0.952032625">
(s(s(s61, s6 � : as in the previous case, but now with nouns re-
2
placed by their stems;
71, s7 � : as in the previous case, but now with nouns re-
2
placed by their soundex codes;
81, s8 � : two strings consisting of only the verbs of S1 and
2
</equation>
<bodyText confidence="0.9956275">
S2, as identified by a POS-tagger, with the original or-
der of the verbs maintained;
</bodyText>
<equation confidence="0.982363285714286">
(s91, s9 � : as in the previous case, but now with verbs re-
2
placed by their stems;
s10
1 , s10~ : as in the previous case, but now with verbs re-
2
placed by their soundex codes.
</equation>
<bodyText confidence="0.999817333333333">
Note that the similarities are measured in terms
of tokens, not characters. For instance, the edit
distance of S1 and S2 is the minimum number of
operations needed to transform S1 to S2, where an
operation is an insertion, deletion or substitution
of a single token. Moreover, we use high-level
</bodyText>
<footnote confidence="0.998232166666667">
3We use Stanford University’s tokenizer and POS-tagger,
and Porter’s stemmer.
4Soundex is an algorithm intended to map English names
to alphanumeric codes, so that names with the same pronun-
ciations receive the same codes, despite spelling differences;
seehttp://en.wikipedia.org/wiki/Soundex.
</footnote>
<page confidence="0.99862">
28
</page>
<bodyText confidence="0.917581045454546">
POS tags only, i.e., we do not consider the num-
ber of nouns, the voice of verbs etc.; this increases
the similarity of positive (s31, s3 ) pairs.
2
A common problem is that the string similar-
ity measures may be misled by differences in the
lengths of S1 and S2. This is illustrated in the fol-
lowing examples, where the underlined part of S1
is much more similar to S2 than the entire S1.
S1: While Bolton apparently fell and was immobilized,
Selenski used the mattress to scale a 10-foot, razor-wire
fence, Fischi said.
S2: After the other inmate fell, Selenski used the mattress
to scale a 10-foot, razor-wire fence, Fischi said.
To address this problem, when we consider a
pair of strings (s1, s2), if s1 is longer than s2, we
obtain all of the substrings s01 of s1 that have the
same length as s2. Then, for each s01, we compute
the nine values fj(s01, s2), where fj (1 G j G 9)
are the string similarity measures. Finally, we lo-
cate the s01 with the best average similarity (over
all similarity measures) to s2, namely s0∗1 :
</bodyText>
<equation confidence="0.997898">
s0∗1 = arg max
s�
1
</equation>
<bodyText confidence="0.984402">
and we keep the nine fj(s0∗1 , s2) values and their
average as ten additional measurements. Simi-
larly, if s2 is longer than s1, we keep the nine
fj(s1, s0∗2 ) values and their average. This process
is applied to pairs (si, s1 ), ..., (s4 1, s4 ), where
2 2
large length differences are more likely to appear,
adding 40 more measurements (features) to the
vector v� of each (S1, S2) pair of input strings.
The measurements discussed above provide 130
numeric features.5 To those, we add two Boolean
features indicating the existence or absence of
negation in S1 or S2, respectively; negation is de-
tected by looking for words like “not”, “won’t”
etc. Finally, we add a length ratio feature, de-
fined as min(LS1,LS2)
max(LS1,LS2), where LS1 and LS2 are the
lengths, in tokens, of S1 and S2. Hence, there is a
total of 133 available features in INIT.
</bodyText>
<subsectionHeader confidence="0.997066">
2.2 Method 2 (INIT+WN)
</subsectionHeader>
<bodyText confidence="0.9998745">
Paraphrasing may involve using synonyms which
cannot be detected by the features we have con-
sidered so far. In the following pair of sentences,
for example, “dispatched” is used as a synonym
</bodyText>
<footnote confidence="0.5528655">
5All feature values are normalized in [−1, 1]. We use our
own implementation of the string similarity measures.
</footnote>
<bodyText confidence="0.9607164">
of “sent”; treating the two verbs as the same to-
ken during the calculation of the string similarity
measures would yield a higher similarity. The sec-
ond method, called INIT+WN, treats words from
S1 and S2 that are synonyms as identical; other-
wise the method is the same as INIT.
S1: Fewer than a dozen FBI agents were dispatched to se-
cure and analyze evidence.
S2: Fewer than a dozen FBI agents will be sent to Iraq to
secure and analyze evidence of the bombing.
</bodyText>
<subsectionHeader confidence="0.997934">
2.3 Method 3 (INIT+WN+DEP)
</subsectionHeader>
<bodyText confidence="0.999993111111111">
The features of the previous two methods op-
erate at the lexical level. The third method,
called INIT+WN+DEP, adds features that operate
on the grammatical relations (dependencies) a de-
pendency grammar parser returns for S1 and S2.
We use three measures to calculate similarity at
the level of grammatical relations, namely S1 de-
pendency recall (R1), S2 dependency recall (R2)
and their F-measure (FR1,R2), defined below:
</bodyText>
<equation confidence="0.999715285714286">
R1 = |common dependencies|
|S1 dependencies|
mmon
R2 = co|S2 dependences|
ies|
2·R1·R2
FR1,R2 = R1+R2
</equation>
<bodyText confidence="0.998915555555556">
The following two examples illustrate the use-
fulness of dependency similarity measures in de-
tecting paraphrases. In the first example S1 and S2
are not paraphrases and the scores are low, while in
the second example where S1 and S2 have almost
identical meanings, the scores are much higher.
Figures 1 and 2 lists the grammatical relations (de-
pendencies) of the two sentences with the common
ones shown in bold.
</bodyText>
<equation confidence="0.383883">
Example 1:
</equation>
<bodyText confidence="0.9919525">
S1: Gyorgy Heizler, head of the local disaster unit, said the
coach was carrying 38 passengers.
S2: The head of the local disaster unit, Gyorgy Heizler, said
the coach driver had failed to heed red stop lights.
</bodyText>
<equation confidence="0.847171">
R1 = 0.43, R2 = 0.32, FR1,R2 = 0.36
Example 2:
</equation>
<listItem confidence="0.96643475">
S1: Amrozi accused his brother, whom he called “the wit-
ness”, of deliberately distorting his evidence.
S2: Referring to him as only “the witness”, Amrozi accused
his brother of deliberately distorting his evidence.
</listItem>
<equation confidence="0.98875475">
R1 = 0.69, R2 = 0.6, FR1,R2 = 0.64
10
E fj(s01, s2)
j=1
</equation>
<page confidence="0.997539">
29
</page>
<tableCaption confidence="0.987812558823529">
Grammatical relations of S1 Grammatical relations of S2
mod(Heizler-2, Gyorgy-1)
arg(said-11, Heizler-2)
mod(Heizler-2, head-4)
mod(head-4, of-5)
mod(unit-9, the-6)
mod(unit-9, local-7)
mod(unit-9, disaster-8)
arg(of-5, unit-9)
mod(coach-13, the-12)
arg(carrying-15, coach-13)
aux(carrying-15, was-14)
arg(said-11, carrying-15)
mod(passengers-17, 38-16)
arg(carrying-15, passengers-17)
mod(head-2, The-1)
arg(said-12, head-2)
mod(head-2, of-3)
mod(unit-7, the-4)
mod(unit-7, local-5)
mod(unit-7, disaster-6)
arg(of-3, unit-7)
mod(Heizler-10, Gyorgy-9)
mod(unit-7, Heizler-10)
mod(driver-15, the-13)
mod(driver-15, coach-14)
arg(failed-17, driver-15)
aux(failed-17, had-16)
arg(said-12, failed-17)
aux(heed-19, to-18)
arg(failed-17, heed-19)
mod(lights-22, red-20)
mod(lights-22, stop-21)
arg(heed-19, lights-22)
</tableCaption>
<figureCaption confidence="0.893854">
Figure 1: Grammatical relations of example 1.
</figureCaption>
<tableCaption confidence="0.909032103448276">
Grammatical relations of S1 Grammatical relations of S2
arg(accused-2, Amrozi-1)
mod(brother-4, his-3)
arg(accused-2, brother-4)
arg(called-8, whom-6)
arg(called-8, he-7)
mod(brother-4, called-8)
mod(witness-11, the-10)
dep(called-8, witness-11)
mod(brother-4, of-14)
mod(distorting-16, deliberately-15)
arg(of-14, distorting-16)
mod(evidence-18, his-17)
arg(distorting-16, evidence-18)
dep(accused-12, Referring-1)
mod(Referring-1, to-2)
arg(to-2, him-3)
cc(him-3, as-4)
dep(as-4, only-5)
mod(witness-8, the-7)
conj(him-3, witness-8)
arg(accused-12, Amrozi-11)
mod(brother-14, his-13)
arg(accused-12, brother-14)
mod(brother-14, of-15)
mod(distorting-17, deliberately-16)
arg(of-15, distorting-17)
mod(evidence-19, his-18)
arg(distorting-17, evidence-19)
</tableCaption>
<figureCaption confidence="0.933068">
Figure 2: Grammatical relations of example 2.
</figureCaption>
<page confidence="0.989362">
30
</page>
<bodyText confidence="0.999987857142857">
As with POS-tags, we use only the highest level
of the tags of the grammatical relations, which in-
creases the similarity of positive pairs of 51 and
52. For the same reason, we ignore the direction-
ality of the dependency arcs which we have found
to improve the results. INIT+WN+DEP employs a
total of 136 features.
</bodyText>
<subsectionHeader confidence="0.995915">
2.4 Feature selection
</subsectionHeader>
<bodyText confidence="0.999982027777778">
Larger feature sets do not necessarily lead to im-
proved classification performance. Despite seem-
ing useful, some features may in fact be too noisy
or irrelevant, increasing the risk of overfitting the
training data. Some features may also be redun-
dant, given other features; thus, feature selection
methods that consider the value of each feature on
its own (e.g., information gain) may lead to sub-
optimal feature sets.
Finding the best subset of a set of available fea-
tures is a search space problem for which several
methods have been proposed (Guyon et al., 2006).
We have experimented with a wrapper approach,
whereby each feature subset is evaluated accord-
ing to the predictive power of a classifier (treated
as a black box) that uses the subset; in our exper-
iments, the predictive power was measured as F-
measure (defined below, not to be confused with
FR1,R2). More precisely, during feature selection,
for each feature subset we performed 10-fold cross
validation on the training data to evaluate its pre-
dictive power. After feature selection, the classi-
fier was trained on all the training data, and it was
evaluated on separate test data.
With large feature sets, an exhaustive search
over all subsets is intractable. Instead, we ex-
perimented with forward hill-climbing and beam
search (Guyon et al., 2006). Forward hill-climbing
starts with an empty feature set, to which it adds
features, one at a time, by preferring to add at each
step the feature that leads to the highest predic-
tive power. Forward beam search is similar, except
that the search frontier contains the k best exam-
ined states (feature subsets) at each time; we used
k = 10. For k = 1, beam search reduces to hill-
climbing.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999972">
We now present our experiments, starting from a
description of the datasets used.
</bodyText>
<subsectionHeader confidence="0.992238">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999118238095238">
We mainly used the Microsoft Research (MSR)
Paraphrasing Corpus (Dolan et al., 2004), which
consists of 5,801 pairs of sentences. Each pair
is manually annotated by two human judges as a
true or false paraphrase; a third judge resolved dis-
agreements. The data are split into 4,076 training
pairs and 1,725 testing pairs.
We have experimented with a dataset we created
from the MTC corpus. MTC is a corpus containing
news articles in Mandarin Chinese; for each article
11 English translations (by different translators)
are also provided. We considered the translations
of the same Chinese sentence as paraphrases. We
obtained all the possible paraphrase pairs and we
added an equal number of randomly selected non
paraphrase pairs, which contained sentences that
were not translations of the same sentence. In this
way, we constructed a dataset containing 82,260
pairs of sentences. The dataset was then split in
training (70%) and test (30%) parts, with an equal
number of positive and negative pairs in each part.
</bodyText>
<subsectionHeader confidence="0.999878">
3.2 Evaluation measures and baseline
</subsectionHeader>
<bodyText confidence="0.969811291666667">
We used four evaluation measures, namely accu-
racy (correctly classified pairs over all pairs), pre-
cision (P, pairs correctly classified in the positive
class over all pairs classified in the positive class),
recall (R, pairs correctly classified in the positive
class over all true positive pairs), and F-measure
(with equal weight on precision and recall, defined
as 2·P ·R
P +R ). These measures are not to be confused
with the R1, R2, and FR1,R2 of section 2.3 which
are used as features.
A reasonable baseline method (BASE) is to use
just the edit distance similarity measure and a
threshold in order to decide whether two phrases
are paraphrases or not. The threshold is chosen
using a grid search utility and 10-fold cross vali-
dation on the training data. More precisely, in a
first step we search the range [-1, 1] with a step
of 0.1.6 In each step, we perform 10-fold cross
validation and the value that achieves the best F-
measure is our initial threshold, th, for the second
step. In the second step, we perform the same pro-
cedure in the range [th - 0.1, th + 0.1] and with a
step of 0.001.
</bodyText>
<footnote confidence="0.898931">
6Recall that we normalize similarity in [-1, 1].
</footnote>
<page confidence="0.999816">
31
</page>
<subsectionHeader confidence="0.989871">
3.3 Experimental results
</subsectionHeader>
<bodyText confidence="0.99897100990099">
With both datasets, we experimented with a Max-
imum Entropy (ME) classifier. However, prelim-
inary results (see table 1) showed that our MTC
dataset is very easy. BASE achieves approximately
95% in accuracy and F-measure, and an approx-
imate performance of 99.5% in all measures (ac-
curacy, precision, recall, F-measure) is achieved
by using ME and only some of the features of
INIT (we use 36 features corresponding to pairs
(sig s2), (s21g s2), (sig s2), (sig s2) plus the two
negation features). Therefore, we did not experi-
ment with the MTC dataset any further.
Table 2 (upper part) lists the results of our ex-
periments on the MSR corpus. We optionally per-
formed feature selection with both forward hill-
climbing (FHC) and forward beam search (FBS).
All of our methods clearly perform better than
BASE. As one might expect, there is a lot of re-
dundancy in the complete feature set. Hence, the
two feature selection methods (FHC and FBS) lead
to competitive results with much fewer features (7
and 10, respectively, instead of 136). However,
feature selection deteriorates performance, espe-
cially accuracy, i.e., the full feature set is better,
despite its redundancy. Table 2 also includes all
other reported results for the MSR corpus that we
are aware of; we are not aware of the exact number
of features used by the other researchers.
It is noteworthy that INIT achieves state of the
art performance, even though the other approaches
use many more linguistic resources. For example,
Wan et al.’s approach (Wan et al., 2006), which
achieved the best previously reported results, is
similar to ours, in that it also trains a classifier with
similarity measures; but some of Wan et al.’s mea-
sures require a dependency grammar parser, unlike
INIT. More precisely, for each pair of sentences,
Wan et al. construct a feature vector with values
that measure lexical and dependency similarities.
The measures are: word overlap, length difference
(in words), BLEU (Papineni et al., 2002), depen-
dency relation overlap (i.e., R1 and R2 but not
FR1,R2), and dependency tree edit distance. The
measures are also applied on sequences containing
the lemmatized words of the original sentences,
similarly to one of our levels of abstraction. Inter-
estingly, INIT achieves the same (and slightly bet-
ter) accuracy as Wan et al.’s system, without em-
ploying any parsing. Our more enhanced methods,
INIT+WN and INIT+WN+DEP, achieve even better
results.
Zhang and Patrick (2005) use a dependency
grammar parser to convert passive voice phrases
to active voice ones. They also use a preprocess-
ing stage to generalize the pairs of sentences. The
preprocessing replaces dates, times, percentages,
etc. with generic tags, something that we have also
done in the MSR corpus, but it also replaces words
and phrases indicating future actions (e.g., “plans
to”, “be expected to”) with the word “will”; the
latter is an example of further preprocessing that
could be added to our system. After the prepro-
cessing, Zhang and Patrick create for each sen-
tence pair a feature vector whose values measure
the lexical similarity between the two sentences;
they appear to be using the maximum number of
consecutive common words, the number of com-
mon words, edit distance (in words), and modi-
fied n-gram precision, a measure similar to BLEU.
The produced vectors are then used to train a de-
cision tree classifier. Hence, Zhang and Patrick’s
approach is similar to ours, but we use more and
different similarity measures and several levels of
abstraction of the two sentences. We also use ME,
along with a wrapper approach to feature selec-
tion, rather than decision tree induction and its em-
bedded information gain-based feature selection.
Furthermore, all of our methods, even INIT which
employs no parsing at all, achieve better results
compared to Zhang and Patrick’s.
Qiu et al. (2006) first convert the sentences into
tuples using parsing and semantic role labeling.
They then match similar tuples across the two sen-
tences, and use an SVM (Vapnik, 1998) classifier to
decide whether or not the tuples that have not been
matched are important or not. If not, the sentences
are paraphrases. Despite using a parser and a se-
mantic role identifier, Qiu et al.’s system performs
worse than our methods.
Finally, Finch et al.’s system (2005) achieved
the second best overall results by employing POS
tagging, synonymy resolution, and an SVM. In-
terestingly, the features of the SVM correspond
to machine translation evaluation metrics, rather
than string similarity measures, unlike our system.
We plan to examine further how the features of
Finch et al. and other ideas from machine trans-
lation can be embedded in our system, although
INIT+WN+DEP outperforms Finch et al.’s system.
Interestingly, even when not using more resources
than Finch et al. as in methods INIT and INIT+WN
</bodyText>
<page confidence="0.996038">
32
</page>
<table confidence="0.995984333333333">
method features accuracy precision recall F-measure
BASE – 95.30 98.16 92.32 95.15
INIT’ 38 99.62 99.50 99.75 99.62
</table>
<tableCaption confidence="0.996705">
Table 1: Results (%) of our methods on our MTC dataset.
</tableCaption>
<table confidence="0.999461909090909">
method features accuracy precision recall F-measure
BASE 1 69.04 72.42 86.31 78.76
INIT 133 75.19 78.51 86.31 82.23
INIT+WN 133 75.48 78.91 86.14 82.37
INIT+WN+DEP 136 76.17 79.35 86.75 82.88
INIT+WN+DEP + FHC 7 73.86 75.14 90.67 82.18
INIT+WN+DEP + FBS 10 73.68 73.68 93.98 82.61
Finch et al. – 74.96 76.58 89.80 82.66
Qiu et al. – 72.00 72.50 93.40 81.60
Wan et al. – 75.00 77.00 90.00 83.00
Zhang &amp; Patrick – 71.90 74.30 88.20 80.70
</table>
<tableCaption confidence="0.836641">
Table 2: Results (%) of our methods (upper part) and other methods (lower part) on the MSR corpus.
we achieve similar or better accuracy results.
</tableCaption>
<sectionHeader confidence="0.999456" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.999981040000001">
We have already made the distinction between
paraphrase (and textual entailment) generators vs.
recognizers, and we have pointed out that rec-
ognizers can be embedded in generators as fil-
ters. The latter is particularly useful in bootstrap-
ping paraphrase generation approaches (Riloff
and Jones, 1999; Barzilay and McKeown, 2001;
Ravichandran and Hovy, 2001; Ravichandran et
al., 2003; Duclaye et al., 2003; Szpektor et al.,
2004), which are typically given seed pairs of
named entities for which a particular relation
holds; the system locates in a document collec-
tion (or the entire Web) contexts were the seeds
cooccur, and uses the contexts as patterns that can
express the relation; the patterns are then used to
locate new named entities that satisfy the relation,
and a new iteration begins. A paraphrase recog-
nizer could be used to filter out erroneous gener-
ated paraphrases between iterations.
Another well known paraphrase generator is Lin
and Pantel’s (2001) DIRT, which produces slotted
semantically equivalent patterns (e.g., “X is the
writer of Y ” q “X wrote Y ” q “Y was writ-
ten by X” q “X is the author of Y ”), based
on the assumption that different paths of depen-
dency trees (obtained from a corpus) that occur
frequently with the same words (slot fillers) at
their ends are often paraphrases. An extension of
DIRT, named LEDIR, has also been proposed (Bha-
gat et al., 2007) to recognize directional textual
entailment rules (e.g., “Y was written by X” ⇒
“Y is the work of X”). Ibrahim et al.’s (2003)
method is similar to DIRT, but it uses only de-
pendency grammar paths from aligned sentences
(from a parallel corpus) that share compatible an-
chors (e.g., identical strings, or entity names of the
same semantic category). Shinyama and Sekine
(2003) adopt a very similar approach.
In another generation approach, Barzilay and
Lee (2002; 2003) look for pairs of slotted word
lattices that share many common slot fillers; the
lattices are generated by applying a multiple-
sequence alignment algorithm to a corpus of mul-
tiple news articles about the same events. Finally,
Pang et al. (2003) create finite state automata by
merging parse trees of aligned sentences from a
parallel corpus; in each automaton, different paths
represent paraphrases. Again, a paraphrase recog-
nizer could be embedded in all of these methods,
to filter out erroneous generated patterns.
</bodyText>
<sectionHeader confidence="0.995539" genericHeader="conclusions">
5 Conclusions and further work
</sectionHeader>
<bodyText confidence="0.999822615384615">
We have presented three methods (INIT, INIT+WN,
INIT+WN+DEP) that recognize paraphrases given
pairs of sentences. These methods employ nine
string similarity measures applied to ten shallow
abstractions of the input sentences. Moreover,
INIT+WN and INIT+WN+DEP exploit WordNet for
synonymy resolution, and INIT+WN+DEP uses ad-
ditional features that measure grammatical rela-
tion similarity. Supervised machine learning is
used to learn how to combine the resulting fea-
tures. We experimented with a Maximum Entropy
classifier on two datasets; the publicly available
MSR corpus and one that we constructed from the
</bodyText>
<page confidence="0.997013">
33
</page>
<bodyText confidence="0.999932423076923">
MTC corpus. However, the latter was found to be
very easy, and consequently we mainly focused on
the MSR corpus.
On the MSR corpus, all of our methods achieved
similar or better performance than the sate of the
art, even INIT, despite the fact that it uses fewer
linguistic resources. Hence, INIT may have prac-
tical advantages in less spoken languages, which
have limited resources. The most elaborate of
our methods, INIT+WN+DEP, achieved the best re-
sults, but it requires WordNet and a reliable depen-
dency grammar parser. Feature selection experi-
ments indicate that there is significant redundancy
in our feature set, though the full feature set leads
to better performance than the subsets produced
by feature selection. Further improvements may
be possible by including in our system additional
features, such as BLEU scores or features for word
alignment.
Our long-term goal is to embed our recognizer
in a bootstrapping paraphrase generator, to filter
out erroneous paraphrases between bootstrapping
iterations. We hope that our recognizer will be ad-
equate for this purpose, possibly in combination
with a human in the loop, who will inspect para-
phrases the recognizer is uncertain of.
</bodyText>
<sectionHeader confidence="0.996922" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999265">
This work was funded by the Greek PENED 2003
programme, which is co-funded by the European
Union (80%), and the Greek General Secretariat
for Research and Technology (20%).
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999914279411765">
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Gi-
ampiccolo, B. Magnini, and I. Szpektor. 2006. The
2nd PASCAL recognising textual entailment chal-
lenge. In Proceedings of the 2nd PASCAL Chal-
lenges Workshop on Recognising Textual Entail-
ment, Venice, Italy.
R. Barzilay and L. Lee. 2002. Bootstrapping lexi-
cal choice via multiple-sequence alignment. In Pro-
ceedings of EMNLP, pages 164–171, Philadelphia,
PA.
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
an unsupervised approach using multiple-sequence
alignment. In Proceedings of HLT-NAACL, pages
16–23, Edmonton, Canada.
R. Barzilay and K. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In Proceedings of
ACL/EACL, pages 50–57, Toulouse, France.
R. Bhagat, P. Pantel, and E. Hovy. 2007. LEDIR:
An unsupervised algorithm for learning directional-
ity of inference rules. In Proceedings of the EMNLP-
CONLL, pages 161–170.
I. Dagan, O. Glickman, and B. Magnini. 2006. The
PASCAL recognising textual entailment challenge.
In Qui˜nonero-Candela et al., editor, LNAI, volume
3904, pages 177–190. Springer-Verlag.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of COLING, page 350, Morristown, NJ.
F. Duclaye, F. Yvon, and O. Collin. 2003. Learning
paraphrases to improve a question-answering sys-
tem. In Proceedings of the EACL Workshop on Nat-
ural Language Processing for Question Answering
Systems, pages 35–41, Budapest, Hungary.
A. Finch, Y. S. Hwang, and E. Sumita. 2005. Using
machine translation evaluation techniques to deter-
mine sentence-level semantic equivalence. In Pro-
ceedings of the 3rd International Workshop on Para-
phrasing, Jeju Island, Korea.
D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan.
2007. The third Pascal recognizing textual entail-
ment challenge. In Proceedings of the ACL-Pascal
Workshop on Textual Entailment and Paraphrasing,
pages 1–9, Prague, Czech Republic.
I. J. Good. 1963. Maximum entropy for hypothesis
formulation, especially for multidimentional conti-
gency tables. Annals of Mathematical Statistics,
34:911–934.
I.M. Guyon, S.R. Gunn, M. Nikravesh, and L. Zadeh,
editors. 2006. Feature Extraction, Foundations and
Applications. Springer.
S. Harabagiu and A. Hickl. 2006. Methods for using
textual entailment in open-domain question answer-
ing. In Proceedings of COLING-ACL, pages 905–
912, Sydney, Australia.
S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca.
2003. Open-domain textual question answer-
ing techniques. Natural Language Engineering,
9(3):231–267.
A. Ibrahim, B. Katz, and J. Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual
corpora. In Proceedings of the ACL Workshop on
Paraphrasing, pages 57–64, Sapporo, Japan.
E. T. Jaynes. 1957. Information theory and statistical
mechanics. Physical Review, 106:620–630.
D. Lin and P. Pantel. 2001. Discovery of inference
rules for question answering. Natural Language En-
gineering, 7:343–360.
</reference>
<page confidence="0.990499">
34
</page>
<reference confidence="0.999802">
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 42–47, Prague, June. Association for Compu-
tational Linguistics.
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-
based alignment of multiple translations: extracting
paraphrases and generating new sentences. In Pro-
ceedings of HLT-NAACL, pages 102–109, Edmon-
ton, Canada.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311–318,
Philadelphia, Pennsylvania.
L. Qiu, M. Y. Kan, and T.S. Chua. 2006. Paraphrase
recognition via dissimilarity significance classifica-
tion. In Proceedings of EMNLP, pages 18–26, Syd-
ney, Australia.
D. Ravichandran and E. Hovy. 2001. Learning surface
text patterns for a question answering system. In
Proceedings of ACL, pages 41–47, Philadelphia, PA.
D. Ravichandran, A. Ittycheriah, and S. Roukos. 2003.
Automatic derivation of surface text patterns for a
maximum entropy based question answering sys-
tem. In Proceedings of HLT-NAACL, pages 85–87,
Edmonton, Canada.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of AAAI, pages 474–479, Orlando,
FL.
Y. Shinyama and S. Sekine. 2003. Paraphrase ac-
quisition for information extraction. In Proceed-
ings of the ACL Workshop on Paraphrasing, Sap-
poro, Japan.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling Web-based acquisition of entailment rela-
tions. In Proceedings of EMNLP, Barcelona, Spain.
V. Vapnik. 1998. Statistical learning theory. John
Wiley.
S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Us-
ing dependency-based features to take the “para-
farce” out of paraphrase. In Proceedings of the Aus-
tralasian Language Technology Workshop, pages
131–138, Sydney, Australia.
Y. Zhang and J. Patrick. 2005. Paraphrase identifi-
cation by text canonicalization. In Proceedings of
the Australasian Language Technology Workshop,
pages 160–166, Sydney, Australia.
</reference>
<page confidence="0.999326">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.691196">
<title confidence="0.998734">Paraphrase Recognition Using Machine Learning to Combine Similarity Measures</title>
<author confidence="0.759836">Prodromos Malakasiotis</author>
<affiliation confidence="0.9950475">Department of Informatics Athens University of Economics and Business</affiliation>
<address confidence="0.929629">Patission 76, GR-104 34 Athens, Greece</address>
<abstract confidence="0.998634066666667">This paper presents three methods that can be used to recognize paraphrases. They all employ string similarity measures applied to shallow abstractions of the input sentences, and a Maximum Entropy classifier to learn how to combine the resulting features. Two of the methods also exploit WordNet to detect synonyms and one of them also exploits a dependency parser. experiment on two datasets, the paraphrasing corpus and a dataset that we created from the corpus. Our system achieves state of the art or better results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bar-Haim</author>
<author>I Dagan</author>
<author>B Dolan</author>
<author>L Ferro</author>
<author>D Giampiccolo</author>
<author>B Magnini</author>
<author>I Szpektor</author>
</authors>
<title>The 2nd PASCAL recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2nd PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="2843" citStr="Bar-Haim et al., 2006" startWordPosition="455" endWordPosition="458">ng the question into a statement) matches any patterns extracted from candidate answers. As a further example, in text summarization, especially multi-document summarization, a paraphrase recognizer can be used to check if a sentence is a paraphrase of any other sentence already present in a partially constructed summary. Note that, although “paraphrasing” and “textual entailment” are sometimes used as synonyms, we use the former to refer to methods that generate or recognize semantically equivalent (or almost equivalent) phrases or patterns, whereas in textual entailment (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) the expressions or patterns are not necessarily semantically equivalent; it suffices if one entails the other, even if the reverse direction does not hold. For example, “Y was written by X” textually entails “Y is the work of X”, but the reverse direction does not necessarily hold (e.g., if Y is a statue); hence, the two sentences are not paraphrases. In this paper, we focus on paraphrase recognition. We propose three methods that employ string similarity measures, which are applied to several abstractions of a pair of input phrases (e.g., the phrases themselves, th</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, B. Magnini, and I. Szpektor. 2006. The 2nd PASCAL recognising textual entailment challenge. In Proceedings of the 2nd PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Bootstrapping lexical choice via multiple-sequence alignment.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>164--171</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="26236" citStr="Barzilay and Lee (2002" startWordPosition="4311" endWordPosition="4314">requently with the same words (slot fillers) at their ends are often paraphrases. An extension of DIRT, named LEDIR, has also been proposed (Bhagat et al., 2007) to recognize directional textual entailment rules (e.g., “Y was written by X” ⇒ “Y is the work of X”). Ibrahim et al.’s (2003) method is similar to DIRT, but it uses only dependency grammar paths from aligned sentences (from a parallel corpus) that share compatible anchors (e.g., identical strings, or entity names of the same semantic category). Shinyama and Sekine (2003) adopt a very similar approach. In another generation approach, Barzilay and Lee (2002; 2003) look for pairs of slotted word lattices that share many common slot fillers; the lattices are generated by applying a multiplesequence alignment algorithm to a corpus of multiple news articles about the same events. Finally, Pang et al. (2003) create finite state automata by merging parse trees of aligned sentences from a parallel corpus; in each automaton, different paths represent paraphrases. Again, a paraphrase recognizer could be embedded in all of these methods, to filter out erroneous generated patterns. 5 Conclusions and further work We have presented three methods (INIT, INIT+</context>
</contexts>
<marker>Barzilay, Lee, 2002</marker>
<rawString>R. Barzilay and L. Lee. 2002. Bootstrapping lexical choice via multiple-sequence alignment. In Proceedings of EMNLP, pages 164–171, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Learning to paraphrase: an unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>16--23</pages>
<location>Edmonton, Canada.</location>
<marker>Barzilay, Lee, 2003</marker>
<rawString>R. Barzilay and L. Lee. 2003. Learning to paraphrase: an unsupervised approach using multiple-sequence alignment. In Proceedings of HLT-NAACL, pages 16–23, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL/EACL,</booktitle>
<pages>50--57</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="24711" citStr="Barzilay and McKeown, 2001" startWordPosition="4051" endWordPosition="4054">.96 76.58 89.80 82.66 Qiu et al. – 72.00 72.50 93.40 81.60 Wan et al. – 75.00 77.00 90.00 83.00 Zhang &amp; Patrick – 71.90 74.30 88.20 80.70 Table 2: Results (%) of our methods (upper part) and other methods (lower part) on the MSR corpus. we achieve similar or better accuracy results. 4 Related work We have already made the distinction between paraphrase (and textual entailment) generators vs. recognizers, and we have pointed out that recognizers can be embedded in generators as filters. The latter is particularly useful in bootstrapping paraphrase generation approaches (Riloff and Jones, 1999; Barzilay and McKeown, 2001; Ravichandran and Hovy, 2001; Ravichandran et al., 2003; Duclaye et al., 2003; Szpektor et al., 2004), which are typically given seed pairs of named entities for which a particular relation holds; the system locates in a document collection (or the entire Web) contexts were the seeds cooccur, and uses the contexts as patterns that can express the relation; the patterns are then used to locate new named entities that satisfy the relation, and a new iteration begins. A paraphrase recognizer could be used to filter out erroneous generated paraphrases between iterations. Another well known paraph</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>R. Barzilay and K. McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of ACL/EACL, pages 50–57, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bhagat</author>
<author>P Pantel</author>
<author>E Hovy</author>
</authors>
<title>LEDIR: An unsupervised algorithm for learning directionality of inference rules.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLPCONLL,</booktitle>
<pages>161--170</pages>
<contexts>
<context position="25775" citStr="Bhagat et al., 2007" startWordPosition="4234" endWordPosition="4238">nd a new iteration begins. A paraphrase recognizer could be used to filter out erroneous generated paraphrases between iterations. Another well known paraphrase generator is Lin and Pantel’s (2001) DIRT, which produces slotted semantically equivalent patterns (e.g., “X is the writer of Y ” q “X wrote Y ” q “Y was written by X” q “X is the author of Y ”), based on the assumption that different paths of dependency trees (obtained from a corpus) that occur frequently with the same words (slot fillers) at their ends are often paraphrases. An extension of DIRT, named LEDIR, has also been proposed (Bhagat et al., 2007) to recognize directional textual entailment rules (e.g., “Y was written by X” ⇒ “Y is the work of X”). Ibrahim et al.’s (2003) method is similar to DIRT, but it uses only dependency grammar paths from aligned sentences (from a parallel corpus) that share compatible anchors (e.g., identical strings, or entity names of the same semantic category). Shinyama and Sekine (2003) adopt a very similar approach. In another generation approach, Barzilay and Lee (2002; 2003) look for pairs of slotted word lattices that share many common slot fillers; the lattices are generated by applying a multipleseque</context>
</contexts>
<marker>Bhagat, Pantel, Hovy, 2007</marker>
<rawString>R. Bhagat, P. Pantel, and E. Hovy. 2007. LEDIR: An unsupervised algorithm for learning directionality of inference rules. In Proceedings of the EMNLPCONLL, pages 161–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2006</date>
<volume>3904</volume>
<pages>177--190</pages>
<editor>In Qui˜nonero-Candela et al., editor, LNAI,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="2820" citStr="Dagan et al., 2006" startWordPosition="451" endWordPosition="454">categories and turning the question into a statement) matches any patterns extracted from candidate answers. As a further example, in text summarization, especially multi-document summarization, a paraphrase recognizer can be used to check if a sentence is a paraphrase of any other sentence already present in a partially constructed summary. Note that, although “paraphrasing” and “textual entailment” are sometimes used as synonyms, we use the former to refer to methods that generate or recognize semantically equivalent (or almost equivalent) phrases or patterns, whereas in textual entailment (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) the expressions or patterns are not necessarily semantically equivalent; it suffices if one entails the other, even if the reverse direction does not hold. For example, “Y was written by X” textually entails “Y is the work of X”, but the reverse direction does not necessarily hold (e.g., if Y is a statue); hence, the two sentences are not paraphrases. In this paper, we focus on paraphrase recognition. We propose three methods that employ string similarity measures, which are applied to several abstractions of a pair of input phrases (e.g., the</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini. 2006. The PASCAL recognising textual entailment challenge. In Qui˜nonero-Candela et al., editor, LNAI, volume 3904, pages 177–190. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dolan</author>
<author>C Quirk</author>
<author>C Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>350</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="4109" citStr="Dolan et al., 2004" startWordPosition="660" endWordPosition="663">e similarity measures are used as features in a Maximum Entropy (ME) classifier (Jaynes, 1957; Good, 1963), which learns to separate true paraphrase pairs from false ones. Two of our methods also exploit WordNet to detect synonyms, and one of them uses additional features to measure similarities of grammatical relations 27 Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 27–35, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP obtained by a dependency parser.1 Our experiments were conducted on two datasets: the publicly available Microsoft Research Paraphrasing corpus (Dolan et al., 2004) and a dataset that we constructed from the MTC corpus.2 The experimental results show that our methods perform very well. Even the simplest one manages to achieve state of the art results, even though it uses fewer linguistic resources than other reported systems. The other two, more elaborate methods perform even better. Section 2 presents the three methods, and section 3 our experiments. Section 4 covers related work. Section 5 concludes and proposes further work. 2 The three methods The main idea underlying our methods is that by capturing similarities at various shallow abstractions of th</context>
<context position="16627" citStr="Dolan et al., 2004" startWordPosition="2706" endWordPosition="2709">ing and beam search (Guyon et al., 2006). Forward hill-climbing starts with an empty feature set, to which it adds features, one at a time, by preferring to add at each step the feature that leads to the highest predictive power. Forward beam search is similar, except that the search frontier contains the k best examined states (feature subsets) at each time; we used k = 10. For k = 1, beam search reduces to hillclimbing. 3 Experiments We now present our experiments, starting from a description of the datasets used. 3.1 Datasets We mainly used the Microsoft Research (MSR) Paraphrasing Corpus (Dolan et al., 2004), which consists of 5,801 pairs of sentences. Each pair is manually annotated by two human judges as a true or false paraphrase; a third judge resolved disagreements. The data are split into 4,076 training pairs and 1,725 testing pairs. We have experimented with a dataset we created from the MTC corpus. MTC is a corpus containing news articles in Mandarin Chinese; for each article 11 English translations (by different translators) are also provided. We considered the translations of the same Chinese sentence as paraphrases. We obtained all the possible paraphrase pairs and we added an equal nu</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>B. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In Proceedings of COLING, page 350, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Duclaye</author>
<author>F Yvon</author>
<author>O Collin</author>
</authors>
<title>Learning paraphrases to improve a question-answering system.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL Workshop on Natural Language Processing for Question Answering Systems,</booktitle>
<pages>35--41</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="24789" citStr="Duclaye et al., 2003" startWordPosition="4063" endWordPosition="4066">90.00 83.00 Zhang &amp; Patrick – 71.90 74.30 88.20 80.70 Table 2: Results (%) of our methods (upper part) and other methods (lower part) on the MSR corpus. we achieve similar or better accuracy results. 4 Related work We have already made the distinction between paraphrase (and textual entailment) generators vs. recognizers, and we have pointed out that recognizers can be embedded in generators as filters. The latter is particularly useful in bootstrapping paraphrase generation approaches (Riloff and Jones, 1999; Barzilay and McKeown, 2001; Ravichandran and Hovy, 2001; Ravichandran et al., 2003; Duclaye et al., 2003; Szpektor et al., 2004), which are typically given seed pairs of named entities for which a particular relation holds; the system locates in a document collection (or the entire Web) contexts were the seeds cooccur, and uses the contexts as patterns that can express the relation; the patterns are then used to locate new named entities that satisfy the relation, and a new iteration begins. A paraphrase recognizer could be used to filter out erroneous generated paraphrases between iterations. Another well known paraphrase generator is Lin and Pantel’s (2001) DIRT, which produces slotted semanti</context>
</contexts>
<marker>Duclaye, Yvon, Collin, 2003</marker>
<rawString>F. Duclaye, F. Yvon, and O. Collin. 2003. Learning paraphrases to improve a question-answering system. In Proceedings of the EACL Workshop on Natural Language Processing for Question Answering Systems, pages 35–41, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Finch</author>
<author>Y S Hwang</author>
<author>E Sumita</author>
</authors>
<title>Using machine translation evaluation techniques to determine sentence-level semantic equivalence.</title>
<date>2005</date>
<booktitle>In Proceedings of the 3rd International Workshop on Paraphrasing, Jeju Island,</booktitle>
<marker>Finch, Hwang, Sumita, 2005</marker>
<rawString>A. Finch, Y. S. Hwang, and E. Sumita. 2005. Using machine translation evaluation techniques to determine sentence-level semantic equivalence. In Proceedings of the 3rd International Workshop on Paraphrasing, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Giampiccolo</author>
<author>B Magnini</author>
<author>I Dagan</author>
<author>B Dolan</author>
</authors>
<title>The third Pascal recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-Pascal Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>1--9</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2870" citStr="Giampiccolo et al., 2007" startWordPosition="459" endWordPosition="462">statement) matches any patterns extracted from candidate answers. As a further example, in text summarization, especially multi-document summarization, a paraphrase recognizer can be used to check if a sentence is a paraphrase of any other sentence already present in a partially constructed summary. Note that, although “paraphrasing” and “textual entailment” are sometimes used as synonyms, we use the former to refer to methods that generate or recognize semantically equivalent (or almost equivalent) phrases or patterns, whereas in textual entailment (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) the expressions or patterns are not necessarily semantically equivalent; it suffices if one entails the other, even if the reverse direction does not hold. For example, “Y was written by X” textually entails “Y is the work of X”, but the reverse direction does not necessarily hold (e.g., if Y is a statue); hence, the two sentences are not paraphrases. In this paper, we focus on paraphrase recognition. We propose three methods that employ string similarity measures, which are applied to several abstractions of a pair of input phrases (e.g., the phrases themselves, their stems, POS tags). The s</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan. 2007. The third Pascal recognizing textual entailment challenge. In Proceedings of the ACL-Pascal Workshop on Textual Entailment and Paraphrasing, pages 1–9, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>Maximum entropy for hypothesis formulation, especially for multidimentional contigency tables.</title>
<date>1963</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>34--911</pages>
<contexts>
<context position="3596" citStr="Good, 1963" startWordPosition="582" endWordPosition="583">n if the reverse direction does not hold. For example, “Y was written by X” textually entails “Y is the work of X”, but the reverse direction does not necessarily hold (e.g., if Y is a statue); hence, the two sentences are not paraphrases. In this paper, we focus on paraphrase recognition. We propose three methods that employ string similarity measures, which are applied to several abstractions of a pair of input phrases (e.g., the phrases themselves, their stems, POS tags). The scores returned by the similarity measures are used as features in a Maximum Entropy (ME) classifier (Jaynes, 1957; Good, 1963), which learns to separate true paraphrase pairs from false ones. Two of our methods also exploit WordNet to detect synonyms, and one of them uses additional features to measure similarities of grammatical relations 27 Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 27–35, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP obtained by a dependency parser.1 Our experiments were conducted on two datasets: the publicly available Microsoft Research Paraphrasing corpus (Dolan et al., 2004) and a dataset that we constructed from the MTC corpus.2 The experimental results show </context>
</contexts>
<marker>Good, 1963</marker>
<rawString>I. J. Good. 1963. Maximum entropy for hypothesis formulation, especially for multidimentional contigency tables. Annals of Mathematical Statistics, 34:911–934.</rawString>
</citation>
<citation valid="true">
<title>Feature Extraction, Foundations and Applications.</title>
<date>2006</date>
<editor>I.M. Guyon, S.R. Gunn, M. Nikravesh, and L. Zadeh, editors.</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="22622" citStr="(2006)" startWordPosition="3708" endWordPosition="3708">, and modified n-gram precision, a measure similar to BLEU. The produced vectors are then used to train a decision tree classifier. Hence, Zhang and Patrick’s approach is similar to ours, but we use more and different similarity measures and several levels of abstraction of the two sentences. We also use ME, along with a wrapper approach to feature selection, rather than decision tree induction and its embedded information gain-based feature selection. Furthermore, all of our methods, even INIT which employs no parsing at all, achieve better results compared to Zhang and Patrick’s. Qiu et al. (2006) first convert the sentences into tuples using parsing and semantic role labeling. They then match similar tuples across the two sentences, and use an SVM (Vapnik, 1998) classifier to decide whether or not the tuples that have not been matched are important or not. If not, the sentences are paraphrases. Despite using a parser and a semantic role identifier, Qiu et al.’s system performs worse than our methods. Finally, Finch et al.’s system (2005) achieved the second best overall results by employing POS tagging, synonymy resolution, and an SVM. Interestingly, the features of the SVM correspond</context>
</contexts>
<marker>2006</marker>
<rawString>I.M. Guyon, S.R. Gunn, M. Nikravesh, and L. Zadeh, editors. 2006. Feature Extraction, Foundations and Applications. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
</authors>
<title>Methods for using textual entailment in open-domain question answering.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>905--912</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1243" citStr="Harabagiu and Hickl, 2006" startWordPosition="191" endWordPosition="194">he MSR paraphrasing corpus and a dataset that we automatically created from the MTC corpus. Our system achieves state of the art or better results. 1 Introduction Recognizing or generating semantically equivalent phrases is of significant importance in many natural language applications. In question answering, for example, a question may be phrased differently than in a document collection (e.g., “Who is the author of War and Peace?” vs. “Leo Tolstoy is the writer of War and Peace.”), and taking such variations into account can improve system performance significantly (Harabagiu et al., 2003; Harabagiu and Hickl, 2006). A paraphrase generator, meaning a module that produces new phrases or patterns that are semantically equivalent (or almost equivalant) to a given input phrase or pattern (e.g., “X is the writer of Y ” q “X wrote Y ” q “Y was written by X” q “X is the author of Y ”, or “X produces Y ” q “X manufactures Y ” q “X is the manufacturer of Y ”) can be used to produce alternative phrasings of the question, before matching it against a document collection. Unlike paraphrase generators, paraphrase recognizers decide whether or not two given phrases (or patterns) are paraphrases, possibly by generalizi</context>
</contexts>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>S. Harabagiu and A. Hickl. 2006. Methods for using textual entailment in open-domain question answering. In Proceedings of COLING-ACL, pages 905– 912, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Harabagiu</author>
<author>S J Maiorano</author>
<author>M A Pasca</author>
</authors>
<title>Open-domain textual question answering techniques.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="1215" citStr="Harabagiu et al., 2003" startWordPosition="187" endWordPosition="190">iment on two datasets, the MSR paraphrasing corpus and a dataset that we automatically created from the MTC corpus. Our system achieves state of the art or better results. 1 Introduction Recognizing or generating semantically equivalent phrases is of significant importance in many natural language applications. In question answering, for example, a question may be phrased differently than in a document collection (e.g., “Who is the author of War and Peace?” vs. “Leo Tolstoy is the writer of War and Peace.”), and taking such variations into account can improve system performance significantly (Harabagiu et al., 2003; Harabagiu and Hickl, 2006). A paraphrase generator, meaning a module that produces new phrases or patterns that are semantically equivalent (or almost equivalant) to a given input phrase or pattern (e.g., “X is the writer of Y ” q “X wrote Y ” q “Y was written by X” q “X is the author of Y ”, or “X produces Y ” q “X manufactures Y ” q “X is the manufacturer of Y ”) can be used to produce alternative phrasings of the question, before matching it against a document collection. Unlike paraphrase generators, paraphrase recognizers decide whether or not two given phrases (or patterns) are paraphr</context>
</contexts>
<marker>Harabagiu, Maiorano, Pasca, 2003</marker>
<rawString>S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca. 2003. Open-domain textual question answering techniques. Natural Language Engineering, 9(3):231–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ibrahim</author>
<author>B Katz</author>
<author>J Lin</author>
</authors>
<title>Extracting structural paraphrases from aligned monolingual corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on Paraphrasing,</booktitle>
<pages>57--64</pages>
<location>Sapporo, Japan.</location>
<marker>Ibrahim, Katz, Lin, 2003</marker>
<rawString>A. Ibrahim, B. Katz, and J. Lin. 2003. Extracting structural paraphrases from aligned monolingual corpora. In Proceedings of the ACL Workshop on Paraphrasing, pages 57–64, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Jaynes</author>
</authors>
<title>Information theory and statistical mechanics. Physical Review,</title>
<date>1957</date>
<pages>106--620</pages>
<contexts>
<context position="3583" citStr="Jaynes, 1957" startWordPosition="580" endWordPosition="581">the other, even if the reverse direction does not hold. For example, “Y was written by X” textually entails “Y is the work of X”, but the reverse direction does not necessarily hold (e.g., if Y is a statue); hence, the two sentences are not paraphrases. In this paper, we focus on paraphrase recognition. We propose three methods that employ string similarity measures, which are applied to several abstractions of a pair of input phrases (e.g., the phrases themselves, their stems, POS tags). The scores returned by the similarity measures are used as features in a Maximum Entropy (ME) classifier (Jaynes, 1957; Good, 1963), which learns to separate true paraphrase pairs from false ones. Two of our methods also exploit WordNet to detect synonyms, and one of them uses additional features to measure similarities of grammatical relations 27 Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 27–35, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP obtained by a dependency parser.1 Our experiments were conducted on two datasets: the publicly available Microsoft Research Paraphrasing corpus (Dolan et al., 2004) and a dataset that we constructed from the MTC corpus.2 The experimental </context>
</contexts>
<marker>Jaynes, 1957</marker>
<rawString>E. T. Jaynes. 1957. Information theory and statistical mechanics. Physical Review, 106:620–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<pages>7--343</pages>
<marker>Lin, Pantel, 2001</marker>
<rawString>D. Lin and P. Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7:343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Malakasiotis</author>
<author>I Androutsopoulos</author>
</authors>
<title>Learning textual entailment using svms and string similarity measures.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague,</location>
<contexts>
<context position="6455" citStr="Malakasiotis and Androutsopoulos, 2007" startWordPosition="1037" endWordPosition="1040">ne string similarity measures: Levenshtein distance (edit distance), Jaro-Winkler distance, Manhattan distance, Euclidean distance, co1We use Stanford University’s ME classifier and parser; seehttp://nlp.stanford.edu/. 2The corpus is available by the LDC, Catalogue Number LDC2002T01, ISBN 1-58563-217-1. sine similarity, n-gram distance (with n = 3), matching coefficient, Dice coefficient, and Jaccard coefficient. To save space, we do not repeat the definitions of the similarity measures here, since they are readily available in the literature and they are also summarized in our previous work (Malakasiotis and Androutsopoulos, 2007). For each pair of input strings hS1, S2i, we form ten new pairs of strings (s11, s1 � , ... , s10 � 1 , s10 2 2 corresponding to ten different levels of abstraction of S1 and S2, and we apply the nine similarity measures to the ten new pairs, resulting in a total of 90 measurements. These measurements are then included as features in the vector v~ that corresponds to hS1, S2i. The (si1, si � pairs are: 2 (s11, s1 � : two strings consisting of the original tokens of S1 2 and S2, respectively, with the original order of the tokens maintained;3 (s(s(s21, s2 � : as in the previous case, but now t</context>
</contexts>
<marker>Malakasiotis, Androutsopoulos, 2007</marker>
<rawString>P. Malakasiotis and I. Androutsopoulos. 2007. Learning textual entailment using svms and string similarity measures. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 42–47, Prague, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Syntaxbased alignment of multiple translations: extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>102--109</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="26487" citStr="Pang et al. (2003)" startWordPosition="4353" endWordPosition="4356">k of X”). Ibrahim et al.’s (2003) method is similar to DIRT, but it uses only dependency grammar paths from aligned sentences (from a parallel corpus) that share compatible anchors (e.g., identical strings, or entity names of the same semantic category). Shinyama and Sekine (2003) adopt a very similar approach. In another generation approach, Barzilay and Lee (2002; 2003) look for pairs of slotted word lattices that share many common slot fillers; the lattices are generated by applying a multiplesequence alignment algorithm to a corpus of multiple news articles about the same events. Finally, Pang et al. (2003) create finite state automata by merging parse trees of aligned sentences from a parallel corpus; in each automaton, different paths represent paraphrases. Again, a paraphrase recognizer could be embedded in all of these methods, to filter out erroneous generated patterns. 5 Conclusions and further work We have presented three methods (INIT, INIT+WN, INIT+WN+DEP) that recognize paraphrases given pairs of sentences. These methods employ nine string similarity measures applied to ten shallow abstractions of the input sentences. Moreover, INIT+WN and INIT+WN+DEP exploit WordNet for synonymy resol</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>B. Pang, K. Knight, and D. Marcu. 2003. Syntaxbased alignment of multiple translations: extracting paraphrases and generating new sentences. In Proceedings of HLT-NAACL, pages 102–109, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="20753" citStr="Papineni et al., 2002" startWordPosition="3400" endWordPosition="3403"> INIT achieves state of the art performance, even though the other approaches use many more linguistic resources. For example, Wan et al.’s approach (Wan et al., 2006), which achieved the best previously reported results, is similar to ours, in that it also trains a classifier with similarity measures; but some of Wan et al.’s measures require a dependency grammar parser, unlike INIT. More precisely, for each pair of sentences, Wan et al. construct a feature vector with values that measure lexical and dependency similarities. The measures are: word overlap, length difference (in words), BLEU (Papineni et al., 2002), dependency relation overlap (i.e., R1 and R2 but not FR1,R2), and dependency tree edit distance. The measures are also applied on sequences containing the lemmatized words of the original sentences, similarly to one of our levels of abstraction. Interestingly, INIT achieves the same (and slightly better) accuracy as Wan et al.’s system, without employing any parsing. Our more enhanced methods, INIT+WN and INIT+WN+DEP, achieve even better results. Zhang and Patrick (2005) use a dependency grammar parser to convert passive voice phrases to active voice ones. They also use a preprocessing stage</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qiu</author>
<author>M Y Kan</author>
<author>T S Chua</author>
</authors>
<title>Paraphrase recognition via dissimilarity significance classification.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>18--26</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="22622" citStr="Qiu et al. (2006)" startWordPosition="3705" endWordPosition="3708"> (in words), and modified n-gram precision, a measure similar to BLEU. The produced vectors are then used to train a decision tree classifier. Hence, Zhang and Patrick’s approach is similar to ours, but we use more and different similarity measures and several levels of abstraction of the two sentences. We also use ME, along with a wrapper approach to feature selection, rather than decision tree induction and its embedded information gain-based feature selection. Furthermore, all of our methods, even INIT which employs no parsing at all, achieve better results compared to Zhang and Patrick’s. Qiu et al. (2006) first convert the sentences into tuples using parsing and semantic role labeling. They then match similar tuples across the two sentences, and use an SVM (Vapnik, 1998) classifier to decide whether or not the tuples that have not been matched are important or not. If not, the sentences are paraphrases. Despite using a parser and a semantic role identifier, Qiu et al.’s system performs worse than our methods. Finally, Finch et al.’s system (2005) achieved the second best overall results by employing POS tagging, synonymy resolution, and an SVM. Interestingly, the features of the SVM correspond</context>
</contexts>
<marker>Qiu, Kan, Chua, 2006</marker>
<rawString>L. Qiu, M. Y. Kan, and T.S. Chua. 2006. Paraphrase recognition via dissimilarity significance classification. In Proceedings of EMNLP, pages 18–26, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>41--47</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="24740" citStr="Ravichandran and Hovy, 2001" startWordPosition="4055" endWordPosition="4058"> al. – 72.00 72.50 93.40 81.60 Wan et al. – 75.00 77.00 90.00 83.00 Zhang &amp; Patrick – 71.90 74.30 88.20 80.70 Table 2: Results (%) of our methods (upper part) and other methods (lower part) on the MSR corpus. we achieve similar or better accuracy results. 4 Related work We have already made the distinction between paraphrase (and textual entailment) generators vs. recognizers, and we have pointed out that recognizers can be embedded in generators as filters. The latter is particularly useful in bootstrapping paraphrase generation approaches (Riloff and Jones, 1999; Barzilay and McKeown, 2001; Ravichandran and Hovy, 2001; Ravichandran et al., 2003; Duclaye et al., 2003; Szpektor et al., 2004), which are typically given seed pairs of named entities for which a particular relation holds; the system locates in a document collection (or the entire Web) contexts were the seeds cooccur, and uses the contexts as patterns that can express the relation; the patterns are then used to locate new named entities that satisfy the relation, and a new iteration begins. A paraphrase recognizer could be used to filter out erroneous generated paraphrases between iterations. Another well known paraphrase generator is Lin and Pan</context>
</contexts>
<marker>Ravichandran, Hovy, 2001</marker>
<rawString>D. Ravichandran and E. Hovy. 2001. Learning surface text patterns for a question answering system. In Proceedings of ACL, pages 41–47, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>A Ittycheriah</author>
<author>S Roukos</author>
</authors>
<title>Automatic derivation of surface text patterns for a maximum entropy based question answering system.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>85--87</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="24767" citStr="Ravichandran et al., 2003" startWordPosition="4059" endWordPosition="4062">0 Wan et al. – 75.00 77.00 90.00 83.00 Zhang &amp; Patrick – 71.90 74.30 88.20 80.70 Table 2: Results (%) of our methods (upper part) and other methods (lower part) on the MSR corpus. we achieve similar or better accuracy results. 4 Related work We have already made the distinction between paraphrase (and textual entailment) generators vs. recognizers, and we have pointed out that recognizers can be embedded in generators as filters. The latter is particularly useful in bootstrapping paraphrase generation approaches (Riloff and Jones, 1999; Barzilay and McKeown, 2001; Ravichandran and Hovy, 2001; Ravichandran et al., 2003; Duclaye et al., 2003; Szpektor et al., 2004), which are typically given seed pairs of named entities for which a particular relation holds; the system locates in a document collection (or the entire Web) contexts were the seeds cooccur, and uses the contexts as patterns that can express the relation; the patterns are then used to locate new named entities that satisfy the relation, and a new iteration begins. A paraphrase recognizer could be used to filter out erroneous generated paraphrases between iterations. Another well known paraphrase generator is Lin and Pantel’s (2001) DIRT, which pr</context>
</contexts>
<marker>Ravichandran, Ittycheriah, Roukos, 2003</marker>
<rawString>D. Ravichandran, A. Ittycheriah, and S. Roukos. 2003. Automatic derivation of surface text patterns for a maximum entropy based question answering system. In Proceedings of HLT-NAACL, pages 85–87, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>474--479</pages>
<location>Orlando, FL.</location>
<contexts>
<context position="24683" citStr="Riloff and Jones, 1999" startWordPosition="4047" endWordPosition="4050"> 82.61 Finch et al. – 74.96 76.58 89.80 82.66 Qiu et al. – 72.00 72.50 93.40 81.60 Wan et al. – 75.00 77.00 90.00 83.00 Zhang &amp; Patrick – 71.90 74.30 88.20 80.70 Table 2: Results (%) of our methods (upper part) and other methods (lower part) on the MSR corpus. we achieve similar or better accuracy results. 4 Related work We have already made the distinction between paraphrase (and textual entailment) generators vs. recognizers, and we have pointed out that recognizers can be embedded in generators as filters. The latter is particularly useful in bootstrapping paraphrase generation approaches (Riloff and Jones, 1999; Barzilay and McKeown, 2001; Ravichandran and Hovy, 2001; Ravichandran et al., 2003; Duclaye et al., 2003; Szpektor et al., 2004), which are typically given seed pairs of named entities for which a particular relation holds; the system locates in a document collection (or the entire Web) contexts were the seeds cooccur, and uses the contexts as patterns that can express the relation; the patterns are then used to locate new named entities that satisfy the relation, and a new iteration begins. A paraphrase recognizer could be used to filter out erroneous generated paraphrases between iteration</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of AAAI, pages 474–479, Orlando, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Shinyama</author>
<author>S Sekine</author>
</authors>
<title>Paraphrase acquisition for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on Paraphrasing,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="26150" citStr="Shinyama and Sekine (2003)" startWordPosition="4298" endWordPosition="4301"> assumption that different paths of dependency trees (obtained from a corpus) that occur frequently with the same words (slot fillers) at their ends are often paraphrases. An extension of DIRT, named LEDIR, has also been proposed (Bhagat et al., 2007) to recognize directional textual entailment rules (e.g., “Y was written by X” ⇒ “Y is the work of X”). Ibrahim et al.’s (2003) method is similar to DIRT, but it uses only dependency grammar paths from aligned sentences (from a parallel corpus) that share compatible anchors (e.g., identical strings, or entity names of the same semantic category). Shinyama and Sekine (2003) adopt a very similar approach. In another generation approach, Barzilay and Lee (2002; 2003) look for pairs of slotted word lattices that share many common slot fillers; the lattices are generated by applying a multiplesequence alignment algorithm to a corpus of multiple news articles about the same events. Finally, Pang et al. (2003) create finite state automata by merging parse trees of aligned sentences from a parallel corpus; in each automaton, different paths represent paraphrases. Again, a paraphrase recognizer could be embedded in all of these methods, to filter out erroneous generated</context>
</contexts>
<marker>Shinyama, Sekine, 2003</marker>
<rawString>Y. Shinyama and S. Sekine. 2003. Paraphrase acquisition for information extraction. In Proceedings of the ACL Workshop on Paraphrasing, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>H Tanev</author>
<author>I Dagan</author>
<author>B Coppola</author>
</authors>
<title>Scaling Web-based acquisition of entailment relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="24813" citStr="Szpektor et al., 2004" startWordPosition="4067" endWordPosition="4070">trick – 71.90 74.30 88.20 80.70 Table 2: Results (%) of our methods (upper part) and other methods (lower part) on the MSR corpus. we achieve similar or better accuracy results. 4 Related work We have already made the distinction between paraphrase (and textual entailment) generators vs. recognizers, and we have pointed out that recognizers can be embedded in generators as filters. The latter is particularly useful in bootstrapping paraphrase generation approaches (Riloff and Jones, 1999; Barzilay and McKeown, 2001; Ravichandran and Hovy, 2001; Ravichandran et al., 2003; Duclaye et al., 2003; Szpektor et al., 2004), which are typically given seed pairs of named entities for which a particular relation holds; the system locates in a document collection (or the entire Web) contexts were the seeds cooccur, and uses the contexts as patterns that can express the relation; the patterns are then used to locate new named entities that satisfy the relation, and a new iteration begins. A paraphrase recognizer could be used to filter out erroneous generated paraphrases between iterations. Another well known paraphrase generator is Lin and Pantel’s (2001) DIRT, which produces slotted semantically equivalent pattern</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. Scaling Web-based acquisition of entailment relations. In Proceedings of EMNLP, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>Statistical learning theory.</title>
<date>1998</date>
<publisher>John Wiley.</publisher>
<contexts>
<context position="22791" citStr="Vapnik, 1998" startWordPosition="3735" endWordPosition="3736">roach is similar to ours, but we use more and different similarity measures and several levels of abstraction of the two sentences. We also use ME, along with a wrapper approach to feature selection, rather than decision tree induction and its embedded information gain-based feature selection. Furthermore, all of our methods, even INIT which employs no parsing at all, achieve better results compared to Zhang and Patrick’s. Qiu et al. (2006) first convert the sentences into tuples using parsing and semantic role labeling. They then match similar tuples across the two sentences, and use an SVM (Vapnik, 1998) classifier to decide whether or not the tuples that have not been matched are important or not. If not, the sentences are paraphrases. Despite using a parser and a semantic role identifier, Qiu et al.’s system performs worse than our methods. Finally, Finch et al.’s system (2005) achieved the second best overall results by employing POS tagging, synonymy resolution, and an SVM. Interestingly, the features of the SVM correspond to machine translation evaluation metrics, rather than string similarity measures, unlike our system. We plan to examine further how the features of Finch et al. and ot</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>V. Vapnik. 1998. Statistical learning theory. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wan</author>
<author>M Dras</author>
<author>R Dale</author>
<author>C Paris</author>
</authors>
<title>Using dependency-based features to take the “parafarce” out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<pages>131--138</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="20298" citStr="Wan et al., 2006" startWordPosition="3328" endWordPosition="3331">ction methods (FHC and FBS) lead to competitive results with much fewer features (7 and 10, respectively, instead of 136). However, feature selection deteriorates performance, especially accuracy, i.e., the full feature set is better, despite its redundancy. Table 2 also includes all other reported results for the MSR corpus that we are aware of; we are not aware of the exact number of features used by the other researchers. It is noteworthy that INIT achieves state of the art performance, even though the other approaches use many more linguistic resources. For example, Wan et al.’s approach (Wan et al., 2006), which achieved the best previously reported results, is similar to ours, in that it also trains a classifier with similarity measures; but some of Wan et al.’s measures require a dependency grammar parser, unlike INIT. More precisely, for each pair of sentences, Wan et al. construct a feature vector with values that measure lexical and dependency similarities. The measures are: word overlap, length difference (in words), BLEU (Papineni et al., 2002), dependency relation overlap (i.e., R1 and R2 but not FR1,R2), and dependency tree edit distance. The measures are also applied on sequences con</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using dependency-based features to take the “parafarce” out of paraphrase. In Proceedings of the Australasian Language Technology Workshop, pages 131–138, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>J Patrick</author>
</authors>
<title>Paraphrase identification by text canonicalization.</title>
<date>2005</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<pages>160--166</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="21230" citStr="Zhang and Patrick (2005)" startWordPosition="3475" endWordPosition="3478">values that measure lexical and dependency similarities. The measures are: word overlap, length difference (in words), BLEU (Papineni et al., 2002), dependency relation overlap (i.e., R1 and R2 but not FR1,R2), and dependency tree edit distance. The measures are also applied on sequences containing the lemmatized words of the original sentences, similarly to one of our levels of abstraction. Interestingly, INIT achieves the same (and slightly better) accuracy as Wan et al.’s system, without employing any parsing. Our more enhanced methods, INIT+WN and INIT+WN+DEP, achieve even better results. Zhang and Patrick (2005) use a dependency grammar parser to convert passive voice phrases to active voice ones. They also use a preprocessing stage to generalize the pairs of sentences. The preprocessing replaces dates, times, percentages, etc. with generic tags, something that we have also done in the MSR corpus, but it also replaces words and phrases indicating future actions (e.g., “plans to”, “be expected to”) with the word “will”; the latter is an example of further preprocessing that could be added to our system. After the preprocessing, Zhang and Patrick create for each sentence pair a feature vector whose val</context>
</contexts>
<marker>Zhang, Patrick, 2005</marker>
<rawString>Y. Zhang and J. Patrick. 2005. Paraphrase identification by text canonicalization. In Proceedings of the Australasian Language Technology Workshop, pages 160–166, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>