<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.992915">
Non-Projective Parsing for Statistical Machine Translation
</title>
<author confidence="0.885782">
Xavier Carreras Michael Collins
</author>
<affiliation confidence="0.316452">
MIT CSAIL, Cambridge, MA 02139, USA
</affiliation>
<email confidence="0.987556">
{carreras,mcollins}@csail.mit.edu
</email>
<sectionHeader confidence="0.997215" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946214285714">
We describe a novel approach for syntax-
based statistical MT, which builds on a
variant of tree adjoining grammar (TAG).
Inspired by work in discriminative depen-
dency parsing, the key idea in our ap-
proach is to allow highly flexible reorder-
ing operations during parsing, in combina-
tion with a discriminative model that can
condition on rich features of the source-
language string. Experiments on trans-
lation from German to English show im-
provements over phrase-based systems,
both in terms of BLEU scores and in hu-
man evaluations.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990877193548387">
Syntax-based models for statistical machine trans-
lation (SMT) have recently shown impressive re-
sults; many such approaches are based on ei-
ther synchronous grammars (e.g., (Chiang, 2005)),
or tree transducers (e.g., (Marcu et al., 2006)).
This paper describes an alternative approach for
syntax-based SMT, which directly leverages meth-
ods from non-projective dependency parsing. The
key idea in our approach is to allow highly flexible
reordering operations, in combination with a dis-
criminative model that can condition on rich fea-
tures of the source-language input string.
Our approach builds on a variant of tree adjoin-
ing grammar (TAG; (Joshi and Schabes, 1997))
(specifically, the formalism of (Carreras et al.,
2008)). The models we describe make use of
phrasal entries augmented with subtrees that pro-
vide syntactic information in the target language.
As one example, when translating the sentence
wir m¨ussen auch diese kritik ernst nehmen from
German into English, the following sequence of
syntactic phrasal entries might be used (we show
each English syntactic fragment above its associ-
ated German sub-string):
wir m¨ussen auch diese kritik ernst nehmen
TAG parsing operations are then used to combine
these fragments into a full parse tree, giving the
final English translation we must also take these
criticisms seriously.
Some key aspects of our approach are as fol-
lows:
</bodyText>
<listItem confidence="0.972634111111111">
• We impose no constraints on entries in the
phrasal lexicon. The method thereby retains the
full set of lexical entries of phrase-based systems
(e.g., (Koehn et al., 2003)).1
• The model allows a straightforward integra-
tion of lexicalized syntactic language models—for
example the models of (Charniak, 2001)—in addi-
tion to a surface language model.
• The operations used to combine tree frag-
</listItem>
<bodyText confidence="0.890676590909091">
ments into a complete parse tree are signifi-
cant generalizations of standard parsing operations
found in TAG; specifically, they are modified to be
highly flexible, potentially allowing any possible
permutation (reordering) of the initial fragments.
As one example of the type of parsing opera-
tions that we will consider, we might allow the
tree fragments shown above for these criticisms
and take to be combined to form a new structure
with the sub-string take these criticisms. This step
in the derivation is necessary to achieve the correct
English word order, and is novel in a couple of re-
spects: first, these criticisms is initially seen to the
left of take, but after the adjunction this order is
reversed; second, and more unusually, the treelet
for seriously has been skipped over, with the re-
sult that the German words translated at this point
(diese, kritik, and nehmen) form a non-contiguous
sequence. More generally, we will allow any two
1Note that in the above example each English phrase con-
sists of a completely connected syntactic structure; this is not,
however, a required constraint, see section 3.2 for discussion.
</bodyText>
<figure confidence="0.993028666666667">
VP
must ADVP
also
S
NP
we
NP
these criticisms
ADVP
seriously
take
VP
</figure>
<page confidence="0.952731">
200
</page>
<note confidence="0.996457">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 200–209,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999956903225806">
tree fragments to be combined during the transla-
tion process, irrespective of the reorderings which
are introduced, or the non-projectivity of the pars-
ing operations that are required.
The use of flexible parsing operations raises two
challenges that will be a major focus of this paper.
First, these operations will allow the model to cap-
ture complex reordering phenomena, but will in
addition introduce many spurious possibilities. In-
spired by work in discriminative dependency pars-
ing (e.g., (McDonald et al., 2005)), we add proba-
bilistic constraints to the model through a discrim-
inative model that links lexical dependencies in the
target language to features of the source language
string. We also investigate hard constraints on the
dependency structures that are created during pars-
ing. Second, there is a need to develop efficient
decoding algorithms for the models. We describe
approximate search methods that involve a signif-
icant extension of decoding algorithms originally
developed for phrase-based translation systems.
Experiments on translation from German to En-
glish show a 0.5% improvement in BLEU score
over a phrase-based system. Human evaluations
show that the syntax-based system gives a sig-
nificant improvement over the phrase-based sys-
tem. The discriminative dependency model gives
a 1.5% BLEU point improvement over a basic
model that does not condition on the source lan-
guage string; the hard constraints on dependency
structures give a 0.8% BLEU improvement.
</bodyText>
<sectionHeader confidence="0.99179" genericHeader="method">
2 Relationship to Previous Work
</sectionHeader>
<bodyText confidence="0.999875878787879">
A number of syntax-based translation systems
have framed translation as a parsing problem,
where search for the most probable translation is
achieved using algorithms that are generalizations
of conventional parsing methods. Early examples
of this work include (Alshawi, 1996; Wu, 1997);
more recent models include (Yamada and Knight,
2001; Eisner, 2003; Melamed, 2004; Zhang and
Gildea, 2005; Chiang, 2005; Quirk et al., 2005;
Marcu et al., 2006; Zollmann and Venugopal,
2006; Nesson et al., 2006; Cherry, 2008; Mi et
al., 2008; Shen et al., 2008). The majority of
these methods make use of synchronous gram-
mars, or tree transducers, which operate over parse
trees in the source and/or target languages. Re-
ordering rules are typically specified through rota-
tions or transductions stated at the level of context-
free rules, or larger fragments, within parse trees.
These rules can be learned automatically from cor-
pora.
A critical difference in our work is to allow
arbitrary reorderings of the source language sen-
tence (as in phrase-based systems), through the
use of flexible parsing operations. Rather than
stating reordering rules at the level of source or
target language parse trees, we capture reorder-
ing phenomena using a discriminative dependency
model. Other factors that distinguish us from pre-
vious work are the use of all phrases proposed by a
phrase-based system, and the use of a dependency
language model that also incorporates constituent
information (although see (Charniak et al., 2003;
Shen et al., 2008) for related approaches).
</bodyText>
<sectionHeader confidence="0.989611" genericHeader="method">
3 A Syntactic Translation Model
</sectionHeader>
<subsectionHeader confidence="0.989067">
3.1 Background
</subsectionHeader>
<bodyText confidence="0.984569821428571">
Our work builds on the variant of tree adjoin-
ing grammar (TAG) introduced by (Carreras et
al., 2008). In this formalism the basic units
in the grammar are spines, which associate tree
fragments with lexical items. These spines can
be combined using a sister-adjunction operation
(Rambow et al., 1995), to form larger pieces of
structure.2 For example, we might have the fol-
lowing operation:
is there is
In this case the spine for there has sister-adjoined
into the S node in the spine for is; we re-
fer to the spine for there as being the modifier
spine, and the spine for is being the head spine.
There are close connections to dependency for-
malisms: in particular in this operation we see
a lexical dependency between the modifier word
there and the head word is. It is possible to de-
fine syntactic language models, similar to (Char-
niak, 2001), which associate probabilities with
these dependencies, roughly speaking of the form
P(wm, sm|wh, sh, pos, Q), where wm and sm are
the identities of the modifier word and spine, wh
and sh are the identities of the head word and
spine, pos is the position in the head spine that is
being adjoined into, and Q is some additional state
(e.g., state that tracks previous modifiers that have
adjoined into the same spine).
</bodyText>
<footnote confidence="0.96614">
2We also make use of the r-adjunction operation defined in
(Carreras et al., 2008), which, together with sister-adjunction,
allows us to model the full range of structures found in the
Penn treebank.
</footnote>
<figure confidence="0.9336466">
⇒ S
NP
VP
S
VP
NP
there
201
S
es gibt keine hierarchie der diskriminierung
</figure>
<figureCaption confidence="0.990343">
Figure 1: A training example consisting of an English (tar-
get language) tree and a German (source language) sentence.
</figureCaption>
<bodyText confidence="0.9966825">
In this paper we will also consider treelets,
which are a generalization of spines, and which
allow lexical entries that include more than one
word. These treelets can again be combined us-
ing a sister-adjunction operation. As an example,
consider the following operation:
In this case the treelet for to respond sister-adjoins
into the treelet for be able. This operation intro-
duces a bi-lexical dependency between the modi-
fier word to and the head word able.
</bodyText>
<subsectionHeader confidence="0.999678">
3.2 S-phrases
</subsectionHeader>
<bodyText confidence="0.991455909090909">
This section describes how phrase entries from
phrase-based translation systems can be modified
to include associated English syntactic structures.
These syntactic phrase-entries (from here on re-
ferred to as “s-phrases”) will form the basis of the
translation models that we describe.
We extract s-phrases from training examples
consisting of a source-language string paired with
a target-language parse tree. For example, con-
sider the training example in figure 1. We as-
sume some method that enumerates a set of pos-
sible phrase entries for each training example:
each phrase entry is a pair ((i, j), (k, l)) speci-
fying that source-language words fi ... fj corre-
spond to target-language words ek ... el in the ex-
ample. For example, one phrase entry for the ex-
ample might be ((1, 2), (1, 2)), representing the
pair (es gibt ⇒ there is). In our experiments
we use standard methods in phrase-based systems
(Koehn et al., 2003) to define the set of phrase en-
tries for each sentence in training data.
es gibt keine hierarchie der
</bodyText>
<figureCaption confidence="0.9796095">
Figure 2: Example syntactic phrase entries. We show Ger-
man sub-strings above their associated sequence of treelets.4
</figureCaption>
<bodyText confidence="0.995309666666667">
For each phrase entry, we add syntactic infor-
mation to the English string. To continue our ex-
ample, the resulting entry would be as follows:
</bodyText>
<equation confidence="0.841746">
es gibt ⇒ S
NP
there
</equation>
<bodyText confidence="0.998041230769231">
To give a more formal description of how syn-
tactic structures are derived for phrases, first note
that each parse tree t is mapped to a TAG deriva-
tion using the method described in (Carreras et al.,
2008). This procedure uses the head finding rules
of (Collins, 1997). The resulting derivation con-
sists of a TAG spine for each word seen in the sen-
tence, together with a set of adjunction operations
which each involve a modifier spine and a head
spine. Given an English string e = el ... er,,, with
an associated parse tree t, the syntactic structure
associated with a substring ek ... el (e.g., there is)
is then defined as follows:
</bodyText>
<listItem confidence="0.997805">
• For each word in the English sub-string, in-
clude its associated TAG spine in t.
• In addition, include any adjunction operations
in t where both the head and modifier word are in
the sub-string ej ... ek.
</listItem>
<bodyText confidence="0.9998625">
In the above example, the resulting structure
(i.e., the structure for there is) is a single treelet.
In other cases, however, we may get a sequence of
treelets, which are disconnected from each other.
For example, another likely phrase-entry for this
training example is (es gibt keine ⇒ there is no)
resulting in the first lexical entry in figure 2, which
has two treelets. Allowing s-phrases with multiple
treelets ensures that all phrases used by phrase-
based systems can be used within our approach.
As a final step, we add additional align-
ment information to each s-phrase. Con-
sider an s-phrase which contains source-language
words fl ... fr,, paired with target-language words
el ... em. The alignment information is a vec-
tor ((al, bi) ... (am, bm)) that specifies for each
word ei its alignment to words fai ... fbi in the
source language. For example, for the phrase en-
</bodyText>
<figure confidence="0.999193">
VP
is NP
NPB
no hierarchy
PP
of NP
discrimination
NP
there
NP
S
VP
PP
DT
NP
is
no
NPB
of
there
hierarchy
VP
be ADJP
able
SG
to VP
respond
⇒ VP
be ADJP
able SG
to VP
respond
VP
is
</figure>
<page confidence="0.989968">
202
</page>
<bodyText confidence="0.95491703030303">
try (es gibt ==&gt;. there is) a correct alignment would
be ((1, 1), (2, 2)), specifying that there is aligned
to es, and is is aligned to gibt (note that in many,
but not all, cases ai = bi, i.e., a target language
word is aligned to a single source language word).
The alignment information in s-phrases will
be useful in tying syntactic dependencies cre-
ated in the target language to positions in the
source language string. In particular, we will con-
sider discriminative models (analogous to models
for dependency parsing, e.g., see (McDonald et
al., 2005)) that estimate the probability of target-
language dependencies conditioned on properties
of the source-language string. Alignments may be
derived in a number of ways; in our method we
directly use phrase entries proposed by a phrase-
based system. Specifically, for each target word ei
in a phrase entry (f1 ... fn, e1 ... em) for a train-
ing example, we find the smallest5 phrase entry
in the same training example that includes ei on
the target side, and is a subset of f1 ... fn on the
source side; the word ei is then aligned to the sub-
set of source language words in this “minimal”
phrase.
In conclusion, s-phrases are defined as follows:
Definition 1 An s-phrase is a 4-tuple (f, e, t, a)
where: f is a sequence of foreign words; e is
a sequence of English words; t is a sequence of
treelets specifying a TAG spine for each English
word, and potentially some adjunctions between
these spines; and a is an alignment. For an s-
phrase q we will sometimes refer to the 4 elements
of q as f(q), e(q), t(q) and a(q).
</bodyText>
<subsectionHeader confidence="0.989183">
3.3 The Model
</subsectionHeader>
<bodyText confidence="0.982973772727273">
We now introduce a model that makes use of s-
phrases, and which is flexible in the reorderings
that it allows. To provide some intuition, and some
motivation for the use of reordering operations,
figure 3 gives several examples of German strings
which have different word orders from English.
The crucial idea will be to use TAG adjunction
operations to combine treelets to form a complete
parse tree, but with a complete relaxation on the
order in which the treelets are combined. For ex-
ample, consider again the example given in the
introduction to this paper. In the first step of a
derivation that builds on these treelets, the treelet
5The “size” of a phrase entry is defined to be n3 + nt
where n3 is the number of source language words in the
phrase, nt is the number of target language words.
1(a) [die verwaltung] [muss] [k¨unftig] [schneller] [reagieren]
[k¨onnen] 1(b) the administration must be able to respond
more quickly in future
2(a) [meiner ansicht nach] [darf] [der erweiterungsprozess]
[nicht] [unn¨otig] [verz¨ogert] [werden] 2(b) in my opinion the
expansion process should not be delayed unnecessarily
</bodyText>
<figure confidence="0.916476333333333">
S NP RB ADVP VP VP
VP the . . . process not unnecessarily delayed be
should
</figure>
<figureCaption confidence="0.9854798">
Figure 3: Examples of translations. In each example (a)
is the original German string, with a possible segmentation
marked with “[“ and “]”; (b) is a translation for (a); and (c)
is a sequence of phrase entries, including syntactic structures,
for the segmentation given in (a).
</figureCaption>
<bodyText confidence="0.999924">
for these criticisms might adjoin into the treelet for
take, giving the following new sequence:
</bodyText>
<subsectionHeader confidence="0.698964">
S ADVP VP
</subsectionHeader>
<bodyText confidence="0.999641">
In the next derivation step seriously is adjoined to
the right of take, giving the following treelets:
</bodyText>
<subsectionHeader confidence="0.763067">
S VP
</subsectionHeader>
<bodyText confidence="0.9997195">
In the final step the second treelet adjoins into the
VP above must, giving a parse tree for the string
we must also take these criticisms seriously, and
completing the translation.
Formally, given an input sentence f, a derivation
d is a pair (q, 7r) where:
</bodyText>
<listItem confidence="0.79657975">
• q = q1 . . . qn is a sequence of s-phrases such
that f = f(q1)® f(q2)® ...® f(qn) (where u ® v
denotes the concatenation of strings u and v).
• 7r is a set of adjunction operations that
connects the sequence of treelets contained in
(t(q1), t(q2), ... , t(qn)) into a parse tree in the
target language. The operations allow a com-
plete relaxation of word order, potentially allow-
ing any of the n! possible orderings of the n s-
phrases. We make use of both sister-adjunction
and r-adjunction operations, as defined in (Car-
reras et al., 2008).6
</listItem>
<bodyText confidence="0.724050833333333">
6In principle we allow any treelet to adjoin into any other
treelet—for example there are no hard, grammar-based con-
straints ruling out the combination of certain pairs of non-
terminals. Note however that in some cases operations will
have probability 0 under the syntactic language model intro-
duced later in this section.
</bodyText>
<figure confidence="0.999530731707317">
S
PP
ADVP
in future
more
quickly
VP
the
admin...
must
1(c) NP
SG
to VP
respond
VP
be ADJP
able
take
these criticisms
VP
must ADVP
also
NP
we
seriously
V
NP
NP
V
ADVP
NP
we
take
seriously
VP
must ADVP
also
these criticisms
2(c) PP
in my
opinion
</figure>
<page confidence="0.892875">
203
</page>
<figureCaption confidence="0.9785375">
Figure 4: A spurious derivation step. The treelets arise
from [keine] [hierarchie der] [diskriminierung].
</figureCaption>
<bodyText confidence="0.9997315">
Given a derivation d = (q, 7r), we define e(d)
to be the target-language string defined by the
derivation, and t(d) to be the complete target-
language parse tree created by the derivation. The
most likely derivation for a foreign sentence f
is arg maxd∈G(f) score(d), where G(f) is the set
of possible derivations for f, and the score for a
derivation is defined as7
</bodyText>
<equation confidence="0.99845875">
score(d) = scoreLM(e(d)) + scoreSY N(t(d))
n
+ scoreR(d) + E scoreP(qj) (1)
j=1
</equation>
<bodyText confidence="0.986125">
The components of the model are as follows:
</bodyText>
<listItem confidence="0.99190655">
• scoreLM(e(d)) is the log probability of the
English string under a trigram language model.
• scoreSY N(t(d)) is the log probability of the
English parse tree under a syntactic language
model, similar to (Charniak, 2001), that associates
probabilities with lexical dependencies.
• scoreR(d) will be used to score the pars-
ing operations in 7r, based on the source-language
string and the alignments in the s-phrases. This
part of the model is described extensively in sec-
tion 4.1 of this paper.
• scoreP(q) is the score for an s-phrase q.
This score is a log-linear combination of var-
ious features, including features that are com-
monly found in phrase-based systems: for exam-
ple log P(f(q)|e(q)), log P(e(q)|f(q)), and lex-
ical translation probabilities. In addition, we in-
clude a feature log P(t(q)|f(q), e(q)), which cap-
tures the probability of the phrase in question hav-
ing the syntactic structure t(q).
</listItem>
<bodyText confidence="0.944074222222222">
Note that a model that includes the terms
scoreLM(e(d)) and Enj=1 scoreP(qj) alone
would essentially be a basic phrase-based
model (with no distortion terms). The terms
scoreSY N(t(d)) and scoreR(d) add syntactic
information to this basic model.
A key motivation for this model is the flexibility
of the reordering operations that it allows. How-
ever, the approach raises two major challenges:
</bodyText>
<footnote confidence="0.9656765">
7In practice, MERT training (Och, 2003) will be used to
train relative weights for the different model components.
</footnote>
<bodyText confidence="0.999766416666667">
Constraints on reorderings. Relaxing the op-
erations in the parsing model will allow complex
reorderings to be captured, but will also introduce
many spurious possibilities. As one example, con-
sider the derivation step shown in figure 4. This
step may receive a high probability from a syntac-
tic or surface language model—no discrimination
is a quite plausible NP in English—but it should
be ruled out for other reasons, for example be-
cause it does not respect the dependencies in the
original German (i.e., keine/no is not a modifier
to diskriminierung/discrimination in the German
string). The challenge will be to develop either
hard constraints which rule out spurious derivation
steps such as these, or soft constraints, encapsu-
lated in scoreR(d), which penalize them.
Efficient search. Exact search for the derivation
which maximizes the score in Eq. 1 cannot be
accomplished efficiently using dynamic program-
ming (as in phrase-based systems, it is easy to
show that the decoding problem is NP-complete).
Approximate search methods will be needed.
The next two sections of this paper describe so-
lutions to these two challenges.
</bodyText>
<sectionHeader confidence="0.998215" genericHeader="method">
4 Constraints on Reorderings
</sectionHeader>
<subsectionHeader confidence="0.996041">
4.1 A Discriminative Dependency Model
</subsectionHeader>
<bodyText confidence="0.999856916666667">
We now describe the model scoreR introduced in
the previous section. Recall that 7r specifies k ad-
junction operations that are used to build a full
parse tree, where k ≥ n is the number of treelets
within the sequence of s-phrases q = (q1 ... qn).
Each of the k adjunction operations creates a
dependency between a modifier word wm within
a phrase qm, and a head word wh within a phrase
qh. For example, in the example in section 3.3
where these criticisms was combined with take,
the modifier word is criticisms and the head word
is take. The modifier and head words have TAG
spines sm and sh respectively. In addition we can
define (am, bm) to be the start and end indices of
the words in the foreign string to which the word
wm is aligned; this information can be recovered
because the s-phrase qm contains alignment infor-
mation for all target words in the phrase, includ-
ing wm. Similarly, we can define (ah, bh) to be
alignment information for the head word wh. Fi-
nally, we can define p to be a binary flag speci-
fying whether or not the adjunction operation in-
volves reordering (in the take criticism example,
this flag is set to true, because the order in En-
</bodyText>
<figure confidence="0.998876">
DT
NP
NP
⇒ NP
NP
hierarchy
of
hierarchy
of
no
no
discrimination
PP
NPB
NPB
DT
discrimination
PP
</figure>
<page confidence="0.556417">
204
</page>
<figure confidence="0.879273">
these criticisms take
wir m¨ussen auch diese kritik ernst nehmen
</figure>
<figureCaption confidence="0.875928333333333">
Figure 5: An adjunction operation that involves the mod-
ifier criticisms and the head take. The phrases involved are
underlined; the dotted lines show alignments within s-phrases
between English words and positions in the German string.
The Γ-dependency in this case includes the head and modi-
fier words, together with their spines, and their alignments to
positions in the German string (kritik and nehmen).
glish is reversed from that in German). This leads
to the following definition:
</figureCaption>
<construct confidence="0.850246">
Definition 2 Given a derivation d = (q, 7r), we
define F(d) to be the set of F-dependencies
in d. Each F-dependency is a tuple
(wm, sm, am, bm, wh, sh, ah, bh, ρ) of elements as
described above.
</construct>
<bodyText confidence="0.917897666666667">
Figure 5 gives an illustration of how an adjunction
creates one such F-dependency.
The model is then defined as
</bodyText>
<equation confidence="0.995093">
�scoreR(d) = scorer(-y,f)
ry∈Γ(d)
</equation>
<bodyText confidence="0.999913476190476">
where scorer(-y, f) is a score associated with the
F-dependency -y. This score can potentially be
sensitive to any information in -y or the source-
language string f; in particular, note that the align-
ment indices (am, bm) and (ah, bh) essentially
anchor the target-language dependency to posi-
tions in the source-language string, allowing the
score for the dependency to be based on features
that have been widely used in discriminative de-
pendency parsing, for example features based on
the proximity of the two positions in the source-
language string, the part-of-speech tags in the sur-
rounding context, and so on. These features have
been shown to be powerful in the context of regu-
lar dependency parsing, and our intent is to lever-
age them in the translation problem.
In our model, we define scorer as follows. We
estimate a model P(y|-y, f) where y E {−1, +1},
and y = +1 indicates that a dependency does exist
between wm and wh, and y = −1 indicates that a
dependency does not exist. We then define
</bodyText>
<equation confidence="0.486565">
scorer(-y, f) = log P(+1|-y, f)
</equation>
<bodyText confidence="0.999884611111111">
To estimate P(y|-y, f), we first extract a set of la-
beled training examples of the form (yi, -yi, fi) for
i = 1... N from our training data as follows:
for each pair of target-language words (wm, wh)
seen in the training data, we can extract associ-
ated spines (sm, sh) from the relevant parse tree,
and also extract a label y indicating whether or not
a head-modifier dependency is seen between the
two words in the parse tree. Given an s-phrase in
the training example that includes wm, we can ex-
tract alignment information (am, bm) from the s-
phrase; we can extract similar information (ah, bh)
for wh. The end result is a training example of the
form (y, -y, f).8 We then estimate P(y|-y, f) using
a simple backed-off model that takes into account
the identity of the two spines, the value for the flag
r, the distance between (am, bm) and (ah, bh), and
part-of-speech information in the source language.
</bodyText>
<subsectionHeader confidence="0.999373">
4.2 Contiguity of 7r-Constituents
</subsectionHeader>
<bodyText confidence="0.993948628571428">
We now describe a second type of constraint,
which limits the amount of non-projectivity in
derivations. Consider again the k adjunction op-
erations in 7r, which are used to connect treelets
into a full parse tree. Each adjunction operation
involves a head treelet that dominates a modifier
treelet. Thus for any treelet t, we can consider its
descendants, that is, the entire set of treelets that
are directly or indirectly dominated by t. We de-
fine a 7r-constituent for treelet t to be the subset
of source-language words dominated by t and its
descendants. We then introduce the following con-
straint on 7r-constituents:
Definition 3 (7r-constituent constraint.) A 7r-
constituent is contiguous iff it consists of a con-
tiguous sequence of words in the source language.
A derivation 7r satisfies the 7r-constituent con-
straint iff all 7r-constituents that it contains are
contiguous.
In this paper we constrain all derivations to sat-
isfy the 7r-constituent constraint (future work may
consider probabilistic versions of the constraint).
The intuition behind the constraint deserves
more discussion. The constraint specifies that the
modifiers to each treelet can appear in any or-
der around the treelet, with arbitrary reorderings
or non-projective operations. However, once a
treelet has taken all its modifiers, the resulting 7r-
constituent must form a contiguous sub-sequence
8To be precise, there may be multiple (or even zero) s-
phrases which include w. or wh, and these s-phrases may
include conflicting alignment information. Given n. differ-
ent alignments seen for w., and nh different alignments seen
for wh, we create n. × nh training examples, which include
all possible combinations of alignments.
</bodyText>
<figure confidence="0.84613525">
NP
DT N
VP
N
</figure>
<page confidence="0.997165">
205
</page>
<bodyText confidence="0.999947238095238">
of the source-language string. As one set of exam-
ples, consider the translations in figure 3, and the
example given in the introduction. These exam-
ples involve reordering of arguments and adjuncts
within clauses, a very common case of reordering
in translation from German to English. The re-
orderings in these translations are quite flexible,
but in all cases satisfy the 7r-constituent constraint.
As an illustration of a derivation that violates
the constraint, consider again the derivation step
shown in figure 4. This step has formed a par-
tial hypothesis, no discrimination, which corre-
sponds to the German words keine and diskrim-
inierung, which do not form a contiguous sub-
string in the German. Consider now a complete
derivation, which derives the string there is hier-
archy ofno discrimination, and which includes the
7r-constituent no discrimination shown in the fig-
ure (i.e., where the treelet discrimination takes no
as its only modifier). This derivation will violate
the 7r-constituent constraint.9
</bodyText>
<sectionHeader confidence="0.988968" genericHeader="method">
5 Decoding
</sectionHeader>
<bodyText confidence="0.9999859">
We now describe decoding algorithms for the syn-
tactic models: we first describe inference rules
that are used to combine pieces of structure, and
then describe heuristic search algorithms that use
these inference rules. Throughout this section,
for brevity and simplicity, we describe algorithms
that apply under the assumption that each s-phrase
has a single associated treelet. The generalization
to the case where an s-phrase may have multiple
treelets is discussed in section 5.3.
</bodyText>
<subsectionHeader confidence="0.960414">
5.1 Inference Rules
</subsectionHeader>
<bodyText confidence="0.999974428571429">
Parsing operations for the TAG grammars de-
scribed in (Carreras et al., 2008) are based on
the dynamic programming algorithms in (Eisner,
2000). A critical idea in dynamic programming al-
gorithms such as these is to associate constituents
in a chart with spans of the input sentence, and
to introduce inference rules that combine con-
stituents into larger pieces of structure. The crucial
step in generalizing these algorithms to the non-
projective case, and to translation, will be to make
use of bit-strings that keep track of which words in
the German have already been translated in a chart
entry. To return to the example from the intro-
duction, again assume that the selected s-phrases
</bodyText>
<footnote confidence="0.960077">
9Note, however, that the derivation step show in figure 4
will be considered in the search, because if discrimination
takes additional modifiers, and thereby forms a ir-constituent
that dominates a contiguous sub-string in the German, then
the resulting derivation will be valid.
</footnote>
<listItem confidence="0.891329625">
0. Data structures: Qi for i = 1... n is a set of hypotheses
for each length i, S is a set of chart entries
1. S ← ∅
2. Initialize Q1 ... Qn with basic chart entries derived
from phrase entries
3. For i = 1 ... n
4. For any A G BEAM(Qi)
5. If S contains a chart entry with the same signature
as A, and which has a higher inside score,
6. continue
7. Else
8. Add A to S
9. For any chart entry C that can be derived from
A together with another chart entry B G S,
add C to the set Qj where j = length(C)
10. Return Qn, a set of items of length n
</listItem>
<figureCaption confidence="0.983214">
Figure 6: A beam search algorithm. A dynamic-
programming signature consists of the regular dynamic-
programming state for the parsing algorithm, together with
the span (bit-string) associated with a constituent.
</figureCaption>
<bodyText confidence="0.999641444444444">
segment the German input into [wir m¨ussen auch]
[diese kritik] [ernst] [nehmen], and the treelets are
as shown in the introduction. Each of these treelets
will form a basic entry in the chart, and will have
an associated bit-string indicating which German
words have been translated by that entry.
These basic chart entries can then be combined
to form larger pieces of structure. For example,
the following inferential step is possible:
</bodyText>
<equation confidence="0.9221">
⇒ VP/0001101
V NP
</equation>
<bodyText confidence="0.9766206875">
take these criticisms
We have shown the bit-string representation for
each consituent: for example, the new constituent
has the bit-string 0001101 representing the fact
that the non-contiguous sub-strings diese kritik
and nehmen have been translated at this point. Any
two constituents can be combined, providing that
the logical AND of their bit-strings is all 0’s.
Inference steps such as that shown above will
have an associated score corresponding to the
TAG adjunction that is involved: in our mod-
els, both scoreSY N and scoreR will contribute to
this score. In addition, we add state—specifically,
word bigrams at the start and end of constituents—
that allows trigram language model scores to be
calculated as constituents are combined.
</bodyText>
<subsectionHeader confidence="0.998509">
5.2 Approximate Search
</subsectionHeader>
<bodyText confidence="0.9989585">
There are 2n possible bit-strings for a sentence of
length n, hence the search space is of exponen-
tial size; approximate algorithms are therefore re-
quired in search for the highest scoring derivation.
Figure 6 shows a beam search algorithm which
makes use of the inference rules described in the
</bodyText>
<figure confidence="0.796563333333333">
NP/0001100 VP/0000001
these criticisms V
take
</figure>
<page confidence="0.997104">
206
</page>
<bodyText confidence="0.9995914">
previous section. The algorithm stores sets Qi
for i = 1... n, where n is the source-language
sentence length; each set Qi stores hypotheses of
length i (i.e., hypotheses with an associated bit-
string with i ones). These sets are initialized with
basic entries derived from s-phrases.
The function BEAM(Qi) returns all items
within Qi that have a high enough score to fall
within a beam (more details for BEAM are given
below). At each iteration (step 4), each item in
turn is taken from BEAM(Qi) and added to a
chart; the inference rules described in the previ-
ous section are used to derive new items which are
added to the appropriate set Qj, where j &gt; i.
We have found the definition of BEAM(Qi) to
be critical to the success of the method. As a first
step, each item in Qi receives a score that is a sum
of an inside score (the cost of all derivation steps
used to create the item) and a future score (an esti-
mate of the cost to complete the translation). The
future score is based on the source-language words
that are still to be translated—this can be directly
inferred from the item’s bit-string—this is similar
to the use of future scores in Pharoah (Koehn et al.,
2003), and in fact we use Pharoah’s future scores
in our model. We then give the following defini-
tion, where N is a parameter (the beam size):
Definition 4 (BEAM) Given Qi, define Qi,j for
j = 1... n to be the subset of items in Qi which
have their j’th bit equal to one (i.e., have the j’th
source language word translated). Define Q′i,j to
be the N highest scoring elements in Qi,j. Then
BEAM(Qi) = ∪n j=1Q′i,j.
To motivate this definition, note that a naive
method would simply define BEAM(Qi) to be
the N highest scoring elements of Qi. This def-
inition, however, assumes that constituents which
form translations of different parts of a sentence
have scores that can be compared—an assumption
that would be true if the future scores were highly
accurate, but which quickly breaks down when fu-
ture scores are inaccurate. In contrast, the defi-
nition above ensures that the top N analyses for
each of the n source language words are stored at
each stage, and hence that all parts of the source
sentence are well represented. In experiments, the
naive approach was essentially a failure, with pars-
ing of some sentences either failing or being hope-
lessly inefficient, depending on the choice of N.
In contrast, definition 4 gives good results.
</bodyText>
<table confidence="0.9934925">
System BLEU score
Syntax-based 25.2
Syntax (no ScoreR) 23.7 (-1.5)
Syntax (no ir-c constraint) 24.4 (-0.8)
</table>
<tableCaption confidence="0.959414">
Table 1: Development set results showing the effect of re-
moving ScoreR or the ir-constituent constraint.
</tableCaption>
<subsectionHeader confidence="0.995089">
5.3 Allowing Multiple Treelets per s-Phrase
</subsectionHeader>
<bodyText confidence="0.9999565">
The decoding algorithms that we have described
apply in the case where each s-phrase has a sin-
gle treelet. The extension of these algorithms
to the case where a phrase may have multiple
treelets (e.g., see figure 2) is straightforward, but
for brevity the details are omitted. The basic idea
is to extend bit-string representations with a record
of “pending” treelets which have not yet been in-
cluded in a derivation. It is also possible to enforce
the 7r-constituent constraint during decoding, as
well as a constraint that ensures that reordering op-
erations do not “break apart” English sub-strings
within s-phrases that have multiple treelets (for ex-
ample, for the s-phrase in figure 2, we ensure that
there is no remains as a contiguous sequence of
words in any translation using this s-phrase).
</bodyText>
<sectionHeader confidence="0.999848" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999976291666667">
We trained the syntax-based system on 751,088
German-English translations from the Europarl
corpus (Koehn, 2005). A syntactic language
model was also trained on the English sentences
in the training data. We used Pharoah (Koehn et
al., 2003) as a baseline system for comparison; the
s-phrases used in our system include all phrases,
with the same scores, as those used by Pharoah,
allowing a direct comparison. For efficiency rea-
sons we report results on sentences of length 30
words or less.10 The syntax-based method gives
a BLEU (Papineni et al., 2002) score of 25.04,
a 0.46 BLEU point gain over Pharoah. This re-
sult was found to be significant (p = 0.021) under
the paired bootstrap resampling method of Koehn
(2004), and is close to significant (p = 0.058) un-
der the sign test of Collins et al. (2005).
Table 1 shows results for the full syntax-based
system, and also results for the system with the
discriminative dependency scores (see section 4.1)
and the 7r-contituent constraint removed from the
system. In both cases we see a clear impact of
these components of the model, with 1.5 and 0.8
BLEU point decrements respectively.
</bodyText>
<footnote confidence="0.992708666666667">
10Both Pharoah and our system have weights trained using
MERT (Och, 2003) on sentences of length 30 words or less,
to ensure that training and test conditions are matched.
</footnote>
<page confidence="0.985636">
207
</page>
<table confidence="0.55726955">
R: in our eyes , the opportunity created by this directive of introducing longer buses on international routes is efficient.
S: the opportunity now presented by this directive is effective in our opinion , to use long buses on international routes .
P: the need for this directive now possibility of longer buses on international routes to is in our opinion, efficiently.
R: europe and asia must work together to intensify the battle against drug trafficking , money laundering , international
crime , terrorism and the sexual exploitation of minors .
S: europe and asia must work together in order to strengthen the fight against drug trafficking , money laundering , against
international crime , terrorism and the sexual exploitation of minors .
P: europe and asia must cooperate in the fight against drug trafficking , money laundering , against international crime ,
terrorism and the sexual exploitation of minors strengthened.
R: equally important for the future of europe - at biarritz and later at nice - will be the debate on the charter of fundamental
rights .
S: it is equally important for the future of europe to speak on the charter of fundamental rights in biarritz , and then in nice .
P: just as important for the future of europe , it will be in biarritz and then in nice on the charter of fundamental rights to
speak.
R: the convention was thus a muddled system , generating irresponsibility , and not particularly favourable to well-ordered
democracy.
S: therefore , the convention has led to a system of a promoter of irresponsibility of the lack of clarity and hardly coincided
with the rules of a proper democracy.
P: the convention therefore led to a system of full of lack of clarity and hardly a promoter of the irresponsibility of the rules
of orderly was a democracy .
</table>
<figureCaption confidence="0.97023675">
Figure 7: Examples where both annotators judged the syntactic system to give an improved translation when compared to
the baseline system. 51 out of 200 translations fall into this category. These examples were chosen at random from these 51
examples. R is the human (reference) translation; S is the translation from the syntax-based system; P is the output from the
baseline (phrase-based) system.
</figureCaption>
<table confidence="0.9997386">
Syntax PB = Total
Syntax 51 3 7 61
PB 1 25 11 37
= 21 14 67 102
Total 73 42 85 200
</table>
<tableCaption confidence="0.997359">
Table 2: Human annotator judgements. Rows show re-
</tableCaption>
<bodyText confidence="0.968140086956522">
sults for annotator 1, and columns for annotator 2. Syntax
and PB show the number of cases where an annotator re-
spectively preferred/dispreferred the syntax-based system. _
gives counts of translations judged to be equal in quality.
In addition, we obtained human evaluations on
200 sentences chosen at random from the test data,
using two annotators. For each example, the ref-
erence translation was presented to the annota-
tor, followed by translations from the syntax-based
and phrase-based systems (in a random order). For
each example, each annotator could either decide
that the two translations were of equal quality, or
that one translation was better than the other. Ta-
ble 2 shows results of this evaluation. Both an-
notators show a clear preference for the syntax-
based system: for annotator 1, 73 translations are
judged to be better for the syntax-based system,
with 42 translations being worse; for annotator 2,
61 translations are improved with 37 being worse;
both annotators’ results are statistically significant
with p &lt; 0.05 under the sign test. Figure 7 shows
some translation examples where the syntax-based
system was judged to give an improvement.
</bodyText>
<sectionHeader confidence="0.9985" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.930754705882353">
We have described a translation model that makes
use of flexible parsing operations, critical ideas
being the definition of s-phrases, F-dependencies,
the π-constituent constraint, and an approximate
search algorithm. A key area for future work
will be further development of the discriminative
dependency model (section 4.1). The model of
scorer(γ, f) that we have described in this paper is
relatively simple; in general, however, there is the
potential for scorer to link target language depen-
dencies to arbitrary properties of the source lan-
guage string f (recall that γ contains a head and
modifier spine in the target language, along with
positions in the source-language string to which
these spines are aligned). For example, we might
introduce features that: a) condition dependencies
created in the target language on dependency re-
lations between their aligned words in the source
language; b) condition target-language dependen-
cies on whether they are aligned to words that
are in the same clause or segment in the source
language string; or, c) condition the grammatical
roles of nouns in the target language on grammat-
ical roles of aligned words in the source language.
These features should improve translation qual-
ity by giving a tighter link between syntax in the
source and target languages, and would be easily
incorporated in the approach we have described.
Acknowledgments We would like to thank Ryan Mc-
Donald for conversations that were influential in this work,
and Meg Aycinena Lippow and Ben Snyder for translation
judgments. This work was supported under the GALE pro-
gram of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022.
</bodyText>
<page confidence="0.99541">
208
</page>
<bodyText confidence="0.9956965">
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
Spmt: Statistical machine translation with syntac-
tified target language phrases. In Proceedings of
EMNLP.
</bodyText>
<sectionHeader confidence="0.894312" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.961865">
H. Alshawi. 1996. Head automata and bilingual tiling:
Translation with minimal representations. In Pro-
ceedings ofACL, pages 167–176.
</bodyText>
<reference confidence="0.999831811111111">
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming and the perceptron for efficient,
feature-rich parsing. In Proc. of CoNLL.
E. Charniak, K. Knight, and K. Yamada. 2003.
Syntax-based language models for machine transla-
tion. In Proceedings ofMT Summit IX.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In Proceedings ofACL 2001.
C. Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of
ACL-08: HLT, pages 72–80, Columbus, Ohio, June.
Association for Computational Linguistics.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause
restructuring for statistical machine translation. In
Proceedings ofACL.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics, pages 16–23, Madrid, Spain,
July. Association for Computational Linguistics.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In H. C. Bunt and A. Ni-
jholt, editors, New Developments in Natural Lan-
guage Parsing, pages 29–62. Kluwer Academic
Publishers.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of
ACL.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and K. Salomaa, ed-
itors, Handbook of Formal Languages, volume 3,
pages 169–124. Springer.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statis-
tical phrase-based translation. In Proceedings of
HLT/NAACL.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of MT
Summit.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings ofACL.
D. Melamed. 2004. Statistical machine translation by
parsing. In Proceedings ofACL.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-based
translation. In Proceedings ofACL-08: HLT, pages
192–199. Association for Computational Linguis-
tics.
R. Nesson, S.M. Shieber, and A. Rush. 2006. In-
duction of probabilistic synchronous tree-insertion
grammars for machine translation. In Proceedings
of the 7th AMTA.
F.J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proceedings ofACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings ofACL, pages 311–318.
Association for Computational Linguistics.
C. Quirk, A. Menezes, and Colin Cherry. 2005. De-
pendency tree translation: Syntactically informed
phrasal smt. In Proceedings ofACL.
O. Rambow, K. Vijay-Shanker, and D. Weir. 1995.
D-tree grammars. In Proceedings of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 151–158, Cambridge, Mas-
sachusetts, USA, June. Association for Computa-
tional Linguistics.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Pro-
ceedings ofACL.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377–404.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings ofACL.
H. Zhang and D. Gildea. 2005. Stochastic lexicalized
inversion transduction grammar for alignment. In
Proceedings ofACL, pages 473–482.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
Proceedings of NAACL 2006 Workshop on Statisti-
cal Machine Translation.
</reference>
<page confidence="0.998961">
209
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.479405">
<title confidence="0.999956">Non-Projective Parsing for Statistical Machine Translation</title>
<author confidence="0.996837">Xavier Carreras Michael Collins</author>
<affiliation confidence="0.495928">MIT CSAIL, Cambridge, MA 02139,</affiliation>
<abstract confidence="0.997769733333333">We describe a novel approach for syntaxbased statistical MT, which builds on a variant of tree adjoining grammar (TAG). Inspired by work in discriminative dependency parsing, the key idea in our approach is to allow highly flexible reordering operations during parsing, in combination with a discriminative model that can condition on rich features of the sourcelanguage string. Experiments on translation from German to English show improvements over phrase-based systems, both in terms of BLEU scores and in human evaluations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>TAG, dynamic programming and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="1435" citStr="Carreras et al., 2008" startWordPosition="213" endWordPosition="216">s; many such approaches are based on either synchronous grammars (e.g., (Chiang, 2005)), or tree transducers (e.g., (Marcu et al., 2006)). This paper describes an alternative approach for syntax-based SMT, which directly leverages methods from non-projective dependency parsing. The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al., 2008)). The models we describe make use of phrasal entries augmented with subtrees that provide syntactic information in the target language. As one example, when translating the sentence wir m¨ussen auch diese kritik ernst nehmen from German into English, the following sequence of syntactic phrasal entries might be used (we show each English syntactic fragment above its associated German sub-string): wir m¨ussen auch diese kritik ernst nehmen TAG parsing operations are then used to combine these fragments into a full parse tree, giving the final English translation we must also take these criticis</context>
<context position="7066" citStr="Carreras et al., 2008" startWordPosition="1105" endWordPosition="1108">le parsing operations. Rather than stating reordering rules at the level of source or target language parse trees, we capture reordering phenomena using a discriminative dependency model. Other factors that distinguish us from previous work are the use of all phrases proposed by a phrase-based system, and the use of a dependency language model that also incorporates constituent information (although see (Charniak et al., 2003; Shen et al., 2008) for related approaches). 3 A Syntactic Translation Model 3.1 Background Our work builds on the variant of tree adjoining grammar (TAG) introduced by (Carreras et al., 2008). In this formalism the basic units in the grammar are spines, which associate tree fragments with lexical items. These spines can be combined using a sister-adjunction operation (Rambow et al., 1995), to form larger pieces of structure.2 For example, we might have the following operation: is there is In this case the spine for there has sister-adjoined into the S node in the spine for is; we refer to the spine for there as being the modifier spine, and the spine for is being the head spine. There are close connections to dependency formalisms: in particular in this operation we see a lexical </context>
<context position="8311" citStr="Carreras et al., 2008" startWordPosition="1323" endWordPosition="1326">modifier word there and the head word is. It is possible to define syntactic language models, similar to (Charniak, 2001), which associate probabilities with these dependencies, roughly speaking of the form P(wm, sm|wh, sh, pos, Q), where wm and sm are the identities of the modifier word and spine, wh and sh are the identities of the head word and spine, pos is the position in the head spine that is being adjoined into, and Q is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2We also make use of the r-adjunction operation defined in (Carreras et al., 2008), which, together with sister-adjunction, allows us to model the full range of structures found in the Penn treebank. ⇒ S NP VP S VP NP there 201 S es gibt keine hierarchie der diskriminierung Figure 1: A training example consisting of an English (target language) tree and a German (source language) sentence. In this paper we will also consider treelets, which are a generalization of spines, and which allow lexical entries that include more than one word. These treelets can again be combined using a sister-adjunction operation. As an example, consider the following operation: In this case the </context>
<context position="10604" citStr="Carreras et al., 2008" startWordPosition="1712" endWordPosition="1715">se-based systems (Koehn et al., 2003) to define the set of phrase entries for each sentence in training data. es gibt keine hierarchie der Figure 2: Example syntactic phrase entries. We show German sub-strings above their associated sequence of treelets.4 For each phrase entry, we add syntactic information to the English string. To continue our example, the resulting entry would be as follows: es gibt ⇒ S NP there To give a more formal description of how syntactic structures are derived for phrases, first note that each parse tree t is mapped to a TAG derivation using the method described in (Carreras et al., 2008). This procedure uses the head finding rules of (Collins, 1997). The resulting derivation consists of a TAG spine for each word seen in the sentence, together with a set of adjunction operations which each involve a modifier spine and a head spine. Given an English string e = el ... er,,, with an associated parse tree t, the syntactic structure associated with a substring ek ... el (e.g., there is) is then defined as follows: • For each word in the English sub-string, include its associated TAG spine in t. • In addition, include any adjunction operations in t where both the head and modifier w</context>
<context position="16376" citStr="Carreras et al., 2008" startWordPosition="2743" endWordPosition="2747">ion. Formally, given an input sentence f, a derivation d is a pair (q, 7r) where: • q = q1 . . . qn is a sequence of s-phrases such that f = f(q1)® f(q2)® ...® f(qn) (where u ® v denotes the concatenation of strings u and v). • 7r is a set of adjunction operations that connects the sequence of treelets contained in (t(q1), t(q2), ... , t(qn)) into a parse tree in the target language. The operations allow a complete relaxation of word order, potentially allowing any of the n! possible orderings of the n sphrases. We make use of both sister-adjunction and r-adjunction operations, as defined in (Carreras et al., 2008).6 6In principle we allow any treelet to adjoin into any other treelet—for example there are no hard, grammar-based constraints ruling out the combination of certain pairs of nonterminals. Note however that in some cases operations will have probability 0 under the syntactic language model introduced later in this section. S PP ADVP in future more quickly VP the admin... must 1(c) NP SG to VP respond VP be ADJP able take these criticisms VP must ADVP also NP we seriously V NP NP V ADVP NP we take seriously VP must ADVP also these criticisms 2(c) PP in my opinion 203 Figure 4: A spurious deriva</context>
<context position="27613" citStr="Carreras et al., 2008" startWordPosition="4625" endWordPosition="4628">e 7r-constituent constraint.9 5 Decoding We now describe decoding algorithms for the syntactic models: we first describe inference rules that are used to combine pieces of structure, and then describe heuristic search algorithms that use these inference rules. Throughout this section, for brevity and simplicity, we describe algorithms that apply under the assumption that each s-phrase has a single associated treelet. The generalization to the case where an s-phrase may have multiple treelets is discussed in section 5.3. 5.1 Inference Rules Parsing operations for the TAG grammars described in (Carreras et al., 2008) are based on the dynamic programming algorithms in (Eisner, 2000). A critical idea in dynamic programming algorithms such as these is to associate constituents in a chart with spans of the input sentence, and to introduce inference rules that combine constituents into larger pieces of structure. The crucial step in generalizing these algorithms to the nonprojective case, and to translation, will be to make use of bit-strings that keep track of which words in the German have already been translated in a chart entry. To return to the example from the introduction, again assume that the selected</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>X. Carreras, M. Collins, and T. Koo. 2008. TAG, dynamic programming and the perceptron for efficient, feature-rich parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>K Knight</author>
<author>K Yamada</author>
</authors>
<title>Syntax-based language models for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofMT Summit IX.</booktitle>
<contexts>
<context position="6873" citStr="Charniak et al., 2003" startWordPosition="1073" endWordPosition="1076"> learned automatically from corpora. A critical difference in our work is to allow arbitrary reorderings of the source language sentence (as in phrase-based systems), through the use of flexible parsing operations. Rather than stating reordering rules at the level of source or target language parse trees, we capture reordering phenomena using a discriminative dependency model. Other factors that distinguish us from previous work are the use of all phrases proposed by a phrase-based system, and the use of a dependency language model that also incorporates constituent information (although see (Charniak et al., 2003; Shen et al., 2008) for related approaches). 3 A Syntactic Translation Model 3.1 Background Our work builds on the variant of tree adjoining grammar (TAG) introduced by (Carreras et al., 2008). In this formalism the basic units in the grammar are spines, which associate tree fragments with lexical items. These spines can be combined using a sister-adjunction operation (Rambow et al., 1995), to form larger pieces of structure.2 For example, we might have the following operation: is there is In this case the spine for there has sister-adjoined into the S node in the spine for is; we refer to th</context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-based language models for machine translation. In Proceedings ofMT Summit IX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="2406" citStr="Charniak, 2001" startWordPosition="368" endWordPosition="369">nt above its associated German sub-string): wir m¨ussen auch diese kritik ernst nehmen TAG parsing operations are then used to combine these fragments into a full parse tree, giving the final English translation we must also take these criticisms seriously. Some key aspects of our approach are as follows: • We impose no constraints on entries in the phrasal lexicon. The method thereby retains the full set of lexical entries of phrase-based systems (e.g., (Koehn et al., 2003)).1 • The model allows a straightforward integration of lexicalized syntactic language models—for example the models of (Charniak, 2001)—in addition to a surface language model. • The operations used to combine tree fragments into a complete parse tree are significant generalizations of standard parsing operations found in TAG; specifically, they are modified to be highly flexible, potentially allowing any possible permutation (reordering) of the initial fragments. As one example of the type of parsing operations that we will consider, we might allow the tree fragments shown above for these criticisms and take to be combined to form a new structure with the sub-string take these criticisms. This step in the derivation is neces</context>
<context position="7810" citStr="Charniak, 2001" startWordPosition="1237" endWordPosition="1239">n be combined using a sister-adjunction operation (Rambow et al., 1995), to form larger pieces of structure.2 For example, we might have the following operation: is there is In this case the spine for there has sister-adjoined into the S node in the spine for is; we refer to the spine for there as being the modifier spine, and the spine for is being the head spine. There are close connections to dependency formalisms: in particular in this operation we see a lexical dependency between the modifier word there and the head word is. It is possible to define syntactic language models, similar to (Charniak, 2001), which associate probabilities with these dependencies, roughly speaking of the form P(wm, sm|wh, sh, pos, Q), where wm and sm are the identities of the modifier word and spine, wh and sh are the identities of the head word and spine, pos is the position in the head spine that is being adjoined into, and Q is some additional state (e.g., state that tracks previous modifiers that have adjoined into the same spine). 2We also make use of the r-adjunction operation defined in (Carreras et al., 2008), which, together with sister-adjunction, allows us to model the full range of structures found in </context>
<context position="17766" citStr="Charniak, 2001" startWordPosition="2987" endWordPosition="2988">erivation, and t(d) to be the complete targetlanguage parse tree created by the derivation. The most likely derivation for a foreign sentence f is arg maxd∈G(f) score(d), where G(f) is the set of possible derivations for f, and the score for a derivation is defined as7 score(d) = scoreLM(e(d)) + scoreSY N(t(d)) n + scoreR(d) + E scoreP(qj) (1) j=1 The components of the model are as follows: • scoreLM(e(d)) is the log probability of the English string under a trigram language model. • scoreSY N(t(d)) is the log probability of the English parse tree under a syntactic language model, similar to (Charniak, 2001), that associates probabilities with lexical dependencies. • scoreR(d) will be used to score the parsing operations in 7r, based on the source-language string and the alignments in the s-phrases. This part of the model is described extensively in section 4.1 of this paper. • scoreP(q) is the score for an s-phrase q. This score is a log-linear combination of various features, including features that are commonly found in phrase-based systems: for example log P(f(q)|e(q)), log P(e(q)|f(q)), and lexical translation probabilities. In addition, we include a feature log P(t(q)|f(q), e(q)), which cap</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. Immediate-head parsing for language models. In Proceedings ofACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
</authors>
<title>Cohesive phrase-based decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>72--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="5881" citStr="Cherry, 2008" startWordPosition="916" endWordPosition="917">d constraints on dependency structures give a 0.8% BLEU improvement. 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from corpora. A critical difference in our work is to allow arbitrary reorderings of the source language sentence (as in phrase-based systems), through the use of flexible parsing operations. Rather than st</context>
</contexts>
<marker>Cherry, 2008</marker>
<rawString>C. Cherry. 2008. Cohesive phrase-based decoding for statistical machine translation. In Proceedings of ACL-08: HLT, pages 72–80, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="899" citStr="Chiang, 2005" startWordPosition="133" endWordPosition="134">G). Inspired by work in discriminative dependency parsing, the key idea in our approach is to allow highly flexible reordering operations during parsing, in combination with a discriminative model that can condition on rich features of the sourcelanguage string. Experiments on translation from German to English show improvements over phrase-based systems, both in terms of BLEU scores and in human evaluations. 1 Introduction Syntax-based models for statistical machine translation (SMT) have recently shown impressive results; many such approaches are based on either synchronous grammars (e.g., (Chiang, 2005)), or tree transducers (e.g., (Marcu et al., 2006)). This paper describes an alternative approach for syntax-based SMT, which directly leverages methods from non-projective dependency parsing. The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al., 2008)). The models we describe make use of phrasal entries augmented </context>
<context position="5776" citStr="Chiang, 2005" startWordPosition="898" endWordPosition="899"> BLEU point improvement over a basic model that does not condition on the source language string; the hard constraints on dependency structures give a 0.8% BLEU improvement. 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from corpora. A critical difference in our work is to allow arbitrary reorderings of the source langu</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="35120" citStr="Collins et al. (2005)" startWordPosition="5917" endWordPosition="5920">the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p = 0.058) under the sign test of Collins et al. (2005). Table 1 shows results for the full syntax-based system, and also results for the system with the discriminative dependency scores (see section 4.1) and the 7r-contituent constraint removed from the system. In both cases we see a clear impact of these components of the model, with 1.5 and 0.8 BLEU point decrements respectively. 10Both Pharoah and our system have weights trained using MERT (Och, 2003) on sentences of length 30 words or less, to ensure that training and test conditions are matched. 207 R: in our eyes , the opportunity created by this directive of introducing longer buses on int</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>M. Collins, P. Koehn, and I. Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Madrid, Spain,</location>
<contexts>
<context position="10667" citStr="Collins, 1997" startWordPosition="1724" endWordPosition="1725">es for each sentence in training data. es gibt keine hierarchie der Figure 2: Example syntactic phrase entries. We show German sub-strings above their associated sequence of treelets.4 For each phrase entry, we add syntactic information to the English string. To continue our example, the resulting entry would be as follows: es gibt ⇒ S NP there To give a more formal description of how syntactic structures are derived for phrases, first note that each parse tree t is mapped to a TAG derivation using the method described in (Carreras et al., 2008). This procedure uses the head finding rules of (Collins, 1997). The resulting derivation consists of a TAG spine for each word seen in the sentence, together with a set of adjunction operations which each involve a modifier spine and a head spine. Given an English string e = el ... er,,, with an associated parse tree t, the syntactic structure associated with a substring ek ... el (e.g., there is) is then defined as follows: • For each word in the English sub-string, include its associated TAG spine in t. • In addition, include any adjunction operations in t where both the head and modifier word are in the sub-string ej ... ek. In the above example, the </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16–23, Madrid, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical grammars and their cubictime parsing algorithms. In</title>
<date>2000</date>
<booktitle>New Developments in Natural Language Parsing,</booktitle>
<pages>29--62</pages>
<editor>H. C. Bunt and A. Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="27679" citStr="Eisner, 2000" startWordPosition="4637" endWordPosition="4638">s for the syntactic models: we first describe inference rules that are used to combine pieces of structure, and then describe heuristic search algorithms that use these inference rules. Throughout this section, for brevity and simplicity, we describe algorithms that apply under the assumption that each s-phrase has a single associated treelet. The generalization to the case where an s-phrase may have multiple treelets is discussed in section 5.3. 5.1 Inference Rules Parsing operations for the TAG grammars described in (Carreras et al., 2008) are based on the dynamic programming algorithms in (Eisner, 2000). A critical idea in dynamic programming algorithms such as these is to associate constituents in a chart with spans of the input sentence, and to introduce inference rules that combine constituents into larger pieces of structure. The crucial step in generalizing these algorithms to the nonprojective case, and to translation, will be to make use of bit-strings that keep track of which words in the German have already been translated in a chart entry. To return to the example from the introduction, again assume that the selected s-phrases 9Note, however, that the derivation step show in figure</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>J. Eisner. 2000. Bilexical grammars and their cubictime parsing algorithms. In H. C. Bunt and A. Nijholt, editors, New Developments in Natural Language Parsing, pages 29–62. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5723" citStr="Eisner, 2003" startWordPosition="890" endWordPosition="891">tem. The discriminative dependency model gives a 1.5% BLEU point improvement over a basic model that does not condition on the source language string; the hard constraints on dependency structures give a 0.8% BLEU improvement. 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from corpora. A critical difference in our work </context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>J. Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-adjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>169--124</pages>
<editor>In G. Rozenberg and K. Salomaa, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1378" citStr="Joshi and Schabes, 1997" startWordPosition="205" endWordPosition="208">ine translation (SMT) have recently shown impressive results; many such approaches are based on either synchronous grammars (e.g., (Chiang, 2005)), or tree transducers (e.g., (Marcu et al., 2006)). This paper describes an alternative approach for syntax-based SMT, which directly leverages methods from non-projective dependency parsing. The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al., 2008)). The models we describe make use of phrasal entries augmented with subtrees that provide syntactic information in the target language. As one example, when translating the sentence wir m¨ussen auch diese kritik ernst nehmen from German into English, the following sequence of syntactic phrasal entries might be used (we show each English syntactic fragment above its associated German sub-string): wir m¨ussen auch diese kritik ernst nehmen TAG parsing operations are then used to combine these fragments into a full parse tree, giving the f</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>A.K. Joshi and Y. Schabes. 1997. Tree-adjoining grammars. In G. Rozenberg and K. Salomaa, editors, Handbook of Formal Languages, volume 3, pages 169–124. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="2270" citStr="Koehn et al., 2003" startWordPosition="347" endWordPosition="350">st nehmen from German into English, the following sequence of syntactic phrasal entries might be used (we show each English syntactic fragment above its associated German sub-string): wir m¨ussen auch diese kritik ernst nehmen TAG parsing operations are then used to combine these fragments into a full parse tree, giving the final English translation we must also take these criticisms seriously. Some key aspects of our approach are as follows: • We impose no constraints on entries in the phrasal lexicon. The method thereby retains the full set of lexical entries of phrase-based systems (e.g., (Koehn et al., 2003)).1 • The model allows a straightforward integration of lexicalized syntactic language models—for example the models of (Charniak, 2001)—in addition to a surface language model. • The operations used to combine tree fragments into a complete parse tree are significant generalizations of standard parsing operations found in TAG; specifically, they are modified to be highly flexible, potentially allowing any possible permutation (reordering) of the initial fragments. As one example of the type of parsing operations that we will consider, we might allow the tree fragments shown above for these cr</context>
<context position="10019" citStr="Koehn et al., 2003" startWordPosition="1607" endWordPosition="1610">s from training examples consisting of a source-language string paired with a target-language parse tree. For example, consider the training example in figure 1. We assume some method that enumerates a set of possible phrase entries for each training example: each phrase entry is a pair ((i, j), (k, l)) specifying that source-language words fi ... fj correspond to target-language words ek ... el in the example. For example, one phrase entry for the example might be ((1, 2), (1, 2)), representing the pair (es gibt ⇒ there is). In our experiments we use standard methods in phrase-based systems (Koehn et al., 2003) to define the set of phrase entries for each sentence in training data. es gibt keine hierarchie der Figure 2: Example syntactic phrase entries. We show German sub-strings above their associated sequence of treelets.4 For each phrase entry, we add syntactic information to the English string. To continue our example, the resulting entry would be as follows: es gibt ⇒ S NP there To give a more formal description of how syntactic structures are derived for phrases, first note that each parse tree t is mapped to a TAG derivation using the method described in (Carreras et al., 2008). This procedur</context>
<context position="32005" citStr="Koehn et al., 2003" startWordPosition="5389" endWordPosition="5392">ious section are used to derive new items which are added to the appropriate set Qj, where j &gt; i. We have found the definition of BEAM(Qi) to be critical to the success of the method. As a first step, each item in Qi receives a score that is a sum of an inside score (the cost of all derivation steps used to create the item) and a future score (an estimate of the cost to complete the translation). The future score is based on the source-language words that are still to be translated—this can be directly inferred from the item’s bit-string—this is similar to the use of future scores in Pharoah (Koehn et al., 2003), and in fact we use Pharoah’s future scores in our model. We then give the following definition, where N is a parameter (the beam size): Definition 4 (BEAM) Given Qi, define Qi,j for j = 1... n to be the subset of items in Qi which have their j’th bit equal to one (i.e., have the j’th source language word translated). Define Q′i,j to be the N highest scoring elements in Qi,j. Then BEAM(Qi) = ∪n j=1Q′i,j. To motivate this definition, note that a naive method would simply define BEAM(Qi) to be the N highest scoring elements of Qi. This definition, however, assumes that constituents which form t</context>
<context position="34554" citStr="Koehn et al., 2003" startWordPosition="5816" endWordPosition="5819">e to enforce the 7r-constituent constraint during decoding, as well as a constraint that ensures that reordering operations do not “break apart” English sub-strings within s-phrases that have multiple treelets (for example, for the s-phrase in figure 2, we ensure that there is no remains as a contiguous sequence of words in any translation using this s-phrase). 6 Experiments We trained the syntax-based system on 751,088 German-English translations from the Europarl corpus (Koehn, 2005). A syntactic language model was also trained on the English sentences in the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p = 0.058) under the sign test of Collins et al. (2005). Table 1 shows results for the fu</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="35034" citStr="Koehn (2004)" startWordPosition="5901" endWordPosition="5902">05). A syntactic language model was also trained on the English sentences in the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p = 0.058) under the sign test of Collins et al. (2005). Table 1 shows results for the full syntax-based system, and also results for the system with the discriminative dependency scores (see section 4.1) and the 7r-contituent constraint removed from the system. In both cases we see a clear impact of these components of the model, with 1.5 and 0.8 BLEU point decrements respectively. 10Both Pharoah and our system have weights trained using MERT (Och, 2003) on sentences of length 30 words or less, to ensure that training and test conditions are matched. 207 R: in o</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<contexts>
<context position="34425" citStr="Koehn, 2005" startWordPosition="5796" endWordPosition="5797">g representations with a record of “pending” treelets which have not yet been included in a derivation. It is also possible to enforce the 7r-constituent constraint during decoding, as well as a constraint that ensures that reordering operations do not “break apart” English sub-strings within s-phrases that have multiple treelets (for example, for the s-phrase in figure 2, we ensure that there is no remains as a contiguous sequence of words in any translation using this s-phrase). 6 Experiments We trained the syntax-based system on 751,088 German-English translations from the Europarl corpus (Koehn, 2005). A syntactic language model was also trained on the English sentences in the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koe</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4370" citStr="McDonald et al., 2005" startWordPosition="685" endWordPosition="688">Methods in Natural Language Processing, pages 200–209, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tree fragments to be combined during the translation process, irrespective of the reorderings which are introduced, or the non-projectivity of the parsing operations that are required. The use of flexible parsing operations raises two challenges that will be a major focus of this paper. First, these operations will allow the model to capture complex reordering phenomena, but will in addition introduce many spurious possibilities. Inspired by work in discriminative dependency parsing (e.g., (McDonald et al., 2005)), we add probabilistic constraints to the model through a discriminative model that links lexical dependencies in the target language to features of the source language string. We also investigate hard constraints on the dependency structures that are created during parsing. Second, there is a need to develop efficient decoding algorithms for the models. We describe approximate search methods that involve a significant extension of decoding algorithms originally developed for phrase-based translation systems. Experiments on translation from German to English show a 0.5% improvement in BLEU sc</context>
<context position="12858" citStr="McDonald et al., 2005" startWordPosition="2119" endWordPosition="2122">le SG to VP respond ⇒ VP be ADJP able SG to VP respond VP is 202 try (es gibt ==&gt;. there is) a correct alignment would be ((1, 1), (2, 2)), specifying that there is aligned to es, and is is aligned to gibt (note that in many, but not all, cases ai = bi, i.e., a target language word is aligned to a single source language word). The alignment information in s-phrases will be useful in tying syntactic dependencies created in the target language to positions in the source language string. In particular, we will consider discriminative models (analogous to models for dependency parsing, e.g., see (McDonald et al., 2005)) that estimate the probability of targetlanguage dependencies conditioned on properties of the source-language string. Alignments may be derived in a number of ways; in our method we directly use phrase entries proposed by a phrasebased system. Specifically, for each target word ei in a phrase entry (f1 ... fn, e1 ... em) for a training example, we find the smallest5 phrase entry in the same training example that includes ei on the target side, and is a subset of f1 ... fn on the source side; the word ei is then aligned to the subset of source language words in this “minimal” phrase. In concl</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="5738" citStr="Melamed, 2004" startWordPosition="892" endWordPosition="893">iminative dependency model gives a 1.5% BLEU point improvement over a basic model that does not condition on the source language string; the hard constraints on dependency structures give a 0.8% BLEU improvement. 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from corpora. A critical difference in our work is to allow arb</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>D. Melamed. 2004. Statistical machine translation by parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Mi</author>
<author>L Huang</author>
<author>Q Liu</author>
</authors>
<title>Forest-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>192--199</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5898" citStr="Mi et al., 2008" startWordPosition="918" endWordPosition="921">on dependency structures give a 0.8% BLEU improvement. 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from corpora. A critical difference in our work is to allow arbitrary reorderings of the source language sentence (as in phrase-based systems), through the use of flexible parsing operations. Rather than stating reordering </context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>H. Mi, L. Huang, and Q. Liu. 2008. Forest-based translation. In Proceedings ofACL-08: HLT, pages 192–199. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nesson</author>
<author>S M Shieber</author>
<author>A Rush</author>
</authors>
<title>Induction of probabilistic synchronous tree-insertion grammars for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th AMTA.</booktitle>
<contexts>
<context position="5867" citStr="Nesson et al., 2006" startWordPosition="912" endWordPosition="915">guage string; the hard constraints on dependency structures give a 0.8% BLEU improvement. 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from corpora. A critical difference in our work is to allow arbitrary reorderings of the source language sentence (as in phrase-based systems), through the use of flexible parsing operations. </context>
</contexts>
<marker>Nesson, Shieber, Rush, 2006</marker>
<rawString>R. Nesson, S.M. Shieber, and A. Rush. 2006. Induction of probabilistic synchronous tree-insertion grammars for machine translation. In Proceedings of the 7th AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="18883" citStr="Och, 2003" startWordPosition="3166" endWordPosition="3167">ranslation probabilities. In addition, we include a feature log P(t(q)|f(q), e(q)), which captures the probability of the phrase in question having the syntactic structure t(q). Note that a model that includes the terms scoreLM(e(d)) and Enj=1 scoreP(qj) alone would essentially be a basic phrase-based model (with no distortion terms). The terms scoreSY N(t(d)) and scoreR(d) add syntactic information to this basic model. A key motivation for this model is the flexibility of the reordering operations that it allows. However, the approach raises two major challenges: 7In practice, MERT training (Och, 2003) will be used to train relative weights for the different model components. Constraints on reorderings. Relaxing the operations in the parsing model will allow complex reorderings to be captured, but will also introduce many spurious possibilities. As one example, consider the derivation step shown in figure 4. This step may receive a high probability from a syntactic or surface language model—no discrimination is a quite plausible NP in English—but it should be ruled out for other reasons, for example because it does not respect the dependencies in the original German (i.e., keine/no is not a</context>
<context position="35524" citStr="Och, 2003" startWordPosition="5984" endWordPosition="5985">haroah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p = 0.058) under the sign test of Collins et al. (2005). Table 1 shows results for the full syntax-based system, and also results for the system with the discriminative dependency scores (see section 4.1) and the 7r-contituent constraint removed from the system. In both cases we see a clear impact of these components of the model, with 1.5 and 0.8 BLEU point decrements respectively. 10Both Pharoah and our system have weights trained using MERT (Och, 2003) on sentences of length 30 words or less, to ensure that training and test conditions are matched. 207 R: in our eyes , the opportunity created by this directive of introducing longer buses on international routes is efficient. S: the opportunity now presented by this directive is effective in our opinion , to use long buses on international routes . P: the need for this directive now possibility of longer buses on international routes to is in our opinion, efficiently. R: europe and asia must work together to intensify the battle against drug trafficking , money laundering , international cri</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F.J. Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="34868" citStr="Papineni et al., 2002" startWordPosition="5869" endWordPosition="5872">ce of words in any translation using this s-phrase). 6 Experiments We trained the syntax-based system on 751,088 German-English translations from the Europarl corpus (Koehn, 2005). A syntactic language model was also trained on the English sentences in the training data. We used Pharoah (Koehn et al., 2003) as a baseline system for comparison; the s-phrases used in our system include all phrases, with the same scores, as those used by Pharoah, allowing a direct comparison. For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah. This result was found to be significant (p = 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p = 0.058) under the sign test of Collins et al. (2005). Table 1 shows results for the full syntax-based system, and also results for the system with the discriminative dependency scores (see section 4.1) and the 7r-contituent constraint removed from the system. In both cases we see a clear impact of these components of the model, with 1.5 and 0.8 BLEU point decrements respectively. 10Both Pharoah an</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings ofACL, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>A Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency tree translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="5796" citStr="Quirk et al., 2005" startWordPosition="900" endWordPosition="903">provement over a basic model that does not condition on the source language string; the hard constraints on dependency structures give a 0.8% BLEU improvement. 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from corpora. A critical difference in our work is to allow arbitrary reorderings of the source language sentence (as in </context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>C. Quirk, A. Menezes, and Colin Cherry. 2005. Dependency tree translation: Syntactically informed phrasal smt. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Rambow</author>
<author>K Vijay-Shanker</author>
<author>D Weir</author>
</authors>
<title>D-tree grammars.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>151--158</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="7266" citStr="Rambow et al., 1995" startWordPosition="1136" endWordPosition="1139">that distinguish us from previous work are the use of all phrases proposed by a phrase-based system, and the use of a dependency language model that also incorporates constituent information (although see (Charniak et al., 2003; Shen et al., 2008) for related approaches). 3 A Syntactic Translation Model 3.1 Background Our work builds on the variant of tree adjoining grammar (TAG) introduced by (Carreras et al., 2008). In this formalism the basic units in the grammar are spines, which associate tree fragments with lexical items. These spines can be combined using a sister-adjunction operation (Rambow et al., 1995), to form larger pieces of structure.2 For example, we might have the following operation: is there is In this case the spine for there has sister-adjoined into the S node in the spine for is; we refer to the spine for there as being the modifier spine, and the spine for is being the head spine. There are close connections to dependency formalisms: in particular in this operation we see a lexical dependency between the modifier word there and the head word is. It is possible to define syntactic language models, similar to (Charniak, 2001), which associate probabilities with these dependencies,</context>
</contexts>
<marker>Rambow, Vijay-Shanker, Weir, 1995</marker>
<rawString>O. Rambow, K. Vijay-Shanker, and D. Weir. 1995. D-tree grammars. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 151–158, Cambridge, Massachusetts, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>J Xu</author>
<author>R Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="5918" citStr="Shen et al., 2008" startWordPosition="922" endWordPosition="925">uctures give a 0.8% BLEU improvement. 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from corpora. A critical difference in our work is to allow arbitrary reorderings of the source language sentence (as in phrase-based systems), through the use of flexible parsing operations. Rather than stating reordering rules at the level o</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>L. Shen, J. Xu, and R. Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="5656" citStr="Wu, 1997" startWordPosition="880" endWordPosition="881">system gives a significant improvement over the phrase-based system. The discriminative dependency model gives a 1.5% BLEU point improvement over a basic model that does not condition on the source language string; the hard constraints on dependency structures give a 0.8% BLEU improvement. 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be lea</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="5709" citStr="Yamada and Knight, 2001" startWordPosition="886" endWordPosition="889">over the phrase-based system. The discriminative dependency model gives a 1.5% BLEU point improvement over a basic model that does not condition on the source language string; the hard constraints on dependency structures give a 0.8% BLEU improvement. 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from corpora. A critical differenc</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>D Gildea</author>
</authors>
<title>Stochastic lexicalized inversion transduction grammar for alignment.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>473--482</pages>
<contexts>
<context position="5762" citStr="Zhang and Gildea, 2005" startWordPosition="894" endWordPosition="897">dency model gives a 1.5% BLEU point improvement over a basic model that does not condition on the source language string; the hard constraints on dependency structures give a 0.8% BLEU improvement. 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from corpora. A critical difference in our work is to allow arbitrary reorderings of th</context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>H. Zhang and D. Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In Proceedings ofACL, pages 473–482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL 2006 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="5846" citStr="Zollmann and Venugopal, 2006" startWordPosition="908" endWordPosition="911">ot condition on the source language string; the hard constraints on dependency structures give a 0.8% BLEU improvement. 2 Relationship to Previous Work A number of syntax-based translation systems have framed translation as a parsing problem, where search for the most probable translation is achieved using algorithms that are generalizations of conventional parsing methods. Early examples of this work include (Alshawi, 1996; Wu, 1997); more recent models include (Yamada and Knight, 2001; Eisner, 2003; Melamed, 2004; Zhang and Gildea, 2005; Chiang, 2005; Quirk et al., 2005; Marcu et al., 2006; Zollmann and Venugopal, 2006; Nesson et al., 2006; Cherry, 2008; Mi et al., 2008; Shen et al., 2008). The majority of these methods make use of synchronous grammars, or tree transducers, which operate over parse trees in the source and/or target languages. Reordering rules are typically specified through rotations or transductions stated at the level of contextfree rules, or larger fragments, within parse trees. These rules can be learned automatically from corpora. A critical difference in our work is to allow arbitrary reorderings of the source language sentence (as in phrase-based systems), through the use of flexible</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>A. Zollmann and A. Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of NAACL 2006 Workshop on Statistical Machine Translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>