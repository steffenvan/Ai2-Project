<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<subsectionHeader confidence="0.224428">
Book Reviews Semantic Interpretation and the Resolution of Ambiguity
</subsectionHeader>
<bodyText confidence="0.998929843137255">
Similar discussions apply to parsing. Fifty thousand
words of the corpus have been parsed by hand, and this
has been used to make a table of the relative frequencies
of different syntactic constructions. Assuming that the
correct parse tree is the one ma-de of the most probable
constituents (to greatly oversimplify in the interests of
saving space), a program was written to parse with
about 50% accuracy. Since the preparation of this book,
continuing work by Eric Atwell and Geoffrey Sampson
at Leeds has greatly improved on this figure, using a
simulated annealing technique (see Sampson 1986).
Other chapters of the book discuss the history of
corpora in linguistic research, a defense of probabilistic
methods, a discussion of speech synthesis and an out-
line of a sophisticated spelling corrector. Not much has
been done on speech synthesis, partly becaiise we do
not as yet have good data on the relation between
syntax and prosody. The spelling corrector is aimed at
errors of word selection, i.e., finding words that, al-
though they appear in the dictionary, should not appear
in the particular sentence being studied (e.g., &amp;quot;They
kingdom come, thy will be done&amp;quot;). All these tools
follow the same model: reliance on statistics from the
corpus.
It is a great relief to read a book like this, which is
based on real texts rather than upon the imaginary
language, sharing a few word forms with English, that is
studied at MIT and some other research institutes (see
Postal 1988). It is amazing that computers, which are
distinguished for their ability to deal with vast quantities
of bytes and their incompetence with even simple
patterns and models, have been used in linguistics
primarily for the implementation of complex logical
models. This book is a start on the exploitation of large
database methods for linguistic information. It is re-
markable for the performance of its methods combined
with their simplicity. Unlike many books on linguistics,
it is easy to understand; it makes one think of the
Moliere character who suddenly found out he had been
speaking prose all his life.
I heartily recommend this book to anyone who
wishes to process language for a useful purpose. Other
workers such as John Sinclair (1987) and Yaacov
Choueka (1988) have also used large text databases for
deriving linguistic information. When I was an under-
graduate, one of my professors said that &amp;quot;mathematical
intuition means having seen the problem before.&amp;quot; Sim-
ilarly, there is no substitute in linguistics for knowing
that a particular construction is likely because it has
appeared many times. This book is a testimony to the
superiority of experience over fantasy.
</bodyText>
<sectionHeader confidence="0.994086" genericHeader="abstract">
REFERENCES
</sectionHeader>
<reference confidence="0.416695666666667">
Choueka, Yaacov 1988 Looking for Needles in a Haystack. In
Proceedings of the RIAO 88, 609-623.
Postal, Paul 1988 Advances in Linguistic Rhetoric. In Natural Lan-
guage and Linguistic Theory 6:129-137.
Sampson, Geoffrey 1986 Simulated Annealing as a Parsing Tech-
nique. In University of Leeds Working Papers in Linguistics and
Phonetics 4:43-60.
Sinclair, John 1987 Looking up. Collins, London, England; Glasgow,
Scotland.
</reference>
<bodyText confidence="0.9093822">
Michael Lesk is division manager of computer science re-
search at Bell Communications Research, 445 South St.,
Morristown, NJ 07960. He uses machine-readable dictionaries
in his research on text handling and retrieval. E-mail:
lesk@wind.bellcore.com
</bodyText>
<sectionHeader confidence="0.997037" genericHeader="categories and subject descriptors">
SEMANTIC INTERPRETATION AND THE RESOLUTION OF
AMBIGUITY
</sectionHeader>
<subsectionHeader confidence="0.877218">
Graeme Hirst
</subsectionHeader>
<affiliation confidence="0.712607">
(University of Toronto)
Cambridge, England: Cambridge University Press,
</affiliation>
<figure confidence="0.7716595">
1987, xiv +263 pp.
ISBN 0-521-32203-0; (hb) $39.50 [20% discount to
ACL members]
Reviewed by
Karen Sparck Jones
University of Cambridge
</figure>
<bodyText confidence="0.994740675496689">
Hirst&apos;s book presents an approach to natural language
interpretation, using as his vehicle a description of the
experimental system he built. It therefore has to be
evaluated as a contribution on how to build NLP
systems from both theoretical and practical points of
view. It also has to be considered for teaching purposes,
since Hirst has vamped up what was originally a thesis
with some pedagogic exposition and test exercises, as
well as a substantial and useful bibliography.
Hirst is very clear about his aims and very honest
about what he has tackled. He presents detail well and
provides excellent summaries, so the essential proper-
ties of his work are well laid out.
His goal was to build an interpretation system that
could handle serious lexical and structural ambiguity,
and handle it in a principled way. His concern is thus
essentially computational; he does not make any claims
for the psycholinguistic relevance of what he is doing,
but he is, on the other hand, willing to exploit psycho-
linguistically derived support for good processing strat-
egies.
The system consists of a syntactic parser, Paragram,
a semantic interpreter, Absity, and two disambiguation
processors: the Polaroid Word (PW) subsystem for
lexical disambiguation and the Semantic Enquiry Desk
for structural disambiguation. The system builds an
explicit meaning representation in the frame language
Frail.
Hirst&apos;s design is motivated by two goals: to allow
processes of different sorts to use different kinds of
information but to interact to construct a sentence
representation; and to do this in the theoretically well-
founded way exemplified by Montague&apos;s work by doing
Computational Linguistics, Volume 14, Number 4, December 1988 91
Book Reviews Semantic Interpretation and the Resolution of Ambiguity
interpretation compositionally with formally proper se-
mantic objects. The specific &amp;quot;non-standard&amp;quot; issues he
addresses within this framework are handling the loose
associative processes that appear to be needed for word
sense selection, and allowing for emergent sense iden-
tification.
In sentence processing, therefore, the basic mecha-
nism is provided by the Paragram parser that Hirst
exploits, working in tandem with his Absity interpreter.
Absity is strongly typed, with types referring to the
various elements and constructs of (his extended) Frail:
for example, generic and instance frames, slot names
and slot filler pairs, and frame determiners. The tandem
operation of parser and interpreter is thus designed to
map syntactic types onto their corresponding semantic
types: for instance, noun phrases into frame statements
(i.e., knowledge base access statements), prepositions
into slot names, or verb phrases into frame descriptors
(i.e., complete descriptions of generic frames). How-
ever, as Paragram is essentially deterministic, Hirst has
to compromise a little and sully his pure compositional
ideology by using pseudo-words and fake objects.
Lexical disambiguation within this framework relies
on the PW processors. These are syntactic type proce-
dures (in principle for all types of word) individualized
with lexical information for each word through which,
in turn, it is possible to access the Frail knowledge base
and hence information about the world. The procedures
develop, i.e, disambiguate, their PWs in two ways: by
exploiting the results of marker passing, which is an
entirely autonomous, non-syntactic process operating
on the frame data for input words and establishing links
between and hence sense selections among these using
slot and isa relationships; or, if the marker passing is not
sufficiently selective, by heavily constrained interword
checking for slot filler connections. These processes are
activated by each incoming word, so PWs may be only
partly developed at intermediate points in the sentence
input.
For structural disambiguation, limited to preposi-
tional phrase attachment and relative clause gap finding
and filling, Paragram looks to the Semantic Enquiry
Desk, which is supplied with relevant PW information
for germane candidates and applies strategies relying
heavily on slot filler restrictions and invoking, e.g.,
Crain and Steedman&apos;s Principle of Referential Success,
which fit with the way semantic representation is han-
dled through objects in the Frail knowledge base. Hirst
is able to treat his two types of phenomenon in a fairly
algorithmic way because he can call on the systemati-
cally organized and rich information available in the
knowledge base.
With these lexical and structural procedures Hirst
can, for example, resolve &amp;quot;The slug operated the vend-
ing machine&amp;quot; and &amp;quot;Ross included the book for Nadia&amp;quot;,
but not &amp;quot;The astronomer married the star&amp;quot; or &amp;quot;The
women discussed the dogs at breakfast&amp;quot;. The fact that
he can successfully disambiguate &amp;quot;The deep philoso-
pher threw the peach pit into the deep pit&amp;quot; may,
perhaps, be regarded as a triumph.
Hirst presents his own ideas in the context of exten-
sive accounts of the problems to be solved, and of other
approaches to them. But he unfortunately missed Al-
shawi&apos;s work (Alshawi 1987, but available as a technical
report in 1984), which also combines more conventional
syntactic and semantic processing with marker passing,
and which attacks text processing above the sentence in
a serious way. Carter (1987) recently described work on
anaphor resolution in text which subscribes to the
multipronged and side-effect philosophy of interpreta-
tion that Hirst espouses, though the more specific
character of his work is different.
Hirst is so open about the limitations of what he has
done—for example, about the semantic phenomena,
like noun modifiers, that Absity cannot handle—that he
to some extent preempts criticism. His final chapter is
particularly disarming, since it cheerfully invites the
reader, through speculations, questions, and suggested
exercises, to think about the way his approach could be
developed to tackle a whole range of interesting and
challenging problems. But Hirst does refer to his
achievements in terms that imply they have been dem-
onstrated, and he emphasizes his claim that Frail ob-
jects are kosher semantic objects of the kind all right-
thinking semantic theories ought to have.
However, though Hirst&apos;s approach is interesting, and
his account of it is quite detailed, he does not really give
us enough evidence, in the form of system data, behav-
iour traces, or output results to evaluate what he has
done or could do; and, as an important part of this, to
judge the validity of his argument that Frail objects rule
OK in the way mere symbols don&apos;t. This is put in a
robustly plausible way, but, in fact, too briskly to
convince. Hirst appears to maintain that because Frail
allows systematic operations on well-defined objects,
and because these internal objects can be taken for
external ones, attaining thereby a Germanic ding-hood,
as it were, those nasty philosophical questions about the
status of the representation language itself somehow
disappear. They don&apos;t quite.
Semantic Interpretation is an interesting and enjoy-
able read. Hirst combines perceptive analyses with
commonsense attitudes (for instance, to the definition
of &amp;quot;polysemy&amp;quot;) in an engaging way, and he has made a
valuable attempt to synthesize some attractive, but
hitherto distinct, ideas about language interpretation.
This is food for thought for the research worker, and
there is matter for digestion for students in the pedago-
gic surveys and in the way Hirst uses his system as a
problem solving illustration. Many of the examples are
a delight, and instructive not least because memorable
(though I&apos;m beginning to feel quite bothered about
Nadia, publicly exhibited for our benefit in deplorable
attitudes, like happily cleaning up after Ross, for in-
stance, though Ross can quite clearly look after him-
self).
</bodyText>
<page confidence="0.843522">
92 Computational Linguistics, Volume 14, Number 4, December 1988
</page>
<note confidence="0.586378">
Book Reviews The Fifth Generation Fallacy: Why Japan is Betting Its Future on Artificial Intelligence
</note>
<bodyText confidence="0.999833555555556">
But the way Hirst has souped up his thesis with
general expository matter has had not altogether satis-
factory results. The textbook-like sections against
which the accounts of his own work are set treat some
topics, like structural ambiguity, at length, but do not
form a very well-balanced or comprehensive whole.
They nevertheless dilute Hirst&apos;s own work enough to
prevent the reader from experiencing that feeling of
excitement that good theses provoke, and to leave her
disappointed in not getting the fuller and more concen-
trated account of Hirst&apos;s system as such, and its perfor-
mance, she would have liked. The idea that language
interpretation involves quite disparate processes is one
deserving investigation, but it is not clear whether
Hirst&apos;s particular way of combining such different lines
as Montague and markers, via frames, is on the right
track, and his book does not compel his reader, in a Pied
Piper imperative, to follow him.
</bodyText>
<sectionHeader confidence="0.991827" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.713589615384616">
Alshawi, H. 1987 Memory and Context for Language Interpretation.
Cambridge University Press, Cambridge, England.
Carter, D.M. 1987 Interpreting Anaphors in Natural Language Texts.
Ellis Horwood, Chichester, England.
Karen Sparck Jones is co-editor of Automatic Natural Lan-
guage Parsing (Ellis Horwood 1983). Her present research
concerns user modeling. Sparck Jones&apos;s address is: Comput-
ing Laboratory, University of Cambridge, Pembroke Street,
Cambridge CB2 3QG, England. E-mail: sparckjones@cl.
cam.uk.ac
NOTE: The author of this book is also the book review editor
of this journal. Therefore, this review was edited by James
Allen, editor of Computational Linguistics.
</reference>
<note confidence="0.2966485">
THE FIFTH GENERATION FALLACY: WHY JAPAN IS
BETTING ITS FUTURE ON ARTIFICIAL INTELLIGENCE
</note>
<author confidence="0.455357">
J. Marshall Unger
</author>
<affiliation confidence="0.7884085">
(University of Hawaii, Honolulu)
Oxford University Press, 1987, x+230 pp.
</affiliation>
<figure confidence="0.66094225">
ISBN 0-19-504-939-X; (hb)
Reviewed by
Harold Somers
UMIST
</figure>
<bodyText confidence="0.999874217391304">
The main proposal of this book is that the motivation
behind the Japanese Fifth Generation project is not a
desire to push the barriers of computer technology
research, nor even to attain economic superiority
through advanced technology, but quite simply to over-
come the problems of the Japanese writing system. In
advancing this claim, Unger demonstrates that he is at
least well read—an impressive array of literature in both
English and Japanese is cited—and the book is well
written, although sometimes in a journalistic rather than
academic style. Despite that, this reviewer, and at least
four other informed colleagues that I have discussed
this book with, find the claim quite preposterous. The
book is an enjoyable read, in the way that the Letters
page in an extremist newspaper is sometimes enjoyable,
though I fear that only those who already have a fair
background knowledge of the issues will get very far
with it: if you don&apos;t know the difference between a kanji
and a kana, then you might find it rather hard going.
The book is divided into three sections, each of two
chapters, dealing with Japanese linguistics and orthog-
raphy, political and cultural issues, and economics and
technology. This is preceded by an Introduction which
sets the scene, actually getting the book off to a very
promising start, in which Unger virulently attacks the
typical reaction to the Fifth Generation, as exemplified
by the well-known Feigenbaum and McCorduck (1983)
book, which exhibits paranoia, misunderstanding of
Japanese attitudes to AT, the weak/strong AT debate,
and the reasons for the Fifth Generation:
What the Japanese have in mind when they speak of Al,
however, turns out to be something else yet again, and
unless one dispels the fog of strong-AI hyperbole that
surrounds the Fifth Generation project, it is impossible to
appreciate its significance. (p.4)
It never occurs to [Feigenbaum and McCorduckj that,
despite Japan&apos;s new-found affluence, its intellectual cli-
mate continues to suffer from such fundamental condi-
tions as geographical and linguistic isolation, academic
factionalism, and the overweening influence of govern-
ment bureaucracy and giant corporations. (p.5)
It is as an answer to this misunderstanding that the
remainder of the book is intended, and it begins with a
detailed description of the Japanese writing system, and
some of its practical consequences. As I suggested
above, this whole section is probably too complex for
the reader who does not know about it already, and this
complexity is not helped by the unusual approach,
starting from a romanization presented in alphabetical
order and working towards the kana rather than vice
versa. Some minor criticisms of this section would
include the fact that the term mora (roughly, the con-
sonant-plus-vowel syllable that each kana represents) is
not included in the otherwise extensive glossary; the
error (pp.21-22) of describing the Hepburn romaniza-
tion as phonemic, when it is precisely not phonemic
(e.g. /tu/ is written tsu), whereas the Cabinet (kunrei)
romanization is; and there is no mention of a third
romanization (especially used by Japanese) which is a
simple transliteration of kana, e.g. Toukyou for Tokyo.
While we are on the subject of romanization, we men-
tion also the quaint use by the publisher of the circum-
flex rather than the macron to indicate long vowels in
the Japanese transcription.
Turning to the question of kanji, it is always difficult
to convey accurately to readers used to alphabetic
writing systems the pros and cons of a radically differ-
ent orthography, and like most Western observers,
Unger overestimates the problems they cause
</bodyText>
<page confidence="0.287087">
Computational Linguistics, Volume 14, Number 4, December 1988 93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.951842">Book Reviews Semantic Interpretation and the Resolution of Ambiguity</title>
<abstract confidence="0.99394068627451">Similar discussions apply to parsing. Fifty thousand words of the corpus have been parsed by hand, and this has been used to make a table of the relative frequencies of different syntactic constructions. Assuming that the parse tree is the one of the most probable constituents (to greatly oversimplify in the interests of saving space), a program was written to parse with about 50% accuracy. Since the preparation of this book, continuing work by Eric Atwell and Geoffrey Sampson at Leeds has greatly improved on this figure, using a simulated annealing technique (see Sampson 1986). Other chapters of the book discuss the history of corpora in linguistic research, a defense of probabilistic methods, a discussion of speech synthesis and an outline of a sophisticated spelling corrector. Not much has been done on speech synthesis, partly becaiise we do not as yet have good data on the relation between syntax and prosody. The spelling corrector is aimed at errors of word selection, i.e., finding words that, although they appear in the dictionary, should not appear in the particular sentence being studied (e.g., &amp;quot;They kingdom come, thy will be done&amp;quot;). All these tools follow the same model: reliance on statistics from the corpus. It is a great relief to read a book like this, which is based on real texts rather than upon the imaginary language, sharing a few word forms with English, that is studied at MIT and some other research institutes (see Postal 1988). It is amazing that computers, which are distinguished for their ability to deal with vast quantities of bytes and their incompetence with even simple patterns and models, have been used in linguistics primarily for the implementation of complex logical models. This book is a start on the exploitation of large database methods for linguistic information. It is remarkable for the performance of its methods combined with their simplicity. Unlike many books on linguistics, it is easy to understand; it makes one think of the Moliere character who suddenly found out he had been speaking prose all his life. I heartily recommend this book to anyone who wishes to process language for a useful purpose. Other workers such as John Sinclair (1987) and Yaacov Choueka (1988) have also used large text databases for deriving linguistic information. When I was an undergraduate, one of my professors said that &amp;quot;mathematical intuition means having seen the problem before.&amp;quot; Similarly, there is no substitute in linguistics for knowing that a particular construction is likely because it has appeared many times. This book is a testimony to the superiority of experience over fantasy.</abstract>
<note confidence="0.335959">REFERENCES Choueka, Yaacov 1988 Looking for Needles in a Haystack. In of the RIAO 88, Paul 1988 Advances in Linguistic Rhetoric. In Lan-</note>
<title confidence="0.485164">and Linguistic Theory</title>
<author confidence="0.622267">Geoffrey Simulated Annealing as a Parsing Tech- Sampson</author>
<affiliation confidence="0.896301">In of Leeds Working Papers in Linguistics and</affiliation>
<address confidence="0.621511">John 1987 up. London, England; Glasgow, Scotland.</address>
<note confidence="0.42476875">Lesk division manager of computer science research at Bell Communications Research, 445 South St., Morristown, NJ 07960. He uses machine-readable dictionaries in his research on text handling and retrieval. E-mail:</note>
<email confidence="0.993928">lesk@wind.bellcore.com</email>
<title confidence="0.8079265">SEMANTIC INTERPRETATION AND THE RESOLUTION OF AMBIGUITY</title>
<author confidence="0.999362">Graeme Hirst</author>
<affiliation confidence="0.835776">(University of Toronto) Cambridge, England: Cambridge University Press,</affiliation>
<address confidence="0.673049">1987, xiv +263 pp.</address>
<note confidence="0.980797333333333">ISBN 0-521-32203-0; (hb) $39.50 [20% discount to ACL members] Reviewed by</note>
<author confidence="0.972156">Karen Sparck Jones</author>
<affiliation confidence="0.991259">University of Cambridge</affiliation>
<abstract confidence="0.989032473988439">Hirst&apos;s book presents an approach to natural language interpretation, using as his vehicle a description of the experimental system he built. It therefore has to be evaluated as a contribution on how to build NLP systems from both theoretical and practical points of view. It also has to be considered for teaching purposes, since Hirst has vamped up what was originally a thesis with some pedagogic exposition and test exercises, as well as a substantial and useful bibliography. Hirst is very clear about his aims and very honest about what he has tackled. He presents detail well and provides excellent summaries, so the essential properties of his work are well laid out. His goal was to build an interpretation system that could handle serious lexical and structural ambiguity, and handle it in a principled way. His concern is thus essentially computational; he does not make any claims for the psycholinguistic relevance of what he is doing, but he is, on the other hand, willing to exploit psycholinguistically derived support for good processing strategies. system consists of a syntactic parser, semantic interpreter, two disambiguation the Word subsystem for disambiguation and the Enquiry Desk for structural disambiguation. The system builds an explicit meaning representation in the frame language Frail. Hirst&apos;s design is motivated by two goals: to allow processes of different sorts to use different kinds of information but to interact to construct a sentence representation; and to do this in the theoretically wellfounded way exemplified by Montague&apos;s work by doing Computational Linguistics, Volume 14, Number 4, December 1988 91 Book Reviews Semantic Interpretation and the Resolution of Ambiguity interpretation compositionally with formally proper semantic objects. The specific &amp;quot;non-standard&amp;quot; issues he addresses within this framework are handling the loose associative processes that appear to be needed for word sense selection, and allowing for emergent sense identification. In sentence processing, therefore, the basic mechanism is provided by the Paragram parser that Hirst exploits, working in tandem with his Absity interpreter. Absity is strongly typed, with types referring to the various elements and constructs of (his extended) Frail: for example, generic and instance frames, slot names and slot filler pairs, and frame determiners. The tandem operation of parser and interpreter is thus designed to map syntactic types onto their corresponding semantic types: for instance, noun phrases into frame statements (i.e., knowledge base access statements), prepositions into slot names, or verb phrases into frame descriptors (i.e., complete descriptions of generic frames). However, as Paragram is essentially deterministic, Hirst has to compromise a little and sully his pure compositional ideology by using pseudo-words and fake objects. Lexical disambiguation within this framework relies on the PW processors. These are syntactic type procedures (in principle for all types of word) individualized with lexical information for each word through which, in turn, it is possible to access the Frail knowledge base and hence information about the world. The procedures develop, i.e, disambiguate, their PWs in two ways: by exploiting the results of marker passing, which is an entirely autonomous, non-syntactic process operating on the frame data for input words and establishing links between and hence sense selections among these using slot and isa relationships; or, if the marker passing is not sufficiently selective, by heavily constrained interword checking for slot filler connections. These processes are activated by each incoming word, so PWs may be only partly developed at intermediate points in the sentence input. For structural disambiguation, limited to prepositional phrase attachment and relative clause gap finding and filling, Paragram looks to the Semantic Enquiry Desk, which is supplied with relevant PW information for germane candidates and applies strategies relying heavily on slot filler restrictions and invoking, e.g., and Steedman&apos;s of Referential Success, which fit with the way semantic representation is handled through objects in the Frail knowledge base. Hirst is able to treat his two types of phenomenon in a fairly algorithmic way because he can call on the systematically organized and rich information available in the knowledge base. With these lexical and structural procedures Hirst can, for example, resolve &amp;quot;The slug operated the vending machine&amp;quot; and &amp;quot;Ross included the book for Nadia&amp;quot;, but not &amp;quot;The astronomer married the star&amp;quot; or &amp;quot;The women discussed the dogs at breakfast&amp;quot;. The fact that he can successfully disambiguate &amp;quot;The deep philosopher threw the peach pit into the deep pit&amp;quot; may, perhaps, be regarded as a triumph. Hirst presents his own ideas in the context of extensive accounts of the problems to be solved, and of other approaches to them. But he unfortunately missed Alshawi&apos;s work (Alshawi 1987, but available as a technical report in 1984), which also combines more conventional syntactic and semantic processing with marker passing, and which attacks text processing above the sentence in a serious way. Carter (1987) recently described work on anaphor resolution in text which subscribes to the multipronged and side-effect philosophy of interpretation that Hirst espouses, though the more specific character of his work is different. Hirst is so open about the limitations of what he has done—for example, about the semantic phenomena, like noun modifiers, that Absity cannot handle—that he to some extent preempts criticism. His final chapter is particularly disarming, since it cheerfully invites the reader, through speculations, questions, and suggested exercises, to think about the way his approach could be developed to tackle a whole range of interesting and challenging problems. But Hirst does refer to his in terms that imply they have been demonstrated, and he emphasizes his claim that Frail obare kosher semantic objects of the kind all rightthinking semantic theories ought to have. However, though Hirst&apos;s approach is interesting, and his account of it is quite detailed, he does not really give us enough evidence, in the form of system data, behaviour traces, or output results to evaluate what he has done or could do; and, as an important part of this, to judge the validity of his argument that Frail objects rule OK in the way mere symbols don&apos;t. This is put in a robustly plausible way, but, in fact, too briskly to convince. Hirst appears to maintain that because Frail allows systematic operations on well-defined objects, and because these internal objects can be taken for external ones, attaining thereby a Germanic ding-hood, as it were, those nasty philosophical questions about the status of the representation language itself somehow disappear. They don&apos;t quite. Interpretation an interesting and enjoyable read. Hirst combines perceptive analyses with commonsense attitudes (for instance, to the definition of &amp;quot;polysemy&amp;quot;) in an engaging way, and he has made a valuable attempt to synthesize some attractive, but hitherto distinct, ideas about language interpretation. This is food for thought for the research worker, and there is matter for digestion for students in the pedagogic surveys and in the way Hirst uses his system as a problem solving illustration. Many of the examples are a delight, and instructive not least because memorable (though I&apos;m beginning to feel quite bothered about Nadia, publicly exhibited for our benefit in deplorable attitudes, like happily cleaning up after Ross, for instance, though Ross can quite clearly look after himself). Linguistics, Volume 14, Number 4, December 1988 Book Reviews The Fifth Generation Fallacy: Why Japan is Betting Its Future on Artificial Intelligence But the way Hirst has souped up his thesis with general expository matter has had not altogether satisfactory results. The textbook-like sections against which the accounts of his own work are set treat some topics, like structural ambiguity, at length, but do not form a very well-balanced or comprehensive whole. They nevertheless dilute Hirst&apos;s own work enough to prevent the reader from experiencing that feeling of excitement that good theses provoke, and to leave her disappointed in not getting the fuller and more concentrated account of Hirst&apos;s system as such, and its performance, she would have liked. The idea that language interpretation involves quite disparate processes is one deserving investigation, but it is not clear whether Hirst&apos;s particular way of combining such different lines as Montague and markers, via frames, is on the right track, and his book does not compel his reader, in a Pied Piper imperative, to follow him. REFERENCES H. 1987 and Context for Language Interpretation.</abstract>
<note confidence="0.705380666666667">Cambridge University Press, Cambridge, England. D.M. 1987 Anaphors in Natural Language Texts. Ellis Horwood, Chichester, England. Sparck Jones co-editor of Natural Lan- Parsing Horwood 1983). Her present research concerns user modeling. Sparck Jones&apos;s address is: Comput-</note>
<affiliation confidence="0.7048">ing Laboratory, University of Cambridge, Pembroke Street,</affiliation>
<address confidence="0.945402">Cambridge CB2 3QG, England. E-mail: sparckjones@cl.</address>
<email confidence="0.749164">cam.uk.ac</email>
<abstract confidence="0.890543">NOTE: The author of this book is also the book review editor of this journal. Therefore, this review was edited by James editor of Linguistics.</abstract>
<affiliation confidence="0.508745">THE FIFTH GENERATION FALLACY: WHY JAPAN IS</affiliation>
<title confidence="0.902445">ON ARTIFICIAL INTELLIGENCE</title>
<author confidence="0.995286">J Marshall Unger</author>
<affiliation confidence="0.622756">(University of Hawaii, Honolulu) Oxford University Press, 1987, x+230 pp.</affiliation>
<note confidence="0.9447335">ISBN 0-19-504-939-X; (hb) Reviewed by</note>
<author confidence="0.618837">Harold Somers</author>
<affiliation confidence="0.412864">UMIST</affiliation>
<abstract confidence="0.976918727272727">The main proposal of this book is that the motivation behind the Japanese Fifth Generation project is not a desire to push the barriers of computer technology research, nor even to attain economic superiority through advanced technology, but quite simply to overcome the problems of the Japanese writing system. In advancing this claim, Unger demonstrates that he is at least well read—an impressive array of literature in both English and Japanese is cited—and the book is well written, although sometimes in a journalistic rather than academic style. Despite that, this reviewer, and at least four other informed colleagues that I have discussed this book with, find the claim quite preposterous. The book is an enjoyable read, in the way that the Letters page in an extremist newspaper is sometimes enjoyable, though I fear that only those who already have a fair background knowledge of the issues will get very far with it: if you don&apos;t know the difference between a kanji and a kana, then you might find it rather hard going. The book is divided into three sections, each of two chapters, dealing with Japanese linguistics and orthography, political and cultural issues, and economics and</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaacov Choueka</author>
</authors>
<title>Looking for Needles in a Haystack.</title>
<date>1988</date>
<booktitle>In Proceedings of the RIAO</booktitle>
<volume>88</volume>
<pages>609--623</pages>
<marker>Choueka, 1988</marker>
<rawString>Choueka, Yaacov 1988 Looking for Needles in a Haystack. In Proceedings of the RIAO 88, 609-623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Postal</author>
</authors>
<date>1988</date>
<booktitle>Advances in Linguistic Rhetoric. In Natural Language and Linguistic Theory</booktitle>
<pages>6--129</pages>
<contexts>
<context position="1553" citStr="Postal 1988" startWordPosition="255" endWordPosition="256">ot as yet have good data on the relation between syntax and prosody. The spelling corrector is aimed at errors of word selection, i.e., finding words that, although they appear in the dictionary, should not appear in the particular sentence being studied (e.g., &amp;quot;They kingdom come, thy will be done&amp;quot;). All these tools follow the same model: reliance on statistics from the corpus. It is a great relief to read a book like this, which is based on real texts rather than upon the imaginary language, sharing a few word forms with English, that is studied at MIT and some other research institutes (see Postal 1988). It is amazing that computers, which are distinguished for their ability to deal with vast quantities of bytes and their incompetence with even simple patterns and models, have been used in linguistics primarily for the implementation of complex logical models. This book is a start on the exploitation of large database methods for linguistic information. It is remarkable for the performance of its methods combined with their simplicity. Unlike many books on linguistics, it is easy to understand; it makes one think of the Moliere character who suddenly found out he had been speaking prose all </context>
</contexts>
<marker>Postal, 1988</marker>
<rawString>Postal, Paul 1988 Advances in Linguistic Rhetoric. In Natural Language and Linguistic Theory 6:129-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
</authors>
<title>Simulated Annealing as a Parsing Technique. In</title>
<date>1986</date>
<booktitle>University of Leeds Working Papers in Linguistics and Phonetics</booktitle>
<pages>4--43</pages>
<contexts>
<context position="667" citStr="Sampson 1986" startWordPosition="104" endWordPosition="105">n of Ambiguity Similar discussions apply to parsing. Fifty thousand words of the corpus have been parsed by hand, and this has been used to make a table of the relative frequencies of different syntactic constructions. Assuming that the correct parse tree is the one ma-de of the most probable constituents (to greatly oversimplify in the interests of saving space), a program was written to parse with about 50% accuracy. Since the preparation of this book, continuing work by Eric Atwell and Geoffrey Sampson at Leeds has greatly improved on this figure, using a simulated annealing technique (see Sampson 1986). Other chapters of the book discuss the history of corpora in linguistic research, a defense of probabilistic methods, a discussion of speech synthesis and an outline of a sophisticated spelling corrector. Not much has been done on speech synthesis, partly becaiise we do not as yet have good data on the relation between syntax and prosody. The spelling corrector is aimed at errors of word selection, i.e., finding words that, although they appear in the dictionary, should not appear in the particular sentence being studied (e.g., &amp;quot;They kingdom come, thy will be done&amp;quot;). All these tools follow t</context>
</contexts>
<marker>Sampson, 1986</marker>
<rawString>Sampson, Geoffrey 1986 Simulated Annealing as a Parsing Technique. In University of Leeds Working Papers in Linguistics and Phonetics 4:43-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
</authors>
<title>Looking up.</title>
<date>1987</date>
<location>Collins, London, England; Glasgow, Scotland.</location>
<marker>Sinclair, 1987</marker>
<rawString>Sinclair, John 1987 Looking up. Collins, London, England; Glasgow, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Memory and Context for Language Interpretation.</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<marker>Alshawi, 1987</marker>
<rawString>Alshawi, H. 1987 Memory and Context for Language Interpretation. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Carter</author>
</authors>
<title>Interpreting Anaphors in Natural Language Texts. Ellis Horwood,</title>
<date>1987</date>
<location>Chichester, England.</location>
<marker>Carter, 1987</marker>
<rawString>Carter, D.M. 1987 Interpreting Anaphors in Natural Language Texts. Ellis Horwood, Chichester, England.</rawString>
</citation>
<citation valid="false">
<title>Sparck Jones is co-editor of Automatic Natural Language Parsing (Ellis Horwood 1983). Her present research concerns user modeling.</title>
<booktitle>Sparck Jones&apos;s address is: Computing Laboratory, University of Cambridge, Pembroke Street, Cambridge CB2 3QG, England. E-mail: sparckjones@cl. cam.uk.ac</booktitle>
<editor>Karen</editor>
<marker></marker>
<rawString>Karen Sparck Jones is co-editor of Automatic Natural Language Parsing (Ellis Horwood 1983). Her present research concerns user modeling. Sparck Jones&apos;s address is: Computing Laboratory, University of Cambridge, Pembroke Street, Cambridge CB2 3QG, England. E-mail: sparckjones@cl. cam.uk.ac</rawString>
</citation>
<citation valid="false">
<title>NOTE: The author of this book is also the book review editor of this journal. Therefore, this review was edited by James Allen, editor of Computational Linguistics.</title>
<marker></marker>
<rawString>NOTE: The author of this book is also the book review editor of this journal. Therefore, this review was edited by James Allen, editor of Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>