<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.057158">
<title confidence="0.997935">
Joint Hebrew Segmentation and Parsing
using a PCFG-LA Lattice Parser
</title>
<author confidence="0.996519">
Yoav Goldberg and Michael Elhadad
</author>
<affiliation confidence="0.999487">
Ben Gurion University of the Negev
Department of Computer Science
</affiliation>
<address confidence="0.865506">
POB 653 Be’er Sheva, 84105, Israel
</address>
<email confidence="0.998861">
{yoavg|elhadad}@cs.bgu.ac.il
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999037357142857">
We experiment with extending a lattice pars-
ing methodology for parsing Hebrew (Gold-
berg and Tsarfaty, 2008; Golderg et al., 2009)
to make use of a stronger syntactic model: the
PCFG-LA Berkeley Parser. We show that the
methodology is very effective: using a small
training set of about 5500 trees, we construct
a parser which parses and segments unseg-
mented Hebrew text with an F-score of almost
80%, an error reduction of over 20% over the
best previous result for this task. This result
indicates that lattice parsing with the Berkeley
parser is an effective methodology for parsing
over uncertain inputs.
</bodyText>
<sectionHeader confidence="0.99952" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999614">
Most work on parsing assumes that the lexical items
in the yield of a parse tree are fully observed, and
correspond to space delimited tokens, perhaps af-
ter a deterministic preprocessing step of tokeniza-
tion. While this is mostly the case for English, the
situation is different in languages such as Chinese,
in which word boundaries are not marked, and the
Semitic languages of Hebrew and Arabic, in which
various particles corresponding to function words
are agglutinated as affixes to content bearing words,
sharing the same space-delimited token. For exam-
ple, the Hebrew token bcl1 can be interpreted as
the single noun meaning “onion”, or as a sequence
of a preposition and a noun b-cl meaning “in (the)
shadow”. In such languages, the sequence of lexical
</bodyText>
<footnote confidence="0.8056895">
1We adopt here the transliteration scheme of (Sima’an et al.,
2001)
</footnote>
<bodyText confidence="0.999718705882353">
items corresponding to an input string is ambiguous,
and cannot be determined using a deterministic pro-
cedure. In this work, we focus on constituency pars-
ing of Modern Hebrew (henceforth Hebrew) from
raw unsegmented text.
A common method of approaching the discrep-
ancy between input strings and space delimited to-
kens is using a pipeline process, in which the in-
put string is pre-segmented prior to handing it to a
parser. The shortcoming of this method, as noted
by (Tsarfaty, 2006), is that many segmentation de-
cisions cannot be resolved based on local context
alone. Rather, they may depend on long distance re-
lations and interact closely with the syntactic struc-
ture of the sentence. Thus, segmentation deci-
sions should be integrated into the parsing process
and not performed as an independent preprocess-
ing step. Goldberg and Tsarfaty (2008) demon-
strated the effectiveness of lattice parsing for jointly
performing segmentation and parsing of Hebrew
text. They experimented with various manual re-
finements of unlexicalized, treebank-derived gram-
mars, and showed that better grammars contribute
to better segmentation accuracies. Goldberg et al.
(2009) showed that segmentation and parsing ac-
curacies can be further improved by extending the
lexical coverage of a lattice-parser using an exter-
nal resource. Recently, Green and Manning (2010)
demonstrated the effectiveness of lattice-parsing for
parsing Arabic.
Here, we report the results of experiments cou-
pling lattice parsing together with the currently best
grammar learning method: the Berkeley PCFG-LA
parser (Petrov et al., 2006).
</bodyText>
<page confidence="0.978291">
704
</page>
<note confidence="0.7913485">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 704–709,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.797663" genericHeader="method">
2 Aspects of Modern Hebrew
</sectionHeader>
<bodyText confidence="0.999931175438597">
Some aspects that make Hebrew challenging from a
language-processing perspective are:
Affixation Common function words are prefixed
to the following word. These include: m(“from”)
f(“who”/“that”) h(“the”) w(“and”) k(“like”) l(“to”)
and b(“in”). Several such elements may attach to-
gether, producing forms such as wfmhfmf (w-f-m-h-
fmf “and-that-from-the-sun”). Notice that the last
part of the token, the noun fmf (“sun”), when ap-
pearing in isolation, can be also interpreted as the
sequence f-mf (“who moved”). The linear order
of such segmental elements within a token is fixed
(disallowing the reading w-f-m-h-f-mf in the previ-
ous example). However, the syntactic relations of
these elements with respect to the rest of the sen-
tence is rather free. The relativizer f (“that”) for
example may attach to an arbitrarily long relative
clause that goes beyond token boundaries. To fur-
ther complicate matters, the definite article h(“the”)
is not realized in writing when following the par-
ticles b(“in”),k(“like”) and l(“to”). Thus, the form
bbit can be interpreted as either b-bit (“in house”) or
b-h-bit (“in the house”). In addition, pronominal el-
ements may attach to nouns, verbs, adverbs, preposi-
tions and others as suffixes (e.g. lqxn(lqx-hn, “took-
them”), elihm(eli-hm,“on them”)). These affixations
result in highly ambiguous token segmentations.
Relatively free constituent order The ordering of
constituents inside a phrase is relatively free. This
is most notably apparent in the verbal phrases and
sentential levels. In particular, while most sentences
follow an SVO order, OVS and VSO configurations
are also possible. Verbal arguments can appear be-
fore or after the verb, and in many ordering. This
results in long and flat VP and S structures and a fair
amount of sparsity.
Rich templatic morphology Hebrew has a very
productive morphological structure, which is based
on a root+template system. The productive mor-
phology results in many distinct word forms and a
high out-of-vocabulary rate which makes it hard to
reliably estimate lexical parameters from annotated
corpora. The root+template system (combined with
the unvocalized writing system and rich affixation)
makes it hard to guess the morphological analyses
of an unknown word based on its prefix and suffix,
as usually done in other languages.
Unvocalized writing system Most vowels are not
marked in everyday Hebrew text, which results in a
very high level of lexical and morphological ambi-
guity. Some tokens can admit as many as 15 distinct
readings.
Agreement Hebrew grammar forces morpholog-
ical agreement between Adjectives and Nouns
(which should agree on Gender and Number and
definiteness), and between Subjects and Verbs
(which should agree on Gender and Number).
</bodyText>
<sectionHeader confidence="0.993545" genericHeader="method">
3 PCFG-LA Grammar Estimation
</sectionHeader>
<bodyText confidence="0.999971064516129">
Klein and Manning (2003) demonstrated that lin-
guistically informed splitting of non-terminal sym-
bols in treebank-derived grammars can result in ac-
curate grammars. Their work triggered investiga-
tions in automatic grammar refinement and state-
splitting (Matsuzaki et al., 2005; Prescher, 2005),
which was then perfected by (Petrov et al., 2006;
Petrov, 2009). The model of (Petrov et al., 2006) and
its publicly available implementation, the Berke-
ley parser2, works by starting with a bare-bones
treebank derived grammar and automatically refin-
ing it in split-merge-smooth cycles. The learning
works by iteratively (1) splitting each non-terminal
category in two, (2) merging back non-effective
splits and (3) smoothing the split non-terminals to-
ward their shared ancestor. Each of the steps is
followed by an EM-based parameter re-estimation.
This process allows learning tree annotations which
capture many latent syntactic interactions. At in-
ference time, the latent annotations are (approxi-
mately) marginalized out, resulting in the (approx-
imate) most probable unannotated tree according to
the refined grammar. This parsing methodology is
very robust, producing state of the art accuracies for
English, as well as many other languages including
German (Petrov and Klein, 2008), French (Candito
et al., 2009) and Chinese (Huang and Harper, 2009)
among others.
The grammar learning process is applied to bi-
narized parse trees, with 1st-order vertical and 0th-
order horizontal markovization. This means that in
</bodyText>
<footnote confidence="0.967732">
2http://code.google.com/p/berkeleyparser/
</footnote>
<page confidence="0.995237">
705
</page>
<figureCaption confidence="0.99404175">
Figure 1: Lattice representation of the sentence bclm hneim. Double-circles denote token boundaries. Lattice arcs correspond
to different segments of the token, each lattice path encodes a possible reading of the sentence. Notice how the token bclm have
analyses which include segments which are not directly present in the unsegmented form, such as the definite article h (1-3) and the
pronominal suffix which is expanded to the sequence fl hm (“of them”, 2-4, 4-5).
</figureCaption>
<bodyText confidence="0.999958818181818">
the initial grammar, each of the non-terminal sym-
bols is effectively conditioned on its parent alone,
and is independent of its sisters. This is a very
strong independence assumption. However, it al-
lows the resulting refined grammar to encode its own
set of dependencies between a node and its sisters, as
well as ordering preferences in long, flat rules. Our
initial experiments on Hebrew confirm that moving
to higher order horizontal markovization degrades
parsing performance, while producing much larger
grammars.
</bodyText>
<sectionHeader confidence="0.81276" genericHeader="method">
4 Lattice Representation and Parsing
</sectionHeader>
<bodyText confidence="0.966214962962963">
Following (Goldberg and Tsarfaty, 2008) we deal
with the ambiguous affixation patterns in Hebrew by
encoding the input sentence as a segmentation lat-
tice. Each token is encoded as a lattice representing
its possible analyses, and the token-lattices are then
concatenated to form the sentence-lattice. Figure 1
presents the lattice for the two token sentence “bclm
hneim”. Each lattice arc correspond to a lexical item.
Lattice Parsing The CKY parsing algorithm can
be extended to accept a lattice as its input (Chap-
pelier et al., 1999). This works by indexing lexi-
cal items by their start and end states in the lattice
instead of by their sentence position, and changing
the initialization procedure of CKY to allow termi-
nal and preterminal sybols of spans of sizes &gt; 1. It is
then relatively straightforward to modify the parsing
mechanism to support this change: not giving spe-
cial treatments for spans of size 1, and distinguish-
ing lexical items from non-terminals by a specified
marking instead of by their position in the chart. We
modified the PCFG-LA Berkeley parser to accept
lattice input at inference time (training is performed
as usual on fully observed treebank trees).
Lattice Construction We construct the token lat-
tices using MILA, a lexicon-based morphological
analyzer which provides a set of possible analyses
for each token (Itai and Wintner, 2008). While being
a high-coverage lexicon, its coverage is not perfect.
For the future, we consider using unknown handling
techniques such as those proposed in (Adler et al.,
2008). Still, the use of the lexicon for lattice con-
struction rather than relying on forms seen in the
treebank is essential to achieve parsing accuracy.
Lexical Probabilities Estimation Lexical p(t �
w) probabilities are defined over individual seg-
ments rather than for complete tokens. It is the role
of the syntactic model to assign probabilities to con-
texts which are larger than a single segment. We
use the default lexical probability estimation of the
Berkeley parser.3
Goldberg et al. (2009) suggest to estimate lexi-
cal probabilities for rare and unseen segments using
emission probabilities of an HMM tagger trained us-
ing EM on large corpora. Our preliminary exper-
iments with this method with the Berkeley parser
3Probabilities for robust segments (lexical items observed
100 times or more in training) are based on the MLE estimates
resulting from the EM procedure. Other segments are assigned
smoothed probabilities which combine the p(w1t) MLE esti-
mate with unigram tag probabilities. Segments which were not
seen in training are assigned a probability based on a single
distribution of tags for rare words. Crucially, we restrict each
segment to appear only with tags which are licensed by a mor-
phological analyzer, as encoded in the lattice.
</bodyText>
<page confidence="0.992693">
706
</page>
<bodyText confidence="0.999967">
showed mixed results. Parsing performance on the
test set dropped slightly.When analyzing the parsing
results on out-of-treebank text, we observed cases
where this estimation method indeed fixed mistakes,
and others where it hurt. We are still uncertain if the
slight drop in performance over the test set is due to
overfitting of the treebank vocabulary, or the inade-
quacy of the method in general.
</bodyText>
<sectionHeader confidence="0.998066" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999856878787879">
Data In all the experiments we use Ver.2 of the
Hebrew treebank (Guthmann et al., 2009), which
was converted to use the tagset of the MILA mor-
phological analyzer (Golderg et al., 2009). We use
the same splits as in previous work, with a train-
ing set of 5240 sentences (484-5724) and a test set
of 483 sentences (1-483). During development, we
evaluated on a random subset of 100 sentences from
the training set. Unless otherwise noted, we used the
basic non-terminal categories, without any extended
information available in them.
Gold Segmentation and Tagging To assess the
adequacy of the Berkeley parser for Hebrew, we per-
formed baseline experiments in which either gold
segmentation and tagging or just gold segmenta-
tion were available to the parser. The numbers are
very high: an F-measure of about 88.8% for the
gold segmentation and tagging, and about 82.8% for
gold segmentation only. This shows the adequacy
of the PCFG-LA methodology for parsing the He-
brew treebank, but also goes to show the highly am-
biguous nature of the tagging. Our baseline lattice
parsing experiment (without the lexicon) results in
an F-score of around 76%.4
Segmentation —* Parsing pipeline As another
baseline, we experimented with a pipeline system
in which the input text is automatically segmented
and tagged using a state-of-the-art HMM pos-tagger
(Goldberg et al., 2008). We then ignore the pro-
duced tagging, and pass the resulting segmented text
as input to the PCFG-LA parsing model as a deter-
ministic input (here the lattice representation is used
while tagging, but the parser sees a deterministic,
</bodyText>
<footnote confidence="0.955132333333333">
4For all the joint segmentation and parsing experiments, we
use a generalization of parseval that takes segmentation into ac-
count. See (Tsarfaty, 2006) for the exact details.
</footnote>
<bodyText confidence="0.999271388888889">
segmented input).5 In the pipeline setting, we either
allow the parser to assign all possible POS-tags, or
restrict it to POS-tags licensed by the lexicon.
Lattice Parsing Experiments Our initial lattice
parsing experiments with the Berkeley parser were
disappointing. The lattice seemed too permissive,
allowing the parser to chose weird analyses. Error
analysis suggested the parser failed to distinguish
among the various kinds of VPs: finite, non-finite
and modals. Once we annotate the treebank verbs
into finite, non-finite and modals6, results improve
a lot. Further improvement was gained by specifi-
cally marking the subject-NPs.7 The parser was not
able to correctly learn these splits on its own, but
once they were manually provided it did a very good
job utilizing this information.8 Marking object NPs
did not help on their own, and slightly degraded the
performance when both subjects and objects were
marked. It appears that the learning procedure man-
aged to learn the structure of objects without our
help. In all the experiments, the use of the morpho-
logical analyzer in producing the lattice was crucial
for parsing accuracy.
Results Our final configuration (marking verbal
forms and subject-NPs, using the analyzer to con-
struct the lattice and training the parser for 5 itera-
tions) produces remarkable parsing accuracy when
parsing from unsegmented text: an F-score of
79.9% (prec: 82.3 rec: 77.6) and seg+tagging F of
93.8%. The pipeline systems with the same gram-
mar achieve substantially lower F-scores of 75.2%
(without the lexicon) and 77.3 (with the lexicon).
For comparison, the previous best results for pars-
ing Hebrew are 84.1%F assuming gold segmenta-
tion and tagging (Tsarfaty and Sima’an, 2010)9, and
73.7%F starting from unsegmented text (Golderg et
</bodyText>
<footnote confidence="0.997974615384615">
5The segmentation+tagging accuracy of the HMM tagger on
the Treebank data is 91.3%F.
6This information is available in both the treebank and the
morphological analyzer, but we removed it at first. Note that the
verb-type distinction is specified only on the pre-terminal level,
and not on the phrase-level.
7Such markings were removed prior to evaluation.
8Candito et al. (2009) also report improvements in accu-
racy when providing the PCFG-LA parser with few manually-
devised linguistically-motivated state-splits.
9The 84.1 figure is for sentences of length &lt; 40, and thus
not strictly comparable with all the other numbers in this paper,
which are based on the entire test-set.
</footnote>
<page confidence="0.982167">
707
</page>
<table confidence="0.9944125">
System Oracle OOV Handling Prec Rec Fl
Tsarfaty and Sima’an 2010 Gold Seg+Tag – - - 84.1
Goldberg et al. 2009 None Lexicon 73.4 74.0 73.8
Seg → PCFG-LA Pipeline None Treebank 75.6 74.8 75.2
Seg → PCFG-LA Pipeline None Lexicon 79.5 75.2 77.3
PCFG-LA + Lattice (Joint) None Lexicon 82.3 77.6 79.9
</table>
<tableCaption confidence="0.999947">
Table 1: Parsing scores of the various systems
</tableCaption>
<bodyText confidence="0.999712973684211">
al., 2009). The numbers are summarized in Table 1.
While the pipeline system already improves over the
previous best results, the lattice-based joint-model
improves results even further. Overall, the PCFG-
LA+Lattice parser improve results by 6 F-points ab-
solute, an error reduction of about 20%. Tagging
accuracies are also remarkable, and constitute state-
of-the-art tagging for Hebrew.
The strengths of the system can be attributed to
three factors: (1) performing segmentation, tagging
and parsing jointly using lattice parsing, (2) relying
on an external resource (lexicon / morphological an-
alyzer) instead of on the Treebank to provide lexical
coverage and (3) using a strong syntactic model.
Running time The lattice representation effec-
tively results in longer inputs to the parser. It is
informative to quantify the effect of the lattice rep-
resentation on the parsing time, which is cubic in
sentence length. The pipeline parser parsed the
483 pre-segmented input sentences in 151 seconds
(3.2 sentences/second) not including segmentation
time, while the lattice parser took 175 seconds (2.7
sents/second) including lattice construction. Parsing
with the lattice representation is slower than in the
pipeline setup, but not prohibitively so.
Analysis and Limitations When analyzing the
learned grammar, we see that it learned to distin-
guish short from long constituents, models conjunc-
tion parallelism fairly well, and picked up a lot
of information regarding the structure of quantities,
dates, named and other kinds of NPs. It also learned
to reasonably model definiteness, and that S ele-
ments have at most one Subject. However, the state-
split model exhibits no notion of syntactic agree-
ment on gender and number. This is troubling, as
we encountered a fair amount of parsing mistakes
which would have been solved if the parser were to
use agreement information.
</bodyText>
<sectionHeader confidence="0.998814" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999995269230769">
We demonstrated that the combination of lattice
parsing with the PCFG-LA Berkeley parser is highly
effective. Lattice parsing allows much needed flexi-
bility in providing input to a parser when the yield of
the tree is not known in advance, and the grammar
refinement and estimation techniques of the Berke-
ley parser provide a strong disambiguation compo-
nent. In this work, we applied the Berkeley+Lattice
parser to the challenging task of joint segmentation
and parsing of Hebrew text. The result is the first
constituency parser which can parse naturally occur-
ring unsegmented Hebrew text with an acceptable
accuracy (an Fi score of 80%).
Many other uses of lattice parsing are possible.
These include joint segmentation and parsing of
Chinese, empty element prediction (see (Cai et al.,
2011) for a successful application), and a princi-
pled handling of multiword-expressions, idioms and
named-entities. The code of the lattice extension to
the Berkeley parser is publicly available.10
Despite its strong performance, we observed that
the Berkeley parser did not learn morphological
agreement patterns. Agreement information could
be very useful for disambiguating various construc-
tions in Hebrew and other morphologically rich lan-
guages. We plan to address this point in future work.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992154571428571">
We thank Slav Petrov for making available and an-
swering questions about the code of his parser, Fed-
erico Sangati for pointing out some important details
regarding the evaluation, and the three anonymous
reviewers for their helpful comments. The work is
supported by the Lynn and William Frankel Center
for Computer Sciences, Ben-Gurion University.
</bodyText>
<footnote confidence="0.886069">
10http://www.cs.bgu.ac.il/—yoavg/software/blatt/
</footnote>
<page confidence="0.994709">
708
</page>
<sectionHeader confidence="0.996042" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999824121621622">
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolu-
tion of unknown words for full morphological analy-
sis. In Proc. of ACL.
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty elements.
In Proc. of ACL (short-paper).
Marie Candito, Benoit Crabb´e, and Djam´e Seddah. 2009.
On statistical parsing of French with supervised and
semi-supervised strategies. In EACL 2009 Workshop
Grammatical inference for Computational Linguistics,
Athens, Greece.
J. Chappelier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice Parsing for Speech Recognition.
In In Sixth Conference sur le Traitement Automatique
du Langage Naturel (TANL99), pages 95–104.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proc. of ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM Can find pretty good HMM POS-Taggers (when
given a good start). In Proc. ofACL.
Yoav Golderg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and em-hmm-based lexical probabilities. In
Proc. of EACL.
Spence Green and Christopher Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proc. of COLING.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proc. of TLT.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proc. of the EMNLP, pages 832–
841. Association for Computational Linguistics.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75–98, March.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. of ACL, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc of ACL.
Slav Petrov and Dan Klein. 2008. Parsing German with
latent variable grammars. In Proceedings of the ACL
Workshop on Parsing German.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of ACL, Sydney,
Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA, USA.
Detlef Prescher. 2005. Inducing head-driven PCFGs
with latent heads: Refining a tree-bank grammar for
parsing. In Proc. of ECML.
Khalil Sima’an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank of
Modern Hebrew text. Traitement Automatique des
Langues, 42(2).
Reut Tsarfaty and Khalil Sima’an. 2010. Model-
ing morphosyntactic agreement in constituency-based
parsing of Modern Hebrew. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Reut Tsarfaty. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Proc. of
ACL-SRW.
</reference>
<page confidence="0.998657">
709
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.737670">
<title confidence="0.962232">Joint Hebrew Segmentation and using a PCFG-LA Lattice Parser Goldberg Elhadad</title>
<author confidence="0.996065">Ben Gurion University of the Negev</author>
<affiliation confidence="0.99726">Department of Computer Science</affiliation>
<address confidence="0.835985">POB 653 Be’er Sheva, 84105,</address>
<abstract confidence="0.999626066666667">We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser. We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task. This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Meni Adler</author>
<author>Yoav Goldberg</author>
<author>David Gabay</author>
<author>Michael Elhadad</author>
</authors>
<title>Unsupervised lexicon-based resolution of unknown words for full morphological analysis.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="10413" citStr="Adler et al., 2008" startWordPosition="1604" endWordPosition="1607">nguishing lexical items from non-terminals by a specified marking instead of by their position in the chart. We modified the PCFG-LA Berkeley parser to accept lattice input at inference time (training is performed as usual on fully observed treebank trees). Lattice Construction We construct the token lattices using MILA, a lexicon-based morphological analyzer which provides a set of possible analyses for each token (Itai and Wintner, 2008). While being a high-coverage lexicon, its coverage is not perfect. For the future, we consider using unknown handling techniques such as those proposed in (Adler et al., 2008). Still, the use of the lexicon for lattice construction rather than relying on forms seen in the treebank is essential to achieve parsing accuracy. Lexical Probabilities Estimation Lexical p(t � w) probabilities are defined over individual segments rather than for complete tokens. It is the role of the syntactic model to assign probabilities to contexts which are larger than a single segment. We use the default lexical probability estimation of the Berkeley parser.3 Goldberg et al. (2009) suggest to estimate lexical probabilities for rare and unseen segments using emission probabilities of an</context>
</contexts>
<marker>Adler, Goldberg, Gabay, Elhadad, 2008</marker>
<rawString>Meni Adler, Yoav Goldberg, David Gabay, and Michael Elhadad. 2008. Unsupervised lexicon-based resolution of unknown words for full morphological analysis. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shu Cai</author>
<author>David Chiang</author>
<author>Yoav Goldberg</author>
</authors>
<title>Language-independent parsing with empty elements.</title>
<date>2011</date>
<booktitle>In Proc. of ACL (short-paper).</booktitle>
<contexts>
<context position="19364" citStr="Cai et al., 2011" startWordPosition="3032" endWordPosition="3035">o a parser when the yield of the tree is not known in advance, and the grammar refinement and estimation techniques of the Berkeley parser provide a strong disambiguation component. In this work, we applied the Berkeley+Lattice parser to the challenging task of joint segmentation and parsing of Hebrew text. The result is the first constituency parser which can parse naturally occurring unsegmented Hebrew text with an acceptable accuracy (an Fi score of 80%). Many other uses of lattice parsing are possible. These include joint segmentation and parsing of Chinese, empty element prediction (see (Cai et al., 2011) for a successful application), and a principled handling of multiword-expressions, idioms and named-entities. The code of the lattice extension to the Berkeley parser is publicly available.10 Despite its strong performance, we observed that the Berkeley parser did not learn morphological agreement patterns. Agreement information could be very useful for disambiguating various constructions in Hebrew and other morphologically rich languages. We plan to address this point in future work. Acknowledgments We thank Slav Petrov for making available and answering questions about the code of his pars</context>
</contexts>
<marker>Cai, Chiang, Goldberg, 2011</marker>
<rawString>Shu Cai, David Chiang, and Yoav Goldberg. 2011. Language-independent parsing with empty elements. In Proc. of ACL (short-paper).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Benoit Crabb´e</author>
<author>Djam´e Seddah</author>
</authors>
<title>On statistical parsing of French with supervised and semi-supervised strategies.</title>
<date>2009</date>
<booktitle>In EACL 2009 Workshop Grammatical inference for Computational Linguistics,</booktitle>
<location>Athens, Greece.</location>
<marker>Candito, Crabb´e, Seddah, 2009</marker>
<rawString>Marie Candito, Benoit Crabb´e, and Djam´e Seddah. 2009. On statistical parsing of French with supervised and semi-supervised strategies. In EACL 2009 Workshop Grammatical inference for Computational Linguistics, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chappelier</author>
<author>M Rajman</author>
<author>R Aragues</author>
<author>A Rozenknop</author>
</authors>
<title>Lattice Parsing for Speech Recognition. In</title>
<date>1999</date>
<booktitle>In Sixth Conference sur le Traitement Automatique du Langage Naturel (TANL99),</booktitle>
<pages>95--104</pages>
<contexts>
<context position="9404" citStr="Chappelier et al., 1999" startWordPosition="1439" endWordPosition="1443"> performance, while producing much larger grammars. 4 Lattice Representation and Parsing Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice. Each token is encoded as a lattice representing its possible analyses, and the token-lattices are then concatenated to form the sentence-lattice. Figure 1 presents the lattice for the two token sentence “bclm hneim”. Each lattice arc correspond to a lexical item. Lattice Parsing The CKY parsing algorithm can be extended to accept a lattice as its input (Chappelier et al., 1999). This works by indexing lexical items by their start and end states in the lattice instead of by their sentence position, and changing the initialization procedure of CKY to allow terminal and preterminal sybols of spans of sizes &gt; 1. It is then relatively straightforward to modify the parsing mechanism to support this change: not giving special treatments for spans of size 1, and distinguishing lexical items from non-terminals by a specified marking instead of by their position in the chart. We modified the PCFG-LA Berkeley parser to accept lattice input at inference time (training is perfor</context>
</contexts>
<marker>Chappelier, Rajman, Aragues, Rozenknop, 1999</marker>
<rawString>J. Chappelier, M. Rajman, R. Aragues, and A. Rozenknop. 1999. Lattice Parsing for Speech Recognition. In In Sixth Conference sur le Traitement Automatique du Langage Naturel (TANL99), pages 95–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A single generative model for joint morphological segmentation and syntactic parsing.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2541" citStr="Goldberg and Tsarfaty (2008)" startWordPosition="408" endWordPosition="411">t. A common method of approaching the discrepancy between input strings and space delimited tokens is using a pipeline process, in which the input string is pre-segmented prior to handing it to a parser. The shortcoming of this method, as noted by (Tsarfaty, 2006), is that many segmentation decisions cannot be resolved based on local context alone. Rather, they may depend on long distance relations and interact closely with the syntactic structure of the sentence. Thus, segmentation decisions should be integrated into the parsing process and not performed as an independent preprocessing step. Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text. They experimented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better grammars contribute to better segmentation accuracies. Goldberg et al. (2009) showed that segmentation and parsing accuracies can be further improved by extending the lexical coverage of a lattice-parser using an external resource. Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic. Here, we report the results of</context>
<context position="8908" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="1359" endWordPosition="1362">nce fl hm (“of them”, 2-4, 4-5). the initial grammar, each of the non-terminal symbols is effectively conditioned on its parent alone, and is independent of its sisters. This is a very strong independence assumption. However, it allows the resulting refined grammar to encode its own set of dependencies between a node and its sisters, as well as ordering preferences in long, flat rules. Our initial experiments on Hebrew confirm that moving to higher order horizontal markovization degrades parsing performance, while producing much larger grammars. 4 Lattice Representation and Parsing Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice. Each token is encoded as a lattice representing its possible analyses, and the token-lattices are then concatenated to form the sentence-lattice. Figure 1 presents the lattice for the two token sentence “bclm hneim”. Each lattice arc correspond to a lexical item. Lattice Parsing The CKY parsing algorithm can be extended to accept a lattice as its input (Chappelier et al., 1999). This works by indexing lexical items by their start and end states in the lattice instead of by their </context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and syntactic parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>EM Can find pretty good HMM POS-Taggers (when given a good start). In</title>
<date>2008</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="13455" citStr="Goldberg et al., 2008" startWordPosition="2097" endWordPosition="2100"> to the parser. The numbers are very high: an F-measure of about 88.8% for the gold segmentation and tagging, and about 82.8% for gold segmentation only. This shows the adequacy of the PCFG-LA methodology for parsing the Hebrew treebank, but also goes to show the highly ambiguous nature of the tagging. Our baseline lattice parsing experiment (without the lexicon) results in an F-score of around 76%.4 Segmentation —* Parsing pipeline As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMM pos-tagger (Goldberg et al., 2008). We then ignore the produced tagging, and pass the resulting segmented text as input to the PCFG-LA parsing model as a deterministic input (here the lattice representation is used while tagging, but the parser sees a deterministic, 4For all the joint segmentation and parsing experiments, we use a generalization of parseval that takes segmentation into account. See (Tsarfaty, 2006) for the exact details. segmented input).5 In the pipeline setting, we either allow the parser to assign all possible POS-tags, or restrict it to POS-tags licensed by the lexicon. Lattice Parsing Experiments Our init</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008. EM Can find pretty good HMM POS-Taggers (when given a good start). In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Golderg</author>
<author>Reut Tsarfaty</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and em-hmm-based lexical probabilities.</title>
<date>2009</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="12278" citStr="Golderg et al., 2009" startWordPosition="1907" endWordPosition="1910">ttice. 706 showed mixed results. Parsing performance on the test set dropped slightly.When analyzing the parsing results on out-of-treebank text, we observed cases where this estimation method indeed fixed mistakes, and others where it hurt. We are still uncertain if the slight drop in performance over the test set is due to overfitting of the treebank vocabulary, or the inadequacy of the method in general. 5 Experiments and Results Data In all the experiments we use Ver.2 of the Hebrew treebank (Guthmann et al., 2009), which was converted to use the tagset of the MILA morphological analyzer (Golderg et al., 2009). We use the same splits as in previous work, with a training set of 5240 sentences (484-5724) and a test set of 483 sentences (1-483). During development, we evaluated on a random subset of 100 sentences from the training set. Unless otherwise noted, we used the basic non-terminal categories, without any extended information available in them. Gold Segmentation and Tagging To assess the adequacy of the Berkeley parser for Hebrew, we performed baseline experiments in which either gold segmentation and tagging or just gold segmentation were available to the parser. The numbers are very high: an</context>
</contexts>
<marker>Golderg, Tsarfaty, Adler, Elhadad, 2009</marker>
<rawString>Yoav Golderg, Reut Tsarfaty, Meni Adler, and Michael Elhadad. 2009. Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and em-hmm-based lexical probabilities. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Christopher Manning</author>
</authors>
<title>Better Arabic parsing: Baselines, evaluations, and analysis.</title>
<date>2010</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="3040" citStr="Green and Manning (2010)" startWordPosition="479" endWordPosition="482">d be integrated into the parsing process and not performed as an independent preprocessing step. Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text. They experimented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better grammars contribute to better segmentation accuracies. Goldberg et al. (2009) showed that segmentation and parsing accuracies can be further improved by extending the lexical coverage of a lattice-parser using an external resource. Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic. Here, we report the results of experiments coupling lattice parsing together with the currently best grammar learning method: the Berkeley PCFG-LA parser (Petrov et al., 2006). 704 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 704–709, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Aspects of Modern Hebrew Some aspects that make Hebrew challenging from a language-processing perspective are: Affixation Common function words a</context>
</contexts>
<marker>Green, Manning, 2010</marker>
<rawString>Spence Green and Christopher Manning. 2010. Better Arabic parsing: Baselines, evaluations, and analysis. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noemie Guthmann</author>
<author>Yuval Krymolowski</author>
<author>Adi Milea</author>
<author>Yoad Winter</author>
</authors>
<title>Automatic annotation of morphosyntactic dependencies in a Modern Hebrew Treebank.</title>
<date>2009</date>
<booktitle>In Proc. of TLT.</booktitle>
<contexts>
<context position="12181" citStr="Guthmann et al., 2009" startWordPosition="1890" endWordPosition="1893">ment to appear only with tags which are licensed by a morphological analyzer, as encoded in the lattice. 706 showed mixed results. Parsing performance on the test set dropped slightly.When analyzing the parsing results on out-of-treebank text, we observed cases where this estimation method indeed fixed mistakes, and others where it hurt. We are still uncertain if the slight drop in performance over the test set is due to overfitting of the treebank vocabulary, or the inadequacy of the method in general. 5 Experiments and Results Data In all the experiments we use Ver.2 of the Hebrew treebank (Guthmann et al., 2009), which was converted to use the tagset of the MILA morphological analyzer (Golderg et al., 2009). We use the same splits as in previous work, with a training set of 5240 sentences (484-5724) and a test set of 483 sentences (1-483). During development, we evaluated on a random subset of 100 sentences from the training set. Unless otherwise noted, we used the basic non-terminal categories, without any extended information available in them. Gold Segmentation and Tagging To assess the adequacy of the Berkeley parser for Hebrew, we performed baseline experiments in which either gold segmentation </context>
</contexts>
<marker>Guthmann, Krymolowski, Milea, Winter, 2009</marker>
<rawString>Noemie Guthmann, Yuval Krymolowski, Adi Milea, and Yoad Winter. 2009. Automatic annotation of morphosyntactic dependencies in a Modern Hebrew Treebank. In Proc. of TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>Selftraining PCFG grammars with latent annotations across languages.</title>
<date>2009</date>
<booktitle>In Proc. of the EMNLP,</booktitle>
<pages>832--841</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7636" citStr="Huang and Harper, 2009" startWordPosition="1168" endWordPosition="1171">lit non-terminals toward their shared ancestor. Each of the steps is followed by an EM-based parameter re-estimation. This process allows learning tree annotations which capture many latent syntactic interactions. At inference time, the latent annotations are (approximately) marginalized out, resulting in the (approximate) most probable unannotated tree according to the refined grammar. This parsing methodology is very robust, producing state of the art accuracies for English, as well as many other languages including German (Petrov and Klein, 2008), French (Candito et al., 2009) and Chinese (Huang and Harper, 2009) among others. The grammar learning process is applied to binarized parse trees, with 1st-order vertical and 0thorder horizontal markovization. This means that in 2http://code.google.com/p/berkeleyparser/ 705 Figure 1: Lattice representation of the sentence bclm hneim. Double-circles denote token boundaries. Lattice arcs correspond to different segments of the token, each lattice path encodes a possible reading of the sentence. Notice how the token bclm have analyses which include segments which are not directly present in the unsegmented form, such as the definite article h (1-3) and the pron</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2009. Selftraining PCFG grammars with latent annotations across languages. In Proc. of the EMNLP, pages 832– 841. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Itai</author>
<author>Shuly Wintner</author>
</authors>
<title>Language resources for Hebrew. Language Resources and Evaluation,</title>
<date>2008</date>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="10237" citStr="Itai and Wintner, 2008" startWordPosition="1576" endWordPosition="1579">ls of spans of sizes &gt; 1. It is then relatively straightforward to modify the parsing mechanism to support this change: not giving special treatments for spans of size 1, and distinguishing lexical items from non-terminals by a specified marking instead of by their position in the chart. We modified the PCFG-LA Berkeley parser to accept lattice input at inference time (training is performed as usual on fully observed treebank trees). Lattice Construction We construct the token lattices using MILA, a lexicon-based morphological analyzer which provides a set of possible analyses for each token (Itai and Wintner, 2008). While being a high-coverage lexicon, its coverage is not perfect. For the future, we consider using unknown handling techniques such as those proposed in (Adler et al., 2008). Still, the use of the lexicon for lattice construction rather than relying on forms seen in the treebank is essential to achieve parsing accuracy. Lexical Probabilities Estimation Lexical p(t � w) probabilities are defined over individual segments rather than for complete tokens. It is the role of the syntactic model to assign probabilities to contexts which are larger than a single segment. We use the default lexical </context>
</contexts>
<marker>Itai, Wintner, 2008</marker>
<rawString>Alon Itai and Shuly Wintner. 2008. Language resources for Hebrew. Language Resources and Evaluation, 42(1):75–98, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="6314" citStr="Klein and Manning (2003)" startWordPosition="973" endWordPosition="976">tion) makes it hard to guess the morphological analyses of an unknown word based on its prefix and suffix, as usually done in other languages. Unvocalized writing system Most vowels are not marked in everyday Hebrew text, which results in a very high level of lexical and morphological ambiguity. Some tokens can admit as many as 15 distinct readings. Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns (which should agree on Gender and Number and definiteness), and between Subjects and Verbs (which should agree on Gender and Number). 3 PCFG-LA Grammar Estimation Klein and Manning (2003) demonstrated that linguistically informed splitting of non-terminal symbols in treebank-derived grammars can result in accurate grammars. Their work triggered investigations in automatic grammar refinement and statesplitting (Matsuzaki et al., 2005; Prescher, 2005), which was then perfected by (Petrov et al., 2006; Petrov, 2009). The model of (Petrov et al., 2006) and its publicly available implementation, the Berkeley parser2, works by starting with a bare-bones treebank derived grammar and automatically refining it in split-merge-smooth cycles. The learning works by iteratively (1) splittin</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. of ACL, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="6563" citStr="Matsuzaki et al., 2005" startWordPosition="1008" endWordPosition="1011">level of lexical and morphological ambiguity. Some tokens can admit as many as 15 distinct readings. Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns (which should agree on Gender and Number and definiteness), and between Subjects and Verbs (which should agree on Gender and Number). 3 PCFG-LA Grammar Estimation Klein and Manning (2003) demonstrated that linguistically informed splitting of non-terminal symbols in treebank-derived grammars can result in accurate grammars. Their work triggered investigations in automatic grammar refinement and statesplitting (Matsuzaki et al., 2005; Prescher, 2005), which was then perfected by (Petrov et al., 2006; Petrov, 2009). The model of (Petrov et al., 2006) and its publicly available implementation, the Berkeley parser2, works by starting with a bare-bones treebank derived grammar and automatically refining it in split-merge-smooth cycles. The learning works by iteratively (1) splitting each non-terminal category in two, (2) merging back non-effective splits and (3) smoothing the split non-terminals toward their shared ancestor. Each of the steps is followed by an EM-based parameter re-estimation. This process allows learning tre</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Parsing German with latent variable grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL Workshop on Parsing German.</booktitle>
<contexts>
<context position="7568" citStr="Petrov and Klein, 2008" startWordPosition="1157" endWordPosition="1160"> two, (2) merging back non-effective splits and (3) smoothing the split non-terminals toward their shared ancestor. Each of the steps is followed by an EM-based parameter re-estimation. This process allows learning tree annotations which capture many latent syntactic interactions. At inference time, the latent annotations are (approximately) marginalized out, resulting in the (approximate) most probable unannotated tree according to the refined grammar. This parsing methodology is very robust, producing state of the art accuracies for English, as well as many other languages including German (Petrov and Klein, 2008), French (Candito et al., 2009) and Chinese (Huang and Harper, 2009) among others. The grammar learning process is applied to binarized parse trees, with 1st-order vertical and 0thorder horizontal markovization. This means that in 2http://code.google.com/p/berkeleyparser/ 705 Figure 1: Lattice representation of the sentence bclm hneim. Double-circles denote token boundaries. Lattice arcs correspond to different segments of the token, each lattice path encodes a possible reading of the sentence. Notice how the token bclm have analyses which include segments which are not directly present in the</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008. Parsing German with latent variable grammars. In Proceedings of the ACL Workshop on Parsing German.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="3286" citStr="Petrov et al., 2006" startWordPosition="514" endWordPosition="517">imented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better grammars contribute to better segmentation accuracies. Goldberg et al. (2009) showed that segmentation and parsing accuracies can be further improved by extending the lexical coverage of a lattice-parser using an external resource. Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic. Here, we report the results of experiments coupling lattice parsing together with the currently best grammar learning method: the Berkeley PCFG-LA parser (Petrov et al., 2006). 704 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 704–709, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Aspects of Modern Hebrew Some aspects that make Hebrew challenging from a language-processing perspective are: Affixation Common function words are prefixed to the following word. These include: m(“from”) f(“who”/“that”) h(“the”) w(“and”) k(“like”) l(“to”) and b(“in”). Several such elements may attach together, producing forms such as wfmhfmf (w-f-m-hfmf “and-that-from-the-sun”). Notice t</context>
<context position="6630" citStr="Petrov et al., 2006" startWordPosition="1019" endWordPosition="1022"> many as 15 distinct readings. Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns (which should agree on Gender and Number and definiteness), and between Subjects and Verbs (which should agree on Gender and Number). 3 PCFG-LA Grammar Estimation Klein and Manning (2003) demonstrated that linguistically informed splitting of non-terminal symbols in treebank-derived grammars can result in accurate grammars. Their work triggered investigations in automatic grammar refinement and statesplitting (Matsuzaki et al., 2005; Prescher, 2005), which was then perfected by (Petrov et al., 2006; Petrov, 2009). The model of (Petrov et al., 2006) and its publicly available implementation, the Berkeley parser2, works by starting with a bare-bones treebank derived grammar and automatically refining it in split-merge-smooth cycles. The learning works by iteratively (1) splitting each non-terminal category in two, (2) merging back non-effective splits and (3) smoothing the split non-terminals toward their shared ancestor. Each of the steps is followed by an EM-based parameter re-estimation. This process allows learning tree annotations which capture many latent syntactic interactions. At </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Coarse-to-Fine Natural Language Processing.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Bekeley,</institution>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="6645" citStr="Petrov, 2009" startWordPosition="1023" endWordPosition="1024">readings. Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns (which should agree on Gender and Number and definiteness), and between Subjects and Verbs (which should agree on Gender and Number). 3 PCFG-LA Grammar Estimation Klein and Manning (2003) demonstrated that linguistically informed splitting of non-terminal symbols in treebank-derived grammars can result in accurate grammars. Their work triggered investigations in automatic grammar refinement and statesplitting (Matsuzaki et al., 2005; Prescher, 2005), which was then perfected by (Petrov et al., 2006; Petrov, 2009). The model of (Petrov et al., 2006) and its publicly available implementation, the Berkeley parser2, works by starting with a bare-bones treebank derived grammar and automatically refining it in split-merge-smooth cycles. The learning works by iteratively (1) splitting each non-terminal category in two, (2) merging back non-effective splits and (3) smoothing the split non-terminals toward their shared ancestor. Each of the steps is followed by an EM-based parameter re-estimation. This process allows learning tree annotations which capture many latent syntactic interactions. At inference time,</context>
</contexts>
<marker>Petrov, 2009</marker>
<rawString>Slav Petrov. 2009. Coarse-to-Fine Natural Language Processing. Ph.D. thesis, University of California at Bekeley, Berkeley, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detlef Prescher</author>
</authors>
<title>Inducing head-driven PCFGs with latent heads: Refining a tree-bank grammar for parsing.</title>
<date>2005</date>
<booktitle>In Proc. of ECML.</booktitle>
<contexts>
<context position="6580" citStr="Prescher, 2005" startWordPosition="1012" endWordPosition="1013">phological ambiguity. Some tokens can admit as many as 15 distinct readings. Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns (which should agree on Gender and Number and definiteness), and between Subjects and Verbs (which should agree on Gender and Number). 3 PCFG-LA Grammar Estimation Klein and Manning (2003) demonstrated that linguistically informed splitting of non-terminal symbols in treebank-derived grammars can result in accurate grammars. Their work triggered investigations in automatic grammar refinement and statesplitting (Matsuzaki et al., 2005; Prescher, 2005), which was then perfected by (Petrov et al., 2006; Petrov, 2009). The model of (Petrov et al., 2006) and its publicly available implementation, the Berkeley parser2, works by starting with a bare-bones treebank derived grammar and automatically refining it in split-merge-smooth cycles. The learning works by iteratively (1) splitting each non-terminal category in two, (2) merging back non-effective splits and (3) smoothing the split non-terminals toward their shared ancestor. Each of the steps is followed by an EM-based parameter re-estimation. This process allows learning tree annotations whi</context>
</contexts>
<marker>Prescher, 2005</marker>
<rawString>Detlef Prescher. 2005. Inducing head-driven PCFGs with latent heads: Refining a tree-bank grammar for parsing. In Proc. of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
<author>Alon Itai</author>
<author>Yoad Winter</author>
<author>Alon Altman</author>
<author>Noa Nativ</author>
</authors>
<title>Building a Tree-Bank of Modern Hebrew text.</title>
<date>2001</date>
<booktitle>Traitement Automatique des Langues,</booktitle>
<volume>42</volume>
<issue>2</issue>
<marker>Sima’an, Itai, Winter, Altman, Nativ, 2001</marker>
<rawString>Khalil Sima’an, Alon Itai, Yoad Winter, Alon Altman, and Noa Nativ. 2001. Building a Tree-Bank of Modern Hebrew text. Traitement Automatique des Langues, 42(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Khalil Sima’an</author>
</authors>
<title>Modeling morphosyntactic agreement in constituency-based parsing of Modern Hebrew.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL/HLT Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL 2010),</booktitle>
<location>Los Angeles, CA.</location>
<marker>Tsarfaty, Sima’an, 2010</marker>
<rawString>Reut Tsarfaty and Khalil Sima’an. 2010. Modeling morphosyntactic agreement in constituency-based parsing of Modern Hebrew. In Proceedings of the NAACL/HLT Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL 2010), Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
</authors>
<title>Integrated Morphological and Syntactic Disambiguation for Modern Hebrew.</title>
<date>2006</date>
<booktitle>In Proc. of ACL-SRW.</booktitle>
<contexts>
<context position="2177" citStr="Tsarfaty, 2006" startWordPosition="352" endWordPosition="353">g “in (the) shadow”. In such languages, the sequence of lexical 1We adopt here the transliteration scheme of (Sima’an et al., 2001) items corresponding to an input string is ambiguous, and cannot be determined using a deterministic procedure. In this work, we focus on constituency parsing of Modern Hebrew (henceforth Hebrew) from raw unsegmented text. A common method of approaching the discrepancy between input strings and space delimited tokens is using a pipeline process, in which the input string is pre-segmented prior to handing it to a parser. The shortcoming of this method, as noted by (Tsarfaty, 2006), is that many segmentation decisions cannot be resolved based on local context alone. Rather, they may depend on long distance relations and interact closely with the syntactic structure of the sentence. Thus, segmentation decisions should be integrated into the parsing process and not performed as an independent preprocessing step. Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text. They experimented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better gra</context>
<context position="13839" citStr="Tsarfaty, 2006" startWordPosition="2161" endWordPosition="2162">around 76%.4 Segmentation —* Parsing pipeline As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMM pos-tagger (Goldberg et al., 2008). We then ignore the produced tagging, and pass the resulting segmented text as input to the PCFG-LA parsing model as a deterministic input (here the lattice representation is used while tagging, but the parser sees a deterministic, 4For all the joint segmentation and parsing experiments, we use a generalization of parseval that takes segmentation into account. See (Tsarfaty, 2006) for the exact details. segmented input).5 In the pipeline setting, we either allow the parser to assign all possible POS-tags, or restrict it to POS-tags licensed by the lexicon. Lattice Parsing Experiments Our initial lattice parsing experiments with the Berkeley parser were disappointing. The lattice seemed too permissive, allowing the parser to chose weird analyses. Error analysis suggested the parser failed to distinguish among the various kinds of VPs: finite, non-finite and modals. Once we annotate the treebank verbs into finite, non-finite and modals6, results improve a lot. Further im</context>
</contexts>
<marker>Tsarfaty, 2006</marker>
<rawString>Reut Tsarfaty. 2006. Integrated Morphological and Syntactic Disambiguation for Modern Hebrew. In Proc. of ACL-SRW.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>