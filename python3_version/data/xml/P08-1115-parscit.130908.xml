<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000600">
<title confidence="0.971869">
Generalizing Word Lattice Translation
</title>
<author confidence="0.998895">
Christopher Dyer*, Smaranda Muresan, Philip Resnik*
</author>
<affiliation confidence="0.9983215">
Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
*Department of Linguistics
University of Maryland
</affiliation>
<address confidence="0.848877">
College Park, MD 20742, USA
</address>
<email confidence="0.715845">
redpony, smara, resnik AT umd.edu
</email>
<sectionHeader confidence="0.982212" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999002538461539">
Word lattice decoding has proven useful in
spoken language translation; we argue that it
provides a compelling model for translation of
text genres, as well. We show that prior work
in translating lattices using finite state tech-
niques can be naturally extended to more ex-
pressive synchronous context-free grammar-
based models. Additionally, we resolve a
significant complication that non-linear word
lattice inputs introduce in reordering mod-
els. Our experiments evaluating the approach
demonstrate substantial gains for Chinese-
English and Arabic-English translation.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999676">
When Brown and colleagues introduced statistical
machine translation in the early 1990s, their key in-
sight – harkening back to Weaver in the late 1940s –
was that translation could be viewed as an instance
of noisy channel modeling (Brown et al., 1990).
They introduced a now standard decomposition that
distinguishes modeling sentences in the target lan-
guage (language models) from modeling the rela-
tionship between source and target language (trans-
lation models). Today, virtually all statistical trans-
lation systems seek the best hypothesis e for a given
input f in the source language, according to
consider all possibilities for f by encoding the alter-
natives compactly as a confusion network or lattice
(Bertoldi et al., 2007; Bertoldi and Federico, 2005;
Koehn et al., 2007).
Why, however, should this advantage be limited
to translation from spoken input? Even for text,
there are often multiple ways to derive a sequence
of words from the input string. Segmentation of
Chinese, decompounding in German, morpholog-
ical analysis for Arabic — across a wide range
of source languages, ambiguity in the input gives
rise to multiple possibilities for the source word se-
quence. Nonetheless, state-of-the-art systems com-
monly identify a single analysis f during a prepro-
cessing step, and decode according to the decision
rule in (1).
In this paper, we go beyond speech translation
by showing that lattice decoding can also yield im-
provements for text by preserving alternative anal-
yses of the input. In addition, we generalize lattice
decoding algorithmically, extending it for the first
time to hierarchical phrase-based translation (Chi-
ang, 2005; Chiang, 2007).
Formally, the approach we take can be thought of
as a “noisier channel”, where an observed signal o
gives rise to a set of source-language strings f&apos; E
F(o) and we seek
</bodyText>
<equation confidence="0.996776">
e� = arg max max
e f�EF(o)
Pr(e, f&apos;|o) (2)
e� = arg maxPr(e|f) (1) = arg max max Pr(e)Pr(f&apos;|e, o) (3)
e e f�EF(o)
</equation>
<bodyText confidence="0.987486916666667">
An exception is the translation of speech recogni-
tion output, where the acoustic signal generally un-
derdetermines the choice of source word sequence
f. There, Bertoldi and others have recently found
that, rather than translating a single-best transcrip-
tion f, it is advantageous to allow the MT decoder to
= arg max max Pr(e)Pr(f&apos;|e)Pr(o|f&apos;)�(4)
e f�EF(o)
Following Och and Ney (2002), we use the maxi-
mum entropy framework (Berger et al., 1996) to di-
rectly model the posterior Pr(e, f&apos;|o) with parame-
ters tuned to minimize a loss function representing
</bodyText>
<page confidence="0.97091">
1012
</page>
<note confidence="0.714023">
Proceedings of ACL-08: HLT, pages 1012–1020,
</note>
<page confidence="0.531556">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.984344">
the quality only of the resulting translations. Thus,
we make use of the following general decision rule:
</bodyText>
<equation confidence="0.9873535">
e� = arg max max
e f�EF(o)
</equation>
<bodyText confidence="0.999935">
In principle, one could decode according to (2)
simply by enumerating and decoding each f� ∈
F(o); however, for any interestingly large F(o) this
will be impractical. We assume that for many in-
teresting cases of F(o), there will be identical sub-
strings that express the same content, and therefore
a lattice representation is appropriate.
In Section 2, we discuss decoding with this model
in general, and then show how two classes of trans-
lation models can easily be adapted for lattice trans-
lation; we achieve a unified treatment of finite-state
and hierarchical phrase-based models by treating
lattices as a subcase of weighted finite state au-
tomata (FSAs). In Section 3, we identify and solve
issues that arise with reordering in non-linear FSAs,
i.e. FSAs where every path does not pass through
every node. Section 4 presents two applications of
the noisier channel paradigm, demonstrating sub-
stantial performance gains in Arabic-English and
Chinese-English translation. In Section 5 we discuss
relevant prior work, and we conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.980622" genericHeader="introduction">
2 Decoding
</sectionHeader>
<bodyText confidence="0.9999003">
Most statistical machine translation systems model
translational equivalence using either finite state
transducers or synchronous context free grammars
(Lopez, to appear 2008). In this section we discuss
the issues associated with adapting decoders from
both classes of formalism to process word lattices.
The first decoder we present is a SCFG-based de-
coder similar to the one described in Chiang (2007).
The second is a phrase-based decoder implementing
the model of Koehn et al. (2003).
</bodyText>
<subsectionHeader confidence="0.993558">
2.1 Word lattices
</subsectionHeader>
<bodyText confidence="0.999492333333333">
A word lattice G = hV, Ei is a directed acyclic
graph that formally is a weighted finite state automa-
ton (FSA). We further stipulate that exactly one node
has no outgoing edges and is designated the ‘end
node’. Figure 1 illustrates three classes of word
lattices.
</bodyText>
<figureCaption confidence="0.998135">
Figure 1: Three examples of word lattices: (a) sentence,
(b) confusion network, and (c) non-linear word lattice.
</figureCaption>
<bodyText confidence="0.999885909090909">
A word lattice is useful for our purposes because
it permits any finite set of strings to be represented
and allows for substrings common to multiple mem-
bers of the set to be represented with a single piece
of structure. Additionally, all paths from one node to
another form an equivalence class representing, in
our model, alternative expressions of the same un-
derlying communicative intent.
For translation, we will find it useful to encode
G in a chart based on a topological ordering of the
nodes, as described by Cheppalier et al. (1999). The
nodes in the lattices shown in Figure 1 are labeled
according to an appropriate numbering.
The chart-representation of the graph is a triple of
2-dimensional matrices hF, p, Ri, which can be con-
structed from the numbered graph. Fi,j is the word
label of the jth transition leaving node i. The cor-
responding transition cost is pi,j. Ri,j is the node
number of the node on the right side of the jth tran-
sition leaving node i. Note that Ri,j &gt; i for all i, j.
Table 1 shows the word lattice from Figure 1 repre-
sented in matrix form as hF, p, Ri.
</bodyText>
<equation confidence="0.575048">
0 1 2
</equation>
<construct confidence="0.967625571428571">
a 1 1 b 1 2 c 1 3
a 31 1 b 1 2 c 21 3
x 1 1 d 2 3
E 3 1
x 21 1 y 1 2 b 21 3
a 1 2 c 1 3
2 2
</construct>
<tableCaption confidence="0.848005666666667">
Table 1: Topologically ordered chart encoding of the
three lattices in Figure 1. Each cell ij in this table is a
triple hFzj, pzj, Rzji
</tableCaption>
<figure confidence="0.897915263157895">
d
b
0 x 1 2 3
c
0
a
x
a
1
y
2
b
c
3
a b c
0 1 2 3
M
λ.φ.(e, f&apos;, o) (5)
.=1
</figure>
<page confidence="0.960416">
1013
</page>
<subsectionHeader confidence="0.999702">
2.2 Parsing word lattices
</subsectionHeader>
<bodyText confidence="0.9999786">
Chiang (2005) introduced hierarchical phrase-based
translation models, which are formally based
on synchronous context-free grammars (SCFGs).
Translation proceeds by parsing the input using the
source language side of the grammar, simultane-
ously building a tree on the target language side via
the target side of the synchronized rules. Since de-
coding is equivalent to parsing, we begin by present-
ing a parser for word lattices, which is a generaliza-
tion of a CKY parser for lattices given in Cheppalier
et al. (1999).
Following Goodman (1999), we present our lat-
tice parser as a deductive proof system in Figure 2.
The parser consists of two kinds of items, the first
with the form [X —* α • Q, i, j] representing rules
that have yet to be completed and span node i to
node j. The other items have the form [X, i, j] and
indicate that non-terminal X spans [i, j]. As with
sentence parsing, the goal is a deduction that covers
the spans of the entire input lattice [5, 0, |V  |− 1].
The three inference rules are: 1) match a terminal
symbol and move across one edge in the lattice 2)
move across an E-edge without advancing the dot in
an incomplete rule 3) advance the dot across a non-
terminal symbol given appropriate antecedents.
</bodyText>
<subsectionHeader confidence="0.999423">
2.3 From parsing to MT decoding
</subsectionHeader>
<bodyText confidence="0.999997666666667">
A target language model is necessary to generate flu-
ent output. To do so, the grammar is intersected with
an n-gram LM. To mitigate the effects of the combi-
natorial explosion of non-terminals the LM intersec-
tion entails, we use cube-pruning to only consider
the most promising expansions (Chiang, 2007).
</bodyText>
<subsectionHeader confidence="0.997008">
2.4 Lattice translation with FSTs
</subsectionHeader>
<bodyText confidence="0.999972894736842">
A second important class of translation models in-
cludes those based formally on FSTs. We present a
description of the decoding process for a word lattice
using a representative FST model, the phrase-based
translation model described in Koehn et al. (2003).
Phrase-based models translate a foreign sentence
f into the target language e by breaking up f into
a sequence of phrases f1, where each phrase fz can
contain one or more contiguous words and is trans-
lated into a target phrase ez of one or more contigu-
ous words. Each word in f must be translated ex-
actly once. To generalize this model to word lattices,
it is necessary to choose both a path through the lat-
tice and a partitioning of the sentence this induces
into a sequence of phrases f1. Although the number
of source phrases in a word lattice can be exponen-
tial in the number of nodes, enumerating the possible
translations of every span in a lattice is in practice
tractable, as described by Bertoldi et al. (2007).
</bodyText>
<subsectionHeader confidence="0.999792">
2.5 Decoding with phrase-based models
</subsectionHeader>
<bodyText confidence="0.999988611111111">
We adapted the Moses phrase-based decoder to
translate word lattices (Koehn et al., 2007). The
unmodified decoder builds a translation hypothesis
from left to right by selecting a range of untrans-
lated words and adding translations of this phrase to
the end of the hypothesis being extended. When no
untranslated words remain, the translation process is
complete.
The word lattice decoder works similarly, only
now the decoder keeps track not of the words that
have been covered, but of the nodes, given a topo-
logical ordering of the nodes. For example, assum-
ing the third lattice in Figure 1 is our input, if the
edge with word a is translated, this will cover two
untranslated nodes [0,1] in the coverage vector, even
though it is only a single word. As with sentence-
based decoding, a translation hypothesis is complete
when all nodes in the input lattice are covered.
</bodyText>
<subsectionHeader confidence="0.988934">
2.6 Non-monotonicity and unreachable nodes
</subsectionHeader>
<bodyText confidence="0.9999935">
The changes described thus far are straightfor-
ward adaptations of the underlying phrase-based
sentence decoder; however, dealing properly with
non-monotonic decoding of word lattices introduces
some minor complexity that is worth mentioning. In
the sentence decoder, any translation of any span of
untranslated words is an allowable extension of a
partial translation hypothesis, provided that the cov-
erage vectors of the extension and the partial hypoth-
esis do not intersect. In a non-linear word lattice,
a further constraint must be enforced ensuring that
there is always a path from the starting node of the
translation extension’s source to the node represent-
ing the nearest right edge of the already-translated
material, as well as a path from the ending node of
the translation extension’s source to future translated
spans. Figure 3 illustrates the problem. If [0,1] is
translated, the decoder must not consider translating
</bodyText>
<page confidence="0.770339">
1014
</page>
<equation confidence="0.968363090909091">
Axioms:
[X → •ry, i, i] : w (X &apos;−→ h,y, αi) ∈ G, i ∈ [0, |V  |− 2]
Inference rules:
[X → α • Fj,k,Q, i, j] : w
[X → αFj,k • ,Q, i, Rj,k] : w × pj,k
[X → α • ,Q, i, j] : w Fj,k = E
[X → α • ,Q, i, Rj,k] : w × pj,k
[Z → α • X,Q, i, k] : w1 [X → -y•, k, j] : w2
[Z → αX • ,Q, i, j] : w1 × w2
Goal state:
[S → 7•, 0, |V  |− 1]
</equation>
<figureCaption confidence="0.985657">
Figure 2: Word lattice parser for an unrestricted context free grammar G.
Figure 3: The span [0, 3] has one inconsistent covering,
[0,1] + [2, 3].
</figureCaption>
<bodyText confidence="0.999838857142857">
[2,3] as a possible extension of this hypothesis since
there is no path from node 1 to node 2 and therefore
the span [1,2] would never be covered. In the parser
that forms the basis of the hierarchical decoder de-
scribed in Section 2.3, no such restriction is neces-
sary since grammar rules are processed in a strictly
left-to-right fashion without any skips.
</bodyText>
<sectionHeader confidence="0.907915" genericHeader="method">
3 Distortion in a non-linear word lattice
</sectionHeader>
<bodyText confidence="0.999854428571429">
In both hierarchical and phrase-based models, the
distance between words in the source sentence is
used to limit where in the target sequence their trans-
lations will be generated. In phrase based transla-
tion, distortion is modeled explicitly. Models that
support non-monotonic decoding generally include
a distortion cost, such as |ai − bi−1 − 1 |where ai is
the starting position of the foreign phrase fi and bi−1
is the ending position of phrase fi−1 (Koehn et al.,
2003). The intuition behind this model is that since
most translation is monotonic, the cost of skipping
ahead or back in the source should be proportional
to the number of words that are skipped. Addition-
ally, a maximum distortion limit is used to restrict
</bodyText>
<figureCaption confidence="0.9943635">
Figure 4: Distance-based distortion problem. What is the
distance between node 4 to node 0?
</figureCaption>
<bodyText confidence="0.998916863636364">
the size of the search space.
In linear word lattices, such as confusion net-
works, the distance metric used for the distortion
penalty and for distortion limits is well defined;
however, in a non-linear word lattice, it poses the
problem illustrated in Figure 4. Assuming the left-
to-right decoding strategy described in the previous
section, if c is generated by the first target word, the
distortion penalty associated with “skipping ahead”
should be either 3 or 2, depending on what path is
chosen to translate the span [0,3]. In large lattices,
where a single arc may span many nodes, the possi-
ble distances may vary quite substantially depending
on what path is ultimately taken, and handling this
properly therefore crucial.
Although hierarchical phrase-based models do
not model distortion explicitly, Chiang (2007) sug-
gests using a span length limit to restrict the win-
dow in which reordering can take place.1 The de-
coder enforces the constraint that a synchronous rule
learned from the training data (the only mechanism
by which reordering can be introduced) can span
</bodyText>
<footnote confidence="0.994901">
1This is done to reduce the size of the search space and be-
cause hierarchical phrase-based translation models are inaccu-
rate models of long-distance distortion.
</footnote>
<figure confidence="0.997009">
0
a
2
1
x
3
a
0 1
x
b
2
y
3
c
4
</figure>
<page confidence="0.908953">
1015
</page>
<table confidence="0.9981662">
Distance metric MT05 MT06
Difference 0.2943 0.2786
Difference+LexRO 0.2974 0.2890
ShortestP 0.2993 0.2865
ShortestP+LexRO 0.3072 0.2992
</table>
<tableCaption confidence="0.990442">
Table 2: Effect of distance metric on phrase-based model
performance.
</tableCaption>
<bodyText confidence="0.9585586">
maximally A words in f. Like the distortion cost
used in phrase-based systems, A is also poorly de-
fined for non-linear lattices.
Since we want a distance metric that will restrict
as few local reorderings as possible on any path,
we use a function �(a, b) returning the length of the
shortest path between nodes a and b. Since this func-
tion is not dependent on the exact path chosen, it can
be computed in advance of decoding using an all-
pairs shortest path algorithm (Cormen et al., 1989).
</bodyText>
<subsectionHeader confidence="0.994018">
3.1 Experimental results
</subsectionHeader>
<bodyText confidence="0.99977948">
We tested the effect of the distance metric on trans-
lation quality using Chinese word segmentation lat-
tices (Section 4.1, below) using both a hierarchical
and phrase-based system modified to translate word
lattices. We compared the shortest-path distance
metric with a baseline which uses the difference in
node number as the distortion distance. For an ad-
ditional datapoint, we added a lexicalized reorder-
ing model that models the probability of each phrase
pair appearing in three different orientations (swap,
monotone, other) in the training corpus (Koehn et
al., 2005).
Table 2 summarizes the results of the phrase-
based systems. On both test sets, the shortest path
metric improved the BLEU scores. As expected,
the lexicalized reordering model improved transla-
tion quality over the baseline; however, the improve-
ment was more substantial in the model that used the
shortest-path distance metric (which was already a
higher baseline). Table 3 summarizes the results of
our experiment comparing the performance of two
distance metrics to determine whether a rule has ex-
ceeded the decoder’s span limit. The pattern is the
same, showing a clear increase in BLEU for the
shortest path metric over the baseline.
</bodyText>
<table confidence="0.997824333333333">
Distance metric MT05 MT06
Difference 0.3063 0.2957
ShortestP 0.3176 0.3043
</table>
<tableCaption confidence="0.975928">
Table 3: Effect of distance metric on hierarchical model
performance.
</tableCaption>
<sectionHeader confidence="0.978603" genericHeader="method">
4 Exploiting Source Language Alternatives
</sectionHeader>
<bodyText confidence="0.999911351351351">
Chinese word segmentation. A necessary first
step in translating Chinese using standard models
is segmenting the character stream into a sequence
of words. Word-lattice translation offers two possi-
ble improvements over the conventional approach.
First, a lattice may represent multiple alternative
segmentations of a sentence; input represented in
this way will be more robust to errors made by the
segmenter.2 Second, different segmentation granu-
larities may be more or less optimal for translating
different spans. By encoding alternatives in the in-
put in a word lattice, the decision as to which granu-
larity to use for a given span can be resolved during
decoding rather than when constructing the system.
Figure 5 illustrates a lattice based on three different
segmentations.
Arabic morphological variation. Arabic orthog-
raphy is problematic for lexical and phrase-based
MT approaches since a large class of functional el-
ements (prepositions, pronouns, tense markers, con-
junctions, definiteness markers) are attached to their
host stems. Thus, while the training data may pro-
vide good evidence for the translation of a partic-
ular stem by itself, the same stem may not be at-
tested when attached to a particular conjunction.
The general solution taken is to take the best pos-
sible morphological analysis of the text (it is of-
ten ambiguous whether a piece of a word is part
of the stem or merely a neighboring functional el-
ement), and then make a subset of the bound func-
tional elements in the language into freestanding to-
kens. Figure 6 illustrates the unsegmented Arabic
surface form as well as the morphological segmen-
tation variant we made use of. The limitation of this
approach is that as the amount and variety of train-
ing data increases, the optimal segmentation strat-
egy changes: more aggressive segmentation results
</bodyText>
<footnote confidence="0.5770385">
2The segmentation process is ambiguous, even for native
speakers of Chinese.
</footnote>
<page confidence="0.986724">
1016
</page>
<figure confidence="0.999131533333333">
1
质
合金
号
5
称
工
8
10
业 牙
齿
0
2
硬
硬质
合
3
金
4
号称
6
&amp;quot;
7
工业
9
牙齿
11
&amp;quot;
12
硬质合金
</figure>
<figureCaption confidence="0.999955">
Figure 5: Sample Chinese segmentation lattice using three segmentations.
</figureCaption>
<bodyText confidence="0.999835166666667">
in fewer OOV tokens, but automatic evaluation met-
rics indicate lower translation quality, presumably
because the smaller units are being translated less
idiomatically (Habash and Sadat, 2006). Lattices al-
low the decoder to make decisions about what gran-
ularity of segmentation to use subsententially.
</bodyText>
<subsectionHeader confidence="0.992862">
4.1 Chinese Word Segmentation Experiments
</subsectionHeader>
<bodyText confidence="0.998949586206897">
In our experiments we used two state-of-the-art Chi-
nese word segmenters: one developed at Harbin
Institute of Technology (Zhao et al., 2001), and
one developed at Stanford University (Tseng et al.,
2005). In addition, we used a character-based seg-
mentation. In the remaining of this paper, we use cs
for character segmentation, hs for Harbin segmenta-
tion and ss for Stanford segmentation. We built two
types of lattices: one that combines the Harbin and
Stanford segmenters (hs+ss), and one which uses
all three segmentations (hs+ss+cs).
Data and Settings. The systems used in these
experiments were trained on the NIST MT06 Eval
corpus without the UN data (approximatively 950K
sentences). The corpus was analyzed with the three
segmentation schemes. For the systems using word
lattices, the training data contained the versions of
the corpus appropriate for the segmentation schemes
used in the input. That is, for the hs+ss condition,
the training data consisted of two copies of the cor-
pus: one segmented with the Harbin segmenter and
the other with the Stanford segmenter.3 A trigram
English language model with modified Kneser-Ney
smoothing (Kneser and Ney, 1995) was trained on
the English side of our training data as well as por-
tions of the Gigaword v2 English Corpus, and was
used for all experiments. The NIST MT03 test set
was used as a development set for optimizing the in-
terpolation weights using minimum error rate train-
</bodyText>
<footnote confidence="0.540828">
3The corpora were word-aligned independently and then
concatenated for rule extraction.
</footnote>
<bodyText confidence="0.998648085714286">
ing (Och, 2003). The testing was done on the NIST
2005 and 2006 evaluation sets (MT05, MT06).
Experimental results: Word-lattices improve
translation quality. We used both a phrase-based
translation model, decoded using our modified ver-
sion of Moses (Koehn et al., 2007), and a hierarchi-
cal phrase-based translation model, using our modi-
fied version of Hiero (Chiang, 2005; Chiang, 2007).
These two translation model types illustrate the ap-
plicability of the theoretical contributions presented
in Section 2 and Section 3.
We observed that the coverage of named entities
(NEs) in our baseline systems was rather poor. Since
names in Chinese can be composed of relatively
long strings of characters that cannot be translated
individually, when generating the segmentation lat-
tices that included cs arcs, we avoided segmenting
NEs of type PERSON, as identified using a Chinese
NE tagger (Florian et al., 2004).
The results are summarized in Table 4. We see
that using word lattices improves BLEU scores both
in the phrase-based model and hierarchical model as
compared to the single-best segmentation approach.
All results using our word-lattice decoding for the
hierarchical models (hs+ss and hs+ss+cs) are sig-
nificantly better than the best segmentation (ss).4
For the phrase-based model, we obtain significant
gains using our word-lattice decoder using all three
segmentations on MT05. The other results, while
better than the best segmentation (hs) by at least
0.3 BLEU points, are not statistically significant.
Even if the results are not statistically significant
for MT06, there is a high decrease in OOV items
when using word-lattices. For example, for MT06
the number of OOVs in the hs translation is 484.
</bodyText>
<footnote confidence="0.9808375">
4Significance testing was carried out using the bootstrap re-
sampling technique advocated by Koehn (2004). Unless other-
wise noted, all reported improvements are signficant at at least
P &lt; 0.05.
</footnote>
<page confidence="0.99301">
1017
</page>
<note confidence="0.948249333333333">
surface wxlAl ftrp AlSyf kAn mEZm AlDjyj AlAElAmy m&amp;ydA llEmAd .
segmented w- xlAl ftrp Al- Syf kAn mEZm Al- Djyj Al- AElAmy m&amp;ydA l- Al- EmAd .
(English) During the summer period, most media buzz was supportive of the general .
</note>
<figureCaption confidence="0.999649">
Figure 6: Example of Arabic morphological segmentation.
</figureCaption>
<bodyText confidence="0.9987265">
The number of OOVs decreased by 19% for hs+ss
and by 75% for hs+ss+cs. As mentioned in Section
3, using lexical reordering for word-lattices further
improves the translation quality.
</bodyText>
<subsectionHeader confidence="0.99706">
4.2 Arabic Morphology Experiments
</subsectionHeader>
<bodyText confidence="0.998714769230769">
We created lattices from an unsegmented version of
the Arabic test data and generated alternative arcs
where clitics as well as the definiteness marker and
the future tense marker were segmented into tokens.
We used the Buckwalter morphological analyzer and
disambiguated the analysis using a simple unigram
model trained on the Penn Arabic Treebank.
Data and Settings. For these experiments we
made use of the entire NIST MT08 training data,
although for training of the system, we used a sub-
sampling method proposed by Kishore Papineni that
aims to include training sentences containing n-
grams in the test data (personal communication).
For all systems, we used a 5-gram English LM
trained on 250M words of English training data.
The NIST MT03 test set was used as development
set for optimizing the interpolation weights using
MER training (Och, 2003). Evaluation was car-
ried out on the NIST 2005 and 2006 evaluation sets
(MT05, MT06).
Experimental results: Word-lattices improve
translation quality. Results are presented in Table
5. Using word-lattices to combine the surface forms
with morphologically segmented forms significantly
improves BLEU scores both in the phrase-based and
hierarchical models.
</bodyText>
<sectionHeader confidence="0.997988" genericHeader="method">
5 Prior work
</sectionHeader>
<bodyText confidence="0.99982074">
Lattice Translation. The ‘noisier channel’ model
of machine translation has been widely used in spo-
ken language translation as an alternative to select-
ing the single-best hypothesis from an ASR system
and translating it (Ney, 1999; Casacuberta et al.,
2004; Zhang et al., 2005; Saleem et al., 2005; Ma-
tusov et al., 2005; Bertoldi et al., 2007; Mathias,
2007). Several authors (e.g. Saleem et al. (2005)
and Bertoldi et al. (2007)) comment directly on
the impracticality of using n-best lists to translate
speech.
Although translation is fundamentally a non-
monotonic relationship between most language
pairs, reordering has tended to be a secondary con-
cern to the researchers who have worked on lattice
translation. Matusov et al. (2005) decodes monoton-
ically and then uses a finite state reordering model
on the single-best translation, along the lines of
Bangalore and Riccardi (2000). Mathias (2007)
and Saleem et al. (2004) only report results of
monotonic decoding for the systems they describe.
Bertoldi et al. (2007) solve the problem by requiring
that their input be in the format of a confusion net-
work, which enables the standard distortion penalty
to be used. Finally, the system described by Zhang
et al. (2005) uses IBM Model 4 features to translate
lattices. For the distortion model, they use the maxi-
mum probability value over all possible paths in the
lattice for each jump considered, which is similar
to the approach we have taken. Mathias and Byrne
(2006) build a phrase-based translation system as a
cascaded series of FSTs which can accept any input
FSA; however, the only reordering that is permitted
is the swapping of two adjacent phrases.
Applications of source lattices outside of the do-
main of spoken language translation have been far
more limited. Costa-juss`a and Fonollosa (2007) take
steps in this direction by using lattices to encode
multiple reorderings of the source language. Dyer
(2007) uses confusion networks to encode mor-
phological alternatives in Czech-English translation,
and Xu et al. (2005) takes an approach very similar
to ours for Chinese-English translation and encodes
multiple word segmentations in a lattice, but which
is decoded with a conventionally trained translation
model and without a sophisticated reordering model.
The Arabic-English morphological segmentation
lattices are similar in spirit to backoff translation
models (Yang and Kirchhoff, 2006), which consider
alternative morphological segmentations and simpli-
</bodyText>
<page confidence="0.949827">
1018
</page>
<table confidence="0.99476">
(Source Type) MT05 MT06
BLEU BLEU
cs 0.2833 0.2694
hs 0.2905 0.2835
ss 0.2894 0.2801
hs+ss 0.2938 0.2870
hs+ss+cs 0.2993 0.2865
hs+ss+cs.lexRo 0.3072 0.2992
(Source Type) MT05 MT06
BLEU BLEU
cs 0.2904 0.2821
hs 0.3008 0.2907
ss 0.3071 0.2964
hs+ss 0.3132 0.3006
hs+ss+cs 0.3176 0.3043
(a) Phrase-based model (b) Hierarchical model
</table>
<tableCaption confidence="0.984195">
Table 4: Chinese Word Segmentation Results
</tableCaption>
<table confidence="0.990938">
(Source Type) MT05 MT06
BLEU BLEU
surface 0.4682 0.3512
morph 0.5087 0.3841
morph+surface 0.5225 0.4008
(Source Type) MT05 MT06
BLEU BLEU
surface 0.5253 0.3991
morph 0.5377 0.4180
morph+surface 0.5453 0.4287
(a) Phrase-based model (b) Hierarchical model
</table>
<tableCaption confidence="0.998485">
Table 5: Arabic Morphology Results
</tableCaption>
<bodyText confidence="0.97548225">
fications of a surface token when the surface token
can not be translated.
Parsing and formal language theory. There has
been considerable work on parsing word lattices,
much of it for language modeling applications in
speech recognition (Ney, 1991; Cheppalier and Raj-
man, 1998). Additionally, Grune and Jacobs (2008)
refines an algorithm originally due to Bar-Hillel for
intersecting an arbitrary FSA (of which word lattices
are a subset) with a CFG. Klein and Manning (2001)
formalize parsing as a hypergraph search problem
and derive an O(n3) parser for lattices.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999603846153846">
We have achieved substantial gains in translation
performance by decoding compact representations
of alternative source language analyses, rather than
single-best representations. Our results generalize
previous gains for lattice translation of spoken lan-
guage input, and we have further generalized the
approach by introducing an algorithm for lattice
decoding using a hierarchical phrase-based model.
Additionally, we have shown that although word
lattices complicate modeling of word reordering, a
simple heuristic offers good performance and en-
ables many standard distortion models to be used
directly with lattice input.
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999877625">
This research was supported by the GALE program
of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-2-0001. The authors wish
to thank Niyu Ge for the Chinese named-entity anal-
ysis, Pi-Chuan Chang for her assistance with the
Stanford Chinese segmenter, and Tie-Jun Zhao and
Congui Zhu for making the Harbin Chinese seg-
menter available to us.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9974275">
S. Bangalore and G. Riccardi. 2000. Finite state models
for lexical reordering in spoken language translation.
In Proc. Int. Conf. on Spoken Language Processing,
pages 422–425, Beijing, China.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural lan-
guage processing. Comput. Linguist., 22(1):39–71.
N. Bertoldi and M. Federico. 2005. A new decoder for
spoken language translation based on confusion net-
works. In Proceedings of the IEEE Automatic Speech
Recognition and Understanding Workshop.
N. Bertoldi, R. Zens, and M. Federico. 2007. Speech
translation by confusion network decoding. In Pro-
ceeding of ICASSP 2007, Honolulu, Hawaii, April.
P.F. Brown, J. Cocke, S. Della-Pietra, V.J. Della-Pietra,
F. Jelinek, J.D. Lafferty, R.L. Mercer, and P.S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16:79–85.
F. Casacuberta, H. Ney, F. J. Och, E. Vidal, J. M. Vilar,
S. Barrachina, I. Garcia-Varea, D. Llorens, C. Mar-
</reference>
<page confidence="0.947151">
1019
</page>
<reference confidence="0.999717398148149">
tinez, S. Molau, F. Nevado, M. Pastor, D. Pico, A. San-
chis, and C. Tillmann. 2004. Some approaches to
statistical and finite-state speech-to-speech translation.
Computer Speech &amp; Language, 18(1):25–47, January.
J. Cheppalier and M. Rajman. 1998. A generalized CYK
algorithm for parsing stochastic CFG. In Proceedings
of the Workshop on Tabulation in Parsing and Deduc-
tion (TAPD98), pages 133–137, Paris, France.
J. Cheppalier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Sixth Conference sur le Traitement Automatique du
Langage Naturel (TANL’99), pages 95–104.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of the 43rd
Annual Meeting of the Association for Computational
Linguistics (ACL’05), pages 263–270.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.
T.H. Cormen, C. E. Leiserson, and R. L. Rivest, 1989.
Introduction to Algorithms, pages 558–565. The MIT
Press and McGraw-Hill Book Company.
M. Costa-juss`a and J.A.R. Fonollosa. 2007. Analy-
sis of statistical and morphological classes to gener-
ate weighted reordering hypotheses on a statistical ma-
chine translation system. In Proc. of the Second Work-
shop on SMT, pages 171–176, Prague.
C. Dyer. 2007. Noisier channel translation: translation
from morphologically complex languages. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, Prague, June.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proc. of HLT-NAACL 2004, pages 1–8.
J. Goodman. 1999. Semiring parsing. Computational
Linguistics, 25:573–605.
D. Grune and C.J. H. Jacobs. 2008. Parsing as intersec-
tion. Parsing Techniques, pages 425–442.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL, New York.
D. Klein and C. D. Manning. 2001. Parsing with hyper-
graphs. In Proceedings of IWPT 2001.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of IEEE
Internation Conference on Acoustics, Speech, and Sig-
nal Processing, pages 181–184.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL
2003, pages 48–54.
P. Koehn, A. Axelrod, A. Birch Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Edinburgh
system description for the 2005 IWSLT speech trans-
lation evaluation. In Proc. of IWSLT 2005, Pittsburgh.
P. Koehn, H. Hoang, A. Birch Mayne, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Annual Meeting
of the Association for Computation Linguistics (ACL),
Demonstration Session, pages 177–180, Jun.
P. Koehn. 2004. Statistical significance tests for machine
translation evluation. In Proc. of the 2004 Conf. on
EMNLP, pages 388–395.
A. Lopez. to appear 2008. Statistical machine transla-
tion. ACM Computing Surveys.
L. Mathias and W. Byrne. 2006. Statistical phrase-
based speech translation. In IEEE Conf. on Acoustics,
Speech and Signal Processing.
L. Mathias. 2007. Statistical Machine Translation
and Automatic Speech Recognition under Uncertainty.
Ph.D. thesis, The Johns Hopkins University.
E. Matusov, S. Kanthak, and H. Ney. 2005. On the in-
tegration of speech recognition and statistical machine
translation. In Proceedings of Interspeech 2005.
H. Ney. 1991. Dynamic programming parsing for
context-free grammars in continuous speech recogni-
tion. IEEE Transactions on Signal Processing, 39(2).
H. Ney. 1999. Speech translation: Coupling of recogni-
tion and translation. In Proc. of ICASSP, pages 517–
520, Phoenix.
F. Och and H. Ney. 2002. Discriminitive training
and maximum entropy models for statistical machine
translation. In Proceedings of the 40th Annual Meet-
ing of the ACL, pages 295–302.
S. Saleem, S.-C. Jou, S. Vogel, and T. Schulz. 2005. Us-
ing word lattice information for a tighter coupling in
speech translation systems. In Proc. of ICSLP, Jeju
Island, Korea.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005. A conditional random field word seg-
menter. In Fourth SIGHAN Workshop on Chinese Lan-
guage Processing.
J. Xu, E. Matusov, R. Zens, and H. Ney. 2005. Inte-
grated Chinese word segmentation in statistical ma-
chine translation. In Proc. of IWSLT 2005, Pittsburgh.
M. Yang and K. Kirchhoff. 2006. Phrase-based back-
off models for machine translation of highly inflected
languages. In Proceedings of the EACL 2006, pages
41–48.
R. Zhang, G. Kikui, H. Yamamoto, and W. Lo. 2005.
A decoding algorithm for word lattice translation in
speech translation. In Proceedings of the 2005 Inter-
national Workshop on Spoken Language Translation.
T. Zhao, L. Yajuan, Y. Muyun, and Y. Hao. 2001. In-
creasing accuracy of chinese segmentation with strat-
egy of multi-step processing. In J Chinese Information
Processing (Chinese Version), volume 1, pages 13–18.
</reference>
<page confidence="0.990656">
1020
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.573182">
<title confidence="0.999968">Generalizing Word Lattice Translation</title>
<author confidence="0.998997">Smaranda Muresan</author>
<author confidence="0.998997">Philip</author>
<affiliation confidence="0.996947">Laboratory for Computational Linguistics and Information Processing Institute for Advanced Computer Studies of Linguistics University of Maryland</affiliation>
<address confidence="0.999983">College Park, MD 20742, USA</address>
<email confidence="0.999291">redpony,smara,resnikATumd.edu</email>
<abstract confidence="0.969314071428571">Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well. We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for Chinese- English and Arabic-English translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>G Riccardi</author>
</authors>
<title>Finite state models for lexical reordering in spoken language translation.</title>
<date>2000</date>
<booktitle>In Proc. Int. Conf. on Spoken Language Processing,</booktitle>
<pages>422--425</pages>
<location>Beijing, China.</location>
<contexts>
<context position="25034" citStr="Bangalore and Riccardi (2000)" startWordPosition="4176" endWordPosition="4179">; Zhang et al., 2005; Saleem et al., 2005; Matusov et al., 2005; Bertoldi et al., 2007; Mathias, 2007). Several authors (e.g. Saleem et al. (2005) and Bertoldi et al. (2007)) comment directly on the impracticality of using n-best lists to translate speech. Although translation is fundamentally a nonmonotonic relationship between most language pairs, reordering has tended to be a secondary concern to the researchers who have worked on lattice translation. Matusov et al. (2005) decodes monotonically and then uses a finite state reordering model on the single-best translation, along the lines of Bangalore and Riccardi (2000). Mathias (2007) and Saleem et al. (2004) only report results of monotonic decoding for the systems they describe. Bertoldi et al. (2007) solve the problem by requiring that their input be in the format of a confusion network, which enables the standard distortion penalty to be used. Finally, the system described by Zhang et al. (2005) uses IBM Model 4 features to translate lattices. For the distortion model, they use the maximum probability value over all possible paths in the lattice for each jump considered, which is similar to the approach we have taken. Mathias and Byrne (2006) build a ph</context>
</contexts>
<marker>Bangalore, Riccardi, 2000</marker>
<rawString>S. Bangalore and G. Riccardi. 2000. Finite state models for lexical reordering in spoken language translation. In Proc. Int. Conf. on Spoken Language Processing, pages 422–425, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Comput. Linguist.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="3293" citStr="Berger et al., 1996" startWordPosition="510" endWordPosition="513">ise to a set of source-language strings f&apos; E F(o) and we seek e� = arg max max e f�EF(o) Pr(e, f&apos;|o) (2) e� = arg maxPr(e|f) (1) = arg max max Pr(e)Pr(f&apos;|e, o) (3) e e f�EF(o) An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f. There, Bertoldi and others have recently found that, rather than translating a single-best transcription f, it is advantageous to allow the MT decoder to = arg max max Pr(e)Pr(f&apos;|e)Pr(o|f&apos;)�(4) e f�EF(o) Following Och and Ney (2002), we use the maximum entropy framework (Berger et al., 1996) to directly model the posterior Pr(e, f&apos;|o) with parameters tuned to minimize a loss function representing 1012 Proceedings of ACL-08: HLT, pages 1012–1020, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics the quality only of the resulting translations. Thus, we make use of the following general decision rule: e� = arg max max e f�EF(o) In principle, one could decode according to (2) simply by enumerating and decoding each f� ∈ F(o); however, for any interestingly large F(o) this will be impractical. We assume that for many interesting cases of F(o), there will</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Comput. Linguist., 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bertoldi</author>
<author>M Federico</author>
</authors>
<title>A new decoder for spoken language translation based on confusion networks.</title>
<date>2005</date>
<booktitle>In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop.</booktitle>
<contexts>
<context position="1662" citStr="Bertoldi and Federico, 2005" startWordPosition="239" endWordPosition="242"> in the late 1940s – was that translation could be viewed as an instance of noisy channel modeling (Brown et al., 1990). They introduced a now standard decomposition that distinguishes modeling sentences in the target language (language models) from modeling the relationship between source and target language (translation models). Today, virtually all statistical translation systems seek the best hypothesis e for a given input f in the source language, according to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (Bertoldi et al., 2007; Bertoldi and Federico, 2005; Koehn et al., 2007). Why, however, should this advantage be limited to translation from spoken input? Even for text, there are often multiple ways to derive a sequence of words from the input string. Segmentation of Chinese, decompounding in German, morphological analysis for Arabic — across a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1). In this paper, we go beyon</context>
</contexts>
<marker>Bertoldi, Federico, 2005</marker>
<rawString>N. Bertoldi and M. Federico. 2005. A new decoder for spoken language translation based on confusion networks. In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bertoldi</author>
<author>R Zens</author>
<author>M Federico</author>
</authors>
<title>Speech translation by confusion network decoding.</title>
<date>2007</date>
<booktitle>In Proceeding of ICASSP 2007,</booktitle>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1633" citStr="Bertoldi et al., 2007" startWordPosition="235" endWordPosition="238">arkening back to Weaver in the late 1940s – was that translation could be viewed as an instance of noisy channel modeling (Brown et al., 1990). They introduced a now standard decomposition that distinguishes modeling sentences in the target language (language models) from modeling the relationship between source and target language (translation models). Today, virtually all statistical translation systems seek the best hypothesis e for a given input f in the source language, according to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (Bertoldi et al., 2007; Bertoldi and Federico, 2005; Koehn et al., 2007). Why, however, should this advantage be limited to translation from spoken input? Even for text, there are often multiple ways to derive a sequence of words from the input string. Segmentation of Chinese, decompounding in German, morphological analysis for Arabic — across a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1</context>
<context position="9619" citStr="Bertoldi et al. (2007)" startWordPosition="1637" endWordPosition="1640">to a sequence of phrases f1, where each phrase fz can contain one or more contiguous words and is translated into a target phrase ez of one or more contiguous words. Each word in f must be translated exactly once. To generalize this model to word lattices, it is necessary to choose both a path through the lattice and a partitioning of the sentence this induces into a sequence of phrases f1. Although the number of source phrases in a word lattice can be exponential in the number of nodes, enumerating the possible translations of every span in a lattice is in practice tractable, as described by Bertoldi et al. (2007). 2.5 Decoding with phrase-based models We adapted the Moses phrase-based decoder to translate word lattices (Koehn et al., 2007). The unmodified decoder builds a translation hypothesis from left to right by selecting a range of untranslated words and adding translations of this phrase to the end of the hypothesis being extended. When no untranslated words remain, the translation process is complete. The word lattice decoder works similarly, only now the decoder keeps track not of the words that have been covered, but of the nodes, given a topological ordering of the nodes. For example, assumi</context>
<context position="24491" citStr="Bertoldi et al., 2007" startWordPosition="4093" endWordPosition="4096">rimental results: Word-lattices improve translation quality. Results are presented in Table 5. Using word-lattices to combine the surface forms with morphologically segmented forms significantly improves BLEU scores both in the phrase-based and hierarchical models. 5 Prior work Lattice Translation. The ‘noisier channel’ model of machine translation has been widely used in spoken language translation as an alternative to selecting the single-best hypothesis from an ASR system and translating it (Ney, 1999; Casacuberta et al., 2004; Zhang et al., 2005; Saleem et al., 2005; Matusov et al., 2005; Bertoldi et al., 2007; Mathias, 2007). Several authors (e.g. Saleem et al. (2005) and Bertoldi et al. (2007)) comment directly on the impracticality of using n-best lists to translate speech. Although translation is fundamentally a nonmonotonic relationship between most language pairs, reordering has tended to be a secondary concern to the researchers who have worked on lattice translation. Matusov et al. (2005) decodes monotonically and then uses a finite state reordering model on the single-best translation, along the lines of Bangalore and Riccardi (2000). Mathias (2007) and Saleem et al. (2004) only report res</context>
</contexts>
<marker>Bertoldi, Zens, Federico, 2007</marker>
<rawString>N. Bertoldi, R. Zens, and M. Federico. 2007. Speech translation by confusion network decoding. In Proceeding of ICASSP 2007, Honolulu, Hawaii, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S Della-Pietra</author>
<author>V J Della-Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--79</pages>
<contexts>
<context position="1154" citStr="Brown et al., 1990" startWordPosition="161" endWordPosition="164">te state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for ChineseEnglish and Arabic-English translation. 1 Introduction When Brown and colleagues introduced statistical machine translation in the early 1990s, their key insight – harkening back to Weaver in the late 1940s – was that translation could be viewed as an instance of noisy channel modeling (Brown et al., 1990). They introduced a now standard decomposition that distinguishes modeling sentences in the target language (language models) from modeling the relationship between source and target language (translation models). Today, virtually all statistical translation systems seek the best hypothesis e for a given input f in the source language, according to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (Bertoldi et al., 2007; Bertoldi and Federico, 2005; Koehn et al., 2007). Why, however, should this advantage be limited to translation from sp</context>
</contexts>
<marker>Brown, Cocke, Della-Pietra, Della-Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>P.F. Brown, J. Cocke, S. Della-Pietra, V.J. Della-Pietra, F. Jelinek, J.D. Lafferty, R.L. Mercer, and P.S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16:79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Casacuberta</author>
<author>H Ney</author>
<author>F J Och</author>
<author>E Vidal</author>
<author>J M Vilar</author>
<author>S Barrachina</author>
<author>I Garcia-Varea</author>
<author>D Llorens</author>
<author>C Martinez</author>
<author>S Molau</author>
<author>F Nevado</author>
<author>M Pastor</author>
<author>D Pico</author>
<author>A Sanchis</author>
<author>C Tillmann</author>
</authors>
<title>Some approaches to statistical and finite-state speech-to-speech translation.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="24405" citStr="Casacuberta et al., 2004" startWordPosition="4076" endWordPosition="4079">. Evaluation was carried out on the NIST 2005 and 2006 evaluation sets (MT05, MT06). Experimental results: Word-lattices improve translation quality. Results are presented in Table 5. Using word-lattices to combine the surface forms with morphologically segmented forms significantly improves BLEU scores both in the phrase-based and hierarchical models. 5 Prior work Lattice Translation. The ‘noisier channel’ model of machine translation has been widely used in spoken language translation as an alternative to selecting the single-best hypothesis from an ASR system and translating it (Ney, 1999; Casacuberta et al., 2004; Zhang et al., 2005; Saleem et al., 2005; Matusov et al., 2005; Bertoldi et al., 2007; Mathias, 2007). Several authors (e.g. Saleem et al. (2005) and Bertoldi et al. (2007)) comment directly on the impracticality of using n-best lists to translate speech. Although translation is fundamentally a nonmonotonic relationship between most language pairs, reordering has tended to be a secondary concern to the researchers who have worked on lattice translation. Matusov et al. (2005) decodes monotonically and then uses a finite state reordering model on the single-best translation, along the lines of </context>
</contexts>
<marker>Casacuberta, Ney, Och, Vidal, Vilar, Barrachina, Garcia-Varea, Llorens, Martinez, Molau, Nevado, Pastor, Pico, Sanchis, Tillmann, 2004</marker>
<rawString>F. Casacuberta, H. Ney, F. J. Och, E. Vidal, J. M. Vilar, S. Barrachina, I. Garcia-Varea, D. Llorens, C. Martinez, S. Molau, F. Nevado, M. Pastor, D. Pico, A. Sanchis, and C. Tillmann. 2004. Some approaches to statistical and finite-state speech-to-speech translation. Computer Speech &amp; Language, 18(1):25–47, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cheppalier</author>
<author>M Rajman</author>
</authors>
<title>A generalized CYK algorithm for parsing stochastic CFG.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Tabulation in Parsing and Deduction (TAPD98),</booktitle>
<pages>133--137</pages>
<location>Paris, France.</location>
<contexts>
<context position="27572" citStr="Cheppalier and Rajman, 1998" startWordPosition="4563" endWordPosition="4567">ierarchical model Table 4: Chinese Word Segmentation Results (Source Type) MT05 MT06 BLEU BLEU surface 0.4682 0.3512 morph 0.5087 0.3841 morph+surface 0.5225 0.4008 (Source Type) MT05 MT06 BLEU BLEU surface 0.5253 0.3991 morph 0.5377 0.4180 morph+surface 0.5453 0.4287 (a) Phrase-based model (b) Hierarchical model Table 5: Arabic Morphology Results fications of a surface token when the surface token can not be translated. Parsing and formal language theory. There has been considerable work on parsing word lattices, much of it for language modeling applications in speech recognition (Ney, 1991; Cheppalier and Rajman, 1998). Additionally, Grune and Jacobs (2008) refines an algorithm originally due to Bar-Hillel for intersecting an arbitrary FSA (of which word lattices are a subset) with a CFG. Klein and Manning (2001) formalize parsing as a hypergraph search problem and derive an O(n3) parser for lattices. 6 Conclusions We have achieved substantial gains in translation performance by decoding compact representations of alternative source language analyses, rather than single-best representations. Our results generalize previous gains for lattice translation of spoken language input, and we have further generaliz</context>
</contexts>
<marker>Cheppalier, Rajman, 1998</marker>
<rawString>J. Cheppalier and M. Rajman. 1998. A generalized CYK algorithm for parsing stochastic CFG. In Proceedings of the Workshop on Tabulation in Parsing and Deduction (TAPD98), pages 133–137, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cheppalier</author>
<author>M Rajman</author>
<author>R Aragues</author>
<author>A Rozenknop</author>
</authors>
<title>Lattice parsing for speech recognition.</title>
<date>1999</date>
<booktitle>In Sixth Conference sur le Traitement Automatique du Langage Naturel (TANL’99),</booktitle>
<pages>95--104</pages>
<contexts>
<context position="6153" citStr="Cheppalier et al. (1999)" startWordPosition="973" endWordPosition="976">d lattices: (a) sentence, (b) confusion network, and (c) non-linear word lattice. A word lattice is useful for our purposes because it permits any finite set of strings to be represented and allows for substrings common to multiple members of the set to be represented with a single piece of structure. Additionally, all paths from one node to another form an equivalence class representing, in our model, alternative expressions of the same underlying communicative intent. For translation, we will find it useful to encode G in a chart based on a topological ordering of the nodes, as described by Cheppalier et al. (1999). The nodes in the lattices shown in Figure 1 are labeled according to an appropriate numbering. The chart-representation of the graph is a triple of 2-dimensional matrices hF, p, Ri, which can be constructed from the numbered graph. Fi,j is the word label of the jth transition leaving node i. The corresponding transition cost is pi,j. Ri,j is the node number of the node on the right side of the jth transition leaving node i. Note that Ri,j &gt; i for all i, j. Table 1 shows the word lattice from Figure 1 represented in matrix form as hF, p, Ri. 0 1 2 a 1 1 b 1 2 c 1 3 a 31 1 b 1 2 c 21 3 x 1 1 d</context>
<context position="7559" citStr="Cheppalier et al. (1999)" startWordPosition="1268" endWordPosition="1271"> d b 0 x 1 2 3 c 0 a x a 1 y 2 b c 3 a b c 0 1 2 3 M λ.φ.(e, f&apos;, o) (5) .=1 1013 2.2 Parsing word lattices Chiang (2005) introduced hierarchical phrase-based translation models, which are formally based on synchronous context-free grammars (SCFGs). Translation proceeds by parsing the input using the source language side of the grammar, simultaneously building a tree on the target language side via the target side of the synchronized rules. Since decoding is equivalent to parsing, we begin by presenting a parser for word lattices, which is a generalization of a CKY parser for lattices given in Cheppalier et al. (1999). Following Goodman (1999), we present our lattice parser as a deductive proof system in Figure 2. The parser consists of two kinds of items, the first with the form [X —* α • Q, i, j] representing rules that have yet to be completed and span node i to node j. The other items have the form [X, i, j] and indicate that non-terminal X spans [i, j]. As with sentence parsing, the goal is a deduction that covers the spans of the entire input lattice [5, 0, |V |− 1]. The three inference rules are: 1) match a terminal symbol and move across one edge in the lattice 2) move across an E-edge without adva</context>
</contexts>
<marker>Cheppalier, Rajman, Aragues, Rozenknop, 1999</marker>
<rawString>J. Cheppalier, M. Rajman, R. Aragues, and A. Rozenknop. 1999. Lattice parsing for speech recognition. In Sixth Conference sur le Traitement Automatique du Langage Naturel (TANL’99), pages 95–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>263--270</pages>
<contexts>
<context position="2549" citStr="Chiang, 2005" startWordPosition="380" endWordPosition="382">for Arabic — across a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1). In this paper, we go beyond speech translation by showing that lattice decoding can also yield improvements for text by preserving alternative analyses of the input. In addition, we generalize lattice decoding algorithmically, extending it for the first time to hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Formally, the approach we take can be thought of as a “noisier channel”, where an observed signal o gives rise to a set of source-language strings f&apos; E F(o) and we seek e� = arg max max e f�EF(o) Pr(e, f&apos;|o) (2) e� = arg maxPr(e|f) (1) = arg max max Pr(e)Pr(f&apos;|e, o) (3) e e f�EF(o) An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f. There, Bertoldi and others have recently found that, rather than translating a single-best transcription f, it is advantageous to allow the MT decod</context>
<context position="7055" citStr="Chiang (2005)" startWordPosition="1190" endWordPosition="1191">de i. The corresponding transition cost is pi,j. Ri,j is the node number of the node on the right side of the jth transition leaving node i. Note that Ri,j &gt; i for all i, j. Table 1 shows the word lattice from Figure 1 represented in matrix form as hF, p, Ri. 0 1 2 a 1 1 b 1 2 c 1 3 a 31 1 b 1 2 c 21 3 x 1 1 d 2 3 E 3 1 x 21 1 y 1 2 b 21 3 a 1 2 c 1 3 2 2 Table 1: Topologically ordered chart encoding of the three lattices in Figure 1. Each cell ij in this table is a triple hFzj, pzj, Rzji d b 0 x 1 2 3 c 0 a x a 1 y 2 b c 3 a b c 0 1 2 3 M λ.φ.(e, f&apos;, o) (5) .=1 1013 2.2 Parsing word lattices Chiang (2005) introduced hierarchical phrase-based translation models, which are formally based on synchronous context-free grammars (SCFGs). Translation proceeds by parsing the input using the source language side of the grammar, simultaneously building a tree on the target language side via the target side of the synchronized rules. Since decoding is equivalent to parsing, we begin by presenting a parser for word lattices, which is a generalization of a CKY parser for lattices given in Cheppalier et al. (1999). Following Goodman (1999), we present our lattice parser as a deductive proof system in Figure </context>
<context position="20885" citStr="Chiang, 2005" startWordPosition="3525" endWordPosition="3526">s used for all experiments. The NIST MT03 test set was used as a development set for optimizing the interpolation weights using minimum error rate train3The corpora were word-aligned independently and then concatenated for rule extraction. ing (Och, 2003). The testing was done on the NIST 2005 and 2006 evaluation sets (MT05, MT06). Experimental results: Word-lattices improve translation quality. We used both a phrase-based translation model, decoded using our modified version of Moses (Koehn et al., 2007), and a hierarchical phrase-based translation model, using our modified version of Hiero (Chiang, 2005; Chiang, 2007). These two translation model types illustrate the applicability of the theoretical contributions presented in Section 2 and Section 3. We observed that the coverage of named entities (NEs) in our baseline systems was rather poor. Since names in Chinese can be composed of relatively long strings of characters that cannot be translated individually, when generating the segmentation lattices that included cs arcs, we avoided segmenting NEs of type PERSON, as identified using a Chinese NE tagger (Florian et al., 2004). The results are summarized in Table 4. We see that using word l</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2564" citStr="Chiang, 2007" startWordPosition="383" endWordPosition="384">cross a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1). In this paper, we go beyond speech translation by showing that lattice decoding can also yield improvements for text by preserving alternative analyses of the input. In addition, we generalize lattice decoding algorithmically, extending it for the first time to hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Formally, the approach we take can be thought of as a “noisier channel”, where an observed signal o gives rise to a set of source-language strings f&apos; E F(o) and we seek e� = arg max max e f�EF(o) Pr(e, f&apos;|o) (2) e� = arg maxPr(e|f) (1) = arg max max Pr(e)Pr(f&apos;|e, o) (3) e e f�EF(o) An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f. There, Bertoldi and others have recently found that, rather than translating a single-best transcription f, it is advantageous to allow the MT decoder to = arg max</context>
<context position="5130" citStr="Chiang (2007)" startWordPosition="801" endWordPosition="802">e noisier channel paradigm, demonstrating substantial performance gains in Arabic-English and Chinese-English translation. In Section 5 we discuss relevant prior work, and we conclude in Section 6. 2 Decoding Most statistical machine translation systems model translational equivalence using either finite state transducers or synchronous context free grammars (Lopez, to appear 2008). In this section we discuss the issues associated with adapting decoders from both classes of formalism to process word lattices. The first decoder we present is a SCFG-based decoder similar to the one described in Chiang (2007). The second is a phrase-based decoder implementing the model of Koehn et al. (2003). 2.1 Word lattices A word lattice G = hV, Ei is a directed acyclic graph that formally is a weighted finite state automaton (FSA). We further stipulate that exactly one node has no outgoing edges and is designated the ‘end node’. Figure 1 illustrates three classes of word lattices. Figure 1: Three examples of word lattices: (a) sentence, (b) confusion network, and (c) non-linear word lattice. A word lattice is useful for our purposes because it permits any finite set of strings to be represented and allows for</context>
<context position="8607" citStr="Chiang, 2007" startWordPosition="1461" endWordPosition="1462">nput lattice [5, 0, |V |− 1]. The three inference rules are: 1) match a terminal symbol and move across one edge in the lattice 2) move across an E-edge without advancing the dot in an incomplete rule 3) advance the dot across a nonterminal symbol given appropriate antecedents. 2.3 From parsing to MT decoding A target language model is necessary to generate fluent output. To do so, the grammar is intersected with an n-gram LM. To mitigate the effects of the combinatorial explosion of non-terminals the LM intersection entails, we use cube-pruning to only consider the most promising expansions (Chiang, 2007). 2.4 Lattice translation with FSTs A second important class of translation models includes those based formally on FSTs. We present a description of the decoding process for a word lattice using a representative FST model, the phrase-based translation model described in Koehn et al. (2003). Phrase-based models translate a foreign sentence f into the target language e by breaking up f into a sequence of phrases f1, where each phrase fz can contain one or more contiguous words and is translated into a target phrase ez of one or more contiguous words. Each word in f must be translated exactly on</context>
<context position="14017" citStr="Chiang (2007)" startWordPosition="2406" endWordPosition="2407">lattice, it poses the problem illustrated in Figure 4. Assuming the leftto-right decoding strategy described in the previous section, if c is generated by the first target word, the distortion penalty associated with “skipping ahead” should be either 3 or 2, depending on what path is chosen to translate the span [0,3]. In large lattices, where a single arc may span many nodes, the possible distances may vary quite substantially depending on what path is ultimately taken, and handling this properly therefore crucial. Although hierarchical phrase-based models do not model distortion explicitly, Chiang (2007) suggests using a span length limit to restrict the window in which reordering can take place.1 The decoder enforces the constraint that a synchronous rule learned from the training data (the only mechanism by which reordering can be introduced) can span 1This is done to reduce the size of the search space and because hierarchical phrase-based translation models are inaccurate models of long-distance distortion. 0 a 2 1 x 3 a 0 1 x b 2 y 3 c 4 1015 Distance metric MT05 MT06 Difference 0.2943 0.2786 Difference+LexRO 0.2974 0.2890 ShortestP 0.2993 0.2865 ShortestP+LexRO 0.3072 0.2992 Table 2: Ef</context>
<context position="20900" citStr="Chiang, 2007" startWordPosition="3527" endWordPosition="3528"> experiments. The NIST MT03 test set was used as a development set for optimizing the interpolation weights using minimum error rate train3The corpora were word-aligned independently and then concatenated for rule extraction. ing (Och, 2003). The testing was done on the NIST 2005 and 2006 evaluation sets (MT05, MT06). Experimental results: Word-lattices improve translation quality. We used both a phrase-based translation model, decoded using our modified version of Moses (Koehn et al., 2007), and a hierarchical phrase-based translation model, using our modified version of Hiero (Chiang, 2005; Chiang, 2007). These two translation model types illustrate the applicability of the theoretical contributions presented in Section 2 and Section 3. We observed that the coverage of named entities (NEs) in our baseline systems was rather poor. Since names in Chinese can be composed of relatively long strings of characters that cannot be translated individually, when generating the segmentation lattices that included cs arcs, we avoided segmenting NEs of type PERSON, as identified using a Chinese NE tagger (Florian et al., 2004). The results are summarized in Table 4. We see that using word lattices improve</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Cormen</author>
<author>C E Leiserson</author>
<author>R L Rivest</author>
</authors>
<title>Introduction to Algorithms,</title>
<date>1989</date>
<pages>558--565</pages>
<publisher>The MIT Press and McGraw-Hill Book Company.</publisher>
<contexts>
<context position="15165" citStr="Cormen et al., 1989" startWordPosition="2605" endWordPosition="2608">0.2890 ShortestP 0.2993 0.2865 ShortestP+LexRO 0.3072 0.2992 Table 2: Effect of distance metric on phrase-based model performance. maximally A words in f. Like the distortion cost used in phrase-based systems, A is also poorly defined for non-linear lattices. Since we want a distance metric that will restrict as few local reorderings as possible on any path, we use a function �(a, b) returning the length of the shortest path between nodes a and b. Since this function is not dependent on the exact path chosen, it can be computed in advance of decoding using an allpairs shortest path algorithm (Cormen et al., 1989). 3.1 Experimental results We tested the effect of the distance metric on translation quality using Chinese word segmentation lattices (Section 4.1, below) using both a hierarchical and phrase-based system modified to translate word lattices. We compared the shortest-path distance metric with a baseline which uses the difference in node number as the distortion distance. For an additional datapoint, we added a lexicalized reordering model that models the probability of each phrase pair appearing in three different orientations (swap, monotone, other) in the training corpus (Koehn et al., 2005)</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1989</marker>
<rawString>T.H. Cormen, C. E. Leiserson, and R. L. Rivest, 1989. Introduction to Algorithms, pages 558–565. The MIT Press and McGraw-Hill Book Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Costa-juss`a</author>
<author>J A R Fonollosa</author>
</authors>
<title>Analysis of statistical and morphological classes to generate weighted reordering hypotheses on a statistical machine translation system.</title>
<date>2007</date>
<booktitle>In Proc. of the Second Workshop on SMT,</booktitle>
<pages>171--176</pages>
<location>Prague.</location>
<marker>Costa-juss`a, Fonollosa, 2007</marker>
<rawString>M. Costa-juss`a and J.A.R. Fonollosa. 2007. Analysis of statistical and morphological classes to generate weighted reordering hypotheses on a statistical machine translation system. In Proc. of the Second Workshop on SMT, pages 171–176, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
</authors>
<title>Noisier channel translation: translation from morphologically complex languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<location>Prague,</location>
<contexts>
<context position="26073" citStr="Dyer (2007)" startWordPosition="4349" endWordPosition="4350">ximum probability value over all possible paths in the lattice for each jump considered, which is similar to the approach we have taken. Mathias and Byrne (2006) build a phrase-based translation system as a cascaded series of FSTs which can accept any input FSA; however, the only reordering that is permitted is the swapping of two adjacent phrases. Applications of source lattices outside of the domain of spoken language translation have been far more limited. Costa-juss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language. Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to ours for Chinese-English translation and encodes multiple word segmentations in a lattice, but which is decoded with a conventionally trained translation model and without a sophisticated reordering model. The Arabic-English morphological segmentation lattices are similar in spirit to backoff translation models (Yang and Kirchhoff, 2006), which consider alternative morphological segmentations and simpli1018 (Source Type) MT05 MT06 BLEU BLEU cs 0.283</context>
</contexts>
<marker>Dyer, 2007</marker>
<rawString>C. Dyer. 2007. Noisier channel translation: translation from morphologically complex languages. In Proceedings of the Second Workshop on Statistical Machine Translation, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Hassan</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>X Luo</author>
<author>N Nicolov</author>
<author>S Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<pages>1--8</pages>
<contexts>
<context position="21420" citStr="Florian et al., 2004" startWordPosition="3607" endWordPosition="3610">ical phrase-based translation model, using our modified version of Hiero (Chiang, 2005; Chiang, 2007). These two translation model types illustrate the applicability of the theoretical contributions presented in Section 2 and Section 3. We observed that the coverage of named entities (NEs) in our baseline systems was rather poor. Since names in Chinese can be composed of relatively long strings of characters that cannot be translated individually, when generating the segmentation lattices that included cs arcs, we avoided segmenting NEs of type PERSON, as identified using a Chinese NE tagger (Florian et al., 2004). The results are summarized in Table 4. We see that using word lattices improves BLEU scores both in the phrase-based model and hierarchical model as compared to the single-best segmentation approach. All results using our word-lattice decoding for the hierarchical models (hs+ss and hs+ss+cs) are significantly better than the best segmentation (ss).4 For the phrase-based model, we obtain significant gains using our word-lattice decoder using all three segmentations on MT05. The other results, while better than the best segmentation (hs) by at least 0.3 BLEU points, are not statistically signi</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N Nicolov, and S Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Proc. of HLT-NAACL 2004, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--573</pages>
<contexts>
<context position="7585" citStr="Goodman (1999)" startWordPosition="1273" endWordPosition="1274"> a b c 0 1 2 3 M λ.φ.(e, f&apos;, o) (5) .=1 1013 2.2 Parsing word lattices Chiang (2005) introduced hierarchical phrase-based translation models, which are formally based on synchronous context-free grammars (SCFGs). Translation proceeds by parsing the input using the source language side of the grammar, simultaneously building a tree on the target language side via the target side of the synchronized rules. Since decoding is equivalent to parsing, we begin by presenting a parser for word lattices, which is a generalization of a CKY parser for lattices given in Cheppalier et al. (1999). Following Goodman (1999), we present our lattice parser as a deductive proof system in Figure 2. The parser consists of two kinds of items, the first with the form [X —* α • Q, i, j] representing rules that have yet to be completed and span node i to node j. The other items have the form [X, i, j] and indicate that non-terminal X spans [i, j]. As with sentence parsing, the goal is a deduction that covers the spans of the entire input lattice [5, 0, |V |− 1]. The three inference rules are: 1) match a terminal symbol and move across one edge in the lattice 2) move across an E-edge without advancing the dot in an incomp</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>J. Goodman. 1999. Semiring parsing. Computational Linguistics, 25:573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Grune</author>
<author>C J H Jacobs</author>
</authors>
<title>Parsing as intersection. Parsing Techniques,</title>
<date>2008</date>
<pages>425--442</pages>
<contexts>
<context position="27611" citStr="Grune and Jacobs (2008)" startWordPosition="4569" endWordPosition="4572">entation Results (Source Type) MT05 MT06 BLEU BLEU surface 0.4682 0.3512 morph 0.5087 0.3841 morph+surface 0.5225 0.4008 (Source Type) MT05 MT06 BLEU BLEU surface 0.5253 0.3991 morph 0.5377 0.4180 morph+surface 0.5453 0.4287 (a) Phrase-based model (b) Hierarchical model Table 5: Arabic Morphology Results fications of a surface token when the surface token can not be translated. Parsing and formal language theory. There has been considerable work on parsing word lattices, much of it for language modeling applications in speech recognition (Ney, 1991; Cheppalier and Rajman, 1998). Additionally, Grune and Jacobs (2008) refines an algorithm originally due to Bar-Hillel for intersecting an arbitrary FSA (of which word lattices are a subset) with a CFG. Klein and Manning (2001) formalize parsing as a hypergraph search problem and derive an O(n3) parser for lattices. 6 Conclusions We have achieved substantial gains in translation performance by decoding compact representations of alternative source language analyses, rather than single-best representations. Our results generalize previous gains for lattice translation of spoken language input, and we have further generalized the approach by introducing an algor</context>
</contexts>
<marker>Grune, Jacobs, 2008</marker>
<rawString>D. Grune and C.J. H. Jacobs. 2008. Parsing as intersection. Parsing Techniques, pages 425–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>F Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL,</booktitle>
<location>New York.</location>
<contexts>
<context position="18837" citStr="Habash and Sadat, 2006" startWordPosition="3198" endWordPosition="3201">ation variant we made use of. The limitation of this approach is that as the amount and variety of training data increases, the optimal segmentation strategy changes: more aggressive segmentation results 2The segmentation process is ambiguous, even for native speakers of Chinese. 1016 1 质 合金 号 5 称 工 8 10 业 牙 齿 0 2 硬 硬质 合 3 金 4 号称 6 &amp;quot; 7 工业 9 牙齿 11 &amp;quot; 12 硬质合金 Figure 5: Sample Chinese segmentation lattice using three segmentations. in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). Lattices allow the decoder to make decisions about what granularity of segmentation to use subsententially. 4.1 Chinese Word Segmentation Experiments In our experiments we used two state-of-the-art Chinese word segmenters: one developed at Harbin Institute of Technology (Zhao et al., 2001), and one developed at Stanford University (Tseng et al., 2005). In addition, we used a character-based segmentation. In the remaining of this paper, we use cs for character segmentation, hs for Harbin segmentation and ss for Stanford segmentation. We built two types of lattices: one that combines the Harbi</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>N. Habash and F. Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In Proc. of NAACL, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Parsing with hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings of IWPT</booktitle>
<contexts>
<context position="27770" citStr="Klein and Manning (2001)" startWordPosition="4595" endWordPosition="4598">face 0.5253 0.3991 morph 0.5377 0.4180 morph+surface 0.5453 0.4287 (a) Phrase-based model (b) Hierarchical model Table 5: Arabic Morphology Results fications of a surface token when the surface token can not be translated. Parsing and formal language theory. There has been considerable work on parsing word lattices, much of it for language modeling applications in speech recognition (Ney, 1991; Cheppalier and Rajman, 1998). Additionally, Grune and Jacobs (2008) refines an algorithm originally due to Bar-Hillel for intersecting an arbitrary FSA (of which word lattices are a subset) with a CFG. Klein and Manning (2001) formalize parsing as a hypergraph search problem and derive an O(n3) parser for lattices. 6 Conclusions We have achieved substantial gains in translation performance by decoding compact representations of alternative source language analyses, rather than single-best representations. Our results generalize previous gains for lattice translation of spoken language input, and we have further generalized the approach by introducing an algorithm for lattice decoding using a hierarchical phrase-based model. Additionally, we have shown that although word lattices complicate modeling of word reorderi</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>D. Klein and C. D. Manning. 2001. Parsing with hypergraphs. In Proceedings of IWPT 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of IEEE Internation Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="20158" citStr="Kneser and Ney, 1995" startWordPosition="3405" endWordPosition="3408">ettings. The systems used in these experiments were trained on the NIST MT06 Eval corpus without the UN data (approximatively 950K sentences). The corpus was analyzed with the three segmentation schemes. For the systems using word lattices, the training data contained the versions of the corpus appropriate for the segmentation schemes used in the input. That is, for the hs+ss condition, the training data consisted of two copies of the corpus: one segmented with the Harbin segmenter and the other with the Stanford segmenter.3 A trigram English language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) was trained on the English side of our training data as well as portions of the Gigaword v2 English Corpus, and was used for all experiments. The NIST MT03 test set was used as a development set for optimizing the interpolation weights using minimum error rate train3The corpora were word-aligned independently and then concatenated for rule extraction. ing (Och, 2003). The testing was done on the NIST 2005 and 2006 evaluation sets (MT05, MT06). Experimental results: Word-lattices improve translation quality. We used both a phrase-based translation model, decoded using our modified version of M</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of IEEE Internation Conference on Acoustics, Speech, and Signal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>48--54</pages>
<contexts>
<context position="5214" citStr="Koehn et al. (2003)" startWordPosition="813" endWordPosition="816">bic-English and Chinese-English translation. In Section 5 we discuss relevant prior work, and we conclude in Section 6. 2 Decoding Most statistical machine translation systems model translational equivalence using either finite state transducers or synchronous context free grammars (Lopez, to appear 2008). In this section we discuss the issues associated with adapting decoders from both classes of formalism to process word lattices. The first decoder we present is a SCFG-based decoder similar to the one described in Chiang (2007). The second is a phrase-based decoder implementing the model of Koehn et al. (2003). 2.1 Word lattices A word lattice G = hV, Ei is a directed acyclic graph that formally is a weighted finite state automaton (FSA). We further stipulate that exactly one node has no outgoing edges and is designated the ‘end node’. Figure 1 illustrates three classes of word lattices. Figure 1: Three examples of word lattices: (a) sentence, (b) confusion network, and (c) non-linear word lattice. A word lattice is useful for our purposes because it permits any finite set of strings to be represented and allows for substrings common to multiple members of the set to be represented with a single pi</context>
<context position="8898" citStr="Koehn et al. (2003)" startWordPosition="1505" endWordPosition="1508">om parsing to MT decoding A target language model is necessary to generate fluent output. To do so, the grammar is intersected with an n-gram LM. To mitigate the effects of the combinatorial explosion of non-terminals the LM intersection entails, we use cube-pruning to only consider the most promising expansions (Chiang, 2007). 2.4 Lattice translation with FSTs A second important class of translation models includes those based formally on FSTs. We present a description of the decoding process for a word lattice using a representative FST model, the phrase-based translation model described in Koehn et al. (2003). Phrase-based models translate a foreign sentence f into the target language e by breaking up f into a sequence of phrases f1, where each phrase fz can contain one or more contiguous words and is translated into a target phrase ez of one or more contiguous words. Each word in f must be translated exactly once. To generalize this model to word lattices, it is necessary to choose both a path through the lattice and a partitioning of the sentence this induces into a sequence of phrases f1. Although the number of source phrases in a word lattice can be exponential in the number of nodes, enumerat</context>
<context position="12851" citStr="Koehn et al., 2003" startWordPosition="2215" endWordPosition="2218">n is necessary since grammar rules are processed in a strictly left-to-right fashion without any skips. 3 Distortion in a non-linear word lattice In both hierarchical and phrase-based models, the distance between words in the source sentence is used to limit where in the target sequence their translations will be generated. In phrase based translation, distortion is modeled explicitly. Models that support non-monotonic decoding generally include a distortion cost, such as |ai − bi−1 − 1 |where ai is the starting position of the foreign phrase fi and bi−1 is the ending position of phrase fi−1 (Koehn et al., 2003). The intuition behind this model is that since most translation is monotonic, the cost of skipping ahead or back in the source should be proportional to the number of words that are skipped. Additionally, a maximum distortion limit is used to restrict Figure 4: Distance-based distortion problem. What is the distance between node 4 to node 0? the size of the search space. In linear word lattices, such as confusion networks, the distance metric used for the distortion penalty and for distortion limits is well defined; however, in a non-linear word lattice, it poses the problem illustrated in Fi</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL 2003, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>A Axelrod</author>
<author>A Birch Mayne</author>
<author>C CallisonBurch</author>
<author>M Osborne</author>
<author>D Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proc. of IWSLT 2005,</booktitle>
<location>Pittsburgh.</location>
<contexts>
<context position="15765" citStr="Koehn et al., 2005" startWordPosition="2697" endWordPosition="2700">ormen et al., 1989). 3.1 Experimental results We tested the effect of the distance metric on translation quality using Chinese word segmentation lattices (Section 4.1, below) using both a hierarchical and phrase-based system modified to translate word lattices. We compared the shortest-path distance metric with a baseline which uses the difference in node number as the distortion distance. For an additional datapoint, we added a lexicalized reordering model that models the probability of each phrase pair appearing in three different orientations (swap, monotone, other) in the training corpus (Koehn et al., 2005). Table 2 summarizes the results of the phrasebased systems. On both test sets, the shortest path metric improved the BLEU scores. As expected, the lexicalized reordering model improved translation quality over the baseline; however, the improvement was more substantial in the model that used the shortest-path distance metric (which was already a higher baseline). Table 3 summarizes the results of our experiment comparing the performance of two distance metrics to determine whether a rule has exceeded the decoder’s span limit. The pattern is the same, showing a clear increase in BLEU for the s</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, CallisonBurch, Osborne, Talbot, 2005</marker>
<rawString>P. Koehn, A. Axelrod, A. Birch Mayne, C. CallisonBurch, M. Osborne, and D. Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proc. of IWSLT 2005, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch Mayne</author>
<author>C CallisonBurch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computation Linguistics (ACL), Demonstration Session,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="1683" citStr="Koehn et al., 2007" startWordPosition="243" endWordPosition="246"> translation could be viewed as an instance of noisy channel modeling (Brown et al., 1990). They introduced a now standard decomposition that distinguishes modeling sentences in the target language (language models) from modeling the relationship between source and target language (translation models). Today, virtually all statistical translation systems seek the best hypothesis e for a given input f in the source language, according to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (Bertoldi et al., 2007; Bertoldi and Federico, 2005; Koehn et al., 2007). Why, however, should this advantage be limited to translation from spoken input? Even for text, there are often multiple ways to derive a sequence of words from the input string. Segmentation of Chinese, decompounding in German, morphological analysis for Arabic — across a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1). In this paper, we go beyond speech translation </context>
<context position="9748" citStr="Koehn et al., 2007" startWordPosition="1656" endWordPosition="1659"> of one or more contiguous words. Each word in f must be translated exactly once. To generalize this model to word lattices, it is necessary to choose both a path through the lattice and a partitioning of the sentence this induces into a sequence of phrases f1. Although the number of source phrases in a word lattice can be exponential in the number of nodes, enumerating the possible translations of every span in a lattice is in practice tractable, as described by Bertoldi et al. (2007). 2.5 Decoding with phrase-based models We adapted the Moses phrase-based decoder to translate word lattices (Koehn et al., 2007). The unmodified decoder builds a translation hypothesis from left to right by selecting a range of untranslated words and adding translations of this phrase to the end of the hypothesis being extended. When no untranslated words remain, the translation process is complete. The word lattice decoder works similarly, only now the decoder keeps track not of the words that have been covered, but of the nodes, given a topological ordering of the nodes. For example, assuming the third lattice in Figure 1 is our input, if the edge with word a is translated, this will cover two untranslated nodes [0,1</context>
<context position="20783" citStr="Koehn et al., 2007" startWordPosition="3507" endWordPosition="3510">rained on the English side of our training data as well as portions of the Gigaword v2 English Corpus, and was used for all experiments. The NIST MT03 test set was used as a development set for optimizing the interpolation weights using minimum error rate train3The corpora were word-aligned independently and then concatenated for rule extraction. ing (Och, 2003). The testing was done on the NIST 2005 and 2006 evaluation sets (MT05, MT06). Experimental results: Word-lattices improve translation quality. We used both a phrase-based translation model, decoded using our modified version of Moses (Koehn et al., 2007), and a hierarchical phrase-based translation model, using our modified version of Hiero (Chiang, 2005; Chiang, 2007). These two translation model types illustrate the applicability of the theoretical contributions presented in Section 2 and Section 3. We observed that the coverage of named entities (NEs) in our baseline systems was rather poor. Since names in Chinese can be composed of relatively long strings of characters that cannot be translated individually, when generating the segmentation lattices that included cs arcs, we avoided segmenting NEs of type PERSON, as identified using a Chi</context>
</contexts>
<marker>Koehn, Hoang, Mayne, CallisonBurch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch Mayne, C. CallisonBurch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Annual Meeting of the Association for Computation Linguistics (ACL), Demonstration Session, pages 177–180, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evluation.</title>
<date>2004</date>
<booktitle>In Proc. of the 2004 Conf. on EMNLP,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="22331" citStr="Koehn (2004)" startWordPosition="3750" endWordPosition="3751">significantly better than the best segmentation (ss).4 For the phrase-based model, we obtain significant gains using our word-lattice decoder using all three segmentations on MT05. The other results, while better than the best segmentation (hs) by at least 0.3 BLEU points, are not statistically significant. Even if the results are not statistically significant for MT06, there is a high decrease in OOV items when using word-lattices. For example, for MT06 the number of OOVs in the hs translation is 484. 4Significance testing was carried out using the bootstrap resampling technique advocated by Koehn (2004). Unless otherwise noted, all reported improvements are signficant at at least P &lt; 0.05. 1017 surface wxlAl ftrp AlSyf kAn mEZm AlDjyj AlAElAmy m&amp;ydA llEmAd . segmented w- xlAl ftrp Al- Syf kAn mEZm Al- Djyj Al- AElAmy m&amp;ydA l- Al- EmAd . (English) During the summer period, most media buzz was supportive of the general . Figure 6: Example of Arabic morphological segmentation. The number of OOVs decreased by 19% for hs+ss and by 75% for hs+ss+cs. As mentioned in Section 3, using lexical reordering for word-lattices further improves the translation quality. 4.2 Arabic Morphology Experiments We c</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evluation. In Proc. of the 2004 Conf. on EMNLP, pages 388–395.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Lopez</author>
</authors>
<title>to appear 2008. Statistical machine translation.</title>
<journal>ACM Computing Surveys.</journal>
<marker>Lopez, </marker>
<rawString>A. Lopez. to appear 2008. Statistical machine translation. ACM Computing Surveys.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mathias</author>
<author>W Byrne</author>
</authors>
<title>Statistical phrasebased speech translation.</title>
<date>2006</date>
<booktitle>In IEEE Conf. on Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="25623" citStr="Mathias and Byrne (2006)" startWordPosition="4276" endWordPosition="4279">es of Bangalore and Riccardi (2000). Mathias (2007) and Saleem et al. (2004) only report results of monotonic decoding for the systems they describe. Bertoldi et al. (2007) solve the problem by requiring that their input be in the format of a confusion network, which enables the standard distortion penalty to be used. Finally, the system described by Zhang et al. (2005) uses IBM Model 4 features to translate lattices. For the distortion model, they use the maximum probability value over all possible paths in the lattice for each jump considered, which is similar to the approach we have taken. Mathias and Byrne (2006) build a phrase-based translation system as a cascaded series of FSTs which can accept any input FSA; however, the only reordering that is permitted is the swapping of two adjacent phrases. Applications of source lattices outside of the domain of spoken language translation have been far more limited. Costa-juss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language. Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to our</context>
</contexts>
<marker>Mathias, Byrne, 2006</marker>
<rawString>L. Mathias and W. Byrne. 2006. Statistical phrasebased speech translation. In IEEE Conf. on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mathias</author>
</authors>
<title>Statistical Machine Translation and Automatic Speech Recognition under Uncertainty.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>The Johns Hopkins University.</institution>
<contexts>
<context position="24507" citStr="Mathias, 2007" startWordPosition="4097" endWordPosition="4098">lattices improve translation quality. Results are presented in Table 5. Using word-lattices to combine the surface forms with morphologically segmented forms significantly improves BLEU scores both in the phrase-based and hierarchical models. 5 Prior work Lattice Translation. The ‘noisier channel’ model of machine translation has been widely used in spoken language translation as an alternative to selecting the single-best hypothesis from an ASR system and translating it (Ney, 1999; Casacuberta et al., 2004; Zhang et al., 2005; Saleem et al., 2005; Matusov et al., 2005; Bertoldi et al., 2007; Mathias, 2007). Several authors (e.g. Saleem et al. (2005) and Bertoldi et al. (2007)) comment directly on the impracticality of using n-best lists to translate speech. Although translation is fundamentally a nonmonotonic relationship between most language pairs, reordering has tended to be a secondary concern to the researchers who have worked on lattice translation. Matusov et al. (2005) decodes monotonically and then uses a finite state reordering model on the single-best translation, along the lines of Bangalore and Riccardi (2000). Mathias (2007) and Saleem et al. (2004) only report results of monotoni</context>
</contexts>
<marker>Mathias, 2007</marker>
<rawString>L. Mathias. 2007. Statistical Machine Translation and Automatic Speech Recognition under Uncertainty. Ph.D. thesis, The Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>S Kanthak</author>
<author>H Ney</author>
</authors>
<title>On the integration of speech recognition and statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<contexts>
<context position="24468" citStr="Matusov et al., 2005" startWordPosition="4088" endWordPosition="4092">ets (MT05, MT06). Experimental results: Word-lattices improve translation quality. Results are presented in Table 5. Using word-lattices to combine the surface forms with morphologically segmented forms significantly improves BLEU scores both in the phrase-based and hierarchical models. 5 Prior work Lattice Translation. The ‘noisier channel’ model of machine translation has been widely used in spoken language translation as an alternative to selecting the single-best hypothesis from an ASR system and translating it (Ney, 1999; Casacuberta et al., 2004; Zhang et al., 2005; Saleem et al., 2005; Matusov et al., 2005; Bertoldi et al., 2007; Mathias, 2007). Several authors (e.g. Saleem et al. (2005) and Bertoldi et al. (2007)) comment directly on the impracticality of using n-best lists to translate speech. Although translation is fundamentally a nonmonotonic relationship between most language pairs, reordering has tended to be a secondary concern to the researchers who have worked on lattice translation. Matusov et al. (2005) decodes monotonically and then uses a finite state reordering model on the single-best translation, along the lines of Bangalore and Riccardi (2000). Mathias (2007) and Saleem et al.</context>
</contexts>
<marker>Matusov, Kanthak, Ney, 2005</marker>
<rawString>E. Matusov, S. Kanthak, and H. Ney. 2005. On the integration of speech recognition and statistical machine translation. In Proceedings of Interspeech 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
</authors>
<title>Dynamic programming parsing for context-free grammars in continuous speech recognition.</title>
<date>1991</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="27542" citStr="Ney, 1991" startWordPosition="4561" endWordPosition="4562">model (b) Hierarchical model Table 4: Chinese Word Segmentation Results (Source Type) MT05 MT06 BLEU BLEU surface 0.4682 0.3512 morph 0.5087 0.3841 morph+surface 0.5225 0.4008 (Source Type) MT05 MT06 BLEU BLEU surface 0.5253 0.3991 morph 0.5377 0.4180 morph+surface 0.5453 0.4287 (a) Phrase-based model (b) Hierarchical model Table 5: Arabic Morphology Results fications of a surface token when the surface token can not be translated. Parsing and formal language theory. There has been considerable work on parsing word lattices, much of it for language modeling applications in speech recognition (Ney, 1991; Cheppalier and Rajman, 1998). Additionally, Grune and Jacobs (2008) refines an algorithm originally due to Bar-Hillel for intersecting an arbitrary FSA (of which word lattices are a subset) with a CFG. Klein and Manning (2001) formalize parsing as a hypergraph search problem and derive an O(n3) parser for lattices. 6 Conclusions We have achieved substantial gains in translation performance by decoding compact representations of alternative source language analyses, rather than single-best representations. Our results generalize previous gains for lattice translation of spoken language input,</context>
</contexts>
<marker>Ney, 1991</marker>
<rawString>H. Ney. 1991. Dynamic programming parsing for context-free grammars in continuous speech recognition. IEEE Transactions on Signal Processing, 39(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
</authors>
<title>Speech translation: Coupling of recognition and translation.</title>
<date>1999</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>517--520</pages>
<location>Phoenix.</location>
<contexts>
<context position="24379" citStr="Ney, 1999" startWordPosition="4074" endWordPosition="4075">(Och, 2003). Evaluation was carried out on the NIST 2005 and 2006 evaluation sets (MT05, MT06). Experimental results: Word-lattices improve translation quality. Results are presented in Table 5. Using word-lattices to combine the surface forms with morphologically segmented forms significantly improves BLEU scores both in the phrase-based and hierarchical models. 5 Prior work Lattice Translation. The ‘noisier channel’ model of machine translation has been widely used in spoken language translation as an alternative to selecting the single-best hypothesis from an ASR system and translating it (Ney, 1999; Casacuberta et al., 2004; Zhang et al., 2005; Saleem et al., 2005; Matusov et al., 2005; Bertoldi et al., 2007; Mathias, 2007). Several authors (e.g. Saleem et al. (2005) and Bertoldi et al. (2007)) comment directly on the impracticality of using n-best lists to translate speech. Although translation is fundamentally a nonmonotonic relationship between most language pairs, reordering has tended to be a secondary concern to the researchers who have worked on lattice translation. Matusov et al. (2005) decodes monotonically and then uses a finite state reordering model on the single-best transl</context>
</contexts>
<marker>Ney, 1999</marker>
<rawString>H. Ney. 1999. Speech translation: Coupling of recognition and translation. In Proc. of ICASSP, pages 517– 520, Phoenix.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Discriminitive training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="3233" citStr="Och and Ney (2002)" startWordPosition="499" endWordPosition="502">as a “noisier channel”, where an observed signal o gives rise to a set of source-language strings f&apos; E F(o) and we seek e� = arg max max e f�EF(o) Pr(e, f&apos;|o) (2) e� = arg maxPr(e|f) (1) = arg max max Pr(e)Pr(f&apos;|e, o) (3) e e f�EF(o) An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f. There, Bertoldi and others have recently found that, rather than translating a single-best transcription f, it is advantageous to allow the MT decoder to = arg max max Pr(e)Pr(f&apos;|e)Pr(o|f&apos;)�(4) e f�EF(o) Following Och and Ney (2002), we use the maximum entropy framework (Berger et al., 1996) to directly model the posterior Pr(e, f&apos;|o) with parameters tuned to minimize a loss function representing 1012 Proceedings of ACL-08: HLT, pages 1012–1020, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics the quality only of the resulting translations. Thus, we make use of the following general decision rule: e� = arg max max e f�EF(o) In principle, one could decode according to (2) simply by enumerating and decoding each f� ∈ F(o); however, for any interestingly large F(o) this will be impractical. W</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. Och and H. Ney. 2002. Discriminitive training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Saleem</author>
<author>S-C Jou</author>
<author>S Vogel</author>
<author>T Schulz</author>
</authors>
<title>Using word lattice information for a tighter coupling in speech translation systems.</title>
<date>2005</date>
<booktitle>In Proc. of ICSLP, Jeju Island,</booktitle>
<contexts>
<context position="24446" citStr="Saleem et al., 2005" startWordPosition="4084" endWordPosition="4087">and 2006 evaluation sets (MT05, MT06). Experimental results: Word-lattices improve translation quality. Results are presented in Table 5. Using word-lattices to combine the surface forms with morphologically segmented forms significantly improves BLEU scores both in the phrase-based and hierarchical models. 5 Prior work Lattice Translation. The ‘noisier channel’ model of machine translation has been widely used in spoken language translation as an alternative to selecting the single-best hypothesis from an ASR system and translating it (Ney, 1999; Casacuberta et al., 2004; Zhang et al., 2005; Saleem et al., 2005; Matusov et al., 2005; Bertoldi et al., 2007; Mathias, 2007). Several authors (e.g. Saleem et al. (2005) and Bertoldi et al. (2007)) comment directly on the impracticality of using n-best lists to translate speech. Although translation is fundamentally a nonmonotonic relationship between most language pairs, reordering has tended to be a secondary concern to the researchers who have worked on lattice translation. Matusov et al. (2005) decodes monotonically and then uses a finite state reordering model on the single-best translation, along the lines of Bangalore and Riccardi (2000). Mathias (2</context>
</contexts>
<marker>Saleem, Jou, Vogel, Schulz, 2005</marker>
<rawString>S. Saleem, S.-C. Jou, S. Vogel, and T. Schulz. 2005. Using word lattice information for a tighter coupling in speech translation systems. In Proc. of ICSLP, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tseng</author>
<author>P Chang</author>
<author>G Andrew</author>
<author>D Jurafsky</author>
<author>C Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="19192" citStr="Tseng et al., 2005" startWordPosition="3252" endWordPosition="3255">Figure 5: Sample Chinese segmentation lattice using three segmentations. in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). Lattices allow the decoder to make decisions about what granularity of segmentation to use subsententially. 4.1 Chinese Word Segmentation Experiments In our experiments we used two state-of-the-art Chinese word segmenters: one developed at Harbin Institute of Technology (Zhao et al., 2001), and one developed at Stanford University (Tseng et al., 2005). In addition, we used a character-based segmentation. In the remaining of this paper, we use cs for character segmentation, hs for Harbin segmentation and ss for Stanford segmentation. We built two types of lattices: one that combines the Harbin and Stanford segmenters (hs+ss), and one which uses all three segmentations (hs+ss+cs). Data and Settings. The systems used in these experiments were trained on the NIST MT06 Eval corpus without the UN data (approximatively 950K sentences). The corpus was analyzed with the three segmentation schemes. For the systems using word lattices, the training d</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Manning. 2005. A conditional random field word segmenter. In Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Integrated Chinese word segmentation in statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of IWSLT 2005,</booktitle>
<location>Pittsburgh.</location>
<contexts>
<context position="26185" citStr="Xu et al. (2005)" startWordPosition="4363" endWordPosition="4366">o the approach we have taken. Mathias and Byrne (2006) build a phrase-based translation system as a cascaded series of FSTs which can accept any input FSA; however, the only reordering that is permitted is the swapping of two adjacent phrases. Applications of source lattices outside of the domain of spoken language translation have been far more limited. Costa-juss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language. Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to ours for Chinese-English translation and encodes multiple word segmentations in a lattice, but which is decoded with a conventionally trained translation model and without a sophisticated reordering model. The Arabic-English morphological segmentation lattices are similar in spirit to backoff translation models (Yang and Kirchhoff, 2006), which consider alternative morphological segmentations and simpli1018 (Source Type) MT05 MT06 BLEU BLEU cs 0.2833 0.2694 hs 0.2905 0.2835 ss 0.2894 0.2801 hs+ss 0.2938 0.2870 hs+ss+cs 0.2993 0.2865 hs+ss+cs.lexRo 0.3072 0.29</context>
</contexts>
<marker>Xu, Matusov, Zens, Ney, 2005</marker>
<rawString>J. Xu, E. Matusov, R. Zens, and H. Ney. 2005. Integrated Chinese word segmentation in statistical machine translation. In Proc. of IWSLT 2005, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yang</author>
<author>K Kirchhoff</author>
</authors>
<title>Phrase-based backoff models for machine translation of highly inflected languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL</booktitle>
<pages>41--48</pages>
<contexts>
<context position="26559" citStr="Yang and Kirchhoff, 2006" startWordPosition="4414" endWordPosition="4417">ss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language. Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to ours for Chinese-English translation and encodes multiple word segmentations in a lattice, but which is decoded with a conventionally trained translation model and without a sophisticated reordering model. The Arabic-English morphological segmentation lattices are similar in spirit to backoff translation models (Yang and Kirchhoff, 2006), which consider alternative morphological segmentations and simpli1018 (Source Type) MT05 MT06 BLEU BLEU cs 0.2833 0.2694 hs 0.2905 0.2835 ss 0.2894 0.2801 hs+ss 0.2938 0.2870 hs+ss+cs 0.2993 0.2865 hs+ss+cs.lexRo 0.3072 0.2992 (Source Type) MT05 MT06 BLEU BLEU cs 0.2904 0.2821 hs 0.3008 0.2907 ss 0.3071 0.2964 hs+ss 0.3132 0.3006 hs+ss+cs 0.3176 0.3043 (a) Phrase-based model (b) Hierarchical model Table 4: Chinese Word Segmentation Results (Source Type) MT05 MT06 BLEU BLEU surface 0.4682 0.3512 morph 0.5087 0.3841 morph+surface 0.5225 0.4008 (Source Type) MT05 MT06 BLEU BLEU surface 0.5253 0</context>
</contexts>
<marker>Yang, Kirchhoff, 2006</marker>
<rawString>M. Yang and K. Kirchhoff. 2006. Phrase-based backoff models for machine translation of highly inflected languages. In Proceedings of the EACL 2006, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zhang</author>
<author>G Kikui</author>
<author>H Yamamoto</author>
<author>W Lo</author>
</authors>
<title>A decoding algorithm for word lattice translation in speech translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="24425" citStr="Zhang et al., 2005" startWordPosition="4080" endWordPosition="4083">ut on the NIST 2005 and 2006 evaluation sets (MT05, MT06). Experimental results: Word-lattices improve translation quality. Results are presented in Table 5. Using word-lattices to combine the surface forms with morphologically segmented forms significantly improves BLEU scores both in the phrase-based and hierarchical models. 5 Prior work Lattice Translation. The ‘noisier channel’ model of machine translation has been widely used in spoken language translation as an alternative to selecting the single-best hypothesis from an ASR system and translating it (Ney, 1999; Casacuberta et al., 2004; Zhang et al., 2005; Saleem et al., 2005; Matusov et al., 2005; Bertoldi et al., 2007; Mathias, 2007). Several authors (e.g. Saleem et al. (2005) and Bertoldi et al. (2007)) comment directly on the impracticality of using n-best lists to translate speech. Although translation is fundamentally a nonmonotonic relationship between most language pairs, reordering has tended to be a secondary concern to the researchers who have worked on lattice translation. Matusov et al. (2005) decodes monotonically and then uses a finite state reordering model on the single-best translation, along the lines of Bangalore and Riccar</context>
</contexts>
<marker>Zhang, Kikui, Yamamoto, Lo, 2005</marker>
<rawString>R. Zhang, G. Kikui, H. Yamamoto, and W. Lo. 2005. A decoding algorithm for word lattice translation in speech translation. In Proceedings of the 2005 International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhao</author>
<author>L Yajuan</author>
<author>Y Muyun</author>
<author>Y Hao</author>
</authors>
<title>Increasing accuracy of chinese segmentation with strategy of multi-step processing.</title>
<date>2001</date>
<booktitle>In J Chinese Information Processing (Chinese Version),</booktitle>
<volume>1</volume>
<pages>13--18</pages>
<contexts>
<context position="19129" citStr="Zhao et al., 2001" startWordPosition="3242" endWordPosition="3245">称 工 8 10 业 牙 齿 0 2 硬 硬质 合 3 金 4 号称 6 &amp;quot; 7 工业 9 牙齿 11 &amp;quot; 12 硬质合金 Figure 5: Sample Chinese segmentation lattice using three segmentations. in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). Lattices allow the decoder to make decisions about what granularity of segmentation to use subsententially. 4.1 Chinese Word Segmentation Experiments In our experiments we used two state-of-the-art Chinese word segmenters: one developed at Harbin Institute of Technology (Zhao et al., 2001), and one developed at Stanford University (Tseng et al., 2005). In addition, we used a character-based segmentation. In the remaining of this paper, we use cs for character segmentation, hs for Harbin segmentation and ss for Stanford segmentation. We built two types of lattices: one that combines the Harbin and Stanford segmenters (hs+ss), and one which uses all three segmentations (hs+ss+cs). Data and Settings. The systems used in these experiments were trained on the NIST MT06 Eval corpus without the UN data (approximatively 950K sentences). The corpus was analyzed with the three segmentati</context>
</contexts>
<marker>Zhao, Yajuan, Muyun, Hao, 2001</marker>
<rawString>T. Zhao, L. Yajuan, Y. Muyun, and Y. Hao. 2001. Increasing accuracy of chinese segmentation with strategy of multi-step processing. In J Chinese Information Processing (Chinese Version), volume 1, pages 13–18.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>