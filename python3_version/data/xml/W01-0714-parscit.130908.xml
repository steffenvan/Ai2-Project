<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000978">
<title confidence="0.981513">
Distributional Phrase Structure Induction
</title>
<author confidence="0.999003">
Dan Klein and Christopher D. Manning
</author>
<affiliation confidence="0.9853815">
Computer Science Department
Stanford University
</affiliation>
<address confidence="0.778695">
Stanford, CA 94305-9040
</address>
<email confidence="0.989159">
klein, manning @cs.stanford.edu
</email>
<sectionHeader confidence="0.993619" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9989745">
Unsupervised grammar induction systems
commonly judge potential constituents on
the basis of their effects on the likelihood
of the data. Linguistic justifications of
constituency, on the other hand, rely on
notions such as substitutability and vary-
ing external contexts. We describe two
systems for distributional grammar induc-
tion which operate on such principles, us-
ing part-of-speech tags as the contextual
features. The advantages and disadvan-
tages of these systems are examined, in-
cluding precision/recall trade-offs, error
analysis, and extensibility.
</bodyText>
<sectionHeader confidence="0.995511" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.999978224489796">
While early work showed that small, artificial
context-free grammars could be induced with the
EM algorithm (Lari and Young, 1990) or with
chunk-merge systems (Stolcke and Omohundro,
1994), studies with large natural language gram-
mars have shown that these methods of completely
unsupervised acquisition are generally ineffective.
For instance, Charniak (1993) describes experi-
ments running the EM algorithm from random start-
ing points, which produced widely varying gram-
mars of extremely poor quality. Because of these
kinds of results, the vast majority of statistical pars-
ing work has focused on parsing as a supervised
learning problem (Collins, 1997; Charniak, 2000).
It remains an open problem whether an entirely un-
supervised method can either produce linguistically
sensible grammars or accurately parse free text.
However, there are compelling motivations for
unsupervised grammar induction. Building super-
vised training data requires considerable resources,
including time and linguistic expertise. Further-
more, investigating unsupervised methods can shed
light on linguistic phenomena which are implic-
itly captured within a supervised parser’s supervi-
sory information, and, therefore, often not explicitly
modeled in such systems. For example, our system
and others have difficulty correctly attaching sub-
jects to verbs above objects. For a supervised CFG
parser, this ordering is implicit in the given structure
of VP and S constituents, however, it seems likely
that to learn attachment order reliably, an unsuper-
vised system will have to model it explicitly.
Our goal in this work is the induction of high-
quality, linguistically sensible grammars, not pars-
ing accuracy. We present two systems, one which
does not do disambiguation well and one which
does not do it at all. Both take tagged but unparsed
Penn treebank sentences as input.1 To whatever de-
gree our systems parse well, it can be taken as evi-
dence that their grammars are sensible, but no effort
was taken to improve parsing accuracy directly.
There is no claim that human language acquisi-
tion is in any way modeled by the systems described
here. However, any success of these methods is evi-
dence of substantial cues present in the data, which
could potentially be exploited by humans as well.
Furthermore, mistakes made by these systems could
indicate points where human acquisition is likely
not being driven by these kinds of statistics.
</bodyText>
<sectionHeader confidence="0.994933" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.8358575">
At the heart of any iterative grammar induction sys-
tem is a method, implicit or explicit, for deciding
how to update the grammar. Two linguistic criteria
for constituency in natural language grammars form
the basis of this work (Radford, 1988):
1. External distribution: A constituent is a se-
quence of words which appears in various
structural positions within larger constituents.
</bodyText>
<footnote confidence="0.9496135">
1The Penn tag and category sets used in examples in this
paper are documented in Manning and Sch¨utze (1999, 413).
</footnote>
<listItem confidence="0.951913">
2. Substitutability: A constituent is a sequence of
</listItem>
<bodyText confidence="0.980829020833333">
words with (simple) variants which can be sub-
stituted for that sequence.
To make use of these intuitions, we use a distribu-
tional notion of context. Let be a part-of-speech
tag sequence. Every occurence of will be in some
context , where and are the adjacent tags or
sentence boundaries. The distribution over contexts
in which occurs is called its signature, which we
denote by .
Criterion 1 regards constituency itself. Consider
the tag sequences IN DT NN and IN DT. The former
is a canonical example of a constituent (of category
PP), while the later, though strictly more common,
is, in general, not a constituent. Frequency alone
does not distinguish these two sequences, but Crite-
rion 1 points to a distributional fact which does. In
particular, IN DT NN occurs in many environments.
It can follow a verb, begin a sentence, end a sen-
tence, and so on. On the other hand, IN DT is gener-
ally followed by some kind of a noun or adjective.
This example suggests that a sequence’s con-
stituency might be roughly indicated by the entropy
of its signature, . This turns out to be
somewhat true, given a few qualifications. Figure 1
shows the actual most frequent constituents along
with their rankings by several other measures. Tag
entropy by itself gives a list that is not particularly
impressive. There are two primary causes for this.
One is that uncommon but possible contexts have
little impact on the tag entropy value. Given the
skewed distribution of short sentences in the tree-
bank, this is somewhat of a problem. To correct for
this, let be the uniform distribution over the
observed contexts for . Using would
have the obvious effect of boosting rare contexts,
and the more subtle effect of biasing the rankings
slightly towards more common sequences. How-
ever, while presumably converges to some
sensible limit given infinite data, will
not, as noise eventually makes all or most counts
non-zero. Let be the uniform distribution over all
contexts. The scaled entropy
turned out to be a useful quantity in practice. Multi-
plying entropies is not theoretically meaningful, but
this quantity does converge to given infi-
nite (noisy) data. The list for scaled entropy still
has notable flaws, mainly relatively low ranks for
common NPs, which does not hurt system perfor-
</bodyText>
<table confidence="0.9992268">
Sequence Actual Freq Entropy Scaled Boundary GREEDY-RE
DT NN 1 2 4 2 1 1
NNP NNP 2 1 - - 4 2
CD CD 3 9 - - - 6
JJ NNS 4 7 3 3 2 4
DT JJ NN 5 - - - 10 8
DT NNS 6 - - - 9 10
JJ NN 7 3 - 7 6 3
CD NN 8 - - - - -
IN NN 9 - - 9 10 -
IN DT NN 10 - - - - -
NN NNS - - 5 6 3 7
NN NN - 8 - 10 7 5
TO VB - - 1 1 - -
DT JJ - 6 - ---
MD VB - - 10 - - -
IN DT - 4 ----
PRP VBZ - - - - 8 9
PRP VBD - - - - 5 -
NNS VBP - - 2 4 - -
NN VBZ - 10 7 5 - -
RB IN - - 8 ---
NN IN - 5 - ---
NNS VBD - - 9 8 - -
NNS IN - - 6 - - -
</table>
<figureCaption confidence="0.996640666666667">
Figure 1: Top non-trivial sequences by actual constituent
counts, raw frequency, raw entropy, scaled entropy, boundary
scaled entropy, and according to GREEDY-RE (see section 4.2).
</figureCaption>
<bodyText confidence="0.999927230769231">
mance, and overly high ranks for short subject-verb
sequences, which does.
The other fundamental problem with these
entropy-based rankings stems from the context fea-
tures themselves. The entropy values will change
dramatically if, for example, all noun tags are col-
lapsed, or if functional tags are split. This depen-
dence on the tagset for constituent identification is
very undesirable. One appealing way to remove this
dependence is to distinguish only two tags: one for
the sentence boundary (#) and another for words.
Scaling entropies by the entropy of this reduced sig-
nature produces the improved list labeled “Bound-
ary.” This quantity was not used in practice because,
although it is an excellent indicator of NP, PP, and
intransitive S constituents, it gives too strong a bias
against other constituents. However, neither system
is driven exclusively by the entropy measure used,
and duplicating the above rankings more accurately
did not always lead to better end results.
Criterion 2 regards the similarity of sequences.
Assume the data were truly generated by a cate-
gorically unambiguous PCFG (i.e., whenever a to-
ken of a sequence is a constituent, its label is deter-
mined) and that we were given infinite data. If so,
then two sequences, restricted to those occurrences
where they are constituents, would have the same
signatures. In practice, the data is finite, not statisti-
cally context-free, and even short sequences can be
categorically ambiguous. However, it remains true
that similar raw signatures indicate similar syntactic
behavior. For example, DT JJ NN and DT NN have
extremely similar signatures, and both are common
NPs. Also, NN IN and NN NN IN have very similar
signatures, and both are primarily non-constituents.
For our experiments, the metric of similarity be-
tween sequences was the Jensen-Shannon diver-
gence of the sequences’ signatures:
Where KL is the Kullback-Leibler divergence be-
tween probability distributions. Of course, just as
various notions of context are possible, so are vari-
ous metrics between signatures. The issues of tagset
dependence and data skew did not seem to matter
for the similarity measure, and unaltered Jensen-
Shannon divergence was used.
Given these ideas, section 4.1 discusses a sys-
tem whose grammar induction steps are guided by
sequence entropy and interchangeability, and sec-
tion 4.2 discusses a maximum likelihood system
where the objective being maximized is the quality
of the constituent/non-constituent distinction, rather
than the likelihood of the sentences.
</bodyText>
<subsectionHeader confidence="0.9994">
2.1 Problems with ML/MDL
</subsectionHeader>
<bodyText confidence="0.999979246575343">
Viewing grammar induction as a search problem,
there are three principal ways in which one can in-
duce a “bad” grammar:
Optimize the wrong objective function.
Choose bad initial conditions.
Be too sensitive to initial conditions.
Our current systems primarily attempt to address
the first two points. Common objective functions
include maximum likelihood (ML) which asserts
that a good grammar is one which best encodes
or compresses the given data. This is potentially
undesirable for two reasons. First, it is strongly
data-dependent. The grammar which maximizes
depends on the corpus , which, in some
sense, the core of a given language’s phrase struc-
ture should not. Second, and more importantly, in
an ML approach, there is pressure for the symbols
and rules in a PCFG to align in ways which maxi-
mize the truth of the conditional independence as-
sumptions embodied by that PCFG. The symbols
and rules of a natural language grammar, on the
other hand, represent syntactically and semantically
coherent units, for which a host of linguistic ar-
guments have been made (Radford, 1988). None
of these arguments have anything to do with con-
ditional independence; traditional linguistic con-
stituency reflects only grammatical possibilty of ex-
pansion. Indeed, there are expected to be strong
connections across phrases (such as are captured by
argument dependencies). For example, in the tree-
bank data used, CD CD is a common object of a verb,
but a very rare subject. However, a linguist would
take this as a selectional characteristic of the data
set, not an indication that CD CD is not an NP. Of
course, it could be that the ML and linguistic crite-
ria align, but in practice they do not always seem to,
and one should not expect that, by maximizing the
former, one will also maximize the latter.
Another common objective function is minimum
description length (MDL), which asserts that a good
analysis is a short one, in that the joint encoding of
the grammar and the data is compact. The “com-
pact grammar” aspect of MDL is perhaps closer to
some traditional linguistic argumentation which at
times has argued for minimal grammars on grounds
of analytical (Harris, 1951) or cognitive (Chomsky
and Halle, 1968) economy. However, some CFGs
which might possibly be seen as the acquisition goal
are anything but compact; take the Penn treebank
covering grammar for an extreme example. Another
serious issue with MDL is that the target grammar
is presumably bounded in size, while adding more
and more data will on average cause MDL methods
to choose ever larger grammars.
In addition to optimizing questionable objective
functions, many systems begin their search pro-
cedure from an extremely unfavorable region of
the grammar space. For example, the randomly
weighted grammars in Carroll and Charniak (1992)
rarely converged to remotely sensible grammars. As
they point out, and quite independently of whether
ML is a good objective function, the EM algorithm
is only locally optimal, and it seems that the space
of PCFGs is riddled with numerous local maxima.
Of course, the issue of initialization is somewhat
tricky in terms of the bias given to the system; for
example, Brill (1994) begins with a uniformly right-
branching structure. For English, right-branching
structure happens to be astonishingly good both as
an initial point for grammar learning and even as a
baseline parsing model. However, it would be un-
likely to perform nearly as well for a VOS language
like Malagasy or VSO languages like Hebrew.
</bodyText>
<sectionHeader confidence="0.914542" genericHeader="method">
3 Search vs. Clustering
</sectionHeader>
<bodyText confidence="0.99962">
Whether grammar induction is viewed as a search
problem or a clustering problem is a matter of per-
</bodyText>
<equation confidence="0.679568">
iS KL KL
</equation>
<bodyText confidence="0.995253674418605">
spective, and the two views are certainly not mutu-
ally exclusive. The search view focuses on the re-
cursive relationships between the non-terminals in
the grammar. The clustering view, which is per-
haps more applicable to the present work, focuses
on membership of (terminal) sequences to classes
represented by the non-terminals. For example, the
non-terminal symbol NP can be thought of as a clus-
ter of (terminal) sequences which can be generated
starting from NP. This clustering is then inherently
soft clustering, since sequences can be ambiguous.
Unlike standard clustering tasks, though, a se-
quence token in a given sentence need not be a con-
stituent at all. For example, DT NN is an extremely
common NP, and when it occurs, it is a constituent
around 82% of the time in the data. However, when
it occurs as a subsequence of DT NN NN it is usually
not a constituent. In fact, the difficult decisions for a
supervised parser, such as attachment level or coor-
dination scope, are decisions as to which sequences
are constituents, not what their tags would be if they
were. For example, DT NN IN DT NN is virtually al-
ways an NP when it is a constituent, but it is only a
constituent 66% of the time, mostly because the PP,
IN DT NN, is attached elsewhere.
One way to deal with this issue is to have an ex-
plicit class for “not a constituent” (see section 4.2).
There are difficulties in modeling such a class,
mainly stemming from the differences between this
class and the constituent classes. In particular, this
class will not be distributionally cohesive. Also, for
example, DT NN and DT 77 NN being generally of
category NP seems to be a highly distributional fact,
while DT NN not being a constituent in the context
DT NN NN seems more properly modeled by the
competing productions of the grammar.
Another approach is to model the non-
constituents either implicitly or independently
of the clustering model (see section 4.1). The draw-
back to insufficiently modeling non-constituency is
that for acquisition systems which essentially work
bottom-up, non-constituent chunks such as NN IN
or IN DT are hard to rule out locally.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="method">
4 Systems
</sectionHeader>
<bodyText confidence="0.999416">
We present two systems. The first, GREEDY-
MERGE, learns symbolic CFGs for partial parsing.
The rules it learns are of high quality (see figures
3 and 4), but parsing coverage is relatively shallow.
The second, CONSTITUENCY-PARSER, learns dis-
tributions over sequences representing the probabil-
</bodyText>
<figure confidence="0.8465405">
TOP
DT NN
</figure>
<figureCaption confidence="0.999841">
Figure 2: The possible contexts of a sequence.
</figureCaption>
<bodyText confidence="0.8119705">
ity that a constituent is realized as that sequence (see
figure 1). It produces full binary parses.
</bodyText>
<subsectionHeader confidence="0.926788">
4.1 GREEDY-MERGE
</subsectionHeader>
<bodyText confidence="0.998676909090909">
GREEDY-MERGE is a precision-oriented system
which, to a first approximation, can be seen as an
agglomerative clustering process over sequences.
For each pair of sequences, a normalized divergence
is calculated as follows:
The pair with the least divergence is merged.2
Merging two sequences involves the creation of a
single new non-terminal category which rewrites as
either sequence. Once there are non-terminal cate-
gories, the definitions of sequences and contexts be-
come slightly more complex. The input sentences
are parsed with the previous grammar state, using
a shallow parser which ties all parentless nodes to-
gether under a TOP root node. Sequences are then
the ordered sets of adjacent sisters in this parse, and
the context of a sequence can either be the pre-
ceding and following tags or a higher node in the
tree. To illustrate, in figure 2, the sequence VBZ RB
could either be considered to be in context [Z1... #]
or [NN... #]. Taking the highest potential context
([Z1... #] in this case) performed slightly better.3
Merging a sequence and a single non-terminal re-
sults in a rule which rewrites the non-terminal as the
sequence (i.e., that sequence is added to that non-
terminal’s class), and merging two non-terminals in-
volves collapsing the two symbols in the grammar
(i.e., those classes are merged). After the merge,
re-analysis of the grammar rule RHSs is necessary.
An important point about GREEDY-MERGE is
that stopping the system at the correct point is crit-
ical. Since our greedy criterion is not a measure
over entire grammar states, we have no way to de-
tect the optimal point beyond heuristics (the same
</bodyText>
<footnote confidence="0.993139625">
2We required that the candidates be among the 250 most
frequent sequences. The exact threshold was not important,
but without some threshold, long singleton sequences with zero
divergence are always chosen. This suggests that we need a
greater bias towards quantity of evidence in our basic method.
3An option which was not tried would be to consider a non-
terminal as a distribution over the tags of the right or left cor-
ners of the sequences belonging to that non-terminal.
</footnote>
<equation confidence="0.702881">
# z1
VBZ RB #
</equation>
<bodyText confidence="0.987742916666667">
category appears in several merges in a row, for ex-
ample) or by using a small supervision set to detect
a parse performance drop. The figures shown are
from stopping the system manually just before the
first significant drop in parsing accuracy.
The grammar rules produced by the system are a
strict subset of general CFG rules in several ways.
First, no unary rewriting is learned. Second, no non-
terminals which have only a single rewrite are ever
proposed, though this situation can occur as a result
of later merges. The effect of these restrictions is
discussed below.
</bodyText>
<subsectionHeader confidence="0.881851">
4.2 CONSTITUENCY-PARSER
</subsectionHeader>
<bodyText confidence="0.999831085714285">
The second system, CONSTITUENCY-PARSER, is
recall-oriented. Unlike GREEDY-MERGE, this sys-
tem always produces a full, binary parse of each in-
put sentence. However, its parsing behavior is sec-
ondary. It is primarily a clustering system which
views the data as the entire set of (sequence, con-
text) pairs that occurred in the sentences.
Each pair token comes from some specific sentence
and is classified with a binary judgement of that to-
ken’s constituency in that sentence. We assume that
these pairs are generated by the following model:
We use EM to maximize the likelihood of these
pairs given the hidden judgements , subject to the
constraints that the judgements for the pairs from a
given sentence must form a valid binary parse.
Initialization was either done by giving initial
seeds for the probabilities above or by forcing a cer-
tain set of parses on the first round. To do the re-
estimation, we must have some method of deciding
which binary bracketing to prefer. The chance of a
pair being a constituent is
and we score a tree by the likelihood product of
its judgements . The best tree is then
As we are considering each pair independently from
the rest of the parse, this model does not correspond
to a generative model of the kind standardly associ-
ated with PCFGs, but can be seen as a random field
over the possible parses, with the features being the
sequences and contexts (see (Abney, 1997)). How-
ever, note that we were primarily interested in the
clustering behavior, not the parsing behavior, and
that the random field parameters have not been fit
to any distribution over trees. The parsing model is
very crude, primarily serving to eliminate systemat-
ically mutually incompatible analyses.
</bodyText>
<subsectionHeader confidence="0.608277">
4.2.1 Sparsity
</subsectionHeader>
<bodyText confidence="0.999916833333333">
Since this system does not postulate any non-
terminal symbols, but works directly with terminal
sequences, sparsity will be extremely severe for any
reasonably long sequences. Substantial smoothing
was done to all terms; for the estimates we
interpolated the previous counts equally with a uni-
form , otherwise most sequences would remain
locked in their initial behaviors. This heavy smooth-
ing made rare sequences behave primarily accord-
ing to their contexts, removed the initial invariance
problem, and, after a few rounds of re-estimation,
had little effect on parser performance.
</bodyText>
<subsubsectionHeader confidence="0.647716">
4.2.2 Parameters
</subsubsectionHeader>
<bodyText confidence="0.999670416666667">
CONSTITUENCY-PARSER’s behavior is determined
by the initialization it is given, either by initial pa-
rameter estimates, or fixed first-round parses. We
used four methods: RANDOM, ENTROPY, RIGHT-
BRANCH, and GREEDY.
For RANDOM, we initially parsed randomly. For
ENTROPY, we weighted proportionally to
. For RIGHTBRANCH, we forced right-
branching structures (thereby introducing a bias to-
wards English structure). Finally, GREEDY used the
output from GREEDY-MERGE (using the grammar
state in figure 3) to parse initially.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.954783166666667">
Two kinds of results are presented. First,
we discuss the grammars learned by GREEDY-
MERGE and the constituent distributions learned by
CONSTITUENCY-PARSER. Then we apply both sys-
tems to parsing free text from the WSJ section of the
Penn treebank.
</bodyText>
<subsectionHeader confidence="0.875451">
5.1 Grammars learned by GREEDY-MERGE
</subsectionHeader>
<bodyText confidence="0.9999231">
Figure 3 shows a grammar learned at one stage of
a run of GREEDY-MERGE on the sentences in the
WSJ section of up to 10 words after the removal of
punctuation ( 7500 sentences). The non-terminal
categories proposed by the systems are internally
given arbitrary designations, but we have relabeled
them to indicate the best recall match for each.
Categories corresponding to NP, VP, PP, and S are
learned, although some are split into sub-categories
(transitive and intransitive VPs, proper NPs and two
</bodyText>
<figure confidence="0.969968785714286">
Transitive VPs
(complementation)
zVP zV JJ
zVP zV zNP
zVP zV zNN
zVP zV zPP
Transitive VPs
(adjunction)
zVP zRB zVP
ZVP zVP zPP
Intransitive S
zS PRP zV
zS zNP zV
zS zNNP zV
</figure>
<figureCaption confidence="0.853262">
Figure 3: A learned grammar.
</figureCaption>
<table confidence="0.901676292307693">
verb groups / intransitive VPs
zV VBZVBDVBP
zV MD VB
zV MD RB VB
zV zV zRB
zV zV zVBG
Transitive S
zSt zNNP zVP
zSt zNN zVP
zSt PRP zVP
N-bar or zero determiner NP
zNN NNNNS
zNN JJ zNN
zNN zNN zNN
NP with determiner
zNP DT zNN
zNP PRP$ zNN
Proper NP
zNNP NNPNNPS
zNNP zNNPzNNP
PP
zPP zIN zNN
zPP zIN zNP
zPP zIN zNNP
N-bar or zero-determiner NP
zNN NNNNS
zNN zNN zNN
zNN JJ zNN
Common NP with determiner
zNP DT zNN
zNP PRP$ zNN
Proper NP
zNNP zNNP zNNP
zNNP NNP
PP
zPP zIN zNN
zPP zIN zNP
zPP zIN zNNP
Transitive Verb Group
zVt VBZtVBDtVBPt
zVt MD zVBt
zVt zVt RB
Intransitive Verb Group
zVP VBZVBDVBP
zVP MD VB
zVP zVPzVBN
VP adjunction
zVP RB zVP
zVP zVP RB
zVP zVP zPP
zVP zVP zJJ
VP complementation
zVP zVt zNP
zVP zVt zNN
S
zS zNNPzVP
zS zNN zVP
zS zNP zVP
zS DT zVP
zS CC zS
zS RB zS
S-bar
zVP IN zS
1 - wrong attachment level
2 - wrong result category
</table>
<bodyText confidence="0.9987011">
kinds of common NPs, and so on).4 Provided one is
willing to accept a verb-group analysis, this gram-
mar seems sensible, though quite a few construc-
tions, such as relative clauses, are missing entirely.
Figure 4 shows a grammar learned at one stage
of a run when verbs were split by transitivity. This
grammar is similar, but includes analyses of sen-
tencial coordination and adverbials, and subordinate
clauses. The only rule in this grammar which seems
overly suspect is ZVP IN ZS which analyzes com-
plementized subordinate clauses as VPs.
In general, the major mistakes the GREEDY-
MERGE system makes are of three sorts:
Mistakes of omission. Even though the gram-
mar shown has correct, recursive analyses of
many categories, no rule can non-trivially in-
corporate a number (CD). There is also no
analysis for many common constructions.
Alternate analyses. The system almost invari-
ably forms verb groups, merging MD VB se-
quences with single main verbs to form verb
group constituents (argued for at times by
some linguists (Halliday, 1994)). Also, PPs are
sometimes attached to NPs below determiners
(which is in fact a standard linguistic analysis
(Abney, 1987)). It is not always clear whether
these analyses should be considered mistakes.
Over-merging. These errors are the most se-
rious. Since at every step two sequences are
merged, the process will eventually learn the
</bodyText>
<footnote confidence="0.861064">
4Splits often occur because unary rewrites are not learned
in the current system.
</footnote>
<figureCaption confidence="0.999658">
Figure 4: A learned grammar (with verbs split).
</figureCaption>
<bodyText confidence="0.9544112">
grammar where X X X and X (any ter-
minal). However, very incorrect merges are
sometimes made relatively early on (such as
merging VPs with PPs, or merging the se-
quences IN NNP IN and IN.
</bodyText>
<subsectionHeader confidence="0.981958">
5.2 CONSTITUENCY-PARSER’s Distributions
</subsectionHeader>
<bodyText confidence="0.999898">
The CONSTITUENCY-PARSER’s state is not a sym-
bolic grammar, but estimates of constituency for ter-
minal sequences. These distributions, while less
compelling a representation for syntactic knowl-
edge than CFGs, clearly have significant facts about
language embedded in them, and accurately learn-
ing them can be seen as a kind of acquisiton.
Figure 5 shows the sequences whose constituency
counts are most incorrect for the GREEDY-RE set-
ting. An interesting analysis given by the system is
the constituency of NNP POS NN sequences as NNP
(POS NN) which is standard in linguistic analyses
(Radford, 1988), as opposed to the treebank’s sys-
tematic (NNP POS) NN. Other common errors, like
the overcount of JJ NN or JJ NNS are partially due
to parsing inside NPs which are flat in the treebank
(see section 5.3).
It is informative to see how re-estimation with
CONSTITUENCY-PARSER improves and worsens
the GREEDY-MERGE initial parses. Coverage is
improved; for example NPs and PPs involving the
CD tag are consistently parsed as constituents while
GREEDY-MERGE did not include them in parses at
all. On the other hand, the GREEDY-MERGE sys-
</bodyText>
<figure confidence="0.999236518518519">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
UL Precision
NCB Precision
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
UL Recall
NP Recall
VP Recall
</figure>
<figureCaption confidence="0.999507">
Figure 6: Unlabeled precision (left) and recall (right) values for various settings.
</figureCaption>
<table confidence="0.999456772727273">
Sequence Overcount Estimated True Total
JJ NN 736 1099 363 1385
NN NN 504 663 159 805
NNP NNP 434 1419 985 2261
PRP VBZ 420 453 33 488
PRP VBD 392 415 23 452
PRP VBP 388 405 17 440
TO VB 324 443 119 538
MD VB 318 355 37 455
NN NNS 283 579 296 618
JJ NNS 283 799 516 836
Sequence Undercount Estimated True Total
NNP POS 127 33 160 224
VBD RB VBN 59 6 65 83
VB DT NN 53 10 63 137
NNP NNP POS 42 8 50 58
VB VBN 42 3 45 141
VB RB 39 6 45 100
VBD VBN 36 17 53 202
VBZ RB JJ 33 18 51 72
RB CD 30 26 56 117
VB DT JJ NN 29 3 32 51
</table>
<figureCaption confidence="0.950688">
Figure 5: Sequences most commonly over- and under-
</figureCaption>
<bodyText confidence="0.9409">
identified as constituents by CONSTITUENCY-PARSER using
GREEDY-RE (ENTROPY-RE is similar). “Total” is the fre-
quency of the sequence in the flat data. “True” is the frequency
as a constituent in the treebank’s parses. “Estimated” is the
frequency as a constituent in the system’s parses.
tem had learned the standard subject-verb-object at-
tachment order, though this has disappeared, as can
be seen in the undercounts of VP sequences. Since
many VPs did not fit the conservative VP grammar
in figure 3, subjects and verbs were often grouped
together frequently even on the initial parses, and
the CONSTITUENCY-PARSER has a further bias to-
wards over-identifying frequent constituents.
</bodyText>
<subsectionHeader confidence="0.998778">
5.3 Parsing results
</subsectionHeader>
<bodyText confidence="0.999678341463415">
Some issues impact the way the results of parsing
treebank sentences should be interpreted. Both sys-
tems, but especially the CONSTITUENCY-PARSER,
tend to form verb groups and often attach the sub-
ject below the object for transitive verbs. Because of
this, certain VPs are systematically incorrect and VP
accuracy suffers dramatically, substantially pulling
down the overall figures.5 Secondly, the treebank’s
grammar is an imperfect standard for an unsuper-
vised learner. For example, transitive sentences
are bracketed [subject [verb object]] (“The presi-
dent [executed the law]”) while nominalizations are
bracketed [[possessive noun] complement] (“[The
president’s execution] of the law”), an arbitrary in-
consistency which is unlikely to be learned automat-
ically. The treebank is also, somewhat purposefully,
very flat. For example, there is no analysis of the
inside of many short noun phrases. The GREEDY-
MERGE grammars above, however, give a (correct)
analysis of the insides of NPs like DT JJ NN NN
for which it will be penalized in terms of unlabeled
precision (though not crossing brackets) when com-
pared to the treebank.
An issue with GREEDY-MERGE is that the gram-
mar learned is symbolic, not probabilistic. Any dis-
ambiguation is done arbitrarily. Therefore, even
adding a linguistically valid rule can degrade nu-
merical performance (sometimes dramatically) by
introducing ambiguity to a greater degree than it im-
proves coverage.
In figure 6, we report summary results for
each system on the 10-word sentences of the
WSJ section. GREEDY is the above snapshot
of the GREEDY-MERGE system. RANDOM, EN-
TROPY, and RIGHTBRANCH are the behaviors
of the random-parse baseline, the right-branching
baseline, and the entropy-scored initialization for
CONSTITUENCY-PARSER. The -RE settings are
the result of context-based re-estimation from
the respective baselines using CONSTITUENCY-
PARSER.6 NCB precision is the percentage of pro-
</bodyText>
<footnote confidence="0.9969215">
5The RIGHTBRANCH baseline is in the opposite situation.
Its high overall figures are in a large part due to extremely high
VP accuracy, while NP and PP accuracy (which is more impor-
tant for tasks such as information extraction) is very low.
6RIGHTBRANCH was invariant under re-estimation, and
RIGHTBRANCH-RE is therefore omitted.
</footnote>
<bodyText confidence="0.996125">
posed brackets which do not cross a correct bracket.
Recall is also shown separately for VPs and NPs to
illustrate the VP effect noted above.
The general results are encouraging. GREEDY
is, as expected, higher precision than the other set-
tings. Re-estimation from that initial point improves
recall at the expense of precision. In general, re-
estimation improves parse accuracy, despite the in-
direct relationship between the criterion being max-
imized (constituency cluster fit) and parse quality.
</bodyText>
<sectionHeader confidence="0.957794" genericHeader="method">
6 Limitations of this study
</sectionHeader>
<bodyText confidence="0.9990955">
This study presents preliminary investigations and
has several significant limitations.
</bodyText>
<subsectionHeader confidence="0.974137">
6.1 Tagged Data
</subsectionHeader>
<bodyText confidence="0.99999792">
A possible criticism of this work is that it relies on
part-of-speech tagged data as input. In particular,
while there has been work on acquiring parts-of-
speech distributionally (Finch et al., 1995; Sch¨utze,
1995), it is clear that manually constructed tag sets
and taggings embody linguistic facts which are not
generally detected by a distributional learner. For
example, transitive and intransitive verbs are identi-
cally tagged yet distributionally dissimilar.
In principle, an acquisition system could be de-
signed to exploit non-distributionality in the tags.
For example, verb subcategorization or selection
could be induced from the ways in which a given
lexical verb’s distribution differs from the average,
as in (Resnik, 1993). However, rather than being ex-
ploited by the systems here, the distributional non-
unity of these tags appears to actually degrade per-
formance. As an example, the systems more reli-
ably group verbs and their objects together (rather
than verbs and their subjects) when transitive and
intransitive verbs are given separate tags.
Future experiments will investigate the impact of
distributional tagging, but, despite the degradation
in tag quality that one would expect, it is also possi-
ble that some current mistakes will be corrected.
</bodyText>
<subsectionHeader confidence="0.983536">
6.2 Individual system limitations
</subsectionHeader>
<bodyText confidence="0.999988777777778">
For GREEDY-MERGE, the primary limitations are
that there is no clear halting condition, there is
no ability to un-merge or to stop merging existing
classes while still increasing coverage, and the sys-
tem is potentially very sensitive to the tagset used.
For CONSTITUENCY-PARSER, the primary limita-
tions are that no labels or recursive grammars are
learned, and that the behavior is highly dependent
on initialization.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999987777777778">
We present two unsupervised grammar induction
systems, one of which is capable of producing
declarative, linguistically plausible grammars and
another which is capable of reliably identifying fre-
quent constituents. Both parse free text with ac-
curacy rivaling that of weakly supervised systems.
Ongoing work includes lexicalization, incorporat-
ing unary rules, enriching the models learned, and
addressing the limitations of the systems.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999906448979592">
Stephen P. Abney. 1987. The English Noun Phrase in its Sen-
tential Aspect. Ph.D. thesis, MIT.
Steven P. Abney. 1997. Stochastic attribute-value grammars.
Computational Linguistics, 23(4):597–618.
E. Brill. 1994. Automatic grammar induction and parsing free
text: A transformation-based approach. In Proc. ARPA Hu-
man Language Technology Workshop ’93, pages 237–242,
Princeton, NJ.
Glenn Carroll and Eugene Charniak. 1992. Two experiments
on learning probabilistic dependency grammars from cor-
pora. In Carl Weir, Stephen Abney, Ralph Grishman, and
Ralph Weischedel, editors, Working Notes of the Workshop
Statistically-Based NLP Techniques, pages 1–13. AAAI
Press, Menlo Park, CA.
Eugene Charniak. 1993. Statistical Language Learning. MIT
Press, Cambridge, MA.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL 1, pages 132–139.
Noam Chomsky and Morris Halle. 1968. The Sound Pattern of
English. Harper &amp; Row, New York.
Michael John Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In ACL 35/EACL 8, pages 16–23.
Steven P. Finch, Nick Chater, and Martin Redington. 1995. Ac-
quiring syntactic information from distributional statistics.
In J. Levy, D. Bairaktaris, J. A. Bullinaria, and P. Cairns, ed-
itors, Connectionist models of memory and language, pages
229–242. UCL Press, London.
M. A. K. Halliday. 1994. An introduction to functional gram-
mar. Edward Arnold, London, 2nd edition.
Zellig Harris. 1951. Methods in Structural Linguistics. Uni-
versity of Chicago Press, Chicago.
K. Lari and S. J. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algorithm.
Computer Speech and Language, 4:35–56.
Christopher D. Manning and Hinrich Sch¨utze. 1999. Foun-
dations of Statistical Natural Language Processing. MIT
Press, Boston, MA.
Andrew Radford. 1988. Transformational Grammar. Cam-
bridge University Press, Cambridge.
Philip Stuart Resnik. 1993. Selection and Information: A
Class-Based Approach to Lexical Relationships. Ph.D. the-
sis, University of Pennsylvania.
Hinrich Sch¨utze. 1995. Distributional part-of-speech tagging.
In EACL 7, pages 141–148.
Andreas Stolcke and Stephen M. Omohundro. 1994. Induc-
ing probabilistic grammars by Bayesian model merging. In
Grammatical Inference and Applications: Proceedings of
the Second International Colloquium on Grammatical In-
ference. Springer Verlag.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000244">
<title confidence="0.998811">Distributional Phrase Structure Induction</title>
<author confidence="0.995556">D Klein</author>
<affiliation confidence="0.832359">Computer Science Stanford</affiliation>
<address confidence="0.968327">Stanford, CA</address>
<email confidence="0.984719">klein,manning@cs.stanford.edu</email>
<abstract confidence="0.996366738095238">Unsupervised grammar induction systems commonly judge potential constituents on the basis of their effects on the likelihood of the data. Linguistic justifications of constituency, on the other hand, rely on notions such as substitutability and varying external contexts. We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features. The advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility. 1 Overview While early work showed that small, artificial context-free grammars could be induced with the EM algorithm (Lari and Young, 1990) or with chunk-merge systems (Stolcke and Omohundro, 1994), studies with large natural language grammars have shown that these methods of completely unsupervised acquisition are generally ineffective. For instance, Charniak (1993) describes experiments running the EM algorithm from random starting points, which produced widely varying grammars of extremely poor quality. Because of these kinds of results, the vast majority of statistical parsing work has focused on parsing as a supervised learning problem (Collins, 1997; Charniak, 2000). It remains an open problem whether an entirely unsupervised method can either produce linguistically sensible grammars or accurately parse free text. However, there are compelling motivations for unsupervised grammar induction. Building supervised training data requires considerable resources, including time and linguistic expertise. Furthermore, investigating unsupervised methods can shed light on linguistic phenomena which are implicitly captured within a supervised parser’s supervisory information, and, therefore, often not explicitly modeled in such systems. For example, our system and others have difficulty correctly attaching subjects to verbs above objects. For a supervised CFG parser, this ordering is implicit in the given structure however, it seems likely that to learn attachment order reliably, an unsupervised system will have to model it explicitly. Our goal in this work is the induction of highquality, linguistically sensible grammars, not parsing accuracy. We present two systems, one which does not do disambiguation well and one which does not do it at all. Both take tagged but unparsed treebank sentences as To whatever degree our systems parse well, it can be taken as evidence that their grammars are sensible, but no effort was taken to improve parsing accuracy directly. There is no claim that human language acquisition is in any way modeled by the systems described here. However, any success of these methods is evidence of substantial cues present in the data, which could potentially be exploited by humans as well. Furthermore, mistakes made by these systems could indicate points where human acquisition is likely not being driven by these kinds of statistics. 2 Approach At the heart of any iterative grammar induction system is a method, implicit or explicit, for deciding how to update the grammar. Two linguistic criteria for constituency in natural language grammars form the basis of this work (Radford, 1988): 1. External distribution: A constituent is a sequence of words which appears in various structural positions within larger constituents. Penn tag and category sets used in examples in this paper are documented in Manning and Sch¨utze (1999, 413). 2. Substitutability: A constituent is a sequence of words with (simple) variants which can be substituted for that sequence. To make use of these intuitions, we use a distributional notion of context. Let be a part-of-speech tag sequence. Every occurence of will be in some context , where and are the adjacent tags sentence boundaries. The distribution over contexts which occurs is called its which we denote by . Criterion 1 regards constituency itself. Consider tag sequences DT NN The former is a canonical example of a constituent (of category while the later, though strictly more common, is, in general, not a constituent. Frequency alone does not distinguish these two sequences, but Criterion 1 points to a distributional fact which does. In DT NN in many environments. It can follow a verb, begin a sentence, end a senand so on. On the other hand, DT generally followed by some kind of a noun or adjective. This example suggests that a sequence’s constituency might be roughly indicated by the entropy of its signature, . This turns out to somewhat true, given a few qualifications. Figure 1 shows the actual most frequent constituents along with their rankings by several other measures. Tag entropy by itself gives a list that is not particularly impressive. There are two primary causes for this. One is that uncommon but possible contexts have little impact on the tag entropy value. Given the skewed distribution of short sentences in the treebank, this is somewhat of a problem. To correct for this, let be the uniform distribution over the observed contexts for . Using would have the obvious effect of boosting rare contexts, and the more subtle effect of biasing the rankings towards more common sequences. However, while presumably converges to some sensible limit given infinite data, will not, as noise eventually makes all or most counts non-zero. Let be the uniform distribution over all contexts. The scaled entropy turned out to be a useful quantity in practice. Multiplying entropies is not theoretically meaningful, but quantity does converge to given nite (noisy) data. The list for scaled entropy still has notable flaws, mainly relatively low ranks for which does not hurt system perfor- Sequence Actual Freq Entropy Scaled Boundary</abstract>
<note confidence="0.921455916666667">DT NN 1 2 4 2 1 1 NNP NNP 2 1 - - 4 2 CD CD 3 9 - - - 6 JJ NNS 4 7 3 3 2 4 DT JJ NN 5 - - - 10 8 DT NNS 6 - - - 9 10 JJ NN 7 3 - 7 6 3 CD NN 8 - - - - - IN NN 9 - - 9 10 - IN DT NN 10 - - - - - NN NNS - - 5 6 3 7 NN NN - 8 - 10 7 5 TO VB - - 1 1 - - DT JJ - 6 - --- MD VB - - 10 - - - IN DT - 4 ---- PRP VBZ - - - - 8 9 PRP VBD - - - - 5 - NNS VBP - - 2 4 - - NN VBZ - 10 7 5 - - RB IN - - 8 --- NN IN - 5 - --- NNS VBD - - 9 8 - - NNS IN - - 6 - - -</note>
<abstract confidence="0.987520644897958">1: non-trivial sequences by actual constituent counts, raw frequency, raw entropy, scaled entropy, boundary entropy, and according to (see section 4.2). mance, and overly high ranks for short subject-verb sequences, which does. The other fundamental problem with these entropy-based rankings stems from the context features themselves. The entropy values will change dramatically if, for example, all noun tags are collapsed, or if functional tags are split. This dependence on the tagset for constituent identification is very undesirable. One appealing way to remove this dependence is to distinguish only two tags: one for the sentence boundary (#) and another for words. Scaling entropies by the entropy of this reduced signature produces the improved list labeled “Boundary.” This quantity was not used in practice because, it is an excellent indicator of and it gives too strong a bias against other constituents. However, neither system is driven exclusively by the entropy measure used, and duplicating the above rankings more accurately did not always lead to better end results. Criterion 2 regards the similarity of sequences. Assume the data were truly generated by a categorically unambiguous PCFG (i.e., whenever a token of a sequence is a constituent, its label is determined) and that we were given infinite data. If so, then two sequences, restricted to those occurrences where they are constituents, would have the same signatures. In practice, the data is finite, not statistically context-free, and even short sequences can be categorically ambiguous. However, it remains true that similar raw signatures indicate similar syntactic For example, JJ NN NN extremely similar signatures, and both are common Also, IN NN IN very similar signatures, and both are primarily non-constituents. For our experiments, the metric of similarity between sequences was the Jensen-Shannon divergence of the sequences’ signatures: KLis the Kullback-Leibler divergence between probability distributions. Of course, just as various notions of context are possible, so are various metrics between signatures. The issues of tagset dependence and data skew did not seem to matter for the similarity measure, and unaltered Jensen- Shannon divergence was used. Given these ideas, section 4.1 discusses a system whose grammar induction steps are guided by sequence entropy and interchangeability, and section 4.2 discusses a maximum likelihood system where the objective being maximized is the quality of the constituent/non-constituent distinction, rather than the likelihood of the sentences. 2.1 Problems with ML/MDL Viewing grammar induction as a search problem, there are three principal ways in which one can induce a “bad” grammar: Optimize the wrong objective function. Choose bad initial conditions. Be too sensitive to initial conditions. Our current systems primarily attempt to address the first two points. Common objective functions include maximum likelihood (ML) which asserts that a good grammar is one which best encodes or compresses the given data. This is potentially undesirable for two reasons. First, it is strongly data-dependent. The grammar which maximizes depends on the corpus , which, in some sense, the core of a given language’s phrase structure should not. Second, and more importantly, in an ML approach, there is pressure for the symbols and rules in a PCFG to align in ways which maximize the truth of the conditional independence assumptions embodied by that PCFG. The symbols and rules of a natural language grammar, on the other hand, represent syntactically and semantically coherent units, for which a host of linguistic arguments have been made (Radford, 1988). None of these arguments have anything to do with conditional independence; traditional linguistic constituency reflects only grammatical possibilty of expansion. Indeed, there are expected to be strong connections across phrases (such as are captured by argument dependencies). For example, in the treedata used, CD a common object of a verb, but a very rare subject. However, a linguist would take this as a selectional characteristic of the data not an indication that CD not an Of course, it could be that the ML and linguistic criteria align, but in practice they do not always seem to, and one should not expect that, by maximizing the former, one will also maximize the latter. Another common objective function is minimum description length (MDL), which asserts that a good analysis is a short one, in that the joint encoding of the grammar and the data is compact. The “compact grammar” aspect of MDL is perhaps closer to some traditional linguistic argumentation which at times has argued for minimal grammars on grounds of analytical (Harris, 1951) or cognitive (Chomsky and Halle, 1968) economy. However, some CFGs which might possibly be seen as the acquisition goal are anything but compact; take the Penn treebank covering grammar for an extreme example. Another serious issue with MDL is that the target grammar is presumably bounded in size, while adding more and more data will on average cause MDL methods to choose ever larger grammars. In addition to optimizing questionable objective functions, many systems begin their search procedure from an extremely unfavorable region of the grammar space. For example, the randomly weighted grammars in Carroll and Charniak (1992) rarely converged to remotely sensible grammars. As they point out, and quite independently of whether ML is a good objective function, the EM algorithm is only locally optimal, and it seems that the space of PCFGs is riddled with numerous local maxima. Of course, the issue of initialization is somewhat tricky in terms of the bias given to the system; for example, Brill (1994) begins with a uniformly rightbranching structure. For English, right-branching structure happens to be astonishingly good both as an initial point for grammar learning and even as a baseline parsing model. However, it would be unlikely to perform nearly as well for a VOS language like Malagasy or VSO languages like Hebrew. 3 Search vs. Clustering Whether grammar induction is viewed as a search or a clustering problem is a matter of periS KL KL spective, and the two views are certainly not mutually exclusive. The search view focuses on the recursive relationships between the non-terminals in the grammar. The clustering view, which is perhaps more applicable to the present work, focuses on membership of (terminal) sequences to classes represented by the non-terminals. For example, the symbol be thought of as a cluster of (terminal) sequences which can be generated from This clustering is then inherently soft clustering, since sequences can be ambiguous. Unlike standard clustering tasks, though, a sequence token in a given sentence need not be a conat all. For example, NN an extremely and when it occurs, it is a constituent around 82% of the time in the data. However, when occurs as a subsequence of NN NN is usually not a constituent. In fact, the difficult decisions for a supervised parser, such as attachment level or coordination scope, are decisions as to which sequences are constituents, not what their tags would be if they For example, NN IN DT NN virtually alan it is a constituent, but it is only a 66% of the time, mostly because the DT is attached elsewhere. One way to deal with this issue is to have an explicit class for “not a constituent” (see section 4.2). There are difficulties in modeling such a class, mainly stemming from the differences between this class and the constituent classes. In particular, this class will not be distributionally cohesive. Also, for NN 77 NN generally of to be a highly distributional fact, NN being a constituent in the context NN NN more properly modeled by the competing productions of the grammar. Another approach is to model the nonconstituents either implicitly or independently of the clustering model (see section 4.1). The drawback to insufficiently modeling non-constituency is that for acquisition systems which essentially work non-constituent chunks such as IN DT hard to rule out locally. 4 Systems present two systems. The first, learns symbolic CFGs for partial parsing. The rules it learns are of high quality (see figures 3 and 4), but parsing coverage is relatively shallow. second, learns disover sequences representing the probabil- TOP DT NN Figure 2: The possible contexts of a sequence. ity that a constituent is realized as that sequence (see figure 1). It produces full binary parses. a precision-oriented system which, to a first approximation, can be seen as an agglomerative clustering process over sequences. For each pair of sequences, a normalized divergence is calculated as follows: pair with the least divergence is Merging two sequences involves the creation of a single new non-terminal category which rewrites as either sequence. Once there are non-terminal categories, the definitions of sequences and contexts become slightly more complex. The input sentences are parsed with the previous grammar state, using a shallow parser which ties all parentless nodes tounder a node. Sequences are then the ordered sets of adjacent sisters in this parse, and the context of a sequence can either be the preceding and following tags or a higher node in the To illustrate, in figure 2, the sequence RB either be considered to be in context #] Merging a sequence and a single non-terminal results in a rule which rewrites the non-terminal as the sequence (i.e., that sequence is added to that nonterminal’s class), and merging two non-terminals involves collapsing the two symbols in the grammar (i.e., those classes are merged). After the merge, re-analysis of the grammar rule RHSs is necessary. important point about that stopping the system at the correct point is critical. Since our greedy criterion is not a measure over entire grammar states, we have no way to detect the optimal point beyond heuristics (the same required that the candidates be among the 250 most frequent sequences. The exact threshold was not important, but without some threshold, long singleton sequences with zero divergence are always chosen. This suggests that we need a greater bias towards quantity of evidence in our basic method. option which was not tried would be to consider a nonterminal as a distribution over the tags of the right or left corners of the sequences belonging to that non-terminal. VBZ RB # category appears in several merges in a row, for example) or by using a small supervision set to detect a parse performance drop. The figures shown are from stopping the system manually just before the first significant drop in parsing accuracy. The grammar rules produced by the system are a strict subset of general CFG rules in several ways. First, no unary rewriting is learned. Second, no nonterminals which have only a single rewrite are ever proposed, though this situation can occur as a result of later merges. The effect of these restrictions is discussed below. second system, is Unlike this system always produces a full, binary parse of each input sentence. However, its parsing behavior is secondary. It is primarily a clustering system which the data as the entire set of (sequence, context) pairs that occurred in the Each pair token comes from some specific sentence and is classified with a binary judgement of that token’s constituency in that sentence. We assume that these pairs are generated by the following model: We use EM to maximize the likelihood of these pairs given the hidden judgements , subject to the constraints that the judgements for the pairs from a given sentence must form a valid binary parse. Initialization was either done by giving initial seeds for the probabilities above or by forcing a certain set of parses on the first round. To do the reestimation, we must have some method of deciding which binary bracketing to prefer. The chance of a pair being a constituent is and we score a tree by the likelihood product of its judgements . The best tree is then As we are considering each pair independently from the rest of the parse, this model does not correspond to a generative model of the kind standardly associated with PCFGs, but can be seen as a random field over the possible parses, with the features being the sequences and contexts (see (Abney, 1997)). However, note that we were primarily interested in the clustering behavior, not the parsing behavior, and that the random field parameters have not been fit to any distribution over trees. The parsing model is very crude, primarily serving to eliminate systematically mutually incompatible analyses. 4.2.1 Sparsity Since this system does not postulate any nonterminal symbols, but works directly with terminal sequences, sparsity will be extremely severe for any reasonably long sequences. Substantial smoothing was done to all terms; for the estimates the previous counts equally with a uniform , otherwise most sequences would locked in their initial behaviors. This heavy smoothing made rare sequences behave primarily according to their contexts, removed the initial invariance problem, and, after a few rounds of re-estimation, had little effect on parser performance. 4.2.2 Parameters behavior is determined by the initialization it is given, either by initial parameter estimates, or fixed first-round parses. We four methods: and we initially parsed randomly. For we weighted proportionally For we forced rightbranching structures (thereby introducing a bias to- English structure). Finally, the from the grammar state in figure 3) to parse initially. 5 Results Two kinds of results are presented. First, discuss the grammars learned by the constituent distributions learned by Then we apply both systems to parsing free text from the WSJ section of the Penn treebank. Grammars learned by Figure 3 shows a grammar learned at one stage of run of the sentences in the WSJ section of up to 10 words after the removal of punctuation ( 7500 sentences). The non-terminal categories proposed by the systems are internally given arbitrary designations, but we have relabeled them to indicate the best recall match for each. corresponding to and learned, although some are split into sub-categories and intransitive proper and two Transitive VPs (complementation) zVP zV JJ zVP zV zNP zVP zV zNN zVP zV zPP Transitive VPs (adjunction) zVP zRB zVP ZVP zVP zPP Intransitive S zS PRP zV zS zNP zV zS zNNP zV Figure 3: A learned grammar. verb groups / intransitive VPs zV VBZVBDVBP zV MD VB zV MD RB VB zV zV zRB zV zV zVBG Transitive S zSt zNNP zVP zSt zNN zVP zSt PRP zVP N-bar or zero determiner NP zNN NNNNS zNN JJ zNN zNN zNN zNN NP with determiner zNP DT zNN zNP PRP$ zNN Proper NP zNNP NNPNNPS zNNP zNNPzNNP PP zPP zIN zNN zPP zIN zNP zPP zIN zNNP N-bar or zero-determiner NP zNN NNNNS zNN zNN zNN zNN JJ zNN Common NP with determiner zNP DT zNN zNP PRP$ zNN Proper NP zNNP zNNP zNNP zNNP NNP PP zPP zIN zNN zPP zIN zNP zPP zIN zNNP Transitive Verb Group zVt VBZtVBDtVBPt zVt MD zVBt zVt zVt RB Intransitive Verb Group zVP VBZVBDVBP zVP MD VB zVP zVPzVBN VP adjunction zVP RB zVP zVP zVP RB zVP zVP zPP zVP zVP zJJ VP complementation zVP zVt zNP zVP zVt zNN S zS zNNPzVP zS zNN zVP zS zNP zVP zS DT zVP zS CC zS zS RB zS S-bar zVP IN zS 1 wrong attachment level 2 wrong result category of common and so Provided one is willing to accept a verb-group analysis, this grammar seems sensible, though quite a few constructions, such as relative clauses, are missing entirely. Figure 4 shows a grammar learned at one stage of a run when verbs were split by transitivity. This grammar is similar, but includes analyses of sentencial coordination and adverbials, and subordinate clauses. The only rule in this grammar which seems suspect is IN analyzes comsubordinate clauses as general, the major mistakes the makes are of three sorts: Mistakes of omission. Even though the grammar shown has correct, recursive analyses of many categories, no rule can non-trivially ina number There is also no analysis for many common constructions. Alternate analyses. The system almost invariforms verb groups, merging VB sequences with single main verbs to form verb group constituents (argued for at times by linguists (Halliday, 1994)). Also, are attached to below determiners (which is in fact a standard linguistic analysis (Abney, 1987)). It is not always clear whether these analyses should be considered mistakes. Over-merging. These errors are the most serious. Since at every step two sequences are merged, the process will eventually learn the often occur because unary rewrites are not learned in the current system. Figure 4: A learned grammar (with verbs split). where X X terminal). However, very incorrect merges are sometimes made relatively early on (such as with or merging the se- NNP IN Distributions state is not a symbolic grammar, but estimates of constituency for terminal sequences. These distributions, while less compelling a representation for syntactic knowledge than CFGs, clearly have significant facts about language embedded in them, and accurately learning them can be seen as a kind of acquisiton. Figure 5 shows the sequences whose constituency are most incorrect for the setting. An interesting analysis given by the system is constituency of POS NN as which is standard in linguistic analyses (Radford, 1988), as opposed to the treebank’s sys- Other common errors, like overcount of NN NNS partially due parsing inside which are flat in the treebank (see section 5.3). It is informative to see how re-estimation with and worsens parses. Coverage is for example and involving the are consistently parsed as constituents while not include them in parses at On the other hand, the sys- 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 UL Precision NCB Precision 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 UL NP VP Recall Figure 6: Unlabeled precision (left) and recall (right) values for various settings.</abstract>
<note confidence="0.836245333333333">Sequence Overcount Estimated True Total JJ NN 736 1099 363 1385 NN NN 504 663 159 805 NNP NNP 434 1419 985 2261 PRP VBZ 420 453 33 488 PRP VBD 392 415 23 452 PRP VBP 388 405 17 440 TO VB 324 443 119 538 MD VB 318 355 37 455 NN NNS 283 579 296 618 JJ NNS 283 799 516 836 Sequence Undercount Estimated True Total NNP POS 127 33 160 224 VBD RB VBN 59 6 65 83 VB DT NN 53 10 63 137 NNP NNP POS 42 8 50 58 VB VBN 42 3 45 141 VB RB 39 6 45 100 VBD VBN 36 17 53 202 VBZ RB JJ 33 18 51 72 RB CD 30 26 56 117</note>
<phone confidence="0.333238">VB DT JJ NN 29 3 32 51</phone>
<abstract confidence="0.999441">5: most commonly overand underas constituents by is similar). “Total” is the frequency of the sequence in the flat data. “True” is the frequency as a constituent in the treebank’s parses. “Estimated” is the frequency as a constituent in the system’s parses. tem had learned the standard subject-verb-object attachment order, though this has disappeared, as can seen in the undercounts of Since did not fit the conservative in figure 3, subjects and verbs were often grouped together frequently even on the initial parses, and a further bias towards over-identifying frequent constituents. 5.3 Parsing results Some issues impact the way the results of parsing treebank sentences should be interpreted. Both sysbut especially the tend to form verb groups and often attach the subject below the object for transitive verbs. Because of certain are systematically incorrect and accuracy suffers dramatically, substantially pulling the overall Secondly, the treebank’s grammar is an imperfect standard for an unsupervised learner. For example, transitive sentences are bracketed [subject [verb object]] (“The president [executed the law]”) while nominalizations are bracketed [[possessive noun] complement] (“[The president’s execution] of the law”), an arbitrary inconsistency which is unlikely to be learned automatically. The treebank is also, somewhat purposefully, very flat. For example, there is no analysis of the of many short noun phrases. The above, however, give a (correct) of the insides of like JJ NN NN for which it will be penalized in terms of unlabeled precision (though not crossing brackets) when compared to the treebank. issue with that the grammar learned is symbolic, not probabilistic. Any disambiguation is done arbitrarily. Therefore, even adding a linguistically valid rule can degrade numerical performance (sometimes dramatically) by introducing ambiguity to a greater degree than it improves coverage. In figure 6, we report summary results for each system on the 10-word sentences of section. the above snapshot the and the behaviors of the random-parse baseline, the right-branching baseline, and the entropy-scored initialization for The -RE settings are the result of context-based re-estimation from respective baselines using NCB precision is the percentage of prois in the opposite situation. Its high overall figures are in a large part due to extremely high while (which is more important for tasks such as information extraction) is very low. invariant under re-estimation, and is therefore omitted. posed brackets which do not cross a correct bracket. is also shown separately for and to the noted above. general results are encouraging. is, as expected, higher precision than the other settings. Re-estimation from that initial point improves recall at the expense of precision. In general, reestimation improves parse accuracy, despite the indirect relationship between the criterion being maximized (constituency cluster fit) and parse quality. 6 Limitations of this study This study presents preliminary investigations and has several significant limitations. 6.1 Tagged Data A possible criticism of this work is that it relies on part-of-speech tagged data as input. In particular, while there has been work on acquiring parts-ofspeech distributionally (Finch et al., 1995; Sch¨utze, 1995), it is clear that manually constructed tag sets and taggings embody linguistic facts which are not generally detected by a distributional learner. For example, transitive and intransitive verbs are identically tagged yet distributionally dissimilar. In principle, an acquisition system could be designed to exploit non-distributionality in the tags. For example, verb subcategorization or selection could be induced from the ways in which a given lexical verb’s distribution differs from the average, as in (Resnik, 1993). However, rather than being exploited by the systems here, the distributional nonunity of these tags appears to actually degrade performance. As an example, the systems more reliably group verbs and their objects together (rather than verbs and their subjects) when transitive and intransitive verbs are given separate tags. Future experiments will investigate the impact of distributional tagging, but, despite the degradation in tag quality that one would expect, it is also possible that some current mistakes will be corrected. 6.2 Individual system limitations the primary limitations are that there is no clear halting condition, there is no ability to un-merge or to stop merging existing classes while still increasing coverage, and the system is potentially very sensitive to the tagset used. the primary limitations are that no labels or recursive grammars are learned, and that the behavior is highly dependent on initialization. 7 Conclusion We present two unsupervised grammar induction systems, one of which is capable of producing declarative, linguistically plausible grammars and another which is capable of reliably identifying frequent constituents. Both parse free text with accuracy rivaling that of weakly supervised systems. Ongoing work includes lexicalization, incorporating unary rules, enriching the models learned, and addressing the limitations of the systems.</abstract>
<note confidence="0.9743825">References P. Abney. 1987. English Noun Phrase in its Sen- Ph.D. thesis, MIT. Steven P. Abney. 1997. Stochastic attribute-value grammars. 23(4):597–618. E. Brill. 1994. Automatic grammar induction and parsing free A transformation-based approach. In ARPA Hu- Language Technology Workshop pages 237–242, Princeton, NJ. Glenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. In Carl Weir, Stephen Abney, Ralph Grishman, and Weischedel, editors, Notes of the Workshop NLP pages 1–13. AAAI Press, Menlo Park, CA. Charniak. 1993. Language MIT Press, Cambridge, MA. Eugene Charniak. 2000. A maximum-entropy-inspired parser. pages 132–139. Chomsky and Morris Halle. 1968. Sound Pattern of Harper &amp; Row, New York. Michael John Collins. 1997. Three generative, lexicalised modfor statistical parsing. In 35/EACL pages 16–23. Steven P. Finch, Nick Chater, and Martin Redington. 1995. Acquiring syntactic information from distributional statistics. In J. Levy, D. Bairaktaris, J. A. Bullinaria, and P. Cairns, edmodels of memory and pages 229–242. UCL Press, London.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephen P Abney</author>
</authors>
<title>The English Noun Phrase in its Sentential Aspect.</title>
<date>1987</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="24068" citStr="Abney, 1987" startWordPosition="4069" endWordPosition="4070">ral, the major mistakes the GREEDYMERGE system makes are of three sorts: Mistakes of omission. Even though the grammar shown has correct, recursive analyses of many categories, no rule can non-trivially incorporate a number (CD). There is also no analysis for many common constructions. Alternate analyses. The system almost invariably forms verb groups, merging MD VB sequences with single main verbs to form verb group constituents (argued for at times by some linguists (Halliday, 1994)). Also, PPs are sometimes attached to NPs below determiners (which is in fact a standard linguistic analysis (Abney, 1987)). It is not always clear whether these analyses should be considered mistakes. Over-merging. These errors are the most serious. Since at every step two sequences are merged, the process will eventually learn the 4Splits often occur because unary rewrites are not learned in the current system. Figure 4: A learned grammar (with verbs split). grammar where X X X and X (any terminal). However, very incorrect merges are sometimes made relatively early on (such as merging VPs with PPs, or merging the sequences IN NNP IN and IN. 5.2 CONSTITUENCY-PARSER’s Distributions The CONSTITUENCY-PARSER’s state</context>
</contexts>
<marker>Abney, 1987</marker>
<rawString>Stephen P. Abney. 1987. The English Noun Phrase in its Sentential Aspect. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="19620" citStr="Abney, 1997" startWordPosition="3312" endWordPosition="3313">the probabilities above or by forcing a certain set of parses on the first round. To do the reestimation, we must have some method of deciding which binary bracketing to prefer. The chance of a pair being a constituent is and we score a tree by the likelihood product of its judgements . The best tree is then As we are considering each pair independently from the rest of the parse, this model does not correspond to a generative model of the kind standardly associated with PCFGs, but can be seen as a random field over the possible parses, with the features being the sequences and contexts (see (Abney, 1997)). However, note that we were primarily interested in the clustering behavior, not the parsing behavior, and that the random field parameters have not been fit to any distribution over trees. The parsing model is very crude, primarily serving to eliminate systematically mutually incompatible analyses. 4.2.1 Sparsity Since this system does not postulate any nonterminal symbols, but works directly with terminal sequences, sparsity will be extremely severe for any reasonably long sequences. Substantial smoothing was done to all terms; for the estimates we interpolated the previous counts equally </context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven P. Abney. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23(4):597–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Automatic grammar induction and parsing free text: A transformation-based approach.</title>
<date>1994</date>
<booktitle>In Proc. ARPA Human Language Technology Workshop ’93,</booktitle>
<pages>237--242</pages>
<location>Princeton, NJ.</location>
<contexts>
<context position="12436" citStr="Brill (1994)" startWordPosition="2092" endWordPosition="2093"> addition to optimizing questionable objective functions, many systems begin their search procedure from an extremely unfavorable region of the grammar space. For example, the randomly weighted grammars in Carroll and Charniak (1992) rarely converged to remotely sensible grammars. As they point out, and quite independently of whether ML is a good objective function, the EM algorithm is only locally optimal, and it seems that the space of PCFGs is riddled with numerous local maxima. Of course, the issue of initialization is somewhat tricky in terms of the bias given to the system; for example, Brill (1994) begins with a uniformly rightbranching structure. For English, right-branching structure happens to be astonishingly good both as an initial point for grammar learning and even as a baseline parsing model. However, it would be unlikely to perform nearly as well for a VOS language like Malagasy or VSO languages like Hebrew. 3 Search vs. Clustering Whether grammar induction is viewed as a search problem or a clustering problem is a matter of periS KL KL spective, and the two views are certainly not mutually exclusive. The search view focuses on the recursive relationships between the non-termin</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>E. Brill. 1994. Automatic grammar induction and parsing free text: A transformation-based approach. In Proc. ARPA Human Language Technology Workshop ’93, pages 237–242, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Eugene Charniak</author>
</authors>
<title>Two experiments on learning probabilistic dependency grammars from corpora.</title>
<date>1992</date>
<booktitle>Working Notes of the Workshop Statistically-Based NLP Techniques,</booktitle>
<pages>1--13</pages>
<editor>In Carl Weir, Stephen Abney, Ralph Grishman, and Ralph Weischedel, editors,</editor>
<publisher>AAAI Press,</publisher>
<location>Menlo Park, CA.</location>
<contexts>
<context position="12057" citStr="Carroll and Charniak (1992)" startWordPosition="2026" endWordPosition="2029">gnitive (Chomsky and Halle, 1968) economy. However, some CFGs which might possibly be seen as the acquisition goal are anything but compact; take the Penn treebank covering grammar for an extreme example. Another serious issue with MDL is that the target grammar is presumably bounded in size, while adding more and more data will on average cause MDL methods to choose ever larger grammars. In addition to optimizing questionable objective functions, many systems begin their search procedure from an extremely unfavorable region of the grammar space. For example, the randomly weighted grammars in Carroll and Charniak (1992) rarely converged to remotely sensible grammars. As they point out, and quite independently of whether ML is a good objective function, the EM algorithm is only locally optimal, and it seems that the space of PCFGs is riddled with numerous local maxima. Of course, the issue of initialization is somewhat tricky in terms of the bias given to the system; for example, Brill (1994) begins with a uniformly rightbranching structure. For English, right-branching structure happens to be astonishingly good both as an initial point for grammar learning and even as a baseline parsing model. However, it wo</context>
</contexts>
<marker>Carroll, Charniak, 1992</marker>
<rawString>Glenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. In Carl Weir, Stephen Abney, Ralph Grishman, and Ralph Weischedel, editors, Working Notes of the Workshop Statistically-Based NLP Techniques, pages 1–13. AAAI Press, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1122" citStr="Charniak (1993)" startWordPosition="152" endWordPosition="153">grammar induction which operate on such principles, using part-of-speech tags as the contextual features. The advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility. 1 Overview While early work showed that small, artificial context-free grammars could be induced with the EM algorithm (Lari and Young, 1990) or with chunk-merge systems (Stolcke and Omohundro, 1994), studies with large natural language grammars have shown that these methods of completely unsupervised acquisition are generally ineffective. For instance, Charniak (1993) describes experiments running the EM algorithm from random starting points, which produced widely varying grammars of extremely poor quality. Because of these kinds of results, the vast majority of statistical parsing work has focused on parsing as a supervised learning problem (Collins, 1997; Charniak, 2000). It remains an open problem whether an entirely unsupervised method can either produce linguistically sensible grammars or accurately parse free text. However, there are compelling motivations for unsupervised grammar induction. Building supervised training data requires considerable res</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>Eugene Charniak. 1993. Statistical Language Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In NAACL 1,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1433" citStr="Charniak, 2000" startWordPosition="201" endWordPosition="202">ree grammars could be induced with the EM algorithm (Lari and Young, 1990) or with chunk-merge systems (Stolcke and Omohundro, 1994), studies with large natural language grammars have shown that these methods of completely unsupervised acquisition are generally ineffective. For instance, Charniak (1993) describes experiments running the EM algorithm from random starting points, which produced widely varying grammars of extremely poor quality. Because of these kinds of results, the vast majority of statistical parsing work has focused on parsing as a supervised learning problem (Collins, 1997; Charniak, 2000). It remains an open problem whether an entirely unsupervised method can either produce linguistically sensible grammars or accurately parse free text. However, there are compelling motivations for unsupervised grammar induction. Building supervised training data requires considerable resources, including time and linguistic expertise. Furthermore, investigating unsupervised methods can shed light on linguistic phenomena which are implicitly captured within a supervised parser’s supervisory information, and, therefore, often not explicitly modeled in such systems. For example, our system and o</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In NAACL 1, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>Morris Halle</author>
</authors>
<title>The Sound Pattern of English.</title>
<date>1968</date>
<publisher>Harper &amp; Row,</publisher>
<location>New York.</location>
<contexts>
<context position="11463" citStr="Chomsky and Halle, 1968" startWordPosition="1933" endWordPosition="1936"> an NP. Of course, it could be that the ML and linguistic criteria align, but in practice they do not always seem to, and one should not expect that, by maximizing the former, one will also maximize the latter. Another common objective function is minimum description length (MDL), which asserts that a good analysis is a short one, in that the joint encoding of the grammar and the data is compact. The “compact grammar” aspect of MDL is perhaps closer to some traditional linguistic argumentation which at times has argued for minimal grammars on grounds of analytical (Harris, 1951) or cognitive (Chomsky and Halle, 1968) economy. However, some CFGs which might possibly be seen as the acquisition goal are anything but compact; take the Penn treebank covering grammar for an extreme example. Another serious issue with MDL is that the target grammar is presumably bounded in size, while adding more and more data will on average cause MDL methods to choose ever larger grammars. In addition to optimizing questionable objective functions, many systems begin their search procedure from an extremely unfavorable region of the grammar space. For example, the randomly weighted grammars in Carroll and Charniak (1992) rarel</context>
</contexts>
<marker>Chomsky, Halle, 1968</marker>
<rawString>Noam Chomsky and Morris Halle. 1968. The Sound Pattern of English. Harper &amp; Row, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In ACL 35/EACL 8,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1416" citStr="Collins, 1997" startWordPosition="199" endWordPosition="200">icial context-free grammars could be induced with the EM algorithm (Lari and Young, 1990) or with chunk-merge systems (Stolcke and Omohundro, 1994), studies with large natural language grammars have shown that these methods of completely unsupervised acquisition are generally ineffective. For instance, Charniak (1993) describes experiments running the EM algorithm from random starting points, which produced widely varying grammars of extremely poor quality. Because of these kinds of results, the vast majority of statistical parsing work has focused on parsing as a supervised learning problem (Collins, 1997; Charniak, 2000). It remains an open problem whether an entirely unsupervised method can either produce linguistically sensible grammars or accurately parse free text. However, there are compelling motivations for unsupervised grammar induction. Building supervised training data requires considerable resources, including time and linguistic expertise. Furthermore, investigating unsupervised methods can shed light on linguistic phenomena which are implicitly captured within a supervised parser’s supervisory information, and, therefore, often not explicitly modeled in such systems. For example,</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael John Collins. 1997. Three generative, lexicalised models for statistical parsing. In ACL 35/EACL 8, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven P Finch</author>
<author>Nick Chater</author>
<author>Martin Redington</author>
</authors>
<title>Acquiring syntactic information from distributional statistics.</title>
<date>1995</date>
<booktitle>Connectionist models of memory and language,</booktitle>
<pages>229--242</pages>
<editor>In J. Levy, D. Bairaktaris, J. A. Bullinaria, and P. Cairns, editors,</editor>
<publisher>UCL Press,</publisher>
<location>London.</location>
<contexts>
<context position="30345" citStr="Finch et al., 1995" startWordPosition="5109" endWordPosition="5112">er precision than the other settings. Re-estimation from that initial point improves recall at the expense of precision. In general, reestimation improves parse accuracy, despite the indirect relationship between the criterion being maximized (constituency cluster fit) and parse quality. 6 Limitations of this study This study presents preliminary investigations and has several significant limitations. 6.1 Tagged Data A possible criticism of this work is that it relies on part-of-speech tagged data as input. In particular, while there has been work on acquiring parts-ofspeech distributionally (Finch et al., 1995; Sch¨utze, 1995), it is clear that manually constructed tag sets and taggings embody linguistic facts which are not generally detected by a distributional learner. For example, transitive and intransitive verbs are identically tagged yet distributionally dissimilar. In principle, an acquisition system could be designed to exploit non-distributionality in the tags. For example, verb subcategorization or selection could be induced from the ways in which a given lexical verb’s distribution differs from the average, as in (Resnik, 1993). However, rather than being exploited by the systems here, t</context>
</contexts>
<marker>Finch, Chater, Redington, 1995</marker>
<rawString>Steven P. Finch, Nick Chater, and Martin Redington. 1995. Acquiring syntactic information from distributional statistics. In J. Levy, D. Bairaktaris, J. A. Bullinaria, and P. Cairns, editors, Connectionist models of memory and language, pages 229–242. UCL Press, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>An introduction to functional grammar. Edward Arnold, London, 2nd edition.</title>
<date>1994</date>
<contexts>
<context position="23945" citStr="Halliday, 1994" startWordPosition="4050" endWordPosition="4051">rule in this grammar which seems overly suspect is ZVP IN ZS which analyzes complementized subordinate clauses as VPs. In general, the major mistakes the GREEDYMERGE system makes are of three sorts: Mistakes of omission. Even though the grammar shown has correct, recursive analyses of many categories, no rule can non-trivially incorporate a number (CD). There is also no analysis for many common constructions. Alternate analyses. The system almost invariably forms verb groups, merging MD VB sequences with single main verbs to form verb group constituents (argued for at times by some linguists (Halliday, 1994)). Also, PPs are sometimes attached to NPs below determiners (which is in fact a standard linguistic analysis (Abney, 1987)). It is not always clear whether these analyses should be considered mistakes. Over-merging. These errors are the most serious. Since at every step two sequences are merged, the process will eventually learn the 4Splits often occur because unary rewrites are not learned in the current system. Figure 4: A learned grammar (with verbs split). grammar where X X X and X (any terminal). However, very incorrect merges are sometimes made relatively early on (such as merging VPs w</context>
</contexts>
<marker>Halliday, 1994</marker>
<rawString>M. A. K. Halliday. 1994. An introduction to functional grammar. Edward Arnold, London, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Methods in Structural Linguistics.</title>
<date>1951</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="11424" citStr="Harris, 1951" startWordPosition="1929" endWordPosition="1930">indication that CD CD is not an NP. Of course, it could be that the ML and linguistic criteria align, but in practice they do not always seem to, and one should not expect that, by maximizing the former, one will also maximize the latter. Another common objective function is minimum description length (MDL), which asserts that a good analysis is a short one, in that the joint encoding of the grammar and the data is compact. The “compact grammar” aspect of MDL is perhaps closer to some traditional linguistic argumentation which at times has argued for minimal grammars on grounds of analytical (Harris, 1951) or cognitive (Chomsky and Halle, 1968) economy. However, some CFGs which might possibly be seen as the acquisition goal are anything but compact; take the Penn treebank covering grammar for an extreme example. Another serious issue with MDL is that the target grammar is presumably bounded in size, while adding more and more data will on average cause MDL methods to choose ever larger grammars. In addition to optimizing questionable objective functions, many systems begin their search procedure from an extremely unfavorable region of the grammar space. For example, the randomly weighted gramma</context>
</contexts>
<marker>Harris, 1951</marker>
<rawString>Zellig Harris. 1951. Methods in Structural Linguistics. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="892" citStr="Lari and Young, 1990" startWordPosition="119" endWordPosition="122"> the basis of their effects on the likelihood of the data. Linguistic justifications of constituency, on the other hand, rely on notions such as substitutability and varying external contexts. We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features. The advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility. 1 Overview While early work showed that small, artificial context-free grammars could be induced with the EM algorithm (Lari and Young, 1990) or with chunk-merge systems (Stolcke and Omohundro, 1994), studies with large natural language grammars have shown that these methods of completely unsupervised acquisition are generally ineffective. For instance, Charniak (1993) describes experiments running the EM algorithm from random starting points, which produced widely varying grammars of extremely poor quality. Because of these kinds of results, the vast majority of statistical parsing work has focused on parsing as a supervised learning problem (Collins, 1997; Charniak, 2000). It remains an open problem whether an entirely unsupervis</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Boston, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Radford</author>
</authors>
<title>Transformational Grammar.</title>
<date>1988</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="3432" citStr="Radford, 1988" startWordPosition="517" endWordPosition="518"> acquisition is in any way modeled by the systems described here. However, any success of these methods is evidence of substantial cues present in the data, which could potentially be exploited by humans as well. Furthermore, mistakes made by these systems could indicate points where human acquisition is likely not being driven by these kinds of statistics. 2 Approach At the heart of any iterative grammar induction system is a method, implicit or explicit, for deciding how to update the grammar. Two linguistic criteria for constituency in natural language grammars form the basis of this work (Radford, 1988): 1. External distribution: A constituent is a sequence of words which appears in various structural positions within larger constituents. 1The Penn tag and category sets used in examples in this paper are documented in Manning and Sch¨utze (1999, 413). 2. Substitutability: A constituent is a sequence of words with (simple) variants which can be substituted for that sequence. To make use of these intuitions, we use a distributional notion of context. Let be a part-of-speech tag sequence. Every occurence of will be in some context , where and are the adjacent tags or sentence boundaries. The di</context>
<context position="10338" citStr="Radford, 1988" startWordPosition="1744" endWordPosition="1745"> undesirable for two reasons. First, it is strongly data-dependent. The grammar which maximizes depends on the corpus , which, in some sense, the core of a given language’s phrase structure should not. Second, and more importantly, in an ML approach, there is pressure for the symbols and rules in a PCFG to align in ways which maximize the truth of the conditional independence assumptions embodied by that PCFG. The symbols and rules of a natural language grammar, on the other hand, represent syntactically and semantically coherent units, for which a host of linguistic arguments have been made (Radford, 1988). None of these arguments have anything to do with conditional independence; traditional linguistic constituency reflects only grammatical possibilty of expansion. Indeed, there are expected to be strong connections across phrases (such as are captured by argument dependencies). For example, in the treebank data used, CD CD is a common object of a verb, but a very rare subject. However, a linguist would take this as a selectional characteristic of the data set, not an indication that CD CD is not an NP. Of course, it could be that the ML and linguistic criteria align, but in practice they do n</context>
<context position="25236" citStr="Radford, 1988" startWordPosition="4258" endWordPosition="4259"> Distributions The CONSTITUENCY-PARSER’s state is not a symbolic grammar, but estimates of constituency for terminal sequences. These distributions, while less compelling a representation for syntactic knowledge than CFGs, clearly have significant facts about language embedded in them, and accurately learning them can be seen as a kind of acquisiton. Figure 5 shows the sequences whose constituency counts are most incorrect for the GREEDY-RE setting. An interesting analysis given by the system is the constituency of NNP POS NN sequences as NNP (POS NN) which is standard in linguistic analyses (Radford, 1988), as opposed to the treebank’s systematic (NNP POS) NN. Other common errors, like the overcount of JJ NN or JJ NNS are partially due to parsing inside NPs which are flat in the treebank (see section 5.3). It is informative to see how re-estimation with CONSTITUENCY-PARSER improves and worsens the GREEDY-MERGE initial parses. Coverage is improved; for example NPs and PPs involving the CD tag are consistently parsed as constituents while GREEDY-MERGE did not include them in parses at all. On the other hand, the GREEDY-MERGE sys1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 UL Precision NCB Precision 1 </context>
</contexts>
<marker>Radford, 1988</marker>
<rawString>Andrew Radford. 1988. Transformational Grammar. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Stuart Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="30884" citStr="Resnik, 1993" startWordPosition="5190" endWordPosition="5191">been work on acquiring parts-ofspeech distributionally (Finch et al., 1995; Sch¨utze, 1995), it is clear that manually constructed tag sets and taggings embody linguistic facts which are not generally detected by a distributional learner. For example, transitive and intransitive verbs are identically tagged yet distributionally dissimilar. In principle, an acquisition system could be designed to exploit non-distributionality in the tags. For example, verb subcategorization or selection could be induced from the ways in which a given lexical verb’s distribution differs from the average, as in (Resnik, 1993). However, rather than being exploited by the systems here, the distributional nonunity of these tags appears to actually degrade performance. As an example, the systems more reliably group verbs and their objects together (rather than verbs and their subjects) when transitive and intransitive verbs are given separate tags. Future experiments will investigate the impact of distributional tagging, but, despite the degradation in tag quality that one would expect, it is also possible that some current mistakes will be corrected. 6.2 Individual system limitations For GREEDY-MERGE, the primary lim</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Philip Stuart Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In EACL 7,</booktitle>
<pages>141--148</pages>
<marker>Sch¨utze, 1995</marker>
<rawString>Hinrich Sch¨utze. 1995. Distributional part-of-speech tagging. In EACL 7, pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Stephen M Omohundro</author>
</authors>
<title>Inducing probabilistic grammars by Bayesian model merging.</title>
<date>1994</date>
<booktitle>In Grammatical Inference and Applications: Proceedings of the Second International Colloquium on Grammatical Inference.</booktitle>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="950" citStr="Stolcke and Omohundro, 1994" startWordPosition="127" endWordPosition="130">e data. Linguistic justifications of constituency, on the other hand, rely on notions such as substitutability and varying external contexts. We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features. The advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility. 1 Overview While early work showed that small, artificial context-free grammars could be induced with the EM algorithm (Lari and Young, 1990) or with chunk-merge systems (Stolcke and Omohundro, 1994), studies with large natural language grammars have shown that these methods of completely unsupervised acquisition are generally ineffective. For instance, Charniak (1993) describes experiments running the EM algorithm from random starting points, which produced widely varying grammars of extremely poor quality. Because of these kinds of results, the vast majority of statistical parsing work has focused on parsing as a supervised learning problem (Collins, 1997; Charniak, 2000). It remains an open problem whether an entirely unsupervised method can either produce linguistically sensible gramm</context>
</contexts>
<marker>Stolcke, Omohundro, 1994</marker>
<rawString>Andreas Stolcke and Stephen M. Omohundro. 1994. Inducing probabilistic grammars by Bayesian model merging. In Grammatical Inference and Applications: Proceedings of the Second International Colloquium on Grammatical Inference. Springer Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>