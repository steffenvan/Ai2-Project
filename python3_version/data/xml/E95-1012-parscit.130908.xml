<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003849">
<title confidence="0.900083">
Stochastic HPSG
</title>
<author confidence="0.983384">
Chris Brew
</author>
<affiliation confidence="0.9972985">
Language Technology Group
HCRC, University of Edinburgh
</affiliation>
<address confidence="0.738376666666667">
2 Buccleuch Place
• Edinburgh EHS 9LW
Scotland, UK
</address>
<email confidence="0.753208">
email: Chris .BrewOedinburgh. ac .uk
</email>
<sectionHeader confidence="0.996352" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999508545454545">
In this paper we provide a probabilis-
tic interpretation for typed feature struc-
tures very similar to those used by Pol-
lard and Sag. We begin with a ver-
sion of the interpretation which lacks
a treatment of re-entrant feature struc-
tures, then provide an extended interpre-
tation which allows them. We sketch al-
gorithms allowing the numerical param-
eters of our probabilistic interpretations
of HPSG to be estimated from corpora.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999987951219512">
The purpose of our paper is to develop a princi-
pled technique for attaching a probabilistic inter-
pretation to feature structures. Our techniques
apply to the feature structures described by Car-
penter (Carpenter, 1992). Since these structures
are the ones which are used in by Pollard and
Sag (Pollard and Sag, 1994) their relevance to
computational grammars is apparent. On the ba-
sis of the usefulness of probabilistic context-free
grammars (Charniak, 1993, ch. 5), it is plausible
to assume that that the extension of probabilistic
techniques to such structures will allow the ap-
plication of known and new techniques of parse
ranking and grammar induction to more interest-
ing grammars than has hitherto been the case.
The paper is structured as follows. We start
by reviewing the training and use of probabilis-
tic context-free grammars (PCFGs). We then de-
velop a technique to allow analogous probabilistic
annotations on type hierarchies. This gives us a
clear account of the relationship between a large
class of feature structures and their probabilities,
but does not treat re-entrancy. We conclude by
sketching a technique which does treat such struc-
tures. While we know of previous work which as-
sociates scores with feature structures (Kim, 1994)
are not aware of any previous treatment which
makes explicit the link to classical probability the-
ory.
We take a slightly unconventional perspective
on feature structures, because it is easier to cast
our theory within the more general framework
of incremental description refinement (Mellish,
1988) than to exploit the usual metaphors of
constraint-based grammar. In fact we can afford
to remain entirely agnostic about the means by
which the HPSG grammar associates signs with
linguistic strings, because all that we need in or-
der to train our stochastic procedures is a corpus
of signs which are known to be valid descriptions
of strings.
</bodyText>
<sectionHeader confidence="0.9904315" genericHeader="introduction">
2 Probabilistic interpretation of
PCFGs
</sectionHeader>
<bodyText confidence="0.9995755">
We review the standard probabilistic interpreta-
tion of PCFGs&apos;.
A PCFG is a four-tuple &lt; W, N, N1, R &gt;
, where W is a set of terminal symbols
{w1, , N is a set of non-terminal symbols
{N1, , Nv}, N1 is the starting symbol and R
is a set of rules of the form Ni (j, where (j
is a string of terminals and non-terminals. Each
rule has a probability P(Ni —+ j) and the prob-
abilities for all the rules that expand a given non-
terminal must sum to one. We associate probabil-
ities with partial phrase markers, which are sets
of terminal and non-terminal nodes generated by
beginning from the starting node successively ex-
panding non-terminal leaves of the partial tree.
Phrase markers are those partial phrase markers
which have no non-terminal leaves. Probabilities
are assigned by the following inductive definition:
</bodyText>
<listItem confidence="0.9985856">
• P(Ni) = 1.
• If T is a partial phrase marker, and T&apos; is a
partial phrase marker which differs from it
only in that a single non-terminal node Nk
in T has been expanded to (in in T&apos;, then
</listItem>
<equation confidence="0.991706">
P(T&apos;) = P(T) x P (Nk (t12).
</equation>
<bodyText confidence="0.996744">
In this definition R acts as a specification of
the accessibility relationships which can hold be-
tween nodes of the trees admitted by the gram-
mar. The rule probabilities specify the cost of
</bodyText>
<footnote confidence="0.782757">
1 Our description is closely based on that given by
Charniak(Charniak, 1993, p. 52 ft)
</footnote>
<page confidence="0.998797">
83
</page>
<bodyText confidence="0.998300096774194">
making particular choices about the way in which
the rules develop. It is going to turn out that
an exactly analogous system of accessibility rela-
tions is present in the probabilistic type hierar-
chies which we define later.
Limitations of PCFGs The definition of
PCFGs implies that the probability of a phrase
marker depends only on the choice of rules used
in expanding non-terminal nodes. In particular,
the probability does not depend on the order in
which the rules are applied. This has the ar-
guably unwelcome consequence that PCFGs are
unable to make certain discriminations between
trees which differ only in their configuration 2.
The models developed in this paper build in simi-
lar independence assumptions. A large part of the
art of probabilistic language modelling resides in
the management of the trade-off between descrip-
tive power (which has the merit of allowing us to
make the discriminations which we want) and in-
dependence assumptions (which have the merit of
making training practical by allowing us to treat
similar situations as equivalent).
The crucial advantage of PCFGs over CFGs is
that they can be trained and/or learned from cor-
pora. Readers for whom this fact is unfamiliar are
referred to Charniak&apos;s textbook (Charniak, 1993,
Chapter 7). We do not have space to recapitu-
late the discussion of training which can be found
there. We do however illustrate the outcome of
training.
</bodyText>
<subsectionHeader confidence="0.999253">
2.1 Applying a PCFG to a simple corpus
</subsectionHeader>
<bodyText confidence="0.999981217391305">
Consider the simple grammar in figure 1 and its
training against the corpus in figure 2. Since there
are 3 plural sentences and only 2 singular sen-
tences, the optimal set of parameters will reflect
the distribution found in the corpus, as shown
in figure 3 One might have hoped that the ra-
tio P(np-singinp)/P(np-plinp) would be 2/3, but
it is instead V2/3. This is a consequence of the
assumption of independence. Effectively the algo-
rithm is ascribing the difference in distribution of
singular and plural sentences to the joint effect of
two independent decisions. What we would really
like it to do is to recognize that the two apparently
independent decisions are (in effect) one and the
same. Also, because the grammar has no means
of enforcing number agreement, the system sys-
tematically prefers plurals to singulars, even when
doing this will lead to agreement clashes. Thus
&amp;quot;buses stop&amp;quot; has estimated 0.55 x 0.55 = 0.3025,
&amp;quot;bus stop&amp;quot; and &amp;quot;buses stops&amp;quot; both have proba-
bility 0.55 x 0.45 = 0.2475 and &amp;quot;bus stops&amp;quot; has
probability 0.45 x 0.45 --= 0.2025. This behaviour
is clearly unmotivated by the corpus, and arises
</bodyText>
<footnote confidence="0.669783">
2The most obvious case is prepositional-phrase
attachment.
</footnote>
<bodyText confidence="0.9344895">
purely because of the inadequacy of the proba-
bilistic model.
</bodyText>
<sectionHeader confidence="0.58931" genericHeader="method">
3 Probabilistic type hierarchies
</sectionHeader>
<bodyText confidence="0.997866575">
ALE signatures Carpenter&apos;s ALE (Carpenter,
1993) allows the user to define the type hierarchy
of a grammar by writing a collection of clauses
which together denote an inheritance hierarchy, a
set of features and a set of&apos; appropriateness condi-
tions. An example of such a hierarchy is given in
ALE syntax in figure 4.
What the ALE signature tells us The inher-
itance information tells us that a sign is a forced
choice between a sentence and a phrase, that a
phrase is a forced choice between a noun-phrase
(np) and a verb-phrase (vp) and that number val-
ues (num) are partitioned into singular (sing) and
plural (pl). The features which are defined are
left,right, and num, and the appropriateness in-
formation says that the feature num introduces a
new instance of the type niun on all phrases, and
that left and right introduce np and vp respec-
tively on sentences.
The parallel with PCFGs The parallel which
makes it possible to apply the PCFG training
scheme almost unchanged is that the sub-types of
a given super-type partition the feature structures
of that type in just the same way that the differ-
ent rules which expand a given non-terminal N of
the PCFG partition the space of trees whose top-
most node is N. Equally, the features defined in
the hierarchy act as an accessibility relation be-
tween nodes in a way which is for our purposes
entirely equivalent to the way in which the right
hand sides of the rules introduce new nodes into
partial phrase markers 3. The hierarchy in figure 4
is related to but not isomorphic with the grammar
in figure 1.
One difference is that num is explicitly intro-
duced as a feature in the hierarchy, where at is
only implicitly present in the original grammar.
The other difference is the use of left and right
as models of the dominance relationships between
nodes.
</bodyText>
<sectionHeader confidence="0.691696" genericHeader="method">
4 A probabilistic interpretation of
typed feature-structures
</sectionHeader>
<bodyText confidence="0.9676134">
For our purposes, a probabilistic type hierarchy
(PTH) is a four-tuple
&lt; MT, NT, NT&apos;, I&gt;
where MT is a set of maximal types 4 ftl, ,tw},
NT is a set of non-maximal types {T1,...,T1&apos;},
</bodyText>
<footnote confidence="0.994555">
3 Each rule of a PCFG also specifies a total ordering
over the nodes which it introduces, but the training
algorithm does not rely on this fact
4We follow Carpenter&apos;s convention for types. The
bottom node is the one containing no information, and
the maximal nodes are the ones containing the maxi-
</footnote>
<page confidence="0.993138">
84
</page>
<figure confidence="0.973048636363636">
s --, np vp
np —* np-sing I np-pl
vp --* vp-sing I vp-pl
bike np-sing bus np-sing
car np-sing cat np-sing
lorry np-sing
bikes np-pl buses np-pl
cars np-pl cats np-pl
lorries np-pl
stops vp-sing crosses vp-sing
stop vp-pl cross vp-pl
</figure>
<figureCaption confidence="0.98544825">
Figure 1: A simple grammar
car stops bus stops lorries stop
bikes stop cats cross
Figure 2 A simple corpus
</figureCaption>
<equation confidence="0.9975068">
P(np vpls) 1.0
P(np-singinp) = 0.45
P(np-plinp) =_ 0.55
P (vp-sing I vp) = 0.45
P(vp-plivp) = 0.55
</equation>
<figureCaption confidence="0.995799">
Figure 3: The results of training a PCFG
</figureCaption>
<bodyText confidence="0.872921909090909">
bot sub [sign,num] .
sign sub Esentence,phrase] .
sentence sub 0
intro [left :np,right:vp] .
phrase sub [np,vp7
intro [nuxn:num] .
np sub 0.
vp sub 0 .
num sub Csing,p1] .
sing sub 0.
pl sub 0.
</bodyText>
<figureCaption confidence="0.976293">
Figure 4: An ALE signature
</figureCaption>
<page confidence="0.997949">
85
</page>
<bodyText confidence="0.998349951219512">
NTi is the starting symbol and / is a set of in-
troduction relationships of the form (T2 T3)
where V is a multiset of maximal and non-
maximal types. Each introduction relationship
has a probability P((Tt T3) k) and the
probabilities for all the introduction relationships
that apply to a given non-maximal type must sum
to one.
As things stand this definition is nearly isomor-
phic to that given for PCFGs, with the major dif-
ferences being two changes which move us from
rules to introduction relationships. Firstly, we
relax the stipulation that the items on the right
hand side of the rules are strings, allowing them
instead to be multisets. Secondly, we introduce an
additional term in the head of introduction rules
to signal the fact that when we apply a partic-
ular introduction relationship to a node we also
specialize the type of the node by picking exactly
one of the direct subtypes of its current type. °Fi-
nally, we need to deal with the case where T.1 is
non-maximal. This is simply achieved by defin-
ing the iterated introduction relationships from r
as being those corresponding to. the chains of in-
troduction relationships from T&apos; which refine the
type to a maximal type. In the probabilistic type
hierarchy, it is the iterated introduction relation-
ships which correspond to the context-free rewrite
rules of a PCFG. A useful side-effect of this is that
we can preserve the invariant that all types except
those at the fringe of the structure are maximal.
The hierarchy whose ALE syntax is given in
figure 4 is captured in the new notation by figure 5
We associate probabilities with feature struc-
tures, which are sets of maximal and non-maximal
nodes generated by beginning from the start-
ing node and successively expanding non-maximal
leaves of the partial tree. Maximally specified fea-
ture structures are those feature structures which
have only maximal leaves. Probabilities are as-
signed by the following inductive definition:
</bodyText>
<listItem confidence="0.996358571428571">
• P(NTi) = 1.
• If F is a feature structure, and F&apos; is a partial
feature structure which differs from it only
in that a single non-maximal node NTk of
type Tok in F has been refined to type Tik
expanded to r&apos; in F&apos;, then P(F&apos;) = P(F)x
P ((TO = Ti) —&gt;
</listItem>
<bodyText confidence="0.996999909090909">
Modulo notation, this definition is identical to
the one given earlier for PCFGs. Given the corre-
spondence between the definitions of a PTH and
a PCFG it should be apparent that the training
methods which apply to one can equally be used
with the other. We will shortly provide an exam-
ple. Because we have not yet treated the crucial
matter of re-entrancy, it would be inappropriate
to call what we so far have stochastic HPSG, so
we refer to it as stochastic HPSG-
mum amounts of information possible.
</bodyText>
<subsectionHeader confidence="0.5547245">
4.1 Using stochastic HPSG– with the
corpus
</subsectionHeader>
<bodyText confidence="0.996991666666666">
Using the hierarchy in figure 4 the analyses of the
five sentences from figure 2 are as in figure 6.
Training is a matter of counting the transitions
which are found the observed results, then us-
ing counts to refine initial estimates of the prob-
abilities of particular transitions. This is entirely
analogous to what went on with PCFGs. The re-
sults of training are essentially identical to those
given earlier, with the optimal assignment being
as shown in figure 7. At this point we have pro-
vided a system which allows us to use feature
structures instead of PCFGs, but we have not
yet dealt with the question of re-entrancy, which
forms a crucial part of the expressive power of
typed feature structures. We will return to this
shortly, but first we consider the detailed implica-
tions of what we have done so far. The similarities
between these results and those in figure 3
</bodyText>
<listItem confidence="0.981396142857143">
• VVe still model the distribution observed in
the corpus by assuming two independent de-
cisions.
• We still get a strange ranking of the parses,
which favours number disagreement ,in spite
of the fact that the grammar which generated
the corpus enforces number agreement.
</listItem>
<bodyText confidence="0.991314">
The differences between these results and the ear-
lier ones are:
</bodyText>
<listItem confidence="0.98598285">
• The hierarchy uses bot rather than s as its
start symbol. The probabilities tell us that
the corpus contains no free-standing struc-
tures of type num.
• The zero probability of
sign = phrase
codifies a similar observation that there are
no free-standing structures with type phrase.
• Since items of type phrase are never intro-
duced at that type, but only in the form
of sub-types, there are no transitions from
phrase in the corpus. Therefore the initial
estimates of the probabilities of such transi-
tions are unaffected by training.
• In the PCFG the symmetry between the ex-
pansions of np and vp to singular and plural
variants is implicit, whereas in the PTH the
distribution of singular and plural variants is
encoded at a single location, namely that at
which num is refined.
</listItem>
<bodyText confidence="0.999768888888889">
The independence assumption which is built
into the training algorithm is that types are to be
refined according to the same probability distribu-
tion irrespective of the context in which they are
expanded. We have already seen a consequence of
this: the PTH lumps together all occasions where
num is expanded, irrespective of whether the en-
closing context is np or vp. For the moment we
are prepared to tolerate this because:
</bodyText>
<page confidence="0.910594">
86
</page>
<equation confidence="0.989612727272727">
MT = {sentence, np, vp, sing, pl}
NT = {bot, sign, phrase, num}
NTi = bot
/ = {(bot = sign)
(bot = num) --+
(sign = sentence) [np, vp]
(sign phrase) [num]
(phrase = np)
(phrase = vp) —+
(num z sing)
(num pl) 01
</equation>
<figureCaption confidence="0.997265">
Figure 5: A more formal version of the simple hierarchy
</figureCaption>
<table confidence="0.997629210526316">
LEFT [NUM sing]
RIGHT np
[NUM sing]
vp
vp
(2 occurrences)
[LEFT
RIGHT
vp
(3 occurrences).
Figure 6: Analyses of the corpus using the ALE-hierarchy
P(bot sign) = 1.0
P(bot num) = 0.0
P(sign sentence) = 1.0
P(sign phrase) = 0.0
P(num sing) = 0.45
P(num pl) = 0.55
P(phrase np) = A
P(phrase vp) = 1 — A
</table>
<figureCaption confidence="0.991625">
Figure 7: The results of training the probabilistic type hierarchy
</figureCaption>
<listItem confidence="0.968302230769231">
• Clarity: The decisions which we have made
lead to a system with a clear probabilistic se-
mantics.
• Trainability: the number of parameters
which must be estimated for a grammar is a
linear function of the size of the type hierar-
chy
• Easy extensibility: There is a clear route
to a more finely grained account if we allow
the expansion probabilities to be conditioned
on surrounding context. This would increase
the number of parameters to be estimated,
which may or may not prove to be a problem.
</listItem>
<sectionHeader confidence="0.892737" genericHeader="method">
5 Adding re-entrancies
</sectionHeader>
<bodyText confidence="0.989484941176471">
We now turn to an extension of the system which
takes proper account of re-entrancies in the struc-
ture. The essence of our approach is to define
a stochastic procedure which simultaneously ex-
pands the nodes of the tree in the way outlined
above and guesses the pattern of re-entrancies
which relate them. It pays to stipulate that the
structures which we build are fully inequated in
the sense defined by Carpenter (Carpenter, 1992,
p120).
The essential insight is that the choice of a
fully inequated feature structure involving a set
of nodes is the same thing as the choice of an
arbitrary equivalence relation over these nodes,
and this is in turn equivalent to the choice of a
partition of the set of nodes into a set of non-
empty sets. These sets of nodes are equivalence
classes. The standard recursive procedure for gen-
erating partitions of k + 1 elements is to non-
deterministically add the k lthq node to each
of the equivalence classes of each of the partitions
of k nodes, and also to nondeterministically con-
sider the new node as a singleton set. The basis
of the stochastic procedure for generating fully-
inequated feature structures is to interleave the
generation of equivalence classes with the expan-
sion from the initial node as described above.
For the purposes of the expansion algorithm, a
fully inequated feature structure consists of a fea-
ture tree (as before) and an equivalence relation5
over all the maximal nodes in that tree. The task
of the algorithm is to generate all such structures
and to equip them with probabilities. We proceed
as in the case without re-entrancy, except that we
only ever expand sub-trees in the case where the
new node begins a new equivalence class. This
avoids the double counting which was a problem
earlier.
The remaining task is that of assigning scores to
equivalence relations. We do not have a fully sat-
5Since maximal types are mutually inconsistent,
this equivalence relation can be efficiently represented
by a associating a separate partition with each maxi-
mal type
isfactory solution to this problem. The reason for
this is that we would ideally like to assign prob-
abilities to intermediate structures in such a way
that the probabilities of fully expanded structures
are independent of the route by which they were
arrived at. This can be done, and the method
which we adopt has the merit of simplicity.
</bodyText>
<subsectionHeader confidence="0.98444">
5.1 Scoring re-entrancies
</subsectionHeader>
<bodyText confidence="0.999245428571429">
We associate a single probabilistic parameter
P(T—) with each type T, and derive the probabil-
ity of the structure in which a particular pairwise
equation of-nodes in type T have been equated
by multiplying the probability of the structure
in which no decision has been made by P(T).
We derive the probability of the corresponding in-
equated structure by multiplying by 1 — P(T=) in
an entirely analogous way. This ensures that the
probabilities of the equated and inequated exten-
sions of the original structure sum to the origi-
nal probability. The cost is a deficiency in mod-
elling, since this takes no account of the fact that
token identity of nodes is transitive, which are
generated. As things stand the stochastic proce-
dure is free to generate structures where ni n2,
n3 but n1 n3, which are not in fact legal
feature structures. This leads to distortions of the
probability estimates since the training algorithm
spends part of its probability mass on impossible
structures.
</bodyText>
<subsectionHeader confidence="0.964524">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.99998448">
Even a crude account of re-entrancy is better than
completely ignoring the issue, and the one pro-
posed gets the right result for cases of double
counting such as those discussed above, but it
should be obvious that there is room for improve-
ment in the treatment which we provide. Intu-
itively what is required is a parametrisable means
of distributing probability mass among the dis-
tinct equivalence relations which extend the cur-
rent structure. One attractive possibility would be
to enumerate the relations which can be obtained
by adding the current node to the various differ-
ent equivalence classes which are available, apply
some scoring function to each class, and then nor-
malize such that the total score over all alterna-
tives is one. But this might introduce unpleas-
ant dependencies of the probabilities of feature
structures on the order in which the stochastic
procedure chooses to expand nodes, because the
normalisation is carried out before we have full
knowledge of the equivalence classes with which
the current node might become associated. It may
be that an appropriate choice of scoring function
will circumvent this difficulty, but this is left as a
matter for further research.
</bodyText>
<page confidence="0.99844">
88
</page>
<sectionHeader confidence="0.999261" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999946466666667">
We have presented two proposals for the associa-
tion of probabilities with typed feature-structures
of the form used in HPSG. As far as we know these
are the most detailed of their type, and the ones
which are most likely to be able to exploit stan-
dard training and parsing algorithms. For typed
feature structures lacking re-entrancy we believe
our proposal to be the simplest and most natural
which is available. The proposal for dealing with
re-entrancy is less satisfactory but offers a basis
for empirical exploration, and has definite advan-
tages over the straightforward use of PCFGs. We
plan to follow up the current work by training and
testing a suitable instantiation of our framework
against manually annotated corpora.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9984035">
I acknowledge the support of the Language Tech-
nology Group of the Human Communication Re-
search Centre, which is a UK ESRC funded insti-
tution.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999788521739131">
Bob Carpenter. 1992. The Logic of Typed Fea-
ture Structures. Cambridge Tracts in Theoreti-
cal Computer Science. CUP. With Applications
to Unification Grammars, Logic Programs and
Constraint Resolution.
Bob Carpenter, 1993. ALE. The Attribute Logic
Engine user&apos;s guide, version 0. Carnegie Mel-
lon University, Pittsburgh, Pa., Laboratory for
Computational Linguistics, MAY.
Eugene Charniak. 1993. Statistical Language
Learning. The MIT Press.
Albert Kim. 1994. Graded unification: A frame-
work for interactive processing. In Proceedings
of the 32nd Annual Meeting of the Association
for Computational Linguistics, pages 313-315,
June.
C.S. Mellish. 1988. Implementing systemic classi-
fication by unification. Computational Linguis-
tics, 14(440-51. Winter.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. CSLI and
University of Chicago Press, Stanford, Ca. and
Chicago, Ill.
</reference>
<page confidence="0.999753">
89
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.126153">
<title confidence="0.999614">Stochastic HPSG</title>
<author confidence="0.999967">Chris Brew</author>
<affiliation confidence="0.999215">Language Technology Group HCRC, University of Edinburgh</affiliation>
<address confidence="0.799883666666667">2 Buccleuch Place • Edinburgh EHS 9LW Scotland, UK</address>
<email confidence="0.270616">.BrewOedinburgh.ac.uk</email>
<abstract confidence="0.989482916666667">In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag. We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures, then provide an extended interpretation which allows them. We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>The Logic of Typed Feature Structures. Cambridge Tracts</title>
<date>1992</date>
<booktitle>in Theoretical Computer Science. CUP. With Applications to Unification Grammars, Logic Programs and Constraint Resolution.</booktitle>
<contexts>
<context position="837" citStr="Carpenter, 1992" startWordPosition="131" endWordPosition="132">interpretation for typed feature structures very similar to those used by Pollard and Sag. We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures, then provide an extended interpretation which allows them. We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora. 1 Introduction The purpose of our paper is to develop a principled technique for attaching a probabilistic interpretation to feature structures. Our techniques apply to the feature structures described by Carpenter (Carpenter, 1992). Since these structures are the ones which are used in by Pollard and Sag (Pollard and Sag, 1994) their relevance to computational grammars is apparent. On the basis of the usefulness of probabilistic context-free grammars (Charniak, 1993, ch. 5), it is plausible to assume that that the extension of probabilistic techniques to such structures will allow the application of known and new techniques of parse ranking and grammar induction to more interesting grammars than has hitherto been the case. The paper is structured as follows. We start by reviewing the training and use of probabilistic co</context>
<context position="16390" citStr="Carpenter, 1992" startWordPosition="2837" endWordPosition="2838">robabilities to be conditioned on surrounding context. This would increase the number of parameters to be estimated, which may or may not prove to be a problem. 5 Adding re-entrancies We now turn to an extension of the system which takes proper account of re-entrancies in the structure. The essence of our approach is to define a stochastic procedure which simultaneously expands the nodes of the tree in the way outlined above and guesses the pattern of re-entrancies which relate them. It pays to stipulate that the structures which we build are fully inequated in the sense defined by Carpenter (Carpenter, 1992, p120). The essential insight is that the choice of a fully inequated feature structure involving a set of nodes is the same thing as the choice of an arbitrary equivalence relation over these nodes, and this is in turn equivalent to the choice of a partition of the set of nodes into a set of nonempty sets. These sets of nodes are equivalence classes. The standard recursive procedure for generating partitions of k + 1 elements is to nondeterministically add the k lthq node to each of the equivalence classes of each of the partitions of k nodes, and also to nondeterministically consider the ne</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Bob Carpenter. 1992. The Logic of Typed Feature Structures. Cambridge Tracts in Theoretical Computer Science. CUP. With Applications to Unification Grammars, Logic Programs and Constraint Resolution.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>ALE. The Attribute Logic Engine user&apos;s guide, version 0.</title>
<date>1993</date>
<institution>Carnegie Mellon University, Pittsburgh, Pa., Laboratory for Computational Linguistics,</institution>
<contexts>
<context position="6633" citStr="Carpenter, 1993" startWordPosition="1106" endWordPosition="1107">use the grammar has no means of enforcing number agreement, the system systematically prefers plurals to singulars, even when doing this will lead to agreement clashes. Thus &amp;quot;buses stop&amp;quot; has estimated 0.55 x 0.55 = 0.3025, &amp;quot;bus stop&amp;quot; and &amp;quot;buses stops&amp;quot; both have probability 0.55 x 0.45 = 0.2475 and &amp;quot;bus stops&amp;quot; has probability 0.45 x 0.45 --= 0.2025. This behaviour is clearly unmotivated by the corpus, and arises 2The most obvious case is prepositional-phrase attachment. purely because of the inadequacy of the probabilistic model. 3 Probabilistic type hierarchies ALE signatures Carpenter&apos;s ALE (Carpenter, 1993) allows the user to define the type hierarchy of a grammar by writing a collection of clauses which together denote an inheritance hierarchy, a set of features and a set of&apos; appropriateness conditions. An example of such a hierarchy is given in ALE syntax in figure 4. What the ALE signature tells us The inheritance information tells us that a sign is a forced choice between a sentence and a phrase, that a phrase is a forced choice between a noun-phrase (np) and a verb-phrase (vp) and that number values (num) are partitioned into singular (sing) and plural (pl). The features which are defined a</context>
</contexts>
<marker>Carpenter, 1993</marker>
<rawString>Bob Carpenter, 1993. ALE. The Attribute Logic Engine user&apos;s guide, version 0. Carnegie Mellon University, Pittsburgh, Pa., Laboratory for Computational Linguistics, MAY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="1076" citStr="Charniak, 1993" startWordPosition="169" endWordPosition="170">allows them. We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora. 1 Introduction The purpose of our paper is to develop a principled technique for attaching a probabilistic interpretation to feature structures. Our techniques apply to the feature structures described by Carpenter (Carpenter, 1992). Since these structures are the ones which are used in by Pollard and Sag (Pollard and Sag, 1994) their relevance to computational grammars is apparent. On the basis of the usefulness of probabilistic context-free grammars (Charniak, 1993, ch. 5), it is plausible to assume that that the extension of probabilistic techniques to such structures will allow the application of known and new techniques of parse ranking and grammar induction to more interesting grammars than has hitherto been the case. The paper is structured as follows. We start by reviewing the training and use of probabilistic context-free grammars (PCFGs). We then develop a technique to allow analogous probabilistic annotations on type hierarchies. This gives us a clear account of the relationship between a large class of feature structures and their probabilitie</context>
<context position="3847" citStr="Charniak, 1993" startWordPosition="644" endWordPosition="645">e those partial phrase markers which have no non-terminal leaves. Probabilities are assigned by the following inductive definition: • P(Ni) = 1. • If T is a partial phrase marker, and T&apos; is a partial phrase marker which differs from it only in that a single non-terminal node Nk in T has been expanded to (in in T&apos;, then P(T&apos;) = P(T) x P (Nk (t12). In this definition R acts as a specification of the accessibility relationships which can hold between nodes of the trees admitted by the grammar. The rule probabilities specify the cost of 1 Our description is closely based on that given by Charniak(Charniak, 1993, p. 52 ft) 83 making particular choices about the way in which the rules develop. It is going to turn out that an exactly analogous system of accessibility relations is present in the probabilistic type hierarchies which we define later. Limitations of PCFGs The definition of PCFGs implies that the probability of a phrase marker depends only on the choice of rules used in expanding non-terminal nodes. In particular, the probability does not depend on the order in which the rules are applied. This has the arguably unwelcome consequence that PCFGs are unable to make certain discriminations betw</context>
<context position="5116" citStr="Charniak, 1993" startWordPosition="853" endWordPosition="854">he models developed in this paper build in similar independence assumptions. A large part of the art of probabilistic language modelling resides in the management of the trade-off between descriptive power (which has the merit of allowing us to make the discriminations which we want) and independence assumptions (which have the merit of making training practical by allowing us to treat similar situations as equivalent). The crucial advantage of PCFGs over CFGs is that they can be trained and/or learned from corpora. Readers for whom this fact is unfamiliar are referred to Charniak&apos;s textbook (Charniak, 1993, Chapter 7). We do not have space to recapitulate the discussion of training which can be found there. We do however illustrate the outcome of training. 2.1 Applying a PCFG to a simple corpus Consider the simple grammar in figure 1 and its training against the corpus in figure 2. Since there are 3 plural sentences and only 2 singular sentences, the optimal set of parameters will reflect the distribution found in the corpus, as shown in figure 3 One might have hoped that the ratio P(np-singinp)/P(np-plinp) would be 2/3, but it is instead V2/3. This is a consequence of the assumption of indepen</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>Eugene Charniak. 1993. Statistical Language Learning. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Kim</author>
</authors>
<title>Graded unification: A framework for interactive processing.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>313--315</pages>
<contexts>
<context position="1872" citStr="Kim, 1994" startWordPosition="298" endWordPosition="299">mmar induction to more interesting grammars than has hitherto been the case. The paper is structured as follows. We start by reviewing the training and use of probabilistic context-free grammars (PCFGs). We then develop a technique to allow analogous probabilistic annotations on type hierarchies. This gives us a clear account of the relationship between a large class of feature structures and their probabilities, but does not treat re-entrancy. We conclude by sketching a technique which does treat such structures. While we know of previous work which associates scores with feature structures (Kim, 1994) are not aware of any previous treatment which makes explicit the link to classical probability theory. We take a slightly unconventional perspective on feature structures, because it is easier to cast our theory within the more general framework of incremental description refinement (Mellish, 1988) than to exploit the usual metaphors of constraint-based grammar. In fact we can afford to remain entirely agnostic about the means by which the HPSG grammar associates signs with linguistic strings, because all that we need in order to train our stochastic procedures is a corpus of signs which are </context>
</contexts>
<marker>Kim, 1994</marker>
<rawString>Albert Kim. 1994. Graded unification: A framework for interactive processing. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 313-315, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Mellish</author>
</authors>
<title>Implementing systemic classification by unification.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<pages>14--440</pages>
<publisher>Winter.</publisher>
<contexts>
<context position="2172" citStr="Mellish, 1988" startWordPosition="343" endWordPosition="344">. This gives us a clear account of the relationship between a large class of feature structures and their probabilities, but does not treat re-entrancy. We conclude by sketching a technique which does treat such structures. While we know of previous work which associates scores with feature structures (Kim, 1994) are not aware of any previous treatment which makes explicit the link to classical probability theory. We take a slightly unconventional perspective on feature structures, because it is easier to cast our theory within the more general framework of incremental description refinement (Mellish, 1988) than to exploit the usual metaphors of constraint-based grammar. In fact we can afford to remain entirely agnostic about the means by which the HPSG grammar associates signs with linguistic strings, because all that we need in order to train our stochastic procedures is a corpus of signs which are known to be valid descriptions of strings. 2 Probabilistic interpretation of PCFGs We review the standard probabilistic interpretation of PCFGs&apos;. A PCFG is a four-tuple &lt; W, N, N1, R &gt; , where W is a set of terminal symbols {w1, , N is a set of non-terminal symbols {N1, , Nv}, N1 is the starting sym</context>
</contexts>
<marker>Mellish, 1988</marker>
<rawString>C.S. Mellish. 1988. Implementing systemic classification by unification. Computational Linguistics, 14(440-51. Winter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<date>1994</date>
<booktitle>HeadDriven Phrase Structure Grammar. CSLI</booktitle>
<institution>and University of Chicago Press,</institution>
<location>Stanford, Ca. and Chicago, Ill.</location>
<contexts>
<context position="935" citStr="Pollard and Sag, 1994" startWordPosition="147" endWordPosition="150">e begin with a version of the interpretation which lacks a treatment of re-entrant feature structures, then provide an extended interpretation which allows them. We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora. 1 Introduction The purpose of our paper is to develop a principled technique for attaching a probabilistic interpretation to feature structures. Our techniques apply to the feature structures described by Carpenter (Carpenter, 1992). Since these structures are the ones which are used in by Pollard and Sag (Pollard and Sag, 1994) their relevance to computational grammars is apparent. On the basis of the usefulness of probabilistic context-free grammars (Charniak, 1993, ch. 5), it is plausible to assume that that the extension of probabilistic techniques to such structures will allow the application of known and new techniques of parse ranking and grammar induction to more interesting grammars than has hitherto been the case. The paper is structured as follows. We start by reviewing the training and use of probabilistic context-free grammars (PCFGs). We then develop a technique to allow analogous probabilistic annotati</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. HeadDriven Phrase Structure Grammar. CSLI and University of Chicago Press, Stanford, Ca. and Chicago, Ill.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>