<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.720758">
A Joint Model for Quotation Attribution and Coreference Resolution
Mariana S. C. Almeida*† Miguel B. Almeida*† Andr´e F. T. Martins*†
</title>
<author confidence="0.772569">
*Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
</author>
<affiliation confidence="0.80974">
†Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, 1049-001 Lisboa, Portugal
</affiliation>
<email confidence="0.977313">
{mla,mba,atm}@priberam.pt
</email>
<sectionHeader confidence="0.993175" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976473684211">
We address the problem of automatically
attributing quotations to speakers, which
has great relevance in text mining and me-
dia monitoring applications. While cur-
rent systems report high accuracies for
this task, they either work at mention-
level (getting credit for detecting uninfor-
mative mentions such as pronouns), or as-
sume the coreferent mentions have been
detected beforehand; the inaccuracies in
this preprocessing step may lead to error
propagation. In this paper, we introduce a
joint model for entity-level quotation attri-
bution and coreference resolution, exploit-
ing correlations between the two tasks. We
design an evaluation metric for attribu-
tion that captures all speakers’ mentions.
We present results showing that both tasks
benefit from being treated jointly.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99585028125">
Quotations are a crucial part of news stories, giv-
ing the perspectives of the participants in the nar-
rated event, and making the news sound objective.
The ability of extracting and organizing these quo-
tations is highly relevant for text mining applica-
tions, as it may aid journalists in fact-checking,
help users browse news threads, and reduce human
intervention in media monitoring. This involves
assigning the correct speaker to each quote—a
problem called quotation attribution (§2).
There is significant literature devoted to this
task, both for narrative genres (Mamede and
Chaleira, 2004; Elson and McKeown, 2010) and
newswire domains (Pouliquen et al., 2007; Sar-
mento et al., 2009; Schneider et al., 2010). While
the earliest works focused on devising lexical and
syntactic rules and hand-crafting grammars, there
has been a recent shift toward machine learning
approaches (Fernandes et al., 2011; O’Keefe et al.,
2012; Pareti et al., 2013), with latest works re-
porting high accuracies for speaker identification
in newswire (in the range 80–95% for direct and
mixed quotes, according to O’Keefe et al. (2012)).
Despite these encouraging results, quotation min-
ing systems are not yet fully satisfactory, even
when only direct quotes are considered. Part of
the problem, as we next describe, has to do with
inaccuracies in coreference resolution (§3).
The “easiest” instances of quotation attribution
problems arise when the speaker and the quote are
semantically connected, e.g., through a reported
speech verb like said. However, in newswire text,
the subject of this verb is commonly a pronoun or
another uninformative anaphoric mention. While
the speaker thus determined may well be correct—
being in most cases consistent with human annota-
tion choices (Pareti, 2012)—from a practical per-
spective, it will be of little use without a corefer-
ence system that correctly resolves the anaphora.
Since the current state of the art in coreference res-
olution is far from perfect, errors at this stage tend
to propagate to the quote attribution system.
Consider the following examples for illustration
(taken from the WSJ-1057 and WSJ-0089 docu-
ments in the Penn Treebank), where we have an-
notated with subscripts some of the mentions:
(a) Rivals carp at “the principle of [Pilson]M,,”
as [NBC’s Arthur Watson]M2 once put it –
“[he]M3’s always expounding that rights are
too high, then [he]M4’s going crazy.” But [the
49-year-old Mr. Pilson]M5 is hardly a man to
ignore the numbers.
(b) [English novelist Dorothy L. Sayers]M, de-
scribed [ringing]M2 as a “passion that finds its
satisfaction in [mathematical completeness]M3
and [mechanical perfection]M4.” [Ringers]M5,
[she]M6 added, are “filled with the solemn intox-
ication that comes of intricate ritual faultlessly
performed.”
In example (a), the pronoun coreference system
used by O’Keefe et al. (2012) erroneously clus-
ters together mentions M2, M3 and M4 (instead
of the correct clustering {M1, M3, M4}). Since it
is unlikely that the speaker is co-referent to a third-
</bodyText>
<page confidence="0.993293">
39
</page>
<note confidence="0.992947">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 39–48,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999969769230769">
person pronoun he inside the quote, a pipeline sys-
tem would likely attribute (incorrectly) this quote
to Pilson. In example (b), there are two quotes
with the same speaker entity (as indicated by the
cue she added). This gives evidence that M1 and
M6 should be coreferent. A pipeline approach
would not be able to exploit these correlations.
We argue that this type of mistakes, among
others, can be prevented by a system that per-
forms quote attribution and coreference resolution
jointly (§4). Our joint model is inspired by re-
cent work in coreference resolution that indepen-
dently ranks the possible mention’s antecedents,
forming a latent coreference tree structure (Denis
and Baldridge, 2008; Fernandes et al., 2012; Dur-
rett et al., 2013; Durrett and Klein, 2013). We con-
sider a generalization of these structures which we
call a quotation-coreference tree. To effectively
couple the two tasks, we need to go beyond simple
arc-factored models and consider paths in the tree.
We formulate the resulting problem as a logic pro-
gram, which we tackle using a dual decomposition
strategy (§5). We provide an empirical compari-
son between our method and baselines for each of
the tasks and a pipeline system, defining suitable
metrics for entity-level quotation attribution (§6).
</bodyText>
<sectionHeader confidence="0.948223" genericHeader="introduction">
2 Quotation Attribution
</sectionHeader>
<bodyText confidence="0.999703136363637">
The task of quotation attribution can be formally
defined as follows. Given a document containing
a sequence of quotations, (q1, ... , qL), and a set
of candidate speakers, {s1, ... , sm}, the goal is to
a assign a speaker to every quote.
Previous work has handled direct and mixed
quotations (Sarmento et al., 2009; O’Keefe et al.,
2012), easily extractable with regular expressions
for detecting quotation marks, as well as indirect
quotations (Pareti et al., 2013), which are more in-
volved and require syntactic or semantic patterns.
In this work, we resort to direct and mixed quo-
tations. Pareti (2012) defines quotation attribu-
tions in terms of their content span (the quotation
text itself), their cue (a lexical anchor of the attri-
bution relation, such as a reported speech verb),
and the source span (the author of the quote).
The same reference introduced the PARC dataset,
which we use in our experiments (§6) and which
is based on the annotation of a database of attribu-
tion relations from the Penn Discourse Treebank
(Prasad et al., 2008). Several machine learning
algorithms have been applied to this task, either
framing the problem as classification (an indepen-
dent decision for each quote), or sequence label-
ing (using greedy methods or linear-chain condi-
tional random fields); see O’Keefe et al. (2012)
for a comparison among these different methods.
In this paper, we distinguish between mention-
level quotation attribution, in which the candi-
date speakers are individual mentions, and entity-
level quotation attribution, in which they are en-
tity clusters comprised of one or more mentions.
With this distinction, we attempt to clarify how
prior work has addressed this task, and design suit-
able baselines and evaluation metrics. For exam-
ple, O’Keefe et al. (2012) applies a coreference
resolver before quotation attribution, whereas de
La Clergerie et al. (2011) does it afterwards, as a
post-processing stage. An important issue when
evaluating quotation attribution systems is to pre-
vent them from getting credit for detecting unin-
formative speakers such as pronouns; we will get
back to this topic in §6.2.
</bodyText>
<sectionHeader confidence="0.976811" genericHeader="method">
3 Coreference Resolution
</sectionHeader>
<bodyText confidence="0.999526857142857">
In coreference resolution, we are given a set of
mentions M :_ {m1, ... , mK}, and the goal
is to cluster them into discourse entities, E :_
{e1,... , ei}, where each ej ⊆ M and ej _� ∅.
We follow Haghighi and Klein (2007) and distin-
guish between proper, nominal, and pronominal
mentions. Each requires different types of infor-
mation to be resolved. Thus, the task involves de-
termining anaphoricity, resolving pronouns, and
identifying semantic compatibility among men-
tions. To resolve these references, one typically
exploits contextual and grammatical clues, as well
as semantic information and world knowledge,
to understand whether mentions refer to people,
places, organizations, and so on. The importance
of coreference resolution has led to it being the
subject of recent CoNLL shared tasks (Pradhan et
al., 2011; Pradhan et al., 2012).
There has been a variety of approaches for
this problem. Early work used local discrimina-
tive classifiers, making independent decisions for
each mention or pair of mentions (Soon et al.,
2001; Ng and Cardie, 2002). Lee et al. (2011)
proposed a competitive non-learned sieve-based
method, which constructs clusters by aglomerat-
ing mentions in a greedy manner. Entity-centric
models define scores for the entire entity clusters
(Culotta et al., 2007; Haghighi and Klein, 2010;
</bodyText>
<page confidence="0.996846">
40
</page>
<bodyText confidence="0.9997035">
Rahman and Ng, 2011) and seek the set of enti-
ties that optimize the sum of scores; this can also
be promoted in a decentralized manner (Durrett et
al., 2013). Pairwise models (Bengtson and Roth,
2008; Finkel et al., 2008; Versley et al., 2008), on
the other hand, define scores for each pair of men-
tions to be coreferent, and define the clusters as
the transitive closure of these pairwise relations.
A disadvantage of these two methods is that they
lead to intractable decoding problems, so approx-
imate methods must be used. For comprehensive
overviews, see Stoyanov et al. (2009), Ng (2010),
Pradhan et al. (2011) and Pradhan et al. (2012).
Our joint approach (to be fully described in
§4) draws inspiration from recent work that shifts
from entity clusters to coreference trees (Fernan-
des et al., 2012; Durrett and Klein, 2013). These
models define scores for each mention to link to
its antecedent or to an artifical root symbol $ (in
which case it is not anaphoric). The computation
of the best tree can be done exactly with spanning
tree algorithms, or by independently choosing the
best antecedent (or the root) for each mention, if
only left-to-right arcs are allowed. The same idea
underlies the antecedent ranking approach of De-
nis and Baldridge (2008). Once the coreference
tree is computed, the set of entity clusters E is ob-
tained by associating each entity set to a branch of
the tree coming out from the root. This is illus-
trated in Figure 1 (left).
</bodyText>
<sectionHeader confidence="0.897192" genericHeader="method">
4 Joint Quotations and Coreferences
</sectionHeader>
<bodyText confidence="0.999225444444444">
In this work, we propose that quotation attribu-
tion and coreference resolution are solved jointly
by treating both mentions and quotations as nodes
in a generalized structure called a quotation-
coreference tree (Figure 1, right). The joint sys-
tem’s decoding process consists in creating such
a tree, from which a clustering of the nodes can
be immediatelly obtained. The clustering is inter-
preted as follows:
</bodyText>
<listItem confidence="0.965768">
• All mention nodes in the cluster are coreferent,
thus they describe one single entity (just like in
a standard coreference tree).
• Quotation nodes that appear together with those
mentions in a cluster will be assigned that entity
as the speaker.
</listItem>
<bodyText confidence="0.992342642857143">
For example, in Figure 1 (right), the en-
tity Dorothy L. Sayers (formed by mentions
{M1, M6}) is assigned as the speaker of quota-
tions Q1 and Q2. We forbid arcs between quotes
and from a quote to a mention, effectively con-
straining the quotes to be leaves in the tree, with
mentions as parents.1 We force a tree with only
left-to-right arcs, by choosing a total ordering of
the nodes that places all the quotations in the right-
most positions (which implies that any arc con-
necting a mention to a quotation will point to the
right). The quotation-coreference tree is obtained
as the best spanning tree that maximizes a score
function, to be described next.
</bodyText>
<subsectionHeader confidence="0.986049">
4.1 Basic Model
</subsectionHeader>
<bodyText confidence="0.999994">
Our basic model is a feature-based linear model
which assigns a score to each candidate arc linking
two mentions (mention-mention arcs), or linking a
mention to a quote (mention-quotation arcs). Our
basic system is called QUOTEBEFORECOREF for
reasons we will detail in section 4.2.
</bodyText>
<subsectionHeader confidence="0.806602">
4.1.1 Coreference features
</subsectionHeader>
<bodyText confidence="0.99993675">
For the mention-mention arcs, we use the same
coreference features as the SURFACE model of the
Berkeley Coreference Resolution System (Durrett
and Klein, 2013), plus features for gender and
number obtained through the dataset of Bergsma
and Lin (2006). This is a very simple lexical-
driven model which achieves state-of-the-art re-
sults. The features are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.498208">
4.1.2 Quotation features
</subsectionHeader>
<bodyText confidence="0.9996841">
For the quote attribution features, we use features
inspired by O’Keefe et al. (2012), shown in Ta-
ble 2. The same set of features works for speakers
that are individual mentions (in the model just de-
scribed), and for speakers that are clusters of men-
tions (used in §6 for the baseline QUOTEAFTER-
COREF). These features include various distances
between the mention and the quote, the indication
of the speaker being inside the quote span, and var-
ious contextual features.
</bodyText>
<subsectionHeader confidence="0.875561">
4.2 Final Model
</subsectionHeader>
<bodyText confidence="0.9999826">
While the basic model just described puts quo-
tations and mentions together, it is not more ex-
pressive than having separate models for the two
tasks. In fact, if we just have scores for individual
arcs, the two problems are decoupled: the optimal
</bodyText>
<footnote confidence="0.850955">
1This is implemented by defining −∞ scores for all the
outgoing arcs in a quotation node, as well as incoming arcs
originating from the root.
</footnote>
<page confidence="0.998868">
41
</page>
<figureCaption confidence="0.9888426">
Figure 1: Left: A typical coreference tree for the text snippet in §1, example (b), with mentions M1 and
M6 clustered together and M2 and M3 left as singletons. Right: A quotation-coreference tree for the
same example. Mention nodes are depicted as green circles, and quotation nodes in shaded blue. The
dashed rectangle represents a branch of the tree, containing the entity cluster associated with the speaker
Dorothy L. Sayers, as well as the quotes she authored.
</figureCaption>
<bodyText confidence="0.365705">
Features on the child mention
</bodyText>
<equation confidence="0.195494833333333">
[ANAPHORIC (T/F)] + [CHILD HEAD WORD]
[ANAPHORIC (T/F)] + [CHILD FIRST WORD]
[ANAPHORIC (T/F)] + [CHILD LAST WORD]
[ANAPHORIC (T/F)] + [CHILD PRECEDING WORD]
[ANAPHORIC (T/F)] + [CHILD FOLLOWING WORD]
[ANAPHORIC (T/F)] + [CHILD LENGTH]
</equation>
<bodyText confidence="0.542581">
Features on the parent mention
</bodyText>
<sectionHeader confidence="0.9226388" genericHeader="method">
[PARENT HEAD WORD]
[PARENT FIRST WORD]
[PARENT LAST WORD]
[PARENT PRECEDING WORD]
[PARENT FOLLOWING WORD]
</sectionHeader>
<figure confidence="0.51696225">
[PARENT LENGTH]
[PARENT GENDER]
[PARENT NUMBER]
Features on the pair
[EXACT STRING MATCH (T/F)]
[HEAD MATCH (T/F)]
[SENTENCE DISTANCE, CAPPED AT 10]
[MENTION DISTANCE, CAPPED AT 10]
</figure>
<tableCaption confidence="0.899718">
Table 1: Coreference features, associated to each
</tableCaption>
<bodyText confidence="0.980512733333333">
candidate mention-mention arc in the tree. As in
Durrett and Klein (2013), we also include con-
junctions of each feature with the child and parent
mention types (proper, nominal, or, if pronominal,
the pronoun word).
quotation-coreference tree can be obtained by first
assigning the highest scored mention to each quo-
tation, and then building a standard coreference
tree involving only the mention nodes. This cor-
responds to the QUOTEBEFORECOREF baseline,
to be used in §6.
To go beyond separate models, we introduce
a final JOINT model, which includes additional
scores that depend not just on arcs, but also on
paths in the tree. Concretely, we select certain
</bodyText>
<subsectionHeader confidence="0.443292">
Features on the quote-speaker pair
</subsectionHeader>
<table confidence="0.972768909090909">
[WORD DISTANCE]
[SENTENCE DISTANCE]
[# IN-BETWEEN QUOTES]
[# IN-BETWEEN SPEAKERS]
[SPEAKER IN QUOTE, 1ST PERS. SG. PRONOUN (T/F)]
[SPEAKER IN QUOTE, 1ST PERS. PL. PRONOUN (T/F)]
[SPEAKER IN QUOTE, OTHER (T/F)]
Features on the speaker
[PREVIOUS WORD IS QUOTE (T/F)]
[PREVIOUS WORD IS SAME QUOTE (T/F)]
[PREVIOUS WORD IS ANOTHER QUOTE (T/F)]
[PREVIOUS WORD IS SPEAKER (T/F)]
[PREVIOUS WORD IS PUNCTUATION (T/F)]
[PREVIOUS WORD IS REPORTED SPEECH VERB (T/F)]
[PREVIOUS WORD IS VERB (T/F)]
[NEXT WORD IS QUOTE (T/F)]
[NEXT WORD IS SAME QUOTE (T/F)]
[NEXT WORD IS ANOTHER QUOTE (T/F)]
[NEXT WORD IS SPEAKER (T/F)]
[NEXT WORD IS PUNCTUATION (T/F)]
[NEXT WORD IS REPORTED SPEACH VERB (T/F)]
[NEXT WORD IS VERB (T/F)]
</table>
<tableCaption confidence="0.995445">
Table 2: Quotation attribution features, associ-
</tableCaption>
<bodyText confidence="0.938379928571429">
ated to each quote-speaker candidate. These
features are used in the QUOTEONLY, QUOTE-
BEFORECOREF, and JOINT systems (where the
speaker is a mention) and in the QUOTEAFTER-
COREF system (where the speaker is an entity).
pairs of nodes and introduce scores for the event
that both nodes are in the same branch of the tree.
Rather than doing this for all pairs—which es-
sentially would revert to the computationally de-
manding pairwise coreference models discussed
in §3—we focus on a small set of pairs that are
mostly related with the interaction between the
two tasks we address jointly. Namely, we consider
the mention-quotation pairs such that the mention
</bodyText>
<page confidence="0.996011">
42
</page>
<table confidence="0.859506428571429">
Mention-inside-quote features
[MENTION IS 1ST PERSON, SING. PRONOUN (T/F)]
[MENTION IS 1ST PERSON, PLUR. PRONOUN (T/F)]
[OTHER MENTION (T/F)]
Consecutive quote features
[DISTANCE IN NUMBER OF WORDS]
[DISTANCE IN NUMBER OF SENTENCES]
</table>
<tableCaption confidence="0.992304">
Table 3: Features used in the JOINT system for
</tableCaption>
<bodyText confidence="0.963174714285714">
mention-quote pairs (only for mentions inside
quotes) and for quote pairs (only for consecutive
quotes). These features are associated to pairs in
the same branch of the quotation-coreference tree.
span is within the quotation span (mention-inside-
quotation pairs), and pairs of quotations that ap-
pear consecutively in the document (consecutive-
quotation pairs). The idea is that, if consecutive
quotations appear on the same branch of the tree,
they will have the same speaker (the entity class
associated with that branch), even though they
are not necessarily siblings. These two pairs are
aligned with the motivating examples (a) and (b)
shown in §1.
</bodyText>
<subsubsectionHeader confidence="0.713157">
4.2.1 Mention-inside-quotation features
</subsubsectionHeader>
<bodyText confidence="0.999966090909091">
The top rows of Table 3 show the features we de-
fined for mentions inside quotes. The features in-
dicate whether the mention is first-person singular
pronominal (I, me, my, myself), which provides
strong evidence that it co-refers with the quotation
author, whether it is first-person plural pronominal
(we, us, our, ourselves), which provides a weaker
evidence (but sometimes works for colective enti-
ties that are organizations), and whether none of
the above happens—in which case, the speaker is
unlikely to be co-referent with the mention.
</bodyText>
<subsectionHeader confidence="0.526443">
4.2.2 Consecutive quotation features
</subsectionHeader>
<bodyText confidence="0.999974666666667">
We show our consecutive quote features in the bot-
tom rows of Table 3. We use only distance fea-
tures, measuring both distance in sentences and
in words, with binning. These simple features are
enough to capture the trend of consecutive quotes
that are close apart to have the same speaker.
</bodyText>
<sectionHeader confidence="0.941486" genericHeader="method">
5 Joint Decoding and Training
</sectionHeader>
<bodyText confidence="0.999971285714286">
While decoding in the basic model is easy—
as pointed out above, it can even be done
by running a mention-level quotation attribu-
tor and the coreference resolver independently
(QUOTEBEFORECOREF)—exact decoding with
the JOINT model is in general intractable, since
this model breaks the independence assumption
between the arcs. However, given the relatively
small amount of node pairs that have scores (only
mentions inside quotations and consecutive quota-
tions), we expect this “perturbation” to be small
enough not to affect the quality of an approxi-
mate decoder. The situation resembles other prob-
lems in NLP, such as non-projective dependency
parsing, which becomes intractable if higher order
interactions between the arcs are considered, but
can still be well approximated. Inspired by work
in parsing (Martins et al., 2009) using linear re-
laxations with multi-commodity flow models, we
propose a similar strategy by defining auxiliary
variables and coupling them in a logic program.
</bodyText>
<subsectionHeader confidence="0.955097">
5.1 Logic Formulation
</subsectionHeader>
<bodyText confidence="0.998927555555555">
We next derive the logic program for joint decod-
ing of coreferences and quotations. The input is a
set of nodes (including an artificial node), a set of
candidate arcs with scores, and a set of node pairs
with scores. To make the exposition lighter, we
index nodes by integers (starting by the root node
0) and we do not distinguish between mention and
quotation nodes. Only arcs from left to right are
allowed. The variables in our logic program are:
</bodyText>
<listItem confidence="0.958890111111111">
• Arc variables ai→j, which take the value 1 if
there is an arc from i to j, and 0 otherwise.
• Pair variables pi,j, which indicate that nodes i
and j are in the same branch of the tree.
• Path variables 7rj→∗k, indicating if there is a
path from j to k.
• Common ancestor variables Oi→∗j,k, indicating
that node i is a common ancestor of nodes j and
k in the tree.
Consistency among these variables is ensured by
the following set of constraints:
• Each node except the root has exactly one par-
ent:
j−1 i=0 ai→j = 1, bj =� 0 (1)
• There is a path from each node to itself:
7ri→∗i = 1, bi (2)
• There is a path from i to k iff there is some j
such that i is connected to j and there is path
</listItem>
<page confidence="0.998985">
43
</page>
<bodyText confidence="0.943389">
from j to k: (ai→j n πj→*k), bi, k (3) the best coreference trees consistent with the gold
�πi→*k = clustering using the pruner model, to eliminate the
i&lt;j≤k need of latent variables in the second stage.
</bodyText>
<listItem confidence="0.9494756">
• Node i is a common ancestor of k and ` iff there
is a path from i to k and from i to `:
ψi→*k,t = πi→*k n πi→*t, bi, k, ` (4)
• Nodes k and ` are in the same branch if they
have a common ancestor which is not the root:
</listItem>
<equation confidence="0.968404">
�pk,t = ψi→*k,t, bk,l. (5)
i6=w
</equation>
<bodyText confidence="0.99971075">
The objective to optimize is linear in the arc and
pair variables (hence the problem can be repre-
sented as an integer linear program by turning the
logical constraints into linear inequalities).
</bodyText>
<subsectionHeader confidence="0.986365">
5.2 Dual Decomposition
</subsectionHeader>
<bodyText confidence="0.999993583333333">
To decode, we employ the alternating direc-
tions dual decomposition algorithm (AD3), which
solves a relaxation of the ILP above. AD3 has
been used successfully in various NLP tasks, such
as dependency parsing (Martins et al., 2011; Mar-
tins et al., 2013), semantic role labeling (Das et al.,
2012), and compressive summarization (Almeida
and Martins, 2013). At test time, if the solution is
not integer, we apply a simple rounding procedure
to obtain an actual tree: for each node j, obtain
the antecedent (or root) i with the highest ai→j,
solving ties arbitrarily.
</bodyText>
<subsectionHeader confidence="0.995504">
5.3 Learning the Model
</subsectionHeader>
<bodyText confidence="0.999995647058823">
We train the joint model with the max-loss variant
of the MIRA algorithm (Crammer et al., 2006),
adapted to latent variables (we simply obtain the
best tree consistent with the gold clustering at each
step of MIRA, before doing cost-augmented de-
coding). The resulting algorithm is very similar
to the latent perceptron algorithm in Fernandes
et al. (2011), but it uses the aggressive stepsize
of MIRA. We set the same costs for coreference
mistakes as Durrett and Klein (2013), and a unit
cost for missing the correct speaker of a quota-
tion. For speeding up decoding, we first train a ba-
sic pruner for the coreference system (using only
the features described in §4.1.1), limiting the num-
ber of candidate antecedents to 10, and discarding
scores whose difference with respect to the best
antecedent is below a threshold. We also freeze
</bodyText>
<sectionHeader confidence="0.998628" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.946357">
6.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999979952380952">
We used the 597 documents of the Wall Street
Journal (WSJ) corpus that were disclosed for the
CoNLL-2011 coreference shared task (Pradhan
et al., 2011) as a dataset for coreference resolu-
tion. This dataset includes train, development and
test partitions, annotated with coreference infor-
mation, as well as gold and automatically gener-
ated syntactic and semantic information.
The CoNLL-2011 corpus does not contain an-
notations of quotation attribution. For that rea-
son, we used the WSJ quotation annotations in the
PARC dataset (Pareti, 2012). We used the same
version of the corpus as O’Keefe et al. (2012),
but with different splits, to match the dataset parti-
tions in the coreference resolution data. This attri-
bution corpus contains 279 documents of the 597
CoNLL-2011 files, having a total of 1199 anno-
tated quotes. As in that work, we only consid-
ered directed speech quotes and the direct part of
mixed quotes (quotes with both direct and undi-
rected speech).
</bodyText>
<subsectionHeader confidence="0.983081">
6.2 Metrics for quotation attribution
</subsectionHeader>
<bodyText confidence="0.999692777777778">
Previous evaluations of quotation attribution sys-
tems were designed at mention level, and are thus
assessed by comparing the predicted speaker men-
tion span with the gold one. This metric assesses
the amount of speaker mentions that were cor-
rectly identified. For compatibility with previous
assessments, we report this score, which we call
Exact Match (EM): this is the percentage of pre-
dicted speakers with the same span as the gold one.
However, for several quotations (about 30% in
the PARC corpus) this information is of little
value, since the gold mention is a pronoun, which
per se does not give any useful information about
the actual speaker entity. Considering this fact,
we propose two other metrics that capture infor-
mation at the entity level, reflecting the amount of
information a system is able to extract about the
speakers:
</bodyText>
<listItem confidence="0.991986333333333">
• Representative Speaker Match (RSM): for each
annotated quote, we obtain the full gold coref-
erence set of the gold annotated speaker, and
</listItem>
<page confidence="0.998593">
44
</page>
<bodyText confidence="0.9993717">
choose a representative speaker from that clus-
ter. We define this representative speaker as
the proper mention which is the closest to the
quote (if available); if the cluster does not con-
tain proper mentions, we use the closest nom-
inal mention; if only pronominal mentions are
available, we use the original annotated speaker.
The final measure is the percentage of predicted
speakers that match the string of the correspond-
ing representative speakers.
</bodyText>
<listItem confidence="0.9790016">
• Entity Cluster F1 (ECF1). Considering that a
system outputs a set of mentions coreferent to
the predicted speakers, we compute the F1 score
between the predicted set and the gold corefer-
ence cluster of the correct speaker.
</listItem>
<bodyText confidence="0.9998624">
The entity level metrics are not only useful for
assessing the quality of an quotation attribution
system—they also reflect the quality of the un-
derlying coreference system used to cluster the re-
lated mentions.
</bodyText>
<subsectionHeader confidence="0.999358">
6.3 Attribution baselines
</subsectionHeader>
<bodyText confidence="0.9999765">
To analyze the task of entity-level quotation attri-
bution, we implemented three baseline systems.
</bodyText>
<listItem confidence="0.833981777777778">
• QUOTEONLY: A quotation attribution system
trained on the representative speaker, instead of
the gold speaker. For fairness, this baseline was
trained with an extra feature indicating the type
of the mention (nominal, pronominal or proper).
• QUOTEAFTERCOREF: An attribution system
directly applied to the output of a predicted
coreference chain. This baseline uses a coref-
erence pre-processing, as applied in O’Keefe et
al. (2012).
• QUOTEBEFORECOREF: An attribution system
trained on the gold speaker, and post-combined
with the output of a coreference system. This
system should be able to provide a set of infor-
mative mentions about a quote, post-resolving
the problem of the pronominal speakers. This
kind of post-coreference approach was used by
de La Clergerie et al. (2011).
</listItem>
<subsectionHeader confidence="0.998319">
6.4 Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.9999115">
We use the coreference results of our basic
QUOTEBEFORECOREF system as a baseline for
coreference resolution. Since this system effec-
tively solves the two problems separately, this can
be considered our implementation of the SURFACE
system of Durrett and Klein (2013) . As reported
in Table 4, the perfromance of our baseline is
comparable with the one of the SURFACE system
of Durrett and Klein (2013), which is denoted as
SURFACE-DK-2013.2
Table 4 also show the CoNLL metrics obtained
for the proposed system of joint coreference reso-
lution and quotation attribution. Our joint system
outperformed the baseline with statistical signifi-
cance (with p &lt; 0.05 and according to a bootstrap
resampling test (Koehn, 2004)) for all metrics ex-
pect for the CEAFE F1 measure, whose value was
only slighty improved. These results confirm that
the coreference resolution task benefits for being
tackled jointly with quotation attribution.
</bodyText>
<subsectionHeader confidence="0.997375">
6.5 Quotation attribution
</subsectionHeader>
<bodyText confidence="0.9999606">
We implemented and trained the three attribution
systems that were described in §6.3 and the system
for joint coreference and author attribution that is
detailed in §4. For each system, Table 5 shows the
mention-based and entity-based metrics that were
described in §6.2.
Training a quotation attribution system using
representative speakers instead of the gold speak-
ers (QUOTEONLY) leads to rather disappointing
results. As expected, we conclude that assigning
the semantically related speaker is considerably
easier than selecting another mention that is coref-
erent with the correct speaker.
Using (predicted) coreference information,
both QUOTEAFTERCOREF and QUOTEBE-
FORECOREF systems considerably increase our
entity-based metrics. This was also expected,
since the coreference chain allows these baselines
to output a set of related mentions. We observed
that, using the coreference resolution clusters as
the attribution entity (QUOTEAFTERCOREF) in-
fluences the results negatively when compared to
a more basic system that runs coreference on top
of attribution result of the QUOTEONLY system
(QUOTEBEFORECOREF). These results indicate
that the quotation attribution task performs better
by looking at the speaker mention that connects
more strongly with the quotation, instead of trying
to match the whole cluster.
Finally, the scores achieved by our JOINT
</bodyText>
<footnote confidence="0.658854428571429">
2To make the systems comparable, we re-trained Durrett
et al.’s coreference system (version 0.9) on the WSJ portion
of the Ontonotes datasets (the portion which has quote anno-
tations from Pareti et al.’s PARC dataset). For this reason, the
values in Table 4 differ from those reported in Durrett and
Klein (2013), which where trained and tested in the entire
Ontonotes.
</footnote>
<page confidence="0.99723">
45
</page>
<table confidence="0.99903875">
MUC F1 BCUB F1 CEAFE F1 Avg.
SURFACE-DK-2013 58.87 62.74 45.46 55.7
SURFACE-OURS [QUOTEBEFORECOREF] 57.89 62.50 45.48 55.3
JOINT 58.78 63.79 45.50 56.0
</table>
<tableCaption confidence="0.990381">
Table 4: Coreference obtained with the CoNLL scorer (version 5) in the test partition of the WJS cor-
</tableCaption>
<bodyText confidence="0.612302">
pus, for the SURFACE system of Durrett and Klein (2013), our baseline implementation of the that sys-
tem (SURFACE-OURS), and our JOINT approach. All systems were trained in the WSJ portion of the
Ontonotes.
</bodyText>
<table confidence="0.9998074">
EM RSM ECF1
QUOTEONLY 49.1% 49.4% 41.2%
QUOTEAFTERCOREF 76.7% 64.6% 70.0%
QUOTEBEFORECOREF 88.7% 74.7% 73.7%
JOINT 88.1% 76.6% 74.1%
</table>
<tableCaption confidence="0.994447">
Table 5: Attribution results obtained, in the test
</tableCaption>
<bodyText confidence="0.950206055555556">
set, for the three baseline systems and our joint
system.
model are slightly above the best baseline sys-
tem QUOTEBEFORECOREF, yielding the best per-
formance on the entity-level quotation attribution
task. The differences, however, were not found
statistically significant, probably due to the small
number of quotes (159) in the test set.
The average decoding runtime of the JOINT
model is 1.6 sec. per document, against 0.2 sec.
for the pipeline system. This slowdown is ex-
pectable given the fact that the pipeline system
only needs to make independent decisions, while
the joint version needs to solve a harder combina-
torial problem. Yet, this runtime is within the or-
der of magnitude of the time necessary to prepro-
cess the documents (which includes tagging and
parsing the sentences).
</bodyText>
<subsectionHeader confidence="0.975793">
6.6 Error Analysis
</subsectionHeader>
<bodyText confidence="0.99993">
To understand the type of errors that are prevented
with the JOINT system, consider the following ex-
ample (from document WSJ-2428):
</bodyText>
<listItem confidence="0.9666016">
• [Robert Dow, a partner and portfolio manager
at Lord, Abbett &amp; Co.]M1, which manages $4
billion of high-yield bonds, says [he]M2 doesn’t
“think there is any fundamental economic ra-
tionale (for the junk bond rout). It was [herd
instinct]M3.” [He]M4 adds: “The junk market
has witnessed some trouble and now some peo-
ple think that if the equity market gets creamed
that means the economy will be terrible and
that’s bad for junk.”
</listItem>
<bodyText confidence="0.993269666666667">
The basic QUOTEBEFORECOREF system
wrongly clusters together M3 and M4 as corefer-
ent, and wrongly assigns M3 as the representative
speaker. On the other hand, the JOINT system
correctly clusters M1, M2 and M4 as coreferent.
This is due to the presence of the consecutive
quote features which aid in understanding that
both quotes have the same speaker, and the
mention-inside-quote features which prevent herd
instinct, which is inside a quote, from being
coreferent with He, which is very likely the author
of the quotes due to the verb adds.
</bodyText>
<sectionHeader confidence="0.999574" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999537">
We presented a framework for joint coreference
resolution and quotation attribution. We repre-
sented the problem as finding an optimal spanning
tree in a graph including both quotation nodes and
mention nodes. To couple the two tasks, we intro-
duce variables that look at paths in the tree, indi-
cating if pairs of nodes are in the same branch, and
we formulate decoding as a logic program. Each
branch from the root can then be interpreted as a
cluster containing all coreferent mentions of an en-
tity and all quotes from that entity.
In addition, we designed an evaluation metric
suitable for entity-level quotation attribution that
takes into account informative speakers. Experi-
mental results show mutual improvements in the
coreference resolution and quotation attribution
tasks.
Future work will include extensions to tackle in-
direct quotations, possibly exploring connections
to semantic role labeling.
</bodyText>
<sectionHeader confidence="0.996514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999768">
We thank all reviewers for their valuable com-
ments, and Silvia Pareti and Tim O’Keefe for pro-
viding us the PARC dataset and answering sev-
eral questions. This work was partially supported
by the EU/FEDER programme, QREN/POR Lis-
boa (Portugal), under the Intelligo project (con-
tract 2012/24803) and by a FCT grant PTDC/EEI-
SII/2312/2012.
</bodyText>
<page confidence="0.99915">
46
</page>
<sectionHeader confidence="0.988555" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997561581818182">
M. B. Almeida and A. F. T. Martins. 2013. Fast and ro-
bust compressive summarization with dual decom-
position and multi-task learning. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding the
value of features for coreference resolution. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 294–303. As-
sociation for Computational Linguistics.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 33–40.
Association for Computational Linguistics.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online Passive-Aggressive Al-
gorithms. Journal of Machine Learning Research,
7:551–585.
Aron Culotta, Michael Wick, Robert Hall, and An-
drew McCallum. 2007. First-order probabilistic
models for coreference resolution. In Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT/NAACL), pages 81–88.
D. Das, A. F. T. Martins, and N. A. Smith. 2012. An
Exact Dual Decomposition Algorithm for Shallow
Semantic Parsing with Constraints. In Proc. of First
Joint Conference on Lexical and Computational Se-
mantics (*SEM).
´Eric de La Clergerie, Benoit Sagot, Rosa Stern, Pas-
cal Denis, Ga¨elle Recourc´e, and Victor Mignot.
2011. Extracting and visualizing quotations from
news wires. In Human Language Technology. Chal-
lenges for Computer Science and Linguistics, pages
522–532. Springer.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 660–
669. Association for Computational Linguistics.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing.
Greg Durrett, David Hall, and Dan Klein. 2013.
Decentralized entity-level modeling for coreference
resolution. In Proc. of Annual Meeting of the Asso-
ciation for Computational Linguistics.
David K Elson and Kathleen McKeown. 2010. Auto-
matic attribution of quoted speech in literary narra-
tive. In AAAI.
William Paulo Ducca Fernandes, Eduardo Motta, and
Ruy Luiz Milidi´u. 2011. Quotation extraction for
portuguese. In Proceedings of the 8th Brazilian
Symposium in Information and Human Language
Technology (STIL 2011), Cuiab´a, pages 204–208.
Eraldo Rezende Fernandes, C´ıcero Nogueira dos San-
tos, and Ruy Luiz Milidi´u. 2012. Latent structure
perceptron with feature induction for unrestricted
coreference resolution. In Joint Conference on
EMNLP and CoNLL-Shared Task, pages 41–48. As-
sociation for Computational Linguistics.
J.R. Finkel, A. Kleeman, and C.D. Manning. 2008. Ef-
ficient, feature-based, conditional random field pars-
ing. Proc. of Annual Meeting on Association for
Computational Linguistics, pages 959–967.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Annual meeting-Association for Compu-
tational Linguistics, volume 45, page 848.
Aria Haghighi and Dan Klein. 2010. Coreference
resolution in a modular, entity-centered model. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
385–393. Association for Computational Linguis-
tics.
P. Koehn. 2004. Statistical signicance tests for ma-
chine translation evaluation. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 28–34. Association for Computational
Linguistics.
Nuno Mamede and Pedro Chaleira. 2004. Char-
acter identification in children stories. In Ad-
vances in Natural Language Processing, pages 82–
90. Springer.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise Integer Linear Programming Formulations
for Dependency Parsing. In Proc. of Annual Meet-
ing of the Association for Computational Linguis-
tics.
A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011. Dual Decomposition
with Many Overlapping Components. In Proc. of
Empirical Methods for Natural Language Process-
ing.
A. F. T. Martins, M. B. Almeida, and N. A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics.
</reference>
<page confidence="0.986907">
47
</page>
<reference confidence="0.999662189873417">
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 104–
111. Association for Computational Linguistics.
V. Ng. 2010. Supervised noun phrase coreference re-
search: The first fifteen years. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics.
Tim O’Keefe, Silvia Pareti, James R Curran, Irena Ko-
prinska, and Matthew Honnibal. 2012. A sequence
labelling approach to quote attribution. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 790–
799. Association for Computational Linguistics.
Silvia Pareti, Tim O’Keefe, Ioannis Konstas, James R.
Curran, and Irena Koprinska. 2013. Automatically
detecting and attributing indirect quotations. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing.
Silvia Pareti. 2012. A database of attribution relations.
In LREC, pages 3213–3217.
Bruno Pouliquen, Ralf Steinberger, and Clive Best.
2007. Automatic detection of quotations in multi-
lingual news. In Proceedings of Recent Advances in
Natural Language Processing, pages 487–492.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. Conll-2011 shared task: Modeling un-
restricted coreference in ontonotes. In Proceedings
of the Fifteenth Conference on Computational Nat-
ural Language Learning: Shared Task, pages 1–27.
Association for Computational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Proceedings
of the Joint Conference on EMNLP and CoNLL:
Shared Task, pages 1–40.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC. Citeseer.
Altaf Rahman and Vincent Ng. 2011. Narrowing the
modeling gap: A cluster-ranking approach to coref-
erence resolution. Journal of Artificial Intelligence
Research, 40(1):469–521.
Luis Sarmento, Sergio Nunes, and E Oliveira. 2009.
Automatic extraction of quotes and topics from news
feeds. In 4th Doctoral Symposium on Informatics
Engineering.
Nathan Schneider, Rebecca Hwa, Philip Gianfortoni,
Dipanjan Das, Michael Heilman, Alan W Black,
Frederick L Crabbe, and Noah A Smith. 2010. Vi-
sualizing topical quotations over time to understand
news discourse. Technical report, Technical Report
CMU-LTI-01-103, CMU.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational linguistics, 27(4):521–544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 656–664. Association for Computational
Linguistics.
Yannick Versley, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
Bart: A modular toolkit for coreference resolution.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics on Human
Language Technologies: Demo Session, pages 9–12.
Association for Computational Linguistics.
</reference>
<page confidence="0.999352">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.289819">
<title confidence="0.991225">A Joint Model for Quotation Attribution and Coreference Resolution</title>
<author confidence="0.925469">S C B F T</author>
<note confidence="0.394746">Labs, Alameda D. Afonso Henriques, 41, 1000-123 Lisboa, Portugal de Instituto Superior T´ecnico, 1049-001 Lisboa,</note>
<abstract confidence="0.9988806">We address the problem of automatically attributing quotations to speakers, which has great relevance in text mining and media monitoring applications. While current systems report high accuracies for this task, they either work at mentionlevel (getting credit for detecting uninformative mentions such as pronouns), or assume the coreferent mentions have been detected beforehand; the inaccuracies in this preprocessing step may lead to error propagation. In this paper, we introduce a joint model for entity-level quotation attribution and coreference resolution, exploiting correlations between the two tasks. We design an evaluation metric for attribution that captures all speakers’ mentions. We present results showing that both tasks benefit from being treated jointly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M B Almeida</author>
<author>A F T Martins</author>
</authors>
<title>Fast and robust compressive summarization with dual decomposition and multi-task learning.</title>
<date>2013</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="21820" citStr="Almeida and Martins, 2013" startWordPosition="3551" endWordPosition="3554">hich is not the root: �pk,t = ψi→*k,t, bk,l. (5) i6=w The objective to optimize is linear in the arc and pair variables (hence the problem can be represented as an integer linear program by turning the logical constraints into linear inequalities). 5.2 Dual Decomposition To decode, we employ the alternating directions dual decomposition algorithm (AD3), which solves a relaxation of the ILP above. AD3 has been used successfully in various NLP tasks, such as dependency parsing (Martins et al., 2011; Martins et al., 2013), semantic role labeling (Das et al., 2012), and compressive summarization (Almeida and Martins, 2013). At test time, if the solution is not integer, we apply a simple rounding procedure to obtain an actual tree: for each node j, obtain the antecedent (or root) i with the highest ai→j, solving ties arbitrarily. 5.3 Learning the Model We train the joint model with the max-loss variant of the MIRA algorithm (Crammer et al., 2006), adapted to latent variables (we simply obtain the best tree consistent with the gold clustering at each step of MIRA, before doing cost-augmented decoding). The resulting algorithm is very similar to the latent perceptron algorithm in Fernandes et al. (2011), but it us</context>
</contexts>
<marker>Almeida, Martins, 2013</marker>
<rawString>M. B. Almeida and A. F. T. Martins. 2013. Fast and robust compressive summarization with dual decomposition and multi-task learning. In Proc. of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>294--303</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9337" citStr="Bengtson and Roth, 2008" startWordPosition="1465" endWordPosition="1468">m. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al.,</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Eric Bengtson and Dan Roth. 2008. Understanding the value of features for coreference resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 294–303. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
</authors>
<title>Bootstrapping path-based pronoun resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12527" citStr="Bergsma and Lin (2006)" startWordPosition="2001" endWordPosition="2004">mizes a score function, to be described next. 4.1 Basic Model Our basic model is a feature-based linear model which assigns a score to each candidate arc linking two mentions (mention-mention arcs), or linking a mention to a quote (mention-quotation arcs). Our basic system is called QUOTEBEFORECOREF for reasons we will detail in section 4.2. 4.1.1 Coreference features For the mention-mention arcs, we use the same coreference features as the SURFACE model of the Berkeley Coreference Resolution System (Durrett and Klein, 2013), plus features for gender and number obtained through the dataset of Bergsma and Lin (2006). This is a very simple lexicaldriven model which achieves state-of-the-art results. The features are shown in Table 1. 4.1.2 Quotation features For the quote attribution features, we use features inspired by O’Keefe et al. (2012), shown in Table 2. The same set of features works for speakers that are individual mentions (in the model just described), and for speakers that are clusters of mentions (used in §6 for the baseline QUOTEAFTERCOREF). These features include various distances between the mention and the quote, the indication of the speaker being inside the quote span, and various conte</context>
</contexts>
<marker>Bergsma, Lin, 2006</marker>
<rawString>Shane Bergsma and Dekang Lin. 2006. Bootstrapping path-based pronoun resolution. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 33–40. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online Passive-Aggressive Algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="22149" citStr="Crammer et al., 2006" startWordPosition="3609" endWordPosition="3612">tion algorithm (AD3), which solves a relaxation of the ILP above. AD3 has been used successfully in various NLP tasks, such as dependency parsing (Martins et al., 2011; Martins et al., 2013), semantic role labeling (Das et al., 2012), and compressive summarization (Almeida and Martins, 2013). At test time, if the solution is not integer, we apply a simple rounding procedure to obtain an actual tree: for each node j, obtain the antecedent (or root) i with the highest ai→j, solving ties arbitrarily. 5.3 Learning the Model We train the joint model with the max-loss variant of the MIRA algorithm (Crammer et al., 2006), adapted to latent variables (we simply obtain the best tree consistent with the gold clustering at each step of MIRA, before doing cost-augmented decoding). The resulting algorithm is very similar to the latent perceptron algorithm in Fernandes et al. (2011), but it uses the aggressive stepsize of MIRA. We set the same costs for coreference mistakes as Durrett and Klein (2013), and a unit cost for missing the correct speaker of a quotation. For speeding up decoding, we first train a basic pruner for the coreference system (using only the features described in §4.1.1), limiting the number of </context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online Passive-Aggressive Algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Michael Wick</author>
<author>Robert Hall</author>
<author>Andrew McCallum</author>
</authors>
<title>First-order probabilistic models for coreference resolution.</title>
<date>2007</date>
<booktitle>In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL),</booktitle>
<pages>81--88</pages>
<contexts>
<context position="9107" citStr="Culotta et al., 2007" startWordPosition="1424" endWordPosition="1427">rganizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov e</context>
</contexts>
<marker>Culotta, Wick, Hall, McCallum, 2007</marker>
<rawString>Aron Culotta, Michael Wick, Robert Hall, and Andrew McCallum. 2007. First-order probabilistic models for coreference resolution. In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>A F T Martins</author>
<author>N A Smith</author>
</authors>
<title>An Exact Dual Decomposition Algorithm for Shallow Semantic Parsing with Constraints.</title>
<date>2012</date>
<booktitle>In Proc. of First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<contexts>
<context position="21761" citStr="Das et al., 2012" startWordPosition="3544" endWordPosition="3547">n the same branch if they have a common ancestor which is not the root: �pk,t = ψi→*k,t, bk,l. (5) i6=w The objective to optimize is linear in the arc and pair variables (hence the problem can be represented as an integer linear program by turning the logical constraints into linear inequalities). 5.2 Dual Decomposition To decode, we employ the alternating directions dual decomposition algorithm (AD3), which solves a relaxation of the ILP above. AD3 has been used successfully in various NLP tasks, such as dependency parsing (Martins et al., 2011; Martins et al., 2013), semantic role labeling (Das et al., 2012), and compressive summarization (Almeida and Martins, 2013). At test time, if the solution is not integer, we apply a simple rounding procedure to obtain an actual tree: for each node j, obtain the antecedent (or root) i with the highest ai→j, solving ties arbitrarily. 5.3 Learning the Model We train the joint model with the max-loss variant of the MIRA algorithm (Crammer et al., 2006), adapted to latent variables (we simply obtain the best tree consistent with the gold clustering at each step of MIRA, before doing cost-augmented decoding). The resulting algorithm is very similar to the latent</context>
</contexts>
<marker>Das, Martins, Smith, 2012</marker>
<rawString>D. Das, A. F. T. Martins, and N. A. Smith. 2012. An Exact Dual Decomposition Algorithm for Shallow Semantic Parsing with Constraints. In Proc. of First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>´Eric de La Clergerie</author>
<author>Benoit Sagot</author>
<author>Rosa Stern</author>
<author>Pascal Denis</author>
<author>Ga¨elle Recourc´e</author>
<author>Victor Mignot</author>
</authors>
<title>Extracting and visualizing quotations from news wires.</title>
<date>2011</date>
<booktitle>In Human Language Technology. Challenges for Computer Science and Linguistics,</booktitle>
<pages>522--532</pages>
<publisher>Springer.</publisher>
<marker>Clergerie, Sagot, Stern, Denis, Recourc´e, Mignot, 2011</marker>
<rawString>´Eric de La Clergerie, Benoit Sagot, Rosa Stern, Pascal Denis, Ga¨elle Recourc´e, and Victor Mignot. 2011. Extracting and visualizing quotations from news wires. In Human Language Technology. Challenges for Computer Science and Linguistics, pages 522–532. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>660--669</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5059" citStr="Denis and Baldridge, 2008" startWordPosition="772" endWordPosition="775">ribute (incorrectly) this quote to Pilson. In example (b), there are two quotes with the same speaker entity (as indicated by the cue she added). This gives evidence that M1 and M6 should be coreferent. A pipeline approach would not be able to exploit these correlations. We argue that this type of mistakes, among others, can be prevented by a system that performs quote attribution and coreference resolution jointly (§4). Our joint model is inspired by recent work in coreference resolution that independently ranks the possible mention’s antecedents, forming a latent coreference tree structure (Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett et al., 2013; Durrett and Klein, 2013). We consider a generalization of these structures which we call a quotation-coreference tree. To effectively couple the two tasks, we need to go beyond simple arc-factored models and consider paths in the tree. We formulate the resulting problem as a logic program, which we tackle using a dual decomposition strategy (§5). We provide an empirical comparison between our method and baselines for each of the tasks and a pipeline system, defining suitable metrics for entity-level quotation attribution (§6). 2 Quotation Attribu</context>
<context position="10400" citStr="Denis and Baldridge (2008)" startWordPosition="1643" endWordPosition="1647">2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al., 2012; Durrett and Klein, 2013). These models define scores for each mention to link to its antecedent or to an artifical root symbol $ (in which case it is not anaphoric). The computation of the best tree can be done exactly with spanning tree algorithms, or by independently choosing the best antecedent (or the root) for each mention, if only left-to-right arcs are allowed. The same idea underlies the antecedent ranking approach of Denis and Baldridge (2008). Once the coreference tree is computed, the set of entity clusters E is obtained by associating each entity set to a branch of the tree coming out from the root. This is illustrated in Figure 1 (left). 4 Joint Quotations and Coreferences In this work, we propose that quotation attribution and coreference resolution are solved jointly by treating both mentions and quotations as nodes in a generalized structure called a quotationcoreference tree (Figure 1, right). The joint system’s decoding process consists in creating such a tree, from which a clustering of the nodes can be immediatelly obtai</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Pascal Denis and Jason Baldridge. 2008. Specialized models and ranking for coreference resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 660– 669. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Easy victories and uphill battles in coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5131" citStr="Durrett and Klein, 2013" startWordPosition="785" endWordPosition="788">quotes with the same speaker entity (as indicated by the cue she added). This gives evidence that M1 and M6 should be coreferent. A pipeline approach would not be able to exploit these correlations. We argue that this type of mistakes, among others, can be prevented by a system that performs quote attribution and coreference resolution jointly (§4). Our joint model is inspired by recent work in coreference resolution that independently ranks the possible mention’s antecedents, forming a latent coreference tree structure (Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett et al., 2013; Durrett and Klein, 2013). We consider a generalization of these structures which we call a quotation-coreference tree. To effectively couple the two tasks, we need to go beyond simple arc-factored models and consider paths in the tree. We formulate the resulting problem as a logic program, which we tackle using a dual decomposition strategy (§5). We provide an empirical comparison between our method and baselines for each of the tasks and a pipeline system, defining suitable metrics for entity-level quotation attribution (§6). 2 Quotation Attribution The task of quotation attribution can be formally defined as follow</context>
<context position="9968" citStr="Durrett and Klein, 2013" startWordPosition="1571" endWordPosition="1574">l et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al., 2012; Durrett and Klein, 2013). These models define scores for each mention to link to its antecedent or to an artifical root symbol $ (in which case it is not anaphoric). The computation of the best tree can be done exactly with spanning tree algorithms, or by independently choosing the best antecedent (or the root) for each mention, if only left-to-right arcs are allowed. The same idea underlies the antecedent ranking approach of Denis and Baldridge (2008). Once the coreference tree is computed, the set of entity clusters E is obtained by associating each entity set to a branch of the tree coming out from the root. This </context>
<context position="12435" citStr="Durrett and Klein, 2013" startWordPosition="1986" endWordPosition="1989"> to the right). The quotation-coreference tree is obtained as the best spanning tree that maximizes a score function, to be described next. 4.1 Basic Model Our basic model is a feature-based linear model which assigns a score to each candidate arc linking two mentions (mention-mention arcs), or linking a mention to a quote (mention-quotation arcs). Our basic system is called QUOTEBEFORECOREF for reasons we will detail in section 4.2. 4.1.1 Coreference features For the mention-mention arcs, we use the same coreference features as the SURFACE model of the Berkeley Coreference Resolution System (Durrett and Klein, 2013), plus features for gender and number obtained through the dataset of Bergsma and Lin (2006). This is a very simple lexicaldriven model which achieves state-of-the-art results. The features are shown in Table 1. 4.1.2 Quotation features For the quote attribution features, we use features inspired by O’Keefe et al. (2012), shown in Table 2. The same set of features works for speakers that are individual mentions (in the model just described), and for speakers that are clusters of mentions (used in §6 for the baseline QUOTEAFTERCOREF). These features include various distances between the mention</context>
<context position="14725" citStr="Durrett and Klein (2013)" startWordPosition="2359" endWordPosition="2362">] + [CHILD FIRST WORD] [ANAPHORIC (T/F)] + [CHILD LAST WORD] [ANAPHORIC (T/F)] + [CHILD PRECEDING WORD] [ANAPHORIC (T/F)] + [CHILD FOLLOWING WORD] [ANAPHORIC (T/F)] + [CHILD LENGTH] Features on the parent mention [PARENT HEAD WORD] [PARENT FIRST WORD] [PARENT LAST WORD] [PARENT PRECEDING WORD] [PARENT FOLLOWING WORD] [PARENT LENGTH] [PARENT GENDER] [PARENT NUMBER] Features on the pair [EXACT STRING MATCH (T/F)] [HEAD MATCH (T/F)] [SENTENCE DISTANCE, CAPPED AT 10] [MENTION DISTANCE, CAPPED AT 10] Table 1: Coreference features, associated to each candidate mention-mention arc in the tree. As in Durrett and Klein (2013), we also include conjunctions of each feature with the child and parent mention types (proper, nominal, or, if pronominal, the pronoun word). quotation-coreference tree can be obtained by first assigning the highest scored mention to each quotation, and then building a standard coreference tree involving only the mention nodes. This corresponds to the QUOTEBEFORECOREF baseline, to be used in §6. To go beyond separate models, we introduce a final JOINT model, which includes additional scores that depend not just on arcs, but also on paths in the tree. Concretely, we select certain Features on </context>
<context position="22530" citStr="Durrett and Klein (2013)" startWordPosition="3671" endWordPosition="3674">o obtain an actual tree: for each node j, obtain the antecedent (or root) i with the highest ai→j, solving ties arbitrarily. 5.3 Learning the Model We train the joint model with the max-loss variant of the MIRA algorithm (Crammer et al., 2006), adapted to latent variables (we simply obtain the best tree consistent with the gold clustering at each step of MIRA, before doing cost-augmented decoding). The resulting algorithm is very similar to the latent perceptron algorithm in Fernandes et al. (2011), but it uses the aggressive stepsize of MIRA. We set the same costs for coreference mistakes as Durrett and Klein (2013), and a unit cost for missing the correct speaker of a quotation. For speeding up decoding, we first train a basic pruner for the coreference system (using only the features described in §4.1.1), limiting the number of candidate antecedents to 10, and discarding scores whose difference with respect to the best antecedent is below a threshold. We also freeze 6 Experiments 6.1 Dataset We used the 597 documents of the Wall Street Journal (WSJ) corpus that were disclosed for the CoNLL-2011 coreference shared task (Pradhan et al., 2011) as a dataset for coreference resolution. This dataset includes</context>
<context position="26994" citStr="Durrett and Klein (2013)" startWordPosition="4390" endWordPosition="4393">An attribution system trained on the gold speaker, and post-combined with the output of a coreference system. This system should be able to provide a set of informative mentions about a quote, post-resolving the problem of the pronominal speakers. This kind of post-coreference approach was used by de La Clergerie et al. (2011). 6.4 Coreference Resolution We use the coreference results of our basic QUOTEBEFORECOREF system as a baseline for coreference resolution. Since this system effectively solves the two problems separately, this can be considered our implementation of the SURFACE system of Durrett and Klein (2013) . As reported in Table 4, the perfromance of our baseline is comparable with the one of the SURFACE system of Durrett and Klein (2013), which is denoted as SURFACE-DK-2013.2 Table 4 also show the CoNLL metrics obtained for the proposed system of joint coreference resolution and quotation attribution. Our joint system outperformed the baseline with statistical significance (with p &lt; 0.05 and according to a bootstrap resampling test (Koehn, 2004)) for all metrics expect for the CEAFE F1 measure, whose value was only slighty improved. These results confirm that the coreference resolution task be</context>
<context position="29355" citStr="Durrett and Klein (2013)" startWordPosition="4747" endWordPosition="4750"> of attribution result of the QUOTEONLY system (QUOTEBEFORECOREF). These results indicate that the quotation attribution task performs better by looking at the speaker mention that connects more strongly with the quotation, instead of trying to match the whole cluster. Finally, the scores achieved by our JOINT 2To make the systems comparable, we re-trained Durrett et al.’s coreference system (version 0.9) on the WSJ portion of the Ontonotes datasets (the portion which has quote annotations from Pareti et al.’s PARC dataset). For this reason, the values in Table 4 differ from those reported in Durrett and Klein (2013), which where trained and tested in the entire Ontonotes. 45 MUC F1 BCUB F1 CEAFE F1 Avg. SURFACE-DK-2013 58.87 62.74 45.46 55.7 SURFACE-OURS [QUOTEBEFORECOREF] 57.89 62.50 45.48 55.3 JOINT 58.78 63.79 45.50 56.0 Table 4: Coreference obtained with the CoNLL scorer (version 5) in the test partition of the WJS corpus, for the SURFACE system of Durrett and Klein (2013), our baseline implementation of the that system (SURFACE-OURS), and our JOINT approach. All systems were trained in the WSJ portion of the Ontonotes. EM RSM ECF1 QUOTEONLY 49.1% 49.4% 41.2% QUOTEAFTERCOREF 76.7% 64.6% 70.0% QUOTEBE</context>
</contexts>
<marker>Durrett, Klein, 2013</marker>
<rawString>Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>David Hall</author>
<author>Dan Klein</author>
</authors>
<title>Decentralized entity-level modeling for coreference resolution.</title>
<date>2013</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5105" citStr="Durrett et al., 2013" startWordPosition="780" endWordPosition="784">le (b), there are two quotes with the same speaker entity (as indicated by the cue she added). This gives evidence that M1 and M6 should be coreferent. A pipeline approach would not be able to exploit these correlations. We argue that this type of mistakes, among others, can be prevented by a system that performs quote attribution and coreference resolution jointly (§4). Our joint model is inspired by recent work in coreference resolution that independently ranks the possible mention’s antecedents, forming a latent coreference tree structure (Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett et al., 2013; Durrett and Klein, 2013). We consider a generalization of these structures which we call a quotation-coreference tree. To effectively couple the two tasks, we need to go beyond simple arc-factored models and consider paths in the tree. We formulate the resulting problem as a logic program, which we tackle using a dual decomposition strategy (§5). We provide an empirical comparison between our method and baselines for each of the tasks and a pipeline system, defining suitable metrics for entity-level quotation attribution (§6). 2 Quotation Attribution The task of quotation attribution can be </context>
<context position="9295" citStr="Durrett et al., 2013" startWordPosition="1459" endWordPosition="1462"> a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity cluste</context>
</contexts>
<marker>Durrett, Hall, Klein, 2013</marker>
<rawString>Greg Durrett, David Hall, and Dan Klein. 2013. Decentralized entity-level modeling for coreference resolution. In Proc. of Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David K Elson</author>
<author>Kathleen McKeown</author>
</authors>
<title>Automatic attribution of quoted speech in literary narrative.</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="1749" citStr="Elson and McKeown, 2010" startWordPosition="253" endWordPosition="256">ion Quotations are a crucial part of news stories, giving the perspectives of the participants in the narrated event, and making the news sound objective. The ability of extracting and organizing these quotations is highly relevant for text mining applications, as it may aid journalists in fact-checking, help users browse news threads, and reduce human intervention in media monitoring. This involves assigning the correct speaker to each quote—a problem called quotation attribution (§2). There is significant literature devoted to this task, both for narrative genres (Mamede and Chaleira, 2004; Elson and McKeown, 2010) and newswire domains (Pouliquen et al., 2007; Sarmento et al., 2009; Schneider et al., 2010). While the earliest works focused on devising lexical and syntactic rules and hand-crafting grammars, there has been a recent shift toward machine learning approaches (Fernandes et al., 2011; O’Keefe et al., 2012; Pareti et al., 2013), with latest works reporting high accuracies for speaker identification in newswire (in the range 80–95% for direct and mixed quotes, according to O’Keefe et al. (2012)). Despite these encouraging results, quotation mining systems are not yet fully satisfactory, even whe</context>
</contexts>
<marker>Elson, McKeown, 2010</marker>
<rawString>David K Elson and Kathleen McKeown. 2010. Automatic attribution of quoted speech in literary narrative. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Paulo Ducca Fernandes</author>
<author>Eduardo Motta</author>
<author>Ruy Luiz Milidi´u</author>
</authors>
<title>Quotation extraction for portuguese.</title>
<date>2011</date>
<booktitle>In Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology (STIL 2011), Cuiab´a,</booktitle>
<pages>204--208</pages>
<marker>Fernandes, Motta, Milidi´u, 2011</marker>
<rawString>William Paulo Ducca Fernandes, Eduardo Motta, and Ruy Luiz Milidi´u. 2011. Quotation extraction for portuguese. In Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology (STIL 2011), Cuiab´a, pages 204–208.</rawString>
</citation>
<citation valid="true">
<title>Eraldo Rezende Fernandes, C´ıcero Nogueira dos Santos, and Ruy Luiz Milidi´u.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL-Shared Task,</booktitle>
<pages>41--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2246" citStr="(2012)" startWordPosition="335" endWordPosition="335">terature devoted to this task, both for narrative genres (Mamede and Chaleira, 2004; Elson and McKeown, 2010) and newswire domains (Pouliquen et al., 2007; Sarmento et al., 2009; Schneider et al., 2010). While the earliest works focused on devising lexical and syntactic rules and hand-crafting grammars, there has been a recent shift toward machine learning approaches (Fernandes et al., 2011; O’Keefe et al., 2012; Pareti et al., 2013), with latest works reporting high accuracies for speaker identification in newswire (in the range 80–95% for direct and mixed quotes, according to O’Keefe et al. (2012)). Despite these encouraging results, quotation mining systems are not yet fully satisfactory, even when only direct quotes are considered. Part of the problem, as we next describe, has to do with inaccuracies in coreference resolution (§3). The “easiest” instances of quotation attribution problems arise when the speaker and the quote are semantically connected, e.g., through a reported speech verb like said. However, in newswire text, the subject of this verb is commonly a pronoun or another uninformative anaphoric mention. While the speaker thus determined may well be correct— being in most </context>
<context position="3985" citStr="(2012)" startWordPosition="605" endWordPosition="605">rp at “the principle of [Pilson]M,,” as [NBC’s Arthur Watson]M2 once put it – “[he]M3’s always expounding that rights are too high, then [he]M4’s going crazy.” But [the 49-year-old Mr. Pilson]M5 is hardly a man to ignore the numbers. (b) [English novelist Dorothy L. Sayers]M, described [ringing]M2 as a “passion that finds its satisfaction in [mathematical completeness]M3 and [mechanical perfection]M4.” [Ringers]M5, [she]M6 added, are “filled with the solemn intoxication that comes of intricate ritual faultlessly performed.” In example (a), the pronoun coreference system used by O’Keefe et al. (2012) erroneously clusters together mentions M2, M3 and M4 (instead of the correct clustering {M1, M3, M4}). Since it is unlikely that the speaker is co-referent to a third39 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 39–48, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics person pronoun he inside the quote, a pipeline system would likely attribute (incorrectly) this quote to Pilson. In example (b), there are two quotes with the same speaker entity (as indicated by the cue she added). This </context>
<context position="6270" citStr="(2012)" startWordPosition="974" endWordPosition="974">The task of quotation attribution can be formally defined as follows. Given a document containing a sequence of quotations, (q1, ... , qL), and a set of candidate speakers, {s1, ... , sm}, the goal is to a assign a speaker to every quote. Previous work has handled direct and mixed quotations (Sarmento et al., 2009; O’Keefe et al., 2012), easily extractable with regular expressions for detecting quotation marks, as well as indirect quotations (Pareti et al., 2013), which are more involved and require syntactic or semantic patterns. In this work, we resort to direct and mixed quotations. Pareti (2012) defines quotation attributions in terms of their content span (the quotation text itself), their cue (a lexical anchor of the attribution relation, such as a reported speech verb), and the source span (the author of the quote). The same reference introduced the PARC dataset, which we use in our experiments (§6) and which is based on the annotation of a database of attribution relations from the Penn Discourse Treebank (Prasad et al., 2008). Several machine learning algorithms have been applied to this task, either framing the problem as classification (an independent decision for each quote),</context>
<context position="9779" citStr="(2012)" startWordPosition="1543" endWordPosition="1543">et of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al., 2012; Durrett and Klein, 2013). These models define scores for each mention to link to its antecedent or to an artifical root symbol $ (in which case it is not anaphoric). The computation of the best tree can be done exactly with spanning tree algorithms, or by independently choosing the best antecedent (or the root) for each mention, if only left-to-right arcs are allowed. The same idea underlies the antecedent ranking approach of Denis</context>
<context position="12757" citStr="(2012)" startWordPosition="2041" endWordPosition="2041">ion arcs). Our basic system is called QUOTEBEFORECOREF for reasons we will detail in section 4.2. 4.1.1 Coreference features For the mention-mention arcs, we use the same coreference features as the SURFACE model of the Berkeley Coreference Resolution System (Durrett and Klein, 2013), plus features for gender and number obtained through the dataset of Bergsma and Lin (2006). This is a very simple lexicaldriven model which achieves state-of-the-art results. The features are shown in Table 1. 4.1.2 Quotation features For the quote attribution features, we use features inspired by O’Keefe et al. (2012), shown in Table 2. The same set of features works for speakers that are individual mentions (in the model just described), and for speakers that are clusters of mentions (used in §6 for the baseline QUOTEAFTERCOREF). These features include various distances between the mention and the quote, the indication of the speaker being inside the quote span, and various contextual features. 4.2 Final Model While the basic model just described puts quotations and mentions together, it is not more expressive than having separate models for the two tasks. In fact, if we just have scores for individual ar</context>
<context position="23522" citStr="(2012)" startWordPosition="3837" endWordPosition="3837">We used the 597 documents of the Wall Street Journal (WSJ) corpus that were disclosed for the CoNLL-2011 coreference shared task (Pradhan et al., 2011) as a dataset for coreference resolution. This dataset includes train, development and test partitions, annotated with coreference information, as well as gold and automatically generated syntactic and semantic information. The CoNLL-2011 corpus does not contain annotations of quotation attribution. For that reason, we used the WSJ quotation annotations in the PARC dataset (Pareti, 2012). We used the same version of the corpus as O’Keefe et al. (2012), but with different splits, to match the dataset partitions in the coreference resolution data. This attribution corpus contains 279 documents of the 597 CoNLL-2011 files, having a total of 1199 annotated quotes. As in that work, we only considered directed speech quotes and the direct part of mixed quotes (quotes with both direct and undirected speech). 6.2 Metrics for quotation attribution Previous evaluations of quotation attribution systems were designed at mention level, and are thus assessed by comparing the predicted speaker mention span with the gold one. This metric assesses the amou</context>
<context position="26348" citStr="(2012)" startWordPosition="4293" endWordPosition="4293">ence system used to cluster the related mentions. 6.3 Attribution baselines To analyze the task of entity-level quotation attribution, we implemented three baseline systems. • QUOTEONLY: A quotation attribution system trained on the representative speaker, instead of the gold speaker. For fairness, this baseline was trained with an extra feature indicating the type of the mention (nominal, pronominal or proper). • QUOTEAFTERCOREF: An attribution system directly applied to the output of a predicted coreference chain. This baseline uses a coreference pre-processing, as applied in O’Keefe et al. (2012). • QUOTEBEFORECOREF: An attribution system trained on the gold speaker, and post-combined with the output of a coreference system. This system should be able to provide a set of informative mentions about a quote, post-resolving the problem of the pronominal speakers. This kind of post-coreference approach was used by de La Clergerie et al. (2011). 6.4 Coreference Resolution We use the coreference results of our basic QUOTEBEFORECOREF system as a baseline for coreference resolution. Since this system effectively solves the two problems separately, this can be considered our implementation of </context>
</contexts>
<marker>2012</marker>
<rawString>Eraldo Rezende Fernandes, C´ıcero Nogueira dos Santos, and Ruy Luiz Milidi´u. 2012. Latent structure perceptron with feature induction for unrestricted coreference resolution. In Joint Conference on EMNLP and CoNLL-Shared Task, pages 41–48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>A Kleeman</author>
<author>C D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>Proc. of Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>959--967</pages>
<contexts>
<context position="9358" citStr="Finkel et al., 2008" startWordPosition="1469" endWordPosition="1472">discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al., 2012; Durrett and Kl</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>J.R. Finkel, A. Kleeman, and C.D. Manning. 2008. Efficient, feature-based, conditional random field parsing. Proc. of Annual Meeting on Association for Computational Linguistics, pages 959–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric bayesian model.</title>
<date>2007</date>
<booktitle>In Annual meeting-Association for Computational Linguistics,</booktitle>
<volume>45</volume>
<pages>848</pages>
<contexts>
<context position="8038" citStr="Haghighi and Klein (2007)" startWordPosition="1264" endWordPosition="1267"> example, O’Keefe et al. (2012) applies a coreference resolver before quotation attribution, whereas de La Clergerie et al. (2011) does it afterwards, as a post-processing stage. An important issue when evaluating quotation attribution systems is to prevent them from getting credit for detecting uninformative speakers such as pronouns; we will get back to this topic in §6.2. 3 Coreference Resolution In coreference resolution, we are given a set of mentions M :_ {m1, ... , mK}, and the goal is to cluster them into discourse entities, E :_ {e1,... , ei}, where each ej ⊆ M and ej _� ∅. We follow Haghighi and Klein (2007) and distinguish between proper, nominal, and pronominal mentions. Each requires different types of information to be resolved. Thus, the task involves determining anaphoricity, resolving pronouns, and identifying semantic compatibility among mentions. To resolve these references, one typically exploits contextual and grammatical clues, as well as semantic information and world knowledge, to understand whether mentions refer to people, places, organizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; P</context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric bayesian model. In Annual meeting-Association for Computational Linguistics, volume 45, page 848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9133" citStr="Haghighi and Klein, 2010" startWordPosition="1428" endWordPosition="1431">n. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), P</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 385–393. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical signicance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27443" citStr="Koehn, 2004" startWordPosition="4465" endWordPosition="4466">tion. Since this system effectively solves the two problems separately, this can be considered our implementation of the SURFACE system of Durrett and Klein (2013) . As reported in Table 4, the perfromance of our baseline is comparable with the one of the SURFACE system of Durrett and Klein (2013), which is denoted as SURFACE-DK-2013.2 Table 4 also show the CoNLL metrics obtained for the proposed system of joint coreference resolution and quotation attribution. Our joint system outperformed the baseline with statistical significance (with p &lt; 0.05 and according to a bootstrap resampling test (Koehn, 2004)) for all metrics expect for the CEAFE F1 measure, whose value was only slighty improved. These results confirm that the coreference resolution task benefits for being tackled jointly with quotation attribution. 6.5 Quotation attribution We implemented and trained the three attribution systems that were described in §6.3 and the system for joint coreference and author attribution that is detailed in §4. For each system, Table 5 shows the mention-based and entity-based metrics that were described in §6.2. Training a quotation attribution system using representative speakers instead of the gold </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical signicance tests for machine translation evaluation. In Proc. of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>28--34</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8892" citStr="Lee et al. (2011)" startWordPosition="1394" endWordPosition="1397">among mentions. To resolve these references, one typically exploits contextual and grammatical clues, as well as semantic information and world knowledge, to understand whether mentions refer to people, places, organizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the t</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 28–34. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuno Mamede</author>
<author>Pedro Chaleira</author>
</authors>
<title>Character identification in children stories.</title>
<date>2004</date>
<booktitle>In Advances in Natural Language Processing,</booktitle>
<pages>82--90</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1723" citStr="Mamede and Chaleira, 2004" startWordPosition="249" endWordPosition="252">reated jointly. 1 Introduction Quotations are a crucial part of news stories, giving the perspectives of the participants in the narrated event, and making the news sound objective. The ability of extracting and organizing these quotations is highly relevant for text mining applications, as it may aid journalists in fact-checking, help users browse news threads, and reduce human intervention in media monitoring. This involves assigning the correct speaker to each quote—a problem called quotation attribution (§2). There is significant literature devoted to this task, both for narrative genres (Mamede and Chaleira, 2004; Elson and McKeown, 2010) and newswire domains (Pouliquen et al., 2007; Sarmento et al., 2009; Schneider et al., 2010). While the earliest works focused on devising lexical and syntactic rules and hand-crafting grammars, there has been a recent shift toward machine learning approaches (Fernandes et al., 2011; O’Keefe et al., 2012; Pareti et al., 2013), with latest works reporting high accuracies for speaker identification in newswire (in the range 80–95% for direct and mixed quotes, according to O’Keefe et al. (2012)). Despite these encouraging results, quotation mining systems are not yet fu</context>
</contexts>
<marker>Mamede, Chaleira, 2004</marker>
<rawString>Nuno Mamede and Pedro Chaleira. 2004. Character identification in children stories. In Advances in Natural Language Processing, pages 82– 90. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Concise Integer Linear Programming Formulations for Dependency Parsing.</title>
<date>2009</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="19463" citStr="Martins et al., 2009" startWordPosition="3106" endWordPosition="3109"> with the JOINT model is in general intractable, since this model breaks the independence assumption between the arcs. However, given the relatively small amount of node pairs that have scores (only mentions inside quotations and consecutive quotations), we expect this “perturbation” to be small enough not to affect the quality of an approximate decoder. The situation resembles other problems in NLP, such as non-projective dependency parsing, which becomes intractable if higher order interactions between the arcs are considered, but can still be well approximated. Inspired by work in parsing (Martins et al., 2009) using linear relaxations with multi-commodity flow models, we propose a similar strategy by defining auxiliary variables and coupling them in a logic program. 5.1 Logic Formulation We next derive the logic program for joint decoding of coreferences and quotations. The input is a set of nodes (including an artificial node), a set of candidate arcs with scores, and a set of node pairs with scores. To make the exposition lighter, we index nodes by integers (starting by the root node 0) and we do not distinguish between mention and quotation nodes. Only arcs from left to right are allowed. The va</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009. Concise Integer Linear Programming Formulations for Dependency Parsing. In Proc. of Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>P M Q Aguiar</author>
<author>M A T Figueiredo</author>
</authors>
<title>Dual Decomposition with Many Overlapping Components.</title>
<date>2011</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="21695" citStr="Martins et al., 2011" startWordPosition="3532" endWordPosition="3535">m i to `: ψi→*k,t = πi→*k n πi→*t, bi, k, ` (4) • Nodes k and ` are in the same branch if they have a common ancestor which is not the root: �pk,t = ψi→*k,t, bk,l. (5) i6=w The objective to optimize is linear in the arc and pair variables (hence the problem can be represented as an integer linear program by turning the logical constraints into linear inequalities). 5.2 Dual Decomposition To decode, we employ the alternating directions dual decomposition algorithm (AD3), which solves a relaxation of the ILP above. AD3 has been used successfully in various NLP tasks, such as dependency parsing (Martins et al., 2011; Martins et al., 2013), semantic role labeling (Das et al., 2012), and compressive summarization (Almeida and Martins, 2013). At test time, if the solution is not integer, we apply a simple rounding procedure to obtain an actual tree: for each node j, obtain the antecedent (or root) i with the highest ai→j, solving ties arbitrarily. 5.3 Learning the Model We train the joint model with the max-loss variant of the MIRA algorithm (Crammer et al., 2006), adapted to latent variables (we simply obtain the best tree consistent with the gold clustering at each step of MIRA, before doing cost-augmente</context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and M. A. T. Figueiredo. 2011. Dual Decomposition with Many Overlapping Components. In Proc. of Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>M B Almeida</author>
<author>N A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order nonprojective turbo parsers.</title>
<date>2013</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="21718" citStr="Martins et al., 2013" startWordPosition="3536" endWordPosition="3540">→*k n πi→*t, bi, k, ` (4) • Nodes k and ` are in the same branch if they have a common ancestor which is not the root: �pk,t = ψi→*k,t, bk,l. (5) i6=w The objective to optimize is linear in the arc and pair variables (hence the problem can be represented as an integer linear program by turning the logical constraints into linear inequalities). 5.2 Dual Decomposition To decode, we employ the alternating directions dual decomposition algorithm (AD3), which solves a relaxation of the ILP above. AD3 has been used successfully in various NLP tasks, such as dependency parsing (Martins et al., 2011; Martins et al., 2013), semantic role labeling (Das et al., 2012), and compressive summarization (Almeida and Martins, 2013). At test time, if the solution is not integer, we apply a simple rounding procedure to obtain an actual tree: for each node j, obtain the antecedent (or root) i with the highest ai→j, solving ties arbitrarily. 5.3 Learning the Model We train the joint model with the max-loss variant of the MIRA algorithm (Crammer et al., 2006), adapted to latent variables (we simply obtain the best tree consistent with the gold clustering at each step of MIRA, before doing cost-augmented decoding). The result</context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>A. F. T. Martins, M. B. Almeida, and N. A. Smith. 2013. Turning on the turbo: Fast third-order nonprojective turbo parsers. In Proc. of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8873" citStr="Ng and Cardie, 2002" startWordPosition="1390" endWordPosition="1393">emantic compatibility among mentions. To resolve these references, one typically exploits contextual and grammatical clues, as well as semantic information and world knowledge, to understand whether mentions refer to people, places, organizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define th</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 104– 111. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Supervised noun phrase coreference research: The first fifteen years.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9730" citStr="Ng (2010)" startWordPosition="1533" endWordPosition="1534"> Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al., 2012; Durrett and Klein, 2013). These models define scores for each mention to link to its antecedent or to an artifical root symbol $ (in which case it is not anaphoric). The computation of the best tree can be done exactly with spanning tree algorithms, or by independently choosing the best antecedent (or the root) for each mention, if only left-to-right arcs are allowed. The same idea u</context>
</contexts>
<marker>Ng, 2010</marker>
<rawString>V. Ng. 2010. Supervised noun phrase coreference research: The first fifteen years. In Proc. of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim O’Keefe</author>
<author>Silvia Pareti</author>
<author>James R Curran</author>
<author>Irena Koprinska</author>
<author>Matthew Honnibal</author>
</authors>
<title>A sequence labelling approach to quote attribution.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>790--799</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>O’Keefe, Pareti, Curran, Koprinska, Honnibal, 2012</marker>
<rawString>Tim O’Keefe, Silvia Pareti, James R Curran, Irena Koprinska, and Matthew Honnibal. 2012. A sequence labelling approach to quote attribution. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 790– 799. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvia Pareti</author>
<author>Tim O’Keefe</author>
<author>Ioannis Konstas</author>
<author>James R Curran</author>
<author>Irena Koprinska</author>
</authors>
<title>Automatically detecting and attributing indirect quotations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Pareti, O’Keefe, Konstas, Curran, Koprinska, 2013</marker>
<rawString>Silvia Pareti, Tim O’Keefe, Ioannis Konstas, James R. Curran, and Irena Koprinska. 2013. Automatically detecting and attributing indirect quotations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvia Pareti</author>
</authors>
<title>A database of attribution relations.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<pages>3213--3217</pages>
<contexts>
<context position="2907" citStr="Pareti, 2012" startWordPosition="435" endWordPosition="436">mining systems are not yet fully satisfactory, even when only direct quotes are considered. Part of the problem, as we next describe, has to do with inaccuracies in coreference resolution (§3). The “easiest” instances of quotation attribution problems arise when the speaker and the quote are semantically connected, e.g., through a reported speech verb like said. However, in newswire text, the subject of this verb is commonly a pronoun or another uninformative anaphoric mention. While the speaker thus determined may well be correct— being in most cases consistent with human annotation choices (Pareti, 2012)—from a practical perspective, it will be of little use without a coreference system that correctly resolves the anaphora. Since the current state of the art in coreference resolution is far from perfect, errors at this stage tend to propagate to the quote attribution system. Consider the following examples for illustration (taken from the WSJ-1057 and WSJ-0089 documents in the Penn Treebank), where we have annotated with subscripts some of the mentions: (a) Rivals carp at “the principle of [Pilson]M,,” as [NBC’s Arthur Watson]M2 once put it – “[he]M3’s always expounding that rights are too hi</context>
<context position="6270" citStr="Pareti (2012)" startWordPosition="973" endWordPosition="974">bution The task of quotation attribution can be formally defined as follows. Given a document containing a sequence of quotations, (q1, ... , qL), and a set of candidate speakers, {s1, ... , sm}, the goal is to a assign a speaker to every quote. Previous work has handled direct and mixed quotations (Sarmento et al., 2009; O’Keefe et al., 2012), easily extractable with regular expressions for detecting quotation marks, as well as indirect quotations (Pareti et al., 2013), which are more involved and require syntactic or semantic patterns. In this work, we resort to direct and mixed quotations. Pareti (2012) defines quotation attributions in terms of their content span (the quotation text itself), their cue (a lexical anchor of the attribution relation, such as a reported speech verb), and the source span (the author of the quote). The same reference introduced the PARC dataset, which we use in our experiments (§6) and which is based on the annotation of a database of attribution relations from the Penn Discourse Treebank (Prasad et al., 2008). Several machine learning algorithms have been applied to this task, either framing the problem as classification (an independent decision for each quote),</context>
<context position="23457" citStr="Pareti, 2012" startWordPosition="3823" endWordPosition="3824">tecedent is below a threshold. We also freeze 6 Experiments 6.1 Dataset We used the 597 documents of the Wall Street Journal (WSJ) corpus that were disclosed for the CoNLL-2011 coreference shared task (Pradhan et al., 2011) as a dataset for coreference resolution. This dataset includes train, development and test partitions, annotated with coreference information, as well as gold and automatically generated syntactic and semantic information. The CoNLL-2011 corpus does not contain annotations of quotation attribution. For that reason, we used the WSJ quotation annotations in the PARC dataset (Pareti, 2012). We used the same version of the corpus as O’Keefe et al. (2012), but with different splits, to match the dataset partitions in the coreference resolution data. This attribution corpus contains 279 documents of the 597 CoNLL-2011 files, having a total of 1199 annotated quotes. As in that work, we only considered directed speech quotes and the direct part of mixed quotes (quotes with both direct and undirected speech). 6.2 Metrics for quotation attribution Previous evaluations of quotation attribution systems were designed at mention level, and are thus assessed by comparing the predicted spea</context>
</contexts>
<marker>Pareti, 2012</marker>
<rawString>Silvia Pareti. 2012. A database of attribution relations. In LREC, pages 3213–3217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Pouliquen</author>
<author>Ralf Steinberger</author>
<author>Clive Best</author>
</authors>
<title>Automatic detection of quotations in multilingual news.</title>
<date>2007</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing,</booktitle>
<pages>487--492</pages>
<contexts>
<context position="1794" citStr="Pouliquen et al., 2007" startWordPosition="260" endWordPosition="263">es, giving the perspectives of the participants in the narrated event, and making the news sound objective. The ability of extracting and organizing these quotations is highly relevant for text mining applications, as it may aid journalists in fact-checking, help users browse news threads, and reduce human intervention in media monitoring. This involves assigning the correct speaker to each quote—a problem called quotation attribution (§2). There is significant literature devoted to this task, both for narrative genres (Mamede and Chaleira, 2004; Elson and McKeown, 2010) and newswire domains (Pouliquen et al., 2007; Sarmento et al., 2009; Schneider et al., 2010). While the earliest works focused on devising lexical and syntactic rules and hand-crafting grammars, there has been a recent shift toward machine learning approaches (Fernandes et al., 2011; O’Keefe et al., 2012; Pareti et al., 2013), with latest works reporting high accuracies for speaker identification in newswire (in the range 80–95% for direct and mixed quotes, according to O’Keefe et al. (2012)). Despite these encouraging results, quotation mining systems are not yet fully satisfactory, even when only direct quotes are considered. Part of </context>
</contexts>
<marker>Pouliquen, Steinberger, Best, 2007</marker>
<rawString>Bruno Pouliquen, Ralf Steinberger, and Clive Best. 2007. Automatic detection of quotations in multilingual news. In Proceedings of Recent Advances in Natural Language Processing, pages 487–492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>Conll-2011 shared task: Modeling unrestricted coreference in ontonotes.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--27</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8635" citStr="Pradhan et al., 2011" startWordPosition="1351" endWordPosition="1354">ghighi and Klein (2007) and distinguish between proper, nominal, and pronominal mentions. Each requires different types of information to be resolved. Thus, the task involves determining anaphoricity, resolving pronouns, and identifying semantic compatibility among mentions. To resolve these references, one typically exploits contextual and grammatical clues, as well as semantic information and world knowledge, to understand whether mentions refer to people, places, organizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also </context>
<context position="23067" citStr="Pradhan et al., 2011" startWordPosition="3762" endWordPosition="3765"> of MIRA. We set the same costs for coreference mistakes as Durrett and Klein (2013), and a unit cost for missing the correct speaker of a quotation. For speeding up decoding, we first train a basic pruner for the coreference system (using only the features described in §4.1.1), limiting the number of candidate antecedents to 10, and discarding scores whose difference with respect to the best antecedent is below a threshold. We also freeze 6 Experiments 6.1 Dataset We used the 597 documents of the Wall Street Journal (WSJ) corpus that were disclosed for the CoNLL-2011 coreference shared task (Pradhan et al., 2011) as a dataset for coreference resolution. This dataset includes train, development and test partitions, annotated with coreference information, as well as gold and automatically generated syntactic and semantic information. The CoNLL-2011 corpus does not contain annotations of quotation attribution. For that reason, we used the WSJ quotation annotations in the PARC dataset (Pareti, 2012). We used the same version of the corpus as O’Keefe et al. (2012), but with different splits, to match the dataset partitions in the coreference resolution data. This attribution corpus contains 279 documents o</context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. Conll-2011 shared task: Modeling unrestricted coreference in ontonotes. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–27. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task,</booktitle>
<pages>1--40</pages>
<contexts>
<context position="8658" citStr="Pradhan et al., 2012" startWordPosition="1355" endWordPosition="1358">) and distinguish between proper, nominal, and pronominal mentions. Each requires different types of information to be resolved. Thus, the task involves determining anaphoricity, resolving pronouns, and identifying semantic compatibility among mentions. To resolve these references, one typically exploits contextual and grammatical clues, as well as semantic information and world knowledge, to understand whether mentions refer to people, places, organizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decent</context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes. In Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind K Joshi</author>
<author>Bonnie L Webber</author>
</authors>
<title>The penn discourse treebank 2.0. In LREC.</title>
<date>2008</date>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6714" citStr="Prasad et al., 2008" startWordPosition="1047" endWordPosition="1050">rect quotations (Pareti et al., 2013), which are more involved and require syntactic or semantic patterns. In this work, we resort to direct and mixed quotations. Pareti (2012) defines quotation attributions in terms of their content span (the quotation text itself), their cue (a lexical anchor of the attribution relation, such as a reported speech verb), and the source span (the author of the quote). The same reference introduced the PARC dataset, which we use in our experiments (§6) and which is based on the annotation of a database of attribution relations from the Penn Discourse Treebank (Prasad et al., 2008). Several machine learning algorithms have been applied to this task, either framing the problem as classification (an independent decision for each quote), or sequence labeling (using greedy methods or linear-chain conditional random fields); see O’Keefe et al. (2012) for a comparison among these different methods. In this paper, we distinguish between mentionlevel quotation attribution, in which the candidate speakers are individual mentions, and entitylevel quotation attribution, in which they are entity clusters comprised of one or more mentions. With this distinction, we attempt to clarif</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber. 2008. The penn discourse treebank 2.0. In LREC. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Narrowing the modeling gap: A cluster-ranking approach to coreference resolution.</title>
<date>2011</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="9158" citStr="Rahman and Ng, 2011" startWordPosition="1433" endWordPosition="1436">nce resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and </context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011. Narrowing the modeling gap: A cluster-ranking approach to coreference resolution. Journal of Artificial Intelligence Research, 40(1):469–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Sarmento</author>
<author>Sergio Nunes</author>
<author>E Oliveira</author>
</authors>
<title>Automatic extraction of quotes and topics from news feeds.</title>
<date>2009</date>
<booktitle>In 4th Doctoral Symposium on Informatics Engineering.</booktitle>
<contexts>
<context position="1817" citStr="Sarmento et al., 2009" startWordPosition="264" endWordPosition="268">ves of the participants in the narrated event, and making the news sound objective. The ability of extracting and organizing these quotations is highly relevant for text mining applications, as it may aid journalists in fact-checking, help users browse news threads, and reduce human intervention in media monitoring. This involves assigning the correct speaker to each quote—a problem called quotation attribution (§2). There is significant literature devoted to this task, both for narrative genres (Mamede and Chaleira, 2004; Elson and McKeown, 2010) and newswire domains (Pouliquen et al., 2007; Sarmento et al., 2009; Schneider et al., 2010). While the earliest works focused on devising lexical and syntactic rules and hand-crafting grammars, there has been a recent shift toward machine learning approaches (Fernandes et al., 2011; O’Keefe et al., 2012; Pareti et al., 2013), with latest works reporting high accuracies for speaker identification in newswire (in the range 80–95% for direct and mixed quotes, according to O’Keefe et al. (2012)). Despite these encouraging results, quotation mining systems are not yet fully satisfactory, even when only direct quotes are considered. Part of the problem, as we next</context>
<context position="5979" citStr="Sarmento et al., 2009" startWordPosition="925" endWordPosition="928">e resulting problem as a logic program, which we tackle using a dual decomposition strategy (§5). We provide an empirical comparison between our method and baselines for each of the tasks and a pipeline system, defining suitable metrics for entity-level quotation attribution (§6). 2 Quotation Attribution The task of quotation attribution can be formally defined as follows. Given a document containing a sequence of quotations, (q1, ... , qL), and a set of candidate speakers, {s1, ... , sm}, the goal is to a assign a speaker to every quote. Previous work has handled direct and mixed quotations (Sarmento et al., 2009; O’Keefe et al., 2012), easily extractable with regular expressions for detecting quotation marks, as well as indirect quotations (Pareti et al., 2013), which are more involved and require syntactic or semantic patterns. In this work, we resort to direct and mixed quotations. Pareti (2012) defines quotation attributions in terms of their content span (the quotation text itself), their cue (a lexical anchor of the attribution relation, such as a reported speech verb), and the source span (the author of the quote). The same reference introduced the PARC dataset, which we use in our experiments </context>
</contexts>
<marker>Sarmento, Nunes, Oliveira, 2009</marker>
<rawString>Luis Sarmento, Sergio Nunes, and E Oliveira. 2009. Automatic extraction of quotes and topics from news feeds. In 4th Doctoral Symposium on Informatics Engineering.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nathan Schneider</author>
<author>Rebecca Hwa</author>
<author>Philip Gianfortoni</author>
</authors>
<publisher>W Black,</publisher>
<location>Dipanjan Das, Michael Heilman, Alan</location>
<marker>Schneider, Hwa, Gianfortoni, </marker>
<rawString>Nathan Schneider, Rebecca Hwa, Philip Gianfortoni, Dipanjan Das, Michael Heilman, Alan W Black,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick L Crabbe</author>
<author>Noah A Smith</author>
</authors>
<title>Visualizing topical quotations over time to understand news discourse.</title>
<date>2010</date>
<tech>Technical report, Technical Report CMU-LTI-01-103, CMU.</tech>
<marker>Crabbe, Smith, 2010</marker>
<rawString>Frederick L Crabbe, and Noah A Smith. 2010. Visualizing topical quotations over time to understand news discourse. Technical report, Technical Report CMU-LTI-01-103, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational linguistics,</journal>
<pages>27--4</pages>
<contexts>
<context position="8851" citStr="Soon et al., 2001" startWordPosition="1386" endWordPosition="1389">, and identifying semantic compatibility among mentions. To resolve these references, one typically exploits contextual and grammatical clues, as well as semantic information and world knowledge, to understand whether mentions refer to people, places, organizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be cor</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>656--664</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9719" citStr="Stoyanov et al. (2009)" startWordPosition="1529" endWordPosition="1532"> al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al., 2012; Durrett and Klein, 2013). These models define scores for each mention to link to its antecedent or to an artifical root symbol $ (in which case it is not anaphoric). The computation of the best tree can be done exactly with spanning tree algorithms, or by independently choosing the best antecedent (or the root) for each mention, if only left-to-right arcs are allowed. The </context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 656–664. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Simone Paolo Ponzetto</author>
<author>Massimo Poesio</author>
<author>Vladimir Eidelman</author>
<author>Alan Jern</author>
<author>Jason Smith</author>
<author>Xiaofeng Yang</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Bart: A modular toolkit for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Demo Session,</booktitle>
<pages>9--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9381" citStr="Versley et al., 2008" startWordPosition="1473" endWordPosition="1476">fiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al., 2012; Durrett and Klein, 2013). These model</context>
</contexts>
<marker>Versley, Ponzetto, Poesio, Eidelman, Jern, Smith, Yang, Moschitti, 2008</marker>
<rawString>Yannick Versley, Simone Paolo Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng Yang, and Alessandro Moschitti. 2008. Bart: A modular toolkit for coreference resolution. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Demo Session, pages 9–12. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>