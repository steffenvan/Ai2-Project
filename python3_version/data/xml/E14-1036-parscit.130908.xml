<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.983414">
Deterministic Parsing using PCFGs
</title>
<author confidence="0.990205">
Mark-Jan Nederhof and Martin McCaffery
</author>
<affiliation confidence="0.9993685">
School of Computer Science
University of St Andrews, UK
</affiliation>
<sectionHeader confidence="0.978848" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965888888889">
We propose the design of deterministic
constituent parsers that choose parser ac-
tions according to the probabilities of
parses of a given probabilistic context-free
grammar. Several variants are presented.
One of these deterministically constructs a
parse structure while postponing commit-
ment to labels. We investigate theoretical
time complexities and report experiments.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993730882353">
Transition-based dependency parsing (Yamada
and Matsumoto, 2003; Nivre, 2008) has attracted
considerable attention, not only due to its high ac-
curacy but also due to its small running time. The
latter is often realized through determinism, i.e.
for each configuration a unique next action is cho-
sen. The action may be a shift of the next word
onto the stack, or it may be the addition of a de-
pendency link between words.
Because of the determinism, the running time
is often linear or close to linear; most of the time
and space resources are spent on deciding the next
parser action. Generalizations that allow nonde-
terminism, while maintaining polynomial running
time, were proposed by (Huang and Sagae, 2010;
Kuhlmann et al., 2011).
This work has influenced, and has been in-
fluenced by, similar developments in constituent
parsing. The challenge here is to deterministi-
cally choose a shift or reduce action. As in the
case of dependency parsing, solutions to this prob-
lem are often expressed in terms of classifiers of
some kind. Common approaches involve maxi-
mum entropy (Ratnaparkhi, 1997; Tsuruoka and
Tsujii, 2005), decision trees (Wong and Wu, 1999;
Kalt, 2004), and support vector machines (Sagae
and Lavie, 2005).
The programming-languages community rec-
ognized early on that large classes of gram-
mars allow deterministic, i.e. linear-time, pars-
ing, provided parsing decisions are postponed as
long as possible. This has led to (deterministic)
LR(k) parsing (Knuth, 1965; Sippu and Soisalon-
Soininen, 1990), which is a form of shift-reduce
parsing. Here the parser needs to commit to a
grammar rule only after all input covered by the
right-hand side of that rule has been processed,
while it may consult the next k symbols (the
lookahead). LR is the optimal, i.e. most determin-
istic, parsing strategy that has this property. De-
terministic LR parsing has also been considered
relevant to psycholinguistics (Shieber, 1983).
Nondeterministic variants of LR(k) parsing, for
use in natural language processing, have been
proposed as well, some using tabulation to en-
sure polynomial running time in the length of
the input string (Tomita, 1988; Billot and Lang,
1989). However, nondeterministic LR(k) pars-
ing is potentially as expensive as, and possibly
more expensive than, traditional tabular parsing
algorithms such as CKY parsing (Younger, 1967;
Aho and Ullman, 1972), as shown by for exam-
ple (Shann, 1991); greater values of k make mat-
ters worse (Lankhorst, 1991). For this reason, LR
parsing is sometimes enhanced by attaching prob-
abilities to transitions (Briscoe and Carroll, 1993),
which allows pruning of the search space (Lavie
and Tomita, 1993). This by itself is not uncon-
troversial, for several reasons. First, the space of
probability distributions expressible by a LR au-
tomaton is incomparable to that expressible by a
CFG (Nederhof and Satta, 2004). Second, because
an LR automaton may have many more transitions
than rules, more training data may be needed to
accurately estimate all parameters.
The approach we propose here retains some im-
portant properties of the above work on LR pars-
ing. First, parser actions are delayed as long as
</bodyText>
<page confidence="0.977712">
338
</page>
<note confidence="0.9930125">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 338–347,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999937487179487">
possible, under the constraint that a rule is com-
mitted to no later than when the input covered by
its right-hand side has been processed. Second, the
parser action that is performed at each step is the
most likely one, given the left context, the looka-
head, and a probability distribution over parses
given by a PCFG.
There are two differences with traditional LR
parsing however. First, there is no explicit repre-
sentation of LR states, and second, probabilities of
actions are computed dynamically from a PCFG
rather than retrieved as part of static transitions.
In particular, this is unlike some other early ap-
proaches to probabilistic LR parsing such as (Ng
and Tomita, 1991).
The mathematical framework is reminiscent of
that used to compute prefix probabilities (Jelinek
and Lafferty, 1991; Stolcke, 1995). One major dif-
ference is that instead of a prefix string, we now
have a stack, which does not need to be parsed. In
the first instance, this seems to make our problem
easier. For our purposes however, we need to add
new mechanisms in order to take lookahead into
consideration.
It is known, e.g. from (Cer et al., 2010; Candito
et al., 2010), that constituent parsing can be used
effectively to achieve dependency parsing. It is
therefore to be expected that our algorithms can be
used for dependency parsing as well. The parsing
steps of shift-reduce parsing with a binary gram-
mar are in fact very close to those of many depen-
dency parsing models. The major difference is,
again, that instead of general-purpose classifiers to
determine the next step, we would rely directly on
a PCFG.
The emphasis of this paper is on deriving the
necessary equations to build several variants of
deterministic shift-reduce parsers, all guided by a
PCFG. We also offer experimental results.
</bodyText>
<sectionHeader confidence="0.95212" genericHeader="method">
2 Shift-reduce parsing
</sectionHeader>
<bodyText confidence="0.983093161290323">
In this section, we summarize the theory of LR
parsing. As usual, a context-free grammar (CFG)
is represented by a 4-tuple (E, N, S, P), where
E and N are two disjoint finite sets of terminals
and nonterminals, respectively, S ∈ N is the start
symbol, and P is a finite set of rules, each of the
form A → α, where A ∈ N and α ∈ (E ∪ N)∗.
By grammar symbol we mean a terminal or non-
terminal. We use symbols A, B, C,... for non-
terminals, a, b, c,... for terminals, v, w, x,... for
strings of terminals, X for grammar symbols, and
α, β, γ,... for strings of grammar symbols. For
technical reasons, a CFG is often augmented by
an additional rule S† → S$, where S† ∈/ N and
$ ∈/ E. The symbol $ acts as an end-of-sentence
marker.
As usual, we have a (right-most) ‘derives’ re-
lation ⇒rm, ⇒∗rm denotes derivation in zero or
more steps, and ⇒+rm denotes derivation in one
or more steps. If d is a string of rules π1 · · · πk,
then α ⇒rm β means that β can be derived from
d
α by applying this list of rules in right-most order.
A string α such that S ⇒∗rm α is called a right-
sentential form.
The last rule A → β used in a derivation
S ⇒+rm α together with the position of (the rel-
evant occurrence of) β in α we call the han-
dle of the derivation. In more detail, such a
derivation can be written as S = A0 ⇒rm
α1A1β1 ⇒∗rm α1A1v1 ⇒rm α1α2A2β2v2 ⇒∗
</bodyText>
<equation confidence="0.66329">
rm
. . . ⇒∗ rmα1 ··· αk−1Ak−1vk−1 ··· v1 ⇒rm
α1 · · · αk−1βvk−1 · · · v1, where k ≥ 1, and
Ai−1 → αiAiβi (1 ≤ i &lt; k) and Ak−1 → β are in
</equation>
<bodyText confidence="0.984985703703704">
P. The underlined symbols are those that are (re-
cursively) rewritten to terminal strings within the
following relation ⇒rm or ⇒∗rm. The handle here
is Ak−1 → β, together with the position of β in
the right-sentential form, just after α1 · · · αk−1. A
prefix of α1 · · · αk−1β is called a viable prefix in
the derivation.
Given an input string w, a shift-reduce parser
finds a right-most derivation of w, but in reverse
order, identifying the last rules first. It manipulates
configurations of the form (α, v$), where α is a
viable prefix (in at least one derivation) and v is
a suffix of w. The initial configuration is (ε, w$),
where ε is the empty string. The two allowable
steps are (α, av$) ` (αa, v$), which is called a
shift, and (αβ, v$) ` (αA, v$) where A → β is in
P, which is called a reduce. Acceptance happens
upon reaching a configuration (S, $).
A 1-item has the form [A → α • β, a], where
A → αβ is a rule. The bullet separates the right-
hand side into two parts, the first of which has been
matched to processed input. The symbol a ∈ E ∪
{$} is called the follower.
In order to decide whether to apply a
shift or reduce after reaching a configuration
(X1 · · · Xk, w), one may construct the sets I0, ... ,
Ik, inductively defined as follows, with 0 ≤ i ≤ k:
</bodyText>
<listItem confidence="0.995808">
• if S → σ in P, then [S → • σ, $] ∈ I0,
</listItem>
<page confidence="0.944181">
339
</page>
<listItem confidence="0.9766826">
• if [A → α • BQ, a] ∈ Ii, B → γ in P, and
Q ⇒∗rm x, then [B → • γ, b] ∈ Ii, where
b = 1 : xa,
• if [A → α • XiQ, a] ∈ Ii−1 then [A →
αXi • Q, a] ∈ Ii.
</listItem>
<bodyText confidence="0.984627739130435">
(The expression 1 : y denotes a if y = az, for
some a and z; we leave it undefined for y = e.)
Exhaustive application of the second clause above
will be referred to as the closure of a set of items.
It is not difficult to show that if [A → α •, a] ∈
Ik, then α is of the form Xj+1 · · · Xk, some j,
and A → α at position j + 1 is the handle of at
least one derivation S ⇒∗rm X1 · · · Xkax, some
x. If furthermore a = 1 : w, where 1 : w is
called the lookahead of the current configuration
(X1 · · · Xk, w), then this justifies a reduce with
A → α, as a step that potentially leads to a com-
plete derivation; this is only ‘potentially’ because
the actual remaining input w may be unlike ax,
apart from the matching one-symbol lookahead.
Similarly, if [A → α • aQ, b] ∈ Ik, then
α = Xj+1 · · · Xk, some j, and if furthermore
a = 1 : w, then a shift of symbol a is a justifiable
step. Potentially, if a is followed by some x such
that Q ⇒∗rm x, then we may eventually obtain a
stack X1 · · · XjαaQ, which is a prefix of a right-
sentential form, with the handle being A → αaQ
at position j + 1.
For a fixed grammar, the collection of all pos-
sible sets of 1-items that may arise in processing
any viable prefix is a finite set. The technique
of LR(1) parsing relies on a precomputation of all
such sets of items, each of which is turned into a
state of the LR(1) automaton. The initial state con-
sists of closure({[S → • σ, $]  |S → σ ∈ P}).
The automaton has a transition labeled X from
I to J if goto(I, X) = J, where goto(I, X)
= closure({[A → αX • Q, a]  |[A → α •
XQ, a] ∈ I}). In the present study, we do not pre-
compute all possible states of the LR(1) automa-
ton, as this would require prohibitive amounts of
time and memory. Instead, our parsers are best
understood as computing LR states dynamically,
while furthermore attaching probabilities to indi-
vidual items.
In the sequel we will assume that all rules either
have the (lexical) form A → a, the (binary) form
A → BC, or the (unary) form A → B. This
means that A ⇒∗rm e is not possible for any A.
The end-of-sentence marker is now introduced by
two augmented rules S† → SS$ and S$ → $.
</bodyText>
<sectionHeader confidence="0.856863" genericHeader="method">
3 Probabilistic shift-reduce parsing
</sectionHeader>
<bodyText confidence="0.976664142857143">
A probabilistic CFG (PCFG) is a 5-tuple (E, N,
S, P, p), where the extra element p maps rules
to probabilities. The probability of a derivation
α ⇒rm Q, with d = 7r1 · · · 7rk, is defined to be
d
p(d) = Hi p(7ri). The probability p(w) of a string
w is defined to be the sum of p(d) for all d with
S d⇒rm w.
We assume properness, i.e. Eπ=A→α p(7r) =
1 for all A, and consistency, i.e. Ew p(w) = 1.
Properness and consistency together imply that for
each nonterminal A, the sum of p(d) for all d with
∃wA d⇒rm w equals 1. We will further assume an
augmented PCFG with extra rules S† → SS$ and
S$ → $ both having probability 1.
Consider a viable prefix A1 · · · Ak on the stack
of a shift-reduce parser, and lookahead a. Each
right-most derivation in which the handle is A →
Ak−1Ak at position k − 1 must be of the form
sketched in Figure 1.
Because of properness and consistency, we may
assume that all possible subderivations generat-
ing strings entirely to the right of the lookahead
have probabilities summing to 1. To compactly
express the remaining probabilities, we need addi-
tional notation. First we define:
V(C, D) = � p(d)
d: ∃wC d⇒rm Dw
for any pair of nonterminals C and D. This will
be used later to ‘factor out’ a common term in a
(potentially infinite) sum of probabilities of sub-
derivations; the w in the expression above corre-
sponds to a substring of the unknown input beyond
the lookahead. In order to compute such values,
we fix an ordering of the nonterminals by N =
{C1, ... , Cr}, with r = |N|. We then construct
a matrix M, such that Mi,j = Eπ=Ci→Cjα p(7r).
In words, we sum the probabilities of all rules that
have left-hand side Ci and a right-hand side begin-
ning with Cj.
A downward path in a parse tree from an oc-
currence of C to an occurrence of D, restricted
to following always the first child, can be of any
length n, including n = 0 if C = D. This means
we need to obtain the matrix M∗ = E0≤n Mn, and
V(Ci, Cj) = M∗i,j for all i and j. Fortunately, M∗i,j
can be effectively computed as (I − M)−1, where
I is the identity matrix of size r and the superscript
denotes matrix inversion.
</bodyText>
<page confidence="0.990989">
340
341
</page>
<equation confidence="0.96380556097561">
X
d: Cd⇒rm D
p(d)
We further define:
U(C, D) =
Y
is 1&lt;i&lt;m
V(Ei−1, Fi) · p(Fi → AiEi) ·
Y
is m&lt;i&lt;k−1
p(Fi → AiEi) · U(Ei, Fi+1) ·
� 1 if E = S† 0 otherwise
F(e, E) =
X
F(αA, E) =
F(α, E&apos;) · V(E&apos;, F) · p(7r)
X
E,B
E(αQ, a, F → Q)
H(α, E, B) · U(E, F) · p(7r) · L(B, a)
=
where:
X
L(B, a) =
V(B, B&apos;) · p(7r)
X
G(α, E, B) =
F(α, E&apos;) · V(E&apos;, F) · p(7r)
X
H(αA, E, B)=
H(α, E&apos;, B) · U(E&apos;, F) · p(7r)
F(α, B) · L(B, a)
X
E(α, a, shift) =
B
7r
E(α, a, 7r)
+ G(αA, E, B)
E&apos;,7r=F→AE
X
E(α, a) = E(α, a, shift) +
</equation>
<bodyText confidence="0.998199333333333">
much as above, but restricting attention to unit
rules.
The expected number of times a handle A →
Ak−1Ak at position k − 1 occurs in a right-most
derivation with viable prefix A1 · · · Ak and looka-
head a is now given by:
</bodyText>
<equation confidence="0.999517833333333">
E(A1 ··· Ak, a, A → Ak−1Ak) =
X
S† = E0, ... , Ek−2, F1, ... , Fk−1 = A,
F, E, B, B&apos;, m : 0 ≤ m &lt; k − 1
V(Em, F) · p(F → EB) · U(E, Fm+1) ·
p(Fk−1 → Ak−1Ak) · V(B, B&apos;) · p(B&apos; → a)
</equation>
<bodyText confidence="0.999527222222222">
Note that the value above is not a probability and
may exceed 1. This is because the same viable
prefix may occur several times in a single right-
most derivation.
At first sight, the computation of E seems to re-
quire an exponential number of steps in k. How-
ever, we can use an idea similar to that commonly
used for computation of forward probabilities for
HMMs (Rabiner, 1989). We first define F:
</bodyText>
<figure confidence="0.998563538461539">
E0
E
B
Fm+1
Am+1 Em+1
Fm
Am Em
F
Ek−2
Fk−1
Ak−1 Ak
B&apos;
a
</figure>
<figureCaption confidence="0.997954">
Figure 1: Right-most derivation leading to
Fk−1 → Ak−1Ak in viable prefix A1 · · · Ak with
lookahead a.
</figureCaption>
<bodyText confidence="0.910401">
Finally, we can express E in terms of these re-
cursive functions, considering the more general
case of any rule 7r = F → Q:
</bodyText>
<equation confidence="0.93832125">
F1
A1 E1
Em−1
E&apos;,7r=F→AE
</equation>
<bodyText confidence="0.984789333333333">
This corresponds to the part of the definition
of E involving A1, ... , Am, E0, ... , Em and
F1, ... , Fm. We build on this by defining:
</bodyText>
<equation confidence="0.837374">
7r=B&apos;→a
E&apos;,7r=F→EB
</equation>
<bodyText confidence="0.986144111111111">
One more recursive function is needed for
what was Am+1, ... , Ak−2, Em+1, ... , Ek−2 and
Fm+1, ... , Fk−2 in the earlier definition of E:
H(e, E, B) =G(e, E, B)
The expected number of times the handle is to
be found to the right of α, with the stack being α
and the lookahead symbol being a, is:
The expected number of times we see a stack α
with lookahead a is:
</bodyText>
<equation confidence="0.645969">
E(α, a, F → Q) = 0 if ¬∃γ α = γQ
</equation>
<bodyText confidence="0.999845642857143">
The probability that a reduce with rule 7r is the
correct action when the stack is α and the looka-
head is a is naturally E(α, a, 7r)/E(α, a) and the
probability that a shift is the correct action is
E(α, a, shift)/E(α, a). For determining the most
likely action we do not need to compute E(α, a);
it suffices to identify the maximum value among
E(α, a, shift) and E(α, a, 7r) for each rule 7r.
A deterministic shift-reduce parser can now be
constructed that always chooses the most likely
next action. For a given input string, the number
of actions performed by this parser is linear in the
input length.
A call of E may lead to a number of recursive
calls of F and H that is linear in the stack size
and thereby in the input length. Note however that
by remembering the values returned by these func-
tion between parser actions, one can ensure that
each additional element pushed on the stack re-
quires a bounded number of additional calls of the
auxiliary functions. Because only linearly many
elements are pushed on the stack, the time com-
plexity becomes linear in the input length.
Complexity analysis seems less favorable if we
consider the number of nonterminals. The defi-
nitions of G and H each involve four nontermi-
nals excluding the stack symbol A, so that the
time complexity is O(|w |· |N|4), where |w |is
the length of the input w. A finer analysis gives
O(|w |· (|N |· |P |+ |N|2 · kPk)), where kPk is
the maximum for all A of the number of rules
of the form F → AE. By splitting up G and
H into smaller functions, we obtain complexity
O(|w |· |N|3), which can still be prohibitive.
Therefore we have implemented an alternative
that has a time complexity that is only quadratic
in the size of the grammar, at the expense of a
quadratic complexity in the length of the input
string, as detailed in Appendix A. This is still
better in practice if the number of nonterminals is
much greater than the length of the input string, as
in the case of the grammars we investigated.
</bodyText>
<sectionHeader confidence="0.992298" genericHeader="method">
4 Structural determinism
</sectionHeader>
<bodyText confidence="0.999369608695652">
We have assumed so far that a deterministic shift-
reduce parser chooses a unique next action in each
configuration, an action being a shift or reduce.
Implicit in this was that if the next action is a re-
duce, then also a unique rule is chosen. However,
if we assume for now that all non-lexical rules
are binary, then we can easily generalize the pars-
ing algorithm to consider all possible rules whose
right-hand sides match the top-most two stack el-
ements, and postpone commitment to any of the
nonterminals in the left-hand sides. This requires
that stack elements now contain sets of grammar
symbols. Each of these is associated with the
probability of the most likely subderivation con-
sistent with the relevant substring of the input.
Each reduce with a binary rule is implicitly fol-
lowed by zero or more reduces with unary rules.
Similarly, each shift is implicitly followed by a re-
duce with a lexical rule and zero or more reduces
with unary rules; see also (Graham et al., 1980).
This uses a precompiled table similar to U, but us-
ing maximization in place of summation, defined
by:
</bodyText>
<equation confidence="0.9758165">
Umax(C, D) = max
d:Cd⇒rm D
</equation>
<bodyText confidence="0.9988546">
More concretely, configurations have the form
(Z1 ... Zk, v$), k ≥ 0, where each ZZ (1 ≤ i ≤ k)
is a set of pairs (A, p), where A is a nonterminal
and p is a (non-zero) probability; each A occurs
at most once in ZZ. A shift turns (α, av$) into
(αZ, v$), where Z consists of all pairs (E, p) such
that p = maxF Umax(E, F) · p(F → a). A gen-
eralized binary reduce now turns (αZ1Z2, v$) into
(αZ, v$), where Z consists of all pairs (E, p) such
that:
</bodyText>
<equation confidence="0.995870666666667">
p = max Umax(E, F) · p(7r) · p1 · p2
7r = F → A1A2,
(A1, p1) ∈ Z1, (A2,p2) ∈ Z2
</equation>
<bodyText confidence="0.999988461538462">
We characterize this parsing procedure as struc-
turally deterministic, as an unlabeled structure is
built deterministically in the first instance. The
exact choices of rules can be postponed until af-
ter reaching the end of the sentence. Then follows
a straightforward process of ‘backtracing’, which
builds the derivation that led to the computed prob-
ability associated with the start symbol.
The time complexity is now O(|w |· |N|5) in
the most straightforward implementation, but we
can reduce this to quadratic in the size of the gram-
mar provided we allow an additional factor |w |as
before. For more details see Appendix B.
</bodyText>
<sectionHeader confidence="0.996756" genericHeader="method">
5 Other variants
</sectionHeader>
<bodyText confidence="0.9999395">
One way to improve accuracy is to increase the
size of the lookahead, beyond the current 1, com-
parable to the generalization from LR(1) to LR(k)
parsing. The formulas are given in Appendix C.
</bodyText>
<equation confidence="0.785469">
p(d)
</equation>
<page confidence="0.981498">
342
</page>
<bodyText confidence="0.999972193548387">
Yet another variant investigates only the top-
most n stack symbols when choosing the next
parser action. In combination with Appendix A,
this brings the time complexity down again to lin-
ear time in the length of the input string. The re-
quired changes to the formulas are given in Ap-
pendix D. There is a slight similarity to (Schuler,
2009), in that no stack elements beyond a bounded
depth are considered at each parsing step, but in
our case the stack can still have arbitrary height.
Whereas we have concentrated on determinism
in this paper, one can also introduce a limited de-
gree of nondeterminism and allow some of the
most promising configurations at each input posi-
tion to compete, applying techniques such as beam
search (Roark, 2001; Zhang and Clark, 2009; Zhu
et al., 2013), best-first search (Sagae and Lavie,
2006), or A∗ search (Klein and Manning, 2003)
in order to keep the running time low. For com-
paring different configurations, one would need to
multiply the values E(α, a) as in Section 3 by the
probabilities of the subderivations associated with
occurrences of grammar symbols in stack α.
Further variants are obtained by replacing the
parsing strategy. One obvious candidate is left-
corner parsing (Rosenkrantz and Lewis II, 1970),
which is considerably simpler than LR parsing.
The resulting algorithm would be very different
from the left-corner models of e.g. (Henderson,
2003), which rely on neural networks instead of
PCFGs.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999926163265306">
We used the WSJ treebank from OntoNotes 4.0
(Hovy et al., 2006), with Sections 2-21 for train-
ing and the 2228 sentences of up to 40 words from
Section 23 for testing. Grammars with different
sizes, and in the required binary form, were ex-
tracted by using the tools from the Berkeley parser
(Petrov et al., 2006), with between 1 and 6 split-
merge cycles. These tools offer a framework for
handling unknown words, which we have adopted.
The implementation of the parsing algorithms
is in C++, running on a desktop with four 3.1GHz
Intel Core i5 CPUs. The main algorithm is that of
Appendix C, with lookahead k between 1 and 3,
also in combination with structural determinism
(Appendix B), which is indicated here by sd. The
variant that consults the stack down to bounded
depth n (Appendix D) will only be reported for
k = 1 and n = 5.
Bracketing recall, precision and F-measure, are
computed using evalb, with settings as in (Collins,
1997), except that punctuation was deleted.1 Ta-
ble 1 reports results.
A nonterminal B in the stack may occur in a
small number of rules of the form A → BC. The
C of one such rule is needed next in order to al-
low a reduction. If future input does not deliver
this C, then parsing may fail. This problem be-
comes more severe as nonterminals become more
specific, which is what happens with an increase of
the number of split-merge cycles. Even more fail-
ures are introduced by removing the ability to con-
sult the complete stack, which explains the poor
results in the case of k = 1, n = 5; lower values
of n lead to even more failures, and higher values
further increase the running time. That the running
time exceeds that of k = 1 is explained by the fact
that with the variant from Appendix D, every pop
or push requires a complete recomputation of all
function values.
Parse failures can be almost completely elimi-
nated however by choosing higher values of k and
by using structural determinism. A combination
thereof leads to high accuracy, not far below that
of the Viterbi parses. Note that one cannot expect
the accuracy of our deterministic parsers to exceed
that of Viterbi parses. Both rely on the same model
(a PCFG), but the first is forced to make local deci-
sions without access to the input string that follows
the bounded lookahead.
</bodyText>
<sectionHeader confidence="0.998587" genericHeader="method">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.968558315789474">
We have shown that deterministic parsers can be
constructed from a given PCFG. Much of the ac-
curacy of the grammar can be retained by choosing
a large lookahead in combination with ‘structural
determinism’, which postpones commitment to
nonterminals until the end of the input is reached.
Parsers of this nature potentially run in linear
time in the length of the input, but our parsers are
better implemented to run in quadratic time. In
terms of the grammar size, the experiments sug-
gest that the number of rules is the dominating fac-
tor. The size of the lookahead strongly affects run-
ning time. The extra time costs of structural deter-
minism are compensated by an increase in accu-
racy and a sharp decrease of the parse failures.
1Evalb otherwise stumbles over e.g. a part of speech con-
sisting of two single quotes in the parsed file, against a part
of speech ‘POS’ in the gold file, for an input token consisting
of a single quote.
</bodyText>
<page confidence="0.9996">
343
</page>
<tableCaption confidence="0.995274">
Table 1: Total time required (seconds), number of parse failures, recall, precision, F-measure, for deter-
ministic parsing, compared to the Viterbi parses as computed with the Berkeley parser.
</tableCaption>
<figure confidence="0.990866583333333">
time fail R P F1
1-split-merge (12,059 rules)
11 67.20 66.67 66.94
0 70.74 71.01 70.88
0 71.41 71.85 71.63
0 68.12 68.52 68.32
0 70.98 71.72 71.35
0 71.31 72.50 71.90
170 66.19 65.67 65.93
Viterbi 0 72.45 74.55 73.49
2-split-merge (32,994 rules)
time fail R P F1
</figure>
<equation confidence="0.93758775">
4-split-merge (269,162 rules)
k = 1 870 115 75.69 73.30 74.48
k = 2 2,257 1 83.48 82.35 82.91
k = 3 4,380 1 84.95 84.06 84.51
k = 1, sd 2,336 1 80.82 80.65 80.74
k = 2, sd 4,747 0 85.52 85.64 85.58
k = 3, sd 7,728 0 86.62 86.82 86.72
k = 1, n = 5 1,152 508 76.21 73.92 75.05
</equation>
<figure confidence="0.611655222222222">
Viterbi 0 87.95 88.10 88.02
5-split-merge (716,575 rules)
1 43
2 99
3 199
1, sd 62
2, sd 135
3, sd 253
1, n = 5 56
</figure>
<equation confidence="0.989274666666667">
k =
k =
k =
k =
k =
k =
k =
k = 1
k = 2
k = 3
k = 1, sd
k = 2, sd
k = 3, sd
k = 1, n
Viterbi
</equation>
<bodyText confidence="0.38621975">
120 33
275 1
568 0
196 0
439 0
770 0
= 5 146 247
0
</bodyText>
<footnote confidence="0.60070675">
72.65 70.50 71.56
78.44 77.26 77.84
79.81 79.27 79.54
74.78 74.96 74.87
79.96 80.40 80.18
80.49 81.20 80.85
72.27 70.34 71.29
82.16 82.69 82.43
</footnote>
<equation confidence="0.994848470588235">
k = 1 3,166 172 76.17 73.44 74.78
k = 2 7,476 2 84.14 82.80 83.46
k = 3 14,231 1 86.05 85.24 85.64
k = 1, sd 7,427 1 81.99 81.44 81.72
k = 2, sd 14,587 0 86.89 87.00 86.95
k = 3, sd 24,553 0 87.67 87.82 87.74
k = 1, n = 5 4,572 559 77.65 75.13 76.37
Viterbi 0 88.65 89.00 88.83
3-split-merge (95,647 rules) 6-split-merge (1,947,915 rules)
k = 1
k = 2
k = 3
k = 1, sd
k = 2, sd
k = 3, sd
k = 1, n
Viterbi
</equation>
<bodyText confidence="0.821311125">
305 75
770 3
1,596 0
757 0
1,531 0
2,595 0
= 5 404 401
0
74.39 72.33 73.35
81.32 80.35 80.83
82.78 82.35 82.56
78.11 78.37 78.24
82.85 83.39 83.12
83.66 84.25 83.96
74.52 72.39 73.44
85.38 86.03 85.71
</bodyText>
<equation confidence="0.996416">
k = 1 7,741 274 76.60 74.08 75.32
k = 2 19,440 5 84.60 83.17 83.88
k = 3 35,712 0 86.02 85.07 85.54
k = 1, sd 19,530 1 82.64 81.95 82.29
k = 2, sd 39,615 0 87.36 87.20 87.28
k = 3, sd 64,906 0 88.16 88.26 88.21
k = 1, n = 5 10,897 652 77.89 75.57 76.71
</equation>
<bodyText confidence="0.953925">
Viterbi 0 88.69 88.99 88.84
There are many advantages over other ap-
proaches to deterministic parsing that rely on
general-purpose classifiers. First, some state-of-
the-art language models are readily available as
PCFGs. Second, most classifiers require tree-
banks, whereas our algorithms are also applicable
to PCFGs that were obtained in any other way, for
example through intersection of language models.
Lastly, our algorithms fit within well understood
automata theory.
Acknowledgments We thank the reviewers.
nonterminal B is now done in G:
</bodyText>
<equation confidence="0.992927222222222">
� 1 if E = S†
0 otherwise
� F&apos;(α, F) · p(7r)
7r=F→AE
� F(α, E) · V(E, F)
E
F&apos;(α, F) · G&apos;(F, E, a)
F
� p(7r) · L(B, a)
F(ε, E) =
F(αA, E) =
F&apos;(α, F) =
�G(α, E, a) =
G&apos;(F, E, a) =
7r=F→EB
H(ε, E, a) =
H(αA, E, a)
�H&apos;(α, F, a) =
</equation>
<bodyText confidence="0.9230485">
A Formulas for quadratic time
complexity
The following are the formulas that correspond
to the first implemented variant. Relative to Sec-
tion 3, some auxiliary functions are broken up, and
associating the lookahead a with an appropriate
</bodyText>
<equation confidence="0.836868909090909">
G(ε, E, a)
H&apos;(α, F, a) · p(7r)
7r=F→AE
+ G(αA, E, a)
H(α, E, a) · U(E, F)
E
344
E(αO, a, F → O) = H&apos;(α, F, a) · p(F → O)
E(α, a, F → O) = 0 if ¬∃γ α = γO
E(αA, a, shift) = G(α, A, a)
E(e, a, shift) = L(5†, a)
</equation>
<bodyText confidence="0.999891083333333">
These equations correspond to a time complex-
ity of O(|w|2 · |N|2 + |w |· |P |). Each definition
except that of G&apos; involves one stack (of linear size)
and, at most, one terminal plus two arbitrary non-
terminals. The full grammar is only considered
once for every input position, in the definition of
G&apos;.
The values are stored as vectors and matrices.
For example, for each distinct lookahead symbol
a, there is a (sparse) matrix containing the value of
G&apos;(F, E, a) at a row and a column uniquely iden-
tified by F and E, respectively.
</bodyText>
<sectionHeader confidence="0.95033" genericHeader="method">
B Formulas for structural determinism
</sectionHeader>
<bodyText confidence="0.9999382">
be the sum of p(d) for all d such that B d⇒rm v.
If I is given for all prefixes of a fixed lookahead
string of length k (this requires cubic time in k),
we can compute L in linear time for all suffixes of
the same string:
</bodyText>
<equation confidence="0.944092857142857">
L(B, v) = X V(B, B&apos;) · L&apos;(B&apos;, v)
B&apos;
L&apos;(B, v) = X p(7r) · I(B1, v1) · L(B2, v2)
7r=B→B1B2,v1,v2: if |v |&gt; 1
v=v1v2,1&lt;|v1|,1&lt;|v2|
L&apos;(B, a) = X p(7r)
7r=B→a
</equation>
<bodyText confidence="0.99713775">
The function H is generalized straightforwardly
by letting it pass on a string v (1 ≤ |v |≤ k) in-
stead of a single terminal a. The same holds for E.
The function G requires a slightly bigger modifica-
tion, leading back to H if not all of the lookahead
has been matched yet:
For the variant from Section 4, we need to change XG(α, E, v) = F&apos;(α, F) · G&apos;(F, E, v) +
only two definitions of auxiliary functions: F
</bodyText>
<equation confidence="0.627724">
F(αZ, E) = X F&apos;(α, F) · p(7r) · p X H&apos;(α, F, v2) · G&apos;&apos;(F, E, v1)
(A,p)EZ,7r=F→AE F,v1,v2:v=v1v2,|v2|&gt;0
</equation>
<table confidence="0.8390966">
X H&apos;(α, F, a) · p(7r) · p X p(7r) · L(B, v)
H(αZ, E, a) = G&apos;(F, E, v) =
7r=F→EB
(A,p)EZ,7r=F→AE XG&apos;&apos;(F, E, v) = p(7r) · I(B,v)
+ G(αZ, E, a) 7r=F→EB
</table>
<bodyText confidence="0.996258">
The only actions are shift and generalized bi-
nary reduce red. The definition of E becomes:
</bodyText>
<equation confidence="0.9363056">
XE(αZ1Z2, a, red)= H&apos;(α, F, a) · p(7r) · p1 · p2
(A1,p1)EZ1,(A2,p2)EZ2
7r=F→A1A2
E(αZ, a, shift) = X G(α, A, a) · p
(A,p)EZ
</equation>
<bodyText confidence="0.663847">
The time complexity now increases to
O(|w|2 · (|N|2 + |P|)) due to the new H.
</bodyText>
<sectionHeader confidence="0.905164" genericHeader="method">
C Formulas for larger lookahead
</sectionHeader>
<bodyText confidence="0.978360421052632">
In order to handle k symbols of lookahead (Sec-
tion 5) some technical problems are best avoided
by having k copies of the end-of-sentence marker
appended behind the input string, with a corre-
sponding augmentation of the grammar. We gen-
eralize L(B, v) to be the sum of p(d) for all d
such that B d⇒rm vx, some x. We let I(B, v)
The time complexity is now O(k · |w|2 · |N|2 +
k3 · |w |· |P|).
D Investigation of top-most n stack
symbols only
As discussed in Section 5, we want to predict the
next parser action without consulting any symbols
in α, when the current stack is αO, with |O |=
n. This is achieved by approximating F(α, E) by
the outside value of E, that is, the sum of p(d)
for all d such that ∃α,w5 d⇒rm αEw. Similarly,
H&apos;(α, F, v) is approximated by PE G(α, E, v) ·
W(E, F) where:
</bodyText>
<equation confidence="0.888616">
W(C,D) = X p(d)
</equation>
<bodyText confidence="0.865259333333333">
d: ∃δC d⇒rm SD
The time complexity (with lookahead k) is now
O(k · n · |w |· |N|2 + k3 · |w |· |P|).
</bodyText>
<page confidence="0.996003">
345
</page>
<note confidence="0.670870444444444">
D. Klein and C.D. Manning. 2003. A* parsing: Fast
exact Viterbi parse selection. In Proceedings of the
2003 Human Language Technology Conference of
the North American Chapter of the ACL, pages 40–
47, Edmonton, Canada, May–June.
References
A.V. Aho and J.D. Ullman. 1972. Parsing, volume 1 of
The Theory of Parsing, Translation and Compiling.
Prentice-Hall, Englewood Cliffs, N.J.
</note>
<reference confidence="0.999802505154639">
S. Billot and B. Lang. 1989. The structure of
shared forests in ambiguous parsing. In 27th An-
nual Meeting of the ACL, Proceedings of the Confer-
ence, pages 143–151, Vancouver, British Columbia,
Canada, June.
T. Briscoe and J. Carroll. 1993. Generalized prob-
abilistic LR parsing of natural language (corpora)
with unification-based grammars. Computational
Linguistics, 19(1):25–59.
M. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010. Benchmarking of statistical de-
pendency parsers for French. In The 23rd Inter-
national Conference on Computational Linguistics,
pages 108–116, Beijing, China, August.
D. Cer, M.-C. de Marneffe, D. Jurafsky, and C. Man-
ning. 2010. Parsing to Stanford dependen-
cies: Trade-offs between speed and accuracy. In
LREC 2010: Seventh International Conference on
Language Resources and Evaluation, Proceedings,
pages 1628–1632, Valletta, Malta, May.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In 35th Annual Meeting of the
ACL, Proceedings of the Conference, pages 16–23,
Madrid, Spain, July.
S.L. Graham, M.A. Harrison, and W.L. Ruzzo. 1980.
An improved context-free recognizer. ACM Trans-
actions on Programming Languages and Systems,
2:415–462.
J. Henderson. 2003. Generative versus discrimina-
tive models for statistical left-corner parsing. In
8th International Workshop on Parsing Technolo-
gies, pages 115–126, LORIA, Nancy, France, April.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference,
pages 57–60, New York, USA, June.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings
of the 48th Annual Meeting of the ACL, pages 1077–
1086, Uppsala, Sweden, July.
F. Jelinek and J.D. Lafferty. 1991. Computation
of the probability of initial substring generation by
stochastic context-free grammars. Computational
Linguistics, 17(3):315–323.
T. Kalt. 2004. Induction of greedy controllers for de-
terministic treebank parsers. In Conference on Em-
pirical Methods in Natural Language Processing,
pages 17–24, Barcelona, Spain, July.
D.E. Knuth. 1965. On the translation of languages
from left to right. Information and Control, 8:607–
639.
M. Kuhlmann, C. G´omez-Rodriguez, and G. Satta.
2011. Dynamic programming algorithms for
transition-based dependency parsers. In 49th An-
nual Meeting of the ACL, Proceedings of the Con-
ference, pages 673–682, Portland, Oregon, June.
M. Lankhorst. 1991. An empirical comparison of gen-
eralized LR tables. In R. Heemels, A. Nijholt, and
K. Sikkel, editors, Tomita’s Algorithm: Extensions
and Applications, Proc. of the first Twente Work-
shop on Language Technology, pages 87–93. Uni-
versity of Twente, September.
A. Lavie and M. Tomita. 1993. GLR* – an effi-
cient noise-skipping parsing algorithm for context
free grammars. In Third International Workshop on
Parsing Technologies, pages 123–134, Tilburg (The
Netherlands) and Durbuy (Belgium), August.
M.-J. Nederhof and G. Satta. 2004. An alternative
method of training probabilistic LR parsers. In 42nd
Annual Meeting of the ACL, Proceedings of the Con-
ference, pages 551–558, Barcelona, Spain, July.
S.-K. Ng and M. Tomita. 1991. Probabilistic LR pars-
ing for general context-free grammars. In Proc. of
the Second International Workshop on Parsing Tech-
nologies, pages 154–163, Cancun, Mexico, Febru-
ary.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513–553.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the ACL, pages 433–440, Sydney,
Australia, July.
L.R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257–286, February.
A. Ratnaparkhi. 1997. A linear observed time statis-
tical parser based on maximum entropy models. In
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, pages 1–
10, Providence, Rhode Island, USA, August.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
</reference>
<page confidence="0.988874">
346
</page>
<reference confidence="0.99993646875">
D.J. Rosenkrantz and P.M. Lewis II. 1970. Determin-
istic left corner parsing. In IEEE Conference Record
of the 11th Annual Symposium on Switching andAu-
tomata Theory, pages 139–152.
K. Sagae and A. Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies, pages 125–132, Vancouver, British
Columbia, Canada, October.
K. Sagae and A. Lavie. 2006. A best-first probabilistic
shift-reduce parser. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and 44th Annual Meeting of the ACL, pages 691–
698, Sydney, Australia, July.
W. Schuler. 2009. Positive results for parsing with
a bounded stack using a model-based right-corner
transform. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the ACL, pages 344–
352, Boulder, Colorado, May–June.
P. Shann. 1991. Experiments with GLR and chart pars-
ing. In M. Tomita, editor, Generalized LR Parsing,
chapter 2, pages 17–34. Kluwer Academic Publish-
ers.
S.M. Shieber. 1983. Sentence disambiguation by
a shift-reduce parsing technique. In 21st Annual
Meeting of the ACL, Proceedings of the Conference,
pages 113–118, Cambridge, Massachusetts, July.
S. Sippu and E. Soisalon-Soininen. 1990. Parsing The-
ory, Vol. II: LR(k) and LL(k) Parsing, volume 20 of
EATCS Monographs on Theoretical Computer Sci-
ence. Springer-Verlag.
A. Stolcke. 1995. An efficient probabilistic context-
free parsing algorithm that computes prefix proba-
bilities. Computational Linguistics, 21(2):167–201.
M. Tomita. 1988. Graph-structured stack and natu-
ral language parsing. In 26th Annual Meeting of
the ACL, Proceedings of the Conference, pages 249–
257, Buffalo, New York, June.
Y. Tsuruoka and J. Tsujii. 2005. Chunk parsing re-
visited. In Proceedings of the Ninth International
Workshop on Parsing Technologies, pages 133–140,
Vancouver, British Columbia, Canada, October.
A. Wong and D. Wu. 1999. Learning a lightweight
robust deterministic parser. In Sixth European Con-
ference on Speech Communication and Technology,
pages 2047–2050.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
8th International Workshop on Parsing Technolo-
gies, pages 195–206, LORIA, Nancy, France, April.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10:189–208.
Y. Zhang and S. Clark. 2009. Transition-based pars-
ing of the Chinese treebank using a global discrimi-
native model. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, pages
162–171, Paris, France, October.
M. Zhu, Y. Zhang, W. Chen, M. Zhang, and J. Zhu.
2013. Fast and accurate shift-reduce constituent
parsing. In 51st Annual Meeting of the ACL, Pro-
ceedings of the Conference, volume 1, pages 434–
443, Sofia, Bulgaria, August.
</reference>
<page confidence="0.998578">
347
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.775519">
<title confidence="0.999093">Deterministic Parsing using PCFGs</title>
<author confidence="0.820523">Nederhof</author>
<affiliation confidence="0.9744325">School of Computer University of St Andrews, UK</affiliation>
<abstract confidence="0.9995443">We propose the design of deterministic constituent parsers that choose parser actions according to the probabilities of parses of a given probabilistic context-free grammar. Several variants are presented. One of these deterministically constructs a parse structure while postponing commitment to labels. We investigate theoretical time complexities and report experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Billot</author>
<author>B Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In 27th Annual Meeting of the ACL, Proceedings of the Conference,</booktitle>
<pages>143--151</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="2697" citStr="Billot and Lang, 1989" startWordPosition="418" endWordPosition="421">parsing. Here the parser needs to commit to a grammar rule only after all input covered by the right-hand side of that rule has been processed, while it may consult the next k symbols (the lookahead). LR is the optimal, i.e. most deterministic, parsing strategy that has this property. Deterministic LR parsing has also been considered relevant to psycholinguistics (Shieber, 1983). Nondeterministic variants of LR(k) parsing, for use in natural language processing, have been proposed as well, some using tabulation to ensure polynomial running time in the length of the input string (Tomita, 1988; Billot and Lang, 1989). However, nondeterministic LR(k) parsing is potentially as expensive as, and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by attaching probabilities to transitions (Briscoe and Carroll, 1993), which allows pruning of the search space (Lavie and Tomita, 1993). This by itself is not uncontroversial, for several reasons. First, the space of probability distributions exp</context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>S. Billot and B. Lang. 1989. The structure of shared forests in ambiguous parsing. In 27th Annual Meeting of the ACL, Proceedings of the Conference, pages 143–151, Vancouver, British Columbia, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="3119" citStr="Briscoe and Carroll, 1993" startWordPosition="483" endWordPosition="486">parsing, for use in natural language processing, have been proposed as well, some using tabulation to ensure polynomial running time in the length of the input string (Tomita, 1988; Billot and Lang, 1989). However, nondeterministic LR(k) parsing is potentially as expensive as, and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by attaching probabilities to transitions (Briscoe and Carroll, 1993), which allows pruning of the search space (Lavie and Tomita, 1993). This by itself is not uncontroversial, for several reasons. First, the space of probability distributions expressible by a LR automaton is incomparable to that expressible by a CFG (Nederhof and Satta, 2004). Second, because an LR automaton may have many more transitions than rules, more training data may be needed to accurately estimate all parameters. The approach we propose here retains some important properties of the above work on LR parsing. First, parser actions are delayed as long as 338 Proceedings of the 14th Confer</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>T. Briscoe and J. Carroll. 1993. Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25–59.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Candito</author>
<author>J Nivre</author>
<author>P Denis</author>
<author>E Henestroza Anguiano</author>
</authors>
<title>Benchmarking of statistical dependency parsers for French.</title>
<date>2010</date>
<booktitle>In The 23rd International Conference on Computational Linguistics,</booktitle>
<pages>108--116</pages>
<location>Beijing, China,</location>
<contexts>
<context position="5055" citStr="Candito et al., 2010" startWordPosition="802" endWordPosition="805"> as part of static transitions. In particular, this is unlike some other early approaches to probabilistic LR parsing such as (Ng and Tomita, 1991). The mathematical framework is reminiscent of that used to compute prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995). One major difference is that instead of a prefix string, we now have a stack, which does not need to be parsed. In the first instance, this seems to make our problem easier. For our purposes however, we need to add new mechanisms in order to take lookahead into consideration. It is known, e.g. from (Cer et al., 2010; Candito et al., 2010), that constituent parsing can be used effectively to achieve dependency parsing. It is therefore to be expected that our algorithms can be used for dependency parsing as well. The parsing steps of shift-reduce parsing with a binary grammar are in fact very close to those of many dependency parsing models. The major difference is, again, that instead of general-purpose classifiers to determine the next step, we would rely directly on a PCFG. The emphasis of this paper is on deriving the necessary equations to build several variants of deterministic shift-reduce parsers, all guided by a PCFG. W</context>
</contexts>
<marker>Candito, Nivre, Denis, Anguiano, 2010</marker>
<rawString>M. Candito, J. Nivre, P. Denis, and E. Henestroza Anguiano. 2010. Benchmarking of statistical dependency parsers for French. In The 23rd International Conference on Computational Linguistics, pages 108–116, Beijing, China, August. D. Cer, M.-C. de Marneffe, D. Jurafsky, and C. Manning. 2010. Parsing to Stanford dependencies: Trade-offs between speed and accuracy. In LREC 2010: Seventh International Conference on Language Resources and Evaluation, Proceedings, pages 1628–1632, Valletta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In 35th Annual Meeting of the ACL, Proceedings of the Conference,</booktitle>
<pages>16--23</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="22013" citStr="Collins, 1997" startWordPosition="4148" endWordPosition="4149">n 1 and 6 splitmerge cycles. These tools offer a framework for handling unknown words, which we have adopted. The implementation of the parsing algorithms is in C++, running on a desktop with four 3.1GHz Intel Core i5 CPUs. The main algorithm is that of Appendix C, with lookahead k between 1 and 3, also in combination with structural determinism (Appendix B), which is indicated here by sd. The variant that consults the stack down to bounded depth n (Appendix D) will only be reported for k = 1 and n = 5. Bracketing recall, precision and F-measure, are computed using evalb, with settings as in (Collins, 1997), except that punctuation was deleted.1 Table 1 reports results. A nonterminal B in the stack may occur in a small number of rules of the form A → BC. The C of one such rule is needed next in order to allow a reduction. If future input does not deliver this C, then parsing may fail. This problem becomes more severe as nonterminals become more specific, which is what happens with an increase of the number of split-merge cycles. Even more failures are introduced by removing the ability to consult the complete stack, which explains the poor results in the case of k = 1, n = 5; lower values of n l</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In 35th Annual Meeting of the ACL, Proceedings of the Conference, pages 16–23, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Graham</author>
<author>M A Harrison</author>
<author>W L Ruzzo</author>
</authors>
<title>An improved context-free recognizer.</title>
<date>1980</date>
<journal>ACM Transactions on Programming Languages and Systems,</journal>
<pages>2--415</pages>
<contexts>
<context position="18108" citStr="Graham et al., 1980" startWordPosition="3451" endWordPosition="3454">to consider all possible rules whose right-hand sides match the top-most two stack elements, and postpone commitment to any of the nonterminals in the left-hand sides. This requires that stack elements now contain sets of grammar symbols. Each of these is associated with the probability of the most likely subderivation consistent with the relevant substring of the input. Each reduce with a binary rule is implicitly followed by zero or more reduces with unary rules. Similarly, each shift is implicitly followed by a reduce with a lexical rule and zero or more reduces with unary rules; see also (Graham et al., 1980). This uses a precompiled table similar to U, but using maximization in place of summation, defined by: Umax(C, D) = max d:Cd⇒rm D More concretely, configurations have the form (Z1 ... Zk, v$), k ≥ 0, where each ZZ (1 ≤ i ≤ k) is a set of pairs (A, p), where A is a nonterminal and p is a (non-zero) probability; each A occurs at most once in ZZ. A shift turns (α, av$) into (αZ, v$), where Z consists of all pairs (E, p) such that p = maxF Umax(E, F) · p(F → a). A generalized binary reduce now turns (αZ1Z2, v$) into (αZ, v$), where Z consists of all pairs (E, p) such that: p = max Umax(E, F) · p(</context>
</contexts>
<marker>Graham, Harrison, Ruzzo, 1980</marker>
<rawString>S.L. Graham, M.A. Harrison, and W.L. Ruzzo. 1980. An improved context-free recognizer. ACM Transactions on Programming Languages and Systems, 2:415–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
</authors>
<title>Generative versus discriminative models for statistical left-corner parsing.</title>
<date>2003</date>
<booktitle>In 8th International Workshop on Parsing Technologies,</booktitle>
<pages>115--126</pages>
<location>LORIA, Nancy, France,</location>
<contexts>
<context position="21011" citStr="Henderson, 2003" startWordPosition="3971" endWordPosition="3972">best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configurations, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very different from the left-corner models of e.g. (Henderson, 2003), which rely on neural networks instead of PCFGs. 6 Experiments We used the WSJ treebank from OntoNotes 4.0 (Hovy et al., 2006), with Sections 2-21 for training and the 2228 sentences of up to 40 words from Section 23 for testing. Grammars with different sizes, and in the required binary form, were extracted by using the tools from the Berkeley parser (Petrov et al., 2006), with between 1 and 6 splitmerge cycles. These tools offer a framework for handling unknown words, which we have adopted. The implementation of the parsing algorithms is in C++, running on a desktop with four 3.1GHz Intel Co</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>J. Henderson. 2003. Generative versus discriminative models for statistical left-corner parsing. In 8th International Workshop on Parsing Technologies, pages 115–126, LORIA, Nancy, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>OntoNotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>57--60</pages>
<location>New York, USA,</location>
<contexts>
<context position="21138" citStr="Hovy et al., 2006" startWordPosition="3991" endWordPosition="3994">comparing different configurations, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very different from the left-corner models of e.g. (Henderson, 2003), which rely on neural networks instead of PCFGs. 6 Experiments We used the WSJ treebank from OntoNotes 4.0 (Hovy et al., 2006), with Sections 2-21 for training and the 2228 sentences of up to 40 words from Section 23 for testing. Grammars with different sizes, and in the required binary form, were extracted by using the tools from the Berkeley parser (Petrov et al., 2006), with between 1 and 6 splitmerge cycles. These tools offer a framework for handling unknown words, which we have adopted. The implementation of the parsing algorithms is in C++, running on a desktop with four 3.1GHz Intel Core i5 CPUs. The main algorithm is that of Appendix C, with lookahead k between 1 and 3, also in combination with structural det</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2006. OntoNotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 57–60, New York, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the ACL,</booktitle>
<pages>1077--1086</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1237" citStr="Huang and Sagae, 2010" startWordPosition="187" endWordPosition="190">ed considerable attention, not only due to its high accuracy but also due to its small running time. The latter is often realized through determinism, i.e. for each configuration a unique next action is chosen. The action may be a shift of the next word onto the stack, or it may be the addition of a dependency link between words. Because of the determinism, the running time is often linear or close to linear; most of the time and space resources are spent on deciding the next parser action. Generalizations that allow nondeterminism, while maintaining polynomial running time, were proposed by (Huang and Sagae, 2010; Kuhlmann et al., 2011). This work has influenced, and has been influenced by, similar developments in constituent parsing. The challenge here is to deterministically choose a shift or reduce action. As in the case of dependency parsing, solutions to this problem are often expressed in terms of classifiers of some kind. Common approaches involve maximum entropy (Ratnaparkhi, 1997; Tsuruoka and Tsujii, 2005), decision trees (Wong and Wu, 1999; Kalt, 2004), and support vector machines (Sagae and Lavie, 2005). The programming-languages community recognized early on that large classes of grammars</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>L. Huang and K. Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the ACL, pages 1077– 1086, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J D Lafferty</author>
</authors>
<title>Computation of the probability of initial substring generation by stochastic context-free grammars.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="4697" citStr="Jelinek and Lafferty, 1991" startWordPosition="736" endWordPosition="739">s performed at each step is the most likely one, given the left context, the lookahead, and a probability distribution over parses given by a PCFG. There are two differences with traditional LR parsing however. First, there is no explicit representation of LR states, and second, probabilities of actions are computed dynamically from a PCFG rather than retrieved as part of static transitions. In particular, this is unlike some other early approaches to probabilistic LR parsing such as (Ng and Tomita, 1991). The mathematical framework is reminiscent of that used to compute prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995). One major difference is that instead of a prefix string, we now have a stack, which does not need to be parsed. In the first instance, this seems to make our problem easier. For our purposes however, we need to add new mechanisms in order to take lookahead into consideration. It is known, e.g. from (Cer et al., 2010; Candito et al., 2010), that constituent parsing can be used effectively to achieve dependency parsing. It is therefore to be expected that our algorithms can be used for dependency parsing as well. The parsing steps of shift-reduce parsing with a binary grammar a</context>
</contexts>
<marker>Jelinek, Lafferty, 1991</marker>
<rawString>F. Jelinek and J.D. Lafferty. 1991. Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics, 17(3):315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kalt</author>
</authors>
<title>Induction of greedy controllers for deterministic treebank parsers.</title>
<date>2004</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>17--24</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1696" citStr="Kalt, 2004" startWordPosition="263" endWordPosition="264">ng the next parser action. Generalizations that allow nondeterminism, while maintaining polynomial running time, were proposed by (Huang and Sagae, 2010; Kuhlmann et al., 2011). This work has influenced, and has been influenced by, similar developments in constituent parsing. The challenge here is to deterministically choose a shift or reduce action. As in the case of dependency parsing, solutions to this problem are often expressed in terms of classifiers of some kind. Common approaches involve maximum entropy (Ratnaparkhi, 1997; Tsuruoka and Tsujii, 2005), decision trees (Wong and Wu, 1999; Kalt, 2004), and support vector machines (Sagae and Lavie, 2005). The programming-languages community recognized early on that large classes of grammars allow deterministic, i.e. linear-time, parsing, provided parsing decisions are postponed as long as possible. This has led to (deterministic) LR(k) parsing (Knuth, 1965; Sippu and SoisalonSoininen, 1990), which is a form of shift-reduce parsing. Here the parser needs to commit to a grammar rule only after all input covered by the right-hand side of that rule has been processed, while it may consult the next k symbols (the lookahead). LR is the optimal, i</context>
</contexts>
<marker>Kalt, 2004</marker>
<rawString>T. Kalt. 2004. Induction of greedy controllers for deterministic treebank parsers. In Conference on Empirical Methods in Natural Language Processing, pages 17–24, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>On the translation of languages from left to right.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<volume>8</volume>
<pages>639</pages>
<contexts>
<context position="2006" citStr="Knuth, 1965" startWordPosition="308" endWordPosition="309">istically choose a shift or reduce action. As in the case of dependency parsing, solutions to this problem are often expressed in terms of classifiers of some kind. Common approaches involve maximum entropy (Ratnaparkhi, 1997; Tsuruoka and Tsujii, 2005), decision trees (Wong and Wu, 1999; Kalt, 2004), and support vector machines (Sagae and Lavie, 2005). The programming-languages community recognized early on that large classes of grammars allow deterministic, i.e. linear-time, parsing, provided parsing decisions are postponed as long as possible. This has led to (deterministic) LR(k) parsing (Knuth, 1965; Sippu and SoisalonSoininen, 1990), which is a form of shift-reduce parsing. Here the parser needs to commit to a grammar rule only after all input covered by the right-hand side of that rule has been processed, while it may consult the next k symbols (the lookahead). LR is the optimal, i.e. most deterministic, parsing strategy that has this property. Deterministic LR parsing has also been considered relevant to psycholinguistics (Shieber, 1983). Nondeterministic variants of LR(k) parsing, for use in natural language processing, have been proposed as well, some using tabulation to ensure poly</context>
</contexts>
<marker>Knuth, 1965</marker>
<rawString>D.E. Knuth. 1965. On the translation of languages from left to right. Information and Control, 8:607– 639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kuhlmann</author>
<author>C G´omez-Rodriguez</author>
<author>G Satta</author>
</authors>
<title>Dynamic programming algorithms for transition-based dependency parsers.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the ACL, Proceedings of the Conference,</booktitle>
<pages>673--682</pages>
<location>Portland, Oregon,</location>
<marker>Kuhlmann, G´omez-Rodriguez, Satta, 2011</marker>
<rawString>M. Kuhlmann, C. G´omez-Rodriguez, and G. Satta. 2011. Dynamic programming algorithms for transition-based dependency parsers. In 49th Annual Meeting of the ACL, Proceedings of the Conference, pages 673–682, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lankhorst</author>
</authors>
<title>An empirical comparison of generalized LR tables. In</title>
<date>1991</date>
<booktitle>Tomita’s Algorithm: Extensions and Applications, Proc. of the first Twente Workshop on Language Technology,</booktitle>
<pages>87--93</pages>
<editor>R. Heemels, A. Nijholt, and K. Sikkel, editors,</editor>
<institution>University of Twente,</institution>
<contexts>
<context position="2998" citStr="Lankhorst, 1991" startWordPosition="467" endWordPosition="468">ing has also been considered relevant to psycholinguistics (Shieber, 1983). Nondeterministic variants of LR(k) parsing, for use in natural language processing, have been proposed as well, some using tabulation to ensure polynomial running time in the length of the input string (Tomita, 1988; Billot and Lang, 1989). However, nondeterministic LR(k) parsing is potentially as expensive as, and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by attaching probabilities to transitions (Briscoe and Carroll, 1993), which allows pruning of the search space (Lavie and Tomita, 1993). This by itself is not uncontroversial, for several reasons. First, the space of probability distributions expressible by a LR automaton is incomparable to that expressible by a CFG (Nederhof and Satta, 2004). Second, because an LR automaton may have many more transitions than rules, more training data may be needed to accurately estimate all parameters. The approach we propose here retains some important pr</context>
</contexts>
<marker>Lankhorst, 1991</marker>
<rawString>M. Lankhorst. 1991. An empirical comparison of generalized LR tables. In R. Heemels, A. Nijholt, and K. Sikkel, editors, Tomita’s Algorithm: Extensions and Applications, Proc. of the first Twente Workshop on Language Technology, pages 87–93. University of Twente, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>M Tomita</author>
</authors>
<title>GLR* – an efficient noise-skipping parsing algorithm for context free grammars.</title>
<date>1993</date>
<booktitle>In Third International Workshop on Parsing Technologies,</booktitle>
<pages>123--134</pages>
<institution>Tilburg (The Netherlands) and Durbuy (Belgium),</institution>
<contexts>
<context position="3186" citStr="Lavie and Tomita, 1993" startWordPosition="494" endWordPosition="497"> well, some using tabulation to ensure polynomial running time in the length of the input string (Tomita, 1988; Billot and Lang, 1989). However, nondeterministic LR(k) parsing is potentially as expensive as, and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by attaching probabilities to transitions (Briscoe and Carroll, 1993), which allows pruning of the search space (Lavie and Tomita, 1993). This by itself is not uncontroversial, for several reasons. First, the space of probability distributions expressible by a LR automaton is incomparable to that expressible by a CFG (Nederhof and Satta, 2004). Second, because an LR automaton may have many more transitions than rules, more training data may be needed to accurately estimate all parameters. The approach we propose here retains some important properties of the above work on LR parsing. First, parser actions are delayed as long as 338 Proceedings of the 14th Conference of the European Chapter of the Association for Computational L</context>
</contexts>
<marker>Lavie, Tomita, 1993</marker>
<rawString>A. Lavie and M. Tomita. 1993. GLR* – an efficient noise-skipping parsing algorithm for context free grammars. In Third International Workshop on Parsing Technologies, pages 123–134, Tilburg (The Netherlands) and Durbuy (Belgium), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>An alternative method of training probabilistic LR parsers.</title>
<date>2004</date>
<booktitle>In 42nd Annual Meeting of the ACL, Proceedings of the Conference,</booktitle>
<pages>551--558</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3395" citStr="Nederhof and Satta, 2004" startWordPosition="528" endWordPosition="531"> and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by attaching probabilities to transitions (Briscoe and Carroll, 1993), which allows pruning of the search space (Lavie and Tomita, 1993). This by itself is not uncontroversial, for several reasons. First, the space of probability distributions expressible by a LR automaton is incomparable to that expressible by a CFG (Nederhof and Satta, 2004). Second, because an LR automaton may have many more transitions than rules, more training data may be needed to accurately estimate all parameters. The approach we propose here retains some important properties of the above work on LR parsing. First, parser actions are delayed as long as 338 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 338–347, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics possible, under the constraint that a rule is committed to no later than when the input covered</context>
</contexts>
<marker>Nederhof, Satta, 2004</marker>
<rawString>M.-J. Nederhof and G. Satta. 2004. An alternative method of training probabilistic LR parsers. In 42nd Annual Meeting of the ACL, Proceedings of the Conference, pages 551–558, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-K Ng</author>
<author>M Tomita</author>
</authors>
<title>Probabilistic LR parsing for general context-free grammars.</title>
<date>1991</date>
<booktitle>In Proc. of the Second International Workshop on Parsing Technologies,</booktitle>
<pages>154--163</pages>
<location>Cancun, Mexico,</location>
<contexts>
<context position="4581" citStr="Ng and Tomita, 1991" startWordPosition="720" endWordPosition="723"> later than when the input covered by its right-hand side has been processed. Second, the parser action that is performed at each step is the most likely one, given the left context, the lookahead, and a probability distribution over parses given by a PCFG. There are two differences with traditional LR parsing however. First, there is no explicit representation of LR states, and second, probabilities of actions are computed dynamically from a PCFG rather than retrieved as part of static transitions. In particular, this is unlike some other early approaches to probabilistic LR parsing such as (Ng and Tomita, 1991). The mathematical framework is reminiscent of that used to compute prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995). One major difference is that instead of a prefix string, we now have a stack, which does not need to be parsed. In the first instance, this seems to make our problem easier. For our purposes however, we need to add new mechanisms in order to take lookahead into consideration. It is known, e.g. from (Cer et al., 2010; Candito et al., 2010), that constituent parsing can be used effectively to achieve dependency parsing. It is therefore to be expected that our algo</context>
</contexts>
<marker>Ng, Tomita, 1991</marker>
<rawString>S.-K. Ng and M. Tomita. 1991. Probabilistic LR parsing for general context-free grammars. In Proc. of the Second International Workshop on Parsing Technologies, pages 154–163, Cancun, Mexico, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<marker>Nivre, 2008</marker>
<rawString>J. Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="21386" citStr="Petrov et al., 2006" startWordPosition="4036" endWordPosition="4039"> parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very different from the left-corner models of e.g. (Henderson, 2003), which rely on neural networks instead of PCFGs. 6 Experiments We used the WSJ treebank from OntoNotes 4.0 (Hovy et al., 2006), with Sections 2-21 for training and the 2228 sentences of up to 40 words from Section 23 for testing. Grammars with different sizes, and in the required binary form, were extracted by using the tools from the Berkeley parser (Petrov et al., 2006), with between 1 and 6 splitmerge cycles. These tools offer a framework for handling unknown words, which we have adopted. The implementation of the parsing algorithms is in C++, running on a desktop with four 3.1GHz Intel Core i5 CPUs. The main algorithm is that of Appendix C, with lookahead k between 1 and 3, also in combination with structural determinism (Appendix B), which is indicated here by sd. The variant that consults the stack down to bounded depth n (Appendix D) will only be reported for k = 1 and n = 5. Bracketing recall, precision and F-measure, are computed using evalb, with set</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<contexts>
<context position="14232" citStr="Rabiner, 1989" startWordPosition="2719" endWordPosition="2720">iable prefix A1 · · · Ak and lookahead a is now given by: E(A1 ··· Ak, a, A → Ak−1Ak) = X S† = E0, ... , Ek−2, F1, ... , Fk−1 = A, F, E, B, B&apos;, m : 0 ≤ m &lt; k − 1 V(Em, F) · p(F → EB) · U(E, Fm+1) · p(Fk−1 → Ak−1Ak) · V(B, B&apos;) · p(B&apos; → a) Note that the value above is not a probability and may exceed 1. This is because the same viable prefix may occur several times in a single rightmost derivation. At first sight, the computation of E seems to require an exponential number of steps in k. However, we can use an idea similar to that commonly used for computation of forward probabilities for HMMs (Rabiner, 1989). We first define F: E0 E B Fm+1 Am+1 Em+1 Fm Am Em F Ek−2 Fk−1 Ak−1 Ak B&apos; a Figure 1: Right-most derivation leading to Fk−1 → Ak−1Ak in viable prefix A1 · · · Ak with lookahead a. Finally, we can express E in terms of these recursive functions, considering the more general case of any rule 7r = F → Q: F1 A1 E1 Em−1 E&apos;,7r=F→AE This corresponds to the part of the definition of E involving A1, ... , Am, E0, ... , Em and F1, ... , Fm. We build on this by defining: 7r=B&apos;→a E&apos;,7r=F→EB One more recursive function is needed for what was Am+1, ... , Ak−2, Em+1, ... , Ek−2 and Fm+1, ... , Fk−2 in the e</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L.R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<location>Providence, Rhode Island, USA,</location>
<contexts>
<context position="1620" citStr="Ratnaparkhi, 1997" startWordPosition="251" endWordPosition="252">inear or close to linear; most of the time and space resources are spent on deciding the next parser action. Generalizations that allow nondeterminism, while maintaining polynomial running time, were proposed by (Huang and Sagae, 2010; Kuhlmann et al., 2011). This work has influenced, and has been influenced by, similar developments in constituent parsing. The challenge here is to deterministically choose a shift or reduce action. As in the case of dependency parsing, solutions to this problem are often expressed in terms of classifiers of some kind. Common approaches involve maximum entropy (Ratnaparkhi, 1997; Tsuruoka and Tsujii, 2005), decision trees (Wong and Wu, 1999; Kalt, 2004), and support vector machines (Sagae and Lavie, 2005). The programming-languages community recognized early on that large classes of grammars allow deterministic, i.e. linear-time, parsing, provided parsing decisions are postponed as long as possible. This has led to (deterministic) LR(k) parsing (Knuth, 1965; Sippu and SoisalonSoininen, 1990), which is a form of shift-reduce parsing. Here the parser needs to commit to a grammar rule only after all input covered by the right-hand side of that rule has been processed, w</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>A. Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 1– 10, Providence, Rhode Island, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="20351" citStr="Roark, 2001" startWordPosition="3866" endWordPosition="3867">ix A, this brings the time complexity down again to linear time in the length of the input string. The required changes to the formulas are given in Appendix D. There is a slight similarity to (Schuler, 2009), in that no stack elements beyond a bounded depth are considered at each parsing step, but in our case the stack can still have arbitrary height. Whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and allow some of the most promising configurations at each input position to compete, applying techniques such as beam search (Roark, 2001; Zhang and Clark, 2009; Zhu et al., 2013), best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configurations, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very dif</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>B. Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Rosenkrantz</author>
<author>P M Lewis</author>
</authors>
<title>Deterministic left corner parsing.</title>
<date>1970</date>
<booktitle>In IEEE Conference Record of the 11th Annual Symposium on Switching andAutomata Theory,</booktitle>
<pages>139--152</pages>
<marker>Rosenkrantz, Lewis, 1970</marker>
<rawString>D.J. Rosenkrantz and P.M. Lewis II. 1970. Deterministic left corner parsing. In IEEE Conference Record of the 11th Annual Symposium on Switching andAutomata Theory, pages 139–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technologies,</booktitle>
<pages>125--132</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1749" citStr="Sagae and Lavie, 2005" startWordPosition="269" endWordPosition="272"> that allow nondeterminism, while maintaining polynomial running time, were proposed by (Huang and Sagae, 2010; Kuhlmann et al., 2011). This work has influenced, and has been influenced by, similar developments in constituent parsing. The challenge here is to deterministically choose a shift or reduce action. As in the case of dependency parsing, solutions to this problem are often expressed in terms of classifiers of some kind. Common approaches involve maximum entropy (Ratnaparkhi, 1997; Tsuruoka and Tsujii, 2005), decision trees (Wong and Wu, 1999; Kalt, 2004), and support vector machines (Sagae and Lavie, 2005). The programming-languages community recognized early on that large classes of grammars allow deterministic, i.e. linear-time, parsing, provided parsing decisions are postponed as long as possible. This has led to (deterministic) LR(k) parsing (Knuth, 1965; Sippu and SoisalonSoininen, 1990), which is a form of shift-reduce parsing. Here the parser needs to commit to a grammar rule only after all input covered by the right-hand side of that rule has been processed, while it may consult the next k symbols (the lookahead). LR is the optimal, i.e. most deterministic, parsing strategy that has thi</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>K. Sagae and A. Lavie. 2005. A classifier-based parser with linear run-time complexity. In Proceedings of the Ninth International Workshop on Parsing Technologies, pages 125–132, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
</authors>
<title>A best-first probabilistic shift-reduce parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>691--698</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="20436" citStr="Sagae and Lavie, 2006" startWordPosition="3878" endWordPosition="3881">gth of the input string. The required changes to the formulas are given in Appendix D. There is a slight similarity to (Schuler, 2009), in that no stack elements beyond a bounded depth are considered at each parsing step, but in our case the stack can still have arbitrary height. Whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and allow some of the most promising configurations at each input position to compete, applying techniques such as beam search (Roark, 2001; Zhang and Clark, 2009; Zhu et al., 2013), best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configurations, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very different from the left-corner models of e.g. (Henderson, 2003), which rely on neural ne</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>K. Sagae and A. Lavie. 2006. A best-first probabilistic shift-reduce parser. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 691– 698, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Schuler</author>
</authors>
<title>Positive results for parsing with a bounded stack using a model-based right-corner transform.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>344--352</pages>
<location>Boulder, Colorado, May–June.</location>
<contexts>
<context position="19948" citStr="Schuler, 2009" startWordPosition="3798" endWordPosition="3799">al factor |w |as before. For more details see Appendix B. 5 Other variants One way to improve accuracy is to increase the size of the lookahead, beyond the current 1, comparable to the generalization from LR(1) to LR(k) parsing. The formulas are given in Appendix C. p(d) 342 Yet another variant investigates only the topmost n stack symbols when choosing the next parser action. In combination with Appendix A, this brings the time complexity down again to linear time in the length of the input string. The required changes to the formulas are given in Appendix D. There is a slight similarity to (Schuler, 2009), in that no stack elements beyond a bounded depth are considered at each parsing step, but in our case the stack can still have arbitrary height. Whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and allow some of the most promising configurations at each input position to compete, applying techniques such as beam search (Roark, 2001; Zhang and Clark, 2009; Zhu et al., 2013), best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configur</context>
</contexts>
<marker>Schuler, 2009</marker>
<rawString>W. Schuler. 2009. Positive results for parsing with a bounded stack using a model-based right-corner transform. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 344– 352, Boulder, Colorado, May–June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Shann</author>
</authors>
<title>Experiments with GLR and chart parsing.</title>
<date>1991</date>
<booktitle>Generalized LR Parsing, chapter 2,</booktitle>
<pages>17--34</pages>
<editor>In M. Tomita, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="2940" citStr="Shann, 1991" startWordPosition="457" endWordPosition="458">strategy that has this property. Deterministic LR parsing has also been considered relevant to psycholinguistics (Shieber, 1983). Nondeterministic variants of LR(k) parsing, for use in natural language processing, have been proposed as well, some using tabulation to ensure polynomial running time in the length of the input string (Tomita, 1988; Billot and Lang, 1989). However, nondeterministic LR(k) parsing is potentially as expensive as, and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by attaching probabilities to transitions (Briscoe and Carroll, 1993), which allows pruning of the search space (Lavie and Tomita, 1993). This by itself is not uncontroversial, for several reasons. First, the space of probability distributions expressible by a LR automaton is incomparable to that expressible by a CFG (Nederhof and Satta, 2004). Second, because an LR automaton may have many more transitions than rules, more training data may be needed to accurately estimate all paramete</context>
</contexts>
<marker>Shann, 1991</marker>
<rawString>P. Shann. 1991. Experiments with GLR and chart parsing. In M. Tomita, editor, Generalized LR Parsing, chapter 2, pages 17–34. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>Sentence disambiguation by a shift-reduce parsing technique.</title>
<date>1983</date>
<booktitle>In 21st Annual Meeting of the ACL, Proceedings of the Conference,</booktitle>
<pages>113--118</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="2456" citStr="Shieber, 1983" startWordPosition="382" endWordPosition="383">w deterministic, i.e. linear-time, parsing, provided parsing decisions are postponed as long as possible. This has led to (deterministic) LR(k) parsing (Knuth, 1965; Sippu and SoisalonSoininen, 1990), which is a form of shift-reduce parsing. Here the parser needs to commit to a grammar rule only after all input covered by the right-hand side of that rule has been processed, while it may consult the next k symbols (the lookahead). LR is the optimal, i.e. most deterministic, parsing strategy that has this property. Deterministic LR parsing has also been considered relevant to psycholinguistics (Shieber, 1983). Nondeterministic variants of LR(k) parsing, for use in natural language processing, have been proposed as well, some using tabulation to ensure polynomial running time in the length of the input string (Tomita, 1988; Billot and Lang, 1989). However, nondeterministic LR(k) parsing is potentially as expensive as, and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by att</context>
</contexts>
<marker>Shieber, 1983</marker>
<rawString>S.M. Shieber. 1983. Sentence disambiguation by a shift-reduce parsing technique. In 21st Annual Meeting of the ACL, Proceedings of the Conference, pages 113–118, Cambridge, Massachusetts, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sippu</author>
<author>E Soisalon-Soininen</author>
</authors>
<title>Parsing Theory, Vol. II: LR(k) and LL(k) Parsing,</title>
<date>1990</date>
<journal>of EATCS Monographs on Theoretical Computer Science.</journal>
<volume>20</volume>
<publisher>Springer-Verlag.</publisher>
<marker>Sippu, Soisalon-Soininen, 1990</marker>
<rawString>S. Sippu and E. Soisalon-Soininen. 1990. Parsing Theory, Vol. II: LR(k) and LL(k) Parsing, volume 20 of EATCS Monographs on Theoretical Computer Science. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An efficient probabilistic contextfree parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="4713" citStr="Stolcke, 1995" startWordPosition="740" endWordPosition="741">the most likely one, given the left context, the lookahead, and a probability distribution over parses given by a PCFG. There are two differences with traditional LR parsing however. First, there is no explicit representation of LR states, and second, probabilities of actions are computed dynamically from a PCFG rather than retrieved as part of static transitions. In particular, this is unlike some other early approaches to probabilistic LR parsing such as (Ng and Tomita, 1991). The mathematical framework is reminiscent of that used to compute prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995). One major difference is that instead of a prefix string, we now have a stack, which does not need to be parsed. In the first instance, this seems to make our problem easier. For our purposes however, we need to add new mechanisms in order to take lookahead into consideration. It is known, e.g. from (Cer et al., 2010; Candito et al., 2010), that constituent parsing can be used effectively to achieve dependency parsing. It is therefore to be expected that our algorithms can be used for dependency parsing as well. The parsing steps of shift-reduce parsing with a binary grammar are in fact very </context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>A. Stolcke. 1995. An efficient probabilistic contextfree parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):167–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Graph-structured stack and natural language parsing.</title>
<date>1988</date>
<booktitle>In 26th Annual Meeting of the ACL, Proceedings of the Conference,</booktitle>
<pages>249--257</pages>
<location>Buffalo, New York,</location>
<contexts>
<context position="2673" citStr="Tomita, 1988" startWordPosition="416" endWordPosition="417"> shift-reduce parsing. Here the parser needs to commit to a grammar rule only after all input covered by the right-hand side of that rule has been processed, while it may consult the next k symbols (the lookahead). LR is the optimal, i.e. most deterministic, parsing strategy that has this property. Deterministic LR parsing has also been considered relevant to psycholinguistics (Shieber, 1983). Nondeterministic variants of LR(k) parsing, for use in natural language processing, have been proposed as well, some using tabulation to ensure polynomial running time in the length of the input string (Tomita, 1988; Billot and Lang, 1989). However, nondeterministic LR(k) parsing is potentially as expensive as, and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by attaching probabilities to transitions (Briscoe and Carroll, 1993), which allows pruning of the search space (Lavie and Tomita, 1993). This by itself is not uncontroversial, for several reasons. First, the space of proba</context>
</contexts>
<marker>Tomita, 1988</marker>
<rawString>M. Tomita. 1988. Graph-structured stack and natural language parsing. In 26th Annual Meeting of the ACL, Proceedings of the Conference, pages 249– 257, Buffalo, New York, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
</authors>
<title>Chunk parsing revisited.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technologies,</booktitle>
<pages>133--140</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1648" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="253" endWordPosition="256">inear; most of the time and space resources are spent on deciding the next parser action. Generalizations that allow nondeterminism, while maintaining polynomial running time, were proposed by (Huang and Sagae, 2010; Kuhlmann et al., 2011). This work has influenced, and has been influenced by, similar developments in constituent parsing. The challenge here is to deterministically choose a shift or reduce action. As in the case of dependency parsing, solutions to this problem are often expressed in terms of classifiers of some kind. Common approaches involve maximum entropy (Ratnaparkhi, 1997; Tsuruoka and Tsujii, 2005), decision trees (Wong and Wu, 1999; Kalt, 2004), and support vector machines (Sagae and Lavie, 2005). The programming-languages community recognized early on that large classes of grammars allow deterministic, i.e. linear-time, parsing, provided parsing decisions are postponed as long as possible. This has led to (deterministic) LR(k) parsing (Knuth, 1965; Sippu and SoisalonSoininen, 1990), which is a form of shift-reduce parsing. Here the parser needs to commit to a grammar rule only after all input covered by the right-hand side of that rule has been processed, while it may consult the next</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Y. Tsuruoka and J. Tsujii. 2005. Chunk parsing revisited. In Proceedings of the Ninth International Workshop on Parsing Technologies, pages 133–140, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Wong</author>
<author>D Wu</author>
</authors>
<title>Learning a lightweight robust deterministic parser.</title>
<date>1999</date>
<booktitle>In Sixth European Conference on Speech Communication and Technology,</booktitle>
<pages>2047--2050</pages>
<contexts>
<context position="1683" citStr="Wong and Wu, 1999" startWordPosition="259" endWordPosition="262">are spent on deciding the next parser action. Generalizations that allow nondeterminism, while maintaining polynomial running time, were proposed by (Huang and Sagae, 2010; Kuhlmann et al., 2011). This work has influenced, and has been influenced by, similar developments in constituent parsing. The challenge here is to deterministically choose a shift or reduce action. As in the case of dependency parsing, solutions to this problem are often expressed in terms of classifiers of some kind. Common approaches involve maximum entropy (Ratnaparkhi, 1997; Tsuruoka and Tsujii, 2005), decision trees (Wong and Wu, 1999; Kalt, 2004), and support vector machines (Sagae and Lavie, 2005). The programming-languages community recognized early on that large classes of grammars allow deterministic, i.e. linear-time, parsing, provided parsing decisions are postponed as long as possible. This has led to (deterministic) LR(k) parsing (Knuth, 1965; Sippu and SoisalonSoininen, 1990), which is a form of shift-reduce parsing. Here the parser needs to commit to a grammar rule only after all input covered by the right-hand side of that rule has been processed, while it may consult the next k symbols (the lookahead). LR is t</context>
</contexts>
<marker>Wong, Wu, 1999</marker>
<rawString>A. Wong and D. Wu. 1999. Learning a lightweight robust deterministic parser. In Sixth European Conference on Speech Communication and Technology, pages 2047–2050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In 8th International Workshop on Parsing Technologies,</booktitle>
<pages>195--206</pages>
<location>LORIA, Nancy, France,</location>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In 8th International Workshop on Parsing Technologies, pages 195–206, LORIA, Nancy, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages</title>
<date>1967</date>
<booktitle>in time n3. Information and Control,</booktitle>
<pages>10--189</pages>
<contexts>
<context position="2878" citStr="Younger, 1967" startWordPosition="445" endWordPosition="446">ookahead). LR is the optimal, i.e. most deterministic, parsing strategy that has this property. Deterministic LR parsing has also been considered relevant to psycholinguistics (Shieber, 1983). Nondeterministic variants of LR(k) parsing, for use in natural language processing, have been proposed as well, some using tabulation to ensure polynomial running time in the length of the input string (Tomita, 1988; Billot and Lang, 1989). However, nondeterministic LR(k) parsing is potentially as expensive as, and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by attaching probabilities to transitions (Briscoe and Carroll, 1993), which allows pruning of the search space (Lavie and Tomita, 1993). This by itself is not uncontroversial, for several reasons. First, the space of probability distributions expressible by a LR automaton is incomparable to that expressible by a CFG (Nederhof and Satta, 2004). Second, because an LR automaton may have many more transitions than rules, more t</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>D.H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10:189–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>Transition-based parsing of the Chinese treebank using a global discriminative model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies,</booktitle>
<pages>162--171</pages>
<location>Paris, France,</location>
<contexts>
<context position="20374" citStr="Zhang and Clark, 2009" startWordPosition="3868" endWordPosition="3871">ings the time complexity down again to linear time in the length of the input string. The required changes to the formulas are given in Appendix D. There is a slight similarity to (Schuler, 2009), in that no stack elements beyond a bounded depth are considered at each parsing step, but in our case the stack can still have arbitrary height. Whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and allow some of the most promising configurations at each input position to compete, applying techniques such as beam search (Roark, 2001; Zhang and Clark, 2009; Zhu et al., 2013), best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configurations, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very different from the left-co</context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Y. Zhang and S. Clark. 2009. Transition-based parsing of the Chinese treebank using a global discriminative model. In Proceedings of the 11th International Conference on Parsing Technologies, pages 162–171, Paris, France, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhu</author>
<author>Y Zhang</author>
<author>W Chen</author>
<author>M Zhang</author>
<author>J Zhu</author>
</authors>
<title>Fast and accurate shift-reduce constituent parsing.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the ACL, Proceedings of the Conference,</booktitle>
<volume>1</volume>
<pages>434--443</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="20393" citStr="Zhu et al., 2013" startWordPosition="3872" endWordPosition="3875">y down again to linear time in the length of the input string. The required changes to the formulas are given in Appendix D. There is a slight similarity to (Schuler, 2009), in that no stack elements beyond a bounded depth are considered at each parsing step, but in our case the stack can still have arbitrary height. Whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and allow some of the most promising configurations at each input position to compete, applying techniques such as beam search (Roark, 2001; Zhang and Clark, 2009; Zhu et al., 2013), best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configurations, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very different from the left-corner models of e.g.</context>
</contexts>
<marker>Zhu, Zhang, Chen, Zhang, Zhu, 2013</marker>
<rawString>M. Zhu, Y. Zhang, W. Chen, M. Zhang, and J. Zhu. 2013. Fast and accurate shift-reduce constituent parsing. In 51st Annual Meeting of the ACL, Proceedings of the Conference, volume 1, pages 434– 443, Sofia, Bulgaria, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>