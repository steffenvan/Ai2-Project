<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.772493">
Multi-Class Confidence Weighted Algorithms
</title>
<author confidence="0.978102">
Mark Dredze† Alex Kulesza*
</author>
<affiliation confidence="0.893713">
†Human Language Technology
Center of Excellence
Johns Hopkins University
</affiliation>
<address confidence="0.629264">
Baltimore, MD 21211
</address>
<email confidence="0.995577">
mdredze@cs.jhu.edu
</email>
<note confidence="0.6358566">
Koby Crammer*
*Department of Computer
and Information Science
University of Pennsylvania
Philadelphia, PA 19104
</note>
<email confidence="0.994238">
{crammer,kulesza}@cis.upenn.edu
</email>
<sectionHeader confidence="0.994688" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999803421052632">
The recently introduced online
confidence-weighted (CW) learning
algorithm for binary classification per-
forms well on many binary NLP tasks.
However, for multi-class problems CW
learning updates and inference cannot
be computed analytically or solved as
convex optimization problems as they are
in the binary case. We derive learning
algorithms for the multi-class CW setting
and provide extensive evaluation using
nine NLP datasets, including three derived
from the recently released New York
Times corpus. Our best algorithm out-
performs state-of-the-art online and batch
methods on eight of the nine tasks. We
also show that the confidence information
maintained during learning yields useful
probabilistic information at test time.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934836363636">
Online learning algorithms such as the Perceptron
process one example at a time, yielding simple and
fast updates. They generally make few statisti-
cal assumptions about the data and are often used
for natural language problems, where high dimen-
sional feature representations, e.g., bags-of-words,
demand efficiency. Most online algorithms, how-
ever, do not take into account the unique properties
of such data, where many features are extremely
rare and a few are very frequent.
Dredze, Crammer and Pereira (Dredze et al.,
2008; Crammer et al., 2008) recently introduced
confidence weighted (CW) online learning for bi-
nary prediction problems. CW learning explicitly
models classifier weight uncertainty using a multi-
variate Gaussian distribution over weight vectors.
The learner makes online updates based on its con-
fidence in the current parameters, making larger
changes in the weights of infrequently observed
features. Empirical evaluation has demonstrated
the advantages of this approach for a number of bi-
nary natural language processing (NLP) problems.
In this work, we develop and test multi-class
confidence weighted online learning algorithms.
For binary problems, the update rule is a sim-
ple convex optimization problem and inference
is analytically computable. However, neither is
true in the multi-class setting. We discuss sev-
eral efficient online learning updates. These up-
date rules can involve one, some, or all of the
competing (incorrect) labels. We then perform an
extensive evaluation of our algorithms using nine
multi-class NLP classification problems, includ-
ing three derived from the recently released New
York Times corpus (Sandhaus, 2008). To the best
of our knowledge, this is the first learning evalua-
tion on these data. Our best algorithm outperforms
state-of-the-art online algorithms and batch algo-
rithms on eight of the nine datasets.
Surprisingly, we find that a simple algorithm in
which updates consider only a single competing
label often performs as well as or better than multi-
constraint variants if it makes multiple passes over
the data. This is especially promising for large
datasets, where the efficiency of the update can
be important. In the true online setting, where
only one iteration is possible, multi-constraint al-
gorithms yield better performance.
Finally, we demonstrate that the label distribu-
tions induced by the Gaussian parameter distribu-
tions resulting from our methods have interesting
properties, such as higher entropy, compared to
those from maximum entropy models. Improved
label distributions may be useful in a variety of
learning settings.
</bodyText>
<sectionHeader confidence="0.952626" genericHeader="method">
2 Problem Setting
</sectionHeader>
<bodyText confidence="0.9025235">
In the multi-class setting, instances from an input
space X take labels from a finite set Y, |Y |= K.
</bodyText>
<page confidence="0.983951">
496
</page>
<note confidence="0.99661">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 496–504,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999944">
We use a standard approach (Collins, 2002) for
generalizing binary classification and assume a
feature function f(x, y) ∈ Rd mapping instances
x ∈ X and labels y ∈ Y into a common space.
We work in the online framework, where learn-
ing is performed in rounds. On each round the
learner receives an input xi, makes a prediction ˆyi
according to its current rule, and then learns the
true label yi. The learner uses the new example
(xi, yi) to modify its prediction rule. Its goal is to
minimize the total number of rounds with incor-
rect predictions, |{i : yi =6 ˆyi}|.
In this work we focus on linear models parame-
terized by weights w and utilizing prediction func-
tions of the form hw(x) = arg maxz w · f(x, z).
Note that since we can choose f(x, y) to be the
vectorized Cartesian product of an input feature
function g(x) and y, this setup generalizes the use
of unique weight vectors for each element of Y.
</bodyText>
<sectionHeader confidence="0.970172" genericHeader="method">
3 Confidence Weighted Learning
</sectionHeader>
<bodyText confidence="0.999580894736842">
Dredze, Crammer, and Pereira (2008) introduced
online confidence weighted (CW) learning for bi-
nary classification, where X = Rd and Y =
{±1}. Rather than using a single parameter vec-
tor w, CW maintains a distribution over param-
eters N (µ, E), where N (µ, E) the multivariate
normal distribution with mean µ ∈ Rd and co-
variance matrix E ∈ Rdxd. Given an input in-
stance x, a Gibbs classifier draws a weight vector
w from the distribution and then makes a predic-
tion according to the sign of w · x.
This prediction rule is robust if the example
is classified correctly with high-probability, that
is, for some confidence parameter .5 ≤ η &lt; 1,
Prw [y (w · x) ≥ 0] ≥ η. To learn a binary CW
classifier in the online framework, the robustness
property is enforced at each iteration while mak-
ing a minimal update to the parameter distribution
in the KL sense:
</bodyText>
<equation confidence="0.95778">
DKL (N (µ, E) k N (µi, Ei))
s.t. Prw [yi (w · xi) ≥ 0] ≥ η (1)
</equation>
<bodyText confidence="0.9193265">
Dredze et al. (2008) showed that this optimization
can be solved in closed form, yielding the updates
</bodyText>
<equation confidence="0.994505">
µi+1 = µi + αiEixi (2)
—1
Ei+1 = (Ei 1 + βixixT) (3)
</equation>
<bodyText confidence="0.918732">
for appropriate αi and βi.
For prediction, they use the Bayesian rule
</bodyText>
<equation confidence="0.981185">
Prw—N(µ,E) [z (x · w) ≥ 0] ,
</equation>
<bodyText confidence="0.9001715">
which for binary labels is equivalent to using the
mean parameters directly, yˆ = sign (µ · x).
</bodyText>
<sectionHeader confidence="0.914201" genericHeader="method">
4 Multi-Class Confidence Weighted
</sectionHeader>
<subsectionHeader confidence="0.782647">
Learning
</subsectionHeader>
<bodyText confidence="0.9999416">
As in the binary case, we maintain a distribution
over weight vectors w ∼ N (µ, E). Given an in-
put instance x, a Gibbs classifier draws a weight
vector w ∼ N (µ, E) and then predicts the label
with the maximal score, arg maxz (w · f(x, z)).
As in the binary case, we use this prediction rule
to define a robustness condition and corresponding
learning updates.
We generalize the robustness condition used in
Crammer et al. (2008). Following the update on
round i, we require that the ith instance is correctly
labeled with probability at least η &lt; 1. Among the
distributions that satisfy this condition, we choose
the one that has the minimal KL distance from the
current distribution. This yields the update
</bodyText>
<equation confidence="0.997187">
(µi+1,Ei+1) = (4)
DKL (N (µ, E) k N (µi, Ei))
s.t. Pr [yi  |xi, µ, E] ≥ η ,
</equation>
<bodyText confidence="0.729333">
where
</bodyText>
<equation confidence="0.9988065">
Pr[y |x, µ, E] = 11
Prw—N(µ,E) I y = arg maYx (w · f (x, z))J .
</equation>
<bodyText confidence="0.999688833333333">
Due to the max operator in the constraint, this op-
timization is not convex when K &gt; 2, and it does
not permit a closed form solution. We therefore
develop approximations that can be solved effi-
ciently. We define the following set of events for a
general input x:
</bodyText>
<equation confidence="0.679076">
Ar,s(x) def = {w : w · f(x, r) ≥ w · f(x, s)}
Br(x) def = {w : w · f(x, r) ≥ w · f(x, s) ∀s}
=n Ar,s(x)
sir
</equation>
<bodyText confidence="0.8242625">
We assume the probability that w · f(x, r) =
w · f(x, s) for some s =6 r is zero, which
</bodyText>
<equation confidence="0.957431285714286">
(µi+1,Ei+1) =
arg min
µ,E
yˆ = arg max
zE1f11
arg min
µ,E
</equation>
<page confidence="0.983185">
497
</page>
<bodyText confidence="0.92135875">
holds for non-trivial distribution parameters and
feature vectors. We rewrite the prediction yˆ =
argmaxr Pr [Br(x)], and the constraint from
Eq. (4) becomes
</bodyText>
<equation confidence="0.715334">
Pr [Byi(x)] &gt; η . (5)
</equation>
<bodyText confidence="0.7431172">
We focus now on approximating the event Byi(x)
in terms of events Ayi,r. We rely on the fact that
the level sets of Pr [Ayi,r] are convex in µ and
E. This leads to convex constraints of the form
Pr [Ayi,r] &gt; γ.
</bodyText>
<listItem confidence="0.995285428571429">
Outer Bound: Since Br(x) C_ Ar,s(x), it holds
trivially that Pr [Byi(x)] &gt; η ==&gt;- Pr [Ayi,r] &gt;
η, br =� yi. Thus we can replace the constraint
Pr [Byi(x)] &gt; η with Pr [Ayi,r] &gt; η to achieve an
outer bound. We can simultaneously apply all of
the pairwise constraints to achieve a tighter bound:
Pr [Ayi,r] &gt; η br =� yi
</listItem>
<bodyText confidence="0.941596321428572">
This yields a convex approximation to Eq. (4) that
may improve the objective value at the cost of
violating the constraint. In the context of learn-
ing, this means that the new parameter distribu-
tion will be close to the previous one, but may not
achieve the desired confidence on the current ex-
ample. This makes the updates more conservative.
Inner Bound: We can also consider an inner
bound. Note that Byi(x)c = (nrAyi,r(x))c =
UrAyi,r(x)c, thus the constraint Pr [Byi(x)] &gt; η
is equivalent to
Pr [UrAyi,r(x)c] G 1 − η ,
and by the union bound, this follows whenever
� Pr [Ayi,r(x)c] G 1 − η .
r
We can achieve this by choosing non-negative
ζr &gt; 0, Er ζr = 1, and constraining
Pr [Ayi,r(x)] &gt; 1 − (1 − η) ζr for r =� yi .
This formulation yields an inner bound on the
original constraint, guaranteeing its satisfaction
while possibly increasing the objective. In the
context of learning, this is a more aggressive up-
date, ensuring that the current example is robustly
classified even if doing so requires a larger change
to the parameter distribution.
Algorithm 1 Multi-Class CW Online Algorithm
Input: Confidence parameter η
Feature function f(x, y) E Rd
</bodyText>
<equation confidence="0.756810142857143">
Initialize: µ1 = 0 , E1 = I
for i = 1, 2 ... do
Receive xi E X
Predict ranking of labels ˆy1, ˆy2, .. .
Receive yi E Y
Set µi+1, Ei+1 by approximately solving
Eq. (4) using one of the following:
</equation>
<bodyText confidence="0.96145425">
Single-constraint update (Sec. 5.1)
Exact many-constraint update (Sec. 5.2)
Seq. many-constraint approx. (Sec. 5.2)
Parallel many-constraint approx. (Sec. 5.2)
</bodyText>
<listItem confidence="0.734363">
end for
Output: Final µ and E
Discussion: The two approximations are quite
similar in form. Both replace the constraint
Pr [Byi(x)] &gt; η with one or more constraints of
the form
Pr [Ayi,r(x)] &gt; ηr . (6)
</listItem>
<bodyText confidence="0.993804272727273">
To achieve an outer bound we choose ηr = η for
any set of r =� yi. To achieve an inner bound we
use all K − 1 possible constraints, setting ηr =
1 − (1 − η) ζr for suitable ζr. A simple choice is
ζr = 1/(K − 1).
In practice, η is a learning parameter whose
value will be optimized for each task. In this case,
the outer bound (when all constraints are included)
and inner bound (when ζr = 1/(K − 1)) can be
seen as equivalent, since for any fixed value of
η(in) for the inner bound we can choose
</bodyText>
<equation confidence="0.992181">
η(out) = 1 − 1 − η(in)
K − 1 ,
</equation>
<bodyText confidence="0.9999906">
for the outer bound and the resulting ηr will be
equal. By optimizing η we automatically tune the
approximation to achieve the best compromise be-
tween the inner and outer bounds. In the follow-
ing, we will therefore assume ηr = η.
</bodyText>
<sectionHeader confidence="0.998103" genericHeader="method">
5 Online Updates
</sectionHeader>
<bodyText confidence="0.999913285714286">
Our algorithms are online and process examples
one at a time. Pseudo-code for our approach is
given in algorithm 1. We approximate the pre-
diction step by ranking each label y according
to the score given by the mean weight vector,
µ · f(xi, y). Although this approach is Bayes op-
timal for binary problems (Dredze et al., 2008),
</bodyText>
<page confidence="0.99454">
498
</page>
<bodyText confidence="0.9999931875">
it is an approximation in general. We note that
more accurate inference can be performed in the
multi-class case by sampling weight vectors from
the distribution N(µ, Σ) or selecting labels sen-
sitive to the variance of prediction; however, in
our experiments this did not improve performance
and required significantly more computation. We
therefore proceed with this simple and effective
approximation.
The update rule is given by an approximation
of the type described in Sec. 4. All that remains
is to choose the constraint set and solve the opti-
mization efficiently. We discuss several schemes
for minimizing KL divergence subject to one or
more constraints of the form Pr [Ayi,r(x)] &gt; η.
We start with a single constraint.
</bodyText>
<subsectionHeader confidence="0.981078">
5.1 Single-Constraint Updates
</subsectionHeader>
<bodyText confidence="0.999844333333333">
The simplest approach is to select the single con-
straint Pr [Ayi,r(x)] &gt; η corresponding to the
highest-ranking label r =� yi. This ensures that,
following the update, the true label is more likely
to be predicted than the label that was its closest
competitor. We refer to this as the k = 1 update.
Whenever we have only a single constraint, we
can reduce the optimization to one of the closed-
form CW updates used for binary classification.
Several have been proposed, based on linear ap-
proximations (Dredze et al., 2008) and exact for-
mulations (Crammer et al., 2008). For simplicity,
we use the Variance method from Dredze et al.
(2008), which did well in our initial evaluations.
This method leads to the following update rules.
Note that in practice Σ is projected to a diagonal
matrix as part of the update; this is necessary due
to the large number of features that we use.
</bodyText>
<equation confidence="0.99988">
µi+1 = µi + αiΣigi,yi,r (7)
(ΣiΣi+1 = 1 + 2αiφgi,yi,rgi,yi,r) (8)
gi,yi,r = f(xi, yi) − f (xi, r) φ = Φ−1(η)
</equation>
<bodyText confidence="0.924398">
The scale αi is given by max(γi, 0), where γi is
equal to
</bodyText>
<equation confidence="0.9075812">
V/
−(1 + 2φmi) + (1 + 2φmi)2 − 8φ(mi − φvi)
4φvi
and
mi = µi · gi,yi,r vi = g&gt; i,yi,rΣigi,yi,r .
</equation>
<bodyText confidence="0.999976">
These rules derive directly from Dredze et al.
(2008) or Figure 1 in Crammer et al. (2008); we
simply substitute yi = 1 and xi = gi,yi,r.
</bodyText>
<subsectionHeader confidence="0.988375">
5.2 Many-Constraints Updates
</subsectionHeader>
<bodyText confidence="0.995669733333333">
A more accurate approximation can be obtained
by selecting multiple constraints. Analogously, we
choose the k ≤ K−1 constraints corresponding to
the labels r1, . .. , rk =� yi that achieve the highest
predicted ranks. The resulting optimization is con-
vex and can be solved by a standard Hildreth-like
algorithm (Censor &amp; Zenios, 1997). We refer to
this update as Exact. However, Exact is expen-
sive to compute, and tends to over-fit in practice
(Sec. 6.2). We propose several approximate alter-
natives.
Sequential Update: The Hildreth algorithm it-
erates over the constraints, updating with respect
to each until convergence is reached. We approxi-
mate this solution by making only a single pass:
</bodyText>
<listItem confidence="0.979863">
• Set µi,0 = µi and Σi,0 = Σi.
• For j = 1, ... , k, set (µi,j, Σi,j) to the solu-
tion of the following optimization:
</listItem>
<equation confidence="0.8353585">
��
DKL �N (µ, Σ) II N �µi,j−1, Σi,j−1
s.t. Pr [Ayi,rj(x)] &gt; η
• Set µi+1 = µi,k and Σi+1 = Σi,k.
</equation>
<bodyText confidence="0.954276285714286">
Parallel Update: As an alternative to the Hil-
dreth algorithm, we consider the simultaneous al-
gorithm of Iusem and Pierro (1987), which finds
an exact solution by iterating over the constraints
in parallel. As above, we approximate the exact
solution by performing only one iteration. The
process is as follows.
</bodyText>
<listItem confidence="0.8883795">
• For j = 1, ... , k, set (µi,j, Σi,j) to the solu-
tion of the following optimization:
DKL (N (µ, Σ) II N (µi, Σi))
s.t. Pr [Ayi,rj(x)] &gt; η
• Let λ be a vector, λj &gt;0 , Ej λj =1.
• Set µi+1 = Ej λjµi,j, Σ−1
</listItem>
<equation confidence="0.8828975">
i+1 = Ej λjΣ−1
i,j .
</equation>
<bodyText confidence="0.995994">
In practice we set λj = 1/k for all j.
</bodyText>
<sectionHeader confidence="0.999794" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.93053">
6.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9998344">
Following the approach of Dredze et al. (2008),
we evaluate using five natural language classifica-
tion tasks over nine datasets that vary in difficulty,
size, and label/feature counts. See Table 1 for an
overview. Brief descriptions follow.
</bodyText>
<equation confidence="0.5969405">
min
µ,Σ
min
µ,Σ
</equation>
<page confidence="0.963649">
499
</page>
<table confidence="0.9999352">
Task Instances Features Labels Bal.
20 News 18,828 252,115 20 Y
Amazon 7 13,580 686,724 7 Y
Amazon 3 7,000 494,481 3 Y
Enron A 3,000 13,559 10 N
Enron B 3,000 18,065 10 N
NYTD 10,000 108,671 26 N
NYTO 10,000 108,671 34 N
NYTS 10,000 114,316 20 N
Reuters 4,000 23,699 4 N
</table>
<tableCaption confidence="0.999161">
Table 1: A summary of the nine datasets, includ-
</tableCaption>
<bodyText confidence="0.974311081081081">
ing the number of instances, features, and labels,
and whether the numbers of examples in each class
are balanced.
Amazon Amazon product reviews. Using the
data of Dredze et al. (2008), we created two do-
main classification datasets from seven product
types (apparel, books, dvds, electronics, kitchen,
music, video). Amazon 7 includes all seven prod-
uct types and Amazon 3 includes books, dvds, and
music. Feature extraction follows Blitzer et al.
(2007) (bigram features and counts).
20 Newsgroups Approximately 20,000 news-
group messages, partitioned across 20 different
newsgroups.1 This dataset is a popular choice for
binary and multi-class text classification as well as
unsupervised clustering. We represent each mes-
sage as a binary bag-of-words.
Enron Automatic sorting of emails into fold-
ers.2 We selected two users with many email
folders and messages: farmer-d (Enron A) and
kaminski-v (Enron B). We used the ten largest
folders for each user, excluding non-archival email
folders such as “inbox,” “deleted items,” and “dis-
cussion threads.” Emails were represented as bi-
nary bags-of-words with stop-words removed.
NY Times To the best of our knowledge we are
the first to evaluate machine learning methods on
the New York Times corpus. The corpus con-
tains 1.8 million articles that appeared from 1987
to 2007 (Sandhaus, 2008). In addition to being
one of the largest collections of raw news text,
it is possibly the largest collection of publicly re-
leased annotated news text, and therefore an ideal
corpus for large scale NLP tasks. Among other
annotations, each article is labeled with the desk
that produced the story (Financial, Sports, etc.)
(NYTD), the online section to which the article was
</bodyText>
<footnote confidence="0.9999075">
1http://people.csail.mit.edu/jrennie/20Newsgroups/
2http://www.cs.cmu.edu/˜enron/
</footnote>
<table confidence="0.9998757">
Task Sequential Parallel Exact
20 News 92.16 91.41 88.08
Amazon 7 77.98 78.35 77.92
Amazon 3 93.54 93.81 93.00
Enron A 82.40 81.30 77.07
Enron B 71.80 72.13 68.00
NYTD 83.43 81.43 80.92
NYTO 82.02 78.67 80.60
NYTS 52.96 54.78 51.62
Reuters 93.60 93.97 93.47
</table>
<tableCaption confidence="0.995039">
Table 2: A comparison of k = ∞ updates. While
</tableCaption>
<bodyText confidence="0.989399857142857">
the two approximations (sequential and parallel)
are roughly the same, the exact solution over-fits.
posted (NYTO), and the section in which the arti-
cle was printed (NYTS). Articles were represented
as bags-of-words with feature counts (stop-words
removed).
Reuters Over 800,000 manually categorized
newswire stories (RCV1-v2/ LYRL2004). Each
article contains one or more labels describing its
general topic, industry, and region. We performed
topic classification with the four general topics:
corporate, economic, government, and markets.
Details on document preparation and feature ex-
traction are given by Lewis et al. (2004).
</bodyText>
<subsectionHeader confidence="0.993744">
6.2 Evaluations
</subsectionHeader>
<bodyText confidence="0.999950458333333">
We first set out to compare the three update ap-
proaches proposed in Sec. 5.2: an exact solution
and two approximations (sequential and parallel).
Results (Table 2) show that the two approxima-
tions perform similarly. For every experiment the
CW parameter η and the number of iterations (up
to 10) were optimized using a single randomized
iteration. However, sequential converges faster,
needing an average of 4.33 iterations compared to
7.56 for parallel across all datasets. Therefore, we
select sequential for our subsequent experiments.
The exact method performs poorly, displaying
the lowest performance on almost every dataset.
This is unsurprising given similar results for bi-
nary CW learning Dredze et al. (2008), where ex-
act updates were shown to over-fit but converged
after a single iteration of training. Similarly, our
exact implementation converges after an average
of 1.25 iterations, much faster than either of the
approximations. However, this rapid convergence
appears to come at the expense of accuracy. Fig. 1
shows the accuracy on Amazon 7 test data after
each training iteration. While both sequential and
parallel improve with several iterations, exact de-
</bodyText>
<page confidence="0.967602">
500
</page>
<figure confidence="0.986646">
1 2 3 4 5
Training Iterations
</figure>
<figureCaption confidence="0.961175">
Figure 1: Accuracy on test data after each iteration
on the Amazon 7 dataset.
</figureCaption>
<bodyText confidence="0.9957825">
grades after the first iteration, suggesting that it
may over-fit to the training data. The approxima-
tions appear to smooth learning and produce better
performance in the long run.
</bodyText>
<subsectionHeader confidence="0.992589">
6.3 Relaxing Many-Constraints
</subsectionHeader>
<bodyText confidence="0.999989885714286">
While enforcing many constraints may seem op-
timal, there are advantages to pruning the con-
straints as well. It may be time consuming to en-
force dozens or hundreds of constraints for tasks
with many labels. Structured prediction tasks of-
ten involve exponentially many constraints, mak-
ing pruning mandatory. Furthermore, many real
world datasets, especially in NLP, are noisy, and
enforcing too many constraints can lead to over-
fitting. Therefore, we consider the impact of re-
ducing the constraint set in terms of both reducing
run-time and improving accuracy.
We compared using all constraints (k = ∞)
with using 5 constraints (k = 5) for the sequential
update method (Table 3). First, we observe that
k = 5 performs better than k = ∞ on nearly every
dataset: fewer constraints help avoid over-fitting
and once again, simpler is better. Additionally,
k = 5 converges faster than k = ∞ in an average
of 2.22 iterations compared with 4.33 iterations.
Therefore, reducing the number of constraints im-
proves both speed and accuracy. In comparing
k = 5 with the further reduced k = 1 results, we
observe the latter improves on seven of the nine
methods. This surprising result suggests that CW
learning can perform well even without consid-
ering more than a single constraint per example.
However, k = 1 exceeds the performance of mul-
tiple constraints only through repeated training it-
erations. k = 5 CW learning converges faster —
2.22 iterations compared with 6.67 for k = 1 — a
desirable property in many resource restricted set-
tings. (In the true online setting, only a single it-
eration may be possible.) Fig. 1 plots the perfor-
mance of k = 1 and k = 5 CW on test data after
each training iteration. While k = 1 does better
in the long run, it lags behind k = 5 for several
iterations. In fact, after a single training iteration,
k = 5 outperforms k = 1 on eight out of nine
datasets. Thus, there is again a tradeoff between
faster convergence (k = 5) and increased accuracy
(k = 1). While the k = 5 update takes longer per
iteration, the time required for the approximate so-
lutions grows only linearly in the number of con-
straints. The evaluation in Fig. 1 required 3 sec-
onds for the first iteration of k = 1, 10 seconds
for k = 5 and 11 seconds for one iteration of all
7 constraints. These differences are insignificant
compared to the cost of performing multiple itera-
tions over a large dataset. We note that, while both
approximate methods took about the same amount
of time, the exact solution took over 4 minutes for
its first iteration.
Finally, we compare CW methods with sev-
eral baselines in Table 3. Online baselines in-
clude Top-1 Perceptron (Collins, 2002), Top-1
Passive-Aggressive (PA), and k-best PA (Cram-
mer &amp; Singer, 2003; McDonald et al., 2004).
Batch algorithms include Maximum Entropy (de-
fault configuration in McCallum (2002)) and sup-
port vector machines (LibSVM (Chang &amp; Lin,
2001) for one-against-one classification and multi-
class (MC) (Crammer &amp; Singer, 2001)). Classifier
parameters (C for PA/SVM and maxent’s Gaus-
sian prior) and number of iterations (up to 10) for
the online methods were optimized using a sin-
gle randomized iteration. On eight of the nine
datasets, CW improves over all baselines. In gen-
eral, CW provides faster and more accurate multi-
class predictions.
</bodyText>
<sectionHeader confidence="0.985328" genericHeader="evaluation">
7 Error and Probabilistic Output
</sectionHeader>
<bodyText confidence="0.999667571428571">
Our focus so far has been on accuracy and speed.
However, there are other important considerations
for selecting learning algorithms. Maximum en-
tropy and other probabilistic classification algo-
rithms are sometimes favored for their probabil-
ity scores, which can be useful for integration
with other learning systems. However, practition-
</bodyText>
<figure confidence="0.9913025">
78.5
78.0
77.5
77.0
K=1
Sequential K=5
Sequential K=All
Parallel K=All
Exact K=All
Test Accuracy
</figure>
<page confidence="0.983412">
501
</page>
<table confidence="0.998996818181818">
Task Perceptron K=1 PA K=1 CW K=oo SVM Maxent
K=5 K=5 1 vs. 1 MC
20 News 81.07 88.59 88.60 **92.90 **92.78 **92.16 85.18 90.33 88.94
Amazon 7 74.93 76.55 76.72 **78.70 **78.04 **77.98 75.11 76.60 76.40
Amazon 3 92.26 92.47 93.29 194.01 **94.29 93.54 92.83 93.60 93.60
Enron A 74.23 79.27 80.77 1183.83 182.23 182.40 80.23 82.60 82.80
Enron B 66.30 69.93 68.90 **73.57 **72.27 **71.80 65.97 71.87 69.47
NYTD 80.67 83.12 81.31 **84.57 *83.94 83.43 82.95 82.00 83.54
NYTO 78.47 81.93 81.22 182.72 182.55 82.02 82.13 81.01 82.53
NYTS 50.80 56.19 55.04 54.67 54.26 52.96 55.81 56.74 53.82
Reuters 92.10 93.12 93.30 93.60 93.67 93.60 92.97 93.32 93.40
</table>
<tableCaption confidence="0.984686">
Table 3: A comparison of CW learning (k = 1, 5, oc with sequential updates) with several baseline
</tableCaption>
<figureCaption confidence="0.882482125">
algorithms. CW learning achieves the best performance eight out of nine times. Statistical significance
(McNemar) is measured against all baselines (* indicates 0.05 and ** 0.001) or against online baselines
(† indicates 0.05 and †† 0.001).
Figure 2: First panel: Error versus prediction entropy on Enron B. As CW converges (right to left) error
and entropy are reduced. Second panel: Number of test examples per prediction probability bin. The
red bars correspond to maxent and the blue bars to CW, with increasing numbers of epochs from left
to right. Third panel: The contribution of each bin to the total test error. Fourth panel: Test error
conditioned on prediction probability.
</figureCaption>
<figure confidence="0.986899">
entropy Bin lower threshold Bin lower threshold Bin lower threshold
33
32
31
30
29
28
12
10
8
6
Test error in bin
4
2
0
800
600
400
200
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75
MC CW
MaxEnt
Number of examples per bin
1200
1000
0
MaxEnt
MC CW
0.8
WaxEnt
WC CW
0.7
Test error given bin
0.6
0.5
0.4
0.3
0.2
0.1
0
MaxEnt
MC CW
</figure>
<bodyText confidence="0.997974276595745">
ers have observed that maxent probabilities can
have low entropy and be unreliable for estimating
prediction confidence (Malkin &amp; Bilmes, 2008).
Since CW also produces label probabilities — and
does so in a conceptually distinct way — we in-
vestigate in this section some empirical properties
of the label distributions induced by CW’s param-
eter distributions and compare them with those of
maxent.
We trained maxent and CW k = 1 classi-
fiers on the Enron B dataset, optimizing parame-
ters as before (maxent’s Gaussian prior and CW’s
η). We estimated the label distributions from our
CW classifiers after each iteration and on every
test example x by Gibbs sampling weight vec-
tors w - N(µ, E), and for each label y count-
ing the fraction of weight vectors for which y =
argmaxz w · f(x, z). Normalizing these counts
yields the label distributions Pr [y|x]. We denote
by yˆ the predicted label for a given x, and refer to
Pr [ˆy|x] as the prediction probability.
The leftmost panel of Fig. 2 plots each
method’s prediction error against the nor-
malized entropy of the label distribution
- ( 1 E Ez Pr [z|xi] log (Pr [z|xi])) / log(K).
m i
Each CW iteration (moving from right to left in
the plot) reduces both error and entropy. From our
maxent results we make the common observation
that maxent distributions have (ironically) low
entropy. In contrast, while CW accuracy exceeds
maxent after its second iteration, normalized
entropy remains high. Higher entropy suggests
a distribution over labels that is less peaked and
potentially more informative than those from
maxent. We found that the average probability
assigned to a correct prediction was 0.75 for
CW versus 0.83 for maxent and for an incorrect
prediction was 0.44 for CW versus 0.56 for
maxent.
Next, we investigate how these probabilities
relate to label accuracy. In the remaining pan-
els, we binned examples according to their pre-
diction probabilities Pr [ˆy|x] = maxy Pr [y|x].
The second panel of Fig. 2 shows the numbers
of test examples with Pr [ˆy|x] E [0, 0 + 0.1) for
0 = 0.1, 0.2 ... 0.9. (Note that since there are 10
</bodyText>
<page confidence="0.9903">
502
</page>
<bodyText confidence="0.999985583333333">
classes in this problem, we must have Pr [ˆy|x] ≥
0.1.) The red (leftmost) bar corresponds to the
maximum entropy classifier, and the blue bars cor-
respond, from left to right, to CW after each suc-
cessive training epoch.
From the plot we observe that the maxent classi-
fier assigns prediction probability greater than 0.9
to more than 1,200 test examples out of 3,000.
Only 50 examples predicted by maxent fall in the
lowest bin, and the rest of examples are distributed
nearly uniformly across the remaining bins. The
large number of examples with very high predic-
tion probability explains the low entropy observed
for the maximum entropy classifier.
In contrast, the CW classifier shows the oppo-
site behavior after one epoch of training (the left-
most blue bar), assigning low prediction probabil-
ity (less than 0.3) to more than 1,200 examples
and prediction probability of at least 0.9 to only
100 examples. As CW makes additional passes
over the training data, its prediction confidence
increases and shifts toward more peaked distribu-
tions. After seven epochs fewer than 100 examples
have low prediction probability and almost 1,000
have high prediction probability. Nonetheless, we
note that this distribution is still less skewed than
that of the maximum entropy classifier.
Given the frequency of high probability maxent
predictions, it seems likely that many of the high
probability maxent labels will be wrong. This is
demonstrated in the third panel, which shows the
contribution of each bin to the total test error. Each
bar reflects the number of mistakes per bin divided
by the size of the complete test set (3,000). Thus,
the sum of the heights of the corresponding bars
in each bin is proportional to test error. Much of
the error of the maxent classifier comes not only
from the low-probability bins, due to their inac-
curacy, but also from the highest bin, due to its
very high population. In contrast, the CW clas-
sifiers see very little error contribution from the
high-probability bins. As training progresses, we
see again that the CW classifiers move in the direc-
tion of the maxent classifier but remain essentially
unimodal.
Finally, the rightmost panel shows the condi-
tional test error given bin identity, or the fraction
of test examples from each bin where the predic-
tion was incorrect. This is the pointwise ratio be-
tween corresponding values of the previous two
histograms. For both methods, there is a monoton-
ically decreasing trend in error as prediction prob-
ability increases; that is, the higher the value of
the prediction probability, the more likely that the
prediction it provides is correct. As CW is trained,
we see an increase in the conditional test error, yet
the overall error decreases (not shown). This sug-
gests that as CW is trained and its overall accuracy
improves, there are more examples with high pre-
diction probability, and the cost for this is a rela-
tive increase in the conditional test error per bin.
The maxent classifier produces an extremely large
number of test examples with very high prediction
probabilities, which yields relatively high condi-
tional test error. In nearly all cases, the conditional
error values for the CW classifiers are smaller than
the corresponding values for maximum entropy.
These observations suggest that CW assigns prob-
abilities more conservatively than maxent does,
and that the (fewer) high confidence predictions it
makes are of a higher quality. This is a potentially
valuable property, e.g., for system combination.
</bodyText>
<sectionHeader confidence="0.99822" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99996915">
We have proposed a series of approximations for
multi-class confidence weighted learning, where
the simple analytical solutions of binary CW
learning do not apply. Our best CW method out-
performs online and batch baselines on eight of
nine NLP tasks, and is highly scalable due to the
use of a single optimization constraint. Alterna-
tively, our multi-constraint algorithms provide im-
proved performance for systems that can afford
only a single pass through the training data, as in
the true online setting. This result stands in con-
trast to previously observed behaviors in non-CW
settings (McDonald et al., 2004). Additionally, we
found improvements in both label entropy and ac-
curacy as compared to a maximum entropy clas-
sifier. We plan to extend these ideas to structured
problems with exponentially many labels and de-
velop methods that efficiently model label correla-
tions. An implementation of CW multi-class algo-
rithms is available upon request from the authors.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9959046">
Blitzer, J., Dredze, M., &amp; Pereira, F. (2007).
Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment
classification. Association for Computational
Linguistics (ACL).
</reference>
<page confidence="0.985765">
503
</page>
<reference confidence="0.99217">
Censor, Y., &amp; Zenios, S. (1997). Parallel opti-
mization: Theory, algorithms, and applications.
Oxford University Press, New York, NY, USA.
Chang, C.-C., &amp; Lin, C.-J. (2001). LIBSVM: a
library for support vector machines. Software
available at http://www.csie.ntu.edu.
tw/˜cjlin/libsvm.
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. Empir-
ical Methods in Natural Language Processing
(EMNLP).
Crammer, K., Dredze, M., &amp; Pereira, F. (2008).
Exact confidence-weighted learning. Advances
in Neural Information Processing Systems 22.
Crammer, K., &amp; Singer, Y. (2001). On the al-
gorithmic implementation of multiclass kernel-
based vector machines. Jornal of Machine
Learning Research, 2, 265–292.
Crammer, K., &amp; Singer, Y. (2003). Ultraconserva-
tive online algorithms for multiclass problems.
Jornal of Machine Learning Research (JMLR),
3, 951–991.
Dredze, M., Crammer, K., &amp; Pereira, F. (2008).
Confidence-weighted linear classification. In-
ternational Conference on Machine Learning
(ICML).
Iusem, A., &amp; Pierro, A. D. (1987). A simultaneous
iterative method for computing projections on
polyhedra. SIAM J. Control and Optimization,
25.
Lewis, D. D., Yang, Y., Rose, T. G., &amp; Li, F.
(2004). Rcv1: Anew benchmark collection for
text categorization research. Journal ofMachine
Learning Research (JMLR), 5, 361–397.
Malkin, J., &amp; Bilmes, J. (2008). Ratio semi-
definite classifiers. IEEE Int. Conf. on Acous-
tics, Speech, and Signal Processing.
McCallum, A. (2002). MALLET: A machine
learning for language toolkit. http://
mallet.cs.umass.edu.
McDonald, R., Crammer, K., &amp; Pereira, F. (2004).
Large margin online learning algorithms for
scalable structured classification. NIPS Work-
shop on Structured Outputs.
Sandhaus, E. (2008). The new york times an-
notated corpus. Linguistic Data Consortium,
Philadelphia.
</reference>
<page confidence="0.998191">
504
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476750">
<title confidence="0.988447">Multi-Class Confidence Weighted Algorithms</title>
<affiliation confidence="0.832495333333333">Language Technology Center of Excellence Johns Hopkins University</affiliation>
<address confidence="0.999573">Baltimore, MD 21211</address>
<email confidence="0.998546">mdredze@cs.jhu.edu</email>
<affiliation confidence="0.996443333333333">of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.999626">Philadelphia, PA 19104</address>
<abstract confidence="0.99946915">The recently introduced online confidence-weighted (CW) learning algorithm for binary classification performs well on many binary NLP tasks. However, for multi-class problems CW learning updates and inference cannot be computed analytically or solved as convex optimization problems as they are in the binary case. We derive learning algorithms for the multi-class CW setting and provide extensive evaluation using nine NLP datasets, including three derived from the recently released New York Times corpus. Our best algorithm outperforms state-of-the-art online and batch methods on eight of the nine tasks. We also show that the confidence information maintained during learning yields useful probabilistic information at test time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. Association for Computational Linguistics (ACL).</title>
<date>2007</date>
<contexts>
<context position="15731" citStr="Blitzer et al. (2007)" startWordPosition="2783" endWordPosition="2786"> B 3,000 18,065 10 N NYTD 10,000 108,671 26 N NYTO 10,000 108,671 34 N NYTS 10,000 114,316 20 N Reuters 4,000 23,699 4 N Table 1: A summary of the nine datasets, including the number of instances, features, and labels, and whether the numbers of examples in each class are balanced. Amazon Amazon product reviews. Using the data of Dredze et al. (2008), we created two domain classification datasets from seven product types (apparel, books, dvds, electronics, kitchen, music, video). Amazon 7 includes all seven product types and Amazon 3 includes books, dvds, and music. Feature extraction follows Blitzer et al. (2007) (bigram features and counts). 20 Newsgroups Approximately 20,000 newsgroup messages, partitioned across 20 different newsgroups.1 This dataset is a popular choice for binary and multi-class text classification as well as unsupervised clustering. We represent each message as a binary bag-of-words. Enron Automatic sorting of emails into folders.2 We selected two users with many email folders and messages: farmer-d (Enron A) and kaminski-v (Enron B). We used the ten largest folders for each user, excluding non-archival email folders such as “inbox,” “deleted items,” and “discussion threads.” Ema</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>Blitzer, J., Dredze, M., &amp; Pereira, F. (2007). Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Censor</author>
<author>S Zenios</author>
</authors>
<title>Parallel optimization: Theory, algorithms, and applications.</title>
<date>1997</date>
<publisher>Oxford University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13522" citStr="Censor &amp; Zenios, 1997" startWordPosition="2375" endWordPosition="2378">), where γi is equal to V/ −(1 + 2φmi) + (1 + 2φmi)2 − 8φ(mi − φvi) 4φvi and mi = µi · gi,yi,r vi = g&gt; i,yi,rΣigi,yi,r . These rules derive directly from Dredze et al. (2008) or Figure 1 in Crammer et al. (2008); we simply substitute yi = 1 and xi = gi,yi,r. 5.2 Many-Constraints Updates A more accurate approximation can be obtained by selecting multiple constraints. Analogously, we choose the k ≤ K−1 constraints corresponding to the labels r1, . .. , rk =� yi that achieve the highest predicted ranks. The resulting optimization is convex and can be solved by a standard Hildreth-like algorithm (Censor &amp; Zenios, 1997). We refer to this update as Exact. However, Exact is expensive to compute, and tends to over-fit in practice (Sec. 6.2). We propose several approximate alternatives. Sequential Update: The Hildreth algorithm iterates over the constraints, updating with respect to each until convergence is reached. We approximate this solution by making only a single pass: • Set µi,0 = µi and Σi,0 = Σi. • For j = 1, ... , k, set (µi,j, Σi,j) to the solution of the following optimization: �� DKL �N (µ, Σ) II N �µi,j−1, Σi,j−1 s.t. Pr [Ayi,rj(x)] &gt; η • Set µi+1 = µi,k and Σi+1 = Σi,k. Parallel Update: As an alte</context>
</contexts>
<marker>Censor, Zenios, 1997</marker>
<rawString>Censor, Y., &amp; Zenios, S. (1997). Parallel optimization: Theory, algorithms, and applications. Oxford University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.</title>
<date>2001</date>
<tech>tw/˜cjlin/libsvm.</tech>
<contexts>
<context position="22417" citStr="Chang &amp; Lin, 2001" startWordPosition="3876" endWordPosition="3879">. These differences are insignificant compared to the cost of performing multiple iterations over a large dataset. We note that, while both approximate methods took about the same amount of time, the exact solution took over 4 minutes for its first iteration. Finally, we compare CW methods with several baselines in Table 3. Online baselines include Top-1 Perceptron (Collins, 2002), Top-1 Passive-Aggressive (PA), and k-best PA (Crammer &amp; Singer, 2003; McDonald et al., 2004). Batch algorithms include Maximum Entropy (default configuration in McCallum (2002)) and support vector machines (LibSVM (Chang &amp; Lin, 2001) for one-against-one classification and multiclass (MC) (Crammer &amp; Singer, 2001)). Classifier parameters (C for PA/SVM and maxent’s Gaussian prior) and number of iterations (up to 10) for the online methods were optimized using a single randomized iteration. On eight of the nine datasets, CW improves over all baselines. In general, CW provides faster and more accurate multiclass predictions. 7 Error and Probabilistic Output Our focus so far has been on accuracy and speed. However, there are other important considerations for selecting learning algorithms. Maximum entropy and other probabilisti</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chang, C.-C., &amp; Lin, C.-J. (2001). LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu. tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4010" citStr="Collins, 2002" startWordPosition="591" endWordPosition="592">e demonstrate that the label distributions induced by the Gaussian parameter distributions resulting from our methods have interesting properties, such as higher entropy, compared to those from maximum entropy models. Improved label distributions may be useful in a variety of learning settings. 2 Problem Setting In the multi-class setting, instances from an input space X take labels from a finite set Y, |Y |= K. 496 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 496–504, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP We use a standard approach (Collins, 2002) for generalizing binary classification and assume a feature function f(x, y) ∈ Rd mapping instances x ∈ X and labels y ∈ Y into a common space. We work in the online framework, where learning is performed in rounds. On each round the learner receives an input xi, makes a prediction ˆyi according to its current rule, and then learns the true label yi. The learner uses the new example (xi, yi) to modify its prediction rule. Its goal is to minimize the total number of rounds with incorrect predictions, |{i : yi =6 ˆyi}|. In this work we focus on linear models parameterized by weights w and utili</context>
<context position="22182" citStr="Collins, 2002" startWordPosition="3842" endWordPosition="3843">or the approximate solutions grows only linearly in the number of constraints. The evaluation in Fig. 1 required 3 seconds for the first iteration of k = 1, 10 seconds for k = 5 and 11 seconds for one iteration of all 7 constraints. These differences are insignificant compared to the cost of performing multiple iterations over a large dataset. We note that, while both approximate methods took about the same amount of time, the exact solution took over 4 minutes for its first iteration. Finally, we compare CW methods with several baselines in Table 3. Online baselines include Top-1 Perceptron (Collins, 2002), Top-1 Passive-Aggressive (PA), and k-best PA (Crammer &amp; Singer, 2003; McDonald et al., 2004). Batch algorithms include Maximum Entropy (default configuration in McCallum (2002)) and support vector machines (LibSVM (Chang &amp; Lin, 2001) for one-against-one classification and multiclass (MC) (Crammer &amp; Singer, 2001)). Classifier parameters (C for PA/SVM and maxent’s Gaussian prior) and number of iterations (up to 10) for the online methods were optimized using a single randomized iteration. On eight of the nine datasets, CW improves over all baselines. In general, CW provides faster and more acc</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, M. (2002). Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<date>2008</date>
<booktitle>Exact confidence-weighted learning. Advances in Neural Information Processing Systems 22.</booktitle>
<contexts>
<context position="1634" citStr="Crammer et al., 2008" startWordPosition="227" endWordPosition="230">ng yields useful probabilistic information at test time. 1 Introduction Online learning algorithms such as the Perceptron process one example at a time, yielding simple and fast updates. They generally make few statistical assumptions about the data and are often used for natural language problems, where high dimensional feature representations, e.g., bags-of-words, demand efficiency. Most online algorithms, however, do not take into account the unique properties of such data, where many features are extremely rare and a few are very frequent. Dredze, Crammer and Pereira (Dredze et al., 2008; Crammer et al., 2008) recently introduced confidence weighted (CW) online learning for binary prediction problems. CW learning explicitly models classifier weight uncertainty using a multivariate Gaussian distribution over weight vectors. The learner makes online updates based on its confidence in the current parameters, making larger changes in the weights of infrequently observed features. Empirical evaluation has demonstrated the advantages of this approach for a number of binary natural language processing (NLP) problems. In this work, we develop and test multi-class confidence weighted online learning algorit</context>
<context position="6644" citStr="Crammer et al. (2008)" startWordPosition="1086" endWordPosition="1089">y use the Bayesian rule Prw—N(µ,E) [z (x · w) ≥ 0] , which for binary labels is equivalent to using the mean parameters directly, yˆ = sign (µ · x). 4 Multi-Class Confidence Weighted Learning As in the binary case, we maintain a distribution over weight vectors w ∼ N (µ, E). Given an input instance x, a Gibbs classifier draws a weight vector w ∼ N (µ, E) and then predicts the label with the maximal score, arg maxz (w · f(x, z)). As in the binary case, we use this prediction rule to define a robustness condition and corresponding learning updates. We generalize the robustness condition used in Crammer et al. (2008). Following the update on round i, we require that the ith instance is correctly labeled with probability at least η &lt; 1. Among the distributions that satisfy this condition, we choose the one that has the minimal KL distance from the current distribution. This yields the update (µi+1,Ei+1) = (4) DKL (N (µ, E) k N (µi, Ei)) s.t. Pr [yi |xi, µ, E] ≥ η , where Pr[y |x, µ, E] = 11 Prw—N(µ,E) I y = arg maYx (w · f (x, z))J . Due to the max operator in the constraint, this optimization is not convex when K &gt; 2, and it does not permit a closed form solution. We therefore develop approximations that </context>
<context position="12445" citStr="Crammer et al., 2008" startWordPosition="2174" endWordPosition="2177">h a single constraint. 5.1 Single-Constraint Updates The simplest approach is to select the single constraint Pr [Ayi,r(x)] &gt; η corresponding to the highest-ranking label r =� yi. This ensures that, following the update, the true label is more likely to be predicted than the label that was its closest competitor. We refer to this as the k = 1 update. Whenever we have only a single constraint, we can reduce the optimization to one of the closedform CW updates used for binary classification. Several have been proposed, based on linear approximations (Dredze et al., 2008) and exact formulations (Crammer et al., 2008). For simplicity, we use the Variance method from Dredze et al. (2008), which did well in our initial evaluations. This method leads to the following update rules. Note that in practice Σ is projected to a diagonal matrix as part of the update; this is necessary due to the large number of features that we use. µi+1 = µi + αiΣigi,yi,r (7) (ΣiΣi+1 = 1 + 2αiφgi,yi,rgi,yi,r) (8) gi,yi,r = f(xi, yi) − f (xi, r) φ = Φ−1(η) The scale αi is given by max(γi, 0), where γi is equal to V/ −(1 + 2φmi) + (1 + 2φmi)2 − 8φ(mi − φvi) 4φvi and mi = µi · gi,yi,r vi = g&gt; i,yi,rΣigi,yi,r . These rules derive direc</context>
</contexts>
<marker>Crammer, Dredze, Pereira, 2008</marker>
<rawString>Crammer, K., Dredze, M., &amp; Pereira, F. (2008). Exact confidence-weighted learning. Advances in Neural Information Processing Systems 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>On the algorithmic implementation of multiclass kernelbased vector machines.</title>
<date>2001</date>
<journal>Jornal of Machine Learning Research,</journal>
<volume>2</volume>
<pages>265--292</pages>
<contexts>
<context position="22497" citStr="Crammer &amp; Singer, 2001" startWordPosition="3887" endWordPosition="3890">ltiple iterations over a large dataset. We note that, while both approximate methods took about the same amount of time, the exact solution took over 4 minutes for its first iteration. Finally, we compare CW methods with several baselines in Table 3. Online baselines include Top-1 Perceptron (Collins, 2002), Top-1 Passive-Aggressive (PA), and k-best PA (Crammer &amp; Singer, 2003; McDonald et al., 2004). Batch algorithms include Maximum Entropy (default configuration in McCallum (2002)) and support vector machines (LibSVM (Chang &amp; Lin, 2001) for one-against-one classification and multiclass (MC) (Crammer &amp; Singer, 2001)). Classifier parameters (C for PA/SVM and maxent’s Gaussian prior) and number of iterations (up to 10) for the online methods were optimized using a single randomized iteration. On eight of the nine datasets, CW improves over all baselines. In general, CW provides faster and more accurate multiclass predictions. 7 Error and Probabilistic Output Our focus so far has been on accuracy and speed. However, there are other important considerations for selecting learning algorithms. Maximum entropy and other probabilistic classification algorithms are sometimes favored for their probability scores, </context>
</contexts>
<marker>Crammer, Singer, 2001</marker>
<rawString>Crammer, K., &amp; Singer, Y. (2001). On the algorithmic implementation of multiclass kernelbased vector machines. Jornal of Machine Learning Research, 2, 265–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Jornal of Machine Learning Research (JMLR),</journal>
<volume>3</volume>
<pages>951--991</pages>
<contexts>
<context position="22252" citStr="Crammer &amp; Singer, 2003" startWordPosition="3850" endWordPosition="3854"> of constraints. The evaluation in Fig. 1 required 3 seconds for the first iteration of k = 1, 10 seconds for k = 5 and 11 seconds for one iteration of all 7 constraints. These differences are insignificant compared to the cost of performing multiple iterations over a large dataset. We note that, while both approximate methods took about the same amount of time, the exact solution took over 4 minutes for its first iteration. Finally, we compare CW methods with several baselines in Table 3. Online baselines include Top-1 Perceptron (Collins, 2002), Top-1 Passive-Aggressive (PA), and k-best PA (Crammer &amp; Singer, 2003; McDonald et al., 2004). Batch algorithms include Maximum Entropy (default configuration in McCallum (2002)) and support vector machines (LibSVM (Chang &amp; Lin, 2001) for one-against-one classification and multiclass (MC) (Crammer &amp; Singer, 2001)). Classifier parameters (C for PA/SVM and maxent’s Gaussian prior) and number of iterations (up to 10) for the online methods were optimized using a single randomized iteration. On eight of the nine datasets, CW improves over all baselines. In general, CW provides faster and more accurate multiclass predictions. 7 Error and Probabilistic Output Our foc</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Crammer, K., &amp; Singer, Y. (2003). Ultraconservative online algorithms for multiclass problems. Jornal of Machine Learning Research (JMLR), 3, 951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dredze</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<date>2008</date>
<booktitle>Confidence-weighted linear classification. International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="1611" citStr="Dredze et al., 2008" startWordPosition="223" endWordPosition="226">ntained during learning yields useful probabilistic information at test time. 1 Introduction Online learning algorithms such as the Perceptron process one example at a time, yielding simple and fast updates. They generally make few statistical assumptions about the data and are often used for natural language problems, where high dimensional feature representations, e.g., bags-of-words, demand efficiency. Most online algorithms, however, do not take into account the unique properties of such data, where many features are extremely rare and a few are very frequent. Dredze, Crammer and Pereira (Dredze et al., 2008; Crammer et al., 2008) recently introduced confidence weighted (CW) online learning for binary prediction problems. CW learning explicitly models classifier weight uncertainty using a multivariate Gaussian distribution over weight vectors. The learner makes online updates based on its confidence in the current parameters, making larger changes in the weights of infrequently observed features. Empirical evaluation has demonstrated the advantages of this approach for a number of binary natural language processing (NLP) problems. In this work, we develop and test multi-class confidence weighted </context>
<context position="5842" citStr="Dredze et al. (2008)" startWordPosition="936" endWordPosition="939">covariance matrix E ∈ Rdxd. Given an input instance x, a Gibbs classifier draws a weight vector w from the distribution and then makes a prediction according to the sign of w · x. This prediction rule is robust if the example is classified correctly with high-probability, that is, for some confidence parameter .5 ≤ η &lt; 1, Prw [y (w · x) ≥ 0] ≥ η. To learn a binary CW classifier in the online framework, the robustness property is enforced at each iteration while making a minimal update to the parameter distribution in the KL sense: DKL (N (µ, E) k N (µi, Ei)) s.t. Prw [yi (w · xi) ≥ 0] ≥ η (1) Dredze et al. (2008) showed that this optimization can be solved in closed form, yielding the updates µi+1 = µi + αiEixi (2) —1 Ei+1 = (Ei 1 + βixixT) (3) for appropriate αi and βi. For prediction, they use the Bayesian rule Prw—N(µ,E) [z (x · w) ≥ 0] , which for binary labels is equivalent to using the mean parameters directly, yˆ = sign (µ · x). 4 Multi-Class Confidence Weighted Learning As in the binary case, we maintain a distribution over weight vectors w ∼ N (µ, E). Given an input instance x, a Gibbs classifier draws a weight vector w ∼ N (µ, E) and then predicts the label with the maximal score, arg maxz (</context>
<context position="11113" citStr="Dredze et al., 2008" startWordPosition="1952" endWordPosition="1955">d we can choose η(out) = 1 − 1 − η(in) K − 1 , for the outer bound and the resulting ηr will be equal. By optimizing η we automatically tune the approximation to achieve the best compromise between the inner and outer bounds. In the following, we will therefore assume ηr = η. 5 Online Updates Our algorithms are online and process examples one at a time. Pseudo-code for our approach is given in algorithm 1. We approximate the prediction step by ranking each label y according to the score given by the mean weight vector, µ · f(xi, y). Although this approach is Bayes optimal for binary problems (Dredze et al., 2008), 498 it is an approximation in general. We note that more accurate inference can be performed in the multi-class case by sampling weight vectors from the distribution N(µ, Σ) or selecting labels sensitive to the variance of prediction; however, in our experiments this did not improve performance and required significantly more computation. We therefore proceed with this simple and effective approximation. The update rule is given by an approximation of the type described in Sec. 4. All that remains is to choose the constraint set and solve the optimization efficiently. We discuss several sche</context>
<context position="12399" citStr="Dredze et al., 2008" startWordPosition="2166" endWordPosition="2169">s of the form Pr [Ayi,r(x)] &gt; η. We start with a single constraint. 5.1 Single-Constraint Updates The simplest approach is to select the single constraint Pr [Ayi,r(x)] &gt; η corresponding to the highest-ranking label r =� yi. This ensures that, following the update, the true label is more likely to be predicted than the label that was its closest competitor. We refer to this as the k = 1 update. Whenever we have only a single constraint, we can reduce the optimization to one of the closedform CW updates used for binary classification. Several have been proposed, based on linear approximations (Dredze et al., 2008) and exact formulations (Crammer et al., 2008). For simplicity, we use the Variance method from Dredze et al. (2008), which did well in our initial evaluations. This method leads to the following update rules. Note that in practice Σ is projected to a diagonal matrix as part of the update; this is necessary due to the large number of features that we use. µi+1 = µi + αiΣigi,yi,r (7) (ΣiΣi+1 = 1 + 2αiφgi,yi,rgi,yi,r) (8) gi,yi,r = f(xi, yi) − f (xi, r) φ = Φ−1(η) The scale αi is given by max(γi, 0), where γi is equal to V/ −(1 + 2φmi) + (1 + 2φmi)2 − 8φ(mi − φvi) 4φvi and mi = µi · gi,yi,r vi =</context>
<context position="14745" citStr="Dredze et al. (2008)" startWordPosition="2613" endWordPosition="2616">tive to the Hildreth algorithm, we consider the simultaneous algorithm of Iusem and Pierro (1987), which finds an exact solution by iterating over the constraints in parallel. As above, we approximate the exact solution by performing only one iteration. The process is as follows. • For j = 1, ... , k, set (µi,j, Σi,j) to the solution of the following optimization: DKL (N (µ, Σ) II N (µi, Σi)) s.t. Pr [Ayi,rj(x)] &gt; η • Let λ be a vector, λj &gt;0 , Ej λj =1. • Set µi+1 = Ej λjµi,j, Σ−1 i+1 = Ej λjΣ−1 i,j . In practice we set λj = 1/k for all j. 6 Experiments 6.1 Datasets Following the approach of Dredze et al. (2008), we evaluate using five natural language classification tasks over nine datasets that vary in difficulty, size, and label/feature counts. See Table 1 for an overview. Brief descriptions follow. min µ,Σ min µ,Σ 499 Task Instances Features Labels Bal. 20 News 18,828 252,115 20 Y Amazon 7 13,580 686,724 7 Y Amazon 3 7,000 494,481 3 Y Enron A 3,000 13,559 10 N Enron B 3,000 18,065 10 N NYTD 10,000 108,671 26 N NYTO 10,000 108,671 34 N NYTS 10,000 114,316 20 N Reuters 4,000 23,699 4 N Table 1: A summary of the nine datasets, including the number of instances, features, and labels, and whether the </context>
<context position="18736" citStr="Dredze et al. (2008)" startWordPosition="3241" endWordPosition="3244">d two approximations (sequential and parallel). Results (Table 2) show that the two approximations perform similarly. For every experiment the CW parameter η and the number of iterations (up to 10) were optimized using a single randomized iteration. However, sequential converges faster, needing an average of 4.33 iterations compared to 7.56 for parallel across all datasets. Therefore, we select sequential for our subsequent experiments. The exact method performs poorly, displaying the lowest performance on almost every dataset. This is unsurprising given similar results for binary CW learning Dredze et al. (2008), where exact updates were shown to over-fit but converged after a single iteration of training. Similarly, our exact implementation converges after an average of 1.25 iterations, much faster than either of the approximations. However, this rapid convergence appears to come at the expense of accuracy. Fig. 1 shows the accuracy on Amazon 7 test data after each training iteration. While both sequential and parallel improve with several iterations, exact de500 1 2 3 4 5 Training Iterations Figure 1: Accuracy on test data after each iteration on the Amazon 7 dataset. grades after the first iterati</context>
</contexts>
<marker>Dredze, Crammer, Pereira, 2008</marker>
<rawString>Dredze, M., Crammer, K., &amp; Pereira, F. (2008). Confidence-weighted linear classification. International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Iusem</author>
<author>A D Pierro</author>
</authors>
<title>A simultaneous iterative method for computing projections on polyhedra.</title>
<date>1987</date>
<journal>SIAM J. Control and Optimization,</journal>
<volume>25</volume>
<contexts>
<context position="14222" citStr="Iusem and Pierro (1987)" startWordPosition="2505" endWordPosition="2508">nd tends to over-fit in practice (Sec. 6.2). We propose several approximate alternatives. Sequential Update: The Hildreth algorithm iterates over the constraints, updating with respect to each until convergence is reached. We approximate this solution by making only a single pass: • Set µi,0 = µi and Σi,0 = Σi. • For j = 1, ... , k, set (µi,j, Σi,j) to the solution of the following optimization: �� DKL �N (µ, Σ) II N �µi,j−1, Σi,j−1 s.t. Pr [Ayi,rj(x)] &gt; η • Set µi+1 = µi,k and Σi+1 = Σi,k. Parallel Update: As an alternative to the Hildreth algorithm, we consider the simultaneous algorithm of Iusem and Pierro (1987), which finds an exact solution by iterating over the constraints in parallel. As above, we approximate the exact solution by performing only one iteration. The process is as follows. • For j = 1, ... , k, set (µi,j, Σi,j) to the solution of the following optimization: DKL (N (µ, Σ) II N (µi, Σi)) s.t. Pr [Ayi,rj(x)] &gt; η • Let λ be a vector, λj &gt;0 , Ej λj =1. • Set µi+1 = Ej λjµi,j, Σ−1 i+1 = Ej λjΣ−1 i,j . In practice we set λj = 1/k for all j. 6 Experiments 6.1 Datasets Following the approach of Dredze et al. (2008), we evaluate using five natural language classification tasks over nine data</context>
</contexts>
<marker>Iusem, Pierro, 1987</marker>
<rawString>Iusem, A., &amp; Pierro, A. D. (1987). A simultaneous iterative method for computing projections on polyhedra. SIAM J. Control and Optimization, 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>Y Yang</author>
<author>T G Rose</author>
<author>F Li</author>
</authors>
<title>Rcv1: Anew benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>Journal ofMachine Learning Research (JMLR),</journal>
<volume>5</volume>
<pages>361--397</pages>
<contexts>
<context position="18000" citStr="Lewis et al. (2004)" startWordPosition="3128" endWordPosition="3131">pproximations (sequential and parallel) are roughly the same, the exact solution over-fits. posted (NYTO), and the section in which the article was printed (NYTS). Articles were represented as bags-of-words with feature counts (stop-words removed). Reuters Over 800,000 manually categorized newswire stories (RCV1-v2/ LYRL2004). Each article contains one or more labels describing its general topic, industry, and region. We performed topic classification with the four general topics: corporate, economic, government, and markets. Details on document preparation and feature extraction are given by Lewis et al. (2004). 6.2 Evaluations We first set out to compare the three update approaches proposed in Sec. 5.2: an exact solution and two approximations (sequential and parallel). Results (Table 2) show that the two approximations perform similarly. For every experiment the CW parameter η and the number of iterations (up to 10) were optimized using a single randomized iteration. However, sequential converges faster, needing an average of 4.33 iterations compared to 7.56 for parallel across all datasets. Therefore, we select sequential for our subsequent experiments. The exact method performs poorly, displayin</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>Lewis, D. D., Yang, Y., Rose, T. G., &amp; Li, F. (2004). Rcv1: Anew benchmark collection for text categorization research. Journal ofMachine Learning Research (JMLR), 5, 361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Malkin</author>
<author>J Bilmes</author>
</authors>
<title>Ratio semidefinite classifiers.</title>
<date>2008</date>
<booktitle>IEEE Int. Conf. on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="25285" citStr="Malkin &amp; Bilmes, 2008" startWordPosition="4367" endWordPosition="4370">st error conditioned on prediction probability. entropy Bin lower threshold Bin lower threshold Bin lower threshold 33 32 31 30 29 28 12 10 8 6 Test error in bin 4 2 0 800 600 400 200 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 MC CW MaxEnt Number of examples per bin 1200 1000 0 MaxEnt MC CW 0.8 WaxEnt WC CW 0.7 Test error given bin 0.6 0.5 0.4 0.3 0.2 0.1 0 MaxEnt MC CW ers have observed that maxent probabilities can have low entropy and be unreliable for estimating prediction confidence (Malkin &amp; Bilmes, 2008). Since CW also produces label probabilities — and does so in a conceptually distinct way — we investigate in this section some empirical properties of the label distributions induced by CW’s parameter distributions and compare them with those of maxent. We trained maxent and CW k = 1 classifiers on the Enron B dataset, optimizing parameters as before (maxent’s Gaussian prior and CW’s η). We estimated the label distributions from our CW classifiers after each iteration and on every test example x by Gibbs sampling weight vectors w - N(µ, E), and for each label y counting the fraction of weight</context>
</contexts>
<marker>Malkin, Bilmes, 2008</marker>
<rawString>Malkin, J., &amp; Bilmes, J. (2008). Ratio semidefinite classifiers. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http:// mallet.cs.umass.edu.</note>
<contexts>
<context position="22360" citStr="McCallum (2002)" startWordPosition="3868" endWordPosition="3869"> and 11 seconds for one iteration of all 7 constraints. These differences are insignificant compared to the cost of performing multiple iterations over a large dataset. We note that, while both approximate methods took about the same amount of time, the exact solution took over 4 minutes for its first iteration. Finally, we compare CW methods with several baselines in Table 3. Online baselines include Top-1 Perceptron (Collins, 2002), Top-1 Passive-Aggressive (PA), and k-best PA (Crammer &amp; Singer, 2003; McDonald et al., 2004). Batch algorithms include Maximum Entropy (default configuration in McCallum (2002)) and support vector machines (LibSVM (Chang &amp; Lin, 2001) for one-against-one classification and multiclass (MC) (Crammer &amp; Singer, 2001)). Classifier parameters (C for PA/SVM and maxent’s Gaussian prior) and number of iterations (up to 10) for the online methods were optimized using a single randomized iteration. On eight of the nine datasets, CW improves over all baselines. In general, CW provides faster and more accurate multiclass predictions. 7 Error and Probabilistic Output Our focus so far has been on accuracy and speed. However, there are other important considerations for selecting le</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>McCallum, A. (2002). MALLET: A machine learning for language toolkit. http:// mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Large margin online learning algorithms for scalable structured classification. NIPS Workshop on Structured Outputs.</title>
<date>2004</date>
<contexts>
<context position="22276" citStr="McDonald et al., 2004" startWordPosition="3855" endWordPosition="3858">luation in Fig. 1 required 3 seconds for the first iteration of k = 1, 10 seconds for k = 5 and 11 seconds for one iteration of all 7 constraints. These differences are insignificant compared to the cost of performing multiple iterations over a large dataset. We note that, while both approximate methods took about the same amount of time, the exact solution took over 4 minutes for its first iteration. Finally, we compare CW methods with several baselines in Table 3. Online baselines include Top-1 Perceptron (Collins, 2002), Top-1 Passive-Aggressive (PA), and k-best PA (Crammer &amp; Singer, 2003; McDonald et al., 2004). Batch algorithms include Maximum Entropy (default configuration in McCallum (2002)) and support vector machines (LibSVM (Chang &amp; Lin, 2001) for one-against-one classification and multiclass (MC) (Crammer &amp; Singer, 2001)). Classifier parameters (C for PA/SVM and maxent’s Gaussian prior) and number of iterations (up to 10) for the online methods were optimized using a single randomized iteration. On eight of the nine datasets, CW improves over all baselines. In general, CW provides faster and more accurate multiclass predictions. 7 Error and Probabilistic Output Our focus so far has been on ac</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2004</marker>
<rawString>McDonald, R., Crammer, K., &amp; Pereira, F. (2004). Large margin online learning algorithms for scalable structured classification. NIPS Workshop on Structured Outputs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sandhaus</author>
</authors>
<title>The new york times annotated corpus. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Philadelphia.</location>
<contexts>
<context position="2750" citStr="Sandhaus, 2008" startWordPosition="392" endWordPosition="393">problems. In this work, we develop and test multi-class confidence weighted online learning algorithms. For binary problems, the update rule is a simple convex optimization problem and inference is analytically computable. However, neither is true in the multi-class setting. We discuss several efficient online learning updates. These update rules can involve one, some, or all of the competing (incorrect) labels. We then perform an extensive evaluation of our algorithms using nine multi-class NLP classification problems, including three derived from the recently released New York Times corpus (Sandhaus, 2008). To the best of our knowledge, this is the first learning evaluation on these data. Our best algorithm outperforms state-of-the-art online algorithms and batch algorithms on eight of the nine datasets. Surprisingly, we find that a simple algorithm in which updates consider only a single competing label often performs as well as or better than multiconstraint variants if it makes multiple passes over the data. This is especially promising for large datasets, where the efficiency of the update can be important. In the true online setting, where only one iteration is possible, multi-constraint a</context>
<context position="16612" citStr="Sandhaus, 2008" startWordPosition="2922" endWordPosition="2923">essage as a binary bag-of-words. Enron Automatic sorting of emails into folders.2 We selected two users with many email folders and messages: farmer-d (Enron A) and kaminski-v (Enron B). We used the ten largest folders for each user, excluding non-archival email folders such as “inbox,” “deleted items,” and “discussion threads.” Emails were represented as binary bags-of-words with stop-words removed. NY Times To the best of our knowledge we are the first to evaluate machine learning methods on the New York Times corpus. The corpus contains 1.8 million articles that appeared from 1987 to 2007 (Sandhaus, 2008). In addition to being one of the largest collections of raw news text, it is possibly the largest collection of publicly released annotated news text, and therefore an ideal corpus for large scale NLP tasks. Among other annotations, each article is labeled with the desk that produced the story (Financial, Sports, etc.) (NYTD), the online section to which the article was 1http://people.csail.mit.edu/jrennie/20Newsgroups/ 2http://www.cs.cmu.edu/˜enron/ Task Sequential Parallel Exact 20 News 92.16 91.41 88.08 Amazon 7 77.98 78.35 77.92 Amazon 3 93.54 93.81 93.00 Enron A 82.40 81.30 77.07 Enron B</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Sandhaus, E. (2008). The new york times annotated corpus. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>