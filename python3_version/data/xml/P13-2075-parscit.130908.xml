<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002022">
<title confidence="0.989223">
Minimum Bayes Risk based Answer Re-ranking for Question Answering
</title>
<author confidence="0.99093">
Nan Duan
</author>
<affiliation confidence="0.981363">
Natural Language Computing
Microsoft Research Asia
</affiliation>
<email confidence="0.983151">
nanduan@microsoft.com
</email>
<sectionHeader confidence="0.99721" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999295625">
This paper presents two minimum Bayes
risk (MBR) based Answer Re-ranking
(MBRAR) approaches for the question
answering (QA) task. The first approach
re-ranks single QA system’s outputs by
using a traditional MBR model, by mea-
suring correlations between answer can-
didates; while the second approach re-
ranks the combined outputs of multiple
QA systems with heterogenous answer ex-
traction components by using a mixture
model-based MBR model. Evaluation-
s are performed on factoid questions se-
lected from two different domains: Jeop-
ardy! and Web, and significant improve-
ments are achieved on all data sets.
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996382">
Minimum Bayes Risk (MBR) techniques have
been successfully applied to a wide range of nat-
ural language processing tasks, such as statisti-
cal machine translation (Kumar and Byrne, 2004),
automatic speech recognition (Goel and Byrne,
2000), parsing (Titov and Henderson, 2006), etc.
This work makes further exploration along this
line of research, by applying MBR technique to
question answering (QA).
The function of a typical factoid question an-
swering system is to automatically give answers to
questions in most case asking about entities, which
usually consists of three key components: ques-
tion understanding, passage retrieval, and answer
extraction. In this paper, we propose two MBR-
based Answer Re-ranking (MBRAR) approaches,
aiming to re-rank answer candidates from either
single and multiple QA systems. The first one
re-ranks answer outputs from single QA system
based on a traditional MBR model by measuring
the correlations between each answer candidates
and all the other candidates; while the second one
re-ranks the combined answer outputs from multi-
ple QA systems based on a mixture model-based
MBR model. The key contribution of this work is
that, our MBRAR approaches assume little about
QA systems and can be easily applied to QA sys-
tems with arbitrary sub-components.
The remainder of this paper is organized as fol-
lows: Section 2 gives a brief review of the QA task
and describe two types of QA systems with differ-
ent pros and cons. Section 3 presents two MBRAR
approaches that can re-rank the answer candidates
from single and multiple QA systems respectively.
The relationship between our approach and pre-
vious work is discussed in Section 4. Section 5
evaluates our methods on large scale questions s-
elected from two domains (Jeopardy! and Web)
and shows promising results. Section 6 concludes
this paper.
</bodyText>
<sectionHeader confidence="0.898268" genericHeader="method">
2 Question Answering
</sectionHeader>
<subsectionHeader confidence="0.88462">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.99937">
Formally, given an input question Q, a typical fac-
toid QA system generates answers on the basis of
the following three procedures:
</bodyText>
<listItem confidence="0.998083166666667">
(1) Question Understanding, which determines
the answer type and identifies necessory informa-
tion contained in Q, such as question focus and
lexical answer type (LAT). Such information will
be encoded and used by the following procedures.
(2) Passage Retrieval, which formulates queries
based on Q, and retrieves passages from offline
corpus or online search engines (e.g. Google and
Bing).
(3) Answer Extraction, which first extracts an-
swer candidates from retrieved passages, and then
ranks them based on specific ranking models.
</listItem>
<page confidence="0.983068">
424
</page>
<note confidence="0.444652">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 424–428,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.996715">
2.2 Two Types of QA Systems
</subsectionHeader>
<bodyText confidence="0.999653428571428">
We present two different QA sysytems, which are
distinguished from three aspects: answer typing,
answer generation, and answer ranking.
The 1st QA system is denoted as Type-
Dependent QA engine (TD-QA). In answer typing
phase, TD-QA assigns the most possible answer
type T� to a given question Q based on:
</bodyText>
<equation confidence="0.998249666666667">
T� = argmax P(T|Q)
T
P(T |Q) is a probabilistic answer-typing mod-
</equation>
<bodyText confidence="0.9839025">
el that is similar to Pinchak and Lin (2006)’s
work. In answer generation phase, TD-QA uses
a CRF-based Named Entity Recognizer to detect
all named entities contained in retrieved passages
with the type T� , and treat them as the answer can-
didate space W(Q):
</bodyText>
<equation confidence="0.999018">
W(Q) = U Ak
k
</equation>
<bodyText confidence="0.593442666666667">
In answer ranking phase, the decision rule de-
scribed below is used to rank answer candidate s-
pace W(Q):
</bodyText>
<equation confidence="0.99789625">
A� = argmax
A∈H(Q)
�= argmax λi • hi(A, T�, Q)
A∈H(Q) i
</equation>
<bodyText confidence="0.999902791666667">
where {hi(�)} is a set of ranking features that
measure the correctness of answer candidates, and
{λi} are their corresponding feature weights.
The 2ed QA system is denoted as Type-
Independent QA engine (TI-QA). In answer typ-
ing phase, TI-QA assigns top N, instead of the
best, answer types TN(Q) for each question Q.
The probability of each type candidate is main-
tained as well. In answer generation phase, TI-
QA extracts all answer candidates from retrieved
passages based on answer types in TN(Q), by the
same NER used in TD-QA. In answer ranking
phase, TI-QA considers the probabilities of differ-
ent answer types as well:
ranking. However, as the answer-typing model is
far from perfect, if prediction errors happen, TD-
QA can no longer give correct answers at all.
On the other hand, TI-QA can provide higher
answer coverage, as it can extract answer candi-
dates with multiple answer types. However, more
answer candidates with different types bring more
difficulties to the answer ranking model to rank the
correct answer to the top 1 position. So the rank-
ing precision of TI-QA is not as good as TD-QA.
</bodyText>
<sectionHeader confidence="0.98912" genericHeader="method">
3 MBR-based Answering Re-ranking
</sectionHeader>
<subsectionHeader confidence="0.943493">
3.1 MBRAR for Single QA System
</subsectionHeader>
<bodyText confidence="0.9990868">
MBR decoding (Bickel and Doksum, 1977) aims
to select the hypothesis that minimizes the expect-
ed loss in classification. In MBRAR, we replace
the loss function with the gain function that mea-
sure the correlation between answer candidates.
Thus, the objective of the MBRAR approach for
single QA system is to find the answer candidate
that is most supported by other candidates under
QA system’s distribution, which can be formally
written as:
</bodyText>
<equation confidence="0.934928">
�
A� = argmax 9(A, Ak) • P(Ak|W(Q))
A∈H(Q) Ak∈H(Q)
</equation>
<bodyText confidence="0.998118666666667">
P(Ak|W(Q)) denotes the hypothesis distribu-
tion estimated on the search space W(Q) based on
the following log-linear formulation:
</bodyText>
<equation confidence="0.981460666666667">
exp(β � P(Ak|Q))
P(Ak|W(Q)) =
EA′∈H exp(β • P(A′|Q))
</equation>
<bodyText confidence="0.956743454545455">
P(Ak|Q) is the posterior probability of the answer
candidate Ak based on QA system’s ranking mod-
el, β is a scaling factor which controls the distri-
bution P(•) sharp (when β &gt; 1) or smooth (when
β &lt; 1).
9(A, Ak) is the gain function that denotes the
degree of how Ak supports A. This function can
be further expanded as a weighted combination of
a set of correlation features as: Ej λj • hj(A, Ak).
The following correlation features are used in
9(•):
</bodyText>
<equation confidence="0.791354">
P(A|
T�, Q)
A� = argmax P(A|Q) • answer-level n-gram correlation feature:
A∈H(Q)
�
= argmax P(A|T, Q) • P(T |Q)
A∈H(Q) T∈TN(Q)
</equation>
<bodyText confidence="0.911633">
On one hand, TD-QA can achieve relative high
ranking precision, as using a unique answer type
greatly reduces the size of the candidate list for
</bodyText>
<equation confidence="0.98229">
�h��swer(A, Ak) = #w(Ak)
w∈A
</equation>
<bodyText confidence="0.864298666666667">
where ω denotes an n-gram in A, #w(Ak)
denotes the number of times that ω occurs in
Ak.
</bodyText>
<page confidence="0.99369">
425
</page>
<listItem confidence="0.969605">
• passage-level n-gram correlation feature:
</listItem>
<equation confidence="0.854627">
�hpassage(A, Ak) = #ω(PAk)
ω∈PA
</equation>
<bodyText confidence="0.96939425">
where PA denotes passages from which A
are extracted. This feature measures the de-
gree of Ak supports A from the context per-
spective.
</bodyText>
<listItem confidence="0.717271">
• answer-type agreement feature:
htype(A, Ak) = 6(TA, TAi)
</listItem>
<bodyText confidence="0.959623666666667">
6(TA, TAk) denotes an indicator function that
equals to 1 when the answer types of A and
Ak are the same, and 0 otherwise.
</bodyText>
<listItem confidence="0.9977668">
• answer-length feature that is used to penalize
long answer candidates.
• averaged passage-length feature that is used
to penalize passages with a long averaged
length.
</listItem>
<subsectionHeader confidence="0.975679">
3.2 MBRAR for Multiple QA Systems
</subsectionHeader>
<bodyText confidence="0.999348166666667">
Aiming to apply MBRAR to the outputs from N
QA systems, we modify MBR components as fol-
lows.
First, the hypothesis space HC(Q) is built by
merging answer candidates of multiple QA sys-
tems:
</bodyText>
<equation confidence="0.971719">
HC(Q) = U Hi(Q)
i
</equation>
<bodyText confidence="0.9992614">
Second, the hypothesis distribution is defined
as a probability distribution over the combined
search space of N component QA systems and
computed as a weighted sum of component model
distributions:
</bodyText>
<equation confidence="0.99317">
N
P(A|HC(Q)) = ai · P(A|Hi(Q))
i=1
</equation>
<bodyText confidence="0.999823833333333">
where a1, ..., aN are coefficients with following
constraints holds1: 0 ≤ ai ≤ 1 and ENi=1 ai = 1,
P(A|Hi(Q)) is the posterior probability of A esti-
mated on the ith QA system’s search space Hi(Q).
Third, the features used in the gain function G(·)
can be grouped into two categories, including:
</bodyText>
<listItem confidence="0.973563">
• system-independent features, which includes
all features described in Section 3.1 for single
system based MBRAR method;
</listItem>
<footnote confidence="0.9745145">
1For simplicity, the coefficients are equally set: αi =
1/N.
</footnote>
<listItem confidence="0.984004">
• system-dependent features, which measure
the correctness of answer candidates based
on information provided by multiple QA sys-
tems:
</listItem>
<bodyText confidence="0.947008428571429">
– system indicator feature hsys(A, QAi),
which equals to 1 when A is generated
by the ith system QAi, and 0 otherwise;
– system ranking feature hrank(A, QAi),
which equals to the reciprocal of the
rank position of A predicted by QAi. If
QAi fails to generate A, then it equals
to 0;
– ensemble feature hcons(A), which e-
quals to 1 when A can be generated by
all individual QA system, and 0 other-
wise.
Thus, the MBRAR for multiple QA systems can
be finally formulated as follows:
</bodyText>
<equation confidence="0.96085">
�
A� = argmax G(A, Ai) · P(Ai|HC(Q))
A∈HC(Q) Ai∈HC(Q)
</equation>
<bodyText confidence="0.999936">
where the training process of the weights in the
gain function is carried out with Ranking SVM2
based on the method described in Verberne et al.
(2009).
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.99993952631579">
MBR decoding have been successfully applied to
many NLP tasks, e.g. machine translation, pars-
ing, speech recognition and etc. As far as we
know, this is the first work that applies MBR prin-
ciple to QA.
Yaman et al. (2009) proposed a classifica-
tion based method for QA task that jointly uses
multiple 5-W QA systems by selecting one opti-
mal QA system for each question. Comparing to
their work, our MBRAR approaches assume few
about the question types, and all QA systems con-
tribute in the re-ranking model. Tellez-Valero et
al. (2008) presented an answer validation method
that helps individual QA systems to automatical-
ly detect its own errors based on information from
multiple QA systems. Chu-Carroll et al. (2003) p-
resented a multi-level answer resolution algorithm
to merge results from the answering agents at the
question, passage, and answer levels. Grappy et al.
</bodyText>
<footnote confidence="0.998784">
2We use SV MRank (Joachims, 2006) that can be found-
ed at www.cs.cornell.edu/people/tj/svm light/svm rank.html/
</footnote>
<page confidence="0.998367">
426
</page>
<bodyText confidence="0.999833333333333">
(2012) proposed to use different score combina-
tions to merge answers from different QA system-
s. Although all methods mentioned above leverage
information provided by multiple QA systems, our
work is the first time to explore the usage of MBR
principle for the QA task.
</bodyText>
<sectionHeader confidence="0.999624" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999551">
5.1 Data and Metric
</subsectionHeader>
<bodyText confidence="0.999964416666667">
Questions from two different domains are used
as our evaluation data sets: the first data set in-
cludes 10,051 factoid question-answer pairs se-
lected from the Jeopardy! quiz show3; while the
second data set includes 360 celebrity-asking web
questions4 selected from a commercial search en-
gine, the answers for each question is labeled by
human annotators.
The evaluation metric Succeed@n is defined as
the number of questions whose correct answers
are successfully ranked to the top n answer can-
didates.
</bodyText>
<subsectionHeader confidence="0.978474">
5.2 MBRAR for Single QA System
</subsectionHeader>
<bodyText confidence="0.999813461538462">
We first evaluate the effectiveness of our MBRAR
for single QA system. Given the N-best answer
outputs from each single QA system, together with
their ranking scores assigned by the corresponding
ranking components, we further perform MBRAR
to re-rank them and show resulting numbers on t-
wo evaluation data sets in Table 1 and 2 respec-
tively.
Both Table 1 and Table 2 show that, by lever-
aging our MBRAR method on individual QA sys-
tems, the rankings of correct answers are consis-
tently improved on both Jeopardy! and web ques-
tions.
</bodyText>
<table confidence="0.9997568">
Joepardy! Succeed@1 Succeed@2 Succeed@3
TD-QA 2,289 2,693 2,885
MBRAR 2,372 2,784 2,982
TI-QA 2,527 3,397 3,821
MBRAR 2,628 3,500 3,931
</table>
<tableCaption confidence="0.907353">
Table 1: Impacts of MBRAR for single QA system
on Jeopardy! questions.
</tableCaption>
<bodyText confidence="0.99574225">
We also notice TI-QA performs significantly
better than TD-QA on Jeopardy! questions, but
worse on web questions. This is due to fac-
t that when the answer type is fixed (PERSON for
</bodyText>
<footnote confidence="0.997433">
3http://www.jeopardy.com/
4The answers of such questions are person names.
</footnote>
<table confidence="0.9994832">
Web Succeed@1 Succeed@2 Succeed@3
TD-QA 97 128 146
MBRAR 99 130 148
TI-QA 95 122 136
MBRAR 97 126 143
</table>
<tableCaption confidence="0.7738025">
Table 2: Impacts of MBRAR for single QA system
on web questions.
</tableCaption>
<bodyText confidence="0.984577">
celebrity-asking questions), TI-QA will generate
candidates with wrong answer types, which will
definitely deteriorate the ranking accuracy.
</bodyText>
<subsectionHeader confidence="0.792209">
5.3 MBRAR for Multiple QA Systems
</subsectionHeader>
<bodyText confidence="0.999856928571429">
We then evaluate the effectiveness of our MBRAR
for multiple QA systems. The mixture model-
based MBRAR method described in Section 3.2
is used to rank the combined answer outputs from
TD-QA and TI-QA, with ranking results shown in
Table 3 and 4.
From Table 3 and Table 4 we can see that, com-
paring to the ranking performances of single QA
systems TD-QA and TI-QA, MBRAR using two
QA systems’ outputs shows significant improve-
ments on both Jeopardy! and web questions. Fur-
thermore, comparing to MBRAR on single QA
system, MBRAR on multiple QA systems can pro-
vide extra gains on both questions sets as well.
</bodyText>
<table confidence="0.999466">
Jeopardy! Succeed@1 Succeed@2 Succeed@3
TD-QA 2,289 2,693 2,885
TI-QA 2,527 3,397 3,821
MBRAR 2,891 3,668 4,033
</table>
<tableCaption confidence="0.966224">
Table 3: Impacts of MBRAR for multiple QA sys-
tems on Jeopardy! questions.
</tableCaption>
<table confidence="0.99979275">
Web Succeed@1 Succeed@2 Succeed@3
TD-QA 97 128 146
TI-QA 95 122 136
MBRAR 108 137 152
</table>
<tableCaption confidence="0.993856">
Table 4: Impacts of MBRAR for multiple QA sys-
tems on web questions.
</tableCaption>
<sectionHeader confidence="0.998569" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9997775">
In this paper, we present two MBR-based answer
re-ranking approaches for QA. Comparing to pre-
vious methods, MBRAR provides a systematic
way to re-rank answers from either single or multi-
ple QA systems, without considering their hetero-
geneous implementations of internal components.
</bodyText>
<page confidence="0.994269">
427
</page>
<bodyText confidence="0.9999545">
Experiments on questions from two different do-
mains show that, our proposed method can sig-
nificantly improve the ranking performances. In
future, we will add more QA systems into our M-
BRAR framework, and design more features for
the MBR gain function.
</bodyText>
<sectionHeader confidence="0.999409" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999808697674418">
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected Topics. Holden-
Day Inc.
Jennifer Chu-Carroll, Krzysztof Czuba, John Prager,
and Abraham Ittycheriah. 2003. In Question An-
swering, Two Heads Are Better Than One. In pro-
ceeding of HLT-NAACL.
Vaibhava Goel and William Byrne. 2000. Minimum
bayes-risk automatic speech recognition, Computer
Speech and Language.
Arnaud Grappy, Brigitte Grau, and Sophie Ros-
set. 2012. Methods Combination and ML-based
Re-ranking of Multiple Hypothesis for Question-
Answering Systems, In proceeding of EACL.
Thorsten Joachims. 2006. Training Linear SVMs in
Linear Time, In proceeding of KDD.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statisti-cal Machine
Translation. In proceeding of HLT-NAACL.
Christopher Pinchak and Dekang Lin. 2006. A Prob-
abilistic Answer Type Model. In proceeding of EA-
CL.
Ivan Titov and James Henderson. 2006. Bayes Risk
Minimization in Natural Language Parsing. Techni-
cal report.
Alberto Tellez-Valero, Manuel Montes-y-Gomez, Luis
Villasenor-Pineda, and Anselmo Penas. 2008. Im-
proving Question Answering by Combining Multiple
Systems via Answer Validation. In proceeding of CI-
CLing.
Suzan Verberne, Clst Ru Nijmegen, Hans Van Hal-
teren, Clst Ru Nijmegen, Daphne Theijssen, Ru Ni-
jmegen, Stephan Raaijmakers, Lou Boves, and Clst
Ru Nijmegen. 2009. Learning to rank qa data. e-
valuating machine learning techniques for ranking
answers to why-questions. In proceeding of SIGIR
workshop.
Sibel Yaman, Dilek Hakkani-Tur, Gokhan Tur, Ralph
Grishman, Mary Harper, Kathleen R. McKe-
own, Adam Meyers, Kartavya Sharma. 2009.
Classification-Based Strategies for Combining Mul-
tiple 5-W Question Answering Systems. In proceed-
ing of INTERSPEECH.
</reference>
<page confidence="0.998348">
428
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.580439">
<title confidence="0.998775">Minimum Bayes Risk based Answer Re-ranking for Question Answering</title>
<author confidence="0.69478">Nan</author>
<affiliation confidence="0.78759">Natural Language Microsoft Research</affiliation>
<email confidence="0.998271">nanduan@microsoft.com</email>
<abstract confidence="0.999036176470588">This paper presents two minimum Bayes risk (MBR) based Answer Re-ranking approaches for the question answering (QA) task. The first approach re-ranks single QA system’s outputs by using a traditional MBR model, by measuring correlations between answer candidates; while the second approach reranks the combined outputs of multiple QA systems with heterogenous answer extraction components by using a mixture model-based MBR model. Evaluations are performed on factoid questions selected from two different domains: Jeopardy! and Web, and significant improvements are achieved on all data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P J Bickel</author>
<author>K A Doksum</author>
</authors>
<title>Mathematical Statistics: Basic Ideas and Selected Topics.</title>
<date>1977</date>
<publisher>HoldenDay Inc.</publisher>
<contexts>
<context position="5530" citStr="Bickel and Doksum, 1977" startWordPosition="890" endWordPosition="893">ties of different answer types as well: ranking. However, as the answer-typing model is far from perfect, if prediction errors happen, TDQA can no longer give correct answers at all. On the other hand, TI-QA can provide higher answer coverage, as it can extract answer candidates with multiple answer types. However, more answer candidates with different types bring more difficulties to the answer ranking model to rank the correct answer to the top 1 position. So the ranking precision of TI-QA is not as good as TD-QA. 3 MBR-based Answering Re-ranking 3.1 MBRAR for Single QA System MBR decoding (Bickel and Doksum, 1977) aims to select the hypothesis that minimizes the expected loss in classification. In MBRAR, we replace the loss function with the gain function that measure the correlation between answer candidates. Thus, the objective of the MBRAR approach for single QA system is to find the answer candidate that is most supported by other candidates under QA system’s distribution, which can be formally written as: � A� = argmax 9(A, Ak) • P(Ak|W(Q)) A∈H(Q) Ak∈H(Q) P(Ak|W(Q)) denotes the hypothesis distribution estimated on the search space W(Q) based on the following log-linear formulation: exp(β � P(Ak|Q)</context>
</contexts>
<marker>Bickel, Doksum, 1977</marker>
<rawString>P. J. Bickel and K. A. Doksum. 1977. Mathematical Statistics: Basic Ideas and Selected Topics. HoldenDay Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Krzysztof Czuba</author>
<author>John Prager</author>
<author>Abraham Ittycheriah</author>
</authors>
<title>In Question Answering, Two Heads Are Better Than One. In proceeding of HLT-NAACL.</title>
<date>2003</date>
<contexts>
<context position="10078" citStr="Chu-Carroll et al. (2003)" startWordPosition="1673" endWordPosition="1676">sing, speech recognition and etc. As far as we know, this is the first work that applies MBR principle to QA. Yaman et al. (2009) proposed a classification based method for QA task that jointly uses multiple 5-W QA systems by selecting one optimal QA system for each question. Comparing to their work, our MBRAR approaches assume few about the question types, and all QA systems contribute in the re-ranking model. Tellez-Valero et al. (2008) presented an answer validation method that helps individual QA systems to automatically detect its own errors based on information from multiple QA systems. Chu-Carroll et al. (2003) presented a multi-level answer resolution algorithm to merge results from the answering agents at the question, passage, and answer levels. Grappy et al. 2We use SV MRank (Joachims, 2006) that can be founded at www.cs.cornell.edu/people/tj/svm light/svm rank.html/ 426 (2012) proposed to use different score combinations to merge answers from different QA systems. Although all methods mentioned above leverage information provided by multiple QA systems, our work is the first time to explore the usage of MBR principle for the QA task. 5 Experiments 5.1 Data and Metric Questions from two differen</context>
</contexts>
<marker>Chu-Carroll, Czuba, Prager, Ittycheriah, 2003</marker>
<rawString>Jennifer Chu-Carroll, Krzysztof Czuba, John Prager, and Abraham Ittycheriah. 2003. In Question Answering, Two Heads Are Better Than One. In proceeding of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vaibhava Goel</author>
<author>William Byrne</author>
</authors>
<title>Minimum bayes-risk automatic speech recognition, Computer Speech and Language.</title>
<date>2000</date>
<contexts>
<context position="1011" citStr="Goel and Byrne, 2000" startWordPosition="147" endWordPosition="150">orrelations between answer candidates; while the second approach reranks the combined outputs of multiple QA systems with heterogenous answer extraction components by using a mixture model-based MBR model. Evaluations are performed on factoid questions selected from two different domains: Jeopardy! and Web, and significant improvements are achieved on all data sets. 1 Introduction Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. This work makes further exploration along this line of research, by applying MBR technique to question answering (QA). The function of a typical factoid question answering system is to automatically give answers to questions in most case asking about entities, which usually consists of three key components: question understanding, passage retrieval, and answer extraction. In this paper, we propose two MBRbased Answer Re-ranking (MBRAR) approaches, aiming to re-rank answer candidates from either single and multiple QA systems. The first one re-ranks a</context>
</contexts>
<marker>Goel, Byrne, 2000</marker>
<rawString>Vaibhava Goel and William Byrne. 2000. Minimum bayes-risk automatic speech recognition, Computer Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnaud Grappy</author>
<author>Brigitte Grau</author>
<author>Sophie Rosset</author>
</authors>
<title>Methods Combination and ML-based Re-ranking of Multiple Hypothesis for QuestionAnswering Systems, In proceeding of EACL.</title>
<date>2012</date>
<marker>Grappy, Grau, Rosset, 2012</marker>
<rawString>Arnaud Grappy, Brigitte Grau, and Sophie Rosset. 2012. Methods Combination and ML-based Re-ranking of Multiple Hypothesis for QuestionAnswering Systems, In proceeding of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training Linear SVMs in Linear Time, In proceeding of KDD.</title>
<date>2006</date>
<contexts>
<context position="10266" citStr="Joachims, 2006" startWordPosition="1705" endWordPosition="1706"> multiple 5-W QA systems by selecting one optimal QA system for each question. Comparing to their work, our MBRAR approaches assume few about the question types, and all QA systems contribute in the re-ranking model. Tellez-Valero et al. (2008) presented an answer validation method that helps individual QA systems to automatically detect its own errors based on information from multiple QA systems. Chu-Carroll et al. (2003) presented a multi-level answer resolution algorithm to merge results from the answering agents at the question, passage, and answer levels. Grappy et al. 2We use SV MRank (Joachims, 2006) that can be founded at www.cs.cornell.edu/people/tj/svm light/svm rank.html/ 426 (2012) proposed to use different score combinations to merge answers from different QA systems. Although all methods mentioned above leverage information provided by multiple QA systems, our work is the first time to explore the usage of MBR principle for the QA task. 5 Experiments 5.1 Data and Metric Questions from two different domains are used as our evaluation data sets: the first data set includes 10,051 factoid question-answer pairs selected from the Jeopardy! quiz show3; while the second data set includes </context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training Linear SVMs in Linear Time, In proceeding of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes-Risk Decoding for Statisti-cal Machine Translation. In proceeding of HLT-NAACL.</title>
<date>2004</date>
<contexts>
<context position="958" citStr="Kumar and Byrne, 2004" startWordPosition="140" endWordPosition="143">tputs by using a traditional MBR model, by measuring correlations between answer candidates; while the second approach reranks the combined outputs of multiple QA systems with heterogenous answer extraction components by using a mixture model-based MBR model. Evaluations are performed on factoid questions selected from two different domains: Jeopardy! and Web, and significant improvements are achieved on all data sets. 1 Introduction Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. This work makes further exploration along this line of research, by applying MBR technique to question answering (QA). The function of a typical factoid question answering system is to automatically give answers to questions in most case asking about entities, which usually consists of three key components: question understanding, passage retrieval, and answer extraction. In this paper, we propose two MBRbased Answer Re-ranking (MBRAR) approaches, aiming to re-rank answer candidates from either sin</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum Bayes-Risk Decoding for Statisti-cal Machine Translation. In proceeding of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Pinchak</author>
<author>Dekang Lin</author>
</authors>
<title>A Probabilistic Answer Type Model. In proceeding of EACL.</title>
<date>2006</date>
<contexts>
<context position="3933" citStr="Pinchak and Lin (2006)" startWordPosition="612" endWordPosition="615">dings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 424–428, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2.2 Two Types of QA Systems We present two different QA sysytems, which are distinguished from three aspects: answer typing, answer generation, and answer ranking. The 1st QA system is denoted as TypeDependent QA engine (TD-QA). In answer typing phase, TD-QA assigns the most possible answer type T� to a given question Q based on: T� = argmax P(T|Q) T P(T |Q) is a probabilistic answer-typing model that is similar to Pinchak and Lin (2006)’s work. In answer generation phase, TD-QA uses a CRF-based Named Entity Recognizer to detect all named entities contained in retrieved passages with the type T� , and treat them as the answer candidate space W(Q): W(Q) = U Ak k In answer ranking phase, the decision rule described below is used to rank answer candidate space W(Q): A� = argmax A∈H(Q) �= argmax λi • hi(A, T�, Q) A∈H(Q) i where {hi(�)} is a set of ranking features that measure the correctness of answer candidates, and {λi} are their corresponding feature weights. The 2ed QA system is denoted as TypeIndependent QA engine (TI-QA). </context>
</contexts>
<marker>Pinchak, Lin, 2006</marker>
<rawString>Christopher Pinchak and Dekang Lin. 2006. A Probabilistic Answer Type Model. In proceeding of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Bayes Risk Minimization in Natural Language Parsing.</title>
<date>2006</date>
<tech>Technical report.</tech>
<contexts>
<context position="1048" citStr="Titov and Henderson, 2006" startWordPosition="152" endWordPosition="155">dates; while the second approach reranks the combined outputs of multiple QA systems with heterogenous answer extraction components by using a mixture model-based MBR model. Evaluations are performed on factoid questions selected from two different domains: Jeopardy! and Web, and significant improvements are achieved on all data sets. 1 Introduction Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. This work makes further exploration along this line of research, by applying MBR technique to question answering (QA). The function of a typical factoid question answering system is to automatically give answers to questions in most case asking about entities, which usually consists of three key components: question understanding, passage retrieval, and answer extraction. In this paper, we propose two MBRbased Answer Re-ranking (MBRAR) approaches, aiming to re-rank answer candidates from either single and multiple QA systems. The first one re-ranks answer outputs from single QA system b</context>
</contexts>
<marker>Titov, Henderson, 2006</marker>
<rawString>Ivan Titov and James Henderson. 2006. Bayes Risk Minimization in Natural Language Parsing. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Tellez-Valero</author>
<author>Manuel Montes-y-Gomez</author>
<author>Luis Villasenor-Pineda</author>
<author>Anselmo Penas</author>
</authors>
<title>Improving Question Answering by Combining Multiple Systems via Answer Validation. In proceeding of CICLing.</title>
<date>2008</date>
<contexts>
<context position="9895" citStr="Tellez-Valero et al. (2008)" startWordPosition="1645" endWordPosition="1648">ut with Ranking SVM2 based on the method described in Verberne et al. (2009). 4 Related Work MBR decoding have been successfully applied to many NLP tasks, e.g. machine translation, parsing, speech recognition and etc. As far as we know, this is the first work that applies MBR principle to QA. Yaman et al. (2009) proposed a classification based method for QA task that jointly uses multiple 5-W QA systems by selecting one optimal QA system for each question. Comparing to their work, our MBRAR approaches assume few about the question types, and all QA systems contribute in the re-ranking model. Tellez-Valero et al. (2008) presented an answer validation method that helps individual QA systems to automatically detect its own errors based on information from multiple QA systems. Chu-Carroll et al. (2003) presented a multi-level answer resolution algorithm to merge results from the answering agents at the question, passage, and answer levels. Grappy et al. 2We use SV MRank (Joachims, 2006) that can be founded at www.cs.cornell.edu/people/tj/svm light/svm rank.html/ 426 (2012) proposed to use different score combinations to merge answers from different QA systems. Although all methods mentioned above leverage infor</context>
</contexts>
<marker>Tellez-Valero, Montes-y-Gomez, Villasenor-Pineda, Penas, 2008</marker>
<rawString>Alberto Tellez-Valero, Manuel Montes-y-Gomez, Luis Villasenor-Pineda, and Anselmo Penas. 2008. Improving Question Answering by Combining Multiple Systems via Answer Validation. In proceeding of CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzan Verberne</author>
</authors>
<title>Clst Ru Nijmegen, Hans Van Halteren, Clst Ru Nijmegen, Daphne Theijssen, Ru Nijmegen, Stephan Raaijmakers, Lou Boves, and Clst Ru Nijmegen.</title>
<date>2009</date>
<booktitle>In proceeding of SIGIR workshop.</booktitle>
<marker>Verberne, 2009</marker>
<rawString>Suzan Verberne, Clst Ru Nijmegen, Hans Van Halteren, Clst Ru Nijmegen, Daphne Theijssen, Ru Nijmegen, Stephan Raaijmakers, Lou Boves, and Clst Ru Nijmegen. 2009. Learning to rank qa data. evaluating machine learning techniques for ranking answers to why-questions. In proceeding of SIGIR workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sibel Yaman</author>
<author>Dilek Hakkani-Tur</author>
<author>Gokhan Tur</author>
<author>Ralph Grishman</author>
<author>Mary Harper</author>
<author>Kathleen R McKeown</author>
<author>Adam Meyers</author>
<author>Kartavya Sharma</author>
</authors>
<title>Classification-Based Strategies for Combining Multiple 5-W Question Answering Systems. In proceeding of INTERSPEECH.</title>
<date>2009</date>
<contexts>
<context position="9582" citStr="Yaman et al. (2009)" startWordPosition="1592" endWordPosition="1595">hcons(A), which equals to 1 when A can be generated by all individual QA system, and 0 otherwise. Thus, the MBRAR for multiple QA systems can be finally formulated as follows: � A� = argmax G(A, Ai) · P(Ai|HC(Q)) A∈HC(Q) Ai∈HC(Q) where the training process of the weights in the gain function is carried out with Ranking SVM2 based on the method described in Verberne et al. (2009). 4 Related Work MBR decoding have been successfully applied to many NLP tasks, e.g. machine translation, parsing, speech recognition and etc. As far as we know, this is the first work that applies MBR principle to QA. Yaman et al. (2009) proposed a classification based method for QA task that jointly uses multiple 5-W QA systems by selecting one optimal QA system for each question. Comparing to their work, our MBRAR approaches assume few about the question types, and all QA systems contribute in the re-ranking model. Tellez-Valero et al. (2008) presented an answer validation method that helps individual QA systems to automatically detect its own errors based on information from multiple QA systems. Chu-Carroll et al. (2003) presented a multi-level answer resolution algorithm to merge results from the answering agents at the q</context>
</contexts>
<marker>Yaman, Hakkani-Tur, Tur, Grishman, Harper, McKeown, Meyers, Sharma, 2009</marker>
<rawString>Sibel Yaman, Dilek Hakkani-Tur, Gokhan Tur, Ralph Grishman, Mary Harper, Kathleen R. McKeown, Adam Meyers, Kartavya Sharma. 2009. Classification-Based Strategies for Combining Multiple 5-W Question Answering Systems. In proceeding of INTERSPEECH.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>