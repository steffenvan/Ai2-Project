<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002819">
<title confidence="0.9889085">
SAGAN: An approach to Semantic Textual Similarity
based on Textual Entailment
</title>
<author confidence="0.851827">
Julio Castillo†$ Paula Estrella$
</author>
<note confidence="0.4087715">
$FaMAF, UNC, Argentina
†UTN-FRC, Argentina
</note>
<email confidence="0.567989">
jotacastillo@gmail.com
pestrella@famaf.unc.edu.ar
</email>
<sectionHeader confidence="0.98669" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999428142857143">
In this paper we report the results obtained
in the Semantic Textual Similarity (STS)
task, with a system primarily developed for
textual entailment. Our results are quite
promising, getting a run ranked 39 in the
official results with overall Pearson, and
ranking 29 with the Mean metric.
</bodyText>
<sectionHeader confidence="0.998129" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972913043478">
For the last couple of years the research com-
munity has focused on a deeper analysis of natural
languages, seeking to capture the meaning of the
text in different contexts: in machine translation
preserving the meaning of the translations is cru-
cial to determine whether a translation is useful or
not, in question-answering understanding the ques-
tion leads to the desired answers (while the oppo-
site case makes a system rather frustrating to the
user) and the examples could continue. In this
newly defined task, Semantic Textual Similarity,
there is hope that efforts in different areas will be
shared and united towards the goal of identifying
meaning and recognizing equivalent, similar or
unrelated texts. Our contribution to the task, is
from a textual entailment point of view, as will be
described below.
The paper is organized as follows: Section 2 de-
scribes the relevant tasks, Section 3 describes the
architecture of the system, then Section 4 shows
the experiments carried out and the results ob-
tained, and Section 5 presents some conclusions
and future work.
</bodyText>
<sectionHeader confidence="0.999729" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999951666666667">
In this section we briefly describe two different
tasks that are closely related and in which our sys-
tem has participated with very promising results.
</bodyText>
<subsectionHeader confidence="0.964149">
2.1 Textual Entailment
</subsectionHeader>
<bodyText confidence="0.997348117647059">
Textual Entailment (TE) is defined as a generic
framework for applied semantic inference, where
the core task is to determine whether the meaning
of a target textual assertion (hypothesis, H) can be
inferred from a given text (T). For example, given
the pair (T,H):
T: Fire bombs were thrown at the Tunisian embas-
sy in Bern
H: The Tunisian embassy in Switzerland was at-
tacked
we can conclude that T entails H.
The recently created challenge “Recognising
Textual Entailment” (RTE) started in 2005 with
the goal of providing a binary answer for each pair
(H,T), namely whether there is entailment or not
(Dagan et al., 2006). The RTE challenge has mu-
tated over the years, aiming at accomplishing more
</bodyText>
<note confidence="0.937752333333333">
667
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 667–672,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999637307692308">
accurate and specific solutions; for example, in
2008 a three-way decision was proposed (instead
of the original binary decision) consisting of “en-
tailment”, “contradiction” and “unknown”; in 2009
the organizers proposed a pilot task, the Textual
Entailment Search (Bentivogli et al, 2009), consist-
ing in finding all the sentences in a set of docu-
ments that entail a given Hypothesis and since
2010 there is a Novelty Detection Task, which
means that RTE systems are required to judge
whether the information contained in each H is
novel with respect to (i.e., not entailed by) the in-
formation contained in the corpus.
</bodyText>
<subsectionHeader confidence="0.999765">
2.2 Semantic Textual Similarity
</subsectionHeader>
<bodyText confidence="0.9999726">
The pilot task STS was recently defined in
Semeval 2012 (Aguirre et al., 2012) and has as
main objective measuring the degree of semantic
equivalence between two text fragments. STS is
related to both Recognizing Textual Entailment
(RTE) and Paraphrase Recognition, but has the
advantage of being a more suitable model for mul-
tiple NLP applications.
As mentioned before, the goal of the RTE task
(Bentivogli et al, 2009) is determining whether the
meaning of a hypothesis H can be inferred from a
text T. Thus, TE is a directional task and we say
that T entails H, if a person reading T would infer
that H is most likely true. The difference with STS
is that STS consists in determining how similar
two text fragments are, in a range from 5 (total
semantic equivalence) to 0 (no relation). Thus,
STS mainly differs from TE in that the classifica-
tion is graded instead of binary. In this manner,
STS is filling the gap between several tasks.
</bodyText>
<sectionHeader confidence="0.939621" genericHeader="method">
3 System architecture
</sectionHeader>
<bodyText confidence="0.9986051">
Sagan is a RTE system (Castillo and Cardenas,
2010) which has taken part of several challenges,
including the Textual Analysis Conference 2009
and TAC 2010, and the Semantic Textual Similari-
ty and Cross Lingual Textual Entailment for con-
tent synchronization as part of the Semeval 2012.
The system is based on a machine learning ap-
proach and it utilizes eight WordNet-based
(Fellbaum, 1998) similarity measures, as explained
in (Castillo, 2011), with the purpose of obtaining
the maximum similarity between two WordNet
concepts. A concept is a cluster of synonymous
terms that is called a synset in WordNet. These
text-to-text similarity measures are based on the
following word-to-word similarity metrics:
(Resnik, 1995), (Lin, 1997), (Jiang and Conrath,
1997), (Pirrò and Seco, 2008), (Wu &amp; Palmer,
1994), Path Metric, (Leacock &amp; Chodorow, 1998),
and a semantic similarity to sentence level named
SemSim (Castillo and Cardenas,2010).
</bodyText>
<subsectionHeader confidence="0.882576">
Fig.1. System architecture
</subsectionHeader>
<bodyText confidence="0.999921411764706">
The system construct a model of the semantic
similarity of two texts (T,H) as a function of the
semantic similarity of the constituent words of
both phrases. In order to reach this objective, we
used a text to text similarity measure which is
based on word to word similarity. Thus, we expect
that combining word to word similarity metrics to
text level would be a good indicator of text to text
similarity.
Additional information about how to produce
feature vectors as well as each word- and sentence-
level metric can be found in (Castillo, 2011). The
architecture of the system is shown in Figure 1.
The training set used for the submitted runs are
those provided by the organizers of the STS. How-
ever we also experimented with RTE datasets as
described in the next Section.
</bodyText>
<figure confidence="0.994876035714286">
Training sets:
MSR+MSRvid
+Europarl
MSR+MSRvid
MSR
Resnik
Normalizer
Sentence Level Semantic Metric
Word Level Semantic Metrics
RUN 3
Lin
RUN 2
Pre-Processing
RUN 1
Extraction Features
Similarity Score
Stemmer
SVM with
Regression
Test Set: MSR,
MSRvid,Europarl,
SMT-news, WN
W&amp;P
SemSim
Parser
...
...
668
</figure>
<sectionHeader confidence="0.957129" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999975181818182">
For preliminary experiments before the STS Chal-
lenge, we used the training set provided by the
organizers, denoted with &amp;quot;_train&amp;quot;, and consisting of
750 pairs of sentences from the MSR Paraphrase
Corpus (MSRpar), 750 pairs of sentences from the
MSRvid Corpus (MSRvid), 459 pairs of sentences
of the Europarl WMT2008 development set (SMT-
eur). We also used the RTE datasets from Pascal
RTE Challenge (Dagan et al., 2006) as part of our
training sets. Additionally, at the testing stage, we
used the 399 pairs of news conversation (SMT-
news) and 750 pairs of sentences where the first
one comes from Ontonotes and the second one
from a WordNet definition (On-WN).
In STS Challenge it was required that participat-
ing systems do not use the test set of MSR-
Paraphrase, the text of the videos in MSR-Video,
and the data from the evaluation tasks at any WMT
to develop or train their systems. Additionally, we
also assumed that the dataset to be processed was
unknown in the testing phase, in order to avoid any
kind of tuning of the system.
</bodyText>
<subsectionHeader confidence="0.998019">
4.1 Preliminary Experiments
</subsectionHeader>
<bodyText confidence="0.999965111111111">
In a preliminary study performed before the final
submission, we experimented with three machine
learning algorithms Support Vector Machine
(SVM) with regression and polynomial kernel,
Multilayer perceptron (MLP), and Linear Regres-
sion (LR). Table 1 shows the results obtained with
10-fold cross validation technique and Table 2
shows the results of testing them with two datasets
and 3 classifiers over MSR_train.
</bodyText>
<table confidence="0.99572275">
Classifier Pearson c.c
SVM with regression 0.54
MLP 0.51
LinearRegression 0.54
</table>
<tableCaption confidence="0.961416">
Table 1. Results obtained using MSR training set
(MSRpar + MSRvid) with 10 fold-cross validation.
</tableCaption>
<table confidence="0.999733714285714">
Training set &amp; ML algorithm Pearson c.c
Europarl + SVM w/ regression 0.61
Europarl + MLP 0.44
Europarl + linear regression 0.61
MSRvid + SVM w/ regression 0.70
MSRvid + MLP 0.52
MSRvid + linear regression 0.69
</table>
<tableCaption confidence="0.99993">
Table 2. Results obtained using MSR training set
</tableCaption>
<bodyText confidence="0.999006897435897">
Results reported in Table 1 show that we
achieved the best performance with SVM with
regression and Linear Regression classifiers and
using MLP we obtained the worst results to predict
each dataset. To our surprise, a linear regression
classifier reports better accuracy that MLP, it may
be mainly due to the correlation coefficient used,
namely Pearson, which is a measure of a linear
dependence between two variables and linear re-
gression builds a model assuming linear influence
of independent features. We believe that using
Spearman correlation should be better than using
the Pearson coefficient given that Spearman as-
sumes non-linear correlation among variables.
However, it is not clear how it behaves when sev-
eral dataset are combined to obtain a global score.
Indeed, further discussion is needed in order to
find the best metric to the STS pilot task. Given
these results, in our submission for the STS pilot
task we used a combination of STS datasets as
training set and the SVM with regression classifier.
Because our approach is mainly based on ma-
chine learning the quality and quantity of dataset is
a key factor to determine the performance of the
system, thus we decided to experiment with RTE
datasets too (Bentivogli et el., 2009) with the aim
of increasing the size of the training set.
To achieve this goal, first we chose the RTE3
dataset because it is simpler than subsequent da-
tasets and it was proved to provide a high accuracy
predicting other datasets (Castillo, 2011). Second,
taking into account that RTE datasets are binary
classified as YES or NO entailment, we assumed
that a non entailment can be treated as a value of
2.0 in the STS pilot task and an entailment can be
thought of as a value of 3.0 in STS. Of course,
many pairs classified as 3.0 could be mostly equiv-
alent (4.0) or completely equivalent (5.0) but we
ignored this fact in the following experiment.
</bodyText>
<table confidence="0.999827333333334">
Training set Test set Pearson
c.c.
RTE3 MSR_train 0.4817
RTE3 MSRvid_train 0.5738
RTE3 Europarl_train 0.4746
MSR_train+RTE3 MSRvid_train 0.5652
MSR_train+RTE3 Europarl_train 0.5498
MSRvid_train+RTE3 MSR_train 0.4559
MSRvid_train+RTE3 Europarl_train 0.4964
</table>
<tableCaption confidence="0.980395">
Table 3. Results obtained using RTE in the training sets
and SVM w/regression as classifier
</tableCaption>
<page confidence="0.8272">
669
</page>
<bodyText confidence="0.999960789473684">
From these experiments we conclude that RTE3
alone is not enough to adequately predict neither of
the STS datasets, and it is understandable if we
note that only one pair with 2.0 and 3.0 scores are
present in this dataset.
On the other hand, by combining RTE3 with a
STS corpus we always obtain a slight decrease in
performance in comparison to using STS alone. It
is likely due to an unbalanced set and possible
contradictory pairs (e.g: a par in RTE3 classified as
3.0 when it should be classified 4.3). Thus, we
conclude that in order to use the RTE datasets our
system needs a manual annotation of the degree of
semantic similarity of every pair &lt;T,H&gt; of RTE
dataset.
Having into account that in our training phase
we obtained a decrease in performance using RTE
datasets we decided not to submit any run using
the RTE datasets.
</bodyText>
<subsectionHeader confidence="0.99342">
4.2 Submission to the STS shared task
</subsectionHeader>
<bodyText confidence="0.99755824050633">
Our participation in the shared task consisted of
three different runs using a SVM classifier with
regression; the runs were set up as follows:
- Run 1: system trained on a subset of the Mi-
crosoft Research Paraphrase Corpus (Dolan and
Brockett, 2005), named MSR and consisting of
750 pairs of sentences marked with a degree of
similarity from 5 to 0.
- Run 2: in addition to the MSR corpus we incor-
porated another 750 sentences extracted from the
Microsoft Research Video Description Corpus
(MSRvid), annotated in the same way as MSR.
- Run 3: to the 1500 sentences from the MSR and
MSRvid corpus we incorporated 734 pairs of sen-
tences from the Europarl corpus used as develop-
ment set in the WMT 2008; all sentences are
annotated with the degree of similarity from 5 to 0.
It is very interesting to note that we used the
same system configurations for every dataset of
each RUN. In this manner, we did not perform any
kind of tuning to a particular dataset before our
submission. We decided to ignore the &amp;quot;name&amp;quot; of
each dataset and apply our system regardless of the
particular dataset. Surely, if we take into account
where each dataset came from we can develop a
particular strategy for every one of them, but we
assumed that this kind of information is unknown
to our system.
The official scores of the STS pilot task is the
Pearson correlation coefficient, and other varia-
tions of Pearson which were proposed by the or-
ganizers with the aim of better understanding the
behavior of the competing systems among the dif-
ferent scenarios.
These metric are named ALL (overall Pearson),
ALLnrm (normalized Pearson) and Mean
(weighted mean), briefly described below:
- ALL: To compute this metric, first a new dataset
with the union of the five gold datasets is created
and then the Pearson correlation is calculated over
this new dataset.
- ALLnrm: In this metric, the Pearson correlation
is computed after the system outputs for each da-
taset are fitted to the gold standard using least
squares.
- Mean: This metric is a weighted mean across the
five datasets, where the weight is given by the
quantity of pairs in each dataset.
Table 5 report the results achieved with these
metrics followed by an individual Pearson correla-
tion for each dataset.
Interestingly, if we analyze the size of data sets,
we see that the larger the training set used, the
greater the efficiency gains with ALL metric. In
effect, RUN3 used 2234 pairs, RUN2 used 1500
pairs and RUN1 was composed by 750 pairs. This
highlights the need for larger datasets for the pur-
pose of building more accurate models.
With ALLnrm our system achieved better re-
sults but since this metric is based on normalized
Pearson correlation which assumes a linear correla-
tion, we believe that this metric is not representa-
tive of the underlying phenomenon. For example,
conducting manual observation we can see that
pairs from SMT-news are much harder to classify
than MSRvid pairs. This results can also be evi-
denced from others participating teams who almost
always achieved better results with MSRvid than
SMT-news dataset.
The last metric proposed is the Mean and we are
ranked 29 among participating teams. It is proba-
bly due to the weight of SMT-news (399 pairs) is
smaller than MSR or MSRvid.
Mean metrics seems to be more suitable for this
task but lack an important issue, do not have into
account the different &amp;quot;complexity&amp;quot; of the datasets.
It is also a issue for all metrics proposed. We be-
lieve that incorporating to Mean metric a complex-
ity factor weighting for each dataset based on a
</bodyText>
<page confidence="0.755516">
670
</page>
<bodyText confidence="0.999795">
human judge assignment could be more suitable
for the STS evaluation. We think in complexity as
an underlying concept referring to the difficulty of
determine how semantically related two sentences
are to one another. Thus, two sentences with high
lexical overlap should have a low complexity and
instead two sentences that requires deep inference
to determine similarity should have a high com-
plexity. This should be heighted by human annota-
tors and could be a method for a more precise
evaluation of STS systems.
Finally, we suggested measuring this new chal-
lenging task using a weighted Mean of the
Spearman&apos;s rho correlation coefficient by incorpo-
rating a factor to weigh the difficulty of each da-
taset.
</bodyText>
<table confidence="0.998893714285714">
Run ALL Rank ALLnrm Rank Mean Rank MSR MSR SMT- On- SMT-
Nrm Mean par vid eur WN news
Best Run ,8239 1 ,8579 2 ,6773 1 ,6830 ,8739 ,5280 ,6641 ,4937
Worst Run -,0260 89 ,5933 89 ,1016 89 ,1109 ,0057 ,0348 ,1788 ,1964
Sagan-RUN1 ,5522 57 ,7904 47 ,5906 29 ,5659 ,7113 ,4739 ,6542 ,4253
Sagan-RUN2 ,6272 42 ,8032 37 ,5838 34 ,5538 ,7706 ,4480 ,6135 ,3894
Sagan-RUN3 ,6311 39 ,7943 45 ,5649 46 ,5394 ,7560 ,4181 ,5904 ,3746
</table>
<tableCaption confidence="0.997339">
Table 5. Official results of the STS challenge
</tableCaption>
<sectionHeader confidence="0.942562" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999502523809524">
In this paper we present Sagan, an RTE system
applied to the task of Semantic Textual Similarity.
After a preliminary study of the classifiers perfor-
mance for the task, we decided to use a combina-
tion of STS datasets for training and the classifier
SVM with regression. With this setup the system
was ranked 39 in the best run with overall Pearson,
and ranked 29 with Mean metric. However, both
rankings are based on the Pearson correlation coef-
ficient and we believe that this coefficient is not
the best suited for this task, thus we proposed a
Mean Spearman&apos;s rho correlation coefficient
weighted by complexity, instead. Therefore, fur-
ther application of other metrics should be one in
order to find the most representative and fair eval-
uation metric for this task. Finally, while promis-
ing results were obtained with our system, it still
needs to be tested on a diversity of settings. This is
work in progress, as the system is being tested as a
metric for the evaluation of machine translation, as
reported in (Castillo and Estrella, 2012).
</bodyText>
<sectionHeader confidence="0.58868" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.885536294117647">
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Arkaitz Zubiaga, and Hassan Sawaf. 1997. Acceler-
ated DP Based Search For Statistical Translation. In
Proceedings of the 5th European Conference on
Speech Communication and Technology
(EUROSPEECH-97).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th AnnualMeeting of the Association for
Computational Linguistics(ACL-02), pages 311–318.
Sonja Nießen, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. A Evaluation Tool for Machine
Translation:Fast Evaluation for MT Research. In
Proceedings of the 2nd International Conference on
Language Resources and Evaluation (LREC-2000).
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality using N-gram Co-occurrence
Statistics. In Proceedings of the 2nd International
Conference on Human Language Technology Re-
search (HLT-02), pages 138–145, San Francisco,
CA, USA.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the 43th Annual Meeting of the
Association of Computational Linguistics (ACL-05),
pages 65–72.
Michael Denkowski and Alon Lavie. 2011. METEOR-
NEXT and the METEOR Paraphrase Tables: Im-
proved Evaluation Support For Five Target Lan-
guages. Proceedings of the ACL 2010 Joint
Workshop on Statistical Machine Translation and
Metrics MATR.
</reference>
<page confidence="0.848998">
671
</page>
<note confidence="0.8390082">
He Yifan, Du Jinhua, Way Andy, and Van Josef . 2010.
The DCU dependency-based metric in WMT-
MetricsMATR 2010. In: WMT 2010 - Joint Fifth
Workshop on Statistical Machine Translation and
Metrics MATR, ACL, Uppsala, Sweden.
</note>
<reference confidence="0.998240289719626">
Chi-kiu Lo and Dekai Wu. 2011. MEANT: inexpensive,
high-accuracy, semi-automatic metric for evaluating
translation utility based on semantic roles. 49th An-
nual Meeting of the Association for Computational
Linguistic (ACL-2011). Portland, Oregon, US.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas
(AMTA-06), pages 223–231.
Ido Dagan, Oren Glickman and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
Challenge. In Quiñonero-Candela, J.; Dagan, I.;
Magnini, B.; d&apos;Alché-Buc, F. (Eds.) Machine Learn-
ing Challenges. Lecture Notes in Computer Science ,
Vol. 3944, pp. 177-190, Springer.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-language entailment modeling for translating
unknown terms. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL.
Stroudsburg, PA, USA, 791-799.
Wilker Aziz and Marc Dymetmany and Shachar Mirkin
and Lucia Specia and Nicola Cancedda and Ido Da-
gan. 2010. Learning an Expert from Human Annota-
tions in Statistical Machine Translation: the Case of
Out-of-VocabularyWords. In: Proceedings of the
14th annual meeting of the European Association for
Machine Translation (EAMT), Saint-Rapha, France.
Dahlmeier, Daniel and Liu, Chang and Ng, Hwee
Tou. 2011.TESLA at WMT 2011: Translation Evalu-
ation and Tunable Metric.In: Proceedings of the
Sixth Workshop on Statistical Machine Translation.
ACL, pages 78-84, Edinburgh, Scotland.
S. Pado, D. Cer, M. Galley, D. Jurafsky and C. Man-
ning. 2009. Measuring Machine Translation Quality
as Semantic Equivalence: A Metric Based on Entail-
ment Features. Journal of MT 23(2-3), 181-193.
S. Pado, M. Galley, D. Jurafsky and C. Manning. 2009a.
Robust Machine Translation Evaluation with Entail-
ment Features. Proceedings of ACL 2009.
Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-
lez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on
Semantic Textual Similarity. In Proceedings of the
6th International Workshop on Semantic Evalua-
tion (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational
Semantics (*SEM 2012).
Bentivogli, Luisa, Dagan Ido, Dang Hoa, Giampiccolo,
Danilo, Magnini Bernardo.2009.The Fifth PASCAL
RTE Challenge. In: Proceedings of the Text Analysis
Conference.
Fellbaum C. 1998. WordNet: An Electronic Lexical
Database, volume 1. MIT Press.
Castillo Julio. 2011. A WordNet-based semantic ap-
proach to textual entailment and cross-lingual textu-
al entailment. International Journal of Machine
Learning and Cybernetics - Springer, Volume 2,
Number 3.
Castillo Julio and Cardenas Marina. 2010. Using sen-
tence semantic similarity based onWordNet in recog-
nizing textual entailment. Iberamia 2010. In LNCS,
vol 6433. Springer, Heidelberg, pp 366–375.
Castillo Julio. 2010. A semantic oriented approach to
textual entailment using WordNet-based measures.
MICAI 2010. LNCS, vol 6437. Springer, Heidelberg,
pp 44–55.
Castillo Julio. 2010. Using machine translation systems
to expand a corpus in textual entailment. In: Proceed-
ings of the Icetal 2010. LNCS, vol 6233, pp 97–102.
Resnik P. 1995. Information content to evaluate seman-
tic similarity in a taxonomy. In: Proceedings of IJCAI
1995, pp 448–453 907.
Castillo Julio, Cardenas Marina. 2011. An Approach to
Cross-Lingual Textual Entailment using Online Ma-
chine Translation Systems. Polibits Journal. Vol 44.
Castillo Julio and Estrella Paula. 2012. Semantic Textu-
al Similarity for MT evaluation. NAACL 2012
Seventh Workshop on Statistical Machine Transla-
tion. WMT 2012, Montreal, Canada.
Lin D. 1997. An information-theoretic definition of
similarity. In: Proceedings of Conference on Machine
Learning, pp 296–304 909.
Jiang J, Conrath D.1997. Semantic similarity based on
corpus statistics and lexical taxonomy. In: Proceed-
ings of theROCLINGX 911
Pirro G., Seco N. 2008. Design, implementation and
evaluation of a new similarity metric combining fea-
ture and intrinsic information content. In: ODBASE
2008, Springer LNCS.
Wu Z, Palmer M. 1994. Verb semantics and lexical
selection. In: Proceedings of the 32nd ACL 916.
Leacock C, Chodorow M. 1998. Combining local con-
text and WordNet similarity for word sense identifi-
cation. MIT Press, pp 265–283 919
Hirst G, St-Onge D . 1998. Lexical chains as represen-
tations of context for the detection and correction of
malapropisms. MIT Press, pp 305–332 922
Banerjee S, Pedersen T. 2002. An adapted lesk algo-
rithm for word sense disambiguation using WordNet.
In: Proceeding of CICLING-02
William B. Dolan and Chris Brockett.2005. Automati-
cally Constructing a Corpus of Sentential Para-
phrases. Third International Workshop on
Paraphrasing (IWP2005). Asia Federation of Natural
Language Processing.
</reference>
<page confidence="0.908932">
672
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.408916">
<title confidence="0.9983205">SAGAN: An approach to Semantic Textual based on Textual Entailment</title>
<author confidence="0.979591">Paula</author>
<affiliation confidence="0.714311">UNC, Argentina</affiliation>
<address confidence="0.7927">Argentina</address>
<email confidence="0.918804">jotacastillo@gmail.compestrella@famaf.unc.edu.ar</email>
<abstract confidence="0.987482125">In this paper we report the results obtained in the Semantic Textual Similarity (STS) task, with a system primarily developed for textual entailment. Our results are quite promising, getting a run ranked 39 in the official results with overall Pearson, and ranking 29 with the Mean metric.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Arkaitz Zubiaga</author>
<author>Hassan Sawaf</author>
</authors>
<title>Accelerated DP Based Search For Statistical Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology (EUROSPEECH-97).</booktitle>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>Christoph Tillmann, Stephan Vogel, Hermann Ney, Arkaitz Zubiaga, and Hassan Sawaf. 1997. Accelerated DP Based Search For Statistical Translation. In Proceedings of the 5th European Conference on Speech Communication and Technology (EUROSPEECH-97).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguistics(ACL-02),</booktitle>
<pages>311--318</pages>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguistics(ACL-02), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz Josef Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>A Evaluation Tool for Machine Translation:Fast Evaluation for MT Research.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC-2000).</booktitle>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz Josef Och, Gregor Leusch, and Hermann Ney. 2000. A Evaluation Tool for Machine Translation:Fast Evaluation for MT Research. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality using N-gram Co-occurrence Statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Conference on Human Language Technology Research (HLT-02),</booktitle>
<pages>138--145</pages>
<location>San Francisco, CA, USA.</location>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic Evaluation of Machine Translation Quality using N-gram Co-occurrence Statistics. In Proceedings of the 2nd International Conference on Human Language Technology Research (HLT-02), pages 138–145, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting of the Association of Computational Linguistics (ACL-05),</booktitle>
<pages>65--72</pages>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the 43th Annual Meeting of the Association of Computational Linguistics (ACL-05), pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>METEORNEXT and the METEOR Paraphrase Tables: Improved Evaluation Support For Five Target Languages.</title>
<date>2011</date>
<booktitle>Proceedings of the ACL 2010 Joint Workshop on Statistical Machine Translation and Metrics MATR.</booktitle>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. METEORNEXT and the METEOR Paraphrase Tables: Improved Evaluation Support For Five Target Languages. Proceedings of the ACL 2010 Joint Workshop on Statistical Machine Translation and Metrics MATR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>MEANT: inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles.</title>
<date>2011</date>
<booktitle>49th Annual Meeting of the Association for Computational Linguistic (ACL-2011).</booktitle>
<location>Portland, Oregon, US.</location>
<marker>Lo, Wu, 2011</marker>
<rawString>Chi-kiu Lo and Dekai Wu. 2011. MEANT: inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles. 49th Annual Meeting of the Association for Computational Linguistic (ACL-2011). Portland, Oregon, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-06),</booktitle>
<pages>223--231</pages>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-06), pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge. In</title>
<date>2006</date>
<journal>Lecture Notes in Computer Science ,</journal>
<volume>3944</volume>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2401" citStr="Dagan et al., 2006" startWordPosition="380" endWordPosition="383">tual Entailment (TE) is defined as a generic framework for applied semantic inference, where the core task is to determine whether the meaning of a target textual assertion (hypothesis, H) can be inferred from a given text (T). For example, given the pair (T,H): T: Fire bombs were thrown at the Tunisian embassy in Bern H: The Tunisian embassy in Switzerland was attacked we can conclude that T entails H. The recently created challenge “Recognising Textual Entailment” (RTE) started in 2005 with the goal of providing a binary answer for each pair (H,T), namely whether there is entailment or not (Dagan et al., 2006). The RTE challenge has mutated over the years, aiming at accomplishing more 667 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 667–672, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics accurate and specific solutions; for example, in 2008 a three-way decision was proposed (instead of the original binary decision) consisting of “entailment”, “contradiction” and “unknown”; in 2009 the organizers proposed a pilot task, the Textual Entailment Search (Bentivogli et al, 2009), consisting in finding all the sentences in a set of documen</context>
<context position="6753" citStr="Dagan et al., 2006" startWordPosition="1087" endWordPosition="1090">UN 3 Lin RUN 2 Pre-Processing RUN 1 Extraction Features Similarity Score Stemmer SVM with Regression Test Set: MSR, MSRvid,Europarl, SMT-news, WN W&amp;P SemSim Parser ... ... 668 4 Experiments and Results For preliminary experiments before the STS Challenge, we used the training set provided by the organizers, denoted with &amp;quot;_train&amp;quot;, and consisting of 750 pairs of sentences from the MSR Paraphrase Corpus (MSRpar), 750 pairs of sentences from the MSRvid Corpus (MSRvid), 459 pairs of sentences of the Europarl WMT2008 development set (SMTeur). We also used the RTE datasets from Pascal RTE Challenge (Dagan et al., 2006) as part of our training sets. Additionally, at the testing stage, we used the 399 pairs of news conversation (SMTnews) and 750 pairs of sentences where the first one comes from Ontonotes and the second one from a WordNet definition (On-WN). In STS Challenge it was required that participating systems do not use the test set of MSRParaphrase, the text of the videos in MSR-Video, and the data from the evaluation tasks at any WMT to develop or train their systems. Additionally, we also assumed that the dataset to be processed was unknown in the testing phase, in order to avoid any kind of tuning </context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In Quiñonero-Candela, J.; Dagan, I.; Magnini, B.; d&apos;Alché-Buc, F. (Eds.) Machine Learning Challenges. Lecture Notes in Computer Science , Vol. 3944, pp. 177-190, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Ido Dagan</author>
<author>Marc Dymetman</author>
<author>Idan Szpektor</author>
</authors>
<title>Source-language entailment modeling for translating unknown terms.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL.</booktitle>
<pages>791--799</pages>
<location>Stroudsburg, PA, USA,</location>
<marker>Mirkin, Specia, Cancedda, Dagan, Dymetman, Szpektor, 2009</marker>
<rawString>Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido Dagan, Marc Dymetman, and Idan Szpektor. 2009. Source-language entailment modeling for translating unknown terms. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL. Stroudsburg, PA, USA, 791-799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilker Aziz</author>
<author>Marc Dymetmany</author>
<author>Shachar Mirkin</author>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Ido Dagan</author>
</authors>
<title>Learning an Expert from Human Annotations in Statistical Machine Translation: the Case of Out-of-VocabularyWords. In:</title>
<date>2010</date>
<booktitle>Proceedings of the 14th annual meeting of the European Association for Machine Translation (EAMT), Saint-Rapha,</booktitle>
<marker>Aziz, Dymetmany, Mirkin, Specia, Cancedda, Dagan, 2010</marker>
<rawString>Wilker Aziz and Marc Dymetmany and Shachar Mirkin and Lucia Specia and Nicola Cancedda and Ido Dagan. 2010. Learning an Expert from Human Annotations in Statistical Machine Translation: the Case of Out-of-VocabularyWords. In: Proceedings of the 14th annual meeting of the European Association for Machine Translation (EAMT), Saint-Rapha, France.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel Dahlmeier</author>
<author>Chang Liu</author>
<author>Hwee Ng</author>
</authors>
<title>Tou. 2011.TESLA at WMT 2011: Translation Evaluation and Tunable Metric.In:</title>
<booktitle>Proceedings of the Sixth Workshop on Statistical Machine Translation. ACL,</booktitle>
<pages>78--84</pages>
<location>Edinburgh, Scotland.</location>
<marker>Dahlmeier, Liu, Ng, </marker>
<rawString>Dahlmeier, Daniel and Liu, Chang and Ng, Hwee Tou. 2011.TESLA at WMT 2011: Translation Evaluation and Tunable Metric.In: Proceedings of the Sixth Workshop on Statistical Machine Translation. ACL, pages 78-84, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>D Cer</author>
<author>M Galley</author>
<author>D Jurafsky</author>
<author>C Manning</author>
</authors>
<title>Measuring Machine Translation Quality as Semantic Equivalence: A Metric Based on Entailment Features.</title>
<date>2009</date>
<journal>Journal of MT</journal>
<volume>23</volume>
<issue>2</issue>
<pages>181--193</pages>
<marker>Pado, Cer, Galley, Jurafsky, Manning, 2009</marker>
<rawString>S. Pado, D. Cer, M. Galley, D. Jurafsky and C. Manning. 2009. Measuring Machine Translation Quality as Semantic Equivalence: A Metric Based on Entailment Features. Journal of MT 23(2-3), 181-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>M Galley</author>
<author>D Jurafsky</author>
<author>C Manning</author>
</authors>
<title>Robust Machine Translation Evaluation with Entailment Features.</title>
<date>2009</date>
<booktitle>Proceedings of ACL</booktitle>
<marker>Pado, Galley, Jurafsky, Manning, 2009</marker>
<rawString>S. Pado, M. Galley, D. Jurafsky and C. Manning. 2009a. Robust Machine Translation Evaluation with Entailment Features. Proceedings of ACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Luisa Bentivogli</author>
<author>Dagan Ido</author>
<author>Dang Hoa</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>Magnini Bernardo.2009.The Fifth PASCAL RTE Challenge. In:</title>
<booktitle>Proceedings of the Text Analysis Conference.</booktitle>
<marker>Bentivogli, Ido, Hoa, Giampiccolo, </marker>
<rawString>Bentivogli, Luisa, Dagan Ido, Dang Hoa, Giampiccolo, Danilo, Magnini Bernardo.2009.The Fifth PASCAL RTE Challenge. In: Proceedings of the Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database,</title>
<date>1998</date>
<volume>1</volume>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4655" citStr="Fellbaum, 1998" startWordPosition="754" endWordPosition="755"> 5 (total semantic equivalence) to 0 (no relation). Thus, STS mainly differs from TE in that the classification is graded instead of binary. In this manner, STS is filling the gap between several tasks. 3 System architecture Sagan is a RTE system (Castillo and Cardenas, 2010) which has taken part of several challenges, including the Textual Analysis Conference 2009 and TAC 2010, and the Semantic Textual Similarity and Cross Lingual Textual Entailment for content synchronization as part of the Semeval 2012. The system is based on a machine learning approach and it utilizes eight WordNet-based (Fellbaum, 1998) similarity measures, as explained in (Castillo, 2011), with the purpose of obtaining the maximum similarity between two WordNet concepts. A concept is a cluster of synonymous terms that is called a synset in WordNet. These text-to-text similarity measures are based on the following word-to-word similarity metrics: (Resnik, 1995), (Lin, 1997), (Jiang and Conrath, 1997), (Pirrò and Seco, 2008), (Wu &amp; Palmer, 1994), Path Metric, (Leacock &amp; Chodorow, 1998), and a semantic similarity to sentence level named SemSim (Castillo and Cardenas,2010). Fig.1. System architecture The system construct a mode</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum C. 1998. WordNet: An Electronic Lexical Database, volume 1. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Castillo Julio</author>
</authors>
<title>A WordNet-based semantic approach to textual entailment and cross-lingual textual entailment.</title>
<date>2011</date>
<journal>International Journal of Machine Learning and Cybernetics - Springer,</journal>
<volume>2</volume>
<marker>Julio, 2011</marker>
<rawString>Castillo Julio. 2011. A WordNet-based semantic approach to textual entailment and cross-lingual textual entailment. International Journal of Machine Learning and Cybernetics - Springer, Volume 2, Number 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Castillo Julio</author>
<author>Cardenas Marina</author>
</authors>
<title>Using sentence semantic similarity based onWordNet in recognizing textual entailment. Iberamia</title>
<date>2010</date>
<booktitle>In LNCS, vol 6433.</booktitle>
<pages>366--375</pages>
<publisher>Springer,</publisher>
<location>Heidelberg,</location>
<marker>Julio, Marina, 2010</marker>
<rawString>Castillo Julio and Cardenas Marina. 2010. Using sentence semantic similarity based onWordNet in recognizing textual entailment. Iberamia 2010. In LNCS, vol 6433. Springer, Heidelberg, pp 366–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Castillo Julio</author>
</authors>
<title>A semantic oriented approach to textual entailment using WordNet-based measures. MICAI</title>
<date>2010</date>
<volume>LNCS, vol</volume>
<pages>6437</pages>
<publisher>Springer,</publisher>
<location>Heidelberg,</location>
<marker>Julio, 2010</marker>
<rawString>Castillo Julio. 2010. A semantic oriented approach to textual entailment using WordNet-based measures. MICAI 2010. LNCS, vol 6437. Springer, Heidelberg, pp 44–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Castillo Julio</author>
</authors>
<title>Using machine translation systems to expand a corpus in textual entailment. In:</title>
<date>2010</date>
<booktitle>Proceedings of the Icetal 2010. LNCS,</booktitle>
<volume>vol</volume>
<pages>6233--97</pages>
<marker>Julio, 2010</marker>
<rawString>Castillo Julio. 2010. Using machine translation systems to expand a corpus in textual entailment. In: Proceedings of the Icetal 2010. LNCS, vol 6233, pp 97–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Information content to evaluate semantic similarity in a taxonomy. In:</title>
<date>1995</date>
<booktitle>Proceedings of IJCAI</booktitle>
<pages>448--453</pages>
<contexts>
<context position="4986" citStr="Resnik, 1995" startWordPosition="802" endWordPosition="803">e Textual Analysis Conference 2009 and TAC 2010, and the Semantic Textual Similarity and Cross Lingual Textual Entailment for content synchronization as part of the Semeval 2012. The system is based on a machine learning approach and it utilizes eight WordNet-based (Fellbaum, 1998) similarity measures, as explained in (Castillo, 2011), with the purpose of obtaining the maximum similarity between two WordNet concepts. A concept is a cluster of synonymous terms that is called a synset in WordNet. These text-to-text similarity measures are based on the following word-to-word similarity metrics: (Resnik, 1995), (Lin, 1997), (Jiang and Conrath, 1997), (Pirrò and Seco, 2008), (Wu &amp; Palmer, 1994), Path Metric, (Leacock &amp; Chodorow, 1998), and a semantic similarity to sentence level named SemSim (Castillo and Cardenas,2010). Fig.1. System architecture The system construct a model of the semantic similarity of two texts (T,H) as a function of the semantic similarity of the constituent words of both phrases. In order to reach this objective, we used a text to text similarity measure which is based on word to word similarity. Thus, we expect that combining word to word similarity metrics to text level woul</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Resnik P. 1995. Information content to evaluate semantic similarity in a taxonomy. In: Proceedings of IJCAI 1995, pp 448–453 907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Castillo Julio</author>
<author>Cardenas Marina</author>
</authors>
<title>An Approach to Cross-Lingual Textual Entailment using Online Machine Translation Systems.</title>
<date>2011</date>
<journal>Polibits Journal. Vol</journal>
<volume>44</volume>
<marker>Julio, Marina, 2011</marker>
<rawString>Castillo Julio, Cardenas Marina. 2011. An Approach to Cross-Lingual Textual Entailment using Online Machine Translation Systems. Polibits Journal. Vol 44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Castillo Julio</author>
<author>Estrella Paula</author>
</authors>
<date>2012</date>
<booktitle>Semantic Textual Similarity for MT evaluation. NAACL 2012 Seventh Workshop on Statistical Machine Translation. WMT 2012,</booktitle>
<location>Montreal, Canada.</location>
<marker>Julio, Paula, 2012</marker>
<rawString>Castillo Julio and Estrella Paula. 2012. Semantic Textual Similarity for MT evaluation. NAACL 2012 Seventh Workshop on Statistical Machine Translation. WMT 2012, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity. In:</title>
<date>1997</date>
<booktitle>Proceedings of Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<contexts>
<context position="4999" citStr="Lin, 1997" startWordPosition="804" endWordPosition="805">is Conference 2009 and TAC 2010, and the Semantic Textual Similarity and Cross Lingual Textual Entailment for content synchronization as part of the Semeval 2012. The system is based on a machine learning approach and it utilizes eight WordNet-based (Fellbaum, 1998) similarity measures, as explained in (Castillo, 2011), with the purpose of obtaining the maximum similarity between two WordNet concepts. A concept is a cluster of synonymous terms that is called a synset in WordNet. These text-to-text similarity measures are based on the following word-to-word similarity metrics: (Resnik, 1995), (Lin, 1997), (Jiang and Conrath, 1997), (Pirrò and Seco, 2008), (Wu &amp; Palmer, 1994), Path Metric, (Leacock &amp; Chodorow, 1998), and a semantic similarity to sentence level named SemSim (Castillo and Cardenas,2010). Fig.1. System architecture The system construct a model of the semantic similarity of two texts (T,H) as a function of the semantic similarity of the constituent words of both phrases. In order to reach this objective, we used a text to text similarity measure which is based on word to word similarity. Thus, we expect that combining word to word similarity metrics to text level would be a good i</context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>Lin D. 1997. An information-theoretic definition of similarity. In: Proceedings of Conference on Machine Learning, pp 296–304 909.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Jiang</author>
</authors>
<title>Conrath D.1997. Semantic similarity based on corpus statistics and lexical taxonomy. In:</title>
<booktitle>Proceedings of theROCLINGX</booktitle>
<pages>911</pages>
<marker>Jiang, </marker>
<rawString>Jiang J, Conrath D.1997. Semantic similarity based on corpus statistics and lexical taxonomy. In: Proceedings of theROCLINGX 911</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Pirro</author>
<author>N Seco</author>
</authors>
<title>Design, implementation and evaluation of a new similarity metric combining feature and intrinsic information content. In:</title>
<date>2008</date>
<booktitle>ODBASE 2008, Springer LNCS.</booktitle>
<marker>Pirro, Seco, 2008</marker>
<rawString>Pirro G., Seco N. 2008. Design, implementation and evaluation of a new similarity metric combining feature and intrinsic information content. In: ODBASE 2008, Springer LNCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wu</author>
<author>M Palmer</author>
</authors>
<title>Verb semantics and lexical selection. In:</title>
<date>1994</date>
<booktitle>Proceedings of the 32nd ACL 916.</booktitle>
<contexts>
<context position="5071" citStr="Wu &amp; Palmer, 1994" startWordPosition="814" endWordPosition="817">rity and Cross Lingual Textual Entailment for content synchronization as part of the Semeval 2012. The system is based on a machine learning approach and it utilizes eight WordNet-based (Fellbaum, 1998) similarity measures, as explained in (Castillo, 2011), with the purpose of obtaining the maximum similarity between two WordNet concepts. A concept is a cluster of synonymous terms that is called a synset in WordNet. These text-to-text similarity measures are based on the following word-to-word similarity metrics: (Resnik, 1995), (Lin, 1997), (Jiang and Conrath, 1997), (Pirrò and Seco, 2008), (Wu &amp; Palmer, 1994), Path Metric, (Leacock &amp; Chodorow, 1998), and a semantic similarity to sentence level named SemSim (Castillo and Cardenas,2010). Fig.1. System architecture The system construct a model of the semantic similarity of two texts (T,H) as a function of the semantic similarity of the constituent words of both phrases. In order to reach this objective, we used a text to text similarity measure which is based on word to word similarity. Thus, we expect that combining word to word similarity metrics to text level would be a good indicator of text to text similarity. Additional information about how to</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Wu Z, Palmer M. 1994. Verb semantics and lexical selection. In: Proceedings of the 32nd ACL 916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification.</title>
<date>1998</date>
<pages>265--283</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="5112" citStr="Leacock &amp; Chodorow, 1998" startWordPosition="820" endWordPosition="823">ailment for content synchronization as part of the Semeval 2012. The system is based on a machine learning approach and it utilizes eight WordNet-based (Fellbaum, 1998) similarity measures, as explained in (Castillo, 2011), with the purpose of obtaining the maximum similarity between two WordNet concepts. A concept is a cluster of synonymous terms that is called a synset in WordNet. These text-to-text similarity measures are based on the following word-to-word similarity metrics: (Resnik, 1995), (Lin, 1997), (Jiang and Conrath, 1997), (Pirrò and Seco, 2008), (Wu &amp; Palmer, 1994), Path Metric, (Leacock &amp; Chodorow, 1998), and a semantic similarity to sentence level named SemSim (Castillo and Cardenas,2010). Fig.1. System architecture The system construct a model of the semantic similarity of two texts (T,H) as a function of the semantic similarity of the constituent words of both phrases. In order to reach this objective, we used a text to text similarity measure which is based on word to word similarity. Thus, we expect that combining word to word similarity metrics to text level would be a good indicator of text to text similarity. Additional information about how to produce feature vectors as well as each </context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Leacock C, Chodorow M. 1998. Combining local context and WordNet similarity for word sense identification. MIT Press, pp 265–283 919</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms.</title>
<date>1998</date>
<pages>305--332</pages>
<publisher>MIT Press,</publisher>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Hirst G, St-Onge D . 1998. Lexical chains as representations of context for the detection and correction of malapropisms. MIT Press, pp 305–332 922</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using WordNet. In: Proceeding of CICLING-02</title>
<date>2002</date>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Banerjee S, Pedersen T. 2002. An adapted lesk algorithm for word sense disambiguation using WordNet. In: Proceeding of CICLING-02</rawString>
</citation>
<citation valid="false">
<authors>
<author>William B Dolan</author>
<author>Chris Brockett 2005</author>
</authors>
<title>Automatically Constructing a Corpus of Sentential Paraphrases.</title>
<booktitle>Third International Workshop on Paraphrasing (IWP2005). Asia Federation of Natural Language Processing.</booktitle>
<marker>Dolan, 2005, </marker>
<rawString>William B. Dolan and Chris Brockett.2005. Automatically Constructing a Corpus of Sentential Paraphrases. Third International Workshop on Paraphrasing (IWP2005). Asia Federation of Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>