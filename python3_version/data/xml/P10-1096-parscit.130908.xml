<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009044">
<title confidence="0.998915">
Bayesian Synchronous Tree-Substitution Grammar Induction
and its Application to Sentence Compression
</title>
<author confidence="0.975369">
Elif Yamangil and Stuart M. Shieber
</author>
<affiliation confidence="0.980073">
Harvard University
</affiliation>
<address confidence="0.708678">
Cambridge, Massachusetts, USA
</address>
<email confidence="0.999511">
{elif,shieber}@seas.harvard.edu
</email>
<sectionHeader confidence="0.994816" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989318181818">
We describe our experiments with training
algorithms for tree-to-tree synchronous
tree-substitution grammar (STSG) for
monolingual translation tasks such as
sentence compression and paraphrasing.
These translation tasks are characterized
by the relative ability to commit to parallel
parse trees and availability of word align-
ments, yet the unavailability of large-scale
data, calling for a Bayesian tree-to-tree
formalism. We formalize nonparametric
Bayesian STSG with epsilon alignment in
full generality, and provide a Gibbs sam-
pling algorithm for posterior inference tai-
lored to the task of extractive sentence
compression. We achieve improvements
against a number of baselines, including
expectation maximization and variational
Bayes training, illustrating the merits of
nonparametric inference over the space of
grammars as opposed to sparse parametric
inference with a fixed grammar.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965553191489">
Given an aligned corpus of tree pairs, we might
want to learn a mapping between the paired trees.
Such induction of tree mappings has application
in a variety of natural-language-processing tasks
including machine translation, paraphrase, and
sentence compression. The induced tree map-
pings can be expressed by synchronous grammars.
Where the tree pairs are isomorphic, synchronous
context-free grammars (SCFG) may suffice, but in
general, non-isomorphism can make the problem
of rule extraction difficult (Galley and McKeown,
2007). More expressive formalisms such as syn-
chronous tree-substitution (Eisner, 2003) or tree-
adjoining grammars may better capture the pair-
ings.
In this work, we explore techniques for inducing
synchronous tree-substitution grammars (STSG)
using as a testbed application extractive sentence
compression. Learning an STSG from aligned
trees is tantamount to determining a segmentation
of the trees into elementary trees of the grammar
along with an alignment of the elementary trees
(see Figure 1 for an example of such a segmenta-
tion), followed by estimation of the weights for the
extracted tree pairs.1 These elementary tree pairs
serve as the rules of the extracted grammar. For
SCFG, segmentation is trivial — each parent with
its immediate children is an elementary tree — but
the formalism then restricts us to deriving isomor-
phic tree pairs. STSG is much more expressive,
especially if we allow some elementary trees on
the source or target side to be unsynchronized, so
that insertions and deletions can be modeled, but
the segmentation and alignment problems become
nontrivial.
Previous approaches to this problem have
treated the two steps — grammar extraction and
weight estimation — with a variety of methods.
One approach is to use word alignments (where
these can be reliably estimated, as in our testbed
application) to align subtrees and extract rules
(Och and Ney, 2004; Galley et al., 2004) but
this leaves open the question of finding the right
level of generality of the rules — how deep the
rules should be and how much lexicalization they
should involve — necessitating resorting to heuris-
tics such as minimality of rules, and leading to
</bodyText>
<footnote confidence="0.993812333333333">
1Throughout the paper we will use the word STSG to re-
fer to the tree-to-tree version of the formalism, although the
string-to-tree version is also commonly used.
</footnote>
<page confidence="0.875158">
937
</page>
<note confidence="0.9468235">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 937–947,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999925023809524">
large grammars. Once a given set of rules is ex-
tracted, weights can be imputed using a discrimi-
native approach to maximize the (joint or condi-
tional) likelihood or the classification margin in
the training data (taking or not taking into account
the derivational ambiguity). This option leverages
a large amount of manual domain knowledge en-
gineering and is not in general amenable to latent
variable problems.
A simpler alternative to this two step approach
is to use a generative model of synchronous
derivation and simultaneously segment and weight
the elementary tree pairs to maximize the prob-
ability of the training data under that model; the
simplest exemplar of this approach uses expecta-
tion maximization (EM) (Dempster et al., 1977).
This approach has two frailties. First, EM search
over the space of all possible rules is computation-
ally impractical. Second, even if such a search
were practical, the method is degenerate, pushing
the probability mass towards larger rules in order
to better approximate the empirical distribution of
the data (Goldwater et al., 2006; DeNero et al.,
2006). Indeed, the optimal grammar would be one
in which each tree pair in the training data is its
own rule. Therefore, proposals for using EM for
this task start with a precomputed subset of rules,
and with EM used just to assign weights within
this grammar. In summary, previous methods suf-
fer from problems of narrowness of search, having
to restrict the space of possible rules, and overfit-
ting in preferring overly specific grammars.
We pursue the use of hierarchical probabilistic
models incorporating sparse priors to simultane-
ously solve both the narrowness and overfitting
problems. Such models have been used as gener-
ative solutions to several other segmentation prob-
lems, ranging from word segmentation (Goldwa-
ter et al., 2006), to parsing (Cohn et al., 2009; Post
and Gildea, 2009) and machine translation (DeN-
ero et al., 2008; Cohn and Blunsom, 2009; Liu
and Gildea, 2009). Segmentation is achieved by
introducing a prior bias towards grammars that are
compact representations of the data, namely by en-
forcing simplicity and sparsity: preferring simple
rules (smaller segments) unless the use of a com-
plex rule is evidenced by the data (through repeti-
tion), and thus mitigating the overfitting problem.
A Dirichlet process (DP) prior is typically used
to achieve this interplay. Interestingly, sampling-
based nonparametric inference further allows the
possibility of searching over the infinite space of
grammars (and, in machine translation, possible
word alignments), thus side-stepping the narrow-
ness problem outlined above as well.
In this work, we use an extension of the afore-
mentioned models of generative segmentation for
STSG induction, and describe an algorithm for
posterior inference under this model that is tai-
lored to the task of extractive sentence compres-
sion. This task is characterized by the availabil-
ity of word alignments, providing a clean testbed
for investigating the effects of grammar extraction.
We achieve substantial improvements against a
number of baselines including EM, support vector
machine (SVM) based discriminative training, and
variational Bayes (VB). By comparing our method
to a range of other methods that are subject dif-
ferentially to the two problems, we can show that
both play an important role in performance limi-
tations, and that our method helps address both as
well. Our results are thus not only encouraging for
grammar estimation using sparse priors but also il-
lustrate the merits of nonparametric inference over
the space of grammars as opposed to sparse para-
metric inference with a fixed grammar.
In the following, we define the task of extrac-
tive sentence compression and the Bayesian STSG
model, and algorithms we used for inference and
prediction. We then describe the experiments in
extractive sentence compression and present our
results in contrast with alternative algorithms. We
conclude by giving examples of compression pat-
terns learned by the Bayesian method.
</bodyText>
<sectionHeader confidence="0.921432" genericHeader="method">
2 Sentence compression
</sectionHeader>
<bodyText confidence="0.9997449">
Sentence compression is the task of summarizing a
sentence while retaining most of the informational
content and remaining grammatical (Jing, 2000).
In extractive sentence compression, which we fo-
cus on in this paper, an order-preserving subset of
the words in the sentence are selected to form the
summary, that is, we summarize by deleting words
(Knight and Marcu, 2002). An example sentence
pair, which we use as a running example, is the
following:
</bodyText>
<listItem confidence="0.9962998">
• Like FaceLift, much of ATM’s screen perfor-
mance depends on the underlying applica-
tion.
• ATM’s screen performance depends on the
underlying application.
</listItem>
<page confidence="0.99735">
938
</page>
<figureCaption confidence="0.999922">
Figure 1: A portion of an STSG derivation of the example sentence and its extractive compression.
</figureCaption>
<bodyText confidence="0.999856428571429">
where the underlined words were deleted. In su-
pervised sentence compression, the goal is to gen-
eralize from a parallel training corpus of sentences
(source) and their compressions (target) to unseen
sentences in a test set to predict their compres-
sions. An unsupervised setup also exists; meth-
ods for the unsupervised problem typically rely
on language models and linguistic/discourse con-
straints (Clarke and Lapata, 2006a; Turner and
Charniak, 2005). Because these methods rely on
dynamic programming to efficiently consider hy-
potheses over the space of all possible compres-
sions of a sentence, they may be harder to extend
to general paraphrasing.
</bodyText>
<sectionHeader confidence="0.998055" genericHeader="method">
3 The STSG Model
</sectionHeader>
<bodyText confidence="0.999919833333333">
Synchronous tree-substitution grammar is a for-
malism for synchronously generating a pair of
non-isomorphic source and target trees (Eisner,
2003). Every grammar rule is a pair of elemen-
tary trees aligned at the leaf level at their frontier
nodes, which we will denote using the form
</bodyText>
<equation confidence="0.906123">
cs/ct — es/et, -y
</equation>
<bodyText confidence="0.997089">
(indices s for source, t for target) where cs, ct are
root nonterminals of the elementary trees es, et re-
spectively and -y is a 1-to-1 correspondence be-
tween the frontier nodes in es and et. For example,
the rule
</bodyText>
<equation confidence="0.994026">
S / S --+ (S (PP (IN Like) NP[,]) NP[1] VP[2]) /
(S NP[1] VP[2])
</equation>
<bodyText confidence="0.99985775">
can be used to delete a subtree rooted at PP. We
use square bracketed indices to represent the align-
ment -y of frontier nodes — NP[1] aligns with
NP[1], VP[2] aligns with VP[2], NP[,] aligns with
the special symbol E denoting a deletion from the
source tree. Symmetrically E-aligned target nodes
are used to represent insertions into the target tree.
Similarly, the rule
</bodyText>
<equation confidence="0.972441">
NP / E —* (NP (NN FaceLift)) / E
</equation>
<bodyText confidence="0.999200608695652">
can be used to continue deriving the deleted sub-
tree. See Figure 1 for an example of how an STSG
with these rules would operate in synchronously
generating our example sentence pair.
STSG is a convenient choice of formalism for
a number of reasons. First, it eliminates the iso-
morphism and strong independence assumptions
of SCFGs. Second, the ability to have rules deeper
than one level provides a principled way of model-
ing lexicalization, whose importance has been em-
phasized (Galley and McKeown, 2007; Yamangil
and Nelken, 2008). Third, we may have our STSG
operate on trees instead of sentences, which allows
for efficient parsing algorithms, as well as provid-
ing syntactic analyses for our predictions, which is
desirable for automatic evaluation purposes.
A straightforward extension of the popular EM
algorithm for probabilistic context free grammars
(PCFG), the inside-outside algorithm (Lari and
Young, 1990), can be used to estimate the rule
weights of a given unweighted STSG based on a
corpus of parallel parse trees t = t1, ... , tN where
tn = tn,s/tn,t for n = 1, ... , N. Similarly, an
</bodyText>
<page confidence="0.996931">
939
</page>
<figureCaption confidence="0.8286305">
Figure 2: Gibbs sampling updates. We illustrate a sampler move to align/unalign a source node with a
target node (top row in blue), and split/merge a deletion rule via aligning with E (bottom row in red).
</figureCaption>
<bodyText confidence="0.9999105">
extension of the Viterbi algorithm is available for
finding the maximum probability derivation, use-
ful for predicting the target analysis tN+1,t for a
test instance tN+1,s. (Eisner, 2003) However, as
noted earlier, EM is subject to the narrowness and
overfitting problems.
</bodyText>
<subsectionHeader confidence="0.999033">
3.1 The Bayesian generative process
</subsectionHeader>
<bodyText confidence="0.999077181818182">
Both of these issues can be addressed by taking
a nonparametric Bayesian approach, namely, as-
suming that the elementary tree pairs are sampled
from an independent collection of Dirichlet pro-
cess (DP) priors. We describe such a process for
sampling a corpus of tree pairs t.
For all pairs of root labels c = cs/ct that we
consider, where up to one of cs or ct can be E (e.g.,
S / S, NP / E), we sample a sparse discrete distribu-
tion Gc over infinitely many elementary tree pairs
e = es/et sharing the common root c from a DP
</bodyText>
<equation confidence="0.613349">
Gc ∼ DP(αc, P0(·  |c)) (1)
</equation>
<bodyText confidence="0.990333710526316">
where the DP has the concentration parameter αc
controlling the sparsity of Gc, and the base dis-
tribution P0(·  |c) is a distribution over novel el-
ementary tree pairs that we describe more fully
shortly.
We then sample a sequence of elementary tree
pairs to serve as a derivation for each observed de-
rived tree pair. For each n = 1, ... , N, we sam-
ple elementary tree pairs en = en,1, ... , en,dn in
a derivation sequence (where dn is the number of
rules used in the derivation), consulting Gc when-
ever an elementary tree pair with root c is to be
sampled.
iid Gc, for all e whose root label is c
e ∼
Given the derivation sequence en, a tree pair tn is
determined, that is,
en,1, ... , en,dn derives tn
otherwise.
(2)
The hyperparameters αc can be incorporated
into the generative model as random variables;
however, we opt to fix these at various constants
to investigate different levels of sparsity.
For the base distribution P0(·  |c) there are a
variety of choices; we used the following simple
scenario. (We take c = cs/ct.)
Synchronous rules For the case where neither cs
nor ct are the special symbol E, the base dis-
tribution first generates es and et indepen-
dently, and then samples an alignment be-
tween the frontier nodes. Given a nontermi-
nal, an elementary tree is generated by first
making a decision to expand the nontermi-
nal (with probability βc) or to leave it as a
frontier node (1 − βc). If the decision to ex-
pand was made, we sample an appropriate
rule from a PCFG which we estimate ahead
</bodyText>
<equation confidence="0.9535365">
� 1
p(tn  |en) = 0
</equation>
<page confidence="0.953087">
940
</page>
<bodyText confidence="0.999989307692308">
of time from the training corpus. We expand
the nonterminal using this rule, and then re-
peat the same procedure for every child gen-
erated that is a nonterminal until there are no
generated nonterminal children left. This is
done independently for both es and et. Fi-
nally, we sample an alignment between the
frontier nodes uniformly at random out of all
possible alingments.
Deletion/insertion rules If ct = c, that is, we
have a deletion rule, we need to generate
e = es/E. (The insertion rule case is symmet-
ric.) The base distribution generates es using
the same process described for synchronous
rules above. Then with probability 1 we align
all frontier nodes in es with c. In essence,
this process generates TSG rules, rather than
STSG rules, which are used to cover deleted
(or inserted) subtrees.
This simple base distribution does nothing to
enforce an alignment between the internal nodes
of es and et. One may come up with more sophis-
ticated base distributions. However the main point
of the base distribution is to encode a control-
lable preference towards simpler rules; we there-
fore make the simplest possible assumption.
</bodyText>
<subsectionHeader confidence="0.994964">
3.2 Posterior inference via Gibbs sampling
</subsectionHeader>
<bodyText confidence="0.997550333333333">
Assuming fixed hyperparameters α = {αe} and
,3 = {Qe}, our inference problem is to find the
posterior distribution of the derivation sequences
</bodyText>
<equation confidence="0.933856">
e = e1, ... , eN given the observations t =
t1, ... , tN. Applying Bayes’ rule, we have
p(e  |t) a p(t  |e)p(e) (3)
</equation>
<bodyText confidence="0.993192222222222">
wherep(t  |e) is a 0/1 distribution (2) which does
not depend on Ge, and p(e) can be obtained by
collapsing Ge for all c.
Consider repeatedly generating elementary tree
pairs e1, ... , ei, all with the same root c, iid from
Ge. Integrating over Ge, the ei become depen-
dent. The conditional prior of the i-th elementary
tree pair given previously generated ones e&lt;i =
e1, ... , ei_1 is given by
</bodyText>
<equation confidence="0.995562">
ne� + α�P0(ei  |c)
p(ei  |e&lt;i) = (4)
i − 1 + αe
</equation>
<bodyText confidence="0.999098912280702">
where nez denotes the number of times ei occurs
in e&lt;i. Since the collapsed model is exchangeable
in the ei, this formula forms the backbone of the
inference procedure that we describe next. It also
makes clear DP’s inductive bias to reuse elemen-
tary tree pairs.
We use Gibbs sampling (Geman and Geman,
1984), a Markov chain Monte Carlo (MCMC)
method, to sample from the posterior (3). A
derivation e of the corpus t is completely specified
by an alignment between the source nodes and the
corresponding target nodes (as well as c on either
side), which we take to be the state of the sampler.
We start at a random derivation of the corpus, and
at every iteration resample a derivation by amend-
ing the current one through local changes made
at the node level, in the style of Goldwater et al.
(2006).
Our sampling updates are extensions of those
used by Cohn and Blunsom (2009) in MT, but are
tailored to our task of extractive sentence compres-
sion. In our task, no target node can align with
c (which would indicate a subtree insertion), and
barring unary branches no source node i can align
with two different target nodes j and j&apos; at the same
time (indicating a tree expansion). Rather, the
configurations of interest are those in which only
source nodes i can align with c, and two source
nodes i and i&apos; can align with the same target node
j. Thus, the alignments of interest are not arbitrary
relations, but (partial) functions from nodes in es
to nodes in et or c. We therefore sample in the
direction from source to target. In particular, we
visit every tree pair and each of its source nodes i,
and update its alignment by selecting between and
within two choices: (a) unaligned, (b) aligned with
some target node j or c. The number of possibil-
ities j in (b) is significantly limited, firstly by the
word alignment (for instance, a source node dom-
inating a deleted subspan cannot be aligned with
a target node), and secondly by the current align-
ment of other nearby aligned source nodes. (See
Cohn and Blunsom (2009) for details of matching
spans under tree constraints.)2
2One reviewer was concerned that since we explicitly dis-
allow insertion rules in our sampling procedure, our model
that generates such rules wastes probability mass and is there-
fore “deficient”. However, we regard sampling as a separate
step from the data generation process, in which we can for-
mulate more effective algorithms by using our domain knowl-
edge that our data set was created by annotators who were
instructed to delete words only. Also, disallowing insertion
rules in the base distribution unnecessarily complicates the
definition of the model, whereas it is straightforward to de-
fine the joint distribution of all (potentially useful) rules and
then use domain knowledge to constrain the support of that
distribution during inference, as we do here. In fact, it is pos-
</bodyText>
<page confidence="0.992424">
941
</page>
<bodyText confidence="0.999693875">
More formally, let eM be the elementary tree
pair rooted at the closest aligned ancestor i&apos; of
node i when it is unaligned; and let eA and eB
be the elementary tree pairs rooted at i&apos; and i re-
spectively when i is aligned with some target node
j or c. Then, by exchangeability of the elementary
trees sharing the same root label, and using (4), we
have
</bodyText>
<equation confidence="0.999882333333333">
neM + αcMP0(eM  |cM)
p(unalign) = (5)
ncM + αcM
neA + αcAP0(eA  |cA)
p(align with j) = (6)
ncA + αcA
neB + αcBP0(eB  |cB)
X (7)
ncB + αcB
</equation>
<bodyText confidence="0.999824523809524">
where the counts ne·, nc· are with respect to the
current derivation of the rest of the corpus; except
for neB, ncB we also make sure to account for hav-
ing generated eA. See Figure 2 for an illustration
of the sampling updates.
It is important to note that the sampler described
can move from any derivation to any other deriva-
tion with positive probability (if only, for example,
by virtue of fully merging and then resegment-
ing), which guarantees convergence to the poste-
rior (3). However some of these transition prob-
abilities can be extremely small due to passing
through low probability states with large elemen-
tary trees; in turn, the sampling procedure is prone
to local modes. In order to counteract this and to
improve mixing we used simulated annealing. The
probability mass function (5-7) was raised to the
power 1/T with T dropping linearly from T = 5
to T = 0. Furthermore, using a final tempera-
ture of zero, we recover a maximum a posteriori
(MAP) estimate which we denote eMAP.
</bodyText>
<subsectionHeader confidence="0.998583">
3.3 Prediction
</subsectionHeader>
<bodyText confidence="0.992611">
We discuss the problem of predicting a target tree
tN+1,t that corresponds to a source tree tN+1,s
unseen in the observed corpus t. The maximum
probability tree (MPT) can be found by consid-
ering all possible ways to derive it. However a
much simpler alternative is to choose the target
tree implied by the maximum probability deriva-
sible to prove that our approach is equivalent up to a rescaling
of the concentration parameters. Since we fit these parame-
ters to the data, our approach is equivalent.
</bodyText>
<equation confidence="0.94065725">
tion (MPD), which we define as
e* = argmax p(e  |ts, t)
e
p(e  |ts, e)p(e  |t)
</equation>
<bodyText confidence="0.999871153846154">
where e denotes a derivation for t = ts/tt. (We
suppress the N + 1 subscripts for brevity.) We
approximate this objective first by substituting
δeMAP(e) for p(e  |t) and secondly using a finite
STSG model for the infinite p(e  |ts, eMAP), which
we obtain simply by normalizing the rule counts in
eMAP. We use dynamic programming for parsing
under this finite model (Eisner, 2003).3
Unfortunately, this approach does not ensure
that the test instances are parsable, since ts may
include unseen structure or novel words. A work-
around is to include all zero-count context free
copy rules such as
</bodyText>
<equation confidence="0.9996265">
NP / NP --+ (NP NP[1] PP[�]) / (NP NP[1] PP[�])
NP / E � (NP NP[,] PP[,]) / E
</equation>
<bodyText confidence="0.999409666666667">
in order to smooth our finite model. We used
Laplace smoothing (adding 1 to all counts) as it
gave us interpretable results.
</bodyText>
<sectionHeader confidence="0.998544" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.993462928571429">
We compared the Gibbs sampling compressor
(GS) against a version of maximum a posteriori
EM (with Dirichlet parameter greater than 1) and
a discriminative STSG based on SVM training
(Cohn and Lapata, 2008) (SVM). EM is a natural
benchmark, while SVM is also appropriate since
it can be taken as the state of the art for our task.4
We used a publicly available extractive sen-
tence compression corpus: the Broadcast News
compressions corpus (BNC) of Clarke and Lap-
ata (2006a). This corpus consists of 1370 sentence
pairs that were manually created from transcribed
Broadcast News stories. We split the pairs into
training, development, and testing sets of 1000,
3We experimented with MPT using Monte Carlo integra-
tion over possible derivations; the results were not signifi-
cantly different from those using MPD.
4The comparison system described by Cohn and Lapata
(2008) attempts to solve a more general problem than ours,
abstractive sentence compression. However, given the nature
of the data that we provided, it can only learn to compress
by deleting words. Since the system is less specialized to the
task, their model requires additional heuristics in decoding
not needed for extractive compression, which might cause a
reduction in performance. Nonetheless, because the compar-
ison system is a generalization of the extractive SVM com-
pressor of Cohn and Lapata (2007), we do not expect that the
results would differ qualitatively.
</bodyText>
<equation confidence="0.6086205">
= argmax
e
�
e
</equation>
<page confidence="0.985614">
942
</page>
<table confidence="0.9978788">
SVM EM GS
Precision 55.60 58.80 58.94
Recall 53.37 56.58 64.59
Relational F1 54.46 57.67 61.64
Compression rate 59.72 64.11 65.52
</table>
<tableCaption confidence="0.962773">
Table 1: Precision, recall, relational F1 and com-
</tableCaption>
<table confidence="0.839604285714286">
pression rate (%) for various systems on the 200-
sentence BNC test set. The compression rate for
the gold standard was 65.67%.
SVM EM GS Gold
Grammar 2.751 2.85* 3.69 4.25
Importance 2.85 2.67* 3.41 3.82
Comp. rate 68.18 64.07 67.97 62.34
</table>
<tableCaption confidence="0.993767">
Table 2: Average grammar and importance scores
</tableCaption>
<bodyText confidence="0.997862170212766">
for various systems on the 20-sentence subsam-
ple. Scores marked with * are significantly dif-
ferent than the corresponding GS score at α &lt; .05
and with † at α &lt; .01 according to post-hoc Tukey
tests. ANOVA was significant at p &lt; .01 both for
grammar and importance.
170, and 200 pairs, respectively. The corpus was
parsed using the Stanford parser (Klein and Man-
ning, 2003).
In our experiments with the publicly available
SVM system we used all except paraphrasal rules
extracted from bilingual corpora (Cohn and Lap-
ata, 2008). The model chosen for testing had pa-
rameter for trade-off between training error and
margin set to C = 0.001, used margin rescaling,
and Hamming distance over bags of tokens with
brevity penalty for loss function. EM used a sub-
set of the rules extracted by SVM, namely all rules
except non-head deleting compression rules, and
was initialized uniformly. Each EM instance was
characterized by two parameters: α, the smooth-
ing parameter for MAP-EM, and δ, the smooth-
ing parameter for augmenting the learned gram-
mar with rules extracted from unseen data (add-
(δ − 1) smoothing was used), both of which were
fit to the development set using grid-search over
(1, 2]. The model chosen for testing was (α, δ) =
(1.0001,1.01).
GS was initialized at a random derivation. We
sampled the alignments of the source nodes in ran-
dom order. The sampler was run for 5000 itera-
tions with annealing. All hyperparameters α,, β,
were held constant at α, β for simplicity and were
fit using grid-search over α E [10−6,106],β E
[10−3,0.5]. The model chosen for testing was
(α, β) = (100, 0.1).
As an automated metric of quality, we compute
F-score based on grammatical relations (relational
F1, or RelF1) (Riezler et al., 2003), by which the
consistency between the set of predicted grammat-
ical relations and those from the gold standard is
measured, which has been shown by Clarke and
Lapata (2006b) to correlate reliably with human
judgments. We also conducted a small human sub-
jective evaluation of the grammaticality and infor-
mativeness of the compressions generated by the
various methods.
</bodyText>
<subsectionHeader confidence="0.997847">
4.1 Automated evaluation
</subsectionHeader>
<bodyText confidence="0.999941764705882">
For all three systems we obtained predictions for
the test set and used the Stanford parser to extract
grammatical relations from predicted trees and the
gold standard. We computed precision, recall,
RelF1 (all based on grammatical relations), and
compression rate (percentage of the words that are
retained), which we report in Table 1. The results
for GS are averages over five independent runs.
EM gives a strong baseline since it already uses
rules that are limited in depth and number of fron-
tier nodes by stipulation, helping with the overfit-
ting we have mentioned, surprisingly outperform-
ing its discriminative counterpart in both precision
and recall (and consequently RelF1). GS however
maintains the same level of precision as EM while
improving recall, bringing an overall improvement
in RelF1.
</bodyText>
<subsectionHeader confidence="0.999506">
4.2 Human evaluation
</subsectionHeader>
<bodyText confidence="0.9998954">
We randomly subsampled our 200-sentence test
set for 20 sentences to be evaluated by human
judges through Amazon Mechanical Turk. We
asked 15 self-reported native English speakers for
their judgments of GS, EM, and SVM output sen-
tences and the gold standard in terms of grammat-
icality (how fluent the compression is) and impor-
tance (how much of the meaning of and impor-
tant information from the original sentence is re-
tained) on a scale of 1 (worst) to 5 (best). We re-
port in Table 2 the average scores. EM and SVM
perform at very similar levels, which we attribute
to using the same set of rules, while GS performs
at a level substantially better than both, and much
closer to human performance in both criteria. The
</bodyText>
<page confidence="0.998443">
943
</page>
<figureCaption confidence="0.9109235">
Figure 3: RelF1, precision, recall plotted against
compression rate for GS, EM, VB.
</figureCaption>
<bodyText confidence="0.990991">
human evaluation indicates that the superiority of
the Bayesian nonparametric method is underap-
preciated by the automated evaluation metric.
</bodyText>
<subsectionHeader confidence="0.991781">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999988125">
The fact that GS performs better than EM can be
attributed to two reasons: (1) GS uses a sparse
prior and selects a compact representation of the
data (grammar sizes ranged from 4K-7K for GS
compared to a grammar of about 35K rules for
EM). (2) GS does not commit to a precomputed
grammar and searches over the space of all gram-
mars to find one that bests represents the corpus.
It is possible to introduce DP-like sparsity in EM
using variational Bayes (VB) training. We exper-
iment with this next in order to understand how
dominant the two factors are. The VB algorithm
requires a simple update to the M-step formulas
for EM where the expected rule counts are normal-
ized, such that instead of updating the rule weight
in the t-th iteration as in the following
</bodyText>
<equation confidence="0.990781666666667">
0t+1
c,e = nc,e + α − 1
nc,. + Kα − K
</equation>
<bodyText confidence="0.999966625">
where nc,e represents the expected count of rule
c —* e, and K is the total number of ways
to rewrite c, we now take into account our
DP(αc, P0(·  |c)) prior in (1), which, when
truncated to a finite grammar, reduces to a
K-dimensional Dirichlet prior with parameter
αcP0(·  |c). Thus in VB we perform a variational
E-step with the subprobabilities given by
</bodyText>
<equation confidence="0.973455">
0t+1= exp (IF(nc,e + αcP0(e  |c)))
c,e exp (IF(nc,. + αc))
</equation>
<bodyText confidence="0.9999947">
where IF denotes the digamma function. (Liu and
Gildea, 2009) (See MacKay (1997) for details.)
Hyperparameters were handled the same way as
for GS.
Instead of selecting a single model on the devel-
opment set, here we provide the whole spectrum of
models and their performances in order to better
understand their comparative behavior. In Figure
3 we plot RelF1 on the test set versus compres-
sion rate and compare GS, EM, and VB (Q = 0.1
fixed, (α, S) ranging in [10−6,106]x(1, 2]). Over-
all, we see that GS maintains roughly the same
level of precision as EM (despite its larger com-
pression rates) while achieving an improvement in
recall, consequently performing at a higher RelF1
level. We note that VB somewhat bridges the gap
between GS and EM, without quite reaching GS
performance. We conclude that the mitigation of
the two factors (narrowness and overfitting) both
contribute to the performance gain of GS.5
</bodyText>
<subsectionHeader confidence="0.997545">
4.4 Example rules learned
</subsectionHeader>
<bodyText confidence="0.989076">
In order to provide some insight into the grammar
extracted by GS, we list in Tables (3) and (4) high
</bodyText>
<footnote confidence="0.9649405">
5We have also experimented with VB with parametric in-
dependent symmetric Dirichlet priors. The results were sim-
ilar to EM with the exception of sparse priors resulting in
smaller grammars and slightly improving performance.
</footnote>
<page confidence="0.988085">
944
</page>
<table confidence="0.998997285714286">
(ROOT (S CC[,] NP[1] VP[2] .[3])) /(ROOT (S NP[1] VP[2] .[3]))
(ROOT (S NP[1] ADVP[,] VP[2] (..))) / (ROOT (S NP[1] VP[2] (. .)))
(ROOT (S ADVP[,] (, ,) NP[1] VP[2] (..))) / (ROOT (S NP[1] VP[2] (. .)))
(ROOT (S PP[,] (, ,) NP[1] VP[2] (..))) / (ROOT (S NP[1] VP[2] (. .)))
(ROOT (S PP[,] ,[,] NP[1] VP[2] .[3])) /(ROOT (S NP[1] VP[2] .[3]))
(ROOT (S NP[,] (VP VBP[,] (SBAR (S NP[1] VP[2]))) .[3])) /(ROOT (S NP[1] VP[2] .[3]))
(ROOT (S ADVP[,] NP[1] (VP MD[2] VP[3]) .[4])) / (ROOT (S NP[1] (VP MD[2] VP[3]) .[4]))
(ROOT (S (SBAR (IN as) S[,]) ,[,] NP[1] VP[2] .[3])) /(ROOT (S NP[1] VP[2] .[3]))
(ROOT (S S[,] (, ,) CC[,] (S NP[1] VP[2]) .[3])) /(ROOT (S NP[1] VP[2] .[3]))
(ROOT (S PP[,] NP[1] VP[2] .[3])) /(ROOT (S NP[1] VP[2] .[3]))
(ROOT (S S[1] (, ,) CC[,] S[2] (. .))) / (ROOT (S NP[1] VP[2] (. .)))
(ROOT (S S[,] ,[,] NP[1] ADVP[2] VP[3] .[4])) / (ROOT (S NP[1] ADVP[2] VP[3] .[4]))
(ROOT (S (NP (NP NNP[,] (POS ’s)) NNP[1] NNP[2]) / (ROOT (S (NP NNP[1] NNP[2])
(VP (VBZ reports)) .[3])) (VP (VBZ reports)) .[3]))
</table>
<tableCaption confidence="0.99121">
Table 3: High probability ROOT / ROOT compression rules from the final state of the sampler.
</tableCaption>
<table confidence="0.880231642857143">
(S NP[1] ADVP[,] VP[2]) / (S NP[1] VP[2])
(S INTJ[,] (, ,) NP[1] VP[2] (..)) / (S NP[1] VP[2] (. .))
(S (INTJ (UH Well)) ,[,] NP[1] VP[2] .[3]) / (S NP[1] VP[2] .[3])
(S PP[,] (, ,) NP[1] VP[2]) / (S NP[1] VP[2])
(S ADVP[,] (, ,) S[1] (, ,) (CC but) S[2] .[3]) / (S S[1] (, ,) (CC but) S[2] .[3])
(S ADVP[,] NP[1] VP[2]) / (S NP[1] VP[2])
(S NP[,] (VP VBP[,] (SBAR (IN that) (S NP[1] VP[2]))) (. .)) / (S NP[1] VP[2] (. .))
(S NP[,] (VP VBZ[,] ADJP[,] SBAR[1])) / S[1]
(S CC[,] PP[,] (, ,) NP[1] VP[2] (..)) / (S NP[1] VP[2] (. .))
(S NP[,] (, ,) NP[1] VP[2] .[3]) / (S NP[1] VP[2] .[3])
(S NP[1] (, ,) ADVP[,] (, ,) VP[2]) / (S NP[1] VP[2])
(S CC[,] (NP PRP[1]) VP[2]) / (S (NP PRP[1]) VP[2])
(S ADVP[,] ,[,] PP[,] ,[,] NP[1] VP[2] .[3]) / (S NP[1] VP[2] .[3])
(S ADVP[,] (, ,) NP[1] VP[2]) / (S NP[1] VP[2])
</table>
<tableCaption confidence="0.9733">
Table 4: High probability S / S compression rules from the final state of the sampler.
</tableCaption>
<bodyText confidence="0.999323363636364">
probability subtree-deletion rules expanding cate-
gories ROOT / ROOT and S / S, respectively. Of
especial interest are deep lexicalized rules such as
a pattern of compression used many times in the
BNC in sentence pairs such as “NPR’s Anne Gar-
rels reports” / “Anne Garrels reports”. Such an
informative rule with nontrivial collocation (be-
tween the possessive marker and the word “re-
ports”) would be hard to extract heuristically and
can only be extracted by reasoning across the
training examples.
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999951913043478">
We explored nonparametric Bayesian learning
of non-isomorphic tree mappings using Dirich-
let process priors. We used the task of extrac-
tive sentence compression as a testbed to investi-
gate the effects of sparse priors and nonparamet-
ric inference over the space of grammars. We
showed that, despite its degeneracy, expectation
maximization is a strong baseline when given a
reasonable grammar. However, Gibbs-sampling–
based nonparametric inference achieves improve-
ments against this baseline. Our investigation with
variational Bayes showed that the improvement is
due both to finding sparse grammars (mitigating
overfitting) and to searching over the space of all
grammars (mitigating narrowness). Overall, we
take these results as being encouraging for STSG
induction via Bayesian nonparametrics for mono-
lingual translation tasks. The future for this work
would involve natural extensions such as mixing
over the space of word alignments; this would al-
low application to MT-like tasks where flexible
word reordering is allowed, such as abstractive
sentence compression and paraphrasing.
</bodyText>
<sectionHeader confidence="0.991849" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.744293">
James Clarke and Mirella Lapata. 2006a. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the 21st Interna-
</reference>
<page confidence="0.994788">
945
</page>
<reference confidence="0.999759451327434">
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 144–151, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
James Clarke and Mirella Lapata. 2006b. Models
for sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 377–384, Sydney, Australia, July.
Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian
model of syntax-directed tree to string grammar in-
duction. In EMNLP ’09: Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 352–361, Morristown, NJ,
USA. Association for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2007. Large mar-
gin synchronous generation and its application to
sentence compression. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing and on Computational Natural Lan-
guage Learning, pages 73–82, Prague. Association
for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In COLING
’08: Proceedings of the 22nd International Confer-
ence on Computational Linguistics, pages 137–144,
Manchester, United Kingdom. Association for Com-
putational Linguistics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In NAACL ’09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 548–556, Morristown, NJ, USA. Association
for Computational Linguistics.
A. Dempster, N. Laird, and D. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39 (Series B):1–38.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why generative phrase models under-
perform surface heuristics. In StatMT ’06: Proceed-
ings of the Workshop on Statistical Machine Trans-
lation, pages 31–38, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
John DeNero, Alexandre Bouchard-Cˆot´e, and Dan
Klein. 2008. Sampling alignment structure under
a Bayesian translation model. In EMNLP ’08: Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 314–323, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In ACL ’03: Pro-
ceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 205–208,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 180–187,
Rochester, New York, April. Association for Com-
putational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation
rule? In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 273–280, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
S. Geman and D. Geman. 1984. Stochastic Relaxation,
Gibbs Distributions and the Bayesian Restoration of
Images. pages 6:721–741.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 673–680,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
sixth conference on Applied natural language pro-
cessing, pages 310–315, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS, pages 3–10. MIT
Press.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91–107.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the Inside-
Outside algorithm. Computer Speech and Lan-
guage, 4:35–56.
Ding Liu and Daniel Gildea. 2009. Bayesian learn-
ing of phrasal tree-to-string templates. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1308–1317,
Singapore, August. Association for Computational
Linguistics.
</reference>
<page confidence="0.987152">
946
</page>
<reference confidence="0.999849774193549">
David J.C. MacKay. 1997. Ensemble learning for hid-
den markov models. Technical report, Cavendish
Laboratory, Cambridge, UK.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417–449.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
45–48, Suntec, Singapore, August. Association for
Computational Linguistics.
Stefan Riezler, Tracy H. King, Richard Crouch, and
Annie Zaenen. 2003. Statistical sentence condensa-
tion using ambiguity packing and stochastic disam-
biguation methods for lexical-functional grammar.
In NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 118–125, Morristown, NJ, USA.
Association for Computational Linguistics.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In ACL ’05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 290–297, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Elif Yamangil and Rani Nelken. 2008. Mining
wikipedia revision histories for improving sentence
compression. In Proceedings of ACL-08: HLT,
Short Papers, pages 137–140, Columbus, Ohio,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.99811">
947
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948787">
<title confidence="0.998189">Bayesian Synchronous Tree-Substitution Grammar Induction and its Application to Sentence Compression</title>
<author confidence="0.999914">Yamangil M Shieber</author>
<affiliation confidence="0.999996">Harvard University</affiliation>
<address confidence="0.999977">Cambridge, Massachusetts, USA</address>
<abstract confidence="0.997827347826087">We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference taito the task of sentence We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Constraintbased sentence compression: An integer programming approach.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>144--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="8863" citStr="Clarke and Lapata, 2006" startWordPosition="1361" endWordPosition="1364">ds on the underlying application. • ATM’s screen performance depends on the underlying application. 938 Figure 1: A portion of an STSG derivation of the example sentence and its extractive compression. where the underlined words were deleted. In supervised sentence compression, the goal is to generalize from a parallel training corpus of sentences (source) and their compressions (target) to unseen sentences in a test set to predict their compressions. An unsupervised setup also exists; methods for the unsupervised problem typically rely on language models and linguistic/discourse constraints (Clarke and Lapata, 2006a; Turner and Charniak, 2005). Because these methods rely on dynamic programming to efficiently consider hypotheses over the space of all possible compressions of a sentence, they may be harder to extend to general paraphrasing. 3 The STSG Model Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). Every grammar rule is a pair of elementary trees aligned at the leaf level at their frontier nodes, which we will denote using the form cs/ct — es/et, -y (indices s for source, t for target) where cs, ct are</context>
<context position="21878" citStr="Clarke and Lapata (2006" startWordPosition="3671" endWordPosition="3675">,] PP[,]) / E in order to smooth our finite model. We used Laplace smoothing (adding 1 to all counts) as it gave us interpretable results. 4 Evaluation We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM). EM is a natural benchmark, while SVM is also appropriate since it can be taken as the state of the art for our task.4 We used a publicly available extractive sentence compression corpus: the Broadcast News compressions corpus (BNC) of Clarke and Lapata (2006a). This corpus consists of 1370 sentence pairs that were manually created from transcribed Broadcast News stories. We split the pairs into training, development, and testing sets of 1000, 3We experimented with MPT using Monte Carlo integration over possible derivations; the results were not significantly different from those using MPD. 4The comparison system described by Cohn and Lapata (2008) attempts to solve a more general problem than ours, abstractive sentence compression. However, given the nature of the data that we provided, it can only learn to compress by deleting words. Since the s</context>
<context position="25227" citStr="Clarke and Lapata (2006" startWordPosition="4232" endWordPosition="4235">rivation. We sampled the alignments of the source nodes in random order. The sampler was run for 5000 iterations with annealing. All hyperparameters α,, β, were held constant at α, β for simplicity and were fit using grid-search over α E [10−6,106],β E [10−3,0.5]. The model chosen for testing was (α, β) = (100, 0.1). As an automated metric of quality, we compute F-score based on grammatical relations (relational F1, or RelF1) (Riezler et al., 2003), by which the consistency between the set of predicted grammatical relations and those from the gold standard is measured, which has been shown by Clarke and Lapata (2006b) to correlate reliably with human judgments. We also conducted a small human subjective evaluation of the grammaticality and informativeness of the compressions generated by the various methods. 4.1 Automated evaluation For all three systems we obtained predictions for the test set and used the Stanford parser to extract grammatical relations from predicted trees and the gold standard. We computed precision, recall, RelF1 (all based on grammatical relations), and compression rate (percentage of the words that are retained), which we report in Table 1. The results for GS are averages over fiv</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>James Clarke and Mirella Lapata. 2006a. Constraintbased sentence compression: An integer programming approach. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 144–151, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Models for sentence compression: A comparison across domains, training requirements and evaluation measures.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>377--384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="8863" citStr="Clarke and Lapata, 2006" startWordPosition="1361" endWordPosition="1364">ds on the underlying application. • ATM’s screen performance depends on the underlying application. 938 Figure 1: A portion of an STSG derivation of the example sentence and its extractive compression. where the underlined words were deleted. In supervised sentence compression, the goal is to generalize from a parallel training corpus of sentences (source) and their compressions (target) to unseen sentences in a test set to predict their compressions. An unsupervised setup also exists; methods for the unsupervised problem typically rely on language models and linguistic/discourse constraints (Clarke and Lapata, 2006a; Turner and Charniak, 2005). Because these methods rely on dynamic programming to efficiently consider hypotheses over the space of all possible compressions of a sentence, they may be harder to extend to general paraphrasing. 3 The STSG Model Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). Every grammar rule is a pair of elementary trees aligned at the leaf level at their frontier nodes, which we will denote using the form cs/ct — es/et, -y (indices s for source, t for target) where cs, ct are</context>
<context position="21878" citStr="Clarke and Lapata (2006" startWordPosition="3671" endWordPosition="3675">,] PP[,]) / E in order to smooth our finite model. We used Laplace smoothing (adding 1 to all counts) as it gave us interpretable results. 4 Evaluation We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM). EM is a natural benchmark, while SVM is also appropriate since it can be taken as the state of the art for our task.4 We used a publicly available extractive sentence compression corpus: the Broadcast News compressions corpus (BNC) of Clarke and Lapata (2006a). This corpus consists of 1370 sentence pairs that were manually created from transcribed Broadcast News stories. We split the pairs into training, development, and testing sets of 1000, 3We experimented with MPT using Monte Carlo integration over possible derivations; the results were not significantly different from those using MPD. 4The comparison system described by Cohn and Lapata (2008) attempts to solve a more general problem than ours, abstractive sentence compression. However, given the nature of the data that we provided, it can only learn to compress by deleting words. Since the s</context>
<context position="25227" citStr="Clarke and Lapata (2006" startWordPosition="4232" endWordPosition="4235">rivation. We sampled the alignments of the source nodes in random order. The sampler was run for 5000 iterations with annealing. All hyperparameters α,, β, were held constant at α, β for simplicity and were fit using grid-search over α E [10−6,106],β E [10−3,0.5]. The model chosen for testing was (α, β) = (100, 0.1). As an automated metric of quality, we compute F-score based on grammatical relations (relational F1, or RelF1) (Riezler et al., 2003), by which the consistency between the set of predicted grammatical relations and those from the gold standard is measured, which has been shown by Clarke and Lapata (2006b) to correlate reliably with human judgments. We also conducted a small human subjective evaluation of the grammaticality and informativeness of the compressions generated by the various methods. 4.1 Automated evaluation For all three systems we obtained predictions for the test set and used the Stanford parser to extract grammatical relations from predicted trees and the gold standard. We computed precision, recall, RelF1 (all based on grammatical relations), and compression rate (percentage of the words that are retained), which we report in Table 1. The results for GS are averages over fiv</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>James Clarke and Mirella Lapata. 2006b. Models for sentence compression: A comparison across domains, training requirements and evaluation measures. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 377–384, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model of syntax-directed tree to string grammar induction.</title>
<date>2009</date>
<booktitle>In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>352--361</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5633" citStr="Cohn and Blunsom, 2009" startWordPosition="858" endWordPosition="861">ithin this grammar. In summary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting problems. Such models have been used as generative solutions to several other segmentation problems, ranging from word segmentation (Goldwater et al., 2006), to parsing (Cohn et al., 2009; Post and Gildea, 2009) and machine translation (DeNero et al., 2008; Cohn and Blunsom, 2009; Liu and Gildea, 2009). Segmentation is achieved by introducing a prior bias towards grammars that are compact representations of the data, namely by enforcing simplicity and sparsity: preferring simple rules (smaller segments) unless the use of a complex rule is evidenced by the data (through repetition), and thus mitigating the overfitting problem. A Dirichlet process (DP) prior is typically used to achieve this interplay. Interestingly, samplingbased nonparametric inference further allows the possibility of searching over the infinite space of grammars (and, in machine translation, possibl</context>
<context position="16538" citStr="Cohn and Blunsom (2009)" startWordPosition="2728" endWordPosition="2731">e pairs. We use Gibbs sampling (Geman and Geman, 1984), a Markov chain Monte Carlo (MCMC) method, to sample from the posterior (3). A derivation e of the corpus t is completely specified by an alignment between the source nodes and the corresponding target nodes (as well as c on either side), which we take to be the state of the sampler. We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al. (2006). Our sampling updates are extensions of those used by Cohn and Blunsom (2009) in MT, but are tailored to our task of extractive sentence compression. In our task, no target node can align with c (which would indicate a subtree insertion), and barring unary branches no source node i can align with two different target nodes j and j&apos; at the same time (indicating a tree expansion). Rather, the configurations of interest are those in which only source nodes i can align with c, and two source nodes i and i&apos; can align with the same target node j. Thus, the alignments of interest are not arbitrary relations, but (partial) functions from nodes in es to nodes in et or c. We the</context>
</contexts>
<marker>Cohn, Blunsom, 2009</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of syntax-directed tree to string grammar induction. In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 352–361, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Large margin synchronous generation and its application to sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and on Computational Natural Language Learning,</booktitle>
<pages>73--82</pages>
<institution>Prague. Association for Computational Linguistics.</institution>
<contexts>
<context position="22778" citStr="Cohn and Lapata (2007)" startWordPosition="3811" endWordPosition="3814">sults were not significantly different from those using MPD. 4The comparison system described by Cohn and Lapata (2008) attempts to solve a more general problem than ours, abstractive sentence compression. However, given the nature of the data that we provided, it can only learn to compress by deleting words. Since the system is less specialized to the task, their model requires additional heuristics in decoding not needed for extractive compression, which might cause a reduction in performance. Nonetheless, because the comparison system is a generalization of the extractive SVM compressor of Cohn and Lapata (2007), we do not expect that the results would differ qualitatively. = argmax e � e 942 SVM EM GS Precision 55.60 58.80 58.94 Recall 53.37 56.58 64.59 Relational F1 54.46 57.67 61.64 Compression rate 59.72 64.11 65.52 Table 1: Precision, recall, relational F1 and compression rate (%) for various systems on the 200- sentence BNC test set. The compression rate for the gold standard was 65.67%. SVM EM GS Gold Grammar 2.751 2.85* 3.69 4.25 Importance 2.85 2.67* 3.41 3.82 Comp. rate 68.18 64.07 67.97 62.34 Table 2: Average grammar and importance scores for various systems on the 20-sentence subsample. S</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Large margin synchronous generation and its application to sentence compression. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and on Computational Natural Language Learning, pages 73–82, Prague. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>137--144</pages>
<institution>Manchester, United Kingdom. Association for Computational Linguistics.</institution>
<contexts>
<context position="21612" citStr="Cohn and Lapata, 2008" startWordPosition="3625" endWordPosition="3628">ely, this approach does not ensure that the test instances are parsable, since ts may include unseen structure or novel words. A workaround is to include all zero-count context free copy rules such as NP / NP --+ (NP NP[1] PP[�]) / (NP NP[1] PP[�]) NP / E � (NP NP[,] PP[,]) / E in order to smooth our finite model. We used Laplace smoothing (adding 1 to all counts) as it gave us interpretable results. 4 Evaluation We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM). EM is a natural benchmark, while SVM is also appropriate since it can be taken as the state of the art for our task.4 We used a publicly available extractive sentence compression corpus: the Broadcast News compressions corpus (BNC) of Clarke and Lapata (2006a). This corpus consists of 1370 sentence pairs that were manually created from transcribed Broadcast News stories. We split the pairs into training, development, and testing sets of 1000, 3We experimented with MPT using Monte Carlo integration over possible derivations; the results were not significantly different from those using </context>
<context position="23852" citStr="Cohn and Lapata, 2008" startWordPosition="3995" endWordPosition="3999">.67* 3.41 3.82 Comp. rate 68.18 64.07 67.97 62.34 Table 2: Average grammar and importance scores for various systems on the 20-sentence subsample. Scores marked with * are significantly different than the corresponding GS score at α &lt; .05 and with † at α &lt; .01 according to post-hoc Tukey tests. ANOVA was significant at p &lt; .01 both for grammar and importance. 170, and 200 pairs, respectively. The corpus was parsed using the Stanford parser (Klein and Manning, 2003). In our experiments with the publicly available SVM system we used all except paraphrasal rules extracted from bilingual corpora (Cohn and Lapata, 2008). The model chosen for testing had parameter for trade-off between training error and margin set to C = 0.001, used margin rescaling, and Hamming distance over bags of tokens with brevity penalty for loss function. EM used a subset of the rules extracted by SVM, namely all rules except non-head deleting compression rules, and was initialized uniformly. Each EM instance was characterized by two parameters: α, the smoothing parameter for MAP-EM, and δ, the smoothing parameter for augmenting the learned grammar with rules extracted from unseen data (add(δ − 1) smoothing was used), both of which w</context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2008. Sentence compression beyond word deletion. In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics, pages 137–144, Manchester, United Kingdom. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing compact but accurate treesubstitution grammars.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>548--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5540" citStr="Cohn et al., 2009" startWordPosition="842" endWordPosition="845">task start with a precomputed subset of rules, and with EM used just to assign weights within this grammar. In summary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting problems. Such models have been used as generative solutions to several other segmentation problems, ranging from word segmentation (Goldwater et al., 2006), to parsing (Cohn et al., 2009; Post and Gildea, 2009) and machine translation (DeNero et al., 2008; Cohn and Blunsom, 2009; Liu and Gildea, 2009). Segmentation is achieved by introducing a prior bias towards grammars that are compact representations of the data, namely by enforcing simplicity and sparsity: preferring simple rules (smaller segments) unless the use of a complex rule is evidenced by the data (through repetition), and thus mitigating the overfitting problem. A Dirichlet process (DP) prior is typically used to achieve this interplay. Interestingly, samplingbased nonparametric inference further allows the possi</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate treesubstitution grammars. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 548–556, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<note>(Series B):1–38.</note>
<contexts>
<context position="4417" citStr="Dempster et al., 1977" startWordPosition="660" endWordPosition="663">ze the (joint or conditional) likelihood or the classification margin in the training data (taking or not taking into account the derivational ambiguity). This option leverages a large amount of manual domain knowledge engineering and is not in general amenable to latent variable problems. A simpler alternative to this two step approach is to use a generative model of synchronous derivation and simultaneously segment and weight the elementary tree pairs to maximize the probability of the training data under that model; the simplest exemplar of this approach uses expectation maximization (EM) (Dempster et al., 1977). This approach has two frailties. First, EM search over the space of all possible rules is computationally impractical. Second, even if such a search were practical, the method is degenerate, pushing the probability mass towards larger rules in order to better approximate the empirical distribution of the data (Goldwater et al., 2006; DeNero et al., 2006). Indeed, the optimal grammar would be one in which each tree pair in the training data is its own rule. Therefore, proposals for using EM for this task start with a precomputed subset of rules, and with EM used just to assign weights within </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39 (Series B):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Gillick</author>
<author>James Zhang</author>
<author>Dan Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In StatMT ’06: Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>31--38</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4775" citStr="DeNero et al., 2006" startWordPosition="717" endWordPosition="720">e model of synchronous derivation and simultaneously segment and weight the elementary tree pairs to maximize the probability of the training data under that model; the simplest exemplar of this approach uses expectation maximization (EM) (Dempster et al., 1977). This approach has two frailties. First, EM search over the space of all possible rules is computationally impractical. Second, even if such a search were practical, the method is degenerate, pushing the probability mass towards larger rules in order to better approximate the empirical distribution of the data (Goldwater et al., 2006; DeNero et al., 2006). Indeed, the optimal grammar would be one in which each tree pair in the training data is its own rule. Therefore, proposals for using EM for this task start with a precomputed subset of rules, and with EM used just to assign weights within this grammar. In summary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting problems. Such models </context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In StatMT ’06: Proceedings of the Workshop on Statistical Machine Translation, pages 31–38, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>314--323</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 314–323, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>205--208</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1747" citStr="Eisner, 2003" startWordPosition="233" endWordPosition="234">Given an aligned corpus of tree pairs, we might want to learn a mapping between the paired trees. Such induction of tree mappings has application in a variety of natural-language-processing tasks including machine translation, paraphrase, and sentence compression. The induced tree mappings can be expressed by synchronous grammars. Where the tree pairs are isomorphic, synchronous context-free grammars (SCFG) may suffice, but in general, non-isomorphism can make the problem of rule extraction difficult (Galley and McKeown, 2007). More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or treeadjoining grammars may better capture the pairings. In this work, we explore techniques for inducing synchronous tree-substitution grammars (STSG) using as a testbed application extractive sentence compression. Learning an STSG from aligned trees is tantamount to determining a segmentation of the trees into elementary trees of the grammar along with an alignment of the elementary trees (see Figure 1 for an example of such a segmentation), followed by estimation of the weights for the extracted tree pairs.1 These elementary tree pairs serve as the rules of the extracted grammar. For SCF</context>
<context position="9254" citStr="Eisner, 2003" startWordPosition="1423" endWordPosition="1424">sentences in a test set to predict their compressions. An unsupervised setup also exists; methods for the unsupervised problem typically rely on language models and linguistic/discourse constraints (Clarke and Lapata, 2006a; Turner and Charniak, 2005). Because these methods rely on dynamic programming to efficiently consider hypotheses over the space of all possible compressions of a sentence, they may be harder to extend to general paraphrasing. 3 The STSG Model Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). Every grammar rule is a pair of elementary trees aligned at the leaf level at their frontier nodes, which we will denote using the form cs/ct — es/et, -y (indices s for source, t for target) where cs, ct are root nonterminals of the elementary trees es, et respectively and -y is a 1-to-1 correspondence between the frontier nodes in es and et. For example, the rule S / S --+ (S (PP (IN Like) NP[,]) NP[1] VP[2]) / (S NP[1] VP[2]) can be used to delete a subtree rooted at PP. We use square bracketed indices to represent the alignment -y of frontier nodes — NP[1] aligns with NP[1], VP[2] aligns </context>
<context position="11590" citStr="Eisner, 2003" startWordPosition="1829" endWordPosition="1830"> algorithm (Lari and Young, 1990), can be used to estimate the rule weights of a given unweighted STSG based on a corpus of parallel parse trees t = t1, ... , tN where tn = tn,s/tn,t for n = 1, ... , N. Similarly, an 939 Figure 2: Gibbs sampling updates. We illustrate a sampler move to align/unalign a source node with a target node (top row in blue), and split/merge a deletion rule via aligning with E (bottom row in red). extension of the Viterbi algorithm is available for finding the maximum probability derivation, useful for predicting the target analysis tN+1,t for a test instance tN+1,s. (Eisner, 2003) However, as noted earlier, EM is subject to the narrowness and overfitting problems. 3.1 The Bayesian generative process Both of these issues can be addressed by taking a nonparametric Bayesian approach, namely, assuming that the elementary tree pairs are sampled from an independent collection of Dirichlet process (DP) priors. We describe such a process for sampling a corpus of tree pairs t. For all pairs of root labels c = cs/ct that we consider, where up to one of cs or ct can be E (e.g., S / S, NP / E), we sample a sparse discrete distribution Gc over infinitely many elementary tree pairs </context>
<context position="20977" citStr="Eisner, 2003" startWordPosition="3515" endWordPosition="3516">prove that our approach is equivalent up to a rescaling of the concentration parameters. Since we fit these parameters to the data, our approach is equivalent. tion (MPD), which we define as e* = argmax p(e |ts, t) e p(e |ts, e)p(e |t) where e denotes a derivation for t = ts/tt. (We suppress the N + 1 subscripts for brevity.) We approximate this objective first by substituting δeMAP(e) for p(e |t) and secondly using a finite STSG model for the infinite p(e |ts, eMAP), which we obtain simply by normalizing the rule counts in eMAP. We use dynamic programming for parsing under this finite model (Eisner, 2003).3 Unfortunately, this approach does not ensure that the test instances are parsable, since ts may include unseen structure or novel words. A workaround is to include all zero-count context free copy rules such as NP / NP --+ (NP NP[1] PP[�]) / (NP NP[1] PP[�]) NP / E � (NP NP[,] PP[,]) / E in order to smooth our finite model. We used Laplace smoothing (adding 1 to all counts) as it gave us interpretable results. 4 Evaluation We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on S</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 205–208, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Lexicalized Markov grammars for sentence compression.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>180--187</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="1666" citStr="Galley and McKeown, 2007" startWordPosition="221" endWordPosition="224">e of grammars as opposed to sparse parametric inference with a fixed grammar. 1 Introduction Given an aligned corpus of tree pairs, we might want to learn a mapping between the paired trees. Such induction of tree mappings has application in a variety of natural-language-processing tasks including machine translation, paraphrase, and sentence compression. The induced tree mappings can be expressed by synchronous grammars. Where the tree pairs are isomorphic, synchronous context-free grammars (SCFG) may suffice, but in general, non-isomorphism can make the problem of rule extraction difficult (Galley and McKeown, 2007). More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or treeadjoining grammars may better capture the pairings. In this work, we explore techniques for inducing synchronous tree-substitution grammars (STSG) using as a testbed application extractive sentence compression. Learning an STSG from aligned trees is tantamount to determining a segmentation of the trees into elementary trees of the grammar along with an alignment of the elementary trees (see Figure 1 for an example of such a segmentation), followed by estimation of the weights for the extracted tree pairs.1</context>
<context position="10596" citStr="Galley and McKeown, 2007" startWordPosition="1662" endWordPosition="1665">arget nodes are used to represent insertions into the target tree. Similarly, the rule NP / E —* (NP (NN FaceLift)) / E can be used to continue deriving the deleted subtree. See Figure 1 for an example of how an STSG with these rules would operate in synchronously generating our example sentence pair. STSG is a convenient choice of formalism for a number of reasons. First, it eliminates the isomorphism and strong independence assumptions of SCFGs. Second, the ability to have rules deeper than one level provides a principled way of modeling lexicalization, whose importance has been emphasized (Galley and McKeown, 2007; Yamangil and Nelken, 2008). Third, we may have our STSG operate on trees instead of sentences, which allows for efficient parsing algorithms, as well as providing syntactic analyses for our predictions, which is desirable for automatic evaluation purposes. A straightforward extension of the popular EM algorithm for probabilistic context free grammars (PCFG), the inside-outside algorithm (Lari and Young, 1990), can be used to estimate the rule weights of a given unweighted STSG based on a corpus of parallel parse trees t = t1, ... , tN where tn = tn,s/tn,t for n = 1, ... , N. Similarly, an 93</context>
</contexts>
<marker>Galley, McKeown, 2007</marker>
<rawString>Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 180–187, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>273--280</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="3070" citStr="Galley et al., 2004" startWordPosition="444" endWordPosition="447">rmalism then restricts us to deriving isomorphic tree pairs. STSG is much more expressive, especially if we allow some elementary trees on the source or target side to be unsynchronized, so that insertions and deletions can be modeled, but the segmentation and alignment problems become nontrivial. Previous approaches to this problem have treated the two steps — grammar extraction and weight estimation — with a variety of methods. One approach is to use word alignments (where these can be reliably estimated, as in our testbed application) to align subtrees and extract rules (Och and Ney, 2004; Galley et al., 2004) but this leaves open the question of finding the right level of generality of the rules — how deep the rules should be and how much lexicalization they should involve — necessitating resorting to heuristics such as minimality of rules, and leading to 1Throughout the paper we will use the word STSG to refer to the tree-to-tree version of the formalism, although the string-to-tree version is also commonly used. 937 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 937–947, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguis</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 273–280, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic Relaxation, Gibbs Distributions and the Bayesian Restoration of Images.</title>
<date>1984</date>
<pages>6--721</pages>
<contexts>
<context position="15969" citStr="Geman and Geman, 1984" startWordPosition="2625" endWordPosition="2628">ider repeatedly generating elementary tree pairs e1, ... , ei, all with the same root c, iid from Ge. Integrating over Ge, the ei become dependent. The conditional prior of the i-th elementary tree pair given previously generated ones e&lt;i = e1, ... , ei_1 is given by ne� + α�P0(ei |c) p(ei |e&lt;i) = (4) i − 1 + αe where nez denotes the number of times ei occurs in e&lt;i. Since the collapsed model is exchangeable in the ei, this formula forms the backbone of the inference procedure that we describe next. It also makes clear DP’s inductive bias to reuse elementary tree pairs. We use Gibbs sampling (Geman and Geman, 1984), a Markov chain Monte Carlo (MCMC) method, to sample from the posterior (3). A derivation e of the corpus t is completely specified by an alignment between the source nodes and the corresponding target nodes (as well as c on either side), which we take to be the state of the sampler. We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al. (2006). Our sampling updates are extensions of those used by Cohn and Blunsom (2009) in MT, but are tailored to our</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>S. Geman and D. Geman. 1984. Stochastic Relaxation, Gibbs Distributions and the Bayesian Restoration of Images. pages 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>673--680</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="4753" citStr="Goldwater et al., 2006" startWordPosition="713" endWordPosition="716">ch is to use a generative model of synchronous derivation and simultaneously segment and weight the elementary tree pairs to maximize the probability of the training data under that model; the simplest exemplar of this approach uses expectation maximization (EM) (Dempster et al., 1977). This approach has two frailties. First, EM search over the space of all possible rules is computationally impractical. Second, even if such a search were practical, the method is degenerate, pushing the probability mass towards larger rules in order to better approximate the empirical distribution of the data (Goldwater et al., 2006; DeNero et al., 2006). Indeed, the optimal grammar would be one in which each tree pair in the training data is its own rule. Therefore, proposals for using EM for this task start with a precomputed subset of rules, and with EM used just to assign weights within this grammar. In summary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting </context>
<context position="16460" citStr="Goldwater et al. (2006)" startWordPosition="2715" endWordPosition="2718">describe next. It also makes clear DP’s inductive bias to reuse elementary tree pairs. We use Gibbs sampling (Geman and Geman, 1984), a Markov chain Monte Carlo (MCMC) method, to sample from the posterior (3). A derivation e of the corpus t is completely specified by an alignment between the source nodes and the corresponding target nodes (as well as c on either side), which we take to be the state of the sampler. We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al. (2006). Our sampling updates are extensions of those used by Cohn and Blunsom (2009) in MT, but are tailored to our task of extractive sentence compression. In our task, no target node can align with c (which would indicate a subtree insertion), and barring unary branches no source node i can align with two different target nodes j and j&apos; at the same time (indicating a tree expansion). Rather, the configurations of interest are those in which only source nodes i can align with c, and two source nodes i and i&apos; can align with the same target node j. Thus, the alignments of interest are not arbitrary r</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 673–680, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the sixth conference on Applied natural language processing,</booktitle>
<pages>310--315</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7879" citStr="Jing, 2000" startWordPosition="1205" endWordPosition="1206">rs as opposed to sparse parametric inference with a fixed grammar. In the following, we define the task of extractive sentence compression and the Bayesian STSG model, and algorithms we used for inference and prediction. We then describe the experiments in extractive sentence compression and present our results in contrast with alternative algorithms. We conclude by giving examples of compression patterns learned by the Bayesian method. 2 Sentence compression Sentence compression is the task of summarizing a sentence while retaining most of the informational content and remaining grammatical (Jing, 2000). In extractive sentence compression, which we focus on in this paper, an order-preserving subset of the words in the sentence are selected to form the summary, that is, we summarize by deleting words (Knight and Marcu, 2002). An example sentence pair, which we use as a running example, is the following: • Like FaceLift, much of ATM’s screen performance depends on the underlying application. • ATM’s screen performance depends on the underlying application. 938 Figure 1: A portion of an STSG derivation of the example sentence and its extractive compression. where the underlined words were delet</context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>Hongyan Jing. 2000. Sentence reduction for automatic text summarization. In Proceedings of the sixth conference on Applied natural language processing, pages 310–315, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15 (NIPS,</booktitle>
<pages>3--10</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="23699" citStr="Klein and Manning, 2003" startWordPosition="3971" endWordPosition="3975">ms on the 200- sentence BNC test set. The compression rate for the gold standard was 65.67%. SVM EM GS Gold Grammar 2.751 2.85* 3.69 4.25 Importance 2.85 2.67* 3.41 3.82 Comp. rate 68.18 64.07 67.97 62.34 Table 2: Average grammar and importance scores for various systems on the 20-sentence subsample. Scores marked with * are significantly different than the corresponding GS score at α &lt; .05 and with † at α &lt; .01 according to post-hoc Tukey tests. ANOVA was significant at p &lt; .01 both for grammar and importance. 170, and 200 pairs, respectively. The corpus was parsed using the Stanford parser (Klein and Manning, 2003). In our experiments with the publicly available SVM system we used all except paraphrasal rules extracted from bilingual corpora (Cohn and Lapata, 2008). The model chosen for testing had parameter for trade-off between training error and margin set to C = 0.001, used margin rescaling, and Hamming distance over bags of tokens with brevity penalty for loss function. EM used a subset of the rules extracted by SVM, namely all rules except non-head deleting compression rules, and was initialized uniformly. Each EM instance was characterized by two parameters: α, the smoothing parameter for MAP-EM,</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15 (NIPS, pages 3–10. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: a probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artif. Intell.,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="8104" citStr="Knight and Marcu, 2002" startWordPosition="1241" endWordPosition="1244">iction. We then describe the experiments in extractive sentence compression and present our results in contrast with alternative algorithms. We conclude by giving examples of compression patterns learned by the Bayesian method. 2 Sentence compression Sentence compression is the task of summarizing a sentence while retaining most of the informational content and remaining grammatical (Jing, 2000). In extractive sentence compression, which we focus on in this paper, an order-preserving subset of the words in the sentence are selected to form the summary, that is, we summarize by deleting words (Knight and Marcu, 2002). An example sentence pair, which we use as a running example, is the following: • Like FaceLift, much of ATM’s screen performance depends on the underlying application. • ATM’s screen performance depends on the underlying application. 938 Figure 1: A portion of an STSG derivation of the example sentence and its extractive compression. where the underlined words were deleted. In supervised sentence compression, the goal is to generalize from a parallel training corpus of sentences (source) and their compressions (target) to unseen sentences in a test set to predict their compressions. An unsup</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artif. Intell., 139(1):91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the InsideOutside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="11010" citStr="Lari and Young, 1990" startWordPosition="1722" endWordPosition="1725">independence assumptions of SCFGs. Second, the ability to have rules deeper than one level provides a principled way of modeling lexicalization, whose importance has been emphasized (Galley and McKeown, 2007; Yamangil and Nelken, 2008). Third, we may have our STSG operate on trees instead of sentences, which allows for efficient parsing algorithms, as well as providing syntactic analyses for our predictions, which is desirable for automatic evaluation purposes. A straightforward extension of the popular EM algorithm for probabilistic context free grammars (PCFG), the inside-outside algorithm (Lari and Young, 1990), can be used to estimate the rule weights of a given unweighted STSG based on a corpus of parallel parse trees t = t1, ... , tN where tn = tn,s/tn,t for n = 1, ... , N. Similarly, an 939 Figure 2: Gibbs sampling updates. We illustrate a sampler move to align/unalign a source node with a target node (top row in blue), and split/merge a deletion rule via aligning with E (bottom row in red). extension of the Viterbi algorithm is available for finding the maximum probability derivation, useful for predicting the target analysis tN+1,t for a test instance tN+1,s. (Eisner, 2003) However, as noted e</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. J. Young. 1990. The estimation of stochastic context-free grammars using the InsideOutside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of phrasal tree-to-string templates.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1308--1317</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5656" citStr="Liu and Gildea, 2009" startWordPosition="862" endWordPosition="865">ummary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting problems. Such models have been used as generative solutions to several other segmentation problems, ranging from word segmentation (Goldwater et al., 2006), to parsing (Cohn et al., 2009; Post and Gildea, 2009) and machine translation (DeNero et al., 2008; Cohn and Blunsom, 2009; Liu and Gildea, 2009). Segmentation is achieved by introducing a prior bias towards grammars that are compact representations of the data, namely by enforcing simplicity and sparsity: preferring simple rules (smaller segments) unless the use of a complex rule is evidenced by the data (through repetition), and thus mitigating the overfitting problem. A Dirichlet process (DP) prior is typically used to achieve this interplay. Interestingly, samplingbased nonparametric inference further allows the possibility of searching over the infinite space of grammars (and, in machine translation, possible word alignments), thu</context>
<context position="28513" citStr="Liu and Gildea, 2009" startWordPosition="4793" endWordPosition="4796">nts are normalized, such that instead of updating the rule weight in the t-th iteration as in the following 0t+1 c,e = nc,e + α − 1 nc,. + Kα − K where nc,e represents the expected count of rule c —* e, and K is the total number of ways to rewrite c, we now take into account our DP(αc, P0(· |c)) prior in (1), which, when truncated to a finite grammar, reduces to a K-dimensional Dirichlet prior with parameter αcP0(· |c). Thus in VB we perform a variational E-step with the subprobabilities given by 0t+1= exp (IF(nc,e + αcP0(e |c))) c,e exp (IF(nc,. + αc)) where IF denotes the digamma function. (Liu and Gildea, 2009) (See MacKay (1997) for details.) Hyperparameters were handled the same way as for GS. Instead of selecting a single model on the development set, here we provide the whole spectrum of models and their performances in order to better understand their comparative behavior. In Figure 3 we plot RelF1 on the test set versus compression rate and compare GS, EM, and VB (Q = 0.1 fixed, (α, S) ranging in [10−6,106]x(1, 2]). Overall, we see that GS maintains roughly the same level of precision as EM (despite its larger compression rates) while achieving an improvement in recall, consequently performing</context>
</contexts>
<marker>Liu, Gildea, 2009</marker>
<rawString>Ding Liu and Daniel Gildea. 2009. Bayesian learning of phrasal tree-to-string templates. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1308–1317, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C MacKay</author>
</authors>
<title>Ensemble learning for hidden markov models.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>Cavendish Laboratory,</institution>
<location>Cambridge, UK.</location>
<contexts>
<context position="28532" citStr="MacKay (1997)" startWordPosition="4798" endWordPosition="4799">at instead of updating the rule weight in the t-th iteration as in the following 0t+1 c,e = nc,e + α − 1 nc,. + Kα − K where nc,e represents the expected count of rule c —* e, and K is the total number of ways to rewrite c, we now take into account our DP(αc, P0(· |c)) prior in (1), which, when truncated to a finite grammar, reduces to a K-dimensional Dirichlet prior with parameter αcP0(· |c). Thus in VB we perform a variational E-step with the subprobabilities given by 0t+1= exp (IF(nc,e + αcP0(e |c))) c,e exp (IF(nc,. + αc)) where IF denotes the digamma function. (Liu and Gildea, 2009) (See MacKay (1997) for details.) Hyperparameters were handled the same way as for GS. Instead of selecting a single model on the development set, here we provide the whole spectrum of models and their performances in order to better understand their comparative behavior. In Figure 3 we plot RelF1 on the test set versus compression rate and compare GS, EM, and VB (Q = 0.1 fixed, (α, S) ranging in [10−6,106]x(1, 2]). Overall, we see that GS maintains roughly the same level of precision as EM (despite its larger compression rates) while achieving an improvement in recall, consequently performing at a higher RelF1 </context>
</contexts>
<marker>MacKay, 1997</marker>
<rawString>David J.C. MacKay. 1997. Ensemble learning for hidden markov models. Technical report, Cavendish Laboratory, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Comput. Linguist.,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="3048" citStr="Och and Ney, 2004" startWordPosition="440" endWordPosition="443">y tree — but the formalism then restricts us to deriving isomorphic tree pairs. STSG is much more expressive, especially if we allow some elementary trees on the source or target side to be unsynchronized, so that insertions and deletions can be modeled, but the segmentation and alignment problems become nontrivial. Previous approaches to this problem have treated the two steps — grammar extraction and weight estimation — with a variety of methods. One approach is to use word alignments (where these can be reliably estimated, as in our testbed application) to align subtrees and extract rules (Och and Ney, 2004; Galley et al., 2004) but this leaves open the question of finding the right level of generality of the rules — how deep the rules should be and how much lexicalization they should involve — necessitating resorting to heuristics such as minimality of rules, and leading to 1Throughout the paper we will use the word STSG to refer to the tree-to-tree version of the formalism, although the string-to-tree version is also commonly used. 937 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 937–947, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Comput. Linguist., 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of a tree substitution grammar.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>45--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="5564" citStr="Post and Gildea, 2009" startWordPosition="846" endWordPosition="849">recomputed subset of rules, and with EM used just to assign weights within this grammar. In summary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting problems. Such models have been used as generative solutions to several other segmentation problems, ranging from word segmentation (Goldwater et al., 2006), to parsing (Cohn et al., 2009; Post and Gildea, 2009) and machine translation (DeNero et al., 2008; Cohn and Blunsom, 2009; Liu and Gildea, 2009). Segmentation is achieved by introducing a prior bias towards grammars that are compact representations of the data, namely by enforcing simplicity and sparsity: preferring simple rules (smaller segments) unless the use of a complex rule is evidenced by the data (through repetition), and thus mitigating the overfitting problem. A Dirichlet process (DP) prior is typically used to achieve this interplay. Interestingly, samplingbased nonparametric inference further allows the possibility of searching over</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Bayesian learning of a tree substitution grammar. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 45–48, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Richard Crouch</author>
<author>Annie Zaenen</author>
</authors>
<title>Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>118--125</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="25056" citStr="Riezler et al., 2003" startWordPosition="4203" endWordPosition="4206">, both of which were fit to the development set using grid-search over (1, 2]. The model chosen for testing was (α, δ) = (1.0001,1.01). GS was initialized at a random derivation. We sampled the alignments of the source nodes in random order. The sampler was run for 5000 iterations with annealing. All hyperparameters α,, β, were held constant at α, β for simplicity and were fit using grid-search over α E [10−6,106],β E [10−3,0.5]. The model chosen for testing was (α, β) = (100, 0.1). As an automated metric of quality, we compute F-score based on grammatical relations (relational F1, or RelF1) (Riezler et al., 2003), by which the consistency between the set of predicted grammatical relations and those from the gold standard is measured, which has been shown by Clarke and Lapata (2006b) to correlate reliably with human judgments. We also conducted a small human subjective evaluation of the grammaticality and informativeness of the compressions generated by the various methods. 4.1 Automated evaluation For all three systems we obtained predictions for the test set and used the Stanford parser to extract grammatical relations from predicted trees and the gold standard. We computed precision, recall, RelF1 (</context>
</contexts>
<marker>Riezler, King, Crouch, Zaenen, 2003</marker>
<rawString>Stefan Riezler, Tracy H. King, Richard Crouch, and Annie Zaenen. 2003. Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 118–125, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenine Turner</author>
<author>Eugene Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>290--297</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8892" citStr="Turner and Charniak, 2005" startWordPosition="1365" endWordPosition="1368">cation. • ATM’s screen performance depends on the underlying application. 938 Figure 1: A portion of an STSG derivation of the example sentence and its extractive compression. where the underlined words were deleted. In supervised sentence compression, the goal is to generalize from a parallel training corpus of sentences (source) and their compressions (target) to unseen sentences in a test set to predict their compressions. An unsupervised setup also exists; methods for the unsupervised problem typically rely on language models and linguistic/discourse constraints (Clarke and Lapata, 2006a; Turner and Charniak, 2005). Because these methods rely on dynamic programming to efficiently consider hypotheses over the space of all possible compressions of a sentence, they may be harder to extend to general paraphrasing. 3 The STSG Model Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). Every grammar rule is a pair of elementary trees aligned at the leaf level at their frontier nodes, which we will denote using the form cs/ct — es/et, -y (indices s for source, t for target) where cs, ct are root nonterminals of the ele</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 290–297, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Yamangil</author>
<author>Rani Nelken</author>
</authors>
<title>Mining wikipedia revision histories for improving sentence compression.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>137--140</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="10624" citStr="Yamangil and Nelken, 2008" startWordPosition="1666" endWordPosition="1669">present insertions into the target tree. Similarly, the rule NP / E —* (NP (NN FaceLift)) / E can be used to continue deriving the deleted subtree. See Figure 1 for an example of how an STSG with these rules would operate in synchronously generating our example sentence pair. STSG is a convenient choice of formalism for a number of reasons. First, it eliminates the isomorphism and strong independence assumptions of SCFGs. Second, the ability to have rules deeper than one level provides a principled way of modeling lexicalization, whose importance has been emphasized (Galley and McKeown, 2007; Yamangil and Nelken, 2008). Third, we may have our STSG operate on trees instead of sentences, which allows for efficient parsing algorithms, as well as providing syntactic analyses for our predictions, which is desirable for automatic evaluation purposes. A straightforward extension of the popular EM algorithm for probabilistic context free grammars (PCFG), the inside-outside algorithm (Lari and Young, 1990), can be used to estimate the rule weights of a given unweighted STSG based on a corpus of parallel parse trees t = t1, ... , tN where tn = tn,s/tn,t for n = 1, ... , N. Similarly, an 939 Figure 2: Gibbs sampling u</context>
</contexts>
<marker>Yamangil, Nelken, 2008</marker>
<rawString>Elif Yamangil and Rani Nelken. 2008. Mining wikipedia revision histories for improving sentence compression. In Proceedings of ACL-08: HLT, Short Papers, pages 137–140, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>