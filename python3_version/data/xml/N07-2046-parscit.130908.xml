<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011728">
<title confidence="0.982923">
Entity Extraction is a Boring Solved Problem – or is it?
</title>
<author confidence="0.914524">
Marc Vilain
</author>
<affiliation confidence="0.751007">
The MITRE Corporation
</affiliation>
<address confidence="0.808002">
Burlington Rd
Bedford MA 01730 USA
</address>
<email confidence="0.994395">
mbv@mitre.org
</email>
<author confidence="0.858984">
Jennifer Su
</author>
<affiliation confidence="0.809121666666667">
The MITRE Corporation and
Cornell University
Ithaca NY 14853 USA
</affiliation>
<email confidence="0.989609">
jfs29@cornell.edu
</email>
<author confidence="0.693203">
Suzi Lubar
</author>
<affiliation confidence="0.573132">
The MITRE Corporation
</affiliation>
<address confidence="0.753553">
Burlington Rd
Bedford MA 01730 USA
</address>
<email confidence="0.995699">
slubar@mitre.org
</email>
<sectionHeader confidence="0.997343" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999511">
This paper presents empirical results that
contradict the prevailing opinion that en-
tity extraction is a boring solved problem.
In particular, we consider data sets that
resemble familiar MUC/ACE data, and re-
port surprisingly poor performance for
both commercial and research systems.
We then give an error analysis that sug-
gests research challenges for entity ex-
traction that are neither boring nor solved.
</bodyText>
<sectionHeader confidence="0.994761" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.999926378378378">
Entity extraction or named entity recognition, as it
is sometimes called, is a known and familiar prob-
lem. Named entity (NE) tagging has been the sub-
ject of numerous shared-task evaluations, including
the seminal MUC 6, MUC 7 and MET evaluations,
the CoNLL shared task, the SIGHAN bake-offs, and
the ACE evaluations. With this track record, and
with commercial vendors now selling named-entity
tagging for a fee, many naturally consider entity
extraction to be an essentially solved problem.
The present paper challenges this view.
The main issue, as we see it, is transfer: NE tag-
gers developed for a specific corpus tend not to
perform well on other data sets. Kosseim and
Poibeau (2001), for one, show that the informal
language of email or speech transcriptions befud-
dles taggers built for journalistic text. Minkov et
al (2005) further explore the systematic differences
between journalistic and informal texts, training
separate taggers for each text source of interest.
Because named entity taggers are so strongly
based on surface features, it isn’t surprising to ob-
serve poor tagger transfer across texts with signifi-
cantly different styles or with unrelated content. In
this paper, we report on the more surprising result
that transfer issues arise even for texts with closely
aligned content or closely aligned styles.
In particular, we consider a range of primarily
business-related texts that are, on the face of it,
close in style and/or substance to the journalistic
stories in existing NE data sets, MUC 6 in particular.
We thus would have expected these texts to sup-
port good transfer performance from taggers con-
figured to the MUC task. Instead, we found the
same kinds of performance drops as Kosseim and
Poibeau had noted for informal texts. Our aim
here is to shed light on the how and why of this.
</bodyText>
<sectionHeader confidence="0.44619" genericHeader="introduction">
2 Scope of the present study
</sectionHeader>
<bodyText confidence="0.999981285714286">
We begin with a disclaimer. Our goal is not so
much to present new technical solutions to NE rec-
ognition, as to draw attention to those aspects of
the problem that remain unsolved. We cover two
main thrusts: (i) a black-box evaluation of several
NE taggers (commercial and research systems); and
(ii) an error analysis of system performance.
</bodyText>
<subsectionHeader confidence="0.99549">
2.1 Evaluation data
</subsectionHeader>
<bodyText confidence="0.999970090909091">
Our evaluation data set contains three distinct sec-
tions. The largest component consists of publicly-
available financial reports filed with the Securities
and Exchange Commission (SEC), in particular the
2003 forms 10-K filed by eight Fortune 500 com-
panies. These corporate annual reports share the
same subject matter as much business news: sales,
profits, acquisitions, business strategies and the
like. They take, however, a more technical slant
and are rich in accounting jargon. They are also
longer, ranging in our study from 22 to 54 pages.
</bodyText>
<page confidence="0.982643">
181
</page>
<note confidence="0.4495555">
Proceedings of NAACL HLT 2007, Companion Volume, pages 181–184,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<table confidence="0.9988018">
Pocahontas Rule-based
Belle Rule-based
Jasmine Statistical, HMM, MUC-trained
Mulan Statistical, CRF, MUC-trained
Ariel Rule-based, 10-K tuning
</table>
<tableCaption confidence="0.999861">
Table 1: system pseudonyms.
</tableCaption>
<bodyText confidence="0.999927666666667">
Preliminary exploration with our own MUC 6
tagger showed these SEC filings to be particularly
hard to tag. Because their sheer length and techni-
cal emphasis seemed implicated in this poor per-
formance, we assembled a second corpus of forty
Web-hosted business stories from such news pro-
viders as MS-NBC, CNN Money, and Motley Fool.
These stories focus on the same eight companies as
our 10-K data set, but are shorter and less techni-
cal, thus allowing us to isolate length and techni-
cality as factors in tagging business texts.
The final portion of our test set consists of ten
news stories that were selected to closely match the
kind of data used in past MUC evaluations. They
were drawn from the New York Times (NYT) and
Wall Street Journal (WSJ) on-line editions, and fo-
cus on current events, thus providing one more
comparable dimension of evaluation.1
</bodyText>
<subsectionHeader confidence="0.995947">
2.2 Evaluated systems
</subsectionHeader>
<bodyText confidence="0.999991352941177">
Five systems participated in our study, represent-
ing a range of commercial tools and research pro-
totypes. Two of these are state-of-the-art hand-
built systems based on rule/pattern interpreters.
Two are open-source statistical systems, one based
on HMMs, and the other on CRFs; both were trained
on the MUC 6 data set. The final system is our own
legacy MUC-style tagger, noted as Ariel in Table 1.
Except as noted below, all the systems were run
out of the box, with no adaptation to the data.
License and privacy concerns prevent us from
identifying all the systems; instead this paper re-
ports most results anonymously, using the names
of Disney heroines as system pseudonyms. We
have, however exposed the identity of our own
system out of fairness, as it benefited somewhat
from earlier tuning to SEC forms 10-K.
</bodyText>
<subsectionHeader confidence="0.995691">
2.3 Evaluation method
</subsectionHeader>
<bodyText confidence="0.959295">
We attempted to replicate the procedure used in the
MUC evaluations, extending it only as required by
1 We will make the non-copyrighted part of our corpus (the
10-Ks) available to other researchers.
the characteristics of the taggers. The test data
were formatted as in MUC 6, and where SGML
markup ran afoul of system I/O characteristics, we
remapped the data manually, resolving, e.g., cross-
ing tags that may have strayed into the output.
To provide scores that could be compared with
the MUC evaluations, we created MUC6-compliant
answer keys (Sundheim, 1995), and remapped sys-
tem output to this standard. We removed system
responses that were considered non-taggable in
MUC (e.g., URLs) and conflated fine-grained dis-
tinctions not made in MUC (e.g., remapping coun-
try tags to location). Scores were assessed with the
venerable MUC scorer, which provides partial
credit for system responses that match the key in
type but not extent, or vice-versa. The scorer also
provides a full error analysis, separately character-
izing each error in a system response.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="method">
3 Findings
</sectionHeader>
<bodyText confidence="0.999434333333333">
Table 2, overleaf, presents our overall findings,
aggregated across the three primary entity types:
person, organization, and location (the ENAMEX
types in the MUC standard). We generally did not
measure the MUC TIMEX (dates, times) and NUMEX
types (moneys, percents) because: (i) neither of the
statistical systems generate them; (ii) those systems
that do generate them tend to do well; (iii) they are
overwhelmingly more frequent in the SEC data than
in news, thus skewing results. For completeness’
sake, however, Table 2 does provide all-entity
news scores in parentheses for those systems that
happened to generate the full set of MUC-6 entities.
Turning now to actual performance measure-
ments, Table 2 does not present an especially
pretty picture. Aside from two systems’ runs on
the MUC-like current events, all the scores are sub-
stantially below those obtained by competitive
MUC systems, which typically reached F scores in
the mid-90s, with a high of F=96 at MUC-6.
SEC. The worst performances were turned in
for SEC filings, as shown in the first block of rows
in Table 2. While precision is generally poor, re-
call is even worse. One reason for this is the very
frequent rightwards shortenings of company names
(e.g., from 3M Corporation to the Corporation), in
contrast to the leftwards shortening (e.g., 3M) fa-
vored in news texts. Ariel had been tuned to tag all
these cases, but the other systems only tagged a
scattershot fraction. To isolate the contribution of
</bodyText>
<page confidence="0.994554">
182
</page>
<table confidence="0.999692444444444">
Pocahontas Belle Jasmine Mulan Ariel
SEC filings R=58 R=28 R=50 R=50 R=71
P=65 P=52 P=43 P=56 P=79
F=61.1 F=36.4 F=42.7 F=52.6 F=74.5
SEC filings, R=71 R=36 R=55 R=60 R=71
“the Corp.”
optional
P=65 P=52 P=40 P=56 P=79
F=68.0 F=42.8 F=46.2 F=57.9 F=74.7
Business R=80 (82) R=64 (69) R=76 R=65 R=71 (75)
news
P=80 (79) P=86 (83) P=63 P=74 P=74 (75)
F=80.1 (81) F=73.5 (75) F=69.1 F=69.2 F=72.3 (75)
Current R=94 (94) R=59 (63) R=79 R=79 R=89 (91)
events
(MUC-like)
P=94 (93) P=82 (80) P=70 P=92 P=91 (92)
F=94.3 (94) F=68.5 (71) F=74.5 F=84.9 F=90.4 (92)
</table>
<tableCaption confidence="0.999696">
Table 2: aggregated extraction scores, ENAMEX only, unless parenthesized (in parens = all entities).
</tableCaption>
<bodyText confidence="0.999750035714286">
these cases to system recall error, we recalculated
the scores by making the cases optional. The
scorer removes missing optional responses from
the recall denominator, and as expected recall im-
proved; see the second block in Table 2.
Business news. The most consistent perform-
ance across systems was achieved with business
news, with scores ranging in F=69-80. This is a
huge improvement over the gaping F=36-75 range
we saw with SEC filings (F=43-75 with optional
short names). This confirms that length and finan-
cial jargon are implicated in the poor performance
on forms 10-K. Nonetheless, these improved
scores are still 15-20 points lower than the better
MUC scores. Is business language just hard to tag?
MUC-like news. Our attempt to replicate the
MUC evaluation data yields an equivocal answer.
Two systems (Pocahontas and Ariel) achieved
MUC6-level scores; it may not be coincidental that
both are next-generation versions of systems that
participated at MUC. Of the other systems, MUC-
trained Mulan also showed substantial improve-
ment going from business news to current events.
While it is good news that three of the systems
that were explicitly trained on MUC (manually or
statistically) did well on MUC-like data, it is disqui-
eting to see how poorly this training generalized to
other news texts.
</bodyText>
<sectionHeader confidence="0.938371" genericHeader="method">
4 Factors affecting performance
</sectionHeader>
<bodyText confidence="0.9908006">
A finer analysis of our three data sets helps trian-
gulate the factors leading to the systematic per-
formance differences shown in Table 2.
Prevalence of organizations. One factor espe-
cially stands out: as Table 3 shows, organizations
are twice as prevalent in the business sources as in
the MUC-like data. As organization scores gener-
ally trail scores for persons and locations (Table 4),
this partly explains why business texts are hard.
Kinds of organizations. But that does not ex-
plain it all. The profiles in Figure 1 show that cur-
rent events favor government/quasi-government
names (e.g.,“Congress,” “Hamas”). They are less
linguistically productive than the corporate and
quasi-corporate names in business texts, and so are
more amenable to being explicitly listed in name
gazetteers. Florian et al (2003) note the effective-
ness of gazetteers for tagging the CoNLL corpus.
Editorial standards. Our business news data
reflect a growing portion of Web-hosted texts that
relax the journalistic editorial rules of traditional
news sources such as the NYT or WSJ. For in-
stance, our data show the same frequent omission
of corporate designators (e.g. “inc.”) that Kosseim
noted in informal text. Whereas news sources of
record will generally mention a company’s desig-
nator at least once in a story, our business data fre-
quently fail to do so at all, thus removing a key
name-tagging cue. By tracing the Ariel rule base,
we found that the absence of any designator was
implicated in 81% of the system’s recall error for
organization names.
Length. Name taggers often overcome this
kind of missing evidence by second-passing a text,
propagating name mentions identified in the first
</bodyText>
<table confidence="0.9834765">
SEC Business MUC
Org 70% 65% 29%
Per 9% 23% 35%
Loc 21% 12% 36%
</table>
<tableCaption confidence="0.999336">
Table 3: Relative distribution of entity types
</tableCaption>
<page confidence="0.95312">
183
</page>
<table confidence="0.999540818181818">
Poca. Belle Jasm. Mul. Ariel
S org F=62 F=10 F=46 F=53 F=83
opt F=74 F=14 F=52 F=61 F=83
S per F=75 F=65 F=49 F=64 F=60
S loc F=79 F=77 F=49 F=74 F=78
B org F=77 F=72 F=70 F=63 F=66
B. per F=90 F=85 F=70 F=69 F=79
B. loc F=78 F=76 F=59 F=75 F=73
M org F=90 F=58 F=48 F=80 F=80
M per F=99 F=90 F=84 F=81 F=92
M loc F=98 F=81 F=74 F=89 F=95
</table>
<tableCaption confidence="0.999401">
Table 4: Type subcores (S=SEC, B=biz., M=MUC)
</tableCaption>
<bodyText confidence="0.999783357142857">
pass to matching but undetected mentions (Mik-
heev, 1999). This strategy runs foul, though, when
the first pass produces precision errors, as these too
can get propagated. Document length is implicated
in this through the greater cumulative likelihood of
making an error on the first pass and of finding a
mention that matches the error on the second pass.
Quasi-names and non-names. A final factor
that especially afflicts the Forms 10-K is the simi-
larity of names and non-names. Non-taggable
product names (“AMD Athlon”) often look like le-
gitimate subsidiaries, while valid operating divi-
sions (“Health Care”) are often hard to distinguish
from generic designations of market segments.
</bodyText>
<sectionHeader confidence="0.999533" genericHeader="conclusions">
5 Implications for further research.
</sectionHeader>
<bodyText confidence="0.999962947368421">
What surprised us most in conducting this study
was to find so obvious a transfer gap among what
appear to be very similar text sources. We were
also surprised by the involvement in this of relaxed
editorial standards around seeming trivia (like the
keyword “inc.”) This suggests, for one, that cur-
rent techniques remain too dependent on skin-deep
word co-occurrence features. It also suggests that
the editorially pristine news texts used in so much
NE research may be atypically easy to tag.
While name-tagging programs may struggle
with editorially informal texts, the absence of sur-
face contextual cues poses no noticeable challenge
to human readers. What cues are left, and there are
many, are semantic in nature: predicate-argument
structure, selectional restrictions, organization of
the lexicon, etc. Recent efforts to create common
propositional banks and lexical ontologies may
thus have much to offer. Indeed, current research
in these areas is just beginning to trickle down to
the name-tagging problem (Mohit &amp; Hwa, 2005).
Another key issue is ensuring tagging coher-
ency at the whole-document level. This might help
alleviate the kind of error propagation with dual-
pass strategies that particularly afflicts long docu-
ments. Recent applications of statistical co-
reference models are beginning to show promise
(Finkel et al, 2005; Ji &amp; Grishman, 2005).
Lastly, we can see this whole study as a particu-
lar challenge case for transfer learning, and indeed
such work as Sutton and McCallum’s (2005) has
looked at the name-tagging task from a transfer
learning standpoint.
It may thus be that today’s exciting emerging
work in “unsolved” areas – semantics, reference,
and learning – could come to play a key role in
what is sometimes maligned as yesterday’s boring
solved problem.
</bodyText>
<sectionHeader confidence="0.999231" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999230263157895">
Finkel J R, Grenager T, Manning C (2005). Incorporat-
ing non-local information into information extraction
systems by Gibbs sampling, Proc ACL, Ann Arbor.
Florian R, Ittycheriah A, Jing H, Zhang T (2003).
Named entity recognition through classifier combina-
tion, Proc CoNLL, Edmonton.
Ji H, Grishman R (2005). Improving name tagging by
reference resolution and relation detection, Proc ACL.
Kosseim L, Poibeau T (2001). Extraction de noms pro-
pres ˆ partir de textes varies. Proc. TALN, Toulouse.
Mikheev A, Moens M, Grover C (1999). Named entity
recognition without gazetteers. Proc. EACL, Bergen.
Minkov E, Wang R, Cohen W (2005). Extracting per-
sonal names from email. Proc HLT/EMNLP, Vancouver.
Mohit B, Hwa R (2005) Syntax-based semi-supervised
named entity tagging. Proc ACL, Ann Arbor MI.
Sundheim B (1995), ed. Proc MUC-6, Columbia MD.
Sutton C, McCallum A (2005). Composition of CRFs for
transfer learning, Proc HLT/EMNLP, Vancouver.
</reference>
<page confidence="0.998693">
184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.996845">Entity Extraction is a Boring Solved Problem – or is it?</title>
<author confidence="0.98895">Marc</author>
<affiliation confidence="0.7505855">The MITRE Burlington</affiliation>
<address confidence="0.992185">Bedford MA 01730</address>
<email confidence="0.972644">mbv@mitre.org</email>
<author confidence="0.983469">Jennifer</author>
<affiliation confidence="0.937881">MITRE Corporation Cornell</affiliation>
<address confidence="0.909744">Ithaca NY 14853</address>
<email confidence="0.992177">jfs29@cornell.edu</email>
<author confidence="0.925472">Suzi</author>
<affiliation confidence="0.693207">The MITRE Burlington</affiliation>
<address confidence="0.996808">Bedford MA 01730</address>
<email confidence="0.993018">slubar@mitre.org</email>
<abstract confidence="0.999455">This paper presents empirical results that contradict the prevailing opinion that entity extraction is a boring solved problem. In particular, we consider data sets that familiar and report surprisingly poor performance for both commercial and research systems. We then give an error analysis that suggests research challenges for entity extraction that are neither boring nor solved. 1 Background Entity extraction or named entity recognition, as it is sometimes called, is a known and familiar prob- Named entity tagging has been the subject of numerous shared-task evaluations, including seminal and task, the and With this track record, and with commercial vendors now selling named-entity tagging for a fee, many naturally consider entity extraction to be an essentially solved problem. The present paper challenges this view. main issue, as we see it, is transfer: taggers developed for a specific corpus tend not to perform well on other data sets. Kosseim and Poibeau (2001), for one, show that the informal language of email or speech transcriptions befudtaggers built for journalistic text. Minkov further explore the systematic differences between journalistic and informal texts, training separate taggers for each text source of interest. Because named entity taggers are so strongly based on surface features, it isn’t surprising to observe poor tagger transfer across texts with significantly different styles or with unrelated content. In this paper, we report on the more surprising result that transfer issues arise even for texts with closely aligned content or closely aligned styles. In particular, we consider a range of primarily business-related texts that are, on the face of it, close in style and/or substance to the journalistic in existing sets, in particular. We thus would have expected these texts to support good transfer performance from taggers conto the Instead, we found the same kinds of performance drops as Kosseim and Poibeau had noted for informal texts. Our aim here is to shed light on the how and why of this. 2 Scope of the present study We begin with a disclaimer. Our goal is not so to present new technical solutions to recognition, as to draw attention to those aspects of the problem that remain unsolved. We cover two main thrusts: (i) a black-box evaluation of several (commercial and research systems); and (ii) an error analysis of system performance. data Our evaluation data set contains three distinct sections. The largest component consists of publiclyavailable financial reports filed with the Securities Exchange Commission in particular the 2003 forms 10-K filed by eight Fortune 500 companies. These corporate annual reports share the same subject matter as much business news: sales, profits, acquisitions, business strategies and the like. They take, however, a more technical slant and are rich in accounting jargon. They are also longer, ranging in our study from 22 to 54 pages.</abstract>
<note confidence="0.446512625">181 of NAACL HLT 2007, Companion pages NY, April 2007. Association for Computational Linguistics Pocahontas Rule-based Belle Rule-based Jasmine Mulan Ariel Rule-based, 10-K tuning</note>
<abstract confidence="0.997107608695652">1: pseudonyms. exploration with our own showed these to be particularly hard to tag. Because their sheer length and technical emphasis seemed implicated in this poor performance, we assembled a second corpus of forty Web-hosted business stories from such news proas and These stories focus on the same eight companies as our 10-K data set, but are shorter and less technical, thus allowing us to isolate length and technicality as factors in tagging business texts. The final portion of our test set consists of ten news stories that were selected to closely match the of data used in past They drawn from the New York Times and Street Journal on-line editions, and focus on current events, thus providing one more dimension of systems Five systems participated in our study, representing a range of commercial tools and research prototypes. Two of these are state-of-the-art handbuilt systems based on rule/pattern interpreters. Two are open-source statistical systems, one based and the other on both were trained the data set. The final system is our own tagger, noted as Table 1. Except as noted below, all the systems were run out of the box, with no adaptation to the data. License and privacy concerns prevent us from identifying all the systems; instead this paper reports most results anonymously, using the names of Disney heroines as system pseudonyms. We have, however exposed the identity of our own system out of fairness, as it benefited somewhat earlier tuning to 10-K. method We attempted to replicate the procedure used in the extending it only as required by will make the non-copyrighted part of our corpus (the 10-Ks) available to other researchers. the characteristics of the taggers. The test data formatted as in and where ran afoul of system we the data manually, resolving, crossing tags that may have strayed into the output. To provide scores that could be compared with we created answer keys (Sundheim, 1995), and remapped system output to this standard. We removed system responses that were considered non-taggable in and conflated fine-grained disnot made in remapping counto Scores were assessed with the which provides partial credit for system responses that match the key in type but not extent, or vice-versa. The scorer also provides a full error analysis, separately characterizing each error in a system response. 3 Findings Table 2, overleaf, presents our overall findings, aggregated across the three primary entity types: and in the We generally did not the TIMEX and because: (i) neither of the statistical systems generate them; (ii) those systems that do generate them tend to do well; (iii) they are more frequent in the than in news, thus skewing results. For completeness’ sake, however, Table 2 does provide all-entity news scores in parentheses for those systems that to generate the full set of entities. Turning now to actual performance measurements, Table 2 does not present an especially pretty picture. Aside from two systems’ runs on current events, all the scores are substantially below those obtained by competitive which typically reached F scores in mid-90s, with a high of F=96 at worst performances were turned in as shown in the first block of rows in Table 2. While precision is generally poor, recall is even worse. One reason for this is the very frequent rightwards shortenings of company names from Corporation in to the leftwards shortening fain news texts. been tuned to tag all these cases, but the other systems only tagged a scattershot fraction. To isolate the contribution of 182</abstract>
<author confidence="0.455217">Pocahontas Belle Jasmine Mulan Ariel</author>
<note confidence="0.8958471">R=58 R=28 R=50 R=50 R=71 P=65 P=52 P=43 P=56 P=79 F=61.1 F=36.4 F=42.7 F=52.6 F=74.5 “the Corp.” optional R=71 R=36 R=55 R=60 R=71 P=65 P=52 P=40 P=56 P=79 F=68.0 F=42.8 F=46.2 F=57.9 F=74.7 Business news R=80 (82) R=64 (69) R=76 R=65 R=71 (75) P=80 (79) P=86 (83) P=63 P=74 P=74 (75) F=80.1 (81) F=73.5 (75) F=69.1 F=69.2 F=72.3 (75) Current events R=94 (94) R=59 (63) R=79 R=79 R=89 (91)</note>
<phone confidence="0.4776385">P=94 (93) P=82 (80) P=70 P=92 P=91 (92) F=94.3 (94) F=68.5 (71) F=74.5 F=84.9 F=90.4 (92)</phone>
<abstract confidence="0.991725267605634">2: extraction scores, unless parenthesized (in parens = all entities). these cases to system recall error, we recalculated the scores by making the cases optional. The scorer removes missing optional responses from the recall denominator, and as expected recall improved; see the second block in Table 2. news. most consistent performance across systems was achieved with business news, with scores ranging in F=69-80. This is a huge improvement over the gaping F=36-75 range saw with (F=43-75 with optional short names). This confirms that length and financial jargon are implicated in the poor performance on forms 10-K. Nonetheless, these improved scores are still 15-20 points lower than the better Is business language just hard to tag? news. attempt to replicate the data yields an equivocal answer. systems achieved scores; it may not be coincidental that both are next-generation versions of systems that at Of the other systems, showed substantial improvement going from business news to current events. While it is good news that three of the systems were explicitly trained on or did well on data, it is disquieting to see how poorly this training generalized to other news texts. 4 Factors affecting performance A finer analysis of our three data sets helps triangulate the factors leading to the systematic performance differences shown in Table 2. of organizations. factor especially stands out: as Table 3 shows, organizations are twice as prevalent in the business sources as in data. As organization scores generally trail scores for persons and locations (Table 4), this partly explains why business texts are hard. of organizations. that does not explain it all. The profiles in Figure 1 show that current events favor government/quasi-government They are less linguistically productive than the corporate and quasi-corporate names in business texts, and so are more amenable to being explicitly listed in name Florian al note the effectiveof gazetteers for tagging the standards. business news data reflect a growing portion of Web-hosted texts that relax the journalistic editorial rules of traditional sources such as the For instance, our data show the same frequent omission corporate designators that Kosseim noted in informal text. Whereas news sources of record will generally mention a company’s designator at least once in a story, our business data frequently fail to do so at all, thus removing a key cue. By tracing the base, we found that the absence of any designator was implicated in 81% of the system’s recall error for organization names. taggers often overcome this kind of missing evidence by second-passing a text, propagating name mentions identified in the first SEC Business MUC Org 70% 65% 29% Per 9% 23% 35% Loc 21% 12% 36% 3: distribution of entity types 183</abstract>
<author confidence="0.839286">Belle Jasm Mul Ariel</author>
<note confidence="0.9845801">S org F=62 F=10 F=46 F=53 F=83 opt F=74 F=14 F=52 F=61 F=83 S per F=75 F=65 F=49 F=64 F=60 S loc F=79 F=77 F=49 F=74 F=78 B org F=77 F=72 F=70 F=63 F=66 B. per F=90 F=85 F=70 F=69 F=79 B. loc F=78 F=76 F=59 F=75 F=73 M org F=90 F=58 F=48 F=80 F=80 M per F=99 F=90 F=84 F=81 F=92 M loc F=98 F=81 F=74 F=89 F=95</note>
<abstract confidence="0.990181814814815">4: subcores pass to matching but undetected mentions (Mikheev, 1999). This strategy runs foul, though, when the first pass produces precision errors, as these too can get propagated. Document length is implicated in this through the greater cumulative likelihood of making an error on the first pass and of finding a mention that matches the error on the second pass. and non-names. final factor that especially afflicts the Forms 10-K is the similarity of names and non-names. Non-taggable names often look like legitimate subsidiaries, while valid operating diviare often hard to distinguish from generic designations of market segments. 5 Implications for further research. What surprised us most in conducting this study was to find so obvious a transfer gap among what appear to be very similar text sources. We were also surprised by the involvement in this of relaxed editorial standards around seeming trivia (like the This suggests, for one, that current techniques remain too dependent on skin-deep word co-occurrence features. It also suggests that the editorially pristine news texts used in so much may be atypically easy to tag. While name-tagging programs may struggle with editorially informal texts, the absence of surface contextual cues poses no noticeable challenge to human readers. What cues are left, and there are many, are semantic in nature: predicate-argument structure, selectional restrictions, organization of lexicon, efforts to create common propositional banks and lexical ontologies may thus have much to offer. Indeed, current research in these areas is just beginning to trickle down to the name-tagging problem (Mohit &amp; Hwa, 2005). Another key issue is ensuring tagging coherency at the whole-document level. This might help alleviate the kind of error propagation with dualstrategies that particularly afflicts long docu- Recent applications of statistical reference models are beginning to show promise 2005; Ji &amp; Grishman, 2005). Lastly, we can see this whole study as a particular challenge case for transfer learning, and indeed such work as Sutton and McCallum’s (2005) has looked at the name-tagging task from a transfer learning standpoint. It may thus be that today’s exciting emerging work in “unsolved” areas – semantics, reference, and learning – could come to play a key role in what is sometimes maligned as yesterday’s boring solved problem.</abstract>
<note confidence="0.767597875">References Finkel J R, Grenager T, Manning C (2005). Incorporating non-local information into information extraction by Gibbs sampling, Ann Arbor. Florian R, Ittycheriah A, Jing H, Zhang T (2003). Named entity recognition through classifier combina- Edmonton. Ji H, Grishman R (2005). Improving name tagging by</note>
<abstract confidence="0.704205916666667">resolution and relation detection, Kosseim L, Poibeau T (2001). Extraction de noms pro- ˆ partir de textes varies. Mikheev A, Moens M, Grover C (1999). Named entity without gazetteers. Bergen. Minkov E, Wang R, Cohen W (2005). Extracting pernames from email. Vancouver. Mohit B, Hwa R (2005) Syntax-based semi-supervised entity tagging. Ann Arbor B (1995), ed. Columbia C, McCallum A (2005). Composition of for learning, Vancouver.</abstract>
<intro confidence="0.371255">184</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling,</title>
<date>2005</date>
<booktitle>Proc ACL,</booktitle>
<location>Ann Arbor.</location>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Finkel J R, Grenager T, Manning C (2005). Incorporating non-local information into information extraction systems by Gibbs sampling, Proc ACL, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>T Zhang</author>
</authors>
<title>Named entity recognition through classifier combination, Proc CoNLL,</title>
<date>2003</date>
<location>Edmonton.</location>
<contexts>
<context position="10900" citStr="Florian et al (2003)" startWordPosition="1762" endWordPosition="1765">tands out: as Table 3 shows, organizations are twice as prevalent in the business sources as in the MUC-like data. As organization scores generally trail scores for persons and locations (Table 4), this partly explains why business texts are hard. Kinds of organizations. But that does not explain it all. The profiles in Figure 1 show that current events favor government/quasi-government names (e.g.,“Congress,” “Hamas”). They are less linguistically productive than the corporate and quasi-corporate names in business texts, and so are more amenable to being explicitly listed in name gazetteers. Florian et al (2003) note the effectiveness of gazetteers for tagging the CoNLL corpus. Editorial standards. Our business news data reflect a growing portion of Web-hosted texts that relax the journalistic editorial rules of traditional news sources such as the NYT or WSJ. For instance, our data show the same frequent omission of corporate designators (e.g. “inc.”) that Kosseim noted in informal text. Whereas news sources of record will generally mention a company’s designator at least once in a story, our business data frequently fail to do so at all, thus removing a key name-tagging cue. By tracing the Ariel ru</context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>Florian R, Ittycheriah A, Jing H, Zhang T (2003). Named entity recognition through classifier combination, Proc CoNLL, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
</authors>
<title>Improving name tagging by reference resolution and relation detection,</title>
<date>2005</date>
<booktitle>Proc ACL.</booktitle>
<marker>Ji, Grishman, 2005</marker>
<rawString>Ji H, Grishman R (2005). Improving name tagging by reference resolution and relation detection, Proc ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kosseim</author>
<author>T Poibeau</author>
</authors>
<title>Extraction de noms propres ˆ partir de textes varies.</title>
<date>2001</date>
<booktitle>Proc. TALN,</booktitle>
<location>Toulouse.</location>
<contexts>
<context position="1439" citStr="Kosseim and Poibeau (2001)" startWordPosition="224" endWordPosition="227">es called, is a known and familiar problem. Named entity (NE) tagging has been the subject of numerous shared-task evaluations, including the seminal MUC 6, MUC 7 and MET evaluations, the CoNLL shared task, the SIGHAN bake-offs, and the ACE evaluations. With this track record, and with commercial vendors now selling named-entity tagging for a fee, many naturally consider entity extraction to be an essentially solved problem. The present paper challenges this view. The main issue, as we see it, is transfer: NE taggers developed for a specific corpus tend not to perform well on other data sets. Kosseim and Poibeau (2001), for one, show that the informal language of email or speech transcriptions befuddles taggers built for journalistic text. Minkov et al (2005) further explore the systematic differences between journalistic and informal texts, training separate taggers for each text source of interest. Because named entity taggers are so strongly based on surface features, it isn’t surprising to observe poor tagger transfer across texts with significantly different styles or with unrelated content. In this paper, we report on the more surprising result that transfer issues arise even for texts with closely al</context>
</contexts>
<marker>Kosseim, Poibeau, 2001</marker>
<rawString>Kosseim L, Poibeau T (2001). Extraction de noms propres ˆ partir de textes varies. Proc. TALN, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
<author>M Moens</author>
<author>C Grover</author>
</authors>
<title>Named entity recognition without gazetteers.</title>
<date>1999</date>
<booktitle>Proc. EACL,</booktitle>
<location>Bergen.</location>
<marker>Mikheev, Moens, Grover, 1999</marker>
<rawString>Mikheev A, Moens M, Grover C (1999). Named entity recognition without gazetteers. Proc. EACL, Bergen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Minkov</author>
<author>R Wang</author>
<author>W Cohen</author>
</authors>
<title>Extracting personal names from email.</title>
<date>2005</date>
<booktitle>Proc HLT/EMNLP,</booktitle>
<location>Vancouver.</location>
<contexts>
<context position="1582" citStr="Minkov et al (2005)" startWordPosition="247" endWordPosition="250">MUC 6, MUC 7 and MET evaluations, the CoNLL shared task, the SIGHAN bake-offs, and the ACE evaluations. With this track record, and with commercial vendors now selling named-entity tagging for a fee, many naturally consider entity extraction to be an essentially solved problem. The present paper challenges this view. The main issue, as we see it, is transfer: NE taggers developed for a specific corpus tend not to perform well on other data sets. Kosseim and Poibeau (2001), for one, show that the informal language of email or speech transcriptions befuddles taggers built for journalistic text. Minkov et al (2005) further explore the systematic differences between journalistic and informal texts, training separate taggers for each text source of interest. Because named entity taggers are so strongly based on surface features, it isn’t surprising to observe poor tagger transfer across texts with significantly different styles or with unrelated content. In this paper, we report on the more surprising result that transfer issues arise even for texts with closely aligned content or closely aligned styles. In particular, we consider a range of primarily business-related texts that are, on the face of it, cl</context>
</contexts>
<marker>Minkov, Wang, Cohen, 2005</marker>
<rawString>Minkov E, Wang R, Cohen W (2005). Extracting personal names from email. Proc HLT/EMNLP, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Mohit</author>
<author>R Hwa</author>
</authors>
<title>Syntax-based semi-supervised named entity tagging.</title>
<date>2005</date>
<booktitle>Proc ACL, Ann Arbor MI. Sundheim B</booktitle>
<editor>ed.</editor>
<location>Columbia MD.</location>
<contexts>
<context position="14028" citStr="Mohit &amp; Hwa, 2005" startWordPosition="2284" endWordPosition="2287">ristine news texts used in so much NE research may be atypically easy to tag. While name-tagging programs may struggle with editorially informal texts, the absence of surface contextual cues poses no noticeable challenge to human readers. What cues are left, and there are many, are semantic in nature: predicate-argument structure, selectional restrictions, organization of the lexicon, etc. Recent efforts to create common propositional banks and lexical ontologies may thus have much to offer. Indeed, current research in these areas is just beginning to trickle down to the name-tagging problem (Mohit &amp; Hwa, 2005). Another key issue is ensuring tagging coherency at the whole-document level. This might help alleviate the kind of error propagation with dualpass strategies that particularly afflicts long documents. Recent applications of statistical coreference models are beginning to show promise (Finkel et al, 2005; Ji &amp; Grishman, 2005). Lastly, we can see this whole study as a particular challenge case for transfer learning, and indeed such work as Sutton and McCallum’s (2005) has looked at the name-tagging task from a transfer learning standpoint. It may thus be that today’s exciting emerging work in </context>
</contexts>
<marker>Mohit, Hwa, 2005</marker>
<rawString>Mohit B, Hwa R (2005) Syntax-based semi-supervised named entity tagging. Proc ACL, Ann Arbor MI. Sundheim B (1995), ed. Proc MUC-6, Columbia MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
</authors>
<title>McCallum A</title>
<date>2005</date>
<booktitle>Proc HLT/EMNLP,</booktitle>
<location>Vancouver.</location>
<marker>Sutton, 2005</marker>
<rawString>Sutton C, McCallum A (2005). Composition of CRFs for transfer learning, Proc HLT/EMNLP, Vancouver.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>