<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.985811">
Using a maximum entropy model to build segmentation lattices for MT
</title>
<author confidence="0.993388">
Chris Dyer
</author>
<affiliation confidence="0.997198333333333">
Laboratory for Computational Linguistics and Information Processing
Department of Linguistics
University of Maryland
</affiliation>
<address confidence="0.883694">
College Park, MD 20742, USA
</address>
<email confidence="0.789731">
redpony AT umd.edu
</email>
<sectionHeader confidence="0.982012" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995116809523809">
Recent work has shown that translating seg-
mentation lattices (lattices that encode alterna-
tive ways of breaking the input to an MT sys-
tem into words), rather than text in any partic-
ular segmentation, improves translation qual-
ity of languages whose orthography does not
mark morpheme boundaries. However, much
of this work has relied on multiple segmenters
that perform differently on the same input to
generate sufficiently diverse source segmen-
tation lattices. In this work, we describe a
maximum entropy model of compound word
splitting that relies on a few general features
that can be used to generate segmentation lat-
tices for most languages with productive com-
pounding. Using a model optimized for Ger-
man translation, we present results showing
significant improvements in translation qual-
ity in German-English, Hungarian-English,
and Turkish-English translation over state-of-
the-art baselines.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999680039215686">
Compound words pose significant challenges to the
lexicalized models that are currently common in sta-
tistical machine translation. This problem has been
widely acknowledged, and the conventional solu-
tion, which has been shown to work well for many
language pairs, is to segment compounds into their
constituent morphemes using either morphological
analyzers or empirical methods and then to trans-
late from or to this segmented variant (Koehn et al.,
2008; Dyer et al., 2008; Yang and Kirchhoff, 2006).
But into what units should a compound word be
segmented? Taken as a stand-alone task, the goal of
a compound splitter is to produce a segmentation for
some input that matches the linguistic intuitions of a
native speaker of the language. However, there are
often advantages to using elements larger than sin-
gle morphemes as the minimal lexical unit for MT,
since they may correspond more closely to the units
of translation. Unfortunately, determining the op-
timal segmentation is challenging, typically requir-
ing extensive experimentation (Koehn and Knight,
2003; Habash and Sadat, 2006; Chang et al., 2008).
Recent work has shown that by combining a vari-
ety of segmentations of the input into a segmentation
lattice and effectively marginalizing over many dif-
ferent segmentations, translations superior to those
resulting from any single single segmentation of the
input can be obtained (Xu et al., 2005; Dyer et al.,
2008; DeNeefe et al., 2008). Unfortunately, this ap-
proach is difficult to utilize because it requires mul-
tiple segmenters that behave differently on the same
input.
In this paper, we describe a maximum entropy
word segmentation model that is trained to assign
high probability to possibly several segmentations of
an input word. This model enables generation of di-
verse, accurate segmentation lattices from a single
model that are appropriate for use in decoders that
accept word lattices as input, such as Moses (Koehn
et al., 2007). Since our model relies a small num-
ber of dense features, its parameters can be tuned
using very small amounts of manually created ref-
erence lattices. Furthermore, since these parame-
ters were chosen to have valid interpretation across
a variety of languages, we find that the weights esti-
mated for one apply quite well to another. We show
that these lattices significantly improve translation
quality when translating into English from three lan-
guages exhibiting productive compounding: Ger-
man, Turkish, and Hungarian.
The paper is structured as follows. In the next sec-
</bodyText>
<page confidence="0.985668">
406
</page>
<note confidence="0.890511">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 406–414,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999979714285714">
tion, we describe translation from segmentation lat-
tices and give a motivating example, Section 3 de-
scribes our segmentation model and its tuning and
how it is used to generate segmentation lattices, Sec-
tion 5 presents experimental results, Section 6 re-
views relevant related work, and in Section 7 we
conclude and discuss future work.
</bodyText>
<sectionHeader confidence="0.903348" genericHeader="introduction">
2 Segmentation lattice translation
</sectionHeader>
<bodyText confidence="0.999284">
In this section we give a brief overview of lattice
translation and then describe the characteristics of
segmentation lattices that are appropriate for trans-
lation.
</bodyText>
<subsectionHeader confidence="0.975582">
2.1 Lattice translation
</subsectionHeader>
<bodyText confidence="0.981365454545455">
Word lattices have been used to represent ambiguous
input to machine translation systems for a variety of
tasks, including translating automatic speech recog-
nition transcriptions and translating from morpho-
logically complex languages (Bertoldi et al., 2007;
Dyer et al., 2008). The intuition behind using lat-
tices in both approaches is to avoid the error propa-
gation effects that are found when a one-best guess
is used. By carrying a certain amount of uncertainty
forward in the processing pipeline, information con-
tained in the translation models can be leveraged to
help resolve the upstream ambiguity. In our case, we
want to propagate uncertainty about the proper seg-
mentation of a compound forward to the decoder,
which can use its full translation model to select
proper segmentation for translation. Mathemati-
cally, this can be understood as follows: whereas the
goal in conventional machine translation is to find
the sentence ˆeI1 that maximizes Pr(eI1|fJ1 ), the lat-
tice adds a latent variable, the path f¯ from a des-
ignated start start to a designated goal state in the
lattice G:
</bodyText>
<equation confidence="0.9995545">
Pr(eI1|G) (1)
1:
= arg max Pr(eI1 |¯f)Pr(¯f|G) (2)
eI1
¯fE9
Pr(eI1|¯f)Pr(¯f|G) (3)
</equation>
<bodyText confidence="0.966908">
If the transduction formalism used is a synchronous
probabilistic context free grammar or weighted finite
</bodyText>
<figure confidence="0.562107">
tonbandaufnahme
</figure>
<figureCaption confidence="0.74272225">
Figure 1: Segmentation lattice examples. The dotted
structure indicates linguistically implausible segmenta-
tion that might be generated using dictionary-driven ap-
proaches.
</figureCaption>
<listItem confidence="0.747130333333333">
state transducer, the search represented by equation
(3) can be carried out efficiently using dynamic pro-
gramming (Dyer et al., 2008).
</listItem>
<subsectionHeader confidence="0.999792">
2.2 Segmentation lattices
</subsectionHeader>
<bodyText confidence="0.999977130434783">
Figure 1 shows two lattices that encode the
most linguistically plausible ways of segment-
ing two prototypical German compounds with
compositional meanings. However, while these
words are structurally quite similar, translating
them into English would seem to require differ-
ent amounts of segmentation. For example, the
dictionary fragment shown in Table 1 illustrates
that tonbandaufnahme can be rendered into En-
glish by following 3 different paths in the lat-
tice, ton/audio band/tape aufnahme/recording, ton-
band/tape aufnahme/recording, and tonbandauf-
nahme/tape recording. In contrast, wiederaufnahme
can only be translated correctly using the unseg-
mented form, even though in German the meaning
of the full form is a composition of the meaning of
the individual morphemes.1
It should be noted that phrase-based models can
translate multiple words as a unit, and therefore cap-
ture non-compositional meaning. Thus, by default if
the training data is processed such that, for example,
aufnahme, in its sense of recording, is segmented
into two words, then more paths in the lattices be-
</bodyText>
<footnote confidence="0.955217">
1The English word resumption is likewise composed of two
morphemes, the prefix re- and a kind of bound morpheme
that never appears in other contexts (sometimes called a ‘cran-
berry’ morpheme), but the meaning of the whole is idiosyncratic
enough that it cannot be called compositional.
</footnote>
<figure confidence="0.940770285714286">
tonband aufnahme
ton auf nahme
band
wiederaufnahme
wieder aufnahme
wie auf nahme
der
ˆeI1 = arg max
el
1
≈ arg max
el
1
max
</figure>
<page confidence="0.6461755">
¯fE9
407
</page>
<figure confidence="0.951769636363636">
German
tonband aufnahme
ton band
English
on, up, in, at, ...
recording, entry
reel, tape, band
auf
aufnahme
band
the, of the
</figure>
<construct confidence="0.8069975">
took (3P-SG-PST)
sound, audio, clay
tape, audio tape
tape recording
how, like, as
again
</construct>
<figure confidence="0.7012285">
resumption
wiederaufnahme
</figure>
<figureCaption confidence="0.9925862">
Figure 2: Manually created reference lattices for the two
words from Figure 1. Although only a subset of all
linguistically plausible segmentations, each path corre-
sponds to a plausible segmentation for word-for-word
German-English translation.
</figureCaption>
<figure confidence="0.997210875">
der
nahme
ton
tonband
tonbandaufnahme
wie
wieder
wiederaufnahme
</figure>
<tableCaption confidence="0.969549">
Table 1: German-English dictionary fragment for words
present in Figure 1.
</tableCaption>
<bodyText confidence="0.999667043478261">
come plausible translations. However, using a strat-
egy of “over segmentation” and relying on phrase
models to learn the non-compositional translations
has been shown to degrade translation quality sig-
nificantly on several tasks (Xu et al., 2004; Habash
and Sadat, 2006). We thus desire lattices containing
as little oversegmentation as possible.
We have now have a concept of a “gold standard”
segmentation lattice for translation: it should con-
tain all linguistically motivated segmentations that
also correspond to plausible word-for-word transla-
tions into English. Figure 2 shows an example of the
reference lattice for the two words we just discussed.
For the experiments in this paper, we generated a
development and test set by randomly choosing 19
German newspaper articles, identifying all words
greater than 6 characters is length, and segmenting
each word so that the resulting units could be trans-
lated compositionally into English. This resulted in
489 training sentences corresponding to 564 paths
for the dev set (which was drawn from 15 articles),
and 279 words (302 paths) for the test set (drawn
from the remaining 4 articles).
</bodyText>
<sectionHeader confidence="0.9618765" genericHeader="method">
3 A maximum entropy segmentation
model
</sectionHeader>
<bodyText confidence="0.987765121212121">
We now turn to the problem of modeling word seg-
mentation in a way that facilitates lattice construc-
tion. As a starting point, we consider the work
of Koehn and Knight (2003) who observe that in
most languages that exhibit compounding, the mor-
phemes used to construct compounds frequently
also appear as individual tokens. Based on this ob-
servation, they propose a model of word segmenta-
tion that splits compound words into pieces found
in the dictionary based on a variety heuristic scoring
criteria. While these models have been reasonably
successful (Koehn et al., 2008), they are problem-
atic for two reasons. First, there is no principled way
to incorporate additional features (such as phonotac-
tics) which might be useful to determining whether
a word break should occur. Second, the heuristic
scoring offers little insight into which segmentations
should be included in a lattice.
We would like our model to consider a wide vari-
ety of segmentations of any word (including perhaps
hypothesized morphemes that are not in the dictio-
nary), to make use of a rich set of features, and to
have a probabilistic interpretation of each hypothe-
sized split (to incorporate into the downstream de-
coder). We decided to use the class of maximum
entropy models, which are probabilistically sound,
can make use of possibly many overlapping features,
and can be trained efficiently (Berger et al., 1996).
We thus define a model of the conditional proba-
bility distribution Pr(sN1 Iw), where w is a surface
form and sN1 is the segmented form consisting of N
segments as:
Nexp Ei λihi(s1 , w) (4 )
</bodyText>
<equation confidence="0.830111">
Pr(s1 |w) = Es, exp Ei λihi(s&apos;, w)
</equation>
<bodyText confidence="0.999105">
To simplify inference and to make the lattice repre-
sentation more natural, we only make use of local
feature functions that depend on properties of each
segment:
</bodyText>
<page confidence="0.909377">
408
</page>
<equation confidence="0.7725195">
XPr(sN1 |w) oc exp
i
</equation>
<subsectionHeader confidence="0.899897">
3.1 From model to segmentation lattice
</subsectionHeader>
<bodyText confidence="0.99999644">
The segmentation model just introduced is equiva-
lent to a lattice where each vertex corresponds to
a particular coverage (in terms of letters consumed
from left to right) of the input word. Since we only
make use of local features, the number of vertices
in a lattice for word w is |w |− m, where m is the
minimum segment length permitted. In all experi-
ments reported in this paper, we use m = 3. Each
edge is labeled with a morpheme s (corresponding
to the morpheme associated with characters delim-
ited by the start and end nodes of the edge) as well
as a weight, Pi Aihi(s, w). The cost of any path
from the start to the goal vertex will be equal to the
numerator in equation (4). The value of the denomi-
nator can be computed using the forward algorithm.
In most of our experiments, s will be identical
to the substring of w that the edge is designated to
cover. However, this is not a requirement. For exam-
ple, German compounds frequently have so-called
Fugenelemente, one or two characters that “glue
together” the primary morphemes in a compound.
Since we permit these characters to be deleted, then
an edge where they are deleted will have fewer char-
acters than the coverage indicated by the edge’s
starting and ending vertices.
</bodyText>
<subsectionHeader confidence="0.999922">
3.2 Lattice pruning
</subsectionHeader>
<bodyText confidence="0.999978866666667">
Except for the minimum segment length restriction,
our model defines probabilities for all segmentations
of an input word, making the resulting segmenta-
tion lattices are quite large. Since large lattices
are costly to deal with during translation (and may
lead to worse translations because poor segmenta-
tions are passed to the decoder), we prune them us-
ing forward-backward pruning so as to contain just
the highest probability paths (Sixtus and Ortmanns,
1999). This works by computing the score of the
best path passing through every edge in the lattice
using the forward-backward algorithm. By finding
the best score overall, we can then prune edges us-
ing a threshold criterion; i.e., edges whose score is
some factor α away from the global best edge score.
</bodyText>
<subsectionHeader confidence="0.996017">
3.3 Maximum likelihood training
</subsectionHeader>
<bodyText confidence="0.999960222222222">
Our model defines a conditional probability distribu-
tion over virtually all segmentations of a word w. To
train our model, we wish to maximize the likelihood
of the segmentations contained in the reference lat-
tices by moving probability mass away from the seg-
mentations that are not in the reference lattice. Thus,
we wish to minimize the following objective (which
can be computed using the forward algorithm over
the unpruned hypothesis lattices):
</bodyText>
<equation confidence="0.8068248">
XL = − log X p(S|wi) (6)
i SETZi
The gradient with respect to the feature weights for
a log linear model is simply:
Ep($|wi)[hk] − Ep($|wi,TZi)[hk] (7)
</equation>
<bodyText confidence="0.999751444444445">
To compute these values, the first expectation is
computed using forward-backward inference over
the full lattice. To compute the second expecta-
tion, the full lattice is intersected with the reference
lattice Ri, and then forward-backward inference
is redone.2 We use the standard quasi-Newtonian
method L-BFGS to optimize the model (Liu et al.,
1989). Training generally converged in only a few
hundred iterations.
</bodyText>
<subsectionHeader confidence="0.726062">
3.3.1 Training to minimize 1-best error
</subsectionHeader>
<bodyText confidence="0.999943181818182">
In some cases, such as when performing word
alignment for translation model construction, lat-
tices cannot be used easily. In these cases, a 1-
best segmentation (which can be determined from
the lattice using the Viterbi algorithm) may be de-
sired. To train the parameters of the model for this
condition (which is arguably slightly different from
the lattice generation case we just considered), we
used the minimum error training (MERT) algorithm
on the segmentation lattices to find the parameters
that minimized the error on our dev set (Macherey
</bodyText>
<footnote confidence="0.906074">
2The second expectation corresponds to the empirical fea-
ture observations in a standard maximum entropy model. Be-
cause this is an expectation and not an invariant observation,
the log likelihood function is not guaranteed to be concave and
the objective surface may have local minima. However, exper-
imentation revealed the optimization performance was largely
invariant with respect to its starting point.
</footnote>
<equation confidence="0.643677">
hi(sj, w) (5)
Ai
XN
j
X=
i
∂L
∂Ak
</equation>
<page confidence="0.75928">
409
</page>
<figure confidence="0.998576">
de-only neutral
-3.55 –
-3.13 -3.31
3.06 3.64
-1.58 -2.11
1.18 2.04
-0.9 -0.79
-0.88 -1.09
-0.76 –
-0.66 -1.18
-0.51 -0.82
-0.32 -0.36
-0.26 -0.45
</figure>
<bodyText confidence="0.998201">
et al., 2008). The error function we used was WER
(the minimum number of insertions, substitutions,
and deletions along any path in the reference lattice,
normalized by the length of this path). The WER on
the held-out test set for a system tuned using MERT
is 9.9%, compared to 11.1% for maximum likeli-
hood training.
</bodyText>
<subsectionHeader confidence="0.610188">
3.4 Features
</subsectionHeader>
<bodyText confidence="0.9808066">
We remark that since we did not have the resources
to generate training data in all the languages we
wished to generate segmentation lattices for, we
have confined ourselves to features that we expect to
be reasonably informative for a broad class of lan-
guages. A secondary advantage of this is that we
used denser features than are often used in maxi-
mum entropy modeling, meaning that we could train
our model with relatively less training data than
might otherwise be required.
The features we used in our compound segmen-
tation model for the experiments reported below are
shown in Table 2. Building on the prior work that
relied heavily on the frequency of the hypothesized
constituent morphemes in a monolingual corpus, we
included features that depend on this value, f(si).
|si |refers to the number of letters in the ith hypothe-
sized segment. Binary predicates evaluate to 1 when
true and 0 otherwise. f(si) is the frequency of the
token si as an independent word in a monolingual
corpus. p(#|si1 · · · si4) is the probability of a word
start preceding the letters si1 · · · si4. We found it
beneficial to include a feature that was the probabil-
ity of a certain string of characters beginning a word,
for which we used a reverse 5-gram character model
and predicted the word boundary given the first five
letters of the hypothesized word split.3 Since we did
have expertise in German morphology, we did build
a special German model. For this, we permitted the
strings s, n, and es to be deleted between words.
Each deletion fired a count feature (listed as fugen
in the table). Analysis of errors indicated that the
segmenter would periodically propose an incorrect
segmentation where a single word could be divided
into a word and a nonword consisting of common in-
3In general, this helped avoid situations where a word may
be segemented into a frequent word and then a non-word string
of characters since the non-word typically violated the phono-
tactics of the language in some way.
Feature
</bodyText>
<equation confidence="0.840642">
†si E N
f(si) &gt; 0.005
f(si) &gt; 0
log p(#|si1si2si3si4)
segment penalty
|si |&gt; 12
oov
†fugen
|si |&lt; 4
|si |&lt; 10, f(si) &gt; 2−10
log f(si)
2−10 &lt; f(si) &lt; 0.005
</equation>
<tableCaption confidence="0.9603425">
Table 2: Features and weights learned by maximum like-
lihood training, sorted by weight magnitude.
</tableCaption>
<bodyText confidence="0.999466">
flectional suffixes. To address this, an additional fea-
ture was added that fired when a proposed segment
was one of a set N of 30 nonwords that we saw quite
frequently. The weights shown in Table 2 are those
learned by maximum likelihood training on models
both with and without the special German features,
which are indicated with †.
</bodyText>
<sectionHeader confidence="0.979087" genericHeader="method">
4 Model evalatuion
</sectionHeader>
<bodyText confidence="0.9999911875">
To give some sense of the performance of the model
in terms of its ability to generate lattices indepen-
dently of a translation task, we present precision and
recall of segmentations for pruning parameters (cf.
Section 3.2) ranging from α = 0 to α = 5. Pre-
cision measures the number of paths in the hypoth-
esized lattice that correspond to paths in the refer-
ence lattice; recall measures the number of paths in
the reference lattices that are found in the hypothesis
lattice. Figure 3 shows the effect of manipulating the
density parameter on the precision and recall of the
German lattices. Note that very high recall is possi-
ble; however, the German-only features have a sig-
nificant impact, especially on recall, because the ref-
erence lattices include paths where Fugenelemente
have been deleted.
</bodyText>
<sectionHeader confidence="0.986635" genericHeader="method">
5 Translation experiments
</sectionHeader>
<bodyText confidence="0.999611333333333">
We now review experiments using segmentation lat-
tices produced by the segmentation model we just
introduced in German-English, Hungarian-English,
</bodyText>
<page confidence="0.993804">
410
</page>
<figure confidence="0.823437">
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Precision
</figure>
<figureCaption confidence="0.9973815">
Figure 3: The effect of the lattice density parameter on
precision and recall.
</figureCaption>
<bodyText confidence="0.9886285">
and Turkish-English translation tasks and then show
results elucidating the effect of the lattice density pa-
rameter. We begin with a description of our MT sys-
tem.
</bodyText>
<subsectionHeader confidence="0.9995">
5.1 Data preparation and system description
</subsectionHeader>
<bodyText confidence="0.999591692307692">
For all experiments, we used a 5-gram English lan-
guage model trained on the AFP and Xinua por-
tions of the Gigaword v3 corpus (Graff et al., 2007)
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995). The training, development, and test
data for German-English and Hungarian-English
systems used were distributed as part of the 2009
EACL Workshop on Machine Translation,4 and the
Turkish-English data corresponds to the training and
test sets used in the work of Oflazer and Durgar El-
Kahlout (2007). Corpus statistics for all language
pairs are summarized in Table 3. We note that in all
language pairs, the 1BEST segmentation variant of
the training data results in a significant reduction in
types.
Word alignment was carried out by running
Giza++ implementation of IBM Model 4 initialized
with 5 iterations of Model 1, 5 of the HMM aligner,
and 3 iterations of Model 4 (Och and Ney, 2003)
in both directions and then symmetrizing using the
grow-diag-final-and heuristic (Koehn et al.,
2003). For each language pair, the corpus was
aligned twice, once in its non-segmented variant and
once using the single-best segmentation variant.
For translation, we used a bottom-up parsing de-
coder that uses cube pruning to intersect the lan-
</bodyText>
<page confidence="0.629495">
4http://www.statmt.org/wmt09
</page>
<bodyText confidence="0.999012842105263">
guage model with the target side of the synchronous
grammar. The grammar rules were extracted from
the word aligned parallel corpus and scored as de-
scribed in Chiang (2007). The features used by the
decoder were the English language model log prob-
ability, log f(¯e |¯f), the ‘lexical translation’ log prob-
abilities in both directions (Koehn et al., 2003), and
a word count feature. For the lattice systems, we
also included the unnormalized log p(¯f|!9), as it is
defined in Section 3, as well as an input word count
feature. The feature weights were tuned on a held-
out development set so as to maximize an equally
weighted linear combination of BLEU and 1-TER
(Papineni et al., 2002; Snover et al., 2006) using the
minimum error training algorithm on a packed for-
est representation of the decoder’s hypothesis space
(Macherey et al., 2008). The weights were indepen-
dently optimized for each language pair and each ex-
perimental condition.
</bodyText>
<subsectionHeader confidence="0.999921">
5.2 Segmentation lattice results
</subsectionHeader>
<bodyText confidence="0.997616037037037">
In this section, we report the results of an experiment
to see if the compound lattices constructed using our
maximum entropy model yield better translations
than either an unsegmented baseline or a baseline
consisting of a single-best segmentation.
For each language pair, we define three condi-
tions: BASELINE, 1BEST, and LATTICE. In the
BASELINE condition, a lowercased and tokenized
(but not segmented) version of the test data is
translated using the grammar derived from a non-
segmented training data. In the 1BEST condition,
the single best segmentation ˆsN1 that maximizes
Pr(sN1 Iw) is chosen for each word using the MERT-
trained model (the German model for German, and
the language-neutral model for Hungarian and Turk-
ish). This variant is translated using a grammar
induced from a parallel corpus that has also been
segmented according to the same decision rule. In
the LATTICE condition, we constructed segmenta-
tion lattices using the technique described in Sec-
tion 3.1. For all languages pairs, we used d = 2 as
the pruning density parameter (which corresponds to
the highest F-score on the held out test set). Addi-
tionally, if the unsegmented form of the word was
removed from the lattice during pruning, it was re-
stored to the lattice with zero weight.
Table 4 summarizes the results of the translation
</bodyText>
<figure confidence="0.994271461538462">
0.99
0.98
0.97
0.96
0.95
0.94
0.93
0.92
1
ML
MERT
ML, no special German
Recall
</figure>
<page confidence="0.991391">
411
</page>
<table confidence="0.999371">
f-tokens f-types e-tokens. e-types
DE-BASELINE 38M 307k 40M 96k
DE-1BEST 40M 136k ” ”
HU-BASELINE 25M 646k 29M 158k
HU-1BEST 27M 334k ” ”
TR-BASELINE 1.0M 56k 1.3M 23k
TR-1BEST 1.1M 41k ” ”
</table>
<tableCaption confidence="0.996669">
Table 3: Training corpus statistics.
</tableCaption>
<table confidence="0.9998619">
BLEU TER
DE-BASELINE 21.0 60.6
DE-1BEST 20.7 60.1
DE-LATTICE 21.6 59.8
HU-BASELINE 11.0 71.1
HU-1BEST 10.7 70.4
HU-LATTICE 12.3 69.1
TR-BASELINE 26.9 61.0
TR-1BEST 27.8 61.2
TR-LATTICE 28.7 59.6
</table>
<tableCaption confidence="0.95019875">
Table 4: Translation results for German (DE)-English,
Hungarian (HU)-English, and Turkish (TR)-English.
Scores were computed using a single reference and are
case insensitive.
</tableCaption>
<bodyText confidence="0.999931235294118">
experiments comparing the three input variants. For
all language pairs, we see significant improvements
in both BLEU and TER when segmentation lattices
are used.5 Additionally, we also confirmed previous
findings that showed that when a large amount of
training data is available, moving to a one-best seg-
mentation does not yield substantial improvements
(Yang and Kirchhoff, 2006). Perhaps most surpris-
ingly, the improvements observed when using lat-
tices with the Hungarian and Turkish systems were
larger than the corresponding improvement in the
German system, but German was the only language
for which we had segmentation training data. The
smaller effect in German is probably due to there be-
ing more in-domain training data in the German sys-
tem than in the (otherwise comparably sized) Hun-
garian system.
</bodyText>
<footnote confidence="0.767735666666667">
5Using bootstrap resampling (Koehn, 2004), the improve-
ments in BLEU, TER, as well as the linear combination used in
tuning are statistically significant at at least P &lt; .05.
</footnote>
<bodyText confidence="0.999808230769231">
Targeted analysis of the translation output shows
that while both the 1BEST and LATTICE systems
generally produce adequate translations of com-
pound words that are out of vocabulary in the BASE-
LINE system, the LATTICE system performs bet-
ter since it recovers from infelicitous splits that the
one-best segmenter makes. For example, one class
of error we frequently observe is that the one-best
segmenter splits an OOV proper name into two
pieces when a portion of the name corresponds to a
known word in the source language (e.g. tom tan-
credo→tom tan credo which is then translated as
tom tan belief).6
</bodyText>
<subsectionHeader confidence="0.998765">
5.3 The effect of the density parameter
</subsectionHeader>
<bodyText confidence="0.9997014">
Figure 4 shows the effect of manipulating the den-
sity parameter (cf. Section 3.2) on the performance
and decoding time of the Turkish-English transla-
tion system. It further confirms the hypothesis that
increased diversity of segmentations encoded in a
segmentation lattice can improve translation perfor-
mance; however, it also shows that once the den-
sity becomes too great, and too many implausible
segmentations are included in the lattice, translation
quality will be harmed.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.9999105">
Aside from improving the vocabulary coverage of
machine translation systems (Koehn et al., 2008;
Yang and Kirchhoff, 2006; Habash and Sadat,
2006), compound word segmentation (also referred
to as decompounding) has been shown to be help-
ful in a variety of NLP tasks including mono- and
</bodyText>
<footnote confidence="0.999333">
6We note that our maximum entropy segmentation model
could easily address this problem by incorporating information
about whether a word is likely to be a named entity as a feature.
</footnote>
<page confidence="0.990391">
412
</page>
<figure confidence="0.5728465">
1 1.5 2 2.5 3 3.5
Segmentation lattice density
</figure>
<figureCaption confidence="0.998709">
Figure 4: The effect of the lattice density parameter on
translation quality and decoding time.
</figureCaption>
<bodyText confidence="0.999694285714286">
crosslingual IR (Airio, 2006) and speech recognition
(Hessen and Jong, 2003). A number of researchers
have demonstrated the value of using lattices to en-
code segmentation alternatives as input to a machine
translation system (Dyer et al., 2008; DeNeefe et al.,
2008; Xu et al., 2004), but this is the first work to
do so using a single segmentation model. Another
strand of inquiry that is closely related is the work on
adjusting the source language segmentation to match
the granularity of the target language as a way of im-
proving translation. The approaches suggested thus
far have been mostly of a heuristic nature tailored to
Chinese-English translation (Bai et al., 2008; Ma et
al., 2007).
</bodyText>
<sectionHeader confidence="0.991653" genericHeader="conclusions">
7 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999983868421053">
In this paper, we have presented a maximum entropy
model for compound word segmentation and used it
to generate segmentation lattices for input into a sta-
tistical machine translation system. These segmen-
tation lattices improve translation quality (over an
already strong baseline) in three typologically dis-
tinct languages (German, Hungarian, Turkish) when
translating into English. Previous approaches to
generating segmentation lattices have been quite la-
borious, relying either on the existence of multiple
segmenters (Dyer et al., 2008; Xu et al., 2005) or
hand-crafted rules (DeNeefe et al., 2008). Although
the segmentation model we propose is discrimina-
tive, we have shown that it can be trained using a
minimal amount of annotated training data. Further-
more, when even this minimal data cannot be ac-
quired for a particular language (as was the situa-
tion we faced with Hungarian and Turkish), we have
demonstrated that the parameters obtained in one
language work surprisingly well for others. Thus,
with virtually no cost, this model can be used with a
variety of diverse languages.
While these results are already quite satisfying,
there are a number of compelling extensions to this
work that we intend to explore in the future. First,
unsupervised segmentation approaches offer a very
compelling alternative to the manually crafted seg-
mentation lattices that we created. Recent work
suggests that unsupervised segmentation of inflec-
tional affixal morphology works quite well (Poon et
al., 2009), and extending this work to compounding
morphology should be feasible, obviating the need
for expensive hand-crafted reference lattices. Sec-
ond, incorporating target language information into
a segmentation model holds considerable promise
for inducing more effective translation models that
perform especially well for segmentation lattice in-
puts.
</bodyText>
<sectionHeader confidence="0.997477" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999753">
Special thanks to Kemal Oflazar and Reyyan Yen-
iterzi of Sabancı University for providing the
Turkish-English corpus and to Philip Resnik, Adam
Lopez, Trevor Cohn, and especially Phil Blunsom
for their helpful suggestions. This research was sup-
ported by the Army Research Laboratory. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this paper are those of the authors and do
not necessarily reflect the view of the sponsors.
</bodyText>
<sectionHeader confidence="0.99835" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989661153846154">
Eija Airio. 2006. Word normalization and decompound-
ing in mono- and bilingual IR. Information Retrieval,
9:249–271.
Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.
2008. Improving word alignment by adjusting Chi-
nese word segmentation. In Proceedings of the Third
International Joint Conference on Natural Language
Processing.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39–71.
N. Bertoldi, R. Zens, and M. Federico. 2007. Speech
</reference>
<page confidence="0.525312">
85
</page>
<figure confidence="0.9875174375">
84.8
84.6
84.4
84.2
84
Translation quality
Decoding time
1-(TER-BLEU)/2
16 secs/sentence
14
12
10
8
6
4
2
</figure>
<page confidence="0.996404">
413
</page>
<reference confidence="0.999826204081632">
translation by confusion network decoding. In Pro-
ceeding of ICASSP 2007, Honolulu, Hawaii, April.
Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Man-
ning. 2008. Optimizing Chinese word segmentation
for machine translation performance. In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, Prague, Czech Republic, June.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.
S. DeNeefe, U. Hermjakob, and K. Knight. 2008. Over-
coming vocabulary sparsity in mt using lattices. In
Proceedings of AMTA, Honolulu, HI.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proceedings of HLT-ACL.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2007. English
gigaword third edition.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL, New York.
Arjan Van Hessen and Franciska De Jong. 2003. Com-
pound decomposition in dutch large vocabulary speech
recognition. In Proceedings of Eurospeech 2003, Gen-
eve, pages 225–228.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of IEEE
Internation Conference on Acoustics, Speech, and Sig-
nal Processing, pages 181–184.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proc. of the EACL 2003.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL
2003, pages 48–54, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch Mayne, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Annual Meeting
of the Association for Computation Linguistics (ACL),
Demonstration Session, pages 177–180, June.
Philipp Koehn, Abhishek Arun, and Hieu Hoang. 2008.
Towards better machine translation quality for the
German-English language pairs. In ACL Workshop on
Statistical Machine Translation.
P. Koehn. 2004. Statistical signficiance tests for machine
translation evluation. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 388–395.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-
cedal. 1989. On the limited memory BFGS method
for large scale optimization. Mathematical Program-
ming B, 45(3):503–528.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 304–311,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proceedings of EMNLP, Honolulu, HI.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proceedings
of the Second Workshop on Statistical Machine Trans-
lation, pages 25–32, Prague, Czech Republic, June.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the ACL, pages 311–318.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proc. of NAACL 2009.
S. Sixtus and S. Ortmanns. 1999. High quality word
graphs using forward-backward pruning. In Proceed-
ings of ICASSP, Phoenix, AZ.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAssociation forMachine Translation
in the Americas.
J. Xu, R. Zens, and H. Ney. 2004. Do we need Chi-
nese word segmentation for statistical machine trans-
lation? In Proceedings of the Third SIGHAN Work-
shop on Chinese Language Learning, pages 122–128,
Barcelona, Spain.
J. Xu, E. Matusov, R. Zens, and H. Ney. 2005. Inte-
grated Chinese word segmentation in statistical ma-
chine translation. In Proc. of IWSLT 2005, Pittsburgh.
M. Yang and K. Kirchhoff. 2006. Phrase-based back-
off models for machine translation of highly inflected
languages. In Proceedings of the EACL 2006, pages
41–48.
</reference>
<page confidence="0.998118">
414
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.871422">
<title confidence="0.999959">Using a maximum entropy model to build segmentation lattices for MT</title>
<author confidence="0.956016">Chris</author>
<affiliation confidence="0.977015333333333">Laboratory for Computational Linguistics and Information Department of University of</affiliation>
<address confidence="0.999555">College Park, MD 20742,</address>
<email confidence="0.995439">redponyATumd.edu</email>
<abstract confidence="0.997732772727273">Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of breaking the input to an MT system into words), rather than text in any particular segmentation, improves translation quality of languages whose orthography does not mark morpheme boundaries. However, much of this work has relied on multiple segmenters that perform differently on the same input to generate sufficiently diverse source segmentation lattices. In this work, we describe a maximum entropy model of compound word splitting that relies on a few general features that can be used to generate segmentation lattices for most languages with productive compounding. Using a model optimized for German translation, we present results showing significant improvements in translation quality in German-English, Hungarian-English, and Turkish-English translation over state-ofthe-art baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eija Airio</author>
</authors>
<title>Word normalization and decompounding in mono- and bilingual IR. Information Retrieval,</title>
<date>2006</date>
<pages>9--249</pages>
<contexts>
<context position="26721" citStr="Airio, 2006" startWordPosition="4335" endWordPosition="4336">ary coverage of machine translation systems (Koehn et al., 2008; Yang and Kirchhoff, 2006; Habash and Sadat, 2006), compound word segmentation (also referred to as decompounding) has been shown to be helpful in a variety of NLP tasks including mono- and 6We note that our maximum entropy segmentation model could easily address this problem by incorporating information about whether a word is likely to be a named entity as a feature. 412 1 1.5 2 2.5 3 3.5 Segmentation lattice density Figure 4: The effect of the lattice density parameter on translation quality and decoding time. crosslingual IR (Airio, 2006) and speech recognition (Hessen and Jong, 2003). A number of researchers have demonstrated the value of using lattices to encode segmentation alternatives as input to a machine translation system (Dyer et al., 2008; DeNeefe et al., 2008; Xu et al., 2004), but this is the first work to do so using a single segmentation model. Another strand of inquiry that is closely related is the work on adjusting the source language segmentation to match the granularity of the target language as a way of improving translation. The approaches suggested thus far have been mostly of a heuristic nature tailored </context>
</contexts>
<marker>Airio, 2006</marker>
<rawString>Eija Airio. 2006. Word normalization and decompounding in mono- and bilingual IR. Information Retrieval, 9:249–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Hong Bai</author>
<author>Keh-Jiann Chen</author>
<author>Jason S Chang</author>
</authors>
<title>Improving word alignment by adjusting Chinese word segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="27369" citStr="Bai et al., 2008" startWordPosition="4441" endWordPosition="4444">and Jong, 2003). A number of researchers have demonstrated the value of using lattices to encode segmentation alternatives as input to a machine translation system (Dyer et al., 2008; DeNeefe et al., 2008; Xu et al., 2004), but this is the first work to do so using a single segmentation model. Another strand of inquiry that is closely related is the work on adjusting the source language segmentation to match the granularity of the target language as a way of improving translation. The approaches suggested thus far have been mostly of a heuristic nature tailored to Chinese-English translation (Bai et al., 2008; Ma et al., 2007). 7 Conclusions and future work In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for input into a statistical machine translation system. These segmentation lattices improve translation quality (over an already strong baseline) in three typologically distinct languages (German, Hungarian, Turkish) when translating into English. Previous approaches to generating segmentation lattices have been quite laborious, relying either on the existence of multiple segmenters (Dyer et al., 2008; Xu et al.</context>
</contexts>
<marker>Bai, Chen, Chang, 2008</marker>
<rawString>Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang. 2008. Improving word alignment by adjusting Chinese word segmentation. In Proceedings of the Third International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="10779" citStr="Berger et al., 1996" startWordPosition="1678" endWordPosition="1681">ccur. Second, the heuristic scoring offers little insight into which segmentations should be included in a lattice. We would like our model to consider a wide variety of segmentations of any word (including perhaps hypothesized morphemes that are not in the dictionary), to make use of a rich set of features, and to have a probabilistic interpretation of each hypothesized split (to incorporate into the downstream decoder). We decided to use the class of maximum entropy models, which are probabilistically sound, can make use of possibly many overlapping features, and can be trained efficiently (Berger et al., 1996). We thus define a model of the conditional probability distribution Pr(sN1 Iw), where w is a surface form and sN1 is the segmented form consisting of N segments as: Nexp Ei λihi(s1 , w) (4 ) Pr(s1 |w) = Es, exp Ei λihi(s&apos;, w) To simplify inference and to make the lattice representation more natural, we only make use of local feature functions that depend on properties of each segment: 408 XPr(sN1 |w) oc exp i 3.1 From model to segmentation lattice The segmentation model just introduced is equivalent to a lattice where each vertex corresponds to a particular coverage (in terms of letters consu</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bertoldi</author>
<author>R Zens</author>
<author>M Federico</author>
</authors>
<title>Speech translation by confusion network decoding.</title>
<date>2007</date>
<booktitle>In Proceeding of ICASSP 2007,</booktitle>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="4705" citStr="Bertoldi et al., 2007" startWordPosition="721" endWordPosition="724">ces, Section 5 presents experimental results, Section 6 reviews relevant related work, and in Section 7 we conclude and discuss future work. 2 Segmentation lattice translation In this section we give a brief overview of lattice translation and then describe the characteristics of segmentation lattices that are appropriate for translation. 2.1 Lattice translation Word lattices have been used to represent ambiguous input to machine translation systems for a variety of tasks, including translating automatic speech recognition transcriptions and translating from morphologically complex languages (Bertoldi et al., 2007; Dyer et al., 2008). The intuition behind using lattices in both approaches is to avoid the error propagation effects that are found when a one-best guess is used. By carrying a certain amount of uncertainty forward in the processing pipeline, information contained in the translation models can be leveraged to help resolve the upstream ambiguity. In our case, we want to propagate uncertainty about the proper segmentation of a compound forward to the decoder, which can use its full translation model to select proper segmentation for translation. Mathematically, this can be understood as follow</context>
</contexts>
<marker>Bertoldi, Zens, Federico, 2007</marker>
<rawString>N. Bertoldi, R. Zens, and M. Federico. 2007. Speech translation by confusion network decoding. In Proceeding of ICASSP 2007, Honolulu, Hawaii, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2277" citStr="Chang et al., 2008" startWordPosition="344" endWordPosition="347">off, 2006). But into what units should a compound word be segmented? Taken as a stand-alone task, the goal of a compound splitter is to produce a segmentation for some input that matches the linguistic intuitions of a native speaker of the language. However, there are often advantages to using elements larger than single morphemes as the minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Knight, 2003; Habash and Sadat, 2006; Chang et al., 2008). Recent work has shown that by combining a variety of segmentations of the input into a segmentation lattice and effectively marginalizing over many different segmentations, translations superior to those resulting from any single single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008). Unfortunately, this approach is difficult to utilize because it requires multiple segmenters that behave differently on the same input. In this paper, we describe a maximum entropy word segmentation model that is trained to assign high probability to possibly</context>
</contexts>
<marker>Chang, Jurafsky, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="21161" citStr="Chiang (2007)" startWordPosition="3433" endWordPosition="3434">, 5 of the HMM aligner, and 3 iterations of Model 4 (Och and Ney, 2003) in both directions and then symmetrizing using the grow-diag-final-and heuristic (Koehn et al., 2003). For each language pair, the corpus was aligned twice, once in its non-segmented variant and once using the single-best segmentation variant. For translation, we used a bottom-up parsing decoder that uses cube pruning to intersect the lan4http://www.statmt.org/wmt09 guage model with the target side of the synchronous grammar. The grammar rules were extracted from the word aligned parallel corpus and scored as described in Chiang (2007). The features used by the decoder were the English language model log probability, log f(¯e |¯f), the ‘lexical translation’ log probabilities in both directions (Koehn et al., 2003), and a word count feature. For the lattice systems, we also included the unnormalized log p(¯f|!9), as it is defined in Section 3, as well as an input word count feature. The feature weights were tuned on a heldout development set so as to maximize an equally weighted linear combination of BLEU and 1-TER (Papineni et al., 2002; Snover et al., 2006) using the minimum error training algorithm on a packed forest repr</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S DeNeefe</author>
<author>U Hermjakob</author>
<author>K Knight</author>
</authors>
<title>Overcoming vocabulary sparsity in mt using lattices.</title>
<date>2008</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<location>Honolulu, HI.</location>
<contexts>
<context position="2616" citStr="DeNeefe et al., 2008" startWordPosition="399" endWordPosition="402">he minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Knight, 2003; Habash and Sadat, 2006; Chang et al., 2008). Recent work has shown that by combining a variety of segmentations of the input into a segmentation lattice and effectively marginalizing over many different segmentations, translations superior to those resulting from any single single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008). Unfortunately, this approach is difficult to utilize because it requires multiple segmenters that behave differently on the same input. In this paper, we describe a maximum entropy word segmentation model that is trained to assign high probability to possibly several segmentations of an input word. This model enables generation of diverse, accurate segmentation lattices from a single model that are appropriate for use in decoders that accept word lattices as input, such as Moses (Koehn et al., 2007). Since our model relies a small number of dense features, its parameters can be tuned using v</context>
<context position="26957" citStr="DeNeefe et al., 2008" startWordPosition="4371" endWordPosition="4374">tasks including mono- and 6We note that our maximum entropy segmentation model could easily address this problem by incorporating information about whether a word is likely to be a named entity as a feature. 412 1 1.5 2 2.5 3 3.5 Segmentation lattice density Figure 4: The effect of the lattice density parameter on translation quality and decoding time. crosslingual IR (Airio, 2006) and speech recognition (Hessen and Jong, 2003). A number of researchers have demonstrated the value of using lattices to encode segmentation alternatives as input to a machine translation system (Dyer et al., 2008; DeNeefe et al., 2008; Xu et al., 2004), but this is the first work to do so using a single segmentation model. Another strand of inquiry that is closely related is the work on adjusting the source language segmentation to match the granularity of the target language as a way of improving translation. The approaches suggested thus far have been mostly of a heuristic nature tailored to Chinese-English translation (Bai et al., 2008; Ma et al., 2007). 7 Conclusions and future work In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for </context>
</contexts>
<marker>DeNeefe, Hermjakob, Knight, 2008</marker>
<rawString>S. DeNeefe, U. Hermjakob, and K. Knight. 2008. Overcoming vocabulary sparsity in mt using lattices. In Proceedings of AMTA, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>S Muresan</author>
<author>P Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of HLT-ACL.</booktitle>
<contexts>
<context position="1641" citStr="Dyer et al., 2008" startWordPosition="243" endWordPosition="246">nts in translation quality in German-English, Hungarian-English, and Turkish-English translation over state-ofthe-art baselines. 1 Introduction Compound words pose significant challenges to the lexicalized models that are currently common in statistical machine translation. This problem has been widely acknowledged, and the conventional solution, which has been shown to work well for many language pairs, is to segment compounds into their constituent morphemes using either morphological analyzers or empirical methods and then to translate from or to this segmented variant (Koehn et al., 2008; Dyer et al., 2008; Yang and Kirchhoff, 2006). But into what units should a compound word be segmented? Taken as a stand-alone task, the goal of a compound splitter is to produce a segmentation for some input that matches the linguistic intuitions of a native speaker of the language. However, there are often advantages to using elements larger than single morphemes as the minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Knight, 2003; Habash </context>
<context position="4725" citStr="Dyer et al., 2008" startWordPosition="725" endWordPosition="728"> experimental results, Section 6 reviews relevant related work, and in Section 7 we conclude and discuss future work. 2 Segmentation lattice translation In this section we give a brief overview of lattice translation and then describe the characteristics of segmentation lattices that are appropriate for translation. 2.1 Lattice translation Word lattices have been used to represent ambiguous input to machine translation systems for a variety of tasks, including translating automatic speech recognition transcriptions and translating from morphologically complex languages (Bertoldi et al., 2007; Dyer et al., 2008). The intuition behind using lattices in both approaches is to avoid the error propagation effects that are found when a one-best guess is used. By carrying a certain amount of uncertainty forward in the processing pipeline, information contained in the translation models can be leveraged to help resolve the upstream ambiguity. In our case, we want to propagate uncertainty about the proper segmentation of a compound forward to the decoder, which can use its full translation model to select proper segmentation for translation. Mathematically, this can be understood as follows: whereas the goal </context>
<context position="6052" citStr="Dyer et al., 2008" startWordPosition="933" endWordPosition="936">s a latent variable, the path f¯ from a designated start start to a designated goal state in the lattice G: Pr(eI1|G) (1) 1: = arg max Pr(eI1 |¯f)Pr(¯f|G) (2) eI1 ¯fE9 Pr(eI1|¯f)Pr(¯f|G) (3) If the transduction formalism used is a synchronous probabilistic context free grammar or weighted finite tonbandaufnahme Figure 1: Segmentation lattice examples. The dotted structure indicates linguistically implausible segmentation that might be generated using dictionary-driven approaches. state transducer, the search represented by equation (3) can be carried out efficiently using dynamic programming (Dyer et al., 2008). 2.2 Segmentation lattices Figure 1 shows two lattices that encode the most linguistically plausible ways of segmenting two prototypical German compounds with compositional meanings. However, while these words are structurally quite similar, translating them into English would seem to require different amounts of segmentation. For example, the dictionary fragment shown in Table 1 illustrates that tonbandaufnahme can be rendered into English by following 3 different paths in the lattice, ton/audio band/tape aufnahme/recording, tonband/tape aufnahme/recording, and tonbandaufnahme/tape recording</context>
<context position="26935" citStr="Dyer et al., 2008" startWordPosition="4367" endWordPosition="4370">n a variety of NLP tasks including mono- and 6We note that our maximum entropy segmentation model could easily address this problem by incorporating information about whether a word is likely to be a named entity as a feature. 412 1 1.5 2 2.5 3 3.5 Segmentation lattice density Figure 4: The effect of the lattice density parameter on translation quality and decoding time. crosslingual IR (Airio, 2006) and speech recognition (Hessen and Jong, 2003). A number of researchers have demonstrated the value of using lattices to encode segmentation alternatives as input to a machine translation system (Dyer et al., 2008; DeNeefe et al., 2008; Xu et al., 2004), but this is the first work to do so using a single segmentation model. Another strand of inquiry that is closely related is the work on adjusting the source language segmentation to match the granularity of the target language as a way of improving translation. The approaches suggested thus far have been mostly of a heuristic nature tailored to Chinese-English translation (Bai et al., 2008; Ma et al., 2007). 7 Conclusions and future work In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segm</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing word lattice translation. In Proceedings of HLT-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
<author>J Kong</author>
<author>K Chen</author>
<author>K Maeda</author>
</authors>
<date>2007</date>
<note>English gigaword third edition.</note>
<contexts>
<context position="19869" citStr="Graff et al., 2007" startWordPosition="3227" endWordPosition="3230">iew experiments using segmentation lattices produced by the segmentation model we just introduced in German-English, Hungarian-English, 410 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 Precision Figure 3: The effect of the lattice density parameter on precision and recall. and Turkish-English translation tasks and then show results elucidating the effect of the lattice density parameter. We begin with a description of our MT system. 5.1 Data preparation and system description For all experiments, we used a 5-gram English language model trained on the AFP and Xinua portions of the Gigaword v3 corpus (Graff et al., 2007) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The training, development, and test data for German-English and Hungarian-English systems used were distributed as part of the 2009 EACL Workshop on Machine Translation,4 and the Turkish-English data corresponds to the training and test sets used in the work of Oflazer and Durgar ElKahlout (2007). Corpus statistics for all language pairs are summarized in Table 3. We note that in all language pairs, the 1BEST segmentation variant of the training data results in a significant reduction in types. Word alignment was carried out by runnin</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2007</marker>
<rawString>D. Graff, J. Kong, K. Chen, and K. Maeda. 2007. English gigaword third edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>F Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL,</booktitle>
<location>New York.</location>
<contexts>
<context position="2256" citStr="Habash and Sadat, 2006" startWordPosition="340" endWordPosition="343">., 2008; Yang and Kirchhoff, 2006). But into what units should a compound word be segmented? Taken as a stand-alone task, the goal of a compound splitter is to produce a segmentation for some input that matches the linguistic intuitions of a native speaker of the language. However, there are often advantages to using elements larger than single morphemes as the minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Knight, 2003; Habash and Sadat, 2006; Chang et al., 2008). Recent work has shown that by combining a variety of segmentations of the input into a segmentation lattice and effectively marginalizing over many different segmentations, translations superior to those resulting from any single single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008). Unfortunately, this approach is difficult to utilize because it requires multiple segmenters that behave differently on the same input. In this paper, we describe a maximum entropy word segmentation model that is trained to assign high pr</context>
<context position="8474" citStr="Habash and Sadat, 2006" startWordPosition="1305" endWordPosition="1308">erence lattices for the two words from Figure 1. Although only a subset of all linguistically plausible segmentations, each path corresponds to a plausible segmentation for word-for-word German-English translation. der nahme ton tonband tonbandaufnahme wie wieder wiederaufnahme Table 1: German-English dictionary fragment for words present in Figure 1. come plausible translations. However, using a strategy of “over segmentation” and relying on phrase models to learn the non-compositional translations has been shown to degrade translation quality significantly on several tasks (Xu et al., 2004; Habash and Sadat, 2006). We thus desire lattices containing as little oversegmentation as possible. We have now have a concept of a “gold standard” segmentation lattice for translation: it should contain all linguistically motivated segmentations that also correspond to plausible word-for-word translations into English. Figure 2 shows an example of the reference lattice for the two words we just discussed. For the experiments in this paper, we generated a development and test set by randomly choosing 19 German newspaper articles, identifying all words greater than 6 characters is length, and segmenting each word so </context>
<context position="26223" citStr="Habash and Sadat, 2006" startWordPosition="4250" endWordPosition="4253">he effect of manipulating the density parameter (cf. Section 3.2) on the performance and decoding time of the Turkish-English translation system. It further confirms the hypothesis that increased diversity of segmentations encoded in a segmentation lattice can improve translation performance; however, it also shows that once the density becomes too great, and too many implausible segmentations are included in the lattice, translation quality will be harmed. 6 Related work Aside from improving the vocabulary coverage of machine translation systems (Koehn et al., 2008; Yang and Kirchhoff, 2006; Habash and Sadat, 2006), compound word segmentation (also referred to as decompounding) has been shown to be helpful in a variety of NLP tasks including mono- and 6We note that our maximum entropy segmentation model could easily address this problem by incorporating information about whether a word is likely to be a named entity as a feature. 412 1 1.5 2 2.5 3 3.5 Segmentation lattice density Figure 4: The effect of the lattice density parameter on translation quality and decoding time. crosslingual IR (Airio, 2006) and speech recognition (Hessen and Jong, 2003). A number of researchers have demonstrated the value o</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>N. Habash and F. Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In Proc. of NAACL, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjan Van Hessen</author>
<author>Franciska De Jong</author>
</authors>
<title>Compound decomposition in dutch large vocabulary speech recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of Eurospeech 2003, Geneve,</booktitle>
<pages>225--228</pages>
<marker>Van Hessen, De Jong, 2003</marker>
<rawString>Arjan Van Hessen and Franciska De Jong. 2003. Compound decomposition in dutch large vocabulary speech recognition. In Proceedings of Eurospeech 2003, Geneve, pages 225–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of IEEE Internation Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="19927" citStr="Kneser and Ney, 1995" startWordPosition="3235" endWordPosition="3238">the segmentation model we just introduced in German-English, Hungarian-English, 410 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 Precision Figure 3: The effect of the lattice density parameter on precision and recall. and Turkish-English translation tasks and then show results elucidating the effect of the lattice density parameter. We begin with a description of our MT system. 5.1 Data preparation and system description For all experiments, we used a 5-gram English language model trained on the AFP and Xinua portions of the Gigaword v3 corpus (Graff et al., 2007) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The training, development, and test data for German-English and Hungarian-English systems used were distributed as part of the 2009 EACL Workshop on Machine Translation,4 and the Turkish-English data corresponds to the training and test sets used in the work of Oflazer and Durgar ElKahlout (2007). Corpus statistics for all language pairs are summarized in Table 3. We note that in all language pairs, the 1BEST segmentation variant of the training data results in a significant reduction in types. Word alignment was carried out by running Giza++ implementation of IBM Model 4 initialized with 5 </context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of IEEE Internation Conference on Acoustics, Speech, and Signal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proc. of the EACL</booktitle>
<contexts>
<context position="2232" citStr="Koehn and Knight, 2003" startWordPosition="336" endWordPosition="339">et al., 2008; Dyer et al., 2008; Yang and Kirchhoff, 2006). But into what units should a compound word be segmented? Taken as a stand-alone task, the goal of a compound splitter is to produce a segmentation for some input that matches the linguistic intuitions of a native speaker of the language. However, there are often advantages to using elements larger than single morphemes as the minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Knight, 2003; Habash and Sadat, 2006; Chang et al., 2008). Recent work has shown that by combining a variety of segmentations of the input into a segmentation lattice and effectively marginalizing over many different segmentations, translations superior to those resulting from any single single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008). Unfortunately, this approach is difficult to utilize because it requires multiple segmenters that behave differently on the same input. In this paper, we describe a maximum entropy word segmentation model that is t</context>
<context position="9562" citStr="Koehn and Knight (2003)" startWordPosition="1481" endWordPosition="1484">randomly choosing 19 German newspaper articles, identifying all words greater than 6 characters is length, and segmenting each word so that the resulting units could be translated compositionally into English. This resulted in 489 training sentences corresponding to 564 paths for the dev set (which was drawn from 15 articles), and 279 words (302 paths) for the test set (drawn from the remaining 4 articles). 3 A maximum entropy segmentation model We now turn to the problem of modeling word segmentation in a way that facilitates lattice construction. As a starting point, we consider the work of Koehn and Knight (2003) who observe that in most languages that exhibit compounding, the morphemes used to construct compounds frequently also appear as individual tokens. Based on this observation, they propose a model of word segmentation that splits compound words into pieces found in the dictionary based on a variety heuristic scoring criteria. While these models have been reasonably successful (Koehn et al., 2008), they are problematic for two reasons. First, there is no principled way to incorporate additional features (such as phonotactics) which might be useful to determining whether a word break should occu</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>P. Koehn and K. Knight. 2003. Empirical methods for compound splitting. In Proc. of the EACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL 2003,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="20721" citStr="Koehn et al., 2003" startWordPosition="3363" endWordPosition="3366">d the Turkish-English data corresponds to the training and test sets used in the work of Oflazer and Durgar ElKahlout (2007). Corpus statistics for all language pairs are summarized in Table 3. We note that in all language pairs, the 1BEST segmentation variant of the training data results in a significant reduction in types. Word alignment was carried out by running Giza++ implementation of IBM Model 4 initialized with 5 iterations of Model 1, 5 of the HMM aligner, and 3 iterations of Model 4 (Och and Ney, 2003) in both directions and then symmetrizing using the grow-diag-final-and heuristic (Koehn et al., 2003). For each language pair, the corpus was aligned twice, once in its non-segmented variant and once using the single-best segmentation variant. For translation, we used a bottom-up parsing decoder that uses cube pruning to intersect the lan4http://www.statmt.org/wmt09 guage model with the target side of the synchronous grammar. The grammar rules were extracted from the word aligned parallel corpus and scored as described in Chiang (2007). The features used by the decoder were the English language model log probability, log f(¯e |¯f), the ‘lexical translation’ log probabilities in both direction</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL 2003, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch Mayne</author>
<author>C CallisonBurch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computation Linguistics (ACL), Demonstration Session,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="3122" citStr="Koehn et al., 2007" startWordPosition="480" endWordPosition="483">ingle single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008). Unfortunately, this approach is difficult to utilize because it requires multiple segmenters that behave differently on the same input. In this paper, we describe a maximum entropy word segmentation model that is trained to assign high probability to possibly several segmentations of an input word. This model enables generation of diverse, accurate segmentation lattices from a single model that are appropriate for use in decoders that accept word lattices as input, such as Moses (Koehn et al., 2007). Since our model relies a small number of dense features, its parameters can be tuned using very small amounts of manually created reference lattices. Furthermore, since these parameters were chosen to have valid interpretation across a variety of languages, we find that the weights estimated for one apply quite well to another. We show that these lattices significantly improve translation quality when translating into English from three languages exhibiting productive compounding: German, Turkish, and Hungarian. The paper is structured as follows. In the next sec406 Human Language Technologi</context>
</contexts>
<marker>Koehn, Hoang, Mayne, CallisonBurch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch Mayne, C. CallisonBurch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Annual Meeting of the Association for Computation Linguistics (ACL), Demonstration Session, pages 177–180, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Abhishek Arun</author>
<author>Hieu Hoang</author>
</authors>
<title>Towards better machine translation quality for the German-English language pairs.</title>
<date>2008</date>
<booktitle>In ACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1622" citStr="Koehn et al., 2008" startWordPosition="239" endWordPosition="242">ignificant improvements in translation quality in German-English, Hungarian-English, and Turkish-English translation over state-ofthe-art baselines. 1 Introduction Compound words pose significant challenges to the lexicalized models that are currently common in statistical machine translation. This problem has been widely acknowledged, and the conventional solution, which has been shown to work well for many language pairs, is to segment compounds into their constituent morphemes using either morphological analyzers or empirical methods and then to translate from or to this segmented variant (Koehn et al., 2008; Dyer et al., 2008; Yang and Kirchhoff, 2006). But into what units should a compound word be segmented? Taken as a stand-alone task, the goal of a compound splitter is to produce a segmentation for some input that matches the linguistic intuitions of a native speaker of the language. However, there are often advantages to using elements larger than single morphemes as the minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Kn</context>
<context position="9961" citStr="Koehn et al., 2008" startWordPosition="1544" endWordPosition="1547">icles). 3 A maximum entropy segmentation model We now turn to the problem of modeling word segmentation in a way that facilitates lattice construction. As a starting point, we consider the work of Koehn and Knight (2003) who observe that in most languages that exhibit compounding, the morphemes used to construct compounds frequently also appear as individual tokens. Based on this observation, they propose a model of word segmentation that splits compound words into pieces found in the dictionary based on a variety heuristic scoring criteria. While these models have been reasonably successful (Koehn et al., 2008), they are problematic for two reasons. First, there is no principled way to incorporate additional features (such as phonotactics) which might be useful to determining whether a word break should occur. Second, the heuristic scoring offers little insight into which segmentations should be included in a lattice. We would like our model to consider a wide variety of segmentations of any word (including perhaps hypothesized morphemes that are not in the dictionary), to make use of a rich set of features, and to have a probabilistic interpretation of each hypothesized split (to incorporate into t</context>
<context position="26172" citStr="Koehn et al., 2008" startWordPosition="4242" endWordPosition="4245">fect of the density parameter Figure 4 shows the effect of manipulating the density parameter (cf. Section 3.2) on the performance and decoding time of the Turkish-English translation system. It further confirms the hypothesis that increased diversity of segmentations encoded in a segmentation lattice can improve translation performance; however, it also shows that once the density becomes too great, and too many implausible segmentations are included in the lattice, translation quality will be harmed. 6 Related work Aside from improving the vocabulary coverage of machine translation systems (Koehn et al., 2008; Yang and Kirchhoff, 2006; Habash and Sadat, 2006), compound word segmentation (also referred to as decompounding) has been shown to be helpful in a variety of NLP tasks including mono- and 6We note that our maximum entropy segmentation model could easily address this problem by incorporating information about whether a word is likely to be a named entity as a feature. 412 1 1.5 2 2.5 3 3.5 Segmentation lattice density Figure 4: The effect of the lattice density parameter on translation quality and decoding time. crosslingual IR (Airio, 2006) and speech recognition (Hessen and Jong, 2003). A </context>
</contexts>
<marker>Koehn, Arun, Hoang, 2008</marker>
<rawString>Philipp Koehn, Abhishek Arun, and Hieu Hoang. 2008. Towards better machine translation quality for the German-English language pairs. In ACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical signficiance tests for machine translation evluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<contexts>
<context position="24809" citStr="Koehn, 2004" startWordPosition="4022" endWordPosition="4023">ge amount of training data is available, moving to a one-best segmentation does not yield substantial improvements (Yang and Kirchhoff, 2006). Perhaps most surprisingly, the improvements observed when using lattices with the Hungarian and Turkish systems were larger than the corresponding improvement in the German system, but German was the only language for which we had segmentation training data. The smaller effect in German is probably due to there being more in-domain training data in the German system than in the (otherwise comparably sized) Hungarian system. 5Using bootstrap resampling (Koehn, 2004), the improvements in BLEU, TER, as well as the linear combination used in tuning are statistically significant at at least P &lt; .05. Targeted analysis of the translation output shows that while both the 1BEST and LATTICE systems generally produce adequate translations of compound words that are out of vocabulary in the BASELINE system, the LATTICE system performs better since it recovers from infelicitous splits that the one-best segmenter makes. For example, one class of error we frequently observe is that the one-best segmenter splits an OOV proper name into two pieces when a portion of the </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical signficiance tests for machine translation evluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="14234" citStr="Liu et al., 1989" startWordPosition="2269" endWordPosition="2272">he following objective (which can be computed using the forward algorithm over the unpruned hypothesis lattices): XL = − log X p(S|wi) (6) i SETZi The gradient with respect to the feature weights for a log linear model is simply: Ep($|wi)[hk] − Ep($|wi,TZi)[hk] (7) To compute these values, the first expectation is computed using forward-backward inference over the full lattice. To compute the second expectation, the full lattice is intersected with the reference lattice Ri, and then forward-backward inference is redone.2 We use the standard quasi-Newtonian method L-BFGS to optimize the model (Liu et al., 1989). Training generally converged in only a few hundred iterations. 3.3.1 Training to minimize 1-best error In some cases, such as when performing word alignment for translation model construction, lattices cannot be used easily. In these cases, a 1- best segmentation (which can be determined from the lattice using the Viterbi algorithm) may be desired. To train the parameters of the model for this condition (which is arguably slightly different from the lattice generation case we just considered), we used the minimum error training (MERT) algorithm on the segmentation lattices to find the parame</context>
</contexts>
<marker>Liu, Nocedal, Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Nicolas Stroppa</author>
<author>Andy Way</author>
</authors>
<title>Bootstrapping word alignment via word packing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>304--311</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="27387" citStr="Ma et al., 2007" startWordPosition="4445" endWordPosition="4448"> number of researchers have demonstrated the value of using lattices to encode segmentation alternatives as input to a machine translation system (Dyer et al., 2008; DeNeefe et al., 2008; Xu et al., 2004), but this is the first work to do so using a single segmentation model. Another strand of inquiry that is closely related is the work on adjusting the source language segmentation to match the granularity of the target language as a way of improving translation. The approaches suggested thus far have been mostly of a heuristic nature tailored to Chinese-English translation (Bai et al., 2008; Ma et al., 2007). 7 Conclusions and future work In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for input into a statistical machine translation system. These segmentation lattices improve translation quality (over an already strong baseline) in three typologically distinct languages (German, Hungarian, Turkish) when translating into English. Previous approaches to generating segmentation lattices have been quite laborious, relying either on the existence of multiple segmenters (Dyer et al., 2008; Xu et al., 2005) or hand-cr</context>
</contexts>
<marker>Ma, Stroppa, Way, 2007</marker>
<rawString>Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. Bootstrapping word alignment via word packing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 304–311, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Josef Och</author>
<author>Ignacio Thayer</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Lattice-based minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Honolulu, HI.</location>
<contexts>
<context position="21829" citStr="Macherey et al., 2008" startWordPosition="3545" endWordPosition="3548">glish language model log probability, log f(¯e |¯f), the ‘lexical translation’ log probabilities in both directions (Koehn et al., 2003), and a word count feature. For the lattice systems, we also included the unnormalized log p(¯f|!9), as it is defined in Section 3, as well as an input word count feature. The feature weights were tuned on a heldout development set so as to maximize an equally weighted linear combination of BLEU and 1-TER (Papineni et al., 2002; Snover et al., 2006) using the minimum error training algorithm on a packed forest representation of the decoder’s hypothesis space (Macherey et al., 2008). The weights were independently optimized for each language pair and each experimental condition. 5.2 Segmentation lattice results In this section, we report the results of an experiment to see if the compound lattices constructed using our maximum entropy model yield better translations than either an unsegmented baseline or a baseline consisting of a single-best segmentation. For each language pair, we define three conditions: BASELINE, 1BEST, and LATTICE. In the BASELINE condition, a lowercased and tokenized (but not segmented) version of the test data is translated using the grammar deriv</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>Wolfgang Macherey, Franz Josef Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In Proceedings of EMNLP, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="20619" citStr="Och and Ney, 2003" startWordPosition="3349" endWordPosition="3352">n-English systems used were distributed as part of the 2009 EACL Workshop on Machine Translation,4 and the Turkish-English data corresponds to the training and test sets used in the work of Oflazer and Durgar ElKahlout (2007). Corpus statistics for all language pairs are summarized in Table 3. We note that in all language pairs, the 1BEST segmentation variant of the training data results in a significant reduction in types. Word alignment was carried out by running Giza++ implementation of IBM Model 4 initialized with 5 iterations of Model 1, 5 of the HMM aligner, and 3 iterations of Model 4 (Och and Ney, 2003) in both directions and then symmetrizing using the grow-diag-final-and heuristic (Koehn et al., 2003). For each language pair, the corpus was aligned twice, once in its non-segmented variant and once using the single-best segmentation variant. For translation, we used a bottom-up parsing decoder that uses cube pruning to intersect the lan4http://www.statmt.org/wmt09 guage model with the target side of the synchronous grammar. The grammar rules were extracted from the word aligned parallel corpus and scored as described in Chiang (2007). The features used by the decoder were the English langua</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
<author>Ilknur Durgar El-Kahlout</author>
</authors>
<title>Exploring different representational units in English-toTurkish statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Oflazer, El-Kahlout, 2007</marker>
<rawString>Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007. Exploring different representational units in English-toTurkish statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 25–32, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="21672" citStr="Papineni et al., 2002" startWordPosition="3520" endWordPosition="3523">he grammar rules were extracted from the word aligned parallel corpus and scored as described in Chiang (2007). The features used by the decoder were the English language model log probability, log f(¯e |¯f), the ‘lexical translation’ log probabilities in both directions (Koehn et al., 2003), and a word count feature. For the lattice systems, we also included the unnormalized log p(¯f|!9), as it is defined in Section 3, as well as an input word count feature. The feature weights were tuned on a heldout development set so as to maximize an equally weighted linear combination of BLEU and 1-TER (Papineni et al., 2002; Snover et al., 2006) using the minimum error training algorithm on a packed forest representation of the decoder’s hypothesis space (Macherey et al., 2008). The weights were independently optimized for each language pair and each experimental condition. 5.2 Segmentation lattice results In this section, we report the results of an experiment to see if the compound lattices constructed using our maximum entropy model yield better translations than either an unsegmented baseline or a baseline consisting of a single-best segmentation. For each language pair, we define three conditions: BASELINE,</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL</booktitle>
<contexts>
<context position="28924" citStr="Poon et al., 2009" startWordPosition="4683" endWordPosition="4686">nd Turkish), we have demonstrated that the parameters obtained in one language work surprisingly well for others. Thus, with virtually no cost, this model can be used with a variety of diverse languages. While these results are already quite satisfying, there are a number of compelling extensions to this work that we intend to explore in the future. First, unsupervised segmentation approaches offer a very compelling alternative to the manually crafted segmentation lattices that we created. Recent work suggests that unsupervised segmentation of inflectional affixal morphology works quite well (Poon et al., 2009), and extending this work to compounding morphology should be feasible, obviating the need for expensive hand-crafted reference lattices. Second, incorporating target language information into a segmentation model holds considerable promise for inducing more effective translation models that perform especially well for segmentation lattice inputs. Acknowledgments Special thanks to Kemal Oflazar and Reyyan Yeniterzi of Sabancı University for providing the Turkish-English corpus and to Philip Resnik, Adam Lopez, Trevor Cohn, and especially Phil Blunsom for their helpful suggestions. This researc</context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In Proc. of NAACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sixtus</author>
<author>S Ortmanns</author>
</authors>
<title>High quality word graphs using forward-backward pruning.</title>
<date>1999</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<location>Phoenix, AZ.</location>
<contexts>
<context position="12949" citStr="Sixtus and Ortmanns, 1999" startWordPosition="2060" endWordPosition="2063">deleted, then an edge where they are deleted will have fewer characters than the coverage indicated by the edge’s starting and ending vertices. 3.2 Lattice pruning Except for the minimum segment length restriction, our model defines probabilities for all segmentations of an input word, making the resulting segmentation lattices are quite large. Since large lattices are costly to deal with during translation (and may lead to worse translations because poor segmentations are passed to the decoder), we prune them using forward-backward pruning so as to contain just the highest probability paths (Sixtus and Ortmanns, 1999). This works by computing the score of the best path passing through every edge in the lattice using the forward-backward algorithm. By finding the best score overall, we can then prune edges using a threshold criterion; i.e., edges whose score is some factor α away from the global best edge score. 3.3 Maximum likelihood training Our model defines a conditional probability distribution over virtually all segmentations of a word w. To train our model, we wish to maximize the likelihood of the segmentations contained in the reference lattices by moving probability mass away from the segmentation</context>
</contexts>
<marker>Sixtus, Ortmanns, 1999</marker>
<rawString>S. Sixtus and S. Ortmanns. 1999. High quality word graphs using forward-backward pruning. In Proceedings of ICASSP, Phoenix, AZ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation forMachine Translation in the Americas.</booktitle>
<contexts>
<context position="21694" citStr="Snover et al., 2006" startWordPosition="3524" endWordPosition="3527">xtracted from the word aligned parallel corpus and scored as described in Chiang (2007). The features used by the decoder were the English language model log probability, log f(¯e |¯f), the ‘lexical translation’ log probabilities in both directions (Koehn et al., 2003), and a word count feature. For the lattice systems, we also included the unnormalized log p(¯f|!9), as it is defined in Section 3, as well as an input word count feature. The feature weights were tuned on a heldout development set so as to maximize an equally weighted linear combination of BLEU and 1-TER (Papineni et al., 2002; Snover et al., 2006) using the minimum error training algorithm on a packed forest representation of the decoder’s hypothesis space (Macherey et al., 2008). The weights were independently optimized for each language pair and each experimental condition. 5.2 Segmentation lattice results In this section, we report the results of an experiment to see if the compound lattices constructed using our maximum entropy model yield better translations than either an unsegmented baseline or a baseline consisting of a single-best segmentation. For each language pair, we define three conditions: BASELINE, 1BEST, and LATTICE. I</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation forMachine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Do we need Chinese word segmentation for statistical machine translation?</title>
<date>2004</date>
<booktitle>In Proceedings of the Third SIGHAN Workshop on Chinese Language Learning,</booktitle>
<pages>122--128</pages>
<location>Barcelona,</location>
<contexts>
<context position="8449" citStr="Xu et al., 2004" startWordPosition="1301" endWordPosition="1304">ually created reference lattices for the two words from Figure 1. Although only a subset of all linguistically plausible segmentations, each path corresponds to a plausible segmentation for word-for-word German-English translation. der nahme ton tonband tonbandaufnahme wie wieder wiederaufnahme Table 1: German-English dictionary fragment for words present in Figure 1. come plausible translations. However, using a strategy of “over segmentation” and relying on phrase models to learn the non-compositional translations has been shown to degrade translation quality significantly on several tasks (Xu et al., 2004; Habash and Sadat, 2006). We thus desire lattices containing as little oversegmentation as possible. We have now have a concept of a “gold standard” segmentation lattice for translation: it should contain all linguistically motivated segmentations that also correspond to plausible word-for-word translations into English. Figure 2 shows an example of the reference lattice for the two words we just discussed. For the experiments in this paper, we generated a development and test set by randomly choosing 19 German newspaper articles, identifying all words greater than 6 characters is length, and</context>
<context position="26975" citStr="Xu et al., 2004" startWordPosition="4375" endWordPosition="4378">and 6We note that our maximum entropy segmentation model could easily address this problem by incorporating information about whether a word is likely to be a named entity as a feature. 412 1 1.5 2 2.5 3 3.5 Segmentation lattice density Figure 4: The effect of the lattice density parameter on translation quality and decoding time. crosslingual IR (Airio, 2006) and speech recognition (Hessen and Jong, 2003). A number of researchers have demonstrated the value of using lattices to encode segmentation alternatives as input to a machine translation system (Dyer et al., 2008; DeNeefe et al., 2008; Xu et al., 2004), but this is the first work to do so using a single segmentation model. Another strand of inquiry that is closely related is the work on adjusting the source language segmentation to match the granularity of the target language as a way of improving translation. The approaches suggested thus far have been mostly of a heuristic nature tailored to Chinese-English translation (Bai et al., 2008; Ma et al., 2007). 7 Conclusions and future work In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for input into a stati</context>
</contexts>
<marker>Xu, Zens, Ney, 2004</marker>
<rawString>J. Xu, R. Zens, and H. Ney. 2004. Do we need Chinese word segmentation for statistical machine translation? In Proceedings of the Third SIGHAN Workshop on Chinese Language Learning, pages 122–128, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Integrated Chinese word segmentation in statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of IWSLT 2005,</booktitle>
<location>Pittsburgh.</location>
<contexts>
<context position="2574" citStr="Xu et al., 2005" startWordPosition="391" endWordPosition="394">ts larger than single morphemes as the minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Knight, 2003; Habash and Sadat, 2006; Chang et al., 2008). Recent work has shown that by combining a variety of segmentations of the input into a segmentation lattice and effectively marginalizing over many different segmentations, translations superior to those resulting from any single single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008). Unfortunately, this approach is difficult to utilize because it requires multiple segmenters that behave differently on the same input. In this paper, we describe a maximum entropy word segmentation model that is trained to assign high probability to possibly several segmentations of an input word. This model enables generation of diverse, accurate segmentation lattices from a single model that are appropriate for use in decoders that accept word lattices as input, such as Moses (Koehn et al., 2007). Since our model relies a small number of dense fea</context>
<context position="27976" citStr="Xu et al., 2005" startWordPosition="4532" endWordPosition="4535">al., 2008; Ma et al., 2007). 7 Conclusions and future work In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for input into a statistical machine translation system. These segmentation lattices improve translation quality (over an already strong baseline) in three typologically distinct languages (German, Hungarian, Turkish) when translating into English. Previous approaches to generating segmentation lattices have been quite laborious, relying either on the existence of multiple segmenters (Dyer et al., 2008; Xu et al., 2005) or hand-crafted rules (DeNeefe et al., 2008). Although the segmentation model we propose is discriminative, we have shown that it can be trained using a minimal amount of annotated training data. Furthermore, when even this minimal data cannot be acquired for a particular language (as was the situation we faced with Hungarian and Turkish), we have demonstrated that the parameters obtained in one language work surprisingly well for others. Thus, with virtually no cost, this model can be used with a variety of diverse languages. While these results are already quite satisfying, there are a numb</context>
</contexts>
<marker>Xu, Matusov, Zens, Ney, 2005</marker>
<rawString>J. Xu, E. Matusov, R. Zens, and H. Ney. 2005. Integrated Chinese word segmentation in statistical machine translation. In Proc. of IWSLT 2005, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yang</author>
<author>K Kirchhoff</author>
</authors>
<title>Phrase-based backoff models for machine translation of highly inflected languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL</booktitle>
<pages>41--48</pages>
<contexts>
<context position="1668" citStr="Yang and Kirchhoff, 2006" startWordPosition="247" endWordPosition="250">quality in German-English, Hungarian-English, and Turkish-English translation over state-ofthe-art baselines. 1 Introduction Compound words pose significant challenges to the lexicalized models that are currently common in statistical machine translation. This problem has been widely acknowledged, and the conventional solution, which has been shown to work well for many language pairs, is to segment compounds into their constituent morphemes using either morphological analyzers or empirical methods and then to translate from or to this segmented variant (Koehn et al., 2008; Dyer et al., 2008; Yang and Kirchhoff, 2006). But into what units should a compound word be segmented? Taken as a stand-alone task, the goal of a compound splitter is to produce a segmentation for some input that matches the linguistic intuitions of a native speaker of the language. However, there are often advantages to using elements larger than single morphemes as the minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Knight, 2003; Habash and Sadat, 2006; Chang et a</context>
<context position="24338" citStr="Yang and Kirchhoff, 2006" startWordPosition="3945" endWordPosition="3948"> 69.1 TR-BASELINE 26.9 61.0 TR-1BEST 27.8 61.2 TR-LATTICE 28.7 59.6 Table 4: Translation results for German (DE)-English, Hungarian (HU)-English, and Turkish (TR)-English. Scores were computed using a single reference and are case insensitive. experiments comparing the three input variants. For all language pairs, we see significant improvements in both BLEU and TER when segmentation lattices are used.5 Additionally, we also confirmed previous findings that showed that when a large amount of training data is available, moving to a one-best segmentation does not yield substantial improvements (Yang and Kirchhoff, 2006). Perhaps most surprisingly, the improvements observed when using lattices with the Hungarian and Turkish systems were larger than the corresponding improvement in the German system, but German was the only language for which we had segmentation training data. The smaller effect in German is probably due to there being more in-domain training data in the German system than in the (otherwise comparably sized) Hungarian system. 5Using bootstrap resampling (Koehn, 2004), the improvements in BLEU, TER, as well as the linear combination used in tuning are statistically significant at at least P &lt; .</context>
<context position="26198" citStr="Yang and Kirchhoff, 2006" startWordPosition="4246" endWordPosition="4249">parameter Figure 4 shows the effect of manipulating the density parameter (cf. Section 3.2) on the performance and decoding time of the Turkish-English translation system. It further confirms the hypothesis that increased diversity of segmentations encoded in a segmentation lattice can improve translation performance; however, it also shows that once the density becomes too great, and too many implausible segmentations are included in the lattice, translation quality will be harmed. 6 Related work Aside from improving the vocabulary coverage of machine translation systems (Koehn et al., 2008; Yang and Kirchhoff, 2006; Habash and Sadat, 2006), compound word segmentation (also referred to as decompounding) has been shown to be helpful in a variety of NLP tasks including mono- and 6We note that our maximum entropy segmentation model could easily address this problem by incorporating information about whether a word is likely to be a named entity as a feature. 412 1 1.5 2 2.5 3 3.5 Segmentation lattice density Figure 4: The effect of the lattice density parameter on translation quality and decoding time. crosslingual IR (Airio, 2006) and speech recognition (Hessen and Jong, 2003). A number of researchers have</context>
</contexts>
<marker>Yang, Kirchhoff, 2006</marker>
<rawString>M. Yang and K. Kirchhoff. 2006. Phrase-based backoff models for machine translation of highly inflected languages. In Proceedings of the EACL 2006, pages 41–48.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>