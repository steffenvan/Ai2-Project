<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.99312">
Iterative Viterbi A* Algorithm for K-Best Sequential Decoding
</title>
<author confidence="0.977735">
Zhiheng Huang†, Yi Chang, Bo Long, Jean-Francois Crespo†,
Anlei Dong, Sathiya Keerthi and Su-Lin Wu
</author>
<affiliation confidence="0.881162">
Yahoo! Labs
</affiliation>
<address confidence="0.9079215">
701 First Avenue, Sunnyvale
CA 94089, USA
</address>
<email confidence="0.8593635">
{zhiheng huang,jfcrespo}@yahoo.com†
{yichang,bolong,anlei,selvarak,sulin}@yahoo-inc.com
</email>
<sectionHeader confidence="0.989759" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998003166666667">
Sequential modeling has been widely used in
a variety of important applications including
named entity recognition and shallow pars-
ing. However, as more and more real time
large-scale tagging applications arise, decod-
ing speed has become a bottleneck for exist-
ing sequential tagging algorithms. In this pa-
per we propose 1-best A*, 1-best iterative A*,
k-best A* and k-best iterative Viterbi A* al-
gorithms for sequential decoding. We show
the efficiency of these proposed algorithms for
five NLP tagging tasks. In particular, we show
that iterative Viterbi A* decoding can be sev-
eral times or orders of magnitude faster than
the state-of-the-art algorithm for tagging tasks
with a large number of labels. This algorithm
makes real-time large-scale tagging applica-
tions with thousands of labels feasible.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894648148148">
Sequence tagging algorithms including HMMs (Ra-
biner, 1989), CRFs (Lafferty et al., 2001), and
Collins’s perceptron (Collins, 2002) have been
widely employed in NLP applications. Sequential
decoding, which finds the best tag sequences for
given inputs, is an important part of the sequential
tagging framework. Traditionally, the Viterbi al-
gorithm (Viterbi, 1967) is used. This algorithm is
quite efficient when the label size of problem mod-
eled is low. Unfortunately, due to its O(T L2) time
complexity, where T is the input token size and L
is the label size, the Viterbi decoding can become
prohibitively slow when the label size is large (say,
larger than 200).
It is not uncommon that the problem modeled
consists of more than 200 labels. The Viterbi al-
gorithm cannot find the best sequences in tolerable
response time. To resolve this, Esposito and Radi-
cioni (2009) have proposed a Carpediem algorithm
which opens only necessary nodes in searching the
best sequence. More recently, Kaji et al. (2010) pro-
posed a staggered decoding algorithm, which proves
to be very efficient on datasets with a large number
of labels.
What the aforementioned literature does not cover
is the k-best sequential decoding problem, which is
indeed frequently required in practice. For example
to pursue a high recall ratio, a named entity recogni-
tion system may have to adopt k-best sequences in
case the true entities are not recognized at the best
one. The k-best parses have been extensively stud-
ied in syntactic parsing context (Huang, 2005; Pauls
and Klein, 2009), but it is not well accommodated
in sequential decoding context. To our best knowl-
edge, the state-of-the-art k-best sequential decoding
algorithm is Viterbi A* 1. In this paper, we general-
ize the iterative process from the work of (Kaji et al.,
2010) and propose a k-best sequential decoding al-
gorithm, namely iterative Viterbi A*. We show that
the proposed algorithm is several times or orders of
magnitude faster than the state-of-the-art in all tag-
ging tasks which consist of more than 200 labels.
Our contributions can be summarized as follows.
(1) We apply the A* search framework to sequential
decoding problem. We show that A* with a proper
heuristic can outperform the classic Viterbi decod-
ing. (2) We propose 1-best A*, 1-best iterative A*
decoding algorithms which are the second and third
fastest decoding algorithms among the five decod-
ing algorithms for comparison, although there is a
significant gap to the fastest 1-best decoding algo-
rithm. (3) We propose k-best A* and k-best iterative
Viterbi A* algorithms. The latter is several times or
orders of magnitude faster than the state-of-the-art
</bodyText>
<footnote confidence="0.999058">
1Implemented in both CRFPP (http://crfpp.sourceforge.net/)
and LingPipe (http://alias-i.com/lingpipe/) packages.
</footnote>
<page confidence="0.896622">
611
</page>
<note confidence="0.986286">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 611–619,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999603">
k-best decoding algorithm. This algorithm makes
real-time large-scale tagging applications with thou-
sands of labels feasible.
</bodyText>
<sectionHeader confidence="0.916917" genericHeader="method">
2 Problem formulation
</sectionHeader>
<bodyText confidence="0.998304">
In this section, we formulate the sequential decod-
ing problem in the context of perceptron algorithm
(Collins, 2002) and CRFs (Lafferty et al., 2001). All
the discussions apply to HMMs as well. Formally, a
perceptron model is
</bodyText>
<equation confidence="0.888746">
θkfk(yt,yt−1,xt), (1)
</equation>
<bodyText confidence="0.970032">
and a CRFs model is
</bodyText>
<equation confidence="0.999244333333333">
T K
p(y|x) = Z(x) exp{E E θkfk(yt, yt−1, xt)}, (2)
t=1 k=1
</equation>
<bodyText confidence="0.999955125">
where x and y is an observation sequence and a la-
bel sequence respectively, t is the sequence position,
T is the sequence size, fk are feature functions and
K is the number of feature functions. θk are the pa-
rameters that need to be estimated. They represent
the importance of feature functions fk in prediction.
For CRFs, Z(x) is an instance-specific normaliza-
tion function
</bodyText>
<equation confidence="0.938525">
θkfk(yt,yt−1,xt)}. (3)
</equation>
<bodyText confidence="0.997344">
If x is given, the decoding is to find the best y which
maximizes the score of f(y, x) for perceptron or the
probability of p(y|x) for CRFs. As Z(x) is a con-
stant for any given input sequence x, the decoding
for perceptron or CRFs is identical, that is,
</bodyText>
<equation confidence="0.7062255">
arg max f(y, x). (4)
Y
</equation>
<bodyText confidence="0.999467666666667">
To simplify the discussion, we divide the features
into two groups: unigram label features and bi-
gram label features. Unigram features are of form
fk(yt, xt) which are concerned with the current la-
bel and arbitrary feature patterns from input se-
quence. Bigram features are of form fk(yt, yt−1, xt)
which are concerned with both the previous and the
current labels. We thus rewrite the decoding prob-
lem as
</bodyText>
<equation confidence="0.750228">
θ2 kf2k (yt, yt−1, xt)).
(5)
</equation>
<bodyText confidence="0.932118">
For a better understanding, one can inter-
pret the term ��1
</bodyText>
<equation confidence="0.692064">
k=1 θ1kf1k (yt, xt) as node yt’s
</equation>
<bodyText confidence="0.890642">
score at position t, and interpret the term
���
k=1 θ2kf2k (yt, yt−1, xt) as edge (yt−1, yt)’s
score. So the sequential decoding problem is cast as
a max score pathfinding problem2. In the discussion
hereafter, we assume scores of nodes and edges are
pre-computed (denoted as n(yt) and e(yt−1, yt)),
and we can thus focus on the analysis of different
decoding algorithms.
</bodyText>
<sectionHeader confidence="0.99448" genericHeader="method">
3 Background
</sectionHeader>
<bodyText confidence="0.99986125">
We present the existing algorithms for both 1-best
and k-best sequential decoding in this section. These
algorithms serve as basis for the proposed algo-
rithms in Section 4.
</bodyText>
<subsectionHeader confidence="0.99953">
3.1 1-Best Viterbi
</subsectionHeader>
<bodyText confidence="0.999823166666666">
The Viterbi algorithm is a classic dynamic program-
ming based decoding algorithm. It has the computa-
tional complexity of O(TL2), where T is the input
sequence size and L is the label size3. Formally, the
Viterbi computes α(yt), the best score from starting
position to label yt, as follows.
</bodyText>
<equation confidence="0.961223">
(αyt−1 + e(yt−1, yt)) + n(yt), (6)
</equation>
<bodyText confidence="0.9993631">
where e(yt−1, yt) is the edge score between nodes
yt−1 and yt, n(yt) is the node score for yt. Note
that the terms αyt−1 and e(yt−1, yt) take value 0 for
t = 0 at initialization. Using the recursion defined
above, we can compute the highest score at end po-
sition T − 1 and its corresponding sequence. The
recursive computation of αyt is denoted as forward
pass since the computing traverses the lattice from
left to right. Conversely, the backward pass com-
putes βyt as the follows.
</bodyText>
<equation confidence="0.714798">
(βyt+1 + e(yt, yt+1) + n(yt+1)). (7)
</equation>
<bodyText confidence="0.999883727272727">
Note that βyT−1 = 0 at initialization. The max
score can be computed using maxy0(β0 + n(y0)).
We can use either forward or backward pass to
compute the best sequence. Table 1 summarizes
the computational complexity of all decoding algo-
rithms including Viterbi, which has the complexity
of TL2 for both best and worst cases. Note that
N/A means the decoding algorithms are not applica-
ble (for example, iterative Viterbi is not applicable
to k-best decoding). The proposed algorithms (see
Section 4) are highlighted in bold.
</bodyText>
<subsectionHeader confidence="0.999951">
3.2 1-Best iterative Viterbi
</subsectionHeader>
<bodyText confidence="0.999369">
Kaji et al. (Kaji et al., 2010) presented an efficient
sequential decoding algorithm named staggered de-
coding. We use the name iterative Viterbi to describe
</bodyText>
<footnote confidence="0.621968">
2With the constraint that the path consists of one and only
one node at each position.
3We ignore the feature size terms for simplicity.
</footnote>
<equation confidence="0.964351083333333">
T
E
t=1
f(y, x) =
K
E
k=1
E
Z(x) =
Y
T
E
t=1
exp{
K
E
k=1
arg max T K1 θ1kf1k (yt, xt) + K2
Y E (E E
t=1 k=1 k=1
max
yt−1
max
yt+1
</equation>
<page confidence="0.989306">
612
</page>
<bodyText confidence="0.999783714285715">
this algorithm for the reason that the iterative pro-
cess plays a central role in this algorithm. Indeed,
this iterative process is generalized in this paper to
handle k-best sequential decoding (see Section 4.4).
The main idea is to start with a coarse lattice
which consists of both active labels and degenerate
labels. A label is referred to as an active label if it
is not grouped (e.g., all labels in Fig. 1 (a) and la-
bel A at each position in Fig. 1 (b)), and otherwise
as an inactive label (i.e., dotted nodes). The new la-
bel, which is made by grouping the inactive labels,
is referred to as a degenerate label (i.e., large nodes
covering the dotted ones). Fig. 1 (a) shows a lattice
which consists of active labels only and (b) shows
a lattice which consists of both active and degener-
ate ones. The score of a degenerate label is the max
score of inactive labels which are included in the de-
generate label. Similarly, the edge score between a
degenerate label z and an active label y&apos; is the max
edge score between any inactive label y E z and y&apos;,
and the score of two degenerate labels z and z&apos; is the
max edge score between any inactive label y E z
and y&apos; E z&apos;. Using the above definitions, the best
sequence derived from a degenerate lattice would be
the upper bound of the sequence derived from the
original lattice. If the best sequence does not include
any degenerate labels, it is indeed the best sequence
for the original lattice.
</bodyText>
<figureCaption confidence="0.998869">
Figure 1: (a) A lattice consisting of active labels only.
(b) A lattice consisting of both active labels and degener-
ate ones. Each position has one active label (A) and one
degenerate label (consisting of B, C. D, E, and F).
</figureCaption>
<bodyText confidence="0.999490357142857">
The pseudo code for this algorithm is shown in
Algorithm 1. The lattice is initialized to include one
active label and one degenerate label at each position
(see Figure 1 (b)). Note that the labels are ranked
by the probabilities estimated from the training data.
The Viterbi algorithm is applied to the lattice to find
the best sequence. If the sequence consists of ac-
tive labels only, the algorithm terminates and returns
such a sequence. Otherwise, the lower bound lb4 of
the active sequence in the lattice is updated and the
lattice is expanded. The lower bound can be initial-
ized to the best sequence score using a beam search
(with beam size being 1). After either a forward or
a backward pass, the lower bound is assigned with
</bodyText>
<footnote confidence="0.948957">
4The maximum score of the active sequences found so far.
</footnote>
<bodyText confidence="0.99832125">
the best active sequence score best(lattice)5 if the
former is less than the latter. The expansion of lat-
tice ensures that the lattice has twice active labels
as before at a given position. Figure 2 shows the
column-wise expansion step. The number of active
labels in the column is doubled only if the best se-
quence of the degenerate lattice passes through the
degenerate label of that column.
</bodyText>
<figure confidence="0.687104111111111">
Algorithm 1 Iterative Viterbi Algorithm
1: lb = best score from beam search
2: init lattice
3: for i=0;;i++ do
4: if i %2 == 0 then
5: y = forward()
6: else
7: y = backward()
8: end if
9: if y consists of active labels only then
10: return y
11: end if
12: if lb &lt; best(lattice) then
13: lb = best(lattice)
14: end if
15: expand lattice
16: end for
Algorithm 2 Forward
</figure>
<listItem confidence="0.734156125">
1: for i=0; i &lt; T; i++ do
2: Compute a(yz) and /3(yz) according to Equations (6) and (7)
3: if a(yz) + /3(yz) &lt; lb then
4: prune yzfrom the current lattice
5: end if
6: end for
7: Node b = arg maxyT 1 a(yT−1)
8: return sequence back tracked by b
</listItem>
<figureCaption confidence="0.910593666666667">
Figure 2: Column-wise lattice expansion: (a) The best
sequence of the initial degenerate lattice, which does not
pass through the degenerate label in the first column. (b)
Column-wise expansion is performed and the best se-
quence is searched again. Notice that the active label in
the first column is not expanded. (c) The final result.
</figureCaption>
<bodyText confidence="0.9996125">
Algorithm 2 shows the forward pass in which the
node pruning is performed. That is, for any node,
if the best score of sequence which passes such a
node is less than the lower bound lb, such a node
is removed from the lattice. This removal is safe
as such a node does not have a chance to form an
optimal sequence. It is worth noting that, if a node
is removed, it can no longer be added into the lattice.
</bodyText>
<footnote confidence="0.8212625">
5We do not update the lower bound lb if we cannot find an
active sequence.
</footnote>
<figure confidence="0.999947558558559">
A
B
C
D
E
F
A
B
C
D
E
F
A
B
C
D
E
F
A
B
C
D
E
F
A A
B
C
D
E
F
B
C
D
E
F
A
B
C
D
E
F
A
B
C
D
E
F
(a) (b) (c)
A A A A
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
A A A A
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
A A A A
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
</figure>
<page confidence="0.994958">
613
</page>
<bodyText confidence="0.984203296296296">
This property ensures the efficiency of the iterative
Viterbi algorithm. The backward pass is similar to
the forward one and it is thus omitted.
The alternative calls of forward and backward
passes (in Algorithm 1) ensure the alternative updat-
ing/lowering of node forward and backward scores,
which makes the node pruning in either forward pass
(see Algorithm 2) or backward pass more efficient.
The lower bound lb is updated once in each iteration
of the main loop in Algorithm 1. While the forward
and backwards scores of nodes gradually decrease
and the lower bound lb increases, more and more
nodes are pruned.
The iterative Viterbi algorithm has computational
complexity of T and TL2 for best and worst cases
respectively. This can be proved as follows (Kaji et
al., 2010). At the m-th iteration in Algorithm 1, it-
erative Viterbi decoding requires order of T4m time
because there are 2m active labels (plus one degen-
erate label). Therefore, it has Emi=0 T4i time com-
plexity if it terminates at the m-th iteration. In the
best case in which m = 0, the time complexity is T.
In the worst case in which m = Flog2 L] − 1 (F.] is
the ceiling function which maps a real number to the
smallest following integer), the time complexity is
order of TL2 because ��log� L�−1 T4i &lt; 4/3TL2.
i=0
</bodyText>
<subsectionHeader confidence="0.99467">
3.3 1-Best Carpediem
</subsectionHeader>
<bodyText confidence="0.999990333333333">
Esposito and Radicioni (2009) have proposed a
novel 1-best6 sequential decoding algorithm, Car-
pediem, which attempts to open only necessary
nodes in searching the best sequence in a given lat-
tice. Carpediem has the complexity of TL log L and
TL2 for the best and worst cases respectively. We
skip the description of this algorithm due to space
limitations. Carpediem is used as a baseline in our
experiments for decoding speed comparison.
</bodyText>
<subsectionHeader confidence="0.967643">
3.4 K-Best Viterbi
</subsectionHeader>
<bodyText confidence="0.999917692307692">
In order to produce k-best sequences, it is not
enough to store 1-best label per node, as the k-
best sequences may include suboptimal labels. The
k-best sequential decoding gives up this 1-best
label memorization in the dynamic programming
paradigm. It stores up to k-best labels which are nec-
essary to form k-best sequences. The k-best Viterbi
algorithm thus has the computational complexity of
KTL2 for both best and worst cases.
Once we store the k-best labels per node in a lat-
tice, the k-best Viterbi algorithm calls either the for-
ward or the backward passes just in the same way as
the 1-best Viterbi decoding does. We can compute
</bodyText>
<footnote confidence="0.489446">
6They did not provide k-best solutions.
</footnote>
<bodyText confidence="0.9089995">
the k highest score at the end position T − 1 and the
corresponding k-best sequences.
</bodyText>
<subsectionHeader confidence="0.984301">
3.5 K-Best Viterbi A*
</subsectionHeader>
<bodyText confidence="0.999951509803922">
To our best knowledge the most efficient k-best se-
quence algorithm is the Viterbi A* algorithm as
shown in Algorithm 3. The algorithm consists of one
forward pass and an A* backward pass. The forward
pass computes and stores the Viterbi forward scores,
which are the best scores from the start to the cur-
rent nodes. In addition, each node stores a backlink
which points to its predecessor.
The major part of Algorithm 3 describes the back-
ward A* pass. Before describing the algorithm, we
note that each node in the agenda represents a se-
quence. So the operations on nodes (push or pop)
correspond to the operations on sequences. Initially,
the L nodes at position T − 1 are pushed to an
agenda. Each of the L nodes ni, i = 0, ... , L − 1,
represents a sequence. That is, node ni represents
the best sequence from the start to itself. The best of
the L sequences is the globally best sequence. How-
ever, the i-th best, i = 2, ... , k, of the L sequence
may not be the globally i-th best sequence. The pri-
ority of each node is set as the score of the sequence
which is derived by such a node. The algorithm then
goes to a loop of k. In each loop, the best node is
popped off from the agenda and is stored in a set r.
The algorithm adds alternative candidate nodes (or
sequences) to the agenda via a double nested loop.
The idea is that, when an optimal node (or sequence)
is popped off, we have to push to the agenda all
nodes (sequences) which are slightly worse than the
just popped one. The interpretation of slightly worse
is to replace one edge from the popped node (se-
quence). The slightly worse sequences can be found
by the exact heuristic derived from the first Viterbi
forward pass.
Figure 3 shows an example of the push operations
for a lattice of T = 4, Y = 4. Suppose an optimal
node 2:B (in red, standing for node B at position 2,
representing the sequence of 0:A 1:D 2:B 3:C) is
popped off, new nodes of 1:A, 1:B, 1:C and 0:B,
0:C and 0:D are pushed to the agenda according to
the double nested for loop in Algorithm 3. Each
of the pushed nodes represents a sequence, for ex-
ample, node 1:B represents a sequence which con-
sists of three parts: Viterb sequence from start to
1:B (0:C 1:B), 2:B and forward link of 2:B (3:C
in this case). All of these pushed nodes (sequences)
are served as candidates for the next agenda pop op-
eration.
The algorithm terminates the loop once it has op-
timal k nodes. The k-best sequences can be de-
rived by the k optimal nodes. This algorithm has
</bodyText>
<page confidence="0.99588">
614
</page>
<figureCaption confidence="0.9982875">
Figure 3: Alternative nodes push after popping an opti-
mal node.
</figureCaption>
<bodyText confidence="0.9953448">
computation complexity of TL2 + TL for both best
and worst cases, with the first term accounting for
Viterbi forward pass and the second term account-
ing for A* backward process. The bottleneck is thus
at the Viterbi forward pass.
</bodyText>
<figure confidence="0.737578619047619">
Algorithm 3 K-Best Viterbi A* algorithm
1: forward()
2: push L best nodes to agenda q
3: c = 0
4: r = {}
5: while c &lt; K do
6: Node n = q.pop()
7: r=rUn
8: for i = n.t − 1; i &gt; 0; i − − do
9: for j = 0; j &lt; L; j + + do
10: if j! = n.backlink.y then
11: create new node s at position i and label j
12: sjorwardlink = n
13: q.push(s)
14: end if
15: end for
16: n = n.backlink
17: end for
18: c + +
19: end while
20: return K best sequences derived by r
</figure>
<sectionHeader confidence="0.92426" genericHeader="method">
4 Proposed Algorithms
</sectionHeader>
<bodyText confidence="0.998684555555556">
In this section, we propose A* based sequen-
tial decoding algorithms that can efficiently handle
datasets with a large number of labels. In particular,
we first propose the A* and the iterative A* decod-
ing algorithm for 1-best sequential decoding. We
then extend the 1-best A* algorithm to a k-best A*
decoding algorithm. We finally apply the iterative
process to the Viterbi A* algorithm, resulting in the
iterative Viterbi A* decoding algorithm.
</bodyText>
<subsectionHeader confidence="0.996614">
4.1 1-Best A*
</subsectionHeader>
<bodyText confidence="0.999965666666667">
A*(Hart et al., 1968; Russell and Norvig, 1995), as
a classic search algorithm, has been successfully ap-
plied in syntactic parsing (Klein and Manning, 2003;
Pauls and Klein, 2009). The general idea of A* is to
consider labels yt which are likely to result in the
best sequence using a score f as follows.
</bodyText>
<equation confidence="0.999102">
f(y) = g(y) + h(y), (8)
</equation>
<bodyText confidence="0.999946111111111">
where g(y) is the score from start to the current node
and h(y) is a heuristic which estimates the score
from the current node to the target. A* uses an
agenda (based on the f score) to decide which nodes
are to be processed next. If the heuristic satisfies the
condition h(yt−1) ? e(yt−1, yt) + h(yt), then h is
called monotone or admissible. In such a case, A* is
guaranteed to find the best sequence. We start with
the naive (but admissible) heuristic as follows
</bodyText>
<equation confidence="0.996706">
T− 1
h(yt) = � (maxn(yi) + maxe(yi−1,yi)). (9)
i=t+1
</equation>
<bodyText confidence="0.99997028">
That is, the heuristic of node yt to the end is the sum
of max edge scores between any two positions and
max node scores per position. Similar to (Pauls and
Klein, 2009) we explore the heuristic in different
coarse levels. We apply the Viterbi backward pass
to different degenerate lattices and use the Viterbi
backward scores as different heuristics. Different
degenerate lattices are generated from different it-
erations of Algorithm 1: The m-th iteration corre-
sponds to a lattice of (2m +1)*T nodes. A larger m
indicates a more accurate heuristic, which results in
a more efficient A* search (fewer nodes being pro-
cessed). However, this efficiency comes with the
price that such an accurate heuristic requires more
computation time in the Viterbi backward pass. In
our experiments, we try the naive heuristic and the
following values of m: 0, 3, 6 and 9.
In the best case, A* expands one node per posi-
tion, and each expansion results in the push of all
nodes at next position to the agenda. The search is
similar to the beam search with beam size being 1.
The complexity is thus TL. In the worst case, A*
expands every node per position, and each expan-
sion results in the push of all nodes at next position
to the agenda. The complexity thus becomes TL2.
</bodyText>
<subsectionHeader confidence="0.997807">
4.2 1-Best Iterative A*
</subsectionHeader>
<bodyText confidence="0.999984285714286">
The iterative process as described in the iterative
Viterbi decoding can be used to boost A* algorithm,
resulting in the iterative A* algorithm. For simplic-
ity, we only make use of the naive heuristic in Equa-
tion (9) in the iterative A* algorithm. We initialize
the lattice with one active label and one degenerate
label at each position (see Figure 1 (b)). We then run
A* algorithm on the degenerate lattice and get the
best sequence. If the sequence is active we return
it. Otherwise we expand the lattice in each iteration
until we find the best active sequence. Similar to
iterative Viterbi algorithm, iterative A* has the com-
plexity of T and TL2 for the best and worst cases
respectively.
</bodyText>
<subsectionHeader confidence="0.997791">
4.3 K-Best A*
</subsectionHeader>
<bodyText confidence="0.906273">
The extension from 1-best A* to k-best A* is again
due to the memorization of k-best labels per node.
T 0 1 2 3
</bodyText>
<figure confidence="0.999783461538462">
A A A A
B
C
D
B
C
D
B
C
D
B
C
D
</figure>
<page confidence="0.996341">
615
</page>
<tableCaption confidence="0.998971">
Table 1: Best case and worst case computational complexity of various decoding algorithms.
</tableCaption>
<table confidence="0.9956849">
1-best decoding K-best decoding
best case worst case best case worst case
beam TL TL KTL KTL
Viterbi TL2 TL2 KTL2 KTL2
iterative Viterbi T TL2 N/A N/A
Carpediem TL log L TL2 N/A N/A
A* TL TL2 KTL KTL2
iterative A* T TL2 N/A N/A
Viterbi A* N/A N/A TL2 + KTL TL2 + KTL
iterative Viterbi A* N/A N/A T + KT TL2 + KTL
</table>
<bodyText confidence="0.999932571428571">
We use either the naive heuristic (Equation (9)) or
different coarse level heuristics by setting m to be 0,
3, 6 or 9 (see Section 4.1). The first k nodes which
are popped off the agenda can be used to back track
the k-best sequences. The k-best A* algorithm has
the computational complexity of KTL and KTL2
for best and worst cases respectively.
</bodyText>
<subsectionHeader confidence="0.997269">
4.4 K-Best Iterative Viterbi A*
</subsectionHeader>
<bodyText confidence="0.998900488888889">
We now present the k-best iterative Viterbi A* algo-
rithm (see Algorithm 4) which applies the iterative
process to k-best Viterbi A* algorithm. The major
difference between 1-best iterative Viterbi A* algo-
rithm (Algorithm 1) and this algorithm is that the
latter calls the k-best Vitebi A* (Algorithm 3) after
the best sequence is found. If the k-best sequences
are all active, we terminate the algorithm and return
the k-best sequences. If we cannot find either the
best active sequence or the k-best active sequences,
we expand the lattice to continue the search in the
next iteration.
As in the iterative Viterbi algorithm (see Section
3.2), nodes are pruned at each position in forward
or backward passes. Efficient pruning contributes
significantly to speeding up decoding. Therefore, to
have a tighter (higher) lower bound lb is important.
We initialize the lower bound lb with the k-th best
score from beam search (with beam size being k) at
line 1. Note that the beam search is performed on the
original lattice which consists of L active labels per
position. The beam search time is negligible com-
pared to the total decoding time. At line 16, we up-
date lb as follows. We enumerate the best active se-
quences backtracked by the nodes at position T − 1.
If the current lb is less than the k-th active sequence
score, we update the lb with the k-th active sequence
score (we do not update lb if there are less than k ac-
tive sequences). At line 19, we use the sequences
returned from Viterbi A* algorithm to update the lb
in the same manner. To enable this update, we re-
quest the Viterbi A* algorithm to return k&apos;, k&apos; &gt; k,
sequences (line 10). A larger number of k&apos; results
in a higher chance to find the k-th active sequence,
which in turn offers a tighter (higher) lb, but it comes
with the expense of additional time (the backward
A* process takes O(TL) time to return one more
sequence). In experiments, we found the lb updates
on line 1 and line 16 are essential for fast decoding.
The updating of lb using Viterbi A* sequences (line
19) can boost the decoding speed further. We exper-
imented with different k&apos; values (k&apos; = nk, where n
is an integer) and selected k&apos; = 2k which results in
the largest decoding speed boost.
Algorithm 4 K-Best iterative Viterbi A* algorithm
</bodyText>
<listItem confidence="0.773042545454545">
1: lb = k-th best (original lattice)
2: init lattice
3: for i = 0;; i + + do
4: if i�2 == 0 then
5: y = forward()
6: else
7: y = backward()
8: end if
9: if y consists of active labels only then
10: ys= k-best Viterbi A* (Algorithm 3)
11: if ys consists of active sequences only then
12: return ys
13: end if
14: end if
15: if lb &lt; k-th best(lattice) then
16: lb = k-th best(lattice)
17: end if
18: if lb &lt; k-th best(ys) then
19: lb = k-th best(ys)
20: end if
21: expand lattice
22: end for
</listItem>
<sectionHeader confidence="0.994813" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999916666666667">
We compare aforementioned 1-best and k-best se-
quential decoding algorithms using five datasets in
this section.
</bodyText>
<subsectionHeader confidence="0.9967">
5.1 Experimental setting
</subsectionHeader>
<bodyText confidence="0.999719857142857">
We apply 1-best and k-best sequential decoding al-
gorithms to five NLP tagging tasks: Penn TreeBank
(PTB) POS tagging, CoNLL2000 joint POS tag-
ging and chunking, CoNLL 2003 joint POS tagging,
chunking and named entity tagging, HPSG supertag-
ging (Matsuzaki et al., 2007) and a search query
named entity recognition (NER) dataset. We used
</bodyText>
<page confidence="0.997706">
616
</page>
<bodyText confidence="0.999478272727273">
sections 02-21 of PTB for training and section 23
for testing in POS task. As in (Kaji et al., 2010),
we combine the POS tags and chunk tags to form
joint tags for CoNLL 2000 dataset, e.g., NN|B-NP.
Similarly we combine the POS tags, chunk tags, and
named entity tags to form joint tags for CoNLL 2003
dataset, e.g., PRP$|I-NP|O. Note that by such tag
joining, we are able to offer different tag decodings
(for example, chunking and named entity tagging)
simultaneously. This indeed is one of the effective
approaches for joint tag decoding problems. The
search query NER dataset is an in-house annotated
dataset which assigns semantic labels, such as prod-
uct, business tags to web search queries.
Table 2 shows the training and test sets size (sen-
tence #), the average token length of test dataset and
the label size for the five datasets. POS and su-
pertag datasets assign tags to tokens while CoNLL
2000 , CoNLL 2003 and search query datasets as-
sign tags to phrases. We use the standard BIO en-
coding for CoNLL 2000, CoNLL 2003 and search
query datasets.
</bodyText>
<tableCaption confidence="0.9846865">
Table 2: Training and test datasets size, average token
length of test set and label size for five datasets.
</tableCaption>
<table confidence="0.998325666666667">
training # test # token length label size
POS 39831 2415 23 45
CoNLL2000 8936 2012 23 319
CoNLL2003 14987 3684 12 443
Supertag 37806 2291 22 2602
search query 79569 6867 3 323
</table>
<bodyText confidence="0.99978935">
Due to the long CRF training time (days to weeks
even for stochastic gradient descent training) for
these large label size datasets, we choose the percep-
tron algorithm for training. The models are averaged
over 10 iterations (Collins, 2002). The training time
takes minutes to hours for all datasets. We note that
the selection of training algorithm does not affect
the decoding process: the decoding is identical for
both CRF and perceptron training algorithms. We
use the common features which are adopted in previ-
ous studies, for example (Sha and Periera, 2003). In
particular, we use the unigrams of the current and its
neighboring words, word bigrams, prefixes and suf-
fixes of the current word, capitalization, all-number,
punctuation, and tag bigrams for POS, CoNLL2000
and CoNLL 2003 datasets. For supertag dataset,
we use the same features for the word inputs, and
the unigrams and bigrams for gold POS inputs. For
search query dataset, we use the same features plus
gazetteer based features.
</bodyText>
<subsectionHeader confidence="0.942676">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999890153846154">
We report the token accuracy for all datasets to facil-
itate comparison to previous work. They are 97.00,
94.70, 95.80, 90.60 and 88.60 for POS, CoNLL
2000, CoNLL 2003, supertag, and search query re-
spectively. We note that all decoding algorithms as
listed in Section 3 and Section 4 are exact. That is,
they produce exactly the same accuracy. The accu-
racy we get for the first four tasks is comparable to
the state-of-the-art. We do not have a baseline to
compare with for the last dataset as it is not pub-
licly available7. Higher accuracy may be achieved if
more task specific features are introduced on top of
the standard features. As this paper is more con-
cerned with the decoding speed, the feature engi-
neering is beyond the scope of this paper.
Table 3 shows how many iterations in average
are required for iterative Viterbi and iterative Viterbi
A* algorithms. Although the max iteration size is
bounded to Fl092 L] for each position (for exam-
ple, 9 for CoNLL 2003 dataset), the total iteration
number for the whole lattice may be greater than
Fl092 L] as different positions may not expand at
the same time. Despite the large number of itera-
tions used in iterative based algorithms (especially
iterative Viterbi A* algorithm), the algorithms are
still very efficient (see below).
</bodyText>
<tableCaption confidence="0.977985">
Table 3: Iteration numbers of iterative Viterbi and itera-
tive Viterbi A* algorithms for five datasets.
</tableCaption>
<bodyText confidence="0.99824136">
Table 4 and 5 show the decoding speed (sen-
tences per second) of 1-best and 5-best decoding al-
gorithms respectively. The proposed decoding algo-
rithms and the largest decoding speeds across differ-
ent decoding algorithms (other than beam) are high-
lighted in bold. We exclude the time for feature ex-
traction in computing the speed. The beam search
decoding is also shown as a baseline. We note that
beam decoding is the only approximate decoding al-
gorithm in this table. All other decoding algorithms
produce exactly the same accuracy, which is usually
much better than the accuracy of beam decoding.
For 1-best decoding, iterative Viterbi always out-
performs other ones. A* with a proper heuristic de-
noted as A* (best), that is, the best A* using naive
heuristic or the values of m being 0, 3, 6 or 9 (see
Section 4.1), can be the second best choice (ex-
cept for the POS task), although the gap between
iterative Viterbi and A* is significant. For exam-
ple, for CoNLL 2003 dataset, the former can de-
code 2239 sentences per second while the latter only
decodes 225 sentences per second. The iterative
process successfully boosts the decoding speed of
iterative Viterbi compared to Viterbi, but it slows
down the decoding speed of iterative A* compared
</bodyText>
<footnote confidence="0.729161">
7The lower accuracy is due to the dynamic nature of queries:
many of test query tokens are unseen in the training set.
</footnote>
<figure confidence="0.988719294117647">
iter Viter
iter Viter A*
CoNLL2000
8.76
16.40
CoNLL2003
9.18
15.41
search query
6.71
9.48
14.42
POS
6.32
Supertag
10.63
18.62
</figure>
<page confidence="0.985865">
617
</page>
<bodyText confidence="0.999946720930233">
to A*(best). This is because in the Viterbi case,
the iterative process has a node pruning procedure,
while it does not have such pruning in A*(best)
algorithm. Take CoNLL 2003 data as an exam-
ple, the removal of the pruning slows down the 1-
best iterative Viterbi decoding from 2239 to 604
sentences/second. Carpediem algorithm performs
poorly in four out of five tasks. This can be ex-
plained as follows. The Carpediem implicitly as-
sumes that the node scores are the dominant factors
to determine the best sequence. However, this as-
sumption does not hold as the edge scores play an
important role.
For 5-best decoding, k-best Viterbi decoding is
very slow. A* with a proper heuristic is still slow.
For example, it only reaches 11 sentences per second
for CoNLL 2003 dataset. The classic Viterbi A* can
usually obtain a decent decoding speed, for example,
40 sentences per second for CoNLL 2003 dataset.
The only exception is supertag dataset, on which the
Viterbi A* decodes 0.1 sentence per second while
the A* decodes 3. This indicates the scalability is-
sue of Viterbi A* algorithm for datasets with more
than one thousand labels. The proposed iterative
Viterbi A* is clearly the winner. It speeds up the
Viterbi A* to factors of 4, 7, 360, and 3 for CoNLL
2000, CoNLL 2003, supertag and query search data
respectively. The decoding speed of iterative Viterbi
A* can even be comparable to that of beam search.
Figure 4 shows k-best decoding algorithms de-
coding speed with respect to different k values for
CoNLL 2003 data . The Viterbi A* and iterative
Viterbi A* algorithms are significantly faster than
the Viterbi and A*(best) algorithms. Although the
iterative Viterbi A* significantly outperforms the
Viterbi A* for k &lt; 30, the speed of the former con-
verges to the latter when k becomes 90 or larger.
This is expected as the k-best sequences span over
the whole lattice: the earlier iteration in iterative
Viterbi A* algorithm cannot provide the k-best se-
quences using the degenerate lattice. The over-
head of multiple iterations slows down the decoding
speed compared to the Viterbi A* algorithm.
</bodyText>
<figure confidence="0.985274076923077">
200
180
160
140
120
100
80
60
40
20
0
10 20 30 40 50 60 70 80 90 100
k
</figure>
<figureCaption confidence="0.841858">
Figure 4: Decoding speed of k-best decoding algorithms
for various k for CoNLL 2003 dataset.
</figureCaption>
<sectionHeader confidence="0.999636" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999992742857143">
The Viterbi algorithm is the only exact algorithm
widely adopted in the NLP applications. Esposito
and Radicioni (2009) proposed an algorithm which
opens necessary nodes in a lattice in searching the
best sequence. The staggered decoding (Kaji et al.,
2010) forms the basis for our work on iterative based
decoding algorithms. Apart from the exact decod-
ing, approximate decoding algorithms such as beam
search are also related to our work. Tsuruoka and
Tsujii (2005) proposed easiest-first deterministic de-
coding. Siddiqi and Moore (2005) presented the pa-
rameter tying approach for fast inference in HMMs.
A similar idea was applied to CRFs as well (Cohn,
2006; Jeong, 2009). We note that the exact algo-
rithm always guarantees the optimality which can-
not be attained in approximate algorithms.
In terms of k-best parsing, Huang and Chiang
(2005) proposed an efficient algorithm which is sim-
ilar to the k-best Viterbi A* algorithm presented in
this paper. Pauls and Klein (2009) proposed an algo-
rithm which replaces the Viterbi forward pass with
an A* search. Their algorithm optimizes the Viterbi
pass, while the proposed iterative Viterbi A* algo-
rithm optimizes both Viterbi and A* passes.
This paper is also related to the coarse to fine
PCFG parsing (Charniak et al., 2006) as the degen-
erate labels can be treated as coarse levels. How-
ever, the difference is that the coarse-to-fine parsing
is an approximate decoding while ours is exact one.
In terms of different coarse levels of heuristic used
in A* decoding, this paper is related to the work of
hierarchical A* framework (Raphael, 2001; Felzen-
szwalb et al., 2007). In terms of iterative process,
this paper is close to (Burkett et al., 2011) as both
exploit the search-and-expand approach.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999959777777778">
We have presented and evaluated the A* and itera-
tive A* algorithms for 1-best sequential decoding in
this paper. In addition, we proposed A* and iterative
Viterbi A* algorithm for k-best sequential decoding.
K-best Iterative A* algorithm can be several times
or orders of magnitude faster than the state-of-the-
art k-best decoding algorithm. It makes real-time
large-scale tagging applications with thousands of
labels feasible.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999466">
We wish to thank Yusuke Miyao and Nobuhiro Kaji
for providing us the HPSG Treebank data. We are
grateful for the invaluable comments offered by the
anonymous reviewers.
</bodyText>
<figure confidence="0.995092166666667">
sentences/second
● Viterbi
A*(best)
Viterbi A*
iterative Viterbi A*
● ● ● ●
</figure>
<page confidence="0.986676">
618
</page>
<tableCaption confidence="0.999897">
Table 4: Decoding speed (sentences per second) of 1-best decoding algorithms for five datasets.
</tableCaption>
<table confidence="0.999847285714286">
POS CoNLL2000 CoNLL2003 supertag query search
beam 7252 1381 1650 395 7571
Viterbi 2779 51 41 0.19 443
iterative Viterbi 5833 972 2239 213 6805
Carpediem 2638 14 20 0.15 243
A* (best) 802 131 225 8 880
iterative A* 1112 84 109 3 501
</table>
<tableCaption confidence="0.99854">
Table 5: Decoding speed (sentences per second) of 5-best decoding algorithms for five datasets.
</tableCaption>
<table confidence="0.998590833333333">
POS CoNLL2000 CoNLL2003 supertag query search
beam 2760 461 592 75 4354
Viterbi 19 0.41 0.25 0.12 3.83
A* (best) 205 4 11 3 92
Viterbi A* 1266 47 40 0.1 357
iterative Viterbi A* 788 200 295 36 1025
</table>
<sectionHeader confidence="0.996858" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999626229508197">
D. Burkett, D. Hall, and D. Klein. 2011. Optimal graph
search with iterated graph cuts. Proceedings of AAAI.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D.
Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore, M.
Pozar, and T. Vu. 2006. Multi-level coarse-to-fine
PCFG parsing. Proceedings of NAACL.
T. Cohn. 2006. Efficient inference in large conditional
random fields. Proceedings of ECML.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. Proceedings of EMNLP.
R. Esposito and D. P. Radicioni. 2009. Carpediem:
Optimizing the Viterbi Algorithm and Applications to
Supervised Sequential Learning. Journal of Machine
Learning Research.
P. Felzenszwalb and D. McAllester. 2007. The general-
ized A* architecture. Journal of Artificial Intelligence
Research.
P. E. Hart, N. J. Nilsson, and B. Raphael. 1968. A For-
mal Basis for the Heuristic Determination of Minimum
Cost Paths. IEEE Transactions on Systems Science
and Cybernetics.
L. Huang and D. Chiang. 2005. Better k-best parsing.
Proceedings of the International Workshops on Parsing
Technologies (IWPT).
M. Jeong, C. Y. Lin, and G. G. Lee. 2009. Efficient infer-
ence of CRFs for large-scale natural language data.
Proceedings of ACL-IJCNLP Short Papers.
N. Kaji, Y. Fujiwara, N. Yoshinaga, and M. Kitsuregawa.
2010. Efficient Staggered Decoding for Sequence La-
beling. Proceedings of ACL.
D. Klein and C. Manning. 2003. A* parsing: Fast exact
Viterbi parse selection. Proceedings of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. Proceedings of
ICML.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2007. Efficient
HPSG parsing with supertagging and CFG-filtering.
Proceedings of IJCAI.
A. Pauls and D. Klein. 2009. K-Best A* Parsing. Pro-
ceedings of ACL.
L. R. Rabiner. 1989. A tutorial on hidden Markov models
and selected applications in speech recognition. Pro-
ceedings of The IEEE.
C. Raphael. 2001. Coarse-to-fine dynamic program-
ming. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence.
S. Russell and P. Norvig. 1995. Artificial Intelligence: A
Modern Approach.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. Proceedings of HLT-NAACL.
S. M. Siddiqi and A. Moore. 2005. Fast inference and
learning in large-state-space HMMs. Proceedings of
ICML.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional in-
ference with the easiest-first strategy for tagging se-
quence data. Proceedings of HLT/EMNLP.
A. J. Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding algo-
rithm. IEEE Transactions on Information Theory.
</reference>
<page confidence="0.998639">
619
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.587174">
<title confidence="0.997794">Viterbi A* Algorithm for Sequential Decoding</title>
<author confidence="0.836017">Yi Chang</author>
<author confidence="0.836017">Bo Long</author>
<author confidence="0.836017">Jean-Francois Anlei Dong</author>
<author confidence="0.836017">Sathiya Keerthi</author>
<author confidence="0.836017">Su-Lin</author>
<affiliation confidence="0.843604">Yahoo!</affiliation>
<address confidence="0.9976535">701 First Avenue, CA 94089,</address>
<abstract confidence="0.996329842105263">Sequential modeling has been widely used in a variety of important applications including named entity recognition and shallow parsing. However, as more and more real time large-scale tagging applications arise, decoding speed has become a bottleneck for existing sequential tagging algorithms. In this pawe propose A*, iterative A*, A* and iterative Viterbi A* algorithms for sequential decoding. We show the efficiency of these proposed algorithms for five NLP tagging tasks. In particular, we show that iterative Viterbi A* decoding can be several times or orders of magnitude faster than the state-of-the-art algorithm for tagging tasks with a large number of labels. This algorithm makes real-time large-scale tagging applications with thousands of labels feasible.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Burkett</author>
<author>D Hall</author>
<author>D Klein</author>
</authors>
<title>Optimal graph search with iterated graph cuts.</title>
<date>2011</date>
<booktitle>Proceedings of AAAI.</booktitle>
<contexts>
<context position="35247" citStr="Burkett et al., 2011" startWordPosition="6284" endWordPosition="6287">imizes the Viterbi pass, while the proposed iterative Viterbi A* algorithm optimizes both Viterbi and A* passes. This paper is also related to the coarse to fine PCFG parsing (Charniak et al., 2006) as the degenerate labels can be treated as coarse levels. However, the difference is that the coarse-to-fine parsing is an approximate decoding while ours is exact one. In terms of different coarse levels of heuristic used in A* decoding, this paper is related to the work of hierarchical A* framework (Raphael, 2001; Felzenszwalb et al., 2007). In terms of iterative process, this paper is close to (Burkett et al., 2011) as both exploit the search-and-expand approach. 7 Conclusions We have presented and evaluated the A* and iterative A* algorithms for 1-best sequential decoding in this paper. In addition, we proposed A* and iterative Viterbi A* algorithm for k-best sequential decoding. K-best Iterative A* algorithm can be several times or orders of magnitude faster than the state-of-theart k-best decoding algorithm. It makes real-time large-scale tagging applications with thousands of labels feasible. Acknowledgments We wish to thank Yusuke Miyao and Nobuhiro Kaji for providing us the HPSG Treebank data. We a</context>
</contexts>
<marker>Burkett, Hall, Klein, 2011</marker>
<rawString>D. Burkett, D. Hall, and D. Klein. 2011. Optimal graph search with iterated graph cuts. Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
<author>M Elsner</author>
<author>J Austerweil</author>
<author>D Ellis</author>
<author>I Haxton</author>
<author>C Hill</author>
<author>R Shrivaths</author>
<author>J Moore</author>
<author>M Pozar</author>
<author>T Vu</author>
</authors>
<title>Multi-level coarse-to-fine PCFG parsing.</title>
<date>2006</date>
<booktitle>Proceedings of NAACL.</booktitle>
<contexts>
<context position="34824" citStr="Charniak et al., 2006" startWordPosition="6211" endWordPosition="6214">, 2006; Jeong, 2009). We note that the exact algorithm always guarantees the optimality which cannot be attained in approximate algorithms. In terms of k-best parsing, Huang and Chiang (2005) proposed an efficient algorithm which is similar to the k-best Viterbi A* algorithm presented in this paper. Pauls and Klein (2009) proposed an algorithm which replaces the Viterbi forward pass with an A* search. Their algorithm optimizes the Viterbi pass, while the proposed iterative Viterbi A* algorithm optimizes both Viterbi and A* passes. This paper is also related to the coarse to fine PCFG parsing (Charniak et al., 2006) as the degenerate labels can be treated as coarse levels. However, the difference is that the coarse-to-fine parsing is an approximate decoding while ours is exact one. In terms of different coarse levels of heuristic used in A* decoding, this paper is related to the work of hierarchical A* framework (Raphael, 2001; Felzenszwalb et al., 2007). In terms of iterative process, this paper is close to (Burkett et al., 2011) as both exploit the search-and-expand approach. 7 Conclusions We have presented and evaluated the A* and iterative A* algorithms for 1-best sequential decoding in this paper. I</context>
</contexts>
<marker>Charniak, Johnson, Elsner, Austerweil, Ellis, Haxton, Hill, Shrivaths, Moore, Pozar, Vu, 2006</marker>
<rawString>E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore, M. Pozar, and T. Vu. 2006. Multi-level coarse-to-fine PCFG parsing. Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
</authors>
<title>Efficient inference in large conditional random fields.</title>
<date>2006</date>
<booktitle>Proceedings of ECML.</booktitle>
<contexts>
<context position="34208" citStr="Cohn, 2006" startWordPosition="6111" endWordPosition="6112">widely adopted in the NLP applications. Esposito and Radicioni (2009) proposed an algorithm which opens necessary nodes in a lattice in searching the best sequence. The staggered decoding (Kaji et al., 2010) forms the basis for our work on iterative based decoding algorithms. Apart from the exact decoding, approximate decoding algorithms such as beam search are also related to our work. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (2005) presented the parameter tying approach for fast inference in HMMs. A similar idea was applied to CRFs as well (Cohn, 2006; Jeong, 2009). We note that the exact algorithm always guarantees the optimality which cannot be attained in approximate algorithms. In terms of k-best parsing, Huang and Chiang (2005) proposed an efficient algorithm which is similar to the k-best Viterbi A* algorithm presented in this paper. Pauls and Klein (2009) proposed an algorithm which replaces the Viterbi forward pass with an A* search. Their algorithm optimizes the Viterbi pass, while the proposed iterative Viterbi A* algorithm optimizes both Viterbi and A* passes. This paper is also related to the coarse to fine PCFG parsing (Charni</context>
</contexts>
<marker>Cohn, 2006</marker>
<rawString>T. Cohn. 2006. Efficient inference in large conditional random fields. Proceedings of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1261" citStr="Collins, 2002" startWordPosition="179" endWordPosition="180">est iterative A*, k-best A* and k-best iterative Viterbi A* algorithms for sequential decoding. We show the efficiency of these proposed algorithms for five NLP tagging tasks. In particular, we show that iterative Viterbi A* decoding can be several times or orders of magnitude faster than the state-of-the-art algorithm for tagging tasks with a large number of labels. This algorithm makes real-time large-scale tagging applications with thousands of labels feasible. 1 Introduction Sequence tagging algorithms including HMMs (Rabiner, 1989), CRFs (Lafferty et al., 2001), and Collins’s perceptron (Collins, 2002) have been widely employed in NLP applications. Sequential decoding, which finds the best tag sequences for given inputs, is an important part of the sequential tagging framework. Traditionally, the Viterbi algorithm (Viterbi, 1967) is used. This algorithm is quite efficient when the label size of problem modeled is low. Unfortunately, due to its O(T L2) time complexity, where T is the input token size and L is the label size, the Viterbi decoding can become prohibitively slow when the label size is large (say, larger than 200). It is not uncommon that the problem modeled consists of more than</context>
<context position="4370" citStr="Collins, 2002" startWordPosition="667" endWordPosition="668">itude faster than the state-of-the-art 1Implemented in both CRFPP (http://crfpp.sourceforge.net/) and LingPipe (http://alias-i.com/lingpipe/) packages. 611 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 611–619, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics k-best decoding algorithm. This algorithm makes real-time large-scale tagging applications with thousands of labels feasible. 2 Problem formulation In this section, we formulate the sequential decoding problem in the context of perceptron algorithm (Collins, 2002) and CRFs (Lafferty et al., 2001). All the discussions apply to HMMs as well. Formally, a perceptron model is θkfk(yt,yt−1,xt), (1) and a CRFs model is T K p(y|x) = Z(x) exp{E E θkfk(yt, yt−1, xt)}, (2) t=1 k=1 where x and y is an observation sequence and a label sequence respectively, t is the sequence position, T is the sequence size, fk are feature functions and K is the number of feature functions. θk are the parameters that need to be estimated. They represent the importance of feature functions fk in prediction. For CRFs, Z(x) is an instance-specific normalization function θkfk(yt,yt−1,x</context>
<context position="27605" citStr="Collins, 2002" startWordPosition="4988" endWordPosition="4989">. We use the standard BIO encoding for CoNLL 2000, CoNLL 2003 and search query datasets. Table 2: Training and test datasets size, average token length of test set and label size for five datasets. training # test # token length label size POS 39831 2415 23 45 CoNLL2000 8936 2012 23 319 CoNLL2003 14987 3684 12 443 Supertag 37806 2291 22 2602 search query 79569 6867 3 323 Due to the long CRF training time (days to weeks even for stochastic gradient descent training) for these large label size datasets, we choose the perceptron algorithm for training. The models are averaged over 10 iterations (Collins, 2002). The training time takes minutes to hours for all datasets. We note that the selection of training algorithm does not affect the decoding process: the decoding is identical for both CRF and perceptron training algorithms. We use the common features which are adopted in previous studies, for example (Sha and Periera, 2003). In particular, we use the unigrams of the current and its neighboring words, word bigrams, prefixes and suffixes of the current word, capitalization, all-number, punctuation, and tag bigrams for POS, CoNLL2000 and CoNLL 2003 datasets. For supertag dataset, we use the same f</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Esposito</author>
<author>D P Radicioni</author>
</authors>
<title>Carpediem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="2001" citStr="Esposito and Radicioni (2009)" startWordPosition="300" endWordPosition="304">en inputs, is an important part of the sequential tagging framework. Traditionally, the Viterbi algorithm (Viterbi, 1967) is used. This algorithm is quite efficient when the label size of problem modeled is low. Unfortunately, due to its O(T L2) time complexity, where T is the input token size and L is the label size, the Viterbi decoding can become prohibitively slow when the label size is large (say, larger than 200). It is not uncommon that the problem modeled consists of more than 200 labels. The Viterbi algorithm cannot find the best sequences in tolerable response time. To resolve this, Esposito and Radicioni (2009) have proposed a Carpediem algorithm which opens only necessary nodes in searching the best sequence. More recently, Kaji et al. (2010) proposed a staggered decoding algorithm, which proves to be very efficient on datasets with a large number of labels. What the aforementioned literature does not cover is the k-best sequential decoding problem, which is indeed frequently required in practice. For example to pursue a high recall ratio, a named entity recognition system may have to adopt k-best sequences in case the true entities are not recognized at the best one. The k-best parses have been ex</context>
<context position="14064" citStr="Esposito and Radicioni (2009)" startWordPosition="2500" endWordPosition="2503"> cases respectively. This can be proved as follows (Kaji et al., 2010). At the m-th iteration in Algorithm 1, iterative Viterbi decoding requires order of T4m time because there are 2m active labels (plus one degenerate label). Therefore, it has Emi=0 T4i time complexity if it terminates at the m-th iteration. In the best case in which m = 0, the time complexity is T. In the worst case in which m = Flog2 L] − 1 (F.] is the ceiling function which maps a real number to the smallest following integer), the time complexity is order of TL2 because ��log� L�−1 T4i &lt; 4/3TL2. i=0 3.3 1-Best Carpediem Esposito and Radicioni (2009) have proposed a novel 1-best6 sequential decoding algorithm, Carpediem, which attempts to open only necessary nodes in searching the best sequence in a given lattice. Carpediem has the complexity of TL log L and TL2 for the best and worst cases respectively. We skip the description of this algorithm due to space limitations. Carpediem is used as a baseline in our experiments for decoding speed comparison. 3.4 K-Best Viterbi In order to produce k-best sequences, it is not enough to store 1-best label per node, as the kbest sequences may include suboptimal labels. The k-best sequential decoding</context>
<context position="33667" citStr="Esposito and Radicioni (2009)" startWordPosition="6022" endWordPosition="6025">atter when k becomes 90 or larger. This is expected as the k-best sequences span over the whole lattice: the earlier iteration in iterative Viterbi A* algorithm cannot provide the k-best sequences using the degenerate lattice. The overhead of multiple iterations slows down the decoding speed compared to the Viterbi A* algorithm. 200 180 160 140 120 100 80 60 40 20 0 10 20 30 40 50 60 70 80 90 100 k Figure 4: Decoding speed of k-best decoding algorithms for various k for CoNLL 2003 dataset. 6 Related work The Viterbi algorithm is the only exact algorithm widely adopted in the NLP applications. Esposito and Radicioni (2009) proposed an algorithm which opens necessary nodes in a lattice in searching the best sequence. The staggered decoding (Kaji et al., 2010) forms the basis for our work on iterative based decoding algorithms. Apart from the exact decoding, approximate decoding algorithms such as beam search are also related to our work. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (2005) presented the parameter tying approach for fast inference in HMMs. A similar idea was applied to CRFs as well (Cohn, 2006; Jeong, 2009). We note that the exact algorithm always gua</context>
</contexts>
<marker>Esposito, Radicioni, 2009</marker>
<rawString>R. Esposito and D. P. Radicioni. 2009. Carpediem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Felzenszwalb</author>
<author>D McAllester</author>
</authors>
<title>The generalized A* architecture.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<marker>Felzenszwalb, McAllester, 2007</marker>
<rawString>P. Felzenszwalb and D. McAllester. 2007. The generalized A* architecture. Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P E Hart</author>
<author>N J Nilsson</author>
<author>B Raphael</author>
</authors>
<title>A Formal Basis for the Heuristic Determination of Minimum Cost Paths.</title>
<date>1968</date>
<booktitle>IEEE Transactions on Systems Science and Cybernetics.</booktitle>
<contexts>
<context position="19016" citStr="Hart et al., 1968" startWordPosition="3428" endWordPosition="3431">nd if 15: end for 16: n = n.backlink 17: end for 18: c + + 19: end while 20: return K best sequences derived by r 4 Proposed Algorithms In this section, we propose A* based sequential decoding algorithms that can efficiently handle datasets with a large number of labels. In particular, we first propose the A* and the iterative A* decoding algorithm for 1-best sequential decoding. We then extend the 1-best A* algorithm to a k-best A* decoding algorithm. We finally apply the iterative process to the Viterbi A* algorithm, resulting in the iterative Viterbi A* decoding algorithm. 4.1 1-Best A* A*(Hart et al., 1968; Russell and Norvig, 1995), as a classic search algorithm, has been successfully applied in syntactic parsing (Klein and Manning, 2003; Pauls and Klein, 2009). The general idea of A* is to consider labels yt which are likely to result in the best sequence using a score f as follows. f(y) = g(y) + h(y), (8) where g(y) is the score from start to the current node and h(y) is a heuristic which estimates the score from the current node to the target. A* uses an agenda (based on the f score) to decide which nodes are to be processed next. If the heuristic satisfies the condition h(yt−1) ? e(yt−1, y</context>
</contexts>
<marker>Hart, Nilsson, Raphael, 1968</marker>
<rawString>P. E. Hart, N. J. Nilsson, and B. Raphael. 1968. A Formal Basis for the Heuristic Determination of Minimum Cost Paths. IEEE Transactions on Systems Science and Cybernetics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>Proceedings of the International Workshops on Parsing Technologies (IWPT).</booktitle>
<contexts>
<context position="34393" citStr="Huang and Chiang (2005)" startWordPosition="6139" endWordPosition="6142">ggered decoding (Kaji et al., 2010) forms the basis for our work on iterative based decoding algorithms. Apart from the exact decoding, approximate decoding algorithms such as beam search are also related to our work. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (2005) presented the parameter tying approach for fast inference in HMMs. A similar idea was applied to CRFs as well (Cohn, 2006; Jeong, 2009). We note that the exact algorithm always guarantees the optimality which cannot be attained in approximate algorithms. In terms of k-best parsing, Huang and Chiang (2005) proposed an efficient algorithm which is similar to the k-best Viterbi A* algorithm presented in this paper. Pauls and Klein (2009) proposed an algorithm which replaces the Viterbi forward pass with an A* search. Their algorithm optimizes the Viterbi pass, while the proposed iterative Viterbi A* algorithm optimizes both Viterbi and A* passes. This paper is also related to the coarse to fine PCFG parsing (Charniak et al., 2006) as the degenerate labels can be treated as coarse levels. However, the difference is that the coarse-to-fine parsing is an approximate decoding while ours is exact one.</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>L. Huang and D. Chiang. 2005. Better k-best parsing. Proceedings of the International Workshops on Parsing Technologies (IWPT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jeong</author>
<author>C Y Lin</author>
<author>G G Lee</author>
</authors>
<title>Efficient inference of CRFs for large-scale natural language data.</title>
<date>2009</date>
<journal>Proceedings of ACL-IJCNLP Short Papers.</journal>
<marker>Jeong, Lin, Lee, 2009</marker>
<rawString>M. Jeong, C. Y. Lin, and G. G. Lee. 2009. Efficient inference of CRFs for large-scale natural language data. Proceedings of ACL-IJCNLP Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kaji</author>
<author>Y Fujiwara</author>
<author>N Yoshinaga</author>
<author>M Kitsuregawa</author>
</authors>
<title>Efficient Staggered Decoding for Sequence Labeling.</title>
<date>2010</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="2136" citStr="Kaji et al. (2010)" startWordPosition="322" endWordPosition="325">is quite efficient when the label size of problem modeled is low. Unfortunately, due to its O(T L2) time complexity, where T is the input token size and L is the label size, the Viterbi decoding can become prohibitively slow when the label size is large (say, larger than 200). It is not uncommon that the problem modeled consists of more than 200 labels. The Viterbi algorithm cannot find the best sequences in tolerable response time. To resolve this, Esposito and Radicioni (2009) have proposed a Carpediem algorithm which opens only necessary nodes in searching the best sequence. More recently, Kaji et al. (2010) proposed a staggered decoding algorithm, which proves to be very efficient on datasets with a large number of labels. What the aforementioned literature does not cover is the k-best sequential decoding problem, which is indeed frequently required in practice. For example to pursue a high recall ratio, a named entity recognition system may have to adopt k-best sequences in case the true entities are not recognized at the best one. The k-best parses have been extensively studied in syntactic parsing context (Huang, 2005; Pauls and Klein, 2009), but it is not well accommodated in sequential deco</context>
<context position="7786" citStr="Kaji et al., 2010" startWordPosition="1264" endWordPosition="1267">yt+1 + e(yt, yt+1) + n(yt+1)). (7) Note that βyT−1 = 0 at initialization. The max score can be computed using maxy0(β0 + n(y0)). We can use either forward or backward pass to compute the best sequence. Table 1 summarizes the computational complexity of all decoding algorithms including Viterbi, which has the complexity of TL2 for both best and worst cases. Note that N/A means the decoding algorithms are not applicable (for example, iterative Viterbi is not applicable to k-best decoding). The proposed algorithms (see Section 4) are highlighted in bold. 3.2 1-Best iterative Viterbi Kaji et al. (Kaji et al., 2010) presented an efficient sequential decoding algorithm named staggered decoding. We use the name iterative Viterbi to describe 2With the constraint that the path consists of one and only one node at each position. 3We ignore the feature size terms for simplicity. T E t=1 f(y, x) = K E k=1 E Z(x) = Y T E t=1 exp{ K E k=1 arg max T K1 θ1kf1k (yt, xt) + K2 Y E (E E t=1 k=1 k=1 max yt−1 max yt+1 612 this algorithm for the reason that the iterative process plays a central role in this algorithm. Indeed, this iterative process is generalized in this paper to handle k-best sequential decoding (see Sec</context>
<context position="13505" citStr="Kaji et al., 2010" startWordPosition="2396" endWordPosition="2399">e calls of forward and backward passes (in Algorithm 1) ensure the alternative updating/lowering of node forward and backward scores, which makes the node pruning in either forward pass (see Algorithm 2) or backward pass more efficient. The lower bound lb is updated once in each iteration of the main loop in Algorithm 1. While the forward and backwards scores of nodes gradually decrease and the lower bound lb increases, more and more nodes are pruned. The iterative Viterbi algorithm has computational complexity of T and TL2 for best and worst cases respectively. This can be proved as follows (Kaji et al., 2010). At the m-th iteration in Algorithm 1, iterative Viterbi decoding requires order of T4m time because there are 2m active labels (plus one degenerate label). Therefore, it has Emi=0 T4i time complexity if it terminates at the m-th iteration. In the best case in which m = 0, the time complexity is T. In the worst case in which m = Flog2 L] − 1 (F.] is the ceiling function which maps a real number to the smallest following integer), the time complexity is order of TL2 because ��log� L�−1 T4i &lt; 4/3TL2. i=0 3.3 1-Best Carpediem Esposito and Radicioni (2009) have proposed a novel 1-best6 sequential</context>
<context position="26124" citStr="Kaji et al., 2010" startWordPosition="4728" endWordPosition="4731"> expand lattice 22: end for 5 Experiments We compare aforementioned 1-best and k-best sequential decoding algorithms using five datasets in this section. 5.1 Experimental setting We apply 1-best and k-best sequential decoding algorithms to five NLP tagging tasks: Penn TreeBank (PTB) POS tagging, CoNLL2000 joint POS tagging and chunking, CoNLL 2003 joint POS tagging, chunking and named entity tagging, HPSG supertagging (Matsuzaki et al., 2007) and a search query named entity recognition (NER) dataset. We used 616 sections 02-21 of PTB for training and section 23 for testing in POS task. As in (Kaji et al., 2010), we combine the POS tags and chunk tags to form joint tags for CoNLL 2000 dataset, e.g., NN|B-NP. Similarly we combine the POS tags, chunk tags, and named entity tags to form joint tags for CoNLL 2003 dataset, e.g., PRP$|I-NP|O. Note that by such tag joining, we are able to offer different tag decodings (for example, chunking and named entity tagging) simultaneously. This indeed is one of the effective approaches for joint tag decoding problems. The search query NER dataset is an in-house annotated dataset which assigns semantic labels, such as product, business tags to web search queries. Ta</context>
<context position="33805" citStr="Kaji et al., 2010" startWordPosition="6044" endWordPosition="6047">* algorithm cannot provide the k-best sequences using the degenerate lattice. The overhead of multiple iterations slows down the decoding speed compared to the Viterbi A* algorithm. 200 180 160 140 120 100 80 60 40 20 0 10 20 30 40 50 60 70 80 90 100 k Figure 4: Decoding speed of k-best decoding algorithms for various k for CoNLL 2003 dataset. 6 Related work The Viterbi algorithm is the only exact algorithm widely adopted in the NLP applications. Esposito and Radicioni (2009) proposed an algorithm which opens necessary nodes in a lattice in searching the best sequence. The staggered decoding (Kaji et al., 2010) forms the basis for our work on iterative based decoding algorithms. Apart from the exact decoding, approximate decoding algorithms such as beam search are also related to our work. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (2005) presented the parameter tying approach for fast inference in HMMs. A similar idea was applied to CRFs as well (Cohn, 2006; Jeong, 2009). We note that the exact algorithm always guarantees the optimality which cannot be attained in approximate algorithms. In terms of k-best parsing, Huang and Chiang (2005) proposed an</context>
</contexts>
<marker>Kaji, Fujiwara, Yoshinaga, Kitsuregawa, 2010</marker>
<rawString>N. Kaji, Y. Fujiwara, N. Yoshinaga, and M. Kitsuregawa. 2010. Efficient Staggered Decoding for Sequence Labeling. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>A* parsing: Fast exact Viterbi parse selection.</title>
<date>2003</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="19151" citStr="Klein and Manning, 2003" startWordPosition="3449" endWordPosition="3452">rithms In this section, we propose A* based sequential decoding algorithms that can efficiently handle datasets with a large number of labels. In particular, we first propose the A* and the iterative A* decoding algorithm for 1-best sequential decoding. We then extend the 1-best A* algorithm to a k-best A* decoding algorithm. We finally apply the iterative process to the Viterbi A* algorithm, resulting in the iterative Viterbi A* decoding algorithm. 4.1 1-Best A* A*(Hart et al., 1968; Russell and Norvig, 1995), as a classic search algorithm, has been successfully applied in syntactic parsing (Klein and Manning, 2003; Pauls and Klein, 2009). The general idea of A* is to consider labels yt which are likely to result in the best sequence using a score f as follows. f(y) = g(y) + h(y), (8) where g(y) is the score from start to the current node and h(y) is a heuristic which estimates the score from the current node to the target. A* uses an agenda (based on the f score) to decide which nodes are to be processed next. If the heuristic satisfies the condition h(yt−1) ? e(yt−1, yt) + h(yt), then h is called monotone or admissible. In such a case, A* is guaranteed to find the best sequence. We start with the naiv</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. A* parsing: Fast exact Viterbi parse selection. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>Proceedings of ICML.</booktitle>
<contexts>
<context position="1219" citStr="Lafferty et al., 2001" startWordPosition="172" endWordPosition="175">lgorithms. In this paper we propose 1-best A*, 1-best iterative A*, k-best A* and k-best iterative Viterbi A* algorithms for sequential decoding. We show the efficiency of these proposed algorithms for five NLP tagging tasks. In particular, we show that iterative Viterbi A* decoding can be several times or orders of magnitude faster than the state-of-the-art algorithm for tagging tasks with a large number of labels. This algorithm makes real-time large-scale tagging applications with thousands of labels feasible. 1 Introduction Sequence tagging algorithms including HMMs (Rabiner, 1989), CRFs (Lafferty et al., 2001), and Collins’s perceptron (Collins, 2002) have been widely employed in NLP applications. Sequential decoding, which finds the best tag sequences for given inputs, is an important part of the sequential tagging framework. Traditionally, the Viterbi algorithm (Viterbi, 1967) is used. This algorithm is quite efficient when the label size of problem modeled is low. Unfortunately, due to its O(T L2) time complexity, where T is the input token size and L is the label size, the Viterbi decoding can become prohibitively slow when the label size is large (say, larger than 200). It is not uncommon that</context>
<context position="4403" citStr="Lafferty et al., 2001" startWordPosition="671" endWordPosition="674">te-of-the-art 1Implemented in both CRFPP (http://crfpp.sourceforge.net/) and LingPipe (http://alias-i.com/lingpipe/) packages. 611 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 611–619, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics k-best decoding algorithm. This algorithm makes real-time large-scale tagging applications with thousands of labels feasible. 2 Problem formulation In this section, we formulate the sequential decoding problem in the context of perceptron algorithm (Collins, 2002) and CRFs (Lafferty et al., 2001). All the discussions apply to HMMs as well. Formally, a perceptron model is θkfk(yt,yt−1,xt), (1) and a CRFs model is T K p(y|x) = Z(x) exp{E E θkfk(yt, yt−1, xt)}, (2) t=1 k=1 where x and y is an observation sequence and a label sequence respectively, t is the sequence position, T is the sequence size, fk are feature functions and K is the number of feature functions. θk are the parameters that need to be estimated. They represent the importance of feature functions fk in prediction. For CRFs, Z(x) is an instance-specific normalization function θkfk(yt,yt−1,xt)}. (3) If x is given, the decod</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Efficient HPSG parsing with supertagging and CFG-filtering.</title>
<date>2007</date>
<booktitle>Proceedings of IJCAI.</booktitle>
<contexts>
<context position="25952" citStr="Matsuzaki et al., 2007" startWordPosition="4696" endWordPosition="4699">: return ys 13: end if 14: end if 15: if lb &lt; k-th best(lattice) then 16: lb = k-th best(lattice) 17: end if 18: if lb &lt; k-th best(ys) then 19: lb = k-th best(ys) 20: end if 21: expand lattice 22: end for 5 Experiments We compare aforementioned 1-best and k-best sequential decoding algorithms using five datasets in this section. 5.1 Experimental setting We apply 1-best and k-best sequential decoding algorithms to five NLP tagging tasks: Penn TreeBank (PTB) POS tagging, CoNLL2000 joint POS tagging and chunking, CoNLL 2003 joint POS tagging, chunking and named entity tagging, HPSG supertagging (Matsuzaki et al., 2007) and a search query named entity recognition (NER) dataset. We used 616 sections 02-21 of PTB for training and section 23 for testing in POS task. As in (Kaji et al., 2010), we combine the POS tags and chunk tags to form joint tags for CoNLL 2000 dataset, e.g., NN|B-NP. Similarly we combine the POS tags, chunk tags, and named entity tags to form joint tags for CoNLL 2003 dataset, e.g., PRP$|I-NP|O. Note that by such tag joining, we are able to offer different tag decodings (for example, chunking and named entity tagging) simultaneously. This indeed is one of the effective approaches for joint </context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2007</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2007. Efficient HPSG parsing with supertagging and CFG-filtering. Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pauls</author>
<author>D Klein</author>
</authors>
<title>K-Best A* Parsing.</title>
<date>2009</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="2684" citStr="Pauls and Klein, 2009" startWordPosition="412" endWordPosition="415">ry nodes in searching the best sequence. More recently, Kaji et al. (2010) proposed a staggered decoding algorithm, which proves to be very efficient on datasets with a large number of labels. What the aforementioned literature does not cover is the k-best sequential decoding problem, which is indeed frequently required in practice. For example to pursue a high recall ratio, a named entity recognition system may have to adopt k-best sequences in case the true entities are not recognized at the best one. The k-best parses have been extensively studied in syntactic parsing context (Huang, 2005; Pauls and Klein, 2009), but it is not well accommodated in sequential decoding context. To our best knowledge, the state-of-the-art k-best sequential decoding algorithm is Viterbi A* 1. In this paper, we generalize the iterative process from the work of (Kaji et al., 2010) and propose a k-best sequential decoding algorithm, namely iterative Viterbi A*. We show that the proposed algorithm is several times or orders of magnitude faster than the state-of-the-art in all tagging tasks which consist of more than 200 labels. Our contributions can be summarized as follows. (1) We apply the A* search framework to sequential</context>
<context position="19175" citStr="Pauls and Klein, 2009" startWordPosition="3453" endWordPosition="3456">e propose A* based sequential decoding algorithms that can efficiently handle datasets with a large number of labels. In particular, we first propose the A* and the iterative A* decoding algorithm for 1-best sequential decoding. We then extend the 1-best A* algorithm to a k-best A* decoding algorithm. We finally apply the iterative process to the Viterbi A* algorithm, resulting in the iterative Viterbi A* decoding algorithm. 4.1 1-Best A* A*(Hart et al., 1968; Russell and Norvig, 1995), as a classic search algorithm, has been successfully applied in syntactic parsing (Klein and Manning, 2003; Pauls and Klein, 2009). The general idea of A* is to consider labels yt which are likely to result in the best sequence using a score f as follows. f(y) = g(y) + h(y), (8) where g(y) is the score from start to the current node and h(y) is a heuristic which estimates the score from the current node to the target. A* uses an agenda (based on the f score) to decide which nodes are to be processed next. If the heuristic satisfies the condition h(yt−1) ? e(yt−1, yt) + h(yt), then h is called monotone or admissible. In such a case, A* is guaranteed to find the best sequence. We start with the naive (but admissible) heuri</context>
<context position="34525" citStr="Pauls and Klein (2009)" startWordPosition="6161" endWordPosition="6164">g, approximate decoding algorithms such as beam search are also related to our work. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (2005) presented the parameter tying approach for fast inference in HMMs. A similar idea was applied to CRFs as well (Cohn, 2006; Jeong, 2009). We note that the exact algorithm always guarantees the optimality which cannot be attained in approximate algorithms. In terms of k-best parsing, Huang and Chiang (2005) proposed an efficient algorithm which is similar to the k-best Viterbi A* algorithm presented in this paper. Pauls and Klein (2009) proposed an algorithm which replaces the Viterbi forward pass with an A* search. Their algorithm optimizes the Viterbi pass, while the proposed iterative Viterbi A* algorithm optimizes both Viterbi and A* passes. This paper is also related to the coarse to fine PCFG parsing (Charniak et al., 2006) as the degenerate labels can be treated as coarse levels. However, the difference is that the coarse-to-fine parsing is an approximate decoding while ours is exact one. In terms of different coarse levels of heuristic used in A* decoding, this paper is related to the work of hierarchical A* framewor</context>
</contexts>
<marker>Pauls, Klein, 2009</marker>
<rawString>A. Pauls and D. Klein. 2009. K-Best A* Parsing. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of The IEEE.</booktitle>
<contexts>
<context position="1189" citStr="Rabiner, 1989" startWordPosition="168" endWordPosition="170">g sequential tagging algorithms. In this paper we propose 1-best A*, 1-best iterative A*, k-best A* and k-best iterative Viterbi A* algorithms for sequential decoding. We show the efficiency of these proposed algorithms for five NLP tagging tasks. In particular, we show that iterative Viterbi A* decoding can be several times or orders of magnitude faster than the state-of-the-art algorithm for tagging tasks with a large number of labels. This algorithm makes real-time large-scale tagging applications with thousands of labels feasible. 1 Introduction Sequence tagging algorithms including HMMs (Rabiner, 1989), CRFs (Lafferty et al., 2001), and Collins’s perceptron (Collins, 2002) have been widely employed in NLP applications. Sequential decoding, which finds the best tag sequences for given inputs, is an important part of the sequential tagging framework. Traditionally, the Viterbi algorithm (Viterbi, 1967) is used. This algorithm is quite efficient when the label size of problem modeled is low. Unfortunately, due to its O(T L2) time complexity, where T is the input token size and L is the label size, the Viterbi decoding can become prohibitively slow when the label size is large (say, larger than</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of The IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Raphael</author>
</authors>
<title>Coarse-to-fine dynamic programming.</title>
<date>2001</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence.</journal>
<contexts>
<context position="35141" citStr="Raphael, 2001" startWordPosition="6267" endWordPosition="6268">oposed an algorithm which replaces the Viterbi forward pass with an A* search. Their algorithm optimizes the Viterbi pass, while the proposed iterative Viterbi A* algorithm optimizes both Viterbi and A* passes. This paper is also related to the coarse to fine PCFG parsing (Charniak et al., 2006) as the degenerate labels can be treated as coarse levels. However, the difference is that the coarse-to-fine parsing is an approximate decoding while ours is exact one. In terms of different coarse levels of heuristic used in A* decoding, this paper is related to the work of hierarchical A* framework (Raphael, 2001; Felzenszwalb et al., 2007). In terms of iterative process, this paper is close to (Burkett et al., 2011) as both exploit the search-and-expand approach. 7 Conclusions We have presented and evaluated the A* and iterative A* algorithms for 1-best sequential decoding in this paper. In addition, we proposed A* and iterative Viterbi A* algorithm for k-best sequential decoding. K-best Iterative A* algorithm can be several times or orders of magnitude faster than the state-of-theart k-best decoding algorithm. It makes real-time large-scale tagging applications with thousands of labels feasible. Ack</context>
</contexts>
<marker>Raphael, 2001</marker>
<rawString>C. Raphael. 2001. Coarse-to-fine dynamic programming. IEEE Transactions on Pattern Analysis and Machine Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Russell</author>
<author>P Norvig</author>
</authors>
<date>1995</date>
<journal>Artificial Intelligence: A Modern Approach.</journal>
<contexts>
<context position="19043" citStr="Russell and Norvig, 1995" startWordPosition="3432" endWordPosition="3435">6: n = n.backlink 17: end for 18: c + + 19: end while 20: return K best sequences derived by r 4 Proposed Algorithms In this section, we propose A* based sequential decoding algorithms that can efficiently handle datasets with a large number of labels. In particular, we first propose the A* and the iterative A* decoding algorithm for 1-best sequential decoding. We then extend the 1-best A* algorithm to a k-best A* decoding algorithm. We finally apply the iterative process to the Viterbi A* algorithm, resulting in the iterative Viterbi A* decoding algorithm. 4.1 1-Best A* A*(Hart et al., 1968; Russell and Norvig, 1995), as a classic search algorithm, has been successfully applied in syntactic parsing (Klein and Manning, 2003; Pauls and Klein, 2009). The general idea of A* is to consider labels yt which are likely to result in the best sequence using a score f as follows. f(y) = g(y) + h(y), (8) where g(y) is the score from start to the current node and h(y) is a heuristic which estimates the score from the current node to the target. A* uses an agenda (based on the f score) to decide which nodes are to be processed next. If the heuristic satisfies the condition h(yt−1) ? e(yt−1, yt) + h(yt), then h is calle</context>
</contexts>
<marker>Russell, Norvig, 1995</marker>
<rawString>S. Russell and P. Norvig. 1995. Artificial Intelligence: A Modern Approach.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>Proceedings of HLT-NAACL.</booktitle>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Siddiqi</author>
<author>A Moore</author>
</authors>
<title>Fast inference and learning in large-state-space HMMs.</title>
<date>2005</date>
<booktitle>Proceedings of ICML.</booktitle>
<contexts>
<context position="34086" citStr="Siddiqi and Moore (2005)" startWordPosition="6087" endWordPosition="6090">ed of k-best decoding algorithms for various k for CoNLL 2003 dataset. 6 Related work The Viterbi algorithm is the only exact algorithm widely adopted in the NLP applications. Esposito and Radicioni (2009) proposed an algorithm which opens necessary nodes in a lattice in searching the best sequence. The staggered decoding (Kaji et al., 2010) forms the basis for our work on iterative based decoding algorithms. Apart from the exact decoding, approximate decoding algorithms such as beam search are also related to our work. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (2005) presented the parameter tying approach for fast inference in HMMs. A similar idea was applied to CRFs as well (Cohn, 2006; Jeong, 2009). We note that the exact algorithm always guarantees the optimality which cannot be attained in approximate algorithms. In terms of k-best parsing, Huang and Chiang (2005) proposed an efficient algorithm which is similar to the k-best Viterbi A* algorithm presented in this paper. Pauls and Klein (2009) proposed an algorithm which replaces the Viterbi forward pass with an A* search. Their algorithm optimizes the Viterbi pass, while the proposed iterative Viterb</context>
</contexts>
<marker>Siddiqi, Moore, 2005</marker>
<rawString>S. M. Siddiqi and A. Moore. 2005. Fast inference and learning in large-state-space HMMs. Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="34014" citStr="Tsuruoka and Tsujii (2005)" startWordPosition="6078" endWordPosition="6081"> 100 80 60 40 20 0 10 20 30 40 50 60 70 80 90 100 k Figure 4: Decoding speed of k-best decoding algorithms for various k for CoNLL 2003 dataset. 6 Related work The Viterbi algorithm is the only exact algorithm widely adopted in the NLP applications. Esposito and Radicioni (2009) proposed an algorithm which opens necessary nodes in a lattice in searching the best sequence. The staggered decoding (Kaji et al., 2010) forms the basis for our work on iterative based decoding algorithms. Apart from the exact decoding, approximate decoding algorithms such as beam search are also related to our work. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (2005) presented the parameter tying approach for fast inference in HMMs. A similar idea was applied to CRFs as well (Cohn, 2006; Jeong, 2009). We note that the exact algorithm always guarantees the optimality which cannot be attained in approximate algorithms. In terms of k-best parsing, Huang and Chiang (2005) proposed an efficient algorithm which is similar to the k-best Viterbi A* algorithm presented in this paper. Pauls and Klein (2009) proposed an algorithm which replaces the Viterbi forward pass with an A* search. Their a</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Y. Tsuruoka and J. Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory.</journal>
<contexts>
<context position="1493" citStr="Viterbi, 1967" startWordPosition="213" endWordPosition="214">g can be several times or orders of magnitude faster than the state-of-the-art algorithm for tagging tasks with a large number of labels. This algorithm makes real-time large-scale tagging applications with thousands of labels feasible. 1 Introduction Sequence tagging algorithms including HMMs (Rabiner, 1989), CRFs (Lafferty et al., 2001), and Collins’s perceptron (Collins, 2002) have been widely employed in NLP applications. Sequential decoding, which finds the best tag sequences for given inputs, is an important part of the sequential tagging framework. Traditionally, the Viterbi algorithm (Viterbi, 1967) is used. This algorithm is quite efficient when the label size of problem modeled is low. Unfortunately, due to its O(T L2) time complexity, where T is the input token size and L is the label size, the Viterbi decoding can become prohibitively slow when the label size is large (say, larger than 200). It is not uncommon that the problem modeled consists of more than 200 labels. The Viterbi algorithm cannot find the best sequences in tolerable response time. To resolve this, Esposito and Radicioni (2009) have proposed a Carpediem algorithm which opens only necessary nodes in searching the best </context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>A. J. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>