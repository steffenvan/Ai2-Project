<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988644">
A Metric-based Framework for Automatic Taxonomy Induction
</title>
<author confidence="0.972319">
Hui Yang
</author>
<affiliation confidence="0.948481">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.997548">
huiyang@cs.cmu.edu
</email>
<author confidence="0.904704">
Jamie Callan
</author>
<affiliation confidence="0.934921333333333">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.998566">
callan@cs.cmu.edu
</email>
<sectionHeader confidence="0.994798" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999454631578948">
This paper presents a novel metric-based
framework for the task of automatic taxonomy
induction. The framework incrementally clus-
ters terms based on ontology metric, a score
indicating semantic distance; and transforms
the task into a multi-criteria optimization
based on minimization of taxonomy structures
and modeling of term abstractness. It com-
bines the strengths of both lexico-syntactic
patterns and clustering through incorporating
heterogeneous features. The flexible design of
the framework allows a further study on which
features are the best for the task under various
conditions. The experiments not only show
that our system achieves higher F1-measure
than other state-of-the-art systems, but also re-
veal the interaction between features and vari-
ous types of relations, as well as the interac-
tion between features and term abstractness.
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992705">
Automatic taxonomy induction is an important
task in the fields of Natural Language
Processing, Knowledge Management, and Se-
mantic Web. It has been receiving increasing
attention because semantic taxonomies, such as
WordNet (Fellbaum, 1998), play an important
role in solving knowledge-rich problems, includ-
ing question answering (Harabagiu et al., 2003)
and textual entailment (Geffet and Dagan, 2005).
Nevertheless, most existing taxonomies are ma-
nually created at great cost. These taxonomies
are rarely complete; it is difficult to include new
terms in them from emerging or rapidly changing
domains. Moreover, manual taxonomy construc-
tion is time-consuming, which may make it un-
feasible for specialized domains and personalized
tasks. Automatic taxonomy induction is a solu-
tion to augment existing resources and to pro-
duce new taxonomies for such domains and
tasks.
Automatic taxonomy induction can be decom-
posed into two subtasks: term extraction and re-
lation formation. Since term extraction is rela-
tively easy, relation formation becomes the focus
of most research on automatic taxonomy induc-
tion. In this paper, we also assume that terms in a
taxonomy are given and concentrate on the sub-
task of relation formation.
Existing work on automatic taxonomy induc-
tion has been conducted under a variety of
names, such as ontology learning, semantic class
learning, semantic relation classification, and
relation extraction. The approaches fall into two
main categories: pattern-based and clustering-
based. Pattern-based approaches define lexical-
syntactic patterns for relations, and use these pat-
terns to discover instances of relations. Cluster-
ing-based approaches hierarchically cluster terms
based on similarities of their meanings usually
represented by a vector of quantifiable features.
Pattern-based approaches are known for their
high accuracy in recognizing instances of rela-
tions if the patterns are carefully chosen, either
manually (Berland and Charniak, 1999; Kozare-
va et al., 2008) or via automatic bootstrapping
(Hearst, 1992; Widdows and Dorow, 2002; Girju
et al., 2003). The approaches, however, suffer
from sparse coverage of patterns in a given cor-
pus. Recent studies (Etzioni et al., 2005; Kozare-
va et al., 2008) show that if the size of a corpus,
such as the Web, is nearly unlimited, a pattern
has a higher chance to explicitly appear in the
corpus. However, corpus size is often not that
large; hence the problem still exists. Moreover,
since patterns usually extract instances in pairs,
the approaches suffer from the problem of incon-
sistent concept chains after connecting pairs of
instances to form taxonomy hierarchies.
Clustering-based approaches have a main ad-
vantage that they are able to discover relations
</bodyText>
<page confidence="0.966734">
271
</page>
<note confidence="0.9996105">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 271–279,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999449272727273">
which do not explicitly appear in text. They also
avoid the problem of inconsistent chains by ad-
dressing the structure of a taxonomy globally
from the outset. Nevertheless, it is generally be-
lieved that clustering-based approaches cannot
generate relations as accurate as pattern-based
approaches. Moreover, their performance is
largely influenced by the types of features used.
The common types of features include contextual
(Lin, 1998), co-occurrence (Yang and Callan,
2008), and syntactic dependency (Pantel and Lin,
2002; Pantel and Ravichandran, 2004). So far
there is no systematic study on which features
are the best for automatic taxonomy induction
under various conditions.
This paper presents a metric-based taxonomy
induction framework. It combines the strengths
of both pattern-based and clustering-based ap-
proaches by incorporating lexico-syntactic pat-
terns as one type of features in a clustering
framework. The framework integrates contex-
tual, co-occurrence, syntactic dependency, lexi-
cal-syntactic patterns, and other features to learn
an ontology metric, a score indicating semantic
distance, for each pair of terms in a taxonomy; it
then incrementally clusters terms based on their
ontology metric scores. The incremental cluster-
ing is transformed into an optimization problem
based on two assumptions: minimum evolution
and abstractness. The flexible design of the
framework allows a further study of the interac-
tion between features and relations, as well as
that between features and term abstractness.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999950712328767">
There has been a substantial amount of research
on automatic taxonomy induction. As we men-
tioned earlier, two main approaches are pattern-
based and clustering-based.
Pattern-based approaches are the main trend
for automatic taxonomy induction. Though suf-
fering from the problems of sparse coverage and
inconsistent chains, they are still popular due to
their simplicity and high accuracy. They have
been applied to extract various types of lexical
and semantic relations, including is-a, part-of,
sibling, synonym, causal, and many others.
Pattern-based approaches started from and still
pay a great deal of attention to the most common
is-a relations. Hearst (1992) pioneered using a
hand crafted list of hyponym patterns as seeds
and employing bootstrapping to discover is-a
relations. Since then, many approaches (Mann,
2002; Etzioni et al., 2005; Snow et al., 2005)
have used Hearst-style patterns in their work on
is-a relations. For instance, Mann (2002) ex-
tracted is-a relations for proper nouns by Hearst-
style patterns. Pantel et al. (2004) extended is-a
relation acquisition towards terascale, and auto-
matically identified hypernym patterns by mi-
nimal edit distance.
Another common relation is sibling, which de-
scribes the relation of sharing similar meanings
and being members of the same class. Terms in
sibling relations are also known as class mem-
bers or similar terms. Inspired by the conjunction
and appositive structures, Riloff and Shepherd
(1997), Roark and Charniak (1998) used co-
occurrence statistics in local context to discover
sibling relations. The KnowItAll system (Etzioni
et al., 2005) extended the work in (Hearst, 1992)
and bootstrapped patterns on the Web to discover
siblings; it also ranked and selected the patterns
by statistical measures. Widdows and Dorow
(2002) combined symmetric patterns and graph
link analysis to discover sibling relations. Davi-
dov and Rappoport (2006) also used symmetric
patterns for this task. Recently, Kozareva et al.
(2008) combined a double-anchored hyponym
pattern with graph structure to extract siblings.
The third common relation is part-of. Berland
and Charniak (1999) used two meronym patterns
to discover part-of relations, and also used statis-
tical measures to rank and select the matching
instances. Girju et al. (2003) took a similar ap-
proach to Hearst (1992) for part-of relations.
Other types of relations that have been studied
by pattern-based approaches include question-
answer relations (such as birthdates and inven-
tor) (Ravichandran and Hovy, 2002), synonyms
and antonyms (Lin et al., 2003), general purpose
analogy (Turney et al., 2003), verb relations (in-
cluding similarity, strength, antonym, enable-
ment and temporal) (Chklovski and Pantel,
2004), entailment (Szpektor et al., 2004), and
more specific relations, such as purpose, creation
(Cimiano and Wenderoth, 2007), LivesIn, and
EmployedBy (Bunescu and Mooney , 2007).
The most commonly used technique in pat-
tern-based approaches is bootstrapping (Hearst,
1992; Etzioni et al., 2005; Girju et al., 2003; Ra-
vichandran and Hovy, 2002; Pantel and Pennac-
chiotti, 2006). It utilizes a few man-crafted seed
patterns to extract instances from corpora, then
extracts new patterns using these instances, and
continues the cycle to find new instances and
new patterns. It is effective and scalable to large
datasets; however, uncontrolled bootstrapping
</bodyText>
<page confidence="0.994237">
272
</page>
<bodyText confidence="0.999878333333333">
soon generates undesired instances once a noisy
pattern brought into the cycle.
To aid bootstrapping, methods of pattern
quality control are widely applied. Statistical
measures, such as point-wise mutual information
(Etzioni et al., 2005; Pantel and Pennacchiotti,
2006) and conditional probability (Cimiano and
Wenderoth, 2007), have been shown to be ef-
fective to rank and select patterns and instances.
Pattern quality control is also investigated by
using WordNet (Girju et al., 2006), graph struc-
tures built among terms (Widdows and Dorow,
2002; Kozareva et al., 2008), and pattern clusters
(Davidov and Rappoport, 2008).
Clustering-based approaches usually represent
word contexts as vectors and cluster words based
on similarities of the vectors (Brown et al., 1992;
Lin, 1998). Besides contextual features, the vec-
tors can also be represented by verb-noun rela-
tions (Pereira et al., 1993), syntactic dependency
(Pantel and Ravichandran, 2004; Snow et al.,
2005), co-occurrence (Yang and Callan, 2008),
conjunction and appositive features (Caraballo,
1999). More work is described in (Buitelaar et
al., 2005; Cimiano and Volker, 2005). Cluster-
ing-based approaches allow discovery of rela-
tions which do not explicitly appear in text. Pan-
tel and Pennacchiotti (2006), however, pointed
out that clustering-based approaches generally
fail to produce coherent cluster for small corpora.
In addition, clustering-based approaches had on-
ly applied to solve is-a and sibling relations.
Many clustering-based approaches face the
challenge of appropriately labeling non-leaf clus-
ters. The labeling amplifies the difficulty in crea-
tion and evaluation of taxonomies. Agglomera-
tive clustering (Brown et al., 1992; Caraballo,
1999; Rosenfeld and Feldman, 2007; Yang and
Callan, 2008) iteratively merges the most similar
clusters into bigger clusters, which need to be
labeled. Divisive clustering, such as CBC (Clus-
tering By Committee) which constructs cluster
centroids by averaging the feature vectors of a
subset of carefully chosen cluster members (Pan-
tel and Lin, 2002; Pantel and Ravichandran,
2004), also need to label the parents of split clus-
ters. In this paper, we take an incremental clus-
tering approach, in which terms and relations are
added into a taxonomy one at a time, and their
parents are from the existing taxonomy. The ad-
vantage of the incremental approach is that it
eliminates the trouble of inventing cluster labels
and concentrates on placing terms in the correct
positions in a taxonomy hierarchy.
The work by Snow et al. (2006) is the most
similar to ours because they also took an incre-
mental approach to construct taxonomies. In their
work, a taxonomy grows based on maximization
of conditional probability of relations given evi-
dence; while in our work based on optimization
of taxonomy structures and modeling of term
abstractness. Moreover, our approach employs
heterogeneous features from a wide range; while
their approach only used syntactic dependency.
We compare system performance between (Snow
et al., 2006) and our framework in Section 5.
</bodyText>
<sectionHeader confidence="0.985404" genericHeader="method">
3 The Features
</sectionHeader>
<bodyText confidence="0.999785631578948">
The features used in this work are indicators of
semantic relations between terms. Given two in-
put terms cx, cy , a feature is defined as a func-
tion generating a single numeric score
h(cx, c y) ∈ ℝ or a vector of numeric scores
h(cx , c y) ∈ ℝn. The features include contextual,
co-occurrence, syntactic dependency, lexical-
syntactic patterns, and miscellaneous.
The first set of features captures contextual in-
formation of terms. According to Distributional
Hypothesis (Harris, 1954), words appearing in
similar contexts tend to be similar. Therefore,
word meanings can be inferred from and
represented by contexts. Based on the hypothe-
sis, we develop the following features: (1) Glob-
al Context KL-Divergence: The global context of
each input term is the search results collected
through querying search engines against several
corpora (Details in Section 5.1). It is built into a
unigram language model without smoothing for
each term. This feature function measures the
Kullback-Leibler divergence (KL divergence)
between the language models associated with the
two inputs. (2) Local Context KL-Divergence:
The local context is the collection of all the left
two and the right two words surrounding an input
term. Similarly, the local context is built into a
unigram language model without smoothing for
each term; the feature function outputs KL diver-
gence between the models.
The second set of features is co-occurrence. In
our work, co-occurrence is measured by point-
wise mutual information between two terms:
where Count(.) is defined as the number of doc-
uments or sentences containing the term(s); or n
as in “Results 1-10 of about n for term” appear-
ing on the first page of Google search results for
a term or the concatenation of a term pair. Based
</bodyText>
<figure confidence="0.952553666666667">
Count
( , )
c c
x
Countc
( x
) ( )
Count cy
pmi(cx,cy) = log
</figure>
<page confidence="0.957932">
273
</page>
<bodyText confidence="0.7618571">
Hypernym Patterns Sibling Patterns
NPx (,)?and/or other NPy NPx and/or NPy
such NPy as NPx Part-of Patterns
NPy (,)? such as NPx NPx of NPy
NPy (,)? including NPx NPy’s NPx
NPy (,)? especially NPx NPy has/had/have NPx
NPy like NPx NPy is made (up)? of NPx
NPy called NPx NPy comprises NPx
NPx is a/an NPy NPy consists of NPx
NPx , a/an
</bodyText>
<tableCaption confidence="0.991269">
Table 1. Lexico-Syntactic Pattern
</tableCaption>
<figure confidence="0.6134485">
s.
http://cemantix.org/assert
1http://www.cs.ualberta.ca/lindek/minipar.htm.2
.
</figure>
<figureCaption confidence="0.999511">
Figure 1. Illustration of Ontology Metric.
</figureCaption>
<bodyText confidence="0.847357">
d abstractness in Section 5.
</bodyText>
<sectionHeader confidence="0.952894" genericHeader="method">
4 The Metric-based Framework
</sectionHeader>
<bodyText confidence="0.896889466666667">
my induction as
optimization
an
a multi-criterion
d solve it by a greedy algorithm; lastly, we
show how to estimate ontology metrics.
full taxonomy is a tree containing all
the terms in
C. A partial taxonomy is a tree containing only a
subset of terms in C.
In our framework, automatic taxonomy induc-
where
. Note that
is
possibly empty. The process starts from the ini-
tial partial taxonomy
and randomly adds terms
from C to
one by one, until a full taxonomy is
formed, i.e., all terms in C are added.
Ontology Metric
We define an ontology metric as a distance
measure between two terms
in a taxonomy
T(C,R). Formally, it is a function d: C x C
where C is the set of terms in T. An ontology
metric d on a taxonomy T with edge weights w
for any term pair
is the sum of all edge
weights along the short
</bodyText>
<equation confidence="0.97892">
T0(S0,R0),
S c C
0
T0
T0
T0
(cx,cy)
→ℝ+,
(cx,cy)∈C
</equation>
<bodyText confidence="0.611457">
est path between the pair:
</bodyText>
<equation confidence="0.667395333333333">
x y ∑ w e
, ) =
( c c
( )
x y
,
</equation>
<bodyText confidence="0.9927906">
on different definitions of Count(.), we have (3)
Document PMI, (4) Sentence PMI, and (5)
Google PMI as the co-occurrence features.
The third set of features employs syntactic de-
pendency analysis. We have (6) Minipar Syntac-
tic Distance to measure the average length of the
shortest syntactic paths (in the first syntactic
parse tree returned by
between two
terms in sentences containing them, (7) Modifier
</bodyText>
<subsubsectionHeader confidence="0.8023">
Overlap, (8) Object Overlap, (9) Subject Over-
</subsubsectionHeader>
<bodyText confidence="0.996137806451613">
lap, and (10) Verb Overlap to measure the num-
ber of overlaps between modifiers, objects, sub-
jects, and verbs, respectively, for the two terms
in sentences containing them. We use Assert2 to
label the semantic roles.
The fourth set of features is lexical-syntactic
patterns. We have (11) Hypernym Patterns based
on patterns proposed by (Hearst, 1992) and
(Snow et al., 2005), (12) Sibling Patterns which
are basically conjunctions, and (13) Part-of Pat-
terns based on patterns proposed by (Girju et al.,
2003) and (Cimiano and Wenderoth, 2007). Ta-
ble 1 lists all patterns. Each feature function re-
turns
of scores for two input terms, one
score per pattern. A score is 1 if two terms match
a pattern in text, 0 otherwise.
The last set of features is miscellaneous. We
have (14) Word Length Difference to measure the
length difference between two terms, and (15)
Definition Overlap to measure the number of
word overlaps between the term definitions ob-
tained by querying Google with
These heterogeneous features vary from sim-
ple statistics to complicated syntactic dependen-
cy features, basic word length to comprehensive
Web-based contextual features. The flexible de-
sign of our learning framework allows us to use
all of them, and even allows us to use different
sets of them under different conditions, for in-
stance, different types of relations an
</bodyText>
<equation confidence="0.64403">
Minipar1)
a vector
“define:term”.
</equation>
<bodyText confidence="0.94023075">
d different
abstraction levels. We study the interaction be-
tween features and relations and that between
features an
This section presents the metric-based frame-
work which incrementally clusters terms to form
taxonomies. By minimizing the changes of tax-
onomy structures and modeling term abstractness
at each step, it finds the optimal position for each
term in a taxonomy. We first introduce defini-
tions, terminologies and assumptions about tax-
onomies; then, we formulate automatic taxono-
</bodyText>
<sectionHeader confidence="0.412667" genericHeader="method">
4.1 Taxonomies, Ontology Metric, Assump-
tions, and Information Functions
</sectionHeader>
<bodyText confidence="0.998732">
We define a taxonomy T as a data model that
represents a set of terms C and a set of relations
R between these terms. T can be written as
T(C,R). Note that for the subtask of relation for-
mation, we assume that the term set C is given. A
tion is the process to construct a full taxonomy Tˆ
given a set of terms C and an initial partial tax-
</bodyText>
<equation confidence="0.906959777777778">
onomy
∈ P x y
( , )
NPy
d
(T, w)
,
ex
y
</equation>
<page confidence="0.980681">
274
</page>
<bodyText confidence="0.9976012">
where P(x, y) is the set of edges defining the
shortest path from term cx to cy . Figure 1 illu-
strates ontology metrics for a 5-node taxonomy.
Section 4.3 presents the details of learning ontol-
ogy metrics.
</bodyText>
<subsectionHeader confidence="0.696555">
Information Functions
</subsectionHeader>
<bodyText confidence="0.999967125">
The amount of information in a taxonomy T is
measured and represented by an information
function Info(T). An information function is de-
fined as the sum of the ontology metrics among a
set of term pairs. The function can be defined
over a taxonomy, or on a single level of a tax-
onomy. For a taxonomy T(C,R), we define its
information function as:
</bodyText>
<equation confidence="0.712011">
Info (T) = ∑ d(cx,cy) (1)
x &lt; y , cx , c y ∈ C
</equation>
<bodyText confidence="0.999863">
Similarly, we define the information function
for an abstraction level Li as:
</bodyText>
<equation confidence="0.934241">
Info i(Li) = ∑ d(cx,c y ) (2)
x&lt; y , cx ,c y Li
∈
</equation>
<bodyText confidence="0.999891333333333">
where Li is the subset of terms lying at the ith lev-
el of a taxonomy T. For example, in Figure 1,
node 1 is at level L1, node 2 and node 5 level L2.
</bodyText>
<sectionHeader confidence="0.637869" genericHeader="method">
Assumptions
</sectionHeader>
<bodyText confidence="0.999821">
Given the above definitions about taxonomies,
we make the following assumptions:
Minimum Evolution Assumption. Inspired by
the minimum evolution tree selection criterion
widely used in phylogeny (Hendy and Penny,
1985), we assume that a good taxonomy not only
minimizes the overall semantic distance among
the terms but also avoid dramatic changes. Con-
struction of a full taxonomy is proceeded by add-
ing terms one at a time, which yields a series of
partial taxonomies. After adding each term, the
current taxonomy Tn+1 from the previous tax-
onomy Tn is one that introduces the least changes
between the information in the two taxonomies:
</bodyText>
<equation confidence="0.98206125">
T n+1 = arg min ( ,
ΔInfo T n T
&apos;
T
</equation>
<bodyText confidence="0.975722761904762">
where the information change function is
ΔInfo T a T b = Info T a − Info T b
( , ) |( ) ( )  |.
Abstractness Assumption. In a taxonomy, con-
crete concepts usually lay at the bottom of the
hierarchy while abstract concepts often occupy
the intermediate and top levels. Concrete con-
cepts often represent physical entities, such as
“basketball” and “mercury pollution”. While ab-
stract concepts, such as “science” and “econo-
my”, do not have a physical form thus we must
imagine their existence. This obvious difference
suggests that there is a need to treat them diffe-
rently in taxonomy induction. Hence we assume
that terms at the same abstraction level have
common characteristics and share the same Info(.)
function. We also assume that terms at different
abstraction levels have different characteristics;
hence they do not necessarily share the same
Info(.) function. That is to say, ∀ concept c ∈ T,
abstraction level Li ⊂ T, c ∈ Li ⇒ c uses Infoi (.).
</bodyText>
<subsectionHeader confidence="0.9331445">
4.2 Problem Formulation
The Minimum Evolution Objective
</subsectionHeader>
<bodyText confidence="0.9998414">
Based on the minimum evolution assumption, we
define the goal of taxonomy induction is to find
the optimal full taxonomy Tˆ such that the infor-
mation changes are the least since the initial par-
tial taxonomy T0, i.e., to find:
</bodyText>
<equation confidence="0.97811775">
T ˆ arg min ( 0, &apos; )
= Δ Info T T (3)
&apos;
T
</equation>
<bodyText confidence="0.99144">
where &apos;
T is a full taxonomy, i.e., the set of terms
in &apos;
T equals C.
To find the optimal solution for Equation (3),
Tˆ , we need to find the optimal term set Cˆ and
the optimal relation set Rˆ . Since the optimal term
set for a full taxonomy is always C, the only un-
known part left is Rˆ . Thus, Equation (3) can be
transformed equivalently into:
</bodyText>
<equation confidence="0.892874333333333">
R ˆ = arg min ΔInfo(T &apos; (C, R ), T 0 (S 0 , R0 ))
&apos;
R
</equation>
<bodyText confidence="0.948476222222222">
Note that in the framework, terms are added
incrementally into a taxonomy. Each term inser-
tion yields a new partial taxonomy T. By the
minimum evolution assumption, the optimal next
partial taxonomy is one gives the least informa-
tion change. Therefore, the updating function for
the set of relations n+1
R after a new term z is in-
serted can be calculated as:
</bodyText>
<figure confidence="0.344842">
ˆR=arg min ΔInfo(T(Sn ∪{ z} ,R&apos;),T(Sn,Rn))
&apos;
R
</figure>
<bodyText confidence="0.9929135">
By plugging in the definition of the information
change function ΔInfo(.,.) in Section 4.1 and Equ-
ation (1), the updating function becomes:
The above updating function can be transformed
into a minimization problem:
The minimization follows the minimum evolu-
tion assumption; hence we call it the minimum
evolution objective.
</bodyText>
<figure confidence="0.929789096774194">
ˆ =argmin  |∑d(cx,cy)− ∑ d(cx,c y ) |
R &apos;
n
R
n
∈
∈
y
z} cx,c
∪{
cx
,c
S
S
y
u
min
∑ d(cx,cy)−∑
,cy Sn
∈ cx c
,
x&lt;y
subject to
∑d
cy∑
z} cx cy
,
c
(
x
∈
,
d
)
cy
(cx
n
S
u
≤
cx
n
∪{
,cyS
∈
,
d
)
cy
(cx
}
z
∪{
u
≤
cx
n
∈
y
S
&apos;
)
</figure>
<page confidence="0.98412">
275
</page>
<subsectionHeader confidence="0.907318">
The Abstractness Objective
</subsectionHeader>
<bodyText confidence="0.931838">
The abstractness assumption suggests that term
abstractness should be modeled explicitly by
learning separate information functions for terms
at different abstraction levels. We approximate
an information function by a linear interpolation
of some underlying feature functions. Each ab-
straction level Li is characterized by its own in-
formation function Infoi(.). The least square fit of
Infoi(.) is: min  |( )
</bodyText>
<subsectionHeader confidence="0.471846">
Infoi Li −WiT Hi |2.
</subsectionHeader>
<bodyText confidence="0.9865705">
By plugging Equation (2) and minimizing over
every abstraction level, we have:
</bodyText>
<equation confidence="0.972051">
min (
∑ ∑ d(cx,cy)−∑ wi,jhi,j(cx,c
i cx c y Li
, ∈
</equation>
<bodyText confidence="0.9812544">
where hi, j (.,.) is the jth underlying feature func-
tion for term pairs at level Li, wi, j is the weight
for hi, j (.,.). This minimization follows the ab-
stractness assumption; hence we call it the ab-
stractness objective.
</bodyText>
<subsectionHeader confidence="0.960203">
The Multi-Criterion Optimization Algorithm
</subsectionHeader>
<bodyText confidence="0.999984714285714">
We propose that both minimum evolution and
abstractness objectives need to be satisfied. To
optimize multiple criteria, the Pareto optimality
needs to be satisfied (Boyd and Vandenberghe,
2004). We handle this by introducing A E [0,1] to
control the contribution of each objective. The
multi-criterion optimization function is:
</bodyText>
<equation confidence="0.9921838">
min (1 )
λ u + − λ v
∑
∑d(cx
,cy)−
v =∑ ∑ d(cx,cy)−∑wi,j hi, j (cx , c y ))
(
i c c L
x y i
, ∈
</equation>
<bodyText confidence="0.9921065">
x&lt;y
The above optimization can be solved by a gree-
dy optimization algorithm. At each term insertion
step, it produces a new partial taxonomy by add-
ing to the existing partial taxonomy a new term z,
and a new set of relations R(z,.). z is attached to
every nodes in the existing partial taxonomy; and
the algorithm selects the optimal position indi-
cated by R(z,.), which minimizes the multi-
criterion objective function. The algorithm is:
</bodyText>
<equation confidence="0.625787571428572">
z C S
∈ \
S S {z}
→ ∪
R ∪ [arg min R(z, (λu + (1− λM);
);
T(S , R) ;
</equation>
<bodyText confidence="0.99025475">
The above algorithm presents a general incre-
mental clustering procedure to construct taxono-
mies. By minimizing the taxonomy structure
changes and modeling term abstractness at each
</bodyText>
<table confidence="0.998309">
Statistics WN/is-a ODP/is-a WN/part-of
#taxonomies 50 50 50
#terms 1,964 2,210 1,812
Avg #terms 39 44 37
Avg depth 6 6 5
</table>
<tableCaption confidence="0.999455">
Table 2. Data Statistics.
</tableCaption>
<bodyText confidence="0.9957465">
step, it finds the optimal position of each term in
the taxonomy hierarchy.
</bodyText>
<subsectionHeader confidence="0.999926">
4.3 Estimating Ontology Metric
</subsectionHeader>
<bodyText confidence="0.998232615384615">
Learning a good ontology metric is important for
the multi-criterion optimization algorithm. In this
work, the estimation and prediction of ontology
metric are achieved by ridge regression (Hastie et
al., 2001). In the training data, an ontology me-
tric d(cx,cy) for a term pair (cx,cy) is generated by
assuming every edge weight as 1 and summing
up all the edge weights along the shortest path
from cx to cy. We assume that there are some un-
derlying feature functions which measure the
semantic distance from term cx to cy. A weighted
combination of these functions approximates the
ontology metric for (cx,cy):
</bodyText>
<subsectionHeader confidence="0.476064">
d(x,y)=∑ j wjhj(cx,cy)
</subsectionHeader>
<bodyText confidence="0.999224333333333">
where wj is the jth weight for hj (cx, cy) , the jth
feature function. The feature functions are gener-
ated as mentioned in Section 3.
</bodyText>
<sectionHeader confidence="0.996416" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.913648">
5.1 Data
</subsectionHeader>
<construct confidence="0.757366454545454">
3 WordNet hypernym taxonomies are from 12 topics: ga-
thering, professional, people, building, place, milk, meal,
water, beverage, alcohol, dish, and herb.
4 ODP hypernym taxonomies are from 16 topics: computers,
robotics, intranet, mobile computing, database, operating
system, linux, tex, software, computer science, data commu-
nication, algorithms, data formats, security multimedia, and
artificial intelligence.
5 WordNet meronym taxonomies are from 15 topics: bed,
car, building, lamp, earth, television, body, drama, theatre,
water, airplane, piano, book, computer, and watch.
</construct>
<figure confidence="0.996008181818182">
y S z } cx ,c S n
c n ∪ [ y∈
∈
∑d(cx,cy) ∑
)
d(cx,c
y
2
))
j
u
subject to
≤
cx
)
d(cx,c
y
u
≤
cx
}
n
cx
S z
n∪[
,cy∈
, c S
y ∈
j
foreach
Output
R→
;
</figure>
<bodyText confidence="0.978370785714286">
The gold standards used in the evaluation are
hypernym taxonomies extracted from WordNet
2 and ODP (Open Directory Project), and me-
ronym taxonomies extracted from WordNet. In
WordNet taxonomy extraction, we only use the
word senses within a particular taxonomy to en-
sure no ambiguity. In ODP taxonomy extraction,
we parse the topic lines, such as “Topic
r:id=`Top/Arts/Movies’”, in the XML databases
to obtain relations, such as is_a(movies, arts). In
total, there are 100 hypernym taxonomies, 50
each extracted from WordNet3 and ODP4, and 50
meronym taxonomies from WordNet5. Table 2
,
</bodyText>
<page confidence="0.993042">
276
</page>
<table confidence="0.9998985">
WordNet/is-a
System Precision Recall F1-measure
HE 0.85 0.32 0.46
GI n/a n/a n/a
PR 0.75 0.73 0.74
ME 0.82 0.79 0.82
ODP/is-a
System Precision Recall F1-measure
HE 0.31 0.29 0.30
GI n/a n/a n/a
PR 0.60 0.72 0.65
ME 0.64 0.70 0.67
WordNet/part-of
System Precision Recall F1-measure
HE n/a n/a n/a
GI 0.75 0.25 0.38
PR 0.68 0.52 0.59
ME 0.69 0.55 0.61
</table>
<tableCaption confidence="0.999053">
Table 3. System Performance.
</tableCaption>
<bodyText confidence="0.797865333333333">
summarizes the data statistics.
We also use two Web-based auxiliary datasets
to generate features mentioned in Section 3:
</bodyText>
<listItem confidence="0.9998692">
• Wikipedia corpus. The entire Wikipedia corpus
is downloaded and indexed by Indri6. The top
100 documents returned by Indri are the global
context of a term when querying with the term.
• Google corpus. A collection of the top 1000
</listItem>
<bodyText confidence="0.984675333333333">
documents by querying Google using each
term, and each term pair. Each top 1000 docu-
ments are the global context of a query term.
Both corpora are split into sentences and are used
to generate contextual, co-occurrence, syntactic
dependency and lexico-syntactic pattern features.
</bodyText>
<subsectionHeader confidence="0.997969">
5.2 Methodology
</subsectionHeader>
<bodyText confidence="0.999987764705883">
We evaluate the quality of automatic generated
taxonomies by comparing them with the gold
standards in terms of precision, recall and F1-
measure. F1-measure is calculated as 2*P*R/
(P+R), where P is precision, the percentage of
correctly returned relations out of the total re-
turned relations, R is recall, the percentage of
correctly returned relations out of the total rela-
tions in the gold standard.
Leave-one-out cross validation is used to aver-
age the system performance across different
training and test datasets. For each 50 datasets
from WordNet hypernyms, WordNet meronyms
or ODP hypernyms, we randomly pick 49 of
them to generate training data, and test on the
remaining dataset. We repeat the process for 50
times, with different training and test sets at each
</bodyText>
<page confidence="0.718185">
6 http://www.lemurproject.org/indri/.
</page>
<tableCaption confidence="0.987928">
Table 4. F1-measure for Features vs. Relations: WordNet.
</tableCaption>
<bodyText confidence="0.999715285714286">
time, and report the averaged precision, recall
and F1-measure across all 50 runs.
We also group the fifteen features in Section 3
into six sets: contextual, co-concurrence, pat-
terns, syntactic dependency, word length differ-
ence and definition. Each set is turned on one by
one for experiments in Section 5.4 and 5.5.
</bodyText>
<subsectionHeader confidence="0.997779">
5.3 Performance of Taxonomy Induction
</subsectionHeader>
<bodyText confidence="0.99992375">
In this section, we compare the following auto-
matic taxonomy induction systems: HE, the sys-
tem by Hearst (1992) with 6 hypernym patterns;
GI, the system by Girju et al. (2003) with 3 me-
ronym patterns; PR, the probabilistic framework
by Snow et al. (2006); and ME, the metric-based
framework proposed in this paper. To have a fair
comparison, for PR, we estimate the conditional
probability of a relation given the evidence
P(Ri;|Ei;), as in (Snow et al. 2006), by using the
same set of features as in ME.
Table 3 shows precision, recall, and F1-
measure of each system for WordNet hypernyms
(is-a), WordNet meronyms (part-of) and ODP
hypernyms (is-a). Bold font indicates the best
performance in a column. Note that HE is not
applicable to part-of, so is GI to is-a.
Table 3 shows that systems using heterogene-
ous features (PR and ME) achieve higher F1-
measure than systems only using patterns (HE
and GI) with a significant absolute gain of &gt;30%.
Generally speaking, pattern-based systems show
higher precision and lower recall, while systems
using heterogeneous features show lower preci-
sion and higher recall. However, when consider-
ing both precision and recall, using heterogene-
ous features is more effective than just using pat-
terns. The proposed system ME consistently pro-
duces the best F1-measure for all three tasks.
The performance of the systems for ODP/is-a
is worse than that for WordNet/is-a. This may be
because there is more noise in ODP than in
</bodyText>
<table confidence="0.97109680952381">
Feature is-a sibling part- Benefited
of Relations
Contextual 0.21 0.42 0.12 sibling
Co-occur. 0.48 0.41 0.28 All
Patterns 0.46 0.41 0.30 All
Syntactic 0.22 0.36 0.12 sibling
Word Leng. 0.16 0.16 0.15 All but
limited
Definition 0.12 0.18 0.10 Sibling but
limited
Best Features Co- Contextual, Co-
occur., co-occur., occur.,
patterns patterns patterns
277
Feature L2 L3 L4 L5 L6
Contextual 0.29 0.31 0.35 0.36 0.36
Co-occurrence 0.47 0.56 0.45 0.41 0.41
Patterns 0.47 0.44 0.42 0.39 0.40
Syntactic 0.31 0.28 0.36 0.38 0.39
Word Length 0.16 0.16 0.16 0.16 0.16
Definition 0.12 0.12 0.12 0.12 0.12
</table>
<tableCaption confidence="0.940862">
Table 5. F1-measure for Features vs. Abstractness:
WordNet/is-a.
</tableCaption>
<table confidence="0.999970428571428">
Feature L2 L3 L4 L5 L6
Contextual 0.30 0.30 0.33 0.29 0.29
Co-occurrence 0.34 0.36 0.34 0.31 0.31
Patterns 0.23 0.25 0.30 0.28 0.28
Syntactic 0.18 0.18 0.23 0.27 0.27
Word Length 0.15 0.15 0.15 0.14 0.14
Definition 0.13 0.13 0.13 0.12 0.12
</table>
<tableCaption confidence="0.847687">
Table 6. F1-measure for Features vs. Abstractness:
</tableCaption>
<bodyText confidence="0.775589285714286">
ODP/is-a.
WordNet. For example, under artificial intelli-
gence, ODP has neural networks, natural lan-
guage and academic departments. Clearly, aca-
demic departments is not a hyponym of artificial
intelligence. The noise in ODP interferes with
the learning process, thus hurts the performance.
</bodyText>
<subsectionHeader confidence="0.982088">
5.4 Features vs. Relations
</subsectionHeader>
<bodyText confidence="0.991999333333333">
This section studies the impact of different sets
of features on different types of relations. Table 4
shows F1-measure of using each set of features
alone on taxonomy induction for WordNet is-a,
sibling, and part-of relations. Bold font means a
feature set gives a major contribution to the task
of automatic taxonomy induction for a particular
type of relation.
Table 4 shows that different relations favor
different sets of features. Both co-occurrence
and lexico-syntactic patterns work well for all
three types of relations. It is interesting to see
that simple co-occurrence statistics work as good
as lexico-syntactic patterns. Contextual features
work well for sibling relations, but not for is-a
and part-of. Syntactic features also work well for
sibling, but not for is-a and part-of. The similar
behavior of contextual and syntactic features
may be because that four out of five syntactic
features (Modifier, Subject, Object, and Verb
overlaps) are just surrounding context for a term.
Comparing the is-a and part-of columns in
Table 4 and the ME rows in Table 3, we notice a
significant difference in F1-measure. It indicates
that combination of heterogeneous features gives
more rise to the system performance than a sin-
gle set of features does.
</bodyText>
<subsectionHeader confidence="0.95532">
5.5 Features vs. Abstractness
</subsectionHeader>
<bodyText confidence="0.997819424242424">
This section studies the impact of different sets
of features on terms at different abstraction le-
vels. In the experiments, F1-measure is evaluated
for terms at each level of a taxonomy, not the
whole taxonomy. Table 5 and 6 demonstrate F1-
measure of using each set of features alone on
each abstraction levels. Columns 2-6 are indices
of the levels in a taxonomy. The larger the indic-
es are, the lower the levels. Higher levels contain
abstract terms, while lower levels contain con-
crete terms. L1 is ignored here since it only con-
tains a single term, the root. Bold font indicates
good performance in a column.
Both tables show that abstract terms and con-
crete terms favor different sets of features. In
particular, contextual, co-occurrence, pattern,
and syntactic features work well for terms at L4-
L6, i.e., concrete terms; co-occurrence works well
for terms at L2-L3, i.e., abstract terms. This differ-
ence indicates that terms at different abstraction
levels have different characteristics; it confirms
our abstractness assumption in Section 4.1.
We also observe that for abstract terms in
WordNet, patterns work better than contextual
features; while for abstract terms in ODP, the
conclusion is the opposite. This may be because
that WordNet has a richer vocabulary and a more
rigid definition of hypernyms, and hence is-a
relations in WordNet are recognized more effec-
tively by using lexico-syntactic patterns; while
ODP contains more noise, and hence it favors
features requiring less rigidity, such as the con-
textual features generated from the Web.
</bodyText>
<sectionHeader confidence="0.998003" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999907714285714">
This paper presents a novel metric-based tax-
onomy induction framework combining the
strengths of lexico-syntactic patterns and cluster-
ing. The framework incrementally clusters terms
and transforms automatic taxonomy induction
into a multi-criteria optimization based on mini-
mization of taxonomy structures and modeling of
term abstractness. The experiments show that our
framework is effective; it achieves higher F1-
measure than three state-of-the-art systems. The
paper also studies which features are the best for
different types of relations and for terms at dif-
ferent abstraction levels.
Most prior work uses a single rule or feature
function for automatic taxonomy induction at all
levels of abstraction. Our work is a more general
framework which allows a wider range of fea-
tures and different metric functions at different
abstraction levels. This more general framework
has the potential to learn more complex taxono-
mies than previous approaches.
</bodyText>
<sectionHeader confidence="0.994836" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.5933366">
This research was supported by NSF grant IIS-
0704210. Any opinions, findings, conclusions, or
recommendations expressed in this paper are of
the authors, and do not necessarily reflect those
of the sponsor.
</reference>
<page confidence="0.997737">
278
</page>
<sectionHeader confidence="0.993857" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999882093457944">
M. Berland and E. Charniak. 1999. Finding parts in very
large corpora. ACL’99.
S. Boyd and L. Vandenberghe. 2004. Convex optimization.
In Cambridge University Press, 2004.
P. Brown, V. D. Pietra, P. deSouza, J. Lai, and R. Mercer.
1992. Class-based ngram models for natural language.
Computational Linguistics, 18(4):468–479.
P. Buitelaar, P. Cimiano, and B. Magnini. 2005. Ontology
Learning from Text: Methods, Evaluation and Applica-
tions. Volume 123 Frontiers in Artificial Intelligence and
Applications.
R. Bunescu and R. Mooney. 2007. Learning to Extract
Relations from the Web using Minimal Supervision.
ACL’07.
S. Caraballo. 1999. Automatic construction of a hypernym-
labeled noun hierarchy from text. ACL’99.
T. Chklovski and P. Pantel. 2004. VerbOcean: mining the
web for fine-grained semantic verb relations. EMNLP
’04.
P. Cimiano and J. Volker. 2005. Towards large-scale, open-
domain and ontology-based named entity classification.
RANLP’07.
P. Cimiano and J. Wenderoth. 2007. Automatic Acquisition
of Ranked Qualia Structures from the Web. ACL’07.
D. Davidov and A. Rappoport. 2006. Efficient Unsuper-
vised Discovery of Word Categories Using Symmetric
Patterns and High Frequency Words. ACL’06.
D. Davidov and A. Rappoport. 2008. Classification of Se-
mantic Relationships between Nominals Using Pattern
Clusters. ACL’08.
D. Downey, O. Etzioni, and S. Soderland. 2005. A Probabil-
istic model of redundancy in information extraction. IJ-
CAI’05.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T.
Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Un-
supervised named-entity extraction from the web: an ex-
perimental study. Artificial Intelligence, 165(1):91–134.
C. Fellbuam. 1998. WordNet: An Electronic Lexical Data-
base. MIT Press. 1998.
M. Geffet and I. Dagan. 2005. The Distributional Inclusion
Hypotheses and Lexical Entailment. ACL’05.
R. Girju, A. Badulescu, and D. Moldovan. 2003. Learning
Semantic Constraints for the Automatic Discovery of
Part-Whole Relations. HLT’03.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Automatic
Discovery of Part-Whole Relations. Computational Lin-
guistics, 32(1): 83-135.
Z. Harris. 1985. Distributional structure. In Word, 10(23):
146-162s, 1954.
T. Hastie, R. Tibshirani and J. Friedman. 2001. The Ele-
ments of Statistical Learning: Data Mining, Inference,
and Prediction. Springer-Verlag, 2001.
M. Hearst. 1992. Automatic acquisition of hyponyms from
large text corpora. COLING’92.
M. D. Hendy and D. Penny. 1982. Branch and bound algo-
rithms to determine minimal evolutionary trees. Mathe-
matical Biosciences 59: 277-290.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic Class
Learning from the Web with Hyponym Pattern Linkage
Graphs. ACL’08.
D. Lin, 1998. Automatic retrieval and clustering of similar
words. COLING’98.
D. Lin, S. Zhao, L. Qin, and M. Zhou. 2003. Identifying
Synonyms among Distributionally Similar Words. IJ-
CAI’03.
G. S. Mann. 2002. Fine-Grained Proper Noun Ontologies
for Question Answering. In Proceedings of SemaNet’ 02:
Building and Using Semantic Networks, Taipei.
P. Pantel and D Lin. 2002. Discovering word senses from
text. SIGKDD’02.
P. Pantel and D. Ravichandran. 2004. Automatically labe-
ling semantic classes. HLT/NAACL’04.
P. Pantel, D. Ravichandran, and E. Hovy. 2004. Towards
terascale knowledge acquisition. COLING’04.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging
Generic Patterns for Automatically Harvesting Semantic
Relations. ACL’06.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clus-
tering of English words. ACL’93.
D. Ravichandran and E. Hovy. 2002. Learning surface text
patterns for a question answering system. ACL’02.
E. Riloff and J. Shepherd. 1997. A corpus-based approach
for building semantic lexicons. EMNLP’97.
B. Roark and E. Charniak. 1998. Noun-phrase co-
occurrence statistics for semi-automatic semantic lexicon
construction. ACL/COLING’98.
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning syntac-
tic patterns for automatic hypernym discovery. NIPS’05.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic Tax-
onomy Induction from Heterogeneous Evidence.
ACL’06.
B. Rosenfeld and R. Feldman. 2007. Clustering for unsu-
pervised relation identification. CIKM’07.
P. Turney, M. Littman, J. Bigham, and V. Shnayder. 2003.
Combining independent modules to solve multiple-
choice synonym and analogy problems. RANLP’03.
S. M. Harabagiu, S. J. Maiorano and M. A. Pasca. 2003.
Open-Domain Textual Question Answering Techniques.
Natural Language Engineering 9 (3): 1-38, 2003.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based acquisition of entailment relations.
EMNLP’04.
D. Widdows and B. Dorow. 2002. A graph model for unsu-
pervised Lexical acquisition. COLING ’02.
H. Yang and J. Callan. 2008. Learning the Distance Metric
in a Personal Ontology. Workshop on Ontologies and In-
formation Systems for the Semantic Web of CIKM’08.
</reference>
<page confidence="0.998551">
279
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953364">
<title confidence="0.999971">A Metric-based Framework for Automatic Taxonomy Induction</title>
<author confidence="0.999828">Hui Yang</author>
<affiliation confidence="0.999289333333333">Language Technologies Institute School of Computer Science Carnegie Mellon University</affiliation>
<email confidence="0.997034">huiyang@cs.cmu.edu</email>
<author confidence="0.999871">Jamie Callan</author>
<affiliation confidence="0.998384">Language Technologies Institute School of Computer Science Carnegie Mellon University</affiliation>
<email confidence="0.99906">callan@cs.cmu.edu</email>
<abstract confidence="0.9981991">This paper presents a novel metric-based framework for the task of automatic taxonomy induction. The framework incrementally clusterms based on a score indicating semantic distance; and transforms the task into a multi-criteria optimization based on minimization of taxonomy structures and modeling of term abstractness. It combines the strengths of both lexico-syntactic patterns and clustering through incorporating heterogeneous features. The flexible design of the framework allows a further study on which features are the best for the task under various conditions. The experiments not only show that our system achieves higher F1-measure than other state-of-the-art systems, but also reveal the interaction between features and various types of relations, as well as the interaction between features and term abstractness.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This research was supported by NSF grant IIS0704210. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.</title>
<marker></marker>
<rawString>This research was supported by NSF grant IIS0704210. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Berland</author>
<author>E Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<pages>99</pages>
<contexts>
<context position="3138" citStr="Berland and Charniak, 1999" startWordPosition="447" endWordPosition="450">ntic class learning, semantic relation classification, and relation extraction. The approaches fall into two main categories: pattern-based and clusteringbased. Pattern-based approaches define lexicalsyntactic patterns for relations, and use these patterns to discover instances of relations. Clustering-based approaches hierarchically cluster terms based on similarities of their meanings usually represented by a vector of quantifiable features. Pattern-based approaches are known for their high accuracy in recognizing instances of relations if the patterns are carefully chosen, either manually (Berland and Charniak, 1999; Kozareva et al., 2008) or via automatic bootstrapping (Hearst, 1992; Widdows and Dorow, 2002; Girju et al., 2003). The approaches, however, suffer from sparse coverage of patterns in a given corpus. Recent studies (Etzioni et al., 2005; Kozareva et al., 2008) show that if the size of a corpus, such as the Web, is nearly unlimited, a pattern has a higher chance to explicitly appear in the corpus. However, corpus size is often not that large; hence the problem still exists. Moreover, since patterns usually extract instances in pairs, the approaches suffer from the problem of inconsistent conce</context>
<context position="7742" citStr="Berland and Charniak (1999)" startWordPosition="1150" endWordPosition="1153">tatistics in local context to discover sibling relations. The KnowItAll system (Etzioni et al., 2005) extended the work in (Hearst, 1992) and bootstrapped patterns on the Web to discover siblings; it also ranked and selected the patterns by statistical measures. Widdows and Dorow (2002) combined symmetric patterns and graph link analysis to discover sibling relations. Davidov and Rappoport (2006) also used symmetric patterns for this task. Recently, Kozareva et al. (2008) combined a double-anchored hyponym pattern with graph structure to extract siblings. The third common relation is part-of. Berland and Charniak (1999) used two meronym patterns to discover part-of relations, and also used statistical measures to rank and select the matching instances. Girju et al. (2003) took a similar approach to Hearst (1992) for part-of relations. Other types of relations that have been studied by pattern-based approaches include questionanswer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al., 2003), general purpose analogy (Turney et al., 2003), verb relations (including similarity, strength, antonym, enablement and temporal) (Chklovski and Pantel, 2004), entai</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>M. Berland and E. Charniak. 1999. Finding parts in very large corpora. ACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Boyd</author>
<author>L Vandenberghe</author>
</authors>
<title>Convex optimization. In</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="23405" citStr="Boyd and Vandenberghe, 2004" startWordPosition="3840" endWordPosition="3843">oi(.) is: min |( ) Infoi Li −WiT Hi |2. By plugging Equation (2) and minimizing over every abstraction level, we have: min ( ∑ ∑ d(cx,cy)−∑ wi,jhi,j(cx,c i cx c y Li , ∈ where hi, j (.,.) is the jth underlying feature function for term pairs at level Li, wi, j is the weight for hi, j (.,.). This minimization follows the abstractness assumption; hence we call it the abstractness objective. The Multi-Criterion Optimization Algorithm We propose that both minimum evolution and abstractness objectives need to be satisfied. To optimize multiple criteria, the Pareto optimality needs to be satisfied (Boyd and Vandenberghe, 2004). We handle this by introducing A E [0,1] to control the contribution of each objective. The multi-criterion optimization function is: min (1 ) λ u + − λ v ∑ ∑d(cx ,cy)− v =∑ ∑ d(cx,cy)−∑wi,j hi, j (cx , c y )) ( i c c L x y i , ∈ x&lt;y The above optimization can be solved by a greedy optimization algorithm. At each term insertion step, it produces a new partial taxonomy by adding to the existing partial taxonomy a new term z, and a new set of relations R(z,.). z is attached to every nodes in the existing partial taxonomy; and the algorithm selects the optimal position indicated by R(z,.), which</context>
</contexts>
<marker>Boyd, Vandenberghe, 2004</marker>
<rawString>S. Boyd and L. Vandenberghe. 2004. Convex optimization. In Cambridge University Press, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>V D Pietra</author>
<author>P deSouza</author>
<author>J Lai</author>
<author>R Mercer</author>
</authors>
<title>Class-based ngram models for natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="9762" citStr="Brown et al., 1992" startWordPosition="1452" endWordPosition="1455">d. Statistical measures, such as point-wise mutual information (Etzioni et al., 2005; Pantel and Pennacchiotti, 2006) and conditional probability (Cimiano and Wenderoth, 2007), have been shown to be effective to rank and select patterns and instances. Pattern quality control is also investigated by using WordNet (Girju et al., 2006), graph structures built among terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Clustering-based approaches allow discovery of relations which do not explicitly appear in text. Pantel and Pennacchiotti (2006), however, pointed out that clustering-based approaches generally fail to produce coherent cluster fo</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P. Brown, V. D. Pietra, P. deSouza, J. Lai, and R. Mercer. 1992. Class-based ngram models for natural language. Computational Linguistics, 18(4):468–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
<author>P Cimiano</author>
<author>B Magnini</author>
</authors>
<date>2005</date>
<booktitle>Ontology Learning from Text: Methods, Evaluation and Applications. Volume 123 Frontiers in Artificial Intelligence and Applications.</booktitle>
<contexts>
<context position="10104" citStr="Buitelaar et al., 2005" startWordPosition="1502" endWordPosition="1505">aph structures built among terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Clustering-based approaches allow discovery of relations which do not explicitly appear in text. Pantel and Pennacchiotti (2006), however, pointed out that clustering-based approaches generally fail to produce coherent cluster for small corpora. In addition, clustering-based approaches had only applied to solve is-a and sibling relations. Many clustering-based approaches face the challenge of appropriately labeling non-leaf clusters. The labeling amplifies the difficulty in creation and evaluation of taxonomies. Agglomerative clustering (Brown et al., 1992; Carabal</context>
</contexts>
<marker>Buitelaar, Cimiano, Magnini, 2005</marker>
<rawString>P. Buitelaar, P. Cimiano, and B. Magnini. 2005. Ontology Learning from Text: Methods, Evaluation and Applications. Volume 123 Frontiers in Artificial Intelligence and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R Mooney</author>
</authors>
<title>Learning to Extract Relations from the Web using Minimal Supervision.</title>
<date>2007</date>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>R. Bunescu and R. Mooney. 2007. Learning to Extract Relations from the Web using Minimal Supervision. ACL’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Caraballo</author>
</authors>
<title>Automatic construction of a hypernymlabeled noun hierarchy from text.</title>
<date>1999</date>
<pages>99</pages>
<contexts>
<context position="10053" citStr="Caraballo, 1999" startWordPosition="1495" endWordPosition="1496">ted by using WordNet (Girju et al., 2006), graph structures built among terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Clustering-based approaches allow discovery of relations which do not explicitly appear in text. Pantel and Pennacchiotti (2006), however, pointed out that clustering-based approaches generally fail to produce coherent cluster for small corpora. In addition, clustering-based approaches had only applied to solve is-a and sibling relations. Many clustering-based approaches face the challenge of appropriately labeling non-leaf clusters. The labeling amplifies the difficulty in creation and evaluation of taxonomies. Ag</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>S. Caraballo. 1999. Automatic construction of a hypernymlabeled noun hierarchy from text. ACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Chklovski</author>
<author>P Pantel</author>
</authors>
<title>VerbOcean: mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<journal>EMNLP</journal>
<volume>04</volume>
<contexts>
<context position="8335" citStr="Chklovski and Pantel, 2004" startWordPosition="1240" endWordPosition="1243">rt-of. Berland and Charniak (1999) used two meronym patterns to discover part-of relations, and also used statistical measures to rank and select the matching instances. Girju et al. (2003) took a similar approach to Hearst (1992) for part-of relations. Other types of relations that have been studied by pattern-based approaches include questionanswer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al., 2003), general purpose analogy (Turney et al., 2003), verb relations (including similarity, strength, antonym, enablement and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al., 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney , 2007). The most commonly used technique in pattern-based approaches is bootstrapping (Hearst, 1992; Etzioni et al., 2005; Girju et al., 2003; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). It utilizes a few man-crafted seed patterns to extract instances from corpora, then extracts new patterns using these instances, and continues the cycle to find new instances and new patterns. It is effective and scalable to l</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>T. Chklovski and P. Pantel. 2004. VerbOcean: mining the web for fine-grained semantic verb relations. EMNLP ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>J Volker</author>
</authors>
<title>Towards large-scale, opendomain and ontology-based named entity classification.</title>
<date>2005</date>
<pages>07</pages>
<contexts>
<context position="10131" citStr="Cimiano and Volker, 2005" startWordPosition="1506" endWordPosition="1509">ng terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Clustering-based approaches allow discovery of relations which do not explicitly appear in text. Pantel and Pennacchiotti (2006), however, pointed out that clustering-based approaches generally fail to produce coherent cluster for small corpora. In addition, clustering-based approaches had only applied to solve is-a and sibling relations. Many clustering-based approaches face the challenge of appropriately labeling non-leaf clusters. The labeling amplifies the difficulty in creation and evaluation of taxonomies. Agglomerative clustering (Brown et al., 1992; Caraballo, 1999; Rosenfeld and Fel</context>
</contexts>
<marker>Cimiano, Volker, 2005</marker>
<rawString>P. Cimiano and J. Volker. 2005. Towards large-scale, opendomain and ontology-based named entity classification. RANLP’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>J Wenderoth</author>
</authors>
<date>2007</date>
<booktitle>Automatic Acquisition of Ranked Qualia Structures from the Web. ACL’07.</booktitle>
<contexts>
<context position="8457" citStr="Cimiano and Wenderoth, 2007" startWordPosition="1257" endWordPosition="1260">sures to rank and select the matching instances. Girju et al. (2003) took a similar approach to Hearst (1992) for part-of relations. Other types of relations that have been studied by pattern-based approaches include questionanswer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al., 2003), general purpose analogy (Turney et al., 2003), verb relations (including similarity, strength, antonym, enablement and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al., 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney , 2007). The most commonly used technique in pattern-based approaches is bootstrapping (Hearst, 1992; Etzioni et al., 2005; Girju et al., 2003; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). It utilizes a few man-crafted seed patterns to extract instances from corpora, then extracts new patterns using these instances, and continues the cycle to find new instances and new patterns. It is effective and scalable to large datasets; however, uncontrolled bootstrapping 272 soon generates undesired instances once a noisy pattern brought int</context>
<context position="16352" citStr="Cimiano and Wenderoth, 2007" startWordPosition="2546" endWordPosition="2549">n sentences containing them, (7) Modifier Overlap, (8) Object Overlap, (9) Subject Overlap, and (10) Verb Overlap to measure the number of overlaps between modifiers, objects, subjects, and verbs, respectively, for the two terms in sentences containing them. We use Assert2 to label the semantic roles. The fourth set of features is lexical-syntactic patterns. We have (11) Hypernym Patterns based on patterns proposed by (Hearst, 1992) and (Snow et al., 2005), (12) Sibling Patterns which are basically conjunctions, and (13) Part-of Patterns based on patterns proposed by (Girju et al., 2003) and (Cimiano and Wenderoth, 2007). Table 1 lists all patterns. Each feature function returns of scores for two input terms, one score per pattern. A score is 1 if two terms match a pattern in text, 0 otherwise. The last set of features is miscellaneous. We have (14) Word Length Difference to measure the length difference between two terms, and (15) Definition Overlap to measure the number of word overlaps between the term definitions obtained by querying Google with These heterogeneous features vary from simple statistics to complicated syntactic dependency features, basic word length to comprehensive Web-based contextual fea</context>
</contexts>
<marker>Cimiano, Wenderoth, 2007</marker>
<rawString>P. Cimiano and J. Wenderoth. 2007. Automatic Acquisition of Ranked Qualia Structures from the Web. ACL’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>A Rappoport</author>
</authors>
<title>Efficient Unsupervised Discovery of Word Categories Using Symmetric Patterns and High Frequency Words.</title>
<date>2006</date>
<contexts>
<context position="7514" citStr="Davidov and Rappoport (2006)" startWordPosition="1116" endWordPosition="1120">bers of the same class. Terms in sibling relations are also known as class members or similar terms. Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roark and Charniak (1998) used cooccurrence statistics in local context to discover sibling relations. The KnowItAll system (Etzioni et al., 2005) extended the work in (Hearst, 1992) and bootstrapped patterns on the Web to discover siblings; it also ranked and selected the patterns by statistical measures. Widdows and Dorow (2002) combined symmetric patterns and graph link analysis to discover sibling relations. Davidov and Rappoport (2006) also used symmetric patterns for this task. Recently, Kozareva et al. (2008) combined a double-anchored hyponym pattern with graph structure to extract siblings. The third common relation is part-of. Berland and Charniak (1999) used two meronym patterns to discover part-of relations, and also used statistical measures to rank and select the matching instances. Girju et al. (2003) took a similar approach to Hearst (1992) for part-of relations. Other types of relations that have been studied by pattern-based approaches include questionanswer relations (such as birthdates and inventor) (Ravichan</context>
</contexts>
<marker>Davidov, Rappoport, 2006</marker>
<rawString>D. Davidov and A. Rappoport. 2006. Efficient Unsupervised Discovery of Word Categories Using Symmetric Patterns and High Frequency Words. ACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>A Rappoport</author>
</authors>
<title>Classification of Semantic Relationships between Nominals Using Pattern Clusters.</title>
<date>2008</date>
<contexts>
<context position="9615" citStr="Davidov and Rappoport, 2008" startWordPosition="1431" endWordPosition="1434">72 soon generates undesired instances once a noisy pattern brought into the cycle. To aid bootstrapping, methods of pattern quality control are widely applied. Statistical measures, such as point-wise mutual information (Etzioni et al., 2005; Pantel and Pennacchiotti, 2006) and conditional probability (Cimiano and Wenderoth, 2007), have been shown to be effective to rank and select patterns and instances. Pattern quality control is also investigated by using WordNet (Girju et al., 2006), graph structures built among terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Clustering-based approaches allow discovery of relations which do not explicitly a</context>
</contexts>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>D. Davidov and A. Rappoport. 2008. Classification of Semantic Relationships between Nominals Using Pattern Clusters. ACL’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>O Etzioni</author>
<author>S Soderland</author>
</authors>
<title>A Probabilistic model of redundancy in information extraction.</title>
<date>2005</date>
<pages>05</pages>
<marker>Downey, Etzioni, Soderland, 2005</marker>
<rawString>D. Downey, O. Etzioni, and S. Soderland. 2005. A Probabilistic model of redundancy in information extraction. IJCAI’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="3375" citStr="Etzioni et al., 2005" startWordPosition="486" endWordPosition="489">hese patterns to discover instances of relations. Clustering-based approaches hierarchically cluster terms based on similarities of their meanings usually represented by a vector of quantifiable features. Pattern-based approaches are known for their high accuracy in recognizing instances of relations if the patterns are carefully chosen, either manually (Berland and Charniak, 1999; Kozareva et al., 2008) or via automatic bootstrapping (Hearst, 1992; Widdows and Dorow, 2002; Girju et al., 2003). The approaches, however, suffer from sparse coverage of patterns in a given corpus. Recent studies (Etzioni et al., 2005; Kozareva et al., 2008) show that if the size of a corpus, such as the Web, is nearly unlimited, a pattern has a higher chance to explicitly appear in the corpus. However, corpus size is often not that large; hence the problem still exists. Moreover, since patterns usually extract instances in pairs, the approaches suffer from the problem of inconsistent concept chains after connecting pairs of instances to form taxonomy hierarchies. Clustering-based approaches have a main advantage that they are able to discover relations 271 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCN</context>
<context position="6453" citStr="Etzioni et al., 2005" startWordPosition="954" endWordPosition="957">omy induction. Though suffering from the problems of sparse coverage and inconsistent chains, they are still popular due to their simplicity and high accuracy. They have been applied to extract various types of lexical and semantic relations, including is-a, part-of, sibling, synonym, causal, and many others. Pattern-based approaches started from and still pay a great deal of attention to the most common is-a relations. Hearst (1992) pioneered using a hand crafted list of hyponym patterns as seeds and employing bootstrapping to discover is-a relations. Since then, many approaches (Mann, 2002; Etzioni et al., 2005; Snow et al., 2005) have used Hearst-style patterns in their work on is-a relations. For instance, Mann (2002) extracted is-a relations for proper nouns by Hearststyle patterns. Pantel et al. (2004) extended is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance. Another common relation is sibling, which describes the relation of sharing similar meanings and being members of the same class. Terms in sibling relations are also known as class members or similar terms. Inspired by the conjunction and appositive structures, Riloff and </context>
<context position="8625" citStr="Etzioni et al., 2005" startWordPosition="1282" endWordPosition="1285">died by pattern-based approaches include questionanswer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al., 2003), general purpose analogy (Turney et al., 2003), verb relations (including similarity, strength, antonym, enablement and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al., 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney , 2007). The most commonly used technique in pattern-based approaches is bootstrapping (Hearst, 1992; Etzioni et al., 2005; Girju et al., 2003; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). It utilizes a few man-crafted seed patterns to extract instances from corpora, then extracts new patterns using these instances, and continues the cycle to find new instances and new patterns. It is effective and scalable to large datasets; however, uncontrolled bootstrapping 272 soon generates undesired instances once a noisy pattern brought into the cycle. To aid bootstrapping, methods of pattern quality control are widely applied. Statistical measures, such as point-wise mutual information (Etzioni et al., 2</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbuam</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<marker>Fellbuam, 1998</marker>
<rawString>C. Fellbuam. 1998. WordNet: An Electronic Lexical Database. MIT Press. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Geffet</author>
<author>I Dagan</author>
</authors>
<date>2005</date>
<booktitle>The Distributional Inclusion Hypotheses and Lexical Entailment. ACL’05.</booktitle>
<contexts>
<context position="1567" citStr="Geffet and Dagan, 2005" startWordPosition="214" endWordPosition="217">es higher F1-measure than other state-of-the-art systems, but also reveal the interaction between features and various types of relations, as well as the interaction between features and term abstractness. 1 Introduction Automatic taxonomy induction is an important task in the fields of Natural Language Processing, Knowledge Management, and Semantic Web. It has been receiving increasing attention because semantic taxonomies, such as WordNet (Fellbaum, 1998), play an important role in solving knowledge-rich problems, including question answering (Harabagiu et al., 2003) and textual entailment (Geffet and Dagan, 2005). Nevertheless, most existing taxonomies are manually created at great cost. These taxonomies are rarely complete; it is difficult to include new terms in them from emerging or rapidly changing domains. Moreover, manual taxonomy construction is time-consuming, which may make it unfeasible for specialized domains and personalized tasks. Automatic taxonomy induction is a solution to augment existing resources and to produce new taxonomies for such domains and tasks. Automatic taxonomy induction can be decomposed into two subtasks: term extraction and relation formation. Since term extraction is </context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>M. Geffet and I. Dagan. 2005. The Distributional Inclusion Hypotheses and Lexical Entailment. ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Badulescu</author>
<author>D Moldovan</author>
</authors>
<title>Learning Semantic Constraints for the Automatic Discovery of Part-Whole Relations.</title>
<date>2003</date>
<tech>HLT’03.</tech>
<contexts>
<context position="3253" citStr="Girju et al., 2003" startWordPosition="466" endWordPosition="469">es: pattern-based and clusteringbased. Pattern-based approaches define lexicalsyntactic patterns for relations, and use these patterns to discover instances of relations. Clustering-based approaches hierarchically cluster terms based on similarities of their meanings usually represented by a vector of quantifiable features. Pattern-based approaches are known for their high accuracy in recognizing instances of relations if the patterns are carefully chosen, either manually (Berland and Charniak, 1999; Kozareva et al., 2008) or via automatic bootstrapping (Hearst, 1992; Widdows and Dorow, 2002; Girju et al., 2003). The approaches, however, suffer from sparse coverage of patterns in a given corpus. Recent studies (Etzioni et al., 2005; Kozareva et al., 2008) show that if the size of a corpus, such as the Web, is nearly unlimited, a pattern has a higher chance to explicitly appear in the corpus. However, corpus size is often not that large; hence the problem still exists. Moreover, since patterns usually extract instances in pairs, the approaches suffer from the problem of inconsistent concept chains after connecting pairs of instances to form taxonomy hierarchies. Clustering-based approaches have a main</context>
<context position="7897" citStr="Girju et al. (2003)" startWordPosition="1175" endWordPosition="1178"> on the Web to discover siblings; it also ranked and selected the patterns by statistical measures. Widdows and Dorow (2002) combined symmetric patterns and graph link analysis to discover sibling relations. Davidov and Rappoport (2006) also used symmetric patterns for this task. Recently, Kozareva et al. (2008) combined a double-anchored hyponym pattern with graph structure to extract siblings. The third common relation is part-of. Berland and Charniak (1999) used two meronym patterns to discover part-of relations, and also used statistical measures to rank and select the matching instances. Girju et al. (2003) took a similar approach to Hearst (1992) for part-of relations. Other types of relations that have been studied by pattern-based approaches include questionanswer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al., 2003), general purpose analogy (Turney et al., 2003), verb relations (including similarity, strength, antonym, enablement and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al., 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and M</context>
<context position="16318" citStr="Girju et al., 2003" startWordPosition="2541" endWordPosition="2544">ed by between two terms in sentences containing them, (7) Modifier Overlap, (8) Object Overlap, (9) Subject Overlap, and (10) Verb Overlap to measure the number of overlaps between modifiers, objects, subjects, and verbs, respectively, for the two terms in sentences containing them. We use Assert2 to label the semantic roles. The fourth set of features is lexical-syntactic patterns. We have (11) Hypernym Patterns based on patterns proposed by (Hearst, 1992) and (Snow et al., 2005), (12) Sibling Patterns which are basically conjunctions, and (13) Part-of Patterns based on patterns proposed by (Girju et al., 2003) and (Cimiano and Wenderoth, 2007). Table 1 lists all patterns. Each feature function returns of scores for two input terms, one score per pattern. A score is 1 if two terms match a pattern in text, 0 otherwise. The last set of features is miscellaneous. We have (14) Word Length Difference to measure the length difference between two terms, and (15) Definition Overlap to measure the number of word overlaps between the term definitions obtained by querying Google with These heterogeneous features vary from simple statistics to complicated syntactic dependency features, basic word length to comp</context>
<context position="29119" citStr="Girju et al. (2003)" startWordPosition="4810" endWordPosition="4813">//www.lemurproject.org/indri/. Table 4. F1-measure for Features vs. Relations: WordNet. time, and report the averaged precision, recall and F1-measure across all 50 runs. We also group the fifteen features in Section 3 into six sets: contextual, co-concurrence, patterns, syntactic dependency, word length difference and definition. Each set is turned on one by one for experiments in Section 5.4 and 5.5. 5.3 Performance of Taxonomy Induction In this section, we compare the following automatic taxonomy induction systems: HE, the system by Hearst (1992) with 6 hypernym patterns; GI, the system by Girju et al. (2003) with 3 meronym patterns; PR, the probabilistic framework by Snow et al. (2006); and ME, the metric-based framework proposed in this paper. To have a fair comparison, for PR, we estimate the conditional probability of a relation given the evidence P(Ri;|Ei;), as in (Snow et al. 2006), by using the same set of features as in ME. Table 3 shows precision, recall, and F1- measure of each system for WordNet hypernyms (is-a), WordNet meronyms (part-of) and ODP hypernyms (is-a). Bold font indicates the best performance in a column. Note that HE is not applicable to part-of, so is GI to is-a. Table 3 </context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>R. Girju, A. Badulescu, and D. Moldovan. 2003. Learning Semantic Constraints for the Automatic Discovery of Part-Whole Relations. HLT’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Badulescu</author>
<author>D Moldovan</author>
</authors>
<title>Automatic Discovery of Part-Whole Relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>83--135</pages>
<contexts>
<context position="9478" citStr="Girju et al., 2006" startWordPosition="1410" endWordPosition="1413">le to find new instances and new patterns. It is effective and scalable to large datasets; however, uncontrolled bootstrapping 272 soon generates undesired instances once a noisy pattern brought into the cycle. To aid bootstrapping, methods of pattern quality control are widely applied. Statistical measures, such as point-wise mutual information (Etzioni et al., 2005; Pantel and Pennacchiotti, 2006) and conditional probability (Cimiano and Wenderoth, 2007), have been shown to be effective to rank and select patterns and instances. Pattern quality control is also investigated by using WordNet (Girju et al., 2006), graph structures built among terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described </context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2006</marker>
<rawString>R. Girju, A. Badulescu, and D. Moldovan. 2006. Automatic Discovery of Part-Whole Relations. Computational Linguistics, 32(1): 83-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1985</date>
<journal>In Word,</journal>
<volume>10</volume>
<issue>23</issue>
<pages>146--162</pages>
<marker>Harris, 1985</marker>
<rawString>Z. Harris. 1985. Distributional structure. In Word, 10(23): 146-162s, 1954.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>R Tibshirani</author>
<author>J Friedman</author>
</authors>
<title>The Elements of Statistical Learning: Data Mining, Inference, and Prediction.</title>
<date>2001</date>
<publisher>Springer-Verlag,</publisher>
<contexts>
<context position="24787" citStr="Hastie et al., 2001" startWordPosition="4092" endWordPosition="4095">nts a general incremental clustering procedure to construct taxonomies. By minimizing the taxonomy structure changes and modeling term abstractness at each Statistics WN/is-a ODP/is-a WN/part-of #taxonomies 50 50 50 #terms 1,964 2,210 1,812 Avg #terms 39 44 37 Avg depth 6 6 5 Table 2. Data Statistics. step, it finds the optimal position of each term in the taxonomy hierarchy. 4.3 Estimating Ontology Metric Learning a good ontology metric is important for the multi-criterion optimization algorithm. In this work, the estimation and prediction of ontology metric are achieved by ridge regression (Hastie et al., 2001). In the training data, an ontology metric d(cx,cy) for a term pair (cx,cy) is generated by assuming every edge weight as 1 and summing up all the edge weights along the shortest path from cx to cy. We assume that there are some underlying feature functions which measure the semantic distance from term cx to cy. A weighted combination of these functions approximates the ontology metric for (cx,cy): d(x,y)=∑ j wjhj(cx,cy) where wj is the jth weight for hj (cx, cy) , the jth feature function. The feature functions are generated as mentioned in Section 3. 5 Experiments 5.1 Data 3 WordNet hypernym</context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2001</marker>
<rawString>T. Hastie, R. Tibshirani and J. Friedman. 2001. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer-Verlag, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<pages>92</pages>
<contexts>
<context position="3207" citStr="Hearst, 1992" startWordPosition="460" endWordPosition="461"> approaches fall into two main categories: pattern-based and clusteringbased. Pattern-based approaches define lexicalsyntactic patterns for relations, and use these patterns to discover instances of relations. Clustering-based approaches hierarchically cluster terms based on similarities of their meanings usually represented by a vector of quantifiable features. Pattern-based approaches are known for their high accuracy in recognizing instances of relations if the patterns are carefully chosen, either manually (Berland and Charniak, 1999; Kozareva et al., 2008) or via automatic bootstrapping (Hearst, 1992; Widdows and Dorow, 2002; Girju et al., 2003). The approaches, however, suffer from sparse coverage of patterns in a given corpus. Recent studies (Etzioni et al., 2005; Kozareva et al., 2008) show that if the size of a corpus, such as the Web, is nearly unlimited, a pattern has a higher chance to explicitly appear in the corpus. However, corpus size is often not that large; hence the problem still exists. Moreover, since patterns usually extract instances in pairs, the approaches suffer from the problem of inconsistent concept chains after connecting pairs of instances to form taxonomy hierar</context>
<context position="6270" citStr="Hearst (1992)" startWordPosition="928" endWordPosition="929">utomatic taxonomy induction. As we mentioned earlier, two main approaches are patternbased and clustering-based. Pattern-based approaches are the main trend for automatic taxonomy induction. Though suffering from the problems of sparse coverage and inconsistent chains, they are still popular due to their simplicity and high accuracy. They have been applied to extract various types of lexical and semantic relations, including is-a, part-of, sibling, synonym, causal, and many others. Pattern-based approaches started from and still pay a great deal of attention to the most common is-a relations. Hearst (1992) pioneered using a hand crafted list of hyponym patterns as seeds and employing bootstrapping to discover is-a relations. Since then, many approaches (Mann, 2002; Etzioni et al., 2005; Snow et al., 2005) have used Hearst-style patterns in their work on is-a relations. For instance, Mann (2002) extracted is-a relations for proper nouns by Hearststyle patterns. Pantel et al. (2004) extended is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance. Another common relation is sibling, which describes the relation of sharing similar meanin</context>
<context position="7938" citStr="Hearst (1992)" startWordPosition="1185" endWordPosition="1186">d and selected the patterns by statistical measures. Widdows and Dorow (2002) combined symmetric patterns and graph link analysis to discover sibling relations. Davidov and Rappoport (2006) also used symmetric patterns for this task. Recently, Kozareva et al. (2008) combined a double-anchored hyponym pattern with graph structure to extract siblings. The third common relation is part-of. Berland and Charniak (1999) used two meronym patterns to discover part-of relations, and also used statistical measures to rank and select the matching instances. Girju et al. (2003) took a similar approach to Hearst (1992) for part-of relations. Other types of relations that have been studied by pattern-based approaches include questionanswer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al., 2003), general purpose analogy (Turney et al., 2003), verb relations (including similarity, strength, antonym, enablement and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al., 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney , 2007). The most commonly used tec</context>
<context position="16160" citStr="Hearst, 1992" startWordPosition="2517" endWordPosition="2518">analysis. We have (6) Minipar Syntactic Distance to measure the average length of the shortest syntactic paths (in the first syntactic parse tree returned by between two terms in sentences containing them, (7) Modifier Overlap, (8) Object Overlap, (9) Subject Overlap, and (10) Verb Overlap to measure the number of overlaps between modifiers, objects, subjects, and verbs, respectively, for the two terms in sentences containing them. We use Assert2 to label the semantic roles. The fourth set of features is lexical-syntactic patterns. We have (11) Hypernym Patterns based on patterns proposed by (Hearst, 1992) and (Snow et al., 2005), (12) Sibling Patterns which are basically conjunctions, and (13) Part-of Patterns based on patterns proposed by (Girju et al., 2003) and (Cimiano and Wenderoth, 2007). Table 1 lists all patterns. Each feature function returns of scores for two input terms, one score per pattern. A score is 1 if two terms match a pattern in text, 0 otherwise. The last set of features is miscellaneous. We have (14) Word Length Difference to measure the length difference between two terms, and (15) Definition Overlap to measure the number of word overlaps between the term definitions obt</context>
<context position="29055" citStr="Hearst (1992)" startWordPosition="4800" endWordPosition="4801">mes, with different training and test sets at each 6 http://www.lemurproject.org/indri/. Table 4. F1-measure for Features vs. Relations: WordNet. time, and report the averaged precision, recall and F1-measure across all 50 runs. We also group the fifteen features in Section 3 into six sets: contextual, co-concurrence, patterns, syntactic dependency, word length difference and definition. Each set is turned on one by one for experiments in Section 5.4 and 5.5. 5.3 Performance of Taxonomy Induction In this section, we compare the following automatic taxonomy induction systems: HE, the system by Hearst (1992) with 6 hypernym patterns; GI, the system by Girju et al. (2003) with 3 meronym patterns; PR, the probabilistic framework by Snow et al. (2006); and ME, the metric-based framework proposed in this paper. To have a fair comparison, for PR, we estimate the conditional probability of a relation given the evidence P(Ri;|Ei;), as in (Snow et al. 2006), by using the same set of features as in ME. Table 3 shows precision, recall, and F1- measure of each system for WordNet hypernyms (is-a), WordNet meronyms (part-of) and ODP hypernyms (is-a). Bold font indicates the best performance in a column. Note </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. COLING’92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Hendy</author>
<author>D Penny</author>
</authors>
<title>Branch and bound algorithms to determine minimal evolutionary trees.</title>
<date>1982</date>
<journal>Mathematical Biosciences</journal>
<volume>59</volume>
<pages>277--290</pages>
<marker>Hendy, Penny, 1982</marker>
<rawString>M. D. Hendy and D. Penny. 1982. Branch and bound algorithms to determine minimal evolutionary trees. Mathematical Biosciences 59: 277-290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Riloff</author>
<author>E Hovy</author>
</authors>
<date>2008</date>
<booktitle>Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. ACL’08.</booktitle>
<contexts>
<context position="3162" citStr="Kozareva et al., 2008" startWordPosition="451" endWordPosition="455">c relation classification, and relation extraction. The approaches fall into two main categories: pattern-based and clusteringbased. Pattern-based approaches define lexicalsyntactic patterns for relations, and use these patterns to discover instances of relations. Clustering-based approaches hierarchically cluster terms based on similarities of their meanings usually represented by a vector of quantifiable features. Pattern-based approaches are known for their high accuracy in recognizing instances of relations if the patterns are carefully chosen, either manually (Berland and Charniak, 1999; Kozareva et al., 2008) or via automatic bootstrapping (Hearst, 1992; Widdows and Dorow, 2002; Girju et al., 2003). The approaches, however, suffer from sparse coverage of patterns in a given corpus. Recent studies (Etzioni et al., 2005; Kozareva et al., 2008) show that if the size of a corpus, such as the Web, is nearly unlimited, a pattern has a higher chance to explicitly appear in the corpus. However, corpus size is often not that large; hence the problem still exists. Moreover, since patterns usually extract instances in pairs, the approaches suffer from the problem of inconsistent concept chains after connecti</context>
<context position="7591" citStr="Kozareva et al. (2008)" startWordPosition="1129" endWordPosition="1132">or similar terms. Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roark and Charniak (1998) used cooccurrence statistics in local context to discover sibling relations. The KnowItAll system (Etzioni et al., 2005) extended the work in (Hearst, 1992) and bootstrapped patterns on the Web to discover siblings; it also ranked and selected the patterns by statistical measures. Widdows and Dorow (2002) combined symmetric patterns and graph link analysis to discover sibling relations. Davidov and Rappoport (2006) also used symmetric patterns for this task. Recently, Kozareva et al. (2008) combined a double-anchored hyponym pattern with graph structure to extract siblings. The third common relation is part-of. Berland and Charniak (1999) used two meronym patterns to discover part-of relations, and also used statistical measures to rank and select the matching instances. Girju et al. (2003) took a similar approach to Hearst (1992) for part-of relations. Other types of relations that have been studied by pattern-based approaches include questionanswer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al., 2003), general purpo</context>
<context position="9563" citStr="Kozareva et al., 2008" startWordPosition="1424" endWordPosition="1427">atasets; however, uncontrolled bootstrapping 272 soon generates undesired instances once a noisy pattern brought into the cycle. To aid bootstrapping, methods of pattern quality control are widely applied. Statistical measures, such as point-wise mutual information (Etzioni et al., 2005; Pantel and Pennacchiotti, 2006) and conditional probability (Cimiano and Wenderoth, 2007), have been shown to be effective to rank and select patterns and instances. Pattern quality control is also investigated by using WordNet (Girju et al., 2006), graph structures built among terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Clustering-based approaches al</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. ACL’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<pages>98</pages>
<contexts>
<context position="4502" citStr="Lin, 1998" startWordPosition="668" endWordPosition="669">er relations 271 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 271–279, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP which do not explicitly appear in text. They also avoid the problem of inconsistent chains by addressing the structure of a taxonomy globally from the outset. Nevertheless, it is generally believed that clustering-based approaches cannot generate relations as accurate as pattern-based approaches. Moreover, their performance is largely influenced by the types of features used. The common types of features include contextual (Lin, 1998), co-occurrence (Yang and Callan, 2008), and syntactic dependency (Pantel and Lin, 2002; Pantel and Ravichandran, 2004). So far there is no systematic study on which features are the best for automatic taxonomy induction under various conditions. This paper presents a metric-based taxonomy induction framework. It combines the strengths of both pattern-based and clustering-based approaches by incorporating lexico-syntactic patterns as one type of features in a clustering framework. The framework integrates contextual, co-occurrence, syntactic dependency, lexical-syntactic patterns, and other fe</context>
<context position="9774" citStr="Lin, 1998" startWordPosition="1456" endWordPosition="1457">res, such as point-wise mutual information (Etzioni et al., 2005; Pantel and Pennacchiotti, 2006) and conditional probability (Cimiano and Wenderoth, 2007), have been shown to be effective to rank and select patterns and instances. Pattern quality control is also investigated by using WordNet (Girju et al., 2006), graph structures built among terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Clustering-based approaches allow discovery of relations which do not explicitly appear in text. Pantel and Pennacchiotti (2006), however, pointed out that clustering-based approaches generally fail to produce coherent cluster for small corp</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin, 1998. Automatic retrieval and clustering of similar words. COLING’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>S Zhao</author>
<author>L Qin</author>
<author>M Zhou</author>
</authors>
<title>Identifying Synonyms among Distributionally Similar Words.</title>
<date>2003</date>
<tech>IJCAI’03.</tech>
<contexts>
<context position="8176" citStr="Lin et al., 2003" startWordPosition="1218" endWordPosition="1221">Recently, Kozareva et al. (2008) combined a double-anchored hyponym pattern with graph structure to extract siblings. The third common relation is part-of. Berland and Charniak (1999) used two meronym patterns to discover part-of relations, and also used statistical measures to rank and select the matching instances. Girju et al. (2003) took a similar approach to Hearst (1992) for part-of relations. Other types of relations that have been studied by pattern-based approaches include questionanswer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al., 2003), general purpose analogy (Turney et al., 2003), verb relations (including similarity, strength, antonym, enablement and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al., 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney , 2007). The most commonly used technique in pattern-based approaches is bootstrapping (Hearst, 1992; Etzioni et al., 2005; Girju et al., 2003; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). It utilizes a few man-crafted seed patterns to extract instances fr</context>
</contexts>
<marker>Lin, Zhao, Qin, Zhou, 2003</marker>
<rawString>D. Lin, S. Zhao, L. Qin, and M. Zhou. 2003. Identifying Synonyms among Distributionally Similar Words. IJCAI’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
</authors>
<title>Fine-Grained Proper Noun Ontologies for Question Answering.</title>
<date>2002</date>
<booktitle>In Proceedings of SemaNet’ 02: Building and Using Semantic Networks,</booktitle>
<location>Taipei.</location>
<contexts>
<context position="6431" citStr="Mann, 2002" startWordPosition="952" endWordPosition="953">omatic taxonomy induction. Though suffering from the problems of sparse coverage and inconsistent chains, they are still popular due to their simplicity and high accuracy. They have been applied to extract various types of lexical and semantic relations, including is-a, part-of, sibling, synonym, causal, and many others. Pattern-based approaches started from and still pay a great deal of attention to the most common is-a relations. Hearst (1992) pioneered using a hand crafted list of hyponym patterns as seeds and employing bootstrapping to discover is-a relations. Since then, many approaches (Mann, 2002; Etzioni et al., 2005; Snow et al., 2005) have used Hearst-style patterns in their work on is-a relations. For instance, Mann (2002) extracted is-a relations for proper nouns by Hearststyle patterns. Pantel et al. (2004) extended is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance. Another common relation is sibling, which describes the relation of sharing similar meanings and being members of the same class. Terms in sibling relations are also known as class members or similar terms. Inspired by the conjunction and appositive s</context>
</contexts>
<marker>Mann, 2002</marker>
<rawString>G. S. Mann. 2002. Fine-Grained Proper Noun Ontologies for Question Answering. In Proceedings of SemaNet’ 02: Building and Using Semantic Networks, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<pages>02</pages>
<contexts>
<context position="4589" citStr="Pantel and Lin, 2002" startWordPosition="678" endWordPosition="681">h IJCNLP of the AFNLP, pages 271–279, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP which do not explicitly appear in text. They also avoid the problem of inconsistent chains by addressing the structure of a taxonomy globally from the outset. Nevertheless, it is generally believed that clustering-based approaches cannot generate relations as accurate as pattern-based approaches. Moreover, their performance is largely influenced by the types of features used. The common types of features include contextual (Lin, 1998), co-occurrence (Yang and Callan, 2008), and syntactic dependency (Pantel and Lin, 2002; Pantel and Ravichandran, 2004). So far there is no systematic study on which features are the best for automatic taxonomy induction under various conditions. This paper presents a metric-based taxonomy induction framework. It combines the strengths of both pattern-based and clustering-based approaches by incorporating lexico-syntactic patterns as one type of features in a clustering framework. The framework integrates contextual, co-occurrence, syntactic dependency, lexical-syntactic patterns, and other features to learn an ontology metric, a score indicating semantic distance, for each pair</context>
<context position="11055" citStr="Pantel and Lin, 2002" startWordPosition="1640" endWordPosition="1644">d to solve is-a and sibling relations. Many clustering-based approaches face the challenge of appropriately labeling non-leaf clusters. The labeling amplifies the difficulty in creation and evaluation of taxonomies. Agglomerative clustering (Brown et al., 1992; Caraballo, 1999; Rosenfeld and Feldman, 2007; Yang and Callan, 2008) iteratively merges the most similar clusters into bigger clusters, which need to be labeled. Divisive clustering, such as CBC (Clustering By Committee) which constructs cluster centroids by averaging the feature vectors of a subset of carefully chosen cluster members (Pantel and Lin, 2002; Pantel and Ravichandran, 2004), also need to label the parents of split clusters. In this paper, we take an incremental clustering approach, in which terms and relations are added into a taxonomy one at a time, and their parents are from the existing taxonomy. The advantage of the incremental approach is that it eliminates the trouble of inventing cluster labels and concentrates on placing terms in the correct positions in a taxonomy hierarchy. The work by Snow et al. (2006) is the most similar to ours because they also took an incremental approach to construct taxonomies. In their work, a t</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>P. Pantel and D Lin. 2002. Discovering word senses from text. SIGKDD’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<pages>04</pages>
<contexts>
<context position="4621" citStr="Pantel and Ravichandran, 2004" startWordPosition="682" endWordPosition="685"> pages 271–279, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP which do not explicitly appear in text. They also avoid the problem of inconsistent chains by addressing the structure of a taxonomy globally from the outset. Nevertheless, it is generally believed that clustering-based approaches cannot generate relations as accurate as pattern-based approaches. Moreover, their performance is largely influenced by the types of features used. The common types of features include contextual (Lin, 1998), co-occurrence (Yang and Callan, 2008), and syntactic dependency (Pantel and Lin, 2002; Pantel and Ravichandran, 2004). So far there is no systematic study on which features are the best for automatic taxonomy induction under various conditions. This paper presents a metric-based taxonomy induction framework. It combines the strengths of both pattern-based and clustering-based approaches by incorporating lexico-syntactic patterns as one type of features in a clustering framework. The framework integrates contextual, co-occurrence, syntactic dependency, lexical-syntactic patterns, and other features to learn an ontology metric, a score indicating semantic distance, for each pair of terms in a taxonomy; it then</context>
<context position="9939" citStr="Pantel and Ravichandran, 2004" startWordPosition="1478" endWordPosition="1481">th, 2007), have been shown to be effective to rank and select patterns and instances. Pattern quality control is also investigated by using WordNet (Girju et al., 2006), graph structures built among terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Clustering-based approaches allow discovery of relations which do not explicitly appear in text. Pantel and Pennacchiotti (2006), however, pointed out that clustering-based approaches generally fail to produce coherent cluster for small corpora. In addition, clustering-based approaches had only applied to solve is-a and sibling relations. Many clustering-based approaches face the challenge of appropriat</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>P. Pantel and D. Ravichandran. 2004. Automatically labeling semantic classes. HLT/NAACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Towards terascale knowledge acquisition.</title>
<date>2004</date>
<pages>04</pages>
<contexts>
<context position="6652" citStr="Pantel et al. (2004)" startWordPosition="987" endWordPosition="990">us types of lexical and semantic relations, including is-a, part-of, sibling, synonym, causal, and many others. Pattern-based approaches started from and still pay a great deal of attention to the most common is-a relations. Hearst (1992) pioneered using a hand crafted list of hyponym patterns as seeds and employing bootstrapping to discover is-a relations. Since then, many approaches (Mann, 2002; Etzioni et al., 2005; Snow et al., 2005) have used Hearst-style patterns in their work on is-a relations. For instance, Mann (2002) extracted is-a relations for proper nouns by Hearststyle patterns. Pantel et al. (2004) extended is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance. Another common relation is sibling, which describes the relation of sharing similar meanings and being members of the same class. Terms in sibling relations are also known as class members or similar terms. Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roark and Charniak (1998) used cooccurrence statistics in local context to discover sibling relations. The KnowItAll system (Etzioni et al., 2005) extended the work in (Hearst, 1992)</context>
</contexts>
<marker>Pantel, Ravichandran, Hovy, 2004</marker>
<rawString>P. Pantel, D. Ravichandran, and E. Hovy. 2004. Towards terascale knowledge acquisition. COLING’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>M Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations.</title>
<date>2006</date>
<contexts>
<context position="8707" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="1295" endWordPosition="1299"> as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al., 2003), general purpose analogy (Turney et al., 2003), verb relations (including similarity, strength, antonym, enablement and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al., 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney , 2007). The most commonly used technique in pattern-based approaches is bootstrapping (Hearst, 1992; Etzioni et al., 2005; Girju et al., 2003; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). It utilizes a few man-crafted seed patterns to extract instances from corpora, then extracts new patterns using these instances, and continues the cycle to find new instances and new patterns. It is effective and scalable to large datasets; however, uncontrolled bootstrapping 272 soon generates undesired instances once a noisy pattern brought into the cycle. To aid bootstrapping, methods of pattern quality control are widely applied. Statistical measures, such as point-wise mutual information (Etzioni et al., 2005; Pantel and Pennacchiotti, 2006) and conditional probability (Cimiano and Wend</context>
<context position="10261" citStr="Pantel and Pennacchiotti (2006)" startWordPosition="1525" endWordPosition="1529">sed approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Clustering-based approaches allow discovery of relations which do not explicitly appear in text. Pantel and Pennacchiotti (2006), however, pointed out that clustering-based approaches generally fail to produce coherent cluster for small corpora. In addition, clustering-based approaches had only applied to solve is-a and sibling relations. Many clustering-based approaches face the challenge of appropriately labeling non-leaf clusters. The labeling amplifies the difficulty in creation and evaluation of taxonomies. Agglomerative clustering (Brown et al., 1992; Caraballo, 1999; Rosenfeld and Feldman, 2007; Yang and Callan, 2008) iteratively merges the most similar clusters into bigger clusters, which need to be labeled. Di</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations. ACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<pages>93</pages>
<contexts>
<context position="9886" citStr="Pereira et al., 1993" startWordPosition="1472" endWordPosition="1475"> conditional probability (Cimiano and Wenderoth, 2007), have been shown to be effective to rank and select patterns and instances. Pattern quality control is also investigated by using WordNet (Girju et al., 2006), graph structures built among terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Clustering-based approaches allow discovery of relations which do not explicitly appear in text. Pantel and Pennacchiotti (2006), however, pointed out that clustering-based approaches generally fail to produce coherent cluster for small corpora. In addition, clustering-based approaches had only applied to solve is-a and sibling relations. Many cluster</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clustering of English words. ACL’93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<pages>02</pages>
<contexts>
<context position="8134" citStr="Ravichandran and Hovy, 2002" startWordPosition="1211" endWordPosition="1214">t (2006) also used symmetric patterns for this task. Recently, Kozareva et al. (2008) combined a double-anchored hyponym pattern with graph structure to extract siblings. The third common relation is part-of. Berland and Charniak (1999) used two meronym patterns to discover part-of relations, and also used statistical measures to rank and select the matching instances. Girju et al. (2003) took a similar approach to Hearst (1992) for part-of relations. Other types of relations that have been studied by pattern-based approaches include questionanswer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al., 2003), general purpose analogy (Turney et al., 2003), verb relations (including similarity, strength, antonym, enablement and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al., 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney , 2007). The most commonly used technique in pattern-based approaches is bootstrapping (Hearst, 1992; Etzioni et al., 2005; Girju et al., 2003; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). It utilizes a few man-cra</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>D. Ravichandran and E. Hovy. 2002. Learning surface text patterns for a question answering system. ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Shepherd</author>
</authors>
<title>A corpus-based approach for building semantic lexicons.</title>
<date>1997</date>
<pages>97</pages>
<contexts>
<context position="7068" citStr="Riloff and Shepherd (1997)" startWordPosition="1050" endWordPosition="1053">t al., 2005; Snow et al., 2005) have used Hearst-style patterns in their work on is-a relations. For instance, Mann (2002) extracted is-a relations for proper nouns by Hearststyle patterns. Pantel et al. (2004) extended is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance. Another common relation is sibling, which describes the relation of sharing similar meanings and being members of the same class. Terms in sibling relations are also known as class members or similar terms. Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roark and Charniak (1998) used cooccurrence statistics in local context to discover sibling relations. The KnowItAll system (Etzioni et al., 2005) extended the work in (Hearst, 1992) and bootstrapped patterns on the Web to discover siblings; it also ranked and selected the patterns by statistical measures. Widdows and Dorow (2002) combined symmetric patterns and graph link analysis to discover sibling relations. Davidov and Rappoport (2006) also used symmetric patterns for this task. Recently, Kozareva et al. (2008) combined a double-anchored hyponym pattern with graph structure to extract s</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>E. Riloff and J. Shepherd. 1997. A corpus-based approach for building semantic lexicons. EMNLP’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Noun-phrase cooccurrence statistics for semi-automatic semantic lexicon construction.</title>
<date>1998</date>
<pages>98</pages>
<contexts>
<context position="7095" citStr="Roark and Charniak (1998)" startWordPosition="1054" endWordPosition="1057">05) have used Hearst-style patterns in their work on is-a relations. For instance, Mann (2002) extracted is-a relations for proper nouns by Hearststyle patterns. Pantel et al. (2004) extended is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance. Another common relation is sibling, which describes the relation of sharing similar meanings and being members of the same class. Terms in sibling relations are also known as class members or similar terms. Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roark and Charniak (1998) used cooccurrence statistics in local context to discover sibling relations. The KnowItAll system (Etzioni et al., 2005) extended the work in (Hearst, 1992) and bootstrapped patterns on the Web to discover siblings; it also ranked and selected the patterns by statistical measures. Widdows and Dorow (2002) combined symmetric patterns and graph link analysis to discover sibling relations. Davidov and Rappoport (2006) also used symmetric patterns for this task. Recently, Kozareva et al. (2008) combined a double-anchored hyponym pattern with graph structure to extract siblings. The third common r</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>B. Roark and E. Charniak. 1998. Noun-phrase cooccurrence statistics for semi-automatic semantic lexicon construction. ACL/COLING’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2005</date>
<pages>05</pages>
<contexts>
<context position="6473" citStr="Snow et al., 2005" startWordPosition="958" endWordPosition="961">suffering from the problems of sparse coverage and inconsistent chains, they are still popular due to their simplicity and high accuracy. They have been applied to extract various types of lexical and semantic relations, including is-a, part-of, sibling, synonym, causal, and many others. Pattern-based approaches started from and still pay a great deal of attention to the most common is-a relations. Hearst (1992) pioneered using a hand crafted list of hyponym patterns as seeds and employing bootstrapping to discover is-a relations. Since then, many approaches (Mann, 2002; Etzioni et al., 2005; Snow et al., 2005) have used Hearst-style patterns in their work on is-a relations. For instance, Mann (2002) extracted is-a relations for proper nouns by Hearststyle patterns. Pantel et al. (2004) extended is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance. Another common relation is sibling, which describes the relation of sharing similar meanings and being members of the same class. Terms in sibling relations are also known as class members or similar terms. Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roa</context>
<context position="9959" citStr="Snow et al., 2005" startWordPosition="1482" endWordPosition="1485">e effective to rank and select patterns and instances. Pattern quality control is also investigated by using WordNet (Girju et al., 2006), graph structures built among terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Clustering-based approaches allow discovery of relations which do not explicitly appear in text. Pantel and Pennacchiotti (2006), however, pointed out that clustering-based approaches generally fail to produce coherent cluster for small corpora. In addition, clustering-based approaches had only applied to solve is-a and sibling relations. Many clustering-based approaches face the challenge of appropriately labeling non-lea</context>
<context position="16184" citStr="Snow et al., 2005" startWordPosition="2520" endWordPosition="2523">6) Minipar Syntactic Distance to measure the average length of the shortest syntactic paths (in the first syntactic parse tree returned by between two terms in sentences containing them, (7) Modifier Overlap, (8) Object Overlap, (9) Subject Overlap, and (10) Verb Overlap to measure the number of overlaps between modifiers, objects, subjects, and verbs, respectively, for the two terms in sentences containing them. We use Assert2 to label the semantic roles. The fourth set of features is lexical-syntactic patterns. We have (11) Hypernym Patterns based on patterns proposed by (Hearst, 1992) and (Snow et al., 2005), (12) Sibling Patterns which are basically conjunctions, and (13) Part-of Patterns based on patterns proposed by (Girju et al., 2003) and (Cimiano and Wenderoth, 2007). Table 1 lists all patterns. Each feature function returns of scores for two input terms, one score per pattern. A score is 1 if two terms match a pattern in text, 0 otherwise. The last set of features is miscellaneous. We have (14) Word Length Difference to measure the length difference between two terms, and (15) Definition Overlap to measure the number of word overlaps between the term definitions obtained by querying Google</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. NIPS’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<date>2006</date>
<booktitle>Semantic Taxonomy Induction from Heterogeneous Evidence. ACL’06.</booktitle>
<contexts>
<context position="11536" citStr="Snow et al. (2006)" startWordPosition="1724" endWordPosition="1727">which constructs cluster centroids by averaging the feature vectors of a subset of carefully chosen cluster members (Pantel and Lin, 2002; Pantel and Ravichandran, 2004), also need to label the parents of split clusters. In this paper, we take an incremental clustering approach, in which terms and relations are added into a taxonomy one at a time, and their parents are from the existing taxonomy. The advantage of the incremental approach is that it eliminates the trouble of inventing cluster labels and concentrates on placing terms in the correct positions in a taxonomy hierarchy. The work by Snow et al. (2006) is the most similar to ours because they also took an incremental approach to construct taxonomies. In their work, a taxonomy grows based on maximization of conditional probability of relations given evidence; while in our work based on optimization of taxonomy structures and modeling of term abstractness. Moreover, our approach employs heterogeneous features from a wide range; while their approach only used syntactic dependency. We compare system performance between (Snow et al., 2006) and our framework in Section 5. 3 The Features The features used in this work are indicators of semantic re</context>
<context position="29198" citStr="Snow et al. (2006)" startWordPosition="4824" endWordPosition="4827">ordNet. time, and report the averaged precision, recall and F1-measure across all 50 runs. We also group the fifteen features in Section 3 into six sets: contextual, co-concurrence, patterns, syntactic dependency, word length difference and definition. Each set is turned on one by one for experiments in Section 5.4 and 5.5. 5.3 Performance of Taxonomy Induction In this section, we compare the following automatic taxonomy induction systems: HE, the system by Hearst (1992) with 6 hypernym patterns; GI, the system by Girju et al. (2003) with 3 meronym patterns; PR, the probabilistic framework by Snow et al. (2006); and ME, the metric-based framework proposed in this paper. To have a fair comparison, for PR, we estimate the conditional probability of a relation given the evidence P(Ri;|Ei;), as in (Snow et al. 2006), by using the same set of features as in ME. Table 3 shows precision, recall, and F1- measure of each system for WordNet hypernyms (is-a), WordNet meronyms (part-of) and ODP hypernyms (is-a). Bold font indicates the best performance in a column. Note that HE is not applicable to part-of, so is GI to is-a. Table 3 shows that systems using heterogeneous features (PR and ME) achieve higher F1- </context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic Taxonomy Induction from Heterogeneous Evidence. ACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rosenfeld</author>
<author>R Feldman</author>
</authors>
<title>Clustering for unsupervised relation identification.</title>
<date>2007</date>
<pages>07</pages>
<contexts>
<context position="10741" citStr="Rosenfeld and Feldman, 2007" startWordPosition="1592" endWordPosition="1595">and Volker, 2005). Clustering-based approaches allow discovery of relations which do not explicitly appear in text. Pantel and Pennacchiotti (2006), however, pointed out that clustering-based approaches generally fail to produce coherent cluster for small corpora. In addition, clustering-based approaches had only applied to solve is-a and sibling relations. Many clustering-based approaches face the challenge of appropriately labeling non-leaf clusters. The labeling amplifies the difficulty in creation and evaluation of taxonomies. Agglomerative clustering (Brown et al., 1992; Caraballo, 1999; Rosenfeld and Feldman, 2007; Yang and Callan, 2008) iteratively merges the most similar clusters into bigger clusters, which need to be labeled. Divisive clustering, such as CBC (Clustering By Committee) which constructs cluster centroids by averaging the feature vectors of a subset of carefully chosen cluster members (Pantel and Lin, 2002; Pantel and Ravichandran, 2004), also need to label the parents of split clusters. In this paper, we take an incremental clustering approach, in which terms and relations are added into a taxonomy one at a time, and their parents are from the existing taxonomy. The advantage of the in</context>
</contexts>
<marker>Rosenfeld, Feldman, 2007</marker>
<rawString>B. Rosenfeld and R. Feldman. 2007. Clustering for unsupervised relation identification. CIKM’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
<author>M Littman</author>
<author>J Bigham</author>
<author>V Shnayder</author>
</authors>
<title>Combining independent modules to solve multiplechoice synonym and analogy problems.</title>
<date>2003</date>
<pages>03</pages>
<contexts>
<context position="8223" citStr="Turney et al., 2003" startWordPosition="1225" endWordPosition="1228">double-anchored hyponym pattern with graph structure to extract siblings. The third common relation is part-of. Berland and Charniak (1999) used two meronym patterns to discover part-of relations, and also used statistical measures to rank and select the matching instances. Girju et al. (2003) took a similar approach to Hearst (1992) for part-of relations. Other types of relations that have been studied by pattern-based approaches include questionanswer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al., 2003), general purpose analogy (Turney et al., 2003), verb relations (including similarity, strength, antonym, enablement and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al., 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney , 2007). The most commonly used technique in pattern-based approaches is bootstrapping (Hearst, 1992; Etzioni et al., 2005; Girju et al., 2003; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). It utilizes a few man-crafted seed patterns to extract instances from corpora, then extracts new patterns using th</context>
</contexts>
<marker>Turney, Littman, Bigham, Shnayder, 2003</marker>
<rawString>P. Turney, M. Littman, J. Bigham, and V. Shnayder. 2003. Combining independent modules to solve multiplechoice synonym and analogy problems. RANLP’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Harabagiu</author>
<author>S J Maiorano</author>
<author>M A Pasca</author>
</authors>
<title>Open-Domain Textual Question Answering Techniques.</title>
<date>2003</date>
<journal>Natural Language Engineering</journal>
<volume>9</volume>
<issue>3</issue>
<pages>1--38</pages>
<contexts>
<context position="1519" citStr="Harabagiu et al., 2003" startWordPosition="207" endWordPosition="210">experiments not only show that our system achieves higher F1-measure than other state-of-the-art systems, but also reveal the interaction between features and various types of relations, as well as the interaction between features and term abstractness. 1 Introduction Automatic taxonomy induction is an important task in the fields of Natural Language Processing, Knowledge Management, and Semantic Web. It has been receiving increasing attention because semantic taxonomies, such as WordNet (Fellbaum, 1998), play an important role in solving knowledge-rich problems, including question answering (Harabagiu et al., 2003) and textual entailment (Geffet and Dagan, 2005). Nevertheless, most existing taxonomies are manually created at great cost. These taxonomies are rarely complete; it is difficult to include new terms in them from emerging or rapidly changing domains. Moreover, manual taxonomy construction is time-consuming, which may make it unfeasible for specialized domains and personalized tasks. Automatic taxonomy induction is a solution to augment existing resources and to produce new taxonomies for such domains and tasks. Automatic taxonomy induction can be decomposed into two subtasks: term extraction a</context>
</contexts>
<marker>Harabagiu, Maiorano, Pasca, 2003</marker>
<rawString>S. M. Harabagiu, S. J. Maiorano and M. A. Pasca. 2003. Open-Domain Textual Question Answering Techniques. Natural Language Engineering 9 (3): 1-38, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>H Tanev</author>
<author>I Dagan</author>
<author>B Coppola</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<pages>04</pages>
<contexts>
<context position="8371" citStr="Szpektor et al., 2004" startWordPosition="1245" endWordPosition="1248">wo meronym patterns to discover part-of relations, and also used statistical measures to rank and select the matching instances. Girju et al. (2003) took a similar approach to Hearst (1992) for part-of relations. Other types of relations that have been studied by pattern-based approaches include questionanswer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al., 2003), general purpose analogy (Turney et al., 2003), verb relations (including similarity, strength, antonym, enablement and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al., 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney , 2007). The most commonly used technique in pattern-based approaches is bootstrapping (Hearst, 1992; Etzioni et al., 2005; Girju et al., 2003; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). It utilizes a few man-crafted seed patterns to extract instances from corpora, then extracts new patterns using these instances, and continues the cycle to find new instances and new patterns. It is effective and scalable to large datasets; however, uncontrolled</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. Scaling web-based acquisition of entailment relations. EMNLP’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
<author>B Dorow</author>
</authors>
<title>A graph model for unsupervised Lexical acquisition.</title>
<date>2002</date>
<journal>COLING</journal>
<volume>02</volume>
<contexts>
<context position="3232" citStr="Widdows and Dorow, 2002" startWordPosition="462" endWordPosition="465">ll into two main categories: pattern-based and clusteringbased. Pattern-based approaches define lexicalsyntactic patterns for relations, and use these patterns to discover instances of relations. Clustering-based approaches hierarchically cluster terms based on similarities of their meanings usually represented by a vector of quantifiable features. Pattern-based approaches are known for their high accuracy in recognizing instances of relations if the patterns are carefully chosen, either manually (Berland and Charniak, 1999; Kozareva et al., 2008) or via automatic bootstrapping (Hearst, 1992; Widdows and Dorow, 2002; Girju et al., 2003). The approaches, however, suffer from sparse coverage of patterns in a given corpus. Recent studies (Etzioni et al., 2005; Kozareva et al., 2008) show that if the size of a corpus, such as the Web, is nearly unlimited, a pattern has a higher chance to explicitly appear in the corpus. However, corpus size is often not that large; hence the problem still exists. Moreover, since patterns usually extract instances in pairs, the approaches suffer from the problem of inconsistent concept chains after connecting pairs of instances to form taxonomy hierarchies. Clustering-based a</context>
<context position="7402" citStr="Widdows and Dorow (2002)" startWordPosition="1101" endWordPosition="1104">. Another common relation is sibling, which describes the relation of sharing similar meanings and being members of the same class. Terms in sibling relations are also known as class members or similar terms. Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roark and Charniak (1998) used cooccurrence statistics in local context to discover sibling relations. The KnowItAll system (Etzioni et al., 2005) extended the work in (Hearst, 1992) and bootstrapped patterns on the Web to discover siblings; it also ranked and selected the patterns by statistical measures. Widdows and Dorow (2002) combined symmetric patterns and graph link analysis to discover sibling relations. Davidov and Rappoport (2006) also used symmetric patterns for this task. Recently, Kozareva et al. (2008) combined a double-anchored hyponym pattern with graph structure to extract siblings. The third common relation is part-of. Berland and Charniak (1999) used two meronym patterns to discover part-of relations, and also used statistical measures to rank and select the matching instances. Girju et al. (2003) took a similar approach to Hearst (1992) for part-of relations. Other types of relations that have been </context>
<context position="9539" citStr="Widdows and Dorow, 2002" startWordPosition="1420" endWordPosition="1423">e and scalable to large datasets; however, uncontrolled bootstrapping 272 soon generates undesired instances once a noisy pattern brought into the cycle. To aid bootstrapping, methods of pattern quality control are widely applied. Statistical measures, such as point-wise mutual information (Etzioni et al., 2005; Pantel and Pennacchiotti, 2006) and conditional probability (Cimiano and Wenderoth, 2007), have been shown to be effective to rank and select patterns and instances. Pattern quality control is also investigated by using WordNet (Girju et al., 2006), graph structures built among terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Cluste</context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>D. Widdows and B. Dorow. 2002. A graph model for unsupervised Lexical acquisition. COLING ’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yang</author>
<author>J Callan</author>
</authors>
<title>Learning the Distance Metric in a Personal Ontology.</title>
<date>2008</date>
<booktitle>Workshop on Ontologies and Information Systems for the Semantic Web of CIKM’08.</booktitle>
<contexts>
<context position="4541" citStr="Yang and Callan, 2008" startWordPosition="671" endWordPosition="674">s of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 271–279, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP which do not explicitly appear in text. They also avoid the problem of inconsistent chains by addressing the structure of a taxonomy globally from the outset. Nevertheless, it is generally believed that clustering-based approaches cannot generate relations as accurate as pattern-based approaches. Moreover, their performance is largely influenced by the types of features used. The common types of features include contextual (Lin, 1998), co-occurrence (Yang and Callan, 2008), and syntactic dependency (Pantel and Lin, 2002; Pantel and Ravichandran, 2004). So far there is no systematic study on which features are the best for automatic taxonomy induction under various conditions. This paper presents a metric-based taxonomy induction framework. It combines the strengths of both pattern-based and clustering-based approaches by incorporating lexico-syntactic patterns as one type of features in a clustering framework. The framework integrates contextual, co-occurrence, syntactic dependency, lexical-syntactic patterns, and other features to learn an ontology metric, a s</context>
<context position="9998" citStr="Yang and Callan, 2008" startWordPosition="1487" endWordPosition="1490">erns and instances. Pattern quality control is also investigated by using WordNet (Girju et al., 2006), graph structures built among terms (Widdows and Dorow, 2002; Kozareva et al., 2008), and pattern clusters (Davidov and Rappoport, 2008). Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992; Lin, 1998). Besides contextual features, the vectors can also be represented by verb-noun relations (Pereira et al., 1993), syntactic dependency (Pantel and Ravichandran, 2004; Snow et al., 2005), co-occurrence (Yang and Callan, 2008), conjunction and appositive features (Caraballo, 1999). More work is described in (Buitelaar et al., 2005; Cimiano and Volker, 2005). Clustering-based approaches allow discovery of relations which do not explicitly appear in text. Pantel and Pennacchiotti (2006), however, pointed out that clustering-based approaches generally fail to produce coherent cluster for small corpora. In addition, clustering-based approaches had only applied to solve is-a and sibling relations. Many clustering-based approaches face the challenge of appropriately labeling non-leaf clusters. The labeling amplifies the </context>
</contexts>
<marker>Yang, Callan, 2008</marker>
<rawString>H. Yang and J. Callan. 2008. Learning the Distance Metric in a Personal Ontology. Workshop on Ontologies and Information Systems for the Semantic Web of CIKM’08.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>