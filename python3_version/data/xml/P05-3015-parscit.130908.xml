<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004276">
<title confidence="0.971871">
Syntax-based Semi-Supervised Named Entity Tagging
</title>
<author confidence="0.874065">
Behrang Mohit Rebecca Hwa
</author>
<affiliation confidence="0.898811">
Intelligent Systems Program Computer Science Department
University of Pittsburgh University of Pittsburgh
</affiliation>
<address confidence="0.813728">
Pittsburgh, PA 152 60 USA Pittsburgh, PA 152 60, USA
</address>
<email confidence="0.99049">
behrang@cs.pitt.edu hwa@cs.pitt.edu
</email>
<sectionHeader confidence="0.993438" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936736842105">
We report an empirical study on the role
of syntactic features in building a semi-
supervised named entity (NE) tagger.
*ur study addresses two questions: What
types of syntactic features are suitable for
extracting potential NEs to train a classi-
fier in a semi-supervised setting? How
good is the resulting NE classifier on test-
ing instances dissimilar from its training
data? *ur study shows that constituency
and dependency parsing constraints are
both suitable features to extract NEs and
train the classifier. Moreover, the classi-
fier showed significant accuracy im-
provement when constituency features are
combined with new dependency feature.
Furthermore, the degradation in accuracy
on unfamiliar test cases is low, suggesting
that the trained classifier generalizes well.
</bodyText>
<sectionHeader confidence="0.961641" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999844916666667">
Named entity (NE) tagging is the task of recogniz-
ing and classifying phrases into one of many se-
mantic classes such as persons, organizations and
locations. Many successful NE tagging systems
rely on a supervised learning framework where
systems use large annotated training resources
(Bikel et. al. 1999). These resources may not al-
ways be available for non-English domains. This
paper examines the practicality of developing a
syntax-based semi-supervised NE tagger. In our
study we compared the effects of two types of syn-
tactic rules (constituency and dependency) in ex-
</bodyText>
<page confidence="0.97672">
57
</page>
<bodyText confidence="0.999925157894737">
tracting and classifying potential named entities.
We train a Naive Bayes classification model on a
combination of labeled and unlabeled examples
with the Expectation Maximization (EM) algo-
rithm. We find that a significant improvement in
classification accuracy can be achieved when we
combine both dependency and constituency extrac-
tion methods. In our experiments, we evaluate the
generalization (coverage) of this bootstrapping ap-
proach under three testing schemas. Each of these
schemas represented a certain level of test data
coverage (recall). Although the system performs
best on (unseen) test data that is extracted by the
syntactic rules (i.e., similar syntactic structures as
the training examples), the performance degrada-
tion is not high when the system is tested on more
general test cases. *ur experimental results suggest
that a semi-supervised NE tagger can be success-
fully developed using syntax-rich features.
</bodyText>
<sectionHeader confidence="0.593952" genericHeader="method">
2 Previous Works and Our Approach
</sectionHeader>
<bodyText confidence="0.9995821875">
Supervised NE Tagging has been studied exten-
sively over the past decade (Bikel et al. 1999,
Baluja et. al. 1999, Tjong Kim Sang and De
Meulder 2003). Recently, there were increasing
interests in semi-supervised learning approaches.
Most relevant to our study, Collins and Singer
(1999) showed that a NE Classifier can be devel-
oped by bootstrapping from a small amount of la-
beled examples. To extract potentially useful
training examples, they first parsed the sentences
and looked for expressions that satisfy two con-
stituency patterns (appositives and prepositional
phrases). A small subset of these expressions was
then manually labeled with their correct NE tags.
The training examples were a combination of the
labeled and unlabeled data. In their studies,
</bodyText>
<subsubsectionHeader confidence="0.358942">
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
</subsubsectionHeader>
<bodyText confidence="0.979528769230769">
pages 57–60, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
Collins and Singer compared several learning
models using this style of semi-supervised training.
Their results were encouraging, and their studies
raised additional questions. First, are there other
appropriate syntactic extraction patterns in addition
to appositives and prepositional phrases? Second,
because the test data were extracted in the same
manner as the training data in their experiments,
the characteristics of the test cases were biased. In
this paper we examine the question of how well a
semi-supervised system can classify arbitrary
named entities. In our empirical study, in addition
to the constituency features proposed by Collins
and Singer, we introduce a new set of dependency
parse features to recognize and classify NEs. We
evaluated the effects of these two sets of syntactic
features on the accuracy of the classification both
separately and in a combined form (union of the
two sets).
Figure 1 represents a general overview of our sys-
tem&apos;s architecture which includes the following
two levels: NE Recognizer and NE Classifier.
Section 3 and 4 describes these two levels in de-
tails and section 5 covers the results of the evalua-
tion of our system.
</bodyText>
<figureCaption confidence="0.936764">
Figure 1: System&apos;s architecture
</figureCaption>
<sectionHeader confidence="0.810194" genericHeader="method">
3 Named Entity Recognition
</sectionHeader>
<bodyText confidence="0.999969909090909">
In this level, the system used a group of syntax-
based rules to recognize and extract potential
named entities from constituency and dependency
parse trees. The rules are used to produce our
training data; therefore they needed to have a nar-
row and precise coverage of each type of named
entities to minimize the level of training noise.
The processing starts from construction of con-
stituency and dependency parse trees from the in-
put text. Potential NEs are detected and extracted
based on these syntactic rules.
</bodyText>
<sectionHeader confidence="0.562408" genericHeader="method">
3. 1 Constituency Parse Features
</sectionHeader>
<bodyText confidence="0.999636">
Replicating the study performed by Collins-Singer
(1999), we used two constituency parse rules to
extract a set of proper nouns (along with their as-
sociated contextual information). These two con-
stituency rules extracted proper nouns within a
noun phrase that contained an appositive phrase
and a proper noun within a prepositional phrase.
</bodyText>
<subsectionHeader confidence="0.997778">
3.2 Dependency Parse Features
</subsectionHeader>
<bodyText confidence="0.9999802">
We observed that a proper noun acting as the sub-
ject or the object of a sentence has a high probabil-
ity of being a particular type of named entity.
Thus, we expanded our syntactic analysis of the
data into dependency parse of the text and ex-
tracted a set of proper nouns that act as the subjects
or objects of the main verb. For each of the sub-
jects and objects, we considered the maximum
span noun phrase that included the modifiers of the
subjects and objects in the dependency parse tree.
</bodyText>
<sectionHeader confidence="0.992991" genericHeader="method">
4 Named Entity Classification
</sectionHeader>
<bodyText confidence="0.9999011">
In this level, the system assigns one of the 4 class
labels (&lt;PER&gt;, &lt;ORG&gt;, &lt;LOC&gt;, &lt;NONE&gt;) to a
given test NE. The NONE class is used for the
expressions mistakenly extracted by syntactic fea-
tures that were not a NE. We will discuss the form
of the test NE in more details in section 5. The
underlying model we consider is a Naive Bayes
classifier; we train it with the Expectation-
Maximization algorithm, an iterative parameter
estimation procedure.
</bodyText>
<sectionHeader confidence="0.982328" genericHeader="method">
4. 1 Features
</sectionHeader>
<bodyText confidence="0.999424">
We used the following syntactic and spelling fea-
tures for the classification:
</bodyText>
<subsubsectionHeader confidence="0.774513">
Full NE Phrase.
</subsubsectionHeader>
<bodyText confidence="0.8651595">
Individual word: This binary feature indicates the
presence of a certain word in the NE.
</bodyText>
<page confidence="0.992008">
58
</page>
<bodyText confidence="0.9728137">
Punctuation pattern: The feature helps to distin-
guish those NEs that hold certain patterns of punc-
tuations like (...) for U.S.A. or (&amp;.) for A&amp;M.
All Capitalization: This binary feature is mainly
useful for some of the NEs that have all capital
letters. such as AP, AFP, CNN, etc.
Constituency Parse Rule: The feature indicates
which of the two constituency rule is used for ex-
tract the NE.
Dependency Parse Rule: The feature indicates if
the NE is the subject or object of the sentence.
Except for the last two features, all features are
spelling features which are extracted from the ac-
tual NE phrase. The constituency and dependency
features are extracted from the NE recognition
phase (section 3). Depending on the type of testing
and training schema, the NEs might have 0 value
for the dependency or constituency features which
indicate the absence of the feature in the recogni-
tion step.
</bodyText>
<subsectionHeader confidence="0.955922">
4.2 Naive Bayes Classifier
</subsectionHeader>
<bodyText confidence="0.999996416666667">
We used a Naive Bayes classifier where each NE
is represented by a set of syntactic and word-level
features (with various distributions) as described
above. The individual words within the noun
phrase are binary features. These, along with other
features with multinomial distributions, fit well
into Naive Bayes assumption where each feature is
dealt independently (given the class value). In or-
der to balance the effects of the large binary fea-
tures on the final class probabilities, we used some
numerical methods techniques to transform some
of the probabilities to the log-space.
</bodyText>
<subsectionHeader confidence="0.996008">
4.3 Semi-supervised learning
</subsectionHeader>
<bodyText confidence="0.999949066666667">
Similar to the work of Nigam et al. (1999) on
document classification, we used Expectation
Maximization (EM) algorithm along with our Na-
ive Bayes classifier to form a semi supervised
learning framework. In this framework, the small
labeled dataset is used to do the initial assignments
of the parameters for the Naive Bayes classifier.
After this initialization step, in each iteration the
Naive Bayes classifier classifies all of the unla-
beled examples and updates its parameters based
on the class probability of the unlabeled and la-
beled NE instances. This iterative procedure con-
tinues until the parameters reach a stable point.
Subsequently the updated Naive Bayes classifies
the test instances for evaluation.
</bodyText>
<sectionHeader confidence="0.986615" genericHeader="method">
5 Empirical Study
</sectionHeader>
<bodyText confidence="0.999481666666667">
*ur study consists of a 9-way comparison that in-
cludes the usage of three types of training features
and three types of testing schema.
</bodyText>
<sectionHeader confidence="0.973257" genericHeader="method">
5. 1 Data
</sectionHeader>
<bodyText confidence="0.9995496">
We used the data from the Automatic Content Ex-
traction (ACE)&apos;s entity detection track as our la-
beled (gold standard) data.1
For every NE that the syntactic rules extract from
the input sentence, we had to find a matching NE
from the gold standard data and label the extracted
NE with the correct NE class label. If the ex-
tracted NE did not match any of the gold standard
NEs (for the sentence), we labeled it with the
&lt;NONE&gt; class label.
We also used the WSJ portion of the Penn Tree
Bank as our unlabeled dataset and ran constituency
and dependency analyses2 to extract a set of unla-
beled named entities for the semi-supervised clas-
sification.
</bodyText>
<subsectionHeader confidence="0.996313">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.994378857142857">
In order to evaluate the effects of each group of
syntactic features, we experimented with three dif-
ferent training strategies (using constituency rules,
dependency rules or combinations of both). We
conducted the comparison study with three types
of test data that represent three levels of coverage
(recall) for the system:
</bodyText>
<listItem confidence="0.838867285714286">
1. Gold Standard NEs: This test set contains in-
stances taken directly from the ACE data, and are
therefore independent of the syntactic rules.
2. Any single or series of proper nouns in the text:
This is a heuristic for locating potential NEs so as
to have the broadest coverage.
3. NEs extracted from text by the syntactic rules.
</listItem>
<bodyText confidence="0.769722333333333">
This evaluation approach is similar to that of Col-
lins and Singer. The main difference is that we
have to match the extracted expressions to a pre-
</bodyText>
<footnote confidence="0.920862">
1 We only used the NE portion of the data and removed the
information for other tracking and extraction tasks.
2 We used the Collins parser (1997) to generate the constitu-
ency parse and a dependency converter (Hwa and Lopez,
2004) to obtain the dependency parse of English sentences.
</footnote>
<page confidence="0.999013">
59
</page>
<bodyText confidence="0.9988478125">
labeled gold standard from ACE rather than per-
forming manual annotations ourselves.
All tests have been performed under a 5-fold cross
validation training-testing setup. Table 1 presents
the accuracy of the NE classification and the size
of labeled data in the different training-testing con-
figurations. The second line of each cell shows the
size of labeled training data and the third line
shows the size of testing data. Each column pre-
sents the result for one type of the syntactic fea-
tures that were used to extract NEs. Each row of
the table presents one of the three testing schema.
We tested the statistical significance of each of the
cross-row accuracy improvements against an alpha
value of 0.1 and observed significant improvement
in all of the testing schemas.
</bodyText>
<table confidence="0.999589">
Testing Data Training Features
Const. Dep. Union
Gold Standard NEs 7 6.7% 78.5% 82.4%
(ACE Data) 668 884 1427
579 579 579
All Proper Nouns 70.2% 71.4% 7 6.1%
668 884 1427
872 872 872
NEs Extracted by 78.2% 80.3% 85.1%
Training Rules 668 884 1427
1 69 217 354
</table>
<tableCaption confidence="0.9790635">
Table 1: Classification Accuracy, labeled training &amp;
testing data size
</tableCaption>
<bodyText confidence="0.999866533333333">
*ur results suggest that dependency parsing fea-
tures are reasonable extraction patterns, as their
accuracy rates are competitive against the model
based solely on constituency rules. Moreover, they
make a good complement to the constituency rules
proposed by Collins and Singer, since the accuracy
rates of the union is higher than either model alone.
As expected, all methods perform the best when
the test data are extracted in the same manner as
the training examples. However, if the systems
were given a well-formed named entity, the per-
formance degradation is reasonably small, about
2% absolute difference for all training methods.
The performance is somewhat lower when classi-
fying very general test cases of all proper nouns.
</bodyText>
<sectionHeader confidence="0.996882" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9998726875">
In this paper, we experimented with different syn-
tactic extraction patterns and different NE recogni-
tion constraints. We find that semi-supervised
methods are compatible with both constituency and
dependency extraction rules. We also find that the
resulting classifier is reasonably robust on test
cases that are different from its training examples.
An area that might benefit from a semi-supervised
NE tagger is machine translation. The semi-
supervised approach is suitable for non-English
languages that do not have very much annotated
NE data. We are currently applying our system to
Arabic. The robustness of the syntactic-based ap-
proach has allowed us to port the system to the
new language with minor changes in our syntactic
rules and classification features.
</bodyText>
<sectionHeader confidence="0.989581" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999449">
We would like to thank the NLP group at Pitt and
the anonymous reviewers for their valuable com-
ments and suggestions.
</bodyText>
<sectionHeader confidence="0.999282" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99891975862069">
Shumeet Baluja, Vibhu Mittal and Rahul Sukthankar,
1999. Applying machine learning for high perform-
ance named-entity extraction. In Proceedings of Pa-
cific Association for Computational Linguistics.
Daniel Bikel, Robert Schwartz &amp; Ralph Weischedel,
1999. An algorithm that learns what&apos;s in a name.
Machine Learning 34.
Michael Collins, 1997. Three generative lexicalized
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the ACL.
Michael Collins, and Yoram Singer, 1999. Unsuper-
vised Classification of Named Entities. In Proceed-
ings of SIGDAT.
A. P. Dempster, N. M. Laird and D. B. Rubin, 1977.
Maximum Likelihood from incomplete data via the
EM algorithm. Journal of Royal Statistical Society,
Series B, 39(1), 1-38.
Rebecca Hwa and Adam Lopez, 2004. *n the Conver-
sion of Constituent Parsers to Dependency Parsers.
Technical Report TR-04-118, Department of Com-
puter Science, University of Pittsburgh.
Kamal Nigam, Andrew McCallum, Sebastian Thrun and
Tom Mitchell, 2000. Text Classification from La-
beled and Unlabeled Documents using EM. Machine
Learning 39(2/3).
Erik F. Tjong Kim Sang and Fien De Meulder, 2003.
Introduction to the CoNLL-2003 Shared Task: Lan-
guage-Independent Named Entity Recognition. In
Proceedings of CoNLL-2003.
</reference>
<page confidence="0.998407">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958896">
<title confidence="0.999695">Syntax-based Semi-Supervised Named Entity Tagging</title>
<author confidence="0.96642">Behrang Mohit Rebecca Hwa</author>
<affiliation confidence="0.999678">Intelligent Systems Program Computer Science Department University of Pittsburgh University of Pittsburgh</affiliation>
<address confidence="0.998965">Pittsburgh, PA 152 60 USA Pittsburgh, PA 152 60, USA</address>
<email confidence="0.999717">behrang@cs.pitt.eduhwa@cs.pitt.edu</email>
<abstract confidence="0.99969555">We report an empirical study on the role of syntactic features in building a semisupervised named entity (NE) tagger. *ur study addresses two questions: What types of syntactic features are suitable for extracting potential NEs to train a classifier in a semi-supervised setting? How good is the resulting NE classifier on testing instances dissimilar from its training data? *ur study shows that constituency and dependency parsing constraints are both suitable features to extract NEs and train the classifier. Moreover, the classifier showed significant accuracy improvement when constituency features are combined with new dependency feature. Furthermore, the degradation in accuracy on unfamiliar test cases is low, suggesting that the trained classifier generalizes well.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shumeet Baluja</author>
</authors>
<title>Vibhu Mittal and Rahul Sukthankar,</title>
<date>1999</date>
<booktitle>In Proceedings of Pacific Association for Computational Linguistics.</booktitle>
<marker>Baluja, 1999</marker>
<rawString>Shumeet Baluja, Vibhu Mittal and Rahul Sukthankar, 1999. Applying machine learning for high performance named-entity extraction. In Proceedings of Pacific Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bikel</author>
<author>Robert Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>An algorithm that learns what&apos;s in a name.</title>
<date>1999</date>
<journal>Machine Learning</journal>
<volume>34</volume>
<contexts>
<context position="2700" citStr="Bikel et al. 1999" startWordPosition="399" endWordPosition="402">ach under three testing schemas. Each of these schemas represented a certain level of test data coverage (recall). Although the system performs best on (unseen) test data that is extracted by the syntactic rules (i.e., similar syntactic structures as the training examples), the performance degradation is not high when the system is tested on more general test cases. *ur experimental results suggest that a semi-supervised NE tagger can be successfully developed using syntax-rich features. 2 Previous Works and Our Approach Supervised NE Tagging has been studied extensively over the past decade (Bikel et al. 1999, Baluja et. al. 1999, Tjong Kim Sang and De Meulder 2003). Recently, there were increasing interests in semi-supervised learning approaches. Most relevant to our study, Collins and Singer (1999) showed that a NE Classifier can be developed by bootstrapping from a small amount of labeled examples. To extract potentially useful training examples, they first parsed the sentences and looked for expressions that satisfy two constituency patterns (appositives and prepositional phrases). A small subset of these expressions was then manually labeled with their correct NE tags. The training examples w</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel Bikel, Robert Schwartz &amp; Ralph Weischedel, 1999. An algorithm that learns what&apos;s in a name. Machine Learning 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL.</booktitle>
<marker>Collins, 1997</marker>
<rawString>Michael Collins, 1997. Three generative lexicalized models for statistical parsing. In Proceedings of the 35th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised Classification of Named Entities.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGDAT.</booktitle>
<contexts>
<context position="2895" citStr="Collins and Singer (1999)" startWordPosition="428" endWordPosition="431">d by the syntactic rules (i.e., similar syntactic structures as the training examples), the performance degradation is not high when the system is tested on more general test cases. *ur experimental results suggest that a semi-supervised NE tagger can be successfully developed using syntax-rich features. 2 Previous Works and Our Approach Supervised NE Tagging has been studied extensively over the past decade (Bikel et al. 1999, Baluja et. al. 1999, Tjong Kim Sang and De Meulder 2003). Recently, there were increasing interests in semi-supervised learning approaches. Most relevant to our study, Collins and Singer (1999) showed that a NE Classifier can be developed by bootstrapping from a small amount of labeled examples. To extract potentially useful training examples, they first parsed the sentences and looked for expressions that satisfy two constituency patterns (appositives and prepositional phrases). A small subset of these expressions was then manually labeled with their correct NE tags. The training examples were a combination of the labeled and unlabeled data. In their studies, Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 57–60, Ann Arbor, June 2005. c�2005 Association </context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins, and Yoram Singer, 1999. Unsupervised Classification of Named Entities. In Proceedings of SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<pages>1--38</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird and D. B. Rubin, 1977. Maximum Likelihood from incomplete data via the EM algorithm. Journal of Royal Statistical Society, Series B, 39(1), 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Adam Lopez</author>
</authors>
<title>n the Conversion of Constituent Parsers to Dependency Parsers.</title>
<date>2004</date>
<tech>Technical Report TR-04-118,</tech>
<institution>Department of Computer Science, University of Pittsburgh.</institution>
<contexts>
<context position="10957" citStr="Hwa and Lopez, 2004" startWordPosition="1760" endWordPosition="1763">, and are therefore independent of the syntactic rules. 2. Any single or series of proper nouns in the text: This is a heuristic for locating potential NEs so as to have the broadest coverage. 3. NEs extracted from text by the syntactic rules. This evaluation approach is similar to that of Collins and Singer. The main difference is that we have to match the extracted expressions to a pre1 We only used the NE portion of the data and removed the information for other tracking and extraction tasks. 2 We used the Collins parser (1997) to generate the constituency parse and a dependency converter (Hwa and Lopez, 2004) to obtain the dependency parse of English sentences. 59 labeled gold standard from ACE rather than performing manual annotations ourselves. All tests have been performed under a 5-fold cross validation training-testing setup. Table 1 presents the accuracy of the NE classification and the size of labeled data in the different training-testing configurations. The second line of each cell shows the size of labeled training data and the third line shows the size of testing data. Each column presents the result for one type of the syntactic features that were used to extract NEs. Each row of the t</context>
</contexts>
<marker>Hwa, Lopez, 2004</marker>
<rawString>Rebecca Hwa and Adam Lopez, 2004. *n the Conversion of Constituent Parsers to Dependency Parsers. Technical Report TR-04-118, Department of Computer Science, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text Classification from Labeled and Unlabeled Documents using EM.</title>
<date>2000</date>
<journal>Machine Learning</journal>
<volume>39</volume>
<issue>2</issue>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew McCallum, Sebastian Thrun and Tom Mitchell, 2000. Text Classification from Labeled and Unlabeled Documents using EM. Machine Learning 39(2/3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder,</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder, 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. In Proceedings of CoNLL-2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>