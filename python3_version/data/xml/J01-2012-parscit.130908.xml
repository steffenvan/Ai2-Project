<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.086925">
<note confidence="0.793905">
Computational Linguistics Volume 27, Number 2
</note>
<title confidence="0.990588">
Learnability in Optimality Theory
</title>
<author confidence="0.946867">
Bruce Tesar and Paul Smolensky
</author>
<affiliation confidence="0.921588">
(Rutgers University and The Johns Hopkins University)
</affiliation>
<address confidence="0.65881">
Cambridge, MA: The MIT Press, 2000,
vii+140 pp; hardbound, ISBN
</address>
<figure confidence="0.828209333333333">
0-262-20126-7, $25.00
Reviewed by
Walter Daelemans
</figure>
<affiliation confidence="0.487478">
University of Antwerp
</affiliation>
<bodyText confidence="0.999978789473684">
There are several ways in which the algorithmic acquisition of language knowledge
and behavior can be studied. One important area of research is the computational
modeling of human language acquisition using statistical, machine learning, or neural
network methods. See Broeder and Murre (2000) for a recent collection of this type of
research. And there is of course the use of statistical and machine learning methods
in computational linguistics and language technology (Manning and Schtitze 1999).
The theoretical study of the learnability of formal grammars in close relationship with
linguistic explanation in generative grammar (&amp;quot;the logical problem of language acqui-
sition&amp;quot;) is yet another relevant area of research.
The new book by Bruce Tesar and Paul Smolensky (T&amp;S) is an example of the
latter approach, but supported by computational modeling. In the book, an approach
to learning in optimality theory (OT) is proposed. OT is an alternative to the principles
and parameters (P&amp;P) approach to Universal Grammar. OT claims that all languages
have in common a set of constraints on well-formedness, and differ only in which con-
straints have priority in case of conflict. Priorities for each language are characterized
in a language-specific ranking (dominance hierarchy). This hierarchy assigns harmony
values to structural descriptions of language input (depending on which constraints
are broken), and the analyses with maximal harmony are the well-formed ones. In OT,
the problem of language learning is therefore transformed from a parameter-setting
problem into a constraint-ranking problem. In P&amp;P approaches to learning, either
comprehensive structural detail has to be encoded in the learning principles (as in
cue learning), or the search is completely uninformed (as in triggering learning). The
central claim of T&amp;S is that OT provides sufficient structure to allow efficient and
grammatically informed learning, and therefore offers an optimal trade-off.
Chapter 1 introduces the problem of language learning and sketches its solution.
The problem is that a learner cannot parse the language input until a grammar has
been learned, but a grammar cannot be learned until the input can be parsed. The solu-
tion proposed for this unsupervised learning problem will be familiar to researchers in
statistical natural language processing: a variant of expectation-maximization in which
the model (grammar) and the input-output pairing (parse) are iteratively optimized.
In Chapter 2, OT is explained as a series of general principles. These principles are
illustrated by means of a phonological example (syllable parsing) and a syntactic ex-
ample (distribution of clausal subjects). This is an excellent chapter for those wanting
a concise and clear introduction to OT. Chapter 3 introduces constraint demotion (CD)
as the grammar-learning part in the expectation-maximization set-up explained earlier.
The basic mechanism is simple and elegant: given an initial constraint ordering, and
pairs of well-formed structural descriptions (winners) and competing not-well-formed
structural descriptions (losers), demote the constraints violated by a winner down in
</bodyText>
<page confidence="0.995393">
316
</page>
<subsectionHeader confidence="0.85122">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999992">
the hierarchy, making each winner more harmonic than its competing losers. But where
do the losers come from? OT presupposes that Universal Grammar provides access
to a function Gen that generates all possible candidate structural descriptions for a
language input. This provides implicit negative evidence for the learner. The selection
of specific losers among a potentially infinite number of possible losers is again based
on a familiar notion for statistical NLP practitioners: error-driven learning. Selection of
learning material is guided by failure of the parser. In Chapter 7, T&amp;S formally show
that the CD algorithm is correct, works both from an initially unordered constraint
hierarchy and from arbitrary initial hierarchies, and that data complexity is favorable.
CD requires n(n — 1)/2 informative data pairs (where n is the number of constraints)
for the initially unordered hierarchy case, and n(n — 1) for the arbitrary initial hier-
archy case. Chapter 4 focuses on robust interpretive parsing (RIP, the parsing part in
the expectation-maximization set-up) and its combination with constraint demotion
(RIP/CD learning for short) in the context of stress grammar learning. Dynamic pro-
gramming is used in RIP to guide the search for structural descriptions for inputs
(explained in more detail in Chapter 8). In Chapter 4 we also find some simulation re-
sults, with success scores (in number of languages for which the correct stress system
is learned) ranging from about 60% to 97% depending on the initial constraint hierar-
chy chosen. From the perspective of computational linguistics research methodology,
this is a disappointing section. The short description leaves out essential details about
data selection and preprocessing, implementation, and experimental set-up, and lacks
a comparison with alternative approaches. We do learn more about cases where the
RIP/CD algorithm can fail. Chapters 5 and 6 deal with a number of interesting issues
and extensions (e.g., the subset principle, learning of underlying forms, and lexicon
learning), but they seem to me more speculative than the rest of the book.
This book is clearly groundbreaking for researchers in OT, formal learning theory,
and generative grammar. But it is a remarkable piece of work also for more agnostic
(computational) linguists interested in algorithms for language learning. I found it es-
pecially exciting to see how OT allows well-known statistical NLP tools like dynamic
programming, expectation-maximization, and error-driven learning to be integrated
and put to use in an elegant, efficient, and original solution to the language learning
problem. The book certainly succeeds in conveying the importance of the T&amp;S ap-
proach to learnability in OT and in general, but the approach needs more extensive
simulation results to be completely convincing.
</bodyText>
<sectionHeader confidence="0.978609" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.9970055">
Broeder, Peter and Jaap Murre. 2000.
Cognitive Models of Language Acquisition.
Cambridge University Press.
Manning, Christopher D. and Hinrich
Schiitze. 1999. Foundations of Statistical
Natural Language Processing. The MIT
Press, Cambridge, MA.
Walter Daelemans is a professor of computational linguistics and artificial intelligence at the
University of Antwerp, and currently president of SIGNLL, ACL&apos;s special interest group on nat-
ural language learning. Daelemans&apos;s address is CNTS Language Technology Group, University
of Antwerp, Universiteitsplein 1, Building A, B-2610 Antwerpen, Belgium; e-mail: daelem@
uia.ac.be.
</reference>
<page confidence="0.998613">
317
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.121659">
<title confidence="0.864986">Computational Linguistics Volume 27, Number 2 Learnability in Optimality Theory</title>
<author confidence="0.999693">Bruce Tesar</author>
<author confidence="0.999693">Paul Smolensky</author>
<affiliation confidence="0.999987">(Rutgers University and The Johns Hopkins University)</affiliation>
<address confidence="0.764822666666667">Cambridge, MA: The MIT Press, 2000, vii+140 pp; hardbound, ISBN 0-262-20126-7, $25.00</address>
<note confidence="0.849256">Reviewed by</note>
<author confidence="0.766169">Walter Daelemans</author>
<affiliation confidence="0.986363">University of Antwerp</affiliation>
<abstract confidence="0.99923084375">There are several ways in which the algorithmic acquisition of language knowledge and behavior can be studied. One important area of research is the computational modeling of human language acquisition using statistical, machine learning, or neural network methods. See Broeder and Murre (2000) for a recent collection of this type of research. And there is of course the use of statistical and machine learning methods in computational linguistics and language technology (Manning and Schtitze 1999). The theoretical study of the learnability of formal grammars in close relationship with linguistic explanation in generative grammar (&amp;quot;the logical problem of language acquisition&amp;quot;) is yet another relevant area of research. The new book by Bruce Tesar and Paul Smolensky (T&amp;S) is an example of the latter approach, but supported by computational modeling. In the book, an approach to learning in optimality theory (OT) is proposed. OT is an alternative to the principles and parameters (P&amp;P) approach to Universal Grammar. OT claims that all languages have in common a set of constraints on well-formedness, and differ only in which constraints have priority in case of conflict. Priorities for each language are characterized in a language-specific ranking (dominance hierarchy). This hierarchy assigns harmony values to structural descriptions of language input (depending on which constraints are broken), and the analyses with maximal harmony are the well-formed ones. In OT, the problem of language learning is therefore transformed from a parameter-setting problem into a constraint-ranking problem. In P&amp;P approaches to learning, either comprehensive structural detail has to be encoded in the learning principles (as in cue learning), or the search is completely uninformed (as in triggering learning). The central claim of T&amp;S is that OT provides sufficient structure to allow efficient and grammatically informed learning, and therefore offers an optimal trade-off. Chapter 1 introduces the problem of language learning and sketches its solution. The problem is that a learner cannot parse the language input until a grammar has been learned, but a grammar cannot be learned until the input can be parsed. The solution proposed for this unsupervised learning problem will be familiar to researchers in statistical natural language processing: a variant of expectation-maximization in which the model (grammar) and the input-output pairing (parse) are iteratively optimized. In Chapter 2, OT is explained as a series of general principles. These principles are illustrated by means of a phonological example (syllable parsing) and a syntactic ex-</abstract>
<intro confidence="0.652182">ample (distribution of clausal subjects). This is an excellent chapter for those wanting</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter Broeder</author>
<author>Jaap Murre</author>
</authors>
<title>Cognitive Models of Language Acquisition.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<marker>Broeder, Murre, 2000</marker>
<rawString>Broeder, Peter and Jaap Murre. 2000. Cognitive Models of Language Acquisition. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Schiitze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Schiitze, 1999</marker>
<rawString>Manning, Christopher D. and Hinrich Schiitze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Walter</author>
</authors>
<title>Daelemans is a professor of computational linguistics and artificial intelligence at the University of Antwerp, and currently president of SIGNLL, ACL&apos;s special interest group on natural language learning.</title>
<booktitle>Daelemans&apos;s address is CNTS Language Technology Group, University of Antwerp, Universiteitsplein 1, Building A, B-2610</booktitle>
<location>Antwerpen, Belgium;</location>
<note>e-mail: daelem@ uia.ac.be.</note>
<marker>Walter, </marker>
<rawString>Walter Daelemans is a professor of computational linguistics and artificial intelligence at the University of Antwerp, and currently president of SIGNLL, ACL&apos;s special interest group on natural language learning. Daelemans&apos;s address is CNTS Language Technology Group, University of Antwerp, Universiteitsplein 1, Building A, B-2610 Antwerpen, Belgium; e-mail: daelem@ uia.ac.be.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>