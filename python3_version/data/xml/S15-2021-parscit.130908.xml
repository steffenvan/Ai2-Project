<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000898">
<title confidence="0.9970405">
ECNU: Using Traditional Similarity Measurements and Word Embedding
for Semantic Textual Similarity Estimation
</title>
<author confidence="0.999825">
Jiang Zhao, Man Lan* , Jun Feng Tian
</author>
<affiliation confidence="0.982824333333333">
Shanghai Key Laboratory of Multidimensional Information Processing
Department of Computer Science and Technology,
East China Normal University, Shanghai 200241, P. R. China
</affiliation>
<email confidence="0.99646">
51121201042,10112130275@ecnu.cn; mlan@cs.ecnu.edu.cn*
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999280727272727">
This paper reports our submissions to seman-
tic textual similarity task, i.e., task 2 in Se-
mantic Evaluation 2015. We built our sys-
tems using various traditional features, such as
string-based, corpus-based and syntactic simi-
larity metrics, as well as novel similarity mea-
sures based on distributed word representa-
tions, which were trained using deep learning
paradigms. Since the training and test datasets
consist of instances collected from various do-
mains, three different strategies of the usage
of training datasets were explored: (1) use all
available training datasets and build a unified
supervised model for all test datasets; (2) se-
lect the most similar training dataset and sep-
arately construct a individual model for each
test set; (3) adopt multi-task learning frame-
work to make full use of available training set-
s. Results on the test datasets show that using
all datasets as training set achieves the best av-
eraged performance and our best system ranks
15 out of 73.
</bodyText>
<sectionHeader confidence="0.999124" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999870744186047">
Estimating the degree of semantic similarity be-
tween two sentences is the building block of many
natural language processing (NLP) applications,
such as textual entailment (Zhao et al., 2014a), tex-
t summarization (Lloret et al., 2008), question an-
swering (Celikyilmaz et al., 2010), etc. Therefore,
semantic textual similarity (STS) has been received
an increasing amount of attention in recent years,
e.g., the Semantic Textual Similarity competition-
s in Semantic Evaluation Exercises have been held
from 2012 to 2014. This year the participants in the
STS task in SemEval 2015 (Agirre et al., 2015) are
required to rate the similar degree of a pair of sen-
tences by a value from 0 (no relation) to 5 (semantic
equivalence) with an optional confidence score.
To identify semantic textual similarity of tex-
t pairs, most existing works adopt at least one of
the following feature types: (1) string based simi-
larity (B¨ar et al., 2012; Jimenez et al., 2012) which
employs common functions to calculate similari-
ties over string sequences extracted from original
strings, e.g., lemma, stem, or n-gram sequences; (2)
corpus based similarity (ˇSari´c et al., 2012; Han et al.,
2013) where distributional models such as Laten-
t Semantic Analysis (LSA) (Landauer and Dumais,
1997), are used to derive the distributional vectors of
words from a large corpus according to their occur-
rence patterns, afterwards, similarities of sentence
pairs are calculated using these vectors; (3) knowl-
edge based method (Shareghi and Bergler, 2013;
Mihalcea et al., 2006) which estimates the similari-
ties with the aid of external resources, such as Word-
Netl. Among them, lots of researchers (Sultan et
al., 2014; Han et al., 2013) leverage different word
alignment strategies to bring word-level similarity to
sentence-level similarity.
In this work, we first borrow aforementioned ef-
fective types of similarity measurements including
string-based, corpus-based, syntactic features and so
on, to capture the semantic similarity between two
sentences. Beside, we also present a novel feature
type based on word embeddings that are induced us-
ing neural language models over a large raw cor-
</bodyText>
<footnote confidence="0.406291">
lhttp://wordnet.princeton.edu/
</footnote>
<page confidence="0.957151">
117
</page>
<bodyText confidence="0.95240116">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 117–122,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
pus (Mikolov et al., 2013b). Then these features are
served as input of a regression model. Notice that,
the organizers provide us seventeen training dataset-
s and five test datasets, which are drawn from dif-
ferent but related domains. Accordingly, we build
three different systems in terms of the usage of train-
ing datasets: (1) exploit all the training datasets and
train a single model for all test datasets; (2) choose
one domain-dependent training dataset for each test
dataset using cosine distance selection criterion and
train models individually for each test dataset; (3) to
overcome overuse or underuse of training datasets,
we adopt multi-task learning (MTL) framework to
make full use of available training datasets, that is,
for each test set the main task is built upon designat-
ed training datasets and the rest training datasets are
used in the auxiliary tasks.
The rest of this paper is organized as follows.
Section 2 describes various similarity measurements
used in our systems. System setups and experimen-
tal results on training and test datasets are presented
in Section 3. Finally, conclusions and future work
are given in Section 4.
</bodyText>
<sectionHeader confidence="0.835015" genericHeader="method">
2 Semantic Similarity Measurements
</sectionHeader>
<bodyText confidence="0.9998002">
Following our previous work (Zhao et al., 2014b),
we adopted the traditional widely-used features (i.e.,
string, corpus, syntactic features) for semantic simi-
larity measurements. In this work, we also proposed
several novel features using word embeddings.
</bodyText>
<subsectionHeader confidence="0.992699">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999782214285714">
Several text preprocessing operations were per-
formed before we extracted features. We first con-
verted the contractions to their formal writings, for
example, doesn’t is rewritten as does not. Then the
WordNet-based Lemmatizer implemented in Natu-
ral Language Toolkit2 was used to lemmatize al-
l words to their nearest base forms in WordNet, for
example, was is lemmatized to be. After that, We re-
placed a word from one sentence with another word
from the other sentence if these two words share the
same meaning, where WordNet was used to look up
synonyms. No word sense disambiguation was per-
formed and all synsets in WordNet for a particular
lemma were considered.
</bodyText>
<footnote confidence="0.948425">
2http://nltk.org/
</footnote>
<subsectionHeader confidence="0.999713">
2.2 String Based Features
</subsectionHeader>
<bodyText confidence="0.970925">
We firstly recorded length information of given
sentences pairs using the following eight mea-
sure functions: |A|, |B|, |A − B|, |B − A|, |A u B|, |A n
B|, (|A|−|B|)
|B |, (|B|−|A|)
|A |, where |A |stands for the num-
ber of non-repeated words in sentence A , |A − B|
means the number of unmatched words found in A
but not in B , |A u B |stands for the set size of non-
repeated words found in either A or B and |A n B|
means the set size of shared words found in both A
and B .
Motivated by the hypothesis that two texts are
considered to be semantic similar if they share more
common strings, we adopted the following five type-
s of measurements: (1) longest common sequence
similarity on the original and lemmatized sentences;
(2) Jaccard, Dice, Overlap coefficient on orig-
inal word sequences; (3) Jaccard similarity using
n-grams, where n-grams were obtained at three dif-
ferent levels, i.e., the original word level (n=1,2,3),
the lemmatized word level (n=1,2,3) and the char-
acter level (n=2,3,4); (4) weighted word overlap
feature (ˇSari´c et al., 2012) that takes the impor-
tance of words into consideration, where Web 1T
5-gram Corpus3 was used to estimate the impor-
tance of words; (5) sentences were represented as
vectors in tf*idf schema based on their lemmatized
forms and then these vectors were used to calcu-
late cosine, Manhattan, Euclidean distance
and Pearson, Spearmanr, Kendalltau corre-
lation coefficients.
Totally, we got thirty-one string based features.
</bodyText>
<subsectionHeader confidence="0.999504">
2.3 Corpus Based Features
</subsectionHeader>
<bodyText confidence="0.999957090909091">
The distributional meanings of words own good se-
mantic properties and Latent Semantic Analysis (L-
SA) (Landauer and Dumais, 1997) is widely used to
estimate the distributional vectors of words. Hence,
we adopted two distributional word sets released by
TakeLab (ˇSari´c et al., 2012), where LSA was per-
formed on the New York Times Annotated Corpus
(NYT)4 and Wikipedia. Then two strategies were
used to convert the distributional meanings of words
to sentence level: (i) simply summing up the distri-
butional vector of each word w in the sentence, (ii)
</bodyText>
<footnote confidence="0.99842">
3https://catalog.ldc.upenn.edu/LDC2006T13
4https://catalog.ldc.upenn.edu/LDC2008T19
</footnote>
<page confidence="0.993191">
118
</page>
<bodyText confidence="0.999956666666667">
using the information content (ˇSari´c et al., 2012) to
weigh the LSA vector of each word w and then sum-
ming them up. After that we used cosine similarity
to measure the similarity of two sentences based on
these vectors. Besides, we used the Co-occurrence
Retrieval Model (CRM) (Weeds, 2003) as another
type of corpus based feature. The CRM was cal-
culated based on a notion of substitutability, that is,
the more appropriate it was to substitute word w1 in
place of word w2 in a suitable natural language task,
the more semantically similar they were.
At last, we obtained six corpus based features.
</bodyText>
<subsectionHeader confidence="0.995881">
2.4 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999946428571429">
Besides semantic similarity, we also estimated the
similarities of sentence pairs at syntactic level. S-
tanford CoreNLP toolkit (Manning et al., 2014) was
used to obtain the POS tag sequences for each sen-
tence. Afterwards, we performed eight measure
functions described above in Section 2.2 over these
sequences, resulting in eight syntactic features.
</bodyText>
<subsectionHeader confidence="0.998417">
2.5 Word Embedding Features
</subsectionHeader>
<bodyText confidence="0.999719521739131">
Recently, deep learning has archived a great success
in the fields of computer vision, automatic speech
recognition and natural language processing. One
result of its application in NLP, i.e., word embed-
dings, has been successfully explored in named en-
tity recognition, chunking (Turian et al., 2010) and
semantic word similarities(Mikolov et al., 2013a),
etc. The distributed representations of words (i.e.,
word embeddings) learned using neural network-
s over a large raw corpus have been shown that
they performed significantly better than LSA for p-
reserving linear regularities among words (Mikolov
et al., 2013a). Due to its superior performance,
we adopted word embeddings to estimate the sim-
ilarities of sentence pairs. In our experiments, we
used two different word embeddings: word2vec
(Mikolov et al., 2013b) and Collobert and West-
on embeddings (Turian et al., 2010). The word
embeddings from Word2vec are distributed within
the word2vec toolkits and they are 300-dimensional
vectors learned from Google News Corpus which
consists of over a 100 billion words. The Col-
lobert and Weston embeddings are learned over a
</bodyText>
<footnote confidence="0.768093">
5https://code.google.com/p/word2vec
</footnote>
<bodyText confidence="0.999941071428571">
part of RCV1 corpus which consists of 63 mil-
lions words, resulting in 100-dimensional contin-
uous vectors. To obtain the sentence representa-
tions from word representations, we used idf to
weigh the embedding vectors of words and sim-
ply summed them up. Although the word embed-
ding is obtained from large corpus in considera-
tion of its context, using this bag of words (BOW)
representation of sentences, the current word se-
quence in sentence is neglected. After that, we
used cosine, Manhattan, Euclidean func-
tions and Pearson, Spearmanr, Kendalltau
correlation coefficients to calculate the similarities
based on these synthetic sentence representations.
</bodyText>
<subsectionHeader confidence="0.962752">
2.6 Other Features
</subsectionHeader>
<bodyText confidence="0.999694923076923">
Besides the shallow semantic similarities between
words and strings, we also calculated the similari-
ties of named entities in two sentences using longest
common sequence function. Seven types of named
entities, i.e., location, organization, date, money,
person, time and percent, recognized by Stanford
CoreNLP toolkit (Manning et al., 2014) were con-
sidered. We designed a binary feature to indicate
whether two sentences in a given pair have the same
polarity (i.e., affirmative or negative) by looking up
a manually-collected negation list with 29 negation
words (e.g., scarcely, no, little). Finally, we obtained
in eight features.
</bodyText>
<sectionHeader confidence="0.998322" genericHeader="evaluation">
3 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.960465">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999049083333333">
Participants built their systems on seventeen dataset-
s in development period and evaluated their systems
on five test datasets in test period. Each dataset con-
sists of a number of sentence pairs and each pair has
a human-assigned similarity score in the range [0, 5]
which increases with similarity. The datasets were
collected from different but related domains. Due
to limitation of page length, we only provide a brief
description of test sets in Table 1. Refer (Agirre et
al., 2014) for more details. As we can see from this
table, datasets from different domains have distinct
average lengths of sentence A and B.
</bodyText>
<page confidence="0.985696">
119
</page>
<figure confidence="0.485615">
Dataset # of pairs average length
answers-forums 2000 (17.56,17.37)
answers-students 1500 (10.49,11.17)
belief 2000 (15.16,14.56)
headlines 1500 ( 7.86,7.91 )
images 1500 (10.59,10.58)
</figure>
<tableCaption confidence="0.9950945">
Table 1: The statistics of test datasets for STS task in
SemEval 2015.
</tableCaption>
<subsectionHeader confidence="0.997532">
3.2 Experimental Setups
</subsectionHeader>
<bodyText confidence="0.999414818181818">
We built three different systems according to the us-
age of training datasets as follows.
allData: We used all the training datasets and
built a single global regression model regardless of
domain information of different test datasets.
DesignatedData: For each test dataset, we cal-
culated the cosine distance with every candidate
training dataset. Then the training dataset with
the lowest distance score was chose as the train-
ing dataset to fit a regression model for specific test
dataset.
</bodyText>
<equation confidence="0.904132333333333">
∑
Dist(Xtst,Xc) = 1−
xiEXt.t
</equation>
<bodyText confidence="0.9984285">
MTL: On one hand, taking all the training dataset-
s into consideration may hurt the performance since
training and test datasets are from different domains.
On the other hand, using the most related dataset-
s leads to insufficient usage of available datasets.
Therefore, we considered to adopt multi-task learn-
ing framework to take full advantage of available
training sets. Under multi-task learning framework,
a main task learns together with other related aux-
iliary tasks at the same time, using a shared repre-
sentation. This often leads to a better model for the
main task, because it allows the learner to use the
commonality among the tasks. Hence, for each test
dataset we selected the datasets whose cosine dis-
tances are less than 0.1 (at least one training set)
as training set to construct the main task, and then
used the remaining training sets to construct auxil-
iary tasks. In this work, we adopted the robust multi-
task feature learning (rMTFL) (Gong et al., 2012),
which assumes that the model W can be decom-
posed into two components: a shared feature struc-
ture P that captures task relatedness and a group-
sparse structure Q that detects outlier tasks. Specifi-
cally, it solves following formulation:
</bodyText>
<equation confidence="0.876817">
∥WiFXi − Yi∥2F + ρ1∥P∥2,1 + ρ2∥QT ∥2,1
subject to : W = P + Q
</equation>
<bodyText confidence="0.999932076923077">
where Xi denotes the input matrix of the i-th task,
Yi denotes its corresponding label, Wi is the model
for task i, the regularization parameter ρ1 controls
the joint feature learning, and the regularization pa-
rameter ρ2 controls the columnwise group sparsity
on Q that detects outliers.
In our preliminary experiments, several regres-
sion algorithms were examined, including Support
Vector Regression (SVR, linear), Random Forest
(RF) and Gradient Boosting (GB) implemented in
the scikit-learn toolkit (Pedregosa et al., 2011). The
system performance is evaluated using Pearson cor-
relation (r).
</bodyText>
<subsectionHeader confidence="0.9603">
3.3 Results on Training Data
</subsectionHeader>
<bodyText confidence="0.999973347826087">
To configure the parameters in the three systems,
i.e., the trade-off parameter c in SVR, the number
of trees n in RF, the number of boosting stages n
in GB in allData and DesignatedData, ρ1,2
in MTL, we conducted a series of experiments on
STS 2014 datasets (eleven datasets for training, six
datasets for development). Table 2 shows the Pear-
son performance of our systems on developmen-
t datasets. We explored a large scale of parameter
values and only the best result for each algorith-
m was listed due to the limitation of page length.
The numbers in the brackets in algorithms colum-
n indicate the parameter values and those in bold
font represent the best performance for each dataset
and system. From the table we find that (1) GB
and SVR obtain the best averaged results in sys-
tem allData and DesignatedData respective-
ly; (2) although DesignatedData uses only one
most-closely dataset for training for each test set, it
achieves comparable or even better performance on
some datasets when compared with allData; (3)
our multi-task learning framework can indeed boost
the performance.
</bodyText>
<subsectionHeader confidence="0.861407">
3.4 Results on Test Data
</subsectionHeader>
<bodyText confidence="0.994357">
According to the results on training datasets,
we configured three submitted runs as following:
</bodyText>
<equation confidence="0.5700865">
cosine(xi, xj)
|Xtst||Xc|
∑
xjEX�
∑t
i=1
min
W
</equation>
<page confidence="0.973781">
120
</page>
<table confidence="0.99984875">
Algorithms deft-forum deft-news headlines images OnWN tweet-news Mean
SVR (0.01) 0.458 0.761 0.728 0.813 0.836 0.727 0.721
RF (65) 0.491 0.751 0.718 0.789 0.873 0.741 0.727
GB (50) 0.499 0.760 0.725 0.805 0.863 0.739 0.732
SVR (0.1) 0.549 0.725 0.765 0.790 0.810 0.740 0.730
RF (75) 0.513 0.709 0.741 0.768 0.814 0.767 0.719
GB (50) 0.504 0.694 0.738 0.790 0.809 0.751 0.714
MTL (0.1, 0.1) 0.556 0.772 0.738 0.808 0.819 0.745 0.740
</table>
<tableCaption confidence="0.99594">
Table 2: Pearson of allData,DesignatedData using different algorithms and MTL on STS 2014 datasets.
</tableCaption>
<table confidence="0.999766166666667">
RUN answers-forums answers-students belief headlines images Mean Rank
ECNU-1stSVMALL 0.715 0.712 0.728 0.798 0.847 0.755 15
ECNU-2ndSVMONE 0.687 0.733 0.698 0.820 0.836 0.747 19
ECNU-3rdMTL 0.692 0.752 0.695 0.805 0.858 0.752 18
DLSCU-S1 0.739 0.773 0.749 0.825 0.864 0.785 1
ExBThemis-themisexp 0.695 0.778 0.748 0.825 0.853 0.773 2
</table>
<tableCaption confidence="0.99994">
Table 3: Results of our three runs on STS 2015 test datasets, as well as top rank runs.
</tableCaption>
<bodyText confidence="0.998613212121212">
ECNU-1stSVMALL which builds a global model
on all datasets using SVR with parameter c=0.1;
ECNU-2ndSVMONE which fits individual model for
each test set on a designated training set using GB
with parameter n=50; ECNU-3rdMTL which em-
ploys robust multi-task feature learning with param-
eter p1 = P2 = 0.1.
Table 3 summarizes the results of our sub-
mitted runs on test datasets officially released
by the organizers, as well as the top rank run-
s. In terms of mean Pearson measuremen-
t, system ECNU-1stSVMALL performs the best,
which is comparable to ECNU-3rdMTL. Howev-
er, the ECNU-2ndSVMONE performs the worst.
This is inconsistent with the results on train-
ing datasets wherein ECNU-3rdMTL yields the
best performance. On test dataset, we find that
ECNU-3rdMTL has much worse performances
than ECNU-1stSVMALL on answers-forums and
belief while it achieves much better results on
answers-students, headlines and images dataset-
s. The possible reason may be that the train-
ing dataset selected from the candidate dataset-
s in main task are ill-suited for answers-forums
and belief test datasets, which is also verified by
the results of system ECNU-2ndSVMONE. It is
noteworthy that on answers-students and headlines
ECNU-2ndSVMONE achieves much better results
than ECNU-1stSVMALL although the former sys-
tem only uses much less training instances (750,750
vs. 10592). In addition, the difference between top
system DLSCU-S1 and our systems is about 3%,
which means our systems are promising.
</bodyText>
<sectionHeader confidence="0.999296" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999934">
We used traditional NLP features including string-
based, corpus-based and syntacitc features, for tex-
tual semantic similarity estimation, as well as nov-
el word embedding features. We also presented
three different systems to compare the strategies of
different usage of training data, i.e., single super-
vised learning with all training datasets and individ-
ual training dataset for each test dataset, and multi-
task learning framework. Our best system achieves
15th place out of 73 systems on test datasets. Notice-
ably each system achieves the best performance on
different test datasets, which indicates the usage of
training datasets is important, we will explore more
sophisticated way to utilize these training datasets in
future work.
</bodyText>
<sectionHeader confidence="0.996122" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.66505">
This research is supported by grants from Science
and Technology Commission of Shanghai Munici-
pality under research grant no. (14DZ2260800 and
15ZR1410700) and Shanghai Collaborative Innova-
tion Center of Trustworthy Software for Internet of
</bodyText>
<page confidence="0.991143">
121
</page>
<bodyText confidence="0.375294">
Things (ZF1213).
</bodyText>
<sectionHeader confidence="0.915377" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999471066666667">
Eneko Agirre, Carmen Banea, and et al. 2014. SemEval-
2014 task 10: Multilingual semantic textual similarity.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 81–91.
Eneko Agirre, Carmen Banea, and et al. 2015. SemEval-
2015 task 2: Semantic textual similarity, English, S-
panish and pilot on interpretability. In Proceedings of
the 9th International Workshop on Semantic Evalua-
tion (SemEval 2015), June.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics, pages 435–
440.
Asli Celikyilmaz, Dilek Hakkani-Tur, and Gokhan Tur.
2010. LDA based similarity modeling for question
answering. In Proceedings of the NAACL HLT 2010
Workshop on Semantic Search, pages 1–9.
Pinghua Gong, Jieping Ye, and Changshui Zhang. 2012.
Robust multi-task feature learning. In Proceedings of
the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 895–
903.
Lushan Han, Abhay L. Kashyap, Tim Finin, James
Mayfield, and Jonathan Weese. 2013. UM-
BC EBIQUITY-CORE: Semantic textual similarity
systems. In Second *SEM, pages 44–52.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft cardinality: A parameterized sim-
ilarity function for text comparison. In *SEM 2012
and (SemEval 2012), pages 449–453.
Thomas K Landauer and Susan T Dumais. 1997. A so-
lution to Plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review.
Elena Lloret, Oscar Ferr´andez, Rafael Munoz, and
Manuel Palomar. 2008. A text summarization ap-
proach under the influence of textual entailment. In
Proceedings of NLPCS 2008, pages 22–31.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd ACL: System
Demonstrations, pages 55–60.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In AAAI, volume 6, pages
775–780.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corra-
do, and Jeff Dean. 2013b. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111–3119.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort,
et al. 2011. Scikit-learn: Machine learning in Python.
The Journal of Machine Learning Research, 12:2825–
2830.
Ehsan Shareghi and Sabine Bergler. 2013. CLaC-CORE:
Exhaustive feature combination for measuring textual
similarity. In Second Joint Conference on Lexical and
Computational Semantics (*SEM).
Md Arafat Sultan, Steven Bethard, and Tamara Sumner.
2014. DLS@CU: sentence similarity from word align-
ment. In SemEval 2014, pages 241–246.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 384–394.
Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder,
and Bojana Dalbelo Ba&amp;quot;si´c. 2012. TakeLab: Systems
for measuring semantic text similarity. In *SEM 2012
and (SemEval 2012), pages 441–448.
Julie Elizabeth Weeds. 2003. Measures and applications
of lexical distributional similarity. Ph.D. thesis, Uni-
versity of Sussex.
Jiang Zhao, Man Lan, Zheng-Yu Niu, and Dong-Hong
Ji. 2014a. Recognizing cross-lingual textual entail-
ment with co-training using similarity and difference
views. In IJCNN 2014, Beijing, China, 2014, pages
3705–3712.
Jiang Zhao, Tiantian Zhu, and Man Lan. 2014b. Ecnu:
One stone two birds: Ensemble of heterogenous mea-
sures for semantic relatedness and textual entailment.
In Proceedings of the SemEval 2014, pages 271–277,
Dublin, Ireland, August.
</reference>
<page confidence="0.997492">
122
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.282414">
<title confidence="0.9981085">ECNU: Using Traditional Similarity Measurements and Word for Semantic Textual Similarity Estimation</title>
<author confidence="0.833026">Man Jun Feng Shanghai Key Laboratory of Multidimensional Information Zhao</author>
<affiliation confidence="0.988286">Department of Computer Science and</affiliation>
<note confidence="0.458943">East China Normal University, Shanghai 200241, P. R.</note>
<abstract confidence="0.997213130434783">This paper reports our submissions to semantic textual similarity task, i.e., task 2 in Semantic Evaluation 2015. We built our systems using various traditional features, such as string-based, corpus-based and syntactic similarity metrics, as well as novel similarity measures based on distributed word representations, which were trained using deep learning paradigms. Since the training and test datasets consist of instances collected from various domains, three different strategies of the usage of training datasets were explored: (1) use all available training datasets and build a unified supervised model for all test datasets; (2) select the most similar training dataset and separately construct a individual model for each test set; (3) adopt multi-task learning framework to make full use of available training sets. Results on the test datasets show that using all datasets as training set achieves the best averaged performance and our best system ranks 15 out of 73.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
</authors>
<title>SemEval2014 task 10: Multilingual semantic textual similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>81--91</pages>
<marker>Agirre, Banea, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, and et al. 2014. SemEval2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
</authors>
<title>SemEval2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>Agirre, Banea, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, and et al. 2015. SemEval2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 435– 440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-Tur</author>
<author>Gokhan Tur</author>
</authors>
<title>LDA based similarity modeling for question answering.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Semantic Search,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="1661" citStr="Celikyilmaz et al., 2010" startWordPosition="247" endWordPosition="250">st similar training dataset and separately construct a individual model for each test set; (3) adopt multi-task learning framework to make full use of available training sets. Results on the test datasets show that using all datasets as training set achieves the best averaged performance and our best system ranks 15 out of 73. 1 Introduction Estimating the degree of semantic similarity between two sentences is the building block of many natural language processing (NLP) applications, such as textual entailment (Zhao et al., 2014a), text summarization (Lloret et al., 2008), question answering (Celikyilmaz et al., 2010), etc. Therefore, semantic textual similarity (STS) has been received an increasing amount of attention in recent years, e.g., the Semantic Textual Similarity competitions in Semantic Evaluation Exercises have been held from 2012 to 2014. This year the participants in the STS task in SemEval 2015 (Agirre et al., 2015) are required to rate the similar degree of a pair of sentences by a value from 0 (no relation) to 5 (semantic equivalence) with an optional confidence score. To identify semantic textual similarity of text pairs, most existing works adopt at least one of the following feature typ</context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, Tur, 2010</marker>
<rawString>Asli Celikyilmaz, Dilek Hakkani-Tur, and Gokhan Tur. 2010. LDA based similarity modeling for question answering. In Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinghua Gong</author>
<author>Jieping Ye</author>
<author>Changshui Zhang</author>
</authors>
<title>Robust multi-task feature learning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>895--903</pages>
<contexts>
<context position="13973" citStr="Gong et al., 2012" startWordPosition="2200" endWordPosition="2203"> training sets. Under multi-task learning framework, a main task learns together with other related auxiliary tasks at the same time, using a shared representation. This often leads to a better model for the main task, because it allows the learner to use the commonality among the tasks. Hence, for each test dataset we selected the datasets whose cosine distances are less than 0.1 (at least one training set) as training set to construct the main task, and then used the remaining training sets to construct auxiliary tasks. In this work, we adopted the robust multitask feature learning (rMTFL) (Gong et al., 2012), which assumes that the model W can be decomposed into two components: a shared feature structure P that captures task relatedness and a groupsparse structure Q that detects outlier tasks. Specifically, it solves following formulation: ∥WiFXi − Yi∥2F + ρ1∥P∥2,1 + ρ2∥QT ∥2,1 subject to : W = P + Q where Xi denotes the input matrix of the i-th task, Yi denotes its corresponding label, Wi is the model for task i, the regularization parameter ρ1 controls the joint feature learning, and the regularization parameter ρ2 controls the columnwise group sparsity on Q that detects outliers. In our prelim</context>
</contexts>
<marker>Gong, Ye, Zhang, 2012</marker>
<rawString>Pinghua Gong, Jieping Ye, and Changshui Zhang. 2012. Robust multi-task feature learning. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 895– 903.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay L Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Jonathan Weese</author>
</authors>
<title>UMBC EBIQUITY-CORE: Semantic textual similarity systems.</title>
<date>2013</date>
<booktitle>In Second *SEM,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="2555" citStr="Han et al., 2013" startWordPosition="393" endWordPosition="396">task in SemEval 2015 (Agirre et al., 2015) are required to rate the similar degree of a pair of sentences by a value from 0 (no relation) to 5 (semantic equivalence) with an optional confidence score. To identify semantic textual similarity of text pairs, most existing works adopt at least one of the following feature types: (1) string based similarity (B¨ar et al., 2012; Jimenez et al., 2012) which employs common functions to calculate similarities over string sequences extracted from original strings, e.g., lemma, stem, or n-gram sequences; (2) corpus based similarity (ˇSari´c et al., 2012; Han et al., 2013) where distributional models such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; (3) knowledge based method (Shareghi and Bergler, 2013; Mihalcea et al., 2006) which estimates the similarities with the aid of external resources, such as WordNetl. Among them, lots of researchers (Sultan et al., 2014; Han et al., 2013) leverage different word alignment strategies to bring word-level simila</context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay L. Kashyap, Tim Finin, James Mayfield, and Jonathan Weese. 2013. UMBC EBIQUITY-CORE: Semantic textual similarity systems. In Second *SEM, pages 44–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Soft cardinality: A parameterized similarity function for text comparison.</title>
<date>2012</date>
<booktitle>In *SEM 2012 and (SemEval</booktitle>
<pages>449--453</pages>
<contexts>
<context position="2334" citStr="Jimenez et al., 2012" startWordPosition="360" endWordPosition="363">has been received an increasing amount of attention in recent years, e.g., the Semantic Textual Similarity competitions in Semantic Evaluation Exercises have been held from 2012 to 2014. This year the participants in the STS task in SemEval 2015 (Agirre et al., 2015) are required to rate the similar degree of a pair of sentences by a value from 0 (no relation) to 5 (semantic equivalence) with an optional confidence score. To identify semantic textual similarity of text pairs, most existing works adopt at least one of the following feature types: (1) string based similarity (B¨ar et al., 2012; Jimenez et al., 2012) which employs common functions to calculate similarities over string sequences extracted from original strings, e.g., lemma, stem, or n-gram sequences; (2) corpus based similarity (ˇSari´c et al., 2012; Han et al., 2013) where distributional models such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; (3) knowledge based method (Shareghi and Bergler, 2013; Mihalcea et al., 2006) which est</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2012</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2012. Soft cardinality: A parameterized similarity function for text comparison. In *SEM 2012 and (SemEval 2012), pages 449–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<note>Psychological review.</note>
<contexts>
<context position="2650" citStr="Landauer and Dumais, 1997" startWordPosition="407" endWordPosition="410">a pair of sentences by a value from 0 (no relation) to 5 (semantic equivalence) with an optional confidence score. To identify semantic textual similarity of text pairs, most existing works adopt at least one of the following feature types: (1) string based similarity (B¨ar et al., 2012; Jimenez et al., 2012) which employs common functions to calculate similarities over string sequences extracted from original strings, e.g., lemma, stem, or n-gram sequences; (2) corpus based similarity (ˇSari´c et al., 2012; Han et al., 2013) where distributional models such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; (3) knowledge based method (Shareghi and Bergler, 2013; Mihalcea et al., 2006) which estimates the similarities with the aid of external resources, such as WordNetl. Among them, lots of researchers (Sultan et al., 2014; Han et al., 2013) leverage different word alignment strategies to bring word-level similarity to sentence-level similarity. In this work, we first borrow aforementioned effective types</context>
<context position="7554" citStr="Landauer and Dumais, 1997" startWordPosition="1193" endWordPosition="1196">(4) weighted word overlap feature (ˇSari´c et al., 2012) that takes the importance of words into consideration, where Web 1T 5-gram Corpus3 was used to estimate the importance of words; (5) sentences were represented as vectors in tf*idf schema based on their lemmatized forms and then these vectors were used to calculate cosine, Manhattan, Euclidean distance and Pearson, Spearmanr, Kendalltau correlation coefficients. Totally, we got thirty-one string based features. 2.3 Corpus Based Features The distributional meanings of words own good semantic properties and Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997) is widely used to estimate the distributional vectors of words. Hence, we adopted two distributional word sets released by TakeLab (ˇSari´c et al., 2012), where LSA was performed on the New York Times Annotated Corpus (NYT)4 and Wikipedia. Then two strategies were used to convert the distributional meanings of words to sentence level: (i) simply summing up the distributional vector of each word w in the sentence, (ii) 3https://catalog.ldc.upenn.edu/LDC2006T13 4https://catalog.ldc.upenn.edu/LDC2008T19 118 using the information content (ˇSari´c et al., 2012) to weigh the LSA vector of each word</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Lloret</author>
<author>Oscar Ferr´andez</author>
<author>Rafael Munoz</author>
<author>Manuel Palomar</author>
</authors>
<title>A text summarization approach under the influence of textual entailment.</title>
<date>2008</date>
<booktitle>In Proceedings of NLPCS</booktitle>
<pages>22--31</pages>
<marker>Lloret, Ferr´andez, Munoz, Palomar, 2008</marker>
<rawString>Elena Lloret, Oscar Ferr´andez, Rafael Munoz, and Manuel Palomar. 2008. A text summarization approach under the influence of textual entailment. In Proceedings of NLPCS 2008, pages 22–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd ACL: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="8838" citStr="Manning et al., 2014" startWordPosition="1395" endWordPosition="1398">ty to measure the similarity of two sentences based on these vectors. Besides, we used the Co-occurrence Retrieval Model (CRM) (Weeds, 2003) as another type of corpus based feature. The CRM was calculated based on a notion of substitutability, that is, the more appropriate it was to substitute word w1 in place of word w2 in a suitable natural language task, the more semantically similar they were. At last, we obtained six corpus based features. 2.4 Syntactic Features Besides semantic similarity, we also estimated the similarities of sentence pairs at syntactic level. Stanford CoreNLP toolkit (Manning et al., 2014) was used to obtain the POS tag sequences for each sentence. Afterwards, we performed eight measure functions described above in Section 2.2 over these sequences, resulting in eight syntactic features. 2.5 Word Embedding Features Recently, deep learning has archived a great success in the fields of computer vision, automatic speech recognition and natural language processing. One result of its application in NLP, i.e., word embeddings, has been successfully explored in named entity recognition, chunking (Turian et al., 2010) and semantic word similarities(Mikolov et al., 2013a), etc. The distr</context>
<context position="11241" citStr="Manning et al., 2014" startWordPosition="1757" endWordPosition="1760">ntences, the current word sequence in sentence is neglected. After that, we used cosine, Manhattan, Euclidean functions and Pearson, Spearmanr, Kendalltau correlation coefficients to calculate the similarities based on these synthetic sentence representations. 2.6 Other Features Besides the shallow semantic similarities between words and strings, we also calculated the similarities of named entities in two sentences using longest common sequence function. Seven types of named entities, i.e., location, organization, date, money, person, time and percent, recognized by Stanford CoreNLP toolkit (Manning et al., 2014) were considered. We designed a binary feature to indicate whether two sentences in a given pair have the same polarity (i.e., affirmative or negative) by looking up a manually-collected negation list with 29 negation words (e.g., scarcely, no, little). Finally, we obtained in eight features. 3 Experiments and Results 3.1 Datasets Participants built their systems on seventeen datasets in development period and evaluated their systems on five test datasets in test period. Each dataset consists of a number of sentence pairs and each pair has a human-assigned similarity score in the range [0, 5] </context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd ACL: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In AAAI,</booktitle>
<volume>6</volume>
<pages>775--780</pages>
<contexts>
<context position="2924" citStr="Mihalcea et al., 2006" startWordPosition="449" endWordPosition="452">al., 2012; Jimenez et al., 2012) which employs common functions to calculate similarities over string sequences extracted from original strings, e.g., lemma, stem, or n-gram sequences; (2) corpus based similarity (ˇSari´c et al., 2012; Han et al., 2013) where distributional models such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; (3) knowledge based method (Shareghi and Bergler, 2013; Mihalcea et al., 2006) which estimates the similarities with the aid of external resources, such as WordNetl. Among them, lots of researchers (Sultan et al., 2014; Han et al., 2013) leverage different word alignment strategies to bring word-level similarity to sentence-level similarity. In this work, we first borrow aforementioned effective types of similarity measurements including string-based, corpus-based, syntactic features and so on, to capture the semantic similarity between two sentences. Beside, we also present a novel feature type based on word embeddings that are induced using neural language models over</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI, volume 6, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="3783" citStr="Mikolov et al., 2013" startWordPosition="573" endWordPosition="576">sentence-level similarity. In this work, we first borrow aforementioned effective types of similarity measurements including string-based, corpus-based, syntactic features and so on, to capture the semantic similarity between two sentences. Beside, we also present a novel feature type based on word embeddings that are induced using neural language models over a large raw corlhttp://wordnet.princeton.edu/ 117 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 117–122, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics pus (Mikolov et al., 2013b). Then these features are served as input of a regression model. Notice that, the organizers provide us seventeen training datasets and five test datasets, which are drawn from different but related domains. Accordingly, we build three different systems in terms of the usage of training datasets: (1) exploit all the training datasets and train a single model for all test datasets; (2) choose one domain-dependent training dataset for each test dataset using cosine distance selection criterion and train models individually for each test dataset; (3) to overcome overuse or underuse of training </context>
<context position="9420" citStr="Mikolov et al., 2013" startWordPosition="1483" endWordPosition="1486">CoreNLP toolkit (Manning et al., 2014) was used to obtain the POS tag sequences for each sentence. Afterwards, we performed eight measure functions described above in Section 2.2 over these sequences, resulting in eight syntactic features. 2.5 Word Embedding Features Recently, deep learning has archived a great success in the fields of computer vision, automatic speech recognition and natural language processing. One result of its application in NLP, i.e., word embeddings, has been successfully explored in named entity recognition, chunking (Turian et al., 2010) and semantic word similarities(Mikolov et al., 2013a), etc. The distributed representations of words (i.e., word embeddings) learned using neural networks over a large raw corpus have been shown that they performed significantly better than LSA for preserving linear regularities among words (Mikolov et al., 2013a). Due to its superior performance, we adopted word embeddings to estimate the similarities of sentence pairs. In our experiments, we used two different word embeddings: word2vec (Mikolov et al., 2013b) and Collobert and Weston embeddings (Turian et al., 2010). The word embeddings from Word2vec are distributed within the word2vec toolk</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="3783" citStr="Mikolov et al., 2013" startWordPosition="573" endWordPosition="576">sentence-level similarity. In this work, we first borrow aforementioned effective types of similarity measurements including string-based, corpus-based, syntactic features and so on, to capture the semantic similarity between two sentences. Beside, we also present a novel feature type based on word embeddings that are induced using neural language models over a large raw corlhttp://wordnet.princeton.edu/ 117 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 117–122, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics pus (Mikolov et al., 2013b). Then these features are served as input of a regression model. Notice that, the organizers provide us seventeen training datasets and five test datasets, which are drawn from different but related domains. Accordingly, we build three different systems in terms of the usage of training datasets: (1) exploit all the training datasets and train a single model for all test datasets; (2) choose one domain-dependent training dataset for each test dataset using cosine distance selection criterion and train models individually for each test dataset; (3) to overcome overuse or underuse of training </context>
<context position="9420" citStr="Mikolov et al., 2013" startWordPosition="1483" endWordPosition="1486">CoreNLP toolkit (Manning et al., 2014) was used to obtain the POS tag sequences for each sentence. Afterwards, we performed eight measure functions described above in Section 2.2 over these sequences, resulting in eight syntactic features. 2.5 Word Embedding Features Recently, deep learning has archived a great success in the fields of computer vision, automatic speech recognition and natural language processing. One result of its application in NLP, i.e., word embeddings, has been successfully explored in named entity recognition, chunking (Turian et al., 2010) and semantic word similarities(Mikolov et al., 2013a), etc. The distributed representations of words (i.e., word embeddings) learned using neural networks over a large raw corpus have been shown that they performed significantly better than LSA for preserving linear regularities among words (Mikolov et al., 2013a). Due to its superior performance, we adopted word embeddings to estimate the similarities of sentence pairs. In our experiments, we used two different word embeddings: word2vec (Mikolov et al., 2013b) and Collobert and Weston embeddings (Turian et al., 2010). The word embeddings from Word2vec are distributed within the word2vec toolk</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2830</pages>
<marker>Pedregosa, Ga¨el Varoquaux, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, et al. 2011. Scikit-learn: Machine learning in Python. The Journal of Machine Learning Research, 12:2825– 2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehsan Shareghi</author>
<author>Sabine Bergler</author>
</authors>
<title>CLaC-CORE: Exhaustive feature combination for measuring textual similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<contexts>
<context position="2900" citStr="Shareghi and Bergler, 2013" startWordPosition="445" endWordPosition="448">g based similarity (B¨ar et al., 2012; Jimenez et al., 2012) which employs common functions to calculate similarities over string sequences extracted from original strings, e.g., lemma, stem, or n-gram sequences; (2) corpus based similarity (ˇSari´c et al., 2012; Han et al., 2013) where distributional models such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; (3) knowledge based method (Shareghi and Bergler, 2013; Mihalcea et al., 2006) which estimates the similarities with the aid of external resources, such as WordNetl. Among them, lots of researchers (Sultan et al., 2014; Han et al., 2013) leverage different word alignment strategies to bring word-level similarity to sentence-level similarity. In this work, we first borrow aforementioned effective types of similarity measurements including string-based, corpus-based, syntactic features and so on, to capture the semantic similarity between two sentences. Beside, we also present a novel feature type based on word embeddings that are induced using neu</context>
</contexts>
<marker>Shareghi, Bergler, 2013</marker>
<rawString>Ehsan Shareghi and Sabine Bergler. 2013. CLaC-CORE: Exhaustive feature combination for measuring textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Md Arafat Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<title>DLS@CU: sentence similarity from word alignment. In SemEval</title>
<date>2014</date>
<pages>241--246</pages>
<contexts>
<context position="3064" citStr="Sultan et al., 2014" startWordPosition="473" endWordPosition="476">, e.g., lemma, stem, or n-gram sequences; (2) corpus based similarity (ˇSari´c et al., 2012; Han et al., 2013) where distributional models such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; (3) knowledge based method (Shareghi and Bergler, 2013; Mihalcea et al., 2006) which estimates the similarities with the aid of external resources, such as WordNetl. Among them, lots of researchers (Sultan et al., 2014; Han et al., 2013) leverage different word alignment strategies to bring word-level similarity to sentence-level similarity. In this work, we first borrow aforementioned effective types of similarity measurements including string-based, corpus-based, syntactic features and so on, to capture the semantic similarity between two sentences. Beside, we also present a novel feature type based on word embeddings that are induced using neural language models over a large raw corlhttp://wordnet.princeton.edu/ 117 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), page</context>
</contexts>
<marker>Sultan, Bethard, Sumner, 2014</marker>
<rawString>Md Arafat Sultan, Steven Bethard, and Tamara Sumner. 2014. DLS@CU: sentence similarity from word alignment. In SemEval 2014, pages 241–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="9368" citStr="Turian et al., 2010" startWordPosition="1476" endWordPosition="1479">ties of sentence pairs at syntactic level. Stanford CoreNLP toolkit (Manning et al., 2014) was used to obtain the POS tag sequences for each sentence. Afterwards, we performed eight measure functions described above in Section 2.2 over these sequences, resulting in eight syntactic features. 2.5 Word Embedding Features Recently, deep learning has archived a great success in the fields of computer vision, automatic speech recognition and natural language processing. One result of its application in NLP, i.e., word embeddings, has been successfully explored in named entity recognition, chunking (Turian et al., 2010) and semantic word similarities(Mikolov et al., 2013a), etc. The distributed representations of words (i.e., word embeddings) learned using neural networks over a large raw corpus have been shown that they performed significantly better than LSA for preserving linear regularities among words (Mikolov et al., 2013a). Due to its superior performance, we adopted word embeddings to estimate the similarities of sentence pairs. In our experiments, we used two different word embeddings: word2vec (Mikolov et al., 2013b) and Collobert and Weston embeddings (Turian et al., 2010). The word embeddings fro</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane Sari´c</author>
<author>Goran Glavas</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Ba&amp;quot;si´c.</title>
<date></date>
<booktitle>In *SEM 2012 and (SemEval</booktitle>
<pages>441--448</pages>
<marker>Sari´c, Glavas, Karan, </marker>
<rawString>Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder, and Bojana Dalbelo Ba&amp;quot;si´c. 2012. TakeLab: Systems for measuring semantic text similarity. In *SEM 2012 and (SemEval 2012), pages 441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Elizabeth Weeds</author>
</authors>
<title>Measures and applications of lexical distributional similarity.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sussex.</institution>
<contexts>
<context position="8357" citStr="Weeds, 2003" startWordPosition="1318" endWordPosition="1319">York Times Annotated Corpus (NYT)4 and Wikipedia. Then two strategies were used to convert the distributional meanings of words to sentence level: (i) simply summing up the distributional vector of each word w in the sentence, (ii) 3https://catalog.ldc.upenn.edu/LDC2006T13 4https://catalog.ldc.upenn.edu/LDC2008T19 118 using the information content (ˇSari´c et al., 2012) to weigh the LSA vector of each word w and then summing them up. After that we used cosine similarity to measure the similarity of two sentences based on these vectors. Besides, we used the Co-occurrence Retrieval Model (CRM) (Weeds, 2003) as another type of corpus based feature. The CRM was calculated based on a notion of substitutability, that is, the more appropriate it was to substitute word w1 in place of word w2 in a suitable natural language task, the more semantically similar they were. At last, we obtained six corpus based features. 2.4 Syntactic Features Besides semantic similarity, we also estimated the similarities of sentence pairs at syntactic level. Stanford CoreNLP toolkit (Manning et al., 2014) was used to obtain the POS tag sequences for each sentence. Afterwards, we performed eight measure functions described</context>
</contexts>
<marker>Weeds, 2003</marker>
<rawString>Julie Elizabeth Weeds. 2003. Measures and applications of lexical distributional similarity. Ph.D. thesis, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Zhao</author>
<author>Man Lan</author>
<author>Zheng-Yu Niu</author>
<author>Dong-Hong Ji</author>
</authors>
<title>Recognizing cross-lingual textual entailment with co-training using similarity and difference views.</title>
<date>2014</date>
<booktitle>In IJCNN 2014,</booktitle>
<pages>3705--3712</pages>
<location>Beijing, China,</location>
<contexts>
<context position="1570" citStr="Zhao et al., 2014" startWordPosition="233" endWordPosition="236">asets and build a unified supervised model for all test datasets; (2) select the most similar training dataset and separately construct a individual model for each test set; (3) adopt multi-task learning framework to make full use of available training sets. Results on the test datasets show that using all datasets as training set achieves the best averaged performance and our best system ranks 15 out of 73. 1 Introduction Estimating the degree of semantic similarity between two sentences is the building block of many natural language processing (NLP) applications, such as textual entailment (Zhao et al., 2014a), text summarization (Lloret et al., 2008), question answering (Celikyilmaz et al., 2010), etc. Therefore, semantic textual similarity (STS) has been received an increasing amount of attention in recent years, e.g., the Semantic Textual Similarity competitions in Semantic Evaluation Exercises have been held from 2012 to 2014. This year the participants in the STS task in SemEval 2015 (Agirre et al., 2015) are required to rate the similar degree of a pair of sentences by a value from 0 (no relation) to 5 (semantic equivalence) with an optional confidence score. To identify semantic textual si</context>
<context position="4995" citStr="Zhao et al., 2014" startWordPosition="767" endWordPosition="770">ning datasets, we adopt multi-task learning (MTL) framework to make full use of available training datasets, that is, for each test set the main task is built upon designated training datasets and the rest training datasets are used in the auxiliary tasks. The rest of this paper is organized as follows. Section 2 describes various similarity measurements used in our systems. System setups and experimental results on training and test datasets are presented in Section 3. Finally, conclusions and future work are given in Section 4. 2 Semantic Similarity Measurements Following our previous work (Zhao et al., 2014b), we adopted the traditional widely-used features (i.e., string, corpus, syntactic features) for semantic similarity measurements. In this work, we also proposed several novel features using word embeddings. 2.1 Preprocessing Several text preprocessing operations were performed before we extracted features. We first converted the contractions to their formal writings, for example, doesn’t is rewritten as does not. Then the WordNet-based Lemmatizer implemented in Natural Language Toolkit2 was used to lemmatize all words to their nearest base forms in WordNet, for example, was is lemmatized to</context>
</contexts>
<marker>Zhao, Lan, Niu, Ji, 2014</marker>
<rawString>Jiang Zhao, Man Lan, Zheng-Yu Niu, and Dong-Hong Ji. 2014a. Recognizing cross-lingual textual entailment with co-training using similarity and difference views. In IJCNN 2014, Beijing, China, 2014, pages 3705–3712.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Zhao</author>
<author>Tiantian Zhu</author>
<author>Man Lan</author>
</authors>
<title>Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of the SemEval</booktitle>
<pages>271--277</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="1570" citStr="Zhao et al., 2014" startWordPosition="233" endWordPosition="236">asets and build a unified supervised model for all test datasets; (2) select the most similar training dataset and separately construct a individual model for each test set; (3) adopt multi-task learning framework to make full use of available training sets. Results on the test datasets show that using all datasets as training set achieves the best averaged performance and our best system ranks 15 out of 73. 1 Introduction Estimating the degree of semantic similarity between two sentences is the building block of many natural language processing (NLP) applications, such as textual entailment (Zhao et al., 2014a), text summarization (Lloret et al., 2008), question answering (Celikyilmaz et al., 2010), etc. Therefore, semantic textual similarity (STS) has been received an increasing amount of attention in recent years, e.g., the Semantic Textual Similarity competitions in Semantic Evaluation Exercises have been held from 2012 to 2014. This year the participants in the STS task in SemEval 2015 (Agirre et al., 2015) are required to rate the similar degree of a pair of sentences by a value from 0 (no relation) to 5 (semantic equivalence) with an optional confidence score. To identify semantic textual si</context>
<context position="4995" citStr="Zhao et al., 2014" startWordPosition="767" endWordPosition="770">ning datasets, we adopt multi-task learning (MTL) framework to make full use of available training datasets, that is, for each test set the main task is built upon designated training datasets and the rest training datasets are used in the auxiliary tasks. The rest of this paper is organized as follows. Section 2 describes various similarity measurements used in our systems. System setups and experimental results on training and test datasets are presented in Section 3. Finally, conclusions and future work are given in Section 4. 2 Semantic Similarity Measurements Following our previous work (Zhao et al., 2014b), we adopted the traditional widely-used features (i.e., string, corpus, syntactic features) for semantic similarity measurements. In this work, we also proposed several novel features using word embeddings. 2.1 Preprocessing Several text preprocessing operations were performed before we extracted features. We first converted the contractions to their formal writings, for example, doesn’t is rewritten as does not. Then the WordNet-based Lemmatizer implemented in Natural Language Toolkit2 was used to lemmatize all words to their nearest base forms in WordNet, for example, was is lemmatized to</context>
</contexts>
<marker>Zhao, Zhu, Lan, 2014</marker>
<rawString>Jiang Zhao, Tiantian Zhu, and Man Lan. 2014b. Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment. In Proceedings of the SemEval 2014, pages 271–277, Dublin, Ireland, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>