<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.990039">
Optimizing Question Answering Accuracy by Maximizing Log-Likelihood
</title>
<author confidence="0.997947">
Matthias H. Heie, Edward W. D. Whittaker and Sadaoki Furui
</author>
<affiliation confidence="0.9997065">
Department of Computer Science
Tokyo Institute of Technology
</affiliation>
<address confidence="0.489875">
Tokyo 152-8552, Japan
</address>
<email confidence="0.999373">
{heie,edw,furui}@furui.cs.titech.ac.jp
</email>
<sectionHeader confidence="0.997419" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998912333333333">
In this paper we demonstrate that there
is a strong correlation between the Ques-
tion Answering (QA) accuracy and the
log-likelihood of the answer typing com-
ponent of our statistical QA model. We
exploit this observation in a clustering al-
gorithm which optimizes QA accuracy by
maximizing the log-likelihood of a set of
question-and-answer pairs. Experimental
results show that we achieve better QA ac-
curacy using the resulting clusters than by
using manually derived clusters.
</bodyText>
<sectionHeader confidence="0.999398" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998795277777778">
Question Answering (QA) distinguishes itself
from other information retrieval tasks in that the
system tries to return accurate answers to queries
posed in natural language. Factoid QA limits it-
self to questions that can usually be answered with
a few words. Typically factoid QA systems em-
ploy some form of question type analysis, so that
a question such as What is the capital of Japan?
will be answered with a geographical term. While
many QA systems use hand-crafted rules for this
task, such an approach is time-consuming and
doesn’t generalize well to other languages. Ma-
chine learning methods have been proposed, such
as question classification using support vector ma-
chines (Zhang and Lee, 2003) and language mod-
eling (Merkel and Klakow, 2007). In these ap-
proaches, question categories are predefined and a
classifier is trained on manually labeled data. This
is an example of supervised learning. In this pa-
per we present an unsupervised method, where we
attempt to cluster question-and-answer (q-a) pairs
without any predefined question categories, hence
no manually class-labeled questions are used.
We use a statistical QA framework, described in
Section 2, where the system is trained with clusters
of q-a pairs. This framework was used in several
TREC evaluations where it placed in the top 10
of participating systems (Whittaker et al., 2006).
In Section 3 we show that answer accuracy is
strongly correlated with the log-likelihood of the
q-a pairs computed by this statistical model. In
Section 4 we propose an algorithm to cluster q-a
pairs by maximizing the log-likelihood of a dis-
joint set of q-a pairs. In Section 5 we evaluate the
QA accuracy by training the QA system with the
resulting clusters.
</bodyText>
<sectionHeader confidence="0.98972" genericHeader="introduction">
2 QA system
</sectionHeader>
<bodyText confidence="0.987376">
In our QA framework we choose to model only
the probability of an answer A given a question Q,
and assume that the answer A depends on two sets
of features: W = W (Q) and X = X(Q):
</bodyText>
<equation confidence="0.996844">
P(A|Q) = P(A|W, X), (1)
</equation>
<bodyText confidence="0.99998325">
where W represents a set of |W  |features describ-
ing the question-type part of Q such as who, when,
where, which, etc., and X is a set of features
which describes the “information-bearing” part of
Q, i.e. what the question is actually about and
what it refers to. For example, in the questions
Where is Mount Fuji? and How high is Mount
Fuji?, the question type features W differ, while
the information-bearing features X are identical.
Finding the best answer A� involves a search over
all A for the one which maximizes the probability
of the above model, i.e.:
</bodyText>
<equation confidence="0.851453">
P(A|W,X). (2)
</equation>
<bodyText confidence="0.998587">
Given the correct probability distribution, this
will give us the optimal answer in a maximum
likelihood sense. Using Bayes’ rule, assuming
uniform P(A) and that W and X are indepen-
dent of each other given A, in addition to ignoring
P(W, X) since it is independent of A, enables us
to rewrite Eq. (2) as
</bodyText>
<figure confidence="0.5383855">
A� = arg max
A
</figure>
<page confidence="0.973877">
236
</page>
<note confidence="0.6714345">
Proceedings of the ACL 2010 Conference Short Papers, pages 236–240,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.699787">
2.1 Retrieval Model
</subsectionHeader>
<bodyText confidence="0.999958545454545">
The retrieval model P(A|X) is essentially a lan-
guage model which models the probability of an
answer sequence A given a set of information-
bearing features X = {x1,... , x|X|}. This set
is constructed by extracting single-word features
from Q that are not present in a stop-list of high-
frequency words. The implementation of the re-
trieval model used for the experiments described
in this paper, models the proximity of A to fea-
tures in X. It is not examined further here;
see (Whittaker et al., 2005) for more details.
</bodyText>
<subsectionHeader confidence="0.939603">
2.2 Filter Model
</subsectionHeader>
<bodyText confidence="0.999702833333333">
The question-type feature set W = {w1, ... , w|W |}
is constructed by extracting n-tuples (n = 1, 2,...)
such as where, in what and when were from the
input question Q. We limit ourselves to extracting
single-word features. The 2522 most frequent
words in a collection of example questions are
considered in-vocabulary words; all other words
are out-of-vocabulary words, and substituted with
hUNKi.
Modeling the complex relationship between
W and A directly is non-trivial. We there-
fore introduce an intermediate variable CE =
{c1,...,c|CE|}, representing a set of classes of
example q-a pairs. In order to construct these
classes, given a set E = {t1,...,t|E|} of ex-
ample q-a pairs, we define a mapping function
f : E 7→ CE which maps each example q-a pair tj
for j = 1... |E |into a particular class f(tj) = ce.
Thus each class ce may be defined as the union of
all component q-a features from each tj satisfy-
ing f(tj) = ce. Hence each class ce constitutes a
cluster of q-a pairs. Finally, to facilitate modeling
we say that W is conditionally independent of A
given ce so that,
</bodyText>
<equation confidence="0.990551">
P(W  |A) = |CE |X P(W  |ceW) · P(ce A  |A), (4)
e=1
</equation>
<bodyText confidence="0.999854416666667">
where ceW and ceA refer to the subsets of question-
type features and example answers for the class ce,
respectively.
P(W  |ce W ) is implemented as trigram langu-
age models with backoff smoothing using absolute
discounting (Huang et al., 2001).
Due to data sparsity, our set of example q-a
pairs cannot be expected to cover all the possi-
ble answers to questions that may ever be asked.
We therefore employ answer class modeling rather
than answer word modeling by expanding Eq. (4)
as follows:
</bodyText>
<equation confidence="0.99588875">
P(W  |ceW)·
|KA |(5)
P P(ceA  |ka)P(ka  |A),
a=1
</equation>
<bodyText confidence="0.956985444444444">
where ka is a concrete class in the set of |KA|
answer classes KA. These classes are generated
using the Kneser-Ney clustering algorithm, com-
monly used for generating class definitions for
class language models (Kneser and Ney, 1993).
In this paper we restrict ourselves to single-
word answers; see (Whittaker et al., 2005) for the
modeling of multi-word answers. We estimate
P(ce A  |kA) as
</bodyText>
<equation confidence="0.7442725">
P(ceA  |kA) = |CE|
f(kA, cgA)
</equation>
<bodyText confidence="0.99713125">
where
and δ(·) is a discrete indicator function which
equals 1 if its argument evaluates true and 0 if
false.
</bodyText>
<equation confidence="0.91601175">
P(ka  |A) is estimated as
1
P(ka  |A) = P δ(A ∈ j). (8)
bj:jEK.
</equation>
<sectionHeader confidence="0.8143765" genericHeader="method">
3 The Relationship between Mean
Reciprocal Rank and Log-Likelihood
</sectionHeader>
<bodyText confidence="0.999867666666667">
We use Mean Reciprocal Rank (MRR) as our
metric when evaluating the QA accuracy on a set
of questions G = {g1...g|G|}:
</bodyText>
<equation confidence="0.941258">
P|G|
i=1 1/Ri
MRR = (9)
</equation>
<figure confidence="0.6773945">
|G|
A� = arg max P(A  |X) · P(W  |A) . (3)
A  |{z }  |{z }
retrieval filter
model model
|CE|
</figure>
<equation confidence="0.775026727272727">
P(W  |A) = P
e=1
f(kA,ceA)
, (6)
f(kA,ceA) =
P
g=1
P
bi:iEcA&apos;
δ(i ∈ kA)
|ceA |, (7)
</equation>
<page confidence="0.889542">
237
</page>
<figureCaption confidence="0.9944145">
Figure 1: MRR vs. LL (average per q-a pair) for
100 random cluster configurations.
</figureCaption>
<bodyText confidence="0.9992608">
where Ri is the rank of the highest ranking correct
candidate answer for gi.
Given a set D = (d1...d|D|) of q-a pairs disjoint
from the q-a pairs in CE, we can, using Eq. (5),
calculate the log-likelihood as
</bodyText>
<equation confidence="0.826433">
LL =
</equation>
<bodyText confidence="0.999980142857143">
To examine the relationship between MRR and
LL, we randomly generate configurations CE,
with a fixed cluster size of 4, and plot the result-
ing MRR and LL, computed on the same data set
D, as data points in a scatter plot, as seen in Fig-
ure 1. We find that LL and MRR are strongly
correlated, with a correlation coefficient p = 0.86.
This observation indicates that we should be
able to improve the answer accuracy of the QA
system by optimizing the LL of the filter model
in isolation, similar to how, in automatic speech
recognition, the LL of the language model can
be optimized in isolation to improve the speech
recognition accuracy (Huang et al., 2001).
</bodyText>
<sectionHeader confidence="0.984464" genericHeader="method">
4 Clustering algorithm
</sectionHeader>
<bodyText confidence="0.9993935">
Using the observation that LL is correlated with
MRR on the same data set, we expect that opti-
mizing LL on a development set (LLdev) will also
improve MRR on an evaluation set (MRReval).
Hence we propose the following greedy algorithm
to maximize LLdev:
</bodyText>
<figure confidence="0.896188095238095">
init: c1 ∈ CE contains all training pairs |E|
while improvement &gt; threshold do
best LLdev ← −∞
for all j = 1...|E |do
original cluster = f(tj)
Take tj out of f(tj)
for e = −1,1...|CE|,|CE |+ 1 do
Put tj in ce
Calculate LLdev
if LLdev &gt; best LLdev then
best LLdev ← LLdev
best cluster ← e
best pair ← j
end if
Take tj out of ce
end for
Put tj back in original cluster
end for
Take tbest pair out of f(tbest pair)
Put tbest pair into cbest cluster
end while
</figure>
<bodyText confidence="0.999788571428572">
In this algorithm, c−1 indicates the set of train-
ing pairs outside the cluster configuration, thus ev-
ery training pair will not necessarily be included
in the final configuration. c|c|+1 refers to a new,
empty cluster, hence this algorithm automatically
finds the optimal number of clusters as well as the
optimal configuration of them.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.993046">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.996976777777778">
For our data sets, we restrict ourselves to questions
that start with who, when or where. Furthermore,
we only use q-a pairs which can be answered with
a single word. As training data we use questions
and answers from the Knowledge-Master collec-
tion1. Development/evaluation questions are the
questions from TREC QA evaluations from TREC
2002 to TREC 2006, the answers to which are to
be retrieved from the AQUAINT corpus. In total
we have 2016 q-a pairs for training and 568 ques-
tions for development/evaluation. We are able to
retrieve the correct answer for 317 of the devel-
opment/evaluation questions, thus the theoretical
upper bound for our experiments is an answer ac-
curacy of MRR = 0.558.
Accuracy is evaluated using 5-fold (rotating)
cross-validation, where in each fold the TREC
QA data is partitioned into a development set of
</bodyText>
<footnote confidence="0.705005">
1http://www.greatauk.com/
</footnote>
<figure confidence="0.96663788">
-1.18 -1.16 -1.14 -1.12
LL
MRR
0.23
0.22
0.21
0.19
0.18
0.17
0.16
0.15
0.2
ρ = 0.86
� |D |log P(Wd|Ad)
d=1
P(Wd  |ce )·
|KA|
E P(ce�  |ka)P(ka  |Ad).
a=1
(10)
log
|cE|
e=1
� |D|
d=1
</figure>
<page confidence="0.988514">
238
</page>
<table confidence="0.9995362">
Configuration LLeval MRReval #clusters
manual -1.18 0.262 3
all-in-one -1.32 0.183 1
one-in-each -0.87 0.263 2016
automatic -0.24 0.281 4
</table>
<tableCaption confidence="0.91451575">
Table 1: LLeval (average per q-a pair) and
MRReval (over all held-out TREC years), and
number of clusters (median of the cross-evaluation
folds) for the various configurations.
</tableCaption>
<figure confidence="0.989945">
LLdev
MRRdev
0 400 800 1200 1600 2000
LL 0
-0.2
-0.4
-0.6
-0.8
-1
-1.2
-1.4
0.32 MRR
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
# iterations
</figure>
<bodyText confidence="0.993444208333333">
4 years’ data and an evaluation set of one year’s
data. For each TREC question the top 50 doc-
uments from the AQUAINT corpus are retrieved
using Lucene2. We use the QA system described
in Section 2 for QA evaluation. Our evaluation
metric is MRReval, and LLdev is our optimiza-
tion criterion, as motivated in Section 3.
Our baseline system uses manual clusters.
These clusters are obtained by putting all who q-a
pairs in one cluster, all when pairs in a second and
all where pairs in a third. We compare this baseline
with using clusters resulting from the algorithm
described in Section 4. We run this algorithm until
there are no further improvements in LLdev. Two
other cluster configurations are also investigated:
all q-a pairs in one cluster (all-in-one), and each q-
a pair in its own cluster (one-in-each). The all-in-
one configuration is equivalent to not using the fil-
ter model, i.e. answer candidates are ranked solely
by the retrieval model. The one-in-each configura-
tion was shown to perform well in the TREC 2006
QA evaluation (Whittaker et al., 2006), where it
ranked 9th among 27 participants on the factoid
QA task.
</bodyText>
<subsectionHeader confidence="0.629697">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.9999675">
In Table 1, we see that the manual clusters (base-
line) achieves an MRReval of 0.262, while the
clusters resulting from the clustering algorithm
give an MRReval of 0.281, which is a relative
improvement of 7%. This improvement is sta-
tistically significant at the 0.01 level using the
Wilcoxon signed-rank test. The one-in-each clus-
ter configuration achieves an MRReval of 0.263,
which is not a statistically significant improvement
over the baseline. The all-in-one cluster configura-
tion (i.e. no filter model) has the lowest accuracy,
with an MRReval of 0.183.
</bodyText>
<footnote confidence="0.641735">
2http://lucene.apache.org/
</footnote>
<figure confidence="0.99987975">
(a) Development set, 4 year’s TREC.
0 400 800 1200 1600 2000
# iterations
(b) Evaluation set, 1 year’s TREC.
</figure>
<figureCaption confidence="0.992900666666667">
Figure 2: MRR and LL (average per q-a pair)
vs. number of algorithm iterations for one cross-
validation fold.
</figureCaption>
<sectionHeader confidence="0.999656" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99988385">
Manual inspection of the automatically derived
clusters showed that the algorithm had constructed
configurations where typically who, when and
where q-a pairs were put in separate clusters, as in
the manual configuration. However, in some cases
both who and where q-a pairs occurred in the same
cluster, so as to better answer questions like Who
won the World Cup?, where the answer could be a
country name.
As can be seen from Table 1, there are only 4
clusters in the automatic configuration, compared
to 2016 in the one-in-each configuration. Since
the computational complexity of the filter model
described in Section 2.2 is linear in the number of
clusters, a beneficial side effect of our clustering
procedure is a significant reduction in the compu-
tational requirement of the filter model.
In Figure 2 we plot LL and MRR for one of
the cross-validation folds over multiple iterations
(the while loop) of the clustering algorithm in Sec-
</bodyText>
<figure confidence="0.998715315789474">
LLeval
MRReval
LL 0
-0.2
-0.4
-0.6
-0.8
-1
-1.2
-1.4
0.32 MRR
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
</figure>
<page confidence="0.994315">
239
</page>
<bodyText confidence="0.999461666666667">
tion 4. It can clearly be seen that the optimization
of LLdev leads to improvement in MRReval, and
that LLeval is also well correlated with MRReval.
</bodyText>
<sectionHeader confidence="0.997728" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999744090909091">
In this paper we have shown that the log-likelihood
of our statistical model is strongly correlated with
answer accuracy. Using this information, we have
clustered training q-a pairs by maximizing log-
likelihood on a disjoint development set of q-a
pairs. The experiments show that with these clus-
ters we achieve better QA accuracy than using
manually clustered training q-a pairs.
In future work we will extend the types of ques-
tions that we consider, and also allow for multi-
word answers.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999912">
The authors wish to thank Dietrich Klakow for his
discussion at the concept stage of this work. The
anonymous reviewers are also thanked for their
constructive feedback.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999617925925926">
[Huang et al.2001] Xuedong Huang, Alex Acero and
Hsiao-Wuen Hon. 2001. Spoken Language Pro-
cessing. Prentice-Hall, Upper Saddle River, NJ,
USA.
[Kneser and Ney1993] Reinhard Kneser and Hermann
Ney. 1993. Improved Clustering Techniques for
Class-based Statistical Language Modelling. Pro-
ceedings of the European Conference on Speech
Communication and Technology (EUROSPEECH).
[Merkel and Klakow2007] Andreas Merkel and Diet-
rich Klakow. 2007. Language Model Based Query
Classification. Proceedings of the European Confer-
ence on Information Retrieval (ECIR).
[Whittaker et al.2005] Edward Whittaker, Sadaoki Fu-
rui and Dietrich Klakow. 2005. A Statistical Clas-
sification Approach to Question Answering using
Web Data. Proceedings of the International Con-
ference on Cyberworlds.
[Whittaker et al.2006] Edward Whittaker, Josef Novak,
Pierre Chatain and Sadaoki Furui. 2006. TREC
2006 Question Answering Experiments at Tokyo In-
stitute of Technology. Proceedings of The Fifteenth
Text REtrieval Conference (TREC).
[Zhang and Lee2003] Dell Zhang and Wee Sun Lee.
2003. Question Classification using Support Vec-
tor Machines. Proceedings of the Special Interest
Group on Information Retrieval (SIGIR).
</reference>
<page confidence="0.997016">
240
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.963123">
<title confidence="0.99999">Optimizing Question Answering Accuracy by Maximizing Log-Likelihood</title>
<author confidence="0.99997">Matthias H Heie</author>
<author confidence="0.99997">Edward W D Whittaker</author>
<author confidence="0.99997">Sadaoki Furui</author>
<affiliation confidence="0.999972">Department of Computer Science Tokyo Institute of Technology</affiliation>
<address confidence="0.977933">Tokyo 152-8552, Japan</address>
<abstract confidence="0.998799615384615">In this paper we demonstrate that there is a strong correlation between the Question Answering (QA) accuracy and the log-likelihood of the answer typing component of our statistical QA model. We exploit this observation in a clustering algorithm which optimizes QA accuracy by maximizing the log-likelihood of a set of question-and-answer pairs. Experimental results show that we achieve better QA accuracy using the resulting clusters than by using manually derived clusters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xuedong Huang</author>
</authors>
<title>Alex Acero and Hsiao-Wuen Hon.</title>
<date>2001</date>
<location>NJ, USA.</location>
<marker>[Huang et al.2001]</marker>
<rawString>Xuedong Huang, Alex Acero and Hsiao-Wuen Hon. 2001. Spoken Language Processing. Prentice-Hall, Upper Saddle River, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Clustering Techniques for Class-based Statistical Language Modelling.</title>
<date>1993</date>
<booktitle>Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH).</booktitle>
<marker>[Kneser and Ney1993]</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1993. Improved Clustering Techniques for Class-based Statistical Language Modelling. Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Merkel</author>
<author>Dietrich Klakow</author>
</authors>
<title>Language Model Based Query Classification.</title>
<date>2007</date>
<booktitle>Proceedings of the European Conference on Information Retrieval (ECIR).</booktitle>
<marker>[Merkel and Klakow2007]</marker>
<rawString>Andreas Merkel and Dietrich Klakow. 2007. Language Model Based Query Classification. Proceedings of the European Conference on Information Retrieval (ECIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Whittaker</author>
<author>Sadaoki Furui</author>
<author>Dietrich Klakow</author>
</authors>
<title>A Statistical Classification Approach to Question Answering using Web Data.</title>
<date>2005</date>
<booktitle>Proceedings of the International Conference on Cyberworlds.</booktitle>
<marker>[Whittaker et al.2005]</marker>
<rawString>Edward Whittaker, Sadaoki Furui and Dietrich Klakow. 2005. A Statistical Classification Approach to Question Answering using Web Data. Proceedings of the International Conference on Cyberworlds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Whittaker</author>
<author>Josef Novak</author>
<author>Pierre Chatain</author>
<author>Sadaoki Furui</author>
</authors>
<title>Question Answering Experiments at Tokyo Institute of Technology.</title>
<date>2006</date>
<journal>TREC</journal>
<booktitle>Proceedings of The Fifteenth Text REtrieval Conference (TREC).</booktitle>
<marker>[Whittaker et al.2006]</marker>
<rawString>Edward Whittaker, Josef Novak, Pierre Chatain and Sadaoki Furui. 2006. TREC 2006 Question Answering Experiments at Tokyo Institute of Technology. Proceedings of The Fifteenth Text REtrieval Conference (TREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>Wee Sun Lee</author>
</authors>
<title>Question Classification using Support Vector Machines.</title>
<date>2003</date>
<booktitle>Proceedings of the Special Interest Group on Information Retrieval (SIGIR).</booktitle>
<marker>[Zhang and Lee2003]</marker>
<rawString>Dell Zhang and Wee Sun Lee. 2003. Question Classification using Support Vector Machines. Proceedings of the Special Interest Group on Information Retrieval (SIGIR).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>