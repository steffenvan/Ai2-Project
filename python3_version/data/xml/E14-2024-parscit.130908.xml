<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003829">
<title confidence="0.702462">
MMAX2 for coreference annotation
</title>
<author confidence="0.981785">
Mateusz Kope´c
</author>
<affiliation confidence="0.995263">
Institute of Computer Science, Polish Academy of Sciences,
</affiliation>
<address confidence="0.904265">
Jana Kazimierza 5, 01-248 Warsaw, Poland
</address>
<email confidence="0.985172">
m.kopec@ipipan.waw.pl
</email>
<sectionHeader confidence="0.993455" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888">
This article presents major modifications
in the MMAX2 manual annotation tool,
which were implemented for the corefer-
ence annotation of Polish texts. Among
other, a new feature of adjudication is de-
scribed, as well as some general insight
into the manual annotation tool selection
process for the natural language process-
ing tasks.
</bodyText>
<sectionHeader confidence="0.998754" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999759596153846">
Recently published Polish Coreference Corpus
(PCC) (Ogrodniczuk et al., 2013) contains a large
number of Polish texts annotated manually with
coreference. During the initial stage of this project
in 2011, a tool had to be selected for the manual
text annotation with coreference.
First issue considered during the selection was
the alternative of desktop versus online annota-
tion tool. Recently, online annotation tools are
becoming increasingly popular (see for example
BRAT (Stenetorp et al., 2012)), with their advan-
tages such as the possibility to monitor the current
state of annotation and make changes to the anno-
tation tool easily, without the need to communi-
cate with the annotators. However, in our opinion
the choice should be made mainly based on the
annotators’ preferences, as their work efficiency is
crucial for the cost of the task.
10 linguists (which were annotators in previ-
ous projects conducted by the authors of this pa-
per) were asked in anonymous survey to choose
one of following three options: 1) that they pre-
fer an online annotation tool (not requiring any
installation), 2) a desktop tool with the possibil-
ity to work without constant internet access, 3)
that they do not have preference. Only one per-
son chose online tool and one person chose the
third option, leaving no choice for the annota-
tion task organizers other than to prepare a desk-
top application. The drawback of this approach
was the need to manage text distribution among
the annotators, as all the work was done on lo-
cal computers. Distribution was controlled by
the DistSys application, available at the webpage
zil.ipipan.waw.pl/DistSys.
After some analysis of the features required
by the project’s scope, the choice was narrowed
to only two tools: MMAX2 (Müller and Strube,
2006) and Palinka (Or˘asan, 2003). The problem
with the latter was that is was not error-prone, and
lack of publicly available sources did not allow
to make project-specific corrections. Therefore
MMAX2 environment was chosen for the man-
ual coreference annotation. It is a general purpose
cross-platform desktop annotation tool (written in
Java), created to be configurable for many natural
language annotation efforts. It’s source is publicly
available at the webpage mmax2.net. MMAX2
interface consists of the main window with the
text being annotated (see for example figure 5) and
several smaller windows facilitating various tasks
(all the other figures).
</bodyText>
<sectionHeader confidence="0.798046" genericHeader="method">
2 Annotation scope
</sectionHeader>
<bodyText confidence="0.999639571428571">
The annotation scope of the Polish Coreference
Corpus project consisted of several subtasks for
an annotator to perform for each text. Because the
automatic preannotation was used, annotator’s job
consisted not only of addition of new markables,
but also removal and correction of existing ones.
Subtasks to perform were to:
</bodyText>
<listItem confidence="0.999567">
• mark all mentions,
• mark each mention’s semantic head (choose
one word the mention consists of),
• cluster coreferent mentions,
• mark dominant expression of each cluster,
</listItem>
<page confidence="0.886602">
93
</page>
<note confidence="0.5456485">
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 93–96,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<listItem confidence="0.946786">
• mark quasi-identity links.
</listItem>
<bodyText confidence="0.999904357142857">
MMAX2 was easy to configure for most of the
subtasks, except the dominant expressions (there
is no possibility to define attributes of clusters,
only markables) and the choice of semantic head
(available types of attributes did not include a pos-
sibility to define a mention-dependent attribute).
Because an estimate of inter-annotator agree-
ment had to be calculated for the corpus, some
texts were annotated independently by two anno-
tators. Agreement measures were then calculated,
but as single gold standard annotation was needed
for the corpus, they also had to be merged into sin-
gle version by the adjudicator. This feature was
also not present in MMAX2.
</bodyText>
<sectionHeader confidence="0.992086" genericHeader="method">
3 New features
</sectionHeader>
<bodyText confidence="0.999842166666667">
Even with it’s great number of features, there
was no possibility to use MMAX2 without any
changes in it’s source code. Some changes were
the result of project’s scope requirements, some
were added in response to annotator requests. New
implemented features include:
</bodyText>
<listItem confidence="0.983255">
1. Internationalization – the interface of the tool
is available in English and Polish and can be
easily translated to other languages. Polish
version was used by the annotators, but for
international articles about the tool (such as
this one) the English interface was used.
2. Semantic head selection – a dropdown list
allows to choose one of the tokens mention
consists of as it’s semantic head. This head is
also underlined in markable browser.
3. Storing user setting – which windows are
opened, where are they located, what is the
font and it’s size – these and other user set-
tings are saved and automatically restored
when the application is reopened.
4. Dominant expressions – clusters can have
their attribute: a dominant expression, which
can be selected as one of the mentions from
the cluster or any other expression entered by
the user.
5. Undo button, reverting last action – very use-
ful feature to revert the last change, regard-
less of it’s nature.
</listItem>
<figureCaption confidence="0.9999845">
Figure 1: Adjudication of mentions
Figure 2: Adjudication of clustering
</figureCaption>
<listItem confidence="0.965612846153846">
6. Merge two mentions – user can merge two
mentions into one with a single click, sum-
ming their words and merging their clusters.
Very useful feature when for example one is
correcting automatic annotation, which failed
to recognize a long named entity name and
instead created two entities, each in it’s sepa-
rate cluster.
7. Improved browser operability – browsers al-
low to operate on mentions, links and clus-
ters, not only to view them.
8. Adjudication feature – it will be covered in
detail in the next section.
</listItem>
<sectionHeader confidence="0.965852" genericHeader="method">
4 Adjudication feature
</sectionHeader>
<bodyText confidence="0.999971153846154">
Adjudication feature of the new Superannotation
plugin allows to compare two versions of anno-
tation of the same text and merge them into one,
adjudicated version. The design is based on the
original MMAX2 Diff plugin, which allowed to
see the differences between two annotations, yet
it was not possible to merge them into one. The
readability of the differences was also limited and
it was improved in our tool.
The adjudication process starts with opening
one annotation in standard way and then the other
via the menu in Superannotation plugin and con-
sist of several steps, each merging particular layer:
</bodyText>
<page confidence="0.995448">
94
</page>
<listItem confidence="0.663425066666667">
1. Mentions – first we need to merge mention
annotations. Differences between the two an-
notations are shown in the figure 1. First col-
umn shows the mention content (and this is
constant in all steps of adjudication), second
shows if that mention is in the first annota-
tion, third column shows if it is in the second
annotation (&amp;quot;+&amp;quot; if yes, &amp;quot;-&amp;quot; if not). Single click
at &amp;quot;+&amp;quot; or the first column highlights given
span in the main window. Double click at one
of the last two columns selects the clicked
version as the proper one and changes the an-
notation in the other file to match the clicked
version. After such double click, the differ-
ence disappears and that row vanishes. After
all rows from that step are gone, mention an-
notations in both versions are the same and
we can proceed to the next step.
2. Comments – this time first column again
shows each mention, for which there is a
difference in comments in both annotations.
Double clicking at 2nd or 3rd column re-
solves the difference in given row.
3. Heads – similar to comments, by double-
clicking we can adjudicate differences in
head annotation.
4. Links – analogously as with heads, we merge
near-identity links annotations.
5. Clusters – this is the most complex adjudi-
cation task. At this point we surely have the
</listItem>
<bodyText confidence="0.796929357142857">
same set of mentions in both annotations, but
they may be clustered differently. Figure 2
presents how differences in clustering are vi-
sualized. Mentions with the same color in
one column are in the same cluster (they have
also the same cluster number). For example,
two occurrences of mention gorzka˛ czekolad˛e
are in the same cluster according to the first
annotation, and are singletons according to
the second annotation. Single click on any of
these options will show it in the main applica-
tion window, while double click will choose
the clicked version as the gold one and update
the other to match it.
</bodyText>
<listItem confidence="0.99290375">
6. Dominating expressions – as the clusters are
now the same, the only annotation left con-
siders cluster attributes: dominating expres-
sions.
</listItem>
<figureCaption confidence="0.999881">
Figure 3: Mention attributes – original MMAX2
Figure 4: Mention attributes – simplified
</figureCaption>
<bodyText confidence="0.9999715">
Key point of the adjudication procedure is to
merge all differences at a given level before pro-
ceeding to the next one. This way, after we resolve
all differences in the dominating expressions, we
are certain that our annotations are fully merged
and in fact the same.
</bodyText>
<sectionHeader confidence="0.990762" genericHeader="method">
5 Removed features
</sectionHeader>
<bodyText confidence="0.999911916666667">
Because MMAX2 is a generic tool, the first im-
pression is that it is very complicated. Numer-
ous options, many windows and menus are over-
whelming and could increase the time of creating
a manual for the annotators (often writing a lot of
text only to inform which options should not be
changed). Therefore we removed many options
and simplified the interface to leave only the fea-
tures required by our annotation task. Compare for
example the mention attribute window from the
original MMAX2 in figure 4 and in our version
in figure 3. Removed features included:
</bodyText>
<listItem confidence="0.997954166666667">
• Distinction of multiple annotation levels –
scope of the project considers only one level,
and the need to explicitly select it in many
places (for example in markable browser) is
unnecessary.
• Possibility to edit the base text – as we per-
</listItem>
<page confidence="0.997752">
95
</page>
<figureCaption confidence="0.999356">
Figure 5: Unnecessary arcs
</figureCaption>
<bodyText confidence="0.746028954545455">
formed the inter-annotator agreement analy-
sis, the base text could not be changed.
• Arcs between coreferent mentions in cluster
(see figure 5 for original visualization) – from
our experience, they decrease the readability
of the cluster annotation. As the mentions
in cluster are already highlighted, there is no
need to show the arcs connecting them (the
arcs are not clickable as in BRAT).
• MMAX Query Language – MMAX2 facili-
tates a query language to search for the an-
notations fulfilling given properties. In our
opinion this feature seems more appropriate
for an analyst, not an annotator. Moreover,
results of such querying would be more in-
formative for a collection of texts, not a sin-
gle document.
• Kappa statistic and coincidence matrix calcu-
lation for multiple annotations of a single text
– again, this feature seems more appropriate
for an analyst and for the whole corpus, not a
single text.
</bodyText>
<sectionHeader confidence="0.999144" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999822137931035">
Every unnecessary action, which has to be re-
peated numerous times by a human annotator, has
a significant cost in terms of time and money.
We claim that annotation efforts are more effi-
cient when there is a step of tool customization
(or even design and implementation from scratch)
beforehand and also during the process, based on
the feedback from the annotators. Using general-
purpose tools has a clear benefit of cheap and
fast initialization of the project, but also there
are major drawbacks: a compromise between the
project needs and the tool capabilities. As we have
seen, even a tool with great customization options
such as MMAX2 doesn’t have all the features one
would need.
Experience of the PCC project shows, that in-
stead of trying to provide a general, configurable
annotation tool (which is very complex due to its
wide application possibilities), another way to pro-
ceed is to create simple, well designed tool fo-
cused on specific task. Such tool can be then
customized or extended by qualified programmers
without much effort and then provide great effi-
ciency of the annotation process.
Presented version of MMAX2 with its source
code is available at http://zil.ipipan.
waw.pl/MMAX4CORE webpage. We encourage
it’s use and modification for other coreference an-
notation projects.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99987975">
The work reported here was cofounded by the
Computer-based methods for coreference resolu-
tion in Polish texts project financed by the Pol-
ish National Science Centre (contract number
6505/B/T02/2011/40) and by the European Union
from resources of the European Social Fund.
Project PO KL „Information technologies: Re-
search and their interdisciplinary applications”.
</bodyText>
<sectionHeader confidence="0.999022" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999029">
Christoph Müller and Michael Strube. 2006. Multi-
level annotation of linguistic data with mmax2. In
Sabine Braun, Kurt Kohn, and Joybrato Mukher-
jee, editors, Corpus Technology and Language Ped-
agogy: New Resources, New Tools, New Methods,
pages 197–214. Peter Lang, Frankfurt a.M., Ger-
many.
Maciej Ogrodniczuk, Katarzyna Głowi´nska, Mateusz
Kope´c, Agata Savary, and Magdalena Zawisławska.
2013. Polish coreference corpus. In Zygmunt Ve-
tulani, editor, Proceedings of the 6th Language &amp;
Technology Conference: Human Language Tech-
nologies as a Challenge for Computer Science
and Linguistics, pages 494–498, Pozna´n, Poland.
Wydawnictwo Pozna´nskie, Fundacja Uniwersytetu
im. Adama Mickiewicza.
Constantin Or˘asan. 2003. PALinkA: a highly cus-
tomizable tool for discourse annotation. In Proceed-
ings of the 4th SIGdial Workshop on Discourse and
Dialog, pages 39 – 43, Sapporo, Japan, July, 5 -6.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c,
Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu-
jii. 2012. brat: a web-based tool for nlp-assisted
text annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 102–107, Avignon, France, April. Association
for Computational Linguistics.
</reference>
<page confidence="0.998453">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.564918">
<title confidence="0.647774333333333">MMAX2 for coreference annotation Mateusz Institute of Computer Science, Polish Academy of</title>
<author confidence="0.771284">Jana Kazimierza</author>
<email confidence="0.994541">m.kopec@ipipan.waw.pl</email>
<abstract confidence="0.9966539">This article presents major modifications in the MMAX2 manual annotation tool, which were implemented for the coreference annotation of Polish texts. Among other, a new feature of adjudication is described, as well as some general insight into the manual annotation tool selection process for the natural language processing tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christoph Müller</author>
<author>Michael Strube</author>
</authors>
<title>Multilevel annotation of linguistic data with mmax2.</title>
<date>2006</date>
<booktitle>Corpus Technology and Language Pedagogy: New Resources,</booktitle>
<pages>197--214</pages>
<editor>In Sabine Braun, Kurt Kohn, and Joybrato Mukherjee, editors,</editor>
<location>New Tools, New Methods,</location>
<contexts>
<context position="2304" citStr="Müller and Strube, 2006" startWordPosition="362" endWordPosition="365"> constant internet access, 3) that they do not have preference. Only one person chose online tool and one person chose the third option, leaving no choice for the annotation task organizers other than to prepare a desktop application. The drawback of this approach was the need to manage text distribution among the annotators, as all the work was done on local computers. Distribution was controlled by the DistSys application, available at the webpage zil.ipipan.waw.pl/DistSys. After some analysis of the features required by the project’s scope, the choice was narrowed to only two tools: MMAX2 (Müller and Strube, 2006) and Palinka (Or˘asan, 2003). The problem with the latter was that is was not error-prone, and lack of publicly available sources did not allow to make project-specific corrections. Therefore MMAX2 environment was chosen for the manual coreference annotation. It is a general purpose cross-platform desktop annotation tool (written in Java), created to be configurable for many natural language annotation efforts. It’s source is publicly available at the webpage mmax2.net. MMAX2 interface consists of the main window with the text being annotated (see for example figure 5) and several smaller wind</context>
</contexts>
<marker>Müller, Strube, 2006</marker>
<rawString>Christoph Müller and Michael Strube. 2006. Multilevel annotation of linguistic data with mmax2. In Sabine Braun, Kurt Kohn, and Joybrato Mukherjee, editors, Corpus Technology and Language Pedagogy: New Resources, New Tools, New Methods, pages 197–214. Peter Lang, Frankfurt a.M., Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maciej Ogrodniczuk</author>
<author>Katarzyna Głowi´nska</author>
<author>Mateusz Kope´c</author>
<author>Agata Savary</author>
<author>Magdalena Zawisławska</author>
</authors>
<title>Polish coreference corpus.</title>
<date>2013</date>
<booktitle>Proceedings of the 6th Language &amp; Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics,</booktitle>
<pages>494--498</pages>
<editor>In Zygmunt Vetulani, editor,</editor>
<location>Pozna´n,</location>
<marker>Ogrodniczuk, Głowi´nska, Kope´c, Savary, Zawisławska, 2013</marker>
<rawString>Maciej Ogrodniczuk, Katarzyna Głowi´nska, Mateusz Kope´c, Agata Savary, and Magdalena Zawisławska. 2013. Polish coreference corpus. In Zygmunt Vetulani, editor, Proceedings of the 6th Language &amp; Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics, pages 494–498, Pozna´n, Poland. Wydawnictwo Pozna´nskie, Fundacja Uniwersytetu im. Adama Mickiewicza.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantin Or˘asan</author>
</authors>
<title>PALinkA: a highly customizable tool for discourse annotation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th SIGdial Workshop on Discourse and Dialog, pages 39 – 43,</booktitle>
<volume>5</volume>
<pages>6</pages>
<location>Sapporo, Japan,</location>
<marker>Or˘asan, 2003</marker>
<rawString>Constantin Or˘asan. 2003. PALinkA: a highly customizable tool for discourse annotation. In Proceedings of the 4th SIGdial Workshop on Discourse and Dialog, pages 39 – 43, Sapporo, Japan, July, 5 -6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
<author>Sampo Pyysalo</author>
<author>Goran Topi´c</author>
<author>Tomoko Ohta</author>
<author>Sophia Ananiadou</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>brat: a web-based tool for nlp-assisted text annotation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>102--107</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<marker>Stenetorp, Pyysalo, Topi´c, Ohta, Ananiadou, Tsujii, 2012</marker>
<rawString>Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c, Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsujii. 2012. brat: a web-based tool for nlp-assisted text annotation. In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 102–107, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>