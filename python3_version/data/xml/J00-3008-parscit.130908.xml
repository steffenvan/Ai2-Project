<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032469">
<note confidence="0.788553">
Computational Linguistics Volume 26, Number 3
</note>
<title confidence="0.984202">
Natural Language Information Retrieval
</title>
<author confidence="0.865888">
Tomek Strzalkowski (editor)
</author>
<affiliation confidence="0.7737045">
(General Electric, Research and Development)
Dordrecht: Kluwer Academic
</affiliation>
<bodyText confidence="0.3067378">
Publishers (Text, speech and language
technology series, edited by Nancy Ide
and Jean Veronis, volume 7), 1999,
xxv+384 pp; hardbound, ISBN
0-7923-5685-3, $144.00, Â£84, Dfl 240.00
</bodyText>
<figure confidence="0.634982666666667">
Reviewed by
Simon Corston-Oliver
Microsoft Research
</figure>
<bodyText confidence="0.998304228571429">
Information Retrieval (IR) in this collection of 14 original papers is broadly construed
to include document retrieval, information extraction, question answering, clustering
and classification. In the introduction Strzalkowski asks the provocative question Why
hasn&apos;t NLP had more success in IR?, the answer to which ought to be of interest to
readers of Computational Linguistics. Unfortunately, the majority of the papers in the
book completely fail to address this question, and several of the papers do not discuss
applications of NLP to IR at all. A brief summary of the papers demonstrates the range
of topics covered.
Karen Sparck Jones, in &amp;quot;What is the role of NLP in text retrieval?,&amp;quot; gives an
overview of linguistically motivated indexing (LMI) and nonlinguistic indexing (NLI).
LMI is not suitable for queries with few words, yet as more words are added, the
conjunction of search terms benefits NLI, raising the bar against which LMI is be-
ing compared. Sparck Jones concludes that LMI might still be useful for displaying
informative information about documents.
Christian Jacquemin and Evelyne Tzoukermann, in &amp;quot;NLP for term variant extrac-
tion: Synergy between morphology, lexicon, and syntax,&amp;quot; perform phrase normaliza-
tion on the basis of full morphological analysis and patterns over parts of speech and
syntactic constituents. They provide an overview of finite-state automata for morpho-
logical analysis and rule ordering for derivational affixation in French, with a tangential
section on Spanish.
Gerda Ruge, in &amp;quot;Combining corpus linguistics and human memory models for
automatic term association,&amp;quot; draws on psycholinguistic research to improve models
of spreading activation within a semantic network sensitive to head/modifier rela-
tionships.
Alan F. Smeaton, in &amp;quot;Using NLP or NLP resources for information retrieval tasks,&amp;quot;
after experiments with matching entire syntactic analyses for TREC yielded results that
were much worse than traditional tf.idf measures, has experimented with selectively
using NLP resources such as WordNet. Retrieval of picture captions with manual word
sense disambiguation outperforms a tfidf baseline.
Tomek Strzalkowski, Fang Lin, Jin Wang, and Jose Perez-Carballo, in &amp;quot;Evaluating
natural language processing techniques for information retrieval: A TREC perspec-
tive,&amp;quot; search for appropriate ways to weight linguistic and nonlinguistic represen-
tations of document content, and explore expansions of the query based on select-
ing entire paragraphs. Manually selected paragraphs yield substantial gains in preci-
sion.
</bodyText>
<page confidence="0.993354">
460
</page>
<subsectionHeader confidence="0.889826">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.99975518367347">
Jussi Karlgren, in &amp;quot;Stylistic experiments in information retrieval,&amp;quot; uses automated
genre classification to rerank documents so that documents of the type preferred by
human judges are more highly weighted.
Ellen Riloff and Jeffrey Lorenzen, in &amp;quot;Extraction-based text categorization: Gener-
ating domain-specific role relationships automatically,&amp;quot; describe two advances in infor-
mation extraction: a method for generating extraction patterns automatically, without
the need for a corpus of manual annotations, and a new variation on their augmented
relevancy signatures algorithm that does not rely on semantic features.
Yorick Wilks and Robert Gaizauskas, in &amp;quot;LARE jumps the gate,&amp;quot; provide a technical
description of the components of GATE, elements of which were used in the LASIE MUC
system.
Joe Zhou, in &amp;quot;Phrasal terms in real-world IR applications,&amp;quot; describes refinements
of algorithms for extracting domain-specific phrases and prototype systems that use
these phrases for summarization and categorization.
Paul Thompson and Christopher C. Dozier, in &amp;quot;Name recognition and retrieval
performance,&amp;quot; apply mature name recognition technology to TREC 1996 queries that
contain personal names. They achieve improvements in precision by weighting the
names differently from other terms.
Jim Cowie, in &amp;quot;Collage: An NLP toolset to support Boolean retrieval,&amp;quot; provides a
technical description of a toolkit for automatic analysis of topics in NL form.
Louise Guthrie, Joe Guthrie, and James Leistensnider, in &amp;quot;Document classifica-
tion and routing: A probabilistic approach,&amp;quot; perform statistical document classifica-
tion based on term occurrence. They augment the classification scores if the docu-
ments satisfy a Boolean specification of the words that members of a given class must
contain.
Julian M. Kupiec, in &amp;quot;MURAX: Finding and organizing answers from text search,&amp;quot;
performs conventional information retrieval followed by shallow linguistic analysis
within the returned set of documents to answer Trivial Pursuit questions using Grolier&apos;s
Encyclopedia. Different strategies are used according to the question type.
Marti A. Hearst, in &amp;quot;The use of categories and clusters for organizing retrieval
results,&amp;quot; provides an overview of approaches to classification and clustering and the
UI problems associated with presenting results to users.
For the most part, the papers are coherent. Many authors have used the space
available to them to combine a solid exposition of material that occurs elsewhere in
the literature with the presentation of exciting new directions. What this collection
lacks is a unifying theme.
As noted above, several of the papers do not address the role of NLP in IR at
all. In a few cases, authors acknowledge the rubric of the book only in order to make
the point that they are not interested in NLP. Hearst, for example, comments (p. 336),
&amp;quot;Although this chapter is part of a book about natural language processing and infor-
mation retrieval, it does not discuss in detail the natural language processing aspects of
categorization and clustering.&amp;quot; Similarly, Guthrie, Guthrie, and Leistensnider (p. 290)
state:
Since the theme of this book is the incorporation of the techniques
of NLP into the problems associated with information retrieval, the
reader might incorrectly assume that in this paper we are attempting
to &amp;quot;understand&amp;quot; a document or portions of a document in much the
same way as may be necessary for machine translation or for summa-
rization.
</bodyText>
<page confidence="0.998305">
461
</page>
<note confidence="0.743501">
Computational Linguistics Volume 26, Number 3
</note>
<bodyText confidence="0.977611125">
Instead, Guthrie, Guthrie, and Leistensnider suggest that mathematical modeling of
terms amounts to a coarser-grained representation of meaning that suffices for classi-
fication.
Since most papers do not address Strzalkowski&apos;s question, it is as well that he
suggests a few possible answers of his own: NLP analyses are simply not deep enough,
or perhaps the traditional focus on precision and recall causes us to overlook new
areas where NLP might be more appropriate. Smeaton proposes a peculiar answer:
NLP systems have historically been tailored to machine translation, which he views
as an easier task than IR, and are therefore difficult to apply to IR.
The closest thing in this collection to a majority answer to Strzalkowski&apos;s question
is the view that permeates many of the papers in this volume: NLP is believed to be
simply too computationally expensive or too poor in quality to be practical, especially
for commercially feasible systems. Zhou, for example, considers even part-of-speech
tagging too computationally expensive in the face of enormous textual databases.
Even for pure research systems such concerns appear paramount. Kupiec, for example,
concedes that more sophisticated NL analysis might be helpful for question answering,
but emphasizes that the less sophisticated analyses he uses yield response times of
less than four seconds per question.
As we witness the incremental improvements in the field achieved by limiting NL
analyses to what is currently tractable, we naturally wonder what the upper bound is
for those techniques. What performance might we expect from more expensive, deeper
analyses? Fortunately, some researchers are willing to look to the horizon to determine
how successful NL techniques might become. Strzalkowski et al. and Smeaton, for ex-
ample, even perform analysis by hand to determine the peak theoretical performance
of their algorithms. Establishing which avenues are worth pursuing enables the fo-
cused development of tractable NL analysis techniques.
The lack of a unifying theme and the fact that the papers cover such a wide range
of topics mean that the book as a whole is unlikely to appeal to individual researchers.
Conversely, all researchers in the field of NLP and IR are likely to find something of
interest.
There is one table that is not referred to in the text (p. 143) and typographi-
cal errors abound, including occasional missing words (leading to interesting garden
path phenomena), mistranscribed phonetic symbols, and transposed acronyms. A well-
structured index more than compensates for these minor flaws.
Simon Corston-Oliver is a discourse linguist in the Natural Language Processing Group at Mi-
crosoft Research, Redmond, USA. His research interests include functional, quantitative, and
machine learning approaches to the study of discourse phenomena, with application to sum-
marization, document retrieval, document macro structure, and the study of aboutness. Corston-
Oliver&apos;s address is Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA; e-mail:
simonco@microsoft.com.
</bodyText>
<page confidence="0.998918">
462
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.086005">
<title confidence="0.880628">Computational Linguistics Volume 26, Number 3 Natural Language Information Retrieval</title>
<author confidence="0.978405">Tomek Strzalkowski</author>
<affiliation confidence="0.6471115">(General Electric, Research and Development) Dordrecht: Kluwer Academic</affiliation>
<note confidence="0.693806">Publishers (Text, speech and language technology series, edited by Nancy Ide and Jean Veronis, volume 7), 1999, xxv+384 pp; hardbound, ISBN 0-7923-5685-3, $144.00, Â£84, Dfl 240.00 Reviewed by Simon Corston-Oliver Microsoft Research Information Retrieval (IR) in this collection of 14 original papers is broadly construed to include document retrieval, information extraction, question answering, clustering</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>