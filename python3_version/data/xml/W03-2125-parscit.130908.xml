<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.060868">
<bodyText confidence="0.865059333333333">
A procedure assistant for astronauts
in a functional programming architecture,
with step previewing and spoken correction of dialogue moves
</bodyText>
<author confidence="0.951197">
Gregory Aist1, Manny Rayner1, John Dowding1,
Beth Ann Hockey1, Susana Early2, and Jim Hieronymus3
</author>
<affiliation confidence="0.983396666666667">
1Research Institute for Advanced Computer Science
2Foothill/DeAnza College
3NASA Ames Research Center
</affiliation>
<address confidence="0.878916">
M/S T35B-1, Moffett Field CA 94035
</address>
<email confidence="0.986667">
{aist, mrayner, jdowding, bahockey, jimh}@riacs.edu; searly@mail.arc.nasa.gov
</email>
<sectionHeader confidence="0.994232" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999860125">
We present a demonstration of a proto-
type system aimed at providing support
with procedural tasks for astronauts on
board the International Space Station.
Current functionality includes navigation
within the procedure, previewing steps,
requesting a list of images or a particular
image, recording voice notes and spoken
alarms, setting parameters such as audio
volume. Dialogue capabilities include
handling spoken corrections for an entire
dialogue move, reestablishing context in
response to a user request, responding to
user barge-in, and help on demand. The
current system has been partially reim-
plemented for better efficiency and in re-
sponse to feedback from astronauts and
astronaut training personnel. Added fea-
tures include visual and spoken step pre-
viewing, and spoken correction of
dialogue moves. The intention is to intro-
duce the system into astronaut training as
a prelude to flight on board the Interna-
tional Space Station.
</bodyText>
<sectionHeader confidence="0.998224" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99973706060606">
Astronauts on board the International Space Sta-
tion engage in a wide variety of tasks on orbit in-
cluding medical procedures, extra vehicular
activity (E V A), scientific payloads, and station
repair and maintenance. These human space flight
activities require extensive and thorough proce-
dures. These procedures are written down in the
form of a number of steps and, with various notes,
cautions, and warnings interspersed throughout the
procedure. Each step may have one or more sub
steps. Procedures also include branch points, call-
outs to other procedures, and instructions to com-
municate with mission control. Since December
2001, the RIALIST group has been developing a
spoken dialogue system for providing assistance
with space station procedures. Aist and Hockey
(2002) and Aist et al. (2002) described the first
version of the system, which operated on a simpli-
fied (and invented) procedure for unpacking and
operating a digital camera and included speech
input and speech output only. Aist et al. (2003)
described a second version of the system with an
XML-based display, and that included support for
not only procedures, but also voice notes and re-
corded alarms, and parameter settings such as in-
creasing and decreasing volume. In this paper, we
describe the third version of the system, with a
reimplemented architecture based on a functional
specification of the domain-specific aspects of the
system combined with an event-driven generic ar-
chitectural framework. We also describe two new
features: previewing of steps, and spoken correc-
tion of dialogue moves.
</bodyText>
<sectionHeader confidence="0.918117" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.99990625">
The March 2003 version of the Intelligent Proce-
dure Assistant is shown in Figure 1, just after
loading a procedure. The March 2003 version pro-
vides the following functions:
Loading a procedure by specifying its name, for
example, “Load water procedure.”
Sequential navigation through individual steps, for
example, “Next step” or “Previous step.”
Navigation to arbitrary steps, for example, “Go to
step two point one.”
Setting system parameters, such as “Increase vol-
ume” or “Decrease volume.”
Handling annotations, such as voice notes or
alarms (“Record a voice note”), or pictures (“Show
the small waste water bag.”).
Previewing steps; for example, “Read step three”.
Issuing spoken corrections (of entire commands),
for example, “I meant go to step two.”
We will discuss previewing steps and issuing spo-
ken corrections in turn.
</bodyText>
<subsectionHeader confidence="0.998534">
2.1 Previewing steps (Reading mode)
</subsectionHeader>
<bodyText confidence="0.999975090909091">
Besides acting on the current step, astronauts indi-
cated that they would like a spoken preview of the
next step. Currently this functionality is imple-
mented as displaying a second procedure window
in the upper right corner of the screen. Further-
more, steps are prefixed with a spoken indication
of previewing, for example, “Reading mode. Note
before step two...” To transition back into normal
(execution) mode, the user may say “Stop read-
ing.” Figure 2 shows the resulting display for the
reading mode.
</bodyText>
<subsectionHeader confidence="0.998408">
2.2 Issuing spoken corrections
</subsectionHeader>
<bodyText confidence="0.999852461538462">
In the March 2003 version of the Checklist system,
the user may issue a spoken correction in the case
of an incorrectly given command, or in the case of
a speech recognition error (e.g. “read me step
three” ® “repeat step three”). The dialogue history
is represented as a list of the prior dialogue states.
Currently we model a correction as a change in the
information state, a rollback of the previous action
plan, and then an application of the new action
plan. Figure 3 shows the display after issuing a
correction, “I meant the wash cloth”. Reading
mode has been exited, and a picture of the wash-
cloth is displayed.
</bodyText>
<figureCaption confidence="0.9995388">
Figure 1. Loading a procedure.
Figure 2. Preview mode, step three.
Figure 3. A subsequent correction, resulting in a
return to execution mode, and the implementation
of the other command.
</figureCaption>
<figure confidence="0.997958722222222">
Speech
Recognizer
Input
Manager
I: input 0 event
Parser
Dialogue
Manager
D: (event, state)
0 (action, state)
Visual
Display
O: action
0 (output, inverse)
Output
Manager
Speech
Synthesizer
</figure>
<figureCaption confidence="0.999346">
Figure 4. Checklist dialogue system architecture.
</figureCaption>
<bodyText confidence="0.880441818181818">
3 Architecture, or, How to write
a dialogue system in three easy steps
There are three main sections to the dialogue han-
dling code: the input manager, dialogue manager,
and the output manager (Figure 4). These are
similar divisions to those proposed in Allen et al.
(2000). Here, we also adopt a further division of
the code into application-specific code and generic
code. Application-specific code computes the fol-
lowing function for each component, as a compila-
tion step:
</bodyText>
<equation confidence="0.97684275">
Input manager: Input 0 Event
Dialogue manager: (Event, State)
0 (Action, State)
Output manager: Action 0 (Output, Inverse)
</equation>
<bodyText confidence="0.961631575757576">
The Output and Inverse computed by the Input
manager are the multimodal output plans and their
multimodal inverses, respectively. The multi-
modal inverses are used when applying a correc-
tion – in conjunction with a return to a previous
state on the history list.
The generic code is an interpretation (or execu-
tion) step; the input manager’s code collects in-
coming events and dispatches the events to the
dialogue manager. The dialogue manager’s code
collects the incoming events, retrieves the previous
state, applies the application-specific function,
saves the new state, and then dispatches the new
action. The output manager takes the action, ap-
plies the application-specific function to compute
the output and its inverse, and then dispatches the
output plan one action at a time. Each action is rep-
resented as an OAA solvable, and dispatched se-
quentially to be handled by the appropriate agent
such as the text-to-speech agent.
The entire dialogue manager is side-effectfree.
(With the minor exception of loading a procedure
file, which causes a change in the “last accessed”
time of the file.) In a more typical dialogue system
architecture such as that shown in Figure 5, the
side effects are represented separately. The inte-
gration of side effects into the output plan has
positive benefits for robustness, since they will be
represented in one place (and thus modified at the
same time when programming changes are made).
Figure 5. A more typical dialogue system ar-
chitecture, with the side effects executed separately
from the spoken output.
</bodyText>
<sectionHeader confidence="0.950042" genericHeader="method">
4 Related Research and Future Work
</sectionHeader>
<bodyText confidence="0.98931325">
Rudnicky, Reed, and Thayer (1996) describe a
system for supporting vehicle maintenance with
speech interfaces. Schreckenghost et al. (2003)
describe a scenario involving similar tasks (life
</bodyText>
<figure confidence="0.9715135">
Parser
Input
Manager
Speech
Recognizer
Dialogue
Manager
Output
Manager
DB
Speech
Synthesizer
</figure>
<bodyText confidence="0.9991308125">
support / maintenance related) but with the com-
puter in more control of the actual task. S &amp; K
Electronics (n.d.) mention a procedure develop-
ment environment for rapidly developing and veri-
fying on-orbit procedures
(http://sk-web.sk-tech.com/proj.html).
Possible future work includes adding procedures
involving inventory management and robot arm
assistance, automating dialogue system construc-
tion from XML procedures, integrating with te-
lemetry to monitor execution of the procedure and
develop error recovery options, improving natural-
ness of the speech output, modeling dialogue to
include dialogue moves and expected user re-
sponses, and improving speech recognition to be
robust to ISS noise.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995874911764706">
G. Aist. J. Dowding, B. A. Hockey, and J. Hieronymus.
2002. An intelligent procedure assistant for astronaut
training and support. Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, refereed demonstration track.
G. Aist and B. A. Hockey. 2002. Generating Training
and Assistive Dialogues for Astronauts from Interna-
tional Space Station Technical Documentation. ITS
2002 Workshop on Integrating Technical and Train-
ing Documentation. Presented along with system
demonstration.
G. Aist, J. Dowding, B. A. Hockey, M. Rayner, J. Hi-
eronymus, D. Bohus, B. Boven, N. Blaylock, E.
Campana, S. Early, G. Gorrell, and S. Phan. 2003.
European Association for Computational Linguistics
(EACL) 2003 meeting, Software Demonstration, Bu-
dapest, Hungary, April 2003.
J. Allen, D. Byron, M. Dzikovska, G. Ferguson, L.
Galescu, and A. Stent. 2000. An architecture for a
generic dialogue shell. Natural Language Engineer-
ing, Special issue on Best Practice in Spoken Lan-
guage Dialogue Systems Engineering, pp. 323-340.
A. Rudnicky, S. Reed, and E. H. Thayer. 1996.
SpeechWear: A mobile speech system.
http://www.speech.cs.cmu.edu/air/papers/speechwear.ps
D. Schreckenghost, C. Thronesbery, P. Bonasso, D.
Kortenkamp and C. Martin, Intelligent Control of
Life Support for Space Missions, in IEEE Intelligent
Systems Magazine, September/October, 2002.
Portions of the dialogue systems described in this paper
were constructed with Rayner, Hockey, and Dowding’s
Regulus open source toolkit. Interested readers may find
the toolkit and supporting documentation online at:
http://sourceforge.net/projects/regulus/.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.352803">
<title confidence="0.997577">A procedure assistant for in a functional programming architecture, with step previewing and spoken correction of dialogue moves</title>
<author confidence="0.888267">Manny John Ann Susana</author>
<author confidence="0.888267">Jim</author>
<affiliation confidence="0.9013175">Institute for Advanced Computer Ames Research</affiliation>
<address confidence="0.817736">M/S T35B-1, Moffett Field CA</address>
<email confidence="0.966994">mrayner,jdowding,bahockey,jimh}@riacs.edu;searly@mail.arc.nasa.gov</email>
<abstract confidence="0.98343732">We present a demonstration of a prototype system aimed at providing support with procedural tasks for astronauts on board the International Space Station. Current functionality includes navigation within the procedure, previewing steps, requesting a list of images or a particular image, recording voice notes and spoken alarms, setting parameters such as audio volume. Dialogue capabilities include handling spoken corrections for an entire dialogue move, reestablishing context in response to a user request, responding to user barge-in, and help on demand. The current system has been partially reimplemented for better efficiency and in response to feedback from astronauts and astronaut training personnel. Added features include visual and spoken step previewing, and spoken correction of dialogue moves. The intention is to introduce the system into astronaut training as a prelude to flight on board the International Space Station.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Dowding</author>
<author>B A Hockey</author>
<author>J Hieronymus</author>
</authors>
<title>An intelligent procedure assistant for astronaut training and support.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, refereed demonstration track.</booktitle>
<marker>Dowding, Hockey, Hieronymus, 2002</marker>
<rawString>G. Aist. J. Dowding, B. A. Hockey, and J. Hieronymus. 2002. An intelligent procedure assistant for astronaut training and support. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, refereed demonstration track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Aist</author>
<author>B A Hockey</author>
</authors>
<title>Generating Training and Assistive Dialogues for Astronauts from International Space Station Technical Documentation.</title>
<date>2002</date>
<booktitle>ITS 2002 Workshop on Integrating Technical and</booktitle>
<contexts>
<context position="2194" citStr="Aist and Hockey (2002)" startWordPosition="319" endWordPosition="322"> A), scientific payloads, and station repair and maintenance. These human space flight activities require extensive and thorough procedures. These procedures are written down in the form of a number of steps and, with various notes, cautions, and warnings interspersed throughout the procedure. Each step may have one or more sub steps. Procedures also include branch points, callouts to other procedures, and instructions to communicate with mission control. Since December 2001, the RIALIST group has been developing a spoken dialogue system for providing assistance with space station procedures. Aist and Hockey (2002) and Aist et al. (2002) described the first version of the system, which operated on a simplified (and invented) procedure for unpacking and operating a digital camera and included speech input and speech output only. Aist et al. (2003) described a second version of the system with an XML-based display, and that included support for not only procedures, but also voice notes and recorded alarms, and parameter settings such as increasing and decreasing volume. In this paper, we describe the third version of the system, with a reimplemented architecture based on a functional specification of the </context>
</contexts>
<marker>Aist, Hockey, 2002</marker>
<rawString>G. Aist and B. A. Hockey. 2002. Generating Training and Assistive Dialogues for Astronauts from International Space Station Technical Documentation. ITS 2002 Workshop on Integrating Technical and Training Documentation. Presented along with system demonstration.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Aist</author>
<author>J Dowding</author>
<author>B A Hockey</author>
<author>M Rayner</author>
<author>J Hieronymus</author>
<author>D Bohus</author>
<author>B Boven</author>
<author>N Blaylock</author>
<author>E Campana</author>
<author>S Early</author>
<author>G Gorrell</author>
<author>S Phan</author>
</authors>
<date>2003</date>
<booktitle>European Association for Computational Linguistics (EACL) 2003 meeting, Software Demonstration,</booktitle>
<location>Budapest, Hungary,</location>
<contexts>
<context position="2430" citStr="Aist et al. (2003)" startWordPosition="359" endWordPosition="362">s, and warnings interspersed throughout the procedure. Each step may have one or more sub steps. Procedures also include branch points, callouts to other procedures, and instructions to communicate with mission control. Since December 2001, the RIALIST group has been developing a spoken dialogue system for providing assistance with space station procedures. Aist and Hockey (2002) and Aist et al. (2002) described the first version of the system, which operated on a simplified (and invented) procedure for unpacking and operating a digital camera and included speech input and speech output only. Aist et al. (2003) described a second version of the system with an XML-based display, and that included support for not only procedures, but also voice notes and recorded alarms, and parameter settings such as increasing and decreasing volume. In this paper, we describe the third version of the system, with a reimplemented architecture based on a functional specification of the domain-specific aspects of the system combined with an event-driven generic architectural framework. We also describe two new features: previewing of steps, and spoken correction of dialogue moves. 2 System Description The March 2003 ve</context>
</contexts>
<marker>Aist, Dowding, Hockey, Rayner, Hieronymus, Bohus, Boven, Blaylock, Campana, Early, Gorrell, Phan, 2003</marker>
<rawString>G. Aist, J. Dowding, B. A. Hockey, M. Rayner, J. Hieronymus, D. Bohus, B. Boven, N. Blaylock, E. Campana, S. Early, G. Gorrell, and S. Phan. 2003. European Association for Computational Linguistics (EACL) 2003 meeting, Software Demonstration, Budapest, Hungary, April 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>D Byron</author>
<author>M Dzikovska</author>
<author>G Ferguson</author>
<author>L Galescu</author>
<author>A Stent</author>
</authors>
<title>An architecture for a generic dialogue shell.</title>
<date>2000</date>
<booktitle>Natural Language Engineering, Special issue on Best Practice in Spoken Language Dialogue Systems Engineering,</booktitle>
<pages>323--340</pages>
<contexts>
<context position="5734" citStr="Allen et al. (2000)" startWordPosition="893" endWordPosition="896">subsequent correction, resulting in a return to execution mode, and the implementation of the other command. Speech Recognizer Input Manager I: input 0 event Parser Dialogue Manager D: (event, state) 0 (action, state) Visual Display O: action 0 (output, inverse) Output Manager Speech Synthesizer Figure 4. Checklist dialogue system architecture. 3 Architecture, or, How to write a dialogue system in three easy steps There are three main sections to the dialogue handling code: the input manager, dialogue manager, and the output manager (Figure 4). These are similar divisions to those proposed in Allen et al. (2000). Here, we also adopt a further division of the code into application-specific code and generic code. Application-specific code computes the following function for each component, as a compilation step: Input manager: Input 0 Event Dialogue manager: (Event, State) 0 (Action, State) Output manager: Action 0 (Output, Inverse) The Output and Inverse computed by the Input manager are the multimodal output plans and their multimodal inverses, respectively. The multimodal inverses are used when applying a correction – in conjunction with a return to a previous state on the history list. The generic </context>
</contexts>
<marker>Allen, Byron, Dzikovska, Ferguson, Galescu, Stent, 2000</marker>
<rawString>J. Allen, D. Byron, M. Dzikovska, G. Ferguson, L. Galescu, and A. Stent. 2000. An architecture for a generic dialogue shell. Natural Language Engineering, Special issue on Best Practice in Spoken Language Dialogue Systems Engineering, pp. 323-340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rudnicky</author>
<author>S Reed</author>
<author>E H Thayer</author>
</authors>
<title>SpeechWear: A mobile speech system. http://www.speech.cs.cmu.edu/air/papers/speechwear.ps</title>
<date>1996</date>
<marker>Rudnicky, Reed, Thayer, 1996</marker>
<rawString>A. Rudnicky, S. Reed, and E. H. Thayer. 1996. SpeechWear: A mobile speech system. http://www.speech.cs.cmu.edu/air/papers/speechwear.ps</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Schreckenghost</author>
<author>C Thronesbery</author>
<author>P Bonasso</author>
<author>D Kortenkamp</author>
<author>C Martin</author>
</authors>
<title>Intelligent Control of Life Support for Space Missions,</title>
<date>2002</date>
<booktitle>in IEEE Intelligent Systems Magazine, September/October,</booktitle>
<marker>Schreckenghost, Thronesbery, Bonasso, Kortenkamp, Martin, 2002</marker>
<rawString>D. Schreckenghost, C. Thronesbery, P. Bonasso, D. Kortenkamp and C. Martin, Intelligent Control of Life Support for Space Missions, in IEEE Intelligent Systems Magazine, September/October, 2002. Portions of the dialogue systems described in this paper were constructed with Rayner, Hockey, and Dowding’s Regulus open source toolkit. Interested readers may find the toolkit and supporting documentation online at: http://sourceforge.net/projects/regulus/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>