<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025514">
<title confidence="0.995477">
Discriminative Feature-Tied Mixture Modeling for Statistical Machine
Translation
</title>
<author confidence="0.798018">
Bing Xiang and Abraham Ittycheriah
</author>
<note confidence="0.472881">
IBM T. J. Watson Research Center
</note>
<address confidence="0.533037">
Yorktown Heights, NY 10598
</address>
<email confidence="0.99676">
{bxiang,abei}@us.ibm.com
</email>
<sectionHeader confidence="0.998661" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99530975">
In this paper we present a novel discrimi-
native mixture model for statistical machine
translation (SMT). We model the feature space
with a log-linear combination of multiple mix-
ture components. Each component contains a
large set of features trained in a maximum-
entropy framework. All features within the
same mixture component are tied and share
the same mixture weights, where the mixture
weights are trained discriminatively to max-
imize the translation performance. This ap-
proach aims at bridging the gap between the
maximum-likelihood training and the discrim-
inative training for SMT. It is shown that the
feature space can be partitioned in a vari-
ety of ways, such as based on feature types,
word alignments, or domains, for various ap-
plications. The proposed approach improves
the translation performance significantly on a
large-scale Arabic-to-English MT task.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999742111111111">
Significant progress has been made in statisti-
cal machine translation (SMT) in recent years.
Among all the proposed approaches, the phrase-
based method (Koehn et al., 2003) has become the
widely adopted one in SMT due to its capability
of capturing local context information from adja-
cent words. There exists significant amount of work
focused on the improvement of translation perfor-
mance with better features. The feature set could be
either small (at the order of 10), or large (up to mil-
lions). For example, the system described in (Koehn
et al., 2003) is a widely known one using small num-
ber of features in a maximum-entropy (log-linear)
model (Och and Ney, 2002). The features include
phrase translation probabilities, lexical probabilities,
number of phrases, and language model scores, etc.
The feature weights are usually optimized with min-
imum error rate training (MERT) as in (Och, 2003).
Besides the MERT-based feature weight opti-
mization, there exist other alternative discriminative
training methods for MT, such as in (Tillmann and
Zhang, 2006; Liang et al., 2006; Blunsom et al.,
2008). However, scalability is a challenge for these
approaches, where all possible translations of each
training example need to be searched, which is com-
putationally expensive.
In (Chiang et al., 2009), there are 11K syntac-
tic features proposed for a hierarchical phrase-based
system. The feature weights are trained with the
Margin Infused Relaxed Algorithm (MIRA) effi-
ciently on a forest of translations from a develop-
ment set. Even though significant improvement has
been obtained compared to the baseline that has
small number of features, it is hard to apply the
same approach to millions of features due to the data
sparseness issue, since the development set is usu-
ally small.
In (Ittycheriah and Roukos, 2007), a maximum
entropy (ME) model is proposed, which utilizes mil-
lions of features. All the feature weights are trained
with a maximum-likelihood (ML) approach on the
full training corpus. It achieves significantly bet-
ter performance than a normal phrase-based system.
However, the estimation of feature weights has no
direct connection with the final translation perfor-
</bodyText>
<page confidence="0.987484">
424
</page>
<note confidence="0.586973">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 424–428,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9983678">
mance.
In this paper, we propose a hybrid framework, a
discriminative mixture model, to bridge the gap be-
tween the ML training and the discriminative train-
ing for SMT. In Section 2, we briefly review the ME
baseline of this work. In Section 3, we introduce the
discriminative mixture model that combines various
types of features. In Section 4, we present experi-
mental results on a large-scale Arabic-English MT
task with focuses on feature combination, alignment
combination, and domain adaptation, respectively.
Section 5 concludes the paper.
where Zk is a normalizing factor for cluster k.
We propose a log-linear mixture model as shown
in Eq. (3).
</bodyText>
<equation confidence="0.9708778125">
p(t,j|s) = p0(t, j|s) Y pk(t, j|s)�k. (3)
Z(s)
k
It can be rewritten in the log domain as
logp(t, j|s) = log p0(t,j|s)
Z(s)
wk log pk(t, j|s)
X
+
k
log p0(t( j|s) _ X wk log Zk(s)
k
X Xwk λkiφki(t, j, s). (4)
+ i
k
2 Maximum-Entropy Model for MT
</equation>
<bodyText confidence="0.99970725">
In this section we give a brief review of a special
maximum-entropy (ME) model as introduced in (It-
tycheriah and Roukos, 2007). The model has the
following form,
</bodyText>
<equation confidence="0.99172125">
X
p(t,j|s) = p0(t, j|s)
Z(s) exp λiφi(t,j,s), (1)
i
</equation>
<bodyText confidence="0.9999541875">
where s is a source phrase, and t is a target phrase.
j is the jump distance from the previously translated
source word to the current source word. During
training j can vary widely due to automatic word
alignment in the parallel corpus. To limit the sparse-
ness created by long jumps, j is capped to a win-
dow of source words (-5 to 5 words) around the last
translated source word. Jumps outside the window
are treated as being to the edge of the window. In
Eq. (1), p0 is a prior distribution, Z is a normal-
izing term, and φi(t, j, s) are the features of the
model, each being a binary question asked about the
source, distortion, and target information. The fea-
ture weights λi can be estimated with the Improved
Iterative Scaling (IIS) algorithm (Della Pietra et al.,
1997), a maximum-likelihood-based approach.
</bodyText>
<sectionHeader confidence="0.998095" genericHeader="method">
3 Discriminative Mixture Model
</sectionHeader>
<subsectionHeader confidence="0.97809">
3.1 Mixture Model
</subsectionHeader>
<bodyText confidence="0.9999702">
Now we introduce the discriminative mixture model.
Suppose we partition the feature space into multiple
clusters (details in Section 3.2). Let the probabil-
ity of target phrase and jump given certain source
phrase for cluster k be
</bodyText>
<equation confidence="0.966381333333333">
1 X
pk(t, j|s) = Zk(s)exp λkiφki(t, j, s), (2)
i
</equation>
<bodyText confidence="0.999907533333333">
The individual feature weights λki for the i-th
feature in cluster k are estimated in the maximum-
entropy framework as in the baseline model. How-
ever, the mixture weights wk can be optimized di-
rectly towards the translation evaluation metric, such
as BLEU (Papineni et al., 2002), along with other
usual costs (e.g. language model scores) on a devel-
opment set. Note that the number of mixture com-
ponents is relatively small (less than 10) compared
to millions of features in baseline. Hence the opti-
mization can be conducted easily to generate reliable
mixture weights for decoding with MERT (Och,
2003) or other optimization algorithms, such as
the Simplex Armijo Downhill algorithm proposed
in (Zhao and Chen, 2009).
</bodyText>
<subsectionHeader confidence="0.999968">
3.2 Partition of Feature Space
</subsectionHeader>
<bodyText confidence="0.999433">
Given the proposed mixture model, how to split the
feature space into multiple regions becomes crucial.
In order to surpass the baseline model, where all
features can be viewed as existing in a single mix-
ture component, the separated mixture components
should be complementary to each other. In this
work, we explore three different ways of partitions,
based on either feature types, word alignment types,
or the domain of training data.
In the feature-type-based partition, we split the
ME features into 8 categories:
</bodyText>
<listItem confidence="0.826441">
• F1: Lexical features that examine source word,
target word and jump;
</listItem>
<page confidence="0.92622">
425
</page>
<listItem confidence="0.992499583333334">
• F2: Lexical context features that examine
source word, target word, the previous source
word, the next source word and jump;
• F3: Lexical context features that examine
source word, target word, the previous source
word, the previous target word and jump;
• F4: Lexical context features that examine
source word, target word, the previous or next
source word and jump;
• F5: Segmentation features based on mor-
phological analysis that examine source mor-
phemes, target word and jump;
• F6: Part-of-speech (POS) features that examine
the source and target POS tags and their neigh-
bors, along with target word and jump;
• F7: Source parse tree features that collect the
information from the parse labels of the source
words and their siblings in the parse trees,
along with target word and jump;
• F8: Coverage features that examine the cover-
age status of the source words to the left and
to the right. They fire only if the left source
is open (untranslated) or the right source is
closed.
</listItem>
<bodyText confidence="0.999583846153846">
All the features falling in the same feature cate-
gory/cluster are tied to each other to share the same
mixture weights at the upper level as in Eq. (3).
Besides the feature-type-based clustering, we can
also divide the feature space based on word align-
ment types, such as supervised alignment versus un-
supervised alignment (to be described in the exper-
iment section). For each type of word alignment,
we build a mixture component with millions of ME
features. On the task of domain adaptation, we
can also split the training data based on their do-
main/resources, with each mixture component rep-
resenting a specific domain.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999116">
4.1 Data and Baseline
</subsectionHeader>
<bodyText confidence="0.999332307692308">
We conduct a set of experiments on an Arabic-to-
English MT task. The training data includes the UN
parallel corpus and LDC-released parallel corpora,
with about 10M sentence pairs and 300M words in
total (counted at the English side). For each sentence
in the training, three types of word alignments are
created: maximum entropy alignment (Ittycheriah
and Roukos, 2005), GIZA++ alignment (Och and
Ney, 2000), and HMM alignment (Vogel et al.,
1996). Our tuning and test sets are extracted from
the GALE DEV10 Newswire set, with no overlap
between tuning and test. There are 1063 sentences
(168 documents) in the tuning set, and 1089 sen-
tences (168 documents) in the test set. Both sets
have one reference translation for each sentence. In-
stead of using all the training data, we sample the
training corpus based on the tuning/test set to train
the systems more efficiently. In the end, about 1.5M
sentence pairs are selected for the sampled training.
A 5-gram language model is trained from the En-
glish Gigaword corpus and the English portion of the
parallel corpus used in the translation model train-
ing. In this work, the decoding weights for both
the baseline and the mixture model are tuned with
the Simplex Armijo Downhill algorithm (Zhao and
Chen, 2009) towards the maximum BLEU.
</bodyText>
<table confidence="0.997977">
System Features BLEU
F1 685K 37.11
F2 5516K 38.43
F3 4457K 37.75
F4 3884K 37.56
F5 103K 36.03
F6 325K 37.89
F7 1584K 38.56
F8 1605K 37.49
Baseline 18159K 39.36
Mixture 18159K 39.97
</table>
<tableCaption confidence="0.995428">
Table 1: MT results with individual mixture component
(F1 to F8), baseline, or mixture model.
</tableCaption>
<subsectionHeader confidence="0.968349">
4.2 Feature Combination
</subsectionHeader>
<bodyText confidence="0.999991">
We first experiment with the feature-type-based
clustering as described in Section 3.2. The trans-
lation results on the test set from the baseline and
the mixture model are listed in Table 1. The MT
performance is measured with the widely adopted
BLEU metric. We also evaluate the systems that uti-
lize only one of the mixture components (F1 to F8).
The number of features used in each system is also
</bodyText>
<page confidence="0.998272">
426
</page>
<bodyText confidence="0.999659761904762">
listed in the table. As we can see, when using all
18M features in the baseline model, without mixture
weighting, the baseline achieved 3.3 points higher
BLEU score than F5 (the worst component), and 0.8
higher BLEU score than F7 (the best component).
With the log-linear mixture model, we obtained 0.6
gain compared to the baseline. Since there are ex-
actly the same number of features in the baseline
and mixture model, the better performance is due
to two facts: separate training of the feature weights
A within each mixture component; the discrimina-
tive training of mixture weights w. The first one al-
lows better parameter estimation given the number
of features in each mixture component is much less
than that in the baseline. The second factor connects
the mixture weighting to the final translation perfor-
mance directly. In the baseline, all feature weights
are trained together solely under the maximum like-
lihood criterion, with no differentiation of the vari-
ous types of features in terms of their contribution to
the translation performance.
</bodyText>
<table confidence="0.998252166666667">
System Features BLEU
ME 5687K 39.04
GIZA 5716K 38.75
HMM 5589K 38.65
Baseline 18159K 39.36
Mixture 16992K 39.86
</table>
<tableCaption confidence="0.988847">
Table 2: MT results with different alignments, baseline,
or mixture model.
</tableCaption>
<subsectionHeader confidence="0.998887">
4.3 Alignment Combination
</subsectionHeader>
<bodyText confidence="0.9998363">
In the baseline mentioned above, three types of word
alignments are used (via corpus concatenation) for
phrase extraction and feature training. Given the
mixture model structure, we can apply it to an align-
ment combination problem. With the phrase table
extracted from all the alignments, we train three
feature mixture components, each on one type of
alignments. Each mixture component contains mil-
lions of features from all feature types described in
Section 3.2. Again, the mixture weights are op-
timized towards the maximum BLEU. The results
are shown in Table 2. The baseline system only
achieved 0.3 minor gain compared to extracting fea-
tures from ME alignment only (note that phrases are
from all the alignments). With the mixture model,
we can achieve another 0.5 gain compared to the
baseline, especially with less number of features.
This presents a new way of doing alignment com-
bination in the feature space instead of in the usual
phrase space.
</bodyText>
<table confidence="0.998407333333333">
System Features BLEU
Newswire 8898K 38.82
Weblog 1990K 38.20
UN 4700K 38.21
Baseline 18159K 39.36
Mixture 15588K 39.81
</table>
<tableCaption confidence="0.9924045">
Table 3: MT results with different training sub-corpora,
baseline, or mixture model.
</tableCaption>
<subsectionHeader confidence="0.99459">
4.4 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.99995065">
Another popular task in SMT is domain adapta-
tion (Foster et al., 2010). It tries to take advantage of
any out-of-domain training data by combining them
with the in-domain data in an appropriate way. In
our sub-sampled training corpus, there exist three
subsets: newswire (1M sentences), weblog (200K),
and UN data (300K). We train three mixture com-
ponents, each on one of the training subsets. All re-
sults are compared in Table 3. The baseline that was
trained on all the data achieved 0.5 gain compared to
using the newswire training data alone (understand-
ably it is the best component given the newswire test
data). Note that since the baseline is trained on sub-
sampled training data, there is already certain do-
main adaptation effect involved. On top of that, the
mixture model results in another 0.45 gain in BLEU.
All the improvements in the mixture models above
against the baseline are statistically significant with
p-value &lt; 0.0001 by using the confidence tool de-
scribed in (Zhang and Vogel, 2004).
</bodyText>
<sectionHeader confidence="0.999546" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998667">
In this paper we presented a novel discriminative
mixture model for bridging the gap between the
maximum-likelihood training and the discriminative
training in SMT. We partition the feature space into
multiple regions. The features in each region are tied
together to share the same mixture weights that are
optimized towards the maximum BLEU scores. It
was shown that the same model structure can be ef-
</bodyText>
<page confidence="0.994863">
427
</page>
<bodyText confidence="0.999963142857143">
fectively applied to feature combination, alignment
combination and domain adaptation. We also point
out that it is straightforward to combine any of these
three. For example, we can cluster the features based
on both feature types and alignments. Further im-
provement may be achieved with other feature space
partition approaches in the future.
</bodyText>
<sectionHeader confidence="0.99809" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999838888888889">
We would like to acknowledge the support of
DARPA under Grant HR0011-08-C-0110 for fund-
ing part of this work. The views, opinions, and/or
findings contained in this article/presentation are
those of the author/presenter and should not be in-
terpreted as representing the official views or poli-
cies, either expressed or implied, of the Defense Ad-
vanced Research Projects Agency or the Department
of Defense.
</bodyText>
<sectionHeader confidence="0.998789" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997957620689655">
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings ofACL-08:HLT.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings ofNAACL-HLT.
Stephen Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in satistical machine translation. In Proceedings
ofEMNLP.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In Proceedings of HLT/EMNLP, pages
89–96, October.
Abraham Ittycheriah and Salim Roukos. 2007. Di-
rect translation model 2. In Proceedings HLT/NAACL,
pages 57–64, April.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL/HLT.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proceedings of
ACL/COLING, pages 761–768, Sydney, Australia.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL,
pages 440–447, Hong Kong, China, October.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translations. In Proceedings of ACL,
pages 295–302, Philadelphia, PA, July.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings ofACL,
pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings ofACL,
pages 311–318.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proceedings of ACL/COLING, pages 721–728, Syd-
ney, Australia.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836–841.
Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for the machine translation evalua-
tion metrics. In Proceedings of The 10th International
Conference on Theoretical and Methodological Issues
in Machine Translation.
Bing Zhao and Shengyuan Chen. 2009. A simplex
armijo downhill algorithm for optimizing statistical
machine translation decoding parameters. In Proceed-
ings ofNAACL-HLT.
</reference>
<page confidence="0.998325">
428
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.488299">
<title confidence="0.999092">Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</title>
<author confidence="0.996387">Bing Xiang</author>
<author confidence="0.996387">Abraham</author>
<affiliation confidence="0.753062">IBM T. J. Watson Research Yorktown Heights, NY</affiliation>
<abstract confidence="0.997114">In this paper we present a novel discriminative mixture model for statistical machine translation (SMT). We model the feature space with a log-linear combination of multiple mixture components. Each component contains a large set of features trained in a maximumentropy framework. All features within the same mixture component are tied and share the same mixture weights, where the mixture weights are trained discriminatively to maximize the translation performance. This approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for SMT. It is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications. The proposed approach improves the translation performance significantly on a large-scale Arabic-to-English MT task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08:HLT.</booktitle>
<contexts>
<context position="2195" citStr="Blunsom et al., 2008" startWordPosition="335" endWordPosition="338">(up to millions). For example, the system described in (Koehn et al., 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). The features include phrase translation probabilities, lexical probabilities, number of phrases, and language model scores, etc. The feature weights are usually optimized with minimum error rate training (MERT) as in (Och, 2003). Besides the MERT-based feature weight optimization, there exist other alternative discriminative training methods for MT, such as in (Tillmann and Zhang, 2006; Liang et al., 2006; Blunsom et al., 2008). However, scalability is a challenge for these approaches, where all possible translations of each training example need to be searched, which is computationally expensive. In (Chiang et al., 2009), there are 11K syntactic features proposed for a hierarchical phrase-based system. The feature weights are trained with the Margin Infused Relaxed Algorithm (MIRA) efficiently on a forest of translations from a development set. Even though significant improvement has been obtained compared to the baseline that has small number of features, it is hard to apply the same approach to millions of featur</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proceedings ofACL-08:HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings ofNAACL-HLT.</booktitle>
<contexts>
<context position="2393" citStr="Chiang et al., 2009" startWordPosition="365" endWordPosition="368">res include phrase translation probabilities, lexical probabilities, number of phrases, and language model scores, etc. The feature weights are usually optimized with minimum error rate training (MERT) as in (Och, 2003). Besides the MERT-based feature weight optimization, there exist other alternative discriminative training methods for MT, such as in (Tillmann and Zhang, 2006; Liang et al., 2006; Blunsom et al., 2008). However, scalability is a challenge for these approaches, where all possible translations of each training example need to be searched, which is computationally expensive. In (Chiang et al., 2009), there are 11K syntactic features proposed for a hierarchical phrase-based system. The feature weights are trained with the Margin Infused Relaxed Algorithm (MIRA) efficiently on a forest of translations from a development set. Even though significant improvement has been obtained compared to the baseline that has small number of features, it is hard to apply the same approach to millions of features due to the data sparseness issue, since the development set is usually small. In (Ittycheriah and Roukos, 2007), a maximum entropy (ME) model is proposed, which utilizes millions of features. All</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings ofNAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence.</journal>
<contexts>
<context position="5367" citStr="Pietra et al., 1997" startWordPosition="871" endWordPosition="874"> training j can vary widely due to automatic word alignment in the parallel corpus. To limit the sparseness created by long jumps, j is capped to a window of source words (-5 to 5 words) around the last translated source word. Jumps outside the window are treated as being to the edge of the window. In Eq. (1), p0 is a prior distribution, Z is a normalizing term, and φi(t, j, s) are the features of the model, each being a binary question asked about the source, distortion, and target information. The feature weights λi can be estimated with the Improved Iterative Scaling (IIS) algorithm (Della Pietra et al., 1997), a maximum-likelihood-based approach. 3 Discriminative Mixture Model 3.1 Mixture Model Now we introduce the discriminative mixture model. Suppose we partition the feature space into multiple clusters (details in Section 3.2). Let the probability of target phrase and jump given certain source phrase for cluster k be 1 X pk(t, j|s) = Zk(s)exp λkiφki(t, j, s), (2) i The individual feature weights λki for the i-th feature in cluster k are estimated in the maximumentropy framework as in the baseline model. However, the mixture weights wk can be optimized directly towards the translation evaluation</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in satistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="13232" citStr="Foster et al., 2010" startWordPosition="2174" endWordPosition="2177">cting features from ME alignment only (note that phrases are from all the alignments). With the mixture model, we can achieve another 0.5 gain compared to the baseline, especially with less number of features. This presents a new way of doing alignment combination in the feature space instead of in the usual phrase space. System Features BLEU Newswire 8898K 38.82 Weblog 1990K 38.20 UN 4700K 38.21 Baseline 18159K 39.36 Mixture 15588K 39.81 Table 3: MT results with different training sub-corpora, baseline, or mixture model. 4.4 Domain Adaptation Another popular task in SMT is domain adaptation (Foster et al., 2010). It tries to take advantage of any out-of-domain training data by combining them with the in-domain data in an appropriate way. In our sub-sampled training corpus, there exist three subsets: newswire (1M sentences), weblog (200K), and UN data (300K). We train three mixture components, each on one of the training subsets. All results are compared in Table 3. The baseline that was trained on all the data achieved 0.5 gain compared to using the newswire training data alone (understandably it is the best component given the newswire test data). Note that since the baseline is trained on subsample</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in satistical machine translation. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for arabic-english machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="9094" citStr="Ittycheriah and Roukos, 2005" startWordPosition="1488" endWordPosition="1491">, we build a mixture component with millions of ME features. On the task of domain adaptation, we can also split the training data based on their domain/resources, with each mixture component representing a specific domain. 4 Experiments 4.1 Data and Baseline We conduct a set of experiments on an Arabic-toEnglish MT task. The training data includes the UN parallel corpus and LDC-released parallel corpora, with about 10M sentence pairs and 300M words in total (counted at the English side). For each sentence in the training, three types of word alignments are created: maximum entropy alignment (Ittycheriah and Roukos, 2005), GIZA++ alignment (Och and Ney, 2000), and HMM alignment (Vogel et al., 1996). Our tuning and test sets are extracted from the GALE DEV10 Newswire set, with no overlap between tuning and test. There are 1063 sentences (168 documents) in the tuning set, and 1089 sentences (168 documents) in the test set. Both sets have one reference translation for each sentence. Instead of using all the training data, we sample the training corpus based on the tuning/test set to train the systems more efficiently. In the end, about 1.5M sentence pairs are selected for the sampled training. A 5-gram language m</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for arabic-english machine translation. In Proceedings of HLT/EMNLP, pages 89–96, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>Direct translation model 2.</title>
<date>2007</date>
<booktitle>In Proceedings HLT/NAACL,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="2909" citStr="Ittycheriah and Roukos, 2007" startWordPosition="449" endWordPosition="452">ations of each training example need to be searched, which is computationally expensive. In (Chiang et al., 2009), there are 11K syntactic features proposed for a hierarchical phrase-based system. The feature weights are trained with the Margin Infused Relaxed Algorithm (MIRA) efficiently on a forest of translations from a development set. Even though significant improvement has been obtained compared to the baseline that has small number of features, it is hard to apply the same approach to millions of features due to the data sparseness issue, since the development set is usually small. In (Ittycheriah and Roukos, 2007), a maximum entropy (ME) model is proposed, which utilizes millions of features. All the feature weights are trained with a maximum-likelihood (ML) approach on the full training corpus. It achieves significantly better performance than a normal phrase-based system. However, the estimation of feature weights has no direct connection with the final translation perfor424 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 424–428, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics mance. In this paper, we pro</context>
<context position="4505" citStr="Ittycheriah and Roukos, 2007" startWordPosition="711" endWordPosition="715">rabic-English MT task with focuses on feature combination, alignment combination, and domain adaptation, respectively. Section 5 concludes the paper. where Zk is a normalizing factor for cluster k. We propose a log-linear mixture model as shown in Eq. (3). p(t,j|s) = p0(t, j|s) Y pk(t, j|s)�k. (3) Z(s) k It can be rewritten in the log domain as logp(t, j|s) = log p0(t,j|s) Z(s) wk log pk(t, j|s) X + k log p0(t( j|s) _ X wk log Zk(s) k X Xwk λkiφki(t, j, s). (4) + i k 2 Maximum-Entropy Model for MT In this section we give a brief review of a special maximum-entropy (ME) model as introduced in (Ittycheriah and Roukos, 2007). The model has the following form, X p(t,j|s) = p0(t, j|s) Z(s) exp λiφi(t,j,s), (1) i where s is a source phrase, and t is a target phrase. j is the jump distance from the previously translated source word to the current source word. During training j can vary widely due to automatic word alignment in the parallel corpus. To limit the sparseness created by long jumps, j is capped to a window of source words (-5 to 5 words) around the last translated source word. Jumps outside the window are treated as being to the edge of the window. In Eq. (1), p0 is a prior distribution, Z is a normalizing</context>
</contexts>
<marker>Ittycheriah, Roukos, 2007</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2007. Direct translation model 2. In Proceedings HLT/NAACL, pages 57–64, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL/HLT.</booktitle>
<contexts>
<context position="1265" citStr="Koehn et al., 2003" startWordPosition="185" endWordPosition="188">ize the translation performance. This approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for SMT. It is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications. The proposed approach improves the translation performance significantly on a large-scale Arabic-to-English MT task. 1 Introduction Significant progress has been made in statistical machine translation (SMT) in recent years. Among all the proposed approaches, the phrasebased method (Koehn et al., 2003) has become the widely adopted one in SMT due to its capability of capturing local context information from adjacent words. There exists significant amount of work focused on the improvement of translation performance with better features. The feature set could be either small (at the order of 10), or large (up to millions). For example, the system described in (Koehn et al., 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). The features include phrase translation probabilities, lexical probabilities, number of phrases, and </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL/COLING,</booktitle>
<pages>761--768</pages>
<location>Sydney, Australia.</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of ACL/COLING, pages 761–768, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>440--447</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="9132" citStr="Och and Ney, 2000" startWordPosition="1494" endWordPosition="1497">E features. On the task of domain adaptation, we can also split the training data based on their domain/resources, with each mixture component representing a specific domain. 4 Experiments 4.1 Data and Baseline We conduct a set of experiments on an Arabic-toEnglish MT task. The training data includes the UN parallel corpus and LDC-released parallel corpora, with about 10M sentence pairs and 300M words in total (counted at the English side). For each sentence in the training, three types of word alignments are created: maximum entropy alignment (Ittycheriah and Roukos, 2005), GIZA++ alignment (Och and Ney, 2000), and HMM alignment (Vogel et al., 1996). Our tuning and test sets are extracted from the GALE DEV10 Newswire set, with no overlap between tuning and test. There are 1063 sentences (168 documents) in the tuning set, and 1089 sentences (168 documents) in the test set. Both sets have one reference translation for each sentence. Instead of using all the training data, we sample the training corpus based on the tuning/test set to train the systems more efficiently. In the end, about 1.5M sentence pairs are selected for the sampled training. A 5-gram language model is trained from the English Gigaw</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL, pages 440–447, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translations.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1762" citStr="Och and Ney, 2002" startWordPosition="271" endWordPosition="274">machine translation (SMT) in recent years. Among all the proposed approaches, the phrasebased method (Koehn et al., 2003) has become the widely adopted one in SMT due to its capability of capturing local context information from adjacent words. There exists significant amount of work focused on the improvement of translation performance with better features. The feature set could be either small (at the order of 10), or large (up to millions). For example, the system described in (Koehn et al., 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). The features include phrase translation probabilities, lexical probabilities, number of phrases, and language model scores, etc. The feature weights are usually optimized with minimum error rate training (MERT) as in (Och, 2003). Besides the MERT-based feature weight optimization, there exist other alternative discriminative training methods for MT, such as in (Tillmann and Zhang, 2006; Liang et al., 2006; Blunsom et al., 2008). However, scalability is a challenge for these approaches, where all possible translations of each training example need to be searched, which is computationally expe</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translations. In Proceedings of ACL, pages 295–302, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="1992" citStr="Och, 2003" startWordPosition="306" endWordPosition="307">words. There exists significant amount of work focused on the improvement of translation performance with better features. The feature set could be either small (at the order of 10), or large (up to millions). For example, the system described in (Koehn et al., 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). The features include phrase translation probabilities, lexical probabilities, number of phrases, and language model scores, etc. The feature weights are usually optimized with minimum error rate training (MERT) as in (Och, 2003). Besides the MERT-based feature weight optimization, there exist other alternative discriminative training methods for MT, such as in (Tillmann and Zhang, 2006; Liang et al., 2006; Blunsom et al., 2008). However, scalability is a challenge for these approaches, where all possible translations of each training example need to be searched, which is computationally expensive. In (Chiang et al., 2009), there are 11K syntactic features proposed for a hierarchical phrase-based system. The feature weights are trained with the Margin Infused Relaxed Algorithm (MIRA) efficiently on a forest of transla</context>
<context position="6336" citStr="Och, 2003" startWordPosition="1032" endWordPosition="1033">, (2) i The individual feature weights λki for the i-th feature in cluster k are estimated in the maximumentropy framework as in the baseline model. However, the mixture weights wk can be optimized directly towards the translation evaluation metric, such as BLEU (Papineni et al., 2002), along with other usual costs (e.g. language model scores) on a development set. Note that the number of mixture components is relatively small (less than 10) compared to millions of features in baseline. Hence the optimization can be conducted easily to generate reliable mixture weights for decoding with MERT (Och, 2003) or other optimization algorithms, such as the Simplex Armijo Downhill algorithm proposed in (Zhao and Chen, 2009). 3.2 Partition of Feature Space Given the proposed mixture model, how to split the feature space into multiple regions becomes crucial. In order to surpass the baseline model, where all features can be viewed as existing in a single mixture component, the separated mixture components should be complementary to each other. In this work, we explore three different ways of partitions, based on either feature types, word alignment types, or the domain of training data. In the feature-</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings ofACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="6012" citStr="Papineni et al., 2002" startWordPosition="976" endWordPosition="979">based approach. 3 Discriminative Mixture Model 3.1 Mixture Model Now we introduce the discriminative mixture model. Suppose we partition the feature space into multiple clusters (details in Section 3.2). Let the probability of target phrase and jump given certain source phrase for cluster k be 1 X pk(t, j|s) = Zk(s)exp λkiφki(t, j, s), (2) i The individual feature weights λki for the i-th feature in cluster k are estimated in the maximumentropy framework as in the baseline model. However, the mixture weights wk can be optimized directly towards the translation evaluation metric, such as BLEU (Papineni et al., 2002), along with other usual costs (e.g. language model scores) on a development set. Note that the number of mixture components is relatively small (less than 10) compared to millions of features in baseline. Hence the optimization can be conducted easily to generate reliable mixture weights for decoding with MERT (Och, 2003) or other optimization algorithms, such as the Simplex Armijo Downhill algorithm proposed in (Zhao and Chen, 2009). 3.2 Partition of Feature Space Given the proposed mixture model, how to split the feature space into multiple regions becomes crucial. In order to surpass the b</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings ofACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A discriminative global training algorithm for statistical mt.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL/COLING,</booktitle>
<pages>721--728</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2152" citStr="Tillmann and Zhang, 2006" startWordPosition="327" endWordPosition="330">e either small (at the order of 10), or large (up to millions). For example, the system described in (Koehn et al., 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). The features include phrase translation probabilities, lexical probabilities, number of phrases, and language model scores, etc. The feature weights are usually optimized with minimum error rate training (MERT) as in (Och, 2003). Besides the MERT-based feature weight optimization, there exist other alternative discriminative training methods for MT, such as in (Tillmann and Zhang, 2006; Liang et al., 2006; Blunsom et al., 2008). However, scalability is a challenge for these approaches, where all possible translations of each training example need to be searched, which is computationally expensive. In (Chiang et al., 2009), there are 11K syntactic features proposed for a hierarchical phrase-based system. The feature weights are trained with the Margin Infused Relaxed Algorithm (MIRA) efficiently on a forest of translations from a development set. Even though significant improvement has been obtained compared to the baseline that has small number of features, it is hard to ap</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2006. A discriminative global training algorithm for statistical mt. In Proceedings of ACL/COLING, pages 721–728, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="9172" citStr="Vogel et al., 1996" startWordPosition="1501" endWordPosition="1504">ation, we can also split the training data based on their domain/resources, with each mixture component representing a specific domain. 4 Experiments 4.1 Data and Baseline We conduct a set of experiments on an Arabic-toEnglish MT task. The training data includes the UN parallel corpus and LDC-released parallel corpora, with about 10M sentence pairs and 300M words in total (counted at the English side). For each sentence in the training, three types of word alignments are created: maximum entropy alignment (Ittycheriah and Roukos, 2005), GIZA++ alignment (Och and Ney, 2000), and HMM alignment (Vogel et al., 1996). Our tuning and test sets are extracted from the GALE DEV10 Newswire set, with no overlap between tuning and test. There are 1063 sentences (168 documents) in the tuning set, and 1089 sentences (168 documents) in the test set. Both sets have one reference translation for each sentence. Instead of using all the training data, we sample the training corpus based on the tuning/test set to train the systems more efficiently. In the end, about 1.5M sentence pairs are selected for the sampled training. A 5-gram language model is trained from the English Gigaword corpus and the English portion of th</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proceedings of COLING, pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
</authors>
<title>Measuring confidence intervals for the machine translation evaluation metrics.</title>
<date>2004</date>
<booktitle>In Proceedings of The 10th International Conference on Theoretical and Methodological Issues in Machine Translation.</booktitle>
<contexts>
<context position="14168" citStr="Zhang and Vogel, 2004" startWordPosition="2331" endWordPosition="2334"> subsets. All results are compared in Table 3. The baseline that was trained on all the data achieved 0.5 gain compared to using the newswire training data alone (understandably it is the best component given the newswire test data). Note that since the baseline is trained on subsampled training data, there is already certain domain adaptation effect involved. On top of that, the mixture model results in another 0.45 gain in BLEU. All the improvements in the mixture models above against the baseline are statistically significant with p-value &lt; 0.0001 by using the confidence tool described in (Zhang and Vogel, 2004). 5 Conclusion In this paper we presented a novel discriminative mixture model for bridging the gap between the maximum-likelihood training and the discriminative training in SMT. We partition the feature space into multiple regions. The features in each region are tied together to share the same mixture weights that are optimized towards the maximum BLEU scores. It was shown that the same model structure can be ef427 fectively applied to feature combination, alignment combination and domain adaptation. We also point out that it is straightforward to combine any of these three. For example, we</context>
</contexts>
<marker>Zhang, Vogel, 2004</marker>
<rawString>Ying Zhang and Stephan Vogel. 2004. Measuring confidence intervals for the machine translation evaluation metrics. In Proceedings of The 10th International Conference on Theoretical and Methodological Issues in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Shengyuan Chen</author>
</authors>
<title>A simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters.</title>
<date>2009</date>
<booktitle>In Proceedings ofNAACL-HLT.</booktitle>
<contexts>
<context position="6450" citStr="Zhao and Chen, 2009" startWordPosition="1047" endWordPosition="1050">mentropy framework as in the baseline model. However, the mixture weights wk can be optimized directly towards the translation evaluation metric, such as BLEU (Papineni et al., 2002), along with other usual costs (e.g. language model scores) on a development set. Note that the number of mixture components is relatively small (less than 10) compared to millions of features in baseline. Hence the optimization can be conducted easily to generate reliable mixture weights for decoding with MERT (Och, 2003) or other optimization algorithms, such as the Simplex Armijo Downhill algorithm proposed in (Zhao and Chen, 2009). 3.2 Partition of Feature Space Given the proposed mixture model, how to split the feature space into multiple regions becomes crucial. In order to surpass the baseline model, where all features can be viewed as existing in a single mixture component, the separated mixture components should be complementary to each other. In this work, we explore three different ways of partitions, based on either feature types, word alignment types, or the domain of training data. In the feature-type-based partition, we split the ME features into 8 categories: • F1: Lexical features that examine source word,</context>
<context position="9983" citStr="Zhao and Chen, 2009" startWordPosition="1640" endWordPosition="1643">ces (168 documents) in the test set. Both sets have one reference translation for each sentence. Instead of using all the training data, we sample the training corpus based on the tuning/test set to train the systems more efficiently. In the end, about 1.5M sentence pairs are selected for the sampled training. A 5-gram language model is trained from the English Gigaword corpus and the English portion of the parallel corpus used in the translation model training. In this work, the decoding weights for both the baseline and the mixture model are tuned with the Simplex Armijo Downhill algorithm (Zhao and Chen, 2009) towards the maximum BLEU. System Features BLEU F1 685K 37.11 F2 5516K 38.43 F3 4457K 37.75 F4 3884K 37.56 F5 103K 36.03 F6 325K 37.89 F7 1584K 38.56 F8 1605K 37.49 Baseline 18159K 39.36 Mixture 18159K 39.97 Table 1: MT results with individual mixture component (F1 to F8), baseline, or mixture model. 4.2 Feature Combination We first experiment with the feature-type-based clustering as described in Section 3.2. The translation results on the test set from the baseline and the mixture model are listed in Table 1. The MT performance is measured with the widely adopted BLEU metric. We also evaluat</context>
</contexts>
<marker>Zhao, Chen, 2009</marker>
<rawString>Bing Zhao and Shengyuan Chen. 2009. A simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters. In Proceedings ofNAACL-HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>