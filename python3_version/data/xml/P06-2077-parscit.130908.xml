<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000079">
<title confidence="0.9979">
Reinforcing English Countability Prediction with One Countability per
Discourse Property
</title>
<author confidence="0.985672">
Ryo Nagata
</author>
<affiliation confidence="0.975833">
Hyogo University of Teacher Education
</affiliation>
<address confidence="0.442641">
6731494, Japan
</address>
<email confidence="0.781758">
rnagata@hyogo-u.ac.jp
</email>
<author confidence="0.987974">
Koichiro Morihiro
</author>
<affiliation confidence="0.994944">
Hyogo University of Teacher Education
</affiliation>
<address confidence="0.507568">
6731494, Japan
</address>
<email confidence="0.90765">
mori@hyogo-u.ac.jp
</email>
<sectionHeader confidence="0.993768" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999508789473684">
Countability of English nouns is impor-
tant in various natural language process-
ing tasks. It especially plays an important
role in machine translation since it deter-
mines the range of possible determiners.
This paper proposes a method for reinforc-
ing countability prediction by introducing
a novel concept called one countability per
discourse. It claims that when a noun
appears more than once in a discourse,
they will all share the same countability in
the discourse. The basic idea of the pro-
posed method is that mispredictions can
be correctly overridden using efficiently
the one countability per discourse prop-
erty. Experiments show that the proposed
method successfully reinforces countabil-
ity prediction and outperforms other meth-
ods used for comparison.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9991776">
Countability of English nouns is important in var-
ious natural language processing tasks. It is par-
ticularly important in machine translation from
a source language that does not have an article
system similar to that of English, such as Chi-
nese and Japanese, into English since it determines
the range of possible determiners including arti-
cles. It also plays an important role in determining
whether a noun can take singular and plural forms.
Another useful application is to detect errors in ar-
ticle usage and singular/plural usage in the writing
of second language learners. Given countability,
these errors can be detected in many cases. For
example, an error can be detected from “We have
a furniture.” given that the noun furniture is un-
</bodyText>
<note confidence="0.338402">
Atsuo Kawai
Mie University
5148507, Japan
</note>
<email confidence="0.925825">
kawai@ai.info.mie-u.ac.jp
</email>
<sectionHeader confidence="0.495776666666667" genericHeader="introduction">
Naoki Isu
Mie University
5148507, Japan
</sectionHeader>
<email confidence="0.954959">
isu@ai.info.mie-u.ac.jp
</email>
<bodyText confidence="0.999824684210526">
countable since uncountable nouns do not tolerate
the indefinite article.
Because of the wide range of applications, re-
searchers have done a lot of work related to
countability. Baldwin and Bond (2003a; 2003b)
have proposed a method for automatically learn-
ing countability from corpus data. Lapata and
Keller (2005) and Peng and Araki (2005) have
proposed web-based models for learning count-
ability. Others including Bond and Vatikiotis-
Bateson (2002) and O’Hara et al. (2003) use on-
tology to determine countability.
In the application to error detection, re-
searchers have explored alternative approaches
since sources of evidence for determining count-
ability are limited compared to other applications.
Articles and the singular/plural distinction, which
are informative for countability, cannot be used in
countability prediction aiming at detecting errors
in article usage and singular/plural usage. Return-
ing to the previous example, the countability of the
noun furniture cannot be determined as uncount-
able by the indefinite article; first, its countabil-
ity has to be predicted without the indefinite arti-
cle, and only then whether or not it tolerates the
indefinite article is examined using the predicted
countability. Also, unlike in machine translation,
the source language is not given in the writing of
second language learners such as essays, which
means that information available is limited.
To overcome these limitations, Nagata
et al. (2005a) have proposed a method for
predicting countability that relies solely on words
(except articles and other determiners) surround-
ing the target noun. Nagata et al. (2005b) have
shown that the method is effective to detecting
errors in article usage and singular/plural usage in
the writing of Japanese learners of English. They
</bodyText>
<page confidence="0.976613">
595
</page>
<note confidence="0.72535">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 595–602,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999966086956522">
also have shown that it is likely that performance
of the error detection will improve as accuracy of
the countability prediction increases since most of
false positives are due to mispredictions.
In this paper, we propose a method for reinforc-
ing countability prediction by introducing a novel
concept called one countability per discourse that
is an extension of one sense per discourse pro-
posed by Gale et al. (1992). It claims that when
a noun appears more than once in a discourse,
they will all share the same countability in the dis-
course. The basic idea of the proposed method
is that initially mispredicted countability can be
corrected using efficiently the one countability per
discourse property.
The next section introduces the one countability
per discourse concept and shows that it can be a
good source of evidence for predicting countabil-
ity. Section 3 discusses how it can be efficiently
exploited to predict countability. Section 4 de-
scribes the proposed method. Section 5 describes
experiments conducted to evaluate the proposed
method and discusses the results.
</bodyText>
<sectionHeader confidence="0.877344" genericHeader="method">
2 One Countability per Discourse
</sectionHeader>
<bodyText confidence="0.999953375">
One countability per discourse is an extension
of one sense per discourse proposed by Gale
et al. (1992). One sense per discourse claims that
when a polysemous word appears more than once
in a discourse it is likely that they will all share
the same sense. Yarowsky (1995) tested the claim
on about 37,000 examples and found that when a
polysemous word appeared more than once in a
discourse, they took on the majority sense for the
discourse 99.8% of the time on average.
Based on one sense per discourse, we hypothe-
size that when a noun appears more than once in a
discourse, they will all share the same countability
in the discourse, that is, one countability per dis-
course. The motivation for this hypothesis is that
if one sense per discourse is satisfied, so is one
countability per discourse because countability is
often determined by word sense. For example, if
the noun paper appears in a discourse and it has
the sense of newspaper, which is countable, the
rest of papers in the discourse also have the same
sense according to one sense per discourse, and
thus they are also countable.
We tested this hypothesis on a set of nouns1
</bodyText>
<subsectionHeader confidence="0.8018145">
&apos;The conditions of this test are shown in Section 5. Note
that although the source of the data is the same as in Section 5,
</subsectionHeader>
<bodyText confidence="0.996063611111111">
as Yarowsky (1995) did. We calculated how ac-
curately the majority countability for each dis-
course predicted countability of the nouns in the
discourse when they appeared more than once. If
the one countability per discourse property is al-
ways satisfied, the majority countability for each
discourse should predict countability with the ac-
curacy of 100%. In other others, the obtained ac-
curacy represents how often the one countability
per discourse property is satisfied.
Table 1 shows the results. “MCD” in Table 1
stands for Majority Countability for Discourse and
its corresponding column denotes accuracy where
countability of individual nouns was predicted
by the majority countability for the discourse in
which they appeared. Also, “Baseline” denotes
accuracy where it was predicted by the majority
countability for the whole corpus used in this test.
</bodyText>
<tableCaption confidence="0.948087">
Table 1: Accuracy obtained by Majority Count-
ability for Discourse
</tableCaption>
<bodyText confidence="0.99507216">
Target noun MCD Baseline
advantage 0.772 0.618
aid 0.943 0.671
authority 0.864 0.771
building 0.850 0.811
cover 0.926 0.537
detail 0.829 0.763
discipline 0.877 0.652
duty 0.839 0.714
football 0.938 0.930
gold 0.929 0.929
hair 0.914 0.902
improvement 0.735 0.685
necessity 0.769 0.590
paper 0.807 0.647
reason 0.858 0.822
sausage 0.821 0.750
sleep 0.901 0.765
stomach 0.778 0.778
study 0.824 0.781
truth 0.783 0.724
use 0.877 0.871
work 0.861 0.777
worry 0.871 0.843
Average 0.851 0.754
</bodyText>
<tableCaption confidence="0.628946666666667">
Table 1 reveals that the one countability per dis-
discourses in which the target noun appears only once are
excluded from this test unlike in Section 5.
</tableCaption>
<page confidence="0.994974">
596
</page>
<bodyText confidence="0.999987666666666">
course property is a good source of evidence for
predicting countability compared to the baseline
while it is not as strong as the one sense per dis-
course property is. It also reveals that the tendency
of one countability per discourse varies from noun
to noun. For instance, nouns such as aid and
cover show a strong tendency while others such
as advantage and improvement do not. On aver-
age, “MCD” achieves an improvement of approx-
imately 10% in accuracy over the baseline.
Having observed the results, it is reasonable to
exploit the one countability per discourse prop-
erty for predicting countability. In order to do
it, however, the following two questions should
be addressed. First, how can the majority count-
ability be obtained from a novel discourse? Since
our intention is to predict values of countability of
instances in a novel discourse, none of them are
known. Second, even if the majority countability
is known, how can it be efficiently exploited for
predicting countability? Although we could sim-
ply predict countability of individual instances of
a target noun in a discourse by the majority count-
ability for the discourse, it is highly possible that
this simple method will cause side effects consid-
ering the results in Table 1. These two questions
are addressed in the next section.
</bodyText>
<sectionHeader confidence="0.997226" genericHeader="method">
3 Basic Idea
</sectionHeader>
<subsectionHeader confidence="0.991878">
3.1 How Can the Majority Countability be
Obtained from a Novel Discourse?
</subsectionHeader>
<bodyText confidence="0.999989894736842">
Although we do not know the true value of the ma-
jority countability for a novel discourse, we can
at least estimate it because we have a method for
predicting countability to be reinforced by the pro-
posed method. That is, we can predict countability
of the target noun in a novel discourse using the
method. Simply counting the results would give
the majority countability for it.
Here, we should note that countability of each
instance is not the true value but a predicted one.
Considering this fact, it is sensible to set a cer-
tain criterion in order to filter out spurious predic-
tions. Fortunately, most methods based on ma-
chine learning algorithms give predictions with
their confidences. We use the confidences as the
criterion. Namely, we only take account of predic-
tions whose confidences are greater than a certain
threshold when we estimate the majority count-
ability for a novel discourse.
</bodyText>
<subsectionHeader confidence="0.9977635">
3.2 How Can the Majority Countability be
Efficiently Exploited?
</subsectionHeader>
<bodyText confidence="0.999990217391304">
In order to efficiently exploit the one countabil-
ity per discourse property, we treat the majority
countability for each discourse as a feature in ad-
dition to other features extracted from instances of
the target noun. Doing so, we let a machine learn-
ing algorithm decide which features are relevant to
the prediction. If the majority countability feature
is relevant, the machine learning algorithm should
give a high weight to it compared to others.
To see this, let us suppose that we have a set
of discourses in which instances of the target noun
are tagged with their countability (either countable
or uncountable2) for the moment; we will describe
how to obtain it in Subsection 4.1. For each dis-
course, we can know its majority countability by
counting the numbers of countables and uncount-
ables. We can also generate a model for predicting
countability from the set of discourses using a ma-
chine learning algorithm. All we have to do is to
extract a set of training data from the tagged in-
stances and to apply a machine learning algorithm
to it. This is where the majority countability fea-
ture comes in. The majority countability for each
instance is added to its corresponding training data
as a feature to create a new set of training data be-
fore applying a machine learning algorithm; then
a machine learning algorithm is applied to the new
set. The resulting model takes the majority count-
ability feature into account as well as the other fea-
tures when making predictions.
It is important to exercise some care in count-
ing the majority countability for each discourse.
Note that one countability per discourse is always
satisfied in discourses where the target noun ap-
pears only once. This suggests that it is highly
possible that the resulting model too strongly fa-
vors the majority countability feature. To avoid
this, we could split the discourses into two sets,
one for where the target noun appears only once
and one for where it appears more than once, and
train a model on each set. However, we do not
take this strategy because we want to use as much
data as possible for training. As a compromise,
we approximate the majority countability for dis-
courses where the target noun appears only once
to the value unknown.
</bodyText>
<footnote confidence="0.853475">
2This paper concentrates solely on countable and un-
countable nouns, since they account for the vast majority of
nouns (Zapata and Keller, 2005).
</footnote>
<page confidence="0.988188">
597
</page>
<figureCaption confidence="0.773905">
Figure 1: Framework of the tagging rules
</figureCaption>
<tableCaption confidence="0.816897">
Table 2: Words used in the tagging rules
</tableCaption>
<figure confidence="0.882213857142857">
(a) (b) (c)
the indefinite article much the definite article
another less demonstrative adjectives
one enough possessive adjectives
each sufficient interrogative adjectives
— —quantifiers
— — ’s genitives
?
? UNCOUNTABLE
COUNTABLE
plural?
�
yes
COUNTABLE
?�
yes � no
�
no
modified by one of the words
in Table 2(a)?
modified by one of the words
in Table 2(b)?
modified by one of the words
in Table 2(c)?
�
yes
�
yes
modified by a little
no
�
yes
no
UNCOUNTABLE
no
</figure>
<sectionHeader confidence="0.761636" genericHeader="method">
4 Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.999475">
4.1 Generating Training Data
</subsectionHeader>
<bodyText confidence="0.999724">
As discussed in Subsection 3.2, training data are
needed to exploit the one countability per dis-
course property. In other words, the proposed
method requires a set of discourses in which in-
stances of the target noun are tagged with their
countability. Fortunately, Nagata et al. (2005b)
have proposed a method for tagging nouns with
their countability. This paper follows it to gener-
ate training data.
To generate training data, first, instances of the
target noun used as a head noun are collected from
a corpus with their surrounding words. This can be
simply done by an existing chunker or parser.
Second, the collected instances are tagged with
either countable or uncountable by tagging rules.
For example, the underlined paper:
... read a paper in the morning ...
is tagged as
... read a paper/countable in the morning ...
because it is modified by the indefinite article.
Figure 1 and Table 2 represent the tagging rules
based on Nagata et al. (2005b)’s method. Fig-
ure 1 shows the framework of the tagging rules.
Each node in Figure 1 represents a question ap-
plied to the instance in question. For instance, the
root node reads “Is the instance in question plu-
ral?”. Each leaf represents a result of the classifi-
cation. For instance, if the answer is “yes” at the
root node, the instance in question is tagged with
countable. Otherwise, the question at the lower
node is applied and so on. The tagging rules do
not classify instances in some cases. These unclas-
sified instances are tagged with the symbol “?”.
Unfortunately, they cannot readily be included in
training data. For simplicity of implementation,
they are excluded from training data (we will dis-
cuss the use of these excluded data in Section 6).
Note that the tagging rules cannot be used for
countability prediction aiming at detecting errors
in article usage and singular/plural usage. The
reason is that they are useless in error detection
where whether determiners and the singular/plural
distinction are correct or not is unknown. Obvi-
ously, the tagging rules assume that the target text
contains no error.
Third, features are extracted from each instance.
As the features, the following three types of con-
textual cues are used: (i) words in the noun phrase
that the instance heads, (ii) three words to the left
of the noun phrase, and (iii) three words to its
right. Here, the words in Table 2 are excluded.
Also, function words (except prepositions) such
as pronouns, cardinal and quasi-cardinal numer-
</bodyText>
<page confidence="0.996225">
598
</page>
<bodyText confidence="0.998587689655172">
als, and the target noun are excluded. All words
are reduced to their morphological stem and con-
verted entirely to lower case when collected. In
addition to the features, the majority countability
is used as a feature. For each discourse, the num-
bers of countables and uncountables are counted
to obtain its majority countability. In case of ties,
it is set to unknown. Also, it is set to unknown
when only one instance appears in the discourse
as explained in Subsection 3.2.
To illustrate feature extraction, let us consider
the following discourse (target noun: paper):
... writing a new paper/countable in his room ...
... read papers/countable with ...
The discourse would give a set of features:
-3=write, NP=new, +3=in, +3=room, MC=c
-3=read, +3=with, MC=c
where “MC=c” denotes that the majority count-
ability for the discourse is countable. In this exam-
ple (and in the following examples), the features
are represented in a somewhat simplified manner
for the purpose of illustration. In practice, features
are represented as a vector.
Finally, the features are stored in a file with their
corresponding countability as training data. Each
piece of training data would be as follows:
-3=read, +3=with, MC=c, LABEL=c
where “LABEL=c” denotes that the countability
for the instance is countable.
</bodyText>
<subsectionHeader confidence="0.998403">
4.2 Model Generation
</subsectionHeader>
<bodyText confidence="0.999827285714286">
The model used in the proposed method can be re-
garded as a function. It takes as its input a feature
vector extracted from the instance in question and
predicts countability (either countable or uncount-
able). Formally, where , , and
denote the model, the feature vector, and
paper, the Maximum Entropy (ME) algorithm is
used which has been shown to be effective in a
wide variety of natural language processing tasks.
Model generation is done by applying the ME
algorithm to the training data. The resulting model
takes account of the features including the major-
ity countability feature and is used for reinforcing
countability prediction.
</bodyText>
<subsectionHeader confidence="0.999733">
4.3 Reinforcing Countability Prediction
</subsectionHeader>
<bodyText confidence="0.998797886363636">
Before explaining the reinforcement procedure, let
us introduce the following discourse for illustra-
tion (target noun: paper):
... writing paper in room ... wrote paper in ...
... submitted paper to ...
Note that articles and the singular/plural distinc-
tion are deliberately removed from the discourse.
This kind of situation can happen in machine
translation from a source language that does not
have articles and the singular/plural distinction3.
The situation is similar in the writing of second
language learners of English since they often omit
articles and the singular/plural distinction or use
improper ones. Here, suppose that the true values
of the countability for all instances are countable.
A method to be reinforced by the proposed
method would predict countability as follows:
... writing paper/countable (0.97) in room ...
... wrote paper/countable (0.98) in ...
... submitted paper/uncountable (0.57) to ...
where the numbers in brackets denote the confi-
dences given by the method. The third instance is
mistakenly predicted as uncountable4.
Now let us move on to the reinforcement pro-
cedure. It is divided into three steps. First, the
majority countability for the discourse in question
is estimated by counting the numbers of the pre-
dicted countables and uncountables whose confi-
dences are greater than a certain threshold. In case
of ties, the values of the majority countability is
set to unknown. In the above example, the major-
ity countability for the discourse is estimated to be
countable when the threshold is set to (two
countables). Second, features explained in Sub-
section 4.1 are extracted from each instance. As
for the majority countability feature, the estimated
one is used. Returning to the above example, the
three instances would give a set of features:
-3=write, +3=in, +3=room, MC=c,
-3=write, +3=in, MC=c,
-3=submit, +3=to, MC=c.
Finally, the model generated in Subsection 4.2
is applied to the features to predict countability.
Because of the majority countability feature, it
</bodyText>
<footnote confidence="0.998055">
3For instance, the Japanese language does not have an ar-
ticle system similar to that of English, neither does it mark
the singular/plural distinction.
4The reason would be that the contextual cues did not ap-
pear in the training data used in the method.
</footnote>
<bodyText confidence="0.998674333333333">
,
respectively; here, 0 and 1 correspond to count-
able and uncountable, respectively.
Given the specification, almost any kind of ma-
chine learning algorithm cab be used to generate
the model used in the proposed method. In this
</bodyText>
<page confidence="0.996966">
599
</page>
<bodyText confidence="0.999805833333333">
is likely that previous mispredictions are overrid-
den by correct ones. In the above example, the
third one would be correctly overridden by count-
able because of the majority countability feature
(MC=c) that is informative for the instance being
countable.
</bodyText>
<sectionHeader confidence="0.999666" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998892">
5.1 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.9999832">
In the experiments, we chose Nagata
et al. (2005a)’s method as the one to be re-
inforced by the proposed method. In this
method, the decision list (DL) learning algo-
rithm (Yarowsky, 1995) is used. However, we
used the ME algorithm because we found that the
method with the ME algorithm instead of the DL
learning algorithm performed better when trained
on the same training data.
As the target noun, we selected 23 nouns that
were also used in Nagata et al. (2005a)’s experi-
ments. They are exemplified as nouns that are used
as both countable and uncountable by Huddleston
and Pullum (2002).
Training data were generated from the writ-
ten part of the British National Corpus (Burnard,
1995). A text tagged with the text tags was used
as a discourse unit. From the corpus, 314 texts,
which amounted to about 10% of all texts, were
randomly taken to obtain test data. The rest of
texts were used to generate training data.
We evaluated performance of prediction by ac-
curacy. We defined accuracy by the ratio of the
number of correct predictions to that of instances
of the target noun in the test data.
</bodyText>
<subsectionHeader confidence="0.997427">
5.2 Experimental Procedures
</subsectionHeader>
<bodyText confidence="0.999854785714286">
First, we generated training data for each target
noun from the texts using the tagging rules ex-
plained in Subsection 4.1. We used the OAK sys-
tem5 to extract noun phrases and their heads. Of
the extracted instances, we excluded those that had
no contextual cues from the training data (and also
the test data). We also generated another set of
training data by removing the majority countabil-
ity features from them. This set of training data
was used for comparison.
Second, we obtained test data by applying the
tagging rules described in Subsection 4.1 to each
instance of the target noun in the 314 texts. Na-
gata et al. (2005b) showed that the tagging rules
</bodyText>
<footnote confidence="0.764388">
5http://www.cs.nyu.edu/ sekine/PROJECT/OAK/
</footnote>
<bodyText confidence="0.998614291666667">
achieved an accuracy of 0.997 in the texts that
contained no errors. Considering these results, we
used the tagging rules to obtain test data. Instances
tagged with “?” were excluded in the experiments.
Third, we applied the ME algorithm6 to the
training data without the majority countability fea-
ture. Using the resulting model, countability of
the target nouns in the test data was predicted.
Then, the predictions were reinforced by the pro-
posed method. The threshold to filter out spu-
rious predictions was set to . For compar-
ison, the predictions obtained by the ME model
were simply replaced with the estimated majority
countability for each discourse. In this method, the
original predictions were used when the estimated
majority countability was unknown. Also, Nagata
et al. (2005a)’s method that was based on the DL
learning algorithm was implemented for compari-
son.
Finally, we calculated accuracy of each method.
In addition to the results, we evaluated the baseline
on the same test data where all predictions were
done by the majority countability for the whole
corpus (training data).
</bodyText>
<subsectionHeader confidence="0.999895">
5.3 Experimental Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999592523809524">
Table 3 shows the accuracies7. “ME” and “Pro-
posed” in Table 3 refer to accuracies of the ME
model and the ME model reinforced by the pro-
posed method, respectively. “ME+MCD” refers
to accuracy obtained by replacing predictions of
the ME model with the estimated majority count-
ability for each discourse. Also, “DL” refers to
accuracy of the DL-based method.
Table 3 shows that the three ME-based meth-
ods (“Proposed”, “ME”, and “ME+MCD”) per-
form better than “DL” and the baseline. Espe-
cially, “Proposed” outperforms the other methods
in most of the target nouns.
Figure 2 summarizes the comparison between
the three ME-based methods. Each plot in Fig-
ure 2 represents each target noun. The horizon-
tal and vertical axises correspond to accuracy of
“ME” and that of “Proposed” (or “ME+MCD”),
respectively. The diagonal line corresponds to the
line . So if “Proposed” (or “ME+MCD”)
achieved no improvement at all over “ME”, all the
</bodyText>
<footnote confidence="0.983727">
6All ME models were generated using the
opennlp.maxent package (http://maxent.sourceforge.net/).
7The baseline in Table 3 is different from that in Table 1
because discourses where the target noun appears only once
are not taken into account in Table 1.
</footnote>
<page confidence="0.995786">
600
</page>
<tableCaption confidence="0.998701">
Table 3: Experimental results
</tableCaption>
<table confidence="0.976385222222223">
Target noun Freq. Baseline Proposed ME ME+MCD DL
advantage 570 0.604 0.933 0.921 0.811 0.882
aid 385 0.665 0.909 0.873 0.896 0.722
authority 1162 0.760 0.857 0.851 0.840 0.804
building 1114 0.803 0.848 0.842 0.829 0.807
cover 210 0.567 0.790 0.757 0.800 0.714
detail 1157 0.760 0.906 0.904 0.821 0.869
discipline 204 0.593 0.804 0.745 0.750 0.696
duty 570 0.700 0.879 0.877 0.828 0.847
football 281 0.907 0.925 0.907 0.925 0.911
gold 140 0.929 0.929 0.929 0.921 0.929
hair 448 0.902 0.908 0.902 0.904 0.904
improvement 362 0.696 0.735 0.715 0.685 0.738
necessity 83 0.566 0.831 0.843 0.831 0.783
paper 1266 0.642 0.859 0.836 0.808 0.839
reason 1163 0.824 0.885 0.893 0.834 0.843
sausage 45 0.778 0.778 0.733 0.756 0.778
sleep 107 0.776 0.925 0.897 0.897 0.813
stomach 30 0.633 0.800 0.800 0.800 0.733
study 1162 0.779 0.832 0.819 0.782 0.808
truth 264 0.720 0.761 0.777 0.765 0.731
use 1390 0.869 0.879 0.863 0.871 0.873
work 3002 0.778 0.858 0.842 0.837 0.806
worry 119 0.798 0.874 0.840 0.849 0.849
Average 662 0.741 0.857 0.842 0.828 0.812
0.7 0.8 0.9 1
Accuracy (ME)
</table>
<figureCaption confidence="0.9658075">
Figure 2: Comparison between “ME” and “Pro-
posed/ME+MCD” in each target noun
</figureCaption>
<bodyText confidence="0.984043730769231">
plots would be on the line. Plots above the line
mean improvement over “ME” and the distance
from the line expresses the amount of improve-
ment. Plots below the line mean the opposite.
Figure 2 clearly shows that most of the plots ( )
corresponding to the comparison between “ME”
and “Proposed” are above the line. This means
that the proposed method successfully reinforced
“ME” in most of the target nouns. Indeed, the av-
erage accuracy of “Proposed” is significantly su-
perior to that of “ME” at the 99% confidence level
(paired t-test). This improvement is close to that
of one sense per discourse (Yarowsky, 1995) (im-
provement ranging from 1.3% to 1.7%), which
seems to be a sensible upper bound of the pro-
posed method. By contrast, about half of the
plots ( ) corresponding to the comparison between
“ME” and “ME+MCD” are below the line.
From these results, it follows that the one count-
ability per discourse property is a good source of
evidence for predicting countability, but it is cru-
cial to devise a way of exploiting the property as
we did in this paper. Namely, simply replacing
original predictions with the majority countabil-
ity for the discourse causes side effects, which
has been already suggested in Table 1. This is
</bodyText>
<figure confidence="0.992614">
Accuracy (Proposed/ME+MCD)
0.9
0.8
0.7
1
ME vs Proposed
ME vs ME+MCD
</figure>
<page confidence="0.991093">
601
</page>
<bodyText confidence="0.999981846153846">
also exemplified as follows. Suppose that sev-
eral instances of the target noun advantage ap-
pear in a discourse and that its majority countably
is countable. Further suppose that an idiomatic
phrase “take advantage of&apos; of which countability
is uncountable happens to appear in it. On one
hand, simply replacing all the predictions with its
majority countability (countable) would lead to a
misprediction for the idiomatic phrase even if the
original prediction is correct. On the other hand,
the proposed method would correctly predict the
countability because the contextual cues strongly
indicate that it is uncountable.
</bodyText>
<sectionHeader confidence="0.999849" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999978125">
This paper has proposed a method for reinforc-
ing English countability prediction by introducing
one countability per discourse. The experiments
have shown that the proposed method successfully
overrode original mispredictions using efficiently
the one countability per discourse property. They
also have shown that it outperformed other meth-
ods used for comparison. From these results, we
conclude that the proposed method is effective in
reinforcing English countability prediction.
In addition, the proposed method has two ad-
vantages. The first is its applicability. It can re-
inforce almost any earlier method. Even to hand-
coded rules, it can be applied as long as they give
predictions with their confidences. This further
gives an additional advantage. Recall that the
instances tagged with “?&apos; by the tagging rules
are discarded when training data are generated
as described in Subsection 4.1. These instances
can be retagged with their countability by using
the proposed method and some kind of bootstrap-
ping (Yarowsky, 1995). This means increase in
training data, which might eventually result in fur-
ther improvement. The second is that the proposed
method is unsupervised. It requires no human in-
tervention to reinforce countability prediction.
For future work, we will investigate what mod-
els are most appropriate for exploiting the one
countability per discourse property. We will also
explore a method for including instances tagged
with “?&apos; in training data by using the proposed
method and bootstrapping.
</bodyText>
<sectionHeader confidence="0.998948" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999585">
The authors would like to thank Satoshi Sekine
who has developed the OAK System. The authors
also would like to thank three anonymous review-
ers for their useful comments on this paper.
</bodyText>
<sectionHeader confidence="0.996465" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999905875">
T. Baldwin and F. Bond. 2003a. Learning the count-
ability of English nouns from corpus data. In Proc.
of 41st Annual Meeting of ACL, pages 463–470.
T. Baldwin and F. Bond. 2003b. A plethora of meth-
ods for learning English countability. In Proc. of
2003 Conference on Empirical Methods in Natural
Language Processing, pages 73–80.
F. Bond and C. Vatikiotis-Bateson. 2002. Using an
ontology to determine English countability. In Proc.
of 19th International Conference on Computational
Linguistics, pages 99–105.
L. Burnard. 1995. Users Reference Guide for the
British National Corpus. version 1.0. Oxford Uni-
versity Computing Services, Oxford.
W.A. Gale, K.W. Church, and D. Yarowsky. 1992. One
sense per discourse. In Proc. of 4th DARPA Speech
and Natural Language Workshop, pages 233–237.
R. Huddleston and G.K. Pullum. 2002. The Cam-
bridge Grammar of the English Language. Cam-
bridge University Press, Cambridge.
M. Lapata and F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1):1–31.
R. Nagata, F. Masui, A. Kawai, and N. Isu. 2005a. An
unsupervised method for distinguishing mass and
count noun in context. In Proc. of 6th International
Workshop on Computational Semantics, pages 213–
224.
R. Nagata, T. Wakana, F. Masui, A. Kawai, and N. Isu.
2005b. Detecting article errors based on the mass
count distinction. In Proc. of 2nd International Joint
Conference on Natural Language Processing, pages
815–826.
T. O’Hara, N. Salay, M. Witbrock, D. Schneider,
B. Aldag, S. Bertolo, K. Panton, F. Lehmann, J. Cur-
tis, M. Smith, D. Baxter, and P. Wagner. 2003. In-
ducing criteria for mass noun lexical mappings using
the Cyc KB, and its extension to WordNet. In Proc.
of 5th International Workshop on Computational Se-
mantics, pages 425–441.
J. Peng and K. Araki. 2005. Detecting the countability
of English compound nouns using web-based mod-
els. In Companion Volume to Proc. of 2nd Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 105–109.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
33rd Annual Meeting ofACL, pages 189–196.
</reference>
<page confidence="0.99817">
602
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.695782">
<title confidence="0.981646">Reinforcing English Countability Prediction with One Countability per Discourse Property</title>
<author confidence="0.995059">Ryo Nagata</author>
<affiliation confidence="0.999864">Hyogo University of Teacher Education</affiliation>
<address confidence="0.999549">6731494, Japan</address>
<email confidence="0.982995">rnagata@hyogo-u.ac.jp</email>
<author confidence="0.816081">Koichiro Morihiro</author>
<affiliation confidence="0.999622">Hyogo University of Teacher Education</affiliation>
<address confidence="0.998524">6731494, Japan</address>
<email confidence="0.989059">mori@hyogo-u.ac.jp</email>
<abstract confidence="0.9957336">Countability of English nouns is important in various natural language processing tasks. It especially plays an important role in machine translation since it determines the range of possible determiners. This paper proposes a method for reinforcing countability prediction by introducing novel concept called countability per It claims that when a noun appears more than once in a discourse, they will all share the same countability in the discourse. The basic idea of the proposed method is that mispredictions can be correctly overridden using efficiently the one countability per discourse property. Experiments show that the proposed method successfully reinforces countability prediction and outperforms other methods used for comparison.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Baldwin</author>
<author>F Bond</author>
</authors>
<title>Learning the countability of English nouns from corpus data.</title>
<date>2003</date>
<booktitle>In Proc. of 41st Annual Meeting of ACL,</booktitle>
<pages>463--470</pages>
<contexts>
<context position="2127" citStr="Baldwin and Bond (2003" startWordPosition="316" endWordPosition="319">seful application is to detect errors in article usage and singular/plural usage in the writing of second language learners. Given countability, these errors can be detected in many cases. For example, an error can be detected from “We have a furniture.” given that the noun furniture is unAtsuo Kawai Mie University 5148507, Japan kawai@ai.info.mie-u.ac.jp Naoki Isu Mie University 5148507, Japan isu@ai.info.mie-u.ac.jp countable since uncountable nouns do not tolerate the indefinite article. Because of the wide range of applications, researchers have done a lot of work related to countability. Baldwin and Bond (2003a; 2003b) have proposed a method for automatically learning countability from corpus data. Lapata and Keller (2005) and Peng and Araki (2005) have proposed web-based models for learning countability. Others including Bond and VatikiotisBateson (2002) and O’Hara et al. (2003) use ontology to determine countability. In the application to error detection, researchers have explored alternative approaches since sources of evidence for determining countability are limited compared to other applications. Articles and the singular/plural distinction, which are informative for countability, cannot be u</context>
</contexts>
<marker>Baldwin, Bond, 2003</marker>
<rawString>T. Baldwin and F. Bond. 2003a. Learning the countability of English nouns from corpus data. In Proc. of 41st Annual Meeting of ACL, pages 463–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Baldwin</author>
<author>F Bond</author>
</authors>
<title>A plethora of methods for learning English countability.</title>
<date>2003</date>
<booktitle>In Proc. of 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="2127" citStr="Baldwin and Bond (2003" startWordPosition="316" endWordPosition="319">seful application is to detect errors in article usage and singular/plural usage in the writing of second language learners. Given countability, these errors can be detected in many cases. For example, an error can be detected from “We have a furniture.” given that the noun furniture is unAtsuo Kawai Mie University 5148507, Japan kawai@ai.info.mie-u.ac.jp Naoki Isu Mie University 5148507, Japan isu@ai.info.mie-u.ac.jp countable since uncountable nouns do not tolerate the indefinite article. Because of the wide range of applications, researchers have done a lot of work related to countability. Baldwin and Bond (2003a; 2003b) have proposed a method for automatically learning countability from corpus data. Lapata and Keller (2005) and Peng and Araki (2005) have proposed web-based models for learning countability. Others including Bond and VatikiotisBateson (2002) and O’Hara et al. (2003) use ontology to determine countability. In the application to error detection, researchers have explored alternative approaches since sources of evidence for determining countability are limited compared to other applications. Articles and the singular/plural distinction, which are informative for countability, cannot be u</context>
</contexts>
<marker>Baldwin, Bond, 2003</marker>
<rawString>T. Baldwin and F. Bond. 2003b. A plethora of methods for learning English countability. In Proc. of 2003 Conference on Empirical Methods in Natural Language Processing, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Bond</author>
<author>C Vatikiotis-Bateson</author>
</authors>
<title>Using an ontology to determine English countability.</title>
<date>2002</date>
<booktitle>In Proc. of 19th International Conference on Computational Linguistics,</booktitle>
<pages>99--105</pages>
<marker>Bond, Vatikiotis-Bateson, 2002</marker>
<rawString>F. Bond and C. Vatikiotis-Bateson. 2002. Using an ontology to determine English countability. In Proc. of 19th International Conference on Computational Linguistics, pages 99–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Burnard</author>
</authors>
<title>Users Reference Guide for the British National Corpus. version 1.0.</title>
<date>1995</date>
<institution>Oxford University Computing Services,</institution>
<location>Oxford.</location>
<contexts>
<context position="21123" citStr="Burnard, 1995" startWordPosition="3442" endWordPosition="3443">be reinforced by the proposed method. In this method, the decision list (DL) learning algorithm (Yarowsky, 1995) is used. However, we used the ME algorithm because we found that the method with the ME algorithm instead of the DL learning algorithm performed better when trained on the same training data. As the target noun, we selected 23 nouns that were also used in Nagata et al. (2005a)’s experiments. They are exemplified as nouns that are used as both countable and uncountable by Huddleston and Pullum (2002). Training data were generated from the written part of the British National Corpus (Burnard, 1995). A text tagged with the text tags was used as a discourse unit. From the corpus, 314 texts, which amounted to about 10% of all texts, were randomly taken to obtain test data. The rest of texts were used to generate training data. We evaluated performance of prediction by accuracy. We defined accuracy by the ratio of the number of correct predictions to that of instances of the target noun in the test data. 5.2 Experimental Procedures First, we generated training data for each target noun from the texts using the tagging rules explained in Subsection 4.1. We used the OAK system5 to extract nou</context>
</contexts>
<marker>Burnard, 1995</marker>
<rawString>L. Burnard. 1995. Users Reference Guide for the British National Corpus. version 1.0. Oxford University Computing Services, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
<author>D Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proc. of 4th DARPA Speech and Natural Language Workshop,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="4286" citStr="Gale et al. (1992)" startWordPosition="644" endWordPosition="647">iting of Japanese learners of English. They 595 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 595–602, Sydney, July 2006. c�2006 Association for Computational Linguistics also have shown that it is likely that performance of the error detection will improve as accuracy of the countability prediction increases since most of false positives are due to mispredictions. In this paper, we propose a method for reinforcing countability prediction by introducing a novel concept called one countability per discourse that is an extension of one sense per discourse proposed by Gale et al. (1992). It claims that when a noun appears more than once in a discourse, they will all share the same countability in the discourse. The basic idea of the proposed method is that initially mispredicted countability can be corrected using efficiently the one countability per discourse property. The next section introduces the one countability per discourse concept and shows that it can be a good source of evidence for predicting countability. Section 3 discusses how it can be efficiently exploited to predict countability. Section 4 describes the proposed method. Section 5 describes experiments condu</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W.A. Gale, K.W. Church, and D. Yarowsky. 1992. One sense per discourse. In Proc. of 4th DARPA Speech and Natural Language Workshop, pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Huddleston</author>
<author>G K Pullum</author>
</authors>
<title>The Cambridge Grammar of the English Language.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="21024" citStr="Huddleston and Pullum (2002)" startWordPosition="3424" endWordPosition="3427">xperiments 5.1 Experimental Conditions In the experiments, we chose Nagata et al. (2005a)’s method as the one to be reinforced by the proposed method. In this method, the decision list (DL) learning algorithm (Yarowsky, 1995) is used. However, we used the ME algorithm because we found that the method with the ME algorithm instead of the DL learning algorithm performed better when trained on the same training data. As the target noun, we selected 23 nouns that were also used in Nagata et al. (2005a)’s experiments. They are exemplified as nouns that are used as both countable and uncountable by Huddleston and Pullum (2002). Training data were generated from the written part of the British National Corpus (Burnard, 1995). A text tagged with the text tags was used as a discourse unit. From the corpus, 314 texts, which amounted to about 10% of all texts, were randomly taken to obtain test data. The rest of texts were used to generate training data. We evaluated performance of prediction by accuracy. We defined accuracy by the ratio of the number of correct predictions to that of instances of the target noun in the test data. 5.2 Experimental Procedures First, we generated training data for each target noun from th</context>
</contexts>
<marker>Huddleston, Pullum, 2002</marker>
<rawString>R. Huddleston and G.K. Pullum. 2002. The Cambridge Grammar of the English Language. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>F Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="2242" citStr="Lapata and Keller (2005)" startWordPosition="333" endWordPosition="336">e learners. Given countability, these errors can be detected in many cases. For example, an error can be detected from “We have a furniture.” given that the noun furniture is unAtsuo Kawai Mie University 5148507, Japan kawai@ai.info.mie-u.ac.jp Naoki Isu Mie University 5148507, Japan isu@ai.info.mie-u.ac.jp countable since uncountable nouns do not tolerate the indefinite article. Because of the wide range of applications, researchers have done a lot of work related to countability. Baldwin and Bond (2003a; 2003b) have proposed a method for automatically learning countability from corpus data. Lapata and Keller (2005) and Peng and Araki (2005) have proposed web-based models for learning countability. Others including Bond and VatikiotisBateson (2002) and O’Hara et al. (2003) use ontology to determine countability. In the application to error detection, researchers have explored alternative approaches since sources of evidence for determining countability are limited compared to other applications. Articles and the singular/plural distinction, which are informative for countability, cannot be used in countability prediction aiming at detecting errors in article usage and singular/plural usage. Returning to </context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>M. Lapata and F. Keller. 2005. Web-based models for natural language processing. ACM Transactions on Speech and Language Processing, 2(1):1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nagata</author>
<author>F Masui</author>
<author>A Kawai</author>
<author>N Isu</author>
</authors>
<title>An unsupervised method for distinguishing mass and count noun in context.</title>
<date>2005</date>
<booktitle>In Proc. of 6th International Workshop on Computational Semantics,</booktitle>
<pages>213--224</pages>
<contexts>
<context position="3383" citStr="Nagata et al. (2005" startWordPosition="504" endWordPosition="507"> at detecting errors in article usage and singular/plural usage. Returning to the previous example, the countability of the noun furniture cannot be determined as uncountable by the indefinite article; first, its countability has to be predicted without the indefinite article, and only then whether or not it tolerates the indefinite article is examined using the predicted countability. Also, unlike in machine translation, the source language is not given in the writing of second language learners such as essays, which means that information available is limited. To overcome these limitations, Nagata et al. (2005a) have proposed a method for predicting countability that relies solely on words (except articles and other determiners) surrounding the target noun. Nagata et al. (2005b) have shown that the method is effective to detecting errors in article usage and singular/plural usage in the writing of Japanese learners of English. They 595 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 595–602, Sydney, July 2006. c�2006 Association for Computational Linguistics also have shown that it is likely that performance of the error detection will improve as accuracy of the countabili</context>
<context position="13435" citStr="Nagata et al. (2005" startWordPosition="2185" endWordPosition="2188">quantifiers — — ’s genitives ? ? UNCOUNTABLE COUNTABLE plural? � yes COUNTABLE ?� yes � no � no modified by one of the words in Table 2(a)? modified by one of the words in Table 2(b)? modified by one of the words in Table 2(c)? � yes � yes modified by a little no � yes no UNCOUNTABLE no 4 Proposed Method 4.1 Generating Training Data As discussed in Subsection 3.2, training data are needed to exploit the one countability per discourse property. In other words, the proposed method requires a set of discourses in which instances of the target noun are tagged with their countability. Fortunately, Nagata et al. (2005b) have proposed a method for tagging nouns with their countability. This paper follows it to generate training data. To generate training data, first, instances of the target noun used as a head noun are collected from a corpus with their surrounding words. This can be simply done by an existing chunker or parser. Second, the collected instances are tagged with either countable or uncountable by tagging rules. For example, the underlined paper: ... read a paper in the morning ... is tagged as ... read a paper/countable in the morning ... because it is modified by the indefinite article. Figur</context>
<context position="20483" citStr="Nagata et al. (2005" startWordPosition="3330" endWordPosition="3333"> in the training data used in the method. , respectively; here, 0 and 1 correspond to countable and uncountable, respectively. Given the specification, almost any kind of machine learning algorithm cab be used to generate the model used in the proposed method. In this 599 is likely that previous mispredictions are overridden by correct ones. In the above example, the third one would be correctly overridden by countable because of the majority countability feature (MC=c) that is informative for the instance being countable. 5 Experiments 5.1 Experimental Conditions In the experiments, we chose Nagata et al. (2005a)’s method as the one to be reinforced by the proposed method. In this method, the decision list (DL) learning algorithm (Yarowsky, 1995) is used. However, we used the ME algorithm because we found that the method with the ME algorithm instead of the DL learning algorithm performed better when trained on the same training data. As the target noun, we selected 23 nouns that were also used in Nagata et al. (2005a)’s experiments. They are exemplified as nouns that are used as both countable and uncountable by Huddleston and Pullum (2002). Training data were generated from the written part of the</context>
<context position="22190" citStr="Nagata et al. (2005" startWordPosition="3628" endWordPosition="3632"> generated training data for each target noun from the texts using the tagging rules explained in Subsection 4.1. We used the OAK system5 to extract noun phrases and their heads. Of the extracted instances, we excluded those that had no contextual cues from the training data (and also the test data). We also generated another set of training data by removing the majority countability features from them. This set of training data was used for comparison. Second, we obtained test data by applying the tagging rules described in Subsection 4.1 to each instance of the target noun in the 314 texts. Nagata et al. (2005b) showed that the tagging rules 5http://www.cs.nyu.edu/ sekine/PROJECT/OAK/ achieved an accuracy of 0.997 in the texts that contained no errors. Considering these results, we used the tagging rules to obtain test data. Instances tagged with “?” were excluded in the experiments. Third, we applied the ME algorithm6 to the training data without the majority countability feature. Using the resulting model, countability of the target nouns in the test data was predicted. Then, the predictions were reinforced by the proposed method. The threshold to filter out spurious predictions was set to . For </context>
</contexts>
<marker>Nagata, Masui, Kawai, Isu, 2005</marker>
<rawString>R. Nagata, F. Masui, A. Kawai, and N. Isu. 2005a. An unsupervised method for distinguishing mass and count noun in context. In Proc. of 6th International Workshop on Computational Semantics, pages 213– 224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nagata</author>
<author>T Wakana</author>
<author>F Masui</author>
<author>A Kawai</author>
<author>N Isu</author>
</authors>
<title>Detecting article errors based on the mass count distinction.</title>
<date>2005</date>
<booktitle>In Proc. of 2nd International Joint Conference on Natural Language Processing,</booktitle>
<pages>815--826</pages>
<contexts>
<context position="3383" citStr="Nagata et al. (2005" startWordPosition="504" endWordPosition="507"> at detecting errors in article usage and singular/plural usage. Returning to the previous example, the countability of the noun furniture cannot be determined as uncountable by the indefinite article; first, its countability has to be predicted without the indefinite article, and only then whether or not it tolerates the indefinite article is examined using the predicted countability. Also, unlike in machine translation, the source language is not given in the writing of second language learners such as essays, which means that information available is limited. To overcome these limitations, Nagata et al. (2005a) have proposed a method for predicting countability that relies solely on words (except articles and other determiners) surrounding the target noun. Nagata et al. (2005b) have shown that the method is effective to detecting errors in article usage and singular/plural usage in the writing of Japanese learners of English. They 595 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 595–602, Sydney, July 2006. c�2006 Association for Computational Linguistics also have shown that it is likely that performance of the error detection will improve as accuracy of the countabili</context>
<context position="13435" citStr="Nagata et al. (2005" startWordPosition="2185" endWordPosition="2188">quantifiers — — ’s genitives ? ? UNCOUNTABLE COUNTABLE plural? � yes COUNTABLE ?� yes � no � no modified by one of the words in Table 2(a)? modified by one of the words in Table 2(b)? modified by one of the words in Table 2(c)? � yes � yes modified by a little no � yes no UNCOUNTABLE no 4 Proposed Method 4.1 Generating Training Data As discussed in Subsection 3.2, training data are needed to exploit the one countability per discourse property. In other words, the proposed method requires a set of discourses in which instances of the target noun are tagged with their countability. Fortunately, Nagata et al. (2005b) have proposed a method for tagging nouns with their countability. This paper follows it to generate training data. To generate training data, first, instances of the target noun used as a head noun are collected from a corpus with their surrounding words. This can be simply done by an existing chunker or parser. Second, the collected instances are tagged with either countable or uncountable by tagging rules. For example, the underlined paper: ... read a paper in the morning ... is tagged as ... read a paper/countable in the morning ... because it is modified by the indefinite article. Figur</context>
<context position="20483" citStr="Nagata et al. (2005" startWordPosition="3330" endWordPosition="3333"> in the training data used in the method. , respectively; here, 0 and 1 correspond to countable and uncountable, respectively. Given the specification, almost any kind of machine learning algorithm cab be used to generate the model used in the proposed method. In this 599 is likely that previous mispredictions are overridden by correct ones. In the above example, the third one would be correctly overridden by countable because of the majority countability feature (MC=c) that is informative for the instance being countable. 5 Experiments 5.1 Experimental Conditions In the experiments, we chose Nagata et al. (2005a)’s method as the one to be reinforced by the proposed method. In this method, the decision list (DL) learning algorithm (Yarowsky, 1995) is used. However, we used the ME algorithm because we found that the method with the ME algorithm instead of the DL learning algorithm performed better when trained on the same training data. As the target noun, we selected 23 nouns that were also used in Nagata et al. (2005a)’s experiments. They are exemplified as nouns that are used as both countable and uncountable by Huddleston and Pullum (2002). Training data were generated from the written part of the</context>
<context position="22190" citStr="Nagata et al. (2005" startWordPosition="3628" endWordPosition="3632"> generated training data for each target noun from the texts using the tagging rules explained in Subsection 4.1. We used the OAK system5 to extract noun phrases and their heads. Of the extracted instances, we excluded those that had no contextual cues from the training data (and also the test data). We also generated another set of training data by removing the majority countability features from them. This set of training data was used for comparison. Second, we obtained test data by applying the tagging rules described in Subsection 4.1 to each instance of the target noun in the 314 texts. Nagata et al. (2005b) showed that the tagging rules 5http://www.cs.nyu.edu/ sekine/PROJECT/OAK/ achieved an accuracy of 0.997 in the texts that contained no errors. Considering these results, we used the tagging rules to obtain test data. Instances tagged with “?” were excluded in the experiments. Third, we applied the ME algorithm6 to the training data without the majority countability feature. Using the resulting model, countability of the target nouns in the test data was predicted. Then, the predictions were reinforced by the proposed method. The threshold to filter out spurious predictions was set to . For </context>
</contexts>
<marker>Nagata, Wakana, Masui, Kawai, Isu, 2005</marker>
<rawString>R. Nagata, T. Wakana, F. Masui, A. Kawai, and N. Isu. 2005b. Detecting article errors based on the mass count distinction. In Proc. of 2nd International Joint Conference on Natural Language Processing, pages 815–826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T O’Hara</author>
<author>N Salay</author>
<author>M Witbrock</author>
<author>D Schneider</author>
<author>B Aldag</author>
<author>S Bertolo</author>
<author>K Panton</author>
<author>F Lehmann</author>
<author>J Curtis</author>
<author>M Smith</author>
<author>D Baxter</author>
<author>P Wagner</author>
</authors>
<title>Inducing criteria for mass noun lexical mappings using the Cyc KB, and its extension to WordNet.</title>
<date>2003</date>
<booktitle>In Proc. of 5th International Workshop on Computational Semantics,</booktitle>
<pages>425--441</pages>
<marker>O’Hara, Salay, Witbrock, Schneider, Aldag, Bertolo, Panton, Lehmann, Curtis, Smith, Baxter, Wagner, 2003</marker>
<rawString>T. O’Hara, N. Salay, M. Witbrock, D. Schneider, B. Aldag, S. Bertolo, K. Panton, F. Lehmann, J. Curtis, M. Smith, D. Baxter, and P. Wagner. 2003. Inducing criteria for mass noun lexical mappings using the Cyc KB, and its extension to WordNet. In Proc. of 5th International Workshop on Computational Semantics, pages 425–441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peng</author>
<author>K Araki</author>
</authors>
<title>Detecting the countability of English compound nouns using web-based models.</title>
<date>2005</date>
<booktitle>In Companion Volume to Proc. of 2nd International Joint Conference on Natural Language Processing,</booktitle>
<pages>105--109</pages>
<contexts>
<context position="2268" citStr="Peng and Araki (2005)" startWordPosition="338" endWordPosition="341">y, these errors can be detected in many cases. For example, an error can be detected from “We have a furniture.” given that the noun furniture is unAtsuo Kawai Mie University 5148507, Japan kawai@ai.info.mie-u.ac.jp Naoki Isu Mie University 5148507, Japan isu@ai.info.mie-u.ac.jp countable since uncountable nouns do not tolerate the indefinite article. Because of the wide range of applications, researchers have done a lot of work related to countability. Baldwin and Bond (2003a; 2003b) have proposed a method for automatically learning countability from corpus data. Lapata and Keller (2005) and Peng and Araki (2005) have proposed web-based models for learning countability. Others including Bond and VatikiotisBateson (2002) and O’Hara et al. (2003) use ontology to determine countability. In the application to error detection, researchers have explored alternative approaches since sources of evidence for determining countability are limited compared to other applications. Articles and the singular/plural distinction, which are informative for countability, cannot be used in countability prediction aiming at detecting errors in article usage and singular/plural usage. Returning to the previous example, the </context>
</contexts>
<marker>Peng, Araki, 2005</marker>
<rawString>J. Peng and K. Araki. 2005. Detecting the countability of English compound nouns using web-based models. In Companion Volume to Proc. of 2nd International Joint Conference on Natural Language Processing, pages 105–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proc. of 33rd Annual Meeting ofACL,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="5255" citStr="Yarowsky (1995)" startWordPosition="804" endWordPosition="805">oncept and shows that it can be a good source of evidence for predicting countability. Section 3 discusses how it can be efficiently exploited to predict countability. Section 4 describes the proposed method. Section 5 describes experiments conducted to evaluate the proposed method and discusses the results. 2 One Countability per Discourse One countability per discourse is an extension of one sense per discourse proposed by Gale et al. (1992). One sense per discourse claims that when a polysemous word appears more than once in a discourse it is likely that they will all share the same sense. Yarowsky (1995) tested the claim on about 37,000 examples and found that when a polysemous word appeared more than once in a discourse, they took on the majority sense for the discourse 99.8% of the time on average. Based on one sense per discourse, we hypothesize that when a noun appears more than once in a discourse, they will all share the same countability in the discourse, that is, one countability per discourse. The motivation for this hypothesis is that if one sense per discourse is satisfied, so is one countability per discourse because countability is often determined by word sense. For example, if </context>
<context position="20621" citStr="Yarowsky, 1995" startWordPosition="3356" endWordPosition="3357">fication, almost any kind of machine learning algorithm cab be used to generate the model used in the proposed method. In this 599 is likely that previous mispredictions are overridden by correct ones. In the above example, the third one would be correctly overridden by countable because of the majority countability feature (MC=c) that is informative for the instance being countable. 5 Experiments 5.1 Experimental Conditions In the experiments, we chose Nagata et al. (2005a)’s method as the one to be reinforced by the proposed method. In this method, the decision list (DL) learning algorithm (Yarowsky, 1995) is used. However, we used the ME algorithm because we found that the method with the ME algorithm instead of the DL learning algorithm performed better when trained on the same training data. As the target noun, we selected 23 nouns that were also used in Nagata et al. (2005a)’s experiments. They are exemplified as nouns that are used as both countable and uncountable by Huddleston and Pullum (2002). Training data were generated from the written part of the British National Corpus (Burnard, 1995). A text tagged with the text tags was used as a discourse unit. From the corpus, 314 texts, which</context>
<context position="26382" citStr="Yarowsky, 1995" startWordPosition="4318" endWordPosition="4319">plots would be on the line. Plots above the line mean improvement over “ME” and the distance from the line expresses the amount of improvement. Plots below the line mean the opposite. Figure 2 clearly shows that most of the plots ( ) corresponding to the comparison between “ME” and “Proposed” are above the line. This means that the proposed method successfully reinforced “ME” in most of the target nouns. Indeed, the average accuracy of “Proposed” is significantly superior to that of “ME” at the 99% confidence level (paired t-test). This improvement is close to that of one sense per discourse (Yarowsky, 1995) (improvement ranging from 1.3% to 1.7%), which seems to be a sensible upper bound of the proposed method. By contrast, about half of the plots ( ) corresponding to the comparison between “ME” and “ME+MCD” are below the line. From these results, it follows that the one countability per discourse property is a good source of evidence for predicting countability, but it is crucial to devise a way of exploiting the property as we did in this paper. Namely, simply replacing original predictions with the majority countability for the discourse causes side effects, which has been already suggested i</context>
<context position="28741" citStr="Yarowsky, 1995" startWordPosition="4694" endWordPosition="4695">method is effective in reinforcing English countability prediction. In addition, the proposed method has two advantages. The first is its applicability. It can reinforce almost any earlier method. Even to handcoded rules, it can be applied as long as they give predictions with their confidences. This further gives an additional advantage. Recall that the instances tagged with “?&apos; by the tagging rules are discarded when training data are generated as described in Subsection 4.1. These instances can be retagged with their countability by using the proposed method and some kind of bootstrapping (Yarowsky, 1995). This means increase in training data, which might eventually result in further improvement. The second is that the proposed method is unsupervised. It requires no human intervention to reinforce countability prediction. For future work, we will investigate what models are most appropriate for exploiting the one countability per discourse property. We will also explore a method for including instances tagged with “?&apos; in training data by using the proposed method and bootstrapping. Acknowledgments The authors would like to thank Satoshi Sekine who has developed the OAK System. The authors also</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. of 33rd Annual Meeting ofACL, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>