<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001623">
<title confidence="0.955424">
Coordinate Noun Phrase Disambiguation in a Generative Parsing Model
</title>
<author confidence="0.961997">
Deirdre Hogan*
</author>
<affiliation confidence="0.9795865">
Computer Science Department
Trinity College Dublin
</affiliation>
<address confidence="0.758539">
Dublin 2, Ireland
</address>
<email confidence="0.997284">
dhogan@computing.dcu.ie
</email>
<sectionHeader confidence="0.995632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999586230769231">
In this paper we present methods for im-
proving the disambiguation of noun phrase
(NP) coordination within the framework of a
lexicalised history-based parsing model. As
well as reducing noise in the data, we look at
modelling two main sources of information
for disambiguation: symmetry in conjunct
structure, and the dependency between con-
junct lexical heads. Our changes to the base-
line model result in an increase in NP coor-
dination dependency f-score from 69.9% to
73.8%, which represents a relative reduction
in f-score error of 13%.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9752350625">
Coordination disambiguation is a relatively little
studied area, yet the correct bracketing of coordina-
tion constructions is one of the most difficult prob-
lems for natural language parsers. In the Collins
parser (Collins, 1999), for example, dependencies
involving coordination achieve an f-score as low as
61.8%, by far the worst performance of all depen-
dency types.
Take the phrase busloads of executives and their
wives (taken from the WSJ treebank). The coordi-
nating conjunction (CC) and and the noun phrase
their wives could attach to the noun phrase exec-
utives, as illustrated in Tree 1, Figure 1. Alterna-
tively, their wives could be incorrectly conjoined to
the noun phrase busloads of executives as in Tree 2,
Figure 1.
</bodyText>
<note confidence="0.265574">
* Now at the National Centre for Language Technology,
Dublin City University, Ireland.
</note>
<bodyText confidence="0.998253941176471">
As with PP attachment, most previous attempts
at tackling coordination as a subproblem of parsing
have treated it as a separate task to parsing and it
is not always obvious how to integrate the methods
proposed for disambiguation into existing parsing
models. We therefore approach coordination disam-
biguation, not as a separate task, but from within the
framework of a generative parsing model.
As noun phrase coordination accounts for over
50% of coordination dependency error in our base-
line model we focus on NP coordination. Us-
ing a model based on the generative parsing model
of (Collins, 1999) Model 1, we attempt to improve
the ability of the parsing model to make the correct
coordination decisions. This is done in the context
of parse reranking, where the n-best parses output
from Bikel’s parser (Bikel, 2004) are reranked ac-
cording to a generative history-based model.
In Section 2 we summarise previous work on co-
ordination disambiguation. There is often a consid-
erable bias toward symmetry in the syntactic struc-
ture of two conjuncts and in Section 3 we introduce
new parameter classes to allow the model to prefer
symmetry in conjunct structure. Section 4 is con-
cerned with modelling the dependency between con-
junct head words and begins by looking at how the
different handling of coordination in noun phrases
and base noun phrases (NPB) affects coordination
disambiguation.1 We look at how we might improve
the model’s handling of coordinate head-head de-
pendencies by altering the model so that a common
&apos;A base noun phrase, as defined in (Collins, 1999), is a noun
phrase which does not directly dominate another noun phrase,
unless that noun phrase is possessive.
</bodyText>
<page confidence="0.968255">
680
</page>
<note confidence="0.8831345">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 680–687,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<figure confidence="0.4559155">
1. NP
2. NP
</figure>
<figureCaption confidence="0.999823">
Figure 1: Tree 1. The correct noun phrase parse.
Tree 2. The incorrect parse for the noun phrase.
</figureCaption>
<bodyText confidence="0.999781352941177">
parameter class is used for coordinate word prob-
ability estimation in both NPs and NPBs. In Sec-
tion 4.2 we focus on improving the estimation of
this parameter class by incorporating BNC data, and
a measure of word similarity based on vector cosine
similarity, to reduce data sparseness. In Section 5 we
suggest a new head-finding rule for NPBs so that the
lexicalisation process for coordinate NPBs is more
similar to that of other NPs.
Section 6 examines inconsistencies in the annota-
tion of coordinate NPs in the Penn Treebank which
can lead to errors in coordination disambiguation.
We show how some coordinate noun phrase incon-
sistencies can be automatically detected and cleaned
from the data sets. Section 7 details how the model is
evaluated, presents the experiments made and gives
a breakdown of results.
</bodyText>
<sectionHeader confidence="0.995622" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.9999469375">
Most previous attempts at tackling coordination
have focused on a particular type of NP coordination
to disambiguate. Both Resnik (1999) and Nakov and
Hearst (2005) consider NP coordinations of the form
n1 and n2 n3 where two structural analyses are pos-
sible: ((n1 and n2) n3) and ((n1) and (n2 n3)). They
aim to show more structure than is shown in trees
following the Penn guidelines, whereas in our ap-
proach we aim to reproduce Penn guideline trees.
To resolve the ambiguities, Resnik combines num-
ber agreement information of candidate conjoined
nouns, an information theoretic measure of semantic
similarity, and a measure of the appropriateness of
noun-noun modification. Nakov and Hearst (2005)
disambiguate by combining Web-based statistics on
head word co-occurrences with other mainly heuris-
tic information sources.
A probabilistic approach is presented in (Gold-
berg, 1999), where an unsupervised maximum en-
tropy statistical model is used to disambiguate coor-
dinate noun phrases of the form n1 preposition n2
cc n3. Here the problem is framed as an attachment
decision: does n3 attach ‘high’ to the first noun, n1,
or ‘low’ to n2?
In (Agarwal and Boggess, 1992) the task is to
identify pre-CC conjuncts which appear in text that
has been part-of-speech (POS) tagged and semi-
parsed, as well as tagged with semantic labels spe-
cific to the domain. The identification of the pre-
CC conjunct is based on heuristics which choose the
pre-CC conjunct that maximises the symmetry be-
tween pre- and post-CC conjuncts.
Insofar as we do not separate coordination dis-
ambiguation from the overall parsing task, our ap-
proach resembles the efforts to improve coordi-
nation disambiguation in (Kurohashi, 1994; Rat-
naparkhi, 1994; Charniak and Johnson, 2005).
In (Kurohashi, 1994) coordination disambiguation
is carried out as the first component of a Japanese
dependency parser using a technique which calcu-
lates similarity between series of words from the left
and right of a conjunction. Similarity is measured
based on matching POS tags, matching words and a
thesaurus-based measure of semantic similarity. In
both the discriminative reranker of Ratnaparkhi et
al. (1994) and that of Charniak and Johnson (2005)
features are included to capture syntactic parallelism
across conjuncts at various depths.
</bodyText>
<sectionHeader confidence="0.696599" genericHeader="method">
3 Modelling Symmetry Between Conjuncts
</sectionHeader>
<bodyText confidence="0.99942525">
There is often a considerable bias toward symme-
try in the syntactic structure of two conjuncts, see
for example (Dubey et al., 2005). Take Figure 2: If
we take as level 0 the level in the coordinate sub-
</bodyText>
<figure confidence="0.99068075">
NP
PP
NP
and NP
NPB
their wives
NPB
busloads
of
NP
NPB
executives
busloads
of NP
NPB
executives
and NP
NPB
their wives
NPB
PP
NP
681
NP1(plains)
</figure>
<figureCaption confidence="0.993246">
Figure 2: Example of symmetry in conjunct structure in a lexicalised subtree.
</figureCaption>
<figure confidence="0.996058833333333">
CC11
and
NP11(states)
IN17
NP18(Delta)
the
Delta
NP3(plains)
DT6
the
PP7(of)
IN8 NP9(Texas)
of NNP10
Texas
NP12(states)
DT15
the
NNS4
plains
JJ5
high
JJ14
northern
PP16(of)
NNS13
of
DT20
NNP19
NP2(plains)
states
</figure>
<bodyText confidence="0.999945195652174">
tree where the coordinating conjunction CC occurs,
then there is exact symmetry in the two conjuncts in
terms of non-terminal labels and head word part-of-
speech tags for levels 0, 1 and 2. Learning a bias
toward parallelism in conjuncts should improve the
parsing model’s ability to correctly attach a coordi-
nation conjunction and second conjunct to the cor-
rect position in the tree.
In history-based models, features are limited to
being functions of the tree generated so far. The task
is to incorporate a feature into the model that cap-
tures a particular bias yet still adheres to derivation-
based restrictions. Parses are generated top-down,
head-first, left-to-right. Each node in the tree in
Figure 2 is annotated with the order the nodes are
generated (we omit, for the sake of clarity, the gen-
eration of the STOP nodes). Note that when the
decision to attach the second conjunct to the head
conjunct is being made (i.e. Step 11, when the CC
and NP(states) nodes are being generated) the sub-
tree rooted at NP(states) has not yet been generated.
Thus at the point that the conjunct attachment de-
cision is made it is not possible to use information
about symmetry of conjunct structure, as the struc-
ture of the second conjunct is not yet known.
It is possible, however, to condition on structure
of the already generated head conjunct when build-
ing the internal structure of the second conjunct. In
our model when the structure of the second conjunct
is being generated we condition on features which
are functions of the first conjunct. When generat-
ing a node Ni in the second conjunct, we retrieve
the corresponding node NipreCC in the first conjunct,
via a left to right traversal of the first conjunct. For
example, from Figure 2 the pre-CC node NP(Texas)
is the node corresponding to NP(Delta) in the post-
CC conjunct. From NipreCC we extract information,
such as its part-of-speech, for use as a feature when
predicting a POS tag for the corresponding node in
the post-CC conjunct.
When generating a second conjunct, instead of
the usual parameter classes for estimating the prob-
ability of the head label Ch and the POS label of a
dependent node ti, we created two new parameter
classes which are used only in the generation of sec-
ond conjunct nodes:
</bodyText>
<equation confidence="0.947886333333333">
PccCh(Ch|ry(headC), Cp, wp, tp, tgp, depth) (1)
Pccti(ti|α(headC), dir, Cp, wp, tp, dist, ti 1, ti 2, depth)
(2)
</equation>
<bodyText confidence="0.999949666666667">
where -y(headC) returns the non-terminal label of
NipreCC for the node in question and α(headC) re-
turns the POS tag of NipreCC . Both functions return
+NOMATCH+ if there is no NipreCC for the node.
Depth is the level of the post-CC conjunct node be-
ing generated.
</bodyText>
<sectionHeader confidence="0.980283" genericHeader="method">
4 Modelling Coordinate Head Words
</sectionHeader>
<bodyText confidence="0.999914818181818">
Some noun pairs are more likely to be conjoined
than others. Take again the trees in Figure 1. The
two head nouns coordinated in Tree 1 are execu-
tives and wives, and in Tree 2: busloads and wives.
Clearly, the former pair of head nouns is more likely
and, for the purpose of discrimination, the model
would benefit if it could learn that executives and
wives is a more likely combination than busloads
and wives.
Bilexical head-head dependencies of the type
found in coordinate structures are a somewhat dif-
</bodyText>
<page confidence="0.993359">
682
</page>
<bodyText confidence="0.999311555555556">
ferent class of dependency to modifier-head depen-
dencies. In the fat cat, for example, there is clearly
one head to the noun phrase: cat. In cats and dogs
however there are two heads, though in the parsing
model just one is chosen, somewhat arbitrarily, to
head the entire noun phrase.
In the baseline model there is essentially one pa-
rameter class for the estimation of word probabili-
ties:
</bodyText>
<equation confidence="0.996026">
Pword(wi|H(i)) (3)
</equation>
<bodyText confidence="0.999649076923077">
where wi is the lexical head of constituent i and
H(i) is the history of the constituent. The history is
made up of conditioning features chosen from struc-
ture that has already been determined in the top-
down derivation of the tree.
In Section 4.1 we discuss how though the coordi-
nate head-head dependency is captured for NPs, it is
not captured for NPBs. We look at how we might
improve the model’s handling of coordinate head-
head dependencies by altering the model so that a
common parameter class in (4) is used for coordi-
nate word probability estimation in both NPs and
NPBs.
</bodyText>
<equation confidence="0.91693">
PcoordWord(wi|wp, H(i)) (4)
</equation>
<bodyText confidence="0.99929">
In Section 4.2 we focus on improving the estimation
of this parameter class by reducing data sparseness.
</bodyText>
<subsectionHeader confidence="0.992005">
4.1 Extending PcoordWord to Coordinate NPBs
</subsectionHeader>
<bodyText confidence="0.99625075">
In the baseline model each node in the tree is an-
notated with a coordination flag which is set to true
for the node immediately following the coordinating
conjunction. For coordinate NPs the head-head de-
pendency is captured when this flag is set to true. In
Figure 1, discarding for simplicity the other features
in the history, the probability of the coordinate head
wives, is estimated in Tree 1 as:
</bodyText>
<equation confidence="0.9842885">
Pword(wi = wives|coord = true, wp = executives, ...)
(5)
</equation>
<bodyText confidence="0.948519">
and in Tree 2:
</bodyText>
<equation confidence="0.988774">
Pword(wi = wives|coord = true, wp = busloads,...) (6)
</equation>
<bodyText confidence="0.999944">
where wp is the head word of the node to which the
node headed by wi is attaching and coord is the co-
ordination flag.
Unlike NPs, in NPBs (i.e. flat, non-recursive NPs)
the coordination flag is not used to mark whether a
node is a coordinated head or not. This flag is always
set to false for NPBs. In addition, modifiers within
NPBs are conditioned on the previously generated
modifier rather than the head of the phrase.2 This
means that in an NPB such as (cats and dogs), the
estimate for the word cats will look like:
</bodyText>
<equation confidence="0.996109">
Pword(wi = cats|coord = false, wp = and,...) (7)
</equation>
<bodyText confidence="0.999996875">
In our new model, for NPs, when the coordination
flag is set to true, we use the parameter class in (4)
to estimate the probability of one lexical head noun,
given another. For NPBs, if a noun is generated di-
rectly after a CC then it is taken to be a coordinate
head, wi, and conditioned on the noun generated be-
fore the coordinating conjunction, which is chosen
as wp, and also estimated using (4).
</bodyText>
<subsectionHeader confidence="0.988276">
4.2 Estimating the PcoordWord parameter class
</subsectionHeader>
<bodyText confidence="0.999915933333333">
Data for bilexical statistics are particularly sparse.
In order to decrease the sparseness of the coordinate
head noun data, we extracted from the BNC exam-
ples of coordinate head noun pairs. We extracted all
noun pairs occurring in a pattern of the form: noun
cc noun, as well as lists of any number of nouns
separated by commas and ending in cc noun.3 To
this data we added all head noun pairs from the WSJ
that occurred together in a coordinate noun phrase,
identified when the coordination flag was set to true.
Every occurrence ni CC nj was also counted as an
occurrence of nj CC ni. This further helps reduce
sparseness.
The probability of one noun, ni being coordinated
with another nj can be calculated simply as:
</bodyText>
<equation confidence="0.983581">
Plex(ni|nj) = |ninj |(8)
|nj|
</equation>
<bodyText confidence="0.999940375">
Again to reduce data sparseness, we introduce a
measure of word similarity. A word can be rep-
resented as a vector where every dimension of the
vector represents another word type. The values of
the vector components, the term weights, are derived
from word co-occurrence counts. Cosine similar-
ity between two word vectors can then be used to
measure the similarity of two words. Measures of
</bodyText>
<footnote confidence="0.9984282">
2A full explanation of the handling of coordination in the
model is given in (Bikel, 2004).
3Extracting coordinate noun pairs from the BNC in such
a fashion follows work on networks of concepts described
in (Widdows, 2004).
</footnote>
<page confidence="0.999073">
683
</page>
<bodyText confidence="0.999201714285714">
similarity between words based on similarity of co-
occurrence vectors have been used before, for exam-
ple, for word sense disambiguation (Sch¨utze, 1998)
and for PP-attachment disambiguation (Zhao and
Lin, 2004). Our measure resembles that of (Cara-
ballo, 99) where co-occurrence is also defined with
respect to coordination patterns, although the exper-
imental details in terms of data collection and vector
term weights differ.
We can now incorporate the similarity measure
into the probability estimate of (8) to give a new
k-NN style method of estimating bilexical statistics
based on weighting events according to the word
similarity measure:
</bodyText>
<equation confidence="0.997632666666667">
EnxEN(n.) sim(nj, nx)|ninx|
Psim (ni |nj) = r &apos; (9)
EnxEN(n j) sim(nj, nx)|nx
</equation>
<bodyText confidence="0.999863375">
where sim(nj, n,,) is a similarity score between
words nj and n,, and N(nj) is the set of words in
the neighbourhood of nj. This neighbourhood can
be based on the k-nearest neighbours of nj, where
nearness is measured with the similarity function.
In order to smooth the bilexical estimate in (9) we
combine it with another estimate, trained from WSJ
data, by way of linear interpolation:
</bodyText>
<equation confidence="0.9997905">
PcoordWord(ni|nj) _
AnjPsim(ni|nj) + (1 − Anj)PMLE(ni|ti) (10)
</equation>
<bodyText confidence="0.999973142857143">
where ti is the POS tag of word ni, PMLE(ni|ti)
is the maximum-likelihood estimate calculated from
annotated WSJ data, and Anj is calculated as in (11).
In (11) we adapt the Witten-Bell method for the
calculation of the weight A, as used in the Collins
parser, so that it incorporates the similarity measure
for all words in the neighbourhood of nj.
</bodyText>
<equation confidence="0.9974855">
Anj = E (11)
nxEN(nj) sim(nj, nx)|nx|
Er //
nxEN(nj) sim(nj, nx)(|nx |+ CD(nx))
</equation>
<bodyText confidence="0.999925727272727">
where C is a constant that can be optimised using
held-out data and D(nj) is the diversity of a word
nj: the number of distinct words with which nj has
been coordinated in the training set.
The estimate in (9) can be viewed as the estimate
with the more general history context than that of (8)
because the context includes not only nj but also
words similar to nj. The final probability estimate
for PcoordWord is calculated as the most specific es-
timate, PLe,,, combined via regular Witten-Bell inter-
polation with the estimate in (10).
</bodyText>
<sectionHeader confidence="0.970921" genericHeader="method">
5 NPB Head-Finding Rules
</sectionHeader>
<bodyText confidence="0.999990029411765">
Head-finding rules for coordinate NPBs differ from
coordinate NPs.4 Take the following two versions
of the noun phrase hard work and harmony: (c) (NP
(NPB hard work and harmony)) and (d) (NP (NP
(NPB hard work)) and (NP (NPB harmony))). In the
first example, harmony is chosen as head word of the
NP; in example (d) the head of the entire NP is work.
The choice of head affects the various dependencies
in the model. However, in the case of two coordinate
NPs which, as in the above example, cover the same
span of words and differ only in whether the coordi-
nate noun phrase is flat as in (c) or structured as in
(d), the choice of head for the phrase is not particu-
larly informative. In both cases the head words be-
ing coordinated are the same and either word could
plausibly head the phrase; discrimination between
trees in such cases should not be influenced by the
choice of head, but rather by other, salient features
that distinguish the trees.5
We would like to alter the head-finding rules for
coordinate NPBs so that, in cases like those above,
the word chosen to head the entire coordinate noun
phrase would be the same for both base and non-
base noun phrases. We experiment with slightly
modified head-finding rules for coordinate NPBs. In
an NPB such as NPB —* n1 CC n2 n3, the head rules
remain unchanged and the head of the phrase is (usu-
ally) the rightmost noun in the phrase. Thus, when
n2 is immediately followed by another noun the de-
fault is to assume nominal modifier coordination and
the head rules stay the same. The modification we
make to the head rules for NPBs is as follows: when
n2 is not immediately followed by a noun then the
noun chosen to head the entire phrase is n1.
</bodyText>
<sectionHeader confidence="0.985723" genericHeader="method">
6 Inconsistencies in WSJ Coordinate NP
Annotation
</sectionHeader>
<bodyText confidence="0.9692585">
An inspection of NP coordination error in the base-
line model revealed inconsistencies in WSJ annota-
</bodyText>
<footnote confidence="0.987433">
4See (Collins, 1999) for the rules used in the baseline model.
5For example, it would be better if discrimination was
largely based on whether hard modifies both work and harmony
(c), or whether it modifies work alone (d).
</footnote>
<page confidence="0.993968">
684
</page>
<bodyText confidence="0.988440722772277">
tion. In this section we outline some types of co- non-nominal modifier, which is unambiguously un-
ordinate NP inconsistency and outline a method for shared, is part of a noun phrase as (a) above, or it
detecting some of these inconsistencies, which we conjoined with another modifier as in (b). We found
later use to automatically clean noise from the data. 202 examples of this in the training set, out of a total
Eliminating noise from treebanks has been previ- of 4,895 coordinate base noun phrases.
ously used successfully to increase overall parser ac- Finally, inconsistencies in POS tagging can also
curacy (Dickinson and Meurers, 2005). lead to problems with coordination. Take the bi-
The annotation of NPs in the Penn Treebank (Bies gram executive officer. We found 151 examples in
et al., 1995) follows somewhat different guidelines the training set of a base noun phrase which ended
to that of other syntactic categories. Because their with this bigram. 48% of the cases were POS tagged
interpretation is so ambiguous, no internal structure JJNN, 52% tagged NNNN. 8 This has repercussions
is shown for nominal modifiers. For NPs with more for coordinate noun phrase structure, as the presence
than one head noun, if the only unshared modifiers of an adjectival pre-modifier indicates a structured
in the constituent are nominal modifiers, then a flat annotation should be given.
structure is also given. Thus in (NP the Manhattan These inconsistencies pose problems both for
phone book and tour guide)6 a flat structure is given training and testing. With a relatively large amount
because although the is a non-nominal modifier, it is of noise in the training set the model learns to give
shared, modifying both tour guide and phone book, structures, which should be very unlikely, too high
and all other modifiers in the phrase are nominal. a probability. In testing, given inconsistencies in
However, we found that out of 1,417 examples the gold standard trees, it becomes more difficult
of NP coordination in sections 02 to 21, involving to judge how well the model is doing. Although it
phrases containing only nouns (common nouns or a would be difficult to automatically detect the POS
mixture of common and proper nouns) and the co- tagging errors, the other inconsistencies outlined
ordinating conjunction, as many as 21.3%, contrary above can be detected automatically by simple pat-
to the guidelines, were given internal structure, in- tern matching. Automatically eliminating such ex-
stead of a flat annotation. When all proper nouns are amples is a simple method of cleaning the data.
involved this phenomenon is even more common.7
Another common source of inconsistency in co-
ordinate noun phrase bracketing occurs when a non-
nominal modifier appears in the coordinate noun
phrase. As previously discussed, according to the
guidelines the modifier is annotated flat if it is
shared. When the non-nominal modifier is un-
shared, more internal structure is shown, as in:
(NP (NP (NNS fangs)) (CC and) (NP (JJ pointed)
(NNS ears))). However, the following two struc-
tured phrases, for example, were given a com-
pletely flat structure in the treebank: (a) (NP (NP
(NN oversight))(CC and) (NP (JJ disciplinary)(NNS
procedures))), (b) (NP (ADJP (JJ moderate)(CC
and)(JJ low-cost))(NN housing)). If we follow the
guidelines then any coordinate NPB which ends
with the following tag sequence can be automat-
ically detected as incorrectly bracketed: CC/non-
nominal modifier/noun. This is because either the
7 Experimental Evaluation
We use a parsing model similar to that described
in (Hogan, 2005) which is based on (Collins, 1999)
Model 1 and uses k-NN for parameter estimation.
The n-best output from Bikel’s parser (Bikel, 2004)
is reranked according to this k-NN parsing model,
which achieves an f-score of 89.4% on section 23.
For the coordination experiments, sections 02 to 21
are used for training, section 23 for testing and the
remaining sections for validation. Results are for
sentences containing 40 words or less.
As outlined in Section 6, the treebank guide-
lines are somewhat ambiguous as to the appropriate
bracketing for coordinate NPs which consist entirely
of proper nouns. We therefore do not include, in the
coordination test and validation sets, coordinate NPs
where in the gold standard NP the leaf nodes consist
entirely of proper nouns (or CCs or commas). In do-
6In this section we do not show the NPB levels.
7In the guidelines it is recognised however that proper names
are frequently annotated with internal structure.
8According to the POS bracketing guidelines (Santorini,
1991) the correct sequence of POS tags should be MVAW.
685
ing so we hope to avoid a situation whereby the suc-
cess of the model is measured in part by how well
it can predict the often inconsistent bracketing deci-
sions made for a particular portion of the treebank.
In addition, and for the same reasons, if a gold
standard tree is inconsistent with the guidelines in
either of the following two ways the tree is not used
when calculating coordinate precision and recall of
the model: the gold tree is a noun phrase which ends
with the sequence CC/non-nominal motifier/noun;
the gold tree is a structured coordinate noun phrase
where each word in the noun phrase is a noun.9 Call
these inconsistencies type a and type b respectively.
This left us with a coordination validation set con-
sisting of 1064 coordinate noun phrases and a test
set of 416 coordinate NPs from section 23.
A coordinate phrase was deemed correct if the
parent constituent label, and the two conjunct node
labels (at level 0) match those in the gold subtree and
if, in addition, each of the conjunct head words are
the same in both test and gold tree. This follows the
definition of a coordinate dependency in (Collins,
1999). Based on these criteria, the baseline f-scores
for test and validation set were 69.1% and 67.1% re-
spectively. The coordination f-score for the oracle
trees on section 23 is 83.56%. In other words: if an
‘oracle’ were to choose from each set of n-best trees
the tree that maximised constituent precision and re-
call, then the resulting set of oracle trees would have
a NP coordination dependency f-score of 83.56%.
For the validation set the oracle trees coordination
dependency f-score is 82.47%.
</bodyText>
<subsectionHeader confidence="0.949707">
7.1 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.999646636363636">
We first eliminated from the training set all coordi-
nate noun phrase subtrees, of type a and type b de-
scribed in Section 7. The effect of this on the vali-
dation set is outlined in Table 1, step 2.
For the new parameter class in (1) we found that
the best results occurred when it was used only in
conjuncts of depth 1 and 2, although the case base
for this parameter class contained head events from
all post-CC conjunct depths. Parameter class (2) was
used for predicting POS tags at level 1 in right-of-
head conjuncts, although again the sample contained
</bodyText>
<footnote confidence="0.615283">
9Recall from §6 that for this latter case the noun phrase
should be flat - an NPB - rather than a noun phrase with internal
structure.
</footnote>
<table confidence="0.99955425">
Model f-score significance
1. Baseline 67.1
2. NoiseElimination 68.7 &gt;&gt; 1
3. Symmetry 69.9 &gt; 2, &gt; &gt;1
4. NPB head rule 70.6 NOT &gt; 3, &gt; 2, &gt;&gt; 1
5. P.ordWord WSJ 71.7 NOT &gt; 4, &gt; 3, &gt;&gt; 2
6. BNC data 72.1 NOT &gt; 5, &gt; 4, &gt;&gt; 3
7. sim(wi, w,) 72.4 NOT &gt; 6, NOT &gt; 5, &gt;&gt; 4
</table>
<tableCaption confidence="0.8267294">
Table 1: Results on the Validation Set. 1064 coordi-
nate noun phrase dependencies. In the significance
column &gt; means at level .05 and » means at level
.005, for McNemar’s test of significance. Results are
cumulative.
</tableCaption>
<bodyText confidence="0.99080459375">
events from all depths.
For the PcoordWord parameter class we extracted
9961 coordinate noun pairs from the WSJ train-
ing set and 815,323 pairs from the BNC. As pairs
are considered symmetric this resulted in a total of
1,650,568 coordinate noun events. The term weights
for the word vectors were dampened co-occurrence
counts, of the form: 1 + log(count). For the es-
timation of P�i,t(ni nj) we found it too computa-
tionally expensive to calculate similarity measures
between nj and each word token collected. The best
results were obtained when the neighbourhood of nj
was taken to be the k-nearest neighbours of nj from
among the set of word that had previously occurred
in a coordination pattern with nj, where k is 1000.
Table 1 shows the effect of the PcoordWord parame-
ter class estimated from WSJ data only (step 5), with
the addition of BNC data (step 6) and finally with the
word similarity measure (step 7).
The result of these experiments, as well as that
involving the change in the head-finding heuristics,
outlined in Section 5, was an increase in coordinate
noun phrase f-score from 69.9% to 73.8% on the test
set. This represents a 13% relative reduction in co-
ordinate f-score error over the baseline, and, using
McNemar’s test for significance, is significant at the
0.05 level (p = 0.034). The reranker f-score for
all constituents (not excluding any coordinate NPs)
for section 23 rose slightly from 89.4% to 89.6%, a
small but significant increase in f-score.10
Finally, we report results on an unaltered coor-
dination test set, that is, a test set from which no
</bodyText>
<footnote confidence="0.9970445">
10Significance was calculated using the software available at
www.cis.upenn.edu/ dbikel/software.html.
</footnote>
<page confidence="0.994796">
686
</page>
<bodyText confidence="0.998095515151515">
noisy events were eliminated. The baseline coordi- Sharon Caraballo. 1999. Automatic construction of a
nation dependency f-score for all NP coordination hypernym-labeled noun hierarchy from text. In Proceedings
dependencies (550 dependencies) from section 23 is of the 37th ACL.
69.27%. This rises to 72.74% when all experiments Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
described in Section 7 are applied, which is also a best Parsing and MaxEnt Discriminative Reranking. In Pro-
statistically significant increase (p = 0.042). ceedings of the 43rd ACL.
8 Conclusion and Future Work Michael Collins. 1999. Head-Driven Statistical Models for
This paper outlined a novel method for modelling Natural Language Parsing. Ph.D. thesis, University of
symmetry in conjunct structure, for modelling the Pennsylvania.
dependency between noun phrase conjunct head Markus Dickinson and W. Detmar Meurers. 2005. Prune dis-
words and for incorporating a measure of word sim- eased branches to get healthy trees! How to find erroneous
ilarity in the estimation of a model parameter. We local trees in a treebank and why it matters. In Proceedings
also demonstrated how simple pattern matching can of the Fourth Workshop on Treebanks and Linguistic Theo-
be used to reduce noise in WSJ noun phrase coor- ries (TLT).
dination data. Combined, these techniques resulted Amit Dubey, Patrick Sturt and Frank Keller. 2005. Parallelism
in a statistically significant improvement in noun in Coordination as an Instance of Syntactic Priming: Evi-
phrase coordination accuracy. dence from Corpus-based Modeling. In Proceedings of the
Coordination disambiguation necessitates in- HLT/EMNP-05.
formation from a variety of sources. Another Miriam Goldberg. 1999. An Unsupervised Model for Statis-
information source important to NP coordinate tically Determining Coordinate Phrase Attachment. In Pro-
disambiguation is the dependency between non- ceedings of the 27th ACL.
nominal modifiers and nouns which cross CCs Deirdre Hogan. 2005. k-NN for Local Probability Estimation
in NPBs. For example, modelling this type of in Generative Parsing Models. In Proceedings of the IWPT-
dependency could help the model learn that the 05.
phrase the cats and dogs should be bracketed flat, Sadao Kurohashi and Makoto Nagao. 1994. A Syntactic Anal-
whereas the phrase the U.S. and Washington should ysis Method of Long Japanese Sentences Based on the De-
be given structure. tection of Conjunctive Structures. In Computational Lin-
Acknowledgements We are grateful to the TCD guistics, 20(4).
Broad Curriculum Fellowship scheme and to the Preslav Nakov and Marti Hearst. 2005. Using the Web as an
SFI Basic Research Grant 04/BR/CS370 for fund- Implicit Training Set: Application to Structural Ambiguity
ing this research. Thanks to P´adraig Cunningham, Resolution. In Proceedings of the HLT/EMNLP-05.
Saturnino Luz, Jennifer Foster and Gerard Hogan Adwait Ratnaparkhi, Salim Roukos and R. Todd Ward. 1994. A
for helpful discussions and feedback on this work. Maximum Entropy Model for Parsing. In Proceedings of the
</bodyText>
<sectionHeader confidence="0.866726" genericHeader="references">
References International Conference on Spoken Language Processing.
</sectionHeader>
<reference confidence="0.993333285714286">
Rajeev Agarwal and Lois Boggess. 1992. A Simple but Useful Philip Resnik. 1999. Semantic Similarity in a Taxonomy: An
Approach to Conjunct Identification. In Proceedings of the Information-Based Measure and its Application to Problems
30th ACL. of Ambiguity in Natural Language. In Journal of Artificial
Ann Bies, Mark Ferguson, Karen Katz and Robert MacIntyre. Intelligence Research, 11:95-130, 1999.
1995. Bracketing Guidelines for Treebank II Style Penn Beatrice Santorini. 1991. Part-of-Speech Tagging Guidelines
Treebank Project. Technical Report. University of Penn- for the Penn Treebank Project. Technical Report. University
sylvania. of Pennsylvania.
Dan Bikel. 2004. On The Parameter Space of Generative Lex- Hinrich Sch¨utze. 1998. Automatic Word Sense Discrimination.
icalized Statistical Parsing Models. Ph.D. thesis, University Computational Linguistics, 24(1):97-123.
of Pennsylvania. Dominic Widdows. 2004. Geometry and Meaning. CSLI Pub-
687 lications, Stanford, USA.
Shaojun Zhao and Dekang Lin. 2004. A Nearest-Neighbor
Method for Resolving PP-Attachment Ambiguity. In Pro-
ceedings of the IJCNLP-04.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.770240">
<title confidence="0.853873">Coordinate Noun Phrase Disambiguation in a Generative Parsing Model</title>
<affiliation confidence="0.998341">Computer Science Department Trinity College Dublin</affiliation>
<address confidence="0.998762">Dublin 2, Ireland</address>
<email confidence="0.992538">dhogan@computing.dcu.ie</email>
<abstract confidence="0.990971857142857">In this paper we present methods for improving the disambiguation of noun phrase (NP) coordination within the framework of a lexicalised history-based parsing model. As well as reducing noise in the data, we look at modelling two main sources of information for disambiguation: symmetry in conjunct structure, and the dependency between conjunct lexical heads. Our changes to the baseline model result in an increase in NP coordependency from 69.9% to 73.8%, which represents a relative reduction error of 13%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>