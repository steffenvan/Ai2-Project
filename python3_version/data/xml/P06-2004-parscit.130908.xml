<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.9961855">
The Effect of Corpus Size in Combining Supervised and
Unsupervised Training for Disambiguation
</title>
<author confidence="0.979412">
Michaela Atterer
</author>
<affiliation confidence="0.9906115">
Institute for NLP
University of Stuttgart
</affiliation>
<email confidence="0.995203">
atterer@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.993809" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997429529411765">
We investigate the effect of corpus size
in combining supervised and unsuper-
vised learning for two types of attach-
ment decisions: relative clause attach-
ment and prepositional phrase attach-
ment. The supervised component is
Collins’ parser, trained on the Wall
Street Journal. The unsupervised com-
ponent gathers lexical statistics from
an unannotated corpus of newswire
text. We find that the combined sys-
tem only improves the performance of
the parser for small training sets. Sur-
prisingly, the size of the unannotated
corpus has little effect due to the noisi-
ness of the lexical statistics acquired by
unsupervised learning.
</bodyText>
<sectionHeader confidence="0.998968" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9521452">
The best performing systems for many tasks in
natural language processing are based on su-
pervised training on annotated corpora such
as the Penn Treebank (Marcus et al., 1993)
and the prepositional phrase data set first de-
scribed in (Ratnaparkhi et al., 1994). How-
ever, the production of training sets is ex-
pensive. They are not available for many
domains and languages. This motivates re-
search on combining supervised with unsu-
pervised learning since unannotated text is in
ample supply for most domains in the major
languages of the world. The question arises
how much annotated and unannotated data
is necessary in combination learning strate-
gies. We investigate this question for two at-
tachment ambiguity problems: relative clause
(RC) attachment and prepositional phrase
(PP) attachment. The supervised component
is Collins’ parser (Collins, 1997), trained on
</bodyText>
<author confidence="0.783411">
Hinrich Schiitze
</author>
<affiliation confidence="0.9681815">
Institute for NLP
University of Stuttgart
</affiliation>
<email confidence="0.980754">
hinrich@hotmail.com
</email>
<bodyText confidence="0.988928965517241">
the Wall Street Journal. The unsupervised
component gathers lexical statistics from an
unannotated corpus of newswire text.
The sizes of both types of corpora, anno-
tated and unannotated, are of interest. We
would expect that large annotated corpora
(training sets) tend to make the additional in-
formation from unannotated corpora redun-
dant. This expectation is confirmed in our
experiments. For example, when using the
maximum training set available for PP attach-
ment, performance decreases when “unanno-
tated&amp;quot; lexical statistics are added.
For unannotated corpora, we would expect
the opposite effect. The larger the unanno-
tated corpus, the better the combined system
should perform. While there is a general ten-
dency to this effect, the improvements in our
experiments reach a plateau quickly as the un-
labeled corpus grows, especially for PP attach-
ment. We attribute this result to the noisiness
of the statistics collected from unlabeled cor-
pora.
The paper is organized as follows. Sections
2, 3 and 4 describe data sets, methods and
experiments. Section 5 evaluates and discusses
experimental results. Section 6 compares our
approach to prior work. Section 7 states our
conclusions.
</bodyText>
<sectionHeader confidence="0.988641" genericHeader="method">
2 Data Sets
</sectionHeader>
<bodyText confidence="0.9075262">
The unlabeled corpus is the Reuters RCV1
corpus, about 80,000,000 words of newswire
text (Lewis et al., 2004). Three different sub-
sets, corresponding to roughly 10%, 50% and
100% of the corpus, were created for experi-
ments related to the size of the unannotated
corpus. (Two weeks after Aug 5, 1997, were
set apart for future experiments.)
The labeled corpus is the Penn Wall Street
Journal treebank (Marcus et al., 1993). We
</bodyText>
<page confidence="0.974418">
25
</page>
<note confidence="0.7514675">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 25–32,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.880452666666667">
created the 5 subsets shown in Table 1 for ex-
periments related to the size of the annotated
corpus.
</bodyText>
<note confidence="0.621928">
or the transaction is performed by written con-
sent.
</note>
<table confidence="0.9961719">
unlabeled R
100% 20/08/1996–05/08/1997 (351 days)
50% 20/08/1996–17/02/1997 (182 days)
10% 20/08/1996–24/09/1996 (36 days)
labeled WSJ
50% sections 00–12 (23412 sentences)
25% lines 1 – 292960 (11637 sentences)
5% lines 1 – 58284 (2304 sentences)
1% lines 1 – 11720 (500 sentences)
0.05% lines 1 – 611 (23 sentences)
</table>
<tableCaption confidence="0.9501495">
Table 1: Corpora used for the experiments:
unlabeled Reuters (R) corpus for attachment
statistics, labeled Penn treebank (WSJ) for
training the Collins parser.
</tableCaption>
<bodyText confidence="0.79178575">
The test set, sections 13-24, is larger than in
most studies because a single section does not
contain a sufficient number of RC attachment
ambiguities for a meaningful evaluation.
</bodyText>
<table confidence="0.9561035">
which-clauses subset highA lowA total
develop set (sec 00-12) 71 211 282
test set (sec 13-24) 71 193 264
PP subset verbA nounA total
develop set (sec 00-12) 5927 6560 12487
test set (sec 13-24) 5930 6273 12203
</table>
<tableCaption confidence="0.947149">
Table 2: RC and PP attachment ambigui-
</tableCaption>
<bodyText confidence="0.990341642857143">
ties in the Penn Treebank. Number of in-
stances with high attachment (highA), low at-
tachment (lowA), verb attachment (verbA),
and noun attachment (nounA) according to
the gold standard.
All instances of RC and PP attachments
were extracted from development and test
sets, yielding about 250 RC ambiguities and
12,000 PP ambiguities per set (Table 2). An
RC attachment ambiguity was defined as a
sentence containing the pattern NP1 Prep NP2
which. For example, the relative clause in Ex-
ample 1 can either attach to mechanism or to
System.
</bodyText>
<sectionHeader confidence="0.385388" genericHeader="method">
(1) ... the exchange-rate mechanism of the
</sectionHeader>
<subsectionHeader confidence="0.820067">
European Monetary System, which
</subsectionHeader>
<bodyText confidence="0.938725076923077">
links the major EC currencies.
A PP attachment ambiguity was defined as
a subtree matching either [VP [NP PP]] or [VP
NP PP]. An example of a PP attachment am-
biguity is Example 2 where either the approval
(2) ... a majority ... have approved the
transaction by written consent ...
Both data sets are available for download
(Web Appendix, 2006). We did not use the
PP data set described by (Ratnaparkhi et al.,
1994) because we are using more context than
the limited context available in that set (see
below).
</bodyText>
<sectionHeader confidence="0.996669" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.997774444444444">
Collins parser. Our baseline method for
ambiguity resolution is the Collins parser as
implemented by Bikel (Collins, 1997; Bikel,
2004). For each ambiguity, we check whether
the attachment ambiguity is resolved correctly
by the 5 parsers corresponding to the different
training sets. If the attachment ambiguity is
not recognized (e.g., because parsing failed),
then the corresponding ambiguity is excluded
for that instance of the parser. As a result, the
size of the effective test set varies from parser
to parser (see Table 4).
Minipar. The unannotated corpus is ana-
lyzed using minipar (Lin, 1998), a partial de-
pendency parser. The corpus is parsed and all
extracted dependencies are stored for later use.
Dependencies in ambiguous PP attachments
(those corresponding to [VP NP PP] and [VP
[NP PP]] subtrees) are not indexed. An ex-
periment with indexing both alternatives for
ambiguous structures yielded poor results. For
example, indexing both alternatives will create
a large number of spurious verb attachments
of of, which in turn will result in incorrect high
attachments by our disambiguation algorithm.
For relative clauses, no such filtering is nec-
essary. For example, spurious subject-verb
dependencies due to RC ambiguities are rare
compared to a large number of subject-verb
dependencies that can be extracted reliably.
Inverted index. Dependencies extracted
by minipar are stored in an inverted index
(Witten et al., 1999), implemented in Lucene
(Lucene, 2006). For example, “john subj
buy&amp;quot;, the analysis returned by minipar for
John buys, is stored as “john buy john&lt;subj
</bodyText>
<page confidence="0.99229">
26
</page>
<bodyText confidence="0.995416711111111">
subj&lt;buy john&lt;subj&lt;buy&amp;quot;. All words, de-
pendencies and partial dependencies of a sen-
tence are stored together as one document.
This storage mechanism enables fast on-line
queries for lexical and dependency statistics,
e.g., how many sentences contain the depen-
dency “john subj buy&amp;quot;, how often does john
occur as a subject, how often does buy have
john as a subject and car as an object etc.
Query results are approximate because double
occurrences are only counted once and struc-
tures giving rise to the same set of dependen-
cies (a piece of a tile of a roof of a house vs.
a piece of a roof of a tile of a house) cannot
be distinguished. We believe that an inverted
index is the most efficient data structure for
our purposes. For example, we need not com-
pute expensive joins as would be required in a
database implementation. Our long-term goal
is to use this inverted index of dependencies
as a versatile component of NLP systems in
analogy to the increasingly important role of
search engines for association and word count
statistics in NLP.
A total of three inverted indexes were cre-
ated, one each for the 10%, 50% and 100%
Reuters subset.
Lattice-Based Disambiguation. Our
disambiguation method is Lattice-Based
Disambiguation (LBD, (Atterer and Schiitze,
2006)). We formalize a possible attachment
as a triple &lt; R, i, X &gt; where X is (the
parse of) a phrase with two or more possible
attachment nodes in a sentence S, i is one of
these attachment nodes and R is (the relevant
part of a parse of) S with X removed. For
example, the two attachments in Example 2
are represented as the triples:
&lt; approvedi1 the transactioni2, i1, by consent &gt;,
&lt; approvedi1 the transactioni2, i2, by consent &gt;.
We decide between attachment possibilities
based on pointwise mutual information, the
well-known measure of how surprising it is to
see R and X together given their individual
frequencies:
</bodyText>
<equation confidence="0.8844835">
P(&lt;R,i,X&gt;)
MI(&lt; R, i, X &gt;) = log2 P (R)P (X)
</equation>
<bodyText confidence="0.9366178">
for P(&lt; R, i, X &gt;), P(R), P(X) =� 0
MI(&lt; R, i, X &gt;) = 0 otherwise
where the probabilities of the dependency
structures &lt; R, i, X &gt;, R and X are estimated
on the unlabeled corpus by querying the in-
</bodyText>
<equation confidence="0.5503128">
MN:pMN
MN:pN
N:pN N:pM MN:p
N:p
0:p
</equation>
<figureCaption confidence="0.8092888">
Figure 1: Lattice of pairs of potential attach-
ment site (NP) and attachment phrase (PP).
M: premodifying adjective or noun (upper or
lower NP), N: head noun (upper or lower NP),
p: Preposition.
</figureCaption>
<bodyText confidence="0.986643885714286">
verted index. Unfortunately, these structures
will often not occur in the corpus. If this is
the case we back off to generalizations of R
and X. The generalizations form a lattice as
shown in Figure 1 for PP attachment. For ex-
ample, MN:pMN corresponds to commercial
transaction by unanimous consent, N:pM to
transaction by unanimous etc. For 0:p we com-
pute MI of the two events “noun attachment”
and “occurrence of p&amp;quot;. Points in the lattice in
Figure 1 are created by successive elimination
of material from the complete context R:X.
A child c directly dominated by a parent p
is created by removing exactly one contextual
element from p, either on the right side (the
attachment phrase) or on the left side (the at-
tachment node). For RC attachment, general-
izations other than elimination are introduced
such as the replacement of a proper noun (e.g.,
Canada) by its category (country) (see below).
The MI of each point in the lattice is com-
puted. We then take the maximum over all
MI values of the lattice as a measure of the
affinity of attachment phrase and attachment
node. The intuition is that we are looking for
the strongest evidence available for the attach-
ment. The strongest evidence is often not pro-
vided by the most specific context (MN:pMN
in the example) since contextual elements like
modifiers will only add noise to the attachment
decision in some cases. The actual syntactic
disambiguation is performed by computing the
affinity (maximum over MI values in the lat-
tice) for each possible attachment and select-
ing the attachment with highest affinity. (The
</bodyText>
<equation confidence="0.8130045">
N:pMN
MN:pM
</equation>
<page confidence="0.971688">
27
</page>
<bodyText confidence="0.99375780952381">
default attachment is selected if the two values
are equal.) The second lattice for PP attach-
ment, the lattice for attachment to the verb,
has a structure identical to Figure 1, but the
attachment node is SV instead of MN, where
S denotes the subject and V the verb. So the
supremum of that lattice is SV:pMN and the
infimum is 0:p (which in this case corresponds
to the MI of verb attachment and occurrence
of the preposition).
LBD is motivated by the desire to use as
much context as possible for disambiguation.
Previous work on attachment disambiguation
has generally used less context than in this
paper (e.g., modifiers have not been used for
PP attachment). No change to LBD is neces-
sary if the lattice of contexts is extended by
adding additional contextual elements (e.g.,
the preposition between the two attachment
nodes in RC, which we do not consider in this
paper).
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.998633941176471">
The Reuters corpus was parsed with minipar
and all dependencies were extracted. Three
inverted indexes were created, corresponding
to 10%, 50% and 100% of the corpus.&apos; Five
parameter sets for the Collins parser were cre-
ated by training it on the WSJ training sets
in Table 1. Sentences with attachment am-
biguities in the WSJ corpus were parsed with
minipar to generate Lucene queries. (We chose
this procedure to ensure compatibility of query
and index formats.) The Lucene queries were
run on the three indexes. LBD disambigua-
tion was then applied based on the statistics
returned by the queries. LBD results are ap-
plied to the output of the Collins parser by
simply replacing all attachment decisions with
LBD decisions.
</bodyText>
<subsectionHeader confidence="0.988952">
4.1 RC attachment
</subsectionHeader>
<bodyText confidence="0.991494894736842">
The lattice for LBD in RC attachment is
shown in Figure 2. When disambiguating
an RC attachment, two instances of the
lattice are formed, one for NP1 and one
&apos;In fact, two different sets of inverted indexes were
created, one each for PP and RC disambiguation. The
RC index indexes all dependencies, including ambigu-
ous PP dependencies. Computing the RC statistics
on the PP index should not affect the RC results pre-
sented here, but we didn’t have time to confirm this
experimentally for this paper.
for NP2 in NP1 Prep NP2 RC. Figure 2
shows the maximum possible lattice. If
contextual elements are not present in a
context (e.g., a modifier), then the lattice
will be smaller. The supremum of the lat-
tice corresponds to a query that includes
the entire NP (including modifying adjec-
tives and nouns)2, the verb and its object.
</bodyText>
<figure confidence="0.9940786">
Example: exchange rate&lt;nn&lt;mechanim
&amp;&amp; mechanism&lt;subj&lt;link &amp;&amp;
currency&lt;obj&lt;link.
MNf:VO
MN:VO Nf:VO
Mn:VO N:VO MN:V Nf:V
MC:VO
C:VO
C:V
[empty]
</figure>
<figureCaption confidence="0.9996">
Figure 2: Lattice of pairs of potential attach-
</figureCaption>
<bodyText confidence="0.816799875">
ment site NP and relative clause X. M: pre-
modifying adjective or noun, Nf: head noun
with lexical modifiers, N: head noun only, n:
head noun in lower case, C: class of NP, V:
verb in relative clause, O: object of verb in
the relative clause.
To generalize contexts in the lattice, the fol-
lowing generalization operations are employed:
</bodyText>
<listItem confidence="0.995784">
• strip the NP of the modifying adjec-
tive/noun (weekly report → report)
• use only the head noun of the NP (Catas-
trophic Care Act → Act)
• use the head noun in lower case (Act → act)
• for named entities use a hypernym of the NP
(American Bell Telephone Co. → company)
• strip the object from X (company have sub-
sidiary → company have)
</listItem>
<bodyText confidence="0.834058">
The most important dependency for disam-
</bodyText>
<footnote confidence="0.718259666666667">
2I&apos;1&apos;om the minipar output, we use all adjectives that
modify the NP via the relation mod, and all nouns that
modify the NP via the relation nn.
</footnote>
<figure confidence="0.880416833333333">
MNf:V
n:VO
MC:V
Mn:V
n:V
N:V
</figure>
<page confidence="0.994963">
28
</page>
<bodyText confidence="0.999135470588235">
biguation is the noun-verb link, but the other
dependencies also improve the accuracy of
disambiguation (Atterer and Schiitze, 2006).
For example, light verbs like make and have
only provide disambiguation information when
their objects are also considered.
Downcasing and hypernym generalizations
were used because proper nouns often cause
sparse data problems. Named entity classes
were identified with LingPipe (LingPipe,
2006). Named entities identified as companies
or organizations are replaced with company in
the query. Locations are replaced with coun-
try. Persons block RC attachment because
which-clauses do not attach to person names,
resulting in an attachment of the RC to the
other NP.
</bodyText>
<table confidence="0.998596470588235">
query MI
+exchange rate(nn(mechanism 12.2
+mechanism(subj(link +currency(obj(link
+exchange rate(nn(mechanism 4.8
+mechanism(subj(link
+mechanism(subj(link +currency(obj(link 10.2
mechanism(subj(link 3.4
+European Monetary System(subj(link 0
+currency(obj(link
+System(subj(link +currency(obj(link 0
European Monetary System(subj(link 0
System(subj(link 0
+system(subj(link +currency(obj(link 0
system(subj(link 1.2
+company(subj(link +currency(obj(link 0
company(subj(link -1.1
empty 3
</table>
<tableCaption confidence="0.9914985">
Table 3: Queries for computing high attach-
ment (above) and low attachment (below) for
Example 1.
Table 3 shows queries and mutual informa-
tion values for Example 1. The highest values
are 12.2 for high attachment (mechanism) and
3 for low attachment (System). The algorithm
therefore selects high attachment.
</tableCaption>
<bodyText confidence="0.994104714285714">
The value 3 for low attachment is the de-
fault value for the empty context. This value
reflects the bias for low attachment: the ma-
jority of relative clauses are attached low. If
all MI-values are zero or otherwise low, this
procedure will automatically result in low at-
tachment.3
3 W experimented with a number of values (2, 3,
and 4) on the development set. Accuracy of the algo-
rithm was best for a value of 3. The results presented
here differ slightly from those in (Atterer and Sch�utze,
2006) due to a coding error.
Decision list. For increased accuracy, LBD
is embedded in the following decision list.
</bodyText>
<listItem confidence="0.928768636363636">
1. If minipar has already chosen high attach-
ment, choose high attachment (this only oc-
curs if NP1 Prep NP2 is a named entity).
2. If there is agreement between the verb and
only one of the NPs, attach to this NP.
3. If one of the NPs is in a list of person entities,
attach to the other NP.4
4. If possible, use LBD.
5. If none of the above strategies was successful
(e.g. in the case of parsing errors), attach
low.
</listItem>
<subsectionHeader confidence="0.977423">
4.2 PP attachment
</subsectionHeader>
<bodyText confidence="0.999965428571429">
The two lattices for LBD applied to PP at-
tachment were described in Section 3 and Fig-
ure 1. The only generalization operation used
in these two lattices is elimination of contex-
tual elements (in particular, there is no down-
casing and named entity recognition). Note
that in RC attachment, we compare affinities
of two instances of the same lattice (the one
shown in Figure 2). In PP attachment, we
compare affinities of two different lattices since
the two attachment points (verb vs. noun) are
different. The basic version of LBD (with the
untuned default value 0 and without decision
lists) was used for PP attachment.
</bodyText>
<sectionHeader confidence="0.9779" genericHeader="evaluation">
5 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.99980895">
Evaluation results are shown in Table 4. The
lines marked LBD evaluate the performance
of LBD separately (without Collins’ parser).
LBD is significantly better than the baseline
for PP attachment (p &lt; 0.001, all tests are
x2 tests). LBD is also better than baseline
for RC attachment, but this result is not sig-
nificant due to the small size of the data set
(264). Note that the baseline for PP attach-
ment is 51.4% as indicated in the table (upper
right corner of PP table), but that the base-
line for RC attachment is 73.1%. The differ-
ence between 73.1% and 76.1% (upper right
corner of RC table) is due to the fact that for
RC attachment LBD proper is embedded in a
decision list. The decision list alone, with an
¢This list contains 136 entries and was semiauto-
matically computed from the Reuters corpus: An-
tecedents of who relative clauses were extracted, and
the top 200 were filtered manually.
</bodyText>
<page confidence="0.995849">
29
</page>
<table confidence="0.9998319375">
RC attachment
Train data # Coll. only 100% R 50% R 10% R 0% R
LBD 264 78.4% 78.0% 76.9% 76.1%
50% 251 71.7% 78.5% 78.1% 76.9% 76.1%
25% 250 70.0% 78.0% 77.6% 76.4% 76.4%
5% 238 68.9% 78.2% 77.7% 76.9% 76.1%
1% 245 67.8% 78.8% 78.4% 77.1% 76.7%
0.05% 194 60.8% 76.8% 76.3% 75.8% 73.7%
PP attachment
Train data # Coll. only 100% R 50% R 10% R 0% R
LBD 12203 73.4% 73.4% 73.0% 51.4%
50% 11953 82.8% 73.6% 73.6% 73.2% 51.7%
25% 11950 81.5% 73.6% 73.7% 73.3% 51.7%
5% 11737 77.4% 74.1% 74.2% 73.7% 52.3%
1% 11803 72.9% 73.6% 73.6% 73.2% 51.6%
0.05% 8486 58.0% 73.9% 73.8% 74.0% 52.8%
</table>
<tableCaption confidence="0.9228105">
Table 4: Experimental results. Results for LBD (without Collins) are given in the first lines. #
is the size of the test set. The baselines are 73.1% (RC) and 51.4% (PP). The combined method
performs better for small training sets. There is no significant difference between 10%, 50% and
100% for the combination method (p &lt; 0.05).
</tableCaption>
<bodyText confidence="0.986103890625">
unlabeled corpus of size 0, achieves a perfor-
mance of 76.1%.
The bottom five lines of each table evalu-
ate combinations of a parameter set trained
on a subset of WSJ (0.05% – 50%) and a par-
ticular size of the unlabeled corpus (100% –
0%). In addition, the third column gives the
performance of Collins’ parser without LBD.
Recall that test set size (second column) varies
because we discard a test instance if Collins’
parser does not recognize that there is an am-
biguity (e.g., because of a parse failure). As
expected, performance increases as the size of
the training set grows, e.g., from 58.0% to
82.8% for PP attachment.
The combination of Collins and LBD is con-
sistently better than Collins for RC attach-
ment (not statistically significant due to the
size of the data set). However, this is not
the case for PP attachment. Due to the good
performance of Collins’ parser for even small
training sets, the combination is only superior
for the two smallest training sets (significant
for the smallest set, p &lt; 0.001).
The most surprising result of the experi-
ments is the small difference between the three
unlabeled corpora. There is no clear pattern in
the data for PP attachment and only a small
effect for RC attachment: an increase between
1% and 2% when corpus size is increased from
10% to 100%.
We performed an analysis of a sample of in-
correctly attached PPs to investigate why un-
labeled corpus size has such a small effect. We
found that the noisiness of the statistics ex-
tracted from Reuters were often responsible
for attachment errors. The noisiness is caused
by our filtering strategy (ambiguous PPs are
not used, resulting in undercounting), by the
approximation of counts by Lucene (Lucene
overcounts and undercounts as discussed in
Section 3) and by minipar parse errors. Parse
errors are particularly harmful in cases like
the impact it would have on prospects, where,
due to the extraction of the NP impact, mini-
par attaches the PP to the verb. We did
not filter out these more complex ambiguous
cases. Finally, the two corpora are from dis-
tinct sources and from distinct time periods
(early nineties vs. mid-nineties). Many topic-
and time-specific dependencies can only be
mined from more similar corpora.
The experiments reveal interesting dif-
ferences between PP and RC attachment.
The dependencies used in RC disambiguation
rarely occur in an ambiguous context (e.g.,
most subject-verb dependencies can be reli-
ably extracted). In contrast, a large propor-
tion of the dependencies needed in PP dis-
ambiguation (verb-prep and noun-prep depen-
dencies) do occur in ambiguous contexts. An-
other difference is that RC attachment is syn-
tactically more complex. It interacts with
agreement, passive and long-distance depen-
</bodyText>
<page confidence="0.995828">
30
</page>
<bodyText confidence="0.99963264">
dencies. The algorithm proposed for RC ap-
plies grammatical constraints successfully. A
final difference is that the baseline for RC is
much higher than for PP and therefore harder
to beat.5
An innovation of our disambiguation system
is the use of a search engine, lucene, for serv-
ing up dependency statistics. The advantage
is that counts can be computed quickly and
dynamically. New text can be added on an
ongoing basis to the index. The updated de-
pendency statistics are immediately available
and can benefit disambiguation performance.
Such a system can adapt easily to new topics
and changes over time. However, this archi-
tecture negatively affects accuracy. The un-
supervised approach of (Hindle and Rooth,
1993) achieves almost 80% accuracy by using
partial dependency statistics to disambiguate
ambiguous sentences in the unlabeled corpus.
Ambiguous sentences were excluded from our
index to make index construction simple and
efficient. Our larger corpus (about 6 times as
large as Hindle et al.&apos;s) did not compensate for
our lower-quality statistics.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999634440677966">
Other work combining supervised and unsu-
pervised learning for parsing includes (Char-
niak, 1997), (Johnson and Riezler, 2000), and
(Schmid, 2002). These papers present inte-
grated formal frameworks for incorporating in-
formation learned from unlabeled corpora, but
they do not explicitly address PP and RC at-
tachment. The same is true for uncorrected
colearning in (Hwa et al., 2003).
Conversely, no previous work on PP and RC
attachment has integrated specialized ambi-
guity resolution into parsing. For example,
(Toutanova et al., 2004) present one of the
best results achieved so far on the WSJ PP
set: 87.5%. They also integrate supervised
and unsupervised learning. But to our knowl-
edge, the relationship to parsing has not been
explored before – even though application to
parsing is the stated objective of most work on
PP attachment.
5However, the baseline is similarly high for the PP
problem if the most likely attachment is chosen per
preposition: 72.2% according to (Collins and Brooks,
1995).
With the exception of (Hindle and Rooth,
1993), most unsupervised work on PP attach-
ment is based on superficial analysis of the
unlabeled corpus without the use of partial
parsing (Volk, 2001; Calvo et al., 2005). We
believe that dependencies offer a better basis
for reliable disambiguation than cooccurrence
and fixed-phrase statistics. The difference to
(Hindle and Rooth, 1993) was discussed above
with respect to analysing the unlabeled cor-
pus. In addition, the decision procedure pre-
sented here is different from Hindle et al.&apos;s.
LBD uses more context and can, in princi-
ple, accommodate arbitrarily large contexts.
However, an evaluation comparing the perfor-
mance of the two methods is necessary.
The LBD model can be viewed as a back-
off model that combines estimates from sev-
eral “backoffs&amp;quot;. In a typical backoff model,
there is a single more general model to back
off to. (Collins and Brooks, 1995) also present
a model with multiple backoffs. One of its vari-
ants computes the estimate in question as the
average of three backoffs. In addition to the
maximum used here, testing other combina-
tion strategies for the MI values in the lattice
(e.g., average, sum, frequency-weighted sum)
would be desirable. In general, MI has not
been used in a backoff model before as far as
we know.
Previous work on relative clause attachment
has been supervised (Siddharthan, 2002a; Sid-
dharthan, 2002b; Yeh and Vilain, 1998).6
(Siddharthan, 2002b)&apos;s accuracy for RC at-
tachment is 76.5%.7
</bodyText>
<sectionHeader confidence="0.99849" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9937255">
Previous work on specific types of ambiguities
(like RC and PP) has not addressed the in-
tegration of specific resolution algorithms into
a generic statistical parser. In this paper, we
have shown for two types of ambiguities, rel-
ative clause and prepositional phrase attach-
ment ambiguity, that integration into a sta-
tistical parser is possible and that the com-
6Strictly speaking, our experiments were not com-
pletely unsupervised since the default value and the
most frequent attachment were determined based on
the development set.
7 W attempted to recreate Siddharthan&apos;s training
and test sets, but were not able to based on the de-
scription in the paper and email communication with
the author.
</bodyText>
<page confidence="0.999583">
31
</page>
<bodyText confidence="0.999974038461538">
bined system performs better than either com-
ponent by itself. However, for PP attachment
this only holds for small training set sizes. For
large training sets, we could only show an im-
provement for RC attachment.
Surprisingly, we only found a small effect
of the size of the unlabeled corpus on disam-
biguation performance due to the noisiness of
statistics extracted from raw text. Once the
unlabeled corpus has reached a certain size (5-
10 million words in our experiments) combined
performance does not increase further.
The results in this paper demonstrate that
the baseline of a state-of-the-art lexicalized
parser for specific disambiguation problems
like RC and PP is quite high compared to
recent results for stand-alone PP disambigua-
tion. For example, (Toutanova et al., 2004)
achieve a performance of 87.6% for a train-
ing set of about 85% of WSJ. That num-
ber is not that far from the 82.8% achieved
by Collins’ parser in our experiments when
trained on 50% of WSJ. Some of the super-
vised approaches to PP attachment may have
to be reevaluated in light of this good perfor-
mance of generic parsers.
</bodyText>
<sectionHeader confidence="0.999081" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999773328947369">
Michaela Atterer and Hinrich Schiitze. 2006. A
lattice-based framework for enhancing statisti-
cal parsers with information from unlabeled cor-
pora. In CoNLL.
Daniel M. Bikel. 2004. Intricacies of Collins’
parsing model. Computational Linguistics,
30(4):479–511.
Hiram Calvo, Alexander Gelbukh, and Adam Kil-
garriff. 2005. Distributional thesaurus vs.
WordNet: A comparison of backoff techniques
for unsupervised PP attachment. In CICLing.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
AAAI/IAAI, pages 598–603.
Michael Collins and James Brooks. 1995. Prepo-
sitional attachment through a backed-off model.
In Third Workshop on Very Large Corpora. As-
sociation for Computational Linguistics.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In ACL.
Donald Hindle and Mats Rooth. 1993. Structural
ambiguity and lexical relations. Computational
Linguistics, 19(1):103–120.
Rebecca Hwa, Miles Osborne, Anoop Sarkar, and
Mark Steedman. 2003. Corrected co-training
for statistical parsers. In Workshop on the Con-
tinuum from Labeled to Unlabeled Data in Ma-
chine Learning and Data Mining, ICML.
Mark Johnson and Stefan Riezler. 2000. Ex-
ploiting auxiliary distributions in stochastic
unification-based grammars. In NAACL.
David D. Lewis, Yiming Yang, Tony G. Rose, and
Fan Li. 2004. RCV1: A new benchmark collec-
tion for text categorization research. J. Mach.
Learn. Res., 5.
Dekang Lin. 1998. Dependency-based evaluation
of MINIPAR. In Workshop on the Evaluation of
Parsing Systems, Granada, Spain.
LingPipe. 2006. http://www.alias-
i.com/lingpipe/.
Lucene. 2006. http://lucene.apache.org.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large natural language corpus of English:
the Penn treebank. Computational Linguistics,
19:313–330.
Adwait Ratnaparkhi, Jeff Reynar, and Salim
Roukos. 1994. A maximum entropy model for
prepositional phrase attachment. In HLT.
Helmut Schmid. 2002. Lexicalization of proba-
bilistic grammars. In Coling.
Advaith Siddharthan. 2002a. Resolving attach-
ment and clause boundary ambiguities for sim-
plifying relative clause constructs. In Student
Research Workshop, ACL.
Advaith Siddharthan. 2002b. Resolving relative
clause attachment ambiguities using machine
learning techniques and wordnet hierarchies. In
4th Discourse Anaphora and Anaphora Resolu-
tion Colloquium.
Kristina Toutanova, Christopher D. Manning, and
Andrew Y. Ng. 2004. Learning random walk
models for inducing word dependency distribu-
tions. In ICML.
Martin Volk. 2001. Exploiting the WWW as a
corpus to resolve pp attachment ambiguities. In
Corpus Linguistics 2001.
Web Appendix. 2006. http://www.ims.uni-
stuttgart.de/∼schuetze/colingacl06/apdx.html.
Ian H. Witten, Alistair Moffat, and Timothy C.
Bell. 1999. Managing Gigabytes: Compressing
and Indexing Documents and Images. Morgan
Kaufman.
Alexander S. Yeh and Marc B. Vilain. 1998. Some
properties of preposition and subordinate con-
junction attachments. In Coling.
</reference>
<page confidence="0.999301">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872440">
<title confidence="0.998453">The Effect of Corpus Size in Combining Supervised and Unsupervised Training for Disambiguation</title>
<author confidence="0.999195">Michaela Atterer</author>
<affiliation confidence="0.9991135">Institute for NLP University of Stuttgart</affiliation>
<email confidence="0.989106">atterer@ims.uni-stuttgart.de</email>
<abstract confidence="0.990657333333334">We investigate the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment. The supervised component is Collins’ parser, trained on the Wall Street Journal. The unsupervised component gathers lexical statistics from an unannotated corpus of newswire text. We find that the combined system only improves the performance of the parser for small training sets. Surprisingly, the size of the unannotated corpus has little effect due to the noisiness of the lexical statistics acquired by unsupervised learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michaela Atterer</author>
<author>Hinrich Schiitze</author>
</authors>
<title>A lattice-based framework for enhancing statistical parsers with information from unlabeled corpora.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="8599" citStr="Atterer and Schiitze, 2006" startWordPosition="1371" endWordPosition="1374"> believe that an inverted index is the most efficient data structure for our purposes. For example, we need not compute expensive joins as would be required in a database implementation. Our long-term goal is to use this inverted index of dependencies as a versatile component of NLP systems in analogy to the increasingly important role of search engines for association and word count statistics in NLP. A total of three inverted indexes were created, one each for the 10%, 50% and 100% Reuters subset. Lattice-Based Disambiguation. Our disambiguation method is Lattice-Based Disambiguation (LBD, (Atterer and Schiitze, 2006)). We formalize a possible attachment as a triple &lt; R, i, X &gt; where X is (the parse of) a phrase with two or more possible attachment nodes in a sentence S, i is one of these attachment nodes and R is (the relevant part of a parse of) S with X removed. For example, the two attachments in Example 2 are represented as the triples: &lt; approvedi1 the transactioni2, i1, by consent &gt;, &lt; approvedi1 the transactioni2, i2, by consent &gt;. We decide between attachment possibilities based on pointwise mutual information, the well-known measure of how surprising it is to see R and X together given their indi</context>
<context position="14949" citStr="Atterer and Schiitze, 2006" startWordPosition="2480" endWordPosition="2483">port → report) • use only the head noun of the NP (Catastrophic Care Act → Act) • use the head noun in lower case (Act → act) • for named entities use a hypernym of the NP (American Bell Telephone Co. → company) • strip the object from X (company have subsidiary → company have) The most important dependency for disam2I&apos;1&apos;om the minipar output, we use all adjectives that modify the NP via the relation mod, and all nouns that modify the NP via the relation nn. MNf:V n:VO MC:V Mn:V n:V N:V 28 biguation is the noun-verb link, but the other dependencies also improve the accuracy of disambiguation (Atterer and Schiitze, 2006). For example, light verbs like make and have only provide disambiguation information when their objects are also considered. Downcasing and hypernym generalizations were used because proper nouns often cause sparse data problems. Named entity classes were identified with LingPipe (LingPipe, 2006). Named entities identified as companies or organizations are replaced with company in the query. Locations are replaced with country. Persons block RC attachment because which-clauses do not attach to person names, resulting in an attachment of the RC to the other NP. query MI +exchange rate(nn(mecha</context>
</contexts>
<marker>Atterer, Schiitze, 2006</marker>
<rawString>Michaela Atterer and Hinrich Schiitze. 2006. A lattice-based framework for enhancing statistical parsers with information from unlabeled corpora. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="5876" citStr="Bikel, 2004" startWordPosition="932" endWordPosition="933">ent ambiguity was defined as a subtree matching either [VP [NP PP]] or [VP NP PP]. An example of a PP attachment ambiguity is Example 2 where either the approval (2) ... a majority ... have approved the transaction by written consent ... Both data sets are available for download (Web Appendix, 2006). We did not use the PP data set described by (Ratnaparkhi et al., 1994) because we are using more context than the limited context available in that set (see below). 3 Methods Collins parser. Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel (Collins, 1997; Bikel, 2004). For each ambiguity, we check whether the attachment ambiguity is resolved correctly by the 5 parsers corresponding to the different training sets. If the attachment ambiguity is not recognized (e.g., because parsing failed), then the corresponding ambiguity is excluded for that instance of the parser. As a result, the size of the effective test set varies from parser to parser (see Table 4). Minipar. The unannotated corpus is analyzed using minipar (Lin, 1998), a partial dependency parser. The corpus is parsed and all extracted dependencies are stored for later use. Dependencies in ambiguous</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiram Calvo</author>
<author>Alexander Gelbukh</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Distributional thesaurus vs. WordNet: A comparison of backoff techniques for unsupervised PP attachment.</title>
<date>2005</date>
<booktitle>In CICLing.</booktitle>
<contexts>
<context position="24842" citStr="Calvo et al., 2005" startWordPosition="4113" endWordPosition="4116">P set: 87.5%. They also integrate supervised and unsupervised learning. But to our knowledge, the relationship to parsing has not been explored before – even though application to parsing is the stated objective of most work on PP attachment. 5However, the baseline is similarly high for the PP problem if the most likely attachment is chosen per preposition: 72.2% according to (Collins and Brooks, 1995). With the exception of (Hindle and Rooth, 1993), most unsupervised work on PP attachment is based on superficial analysis of the unlabeled corpus without the use of partial parsing (Volk, 2001; Calvo et al., 2005). We believe that dependencies offer a better basis for reliable disambiguation than cooccurrence and fixed-phrase statistics. The difference to (Hindle and Rooth, 1993) was discussed above with respect to analysing the unlabeled corpus. In addition, the decision procedure presented here is different from Hindle et al.&apos;s. LBD uses more context and can, in principle, accommodate arbitrarily large contexts. However, an evaluation comparing the performance of the two methods is necessary. The LBD model can be viewed as a backoff model that combines estimates from several “backoffs&amp;quot;. In a typical </context>
</contexts>
<marker>Calvo, Gelbukh, Kilgarriff, 2005</marker>
<rawString>Hiram Calvo, Alexander Gelbukh, and Adam Kilgarriff. 2005. Distributional thesaurus vs. WordNet: A comparison of backoff techniques for unsupervised PP attachment. In CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In AAAI/IAAI,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="23723" citStr="Charniak, 1997" startWordPosition="3932" endWordPosition="3934">an adapt easily to new topics and changes over time. However, this architecture negatively affects accuracy. The unsupervised approach of (Hindle and Rooth, 1993) achieves almost 80% accuracy by using partial dependency statistics to disambiguate ambiguous sentences in the unlabeled corpus. Ambiguous sentences were excluded from our index to make index construction simple and efficient. Our larger corpus (about 6 times as large as Hindle et al.&apos;s) did not compensate for our lower-quality statistics. 6 Related Work Other work combining supervised and unsupervised learning for parsing includes (Charniak, 1997), (Johnson and Riezler, 2000), and (Schmid, 2002). These papers present integrated formal frameworks for incorporating information learned from unlabeled corpora, but they do not explicitly address PP and RC attachment. The same is true for uncorrected colearning in (Hwa et al., 2003). Conversely, no previous work on PP and RC attachment has integrated specialized ambiguity resolution into parsing. For example, (Toutanova et al., 2004) present one of the best results achieved so far on the WSJ PP set: 87.5%. They also integrate supervised and unsupervised learning. But to our knowledge, the re</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In AAAI/IAAI, pages 598–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>James Brooks</author>
</authors>
<title>Prepositional attachment through a backed-off model.</title>
<date>1995</date>
<booktitle>In Third Workshop on Very Large Corpora. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="24628" citStr="Collins and Brooks, 1995" startWordPosition="4077" endWordPosition="4080">2003). Conversely, no previous work on PP and RC attachment has integrated specialized ambiguity resolution into parsing. For example, (Toutanova et al., 2004) present one of the best results achieved so far on the WSJ PP set: 87.5%. They also integrate supervised and unsupervised learning. But to our knowledge, the relationship to parsing has not been explored before – even though application to parsing is the stated objective of most work on PP attachment. 5However, the baseline is similarly high for the PP problem if the most likely attachment is chosen per preposition: 72.2% according to (Collins and Brooks, 1995). With the exception of (Hindle and Rooth, 1993), most unsupervised work on PP attachment is based on superficial analysis of the unlabeled corpus without the use of partial parsing (Volk, 2001; Calvo et al., 2005). We believe that dependencies offer a better basis for reliable disambiguation than cooccurrence and fixed-phrase statistics. The difference to (Hindle and Rooth, 1993) was discussed above with respect to analysing the unlabeled corpus. In addition, the decision procedure presented here is different from Hindle et al.&apos;s. LBD uses more context and can, in principle, accommodate arbit</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Michael Collins and James Brooks. 1995. Prepositional attachment through a backed-off model. In Third Workshop on Very Large Corpora. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1684" citStr="Collins, 1997" startWordPosition="254" endWordPosition="255">aparkhi et al., 1994). However, the production of training sets is expensive. They are not available for many domains and languages. This motivates research on combining supervised with unsupervised learning since unannotated text is in ample supply for most domains in the major languages of the world. The question arises how much annotated and unannotated data is necessary in combination learning strategies. We investigate this question for two attachment ambiguity problems: relative clause (RC) attachment and prepositional phrase (PP) attachment. The supervised component is Collins’ parser (Collins, 1997), trained on Hinrich Schiitze Institute for NLP University of Stuttgart hinrich@hotmail.com the Wall Street Journal. The unsupervised component gathers lexical statistics from an unannotated corpus of newswire text. The sizes of both types of corpora, annotated and unannotated, are of interest. We would expect that large annotated corpora (training sets) tend to make the additional information from unannotated corpora redundant. This expectation is confirmed in our experiments. For example, when using the maximum training set available for PP attachment, performance decreases when “unannotated</context>
<context position="5862" citStr="Collins, 1997" startWordPosition="930" endWordPosition="931">s. A PP attachment ambiguity was defined as a subtree matching either [VP [NP PP]] or [VP NP PP]. An example of a PP attachment ambiguity is Example 2 where either the approval (2) ... a majority ... have approved the transaction by written consent ... Both data sets are available for download (Web Appendix, 2006). We did not use the PP data set described by (Ratnaparkhi et al., 1994) because we are using more context than the limited context available in that set (see below). 3 Methods Collins parser. Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel (Collins, 1997; Bikel, 2004). For each ambiguity, we check whether the attachment ambiguity is resolved correctly by the 5 parsers corresponding to the different training sets. If the attachment ambiguity is not recognized (e.g., because parsing failed), then the corresponding ambiguity is excluded for that instance of the parser. As a result, the size of the effective test set varies from parser to parser (see Table 4). Minipar. The unannotated corpus is analyzed using minipar (Lin, 1998), a partial dependency parser. The corpus is parsed and all extracted dependencies are stored for later use. Dependencie</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="23270" citStr="Hindle and Rooth, 1993" startWordPosition="3864" endWordPosition="3867">ifference is that the baseline for RC is much higher than for PP and therefore harder to beat.5 An innovation of our disambiguation system is the use of a search engine, lucene, for serving up dependency statistics. The advantage is that counts can be computed quickly and dynamically. New text can be added on an ongoing basis to the index. The updated dependency statistics are immediately available and can benefit disambiguation performance. Such a system can adapt easily to new topics and changes over time. However, this architecture negatively affects accuracy. The unsupervised approach of (Hindle and Rooth, 1993) achieves almost 80% accuracy by using partial dependency statistics to disambiguate ambiguous sentences in the unlabeled corpus. Ambiguous sentences were excluded from our index to make index construction simple and efficient. Our larger corpus (about 6 times as large as Hindle et al.&apos;s) did not compensate for our lower-quality statistics. 6 Related Work Other work combining supervised and unsupervised learning for parsing includes (Charniak, 1997), (Johnson and Riezler, 2000), and (Schmid, 2002). These papers present integrated formal frameworks for incorporating information learned from unl</context>
<context position="24676" citStr="Hindle and Rooth, 1993" startWordPosition="4085" endWordPosition="4088">ttachment has integrated specialized ambiguity resolution into parsing. For example, (Toutanova et al., 2004) present one of the best results achieved so far on the WSJ PP set: 87.5%. They also integrate supervised and unsupervised learning. But to our knowledge, the relationship to parsing has not been explored before – even though application to parsing is the stated objective of most work on PP attachment. 5However, the baseline is similarly high for the PP problem if the most likely attachment is chosen per preposition: 72.2% according to (Collins and Brooks, 1995). With the exception of (Hindle and Rooth, 1993), most unsupervised work on PP attachment is based on superficial analysis of the unlabeled corpus without the use of partial parsing (Volk, 2001; Calvo et al., 2005). We believe that dependencies offer a better basis for reliable disambiguation than cooccurrence and fixed-phrase statistics. The difference to (Hindle and Rooth, 1993) was discussed above with respect to analysing the unlabeled corpus. In addition, the decision procedure presented here is different from Hindle et al.&apos;s. LBD uses more context and can, in principle, accommodate arbitrarily large contexts. However, an evaluation co</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Donald Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Miles Osborne</author>
<author>Anoop Sarkar</author>
<author>Mark Steedman</author>
</authors>
<title>Corrected co-training for statistical parsers.</title>
<date>2003</date>
<booktitle>In Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, ICML.</booktitle>
<contexts>
<context position="24008" citStr="Hwa et al., 2003" startWordPosition="3976" endWordPosition="3979"> corpus. Ambiguous sentences were excluded from our index to make index construction simple and efficient. Our larger corpus (about 6 times as large as Hindle et al.&apos;s) did not compensate for our lower-quality statistics. 6 Related Work Other work combining supervised and unsupervised learning for parsing includes (Charniak, 1997), (Johnson and Riezler, 2000), and (Schmid, 2002). These papers present integrated formal frameworks for incorporating information learned from unlabeled corpora, but they do not explicitly address PP and RC attachment. The same is true for uncorrected colearning in (Hwa et al., 2003). Conversely, no previous work on PP and RC attachment has integrated specialized ambiguity resolution into parsing. For example, (Toutanova et al., 2004) present one of the best results achieved so far on the WSJ PP set: 87.5%. They also integrate supervised and unsupervised learning. But to our knowledge, the relationship to parsing has not been explored before – even though application to parsing is the stated objective of most work on PP attachment. 5However, the baseline is similarly high for the PP problem if the most likely attachment is chosen per preposition: 72.2% according to (Colli</context>
</contexts>
<marker>Hwa, Osborne, Sarkar, Steedman, 2003</marker>
<rawString>Rebecca Hwa, Miles Osborne, Anoop Sarkar, and Mark Steedman. 2003. Corrected co-training for statistical parsers. In Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stefan Riezler</author>
</authors>
<title>Exploiting auxiliary distributions in stochastic unification-based grammars.</title>
<date>2000</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="23752" citStr="Johnson and Riezler, 2000" startWordPosition="3935" endWordPosition="3938"> new topics and changes over time. However, this architecture negatively affects accuracy. The unsupervised approach of (Hindle and Rooth, 1993) achieves almost 80% accuracy by using partial dependency statistics to disambiguate ambiguous sentences in the unlabeled corpus. Ambiguous sentences were excluded from our index to make index construction simple and efficient. Our larger corpus (about 6 times as large as Hindle et al.&apos;s) did not compensate for our lower-quality statistics. 6 Related Work Other work combining supervised and unsupervised learning for parsing includes (Charniak, 1997), (Johnson and Riezler, 2000), and (Schmid, 2002). These papers present integrated formal frameworks for incorporating information learned from unlabeled corpora, but they do not explicitly address PP and RC attachment. The same is true for uncorrected colearning in (Hwa et al., 2003). Conversely, no previous work on PP and RC attachment has integrated specialized ambiguity resolution into parsing. For example, (Toutanova et al., 2004) present one of the best results achieved so far on the WSJ PP set: 87.5%. They also integrate supervised and unsupervised learning. But to our knowledge, the relationship to parsing has not</context>
</contexts>
<marker>Johnson, Riezler, 2000</marker>
<rawString>Mark Johnson and Stefan Riezler. 2000. Exploiting auxiliary distributions in stochastic unification-based grammars. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony G Rose</author>
<author>Fan Li</author>
</authors>
<title>RCV1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>5</volume>
<contexts>
<context position="3083" citStr="Lewis et al., 2004" startWordPosition="469" endWordPosition="472">e there is a general tendency to this effect, the improvements in our experiments reach a plateau quickly as the unlabeled corpus grows, especially for PP attachment. We attribute this result to the noisiness of the statistics collected from unlabeled corpora. The paper is organized as follows. Sections 2, 3 and 4 describe data sets, methods and experiments. Section 5 evaluates and discusses experimental results. Section 6 compares our approach to prior work. Section 7 states our conclusions. 2 Data Sets The unlabeled corpus is the Reuters RCV1 corpus, about 80,000,000 words of newswire text (Lewis et al., 2004). Three different subsets, corresponding to roughly 10%, 50% and 100% of the corpus, were created for experiments related to the size of the unannotated corpus. (Two weeks after Aug 5, 1997, were set apart for future experiments.) The labeled corpus is the Penn Wall Street Journal treebank (Marcus et al., 1993). We 25 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 25–32, Sydney, July 2006. c�2006 Association for Computational Linguistics created the 5 subsets shown in Table 1 for experiments related to the size of the annotated corpus. or the transaction is performed</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: A new benchmark collection for text categorization research. J. Mach. Learn. Res., 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems,</booktitle>
<location>Granada,</location>
<contexts>
<context position="6342" citStr="Lin, 1998" startWordPosition="1006" endWordPosition="1007"> Methods Collins parser. Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel (Collins, 1997; Bikel, 2004). For each ambiguity, we check whether the attachment ambiguity is resolved correctly by the 5 parsers corresponding to the different training sets. If the attachment ambiguity is not recognized (e.g., because parsing failed), then the corresponding ambiguity is excluded for that instance of the parser. As a result, the size of the effective test set varies from parser to parser (see Table 4). Minipar. The unannotated corpus is analyzed using minipar (Lin, 1998), a partial dependency parser. The corpus is parsed and all extracted dependencies are stored for later use. Dependencies in ambiguous PP attachments (those corresponding to [VP NP PP] and [VP [NP PP]] subtrees) are not indexed. An experiment with indexing both alternatives for ambiguous structures yielded poor results. For example, indexing both alternatives will create a large number of spurious verb attachments of of, which in turn will result in incorrect high attachments by our disambiguation algorithm. For relative clauses, no such filtering is necessary. For example, spurious subject-ve</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LingPipe</author>
</authors>
<date>2006</date>
<note>http://lucene.apache.org.</note>
<contexts>
<context position="15247" citStr="LingPipe, 2006" startWordPosition="2523" endWordPosition="2524">or disam2I&apos;1&apos;om the minipar output, we use all adjectives that modify the NP via the relation mod, and all nouns that modify the NP via the relation nn. MNf:V n:VO MC:V Mn:V n:V N:V 28 biguation is the noun-verb link, but the other dependencies also improve the accuracy of disambiguation (Atterer and Schiitze, 2006). For example, light verbs like make and have only provide disambiguation information when their objects are also considered. Downcasing and hypernym generalizations were used because proper nouns often cause sparse data problems. Named entity classes were identified with LingPipe (LingPipe, 2006). Named entities identified as companies or organizations are replaced with company in the query. Locations are replaced with country. Persons block RC attachment because which-clauses do not attach to person names, resulting in an attachment of the RC to the other NP. query MI +exchange rate(nn(mechanism 12.2 +mechanism(subj(link +currency(obj(link +exchange rate(nn(mechanism 4.8 +mechanism(subj(link +mechanism(subj(link +currency(obj(link 10.2 mechanism(subj(link 3.4 +European Monetary System(subj(link 0 +currency(obj(link +System(subj(link +currency(obj(link 0 European Monetary System(subj(</context>
</contexts>
<marker>LingPipe, 2006</marker>
<rawString>LingPipe. 2006. http://www.aliasi.com/lingpipe/. Lucene. 2006. http://lucene.apache.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large natural language corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--313</pages>
<contexts>
<context position="1007" citStr="Marcus et al., 1993" startWordPosition="148" endWordPosition="151">he supervised component is Collins’ parser, trained on the Wall Street Journal. The unsupervised component gathers lexical statistics from an unannotated corpus of newswire text. We find that the combined system only improves the performance of the parser for small training sets. Surprisingly, the size of the unannotated corpus has little effect due to the noisiness of the lexical statistics acquired by unsupervised learning. 1 Introduction The best performing systems for many tasks in natural language processing are based on supervised training on annotated corpora such as the Penn Treebank (Marcus et al., 1993) and the prepositional phrase data set first described in (Ratnaparkhi et al., 1994). However, the production of training sets is expensive. They are not available for many domains and languages. This motivates research on combining supervised with unsupervised learning since unannotated text is in ample supply for most domains in the major languages of the world. The question arises how much annotated and unannotated data is necessary in combination learning strategies. We investigate this question for two attachment ambiguity problems: relative clause (RC) attachment and prepositional phrase</context>
<context position="3395" citStr="Marcus et al., 1993" startWordPosition="522" endWordPosition="525">nd 4 describe data sets, methods and experiments. Section 5 evaluates and discusses experimental results. Section 6 compares our approach to prior work. Section 7 states our conclusions. 2 Data Sets The unlabeled corpus is the Reuters RCV1 corpus, about 80,000,000 words of newswire text (Lewis et al., 2004). Three different subsets, corresponding to roughly 10%, 50% and 100% of the corpus, were created for experiments related to the size of the unannotated corpus. (Two weeks after Aug 5, 1997, were set apart for future experiments.) The labeled corpus is the Penn Wall Street Journal treebank (Marcus et al., 1993). We 25 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 25–32, Sydney, July 2006. c�2006 Association for Computational Linguistics created the 5 subsets shown in Table 1 for experiments related to the size of the annotated corpus. or the transaction is performed by written consent. unlabeled R 100% 20/08/1996–05/08/1997 (351 days) 50% 20/08/1996–17/02/1997 (182 days) 10% 20/08/1996–24/09/1996 (36 days) labeled WSJ 50% sections 00–12 (23412 sentences) 25% lines 1 – 292960 (11637 sentences) 5% lines 1 – 58284 (2304 sentences) 1% lines 1 – 11720 (500 sentences) 0.05% lin</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large natural language corpus of English: the Penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Jeff Reynar</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy model for prepositional phrase attachment.</title>
<date>1994</date>
<booktitle>In HLT.</booktitle>
<contexts>
<context position="1091" citStr="Ratnaparkhi et al., 1994" startWordPosition="162" endWordPosition="165"> The unsupervised component gathers lexical statistics from an unannotated corpus of newswire text. We find that the combined system only improves the performance of the parser for small training sets. Surprisingly, the size of the unannotated corpus has little effect due to the noisiness of the lexical statistics acquired by unsupervised learning. 1 Introduction The best performing systems for many tasks in natural language processing are based on supervised training on annotated corpora such as the Penn Treebank (Marcus et al., 1993) and the prepositional phrase data set first described in (Ratnaparkhi et al., 1994). However, the production of training sets is expensive. They are not available for many domains and languages. This motivates research on combining supervised with unsupervised learning since unannotated text is in ample supply for most domains in the major languages of the world. The question arises how much annotated and unannotated data is necessary in combination learning strategies. We investigate this question for two attachment ambiguity problems: relative clause (RC) attachment and prepositional phrase (PP) attachment. The supervised component is Collins’ parser (Collins, 1997), train</context>
<context position="5636" citStr="Ratnaparkhi et al., 1994" startWordPosition="892" endWordPosition="895"> containing the pattern NP1 Prep NP2 which. For example, the relative clause in Example 1 can either attach to mechanism or to System. (1) ... the exchange-rate mechanism of the European Monetary System, which links the major EC currencies. A PP attachment ambiguity was defined as a subtree matching either [VP [NP PP]] or [VP NP PP]. An example of a PP attachment ambiguity is Example 2 where either the approval (2) ... a majority ... have approved the transaction by written consent ... Both data sets are available for download (Web Appendix, 2006). We did not use the PP data set described by (Ratnaparkhi et al., 1994) because we are using more context than the limited context available in that set (see below). 3 Methods Collins parser. Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel (Collins, 1997; Bikel, 2004). For each ambiguity, we check whether the attachment ambiguity is resolved correctly by the 5 parsers corresponding to the different training sets. If the attachment ambiguity is not recognized (e.g., because parsing failed), then the corresponding ambiguity is excluded for that instance of the parser. As a result, the size of the effective test set varies </context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994. A maximum entropy model for prepositional phrase attachment. In HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Lexicalization of probabilistic grammars.</title>
<date>2002</date>
<booktitle>In Coling.</booktitle>
<contexts>
<context position="23772" citStr="Schmid, 2002" startWordPosition="3940" endWordPosition="3941">. However, this architecture negatively affects accuracy. The unsupervised approach of (Hindle and Rooth, 1993) achieves almost 80% accuracy by using partial dependency statistics to disambiguate ambiguous sentences in the unlabeled corpus. Ambiguous sentences were excluded from our index to make index construction simple and efficient. Our larger corpus (about 6 times as large as Hindle et al.&apos;s) did not compensate for our lower-quality statistics. 6 Related Work Other work combining supervised and unsupervised learning for parsing includes (Charniak, 1997), (Johnson and Riezler, 2000), and (Schmid, 2002). These papers present integrated formal frameworks for incorporating information learned from unlabeled corpora, but they do not explicitly address PP and RC attachment. The same is true for uncorrected colearning in (Hwa et al., 2003). Conversely, no previous work on PP and RC attachment has integrated specialized ambiguity resolution into parsing. For example, (Toutanova et al., 2004) present one of the best results achieved so far on the WSJ PP set: 87.5%. They also integrate supervised and unsupervised learning. But to our knowledge, the relationship to parsing has not been explored befor</context>
</contexts>
<marker>Schmid, 2002</marker>
<rawString>Helmut Schmid. 2002. Lexicalization of probabilistic grammars. In Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Resolving attachment and clause boundary ambiguities for simplifying relative clause constructs.</title>
<date>2002</date>
<booktitle>In Student Research Workshop, ACL.</booktitle>
<contexts>
<context position="26003" citStr="Siddharthan, 2002" startWordPosition="4305" endWordPosition="4306">ombines estimates from several “backoffs&amp;quot;. In a typical backoff model, there is a single more general model to back off to. (Collins and Brooks, 1995) also present a model with multiple backoffs. One of its variants computes the estimate in question as the average of three backoffs. In addition to the maximum used here, testing other combination strategies for the MI values in the lattice (e.g., average, sum, frequency-weighted sum) would be desirable. In general, MI has not been used in a backoff model before as far as we know. Previous work on relative clause attachment has been supervised (Siddharthan, 2002a; Siddharthan, 2002b; Yeh and Vilain, 1998).6 (Siddharthan, 2002b)&apos;s accuracy for RC attachment is 76.5%.7 7 Conclusion Previous work on specific types of ambiguities (like RC and PP) has not addressed the integration of specific resolution algorithms into a generic statistical parser. In this paper, we have shown for two types of ambiguities, relative clause and prepositional phrase attachment ambiguity, that integration into a statistical parser is possible and that the com6Strictly speaking, our experiments were not completely unsupervised since the default value and the most frequent atta</context>
</contexts>
<marker>Siddharthan, 2002</marker>
<rawString>Advaith Siddharthan. 2002a. Resolving attachment and clause boundary ambiguities for simplifying relative clause constructs. In Student Research Workshop, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Resolving relative clause attachment ambiguities using machine learning techniques and wordnet hierarchies.</title>
<date>2002</date>
<booktitle>In 4th Discourse Anaphora and Anaphora Resolution Colloquium.</booktitle>
<contexts>
<context position="26003" citStr="Siddharthan, 2002" startWordPosition="4305" endWordPosition="4306">ombines estimates from several “backoffs&amp;quot;. In a typical backoff model, there is a single more general model to back off to. (Collins and Brooks, 1995) also present a model with multiple backoffs. One of its variants computes the estimate in question as the average of three backoffs. In addition to the maximum used here, testing other combination strategies for the MI values in the lattice (e.g., average, sum, frequency-weighted sum) would be desirable. In general, MI has not been used in a backoff model before as far as we know. Previous work on relative clause attachment has been supervised (Siddharthan, 2002a; Siddharthan, 2002b; Yeh and Vilain, 1998).6 (Siddharthan, 2002b)&apos;s accuracy for RC attachment is 76.5%.7 7 Conclusion Previous work on specific types of ambiguities (like RC and PP) has not addressed the integration of specific resolution algorithms into a generic statistical parser. In this paper, we have shown for two types of ambiguities, relative clause and prepositional phrase attachment ambiguity, that integration into a statistical parser is possible and that the com6Strictly speaking, our experiments were not completely unsupervised since the default value and the most frequent atta</context>
</contexts>
<marker>Siddharthan, 2002</marker>
<rawString>Advaith Siddharthan. 2002b. Resolving relative clause attachment ambiguities using machine learning techniques and wordnet hierarchies. In 4th Discourse Anaphora and Anaphora Resolution Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning random walk models for inducing word dependency distributions.</title>
<date>2004</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="24162" citStr="Toutanova et al., 2004" startWordPosition="3999" endWordPosition="4002"> as Hindle et al.&apos;s) did not compensate for our lower-quality statistics. 6 Related Work Other work combining supervised and unsupervised learning for parsing includes (Charniak, 1997), (Johnson and Riezler, 2000), and (Schmid, 2002). These papers present integrated formal frameworks for incorporating information learned from unlabeled corpora, but they do not explicitly address PP and RC attachment. The same is true for uncorrected colearning in (Hwa et al., 2003). Conversely, no previous work on PP and RC attachment has integrated specialized ambiguity resolution into parsing. For example, (Toutanova et al., 2004) present one of the best results achieved so far on the WSJ PP set: 87.5%. They also integrate supervised and unsupervised learning. But to our knowledge, the relationship to parsing has not been explored before – even though application to parsing is the stated objective of most work on PP attachment. 5However, the baseline is similarly high for the PP problem if the most likely attachment is chosen per preposition: 72.2% according to (Collins and Brooks, 1995). With the exception of (Hindle and Rooth, 1993), most unsupervised work on PP attachment is based on superficial analysis of the unla</context>
</contexts>
<marker>Toutanova, Manning, Ng, 2004</marker>
<rawString>Kristina Toutanova, Christopher D. Manning, and Andrew Y. Ng. 2004. Learning random walk models for inducing word dependency distributions. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Volk</author>
</authors>
<title>Exploiting the WWW as a corpus to resolve pp attachment ambiguities.</title>
<date>2001</date>
<booktitle>In Corpus Linguistics</booktitle>
<contexts>
<context position="24821" citStr="Volk, 2001" startWordPosition="4111" endWordPosition="4112">on the WSJ PP set: 87.5%. They also integrate supervised and unsupervised learning. But to our knowledge, the relationship to parsing has not been explored before – even though application to parsing is the stated objective of most work on PP attachment. 5However, the baseline is similarly high for the PP problem if the most likely attachment is chosen per preposition: 72.2% according to (Collins and Brooks, 1995). With the exception of (Hindle and Rooth, 1993), most unsupervised work on PP attachment is based on superficial analysis of the unlabeled corpus without the use of partial parsing (Volk, 2001; Calvo et al., 2005). We believe that dependencies offer a better basis for reliable disambiguation than cooccurrence and fixed-phrase statistics. The difference to (Hindle and Rooth, 1993) was discussed above with respect to analysing the unlabeled corpus. In addition, the decision procedure presented here is different from Hindle et al.&apos;s. LBD uses more context and can, in principle, accommodate arbitrarily large contexts. However, an evaluation comparing the performance of the two methods is necessary. The LBD model can be viewed as a backoff model that combines estimates from several “bac</context>
</contexts>
<marker>Volk, 2001</marker>
<rawString>Martin Volk. 2001. Exploiting the WWW as a corpus to resolve pp attachment ambiguities. In Corpus Linguistics 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Web Appendix</author>
</authors>
<date>2006</date>
<note>http://www.ims.unistuttgart.de/∼schuetze/colingacl06/apdx.html.</note>
<contexts>
<context position="5564" citStr="Appendix, 2006" startWordPosition="880" endWordPosition="881">Table 2). An RC attachment ambiguity was defined as a sentence containing the pattern NP1 Prep NP2 which. For example, the relative clause in Example 1 can either attach to mechanism or to System. (1) ... the exchange-rate mechanism of the European Monetary System, which links the major EC currencies. A PP attachment ambiguity was defined as a subtree matching either [VP [NP PP]] or [VP NP PP]. An example of a PP attachment ambiguity is Example 2 where either the approval (2) ... a majority ... have approved the transaction by written consent ... Both data sets are available for download (Web Appendix, 2006). We did not use the PP data set described by (Ratnaparkhi et al., 1994) because we are using more context than the limited context available in that set (see below). 3 Methods Collins parser. Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel (Collins, 1997; Bikel, 2004). For each ambiguity, we check whether the attachment ambiguity is resolved correctly by the 5 parsers corresponding to the different training sets. If the attachment ambiguity is not recognized (e.g., because parsing failed), then the corresponding ambiguity is excluded for that instanc</context>
</contexts>
<marker>Appendix, 2006</marker>
<rawString>Web Appendix. 2006. http://www.ims.unistuttgart.de/∼schuetze/colingacl06/apdx.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Alistair Moffat</author>
<author>Timothy C Bell</author>
</authors>
<title>Managing Gigabytes: Compressing and Indexing Documents and Images.</title>
<date>1999</date>
<publisher>Morgan Kaufman.</publisher>
<contexts>
<context position="7180" citStr="Witten et al., 1999" startWordPosition="1132" endWordPosition="1135">t indexed. An experiment with indexing both alternatives for ambiguous structures yielded poor results. For example, indexing both alternatives will create a large number of spurious verb attachments of of, which in turn will result in incorrect high attachments by our disambiguation algorithm. For relative clauses, no such filtering is necessary. For example, spurious subject-verb dependencies due to RC ambiguities are rare compared to a large number of subject-verb dependencies that can be extracted reliably. Inverted index. Dependencies extracted by minipar are stored in an inverted index (Witten et al., 1999), implemented in Lucene (Lucene, 2006). For example, “john subj buy&amp;quot;, the analysis returned by minipar for John buys, is stored as “john buy john&lt;subj 26 subj&lt;buy john&lt;subj&lt;buy&amp;quot;. All words, dependencies and partial dependencies of a sentence are stored together as one document. This storage mechanism enables fast on-line queries for lexical and dependency statistics, e.g., how many sentences contain the dependency “john subj buy&amp;quot;, how often does john occur as a subject, how often does buy have john as a subject and car as an object etc. Query results are approximate because double occurrences </context>
</contexts>
<marker>Witten, Moffat, Bell, 1999</marker>
<rawString>Ian H. Witten, Alistair Moffat, and Timothy C. Bell. 1999. Managing Gigabytes: Compressing and Indexing Documents and Images. Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander S Yeh</author>
<author>Marc B Vilain</author>
</authors>
<title>Some properties of preposition and subordinate conjunction attachments. In Coling.</title>
<date>1998</date>
<contexts>
<context position="26047" citStr="Yeh and Vilain, 1998" startWordPosition="4310" endWordPosition="4313">&amp;quot;. In a typical backoff model, there is a single more general model to back off to. (Collins and Brooks, 1995) also present a model with multiple backoffs. One of its variants computes the estimate in question as the average of three backoffs. In addition to the maximum used here, testing other combination strategies for the MI values in the lattice (e.g., average, sum, frequency-weighted sum) would be desirable. In general, MI has not been used in a backoff model before as far as we know. Previous work on relative clause attachment has been supervised (Siddharthan, 2002a; Siddharthan, 2002b; Yeh and Vilain, 1998).6 (Siddharthan, 2002b)&apos;s accuracy for RC attachment is 76.5%.7 7 Conclusion Previous work on specific types of ambiguities (like RC and PP) has not addressed the integration of specific resolution algorithms into a generic statistical parser. In this paper, we have shown for two types of ambiguities, relative clause and prepositional phrase attachment ambiguity, that integration into a statistical parser is possible and that the com6Strictly speaking, our experiments were not completely unsupervised since the default value and the most frequent attachment were determined based on the developm</context>
</contexts>
<marker>Yeh, Vilain, 1998</marker>
<rawString>Alexander S. Yeh and Marc B. Vilain. 1998. Some properties of preposition and subordinate conjunction attachments. In Coling.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>