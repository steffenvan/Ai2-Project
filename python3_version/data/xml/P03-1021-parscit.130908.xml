<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000448">
<title confidence="0.989727">
Minimum Error Rate Training in Statistical Machine Translation
</title>
<author confidence="0.996652">
Franz Josef Och
</author>
<affiliation confidence="0.899017">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.62321">
Marina del Rey, CA 90292
</address>
<email confidence="0.999054">
och@isi.edu
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996675">
Often, the training procedure for statisti-
cal machine translation models is based on
maximum likelihood or related criteria. A
general problem of this approach is that
there is only a loose relation to the final
translation quality on unseen text. In this
paper, we analyze various training criteria
which directly optimize translation qual-
ity. These training criteria make use of re-
cently proposed automatic evaluation met-
rics. We describe a new algorithm for effi-
cient training an unsmoothed error count.
We show that significantly better results
can often be obtained if the final evalua-
tion criterion is taken directly into account
as part of the training procedure.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999868521739131">
Many tasks in natural language processing have
evaluation criteria that go beyond simply count-
ing the number of wrong decisions the system
makes. Some often used criteria are, for example,
F-Measure for parsing, mean average precision for
ranked retrieval, and BLEU or multi-reference word
error rate for statistical machine translation. The use
of statistical techniques in natural language process-
ing often starts out with the simplifying (often im-
plicit) assumption that the final scoring is based on
simply counting the number of wrong decisions, for
instance, the number of sentences incorrectly trans-
lated in machine translation. Hence, there is a mis-
match between the basic assumptions of the used
statistical approach and the final evaluation criterion
used to measure success in a task.
Ideally, we would like to train our model param-
eters such that the end-to-end performance in some
application is optimal. In this paper, we investigate
methods to efficiently optimize model parameters
with respect to machine translation quality as mea-
sured by automatic evaluation criteria such as word
error rate and BLEU.
</bodyText>
<sectionHeader confidence="0.96781" genericHeader="introduction">
2 Statistical Machine Translation with
Log-linear Models
</sectionHeader>
<bodyText confidence="0.9996435">
Let us assume that we are given a source (‘French’)
sentence , which is
to be translated into a target (‘English’) sentence
Among all possible
target sentences, we will choose the sentence with
the highest probability:1
</bodyText>
<equation confidence="0.946582">
Pr (1)
</equation>
<bodyText confidence="0.933427190476191">
The argmax operation denotes the search problem,
i.e. the generation of the output sentence in the tar-
get language. The decision in Eq. 1 minimizes the
number of decision errors. Hence, under a so-called
zero-one loss function this decision rule is optimal
(Duda and Hart, 1973). Note that using a differ-
ent loss function—for example, one induced by the
BLEU metric—a different decision rule would be
optimal.
&apos;The notational convention will be as follows. We use the
symbol Pr to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol .
As the true probability distribution Pr is un-
known, we have to develop a model that ap-
proximates Pr . We directly model the posterior
probability Pr by using a log-linear model. In
this framework, we have a set of feature functions
. For each feature function,
there exists a model parameter .
The direct translation probability is given by:
</bodyText>
<listItem confidence="0.6212">
(2)
(3)
</listItem>
<bodyText confidence="0.979091625">
In this framework, the modeling problem amounts
to developing suitable feature functions that capture
the relevant properties of the translation task. The
training problem amounts to obtaining suitable pa-
rameter values . A standard criterion for log-
linear models is the MMI (maximum mutual infor-
mation) criterion, which can be derived from the
maximum entropy principle:
</bodyText>
<equation confidence="0.704289">
(4)
</equation>
<bodyText confidence="0.999930769230769">
The optimization problem under this criterion has
very nice properties: there is one unique global op-
timum, and there are algorithms (e.g. gradient de-
scent) that are guaranteed to converge to the global
optimum. Yet, the ultimate goal is to obtain good
translation quality on unseen test data. Experience
shows that good results can be obtained using this
approach, yet there is no reason to assume that an
optimization of the model parameters using Eq. 4
yields parameters that are optimal with respect to
translation quality.
The goal of this paper is to investigate alterna-
tive training criteria and corresponding training al-
gorithms, which are directly related to translation
quality measured with automatic evaluation criteria.
In Section 3, we review various automatic evalua-
tion criteria used in statistical machine translation.
In Section 4, we present two different training crite-
ria which try to directly optimize an error count. In
Section 5, we sketch a new training algorithm which
efficiently optimizes an unsmoothed error count. In
Section 6, we describe the used feature functions and
our approach to compute the candidate translations
that are the basis for our training procedure. In Sec-
tion 7, we evaluate the different training criteria in
the context of several MT experiments.
</bodyText>
<sectionHeader confidence="0.92656" genericHeader="method">
3 Automatic Assessment of Translation
Quality
</sectionHeader>
<bodyText confidence="0.999478735294118">
In recent years, various methods have been pro-
posed to automatically evaluate machine translation
quality by comparing hypothesis translations with
reference translations. Examples of such methods
are word error rate, position-independent word error
rate (Tillmann et al., 1997), generation string accu-
racy (Bangalore et al., 2000), multi-reference word
error rate (Nießen et al., 2000), BLEU score (Pap-
ineni et al., 2001), NIST score (Doddington, 2002).
All these criteria try to approximate human assess-
ment and often achieve an astonishing degree of cor-
relation to human subjective evaluation of fluency
and adequacy (Papineni et al., 2001; Doddington,
2002).
In this paper, we use the following methods:
multi-reference word error rate (mWER):
When this method is used, the hypothesis trans-
lation is compared to various reference transla-
tions by computing the edit distance (minimum
number of substitutions, insertions, deletions)
between the hypothesis and the closest of the
given reference translations.
multi-reference position independent error rate
(mPER): This criterion ignores the word order
by treating a sentence as a bag-of-words and
computing the minimum number of substitu-
tions, insertions, deletions needed to transform
the hypothesis into the closest of the given ref-
erence translations.
BLEU score: This criterion computes the ge-
ometric mean of the precision of-grams of
various lengths between a hypothesis and a set
of reference translations multiplied by a factor
BP that penalizes short sentences:
</bodyText>
<footnote confidence="0.447962333333333">
BLEU BP
Heredenotes the precision of-grams in the
hypothesis translation. We use .
</footnote>
<equation confidence="0.600964666666667">
Pr
exp
exp
</equation>
<bodyText confidence="0.971085275862069">
NIST score: This criterion computes a
weighted precision of-grams between a hy-
pothesis and a set of reference translations mul-
tiplied by a factor BP’ that penalizes short
sentences:
Heredenotes the weighted precision of-
grams in the translation. We use .
Both, NIST and BLEU are accuracy measures,
and thus larger values reflect better translation qual-
ity. Note that NIST and BLEU scores are not addi-
tive for different sentences, i.e. the score for a doc-
ument cannot be obtained by simply summing over
scores for individual sentences.
It includes an argmax operation (Eq. 6). There-
fore, it is not possible to compute a gradient
and we cannot use gradient descent methods to
perform optimization.
The objective function has many different local
optima. The optimization algorithm must han-
dle this.
In addition, even if we manage to solve the optimiza-
tion problem, we might face the problem of overfit-
ting the training data. In Section 5, we describe an
efficient optimization algorithm.
To be able to compute a gradient and to make the
objective function smoother, we can use the follow-
ing error criterion which is essentially a smoothed
error count, with a parameter to adjust the smooth-
ness:
</bodyText>
<figure confidence="0.889127666666667">
NIST BP’
4 Training Criteria for Minimum Error
Rate Training
</figure>
<bodyText confidence="0.994064235294117">
In the following, we assume that we can measure
the number of errors in sentence by comparing it
with a reference sentenceusing a function E .
However, the following exposition can be easily
adapted to accuracy metrics and to metrics that make
use of multiple references.
We assume that the number of errors for a set
of sentences is obtained by summing the er-
rors for the individual sentences:
.
Our goal is to obtain a minimal error count on a
representative corpus with given reference trans-
lations and a set of different candidate transla-
tions for each input sentence
.
In the extreme case, for , Eq. 7 converges
to the unsmoothed criterion of Eq. 5 (except in the
case of ties). Note, that the resulting objective func-
tion might still have local optima, which makes the
optimization hard compared to using the objective
function of Eq. 4 which does not have different lo-
cal optima. The use of this type of smoothed error
count is a common approach in the speech commu-
nity (Juang et al., 1995; Schl¨uter and Ney, 2001).
Figure 1 shows the actual shape of the smoothed
and the unsmoothed error count for two parame-
ters in our translation system. We see that the un-
smoothed error count has many different local op-
tima and is very unstable. The smoothed error count
is much more stable and has fewer local optima. But
(5) as we show in Section 7, the performance on our
task obtained with the smoothed error count does
not differ significantly from that obtained with the
unsmoothed error count.
</bodyText>
<equation confidence="0.912354">
(7)
(6)
</equation>
<bodyText confidence="0.987938">
The above stated optimization criterion is not easy
to handle:
</bodyText>
<sectionHeader confidence="0.997868" genericHeader="method">
5 Optimization Algorithm for
Unsmoothed Error Count
</sectionHeader>
<bodyText confidence="0.9998994">
A standard algorithm for the optimization of the
unsmoothed error count (Eq. 5) is Powells algo-
rithm combined with a grid-based line optimiza-
tion method (Press et al., 2002). We start at a ran-
dom point in the -dimensional parameter space
</bodyText>
<figure confidence="0.9880565">
with
9450
error count
9440
9430
9450
9445
9440
9435
error count
9430
9425
9420
9415
9410
9405
unsmoothed error count
smoothed error rate (alpha=3)
-4 -3 -2 -1 0 1 2 3 4
unsmoothed error count
smoothed error rate (alpha=3)
-4 -3 -2 -1 0 1 2 3 4
9480
9470
9460
9420
9410
9400
</figure>
<figureCaption confidence="0.765999">
Figure 1: Shape of error count and smoothed error count for two different model parameters. These curves
have been computed on the development corpus (see Section 7, Table 1) using alternatives per source
sentence. The smoothed error count has been computed with a smoothing parameter .
</figureCaption>
<bodyText confidence="0.997837796296296">
and try to find a better scoring point in the param-
eter space by making a one-dimensional line min-
imization along the directions given by optimizing
one parameter while keeping all other parameters
fixed. To avoid finding a poor local optimum, we
start from different initial parameter values. A major
problem with the standard approach is the fact that
grid-based line optimization is hard to adjust such
that both good performance and efficient search are
guaranteed. If a fine-grained grid is used then the
algorithm is slow. If a large grid is used then the
optimal solution might be missed.
In the following, we describe a new algorithm for
efficient line optimization of the unsmoothed error
count (Eq. 5) using a log-linear model (Eq. 3) which
is guaranteed to find the optimal solution. The new
algorithm is much faster and more stable than the
grid-based line optimization method.
Computing the most probable sentence out of a
set of candidate translation (see
Eq. 6) along a line with parameter
results in an optimization problem of the following
functional form:
Here, and are constants with respect to .
Hence, every candidate translation in corresponds
to a line. The function
is piecewise linear (Papineni, 1999). This allows us
to compute an efficient exhaustive representation of
that function.
In the following, we sketch the new algorithm
to optimize Eq. 5: We compute the ordered se-
quence of linear intervals constituting for ev-
ery sentence together with the incremental change
in error count from the previous to the next inter-
val. Hence, we obtain for every sentence a se-
quence which denote the
interval boundaries and a corresponding sequence
for the change in error count involved at the corre-
sponding interval boundary
Here, denotes the change in the error count at
.
position to the error count at position
. By merging all sequences and
for all different sentences of our corpus, the
complete set of interval boundaries and error count
changes on the whole corpus are obtained. The op-
timal can now be computed easily by traversing
the sequence of interval boundaries while updating
an error count.
It is straightforward to refine this algorithm to
also handle the BLEU and NIST scores instead of
sentence-level error counts by accumulating the rel-
evant statistics for computing these scores (n-gram
precision, translation length and reference length) .
</bodyText>
<sectionHeader confidence="0.997352" genericHeader="method">
6 Baseline Translation Approach
</sectionHeader>
<bodyText confidence="0.999981151515152">
The basic feature functions of our model are iden-
tical to the alignment template approach (Och and
Ney, 2002). In this translation model, a sentence
is translated by segmenting the input sentence into
phrases, translating these phrases and reordering the
translations in the target language. In addition to the
feature functions described in (Och and Ney, 2002),
our system includes a phrase penalty (the number
of alignment templates used) and special alignment
features. Altogether, the log-linear model includes
different features.
Note that many of the used feature functions are
derived from probabilistic models: the feature func-
tion is defined as the negative logarithm of the cor-
responding probabilistic model. Therefore, the fea-
ture functions are much more ’informative’ than for
instance the binary feature functions used in stan-
dard maximum entropy models in natural language
processing.
For search, we use a dynamic programming
beam-search algorithm to explore a subset of all pos-
sible translations (Och et al., 1999) and extract-
best candidate translations using A* search (Ueffing
et al., 2002).
Using an-best approximation, we might face the
problem that the parameters trained are good for the
list oftranslations used, but yield worse transla-
tion results if these parameters are used in the dy-
namic programming search. Hence, it is possible
that our new search produces translations with more
errors on the training corpus. This can happen be-
cause with the modified model scaling factors the
-best list can change significantly and can include
sentences not in the existing-best list. To avoid
this problem, we adopt the following solution: First,
we perform search (using a manually defined set of
parameter values) and compute an-best list, and
use this-best list to train the model parameters.
Second, we use the new model parameters in a new
search and compute a new-best list, which is com-
bined with the existing-best list. Third, using this
extended-best list new model parameters are com-
puted. This is iterated until the resulting-best list
does not change. In this algorithm convergence is
guaranteed as, in the limit, the-best list will con-
tain all possible translations. In our experiments,
we compute in every iteration about 200 alternative
translations. In practice, the algorithm converges af-
ter about five to seven iterations. As a result, error
rate cannot increase on the training corpus.
A major problem in applying the MMI criterion
is the fact that the reference translations need to be
part of the provided-best list. Quite often, none of
the given reference translations is part of the-best
list because the search algorithm performs pruning,
which in principle limits the possible translations
that can be produced given a certain input sentence.
To solve this problem, we define for the MMI train-
ing new pseudo-references by selecting from the-
best list all the sentences which have a minimal num-
ber of word errors with respect to any of the true ref-
erences. Note that due to this selection approach, the
results of the MMI criterion might be biased toward
the mWER criterion. It is a major advantage of the
minimum error rate training that it is not necessary
to choose pseudo-references.
</bodyText>
<sectionHeader confidence="0.9999" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.977793230769231">
We present results on the 2002 TIDES Chinese–
English small data track task. The goal is the trans-
lation of news text from Chinese to English. Ta-
ble 1 provides some statistics on the training, de-
velopment and test corpus used. The system we use
does not include rule-based components to translate
numbers, dates or names. The basic feature func-
tions were trained using the training corpus. The de-
velopment corpus was used to optimize the parame-
ters of the log-linear model. Translation results are
reported on the test corpus.
Table 2 shows the results obtained on the develop-
ment corpus and Table 3 shows the results obtained
</bodyText>
<tableCaption confidence="0.622078">
Table 2: Effect of different error criteria in training on the development corpus. Note that better results
correspond to larger BLEU and NIST scores and to smaller error rates. Italic numbers refer to results for
which the difference to the best result (indicated in bold) is not statistically significant.
</tableCaption>
<table confidence="0.993021111111111">
error criterion used in training mWER [%] mPER [%] BLEU [%] NIST # words
confidence intervals +/- 2.4 +/- 1.8 +/- 1.2 +/- 0.2 -
MMI 70.7 55.3 12.2 5.12 10382
mWER 69.7 52.9 15.4 5.93 10914
smoothed-mWER 69.8 53.0 15.2 5.93 10925
mPER 71.9 51.6 17.2 6.61 11671
smoothed-mPER 71.8 51.8 17.0 6.56 11625
BLEU 76.8 54.6 19.6 6.93 13325
NIST 73.8 52.8 18.9 7.08 12722
</table>
<tableCaption confidence="0.955569">
Table 1: Characteristics of training corpus (Train),
manual lexicon (Lex), development corpus (Dev),
test corpus (Test).
</tableCaption>
<table confidence="0.9994129">
Chinese English
Train Sentences 5 109
Words 89 121 111 251
Singletons 3 419 4 130
Vocabulary 8 088 8 807
Lex Entries 82 103
Dev Sentences 640
Words 11 746 13 573
Test Sentences 878
Words 24 323 26 489
</table>
<bodyText confidence="0.999925">
on the test corpus. Italic numbers refer to results
for which the difference to the best result (indicated
in bold) is not statistically significant. For all error
rates, we show the maximal occurring 95% confi-
dence interval in any of the experiments for that col-
umn. The confidence intervals are computed using
bootstrap resampling (Press et al., 2002). The last
column provides the number of words in the pro-
duced translations which can be compared with the
average number of reference words occurring in the
development and test corpora given in Table 1.
We observe that if we choose a certain error crite-
rion in training, we obtain in most cases the best re-
sults using the same criterion as the evaluation met-
ric on the test data. The differences can be quite
large: If we optimize with respect to word error rate,
the results are mWER=68.3%, which is better than
if we optimize with respect to BLEU or NIST and
the difference is statistically significant. Between
BLEU and NIST, the differences are more moderate,
but by optimizing on NIST, we still obtain a large
improvement when measured with NIST compared
to optimizing on BLEU.
The MMI criterion produces significantly worse
results on all error rates besides mWER. Note that,
due to the re-definition of the notion of reference
translation by using minimum edit distance, the re-
sults of the MMI criterion are biased toward mWER.
It can be expected that by using a suitably defined-
gram precision to define the pseudo-references for
MMI instead of using edit distance, it is possible to
obtain better BLEU or NIST scores.
An important part of the differences in the trans-
lation scores is due to the different translation length
(last column in Table 3). The mWER and MMI cri-
teria prefer shorter translations which are heavily pe-
nalized by the BLEU and NIST brevity penalty.
We observe that the smoothed error count gives
almost identical results to the unsmoothed error
count. This might be due to the fact that the number
of parameters trained is small and no serious overfit-
ting occurs using the unsmoothed error count.
</bodyText>
<sectionHeader confidence="0.999774" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.906381888888889">
The use of log-linear models for statistical machine
translation was suggested by Papineni et al. (1997)
and Och and Ney (2002).
The use of minimum classification error
training and using a smoothed error count is
common in the pattern recognition and speech
Table 3: Effect of different error criteria used in training on the test corpus. Note that better results corre-
spond to larger BLEU and NIST scores and to smaller error rates. Italic numbers refer to results for which
the difference to the best result (indicated in bold) is not statistically significant.
</bodyText>
<table confidence="0.995015">
error criterion used in training mWER [%] mPER [%] BLEU [%] NIST # words
confidence intervals +/- 2.7 +/- 1.9 +/- 0.8 +/- 0.12 -
MMI 68.0 51.0 11.3 5.76 21933
mWER 68.3 50.2 13.5 6.28 22914
smoothed-mWER 68.2 50.2 13.2 6.27 22902
mPER 70.2 49.8 15.2 6.71 24399
smoothed-mPER 70.0 49.7 15.2 6.69 24198
BLEU 76.1 53.2 17.2 6.66 28002
NIST 73.3 51.5 16.4 6.80 26602
</table>
<bodyText confidence="0.998622947368421">
recognition community (Duda and Hart, 1973;
Juang et al., 1995; Schl¨uter and Ney, 2001).
Paciorek and Rosenfeld (2000) use minimum clas-
sification error training for optimizing parameters
of a whole-sentence maximum entropy language
model.
A technically very different approach that has a
similar goal is the minimum Bayes risk approach, in
which an optimal decision rule with respect to an
application specific risk/loss function is used, which
will normally differ from Eq. 3. The loss function is
either identical or closely related to the final evalua-
tion criterion. In contrast to the approach presented
in this paper, the training criterion and the statisti-
cal models used remain unchanged in the minimum
Bayes risk approach. In the field of natural language
processing this approach has been applied for exam-
ple in parsing (Goodman, 1996) and word alignment
(Kumar and Byrne, 2002).
</bodyText>
<sectionHeader confidence="0.997387" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999969327868853">
We presented alternative training criteria for log-
linear statistical machine translation models which
are directly related to translation quality: an un-
smoothed error count and a smoothed error count
on a development corpus. For the unsmoothed er-
ror count, we presented a new line optimization al-
gorithm which can efficiently find the optimal solu-
tion along a line. We showed that this approach ob-
tains significantly better results than using the MMI
training criterion (with our method to define pseudo-
references) and that optimizing error rate as part of
the training criterion helps to obtain better error rate
on unseen test data. As a result, we expect that ac-
tual ’true’ translation quality is improved, as previ-
ous work has shown that for some evaluation cri-
teria there is a correlation with human subjective
evaluation of fluency and adequacy (Papineni et al.,
2001; Doddington, 2002). However, the different
evaluation criteria yield quite different results on our
Chinese–English translation task and therefore we
expect that not all of them correlate equally well to
human translation quality.
The following important questions should be an-
swered in the future:
How many parameters can be reliably esti-
mated using unsmoothed minimum error rate
criteria using a given development corpus size?
We expect that directly optimizing error rate for
many more parameters would lead to serious
overfitting problems. Is it possible to optimize
more parameters using the smoothed error rate
criterion?
Which error rate should be optimized during
training? This relates to the important question
of which automatic evaluation measure is opti-
mally correlated to human assessment of trans-
lation quality.
Note, that this approach can be applied to any
evaluation criterion. Hence, if an improved auto-
matic evaluation criterion is developed that has an
even better correlation with human judgments than
BLEU and NIST, we can plug this alternative cri-
terion directly into the training procedure and opti-
mize the model parameters for it. This means that
improved translation evaluation measures lead di-
rectly to improved machine translation quality. Of
course, the approach presented here places a high
demand on the fidelity of the measure being opti-
mized. It might happen that by directly optimiz-
ing an error measure in the way described above,
weaknesses in the measure might be exploited that
could yield better scores without improved transla-
tion quality. Hence, this approach poses new chal-
lenges for developers of automatic evaluation crite-
ria.
Many tasks in natural language processing, for in-
stance summarization, have evaluation criteria that
go beyond simply counting the number of wrong
system decisions and the framework presented here
might yield improved systems for these tasks as
well.
</bodyText>
<sectionHeader confidence="0.996042" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9967215">
This work was supported by DARPA-ITO grant
66001-00-1-9814.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99980824">
Srinivas Bangalore, O. Rambox, and S. Whittaker. 2000.
Evaluation metrics for generation. In Proceedings
of the International Conference on Natural Language
Generation, Mitzpe Ramon, Israel.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology.
Richhard O. Duda and Peter E. Hart. 1973. Pattern Clas-
sification and Scene Analysis. John Wiley, New York,
NY.
Joshua Goodman. 1996. Parsing algorithms and metrics.
In Proceedings of the 34th Annual Meeting of the ACL,
pages 177–183, Santa Cruz, CA, June.
B. H. Juang, W. Chou, and C. H. Lee. 1995. Statisti-
cal and discriminative methods for speech recognition.
In A. J. Rubio Ayuso and J. M. Lopez Soler, editors,
Speech Recognition and Coding - New Advances and
Trends. Springer Verlag, Berlin, Germany.
Shankar Kumar and William Byrne. 2002. Minimum
bayes-risk alignment of bilingual texts. In Proc. of
the Conference on Empirical Methods in Natural Lan-
guage Processing, Philadelphia, PA.
Sonja Nießen, Franz J. Och, G. Leusch, and Hermann
Ney. 2000. An evaluation tool for machine transla-
tion: Fast evaluation for machine translation research.
In Proc. of the Second Int. Conf. on Language Re-
sources and Evaluation (LREC), pages 39–45, Athens,
Greece, May.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of the 40th Annual
Meeting ofthe Association for Computational Linguis-
tics (ACL), Philadelphia, PA, July.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proc. of the Joint SIGDAT Conf.
on Empirical Methods in Natural Language Process-
ing and Very Large Corpora, pages 20–28, University
of Maryland, College Park, MD, June.
Chris Paciorek and Roni Rosenfeld. 2000. Minimum
classification error training in exponential language
models. In NIST/DARPA Speech Transcription Work-
shop, May.
Kishore A. Papineni, Salim Roukos, and R. T. Ward.
1997. Feature-based language understanding. In Eu-
ropean Conf. on Speech Communication and Technol-
ogy, pages 1435–1438, Rhodes, Greece, September.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for auto-
matic evaluation of machine translation. Technical
Report RC22176 (W0109-022), IBM Research Divi-
sion, Thomas J. Watson Research Center, Yorktown
Heights, NY, September.
Kishore A. Papineni. 1999. Discriminative training via
linear programming. In Proceedings of the 1999 IEEE
International Conference on Acoustics, Speech &amp; Sig-
nal Processing, Atlanta, March.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++. Cambridge University Press, Cambridge,
UK.
Ralf Schl¨uter and Hermann Ney. 2001. Model-based
MCE bound to the true Bayes’ error. IEEE Signal Pro-
cessing Letters, 8(5):131–133, May.
Christoph Tillmann, Stephan Vogel, Hermann Ney, Alex
Zubiaga, and Hassan Sawaf. 1997. Accelerated
DP based search for statistical translation. In Euro-
pean Conf. on Speech Communication and Technol-
ogy, pages 2667–2670, Rhodes, Greece, September.
Nicola Ueffing, Franz Josef Och, and Hermann Ney.
2002. Generation of word graphs in statistical ma-
chine translation. In Proc. Conference on Empiri-
cal Methods for Natural Language Processing, pages
156–163, Philadelphia, PE, July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986610">
<title confidence="0.999915">Minimum Error Rate Training in Statistical Machine Translation</title>
<author confidence="0.999795">Franz Josef Och</author>
<affiliation confidence="0.9986745">Information Sciences Institute University of Southern California</affiliation>
<address confidence="0.997708">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292</address>
<email confidence="0.999192">och@isi.edu</email>
<abstract confidence="0.999695117647059">Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>O Rambox</author>
<author>S Whittaker</author>
</authors>
<title>Evaluation metrics for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Natural Language Generation,</booktitle>
<location>Mitzpe Ramon,</location>
<contexts>
<context position="5357" citStr="Bangalore et al., 2000" startWordPosition="832" endWordPosition="835">n 6, we describe the used feature functions and our approach to compute the candidate translations that are the basis for our training procedure. In Section 7, we evaluate the different training criteria in the context of several MT experiments. 3 Automatic Assessment of Translation Quality In recent years, various methods have been proposed to automatically evaluate machine translation quality by comparing hypothesis translations with reference translations. Examples of such methods are word error rate, position-independent word error rate (Tillmann et al., 1997), generation string accuracy (Bangalore et al., 2000), multi-reference word error rate (Nießen et al., 2000), BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002). All these criteria try to approximate human assessment and often achieve an astonishing degree of correlation to human subjective evaluation of fluency and adequacy (Papineni et al., 2001; Doddington, 2002). In this paper, we use the following methods: multi-reference word error rate (mWER): When this method is used, the hypothesis translation is compared to various reference translations by computing the edit distance (minimum number of substitutions, insertions, deletio</context>
</contexts>
<marker>Bangalore, Rambox, Whittaker, 2000</marker>
<rawString>Srinivas Bangalore, O. Rambox, and S. Whittaker. 2000. Evaluation metrics for generation. In Proceedings of the International Conference on Natural Language Generation, Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="5479" citStr="Doddington, 2002" startWordPosition="853" endWordPosition="854">aining procedure. In Section 7, we evaluate the different training criteria in the context of several MT experiments. 3 Automatic Assessment of Translation Quality In recent years, various methods have been proposed to automatically evaluate machine translation quality by comparing hypothesis translations with reference translations. Examples of such methods are word error rate, position-independent word error rate (Tillmann et al., 1997), generation string accuracy (Bangalore et al., 2000), multi-reference word error rate (Nießen et al., 2000), BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002). All these criteria try to approximate human assessment and often achieve an astonishing degree of correlation to human subjective evaluation of fluency and adequacy (Papineni et al., 2001; Doddington, 2002). In this paper, we use the following methods: multi-reference word error rate (mWER): When this method is used, the hypothesis translation is compared to various reference translations by computing the edit distance (minimum number of substitutions, insertions, deletions) between the hypothesis and the closest of the given reference translations. multi-reference position independent error</context>
<context position="22404" citStr="Doddington, 2002" startWordPosition="3668" endWordPosition="3669"> line optimization algorithm which can efficiently find the optimal solution along a line. We showed that this approach obtains significantly better results than using the MMI training criterion (with our method to define pseudoreferences) and that optimizing error rate as part of the training criterion helps to obtain better error rate on unseen test data. As a result, we expect that actual ’true’ translation quality is improved, as previous work has shown that for some evaluation criteria there is a correlation with human subjective evaluation of fluency and adequacy (Papineni et al., 2001; Doddington, 2002). However, the different evaluation criteria yield quite different results on our Chinese–English translation task and therefore we expect that not all of them correlate equally well to human translation quality. The following important questions should be answered in the future: How many parameters can be reliably estimated using unsmoothed minimum error rate criteria using a given development corpus size? We expect that directly optimizing error rate for many more parameters would lead to serious overfitting problems. Is it possible to optimize more parameters using the smoothed error rate c</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richhard O Duda</author>
<author>Peter E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>John Wiley,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="2589" citStr="Duda and Hart, 1973" startWordPosition="400" endWordPosition="403">tic evaluation criteria such as word error rate and BLEU. 2 Statistical Machine Translation with Log-linear Models Let us assume that we are given a source (‘French’) sentence , which is to be translated into a target (‘English’) sentence Among all possible target sentences, we will choose the sentence with the highest probability:1 Pr (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. The decision in Eq. 1 minimizes the number of decision errors. Hence, under a so-called zero-one loss function this decision rule is optimal (Duda and Hart, 1973). Note that using a different loss function—for example, one induced by the BLEU metric—a different decision rule would be optimal. &apos;The notational convention will be as follows. We use the symbol Pr to denote general probability distributions with (nearly) no specific assumptions. In contrast, for model-based probability distributions, we use the generic symbol . As the true probability distribution Pr is unknown, we have to develop a model that approximates Pr . We directly model the posterior probability Pr by using a log-linear model. In this framework, we have a set of feature functions .</context>
<context position="20650" citStr="Duda and Hart, 1973" startWordPosition="3385" endWordPosition="3388">etter results correspond to larger BLEU and NIST scores and to smaller error rates. Italic numbers refer to results for which the difference to the best result (indicated in bold) is not statistically significant. error criterion used in training mWER [%] mPER [%] BLEU [%] NIST # words confidence intervals +/- 2.7 +/- 1.9 +/- 0.8 +/- 0.12 - MMI 68.0 51.0 11.3 5.76 21933 mWER 68.3 50.2 13.5 6.28 22914 smoothed-mWER 68.2 50.2 13.2 6.27 22902 mPER 70.2 49.8 15.2 6.71 24399 smoothed-mPER 70.0 49.7 15.2 6.69 24198 BLEU 76.1 53.2 17.2 6.66 28002 NIST 73.3 51.5 16.4 6.80 26602 recognition community (Duda and Hart, 1973; Juang et al., 1995; Schl¨uter and Ney, 2001). Paciorek and Rosenfeld (2000) use minimum classification error training for optimizing parameters of a whole-sentence maximum entropy language model. A technically very different approach that has a similar goal is the minimum Bayes risk approach, in which an optimal decision rule with respect to an application specific risk/loss function is used, which will normally differ from Eq. 3. The loss function is either identical or closely related to the final evaluation criterion. In contrast to the approach presented in this paper, the training crite</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>Richhard O. Duda and Peter E. Hart. 1973. Pattern Classification and Scene Analysis. John Wiley, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL,</booktitle>
<pages>177--183</pages>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="21453" citStr="Goodman, 1996" startWordPosition="3514" endWordPosition="3515">e model. A technically very different approach that has a similar goal is the minimum Bayes risk approach, in which an optimal decision rule with respect to an application specific risk/loss function is used, which will normally differ from Eq. 3. The loss function is either identical or closely related to the final evaluation criterion. In contrast to the approach presented in this paper, the training criterion and the statistical models used remain unchanged in the minimum Bayes risk approach. In the field of natural language processing this approach has been applied for example in parsing (Goodman, 1996) and word alignment (Kumar and Byrne, 2002). 9 Conclusions We presented alternative training criteria for loglinear statistical machine translation models which are directly related to translation quality: an unsmoothed error count and a smoothed error count on a development corpus. For the unsmoothed error count, we presented a new line optimization algorithm which can efficiently find the optimal solution along a line. We showed that this approach obtains significantly better results than using the MMI training criterion (with our method to define pseudoreferences) and that optimizing error </context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing algorithms and metrics. In Proceedings of the 34th Annual Meeting of the ACL, pages 177–183, Santa Cruz, CA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B H Juang</author>
<author>W Chou</author>
<author>C H Lee</author>
</authors>
<title>Statistical and discriminative methods for speech recognition. In</title>
<date>1995</date>
<booktitle>Speech Recognition and Coding - New Advances and Trends.</booktitle>
<editor>A. J. Rubio Ayuso and J. M. Lopez Soler, editors,</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="8892" citStr="Juang et al., 1995" startWordPosition="1415" endWordPosition="1418">idual sentences: . Our goal is to obtain a minimal error count on a representative corpus with given reference translations and a set of different candidate translations for each input sentence . In the extreme case, for , Eq. 7 converges to the unsmoothed criterion of Eq. 5 (except in the case of ties). Note, that the resulting objective function might still have local optima, which makes the optimization hard compared to using the objective function of Eq. 4 which does not have different local optima. The use of this type of smoothed error count is a common approach in the speech community (Juang et al., 1995; Schl¨uter and Ney, 2001). Figure 1 shows the actual shape of the smoothed and the unsmoothed error count for two parameters in our translation system. We see that the unsmoothed error count has many different local optima and is very unstable. The smoothed error count is much more stable and has fewer local optima. But (5) as we show in Section 7, the performance on our task obtained with the smoothed error count does not differ significantly from that obtained with the unsmoothed error count. (7) (6) The above stated optimization criterion is not easy to handle: 5 Optimization Algorithm for</context>
<context position="20670" citStr="Juang et al., 1995" startWordPosition="3389" endWordPosition="3392">ond to larger BLEU and NIST scores and to smaller error rates. Italic numbers refer to results for which the difference to the best result (indicated in bold) is not statistically significant. error criterion used in training mWER [%] mPER [%] BLEU [%] NIST # words confidence intervals +/- 2.7 +/- 1.9 +/- 0.8 +/- 0.12 - MMI 68.0 51.0 11.3 5.76 21933 mWER 68.3 50.2 13.5 6.28 22914 smoothed-mWER 68.2 50.2 13.2 6.27 22902 mPER 70.2 49.8 15.2 6.71 24399 smoothed-mPER 70.0 49.7 15.2 6.69 24198 BLEU 76.1 53.2 17.2 6.66 28002 NIST 73.3 51.5 16.4 6.80 26602 recognition community (Duda and Hart, 1973; Juang et al., 1995; Schl¨uter and Ney, 2001). Paciorek and Rosenfeld (2000) use minimum classification error training for optimizing parameters of a whole-sentence maximum entropy language model. A technically very different approach that has a similar goal is the minimum Bayes risk approach, in which an optimal decision rule with respect to an application specific risk/loss function is used, which will normally differ from Eq. 3. The loss function is either identical or closely related to the final evaluation criterion. In contrast to the approach presented in this paper, the training criterion and the statist</context>
</contexts>
<marker>Juang, Chou, Lee, 1995</marker>
<rawString>B. H. Juang, W. Chou, and C. H. Lee. 1995. Statistical and discriminative methods for speech recognition. In A. J. Rubio Ayuso and J. M. Lopez Soler, editors, Speech Recognition and Coding - New Advances and Trends. Springer Verlag, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum bayes-risk alignment of bilingual texts.</title>
<date>2002</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="21496" citStr="Kumar and Byrne, 2002" startWordPosition="3519" endWordPosition="3522">nt approach that has a similar goal is the minimum Bayes risk approach, in which an optimal decision rule with respect to an application specific risk/loss function is used, which will normally differ from Eq. 3. The loss function is either identical or closely related to the final evaluation criterion. In contrast to the approach presented in this paper, the training criterion and the statistical models used remain unchanged in the minimum Bayes risk approach. In the field of natural language processing this approach has been applied for example in parsing (Goodman, 1996) and word alignment (Kumar and Byrne, 2002). 9 Conclusions We presented alternative training criteria for loglinear statistical machine translation models which are directly related to translation quality: an unsmoothed error count and a smoothed error count on a development corpus. For the unsmoothed error count, we presented a new line optimization algorithm which can efficiently find the optimal solution along a line. We showed that this approach obtains significantly better results than using the MMI training criterion (with our method to define pseudoreferences) and that optimizing error rate as part of the training criterion help</context>
</contexts>
<marker>Kumar, Byrne, 2002</marker>
<rawString>Shankar Kumar and William Byrne. 2002. Minimum bayes-risk alignment of bilingual texts. In Proc. of the Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz J Och</author>
<author>G Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>An evaluation tool for machine translation: Fast evaluation for machine translation research.</title>
<date>2000</date>
<booktitle>In Proc. of the Second Int. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<pages>39--45</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="5412" citStr="Nießen et al., 2000" startWordPosition="840" endWordPosition="843">ch to compute the candidate translations that are the basis for our training procedure. In Section 7, we evaluate the different training criteria in the context of several MT experiments. 3 Automatic Assessment of Translation Quality In recent years, various methods have been proposed to automatically evaluate machine translation quality by comparing hypothesis translations with reference translations. Examples of such methods are word error rate, position-independent word error rate (Tillmann et al., 1997), generation string accuracy (Bangalore et al., 2000), multi-reference word error rate (Nießen et al., 2000), BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002). All these criteria try to approximate human assessment and often achieve an astonishing degree of correlation to human subjective evaluation of fluency and adequacy (Papineni et al., 2001; Doddington, 2002). In this paper, we use the following methods: multi-reference word error rate (mWER): When this method is used, the hypothesis translation is compared to various reference translations by computing the edit distance (minimum number of substitutions, insertions, deletions) between the hypothesis and the closest of the given</context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz J. Och, G. Leusch, and Hermann Ney. 2000. An evaluation tool for machine translation: Fast evaluation for machine translation research. In Proc. of the Second Int. Conf. on Language Resources and Evaluation (LREC), pages 39–45, Athens, Greece, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL),</booktitle>
<location>Philadelphia, PA,</location>
<contexts>
<context position="12835" citStr="Och and Ney, 2002" startWordPosition="2079" endWordPosition="2082">us, the complete set of interval boundaries and error count changes on the whole corpus are obtained. The optimal can now be computed easily by traversing the sequence of interval boundaries while updating an error count. It is straightforward to refine this algorithm to also handle the BLEU and NIST scores instead of sentence-level error counts by accumulating the relevant statistics for computing these scores (n-gram precision, translation length and reference length) . 6 Baseline Translation Approach The basic feature functions of our model are identical to the alignment template approach (Och and Ney, 2002). In this translation model, a sentence is translated by segmenting the input sentence into phrases, translating these phrases and reordering the translations in the target language. In addition to the feature functions described in (Och and Ney, 2002), our system includes a phrase penalty (the number of alignment templates used) and special alignment features. Altogether, the log-linear model includes different features. Note that many of the used feature functions are derived from probabilistic models: the feature function is defined as the negative logarithm of the corresponding probabilist</context>
<context position="19807" citStr="Och and Ney (2002)" startWordPosition="3240" endWordPosition="3243"> differences in the translation scores is due to the different translation length (last column in Table 3). The mWER and MMI criteria prefer shorter translations which are heavily penalized by the BLEU and NIST brevity penalty. We observe that the smoothed error count gives almost identical results to the unsmoothed error count. This might be due to the fact that the number of parameters trained is small and no serious overfitting occurs using the unsmoothed error count. 8 Related Work The use of log-linear models for statistical machine translation was suggested by Papineni et al. (1997) and Och and Ney (2002). The use of minimum classification error training and using a smoothed error count is common in the pattern recognition and speech Table 3: Effect of different error criteria used in training on the test corpus. Note that better results correspond to larger BLEU and NIST scores and to smaller error rates. Italic numbers refer to results for which the difference to the best result (indicated in bold) is not statistically significant. error criterion used in training mWER [%] mPER [%] BLEU [%] NIST # words confidence intervals +/- 2.7 +/- 1.9 +/- 0.8 +/- 0.12 - MMI 68.0 51.0 11.3 5.76 21933 mWE</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL), Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<institution>University of Maryland,</institution>
<location>College Park, MD,</location>
<contexts>
<context position="13755" citStr="Och et al., 1999" startWordPosition="2218" endWordPosition="2221">of alignment templates used) and special alignment features. Altogether, the log-linear model includes different features. Note that many of the used feature functions are derived from probabilistic models: the feature function is defined as the negative logarithm of the corresponding probabilistic model. Therefore, the feature functions are much more ’informative’ than for instance the binary feature functions used in standard maximum entropy models in natural language processing. For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al., 1999) and extractbest candidate translations using A* search (Ueffing et al., 2002). Using an-best approximation, we might face the problem that the parameters trained are good for the list oftranslations used, but yield worse translation results if these parameters are used in the dynamic programming search. Hence, it is possible that our new search produces translations with more errors on the training corpus. This can happen because with the modified model scaling factors the -best list can change significantly and can include sentences not in the existing-best list. To avoid this problem, we ad</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz J. Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28, University of Maryland, College Park, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Paciorek</author>
<author>Roni Rosenfeld</author>
</authors>
<title>Minimum classification error training in exponential language models.</title>
<date>2000</date>
<booktitle>In NIST/DARPA Speech Transcription Workshop,</booktitle>
<contexts>
<context position="20727" citStr="Paciorek and Rosenfeld (2000)" startWordPosition="3397" endWordPosition="3400">ler error rates. Italic numbers refer to results for which the difference to the best result (indicated in bold) is not statistically significant. error criterion used in training mWER [%] mPER [%] BLEU [%] NIST # words confidence intervals +/- 2.7 +/- 1.9 +/- 0.8 +/- 0.12 - MMI 68.0 51.0 11.3 5.76 21933 mWER 68.3 50.2 13.5 6.28 22914 smoothed-mWER 68.2 50.2 13.2 6.27 22902 mPER 70.2 49.8 15.2 6.71 24399 smoothed-mPER 70.0 49.7 15.2 6.69 24198 BLEU 76.1 53.2 17.2 6.66 28002 NIST 73.3 51.5 16.4 6.80 26602 recognition community (Duda and Hart, 1973; Juang et al., 1995; Schl¨uter and Ney, 2001). Paciorek and Rosenfeld (2000) use minimum classification error training for optimizing parameters of a whole-sentence maximum entropy language model. A technically very different approach that has a similar goal is the minimum Bayes risk approach, in which an optimal decision rule with respect to an application specific risk/loss function is used, which will normally differ from Eq. 3. The loss function is either identical or closely related to the final evaluation criterion. In contrast to the approach presented in this paper, the training criterion and the statistical models used remain unchanged in the minimum Bayes ri</context>
</contexts>
<marker>Paciorek, Rosenfeld, 2000</marker>
<rawString>Chris Paciorek and Roni Rosenfeld. 2000. Minimum classification error training in exponential language models. In NIST/DARPA Speech Transcription Workshop, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore A Papineni</author>
<author>Salim Roukos</author>
<author>R T Ward</author>
</authors>
<title>Feature-based language understanding.</title>
<date>1997</date>
<booktitle>In European Conf. on Speech Communication and Technology,</booktitle>
<pages>1435--1438</pages>
<location>Rhodes, Greece,</location>
<contexts>
<context position="19784" citStr="Papineni et al. (1997)" startWordPosition="3235" endWordPosition="3238">s. An important part of the differences in the translation scores is due to the different translation length (last column in Table 3). The mWER and MMI criteria prefer shorter translations which are heavily penalized by the BLEU and NIST brevity penalty. We observe that the smoothed error count gives almost identical results to the unsmoothed error count. This might be due to the fact that the number of parameters trained is small and no serious overfitting occurs using the unsmoothed error count. 8 Related Work The use of log-linear models for statistical machine translation was suggested by Papineni et al. (1997) and Och and Ney (2002). The use of minimum classification error training and using a smoothed error count is common in the pattern recognition and speech Table 3: Effect of different error criteria used in training on the test corpus. Note that better results correspond to larger BLEU and NIST scores and to smaller error rates. Italic numbers refer to results for which the difference to the best result (indicated in bold) is not statistically significant. error criterion used in training mWER [%] mPER [%] BLEU [%] NIST # words confidence intervals +/- 2.7 +/- 1.9 +/- 0.8 +/- 0.12 - MMI 68.0 5</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1997</marker>
<rawString>Kishore A. Papineni, Salim Roukos, and R. T. Ward. 1997. Feature-based language understanding. In European Conf. on Speech Communication and Technology, pages 1435–1438, Rhodes, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore A Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<location>Center, Yorktown Heights, NY,</location>
<contexts>
<context position="5448" citStr="Papineni et al., 2001" startWordPosition="846" endWordPosition="850">ations that are the basis for our training procedure. In Section 7, we evaluate the different training criteria in the context of several MT experiments. 3 Automatic Assessment of Translation Quality In recent years, various methods have been proposed to automatically evaluate machine translation quality by comparing hypothesis translations with reference translations. Examples of such methods are word error rate, position-independent word error rate (Tillmann et al., 1997), generation string accuracy (Bangalore et al., 2000), multi-reference word error rate (Nießen et al., 2000), BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002). All these criteria try to approximate human assessment and often achieve an astonishing degree of correlation to human subjective evaluation of fluency and adequacy (Papineni et al., 2001; Doddington, 2002). In this paper, we use the following methods: multi-reference word error rate (mWER): When this method is used, the hypothesis translation is compared to various reference translations by computing the edit distance (minimum number of substitutions, insertions, deletions) between the hypothesis and the closest of the given reference translations. multi-refer</context>
<context position="22385" citStr="Papineni et al., 2001" startWordPosition="3664" endWordPosition="3667">unt, we presented a new line optimization algorithm which can efficiently find the optimal solution along a line. We showed that this approach obtains significantly better results than using the MMI training criterion (with our method to define pseudoreferences) and that optimizing error rate as part of the training criterion helps to obtain better error rate on unseen test data. As a result, we expect that actual ’true’ translation quality is improved, as previous work has shown that for some evaluation criteria there is a correlation with human subjective evaluation of fluency and adequacy (Papineni et al., 2001; Doddington, 2002). However, the different evaluation criteria yield quite different results on our Chinese–English translation task and therefore we expect that not all of them correlate equally well to human translation quality. The following important questions should be answered in the future: How many parameters can be reliably estimated using unsmoothed minimum error rate criteria using a given development corpus size? We expect that directly optimizing error rate for many more parameters would lead to serious overfitting problems. Is it possible to optimize more parameters using the sm</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore A. Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore A Papineni</author>
</authors>
<title>Discriminative training via linear programming.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 IEEE International Conference on Acoustics, Speech &amp; Signal Processing,</booktitle>
<location>Atlanta,</location>
<contexts>
<context position="11541" citStr="Papineni, 1999" startWordPosition="1869" endWordPosition="1870">g, we describe a new algorithm for efficient line optimization of the unsmoothed error count (Eq. 5) using a log-linear model (Eq. 3) which is guaranteed to find the optimal solution. The new algorithm is much faster and more stable than the grid-based line optimization method. Computing the most probable sentence out of a set of candidate translation (see Eq. 6) along a line with parameter results in an optimization problem of the following functional form: Here, and are constants with respect to . Hence, every candidate translation in corresponds to a line. The function is piecewise linear (Papineni, 1999). This allows us to compute an efficient exhaustive representation of that function. In the following, we sketch the new algorithm to optimize Eq. 5: We compute the ordered sequence of linear intervals constituting for every sentence together with the incremental change in error count from the previous to the next interval. Hence, we obtain for every sentence a sequence which denote the interval boundaries and a corresponding sequence for the change in error count involved at the corresponding interval boundary Here, denotes the change in the error count at . position to the error count at pos</context>
</contexts>
<marker>Papineni, 1999</marker>
<rawString>Kishore A. Papineni. 1999. Discriminative training via linear programming. In Proceedings of the 1999 IEEE International Conference on Acoustics, Speech &amp; Signal Processing, Atlanta, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes in C++.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="9689" citStr="Press et al., 2002" startWordPosition="1551" endWordPosition="1554">d error count has many different local optima and is very unstable. The smoothed error count is much more stable and has fewer local optima. But (5) as we show in Section 7, the performance on our task obtained with the smoothed error count does not differ significantly from that obtained with the unsmoothed error count. (7) (6) The above stated optimization criterion is not easy to handle: 5 Optimization Algorithm for Unsmoothed Error Count A standard algorithm for the optimization of the unsmoothed error count (Eq. 5) is Powells algorithm combined with a grid-based line optimization method (Press et al., 2002). We start at a random point in the -dimensional parameter space with 9450 error count 9440 9430 9450 9445 9440 9435 error count 9430 9425 9420 9415 9410 9405 unsmoothed error count smoothed error rate (alpha=3) -4 -3 -2 -1 0 1 2 3 4 unsmoothed error count smoothed error rate (alpha=3) -4 -3 -2 -1 0 1 2 3 4 9480 9470 9460 9420 9410 9400 Figure 1: Shape of error count and smoothed error count for two different model parameters. These curves have been computed on the development corpus (see Section 7, Table 1) using alternatives per source sentence. The smoothed error count has been computed wit</context>
<context position="17937" citStr="Press et al., 2002" startWordPosition="2919" endWordPosition="2922">ual lexicon (Lex), development corpus (Dev), test corpus (Test). Chinese English Train Sentences 5 109 Words 89 121 111 251 Singletons 3 419 4 130 Vocabulary 8 088 8 807 Lex Entries 82 103 Dev Sentences 640 Words 11 746 13 573 Test Sentences 878 Words 24 323 26 489 on the test corpus. Italic numbers refer to results for which the difference to the best result (indicated in bold) is not statistically significant. For all error rates, we show the maximal occurring 95% confidence interval in any of the experiments for that column. The confidence intervals are computed using bootstrap resampling (Press et al., 2002). The last column provides the number of words in the produced translations which can be compared with the average number of reference words occurring in the development and test corpora given in Table 1. We observe that if we choose a certain error criterion in training, we obtain in most cases the best results using the same criterion as the evaluation metric on the test data. The differences can be quite large: If we optimize with respect to word error rate, the results are mWER=68.3%, which is better than if we optimize with respect to BLEU or NIST and the difference is statistically signi</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 2002</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 2002. Numerical Recipes in C++. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Schl¨uter</author>
<author>Hermann Ney</author>
</authors>
<title>Model-based MCE bound to the true Bayes’ error.</title>
<date>2001</date>
<journal>IEEE Signal Processing Letters,</journal>
<volume>8</volume>
<issue>5</issue>
<marker>Schl¨uter, Ney, 2001</marker>
<rawString>Ralf Schl¨uter and Hermann Ney. 2001. Model-based MCE bound to the true Bayes’ error. IEEE Signal Processing Letters, 8(5):131–133, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Alex Zubiaga</author>
<author>Hassan Sawaf</author>
</authors>
<title>Accelerated DP based search for statistical translation.</title>
<date>1997</date>
<booktitle>In European Conf. on Speech Communication and Technology,</booktitle>
<pages>2667--2670</pages>
<location>Rhodes, Greece,</location>
<contexts>
<context position="5304" citStr="Tillmann et al., 1997" startWordPosition="824" endWordPosition="827">ently optimizes an unsmoothed error count. In Section 6, we describe the used feature functions and our approach to compute the candidate translations that are the basis for our training procedure. In Section 7, we evaluate the different training criteria in the context of several MT experiments. 3 Automatic Assessment of Translation Quality In recent years, various methods have been proposed to automatically evaluate machine translation quality by comparing hypothesis translations with reference translations. Examples of such methods are word error rate, position-independent word error rate (Tillmann et al., 1997), generation string accuracy (Bangalore et al., 2000), multi-reference word error rate (Nießen et al., 2000), BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002). All these criteria try to approximate human assessment and often achieve an astonishing degree of correlation to human subjective evaluation of fluency and adequacy (Papineni et al., 2001; Doddington, 2002). In this paper, we use the following methods: multi-reference word error rate (mWER): When this method is used, the hypothesis translation is compared to various reference translations by computing the edit distance </context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>Christoph Tillmann, Stephan Vogel, Hermann Ney, Alex Zubiaga, and Hassan Sawaf. 1997. Accelerated DP based search for statistical translation. In European Conf. on Speech Communication and Technology, pages 2667–2670, Rhodes, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Generation of word graphs in statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. Conference on Empirical Methods for Natural Language Processing,</booktitle>
<pages>156--163</pages>
<location>Philadelphia, PE,</location>
<contexts>
<context position="13833" citStr="Ueffing et al., 2002" startWordPosition="2230" endWordPosition="2233">he log-linear model includes different features. Note that many of the used feature functions are derived from probabilistic models: the feature function is defined as the negative logarithm of the corresponding probabilistic model. Therefore, the feature functions are much more ’informative’ than for instance the binary feature functions used in standard maximum entropy models in natural language processing. For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al., 1999) and extractbest candidate translations using A* search (Ueffing et al., 2002). Using an-best approximation, we might face the problem that the parameters trained are good for the list oftranslations used, but yield worse translation results if these parameters are used in the dynamic programming search. Hence, it is possible that our new search produces translations with more errors on the training corpus. This can happen because with the modified model scaling factors the -best list can change significantly and can include sentences not in the existing-best list. To avoid this problem, we adopt the following solution: First, we perform search (using a manually defined</context>
</contexts>
<marker>Ueffing, Och, Ney, 2002</marker>
<rawString>Nicola Ueffing, Franz Josef Och, and Hermann Ney. 2002. Generation of word graphs in statistical machine translation. In Proc. Conference on Empirical Methods for Natural Language Processing, pages 156–163, Philadelphia, PE, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>